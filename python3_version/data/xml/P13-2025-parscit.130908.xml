<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008073">
<title confidence="0.998321">
On the Predictability of Human Assessment: when Matrix Completion
Meets NLP Evaluation
</title>
<author confidence="0.911805">
Guillaume Wisniewski
</author>
<affiliation confidence="0.586388">
Universit´e Paris Sud
</affiliation>
<note confidence="0.744602">
LIMSI–CNRS
</note>
<address confidence="0.685054">
Orsay, France
</address>
<email confidence="0.992026">
guillaume.wisniewski@limsi.fr
</email>
<sectionHeader confidence="0.993685" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999788066666667">
This paper tackles the problem of collect-
ing reliable human assessments. We show
that knowing multiple scores for each ex-
ample instead of a single score results in
a more reliable estimation of a system
quality. To reduce the cost of collect-
ing these multiple ratings, we propose to
use matrix completion techniques to pre-
dict some scores knowing only scores of
other judges and some common ratings.
Even if prediction performance is pretty
low, decisions made using the predicted
score proved to be more reliable than de-
cision based on a single rating of each ex-
ample.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999822649122807">
Human assessment is often considered as the best,
if not the only, way to evaluate ‘subjective’ NLP
tasks like MT or speech generation. However,
human evaluations are doomed to be noisy and,
sometimes, even contradictory as they depend on
individual perception and understanding of the
score scale that annotators generally use in re-
markably different ways (Koehn and Monz, 2006).
Moreover, annotation is known to be a long and
frustrating process and annotator fatigue has been
identified as another source of noise (Pighin et al.,
2012).
In addition to defining and enforcing stricter
guidelines, several solutions have been proposed
to reduce the annotation effort and produce more
reliable ratings. For instance, to limit the impact
of the score scale interpretation, in the WMT eval-
uation campaign (Callison-Burch et al., 2012), an-
notators are asked to rank translation hypotheses
from best to worst instead of providing absolute
scores (e.g. in terms of adequacy or fluency). Gen-
eralizing this approach, several works (Pighin et
al., 2012; Lopez, 2012) have defined novel annota-
tion protocols to reduce the number of judgments
that need to be collected. However, all these meth-
ods suffer from several limitations: first, they pro-
vide no interpretable information about the quality
of the system (only a relative comparison between
two systems is possible); second, (Koehn, 2012)
has recently shown that the ranking they induce is
not reliable.
In this work, we study an alternative approach
to the problem of collecting reliable human as-
sessments. Our basic assumption, motivated by
the success of ensemble methods, is that hav-
ing several judgments for each example, even if
they are noisy, will result in a more reliable de-
cision than having a single judgment. An evalu-
ation campaign should therefore aim at gathering
a score matrix, in which each example is rated by
all judges instead of having each judge rate only
a small subset of examples, thereby minimizing
redundancy. Obviously, the former approach re-
quires a large annotation effort and is, in practice,
not feasible. That is why, to reduce the number
of judgments that must be collected, we propose
to investigate the possibility of using matrix com-
pletion techniques to recover the entire score ma-
trix from a sample of its entries. The question
we try to answer is whether the missing scores of
one judge can be predicted knowing only scores of
other judges and some shared ratings.
The contributions of this paper are twofold: i)
we show how knowing the full score matrix in-
stead of a single score for each example provides a
more reliable estimation of a system quality (Sec-
tion 3); ii) we present preliminary experiments
</bodyText>
<page confidence="0.970884">
137
</page>
<note confidence="0.529624">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 137–142,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.892512" genericHeader="introduction">
3 Corpora
</sectionHeader>
<bodyText confidence="0.999882142857143">
showing that missing data techniques can be used
to recover the score matrix from a sample of its
entries despite the low inter-rater agreement (Sec-
tion 4).
For our experiments we considered two publicly
available corpora in which multiple human ratings
(i.e. scores on an ordinal scale) were available.
</bodyText>
<sectionHeader confidence="0.964925" genericHeader="method">
2 Matrix Completion
</sectionHeader>
<bodyText confidence="0.992395294117647">
The recovering of a matrix from a sampling of its
entries is a task of considerable interest (Cand`es
and Recht, 2012). It can be used, for instance, in
recommender systems: rows of the matrix repre-
sent users that are rating movies (columns of the
matrix); the resulting matrix is mostly unknown
(each user only rates a few movies) and the task
consists in completing the matrix so that movies
that any user is likely to like can be predicted.
Matrix completion generally relies on the low
rank hypothesis: because of hidden factors be-
tween the observations (the columns of the ma-
trix), the matrix has a low rank. For instance,
in recommender systems it is commonly believed
that only a few factors contribute to an individual’s
tastes. Formally, recovering a matrix M amounts
at solving:
</bodyText>
<equation confidence="0.890764333333333">
minimize rank X
(1)
subject to Xid = Mid (i, j) ∈ Q
</equation>
<bodyText confidence="0.999504315789474">
where X is the decision variable and Q is the set of
known entries. This optimization problem seeks
the simplest explanation fitting the observed data.
Solving the rank minimization problem has
been proved to be NP-hard (Chistov and
Grigor’ev, 1984). However several convex relax-
ations of this program have been proposed. In
this work, we will consider the relaxation of the
rank by the nuclear norm1 that can be efficiently
solved by semidefinite programming (Becker et
al., 2011). This relaxation enjoys many theoret-
ical guarantees with respect to the optimality of
its solution (under mild assumptions its solution is
also the solution of the original problem), the con-
ditions under which the matrix can be recovered
and the number of entries that must be sampled
to recover the original matrix. In our experiments
we used TFOCS,2 a free implementation of this
method.
</bodyText>
<footnote confidence="0.9899595">
1The nuclear norm of a matrix is the sum of its singular
values; the relation between rank an nuclear norm is similar
to the one between f0 and f, norms.
2http://cvxr.com/tfocs/
</footnote>
<bodyText confidence="0.999916609756098">
The CE Corpus The first corpus of human judg-
ments we have considered has been collected
for the WMT12 shared task on quality estima-
tion (Callison-Burch et al., 2012).3 The data set is
made of 2,254 English sentences and their auto-
matic translations in Spanish predicted by a stan-
dard Moses system. Each sentence pair is accom-
panied by three estimates in the range 1 to 5 of
its translation quality expressed in terms of post-
editing effort. These human grades are in the range
1 to 5, the latter standing for a very good trans-
lation that hardly requires post-editing, while the
former identifies very poor automatic translations
that are not deemed to be worth the post-editing
effort.
As pointed out by the task organizers, despite
the special care that was taken to ensure the quality
of the data, the inter-raters agreement was much
lower than what is typically observed in NLP
tasks (Artstein and Poesio, 2008): the weighted
n ranged from 0.39 to 0.50 depending on the pair
of annotators considered4; the Fleiss coefficient (a
generalization of n to multi-raters) was 0.25 and
the Kendall τb correlation coefficient5 between
0.64 and 0.68, meaning that, on average, two raters
do not agree on the relative order of two transla-
tions almost two out of five times. In fact, as of-
ten observed for the sentence level human evalua-
tion of MT outputs, the different judges have used
the score scale differently: the second judge had
a clear tendency to give more ‘medium’ scores
than the others, and the variance of her scores
was low. Because theirs distributions are differ-
ent, standardizing the scores has only a very lim-
ited impact on the agreement.
If, as in many manual evaluations, each exam-
ple had been rated by a single judge chosen ran-
domly, the resulting scores would have been only
moderately correlated with the average of the three
scores which is, intuitively, a better estimate of the
‘true’ quality: the 95% confidence interval of the
</bodyText>
<footnote confidence="0.999383666666667">
3The corpus is available from http://www.statmt.
org/wmt12/quality-estimation-task.html
4The weighted κ is a generalization of the κ to ordinal
data; a linear weighting schema was used.
5Note that, in statistics, agreement is a stronger notion
than correlation, as the former compare the actual values.
</footnote>
<page confidence="0.995462">
138
</page>
<bodyText confidence="0.999991114285714">
τb between the averaged scores and the ‘sampled’
score is 0.754–0.755.
TIDES The second corpus considered was col-
lected for the DARPA TIDES program: a team of
human judges provided multiple assessments of
adequacy and fluency for Arabic to English and
Chinese to English automatic translations.6 For
space reasons, only results on the Chinese to En-
glish fluency corpus will be presented; similar re-
sults were achieved on the other corpora.
In the considered corpus, 31 sets of automatic
translations, generated by three systems, have
been rated by two judges on a scale of 1 to 5. The
inter-rater agreement is very low: depending on
the pair of judges, the weighted κ is between -0.05
and 0.2, meaning that agreement occurs less of-
ten than predicted by chance alone. More impor-
tantly, if the ratings of a pair of judges were used
to decide which is the best system among two, the
two judges will disagree 36% of the time. This
‘agreement’ score is computed as follows: if mA,i
is the mean of the scores given to system A by
the i-th annotator, we say that there is no agree-
ment in a pairwise comparison if mA,i &gt; mB,i
and mA,j &lt; mB,j, i.e. if two judges rank two sys-
tems in a different order; the score is then the per-
centage of agreement when considering all pairs
of systems and judges.
Considering the full scoring matrix instead of
single scores has a large impact: if each example is
rated by a single judge (chosen randomly), the re-
sulting comparison between the two systems will
be different from the decision made by averaging
the two scores of the full score matrix in almost
20% of the comparisons.
</bodyText>
<sectionHeader confidence="0.999748" genericHeader="method">
4 Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.999863">
4.1 Testing the Low-Rank Hypothesis
</subsectionHeader>
<bodyText confidence="0.9988238">
Matrix completion relies on the hypothesis that
the matrix has a low rank. We first propose to
test this hypothesis on simulated data, using a
method similar to the one proposed in (Mathet
et al., 2012), to evaluate the impact of noise in
human judgments on the score matrix rank. Ar-
tificial ratings are generated as follows: a MT
system is producing n translations the quality of
which, qi, is estimated by a continuous value,
that represents, for instance, a hTER score. This
</bodyText>
<footnote confidence="0.779913">
6These corpora are available from LDC under the refer-
ences ldc2003t17 and ldc2003t18
</footnote>
<bodyText confidence="0.982752642857143">
value is drawn from N (µ, σ2). Based on this
‘intrinsic’ quality, two ratings, ai and bi, are
generated according to three strategies: in the
first, ai and bi are sampled from N (qi, 0); in
the second, ai ∼ N (qi + θ2, σ02) and bi ∼
N (qi − θ2, σ02) and in the third, ai ∼ N (qi, σ02)
and the bi is drawn from a bimodal distribu-
tion 1 (N (qi − θ 2, σ02) + N (qi + θ 2, σ02)) (with
2
σ02 &lt; θ2). 0 describes the noise level.
Each of these strategies models a different kind
of noise that has been observed in different evalua-
tion campaigns (Koehn and Monz, 2006): the first
one describes random noise in the ratings; the sec-
ond a systematic difference in the annotators’ in-
terpretation of the score scale and the third, the sit-
uation in which one annotator gives medium score
while the other one tend to commit more strongly
to whether she considered the translation good or
bad. Stacking all these judgments results in a n×2
score matrix. To test whether this matrix has a low
rank or not, we assess how close it is to its ap-
proximation by a rank 1 matrix. A well-known
result (Lawson and Hanson, 1974) states that the
Frobenius norm of the difference of these matri-
ces is equal to the 2nd singular value of the orig-
inal matrix; the quality of the approximation can
thus be estimated by p, defined as the 2nd eigen-
value of the matrix normalized by its norm (Leon,
1994). Intuitively, the smaller p, the better the ap-
proximation.
Figure 1 represents the impact of the noise level
on the condition number. As a baseline, we have
also represented p for a random matrix. All values
are averaged over 100 simulations. As it could be
expected, p is close to 0 for small noise level; but
even for moderate noise level, the second eigen-
value continue to be small, suggesting that the ma-
trix can still be approximated by a matrix of rank 1
without much loss of information. As a compari-
son, on average, p = 0.08 for the CE score matrix,
in spite of the low inter-rater agreement.
</bodyText>
<subsectionHeader confidence="0.996379">
4.2 Prediction Performance
</subsectionHeader>
<bodyText confidence="0.999704">
We conducted several experiments to evaluate the
possibility to use matrix completion to recover a
score matrix. Experiments consist in choosing
randomly k% of the entries of a matrix; these en-
tries are considered unknown and predicted using
the method introduced in Section 2 denoted pred
in the following. In our experiments k varies from
10% to 40%. Note that, when, as in our exper-
</bodyText>
<page confidence="0.998535">
139
</page>
<figureCaption confidence="0.899093666666667">
Figure 1: Evolution of the condition number p
with the noise level 0 for the different strategies
(see text for details)
</figureCaption>
<bodyText confidence="0.999827529411765">
iments, only two judges are involved, k = 50%
would mean that each example is rated by a sin-
gle judge. Two simple methods for handling miss-
ing data are used as baselines: in the first one, de-
noted rand, missing scores are chosen randomly;
the second one, denoted mean, predicts for all the
missing scores of a judge the mean of her known
scores.
We propose to evaluate the quality of the recov-
ery, first by comparing the predicted score to their
true value and then by evaluating the decision that
will be made when considering the recovered ma-
trix instead of the full matrix.
Prediction Performance Comparing the com-
pleted matrix to the original score matrix can be
done in terms of Mean Absolute Error (MAE) de-
fined as N �i 1  |yi − ˆyi  |where ˆyi is the predicted
value and yi the corresponding ‘true’ value; the
sum runs over all unknown values of the matrix.
Table 1 presents the results achieved by the dif-
ferent methods. All reported results are averaged
over 10 runs (i.e.: sampling of the score matrix
and prediction of the missing scores) and over all
pairs of judges. All tables also report the 95% con-
fidence interval. The MAE of the rand method is
almost constant, whatever the number of samples
is. Performance of the matrix completion tech-
nique is not so good: predicted scores are quite
different than true scores. In particular, perfor-
mance falls quickly when the number of missing
data increases. This observation is not surprising:
when 40% of the scores are missing, only a few
examples have more than a single score and many
have no score at all. In these conditions recovering
</bodyText>
<table confidence="0.9990752">
missing data pred mean
40% 0.78 ±6.21 x 10−3 0.72 ±8.86 x 10−3
30% 0.83 ±3.19 x 10−3 0.80 ±5.42 x 10−3
20% 0.88 ±2.49 x 10−3 0.87 ±3.54 x 10−3
10% 0.93 ±1.76 x 10−3 0.92 ±1.51 x 10−3
</table>
<tableCaption confidence="0.998283">
Table 2: Correlation between the rankings induced
</tableCaption>
<bodyText confidence="0.981193142857143">
by the recovered matrix and the original score ma-
trix for the CE corpus
the matrix is almost impossible. The performance
of the simple mean technique is, comparatively,
pretty good, especially when only a few entries
are known. However, the pred method always
outperform the rand method showing that there
are dependencies between the two ratings even if
statistical measures of agreement are low.
Impact on the Decision The negative results of
the previous paragraph only provide indirect mea-
sure of the recovery quality as it is not the value of
the score that is important but the decision that it
will support. That is why, we also evaluated ma-
trix recovery in a more task-oriented way by com-
paring the decision made when considering the re-
covered score matrix instead of the ‘true’ score
matrix.
For the CE corpus, a task-oriented evaluation
can be done by comparing the rankings induced
by the recovered matrix and by the original matrix
when examples are ordered according to their av-
eraged score. Such a ranking can be used by a MT
user to set a quality threshold granting her con-
trol over translation quality (Soricut and Echihabi,
2010). Table 2 shows the correlation between the
two rankings as evaluated by Tb. The two rankings
appear to be highly correlated, the matrix comple-
tion technique outperforming slightly the mean
baseline. More importantly, even when 40% of
the data are missing, the ranking induced by the
true scores is better correlated to the ranking in-
duced by the predicted scores than to the ranking
induced when each example is only rated once: as
reported in Section 3, the Tb is, in this case, 0.75.
For the TIDES corpus, we computed the num-
ber of pairs of judges for which the results of a
pairwise comparison between two systems is dif-
ferent when the systems are evaluated using the
predicted scores and the true scores. Results pre-
sented in Table 3 show that considering the pre-
dicted matrix is far better than having judges rate
</bodyText>
<figure confidence="0.999690777777778">
0 0.1 0.2 0.3 0.4
0
ρ
0.4
random
0.3
0.2
0.1
0
</figure>
<footnote confidence="0.980271333333333">
1st strat.
2nd strat.
3rd strat.
</footnote>
<page confidence="0.982484">
140
</page>
<table confidence="0.9619985">
QE TIDES
k pred mean rand pred mean rand
40% 1.14 ±2.9 · 10−2 0.78 ±6.6 · 10−3 1.45 — — —
30% 0.94 ±2.9 · 10−2 0.78 ±7.4 · 10−3 1.44 0.95 ±2.7 · 10−2 0.43 ±2.6 · 10−2 1.37
20% 0.77 ±3.4 · 10−2 0.78 ±1.0 · 10−2 1.45 0.76 ±2.6 · 10−2 0.41 ±2.5 · 10−2 1.38
10% 0.65 ±2.1 · 10−2 0.79 ±1.9 · 10−2 1.47 0.48 ±3.0 · 10−2 0.41 ±2.5 · 10−2 1.36
</table>
<tableCaption confidence="0.946455">
Table 1: Completion performance as evaluated by the MAE for the three prediction methods and the
three corpora considered.
</tableCaption>
<bodyText confidence="0.999042545454545">
random samples of the examples: the number of
disagreement falls from 20% (Sect. 3) to less than
4%. While the mean method outperforms the
pred method, this result shows that, even in case
of low inter-rater agreement, there is still enough
information to predict the score of one annotator
knowing only the score of the others.
For the tasks considered, decisions based on a
recovered matrix are therefore more similar to de-
cisions made considering the full score matrix than
decisions based on a single rating of each example.
</bodyText>
<sectionHeader confidence="0.998835" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999985222222222">
This paper proposed a new way of collecting reli-
able human assessment. We showed, on two cor-
pora, that knowing multiple scores for each exam-
ple instead of a single score results in a more reli-
able estimation of the quality of a NLP system. We
proposed to used matrix completion techniques
to reduce the annotation effort required to collect
these multiple ratings. Our experiments showed
that while scores predicted using these techniques
are pretty different from the true scores, decisions
considering them are more reliable than decisions
based on a single score.
Even if it can not predict scores accurately, we
believe that the connection between NLP evalua-
tion and matrix completion has many potential ap-
plications. For instance, it can be applied to iden-
tify errors made when collecting scores by com-
paring the predicted and actual scores.
</bodyText>
<sectionHeader confidence="0.999514" genericHeader="acknowledgments">
6 Acknowledgments
</sectionHeader>
<bodyText confidence="0.9996724">
This work was partly supported by ANR project
Trace (ANR-09-CORD-023). The author would
like to thank Franc¸ois Yvon and Nicolas P´echeux
for their helpful questions and comments on the
various drafts of this work.
</bodyText>
<table confidence="0.99811975">
% missing data pred mean
30% 9.24% 3.53 %
20% 6.45% 2.10 %
10% 3.66% 1.20 %
</table>
<tableCaption confidence="0.993742">
Table 3: Disagreements in a pairwise comparison
</tableCaption>
<bodyText confidence="0.970135">
of two systems of the TIDES corpus, when the
systems are evaluated using the predicted scores
and the true scores
</bodyText>
<sectionHeader confidence="0.999027" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999852551724138">
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Comput.
Linguist., 34(4):555–596, December.
Stephen R. Becker, Emmanuel J. Cand`es, and
Michael C. Grant. 2011. Templates for convex cone
problems with applications to sparse signal recovery.
Math. Prog. Comput., 3(3):165–218.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proc. of WMT, pages 10–51,
Montr´eal, Canada, June. ACL.
Emmanuel Cand`es and Benjamin Recht. 2012. Exact
matrix completion via convex optimization. Com-
mun. ACM, 55(6):111–119, June.
A. Chistov and D. Grigor’ev. 1984. Complexity of
quantifier elimination in the theory of algebraically
closed fields. In M. Chytil and V. Koubek, editors,
Math. Found. of Comp. Science, volume 176, pages
17–31. Springer Berlin / Heidelberg.
Philipp Koehn and Christof Monz. 2006. Manual and
automatic evaluation of machine translation between
european languages. In Proc. WMT, pages 102–121,
New York City, June. ACL.
Philipp Koehn. 2012. Simulating human judgment in
machine translation evaluation campaigns. In Proc.
of IWSLT.
Charles L. Lawson and Richard J. Hanson. 1974. Solv-
ing Least Squares Problems. Prentice Hall.
</reference>
<page confidence="0.979209">
141
</page>
<reference confidence="0.999951333333333">
Stephen J: Leon. 1994. Linear Algebra with Applica-
tions. Macmillan,.
Adam Lopez. 2012. Putting human assessments of
machine translation systems in order. In Proc. of
WMT, pages 1–9, Montr´eal, Canada, June. ACL.
Yann Mathet, Antoine Widlcher, Kar¨en Fort, Claire
Franc¸ois, Olivier Galibert, Cyril Grouin, Juliette
Kahn, Sophie Rosset, and Pierre Zweigenbaum.
2012. Manual corpus annotation: Giving meaning
to the evaluation metrics. In Proceedings of COL-
ING 2012: Posters, pages 809–818, Mumbai, India,
December.
Daniele Pighin, Llu´ıs Formiga, and Llu´ıs M`arquez.
2012. A graph-based strategy to streamline trans-
lation quality assessments. In Proc. ofAMTA.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. Fluency, adequacy, or
HTER? Exploring different human judgments with
a tunable MT metric. In Proc. of WMT, pages 259–
268, Athens, Greece, March. ACL.
Radu Soricut and Abdessamad Echihabi. 2010.
Trustrank: Inducing trust in automatic translations
via ranking. In Proc. of ACL, pages 612–621, Upp-
sala, Sweden, July. ACL.
</reference>
<page confidence="0.997731">
142
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.707266">
<title confidence="0.9972285">On the Predictability of Human Assessment: when Matrix Meets NLP Evaluation</title>
<author confidence="0.986885">Guillaume</author>
<affiliation confidence="0.998434">Universit´e Paris</affiliation>
<address confidence="0.918824">Orsay,</address>
<email confidence="0.999114">guillaume.wisniewski@limsi.fr</email>
<abstract confidence="0.9861659375">This paper tackles the problem of collecting reliable human assessments. We show that knowing multiple scores for each example instead of a single score results in a more reliable estimation of a system quality. To reduce the cost of collecting these multiple ratings, we propose to use matrix completion techniques to predict some scores knowing only scores of other judges and some common ratings. Even if prediction performance is pretty low, decisions made using the predicted score proved to be more reliable than decision based on a single rating of each example.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ron Artstein</author>
<author>Massimo Poesio</author>
</authors>
<title>Inter-coder agreement for computational linguistics.</title>
<date>2008</date>
<journal>Comput. Linguist.,</journal>
<volume>34</volume>
<issue>4</issue>
<contexts>
<context position="6801" citStr="Artstein and Poesio, 2008" startWordPosition="1117" endWordPosition="1120">es system. Each sentence pair is accompanied by three estimates in the range 1 to 5 of its translation quality expressed in terms of postediting effort. These human grades are in the range 1 to 5, the latter standing for a very good translation that hardly requires post-editing, while the former identifies very poor automatic translations that are not deemed to be worth the post-editing effort. As pointed out by the task organizers, despite the special care that was taken to ensure the quality of the data, the inter-raters agreement was much lower than what is typically observed in NLP tasks (Artstein and Poesio, 2008): the weighted n ranged from 0.39 to 0.50 depending on the pair of annotators considered4; the Fleiss coefficient (a generalization of n to multi-raters) was 0.25 and the Kendall τb correlation coefficient5 between 0.64 and 0.68, meaning that, on average, two raters do not agree on the relative order of two translations almost two out of five times. In fact, as often observed for the sentence level human evaluation of MT outputs, the different judges have used the score scale differently: the second judge had a clear tendency to give more ‘medium’ scores than the others, and the variance of he</context>
</contexts>
<marker>Artstein, Poesio, 2008</marker>
<rawString>Ron Artstein and Massimo Poesio. 2008. Inter-coder agreement for computational linguistics. Comput. Linguist., 34(4):555–596, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen R Becker</author>
<author>Emmanuel J Cand`es</author>
<author>Michael C Grant</author>
</authors>
<title>Templates for convex cone problems with applications to sparse signal recovery.</title>
<date>2011</date>
<journal>Math. Prog. Comput.,</journal>
<volume>3</volume>
<issue>3</issue>
<marker>Becker, Cand`es, Grant, 2011</marker>
<rawString>Stephen R. Becker, Emmanuel J. Cand`es, and Michael C. Grant. 2011. Templates for convex cone problems with applications to sparse signal recovery. Math. Prog. Comput., 3(3):165–218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<title>Findings of the 2012 workshop on statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proc. of WMT,</booktitle>
<pages>10--51</pages>
<publisher>ACL.</publisher>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="1611" citStr="Callison-Burch et al., 2012" startWordPosition="248" endWordPosition="251">en contradictory as they depend on individual perception and understanding of the score scale that annotators generally use in remarkably different ways (Koehn and Monz, 2006). Moreover, annotation is known to be a long and frustrating process and annotator fatigue has been identified as another source of noise (Pighin et al., 2012). In addition to defining and enforcing stricter guidelines, several solutions have been proposed to reduce the annotation effort and produce more reliable ratings. For instance, to limit the impact of the score scale interpretation, in the WMT evaluation campaign (Callison-Burch et al., 2012), annotators are asked to rank translation hypotheses from best to worst instead of providing absolute scores (e.g. in terms of adequacy or fluency). Generalizing this approach, several works (Pighin et al., 2012; Lopez, 2012) have defined novel annotation protocols to reduce the number of judgments that need to be collected. However, all these methods suffer from several limitations: first, they provide no interpretable information about the quality of the system (only a relative comparison between two systems is possible); second, (Koehn, 2012) has recently shown that the ranking they induce</context>
<context position="6053" citStr="Callison-Burch et al., 2012" startWordPosition="988" endWordPosition="991"> mild assumptions its solution is also the solution of the original problem), the conditions under which the matrix can be recovered and the number of entries that must be sampled to recover the original matrix. In our experiments we used TFOCS,2 a free implementation of this method. 1The nuclear norm of a matrix is the sum of its singular values; the relation between rank an nuclear norm is similar to the one between f0 and f, norms. 2http://cvxr.com/tfocs/ The CE Corpus The first corpus of human judgments we have considered has been collected for the WMT12 shared task on quality estimation (Callison-Burch et al., 2012).3 The data set is made of 2,254 English sentences and their automatic translations in Spanish predicted by a standard Moses system. Each sentence pair is accompanied by three estimates in the range 1 to 5 of its translation quality expressed in terms of postediting effort. These human grades are in the range 1 to 5, the latter standing for a very good translation that hardly requires post-editing, while the former identifies very poor automatic translations that are not deemed to be worth the post-editing effort. As pointed out by the task organizers, despite the special care that was taken t</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Post, Soricut, Specia, 2012</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2012. Findings of the 2012 workshop on statistical machine translation. In Proc. of WMT, pages 10–51, Montr´eal, Canada, June. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emmanuel Cand`es</author>
<author>Benjamin Recht</author>
</authors>
<title>Exact matrix completion via convex optimization.</title>
<date>2012</date>
<journal>Commun. ACM,</journal>
<volume>55</volume>
<issue>6</issue>
<marker>Cand`es, Recht, 2012</marker>
<rawString>Emmanuel Cand`es and Benjamin Recht. 2012. Exact matrix completion via convex optimization. Commun. ACM, 55(6):111–119, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Chistov</author>
<author>D Grigor’ev</author>
</authors>
<title>Complexity of quantifier elimination in the theory of algebraically closed fields.</title>
<date>1984</date>
<journal>Math. Found. of Comp. Science,</journal>
<volume>176</volume>
<pages>17--31</pages>
<editor>In M. Chytil and V. Koubek, editors,</editor>
<publisher>Springer</publisher>
<location>Berlin / Heidelberg.</location>
<marker>Chistov, Grigor’ev, 1984</marker>
<rawString>A. Chistov and D. Grigor’ev. 1984. Complexity of quantifier elimination in the theory of algebraically closed fields. In M. Chytil and V. Koubek, editors, Math. Found. of Comp. Science, volume 176, pages 17–31. Springer Berlin / Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
</authors>
<title>Manual and automatic evaluation of machine translation between european languages. In</title>
<date>2006</date>
<booktitle>Proc. WMT,</booktitle>
<pages>102--121</pages>
<publisher>ACL.</publisher>
<location>New York City,</location>
<contexts>
<context position="1158" citStr="Koehn and Monz, 2006" startWordPosition="178" endWordPosition="181">ing only scores of other judges and some common ratings. Even if prediction performance is pretty low, decisions made using the predicted score proved to be more reliable than decision based on a single rating of each example. 1 Introduction Human assessment is often considered as the best, if not the only, way to evaluate ‘subjective’ NLP tasks like MT or speech generation. However, human evaluations are doomed to be noisy and, sometimes, even contradictory as they depend on individual perception and understanding of the score scale that annotators generally use in remarkably different ways (Koehn and Monz, 2006). Moreover, annotation is known to be a long and frustrating process and annotator fatigue has been identified as another source of noise (Pighin et al., 2012). In addition to defining and enforcing stricter guidelines, several solutions have been proposed to reduce the annotation effort and produce more reliable ratings. For instance, to limit the impact of the score scale interpretation, in the WMT evaluation campaign (Callison-Burch et al., 2012), annotators are asked to rank translation hypotheses from best to worst instead of providing absolute scores (e.g. in terms of adequacy or fluency</context>
<context position="10933" citStr="Koehn and Monz, 2006" startWordPosition="1848" endWordPosition="1851">able from LDC under the references ldc2003t17 and ldc2003t18 value is drawn from N (µ, σ2). Based on this ‘intrinsic’ quality, two ratings, ai and bi, are generated according to three strategies: in the first, ai and bi are sampled from N (qi, 0); in the second, ai ∼ N (qi + θ2, σ02) and bi ∼ N (qi − θ2, σ02) and in the third, ai ∼ N (qi, σ02) and the bi is drawn from a bimodal distribution 1 (N (qi − θ 2, σ02) + N (qi + θ 2, σ02)) (with 2 σ02 &lt; θ2). 0 describes the noise level. Each of these strategies models a different kind of noise that has been observed in different evaluation campaigns (Koehn and Monz, 2006): the first one describes random noise in the ratings; the second a systematic difference in the annotators’ interpretation of the score scale and the third, the situation in which one annotator gives medium score while the other one tend to commit more strongly to whether she considered the translation good or bad. Stacking all these judgments results in a n×2 score matrix. To test whether this matrix has a low rank or not, we assess how close it is to its approximation by a rank 1 matrix. A well-known result (Lawson and Hanson, 1974) states that the Frobenius norm of the difference of these </context>
</contexts>
<marker>Koehn, Monz, 2006</marker>
<rawString>Philipp Koehn and Christof Monz. 2006. Manual and automatic evaluation of machine translation between european languages. In Proc. WMT, pages 102–121, New York City, June. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Simulating human judgment in machine translation evaluation campaigns.</title>
<date>2012</date>
<booktitle>In Proc. of IWSLT.</booktitle>
<contexts>
<context position="2163" citStr="Koehn, 2012" startWordPosition="337" endWordPosition="338">in the WMT evaluation campaign (Callison-Burch et al., 2012), annotators are asked to rank translation hypotheses from best to worst instead of providing absolute scores (e.g. in terms of adequacy or fluency). Generalizing this approach, several works (Pighin et al., 2012; Lopez, 2012) have defined novel annotation protocols to reduce the number of judgments that need to be collected. However, all these methods suffer from several limitations: first, they provide no interpretable information about the quality of the system (only a relative comparison between two systems is possible); second, (Koehn, 2012) has recently shown that the ranking they induce is not reliable. In this work, we study an alternative approach to the problem of collecting reliable human assessments. Our basic assumption, motivated by the success of ensemble methods, is that having several judgments for each example, even if they are noisy, will result in a more reliable decision than having a single judgment. An evaluation campaign should therefore aim at gathering a score matrix, in which each example is rated by all judges instead of having each judge rate only a small subset of examples, thereby minimizing redundancy. </context>
</contexts>
<marker>Koehn, 2012</marker>
<rawString>Philipp Koehn. 2012. Simulating human judgment in machine translation evaluation campaigns. In Proc. of IWSLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles L Lawson</author>
<author>Richard J Hanson</author>
</authors>
<title>Solving Least Squares Problems.</title>
<date>1974</date>
<publisher>Prentice Hall.</publisher>
<contexts>
<context position="11474" citStr="Lawson and Hanson, 1974" startWordPosition="1946" endWordPosition="1949">ise that has been observed in different evaluation campaigns (Koehn and Monz, 2006): the first one describes random noise in the ratings; the second a systematic difference in the annotators’ interpretation of the score scale and the third, the situation in which one annotator gives medium score while the other one tend to commit more strongly to whether she considered the translation good or bad. Stacking all these judgments results in a n×2 score matrix. To test whether this matrix has a low rank or not, we assess how close it is to its approximation by a rank 1 matrix. A well-known result (Lawson and Hanson, 1974) states that the Frobenius norm of the difference of these matrices is equal to the 2nd singular value of the original matrix; the quality of the approximation can thus be estimated by p, defined as the 2nd eigenvalue of the matrix normalized by its norm (Leon, 1994). Intuitively, the smaller p, the better the approximation. Figure 1 represents the impact of the noise level on the condition number. As a baseline, we have also represented p for a random matrix. All values are averaged over 100 simulations. As it could be expected, p is close to 0 for small noise level; but even for moderate noi</context>
</contexts>
<marker>Lawson, Hanson, 1974</marker>
<rawString>Charles L. Lawson and Richard J. Hanson. 1974. Solving Least Squares Problems. Prentice Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen J Leon</author>
</authors>
<title>Linear Algebra with Applications.</title>
<date>1994</date>
<publisher>Macmillan,.</publisher>
<contexts>
<context position="11741" citStr="Leon, 1994" startWordPosition="1998" endWordPosition="1999">ves medium score while the other one tend to commit more strongly to whether she considered the translation good or bad. Stacking all these judgments results in a n×2 score matrix. To test whether this matrix has a low rank or not, we assess how close it is to its approximation by a rank 1 matrix. A well-known result (Lawson and Hanson, 1974) states that the Frobenius norm of the difference of these matrices is equal to the 2nd singular value of the original matrix; the quality of the approximation can thus be estimated by p, defined as the 2nd eigenvalue of the matrix normalized by its norm (Leon, 1994). Intuitively, the smaller p, the better the approximation. Figure 1 represents the impact of the noise level on the condition number. As a baseline, we have also represented p for a random matrix. All values are averaged over 100 simulations. As it could be expected, p is close to 0 for small noise level; but even for moderate noise level, the second eigenvalue continue to be small, suggesting that the matrix can still be approximated by a matrix of rank 1 without much loss of information. As a comparison, on average, p = 0.08 for the CE score matrix, in spite of the low inter-rater agreement</context>
</contexts>
<marker>Leon, 1994</marker>
<rawString>Stephen J: Leon. 1994. Linear Algebra with Applications. Macmillan,.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Lopez</author>
</authors>
<title>Putting human assessments of machine translation systems in order.</title>
<date>2012</date>
<booktitle>In Proc. of WMT,</booktitle>
<pages>1--9</pages>
<publisher>ACL.</publisher>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="1837" citStr="Lopez, 2012" startWordPosition="286" endWordPosition="287">s and annotator fatigue has been identified as another source of noise (Pighin et al., 2012). In addition to defining and enforcing stricter guidelines, several solutions have been proposed to reduce the annotation effort and produce more reliable ratings. For instance, to limit the impact of the score scale interpretation, in the WMT evaluation campaign (Callison-Burch et al., 2012), annotators are asked to rank translation hypotheses from best to worst instead of providing absolute scores (e.g. in terms of adequacy or fluency). Generalizing this approach, several works (Pighin et al., 2012; Lopez, 2012) have defined novel annotation protocols to reduce the number of judgments that need to be collected. However, all these methods suffer from several limitations: first, they provide no interpretable information about the quality of the system (only a relative comparison between two systems is possible); second, (Koehn, 2012) has recently shown that the ranking they induce is not reliable. In this work, we study an alternative approach to the problem of collecting reliable human assessments. Our basic assumption, motivated by the success of ensemble methods, is that having several judgments for</context>
</contexts>
<marker>Lopez, 2012</marker>
<rawString>Adam Lopez. 2012. Putting human assessments of machine translation systems in order. In Proc. of WMT, pages 1–9, Montr´eal, Canada, June. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yann Mathet</author>
<author>Antoine Widlcher</author>
<author>Kar¨en Fort</author>
<author>Claire Franc¸ois</author>
<author>Olivier Galibert</author>
<author>Cyril Grouin</author>
<author>Juliette Kahn</author>
<author>Sophie Rosset</author>
<author>Pierre Zweigenbaum</author>
</authors>
<title>Manual corpus annotation: Giving meaning to the evaluation metrics.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING 2012: Posters,</booktitle>
<pages>809--818</pages>
<location>Mumbai, India,</location>
<marker>Mathet, Widlcher, Fort, Franc¸ois, Galibert, Grouin, Kahn, Rosset, Zweigenbaum, 2012</marker>
<rawString>Yann Mathet, Antoine Widlcher, Kar¨en Fort, Claire Franc¸ois, Olivier Galibert, Cyril Grouin, Juliette Kahn, Sophie Rosset, and Pierre Zweigenbaum. 2012. Manual corpus annotation: Giving meaning to the evaluation metrics. In Proceedings of COLING 2012: Posters, pages 809–818, Mumbai, India, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniele Pighin</author>
<author>Llu´ıs Formiga</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>A graph-based strategy to streamline translation quality assessments.</title>
<date>2012</date>
<booktitle>In Proc. ofAMTA.</booktitle>
<marker>Pighin, Formiga, M`arquez, 2012</marker>
<rawString>Daniele Pighin, Llu´ıs Formiga, and Llu´ıs M`arquez. 2012. A graph-based strategy to streamline translation quality assessments. In Proc. ofAMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Nitin Madnani</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
</authors>
<title>Fluency, adequacy, or HTER? Exploring different human judgments with a tunable MT metric.</title>
<date>2009</date>
<booktitle>In Proc. of WMT,</booktitle>
<pages>259--268</pages>
<publisher>ACL.</publisher>
<location>Athens, Greece,</location>
<marker>Snover, Madnani, Dorr, Schwartz, 2009</marker>
<rawString>Matthew Snover, Nitin Madnani, Bonnie Dorr, and Richard Schwartz. 2009. Fluency, adequacy, or HTER? Exploring different human judgments with a tunable MT metric. In Proc. of WMT, pages 259– 268, Athens, Greece, March. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Soricut</author>
<author>Abdessamad Echihabi</author>
</authors>
<title>Trustrank: Inducing trust in automatic translations via ranking. In</title>
<date>2010</date>
<booktitle>Proc. of ACL,</booktitle>
<pages>612--621</pages>
<publisher>ACL.</publisher>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="15857" citStr="Soricut and Echihabi, 2010" startWordPosition="2724" endWordPosition="2727">ty as it is not the value of the score that is important but the decision that it will support. That is why, we also evaluated matrix recovery in a more task-oriented way by comparing the decision made when considering the recovered score matrix instead of the ‘true’ score matrix. For the CE corpus, a task-oriented evaluation can be done by comparing the rankings induced by the recovered matrix and by the original matrix when examples are ordered according to their averaged score. Such a ranking can be used by a MT user to set a quality threshold granting her control over translation quality (Soricut and Echihabi, 2010). Table 2 shows the correlation between the two rankings as evaluated by Tb. The two rankings appear to be highly correlated, the matrix completion technique outperforming slightly the mean baseline. More importantly, even when 40% of the data are missing, the ranking induced by the true scores is better correlated to the ranking induced by the predicted scores than to the ranking induced when each example is only rated once: as reported in Section 3, the Tb is, in this case, 0.75. For the TIDES corpus, we computed the number of pairs of judges for which the results of a pairwise comparison be</context>
</contexts>
<marker>Soricut, Echihabi, 2010</marker>
<rawString>Radu Soricut and Abdessamad Echihabi. 2010. Trustrank: Inducing trust in automatic translations via ranking. In Proc. of ACL, pages 612–621, Uppsala, Sweden, July. ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>