<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.056896">
<title confidence="0.991824">
A probabilistic generative model for an intermediate
constituency-dependency representation
</title>
<author confidence="0.997563">
Federico Sangati
</author>
<affiliation confidence="0.9975105">
Institute for Logic, Language and Computation
University of Amsterdam, the Netherlands
</affiliation>
<email confidence="0.995536">
f.sangati@uva.nl
</email>
<sectionHeader confidence="0.993817" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999028">
We present a probabilistic model exten-
sion to the Tesni`ere Dependency Structure
(TDS) framework formulated in (Sangati
and Mazza, 2009). This representation in-
corporates aspects from both constituency
and dependency theory. In addition, it
makes use of junction structures to handle
coordination constructions. We test our
model on parsing the English Penn WSJ
treebank using a re-ranking framework.
This technique allows us to efficiently test
our model without needing a specialized
parser, and to use the standard evalua-
tion metric on the original Phrase Struc-
ture version of the treebank. We obtain
encouraging results: we achieve a small
improvement over state-of-the-art results
when re-ranking a small number of candi-
date structures, on all the evaluation met-
rics except for chunking.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99987741025641">
Since its origin, computational linguistics has
been dominated by Constituency/Phrase Structure
(PS) representation of sentence structure. How-
ever, recently, we observe a steady increase in
popularity of Dependency Structure (DS) for-
malisms. Several researchers have compared
the two alternatives, in terms of linguistic ade-
quacy (Nivre, 2005; Schneider, 2008), practical
applications (Ding and Palmer, 2005), and eval-
uations (Lin, 1995).
Dependency theory is historically accredited to
Lucien Tesni`ere (1959), although the relation of
dependency between words was only one of the
various key elements proposed to represent sen-
tence structures. In fact, the original formulation
incorporates the notion of chunk, as well as a spe-
cial type of structure to represent coordination.
The Tesni`ere Dependency Structure (TDS) rep-
resentation we propose in (Sangati and Mazza,
2009), is an attempt to formalize the original work
of Tesni`ere, with the intention to develop a simple
but consistent representation which combines con-
stituencies and dependencies. As part of this work,
we have implemented an automatic conversion1 of
the English Penn Wall Street Journal (WSJ) tree-
bank into the new annotation scheme.
In the current work, after introducing the key
elements of TDS (section 2), we describe a first
probabilistic extension to this framework, which
aims at modeling the different levels of the repre-
sentation (section 3). We test our model on parsing
the WSJ treebank using a re-ranking framework.
This technique allows us to efficiently test our sys-
tem without needing a specialized parser, and to
use the standard evaluation metric on the original
PS version of the treebank. In section 3.4 we also
introduce new evaluation schemes on specific as-
pects of the new TDS representation which we will
include in the results presented in section 3.4.
</bodyText>
<sectionHeader confidence="0.976153" genericHeader="method">
2 TDS representation
</sectionHeader>
<bodyText confidence="0.999795111111111">
It is beyond the scope of this paper to provide an
exhaustive description of the TDS representation
of the WSJ. It is nevertheless important to give the
reader a brief summary of its key elements, and
compare it with some of the other representations
of the WSJ which have been proposed. Figure 1
shows the original PS of a WSJ tree (a), together
with 3 other representations: (b) TDS, (c) DS2,
and (d) CCG (Hockenmaier and Steedman, 2007).
</bodyText>
<footnote confidence="0.970678555555555">
1staff.science.uva.nl/˜fsangati/TDS
2The DS representation is taken from the conversion pro-
cedure used in the CoNLL 2007 Shared Task on dependency
parsing (Nivre et al., 2007). Although more elaborate rep-
resentation have been proposed (de Marneffe and Manning,
2008; Cinkov´a et al., 2009) we have chosen this DS repre-
sentation because it is one of the most commonly used within
the CL community, given that it relies on a fully automatic
conversion procedure.
</footnote>
<page confidence="0.983703">
19
</page>
<note confidence="0.8855465">
Proceedings of the ACL 2010 Student Research Workshop, pages 19–24,
Uppsala, Sweden, 13 July 2010. c�2010 Association for Computational Linguistics
</note>
<figureCaption confidence="0.996628">
Figure 1: Four different structure representations, derived from a sentence of the WSJ treebank (section
00, #977). (a) PS (original), (b) CCG, (c) DS, (d) TDS.
</figureCaption>
<figure confidence="0.999399233766234">
WDT
that
VBP ,VBP
encourage , promote
conj
or
(S[dcl]\NP)/NP
advocate
(a) (b)
NP
N
or
, promote
V
V
(NP\NP)/(S[dcl]\NP)
that
S[dcl]\NP
NP
NNS
activities
VBP
encourage
,
VBP
advocate
NP
NN
abortion NP
VBP CC
(c) (d)N
NP
WHNP
WDT
that
S
VP
V
NP\NP
NP
N
activities
NNS
activities
activities
N
J
J
J
J
advocate
that encourage
, promote or
V
N
abortion
N
SBAR
(S[dcl]\NP)/NP
CC
or
(S[dcl]\NP)/NP[conj]
NP
N
abortion
(S[dcl]\NP)/NP
encourage
,
,
(S[dcl]\NP)/NP
VBP
advocate
NN
abortion
(S[dcl]\NP)/NP[conj]
(S[dcl]\NP)/NP
promote
</figure>
<bodyText confidence="0.999444741935484">
Words and Blocks In TDS, words are di-
vided in functional words (determiners, preposi-
tions, etc.) and content words (verbs, nouns, etc.).
Blocks are the basic elements (chunks) of a struc-
ture, which can be combined either via the depen-
dency relation or the junction operation. Blocks
can be of two types: standard and junction blocks.
Both types may contain any sequence of func-
tional words. Standard blocks (depicted as black
boxes) represent the elementary chunks of the
original PS, and include exactly one content word.
Coordination Junction blocks (depicted as yel-
low boxes) are used to represent coordinated struc-
tures. They contain two or more blocks (con-
juncts) possibly coordinated by means of func-
tional words (conjunctions). In Figure 1(d) the
yellow junction block contains three separate stan-
dard blocks. This representation allows to cap-
ture the fact that these conjuncts occupy the same
role: they all share the relativizer ‘that’, they all
depend on the noun ‘activities’, and they all gov-
ern the noun ‘abortion’. In Figure 1(a,c), we can
notice that both PS and DS do not adequately rep-
resent coordination structures: the PS annotation
is rather flat, avoiding to group the three verbs in a
unique unit, while in the DS the last noun ‘abor-
tion’ is at the same level of the verbs it should
be a dependent of. On the other hand, the CCG
structure of Figure 1(d), properly represents the
coordination. It does so by grouping the first three
verbs in a unique constituent which is in turn bi-
narized in a right-branching structure. One of the
strongest advantages of the CCG formalism, is
that every structure can be automatically mapped
to a logical-form representation. This is one rea-
son why it needs to handle coordinations properly.
Nevertheless, we conjecture that this representa-
tion of coordination might introduce some diffi-
culties for parsing: it is very hard to capture the
relation between ‘advocate’ and ‘abortion’ since
they are several levels away in the structure.
Categories and Transference There are 4 dif-
ferent block categories, which are indicated with
little colored bricks (as well as one-letter abbrevi-
ation) on top and at the bottom of the correspond-
ing blocks: verbs (red, V), nouns (blue, N), ad-
verbs (yellow, A), and adjectives (green, J). Every
block displays at the bottom the original category
determined by the content word (or the original
category of the conjuncts if it is a junction struc-
ture), and at the top, the derived category which
relates to the grammatical role of the whole block
in relation to the governing block. In several cases
we can observe a shift in the categories of a block,
from the original to the derived category. This
phenomenon is called transference and often oc-
curs by means of functional words in the block. In
Figure 1(b) we can observe the transference of the
junction block, which has the original category of
a verb, but takes the role of an adjective (through
the relativizer ‘that’) in modifying the noun ‘activ-
ities’.
</bodyText>
<page confidence="0.704862">
20
</page>
<equation confidence="0.998388857142857">
P(S) = PBGM(S) - PBEM(S) - PWFM(S)
PBGM(S) = � P(B|parent(B), direction(B), leftSibling(B))
B ∈ dependentBlocks(S)
PBEM(S) = � P(elements(B)|derivedCat(B))
B ∈ blocks(S)
PWFM(S) = � P(cw(B)|cw(parent(B)), cats(B), fw(B),context(B))
B ∈ standardBlocks(S)
</equation>
<tableCaption confidence="0.963695666666667">
Table 1: Equation (1) gives the likelihood of a structure S as the product of the likelihoods of generating
three aspects of the structure, according to the three models (BGM, BEM, WFM) specified in equations
(2-4) and explained in the main text.
</tableCaption>
<sectionHeader confidence="0.99378" genericHeader="method">
3 A probabilistic Model for TDS
</sectionHeader>
<bodyText confidence="0.9999894">
This section describes the probabilistic generative
model which was implemented in order to dis-
ambiguate TDS structures. We have chosen the
same strategy we have described in (Sangati et al.,
2009). The idea consists of utilizing a state of the
art parser to compute a list of k-best candidates of
a test sentence, and evaluate the new model by us-
ing it as a reranker. How well does it select the
most probable structure among the given candi-
dates? Since no parser currently exists for the TDS
representation, we utilize a state of the art parser
for PS trees (Charniak, 1999), and transform each
candidate to TDS. This strategy can be considered
a first step to efficiently test and compare different
models before implementing a full-fledged parser.
</bodyText>
<subsectionHeader confidence="0.998585">
3.1 Model description
</subsectionHeader>
<bodyText confidence="0.960800840909091">
In order to compute the probability of a given TDS
structure, we make use of three separate proba-
bilistic generative models, each responsible for a
specific aspect of the structure being generated.
The probability of a TDS structure is obtained by
multiplying its probabilities in the three models, as
reported in the first equation of Table 2.
The first model (equation 2) is the Block Gen-
eration Model (BGM). It describes the event of
generating a block B as a dependent of its parent
block (governor). The dependent block B is identi-
fied with its categories (both original and derived),
and its functional words, while the parent block is
characterized by the original category only. More-
over, in the conditioning context we specify the
direction of the dependent with respect to the par-
ent3, and its adjacent left sister (null if not present)
specified with the same level of details of B. The
model applies only to dependent blocks4.
The second model (equation 3) is the Block Ex-
pansion Model (BEM). It computes the probabil-
ity of a generic block B of known derived cate-
gory, to expand to the list of elements it is com-
posed of. The list includes the category of the
content word, in case the expansion leads to a
standard block. In case of a junction structure, it
contains the conjunctions and the conjunct blocks
(each identified with its categories and its func-
tional words) in the order they appear. Moreover,
all functional words in the block are added to the
list5. The model applies to all blocks.
The third model (equation 4) is the Word Fill-
ing Model (WFM), which applies to each stan-
dard block B of the structure. It models the event
of filling B with a content word (cw), given the
content word of the governing block, the cate-
gories (cats) and functional words (fw) of B, and
further information about the context6 in which B
occurs. This model becomes particularly interest-
3A dependent block can have three different positions
with respect to the parent block: left, right, inner. The first
two are self-explanatory. The inner case occurs when the de-
pendent block starts after the beginning of the parent block
but ends before it (e.g. a nice dog).
</bodyText>
<footnote confidence="0.822422636363636">
4A block is a dependent block if it is not a conjunct. In
other words, it must be connected with a line to its governor.
5The attentive reader might notice that the functional
words are generated twice (in BGM and BEM). This deci-
sion, although not fully justified from a statistical viewpoint,
seems to drive the model towards a better disambiguation.
6context(B) comprises information about the grandpar-
ent block (original category), the adjacent left sibling block
(derived category), the direction of the content word with re-
spect to its governor (in this case only left and right), and the
absolute distance between the two words.
</footnote>
<page confidence="0.999287">
21
</page>
<bodyText confidence="0.999952333333333">
ing when a standard block is a dependent of a junc-
tion block (such as ‘abortion’ in Figure 1(d)). In
this case, the model needs to capture the depen-
dency relation between the content word of the
dependent block and each of the content words be-
longing to the junction block7.
</bodyText>
<subsectionHeader confidence="0.999189">
3.2 Smoothing
</subsectionHeader>
<bodyText confidence="0.9999905">
In all the three models we have adopted a smooth-
ing techniques based on back-off level estima-
tion as proposed by Collins (1999). The different
back-off estimates, which are listed in decreasing
levels of details, are interpolated with confidence
weights8 derived from the training corpus.
The first two models are implemented with two
levels of back-off, in which the last is a constant
value (10−6) to make the overall probability small
but not zero, for unknown events.
The third model is implemented with three lev-
els of back-off: the last is set to the same con-
stant value (10−6), the first encodes the depen-
dency event using both pos-tags and lexical infor-
mation of the governor and the dependent word,
while the second specifies only pos-tags.
</bodyText>
<subsectionHeader confidence="0.997065">
3.3 Experiment Setup
</subsectionHeader>
<bodyText confidence="0.916188541666667">
We have tested our model on the WSJ section of
Penn Treebank (Marcus et al., 1993), using sec-
tions 02-21 as training and section 22 for testing.
We employ the Max-Ent parser, implemented by
Charniak (1999), to generate a list of k-best PS
candidates for the test sentences, which are then
converted into TDS representation.
Instead of using Charniak’s parser in its origi-
nal settings, we train it on a version of the corpus
in which we add a special suffix to constituents
which have circumstantial role9. This decision is
based on the observation that the TDS formalism
well captures the argument structure of verbs, and
7In order to derive the probability of this multi-event we
compute the average between the probabilities of the single
events which compose it.
8Each back-off level obtains a confidence weight which
decreases with the increase of the diversity of the context
(B(Ci)), which is the number of separate events occurring
with the same context (Ci). More formally if f(Ci) is the
frequency of the conditioning context of the current event,
the weight is obtained as f(Ci)/(f(Ci) · µ · B(Ci)); see
also (Bikel, 2004). In our model we have chosen µ to be
5 for the first model, and 50 for the second and the third.
</bodyText>
<footnote confidence="0.8478">
9Those which have certain function tags (e.g. ADV, LOC,
TMP). The full list is reported in (Sangati and Mazza, 2009).
It was surprising to notice that the performance of this slightly
modified parser (in terms of F-score) is only slightly lower
than how it performs out-of-the-box (0.13%).
</footnote>
<bodyText confidence="0.994064833333333">
we believe that this additional information might
benefit our model.
We then applied our probabilistic model to re-
rank the list of available k-best TDS, and evalu-
ate the selected candidates using several metrics
which will be introduced next.
</bodyText>
<subsectionHeader confidence="0.987229">
3.4 Evaluation Metrics for TDS
</subsectionHeader>
<bodyText confidence="0.993746893617021">
The re-ranking framework described above, al-
lows us to keep track of the original PS of each
TDS candidate. This provides an implicit advan-
tage for evaluating our system, viz. it allows us to
evaluate the re-ranked structures both in terms of
the standard evaluation benchmark on the original
PS (F-score) as well as on more refined metrics
derived from the converted TDS representation.
In addition, the specific head assignment that the
TDS conversion procedure performs on the origi-
nal PS, allows us to convert every PS candidate to
a standard projective DS, and from this represen-
tation we can in turn compute the standard bench-
mark evaluation for DS, i.e. unlabeled attachment
score10 (UAS) (Lin, 1995; Nivre et al., 2007).
Concerning the TDS representation, we have
formulated 3 evaluation metrics which reflect the
accuracy of the chosen structure with respect to the
gold structure (the one derived from the manually
annotated PS), regarding the different components
of the representation:
Block Detection Score (BDS): the accuracy of de-
tecting the correct boundaries of the blocks in the
structure11.
Block Attachment Score (BAS): the accuracy
of detecting the correct governing block of each
block in the structure12.
Junction Detection Score (JDS): the accuracy of
detecting the correct list of content-words com-
posing each junction block in the structure13.
10UAS measures the percentage of words (excluding punc-
tuation) having the correct governing word.
11It is calculated as the harmonic mean between recall and
precision between the test and gold set of blocks, where each
block is identified with two numerical values representing the
start and the end position (punctuation words are discarded).
12It is computed as the percentage of words (both func-
tional and content words, excluding punctuation) having the
correct governing block. The governing block of a word, is
defined as the governor of the block it belongs to. If the block
is a conjunct, its governing block is computed recursively as
the governing block of the junction block it belongs to.
13It is calculated as the harmonic mean between recall and
precision between the test and gold set of junction blocks ex-
pansions, where each expansion is identified with the list of
content words belonging to the junction block. A recursive
junction structure expands to a list of lists of content-words.
</bodyText>
<page confidence="0.996977">
22
</page>
<table confidence="0.9998125">
F-Score UAS BDS BAS JDS
Charniak (k = 1) 89.41 92.24 94.82 89.29 75.82
Oracle Best F-Score (k = 1000) 97.47 96.98 97.03 95.79 82.26
Oracle Worst F-Score (k = 1000) 57.04 77.04 84.71 70.10 43.01
Oracle Best JDS (k = 1000) 90.54 93.77 96.20 90.57 93.55
PCFG-reranker (k = 5) 89.03 92.12 94.86 88.94 75.88
PCFG-reranker (k = 1000) 83.52 87.04 92.07 82.32 69.17
TDS-reranker (k = 5) 89.65 92.33 94.77 89.35 76.23
TDS-reranker (k = 10) 89.10 92.11 94.58 88.94 75.47
TDS-reranker (k = 100) 86.64 90.24 93.11 86.34 69.60
TDS-reranker (k = 500) 84.94 88.62 91.97 84.43 65.30
TDS-reranker (k = 1000) 84.31 87.89 91.42 83.69 63.65
</table>
<tableCaption confidence="0.9316325">
Table 2: Results of Charniak’s parser, the TDS-reranker, and the PCFG-reranker according to several
evaluation metrics, when the number k of best-candidates increases.
</tableCaption>
<figureCaption confidence="0.843339666666667">
Figure 2: Left: results of the TDS-reranking model according to several evaluation metrics as in Table 2.
Right: comparison between the F-scores of the TDS-reranker and a vanilla PCFG-reranker (together
with the lower and the upper bound), with the increase of the number of best candidates.
</figureCaption>
<sectionHeader confidence="0.943708" genericHeader="evaluation">
3.5 Results
</sectionHeader>
<bodyText confidence="0.99995675">
Table 2 reports the results we obtain when re-
ranking with our model an increasing number of
k-best candidates provided by Charniak’s parser
(the same results are shown in the left graph of
Figure 2). We also report the results relative to a
PCFG-reranker obtained by computing the prob-
ability of the k-best candidates using a standard
vanilla-PCFG model derived from the same train-
ing corpus. Moreover, we evaluate, by means of an
oracle, the upper and lower bound of the F-Score
and JDS metric, by selecting the structures which
maximizes/minimizes the results.
Our re-ranking model performs rather well for
a limited number of candidate structures, and out-
performs Charniak’s model when k = 5. In this
case we observe a small boost in performance for
the detection of junction structures, as well as for
all other evaluation metrics, except for the BDS.
The right graph in Figure 2 compares the F-
score performance of the TDS-reranker against the
PCFG-reranker. Our system consistently outper-
forms the PCFG model on this metric, as for UAS,
and BAS. Concerning the other metrics, as the
number of k-best candidates increases, the PCFG
model outperforms the TDS-reranker both accord-
ing to the BDS and the JDS.
Unfortunately, the performance of the re-
ranking model worsens progressively with the in-
crease of k. We find that this is primarily due to
the lack of robustness of the model in detecting the
block boundaries. This suggests that the system
might benefit from a separate preprocessing step
which could chunk the input sentence with higher
accuracy (Sang et al., 2000). In addition the same
module could detect local (intra-clausal) coordina-
tions, as illustrated by (Marinˇciˇc et al., 2009).
</bodyText>
<page confidence="0.997481">
23
</page>
<sectionHeader confidence="0.999558" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.99979484375">
In this paper, we have presented a probabilistic
generative model for parsing TDS syntactic rep-
resentation of English sentences. We have given
evidence for the usefulness of this formalism: we
consider it a valid alternative to commonly used
PS and DS representations, since it incorporates
the most relevant features of both notations; in ad-
dition, it makes use of junction structures to repre-
sent coordination, a linguistic phenomena highly
abundant in natural language production, but of-
ten neglected when it comes to evaluating parsing
resources. We have therefore proposed a special
evaluation metrics for junction detection, with the
hope that other researchers might benefit from it
in the future. Remarkably, Charniak’s parser per-
forms extremely well in all the evaluation metrics
besides the one related to coordination.
Our parsing results are encouraging: the over-
all system, although only when the candidates are
highly reliable, can improve on Charniak’s parser
on all the evaluation metrics with the exception of
chunking score (BDS). The weakness on perform-
ing chunking is the major factor responsible for
the lack of robustness of our system. We are con-
sidering to use a dedicated pre-processing module
to perform this step with higher accuracy.
Acknowledgments The author gratefully ac-
knowledge funding by the Netherlands Organiza-
tion for Scientific Research (NWO): this work is
funded through a Vici-grant “Integrating Cogni-
tion” (277.70.006) to Rens Bod. We also thank
3 anonymous reviewers for very useful comments.
</bodyText>
<sectionHeader confidence="0.999263" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999951869565217">
Daniel M. Bikel. 2004. Intricacies of Collins’ Parsing
Model. Comput. Linguist., 30(4):479–511.
Eugene Charniak. 1999. A Maximum-Entropy-
Inspired Parser. Technical report, Providence, RI,
USA.
Silvie Cinkov´a, Josef Toman, Jan Hajiˇc, Krist´yna
ˇCerm´akov´a, V´aclav Klimeˇs, Lucie Mladov´a,
Jana ˇSindlerov´a, Krist´yna Tomˇs˚u, and Zdenˇek
ˇZabokrtsk´y. 2009. Tectogrammatical Annotation
of the Wall Street Journal. The Prague Bulletin of
Mathematical Linguistics, (92).
Michael J. Collins. 1999. Head-Driven Statistical
Models for Natural Language Parsing. Ph.D. the-
sis, University of Pennsylvania.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford Typed Dependencies
Representation. In Coling 2008: Proceedings of the
workshop on Cross-Framework and Cross-Domain
Parser Evaluation, pages 1–8, Manchester, UK.
Yuan Ding and Martha Palmer. 2005. Machine Trans-
lation Using Probabilistic Synchronous Dependency
Insertion Grammars. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL’05), pages 541–548.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A Corpus of CCG Derivations and Depen-
dency Structures Extracted from the Penn Treebank.
Computational Linguistics, 33(3):355–396.
Dekang Lin. 1995. A Dependency-based Method for
Evaluating Broad-Coverage Parsers. In In Proceed-
ings of IJCAI-95, pages 1420–1425.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a Large Anno-
tated Corpus of English: The Penn Treebank. Com-
putational Linguistics, 19(2):313–330.
Domen Marinˇciˇc, Matjaˇz Gams, and Tomaˇz ˇSef. 2009.
Intraclausal Coordination and Clause Detection as a
Preprocessing Step to Dependency Parsing. In TSD
’09: Proceedings of the 12th International Confer-
ence on Text, Speech and Dialogue, pages 147–153,
Berlin, Heidelberg. Springer-Verlag.
Joakim Nivre, Johan Hall, Sandra K¨ubler, Ryan Mc-
donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 Shared Task on
Dependency Parsing. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
915–932, Prague, Czech Republic.
Joakim Nivre. 2005. Dependency Grammar and De-
pendency Parsing. Technical report, V¨axj¨o Univer-
sity: School of Mathematics and Systems Engineer-
ing.
Erik F. Tjong Kim Sang, Sabine Buchholz, and Kim
Sang. 2000. Introduction to the CoNLL-2000
Shared Task: Chunking. In Proceedings of CoNLL-
2000 and LLL-2000, Lisbon, Portugal.
Federico Sangati and Chiara Mazza. 2009. An English
Dependency Treebank a` la Tesni`ere. In The 8th In-
ternational Workshop on Treebanks and Linguistic
Theories, pages 173–184, Milan, Italy.
Federico Sangati, Willem Zuidema, and Rens Bod.
2009. A generative re-ranking model for depen-
dency parsing. In Proceedings of the 11th In-
ternational Conference on Parsing Technologies
(IWPT’09), pages 238–241, Paris, France, October.
Gerold Schneider. 2008. Hybrid long-distance func-
tional dependency parsing. Ph.D. thesis, University
of Zurich.
Lucien Tesni`ere. 1959. El´ements de syntaxe struc-
turale. Editions Klincksieck, Paris.
</reference>
<page confidence="0.999179">
24
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.896181">
<title confidence="0.9897755">A probabilistic generative model for an intermediate constituency-dependency representation</title>
<author confidence="0.999945">Federico Sangati</author>
<affiliation confidence="0.9908125">Institute for Logic, Language and Computation University of Amsterdam, the Netherlands</affiliation>
<email confidence="0.997094">f.sangati@uva.nl</email>
<abstract confidence="0.99670619047619">We present a probabilistic model extension to the Tesni`ere Dependency Structure (TDS) framework formulated in (Sangati and Mazza, 2009). This representation incorporates aspects from both constituency and dependency theory. In addition, it use of to handle coordination constructions. We test our model on parsing the English Penn WSJ treebank using a re-ranking framework. This technique allows us to efficiently test our model without needing a specialized parser, and to use the standard evaluation metric on the original Phrase Structure version of the treebank. We obtain encouraging results: we achieve a small improvement over state-of-the-art results when re-ranking a small number of candidate structures, on all the evaluation metrics except for chunking.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
</authors>
<date>2004</date>
<journal>Intricacies of Collins’ Parsing Model. Comput. Linguist.,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="13948" citStr="Bikel, 2004" startWordPosition="2289" endWordPosition="2290">is based on the observation that the TDS formalism well captures the argument structure of verbs, and 7In order to derive the probability of this multi-event we compute the average between the probabilities of the single events which compose it. 8Each back-off level obtains a confidence weight which decreases with the increase of the diversity of the context (B(Ci)), which is the number of separate events occurring with the same context (Ci). More formally if f(Ci) is the frequency of the conditioning context of the current event, the weight is obtained as f(Ci)/(f(Ci) · µ · B(Ci)); see also (Bikel, 2004). In our model we have chosen µ to be 5 for the first model, and 50 for the second and the third. 9Those which have certain function tags (e.g. ADV, LOC, TMP). The full list is reported in (Sangati and Mazza, 2009). It was surprising to notice that the performance of this slightly modified parser (in terms of F-score) is only slightly lower than how it performs out-of-the-box (0.13%). we believe that this additional information might benefit our model. We then applied our probabilistic model to rerank the list of available k-best TDS, and evaluate the selected candidates using several metrics </context>
</contexts>
<marker>Bikel, 2004</marker>
<rawString>Daniel M. Bikel. 2004. Intricacies of Collins’ Parsing Model. Comput. Linguist., 30(4):479–511.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A Maximum-EntropyInspired Parser.</title>
<date>1999</date>
<tech>Technical report,</tech>
<location>Providence, RI, USA.</location>
<contexts>
<context position="8752" citStr="Charniak, 1999" startWordPosition="1404" endWordPosition="1405"> text. 3 A probabilistic Model for TDS This section describes the probabilistic generative model which was implemented in order to disambiguate TDS structures. We have chosen the same strategy we have described in (Sangati et al., 2009). The idea consists of utilizing a state of the art parser to compute a list of k-best candidates of a test sentence, and evaluate the new model by using it as a reranker. How well does it select the most probable structure among the given candidates? Since no parser currently exists for the TDS representation, we utilize a state of the art parser for PS trees (Charniak, 1999), and transform each candidate to TDS. This strategy can be considered a first step to efficiently test and compare different models before implementing a full-fledged parser. 3.1 Model description In order to compute the probability of a given TDS structure, we make use of three separate probabilistic generative models, each responsible for a specific aspect of the structure being generated. The probability of a TDS structure is obtained by multiplying its probabilities in the three models, as reported in the first equation of Table 2. The first model (equation 2) is the Block Generation Mode</context>
<context position="13021" citStr="Charniak (1999)" startWordPosition="2135" endWordPosition="2136">ch the last is a constant value (10−6) to make the overall probability small but not zero, for unknown events. The third model is implemented with three levels of back-off: the last is set to the same constant value (10−6), the first encodes the dependency event using both pos-tags and lexical information of the governor and the dependent word, while the second specifies only pos-tags. 3.3 Experiment Setup We have tested our model on the WSJ section of Penn Treebank (Marcus et al., 1993), using sections 02-21 as training and section 22 for testing. We employ the Max-Ent parser, implemented by Charniak (1999), to generate a list of k-best PS candidates for the test sentences, which are then converted into TDS representation. Instead of using Charniak’s parser in its original settings, we train it on a version of the corpus in which we add a special suffix to constituents which have circumstantial role9. This decision is based on the observation that the TDS formalism well captures the argument structure of verbs, and 7In order to derive the probability of this multi-event we compute the average between the probabilities of the single events which compose it. 8Each back-off level obtains a confiden</context>
</contexts>
<marker>Charniak, 1999</marker>
<rawString>Eugene Charniak. 1999. A Maximum-EntropyInspired Parser. Technical report, Providence, RI, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silvie Cinkov´a</author>
<author>Josef Toman</author>
</authors>
<title>Hajiˇc, Krist´yna ˇCerm´akov´a, V´aclav Klimeˇs, Lucie Mladov´a, Jana ˇSindlerov´a, Krist´yna Tomˇs˚u, and Zdenˇek ˇZabokrtsk´y.</title>
<date></date>
<journal>Tectogrammatical Annotation of the Wall Street Journal. The Prague Bulletin of Mathematical Linguistics,</journal>
<volume>92</volume>
<marker>Cinkov´a, Toman, </marker>
<rawString>Silvie Cinkov´a, Josef Toman, Jan Hajiˇc, Krist´yna ˇCerm´akov´a, V´aclav Klimeˇs, Lucie Mladov´a, Jana ˇSindlerov´a, Krist´yna Tomˇs˚u, and Zdenˇek ˇZabokrtsk´y. 2009. Tectogrammatical Annotation of the Wall Street Journal. The Prague Bulletin of Mathematical Linguistics, (92).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael J Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="12172" citStr="Collins (1999)" startWordPosition="1993" endWordPosition="1994">djacent left sibling block (derived category), the direction of the content word with respect to its governor (in this case only left and right), and the absolute distance between the two words. 21 ing when a standard block is a dependent of a junction block (such as ‘abortion’ in Figure 1(d)). In this case, the model needs to capture the dependency relation between the content word of the dependent block and each of the content words belonging to the junction block7. 3.2 Smoothing In all the three models we have adopted a smoothing techniques based on back-off level estimation as proposed by Collins (1999). The different back-off estimates, which are listed in decreasing levels of details, are interpolated with confidence weights8 derived from the training corpus. The first two models are implemented with two levels of back-off, in which the last is a constant value (10−6) to make the overall probability small but not zero, for unknown events. The third model is implemented with three levels of back-off: the last is set to the same constant value (10−6), the first encodes the dependency event using both pos-tags and lexical information of the governor and the dependent word, while the second sp</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael J. Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Christopher D Manning</author>
</authors>
<title>The Stanford Typed Dependencies Representation. In Coling</title>
<date>2008</date>
<booktitle>Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation,</booktitle>
<pages>1--8</pages>
<location>Manchester, UK.</location>
<marker>de Marneffe, Manning, 2008</marker>
<rawString>Marie-Catherine de Marneffe and Christopher D. Manning. 2008. The Stanford Typed Dependencies Representation. In Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation, pages 1–8, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuan Ding</author>
<author>Martha Palmer</author>
</authors>
<title>Machine Translation Using Probabilistic Synchronous Dependency Insertion Grammars.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>541--548</pages>
<contexts>
<context position="1438" citStr="Ding and Palmer, 2005" startWordPosition="197" endWordPosition="200">We obtain encouraging results: we achieve a small improvement over state-of-the-art results when re-ranking a small number of candidate structures, on all the evaluation metrics except for chunking. 1 Introduction Since its origin, computational linguistics has been dominated by Constituency/Phrase Structure (PS) representation of sentence structure. However, recently, we observe a steady increase in popularity of Dependency Structure (DS) formalisms. Several researchers have compared the two alternatives, in terms of linguistic adequacy (Nivre, 2005; Schneider, 2008), practical applications (Ding and Palmer, 2005), and evaluations (Lin, 1995). Dependency theory is historically accredited to Lucien Tesni`ere (1959), although the relation of dependency between words was only one of the various key elements proposed to represent sentence structures. In fact, the original formulation incorporates the notion of chunk, as well as a special type of structure to represent coordination. The Tesni`ere Dependency Structure (TDS) representation we propose in (Sangati and Mazza, 2009), is an attempt to formalize the original work of Tesni`ere, with the intention to develop a simple but consistent representation whi</context>
</contexts>
<marker>Ding, Palmer, 2005</marker>
<rawString>Yuan Ding and Martha Palmer. 2005. Machine Translation Using Probabilistic Synchronous Dependency Insertion Grammars. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 541–548.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>CCGbank: A Corpus of CCG Derivations and Dependency Structures Extracted from the Penn Treebank.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>3</issue>
<contexts>
<context position="3340" citStr="Hockenmaier and Steedman, 2007" startWordPosition="508" endWordPosition="511">. In section 3.4 we also introduce new evaluation schemes on specific aspects of the new TDS representation which we will include in the results presented in section 3.4. 2 TDS representation It is beyond the scope of this paper to provide an exhaustive description of the TDS representation of the WSJ. It is nevertheless important to give the reader a brief summary of its key elements, and compare it with some of the other representations of the WSJ which have been proposed. Figure 1 shows the original PS of a WSJ tree (a), together with 3 other representations: (b) TDS, (c) DS2, and (d) CCG (Hockenmaier and Steedman, 2007). 1staff.science.uva.nl/˜fsangati/TDS 2The DS representation is taken from the conversion procedure used in the CoNLL 2007 Shared Task on dependency parsing (Nivre et al., 2007). Although more elaborate representation have been proposed (de Marneffe and Manning, 2008; Cinkov´a et al., 2009) we have chosen this DS representation because it is one of the most commonly used within the CL community, given that it relies on a fully automatic conversion procedure. 19 Proceedings of the ACL 2010 Student Research Workshop, pages 19–24, Uppsala, Sweden, 13 July 2010. c�2010 Association for Computationa</context>
</contexts>
<marker>Hockenmaier, Steedman, 2007</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2007. CCGbank: A Corpus of CCG Derivations and Dependency Structures Extracted from the Penn Treebank. Computational Linguistics, 33(3):355–396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>A Dependency-based Method for Evaluating Broad-Coverage Parsers. In</title>
<date>1995</date>
<booktitle>In Proceedings of IJCAI-95,</booktitle>
<pages>1420--1425</pages>
<contexts>
<context position="1467" citStr="Lin, 1995" startWordPosition="204" endWordPosition="205"> a small improvement over state-of-the-art results when re-ranking a small number of candidate structures, on all the evaluation metrics except for chunking. 1 Introduction Since its origin, computational linguistics has been dominated by Constituency/Phrase Structure (PS) representation of sentence structure. However, recently, we observe a steady increase in popularity of Dependency Structure (DS) formalisms. Several researchers have compared the two alternatives, in terms of linguistic adequacy (Nivre, 2005; Schneider, 2008), practical applications (Ding and Palmer, 2005), and evaluations (Lin, 1995). Dependency theory is historically accredited to Lucien Tesni`ere (1959), although the relation of dependency between words was only one of the various key elements proposed to represent sentence structures. In fact, the original formulation incorporates the notion of chunk, as well as a special type of structure to represent coordination. The Tesni`ere Dependency Structure (TDS) representation we propose in (Sangati and Mazza, 2009), is an attempt to formalize the original work of Tesni`ere, with the intention to develop a simple but consistent representation which combines constituencies an</context>
<context position="15316" citStr="Lin, 1995" startWordPosition="2519" endWordPosition="2520">candidate. This provides an implicit advantage for evaluating our system, viz. it allows us to evaluate the re-ranked structures both in terms of the standard evaluation benchmark on the original PS (F-score) as well as on more refined metrics derived from the converted TDS representation. In addition, the specific head assignment that the TDS conversion procedure performs on the original PS, allows us to convert every PS candidate to a standard projective DS, and from this representation we can in turn compute the standard benchmark evaluation for DS, i.e. unlabeled attachment score10 (UAS) (Lin, 1995; Nivre et al., 2007). Concerning the TDS representation, we have formulated 3 evaluation metrics which reflect the accuracy of the chosen structure with respect to the gold structure (the one derived from the manually annotated PS), regarding the different components of the representation: Block Detection Score (BDS): the accuracy of detecting the correct boundaries of the blocks in the structure11. Block Attachment Score (BAS): the accuracy of detecting the correct governing block of each block in the structure12. Junction Detection Score (JDS): the accuracy of detecting the correct list of </context>
</contexts>
<marker>Lin, 1995</marker>
<rawString>Dekang Lin. 1995. A Dependency-based Method for Evaluating Broad-Coverage Parsers. In In Proceedings of IJCAI-95, pages 1420–1425.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="12898" citStr="Marcus et al., 1993" startWordPosition="2113" endWordPosition="2116">h confidence weights8 derived from the training corpus. The first two models are implemented with two levels of back-off, in which the last is a constant value (10−6) to make the overall probability small but not zero, for unknown events. The third model is implemented with three levels of back-off: the last is set to the same constant value (10−6), the first encodes the dependency event using both pos-tags and lexical information of the governor and the dependent word, while the second specifies only pos-tags. 3.3 Experiment Setup We have tested our model on the WSJ section of Penn Treebank (Marcus et al., 1993), using sections 02-21 as training and section 22 for testing. We employ the Max-Ent parser, implemented by Charniak (1999), to generate a list of k-best PS candidates for the test sentences, which are then converted into TDS representation. Instead of using Charniak’s parser in its original settings, we train it on a version of the corpus in which we add a special suffix to constituents which have circumstantial role9. This decision is based on the observation that the TDS formalism well captures the argument structure of verbs, and 7In order to derive the probability of this multi-event we c</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Domen Marinˇciˇc</author>
<author>Matjaˇz Gams</author>
<author>Tomaˇz ˇSef</author>
</authors>
<title>Intraclausal Coordination and Clause Detection as a Preprocessing Step to Dependency Parsing.</title>
<date>2009</date>
<booktitle>In TSD ’09: Proceedings of the 12th International Conference on Text, Speech and Dialogue,</booktitle>
<pages>147--153</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<marker>Marinˇciˇc, Gams, ˇSef, 2009</marker>
<rawString>Domen Marinˇciˇc, Matjaˇz Gams, and Tomaˇz ˇSef. 2009. Intraclausal Coordination and Clause Detection as a Preprocessing Step to Dependency Parsing. In TSD ’09: Proceedings of the 12th International Conference on Text, Speech and Dialogue, pages 147–153, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Sandra K¨ubler</author>
<author>Ryan Mcdonald</author>
<author>Jens Nilsson</author>
<author>Sebastian Riedel</author>
<author>Deniz Yuret</author>
</authors>
<title>The CoNLL</title>
<date>2007</date>
<booktitle>In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007,</booktitle>
<pages>915--932</pages>
<location>Prague, Czech Republic.</location>
<marker>Nivre, Hall, K¨ubler, Mcdonald, Nilsson, Riedel, Yuret, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Sandra K¨ubler, Ryan Mcdonald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007. The CoNLL 2007 Shared Task on Dependency Parsing. In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pages 915–932, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Dependency Grammar and Dependency Parsing.</title>
<date>2005</date>
<tech>Technical report,</tech>
<institution>V¨axj¨o University: School of Mathematics and Systems Engineering.</institution>
<contexts>
<context position="1372" citStr="Nivre, 2005" startWordPosition="191" endWordPosition="192">the original Phrase Structure version of the treebank. We obtain encouraging results: we achieve a small improvement over state-of-the-art results when re-ranking a small number of candidate structures, on all the evaluation metrics except for chunking. 1 Introduction Since its origin, computational linguistics has been dominated by Constituency/Phrase Structure (PS) representation of sentence structure. However, recently, we observe a steady increase in popularity of Dependency Structure (DS) formalisms. Several researchers have compared the two alternatives, in terms of linguistic adequacy (Nivre, 2005; Schneider, 2008), practical applications (Ding and Palmer, 2005), and evaluations (Lin, 1995). Dependency theory is historically accredited to Lucien Tesni`ere (1959), although the relation of dependency between words was only one of the various key elements proposed to represent sentence structures. In fact, the original formulation incorporates the notion of chunk, as well as a special type of structure to represent coordination. The Tesni`ere Dependency Structure (TDS) representation we propose in (Sangati and Mazza, 2009), is an attempt to formalize the original work of Tesni`ere, with t</context>
</contexts>
<marker>Nivre, 2005</marker>
<rawString>Joakim Nivre. 2005. Dependency Grammar and Dependency Parsing. Technical report, V¨axj¨o University: School of Mathematics and Systems Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F Tjong Kim Sang</author>
<author>Sabine Buchholz</author>
<author>Kim Sang</author>
</authors>
<title>Introduction to the CoNLL-2000 Shared Task: Chunking.</title>
<date>2000</date>
<booktitle>In Proceedings of CoNLL2000 and LLL-2000,</booktitle>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="19657" citStr="Sang et al., 2000" startWordPosition="3226" endWordPosition="3229">nker. Our system consistently outperforms the PCFG model on this metric, as for UAS, and BAS. Concerning the other metrics, as the number of k-best candidates increases, the PCFG model outperforms the TDS-reranker both according to the BDS and the JDS. Unfortunately, the performance of the reranking model worsens progressively with the increase of k. We find that this is primarily due to the lack of robustness of the model in detecting the block boundaries. This suggests that the system might benefit from a separate preprocessing step which could chunk the input sentence with higher accuracy (Sang et al., 2000). In addition the same module could detect local (intra-clausal) coordinations, as illustrated by (Marinˇciˇc et al., 2009). 23 4 Conclusions In this paper, we have presented a probabilistic generative model for parsing TDS syntactic representation of English sentences. We have given evidence for the usefulness of this formalism: we consider it a valid alternative to commonly used PS and DS representations, since it incorporates the most relevant features of both notations; in addition, it makes use of junction structures to represent coordination, a linguistic phenomena highly abundant in nat</context>
</contexts>
<marker>Sang, Buchholz, Sang, 2000</marker>
<rawString>Erik F. Tjong Kim Sang, Sabine Buchholz, and Kim Sang. 2000. Introduction to the CoNLL-2000 Shared Task: Chunking. In Proceedings of CoNLL2000 and LLL-2000, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Federico Sangati</author>
<author>Chiara Mazza</author>
</authors>
<title>An English Dependency Treebank a` la Tesni`ere.</title>
<date>2009</date>
<booktitle>In The 8th International Workshop on Treebanks and Linguistic Theories,</booktitle>
<pages>173--184</pages>
<location>Milan, Italy.</location>
<contexts>
<context position="1905" citStr="Sangati and Mazza, 2009" startWordPosition="268" endWordPosition="271">esearchers have compared the two alternatives, in terms of linguistic adequacy (Nivre, 2005; Schneider, 2008), practical applications (Ding and Palmer, 2005), and evaluations (Lin, 1995). Dependency theory is historically accredited to Lucien Tesni`ere (1959), although the relation of dependency between words was only one of the various key elements proposed to represent sentence structures. In fact, the original formulation incorporates the notion of chunk, as well as a special type of structure to represent coordination. The Tesni`ere Dependency Structure (TDS) representation we propose in (Sangati and Mazza, 2009), is an attempt to formalize the original work of Tesni`ere, with the intention to develop a simple but consistent representation which combines constituencies and dependencies. As part of this work, we have implemented an automatic conversion1 of the English Penn Wall Street Journal (WSJ) treebank into the new annotation scheme. In the current work, after introducing the key elements of TDS (section 2), we describe a first probabilistic extension to this framework, which aims at modeling the different levels of the representation (section 3). We test our model on parsing the WSJ treebank usin</context>
<context position="14162" citStr="Sangati and Mazza, 2009" startWordPosition="2329" endWordPosition="2332">ties of the single events which compose it. 8Each back-off level obtains a confidence weight which decreases with the increase of the diversity of the context (B(Ci)), which is the number of separate events occurring with the same context (Ci). More formally if f(Ci) is the frequency of the conditioning context of the current event, the weight is obtained as f(Ci)/(f(Ci) · µ · B(Ci)); see also (Bikel, 2004). In our model we have chosen µ to be 5 for the first model, and 50 for the second and the third. 9Those which have certain function tags (e.g. ADV, LOC, TMP). The full list is reported in (Sangati and Mazza, 2009). It was surprising to notice that the performance of this slightly modified parser (in terms of F-score) is only slightly lower than how it performs out-of-the-box (0.13%). we believe that this additional information might benefit our model. We then applied our probabilistic model to rerank the list of available k-best TDS, and evaluate the selected candidates using several metrics which will be introduced next. 3.4 Evaluation Metrics for TDS The re-ranking framework described above, allows us to keep track of the original PS of each TDS candidate. This provides an implicit advantage for eval</context>
</contexts>
<marker>Sangati, Mazza, 2009</marker>
<rawString>Federico Sangati and Chiara Mazza. 2009. An English Dependency Treebank a` la Tesni`ere. In The 8th International Workshop on Treebanks and Linguistic Theories, pages 173–184, Milan, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Federico Sangati</author>
<author>Willem Zuidema</author>
<author>Rens Bod</author>
</authors>
<title>A generative re-ranking model for dependency parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of the 11th International Conference on Parsing Technologies (IWPT’09),</booktitle>
<pages>238--241</pages>
<location>Paris, France,</location>
<contexts>
<context position="8373" citStr="Sangati et al., 2009" startWordPosition="1332" endWordPosition="1335">ks(S) PBEM(S) = � P(elements(B)|derivedCat(B)) B ∈ blocks(S) PWFM(S) = � P(cw(B)|cw(parent(B)), cats(B), fw(B),context(B)) B ∈ standardBlocks(S) Table 1: Equation (1) gives the likelihood of a structure S as the product of the likelihoods of generating three aspects of the structure, according to the three models (BGM, BEM, WFM) specified in equations (2-4) and explained in the main text. 3 A probabilistic Model for TDS This section describes the probabilistic generative model which was implemented in order to disambiguate TDS structures. We have chosen the same strategy we have described in (Sangati et al., 2009). The idea consists of utilizing a state of the art parser to compute a list of k-best candidates of a test sentence, and evaluate the new model by using it as a reranker. How well does it select the most probable structure among the given candidates? Since no parser currently exists for the TDS representation, we utilize a state of the art parser for PS trees (Charniak, 1999), and transform each candidate to TDS. This strategy can be considered a first step to efficiently test and compare different models before implementing a full-fledged parser. 3.1 Model description In order to compute the</context>
</contexts>
<marker>Sangati, Zuidema, Bod, 2009</marker>
<rawString>Federico Sangati, Willem Zuidema, and Rens Bod. 2009. A generative re-ranking model for dependency parsing. In Proceedings of the 11th International Conference on Parsing Technologies (IWPT’09), pages 238–241, Paris, France, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerold Schneider</author>
</authors>
<title>Hybrid long-distance functional dependency parsing.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Zurich.</institution>
<contexts>
<context position="1390" citStr="Schneider, 2008" startWordPosition="193" endWordPosition="194">Phrase Structure version of the treebank. We obtain encouraging results: we achieve a small improvement over state-of-the-art results when re-ranking a small number of candidate structures, on all the evaluation metrics except for chunking. 1 Introduction Since its origin, computational linguistics has been dominated by Constituency/Phrase Structure (PS) representation of sentence structure. However, recently, we observe a steady increase in popularity of Dependency Structure (DS) formalisms. Several researchers have compared the two alternatives, in terms of linguistic adequacy (Nivre, 2005; Schneider, 2008), practical applications (Ding and Palmer, 2005), and evaluations (Lin, 1995). Dependency theory is historically accredited to Lucien Tesni`ere (1959), although the relation of dependency between words was only one of the various key elements proposed to represent sentence structures. In fact, the original formulation incorporates the notion of chunk, as well as a special type of structure to represent coordination. The Tesni`ere Dependency Structure (TDS) representation we propose in (Sangati and Mazza, 2009), is an attempt to formalize the original work of Tesni`ere, with the intention to de</context>
</contexts>
<marker>Schneider, 2008</marker>
<rawString>Gerold Schneider. 2008. Hybrid long-distance functional dependency parsing. Ph.D. thesis, University of Zurich.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucien Tesni`ere</author>
</authors>
<title>El´ements de syntaxe structurale. Editions Klincksieck,</title>
<date>1959</date>
<location>Paris.</location>
<marker>Tesni`ere, 1959</marker>
<rawString>Lucien Tesni`ere. 1959. El´ements de syntaxe structurale. Editions Klincksieck, Paris.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>