<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000428">
<title confidence="0.610784666666667">
RELATING COMPLEXITY TO PRACTICAL
PERFORMANCE IN PARSING WITH WIDE-COVERAGE
UNIFICATION GRAMMARS
</title>
<author confidence="0.991335">
John Carroll
</author>
<affiliation confidence="0.999916">
University of Cambridge, Computer Laboratory
</affiliation>
<address confidence="0.751578">
Pembroke Street, Cambridge CB2 3QG, UK
</address>
<email confidence="0.998448">
jac@cl.cam.ac.uk
</email>
<sectionHeader confidence="0.980056" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999898555555556">
The paper demonstrates that exponential com-
plexities with respect to grammar size and input
length have little impact on the performance of
three unification-based parsing algorithms, using
a wide-coverage grammar. The results imply that
the study and optimisation of unification-based
parsing must rely on empirical data until complex-
ity theory can more accurately predict the practi-
cal behaviour of such parsers&apos;.
</bodyText>
<sectionHeader confidence="0.995267" genericHeader="introduction">
1. INTRODUCTION
</sectionHeader>
<bodyText confidence="0.975545692307692">
General-purpose natural language (NL) analysis
systems have recently started to use declarative
unification-based sentence grammar formalisms;
systems of this type include SRI&apos;s CLARE sys-
tem (Alshawi et al., 1992) and the Alvey NL Tools
(ANLT; Briscoe et al., 1987a). Using a declarative
formalism helps ease the task of developing and
maintaining the grammar (Kaplan, 1987). In ad-
dition to syntactic processing, the systems incor-
porate lexical, morphological, and semantic pro-
cessing, and have been applied successfully to the
analysis of naturally-occurring texts (e.g. Alshawi
et at., 1992; Briscoe &amp; Carroll, 1993).
Evaluations of the grammars in these par-
ticular systems have shown them to have wide
coverage (Alshawi et at., 1992; Taylor, Grover &amp;
Briscoe, 1989)2. However, although the practical
throughput of parsers with such realistic gram-
mars is important, for example when process-
&apos;This research was supported by SERC/DTI
project 4/1/1261 &apos;Extensions to the Alvey Natu-
ral Language Tools&apos; and by EC ESPRIT BRA-7315
`ACQUILEX-IF. I am grateful to Ted Briscoe for com-
ments on an earlier version of this paper, to David
Weir for valuable discussions, and to Hiyan Alshawi
for assistance with the CLARE system.
</bodyText>
<footnote confidence="0.60034075">
2For example, Taylor et at. demonstrate that the
ANLT grammar is in principle able to analyse 96.8%
of a corpus of 10,000 noun phrases taken from a variety
of corpora.
</footnote>
<bodyText confidence="0.999786023255814">
ing large amounts of text or in interactive ap-
plications, there is little published research that
compares the performance of different parsing
algorithms using wide-coverage unification-based
grammars. Previous comparisons have either fo-
cussed on context-free (CF) or augmented CF
parsing (Tomita, 1987; Billot &amp; Lang, 1989),
or have used relatively small, limited-coverage
unification grammars and lexicons (Shann, 1989;
Bouma &amp; van Noord, 1993; Maxwell &amp; Kaplan,
1993). It is not clear that these results scale
up to reflect accurately the behaviour of parsers
using realistic, complex unification-based gram-
mars: in particular, with grammars admitting less
ambiguity parse time will tend to increase more
slowly with increasing input length, and also with
smaller grammars rule application can be con-
strained tightly with relatively simple predictive
techniques. Also, since none of these studies relate
observed performance to that of other comparable
parsing systems, implementational oversights may
not be apparent and so be a confounding factor in
any general conclusions made.
Other research directed towards improving
the throughput of unification-based parsing sys-
tems has been concerned with the unification oper-
ation itself, which can consume up to 90% of parse
time (e.g. Tomabechi, 1991) in systems using lex-
icalist grammar formalisms (e.g. HPSG; Pollard
&amp; Sag, 1987). However, parsing algorithms as-
sume more importance for grammars having more
substantial phrase structure components, such as
CLARE (which although employing some HPSG-
like analyses still contains several tens of rules)
and the ANLT (which uses a formalism derived
from GPSG; Gazdar et at., 1985), since the more
specific rule set can be used to control which uni-
fications are performed.
In NL analysis, the syntactic information as-
sociated with lexical items makes top-down pars-
ing less attractive than bottom-up (e.g. CKY;
Kasami, 1965; Younger, 1967), although the lat-
ter is often augmented with top-down predic-
</bodyText>
<page confidence="0.992302">
287
</page>
<bodyText confidence="0.9996812">
tion to improve performance (e.g. Earley, 1970;
Lang, 1974; Pratt, 1975). Section 2 describes
three unification-based parsers which are related
to polynomial-complexity bottom-up CF parsing
algorithms. Although incorporating unification
increases their complexity to exponential on gram-
mar size and input length (section 3), this ap-
pears to have little impact on practical perfor-
mance (section 4). Sections 5 and 6 discuss these
findings and present conclusions.
</bodyText>
<sectionHeader confidence="0.995025" genericHeader="method">
2. THE PARSERS
</sectionHeader>
<bodyText confidence="0.999930071428571">
The three parsers in this study are: a bottom-
up left-corner parser, a (non-deterministic) LR
parser, and an LR-like parser based on an algo-
rithm devised by Schabes (1991). All three parsers
accept grammars written in the ANLT formal-
ism (Briscoe et al., 1987a), and the first two are
distributed as part of the ANLT package. The
parsers create parse forests (Tomita, 1987) that
incorporate subtree sharing (in which identical
sub-analyses are shared between differing super-
ordinate analyses) and node packing (where sub-
analyses covering the same portion of input whose
root categories are in a subsumption relationship
are merged into a single node).
</bodyText>
<sectionHeader confidence="0.9580925" genericHeader="method">
THE BOTTOM-UP LEFT-CORNER
PARSER
</sectionHeader>
<bodyText confidence="0.934010333333333">
The bottom-up left-corner (BU-LC) parser oper-
ates left-to-right and breadth-first, storing partial
(active) constituents in a chart; Carroll (1993)
gives a full description. Although pure bottom-
up parsing is not usually thought of as provid-
ing high performance, the actual implementation
achieves very good throughput (see section 4) due
to a number of significant optimisations, amongst
which are:
</bodyText>
<listItem confidence="0.941501">
• Efficient rule invocation from cheap (static) rule
indexing, using discrimination trees keyed on
the feature values in each rule&apos;s first daughter
to interleave rule access with unification and
also to share unification results across groups
of rules.
• Dynamic indexing of partial and complete con-
stituents on category types to avoid attempt-
ing unification or subsumption operations which
static analysis shows will always fail.
• Dynamic storage minimisation, deferring struc-
ture copying—e.g. required by the unification
operation or by constituent creation—until ab-
solutely necessary (e g unification success or
parse success, respectively).
</listItem>
<bodyText confidence="0.998407">
The optimisations improve throughput by a factor
of more than three.
</bodyText>
<sectionHeader confidence="0.6479705" genericHeader="method">
THE NON-DETERMINISTIC LR
PARSER
</sectionHeader>
<bodyText confidence="0.999835055555555">
Briscoe &amp; Carroll (1993) describe a methodology
for constructing an LR parser for a unification-
based grammar, in which a CF &apos;backbone&apos; gram-
mar is automatically constructed from the unifi-
cation grammar, a parse table is constructed from
the backbone grammar, and a parser is driven by
the table and further controlled by unification of
the &apos;residue&apos; of features in the unification gram-
mar that are not encoded in the backbone. In
this parser, the LALR(1) technique (Aho, Sethi
&amp; Ullman, 1986) is used, in conjunction with
a graph-structured stack (Tomita, 1987), adapt-
ing for unification-based parsing Kipps&apos; (1989)
Tomita-like recogniser that achieves polynomial
complexity on input length through caching.
On each reduction the parser performs the
unifications specified by the unification grammar
version of the CF backbone rule being applied.
This constitutes an on-line parsing algorithm. In
the general case, the off-line variant (in which all
unifications are deferred until the complete CF
parse forest has been constructed) is not guaran-
teed to terminate; indeed, it usually does not do so
with the ANLT grammar. However, a drawback
to the on-line algorithm is that a variant of Kipps&apos;
caching cannot be used, since the cache must nec-
essarily assume that all reductions at a given ver-
tex with all rules with the same number of daugh-
ters build exactly the same constituent every time;
in general this is not the case when the daughters
are unification categories. A weaker kind of cache
on partial analyses (and thus unification results)
was found to be necessary in the implementation,
though, to avoid duplication of unifications; this
sped the parser up by a factor of about three, at
little space cost.
</bodyText>
<sectionHeader confidence="0.784896" genericHeader="method">
THE COMPILED-EARLEY PARSER
</sectionHeader>
<bodyText confidence="0.999204">
The Compiled-Earley (CE) parser is based on a
predictive chart-based CF parsing algorithm de-
vised by Schabes (1991) which is driven by a table
compiling out the predictive component of Ear-
ley&apos;s (1970) parser. The size of the table is related
linearly to the size of the grammar (unlike the LR
technique). Schabes demonstrates that this parser
always takes fewer steps than Earley&apos;s, although
its time complexity is the same: 0(n3). The space
complexity is also cubic, since the parser uses Ear-
ley&apos;s representation of parse forests.
The incorporation of unification into the CE
parser follows the methodology developed for
unification-based LR parsing described in the pre-
vious section: a table is computed from a CF
&apos;backbone&apos;, and a parser, augmented with on-line
unification and feature-based subsumption opera-
</bodyText>
<page confidence="0.99538">
288
</page>
<bodyText confidence="0.999273888888889">
tions, is driven by the table. To allow meaningful
comparison with the LR parser, the CE parser uses
a one-word lookahead version of the table, con-
structed using a modified LALR technique (Car-
roll, 1993)3.
To achieve the cubic time bound, the parser
must be able to retrieve in unit time all items in
the chart having a given state, and start and end
position in the input string. However, the obvious
array implementation, for say a ten word sentence
with the ANLT grammar, would contain almost
500000 elements. For this reason, the implementa-
tion employs a sparse representation for the array,
since only a small proportion of the elements are
ever filled. In this parser, the same sort of dupli-
cation of unifications occurs as in the LR parser,
so lists of partial analyses are cached in the same
Way
</bodyText>
<sectionHeader confidence="0.9833885" genericHeader="method">
3. COMPLEXITIES OF THE
PARSERS
</sectionHeader>
<bodyText confidence="0.9997482">
The two variables that determine a parser&apos;s com-
putational complexity are the grammar and the
input string (Barton, Berwick &amp; Ristad, 1987).
These are considered separately in the next two
sections.
</bodyText>
<sectionHeader confidence="0.726906" genericHeader="method">
GRAMMAR-DEPENDENT
COMPLEXITY
</sectionHeader>
<bodyText confidence="0.997979947368421">
The term dependent on the grammar in the time
complexity of the BU-LC unification-based parser
described above is 0(1C121R13), where ClI is the
number of categories implicit in the grammar, and
I RI, the number of rules. The space complexity is
dominated by the size of the parse forest, 0(ICI)
(these results are proved by Carroll, 1993). For
the ANLT grammar, in which features are nested
to a maximum depth of two, ClI is finite but nev-
ertheless extremely large (Briscoe et at., 1987b)4.
The grammar-dependent complexity of the
LR parser makes it also appear intractable: John-
son (1989) shows that the number of LR(0) states
for certain (pathological) grammars is exponen-
tially related to the size of the grammar, and that
there are some inputs which force an LR parser
to visit all of these states in the course of a parse.
3Schabes describes a table with no loolcahead; the
successful application of this technique supports Sch-
abes&apos; (1991:109) assertion that &amp;quot;several other methods
(such as LR(k)-like and SLR(k)-like) can also be used
for constructing the parsing tables [...]&amp;quot;
4 Barton, Berwick &amp; Ristad (1987:221) calculate
that GPSG, also with a maximum nesting depth of
two, licences more than 10775 distinct syntactic cate-
gories. The number of categories is actually infinite in
grammars that use a fully recursive feature system.
Thus the total number of operations performed,
and also space consumed (by the vertices in the
graph-structured stack), is an exponential func-
tion of the size of the grammar.
To avoid this complexity, the CE parser em-
ploys a table construction method which ensures
that the number of states in the parse table is
linearly related to the size of the grammar, re-
sulting in the number of operations performed by
the parser being at worst a polynomial function of
grammar size.
</bodyText>
<sectionHeader confidence="0.9350675" genericHeader="method">
INPUT-DEPENDENT
COMPLEXITY
</sectionHeader>
<bodyText confidence="0.972666488372093">
Although the complexity of returning all parses
for a string is always related exponentially to its
length (since the number of parses is exponen-
tial, and they must all at least be enumerated),
the complexity of a parser is usually measured for
the computation of a parse forest (unless extract-
ing a single analysis from the forest is worse than
linear)5.
If one of the features of the ANLT grammar
formalism, the kleene operator (allowing indefinite
repetition of rule daughters), is disallowed, then
the complexity of the BU-LC parser with respect
to the length of the input string is 0(nP+1), where
p is the maximum number of daughters in a rule
(Carroll, 1993). The inclusion of the operator in-
creases the complexity to exponential. To retain
the polynomial time bound, new rules can be in-
troduced to produce recursive tree structures in-
stead of an iterated fiat tree structure. However,
when this technique is applied to the ANLT gram-
mar the increased overheads in rule invocation and
structure building actually slow the parser down.
Although the time and space complexities of
CF versions of the LR and CE parsers are 0(n3),
the unification versions of these parsers both turn
out to have time bounds that are greater than cu-
bic, in the general case. The CF versions implicitly
pack identical sequences of sub-analyses, and in
all reductions at a given point with rules with the
same number of daughters, the packed sequences
can be formed into higher-level constituents as
they stand without further processing. However,
in the unification versions, on each reduce action
the daughters of the rule involved have to be uni-
fied with every possible alternative sequence of the
sub-analyses that are being consumed by the rule
5This complexity measure does correspond to real
world usage of a parser, since practical systems can
usually afford to extract only a small number of parses
from the frequently very large number encoded in a
forest; this is often done on the basis of preference-
based or probabilistic factors (e.g. Carroll &amp; Briscoe,
1992).
</bodyText>
<page confidence="0.99179">
289
</page>
<bodyText confidence="0.999749">
(in effect expanding and flattening out the packed
sequences), leading to a bound of nP+1 on the total
number of unifications.
</bodyText>
<sectionHeader confidence="0.982854" genericHeader="method">
4. PRACTICAL RESULTS
</sectionHeader>
<bodyText confidence="0.999992757575758">
To assess the practical performance of the three
unification-based parsers described above, a series
of experiments were conducted using the ANLT
grammar (Grover, Carroll &amp; Briscoe, 1993), a
wide-coverage grammar of English. The gram-
mar is defined in metagrammatical formalism
which is compiled into a unification-based &apos;ob-
ject grammar&apos;—a syntactic variant of the Defi-
nite Clause Grammar formalism (Pereira &amp; War-
ren, 1980)—containing 84 features and 782 phrase
structure rules. Parsing uses fixed-arity term uni-
fication. The grammar provides full coverage
of the following constructions: declarative sen-
tences, imperatives and questions (yes/no, tag and
wh-questions); all unbounded dependency types
(topicalisation, relativisation, wh-questions); a
relatively exhaustive treatment of verb and ad-
jective complement types; phrasal and preposi-
tional verbs of many complement types; passivi-
sation; verb phrase extraposition; sentence and
verb phrase modification; noun phrase comple-
ments and pre- and post-modification; partitives;
coordination of all major category types; and nom-
inal and adjectival comparatives.
Although the grammar is linked to a lexi-
con containing definitions for 40000 base forms of
words, the experiments draw on a much smaller
lexicon of 600 words (consisting of closed class
vocabulary and, for open-class vocabulary, defi-
nitions of just a sample of words which taken to-
gether exhibit the full range of possible comple-
mentation patterns), since issues of lexical cover-
age are of no concern here.
</bodyText>
<sectionHeader confidence="0.991933" genericHeader="method">
COMPARING THE PARSERS
</sectionHeader>
<bodyText confidence="0.999906823529412">
In the first experiment, the ANLT grammar was
loaded and a set of sentences was input to each
of the three parsers. In order to provide an inde-
pendent basis for comparison, the same sentences
were also input to the SRI Core Language En-
gine (CLE) parser (Moore &amp; Alshawi, 1992) with
the CLARE2.5 grammar (Alshawi et al., 1992), a
state-of-the-art system accessible to the author.
The sentences were taken from an initial sam-
ple of 175 representative sentences extracted from
a corpus of approximately 1500 that form part of
the ANLT package. This corpus, implicitly defin-
ing the types of construction the grammar is in-
tended to cover, was written by the linguist who
developed the ANLT grammar and is used to check
for any adverse effects on coverage when the gram-
mar is modified during grammar development. Of
</bodyText>
<table confidence="0.999669">
Parser Grammar CPU time Storage
allocated
BU-LC ANLT 75.5 47.0
LR ANLT 48.9 33.6
CE ANLT 98.4 38.5
CLE CLARE2.5 277.7 –
</table>
<tableCaption confidence="0.999314">
Table 1: Parse times (in CPU seconds on a Sun
</tableCaption>
<bodyText confidence="0.974857978723404">
Sparc ELC workstation) and storage allocated (in
megabytes) while parsing the 129 test sentences
(1-12 words in length).
the initial 175 sentences, the CLARE2.5 grammar
failed to parse 42 (in several cases because punc-
tuation is strictly required but is missing from the
corpus). The ANLT grammar also failed to parse
three of these, plus an additional four. These sen-
tences were removed from the sample, leaving 129
(mean length 6.7 words) of which 47 were declar-
ative sentences, 38 wh-questions and other sen-
tences with gaps, 20 passives, and 24 sentences
containing co-ordination.
Table 1 shows the total parse times and stor-
age allocated for the BU-LC parser, the LR parser,
and the CE parser, all with ANLT grammar
and lexicon. All three parsers have been im-
plemented by the author to a similar high stan-
dard: similar implementation techniques are used
in all the parsers, the parsers share the same uni-
fication module, run in the same Lisp environ-
ment, have been compiled with the same optimisa-
tion settings, and have all been profiled with the
same tools and hand-optimised to a similar ex-
tent. (Thus any difference in performance of more
than around 15% is likely to stem from algorithmic
rather than implementational reasons). Both of
the predictive parsers employ one symbol of looka-
head, incorporated into the parsing tables by the
LALR technique. Table 1 also shows the results
for the CLE parser with the CLARE2.5 grammar
and lexicon. The figures include garbage collection
time, and phrasal (where appropriate) processing,
but not parse forest unpacking. Both grammars
give a total of around 280 analyses at a similar
level of detail.
The results show that the LR parser is ap-
proximately 35% faster than the BU-LC parser,
and allocates about 30% less storage. The mag-
nitude of the speed-up is less than might be ex-
pected, given the enthusiastic advocation of non-
deterministic CF LR parsing for NL by some re-
searchers (e.g. Tomita, 1987; Wright, Wrigley &amp;
Sharman, 1991), and in the light of improvements
observed for predictive over pure bottom-up pars-
ing (e.g. Moore &amp; Dowding, 1991). However, on
the assumption that incorrect prediction of gaps is
</bodyText>
<page confidence="0.987011">
290
</page>
<bodyText confidence="0.99997428125">
the main avoidable source of performance degra-
dation (c.f. Moore &amp; Dowding), further investiga-
tion shows that the speed-up is near the maximum
that is possible with the ANLT grammar (around
50%).
The throughput of the CE parser is half that
of the LR parser, and also less than that of the
BU-LC parser. However, it is intermediate be-
tween the two in terms of storage allocated. Part
of the difference in performance between it and
the LR parser is due to the fact that it performs
around 15% more unifications. This might be
expected since the corresponding finite state au-
tomaton is not determinised—to avoid theoretical
exponential time complexity on grammar size
thus paying a price at run time. Additional rea-
sons for the relatively poor performance of the CE
parser are the overheads involved in maintaining
a sparse representation of the chart, and the fact
that with the ANLT grammar it generates less
&amp;quot;densely packed&amp;quot; parse forests, since its parse ta-
ble, with 14% more states (though fewer actions)
than the LALR(1) table, encodes more contextual
distinctions (Billot &amp; Lang, 1989:146).
Given that the ANLT and CLARE2.5 gram-
mars have broadly similar (wide) coverage and re-
turn very similar numbers of syntactic analyses for
the same inputs, the significantly better through-
put of the three parsers described in this paper
over the CLE parser6 indicates that they do not
contain any significant implementational deficien-
cies which would bias the results7.
</bodyText>
<sectionHeader confidence="0.9936865" genericHeader="method">
SWAPPING THE GRAMMARS
OVER
</sectionHeader>
<bodyText confidence="0.999989090909091">
A second experiment was carried out with the
CLE parser, in which the built-in grammar and
lexicon were replaced by versions of the ANLT ob-
ject grammar and lexical entries translated (auto-
matically) into the CLE formalism. (The reverse
of this configuration, in which the CLARE2.5
grammar is translated into the ANLT formalism,
is not possible since some central rules contain
sequences of daughters specified by a single &apos;list&apos;
variable, which has no counterpart in the ANLT
and cannot directly be simulated). The through-
</bodyText>
<footnote confidence="0.56846525">
6Although the ANLT parser is implemented in
Common Lisp and the CLE parser in Prolog, compar-
ing parse times is a valid exercise since current com-
piler and run-time support technologies for both lan-
guages are quite well-developed, and in fact the CLE
parser takes advantage of Prolog&apos;s built-in unification
operation which will have been very tightly coded.
7The ANLT&apos;s speed advantage over CLARE is less
pronounced if the time for morphological analysis and
creation of logical forms is taken into account, proba-
bly because the systems use different processing tech-
niques in these modules.
</footnote>
<bodyText confidence="0.999681625">
put of this configuration was only one fiftieth of
that of the BU-LC parser. The ANLT grammar
contains more than five times as many rules than
does the sentence-level portion of the CLARE2.5
grammar, and Alshawi (personal communication)
points out that the CLE parser had not previously
been run with a grammar containing such a large
number of rules, in contrast to the ANLT parsers.
</bodyText>
<sectionHeader confidence="0.972771" genericHeader="method">
THE EFFECT OF SENTENCE
LENGTH
</sectionHeader>
<bodyText confidence="0.999952">
Although the mean sentence length in the first two
experiments is much shorter than the 20-30 word
length (depending on genre etc.) that is common
in real texts, the test sentences cover a wide range
of syntactic constructions and exhibit less con-
structional bias than would a set of sentences ex-
tracted at random from a single corpus. However,
to investigate performance on longer sentences and
the relationship between sentence length and parse
time, a further set of 100 sentences with lengths
distributed uniformly between 13 and 30 words
was created by hand by the author and added to
the previous test data. Table 2 shows the relation-
ship between sentence length and mean parse time
with the BU-LC and LR parsers.
In contrast to the results from the first exper-
iment, the throughput of the LR parser is only
4% better than that of the BU-LC parser for sen-
tences of 13-27 words in length. The former parses
many sentences up to twice as fast, but a small
proportion of the others are parsed almost twice
as slowly. As well as their wide variability with
respect to the BU-LC parser, the absolute vari-
ability of the LR parse times is high (reflected in
large standard deviations—a--see Table 2). Most
of the sentences for which LR performance is worse
contain more than one occurrence of the passive
construction: due to their length this is particu-
larly the case for the group of sentences of 28-30
words with which the LR parser performed partic-
ularly badly. However, it is likely that if the con-
straining power of the parse table were improved
in this area the difference in throughput between
LR and BU-LC would revert to nearer the 35%
figure seen in the first experiment.
The standard deviations for numbers of parses
are also relatively large. The maximum number of
parses was 2736 for one 29-word sentence, but on
the other hand some of even the longest sentences
had fewer than ten parses. (But note that since
the time taken for parse forest unpacking is not
included in parse times, the latter do not vary by
such a large magnitude).
The results of this experiment are displayed
graphically in Figure 1, together with a quadratic
function. Comparison with the function suggests
</bodyText>
<page confidence="0.994966">
291
</page>
<table confidence="0.998716769230769">
Sentence BU-LC time LR time Number of
length Parse a Parse a parses a
(words) Mean Mean Mean
1-3 0.11 0.06 0.05 0.02 1.3 0.7
4-6 0.23 0.18 0.15 0.11 1.4 0.8
7-9 0.42 0.24 0.28 0.17 1.8 1.3
10-12 1.17 0.92 0.76 0.52 3.8 2.4
13-15 0.97 0.28 0.86 0.38 10.0 13.7
16-18 1.92 0.75 1.89 1.00 14.3 17.5
19-21 3.54 1.42 3.74 2.46 60.1 117.3
22-24 3.87 1.62 3.61 3.07 143.8 200.1
25-27 5.45 1.98 5.05 3.59 168.8 303.1
28-30 7.86 2.37 12.89 5.65 343.5 693.7
</table>
<tableCaption confidence="0.993795">
Table 2: Mean and standard deviation parse times (in CPU seconds on an HP9000/710 workstation), and
</tableCaption>
<bodyText confidence="0.968438714285714">
numbers of parses for the 229 test sentences (1-30 words in length) with the BU-LC and LR parsers.
that, at least for the BU-LC parser, parse time is
related roughly quadratically to input length.
In previous work with the ANLT (Briscoe &amp;
Carroll, 1993), throughput with raw corpus data
was worse than that observed in these experi-
ments, though probably only by a constant factor.
This could be due to the fact that the vocabu-
lary of the corpus concerned exhibits significantly
higher lexical ambiguity; however, for sentences
taken from a specific corpus, constructional bias
observed in a training phase could be exploited to
improve performance (e.g. Samuelsson &amp; Rayner,
1991).
</bodyText>
<sectionHeader confidence="0.99693" genericHeader="evaluation">
5. DISCUSSION
</sectionHeader>
<bodyText confidence="0.999973946428572">
All three of the parsers have theoretical worst-case
complexities that are either exponential, or poly-
nomial on grammar size but with an extremely
large multiplier. Despite this, in the practical
experiments reported in the previous section the
parsers achieve relatively good throughput with a
general-purpose wide-coverage grammar of a nat-
ural language. It therefore seems likely that gram-
mars of the type considered in this paper (i.e. with
relatively detailed phrase structure components,
but comparatively simple from a unification per-
spective), although realistic, do not bring the pars-
ing algorithms involved anywhere near the worst-
case complexity.
In the experiments, the CE technique results
in a parser with worse performance than the nor-
mal LR technique. Indeed, for the ANLT gram-
mar, the number of states-the term that the CE
technique reduces from exponential to linear on
the grammar size-is actually smaller in the stan-
dard LALR(1) table. This suggests that, when
considering the complexity of parsers, the issue of
parse table size is of minor importance for realistic
NL grammars (as long as an implementation rep-
resents the table compactly), and that improve-
ments to complexity results with respect to gram-
mar size, although interesting from a theoretical
standpoint, may have little practical relevance for
the processing of natural language.
Although Schabes (1991:107) claims that the
problem of exponential grammar complexity &amp;quot;is
particularly acute for natural language processing
since in this context the input length is typically
small (10-20 words) and the grammar size very
large (hundreds or thousands of rules and sym-
bols)&amp;quot;, the experiments indicate that, with a wide-
coverage NL grammar, inputs of this length can
be parsed quite quickly; however, longer inputs
(of more than about 30 words in length)-which
occur relatively frequently in written text-are a
problem. Unless grammar size takes on propor-
tionately much more significance for such longer
inputs, which seems implausible, it appears that
in fact the major problems do not lie in the area
of grammar size, but in input length.
All three parsers have worst-case complexities
that are exponential on input length. This theo-
retical bound might suggest that parsing perfor-
mance would be severely degraded on long sen-
tences; however, the relationship between length
of sentence and parse time with the ANLT gram-
mar and the sentences tested appears to be ap-
proximately only quadratic. There are probably
many reasons why performance is much better
than the complexity results suggest, but the most
important may be that:
</bodyText>
<listItem confidence="0.985500142857143">
• kleene star is used only in a very limited context
(for the analysis of coordination),
• more than 90% of the rules in the grammar have
no more than two daughters, and
• very few rules license both left and right re-
cursion (for instance of the sort that is typi-
cally used to analyse noun compounding, i.e.
</listItem>
<page confidence="0.99466">
292
</page>
<figureCaption confidence="0.9902695">
Figure 1: Mean parse times (in CPU seconds on an HP9000/710 workstation) for the test sentences with
the BU-LC and LR parsers. A quadratic function is also displayed.
</figureCaption>
<figure confidence="0.520023333333333">
4-6 7-9 10-12 13-15 16-18 19-21 22-24 25-27 28-30
Sentence length (n)
N --&gt; N N).
</figure>
<bodyText confidence="0.999468588235294">
Despite little apparent theoretical difference
between the CLE and ANLT grammar formalisms,
and the fact that no explicit or formal process
of &apos;tuning&apos; parsers and grammars to perform well
with each other has been carried out in either of
the ANLT or CLARE systems, the results of the
experiment comparing the performance of the re-
spective parsers using the ANLT grammar sug-
gests that the parallel development of the software
and grammars that has occurred nevertheless ap-
pears to have caused this to happen automatically.
It therefore seems likely that implementational de-
cisions and optimisations based on subtle proper-
ties of specific grammars can, and may very of-
ten be, more important than worst-case complex-
ity when considering the practical performance of
parsing algorithms.
</bodyText>
<sectionHeader confidence="0.999934" genericHeader="conclusions">
6. CONCLUSIONS
</sectionHeader>
<bodyText confidence="0.999956">
The research reported is in a similar vein to
that of, for example, Moore &amp; Dowding (1991),
Samuelsson 8,z Rayner (1991), and Maxwell Si Ka-
plan (1993), in that it relies on empirical results
for the study and optimisation of parsing algo-
rithms rather than on traditional techniques of
complexity analysis. The paper demonstrates that
research in this area will have to rely on empiri-
cal data until complexity theory is developed to a
point where it is sufficiently fine-grained and ac-
curate to predict how the properties of individual
unification-based grammars will interact with par-
ticular parsing algorithms to determine practical
performance.
</bodyText>
<sectionHeader confidence="0.994956" genericHeader="references">
REFERENCES
</sectionHeader>
<bodyText confidence="0.978159857142857">
Aho, A., R. Sethi &amp; J. Ullman (1986) Compilers:
principles, techniques and tools. Reading, MA:
Addison-Wesley.
Alshawi, H., D. Carter, R. Crouch, S. Pulman, M.
Rayner &amp; A. Smith (1992) CLARE: a contex-
tual reasoning and cooperative response frame-
work for the Core Language Engine. SRI In-
</bodyText>
<reference confidence="0.954044888888889">
ternational, Cambridge, UK.
Barton, G., R. Berwick &amp; E. Ristad (1987) Com-
putational complexity and natural language.
Cambridge, MA: MIT Press.
Billot, S. Si B. Lang (1989) &amp;quot;The structure of
shared forests in ambiguous parsing.&amp;quot; In Pro-
ceedings of the 27th Meeting of the Association
for Computational Linguistics. 143-151.
Bouma, G. &amp; G. van Noord (1993) &amp;quot;Head-driven
parsing for lexicalist grammars: experimental
results.&amp;quot; In Proceedings of the 6th Conference
of the European Chapter of the Association for
Computational Linguistics. 101-105.
Briscoe, E., C. Grover, B. Boguraev &amp; J. Carroll
(1987a) &amp;quot;A formalism and environment for the
development of a large grammar of English.&amp;quot;
In Proceedings of the 10th International Joint
Conference on Artificial Intelligence. 703-708.
</reference>
<page confidence="0.993803">
293
</page>
<reference confidence="0.982275036363637">
Briscoe, E., C. Grover, B. Boguraev &amp; J. Carroll
(1987b) &amp;quot;Feature defaults, propagation and
reentrancy.&amp;quot; In Categories, Polymorphism and
Unification, edited by E. Klein &amp; J. van Ben-
them, Centre for Cognitive Science, Edinburgh
University, UK. 19-34.
Briscoe, E. &amp; J. Carroll (1993) &amp;quot;Generalised
probabilistic LR parsing of natural language
(corpora) with unification-based grammars.&amp;quot;
Computational Linguistics, 19(1): 25-59.
Carroll, J. (1993) Practical unification-based pars-
ing of natural language. Computer Laboratory,
Cambridge University, UK, Technical Report
314.
Carroll, J. &amp; E. Briscoe (1992) &amp;quot;Probabilistic
normalisation and unpacking of packed parse
forests for unification-based grammars.&amp;quot; In
Proceedings of the AAAI Fall Symposium on
Probabilistic Approaches to Natural Language.
33-38.
Earley, .J. (1970) &amp;quot;An efficient context-free pars-
ing algorithm.&amp;quot; Communications of the ACM,
13.2: 94-102.
Gazdar, G., E. Klein, G. Pullum &amp; I. Sag (1985)
Generalized phrase structure grammar. Ox-
ford, UK: Blackwell.
Grover, C., J. Carroll &amp; E. Briscoe (1993) The
Alvey natural language tools grammar (4th re-
lease). Computer Laboratory, Cambridge Uni-
versity, UK, Technical Report 284.
Johnson, M. (1989) &amp;quot;The computational complex-
ity of Tomita&apos;s algorithm.&amp;quot; In Proceedings of
the 1st International Workshop on Parsing
Technologies. 203-208.
Kaplan, R. (1987) &amp;quot;Three seductions of compu-
tational psycholinguistics.&amp;quot; In Linguistic The-
ory and Computer Applications, edited by P.
Whitelock et al., New York: Academic Press.
149-188.
Kasami, J. (1965) An efficient recognition and
syntax analysis algorithm for context-free lan-
guages. Air Force Cambridge Research Labo-
ratory, Bedford, MA, Report AFCRL-65-758.
Kipps, J. (1989) &amp;quot;Analysis of Tomita&apos;s algorithm
for general context-free parsing.&amp;quot; In Proceed-
ings of the 1st International Workshop on
Parsing Technologies. 193-202.
Lang, B. (1974) &amp;quot;Deterministic techniques for effi-
cient non-deterministic parsers.&amp;quot; In Automata,
Languages and Programming, Lecture Notes
in Computer Science 14, edited by J. Loeckx,
Berlin, Germany: Springer-Verlag. 255-269.
Maxwell, J. III &amp; R. Kaplan (1993) &amp;quot;The interface
between phrasal and functional constraints.&amp;quot;
Computational Linguistics, 19(4): 571-590.
Moore, R. &amp; H. Alshawi (1992) &amp;quot;Syntactic and se-
mantic processing.&amp;quot; In The Core Language En-
gine, edited by H. Alshawi, Cambridge, MA:
MIT Press. 129-148.
Moore, R. &amp; J. Dowding (1991) &amp;quot;Efficient bottom-
up parsing.&amp;quot; In Proceedings of the DARPA
Speech and Natural Language Workshop. 200-
203.
Pereira, F. &amp; D. Warren (1980) &amp;quot;Definite clause
grammars for language analysis—a survey of
the formalism and a comparison with aug-
mented transition networks.&amp;quot; Artificial Intel-
ligence, 13(3): 231-278.
Pollard, C. &amp; I. Sag (1987) Information-based syn-
tax and semantics: volume 1-fundamentals.
Chicago, IL: University of Chicago Press.
Pratt, V. (1975) &amp;quot;LINGOL - a progress report.&amp;quot;
In Proceedings of the 5th International Joint
Conference on Artificial Intelligence. 422-428.
Samuelsson, C. &amp; M. Rayner (1991) &amp;quot;Quantita-
tive evaluation of explanation-based learning
as an optimization tool for a large-scale nat-
ural language system.&amp;quot; In Proceedings of the
12th International Joint Conference on Artifi-
cial Intelligence. 609-615.
Schabes, Y. (1991) &amp;quot;Polynomial time and space
shift-reduce parsing of arbitrary context-free
grammars.&amp;quot; In Proceedings of the 29th Annual
Meeting of the Association for Computational
Linguistics. 106-113.
Taylor, L., C. Grover &amp; E. Briscoe (1989) &amp;quot;The
syntactic regularity of English noun phrases.&amp;quot;
In Proceedings of the 4th European Meeting of
the Association for Computational Linguistics.
256-263.
Tomabechi, H. (1991) &amp;quot;Quasi-destructive graph
unification.&amp;quot; In Proceedings of the 29th Annual
Meeting of the Association for Computational
Linguistics. 315-322.
Tomita, M. (1987) &amp;quot;An efficient augmented-
context-free parsing algorithm.&amp;quot; Computa-
tional Linguistics, 13(1): 31-46.
Shann, P. (1989) &amp;quot;The selection of a parsing strat-
egy for an on-line machine translation system
in a sublanguage domain. A new practical
comparison.&amp;quot; In Proceedings of the 1st Inter-
national Workshop on Parsing Technologies.
264-276.
Wright, J., E. Wrigley &amp; R. Sharman (1991)
&amp;quot;Adaptive probabilistic generalized LR pars-
ing.&amp;quot; In Proceedings of the 2nd International
Workshop on Parsing Technologies. 154-163.
Younger, D. (1967) &amp;quot;Recognition and parsing of
context-free languages in time id.&amp;quot; Informa-
tion and Control, 10(2): 189-208.
</reference>
<page confidence="0.998268">
294
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.862941">
<title confidence="0.993269333333333">RELATING COMPLEXITY TO PRACTICAL PERFORMANCE IN PARSING WITH WIDE-COVERAGE UNIFICATION GRAMMARS</title>
<author confidence="0.999317">John Carroll</author>
<affiliation confidence="0.999994">University of Cambridge, Computer Laboratory</affiliation>
<address confidence="0.997137">Pembroke Street, Cambridge CB2 3QG, UK</address>
<email confidence="0.99834">jac@cl.cam.ac.uk</email>
<abstract confidence="0.9876887">The paper demonstrates that exponential complexities with respect to grammar size and input length have little impact on the performance of three unification-based parsing algorithms, using a wide-coverage grammar. The results imply that the study and optimisation of unification-based parsing must rely on empirical data until complexity theory can more accurately predict the practical behaviour of such parsers&apos;.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>ternational</author>
</authors>
<location>Cambridge, UK.</location>
<marker>ternational, </marker>
<rawString>ternational, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Barton</author>
<author>R Berwick</author>
<author>E Ristad</author>
</authors>
<title>Computational complexity and natural language.</title>
<date>1987</date>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA:</location>
<marker>Barton, Berwick, Ristad, 1987</marker>
<rawString>Barton, G., R. Berwick &amp; E. Ristad (1987) Computational complexity and natural language. Cambridge, MA: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Si B Lang Billot</author>
</authors>
<title>The structure of shared forests in ambiguous parsing.&amp;quot;</title>
<date>1989</date>
<booktitle>In Proceedings of the 27th Meeting of the Association for Computational Linguistics.</booktitle>
<pages>143--151</pages>
<marker>Billot, 1989</marker>
<rawString>Billot, S. Si B. Lang (1989) &amp;quot;The structure of shared forests in ambiguous parsing.&amp;quot; In Proceedings of the 27th Meeting of the Association for Computational Linguistics. 143-151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Bouma</author>
<author>G van Noord</author>
</authors>
<title>Head-driven parsing for lexicalist grammars: experimental results.&amp;quot;</title>
<date>1993</date>
<booktitle>In Proceedings of the 6th Conference of the European Chapter of the Association for Computational Linguistics.</booktitle>
<pages>101--105</pages>
<marker>Bouma, van Noord, 1993</marker>
<rawString>Bouma, G. &amp; G. van Noord (1993) &amp;quot;Head-driven parsing for lexicalist grammars: experimental results.&amp;quot; In Proceedings of the 6th Conference of the European Chapter of the Association for Computational Linguistics. 101-105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Briscoe</author>
<author>C Grover</author>
<author>B Boguraev</author>
<author>J Carroll</author>
</authors>
<title>A formalism and environment for the development of a large grammar of English.&amp;quot;</title>
<date>1987</date>
<booktitle>In Proceedings of the 10th International Joint Conference on Artificial Intelligence.</booktitle>
<pages>703--708</pages>
<contexts>
<context position="915" citStr="Briscoe et al., 1987" startWordPosition="124" endWordPosition="127"> size and input length have little impact on the performance of three unification-based parsing algorithms, using a wide-coverage grammar. The results imply that the study and optimisation of unification-based parsing must rely on empirical data until complexity theory can more accurately predict the practical behaviour of such parsers&apos;. 1. INTRODUCTION General-purpose natural language (NL) analysis systems have recently started to use declarative unification-based sentence grammar formalisms; systems of this type include SRI&apos;s CLARE system (Alshawi et al., 1992) and the Alvey NL Tools (ANLT; Briscoe et al., 1987a). Using a declarative formalism helps ease the task of developing and maintaining the grammar (Kaplan, 1987). In addition to syntactic processing, the systems incorporate lexical, morphological, and semantic processing, and have been applied successfully to the analysis of naturally-occurring texts (e.g. Alshawi et at., 1992; Briscoe &amp; Carroll, 1993). Evaluations of the grammars in these particular systems have shown them to have wide coverage (Alshawi et at., 1992; Taylor, Grover &amp; Briscoe, 1989)2. However, although the practical throughput of parsers with such realistic grammars is importa</context>
<context position="4771" citStr="Briscoe et al., 1987" startWordPosition="717" endWordPosition="720">ed parsers which are related to polynomial-complexity bottom-up CF parsing algorithms. Although incorporating unification increases their complexity to exponential on grammar size and input length (section 3), this appears to have little impact on practical performance (section 4). Sections 5 and 6 discuss these findings and present conclusions. 2. THE PARSERS The three parsers in this study are: a bottomup left-corner parser, a (non-deterministic) LR parser, and an LR-like parser based on an algorithm devised by Schabes (1991). All three parsers accept grammars written in the ANLT formalism (Briscoe et al., 1987a), and the first two are distributed as part of the ANLT package. The parsers create parse forests (Tomita, 1987) that incorporate subtree sharing (in which identical sub-analyses are shared between differing superordinate analyses) and node packing (where subanalyses covering the same portion of input whose root categories are in a subsumption relationship are merged into a single node). THE BOTTOM-UP LEFT-CORNER PARSER The bottom-up left-corner (BU-LC) parser operates left-to-right and breadth-first, storing partial (active) constituents in a chart; Carroll (1993) gives a full description. </context>
</contexts>
<marker>Briscoe, Grover, Boguraev, Carroll, 1987</marker>
<rawString>Briscoe, E., C. Grover, B. Boguraev &amp; J. Carroll (1987a) &amp;quot;A formalism and environment for the development of a large grammar of English.&amp;quot; In Proceedings of the 10th International Joint Conference on Artificial Intelligence. 703-708.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Briscoe</author>
<author>C Grover</author>
<author>B Boguraev</author>
<author>J Carroll</author>
</authors>
<title>Feature defaults, propagation and reentrancy.&amp;quot; In Categories, Polymorphism and Unification,</title>
<date>1987</date>
<institution>Centre for Cognitive Science, Edinburgh University,</institution>
<location>UK.</location>
<note>edited by</note>
<contexts>
<context position="915" citStr="Briscoe et al., 1987" startWordPosition="124" endWordPosition="127"> size and input length have little impact on the performance of three unification-based parsing algorithms, using a wide-coverage grammar. The results imply that the study and optimisation of unification-based parsing must rely on empirical data until complexity theory can more accurately predict the practical behaviour of such parsers&apos;. 1. INTRODUCTION General-purpose natural language (NL) analysis systems have recently started to use declarative unification-based sentence grammar formalisms; systems of this type include SRI&apos;s CLARE system (Alshawi et al., 1992) and the Alvey NL Tools (ANLT; Briscoe et al., 1987a). Using a declarative formalism helps ease the task of developing and maintaining the grammar (Kaplan, 1987). In addition to syntactic processing, the systems incorporate lexical, morphological, and semantic processing, and have been applied successfully to the analysis of naturally-occurring texts (e.g. Alshawi et at., 1992; Briscoe &amp; Carroll, 1993). Evaluations of the grammars in these particular systems have shown them to have wide coverage (Alshawi et at., 1992; Taylor, Grover &amp; Briscoe, 1989)2. However, although the practical throughput of parsers with such realistic grammars is importa</context>
<context position="4771" citStr="Briscoe et al., 1987" startWordPosition="717" endWordPosition="720">ed parsers which are related to polynomial-complexity bottom-up CF parsing algorithms. Although incorporating unification increases their complexity to exponential on grammar size and input length (section 3), this appears to have little impact on practical performance (section 4). Sections 5 and 6 discuss these findings and present conclusions. 2. THE PARSERS The three parsers in this study are: a bottomup left-corner parser, a (non-deterministic) LR parser, and an LR-like parser based on an algorithm devised by Schabes (1991). All three parsers accept grammars written in the ANLT formalism (Briscoe et al., 1987a), and the first two are distributed as part of the ANLT package. The parsers create parse forests (Tomita, 1987) that incorporate subtree sharing (in which identical sub-analyses are shared between differing superordinate analyses) and node packing (where subanalyses covering the same portion of input whose root categories are in a subsumption relationship are merged into a single node). THE BOTTOM-UP LEFT-CORNER PARSER The bottom-up left-corner (BU-LC) parser operates left-to-right and breadth-first, storing partial (active) constituents in a chart; Carroll (1993) gives a full description. </context>
</contexts>
<marker>Briscoe, Grover, Boguraev, Carroll, 1987</marker>
<rawString>Briscoe, E., C. Grover, B. Boguraev &amp; J. Carroll (1987b) &amp;quot;Feature defaults, propagation and reentrancy.&amp;quot; In Categories, Polymorphism and Unification, edited by E. Klein &amp; J. van Benthem, Centre for Cognitive Science, Edinburgh University, UK. 19-34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Briscoe</author>
<author>J Carroll</author>
</authors>
<title>Generalised probabilistic LR parsing of natural language (corpora) with unification-based grammars.&amp;quot;</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<pages>25--59</pages>
<contexts>
<context position="1269" citStr="Briscoe &amp; Carroll, 1993" startWordPosition="176" endWordPosition="179">CTION General-purpose natural language (NL) analysis systems have recently started to use declarative unification-based sentence grammar formalisms; systems of this type include SRI&apos;s CLARE system (Alshawi et al., 1992) and the Alvey NL Tools (ANLT; Briscoe et al., 1987a). Using a declarative formalism helps ease the task of developing and maintaining the grammar (Kaplan, 1987). In addition to syntactic processing, the systems incorporate lexical, morphological, and semantic processing, and have been applied successfully to the analysis of naturally-occurring texts (e.g. Alshawi et at., 1992; Briscoe &amp; Carroll, 1993). Evaluations of the grammars in these particular systems have shown them to have wide coverage (Alshawi et at., 1992; Taylor, Grover &amp; Briscoe, 1989)2. However, although the practical throughput of parsers with such realistic grammars is important, for example when process&apos;This research was supported by SERC/DTI project 4/1/1261 &apos;Extensions to the Alvey Natural Language Tools&apos; and by EC ESPRIT BRA-7315 `ACQUILEX-IF. I am grateful to Ted Briscoe for comments on an earlier version of this paper, to David Weir for valuable discussions, and to Hiyan Alshawi for assistance with the CLARE system. 2</context>
<context position="6367" citStr="Briscoe &amp; Carroll (1993)" startWordPosition="951" endWordPosition="954"> to interleave rule access with unification and also to share unification results across groups of rules. • Dynamic indexing of partial and complete constituents on category types to avoid attempting unification or subsumption operations which static analysis shows will always fail. • Dynamic storage minimisation, deferring structure copying—e.g. required by the unification operation or by constituent creation—until absolutely necessary (e g unification success or parse success, respectively). The optimisations improve throughput by a factor of more than three. THE NON-DETERMINISTIC LR PARSER Briscoe &amp; Carroll (1993) describe a methodology for constructing an LR parser for a unificationbased grammar, in which a CF &apos;backbone&apos; grammar is automatically constructed from the unification grammar, a parse table is constructed from the backbone grammar, and a parser is driven by the table and further controlled by unification of the &apos;residue&apos; of features in the unification grammar that are not encoded in the backbone. In this parser, the LALR(1) technique (Aho, Sethi &amp; Ullman, 1986) is used, in conjunction with a graph-structured stack (Tomita, 1987), adapting for unification-based parsing Kipps&apos; (1989) Tomita-li</context>
<context position="24714" citStr="Briscoe &amp; Carroll, 1993" startWordPosition="3994" endWordPosition="3997">0.17 1.8 1.3 10-12 1.17 0.92 0.76 0.52 3.8 2.4 13-15 0.97 0.28 0.86 0.38 10.0 13.7 16-18 1.92 0.75 1.89 1.00 14.3 17.5 19-21 3.54 1.42 3.74 2.46 60.1 117.3 22-24 3.87 1.62 3.61 3.07 143.8 200.1 25-27 5.45 1.98 5.05 3.59 168.8 303.1 28-30 7.86 2.37 12.89 5.65 343.5 693.7 Table 2: Mean and standard deviation parse times (in CPU seconds on an HP9000/710 workstation), and numbers of parses for the 229 test sentences (1-30 words in length) with the BU-LC and LR parsers. that, at least for the BU-LC parser, parse time is related roughly quadratically to input length. In previous work with the ANLT (Briscoe &amp; Carroll, 1993), throughput with raw corpus data was worse than that observed in these experiments, though probably only by a constant factor. This could be due to the fact that the vocabulary of the corpus concerned exhibits significantly higher lexical ambiguity; however, for sentences taken from a specific corpus, constructional bias observed in a training phase could be exploited to improve performance (e.g. Samuelsson &amp; Rayner, 1991). 5. DISCUSSION All three of the parsers have theoretical worst-case complexities that are either exponential, or polynomial on grammar size but with an extremely large mult</context>
</contexts>
<marker>Briscoe, Carroll, 1993</marker>
<rawString>Briscoe, E. &amp; J. Carroll (1993) &amp;quot;Generalised probabilistic LR parsing of natural language (corpora) with unification-based grammars.&amp;quot; Computational Linguistics, 19(1): 25-59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carroll</author>
</authors>
<title>Practical unification-based parsing of natural language.</title>
<date>1993</date>
<tech>Technical Report 314.</tech>
<institution>Computer Laboratory, Cambridge University, UK,</institution>
<contexts>
<context position="1269" citStr="Carroll, 1993" startWordPosition="178" endWordPosition="179">ral-purpose natural language (NL) analysis systems have recently started to use declarative unification-based sentence grammar formalisms; systems of this type include SRI&apos;s CLARE system (Alshawi et al., 1992) and the Alvey NL Tools (ANLT; Briscoe et al., 1987a). Using a declarative formalism helps ease the task of developing and maintaining the grammar (Kaplan, 1987). In addition to syntactic processing, the systems incorporate lexical, morphological, and semantic processing, and have been applied successfully to the analysis of naturally-occurring texts (e.g. Alshawi et at., 1992; Briscoe &amp; Carroll, 1993). Evaluations of the grammars in these particular systems have shown them to have wide coverage (Alshawi et at., 1992; Taylor, Grover &amp; Briscoe, 1989)2. However, although the practical throughput of parsers with such realistic grammars is important, for example when process&apos;This research was supported by SERC/DTI project 4/1/1261 &apos;Extensions to the Alvey Natural Language Tools&apos; and by EC ESPRIT BRA-7315 `ACQUILEX-IF. I am grateful to Ted Briscoe for comments on an earlier version of this paper, to David Weir for valuable discussions, and to Hiyan Alshawi for assistance with the CLARE system. 2</context>
<context position="5344" citStr="Carroll (1993)" startWordPosition="802" endWordPosition="803"> the ANLT formalism (Briscoe et al., 1987a), and the first two are distributed as part of the ANLT package. The parsers create parse forests (Tomita, 1987) that incorporate subtree sharing (in which identical sub-analyses are shared between differing superordinate analyses) and node packing (where subanalyses covering the same portion of input whose root categories are in a subsumption relationship are merged into a single node). THE BOTTOM-UP LEFT-CORNER PARSER The bottom-up left-corner (BU-LC) parser operates left-to-right and breadth-first, storing partial (active) constituents in a chart; Carroll (1993) gives a full description. Although pure bottomup parsing is not usually thought of as providing high performance, the actual implementation achieves very good throughput (see section 4) due to a number of significant optimisations, amongst which are: • Efficient rule invocation from cheap (static) rule indexing, using discrimination trees keyed on the feature values in each rule&apos;s first daughter to interleave rule access with unification and also to share unification results across groups of rules. • Dynamic indexing of partial and complete constituents on category types to avoid attempting u</context>
<context position="9103" citStr="Carroll, 1993" startWordPosition="1394" endWordPosition="1396">lexity is the same: 0(n3). The space complexity is also cubic, since the parser uses Earley&apos;s representation of parse forests. The incorporation of unification into the CE parser follows the methodology developed for unification-based LR parsing described in the previous section: a table is computed from a CF &apos;backbone&apos;, and a parser, augmented with on-line unification and feature-based subsumption opera288 tions, is driven by the table. To allow meaningful comparison with the LR parser, the CE parser uses a one-word lookahead version of the table, constructed using a modified LALR technique (Carroll, 1993)3. To achieve the cubic time bound, the parser must be able to retrieve in unit time all items in the chart having a given state, and start and end position in the input string. However, the obvious array implementation, for say a ten word sentence with the ANLT grammar, would contain almost 500000 elements. For this reason, the implementation employs a sparse representation for the array, since only a small proportion of the elements are ever filled. In this parser, the same sort of duplication of unifications occurs as in the LR parser, so lists of partial analyses are cached in the same Way</context>
<context position="12469" citStr="Carroll, 1993" startWordPosition="1957" endWordPosition="1958">r a string is always related exponentially to its length (since the number of parses is exponential, and they must all at least be enumerated), the complexity of a parser is usually measured for the computation of a parse forest (unless extracting a single analysis from the forest is worse than linear)5. If one of the features of the ANLT grammar formalism, the kleene operator (allowing indefinite repetition of rule daughters), is disallowed, then the complexity of the BU-LC parser with respect to the length of the input string is 0(nP+1), where p is the maximum number of daughters in a rule (Carroll, 1993). The inclusion of the operator increases the complexity to exponential. To retain the polynomial time bound, new rules can be introduced to produce recursive tree structures instead of an iterated fiat tree structure. However, when this technique is applied to the ANLT grammar the increased overheads in rule invocation and structure building actually slow the parser down. Although the time and space complexities of CF versions of the LR and CE parsers are 0(n3), the unification versions of these parsers both turn out to have time bounds that are greater than cubic, in the general case. The CF</context>
<context position="24714" citStr="Carroll, 1993" startWordPosition="3996" endWordPosition="3997">.3 10-12 1.17 0.92 0.76 0.52 3.8 2.4 13-15 0.97 0.28 0.86 0.38 10.0 13.7 16-18 1.92 0.75 1.89 1.00 14.3 17.5 19-21 3.54 1.42 3.74 2.46 60.1 117.3 22-24 3.87 1.62 3.61 3.07 143.8 200.1 25-27 5.45 1.98 5.05 3.59 168.8 303.1 28-30 7.86 2.37 12.89 5.65 343.5 693.7 Table 2: Mean and standard deviation parse times (in CPU seconds on an HP9000/710 workstation), and numbers of parses for the 229 test sentences (1-30 words in length) with the BU-LC and LR parsers. that, at least for the BU-LC parser, parse time is related roughly quadratically to input length. In previous work with the ANLT (Briscoe &amp; Carroll, 1993), throughput with raw corpus data was worse than that observed in these experiments, though probably only by a constant factor. This could be due to the fact that the vocabulary of the corpus concerned exhibits significantly higher lexical ambiguity; however, for sentences taken from a specific corpus, constructional bias observed in a training phase could be exploited to improve performance (e.g. Samuelsson &amp; Rayner, 1991). 5. DISCUSSION All three of the parsers have theoretical worst-case complexities that are either exponential, or polynomial on grammar size but with an extremely large mult</context>
</contexts>
<marker>Carroll, 1993</marker>
<rawString>Carroll, J. (1993) Practical unification-based parsing of natural language. Computer Laboratory, Cambridge University, UK, Technical Report 314.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carroll</author>
<author>E Briscoe</author>
</authors>
<title>Probabilistic normalisation and unpacking of packed parse forests for unification-based grammars.&amp;quot;</title>
<date>1992</date>
<booktitle>In Proceedings of the AAAI Fall Symposium on Probabilistic Approaches to Natural Language.</booktitle>
<pages>33--38</pages>
<contexts>
<context position="13859" citStr="Carroll &amp; Briscoe, 1992" startWordPosition="2185" endWordPosition="2188">es can be formed into higher-level constituents as they stand without further processing. However, in the unification versions, on each reduce action the daughters of the rule involved have to be unified with every possible alternative sequence of the sub-analyses that are being consumed by the rule 5This complexity measure does correspond to real world usage of a parser, since practical systems can usually afford to extract only a small number of parses from the frequently very large number encoded in a forest; this is often done on the basis of preferencebased or probabilistic factors (e.g. Carroll &amp; Briscoe, 1992). 289 (in effect expanding and flattening out the packed sequences), leading to a bound of nP+1 on the total number of unifications. 4. PRACTICAL RESULTS To assess the practical performance of the three unification-based parsers described above, a series of experiments were conducted using the ANLT grammar (Grover, Carroll &amp; Briscoe, 1993), a wide-coverage grammar of English. The grammar is defined in metagrammatical formalism which is compiled into a unification-based &apos;object grammar&apos;—a syntactic variant of the Definite Clause Grammar formalism (Pereira &amp; Warren, 1980)—containing 84 features </context>
</contexts>
<marker>Carroll, Briscoe, 1992</marker>
<rawString>Carroll, J. &amp; E. Briscoe (1992) &amp;quot;Probabilistic normalisation and unpacking of packed parse forests for unification-based grammars.&amp;quot; In Proceedings of the AAAI Fall Symposium on Probabilistic Approaches to Natural Language. 33-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Earley</author>
</authors>
<title>An efficient context-free parsing algorithm.&amp;quot;</title>
<date>1970</date>
<journal>Communications of the ACM,</journal>
<volume>13</volume>
<pages>94--102</pages>
<contexts>
<context position="4082" citStr="Earley, 1970" startWordPosition="613" endWordPosition="614"> for grammars having more substantial phrase structure components, such as CLARE (which although employing some HPSGlike analyses still contains several tens of rules) and the ANLT (which uses a formalism derived from GPSG; Gazdar et at., 1985), since the more specific rule set can be used to control which unifications are performed. In NL analysis, the syntactic information associated with lexical items makes top-down parsing less attractive than bottom-up (e.g. CKY; Kasami, 1965; Younger, 1967), although the latter is often augmented with top-down predic287 tion to improve performance (e.g. Earley, 1970; Lang, 1974; Pratt, 1975). Section 2 describes three unification-based parsers which are related to polynomial-complexity bottom-up CF parsing algorithms. Although incorporating unification increases their complexity to exponential on grammar size and input length (section 3), this appears to have little impact on practical performance (section 4). Sections 5 and 6 discuss these findings and present conclusions. 2. THE PARSERS The three parsers in this study are: a bottomup left-corner parser, a (non-deterministic) LR parser, and an LR-like parser based on an algorithm devised by Schabes (199</context>
</contexts>
<marker>Earley, 1970</marker>
<rawString>Earley, .J. (1970) &amp;quot;An efficient context-free parsing algorithm.&amp;quot; Communications of the ACM, 13.2: 94-102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gazdar</author>
<author>E Klein</author>
<author>G Pullum</author>
<author>I Sag</author>
</authors>
<title>Generalized phrase structure grammar.</title>
<date>1985</date>
<publisher>Blackwell.</publisher>
<location>Oxford, UK:</location>
<marker>Gazdar, Klein, Pullum, Sag, 1985</marker>
<rawString>Gazdar, G., E. Klein, G. Pullum &amp; I. Sag (1985) Generalized phrase structure grammar. Oxford, UK: Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Grover</author>
<author>J Carroll</author>
<author>E Briscoe</author>
</authors>
<title>The Alvey natural language tools grammar (4th release).</title>
<date>1993</date>
<tech>Technical Report 284.</tech>
<institution>Computer Laboratory, Cambridge University, UK,</institution>
<marker>Grover, Carroll, Briscoe, 1993</marker>
<rawString>Grover, C., J. Carroll &amp; E. Briscoe (1993) The Alvey natural language tools grammar (4th release). Computer Laboratory, Cambridge University, UK, Technical Report 284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
</authors>
<title>The computational complexity of Tomita&apos;s algorithm.&amp;quot;</title>
<date>1989</date>
<booktitle>In Proceedings of the 1st International Workshop on Parsing Technologies.</booktitle>
<pages>203--208</pages>
<contexts>
<context position="10551" citStr="Johnson (1989)" startWordPosition="1637" endWordPosition="1639">MAR-DEPENDENT COMPLEXITY The term dependent on the grammar in the time complexity of the BU-LC unification-based parser described above is 0(1C121R13), where ClI is the number of categories implicit in the grammar, and I RI, the number of rules. The space complexity is dominated by the size of the parse forest, 0(ICI) (these results are proved by Carroll, 1993). For the ANLT grammar, in which features are nested to a maximum depth of two, ClI is finite but nevertheless extremely large (Briscoe et at., 1987b)4. The grammar-dependent complexity of the LR parser makes it also appear intractable: Johnson (1989) shows that the number of LR(0) states for certain (pathological) grammars is exponentially related to the size of the grammar, and that there are some inputs which force an LR parser to visit all of these states in the course of a parse. 3Schabes describes a table with no loolcahead; the successful application of this technique supports Schabes&apos; (1991:109) assertion that &amp;quot;several other methods (such as LR(k)-like and SLR(k)-like) can also be used for constructing the parsing tables [...]&amp;quot; 4 Barton, Berwick &amp; Ristad (1987:221) calculate that GPSG, also with a maximum nesting depth of two, lice</context>
</contexts>
<marker>Johnson, 1989</marker>
<rawString>Johnson, M. (1989) &amp;quot;The computational complexity of Tomita&apos;s algorithm.&amp;quot; In Proceedings of the 1st International Workshop on Parsing Technologies. 203-208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kaplan</author>
</authors>
<title>Three seductions of computational psycholinguistics.&amp;quot; In Linguistic Theory and Computer Applications, edited by P. Whitelock et al.,</title>
<date>1987</date>
<pages>149--188</pages>
<publisher>Academic Press.</publisher>
<location>New York:</location>
<contexts>
<context position="1025" citStr="Kaplan, 1987" startWordPosition="142" endWordPosition="143">ide-coverage grammar. The results imply that the study and optimisation of unification-based parsing must rely on empirical data until complexity theory can more accurately predict the practical behaviour of such parsers&apos;. 1. INTRODUCTION General-purpose natural language (NL) analysis systems have recently started to use declarative unification-based sentence grammar formalisms; systems of this type include SRI&apos;s CLARE system (Alshawi et al., 1992) and the Alvey NL Tools (ANLT; Briscoe et al., 1987a). Using a declarative formalism helps ease the task of developing and maintaining the grammar (Kaplan, 1987). In addition to syntactic processing, the systems incorporate lexical, morphological, and semantic processing, and have been applied successfully to the analysis of naturally-occurring texts (e.g. Alshawi et at., 1992; Briscoe &amp; Carroll, 1993). Evaluations of the grammars in these particular systems have shown them to have wide coverage (Alshawi et at., 1992; Taylor, Grover &amp; Briscoe, 1989)2. However, although the practical throughput of parsers with such realistic grammars is important, for example when process&apos;This research was supported by SERC/DTI project 4/1/1261 &apos;Extensions to the Alvey</context>
</contexts>
<marker>Kaplan, 1987</marker>
<rawString>Kaplan, R. (1987) &amp;quot;Three seductions of computational psycholinguistics.&amp;quot; In Linguistic Theory and Computer Applications, edited by P. Whitelock et al., New York: Academic Press. 149-188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kasami</author>
</authors>
<title>An efficient recognition and syntax analysis algorithm for context-free languages. Air Force Cambridge Research Laboratory,</title>
<date>1965</date>
<pages>65--758</pages>
<location>Bedford, MA, Report</location>
<contexts>
<context position="3955" citStr="Kasami, 1965" startWordPosition="593" endWordPosition="594">ystems using lexicalist grammar formalisms (e.g. HPSG; Pollard &amp; Sag, 1987). However, parsing algorithms assume more importance for grammars having more substantial phrase structure components, such as CLARE (which although employing some HPSGlike analyses still contains several tens of rules) and the ANLT (which uses a formalism derived from GPSG; Gazdar et at., 1985), since the more specific rule set can be used to control which unifications are performed. In NL analysis, the syntactic information associated with lexical items makes top-down parsing less attractive than bottom-up (e.g. CKY; Kasami, 1965; Younger, 1967), although the latter is often augmented with top-down predic287 tion to improve performance (e.g. Earley, 1970; Lang, 1974; Pratt, 1975). Section 2 describes three unification-based parsers which are related to polynomial-complexity bottom-up CF parsing algorithms. Although incorporating unification increases their complexity to exponential on grammar size and input length (section 3), this appears to have little impact on practical performance (section 4). Sections 5 and 6 discuss these findings and present conclusions. 2. THE PARSERS The three parsers in this study are: a bo</context>
</contexts>
<marker>Kasami, 1965</marker>
<rawString>Kasami, J. (1965) An efficient recognition and syntax analysis algorithm for context-free languages. Air Force Cambridge Research Laboratory, Bedford, MA, Report AFCRL-65-758.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kipps</author>
</authors>
<title>Analysis of Tomita&apos;s algorithm for general context-free parsing.&amp;quot;</title>
<date>1989</date>
<booktitle>In Proceedings of the 1st International Workshop on Parsing Technologies.</booktitle>
<pages>193--202</pages>
<marker>Kipps, 1989</marker>
<rawString>Kipps, J. (1989) &amp;quot;Analysis of Tomita&apos;s algorithm for general context-free parsing.&amp;quot; In Proceedings of the 1st International Workshop on Parsing Technologies. 193-202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Lang</author>
</authors>
<title>Deterministic techniques for efficient non-deterministic parsers.&amp;quot;</title>
<date>1974</date>
<booktitle>In Automata, Languages and Programming, Lecture Notes in Computer Science 14,</booktitle>
<pages>255--269</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Germany:</location>
<note>edited by</note>
<contexts>
<context position="4094" citStr="Lang, 1974" startWordPosition="615" endWordPosition="616">having more substantial phrase structure components, such as CLARE (which although employing some HPSGlike analyses still contains several tens of rules) and the ANLT (which uses a formalism derived from GPSG; Gazdar et at., 1985), since the more specific rule set can be used to control which unifications are performed. In NL analysis, the syntactic information associated with lexical items makes top-down parsing less attractive than bottom-up (e.g. CKY; Kasami, 1965; Younger, 1967), although the latter is often augmented with top-down predic287 tion to improve performance (e.g. Earley, 1970; Lang, 1974; Pratt, 1975). Section 2 describes three unification-based parsers which are related to polynomial-complexity bottom-up CF parsing algorithms. Although incorporating unification increases their complexity to exponential on grammar size and input length (section 3), this appears to have little impact on practical performance (section 4). Sections 5 and 6 discuss these findings and present conclusions. 2. THE PARSERS The three parsers in this study are: a bottomup left-corner parser, a (non-deterministic) LR parser, and an LR-like parser based on an algorithm devised by Schabes (1991). All thre</context>
</contexts>
<marker>Lang, 1974</marker>
<rawString>Lang, B. (1974) &amp;quot;Deterministic techniques for efficient non-deterministic parsers.&amp;quot; In Automata, Languages and Programming, Lecture Notes in Computer Science 14, edited by J. Loeckx, Berlin, Germany: Springer-Verlag. 255-269.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Maxwell</author>
<author>R Kaplan</author>
</authors>
<title>The interface between phrasal and functional constraints.&amp;quot;</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>4</issue>
<pages>571--590</pages>
<contexts>
<context position="2507" citStr="Maxwell &amp; Kaplan, 1993" startWordPosition="370" endWordPosition="373">ylor et at. demonstrate that the ANLT grammar is in principle able to analyse 96.8% of a corpus of 10,000 noun phrases taken from a variety of corpora. ing large amounts of text or in interactive applications, there is little published research that compares the performance of different parsing algorithms using wide-coverage unification-based grammars. Previous comparisons have either focussed on context-free (CF) or augmented CF parsing (Tomita, 1987; Billot &amp; Lang, 1989), or have used relatively small, limited-coverage unification grammars and lexicons (Shann, 1989; Bouma &amp; van Noord, 1993; Maxwell &amp; Kaplan, 1993). It is not clear that these results scale up to reflect accurately the behaviour of parsers using realistic, complex unification-based grammars: in particular, with grammars admitting less ambiguity parse time will tend to increase more slowly with increasing input length, and also with smaller grammars rule application can be constrained tightly with relatively simple predictive techniques. Also, since none of these studies relate observed performance to that of other comparable parsing systems, implementational oversights may not be apparent and so be a confounding factor in any general con</context>
</contexts>
<marker>Maxwell, Kaplan, 1993</marker>
<rawString>Maxwell, J. III &amp; R. Kaplan (1993) &amp;quot;The interface between phrasal and functional constraints.&amp;quot; Computational Linguistics, 19(4): 571-590.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Moore</author>
<author>H Alshawi</author>
</authors>
<title>Syntactic and semantic processing.&amp;quot; In The Core Language Engine, edited by H. Alshawi,</title>
<date>1992</date>
<pages>129--148</pages>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA:</location>
<contexts>
<context position="15830" citStr="Moore &amp; Alshawi, 1992" startWordPosition="2486" endWordPosition="2489">orms of words, the experiments draw on a much smaller lexicon of 600 words (consisting of closed class vocabulary and, for open-class vocabulary, definitions of just a sample of words which taken together exhibit the full range of possible complementation patterns), since issues of lexical coverage are of no concern here. COMPARING THE PARSERS In the first experiment, the ANLT grammar was loaded and a set of sentences was input to each of the three parsers. In order to provide an independent basis for comparison, the same sentences were also input to the SRI Core Language Engine (CLE) parser (Moore &amp; Alshawi, 1992) with the CLARE2.5 grammar (Alshawi et al., 1992), a state-of-the-art system accessible to the author. The sentences were taken from an initial sample of 175 representative sentences extracted from a corpus of approximately 1500 that form part of the ANLT package. This corpus, implicitly defining the types of construction the grammar is intended to cover, was written by the linguist who developed the ANLT grammar and is used to check for any adverse effects on coverage when the grammar is modified during grammar development. Of Parser Grammar CPU time Storage allocated BU-LC ANLT 75.5 47.0 LR </context>
</contexts>
<marker>Moore, Alshawi, 1992</marker>
<rawString>Moore, R. &amp; H. Alshawi (1992) &amp;quot;Syntactic and semantic processing.&amp;quot; In The Core Language Engine, edited by H. Alshawi, Cambridge, MA: MIT Press. 129-148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Moore</author>
<author>J Dowding</author>
</authors>
<title>Efficient bottomup parsing.&amp;quot;</title>
<date>1991</date>
<booktitle>In Proceedings of the DARPA Speech and Natural Language Workshop.</booktitle>
<pages>200--203</pages>
<contexts>
<context position="18628" citStr="Moore &amp; Dowding, 1991" startWordPosition="2959" endWordPosition="2962">collection time, and phrasal (where appropriate) processing, but not parse forest unpacking. Both grammars give a total of around 280 analyses at a similar level of detail. The results show that the LR parser is approximately 35% faster than the BU-LC parser, and allocates about 30% less storage. The magnitude of the speed-up is less than might be expected, given the enthusiastic advocation of nondeterministic CF LR parsing for NL by some researchers (e.g. Tomita, 1987; Wright, Wrigley &amp; Sharman, 1991), and in the light of improvements observed for predictive over pure bottom-up parsing (e.g. Moore &amp; Dowding, 1991). However, on the assumption that incorrect prediction of gaps is 290 the main avoidable source of performance degradation (c.f. Moore &amp; Dowding), further investigation shows that the speed-up is near the maximum that is possible with the ANLT grammar (around 50%). The throughput of the CE parser is half that of the LR parser, and also less than that of the BU-LC parser. However, it is intermediate between the two in terms of storage allocated. Part of the difference in performance between it and the LR parser is due to the fact that it performs around 15% more unifications. This might be expe</context>
<context position="29211" citStr="Moore &amp; Dowding (1991)" startWordPosition="4719" endWordPosition="4722">the results of the experiment comparing the performance of the respective parsers using the ANLT grammar suggests that the parallel development of the software and grammars that has occurred nevertheless appears to have caused this to happen automatically. It therefore seems likely that implementational decisions and optimisations based on subtle properties of specific grammars can, and may very often be, more important than worst-case complexity when considering the practical performance of parsing algorithms. 6. CONCLUSIONS The research reported is in a similar vein to that of, for example, Moore &amp; Dowding (1991), Samuelsson 8,z Rayner (1991), and Maxwell Si Kaplan (1993), in that it relies on empirical results for the study and optimisation of parsing algorithms rather than on traditional techniques of complexity analysis. The paper demonstrates that research in this area will have to rely on empirical data until complexity theory is developed to a point where it is sufficiently fine-grained and accurate to predict how the properties of individual unification-based grammars will interact with particular parsing algorithms to determine practical performance. REFERENCES Aho, A., R. Sethi &amp; J. Ullman (1</context>
</contexts>
<marker>Moore, Dowding, 1991</marker>
<rawString>Moore, R. &amp; J. Dowding (1991) &amp;quot;Efficient bottomup parsing.&amp;quot; In Proceedings of the DARPA Speech and Natural Language Workshop. 200-203.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
<author>D Warren</author>
</authors>
<title>Definite clause grammars for language analysis—a survey of the formalism and a comparison with augmented transition networks.&amp;quot;</title>
<date>1980</date>
<journal>Artificial Intelligence,</journal>
<volume>13</volume>
<issue>3</issue>
<pages>231--278</pages>
<contexts>
<context position="14435" citStr="Pereira &amp; Warren, 1980" startWordPosition="2272" endWordPosition="2276">bilistic factors (e.g. Carroll &amp; Briscoe, 1992). 289 (in effect expanding and flattening out the packed sequences), leading to a bound of nP+1 on the total number of unifications. 4. PRACTICAL RESULTS To assess the practical performance of the three unification-based parsers described above, a series of experiments were conducted using the ANLT grammar (Grover, Carroll &amp; Briscoe, 1993), a wide-coverage grammar of English. The grammar is defined in metagrammatical formalism which is compiled into a unification-based &apos;object grammar&apos;—a syntactic variant of the Definite Clause Grammar formalism (Pereira &amp; Warren, 1980)—containing 84 features and 782 phrase structure rules. Parsing uses fixed-arity term unification. The grammar provides full coverage of the following constructions: declarative sentences, imperatives and questions (yes/no, tag and wh-questions); all unbounded dependency types (topicalisation, relativisation, wh-questions); a relatively exhaustive treatment of verb and adjective complement types; phrasal and prepositional verbs of many complement types; passivisation; verb phrase extraposition; sentence and verb phrase modification; noun phrase complements and pre- and post-modification; parti</context>
</contexts>
<marker>Pereira, Warren, 1980</marker>
<rawString>Pereira, F. &amp; D. Warren (1980) &amp;quot;Definite clause grammars for language analysis—a survey of the formalism and a comparison with augmented transition networks.&amp;quot; Artificial Intelligence, 13(3): 231-278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Pollard</author>
<author>I Sag</author>
</authors>
<title>Information-based syntax and semantics: volume 1-fundamentals.</title>
<date>1987</date>
<publisher>University of Chicago Press.</publisher>
<location>Chicago, IL:</location>
<contexts>
<context position="3418" citStr="Pollard &amp; Sag, 1987" startWordPosition="507" endWordPosition="510">aller grammars rule application can be constrained tightly with relatively simple predictive techniques. Also, since none of these studies relate observed performance to that of other comparable parsing systems, implementational oversights may not be apparent and so be a confounding factor in any general conclusions made. Other research directed towards improving the throughput of unification-based parsing systems has been concerned with the unification operation itself, which can consume up to 90% of parse time (e.g. Tomabechi, 1991) in systems using lexicalist grammar formalisms (e.g. HPSG; Pollard &amp; Sag, 1987). However, parsing algorithms assume more importance for grammars having more substantial phrase structure components, such as CLARE (which although employing some HPSGlike analyses still contains several tens of rules) and the ANLT (which uses a formalism derived from GPSG; Gazdar et at., 1985), since the more specific rule set can be used to control which unifications are performed. In NL analysis, the syntactic information associated with lexical items makes top-down parsing less attractive than bottom-up (e.g. CKY; Kasami, 1965; Younger, 1967), although the latter is often augmented with t</context>
</contexts>
<marker>Pollard, Sag, 1987</marker>
<rawString>Pollard, C. &amp; I. Sag (1987) Information-based syntax and semantics: volume 1-fundamentals. Chicago, IL: University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Pratt</author>
</authors>
<title>LINGOL - a progress report.&amp;quot;</title>
<date>1975</date>
<booktitle>In Proceedings of the 5th International Joint Conference on Artificial Intelligence.</booktitle>
<pages>422--428</pages>
<contexts>
<context position="4108" citStr="Pratt, 1975" startWordPosition="617" endWordPosition="618">substantial phrase structure components, such as CLARE (which although employing some HPSGlike analyses still contains several tens of rules) and the ANLT (which uses a formalism derived from GPSG; Gazdar et at., 1985), since the more specific rule set can be used to control which unifications are performed. In NL analysis, the syntactic information associated with lexical items makes top-down parsing less attractive than bottom-up (e.g. CKY; Kasami, 1965; Younger, 1967), although the latter is often augmented with top-down predic287 tion to improve performance (e.g. Earley, 1970; Lang, 1974; Pratt, 1975). Section 2 describes three unification-based parsers which are related to polynomial-complexity bottom-up CF parsing algorithms. Although incorporating unification increases their complexity to exponential on grammar size and input length (section 3), this appears to have little impact on practical performance (section 4). Sections 5 and 6 discuss these findings and present conclusions. 2. THE PARSERS The three parsers in this study are: a bottomup left-corner parser, a (non-deterministic) LR parser, and an LR-like parser based on an algorithm devised by Schabes (1991). All three parsers acce</context>
</contexts>
<marker>Pratt, 1975</marker>
<rawString>Pratt, V. (1975) &amp;quot;LINGOL - a progress report.&amp;quot; In Proceedings of the 5th International Joint Conference on Artificial Intelligence. 422-428.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Samuelsson</author>
<author>M Rayner</author>
</authors>
<title>Quantitative evaluation of explanation-based learning as an optimization tool for a large-scale natural language system.&amp;quot;</title>
<date>1991</date>
<booktitle>In Proceedings of the 12th International Joint Conference on Artificial Intelligence.</booktitle>
<pages>609--615</pages>
<contexts>
<context position="25141" citStr="Samuelsson &amp; Rayner, 1991" startWordPosition="4061" endWordPosition="4064">ds in length) with the BU-LC and LR parsers. that, at least for the BU-LC parser, parse time is related roughly quadratically to input length. In previous work with the ANLT (Briscoe &amp; Carroll, 1993), throughput with raw corpus data was worse than that observed in these experiments, though probably only by a constant factor. This could be due to the fact that the vocabulary of the corpus concerned exhibits significantly higher lexical ambiguity; however, for sentences taken from a specific corpus, constructional bias observed in a training phase could be exploited to improve performance (e.g. Samuelsson &amp; Rayner, 1991). 5. DISCUSSION All three of the parsers have theoretical worst-case complexities that are either exponential, or polynomial on grammar size but with an extremely large multiplier. Despite this, in the practical experiments reported in the previous section the parsers achieve relatively good throughput with a general-purpose wide-coverage grammar of a natural language. It therefore seems likely that grammars of the type considered in this paper (i.e. with relatively detailed phrase structure components, but comparatively simple from a unification perspective), although realistic, do not bring </context>
</contexts>
<marker>Samuelsson, Rayner, 1991</marker>
<rawString>Samuelsson, C. &amp; M. Rayner (1991) &amp;quot;Quantitative evaluation of explanation-based learning as an optimization tool for a large-scale natural language system.&amp;quot; In Proceedings of the 12th International Joint Conference on Artificial Intelligence. 609-615.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Schabes</author>
</authors>
<title>Polynomial time and space shift-reduce parsing of arbitrary context-free grammars.&amp;quot;</title>
<date>1991</date>
<booktitle>In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<pages>106--113</pages>
<contexts>
<context position="4684" citStr="Schabes (1991)" startWordPosition="704" endWordPosition="705">Earley, 1970; Lang, 1974; Pratt, 1975). Section 2 describes three unification-based parsers which are related to polynomial-complexity bottom-up CF parsing algorithms. Although incorporating unification increases their complexity to exponential on grammar size and input length (section 3), this appears to have little impact on practical performance (section 4). Sections 5 and 6 discuss these findings and present conclusions. 2. THE PARSERS The three parsers in this study are: a bottomup left-corner parser, a (non-deterministic) LR parser, and an LR-like parser based on an algorithm devised by Schabes (1991). All three parsers accept grammars written in the ANLT formalism (Briscoe et al., 1987a), and the first two are distributed as part of the ANLT package. The parsers create parse forests (Tomita, 1987) that incorporate subtree sharing (in which identical sub-analyses are shared between differing superordinate analyses) and node packing (where subanalyses covering the same portion of input whose root categories are in a subsumption relationship are merged into a single node). THE BOTTOM-UP LEFT-CORNER PARSER The bottom-up left-corner (BU-LC) parser operates left-to-right and breadth-first, stor</context>
<context position="8199" citStr="Schabes (1991)" startWordPosition="1249" endWordPosition="1250">essarily assume that all reductions at a given vertex with all rules with the same number of daughters build exactly the same constituent every time; in general this is not the case when the daughters are unification categories. A weaker kind of cache on partial analyses (and thus unification results) was found to be necessary in the implementation, though, to avoid duplication of unifications; this sped the parser up by a factor of about three, at little space cost. THE COMPILED-EARLEY PARSER The Compiled-Earley (CE) parser is based on a predictive chart-based CF parsing algorithm devised by Schabes (1991) which is driven by a table compiling out the predictive component of Earley&apos;s (1970) parser. The size of the table is related linearly to the size of the grammar (unlike the LR technique). Schabes demonstrates that this parser always takes fewer steps than Earley&apos;s, although its time complexity is the same: 0(n3). The space complexity is also cubic, since the parser uses Earley&apos;s representation of parse forests. The incorporation of unification into the CE parser follows the methodology developed for unification-based LR parsing described in the previous section: a table is computed from a CF</context>
<context position="26541" citStr="Schabes (1991" startWordPosition="4279" endWordPosition="4280">d, for the ANLT grammar, the number of states-the term that the CE technique reduces from exponential to linear on the grammar size-is actually smaller in the standard LALR(1) table. This suggests that, when considering the complexity of parsers, the issue of parse table size is of minor importance for realistic NL grammars (as long as an implementation represents the table compactly), and that improvements to complexity results with respect to grammar size, although interesting from a theoretical standpoint, may have little practical relevance for the processing of natural language. Although Schabes (1991:107) claims that the problem of exponential grammar complexity &amp;quot;is particularly acute for natural language processing since in this context the input length is typically small (10-20 words) and the grammar size very large (hundreds or thousands of rules and symbols)&amp;quot;, the experiments indicate that, with a widecoverage NL grammar, inputs of this length can be parsed quite quickly; however, longer inputs (of more than about 30 words in length)-which occur relatively frequently in written text-are a problem. Unless grammar size takes on proportionately much more significance for such longer inpu</context>
</contexts>
<marker>Schabes, 1991</marker>
<rawString>Schabes, Y. (1991) &amp;quot;Polynomial time and space shift-reduce parsing of arbitrary context-free grammars.&amp;quot; In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics. 106-113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Taylor</author>
<author>C Grover</author>
<author>E Briscoe</author>
</authors>
<title>The syntactic regularity of English noun phrases.&amp;quot;</title>
<date>1989</date>
<booktitle>In Proceedings of the 4th European Meeting of the Association for Computational Linguistics.</booktitle>
<pages>256--263</pages>
<marker>Taylor, Grover, Briscoe, 1989</marker>
<rawString>Taylor, L., C. Grover &amp; E. Briscoe (1989) &amp;quot;The syntactic regularity of English noun phrases.&amp;quot; In Proceedings of the 4th European Meeting of the Association for Computational Linguistics. 256-263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Tomabechi</author>
</authors>
<title>Quasi-destructive graph unification.&amp;quot;</title>
<date>1991</date>
<booktitle>In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<pages>315--322</pages>
<contexts>
<context position="3338" citStr="Tomabechi, 1991" startWordPosition="496" endWordPosition="497"> tend to increase more slowly with increasing input length, and also with smaller grammars rule application can be constrained tightly with relatively simple predictive techniques. Also, since none of these studies relate observed performance to that of other comparable parsing systems, implementational oversights may not be apparent and so be a confounding factor in any general conclusions made. Other research directed towards improving the throughput of unification-based parsing systems has been concerned with the unification operation itself, which can consume up to 90% of parse time (e.g. Tomabechi, 1991) in systems using lexicalist grammar formalisms (e.g. HPSG; Pollard &amp; Sag, 1987). However, parsing algorithms assume more importance for grammars having more substantial phrase structure components, such as CLARE (which although employing some HPSGlike analyses still contains several tens of rules) and the ANLT (which uses a formalism derived from GPSG; Gazdar et at., 1985), since the more specific rule set can be used to control which unifications are performed. In NL analysis, the syntactic information associated with lexical items makes top-down parsing less attractive than bottom-up (e.g. </context>
</contexts>
<marker>Tomabechi, 1991</marker>
<rawString>Tomabechi, H. (1991) &amp;quot;Quasi-destructive graph unification.&amp;quot; In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics. 315-322.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Tomita</author>
</authors>
<title>An efficient augmentedcontext-free parsing algorithm.&amp;quot;</title>
<date>1987</date>
<journal>Computational Linguistics,</journal>
<volume>13</volume>
<issue>1</issue>
<pages>31--46</pages>
<contexts>
<context position="2339" citStr="Tomita, 1987" startWordPosition="347" endWordPosition="348">ents on an earlier version of this paper, to David Weir for valuable discussions, and to Hiyan Alshawi for assistance with the CLARE system. 2For example, Taylor et at. demonstrate that the ANLT grammar is in principle able to analyse 96.8% of a corpus of 10,000 noun phrases taken from a variety of corpora. ing large amounts of text or in interactive applications, there is little published research that compares the performance of different parsing algorithms using wide-coverage unification-based grammars. Previous comparisons have either focussed on context-free (CF) or augmented CF parsing (Tomita, 1987; Billot &amp; Lang, 1989), or have used relatively small, limited-coverage unification grammars and lexicons (Shann, 1989; Bouma &amp; van Noord, 1993; Maxwell &amp; Kaplan, 1993). It is not clear that these results scale up to reflect accurately the behaviour of parsers using realistic, complex unification-based grammars: in particular, with grammars admitting less ambiguity parse time will tend to increase more slowly with increasing input length, and also with smaller grammars rule application can be constrained tightly with relatively simple predictive techniques. Also, since none of these studies re</context>
<context position="4885" citStr="Tomita, 1987" startWordPosition="738" endWordPosition="739"> increases their complexity to exponential on grammar size and input length (section 3), this appears to have little impact on practical performance (section 4). Sections 5 and 6 discuss these findings and present conclusions. 2. THE PARSERS The three parsers in this study are: a bottomup left-corner parser, a (non-deterministic) LR parser, and an LR-like parser based on an algorithm devised by Schabes (1991). All three parsers accept grammars written in the ANLT formalism (Briscoe et al., 1987a), and the first two are distributed as part of the ANLT package. The parsers create parse forests (Tomita, 1987) that incorporate subtree sharing (in which identical sub-analyses are shared between differing superordinate analyses) and node packing (where subanalyses covering the same portion of input whose root categories are in a subsumption relationship are merged into a single node). THE BOTTOM-UP LEFT-CORNER PARSER The bottom-up left-corner (BU-LC) parser operates left-to-right and breadth-first, storing partial (active) constituents in a chart; Carroll (1993) gives a full description. Although pure bottomup parsing is not usually thought of as providing high performance, the actual implementation </context>
<context position="6903" citStr="Tomita, 1987" startWordPosition="1041" endWordPosition="1042"> of more than three. THE NON-DETERMINISTIC LR PARSER Briscoe &amp; Carroll (1993) describe a methodology for constructing an LR parser for a unificationbased grammar, in which a CF &apos;backbone&apos; grammar is automatically constructed from the unification grammar, a parse table is constructed from the backbone grammar, and a parser is driven by the table and further controlled by unification of the &apos;residue&apos; of features in the unification grammar that are not encoded in the backbone. In this parser, the LALR(1) technique (Aho, Sethi &amp; Ullman, 1986) is used, in conjunction with a graph-structured stack (Tomita, 1987), adapting for unification-based parsing Kipps&apos; (1989) Tomita-like recogniser that achieves polynomial complexity on input length through caching. On each reduction the parser performs the unifications specified by the unification grammar version of the CF backbone rule being applied. This constitutes an on-line parsing algorithm. In the general case, the off-line variant (in which all unifications are deferred until the complete CF parse forest has been constructed) is not guaranteed to terminate; indeed, it usually does not do so with the ANLT grammar. However, a drawback to the on-line algo</context>
<context position="18479" citStr="Tomita, 1987" startWordPosition="2937" endWordPosition="2938">y the LALR technique. Table 1 also shows the results for the CLE parser with the CLARE2.5 grammar and lexicon. The figures include garbage collection time, and phrasal (where appropriate) processing, but not parse forest unpacking. Both grammars give a total of around 280 analyses at a similar level of detail. The results show that the LR parser is approximately 35% faster than the BU-LC parser, and allocates about 30% less storage. The magnitude of the speed-up is less than might be expected, given the enthusiastic advocation of nondeterministic CF LR parsing for NL by some researchers (e.g. Tomita, 1987; Wright, Wrigley &amp; Sharman, 1991), and in the light of improvements observed for predictive over pure bottom-up parsing (e.g. Moore &amp; Dowding, 1991). However, on the assumption that incorrect prediction of gaps is 290 the main avoidable source of performance degradation (c.f. Moore &amp; Dowding), further investigation shows that the speed-up is near the maximum that is possible with the ANLT grammar (around 50%). The throughput of the CE parser is half that of the LR parser, and also less than that of the BU-LC parser. However, it is intermediate between the two in terms of storage allocated. Pa</context>
</contexts>
<marker>Tomita, 1987</marker>
<rawString>Tomita, M. (1987) &amp;quot;An efficient augmentedcontext-free parsing algorithm.&amp;quot; Computational Linguistics, 13(1): 31-46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Shann</author>
</authors>
<title>The selection of a parsing strategy for an on-line machine translation system in a sublanguage domain. A new practical comparison.&amp;quot;</title>
<date>1989</date>
<booktitle>In Proceedings of the 1st International Workshop on Parsing Technologies.</booktitle>
<pages>264--276</pages>
<contexts>
<context position="2457" citStr="Shann, 1989" startWordPosition="363" endWordPosition="364">ith the CLARE system. 2For example, Taylor et at. demonstrate that the ANLT grammar is in principle able to analyse 96.8% of a corpus of 10,000 noun phrases taken from a variety of corpora. ing large amounts of text or in interactive applications, there is little published research that compares the performance of different parsing algorithms using wide-coverage unification-based grammars. Previous comparisons have either focussed on context-free (CF) or augmented CF parsing (Tomita, 1987; Billot &amp; Lang, 1989), or have used relatively small, limited-coverage unification grammars and lexicons (Shann, 1989; Bouma &amp; van Noord, 1993; Maxwell &amp; Kaplan, 1993). It is not clear that these results scale up to reflect accurately the behaviour of parsers using realistic, complex unification-based grammars: in particular, with grammars admitting less ambiguity parse time will tend to increase more slowly with increasing input length, and also with smaller grammars rule application can be constrained tightly with relatively simple predictive techniques. Also, since none of these studies relate observed performance to that of other comparable parsing systems, implementational oversights may not be apparent</context>
</contexts>
<marker>Shann, 1989</marker>
<rawString>Shann, P. (1989) &amp;quot;The selection of a parsing strategy for an on-line machine translation system in a sublanguage domain. A new practical comparison.&amp;quot; In Proceedings of the 1st International Workshop on Parsing Technologies. 264-276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wright</author>
<author>E Wrigley</author>
<author>R Sharman</author>
</authors>
<title>Adaptive probabilistic generalized LR parsing.&amp;quot;</title>
<date>1991</date>
<booktitle>In Proceedings of the 2nd International Workshop on Parsing Technologies.</booktitle>
<pages>154--163</pages>
<marker>Wright, Wrigley, Sharman, 1991</marker>
<rawString>Wright, J., E. Wrigley &amp; R. Sharman (1991) &amp;quot;Adaptive probabilistic generalized LR parsing.&amp;quot; In Proceedings of the 2nd International Workshop on Parsing Technologies. 154-163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Younger</author>
</authors>
<title>Recognition and parsing of context-free languages in time id.&amp;quot;</title>
<date>1967</date>
<journal>Information and Control,</journal>
<volume>10</volume>
<issue>2</issue>
<pages>189--208</pages>
<contexts>
<context position="3971" citStr="Younger, 1967" startWordPosition="595" endWordPosition="596">exicalist grammar formalisms (e.g. HPSG; Pollard &amp; Sag, 1987). However, parsing algorithms assume more importance for grammars having more substantial phrase structure components, such as CLARE (which although employing some HPSGlike analyses still contains several tens of rules) and the ANLT (which uses a formalism derived from GPSG; Gazdar et at., 1985), since the more specific rule set can be used to control which unifications are performed. In NL analysis, the syntactic information associated with lexical items makes top-down parsing less attractive than bottom-up (e.g. CKY; Kasami, 1965; Younger, 1967), although the latter is often augmented with top-down predic287 tion to improve performance (e.g. Earley, 1970; Lang, 1974; Pratt, 1975). Section 2 describes three unification-based parsers which are related to polynomial-complexity bottom-up CF parsing algorithms. Although incorporating unification increases their complexity to exponential on grammar size and input length (section 3), this appears to have little impact on practical performance (section 4). Sections 5 and 6 discuss these findings and present conclusions. 2. THE PARSERS The three parsers in this study are: a bottomup left-corn</context>
</contexts>
<marker>Younger, 1967</marker>
<rawString>Younger, D. (1967) &amp;quot;Recognition and parsing of context-free languages in time id.&amp;quot; Information and Control, 10(2): 189-208.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>