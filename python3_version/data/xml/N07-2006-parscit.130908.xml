<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001567">
<title confidence="0.9989255">
Translation Model Pruning via Usage Statistics
for Statistical Machine Translation
</title>
<author confidence="0.998229">
Matthias Eck, Stephan Vogel, and Alex Waibel
</author>
<affiliation confidence="0.9859935">
InterACT Research
Carnegie Mellon University, Pittsburgh, USA
</affiliation>
<email confidence="0.993556">
matteck@cs.cmu.edu, vogel+@cs.cmu.edu, ahw@cs.cmu.edu
</email>
<sectionHeader confidence="0.995637" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999947454545454">
We describe a new pruning approach to
remove phrase pairs from translation mod-
els of statistical machine translation sys-
tems. The approach applies the original
translation system to a large amount of text
and calculates usage statistics for the
phrase pairs. Using these statistics the rele-
vance of each phrase pair can be estimated.
The approach is tested against a strong
baseline based on previous work and shows
significant improvements.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999959642857143">
A relatively new device for translation systems are
small portable devices like cell phones, PDAs and
handheld game consoles. The idea here is to have a
lightweight and convenient translation device e.g.
for tourists that can be easily carried. Other appli-
cations include medical, relief, and military scenar-
ios.
Preferably such a device will offer speech-to-
speech translation for both (or multiple) translation
directions. These devices have been researched and
are starting to become commercially available (e.g.
Isotani et al., 2003). The main challenges here are
the severe restrictions regarding both memory and
computing power on such a small portable device.
</bodyText>
<subsectionHeader confidence="0.993189">
1.1 Statistical Machine Translation
</subsectionHeader>
<bodyText confidence="0.5867835">
Generally statistical machine translation systems
have recently outperformed other translation ap-
proaches so it seems natural to also apply them in
these scenarios.
</bodyText>
<page confidence="0.990612">
21
</page>
<bodyText confidence="0.999636470588235">
A main component of every statistical machine
translation system is the translation model. The
translation model assigns translation probabilities
to phrase1 pairs of source and target phrases ex-
tracted from a parallel bilingual text. These phrase
pairs are applied during the decoding process and
their target sides are combined to form the final
translation. A variety of algorithms to extract
phrase pairs has been proposed. (e.g. Och and Ney,
2000 and Vogel, 2005).
Our proposed approach now tries to remove
phrase pairs, which have little influence on the fi-
nal translation performance, from a translation sys-
tem (pruning of the translation model2). The goal
is to reduce the number of phrase pairs and in turn
the memory requirement of the whole translation
system, while not impacting the translation per-
formance too heavily.
The approach does not depend on the actual al-
gorithm used to extract the phrase pairs and can be
applied to every imaginable method that assigns
probabilities to phrase pairs. We assume that the
phrase pairs were pre-extracted before decoding.
(in contrast to the proposed approaches to “online
phrase extraction” (Zhang and Vogel, 2005; Calli-
son-Burch et al., 2005)).
The task now is to remove enough pre-extracted
phrase pairs in order to accommodate the possibly
strict memory limitations of a portable device
while restricting performance degradation as much
as possible.
We will not specifically address the computing
power limitations of the portable devices in this
paper.
</bodyText>
<footnote confidence="0.917543666666667">
1 A “phrase” here can also refer to a single word.
2 Small language models are also desirable and the approaches
could be applied as well but this was not investigated yet.
</footnote>
<note confidence="0.7735865">
Proceedings of NAACL HLT 2007, Companion Volume, pages 21–24,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.97805" genericHeader="method">
2 Previous work
</sectionHeader>
<bodyText confidence="0.9997005">
Previous work mainly introduced two natural ideas
to prune phrase pairs. Both are for example di-
rectly available in the Pharaoh decoder (Koehn,
2004).
</bodyText>
<subsubsectionHeader confidence="0.578789">
Probability threshold
</subsubsectionHeader>
<bodyText confidence="0.971735555555556">
A very simple way to prune phrase pairs from a
translation model is to use a probability threshold
and remove all pairs for which the translation
probability is below the threshold. The reasoning
for this is that it is very unlikely that a translation
with a very low probability will be chosen (over
another translation candidate with a higher prob-
ability).
Translation variety threshold
Another way to prune phrase pairs is to impose a
limit on the number of translation candidates for a
certain phrase. That means the pruned translation
model can only have equal or fewer possible trans-
lations for a given source phrase than the thresh-
old. This is accomplished by sorting the phrase
pairs for each source phrase according to their
probability and eliminating low probability ones
until the threshold is reached.
</bodyText>
<sectionHeader confidence="0.942718" genericHeader="method">
3 Pruning via Usage Statistics
</sectionHeader>
<bodyText confidence="0.9947719375">
The approach presented here uses a different idea
inspired by the Optimal Brain Damage algorithm
for neural networks (Le Cun et al., 1990).
The Optimal Brain Damage algorithm for neural
networks computes a saliency for each network
element. The saliency is the relevance for the per-
formance of the network. In each pruning step the
element with the smallest saliency is removed, and
the network is re-trained and all saliencies are re-
calculated etc.
We can analogously view each phrase pair in the
translation system as such a network element. The
question is of course how to calculate the relevance
for the performance for each phrase pair.
A simple approximation was already done in the
previous work using a probability or variety
threshold. Here the relevance is estimated using the
phrase pair probability or the phrase pair rank as
relevance indicators.
But these are not the only factors that influence
the final selection of a phrase pair and most of
these factors are not established during the training
and phrase extraction process. Especially the fol-
lowing two additional factors play a major role in
the importance of a phrase pair.
Frequency of the source phrase
We can clearly say that a phrase pair with a very
common source phrase will be much more impor-
tant than a phrase pair where the source phrase oc-
curs only very rarely.
Actual use of the phrase-pair
But even phrase-pairs with very common source
phrases might not be used for the final translation
hypothesis. It is for example possible that it is part
of a longer phrase pair that gets a higher probabil-
ity so that the shorter phrase pair is not used.
Generally there are a lot of different factors influ-
encing the estimated importance of a phrase pair
and it seems hard to consider every influence sepa-
rately. Hence the proposed idea does not use a
combination of features to estimate the phrase pair
importance. Instead the idea is to just apply the
translation system to a large amount of text and see
how often a phrase pair is actually used (i.e. influ-
ences the translation performance). If the translated
text is large enough this will give a good statistics
of the relevance of this respective phrase pair. This
leads to the following algorithm:
</bodyText>
<subsubsectionHeader confidence="0.426542">
Algorithm
</subsubsectionHeader>
<bodyText confidence="0.99828025">
Translate a large amount of (in-domain) data with
the translation system (tuned on a development set)
and collect the following two statistics for each
phrase pair in the translation model.
</bodyText>
<listItem confidence="0.9798865">
• c(phrase pair) = Count how often a phrase pair
was considered during decoding (i.e. was
added to the translation lattice)
• u(phrase pair) = Count how often a phrase pair
was used in the final translation (i.e. in the
chosen path through the lattice).
</listItem>
<bodyText confidence="0.985468">
The overall score for a phrase pair with simple
smoothing (+1) is calculated as:
score(phrase pair)
[log(c(phrase pair) 1)] * [ (
+ u phrase pair) 1]
+
We use the logarithm function to limit the influ-
ence of the c value. The u value is more important
as this measures how often a phrase was actually
used in a translation hypothesis. This scoring func-
</bodyText>
<page confidence="0.982734">
22
</page>
<bodyText confidence="0.999973">
tion was empirically found after experimenting
with a variety of possible scoring terms.
The phrase pairs can then be sorted according to
this score and the top n phrase pairs can be selected
for a smaller phrase translation model.
</bodyText>
<sectionHeader confidence="0.985071" genericHeader="method">
4 Data and Experiments
</sectionHeader>
<subsectionHeader confidence="0.875358">
4.1 Experimental Setup &amp; Baseline
</subsectionHeader>
<bodyText confidence="0.962355512820513">
Translation system
The translation system that was used for the ex-
periments is a state-of-the-art statistical machine
translation system (Eck et al. 2006). The system
uses a phrase extraction method described in Vogel
(2005) and a 6-gram language model.
Training and testing data
The training data for all experiments consisted of
the BTEC corpus (Takezawa et al., 2002) with
162,318 lines of parallel Japanese-English text. All
translations were done from Japanese to English.
The language model was trained on the English
part of the training data.
The test set from the evaluation campaign of
IWSLT 2004 (Akiba et al., 2004) was used as test-
ing data. This data consists of 500 lines of tourism
data. 16 reference translations to English were
available.
Extracted phrases
Phrase pairs for n-grams up to length 10 were ex-
tracted (with low frequency thresholds for higher
n-grams). This gave 4,684,044 phrase pairs
(273,459 distinct source phrases). The baseline
score using all phrase pairs was 59.11 (BLEU,
Papineni et al., 2002) with a 95% confidence inter-
val of [57.13, 61.09].
Baseline pruning
The approaches presented in previous work served
as a baseline. The probability threshold was tested
for 8 values (0 (no pruning), 0.0001, 0.0005, 0.001,
0.005, 0.01, 0.05, 0.1) while the variety threshold
tested for 14 values (1, 2, 3, 4, 5, 6, 8, 10, 15, 20,
50, 100, 200, 500 (no pruning in this case)) and all
combinations thereof. The final translation scores
for different settings are very fluctuating. For that
reason we defined the baseline score for each pos-
sible size as the best score that was reached with
equal or less phrase pairs than the given size in any
of the tested combinations.
</bodyText>
<sectionHeader confidence="0.964304" genericHeader="evaluation">
4.2 Results for
</sectionHeader>
<subsectionHeader confidence="0.958807">
Pruning via Usage Statistics
</subsectionHeader>
<bodyText confidence="0.999593466666667">
For the proposed approach “Pruning via Usage
Statistics”, the translation system was applied to
the 162,318 lines of Japanese training data.
As explained in section 3 it was now counted for
each phrase pair how often it occurred in a transla-
tion lattice and how often it was used for the final
translation. The phrase pairs were then sorted ac-
cording to their relevance estimation and the top n
phrase pairs were chosen for different values of n.
The pruned phrase table was then used to translate
the IWSLT 2004 test set. Table 1 shows the results
comparing the baseline scores with the results us-
ing the described pruning. Figure 1 illustrates the
scores. The plateaus in the baseline graph are due
to the baseline definition as stated above.
</bodyText>
<table confidence="0.9995001875">
BLEU scores
# of Phrase Baseline Pruning Relative score
Pairs (n) improvement
100,000 - 0.4735 -
200,000 0.3162 0.5008 58.38%
300,000 0.4235 0.5154 21.70%
400,000 0.4743 0.5241 10.50%
500,000 0.4743 0.5269 11.09%
600,000 0.4890 0.5359 9.59%
800,000 0.5194 0.5394 3.85%
1,000,000 0.5355 0.5442 1.62%
1,500,000 0.5413 0.5523 2.03%
2,000,000 0.5630 0.5749 2.11%
3,000,000 0.5778 0.5798 0.35%
4,000,000 0.5855 0.5865 0.17%
4,684,044 0.5911 0.5911 0.00%
</table>
<tableCaption confidence="0.826378">
Table 1: BLEU scores at different levels of pruning
(Baseline: Best score with equal or less phrase
pairs)
</tableCaption>
<bodyText confidence="0.9998072">
For more than 1 million phrase pairs the differ-
ences are not very pronounced. However the trans-
lation score for the proposed pruning algorithm is
still not significantly lower than the 59.11 score at
2 million phrase pairs while the baseline drops
slightly faster. For less than 1 million phrase pairs
the differences become much more pronounced
with relative improvements of up to 58% at
200,000 phrase pairs. It is interesting to note that
the improved pruning removes infrequent source
</bodyText>
<page confidence="0.995451">
23
</page>
<bodyText confidence="0.9578815">
phrases and to a lesser extent source vocabulary
even for larger numbers of phrase pairs.
</bodyText>
<figureCaption confidence="0.99802">
Figure 1: Pruning and baseline comparison
</figureCaption>
<sectionHeader confidence="0.996611" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999980423076923">
The proposed pruning algorithm is able to outper-
form a strong baseline based on previously intro-
duced threshold pruning ideas. Over 50% of phrase
pairs can be pruned without a significant loss of
performance. Even for very low memory situations
the improved pruning remains a viable option
while the baseline pruning performance drops
heavily.
One idea to improve this new pruning approach
is to exchange the used count with the count of the
phrase occurring in the best path of the lattice ac-
cording to a scoring metric. This would require
having a reference translation available to be able
to tell which path is the actual best one (metric-
best path). It would be interesting to compare the
performance if the statistics is done using the met-
ric-best path on a smaller amount of data to the
performance if the statistics is done using the
model-best path on a larger amount (as there is no
reference translation necessary).
The Optimal Brain Damage algorithm recalcu-
lates the saliency after removing each network
element. It could also be beneficial to sequentially
prune the phrase pairs and always re-calculate the
statistics after removing a certain number of phrase
pairs.
</bodyText>
<sectionHeader confidence="0.999343" genericHeader="acknowledgments">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.9969515">
This work was partly supported by the US DARPA
under the programs GALE and TRANSTAC.
</bodyText>
<sectionHeader confidence="0.998493" genericHeader="references">
7 References
</sectionHeader>
<reference confidence="0.999432413043478">
Yasuhiro Akiba, Marcello Federico, Noriko Kando,
Hiromi Nakaiwa, Michael Paul, and Jun&apos;ichi Tsujii}.
2004. Overview of the IWSLT04 Evaluation Cam-
paign. Proceedings of IWSLT 2004, Kyoto, Japan.
Chris Callison-Burch, Colin Bannard, and Josh Schroe-
der. 2005. Scaling Phrase-Based Statistical Machine
Translation to Larger Corpora and Longer Phrases.
Proceedings of ACL 2005, Ann Arbor, MI, USA.
Yann Le Cun, John S. Denker, and Sara A. Solla. 1990.
Optimal brain damage. In Advances in Neural In-
formation Processing Systems 2, pages 598-605.
Morgan Kaufmann, 1990.
Matthias Eck, Ian Lane, Nguyen Bach, Sanjika He-
wavitharana, Muntsin Kolss, Bing Zhao, Almut Silja
Hildebrand, Stephan Vogel, and Alex Waibel. 2006.
The UKA/CMU Statistical Machine Translation Sys-
tem for IWSLT 2006. Proceedings of IWSLT 2006,
Kyoto, Japan.
Ryosuke Isotani, Kyoshi Yamabana, Shinichi Ando,
Ken Hanazawa, Shin-ya Ishikawa and Ken.ichi Iso.
2003. Speech-to-speech translation software on
PDAs for travel conversation. NEC research &amp; de-
velopment, Tokyo, Japan.
Philipp Koehn. 2004. A Beam Search Decoder for Sta-
tistical Machine Translation Models. Proceedings of
AMTA 2004, Baltimore, MD, USA.
Franz Josef Och and Hermann Ney, 2000. Improved
statistical alignment models, Proceedings of ACL
2000, Hongkong, China.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. Proceedings of
ACL 2002, Philadelphia, PA, USA.
Toshiyuki Takezawa, Eiichiro Sumita, Fumiaki Sugaya,
Hirofumi Yamamoto, and Seiichi Yamamoto. 2002.
Toward a Broad-coverage Bilingual Corpus for
Speech Translation of Travel Conversation in the
Real World. Proceedings of LREC 2002, Las Palmas,
Spain.
Stephan Vogel. 2005. PESA: Phrase Pair Extraction as
Sentence Splitting. Proceedings of MTSummit X,
Phuket, Thailand.
Ying Zhang and Stephan Vogel. 2005. An Efficient
Phrase-to-Phrase Alignment Model for Arbitrarily
Long Phrases and Large Corpora. Proceedings of
EAMT 2005, Budapest, Hungary.
</reference>
<figure confidence="0.999300454545455">
BLEU score
0.60
0.55
0.50
0.45
0.40
0.35
0.30
0 1,000,000 2,000,000 3,000,000 4,000,000
phrase pairs
Baseline Pruning
</figure>
<page confidence="0.973833">
24
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.928699">
<title confidence="0.9995885">Translation Model Pruning via Usage for Statistical Machine Translation</title>
<author confidence="0.997597">Matthias Eck</author>
<author confidence="0.997597">Stephan Vogel</author>
<author confidence="0.997597">Alex</author>
<affiliation confidence="0.9695365">InterACT Carnegie Mellon University, Pittsburgh,</affiliation>
<email confidence="0.999418">matteck@cs.cmu.edu,vogel+@cs.cmu.edu,ahw@cs.cmu.edu</email>
<abstract confidence="0.999260166666667">We describe a new pruning approach to remove phrase pairs from translation models of statistical machine translation systems. The approach applies the original translation system to a large amount of text and calculates usage statistics for the phrase pairs. Using these statistics the relevance of each phrase pair can be estimated. The approach is tested against a strong baseline based on previous work and shows significant improvements.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yasuhiro Akiba</author>
<author>Marcello Federico</author>
<author>Noriko Kando</author>
<author>Hiromi Nakaiwa</author>
<author>Michael Paul</author>
<author>Jun&apos;ichi Tsujii</author>
</authors>
<date>2004</date>
<booktitle>Overview of the IWSLT04 Evaluation Campaign. Proceedings of IWSLT</booktitle>
<location>Kyoto, Japan.</location>
<contexts>
<context position="8392" citStr="Akiba et al., 2004" startWordPosition="1354" endWordPosition="1357">ion system The translation system that was used for the experiments is a state-of-the-art statistical machine translation system (Eck et al. 2006). The system uses a phrase extraction method described in Vogel (2005) and a 6-gram language model. Training and testing data The training data for all experiments consisted of the BTEC corpus (Takezawa et al., 2002) with 162,318 lines of parallel Japanese-English text. All translations were done from Japanese to English. The language model was trained on the English part of the training data. The test set from the evaluation campaign of IWSLT 2004 (Akiba et al., 2004) was used as testing data. This data consists of 500 lines of tourism data. 16 reference translations to English were available. Extracted phrases Phrase pairs for n-grams up to length 10 were extracted (with low frequency thresholds for higher n-grams). This gave 4,684,044 phrase pairs (273,459 distinct source phrases). The baseline score using all phrase pairs was 59.11 (BLEU, Papineni et al., 2002) with a 95% confidence interval of [57.13, 61.09]. Baseline pruning The approaches presented in previous work served as a baseline. The probability threshold was tested for 8 values (0 (no pruning</context>
</contexts>
<marker>Akiba, Federico, Kando, Nakaiwa, Paul, Tsujii, 2004</marker>
<rawString>Yasuhiro Akiba, Marcello Federico, Noriko Kando, Hiromi Nakaiwa, Michael Paul, and Jun&apos;ichi Tsujii}. 2004. Overview of the IWSLT04 Evaluation Campaign. Proceedings of IWSLT 2004, Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Colin Bannard</author>
<author>Josh Schroeder</author>
</authors>
<title>Scaling Phrase-Based Statistical Machine Translation to Larger Corpora and Longer Phrases.</title>
<date>2005</date>
<booktitle>Proceedings of ACL 2005,</booktitle>
<location>Ann Arbor, MI, USA.</location>
<contexts>
<context position="2779" citStr="Callison-Burch et al., 2005" startWordPosition="415" endWordPosition="419">slation performance, from a translation system (pruning of the translation model2). The goal is to reduce the number of phrase pairs and in turn the memory requirement of the whole translation system, while not impacting the translation performance too heavily. The approach does not depend on the actual algorithm used to extract the phrase pairs and can be applied to every imaginable method that assigns probabilities to phrase pairs. We assume that the phrase pairs were pre-extracted before decoding. (in contrast to the proposed approaches to “online phrase extraction” (Zhang and Vogel, 2005; Callison-Burch et al., 2005)). The task now is to remove enough pre-extracted phrase pairs in order to accommodate the possibly strict memory limitations of a portable device while restricting performance degradation as much as possible. We will not specifically address the computing power limitations of the portable devices in this paper. 1 A “phrase” here can also refer to a single word. 2 Small language models are also desirable and the approaches could be applied as well but this was not investigated yet. Proceedings of NAACL HLT 2007, Companion Volume, pages 21–24, Rochester, NY, April 2007. c�2007 Association for C</context>
</contexts>
<marker>Callison-Burch, Bannard, Schroeder, 2005</marker>
<rawString>Chris Callison-Burch, Colin Bannard, and Josh Schroeder. 2005. Scaling Phrase-Based Statistical Machine Translation to Larger Corpora and Longer Phrases. Proceedings of ACL 2005, Ann Arbor, MI, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yann Le Cun</author>
<author>John S Denker</author>
<author>Sara A Solla</author>
</authors>
<title>Optimal brain damage.</title>
<date>1990</date>
<booktitle>In Advances in Neural Information Processing Systems 2,</booktitle>
<pages>598--605</pages>
<publisher>Morgan Kaufmann,</publisher>
<marker>Le Cun, Denker, Solla, 1990</marker>
<rawString>Yann Le Cun, John S. Denker, and Sara A. Solla. 1990. Optimal brain damage. In Advances in Neural Information Processing Systems 2, pages 598-605. Morgan Kaufmann, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias Eck</author>
<author>Ian Lane</author>
<author>Nguyen Bach</author>
</authors>
<title>Sanjika Hewavitharana, Muntsin Kolss, Bing Zhao, Almut Silja</title>
<date>2006</date>
<booktitle>The UKA/CMU Statistical Machine Translation System for IWSLT 2006. Proceedings of IWSLT</booktitle>
<location>Kyoto, Japan.</location>
<contexts>
<context position="7919" citStr="Eck et al. 2006" startWordPosition="1277" endWordPosition="1280"> to limit the influence of the c value. The u value is more important as this measures how often a phrase was actually used in a translation hypothesis. This scoring func22 tion was empirically found after experimenting with a variety of possible scoring terms. The phrase pairs can then be sorted according to this score and the top n phrase pairs can be selected for a smaller phrase translation model. 4 Data and Experiments 4.1 Experimental Setup &amp; Baseline Translation system The translation system that was used for the experiments is a state-of-the-art statistical machine translation system (Eck et al. 2006). The system uses a phrase extraction method described in Vogel (2005) and a 6-gram language model. Training and testing data The training data for all experiments consisted of the BTEC corpus (Takezawa et al., 2002) with 162,318 lines of parallel Japanese-English text. All translations were done from Japanese to English. The language model was trained on the English part of the training data. The test set from the evaluation campaign of IWSLT 2004 (Akiba et al., 2004) was used as testing data. This data consists of 500 lines of tourism data. 16 reference translations to English were available</context>
</contexts>
<marker>Eck, Lane, Bach, 2006</marker>
<rawString>Matthias Eck, Ian Lane, Nguyen Bach, Sanjika Hewavitharana, Muntsin Kolss, Bing Zhao, Almut Silja Hildebrand, Stephan Vogel, and Alex Waibel. 2006. The UKA/CMU Statistical Machine Translation System for IWSLT 2006. Proceedings of IWSLT 2006, Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryosuke Isotani</author>
<author>Kyoshi Yamabana</author>
<author>Shinichi Ando</author>
<author>Ken Hanazawa</author>
</authors>
<title>Shin-ya Ishikawa and Ken.ichi Iso.</title>
<date>2003</date>
<location>Tokyo, Japan.</location>
<contexts>
<context position="1246" citStr="Isotani et al., 2003" startWordPosition="177" endWordPosition="180">g baseline based on previous work and shows significant improvements. 1 Introduction A relatively new device for translation systems are small portable devices like cell phones, PDAs and handheld game consoles. The idea here is to have a lightweight and convenient translation device e.g. for tourists that can be easily carried. Other applications include medical, relief, and military scenarios. Preferably such a device will offer speech-tospeech translation for both (or multiple) translation directions. These devices have been researched and are starting to become commercially available (e.g. Isotani et al., 2003). The main challenges here are the severe restrictions regarding both memory and computing power on such a small portable device. 1.1 Statistical Machine Translation Generally statistical machine translation systems have recently outperformed other translation approaches so it seems natural to also apply them in these scenarios. 21 A main component of every statistical machine translation system is the translation model. The translation model assigns translation probabilities to phrase1 pairs of source and target phrases extracted from a parallel bilingual text. These phrase pairs are applied </context>
</contexts>
<marker>Isotani, Yamabana, Ando, Hanazawa, 2003</marker>
<rawString>Ryosuke Isotani, Kyoshi Yamabana, Shinichi Ando, Ken Hanazawa, Shin-ya Ishikawa and Ken.ichi Iso. 2003. Speech-to-speech translation software on PDAs for travel conversation. NEC research &amp; development, Tokyo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>A Beam Search Decoder for Statistical Machine Translation Models.</title>
<date>2004</date>
<booktitle>Proceedings of AMTA 2004,</booktitle>
<location>Baltimore, MD, USA.</location>
<contexts>
<context position="3569" citStr="Koehn, 2004" startWordPosition="542" endWordPosition="543">egradation as much as possible. We will not specifically address the computing power limitations of the portable devices in this paper. 1 A “phrase” here can also refer to a single word. 2 Small language models are also desirable and the approaches could be applied as well but this was not investigated yet. Proceedings of NAACL HLT 2007, Companion Volume, pages 21–24, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics 2 Previous work Previous work mainly introduced two natural ideas to prune phrase pairs. Both are for example directly available in the Pharaoh decoder (Koehn, 2004). Probability threshold A very simple way to prune phrase pairs from a translation model is to use a probability threshold and remove all pairs for which the translation probability is below the threshold. The reasoning for this is that it is very unlikely that a translation with a very low probability will be chosen (over another translation candidate with a higher probability). Translation variety threshold Another way to prune phrase pairs is to impose a limit on the number of translation candidates for a certain phrase. That means the pruned translation model can only have equal or fewer p</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. A Beam Search Decoder for Statistical Machine Translation Models. Proceedings of AMTA 2004, Baltimore, MD, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved statistical alignment models,</title>
<date>2000</date>
<booktitle>Proceedings of ACL</booktitle>
<location>Hongkong, China.</location>
<contexts>
<context position="2031" citStr="Och and Ney, 2000" startWordPosition="295" endWordPosition="298">nerally statistical machine translation systems have recently outperformed other translation approaches so it seems natural to also apply them in these scenarios. 21 A main component of every statistical machine translation system is the translation model. The translation model assigns translation probabilities to phrase1 pairs of source and target phrases extracted from a parallel bilingual text. These phrase pairs are applied during the decoding process and their target sides are combined to form the final translation. A variety of algorithms to extract phrase pairs has been proposed. (e.g. Och and Ney, 2000 and Vogel, 2005). Our proposed approach now tries to remove phrase pairs, which have little influence on the final translation performance, from a translation system (pruning of the translation model2). The goal is to reduce the number of phrase pairs and in turn the memory requirement of the whole translation system, while not impacting the translation performance too heavily. The approach does not depend on the actual algorithm used to extract the phrase pairs and can be applied to every imaginable method that assigns probabilities to phrase pairs. We assume that the phrase pairs were pre-e</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney, 2000. Improved statistical alignment models, Proceedings of ACL 2000, Hongkong, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>Proceedings of ACL 2002,</booktitle>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="8796" citStr="Papineni et al., 2002" startWordPosition="1419" endWordPosition="1422">e-English text. All translations were done from Japanese to English. The language model was trained on the English part of the training data. The test set from the evaluation campaign of IWSLT 2004 (Akiba et al., 2004) was used as testing data. This data consists of 500 lines of tourism data. 16 reference translations to English were available. Extracted phrases Phrase pairs for n-grams up to length 10 were extracted (with low frequency thresholds for higher n-grams). This gave 4,684,044 phrase pairs (273,459 distinct source phrases). The baseline score using all phrase pairs was 59.11 (BLEU, Papineni et al., 2002) with a 95% confidence interval of [57.13, 61.09]. Baseline pruning The approaches presented in previous work served as a baseline. The probability threshold was tested for 8 values (0 (no pruning), 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1) while the variety threshold tested for 14 values (1, 2, 3, 4, 5, 6, 8, 10, 15, 20, 50, 100, 200, 500 (no pruning in this case)) and all combinations thereof. The final translation scores for different settings are very fluctuating. For that reason we defined the baseline score for each possible size as the best score that was reached with equal or less</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a Method for Automatic Evaluation of Machine Translation. Proceedings of ACL 2002, Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Toshiyuki Takezawa</author>
<author>Eiichiro Sumita</author>
<author>Fumiaki Sugaya</author>
<author>Hirofumi Yamamoto</author>
<author>Seiichi Yamamoto</author>
</authors>
<title>Toward a Broad-coverage Bilingual Corpus for Speech Translation of Travel Conversation in the Real World.</title>
<date>2002</date>
<booktitle>Proceedings of LREC 2002,</booktitle>
<location>Las Palmas,</location>
<contexts>
<context position="8135" citStr="Takezawa et al., 2002" startWordPosition="1312" endWordPosition="1315">rimenting with a variety of possible scoring terms. The phrase pairs can then be sorted according to this score and the top n phrase pairs can be selected for a smaller phrase translation model. 4 Data and Experiments 4.1 Experimental Setup &amp; Baseline Translation system The translation system that was used for the experiments is a state-of-the-art statistical machine translation system (Eck et al. 2006). The system uses a phrase extraction method described in Vogel (2005) and a 6-gram language model. Training and testing data The training data for all experiments consisted of the BTEC corpus (Takezawa et al., 2002) with 162,318 lines of parallel Japanese-English text. All translations were done from Japanese to English. The language model was trained on the English part of the training data. The test set from the evaluation campaign of IWSLT 2004 (Akiba et al., 2004) was used as testing data. This data consists of 500 lines of tourism data. 16 reference translations to English were available. Extracted phrases Phrase pairs for n-grams up to length 10 were extracted (with low frequency thresholds for higher n-grams). This gave 4,684,044 phrase pairs (273,459 distinct source phrases). The baseline score u</context>
</contexts>
<marker>Takezawa, Sumita, Sugaya, Yamamoto, Yamamoto, 2002</marker>
<rawString>Toshiyuki Takezawa, Eiichiro Sumita, Fumiaki Sugaya, Hirofumi Yamamoto, and Seiichi Yamamoto. 2002. Toward a Broad-coverage Bilingual Corpus for Speech Translation of Travel Conversation in the Real World. Proceedings of LREC 2002, Las Palmas, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
</authors>
<title>PESA: Phrase Pair Extraction as Sentence Splitting.</title>
<date>2005</date>
<booktitle>Proceedings of MTSummit X,</booktitle>
<location>Phuket, Thailand.</location>
<contexts>
<context position="2048" citStr="Vogel, 2005" startWordPosition="300" endWordPosition="301">chine translation systems have recently outperformed other translation approaches so it seems natural to also apply them in these scenarios. 21 A main component of every statistical machine translation system is the translation model. The translation model assigns translation probabilities to phrase1 pairs of source and target phrases extracted from a parallel bilingual text. These phrase pairs are applied during the decoding process and their target sides are combined to form the final translation. A variety of algorithms to extract phrase pairs has been proposed. (e.g. Och and Ney, 2000 and Vogel, 2005). Our proposed approach now tries to remove phrase pairs, which have little influence on the final translation performance, from a translation system (pruning of the translation model2). The goal is to reduce the number of phrase pairs and in turn the memory requirement of the whole translation system, while not impacting the translation performance too heavily. The approach does not depend on the actual algorithm used to extract the phrase pairs and can be applied to every imaginable method that assigns probabilities to phrase pairs. We assume that the phrase pairs were pre-extracted before d</context>
<context position="7989" citStr="Vogel (2005)" startWordPosition="1290" endWordPosition="1291">his measures how often a phrase was actually used in a translation hypothesis. This scoring func22 tion was empirically found after experimenting with a variety of possible scoring terms. The phrase pairs can then be sorted according to this score and the top n phrase pairs can be selected for a smaller phrase translation model. 4 Data and Experiments 4.1 Experimental Setup &amp; Baseline Translation system The translation system that was used for the experiments is a state-of-the-art statistical machine translation system (Eck et al. 2006). The system uses a phrase extraction method described in Vogel (2005) and a 6-gram language model. Training and testing data The training data for all experiments consisted of the BTEC corpus (Takezawa et al., 2002) with 162,318 lines of parallel Japanese-English text. All translations were done from Japanese to English. The language model was trained on the English part of the training data. The test set from the evaluation campaign of IWSLT 2004 (Akiba et al., 2004) was used as testing data. This data consists of 500 lines of tourism data. 16 reference translations to English were available. Extracted phrases Phrase pairs for n-grams up to length 10 were extr</context>
</contexts>
<marker>Vogel, 2005</marker>
<rawString>Stephan Vogel. 2005. PESA: Phrase Pair Extraction as Sentence Splitting. Proceedings of MTSummit X, Phuket, Thailand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Zhang</author>
<author>Stephan Vogel</author>
</authors>
<title>An Efficient Phrase-to-Phrase Alignment Model for Arbitrarily Long Phrases and Large Corpora.</title>
<date>2005</date>
<booktitle>Proceedings of EAMT 2005,</booktitle>
<location>Budapest, Hungary.</location>
<contexts>
<context position="2749" citStr="Zhang and Vogel, 2005" startWordPosition="411" endWordPosition="414">uence on the final translation performance, from a translation system (pruning of the translation model2). The goal is to reduce the number of phrase pairs and in turn the memory requirement of the whole translation system, while not impacting the translation performance too heavily. The approach does not depend on the actual algorithm used to extract the phrase pairs and can be applied to every imaginable method that assigns probabilities to phrase pairs. We assume that the phrase pairs were pre-extracted before decoding. (in contrast to the proposed approaches to “online phrase extraction” (Zhang and Vogel, 2005; Callison-Burch et al., 2005)). The task now is to remove enough pre-extracted phrase pairs in order to accommodate the possibly strict memory limitations of a portable device while restricting performance degradation as much as possible. We will not specifically address the computing power limitations of the portable devices in this paper. 1 A “phrase” here can also refer to a single word. 2 Small language models are also desirable and the approaches could be applied as well but this was not investigated yet. Proceedings of NAACL HLT 2007, Companion Volume, pages 21–24, Rochester, NY, April </context>
</contexts>
<marker>Zhang, Vogel, 2005</marker>
<rawString>Ying Zhang and Stephan Vogel. 2005. An Efficient Phrase-to-Phrase Alignment Model for Arbitrarily Long Phrases and Large Corpora. Proceedings of EAMT 2005, Budapest, Hungary.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>