<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000537">
<title confidence="0.987828">
Unsupervised Modeling of Twitter Conversations
</title>
<author confidence="0.9642345">
Alan Ritter*
Computer Sci. &amp; Eng.
</author>
<affiliation confidence="0.996804">
University of Washington
</affiliation>
<address confidence="0.960288">
Seattle, WA 98195
</address>
<email confidence="0.999304">
aritter@cs.washington.edu
</email>
<author confidence="0.992841">
Colin Cherry*
</author>
<affiliation confidence="0.987203">
National Research Council Canada
</affiliation>
<address confidence="0.798174">
Ottawa, Ontario, K1A 0R6
</address>
<email confidence="0.730289">
Colin.Cherry@nrc-cnrc.gc.ca
</email>
<author confidence="0.95813">
Bill Dolan
</author>
<affiliation confidence="0.929497">
Microsoft Research
</affiliation>
<address confidence="0.972973">
Redmond, WA 98052
</address>
<email confidence="0.998756">
billdol@microsoft.com
</email>
<sectionHeader confidence="0.993894" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999982722222222">
We propose the first unsupervised approach
to the problem of modeling dialogue acts in
an open domain. Trained on a corpus of
noisy Twitter conversations, our method dis-
covers dialogue acts by clustering raw utter-
ances. Because it accounts for the sequential
behaviour of these acts, the learned model can
provide insight into the shape of communica-
tion in a new medium. We address the chal-
lenge of evaluating the emergent model with a
qualitative visualization and an intrinsic con-
versation ordering task. This work is inspired
by a corpus of 1.3 million Twitter conversa-
tions, which will be made publicly available.
This huge amount of data, available only be-
cause Twitter blurs the line between chatting
and publishing, highlights the need to be able
to adapt quickly to a new medium.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999762583333333">
Automatic detection of dialogue structure is an im-
portant first step toward deep understanding of hu-
man conversations. Dialogue acts1 provide an
initial level of structure by annotating utterances
with shallow discourse roles such as “statement”,
“question” and “answer”. These acts are useful in
many applications, including conversational agents
(Wilks, 2006), dialogue systems (Allen et al., 2007),
dialogue summarization (Murray et al., 2006), and
flirtation detection (Ranganath et al., 2009).
Dialogue act tagging has traditionally followed an
annotate-train-test paradigm, which begins with the
</bodyText>
<footnote confidence="0.956536">
*This work was conducted at Microsoft Research.
1Also called “speech acts”
</footnote>
<bodyText confidence="0.999259361111111">
design of annotation guidelines, followed by the col-
lection and labeling of corpora (Jurafsky et al., 1997;
Dhillon et al., 2004). Only then can one train a tag-
ger to automatically recognize dialogue acts (Stol-
cke et al., 2000). This paradigm has been quite suc-
cessful, but the labeling process is both slow and
expensive, limiting the amount of data available for
training. The expense is compounded as we con-
sider new methods of communication, which may
require not only new annotations, but new annota-
tion guidelines and new dialogue acts. This issue be-
comes more pressing as the Internet continues to ex-
pand the number of ways in which we communicate,
bringing us e-mail, newsgroups, IRC, forums, blogs,
Facebook, Twitter, and whatever is on the horizon.
Previous work has taken a variety of approaches
to dialogue act tagging in new media. Cohen et al.
(2004) develop an inventory of dialogue acts specific
to e-mail in an office domain. They design their in-
ventory by inspecting a large corpus of e-mail, and
refine it during the manual tagging process. Jeong et
al. (2009) use semi-supervised learning to transfer
dialogue acts from labeled speech corpora to the In-
ternet media of forums and e-mail. They manually
restructure the source act inventories in an attempt
to create coarse, domain-independent acts. Each ap-
proach relies on a human designer to inject knowl-
edge into the system through the inventory of avail-
able acts.
As an alternative solution for new media, we pro-
pose a series of unsupervised conversation models,
where the discovery of acts amounts to clustering
utterances with similar conversational roles. This
avoids manual construction of an act inventory, and
allows the learning algorithm to tell us something
about how people converse in a new medium.
</bodyText>
<page confidence="0.965076">
172
</page>
<note confidence="0.7531325">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 172–180,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.99995447368421">
There is surprisingly little work in unsupervised
dialogue act tagging. Woszczyna and Waibel (1994)
propose an unsupervised Hidden Markov Model
(HMM) for dialogue structure in a meeting schedul-
ing domain, but model dialogue state at the word
level. Crook et al. (2009) use Dirichlet process mix-
ture models to cluster utterances into a flexible num-
ber of acts in a travel-planning domain, but do not
examine the sequential structure of dialogue.2
In contrast to previous work, we address the prob-
lem of discovering dialogue acts in an informal,
open-topic domain, where an unsupervised learner
may be distracted by strong topic clusters. We also
train and test our models in a new medium: Twit-
ter. Rather than test against existing dialogue inven-
tories, we evaluate using qualitative visualizations
and a novel conversation ordering task, to ensure our
models have the opportunity to discover dialogue
phenomena unique to this medium.
</bodyText>
<sectionHeader confidence="0.992216" genericHeader="introduction">
2 Data
</sectionHeader>
<bodyText confidence="0.999957782608696">
To enable the study of large-data solutions to di-
alogue modeling, we have collected a corpus of
1.3 million conversations drawn from the micro-
blogging service, Twitter. 3 To our knowledge,
this is the largest corpus of naturally occurring chat
data that has been available for study thus far. Sim-
ilar datasets include the NUS SMS corpus (How
and Kan, 2005), several IRC chat corpora (Elsner
and Charniak, 2008; Forsyth and Martell, 2007),
and blog datasets (Yano et al., 2009; Gamon et al.,
2008), which can display conversational structure in
the blog comments.
As it characterizes itself as a micro-blog, it should
not be surprising that structurally, Twitter conversa-
tions lie somewhere between chat and blogs. Like
blogs, conversations on Twitter occur in a public en-
vironment, where they can be collected for research
purposes. However, Twitter posts are restricted to be
no longer than 140 characters, which keeps interac-
tions chat-like. Like e-mail and unlike IRC, Twit-
ter conversations are carried out by replying to spe-
cific posts. The Twitter API provides a link from
each reply to the post it is responding to, allowing
</bodyText>
<footnote confidence="0.97255275">
2The Crook et al. model should be able to be combined with
the models we present here.
3Will be available at http://www.cs.washington.
edu/homes/aritter/twitter_chat/
</footnote>
<equation confidence="0.723784333333333">
log frequency 0 2 4 6 8 10 12 14
1 2 3 4 5
log length
</equation>
<figureCaption confidence="0.998935">
Figure 1: Conversation length versus frequency
</figureCaption>
<bodyText confidence="0.991789121212121">
accurate thread reconstruction without requiring a
conversation disentanglement step (Elsner and Char-
niak, 2008). The proportion of posts on Twitter that
are conversational in nature are somewhere around
37% (Kelly, 2009).
To collect this corpus, we crawled Twitter using
its publicly available API. We monitored the public
timeline4 to obtain a sample of active Twitter users.
To expand our user list, we also crawled up to 10
users who had engaged in dialogue with each seed
user. For each user, we retrieved all posts, retain-
ing only those that were in reply to some other post.
We recursively followed the chain of replies to re-
cover the entire conversation. A simple function-
word-driven filter was used to remove non-English
conversations.
We crawled Twitter for a 2 month period during
the summer of 2009. The resulting corpus consists
of about 1.3 million conversations, with each con-
versation containing between 2 and 243 posts. The
majority of conversations on Twitter are very short;
those of length 2 (one status post and a reply) ac-
count for 69% of the data. As shown in Figure 1, the
frequencies of conversation lengths follow a power-
law relationship.
While the style of writing used on Twitter is
widely varied, much of the text is very similar to
SMS text messages. This is likely because many
users access Twitter through mobile devices. Posts
are often highly ungrammatical, and filled with
spelling errors. In order to illustrate the spelling
variation found on Twitter, we ran the Jcluster word
clustering algorithm (Goodman, 2001) on our cor-
</bodyText>
<footnote confidence="0.998539">
4http://twitter.com/public_timeline pro-
vides the 20 most recent posts on Twitter
</footnote>
<page confidence="0.996018">
173
</page>
<table confidence="0.996038857142857">
coming comming
enough enought enuff enuf
be4 b4 befor before
yuhr yur your yor ur youur yhur
msgs messages
couldnt culdnt cldnt cannae cudnt couldent
about bou abt abour abut bowt
</table>
<tableCaption confidence="0.999894">
Table 1: A sample of Twitter spelling variation.
</tableCaption>
<bodyText confidence="0.9991791">
pus, and manually picked out clusters of spelling
variants; a sample is displayed in Table 1.
Twitter’s noisy style makes processing Twitter
text more difficult than other domains. While mov-
ing to a new domain (e.g. biomedical text) is a chal-
lenging task, at least the new words found in the
vocabulary are limited mostly to verbs and nouns,
while function words remain constant. On Twit-
ter, even closed-class words such as prepositions and
pronouns are spelled in many different ways.
</bodyText>
<sectionHeader confidence="0.984813" genericHeader="method">
3 Dialogue Analysis
</sectionHeader>
<bodyText confidence="0.999927571428571">
We propose two models to discover dialogue acts in
an unsupervised manner. An ideal model will give
insight into the sorts of conversations that happen
on Twitter, while providing a useful tool for later
processing. We first introduce the summarization
technology we apply to this task, followed by two
Bayesian extensions.
</bodyText>
<subsectionHeader confidence="0.998847">
3.1 Conversation model
</subsectionHeader>
<bodyText confidence="0.999758277777778">
Our base model structure is inspired by the con-
tent model proposed by Barzilay and Lee (2004)
for multi-document summarization. Their sentence-
level HMM discovers the sequence of topics used
to describe a particular type of news event, such as
earthquakes. A news story is modeled by first gen-
erating a sequence of hidden topics according to a
Markov model, with each topic generating an ob-
served sentence according to a topic-specific lan-
guage model. These models capture the sequential
structure of news stories, and can be used for sum-
marization tasks such as sentence extraction and or-
dering.
Our goals are not so different: we wish to dis-
cover the sequential dialogue structure of conversa-
tion. Rather than learning a disaster’s location is
followed by its death toll, we instead wish to learn
that a question is followed by an answer. An initial
</bodyText>
<figureCaption confidence="0.999988">
Figure 2: Conversation Model
Figure 3: Conversation + Topic Model
</figureCaption>
<bodyText confidence="0.99997005882353">
conversation model can be created by simply apply-
ing the content modeling framework to conversation
data. We rename the hidden states acts, and assume
each post in a Twitter conversation is generated by
a single act.5 During development, we found that a
unigram language model performed best as the act
emission distribution.
The resulting conversation model is shown as a
plate diagram in Figure 2. Each conversation C is
a sequence of acts a, and each act produces a post,
represented by a bag of words shown using the W
plates. The number of acts available to the model
is fixed; we experimented with between 5 and 40.
Starting with a random assignment of acts, we train
our conversation model using EM, with forward-
backward providing act distributions during the ex-
pectation step. The model structure in Figure 2 is
</bodyText>
<footnote confidence="0.905114">
5The short length of Twitter posts makes this assumption
reasonable.
</footnote>
<figure confidence="0.91539895">
Ck
W0
w0,j
a0 a1 a2
W1
w1,j
W2
w2,j
Ck
W0 s0,j W1
w0,j
a0 a1 a2
Bk
w1,j w2,j
WE
s1,j
Wk
W2
s2
,j
</figure>
<page confidence="0.983571">
174
</page>
<bodyText confidence="0.99957875">
sadly no. some pasta bake, but coffee and pasta bake is not a
contender for tea and toast... .
yum! Ground beef tacos? We ’re grilling out. Turkey dogs for
me, a Bubba Burger for my dh, and combo for the kids.
ha! They gotcha! You had to think about Arby’s to write that tweet.
Arby’s is conducting a psychlogical study. Of roast beef.
Rumbly tummy soon to be tamed by Dominos for lunch! Nom
nom nom!
</bodyText>
<tableCaption confidence="0.5421255">
Table 2: Example of a topical cluster discovered by
the EM Conversation Model.
</tableCaption>
<bodyText confidence="0.991069666666667">
similar to previous HMMs for supervised dialogue
act recognition (Stolcke et al., 2000), but our model
is trained unsupervised.
</bodyText>
<subsectionHeader confidence="0.999367">
3.2 Conversation + Topic model
</subsectionHeader>
<bodyText confidence="0.999955555555556">
Our conversations are not restricted to any partic-
ular topic: Twitter users can and will talk about
anything. Therefore, there is no guarantee that our
model, charged with discovering clusters of posts
that aid in the prediction of the next cluster, will nec-
essarily discover dialogue acts. The sequence model
could instead partition entire conversations into top-
ics, such as food, computers and music, and then pre-
dict that each topic self-transitions with high proba-
bility: if we begin talking about food, we are likely
to continue to do so. Since we began with a content
model, it is perhaps not surprising that our Conversa-
tion Model tends to discover a mixture of dialogue
and topic structure. Several high probability posts
from a topic-focused cluster discovered by EM are
shown in Table 2. These clusters are undesirable, as
they have little to do with dialogue structure.
In general, unsupervised sentence clustering tech-
niques need some degree of direction when a par-
ticular level of granularity is desired. Barzilay and
Lee (2004) mask named entities in their content
models, forcing their model to cluster topics about
earthquakes in general, and not instances of specific
earthquakes. This solution is not a good fit for Twit-
ter. As explained in Section 2, Twitter’s noisiness
resists off-the-shelf tools, such as named-entity rec-
ognizers and noun-phrase chunkers. Furthermore,
we would require a more drastic form of prepro-
cessing in order to mask all topic words, and not
just alter the topic granularity. During development,
we explored coarse methods to abstract away con-
tent while maintaining syntax, such as replacing to-
kens with either parts-of-speech or automatically-
generated word clusters, but we found that these ap-
proaches degrade model performance.
Another approach to filtering out topic informa-
tion leaves the data intact, but modifies the model
to account for topic. To that end, we adopt a Latent
Dirichlet Allocation, or LDA, framework (Blei et al.,
2003) similar to approaches used recently in sum-
marization (Daum´e III and Marcu, 2006; Haghighi
and Vanderwende, 2009). The goal of this extended
model is to separate content words from dialogue in-
dicators. Each word in a conversation is generated
from one of three sources:
</bodyText>
<listItem confidence="0.998458666666667">
• The current post’s dialogue act
• The conversation’s topic
• General English
</listItem>
<bodyText confidence="0.999982148148148">
The extended model is shown in Figure 3.6 In addi-
tion to act emission and transition parameters, the
model now includes a conversation-specific word
multinomial Bk that represents the topic, as well as a
universal general English multinomial OE. A new
hidden variable, s determines the source of each
word, and is drawn from a conversation-specific dis-
tribution over sources Irk. Following LDA conven-
tions, we place a symmetric Dirichlet prior over
each of the multinomials. Dirichlet concentration
parameters for act emission, act transition, conver-
sation topic, general English, and source become the
hyper-parameters of our model.
The multinomials Bk, Irk and OE create non-local
dependencies in our model, breaking our HMM dy-
namic programing. Therefore we adopt Gibbs sam-
pling as our inference engine. Each hidden vari-
able is sampled in turn, conditioned on a complete
assignment of all other hidden variables through-
out the data set. Again following LDA convention,
we carry out collapsed sampling, where the various
multinomials are integrated out, and are never ex-
plicitly estimated. This results in a sampling se-
quence where for each post we first sample its act,
and then sample a source for each word in the post.
The hidden act and source variables are sampled ac-
cording to the following transition distributions:
</bodyText>
<footnote confidence="0.999173">
6This figure omits hyperparameters as well as act transition
and emission multinomials to reduce clutter. Dirichlet priors are
placed over all multinomials.
</footnote>
<page confidence="0.965757">
175
</page>
<equation confidence="0.997298166666667">
Ptrans(ai|a−i, s, w) ∝
Wi
P(ai|a−i) H P(wi,j|a, s, w−(i,j))
j=1
Ptrans(si,j|a, s−(i,j), w) ∝
P(si,j|s−(i,j))P(wi,j|a, s, w−(i,j))
</equation>
<bodyText confidence="0.999965642857143">
These probabilities can be computed analogously to
the calculations used in the collapsed sampler for a
bigram HMM (Goldwater and Griffiths, 2007), and
those used for LDA (Griffiths and Steyvers, 2004).
Note that our model contains five hyperparame-
ters. Rather than attempt to set them using an ex-
pensive grid search, we treat the concentration pa-
rameters as additional hidden variables and sample
each in turn, conditioned on the current assignment
to all other variables. Because these variables are
continuous, we apply slice sampling (Neal, 2003).
Slice sampling is a general technique for drawing
samples from a distribution by sampling uniformly
from the area under its density function.
</bodyText>
<subsectionHeader confidence="0.999355">
3.3 Estimating Likelihood on Held-Out Data
</subsectionHeader>
<bodyText confidence="0.999869636363637">
In Section 4.2 we evaluate our models by comparing
their probability on held-out test conversations. As
computing this probability exactly is intractable in
our model, we employ a recently proposed Chibb-
style estimator (Murray and Salakhutdinov, 2008;
Wallach et al., 2009). Chibb estimators estimate the
probability of unseen data, P(w) by selecting a high
probability assignment to hidden variables h∗, and
taking advantage of the following equality which
can be easily derived from the definition of condi-
tional probability:
</bodyText>
<equation confidence="0.9991685">
P(w, h∗)
P(w) = P(h∗|w)
</equation>
<bodyText confidence="0.999993166666667">
As the numerator can be computed exactly, this re-
duces the problem of estimating P(w) to the eas-
ier problem of estimating P(h∗|w). Murray and
Salakhutdinov (2008) provide an unbiased estimator
for P(h∗|w), which is calculated using the station-
ary distribution of the Gibbs sampler.
</bodyText>
<subsectionHeader confidence="0.909292">
3.4 Bayesian Conversation model
</subsectionHeader>
<bodyText confidence="0.999921642857143">
Given the infrastructure necessary for the Conver-
sation+Topic model described above, it is straight-
forward to also implement a Bayesian version of
of the conversation model described in Section 3.1.
This amounts to replacing the add-x smoothing of
dialogue act emission and transition probabilities
with (potentially sparse) Dirichlet priors, and replac-
ing EM with Gibbs sampling. There is reason to
believe that integrating out multinomials and using
sparse priors will improve the performance of the
conversation model, as improvements have been ob-
served when using a Bayesian HMM for unsuper-
vised part-of-speech tagging (Goldwater and Grif-
fiths, 2007).
</bodyText>
<sectionHeader confidence="0.999584" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999920911764706">
Evaluating automatically discovered dialogue acts
is a difficult problem. Unlike previous work, our
model automatically discovers an appropriate set of
dialogue acts for a new medium; these acts will
not necessarily have a close correspondence to di-
alogue act inventories manually designed for other
corpora. Instead of comparing against human anno-
tations, we present a visualization of the automati-
cally discovered dialogue acts, in addition to mea-
suring the ability of our models to predict post order
in unseen conversations. Ideally we would evaluate
performance using an end-use application such as a
conversational agent; however as this is outside the
scope of this paper, we leave such an evaluation to
future work.
For all experiments we train our models on a set of
10,000 randomly sampled conversations with con-
versation length in posts ranging from 3 to 6. Note
that our implementations can likely scale to larger
data by using techniques such as SparseLDA (Yao
et al., 2009). We limit our vocabulary to the 5,000
most frequent words in the corpus.
When using EM, we train for 100 iterations, eval-
uating performance on the test set at each iteration,
and reporting the maximum. Smoothing parameters
are set using grid search on a development set.
When performing inference with Gibbs Sam-
pling, we use 1,000 samples for burn-in and take
10 samples at a lag of 100. Although using multi-
ple samples introduces the possibility of poor results
due to “act drift”, we found this not to be a problem
in practice; in fact, taking multiple samples substan-
tially improved performance during development.
Recall that we infer hyperparameters using slice
</bodyText>
<page confidence="0.997392">
176
</page>
<bodyText confidence="0.9996625">
sampling. The concentration parameters chosen in
this manner were always sparse (&lt; 1), which pro-
duced a moderate improvement over an uninformed
prior.
</bodyText>
<subsectionHeader confidence="0.99434">
4.1 Qualitative Evaluation
</subsectionHeader>
<bodyText confidence="0.999990837837838">
We are quite interested in what our models can tell
us about how people converse on Twitter. To vi-
sualize and interpret our competing models, we ex-
amined act-emission distributions, posts with high-
confidence acts, and act-transition diagrams. Of
the three competing systems, we found the Conver-
sation+Topic model by far the easiest to interpret:
the 10-act model has 8 acts that we found intuitive,
while the other 2 are used only with low probabil-
ity. Conversely, the Conversation model, whether
trained by EM or Gibbs sampling, suffered from
the inclusion of general terms and from the confla-
tion of topic and dialogue. For example, the EM-
trained conversation model discovered an “act” that
was clearly a collection of posts about food, with no
underlying dialogue theme (see Table 2).
In the remainder of this section, we reproduce
our visualization for the 10-act Conversation+Topic
model. Word lists summarizing the discovered dia-
logue acts are shown in Table 3. For each act, the
top 40 words are listed in order of decreasing emis-
sion probability. An example post, drawn from the
set of highest-confidence posts for that act, is also
included. Figure 4 provides a visualization of the
matrix of transition probabilities between dialogue
acts. An arrow is drawn from one act to the next
if the probability of transition is above 0.15.7 Note
that a uniform model would transition to each act
with probability 0.10. In both Table 3 and Figure 4,
we use intuitive names in place of cluster numbers.
These are based on our interpretations of the clus-
ters, and are provided only to benefit the reader when
interpreting the transition diagram.8
From inspecting the transition diagram (Figure 4),
one can see that the model employs three distinct
acts to initiate Twitter conversations. These initial
acts are quite different from one another, and lead to
</bodyText>
<footnote confidence="0.9965966">
7After setting this threshold, two Acts were cut off from the
rest of the graph (had no incoming edges), and were therefore
removed
8In some cases, the choice in name is somewhat arbitrary,
ie: answer versus response, reaction versus comment.
</footnote>
<figureCaption confidence="0.929679">
Figure 4: Transitions between dialogue acts. See
</figureCaption>
<bodyText confidence="0.976090964285715">
table 3 for word lists and example posts for each act
different sets of possible responses. We discuss each
of these in turn.
The Status act appears to represent a post in which
the user is broadcasting information about what they
are currently doing. This can be seen by the high
amount of probability mass given to words like I
and my, in addition to verbs such as go and get, as
well as temporal nouns such as today, tomorrow and
tonight.
The Reference Broadcast act consists mostly of
usernames and urls.9 Also prominent is the word rt,
which has special significance on Twitter, indicating
that the user is re-posting another user’s post. This
act represents a user broadcasting an interesting link
or quote to their followers. Also note that this node
transitions to the Reaction act with high probability.
Reaction appears to cover excited or appreciative re-
sponses to new information, assigning high proba-
bility to !, !!, !!!, lol, thanks, and haha.
Finally Question to Followers represents a user
asking a question to their followers. The presence
of the question mark and WH question words indi-
cate a question, while words like anyone and know
indicate that the user is asking for information or an
opinion. Note that this is distinct from the Question
act, which is in response to an initial post.
Another interesting point is the alternation be-
</bodyText>
<footnote confidence="0.666803">
9As part of the preprocessing of our corpus we replaced all
usernames and urls with the special tokens -usr- and -url-.
</footnote>
<page confidence="0.977957">
177
</page>
<table confidence="0.998831846153846">
Status I . to ! my , is for up in ... and going was today so at go get back day got this am but Im now tomorrow night work
tonight off morning home had gon need !! be just getting
I just changed my twitter page bkgornd and now I can’t stop looking at it, lol!!
Question to Followers ? you is do I to -url- what -usr- me , know if anyone why who can “ this or of that how does - : on your are need
any rt u should people want get did have would tell
anyone using google voice? just got my invite, should i?? don’t know what it is? -url- for the video and break
down
Reference Broadcast -usr- ! -url- rt : -usr-: - “ my the , is ( you new – ? !! ) this for at in follow of on ¡ lol u are twitter your thanks via
!!! by :) here 2 please check
rt -usr-: -usr- word that mac lip gloss give u lockjaw! lol
Question ? you what ! are is how u do the did your that, lol where why or ?? hey about was have who it in so haha on
doing going know good up get like were for there:) can
DWL!! what song is that??
Reaction ! you I :) !! , thanks lol it haha that love so good too your thank is are u !!! was for :d me -usr- ¡ hope ? my 3 omg
... oh great hey awesome - happy now aww
sweet! im so stoked now!
Comment you I . to , ! do ? it be if me your know have we can get will :) but u that see lol would are so want go let up well
need - come ca make or think them
why are you in tx and why am I just now finding out about it?! i’m in dfw, till I get a job. i’ll have to come to
Htown soon!
Answer . I , you it “ that ? is but do was he the of a they if not would know be did or does think ) like ( as have what in are
- no them said who say ‘
my fave was “keeping on top of other week”
Response . I , it was that lol but is yeah ! haha he my know yes you :) like too did well she so its ... though do had no - one
as im thanks they think would not good oh
nah im out in maryland, leaving for tour in a few days.
</table>
<tableCaption confidence="0.779994">
Table 3: Word lists and example posts for each Dialogue Act. Words are listed in decreasing order of
probability given the act. Example posts are in italics.
</tableCaption>
<bodyText confidence="0.99995">
tween the personal pronouns you and I in the acts
due to the focus of conversation and speaker. The
Status act generates the word I with high probability,
whereas the likely response state Question generates
you, followed by Response which again generates I.
</bodyText>
<subsectionHeader confidence="0.997788">
4.2 Quantitative Evaluation
</subsectionHeader>
<bodyText confidence="0.99998772">
Qualitative evaluations are both time-consuming
and subjective. The above visualization is useful for
understanding the Twitter domain, but it is of little
use when comparing model variants or selecting pa-
rameters. Therefore, we also propose a novel quan-
titative evaluation that measures the intrinsic qual-
ity of a conversation model by its ability to predict
the ordering of posts in a conversation. This mea-
sures the model’s predictive power, while requiring
no tagged data, and no commitment to an existing
tag inventory.
Our test set consists of 1,000 randomly selected
conversations not found in the training data. For
each conversation in the test set, we generate all
n! permutations of the posts. The probability of
each permutation is then evaluated as if it were an
unseen conversation, using either the forward algo-
rithm (EM) or the Chibb-style estimator (Gibbs).
Following work from the summarization community
(Barzilay and Lee, 2004), we employ Kendall’s r to
measure the similarity of the max-probability per-
mutation to the original order.
The Kendall r rank correlation coefficient mea-
sures the similarity between two permutations based
on their agreement in pairwise orderings:
</bodyText>
<equation confidence="0.70014">
n+ − n_
�� )
2
</equation>
<bodyText confidence="0.999951">
where n+ is the number of pairs that share the same
order in both permutations, and n_ is the number
that do not. This statistic ranges between -1 and +1,
where -1 indicates inverse order, and +1 indicates
identical order. A value greater than 0 indicates a
positive correlation.
Predicting post order on open-domain Twitter
conversations is a much more difficult task than on
topic-focused news data (Barzilay and Lee, 2004).
We found that a simple bigram model baseline does
very poorly at predicting order on Twitter, achieving
only a weak positive correlation of r = 0.0358 on
our test data as compared with 0.19-0.74 reported by
Barzilay and Lee on news data.
Note that r is not a perfect measure of model qual-
ity for conversations; in some cases, multiple order-
</bodyText>
<equation confidence="0.868206">
r =
</equation>
<page confidence="0.962037">
178
</page>
<figure confidence="0.693802">
# acts
</figure>
<figureCaption confidence="0.997576">
Figure 5: Performance at conversation ordering task.
</figureCaption>
<bodyText confidence="0.99999092">
ings of the same set of posts may form a perfectly
acceptable conversation. On the other hand, there
are often strong constraints on the type of response
we might expect to follow a particular dialogue act;
for example, answers follow questions. We would
expect an effective model to use these constraints to
predict order.
Performance at the conversation ordering task
while varying the number of acts for each model is
displayed in Figure 5. In general, we found that us-
ing Bayesian inference outperforms EM. Also note
that the Bayesian Conversation model outperforms
the Conversation+Topic model at predicting conver-
sation order. This is likely because modeling conver-
sation content as a sequence can in some cases help
to predict post ordering; for example, adjacent posts
are more likely to contain similar content words. Re-
call though that we found the Conversation+Topic
model to be far more interpretable.
Additionally we compare the likelihood of these
models on held out test data in Figure 6. Note that
the Bayesian methods produce models with much
higher likelihood.10 For the EM models, likelihood
tends to decrease on held out test data as we increase
the number of hidden states, due to overfitting.
</bodyText>
<sectionHeader confidence="0.999259" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99991">
We have presented an approach that allows the
unsupervised induction of dialogue structure from
naturally-occurring open-topic conversational data.
</bodyText>
<footnote confidence="0.394665666666667">
10Likelihood of the test data is estimated using the Chibb
Style estimator described in (Murray and Salakhutdinov, 2008;
Wallach et al., 2009). This method under-estimates likelihood
in expectation. The maximum likelihood (EM) likelihoods are
exact.
# acts
</footnote>
<figureCaption confidence="0.9020865">
Figure 6: Negative log likelihood on held out test
data (smaller values indicate higher likelihood).
</figureCaption>
<bodyText confidence="0.999981">
By visualizing the learned models, coherent patterns
emerge from a stew of data that human readers find
difficult to follow. We have extended a conversa-
tion sequence model to separate topic and dialogue
words, resulting in an interpretable set of automat-
ically generated dialogue acts. These discovered
acts have interesting differences from those found
in other domains, and reflect Twitter’s nature as a
micro-blog.
We have introduced the task of conversation or-
dering as an intrinsic measure of conversation model
quality. We found this measure quite useful in
the development of our models and algorithms, al-
though our experiments show that it does not nec-
essarily correlate with interpretability. We have di-
rectly compared Bayesian inference to EM on our
conversation ordering task, showing a clear advan-
tage for Bayesian methods.
Finally, we have collected a corpus of 1.3 million
Twitter conversations, which we will make available
to the research community, and which we hope will
be useful beyond the study of dialogue. In the fu-
ture, we wish to scale our models to the full corpus,
and extend them with more complex notions of dis-
course, topic and community. Ultimately, we hope
to put the learned conversation structure to use in the
construction of a data-driven, conversational agent.
</bodyText>
<sectionHeader confidence="0.996513" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999855">
We are grateful to everyone in the NLP and TMSN
groups at Microsoft Research for helpful discussions
and feedback. We thank Oren Etzioni, Michael Ga-
mon, Mausam and Fei Wu, and the anonymous re-
viewers for helpful comments on a previous draft.
</bodyText>
<figure confidence="0.999712384615385">
5 10 15 20 25 30 35 40
tau
0.0 0.1 0.2 0.3 0.4
EM Conversation
Conversation+Topic
Bayesian Conversation
negative log likelihood
310000 315000 320000 325000 330000 335000 340000
5 10 15
20 25 30 35 40
EM Conversation
Conversation+Topic
Bayesian Conversation
</figure>
<page confidence="0.990343">
179
</page>
<sectionHeader confidence="0.995034" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999964075268818">
James Allen, Nathanael Chambers, George Ferguson,
Lucian Galescu, Hyuckchul Jung, Mary Swift, and
William Taysom. 2007. Plow: a collaborative task
learning agent. In Proceedings of AAAI.
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In Proceedings of
HLT-NAACL, pages 113–120.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. J. Mach. Learn.
Res., 3:993–1022.
William W. Cohen, Vitor R. Carvalho, and Tom M.
Mitchell. 2004. Learning to classify email into
“speech acts”. In Proceedings of EMNLP.
Nigel Crook, Ramon Granell, and Stephen Pulman.
2009. Unsupervised classification of dialogue acts us-
ing a Dirichlet process mixture model. In Proceedings
of SIGDIAL, pages 341–348.
Hal Daum´e III and Daniel Marcu. 2006. Bayesian query-
focused summarization. In Proceedings of ACL.
Rajdip Dhillon, Sonali Bhagat, Hannah Carvey, and Eliz-
abeth Shriberg. 2004. Meeting recorder project: Dia-
log act labeling guide. Technical report, International
Computer Science Institute.
Micha Elsner and Eugene Charniak. 2008. You talking
to me? A corpus and algorithm for conversation dis-
entanglement. In Proceedings of ACL-HLT.
Eric N. Forsyth and Craig H. Martell. 2007. Lexical and
discourse analysis of online chat dialog. In Proceed-
ings of ICSC.
Michael Gamon, Sumit Basu, Dmitriy Belenko, Danyel
Fisher, Matthew Hurst, and Arnd Christian Knig.
2008. Blews: Using blogs to provide context for news
articles. In Proceedings of ICWSM.
Sharon Goldwater and Tom Griffiths. 2007. A fully
bayesian approach to unsupervised part-of-speech tag-
ging. In Proceedings of ACL, pages 744–751.
Joshua T. Goodman. 2001. A bit of progress in language
modeling. Technical report.
T. L. Griffiths and M. Steyvers. 2004. Finding scientific
topics. Proc Natl Acad Sci, 101 Suppl 1:5228–5235.
Aria Haghighi and Lucy Vanderwende. 2009. Exploring
content models for multi-document summarization. In
Proceedings of HLT-NAACL, pages 362–370.
Yijue How and Min-Yen Kan. 2005. Optimizing pre-
dictive text entry for short message service on mobile
phones. In Proceedings of HCII.
Minwoo Jeong, Chin-Yew Lin, and Gary Geunbae Lee.
2009. Semi-supervised speech act recognition in
emails and forums. In Proceedings of EMNLP, pages
1250–1259.
Dan Jurafsky, Liz Shriberg, and Debra Biasca. 1997.
Switchboard swbd-damsl shallow-discourse-function
annotation coders manual, draft 13. Technical report,
University of Colorado Institute of Cognitive Science.
Ryan Kelly. 2009. Pear analytics twitter study. Whitepa-
per, August.
Iain Murray and Ruslan Salakhutdinov. 2008. Evalu-
ating probabilities under high-dimensional latent vari-
able models. In Proceedings of NIPS, pages 1137–
1144.
Gabriel Murray, Steve Renals, Jean Carletta, and Johanna
Moore. 2006. Incorporating speaker and discourse
features into speech summarization. In Proceedings of
HLT-NAACL, pages 367–374.
Radford M. Neal. 2003. Slice sampling. Annals of
Statistics, 31:705–767.
Rajesh Ranganath, Dan Jurafsky, and Dan Mcfarland.
2009. It’s not you, it’s me: Detecting flirting and
its misperception in speed-dates. In Proceedings of
EMNLP, pages 334–342.
Andreas Stolcke, Noah Coccaro, Rebecca Bates, Paul
Taylor, Carol Van Ess-Dykema, Klaus Ries, Eliza-
beth Shriberg, Daniel Jurafsky, Rachel Martin, and
Marie Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational Linguistics, 26(3):339–373.
Hanna M. Wallach, Iain Murray, Ruslan Salakhutdinov,
and David M. Mimno. 2009. Evaluation methods for
topic models. In Proceedings of ICML, page 139.
Yorick Wilks. 2006. Artificial companions as a new kind
of interface to the future internet. In OII Research Re-
port No. 13.
M. Woszczyna and A. Waibel. 1994. Inferring linguistic
structure in spoken language. In Proceedings of IC-
SLP.
Tae Yano, William W. Cohen, and Noah A. Smith. 2009.
Predicting response to political blog posts with topic
models. In Proceedings of NAACL, pages 477–485.
Limin Yao, David Mimno, and Andrew McCallum.
2009. Efficient methods for topic model inference on
streaming document collections. In Proceedings of
KDD, pages 937–946.
</reference>
<page confidence="0.997761">
180
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.224107">
<title confidence="0.901538">Unsupervised Modeling of Twitter Conversations</title>
<affiliation confidence="0.999507">University of</affiliation>
<address confidence="0.999878">Seattle, WA 98195</address>
<email confidence="0.999581">aritter@cs.washington.edu</email>
<affiliation confidence="0.993817">National Research Council Canada</affiliation>
<address confidence="0.985282">Ottawa, Ontario, K1A 0R6</address>
<email confidence="0.523614">Colin.Cherry@nrc-cnrc.gc.ca</email>
<author confidence="0.931742">Bill</author>
<affiliation confidence="0.932956">Microsoft</affiliation>
<address confidence="0.999739">Redmond, WA 98052</address>
<email confidence="0.999727">billdol@microsoft.com</email>
<abstract confidence="0.998399210526316">We propose the first unsupervised approach to the problem of modeling dialogue acts in an open domain. Trained on a corpus of noisy Twitter conversations, our method discovers dialogue acts by clustering raw utterances. Because it accounts for the sequential behaviour of these acts, the learned model can provide insight into the shape of communication in a new medium. We address the challenge of evaluating the emergent model with a qualitative visualization and an intrinsic conversation ordering task. This work is inspired by a corpus of 1.3 million Twitter conversations, which will be made publicly available. This huge amount of data, available only because Twitter blurs the line between chatting and publishing, highlights the need to be able to adapt quickly to a new medium.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James Allen</author>
<author>Nathanael Chambers</author>
<author>George Ferguson</author>
<author>Lucian Galescu</author>
<author>Hyuckchul Jung</author>
<author>Mary Swift</author>
<author>William Taysom</author>
</authors>
<title>Plow: a collaborative task learning agent.</title>
<date>2007</date>
<booktitle>In Proceedings of AAAI.</booktitle>
<contexts>
<context position="1532" citStr="Allen et al., 2007" startWordPosition="225" endWordPosition="228"> will be made publicly available. This huge amount of data, available only because Twitter blurs the line between chatting and publishing, highlights the need to be able to adapt quickly to a new medium. 1 Introduction Automatic detection of dialogue structure is an important first step toward deep understanding of human conversations. Dialogue acts1 provide an initial level of structure by annotating utterances with shallow discourse roles such as “statement”, “question” and “answer”. These acts are useful in many applications, including conversational agents (Wilks, 2006), dialogue systems (Allen et al., 2007), dialogue summarization (Murray et al., 2006), and flirtation detection (Ranganath et al., 2009). Dialogue act tagging has traditionally followed an annotate-train-test paradigm, which begins with the *This work was conducted at Microsoft Research. 1Also called “speech acts” design of annotation guidelines, followed by the collection and labeling of corpora (Jurafsky et al., 1997; Dhillon et al., 2004). Only then can one train a tagger to automatically recognize dialogue acts (Stolcke et al., 2000). This paradigm has been quite successful, but the labeling process is both slow and expensive, </context>
</contexts>
<marker>Allen, Chambers, Ferguson, Galescu, Jung, Swift, Taysom, 2007</marker>
<rawString>James Allen, Nathanael Chambers, George Ferguson, Lucian Galescu, Hyuckchul Jung, Mary Swift, and William Taysom. 2007. Plow: a collaborative task learning agent. In Proceedings of AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Catching the drift: Probabilistic content models, with applications to generation and summarization.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>113--120</pages>
<contexts>
<context position="8953" citStr="Barzilay and Lee (2004)" startWordPosition="1429" endWordPosition="1432">o verbs and nouns, while function words remain constant. On Twitter, even closed-class words such as prepositions and pronouns are spelled in many different ways. 3 Dialogue Analysis We propose two models to discover dialogue acts in an unsupervised manner. An ideal model will give insight into the sorts of conversations that happen on Twitter, while providing a useful tool for later processing. We first introduce the summarization technology we apply to this task, followed by two Bayesian extensions. 3.1 Conversation model Our base model structure is inspired by the content model proposed by Barzilay and Lee (2004) for multi-document summarization. Their sentencelevel HMM discovers the sequence of topics used to describe a particular type of news event, such as earthquakes. A news story is modeled by first generating a sequence of hidden topics according to a Markov model, with each topic generating an observed sentence according to a topic-specific language model. These models capture the sequential structure of news stories, and can be used for summarization tasks such as sentence extraction and ordering. Our goals are not so different: we wish to discover the sequential dialogue structure of conversa</context>
<context position="12447" citStr="Barzilay and Lee (2004)" startWordPosition="2029" endWordPosition="2032">t that each topic self-transitions with high probability: if we begin talking about food, we are likely to continue to do so. Since we began with a content model, it is perhaps not surprising that our Conversation Model tends to discover a mixture of dialogue and topic structure. Several high probability posts from a topic-focused cluster discovered by EM are shown in Table 2. These clusters are undesirable, as they have little to do with dialogue structure. In general, unsupervised sentence clustering techniques need some degree of direction when a particular level of granularity is desired. Barzilay and Lee (2004) mask named entities in their content models, forcing their model to cluster topics about earthquakes in general, and not instances of specific earthquakes. This solution is not a good fit for Twitter. As explained in Section 2, Twitter’s noisiness resists off-the-shelf tools, such as named-entity recognizers and noun-phrase chunkers. Furthermore, we would require a more drastic form of preprocessing in order to mask all topic words, and not just alter the topic granularity. During development, we explored coarse methods to abstract away content while maintaining syntax, such as replacing toke</context>
<context position="26387" citStr="Barzilay and Lee, 2004" startWordPosition="4409" endWordPosition="4412">ion model by its ability to predict the ordering of posts in a conversation. This measures the model’s predictive power, while requiring no tagged data, and no commitment to an existing tag inventory. Our test set consists of 1,000 randomly selected conversations not found in the training data. For each conversation in the test set, we generate all n! permutations of the posts. The probability of each permutation is then evaluated as if it were an unseen conversation, using either the forward algorithm (EM) or the Chibb-style estimator (Gibbs). Following work from the summarization community (Barzilay and Lee, 2004), we employ Kendall’s r to measure the similarity of the max-probability permutation to the original order. The Kendall r rank correlation coefficient measures the similarity between two permutations based on their agreement in pairwise orderings: n+ − n_ �� ) 2 where n+ is the number of pairs that share the same order in both permutations, and n_ is the number that do not. This statistic ranges between -1 and +1, where -1 indicates inverse order, and +1 indicates identical order. A value greater than 0 indicates a positive correlation. Predicting post order on open-domain Twitter conversation</context>
</contexts>
<marker>Barzilay, Lee, 2004</marker>
<rawString>Regina Barzilay and Lillian Lee. 2004. Catching the drift: Probabilistic content models, with applications to generation and summarization. In Proceedings of HLT-NAACL, pages 113–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>3--993</pages>
<contexts>
<context position="13392" citStr="Blei et al., 2003" startWordPosition="2178" endWordPosition="2181">chunkers. Furthermore, we would require a more drastic form of preprocessing in order to mask all topic words, and not just alter the topic granularity. During development, we explored coarse methods to abstract away content while maintaining syntax, such as replacing tokens with either parts-of-speech or automaticallygenerated word clusters, but we found that these approaches degrade model performance. Another approach to filtering out topic information leaves the data intact, but modifies the model to account for topic. To that end, we adopt a Latent Dirichlet Allocation, or LDA, framework (Blei et al., 2003) similar to approaches used recently in summarization (Daum´e III and Marcu, 2006; Haghighi and Vanderwende, 2009). The goal of this extended model is to separate content words from dialogue indicators. Each word in a conversation is generated from one of three sources: • The current post’s dialogue act • The conversation’s topic • General English The extended model is shown in Figure 3.6 In addition to act emission and transition parameters, the model now includes a conversation-specific word multinomial Bk that represents the topic, as well as a universal general English multinomial OE. A ne</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. J. Mach. Learn. Res., 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William W Cohen</author>
<author>Vitor R Carvalho</author>
<author>Tom M Mitchell</author>
</authors>
<title>Learning to classify email into “speech acts”.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="2673" citStr="Cohen et al. (2004)" startWordPosition="408" endWordPosition="411">s been quite successful, but the labeling process is both slow and expensive, limiting the amount of data available for training. The expense is compounded as we consider new methods of communication, which may require not only new annotations, but new annotation guidelines and new dialogue acts. This issue becomes more pressing as the Internet continues to expand the number of ways in which we communicate, bringing us e-mail, newsgroups, IRC, forums, blogs, Facebook, Twitter, and whatever is on the horizon. Previous work has taken a variety of approaches to dialogue act tagging in new media. Cohen et al. (2004) develop an inventory of dialogue acts specific to e-mail in an office domain. They design their inventory by inspecting a large corpus of e-mail, and refine it during the manual tagging process. Jeong et al. (2009) use semi-supervised learning to transfer dialogue acts from labeled speech corpora to the Internet media of forums and e-mail. They manually restructure the source act inventories in an attempt to create coarse, domain-independent acts. Each approach relies on a human designer to inject knowledge into the system through the inventory of available acts. As an alternative solution fo</context>
</contexts>
<marker>Cohen, Carvalho, Mitchell, 2004</marker>
<rawString>William W. Cohen, Vitor R. Carvalho, and Tom M. Mitchell. 2004. Learning to classify email into “speech acts”. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nigel Crook</author>
<author>Ramon Granell</author>
<author>Stephen Pulman</author>
</authors>
<title>Unsupervised classification of dialogue acts using a Dirichlet process mixture model.</title>
<date>2009</date>
<booktitle>In Proceedings of SIGDIAL,</booktitle>
<pages>341--348</pages>
<contexts>
<context position="4060" citStr="Crook et al. (2009)" startWordPosition="626" endWordPosition="629">is avoids manual construction of an act inventory, and allows the learning algorithm to tell us something about how people converse in a new medium. 172 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 172–180, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics There is surprisingly little work in unsupervised dialogue act tagging. Woszczyna and Waibel (1994) propose an unsupervised Hidden Markov Model (HMM) for dialogue structure in a meeting scheduling domain, but model dialogue state at the word level. Crook et al. (2009) use Dirichlet process mixture models to cluster utterances into a flexible number of acts in a travel-planning domain, but do not examine the sequential structure of dialogue.2 In contrast to previous work, we address the problem of discovering dialogue acts in an informal, open-topic domain, where an unsupervised learner may be distracted by strong topic clusters. We also train and test our models in a new medium: Twitter. Rather than test against existing dialogue inventories, we evaluate using qualitative visualizations and a novel conversation ordering task, to ensure our models have the </context>
</contexts>
<marker>Crook, Granell, Pulman, 2009</marker>
<rawString>Nigel Crook, Ramon Granell, and Stephen Pulman. 2009. Unsupervised classification of dialogue acts using a Dirichlet process mixture model. In Proceedings of SIGDIAL, pages 341–348.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>Bayesian queryfocused summarization.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL.</booktitle>
<marker>Daum´e, Marcu, 2006</marker>
<rawString>Hal Daum´e III and Daniel Marcu. 2006. Bayesian queryfocused summarization. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rajdip Dhillon</author>
<author>Sonali Bhagat</author>
<author>Hannah Carvey</author>
<author>Elizabeth Shriberg</author>
</authors>
<title>Meeting recorder project: Dialog act labeling guide.</title>
<date>2004</date>
<tech>Technical report,</tech>
<institution>International Computer Science Institute.</institution>
<contexts>
<context position="1938" citStr="Dhillon et al., 2004" startWordPosition="283" endWordPosition="286"> utterances with shallow discourse roles such as “statement”, “question” and “answer”. These acts are useful in many applications, including conversational agents (Wilks, 2006), dialogue systems (Allen et al., 2007), dialogue summarization (Murray et al., 2006), and flirtation detection (Ranganath et al., 2009). Dialogue act tagging has traditionally followed an annotate-train-test paradigm, which begins with the *This work was conducted at Microsoft Research. 1Also called “speech acts” design of annotation guidelines, followed by the collection and labeling of corpora (Jurafsky et al., 1997; Dhillon et al., 2004). Only then can one train a tagger to automatically recognize dialogue acts (Stolcke et al., 2000). This paradigm has been quite successful, but the labeling process is both slow and expensive, limiting the amount of data available for training. The expense is compounded as we consider new methods of communication, which may require not only new annotations, but new annotation guidelines and new dialogue acts. This issue becomes more pressing as the Internet continues to expand the number of ways in which we communicate, bringing us e-mail, newsgroups, IRC, forums, blogs, Facebook, Twitter, an</context>
</contexts>
<marker>Dhillon, Bhagat, Carvey, Shriberg, 2004</marker>
<rawString>Rajdip Dhillon, Sonali Bhagat, Hannah Carvey, and Elizabeth Shriberg. 2004. Meeting recorder project: Dialog act labeling guide. Technical report, International Computer Science Institute.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Micha Elsner</author>
<author>Eugene Charniak</author>
</authors>
<title>You talking to me? A corpus and algorithm for conversation disentanglement.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-HLT.</booktitle>
<contexts>
<context position="5142" citStr="Elsner and Charniak, 2008" startWordPosition="803" endWordPosition="806">sting dialogue inventories, we evaluate using qualitative visualizations and a novel conversation ordering task, to ensure our models have the opportunity to discover dialogue phenomena unique to this medium. 2 Data To enable the study of large-data solutions to dialogue modeling, we have collected a corpus of 1.3 million conversations drawn from the microblogging service, Twitter. 3 To our knowledge, this is the largest corpus of naturally occurring chat data that has been available for study thus far. Similar datasets include the NUS SMS corpus (How and Kan, 2005), several IRC chat corpora (Elsner and Charniak, 2008; Forsyth and Martell, 2007), and blog datasets (Yano et al., 2009; Gamon et al., 2008), which can display conversational structure in the blog comments. As it characterizes itself as a micro-blog, it should not be surprising that structurally, Twitter conversations lie somewhere between chat and blogs. Like blogs, conversations on Twitter occur in a public environment, where they can be collected for research purposes. However, Twitter posts are restricted to be no longer than 140 characters, which keeps interactions chat-like. Like e-mail and unlike IRC, Twitter conversations are carried out</context>
</contexts>
<marker>Elsner, Charniak, 2008</marker>
<rawString>Micha Elsner and Eugene Charniak. 2008. You talking to me? A corpus and algorithm for conversation disentanglement. In Proceedings of ACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric N Forsyth</author>
<author>Craig H Martell</author>
</authors>
<title>Lexical and discourse analysis of online chat dialog.</title>
<date>2007</date>
<booktitle>In Proceedings of ICSC.</booktitle>
<contexts>
<context position="5170" citStr="Forsyth and Martell, 2007" startWordPosition="807" endWordPosition="810"> we evaluate using qualitative visualizations and a novel conversation ordering task, to ensure our models have the opportunity to discover dialogue phenomena unique to this medium. 2 Data To enable the study of large-data solutions to dialogue modeling, we have collected a corpus of 1.3 million conversations drawn from the microblogging service, Twitter. 3 To our knowledge, this is the largest corpus of naturally occurring chat data that has been available for study thus far. Similar datasets include the NUS SMS corpus (How and Kan, 2005), several IRC chat corpora (Elsner and Charniak, 2008; Forsyth and Martell, 2007), and blog datasets (Yano et al., 2009; Gamon et al., 2008), which can display conversational structure in the blog comments. As it characterizes itself as a micro-blog, it should not be surprising that structurally, Twitter conversations lie somewhere between chat and blogs. Like blogs, conversations on Twitter occur in a public environment, where they can be collected for research purposes. However, Twitter posts are restricted to be no longer than 140 characters, which keeps interactions chat-like. Like e-mail and unlike IRC, Twitter conversations are carried out by replying to specific pos</context>
</contexts>
<marker>Forsyth, Martell, 2007</marker>
<rawString>Eric N. Forsyth and Craig H. Martell. 2007. Lexical and discourse analysis of online chat dialog. In Proceedings of ICSC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Gamon</author>
<author>Sumit Basu</author>
<author>Dmitriy Belenko</author>
<author>Danyel Fisher</author>
<author>Matthew Hurst</author>
<author>Arnd Christian Knig</author>
</authors>
<title>Blews: Using blogs to provide context for news articles.</title>
<date>2008</date>
<booktitle>In Proceedings of ICWSM.</booktitle>
<contexts>
<context position="5229" citStr="Gamon et al., 2008" startWordPosition="818" endWordPosition="821">tion ordering task, to ensure our models have the opportunity to discover dialogue phenomena unique to this medium. 2 Data To enable the study of large-data solutions to dialogue modeling, we have collected a corpus of 1.3 million conversations drawn from the microblogging service, Twitter. 3 To our knowledge, this is the largest corpus of naturally occurring chat data that has been available for study thus far. Similar datasets include the NUS SMS corpus (How and Kan, 2005), several IRC chat corpora (Elsner and Charniak, 2008; Forsyth and Martell, 2007), and blog datasets (Yano et al., 2009; Gamon et al., 2008), which can display conversational structure in the blog comments. As it characterizes itself as a micro-blog, it should not be surprising that structurally, Twitter conversations lie somewhere between chat and blogs. Like blogs, conversations on Twitter occur in a public environment, where they can be collected for research purposes. However, Twitter posts are restricted to be no longer than 140 characters, which keeps interactions chat-like. Like e-mail and unlike IRC, Twitter conversations are carried out by replying to specific posts. The Twitter API provides a link from each reply to the </context>
</contexts>
<marker>Gamon, Basu, Belenko, Fisher, Hurst, Knig, 2008</marker>
<rawString>Michael Gamon, Sumit Basu, Dmitriy Belenko, Danyel Fisher, Matthew Hurst, and Arnd Christian Knig. 2008. Blews: Using blogs to provide context for news articles. In Proceedings of ICWSM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Tom Griffiths</author>
</authors>
<title>A fully bayesian approach to unsupervised part-of-speech tagging.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>744--751</pages>
<contexts>
<context position="15504" citStr="Goldwater and Griffiths, 2007" startWordPosition="2510" endWordPosition="2513"> for each post we first sample its act, and then sample a source for each word in the post. The hidden act and source variables are sampled according to the following transition distributions: 6This figure omits hyperparameters as well as act transition and emission multinomials to reduce clutter. Dirichlet priors are placed over all multinomials. 175 Ptrans(ai|a−i, s, w) ∝ Wi P(ai|a−i) H P(wi,j|a, s, w−(i,j)) j=1 Ptrans(si,j|a, s−(i,j), w) ∝ P(si,j|s−(i,j))P(wi,j|a, s, w−(i,j)) These probabilities can be computed analogously to the calculations used in the collapsed sampler for a bigram HMM (Goldwater and Griffiths, 2007), and those used for LDA (Griffiths and Steyvers, 2004). Note that our model contains five hyperparameters. Rather than attempt to set them using an expensive grid search, we treat the concentration parameters as additional hidden variables and sample each in turn, conditioned on the current assignment to all other variables. Because these variables are continuous, we apply slice sampling (Neal, 2003). Slice sampling is a general technique for drawing samples from a distribution by sampling uniformly from the area under its density function. 3.3 Estimating Likelihood on Held-Out Data In Sectio</context>
<context position="17615" citStr="Goldwater and Griffiths, 2007" startWordPosition="2832" endWordPosition="2836"> necessary for the Conversation+Topic model described above, it is straightforward to also implement a Bayesian version of of the conversation model described in Section 3.1. This amounts to replacing the add-x smoothing of dialogue act emission and transition probabilities with (potentially sparse) Dirichlet priors, and replacing EM with Gibbs sampling. There is reason to believe that integrating out multinomials and using sparse priors will improve the performance of the conversation model, as improvements have been observed when using a Bayesian HMM for unsupervised part-of-speech tagging (Goldwater and Griffiths, 2007). 4 Experiments Evaluating automatically discovered dialogue acts is a difficult problem. Unlike previous work, our model automatically discovers an appropriate set of dialogue acts for a new medium; these acts will not necessarily have a close correspondence to dialogue act inventories manually designed for other corpora. Instead of comparing against human annotations, we present a visualization of the automatically discovered dialogue acts, in addition to measuring the ability of our models to predict post order in unseen conversations. Ideally we would evaluate performance using an end-use </context>
</contexts>
<marker>Goldwater, Griffiths, 2007</marker>
<rawString>Sharon Goldwater and Tom Griffiths. 2007. A fully bayesian approach to unsupervised part-of-speech tagging. In Proceedings of ACL, pages 744–751.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua T Goodman</author>
</authors>
<title>A bit of progress in language modeling.</title>
<date>2001</date>
<tech>Technical report.</tech>
<contexts>
<context position="7682" citStr="Goodman, 2001" startWordPosition="1223" endWordPosition="1224"> The majority of conversations on Twitter are very short; those of length 2 (one status post and a reply) account for 69% of the data. As shown in Figure 1, the frequencies of conversation lengths follow a powerlaw relationship. While the style of writing used on Twitter is widely varied, much of the text is very similar to SMS text messages. This is likely because many users access Twitter through mobile devices. Posts are often highly ungrammatical, and filled with spelling errors. In order to illustrate the spelling variation found on Twitter, we ran the Jcluster word clustering algorithm (Goodman, 2001) on our cor4http://twitter.com/public_timeline provides the 20 most recent posts on Twitter 173 coming comming enough enought enuff enuf be4 b4 befor before yuhr yur your yor ur youur yhur msgs messages couldnt culdnt cldnt cannae cudnt couldent about bou abt abour abut bowt Table 1: A sample of Twitter spelling variation. pus, and manually picked out clusters of spelling variants; a sample is displayed in Table 1. Twitter’s noisy style makes processing Twitter text more difficult than other domains. While moving to a new domain (e.g. biomedical text) is a challenging task, at least the new wo</context>
</contexts>
<marker>Goodman, 2001</marker>
<rawString>Joshua T. Goodman. 2001. A bit of progress in language modeling. Technical report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T L Griffiths</author>
<author>M Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>Proc Natl Acad Sci, 101 Suppl</booktitle>
<pages>1--5228</pages>
<contexts>
<context position="15559" citStr="Griffiths and Steyvers, 2004" startWordPosition="2519" endWordPosition="2522">a source for each word in the post. The hidden act and source variables are sampled according to the following transition distributions: 6This figure omits hyperparameters as well as act transition and emission multinomials to reduce clutter. Dirichlet priors are placed over all multinomials. 175 Ptrans(ai|a−i, s, w) ∝ Wi P(ai|a−i) H P(wi,j|a, s, w−(i,j)) j=1 Ptrans(si,j|a, s−(i,j), w) ∝ P(si,j|s−(i,j))P(wi,j|a, s, w−(i,j)) These probabilities can be computed analogously to the calculations used in the collapsed sampler for a bigram HMM (Goldwater and Griffiths, 2007), and those used for LDA (Griffiths and Steyvers, 2004). Note that our model contains five hyperparameters. Rather than attempt to set them using an expensive grid search, we treat the concentration parameters as additional hidden variables and sample each in turn, conditioned on the current assignment to all other variables. Because these variables are continuous, we apply slice sampling (Neal, 2003). Slice sampling is a general technique for drawing samples from a distribution by sampling uniformly from the area under its density function. 3.3 Estimating Likelihood on Held-Out Data In Section 4.2 we evaluate our models by comparing their probabi</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>T. L. Griffiths and M. Steyvers. 2004. Finding scientific topics. Proc Natl Acad Sci, 101 Suppl 1:5228–5235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Lucy Vanderwende</author>
</authors>
<title>Exploring content models for multi-document summarization.</title>
<date>2009</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>362--370</pages>
<contexts>
<context position="13506" citStr="Haghighi and Vanderwende, 2009" startWordPosition="2195" endWordPosition="2198">ic words, and not just alter the topic granularity. During development, we explored coarse methods to abstract away content while maintaining syntax, such as replacing tokens with either parts-of-speech or automaticallygenerated word clusters, but we found that these approaches degrade model performance. Another approach to filtering out topic information leaves the data intact, but modifies the model to account for topic. To that end, we adopt a Latent Dirichlet Allocation, or LDA, framework (Blei et al., 2003) similar to approaches used recently in summarization (Daum´e III and Marcu, 2006; Haghighi and Vanderwende, 2009). The goal of this extended model is to separate content words from dialogue indicators. Each word in a conversation is generated from one of three sources: • The current post’s dialogue act • The conversation’s topic • General English The extended model is shown in Figure 3.6 In addition to act emission and transition parameters, the model now includes a conversation-specific word multinomial Bk that represents the topic, as well as a universal general English multinomial OE. A new hidden variable, s determines the source of each word, and is drawn from a conversation-specific distribution ov</context>
</contexts>
<marker>Haghighi, Vanderwende, 2009</marker>
<rawString>Aria Haghighi and Lucy Vanderwende. 2009. Exploring content models for multi-document summarization. In Proceedings of HLT-NAACL, pages 362–370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yijue How</author>
<author>Min-Yen Kan</author>
</authors>
<title>Optimizing predictive text entry for short message service on mobile phones.</title>
<date>2005</date>
<booktitle>In Proceedings of HCII.</booktitle>
<contexts>
<context position="5089" citStr="How and Kan, 2005" startWordPosition="795" endWordPosition="798"> medium: Twitter. Rather than test against existing dialogue inventories, we evaluate using qualitative visualizations and a novel conversation ordering task, to ensure our models have the opportunity to discover dialogue phenomena unique to this medium. 2 Data To enable the study of large-data solutions to dialogue modeling, we have collected a corpus of 1.3 million conversations drawn from the microblogging service, Twitter. 3 To our knowledge, this is the largest corpus of naturally occurring chat data that has been available for study thus far. Similar datasets include the NUS SMS corpus (How and Kan, 2005), several IRC chat corpora (Elsner and Charniak, 2008; Forsyth and Martell, 2007), and blog datasets (Yano et al., 2009; Gamon et al., 2008), which can display conversational structure in the blog comments. As it characterizes itself as a micro-blog, it should not be surprising that structurally, Twitter conversations lie somewhere between chat and blogs. Like blogs, conversations on Twitter occur in a public environment, where they can be collected for research purposes. However, Twitter posts are restricted to be no longer than 140 characters, which keeps interactions chat-like. Like e-mail </context>
</contexts>
<marker>How, Kan, 2005</marker>
<rawString>Yijue How and Min-Yen Kan. 2005. Optimizing predictive text entry for short message service on mobile phones. In Proceedings of HCII.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minwoo Jeong</author>
<author>Chin-Yew Lin</author>
<author>Gary Geunbae Lee</author>
</authors>
<title>Semi-supervised speech act recognition in emails and forums.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1250--1259</pages>
<contexts>
<context position="2888" citStr="Jeong et al. (2009)" startWordPosition="445" endWordPosition="448">ire not only new annotations, but new annotation guidelines and new dialogue acts. This issue becomes more pressing as the Internet continues to expand the number of ways in which we communicate, bringing us e-mail, newsgroups, IRC, forums, blogs, Facebook, Twitter, and whatever is on the horizon. Previous work has taken a variety of approaches to dialogue act tagging in new media. Cohen et al. (2004) develop an inventory of dialogue acts specific to e-mail in an office domain. They design their inventory by inspecting a large corpus of e-mail, and refine it during the manual tagging process. Jeong et al. (2009) use semi-supervised learning to transfer dialogue acts from labeled speech corpora to the Internet media of forums and e-mail. They manually restructure the source act inventories in an attempt to create coarse, domain-independent acts. Each approach relies on a human designer to inject knowledge into the system through the inventory of available acts. As an alternative solution for new media, we propose a series of unsupervised conversation models, where the discovery of acts amounts to clustering utterances with similar conversational roles. This avoids manual construction of an act invento</context>
</contexts>
<marker>Jeong, Lin, Lee, 2009</marker>
<rawString>Minwoo Jeong, Chin-Yew Lin, and Gary Geunbae Lee. 2009. Semi-supervised speech act recognition in emails and forums. In Proceedings of EMNLP, pages 1250–1259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Jurafsky</author>
<author>Liz Shriberg</author>
<author>Debra Biasca</author>
</authors>
<title>Switchboard swbd-damsl shallow-discourse-function annotation coders manual, draft 13.</title>
<date>1997</date>
<tech>Technical report,</tech>
<institution>University of Colorado Institute of Cognitive Science.</institution>
<contexts>
<context position="1915" citStr="Jurafsky et al., 1997" startWordPosition="279" endWordPosition="282">structure by annotating utterances with shallow discourse roles such as “statement”, “question” and “answer”. These acts are useful in many applications, including conversational agents (Wilks, 2006), dialogue systems (Allen et al., 2007), dialogue summarization (Murray et al., 2006), and flirtation detection (Ranganath et al., 2009). Dialogue act tagging has traditionally followed an annotate-train-test paradigm, which begins with the *This work was conducted at Microsoft Research. 1Also called “speech acts” design of annotation guidelines, followed by the collection and labeling of corpora (Jurafsky et al., 1997; Dhillon et al., 2004). Only then can one train a tagger to automatically recognize dialogue acts (Stolcke et al., 2000). This paradigm has been quite successful, but the labeling process is both slow and expensive, limiting the amount of data available for training. The expense is compounded as we consider new methods of communication, which may require not only new annotations, but new annotation guidelines and new dialogue acts. This issue becomes more pressing as the Internet continues to expand the number of ways in which we communicate, bringing us e-mail, newsgroups, IRC, forums, blogs</context>
</contexts>
<marker>Jurafsky, Shriberg, Biasca, 1997</marker>
<rawString>Dan Jurafsky, Liz Shriberg, and Debra Biasca. 1997. Switchboard swbd-damsl shallow-discourse-function annotation coders manual, draft 13. Technical report, University of Colorado Institute of Cognitive Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan Kelly</author>
</authors>
<title>Pear analytics twitter study.</title>
<date>2009</date>
<location>Whitepaper,</location>
<contexts>
<context position="6353" citStr="Kelly, 2009" startWordPosition="999" endWordPosition="1000">t by replying to specific posts. The Twitter API provides a link from each reply to the post it is responding to, allowing 2The Crook et al. model should be able to be combined with the models we present here. 3Will be available at http://www.cs.washington. edu/homes/aritter/twitter_chat/ log frequency 0 2 4 6 8 10 12 14 1 2 3 4 5 log length Figure 1: Conversation length versus frequency accurate thread reconstruction without requiring a conversation disentanglement step (Elsner and Charniak, 2008). The proportion of posts on Twitter that are conversational in nature are somewhere around 37% (Kelly, 2009). To collect this corpus, we crawled Twitter using its publicly available API. We monitored the public timeline4 to obtain a sample of active Twitter users. To expand our user list, we also crawled up to 10 users who had engaged in dialogue with each seed user. For each user, we retrieved all posts, retaining only those that were in reply to some other post. We recursively followed the chain of replies to recover the entire conversation. A simple functionword-driven filter was used to remove non-English conversations. We crawled Twitter for a 2 month period during the summer of 2009. The resul</context>
</contexts>
<marker>Kelly, 2009</marker>
<rawString>Ryan Kelly. 2009. Pear analytics twitter study. Whitepaper, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iain Murray</author>
<author>Ruslan Salakhutdinov</author>
</authors>
<title>Evaluating probabilities under high-dimensional latent variable models.</title>
<date>2008</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<pages>1137--1144</pages>
<contexts>
<context position="16345" citStr="Murray and Salakhutdinov, 2008" startWordPosition="2639" endWordPosition="2642">s additional hidden variables and sample each in turn, conditioned on the current assignment to all other variables. Because these variables are continuous, we apply slice sampling (Neal, 2003). Slice sampling is a general technique for drawing samples from a distribution by sampling uniformly from the area under its density function. 3.3 Estimating Likelihood on Held-Out Data In Section 4.2 we evaluate our models by comparing their probability on held-out test conversations. As computing this probability exactly is intractable in our model, we employ a recently proposed Chibbstyle estimator (Murray and Salakhutdinov, 2008; Wallach et al., 2009). Chibb estimators estimate the probability of unseen data, P(w) by selecting a high probability assignment to hidden variables h∗, and taking advantage of the following equality which can be easily derived from the definition of conditional probability: P(w, h∗) P(w) = P(h∗|w) As the numerator can be computed exactly, this reduces the problem of estimating P(w) to the easier problem of estimating P(h∗|w). Murray and Salakhutdinov (2008) provide an unbiased estimator for P(h∗|w), which is calculated using the stationary distribution of the Gibbs sampler. 3.4 Bayesian Con</context>
<context position="28980" citStr="Murray and Salakhutdinov, 2008" startWordPosition="4832" endWordPosition="4835">sation+Topic model to be far more interpretable. Additionally we compare the likelihood of these models on held out test data in Figure 6. Note that the Bayesian methods produce models with much higher likelihood.10 For the EM models, likelihood tends to decrease on held out test data as we increase the number of hidden states, due to overfitting. 5 Conclusion We have presented an approach that allows the unsupervised induction of dialogue structure from naturally-occurring open-topic conversational data. 10Likelihood of the test data is estimated using the Chibb Style estimator described in (Murray and Salakhutdinov, 2008; Wallach et al., 2009). This method under-estimates likelihood in expectation. The maximum likelihood (EM) likelihoods are exact. # acts Figure 6: Negative log likelihood on held out test data (smaller values indicate higher likelihood). By visualizing the learned models, coherent patterns emerge from a stew of data that human readers find difficult to follow. We have extended a conversation sequence model to separate topic and dialogue words, resulting in an interpretable set of automatically generated dialogue acts. These discovered acts have interesting differences from those found in othe</context>
</contexts>
<marker>Murray, Salakhutdinov, 2008</marker>
<rawString>Iain Murray and Ruslan Salakhutdinov. 2008. Evaluating probabilities under high-dimensional latent variable models. In Proceedings of NIPS, pages 1137– 1144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriel Murray</author>
<author>Steve Renals</author>
<author>Jean Carletta</author>
<author>Johanna Moore</author>
</authors>
<title>Incorporating speaker and discourse features into speech summarization.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>367--374</pages>
<contexts>
<context position="1578" citStr="Murray et al., 2006" startWordPosition="231" endWordPosition="234">mount of data, available only because Twitter blurs the line between chatting and publishing, highlights the need to be able to adapt quickly to a new medium. 1 Introduction Automatic detection of dialogue structure is an important first step toward deep understanding of human conversations. Dialogue acts1 provide an initial level of structure by annotating utterances with shallow discourse roles such as “statement”, “question” and “answer”. These acts are useful in many applications, including conversational agents (Wilks, 2006), dialogue systems (Allen et al., 2007), dialogue summarization (Murray et al., 2006), and flirtation detection (Ranganath et al., 2009). Dialogue act tagging has traditionally followed an annotate-train-test paradigm, which begins with the *This work was conducted at Microsoft Research. 1Also called “speech acts” design of annotation guidelines, followed by the collection and labeling of corpora (Jurafsky et al., 1997; Dhillon et al., 2004). Only then can one train a tagger to automatically recognize dialogue acts (Stolcke et al., 2000). This paradigm has been quite successful, but the labeling process is both slow and expensive, limiting the amount of data available for trai</context>
</contexts>
<marker>Murray, Renals, Carletta, Moore, 2006</marker>
<rawString>Gabriel Murray, Steve Renals, Jean Carletta, and Johanna Moore. 2006. Incorporating speaker and discourse features into speech summarization. In Proceedings of HLT-NAACL, pages 367–374.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radford M Neal</author>
</authors>
<title>Slice sampling.</title>
<date>2003</date>
<journal>Annals of Statistics,</journal>
<pages>31--705</pages>
<contexts>
<context position="15908" citStr="Neal, 2003" startWordPosition="2576" endWordPosition="2577">s(si,j|a, s−(i,j), w) ∝ P(si,j|s−(i,j))P(wi,j|a, s, w−(i,j)) These probabilities can be computed analogously to the calculations used in the collapsed sampler for a bigram HMM (Goldwater and Griffiths, 2007), and those used for LDA (Griffiths and Steyvers, 2004). Note that our model contains five hyperparameters. Rather than attempt to set them using an expensive grid search, we treat the concentration parameters as additional hidden variables and sample each in turn, conditioned on the current assignment to all other variables. Because these variables are continuous, we apply slice sampling (Neal, 2003). Slice sampling is a general technique for drawing samples from a distribution by sampling uniformly from the area under its density function. 3.3 Estimating Likelihood on Held-Out Data In Section 4.2 we evaluate our models by comparing their probability on held-out test conversations. As computing this probability exactly is intractable in our model, we employ a recently proposed Chibbstyle estimator (Murray and Salakhutdinov, 2008; Wallach et al., 2009). Chibb estimators estimate the probability of unseen data, P(w) by selecting a high probability assignment to hidden variables h∗, and taki</context>
</contexts>
<marker>Neal, 2003</marker>
<rawString>Radford M. Neal. 2003. Slice sampling. Annals of Statistics, 31:705–767.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rajesh Ranganath</author>
<author>Dan Jurafsky</author>
<author>Dan Mcfarland</author>
</authors>
<title>It’s not you, it’s me: Detecting flirting and its misperception in speed-dates.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>334--342</pages>
<contexts>
<context position="1629" citStr="Ranganath et al., 2009" startWordPosition="238" endWordPosition="241">urs the line between chatting and publishing, highlights the need to be able to adapt quickly to a new medium. 1 Introduction Automatic detection of dialogue structure is an important first step toward deep understanding of human conversations. Dialogue acts1 provide an initial level of structure by annotating utterances with shallow discourse roles such as “statement”, “question” and “answer”. These acts are useful in many applications, including conversational agents (Wilks, 2006), dialogue systems (Allen et al., 2007), dialogue summarization (Murray et al., 2006), and flirtation detection (Ranganath et al., 2009). Dialogue act tagging has traditionally followed an annotate-train-test paradigm, which begins with the *This work was conducted at Microsoft Research. 1Also called “speech acts” design of annotation guidelines, followed by the collection and labeling of corpora (Jurafsky et al., 1997; Dhillon et al., 2004). Only then can one train a tagger to automatically recognize dialogue acts (Stolcke et al., 2000). This paradigm has been quite successful, but the labeling process is both slow and expensive, limiting the amount of data available for training. The expense is compounded as we consider new </context>
</contexts>
<marker>Ranganath, Jurafsky, Mcfarland, 2009</marker>
<rawString>Rajesh Ranganath, Dan Jurafsky, and Dan Mcfarland. 2009. It’s not you, it’s me: Detecting flirting and its misperception in speed-dates. In Proceedings of EMNLP, pages 334–342.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
<author>Noah Coccaro</author>
<author>Rebecca Bates</author>
<author>Paul Taylor</author>
<author>Carol Van Ess-Dykema</author>
<author>Klaus Ries</author>
<author>Elizabeth Shriberg</author>
<author>Daniel Jurafsky</author>
<author>Rachel Martin</author>
<author>Marie Meteer</author>
</authors>
<title>Dialogue act modeling for automatic tagging and recognition of conversational speech.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>3</issue>
<marker>Stolcke, Coccaro, Bates, Taylor, Van Ess-Dykema, Ries, Shriberg, Jurafsky, Martin, Meteer, 2000</marker>
<rawString>Andreas Stolcke, Noah Coccaro, Rebecca Bates, Paul Taylor, Carol Van Ess-Dykema, Klaus Ries, Elizabeth Shriberg, Daniel Jurafsky, Rachel Martin, and Marie Meteer. 2000. Dialogue act modeling for automatic tagging and recognition of conversational speech. Computational Linguistics, 26(3):339–373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hanna M Wallach</author>
<author>Iain Murray</author>
<author>Ruslan Salakhutdinov</author>
<author>David M Mimno</author>
</authors>
<title>Evaluation methods for topic models.</title>
<date>2009</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>139</pages>
<contexts>
<context position="16368" citStr="Wallach et al., 2009" startWordPosition="2643" endWordPosition="2646">d sample each in turn, conditioned on the current assignment to all other variables. Because these variables are continuous, we apply slice sampling (Neal, 2003). Slice sampling is a general technique for drawing samples from a distribution by sampling uniformly from the area under its density function. 3.3 Estimating Likelihood on Held-Out Data In Section 4.2 we evaluate our models by comparing their probability on held-out test conversations. As computing this probability exactly is intractable in our model, we employ a recently proposed Chibbstyle estimator (Murray and Salakhutdinov, 2008; Wallach et al., 2009). Chibb estimators estimate the probability of unseen data, P(w) by selecting a high probability assignment to hidden variables h∗, and taking advantage of the following equality which can be easily derived from the definition of conditional probability: P(w, h∗) P(w) = P(h∗|w) As the numerator can be computed exactly, this reduces the problem of estimating P(w) to the easier problem of estimating P(h∗|w). Murray and Salakhutdinov (2008) provide an unbiased estimator for P(h∗|w), which is calculated using the stationary distribution of the Gibbs sampler. 3.4 Bayesian Conversation model Given t</context>
<context position="29003" citStr="Wallach et al., 2009" startWordPosition="4836" endWordPosition="4839">e interpretable. Additionally we compare the likelihood of these models on held out test data in Figure 6. Note that the Bayesian methods produce models with much higher likelihood.10 For the EM models, likelihood tends to decrease on held out test data as we increase the number of hidden states, due to overfitting. 5 Conclusion We have presented an approach that allows the unsupervised induction of dialogue structure from naturally-occurring open-topic conversational data. 10Likelihood of the test data is estimated using the Chibb Style estimator described in (Murray and Salakhutdinov, 2008; Wallach et al., 2009). This method under-estimates likelihood in expectation. The maximum likelihood (EM) likelihoods are exact. # acts Figure 6: Negative log likelihood on held out test data (smaller values indicate higher likelihood). By visualizing the learned models, coherent patterns emerge from a stew of data that human readers find difficult to follow. We have extended a conversation sequence model to separate topic and dialogue words, resulting in an interpretable set of automatically generated dialogue acts. These discovered acts have interesting differences from those found in other domains, and reflect </context>
</contexts>
<marker>Wallach, Murray, Salakhutdinov, Mimno, 2009</marker>
<rawString>Hanna M. Wallach, Iain Murray, Ruslan Salakhutdinov, and David M. Mimno. 2009. Evaluation methods for topic models. In Proceedings of ICML, page 139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yorick Wilks</author>
</authors>
<title>Artificial companions as a new kind of interface to the future internet.</title>
<date>2006</date>
<journal>In OII Research Report</journal>
<volume>13</volume>
<contexts>
<context position="1493" citStr="Wilks, 2006" startWordPosition="221" endWordPosition="222">ion Twitter conversations, which will be made publicly available. This huge amount of data, available only because Twitter blurs the line between chatting and publishing, highlights the need to be able to adapt quickly to a new medium. 1 Introduction Automatic detection of dialogue structure is an important first step toward deep understanding of human conversations. Dialogue acts1 provide an initial level of structure by annotating utterances with shallow discourse roles such as “statement”, “question” and “answer”. These acts are useful in many applications, including conversational agents (Wilks, 2006), dialogue systems (Allen et al., 2007), dialogue summarization (Murray et al., 2006), and flirtation detection (Ranganath et al., 2009). Dialogue act tagging has traditionally followed an annotate-train-test paradigm, which begins with the *This work was conducted at Microsoft Research. 1Also called “speech acts” design of annotation guidelines, followed by the collection and labeling of corpora (Jurafsky et al., 1997; Dhillon et al., 2004). Only then can one train a tagger to automatically recognize dialogue acts (Stolcke et al., 2000). This paradigm has been quite successful, but the labeli</context>
</contexts>
<marker>Wilks, 2006</marker>
<rawString>Yorick Wilks. 2006. Artificial companions as a new kind of interface to the future internet. In OII Research Report No. 13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Woszczyna</author>
<author>A Waibel</author>
</authors>
<title>Inferring linguistic structure in spoken language.</title>
<date>1994</date>
<booktitle>In Proceedings of ICSLP.</booktitle>
<contexts>
<context position="3891" citStr="Woszczyna and Waibel (1994)" startWordPosition="598" endWordPosition="601">lution for new media, we propose a series of unsupervised conversation models, where the discovery of acts amounts to clustering utterances with similar conversational roles. This avoids manual construction of an act inventory, and allows the learning algorithm to tell us something about how people converse in a new medium. 172 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 172–180, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics There is surprisingly little work in unsupervised dialogue act tagging. Woszczyna and Waibel (1994) propose an unsupervised Hidden Markov Model (HMM) for dialogue structure in a meeting scheduling domain, but model dialogue state at the word level. Crook et al. (2009) use Dirichlet process mixture models to cluster utterances into a flexible number of acts in a travel-planning domain, but do not examine the sequential structure of dialogue.2 In contrast to previous work, we address the problem of discovering dialogue acts in an informal, open-topic domain, where an unsupervised learner may be distracted by strong topic clusters. We also train and test our models in a new medium: Twitter. Ra</context>
</contexts>
<marker>Woszczyna, Waibel, 1994</marker>
<rawString>M. Woszczyna and A. Waibel. 1994. Inferring linguistic structure in spoken language. In Proceedings of ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tae Yano</author>
<author>William W Cohen</author>
<author>Noah A Smith</author>
</authors>
<title>Predicting response to political blog posts with topic models.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>477--485</pages>
<contexts>
<context position="5208" citStr="Yano et al., 2009" startWordPosition="814" endWordPosition="817">nd a novel conversation ordering task, to ensure our models have the opportunity to discover dialogue phenomena unique to this medium. 2 Data To enable the study of large-data solutions to dialogue modeling, we have collected a corpus of 1.3 million conversations drawn from the microblogging service, Twitter. 3 To our knowledge, this is the largest corpus of naturally occurring chat data that has been available for study thus far. Similar datasets include the NUS SMS corpus (How and Kan, 2005), several IRC chat corpora (Elsner and Charniak, 2008; Forsyth and Martell, 2007), and blog datasets (Yano et al., 2009; Gamon et al., 2008), which can display conversational structure in the blog comments. As it characterizes itself as a micro-blog, it should not be surprising that structurally, Twitter conversations lie somewhere between chat and blogs. Like blogs, conversations on Twitter occur in a public environment, where they can be collected for research purposes. However, Twitter posts are restricted to be no longer than 140 characters, which keeps interactions chat-like. Like e-mail and unlike IRC, Twitter conversations are carried out by replying to specific posts. The Twitter API provides a link fr</context>
</contexts>
<marker>Yano, Cohen, Smith, 2009</marker>
<rawString>Tae Yano, William W. Cohen, and Noah A. Smith. 2009. Predicting response to political blog posts with topic models. In Proceedings of NAACL, pages 477–485.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Limin Yao</author>
<author>David Mimno</author>
<author>Andrew McCallum</author>
</authors>
<title>Efficient methods for topic model inference on streaming document collections.</title>
<date>2009</date>
<booktitle>In Proceedings of KDD,</booktitle>
<pages>937--946</pages>
<contexts>
<context position="18618" citStr="Yao et al., 2009" startWordPosition="2993" endWordPosition="2996">visualization of the automatically discovered dialogue acts, in addition to measuring the ability of our models to predict post order in unseen conversations. Ideally we would evaluate performance using an end-use application such as a conversational agent; however as this is outside the scope of this paper, we leave such an evaluation to future work. For all experiments we train our models on a set of 10,000 randomly sampled conversations with conversation length in posts ranging from 3 to 6. Note that our implementations can likely scale to larger data by using techniques such as SparseLDA (Yao et al., 2009). We limit our vocabulary to the 5,000 most frequent words in the corpus. When using EM, we train for 100 iterations, evaluating performance on the test set at each iteration, and reporting the maximum. Smoothing parameters are set using grid search on a development set. When performing inference with Gibbs Sampling, we use 1,000 samples for burn-in and take 10 samples at a lag of 100. Although using multiple samples introduces the possibility of poor results due to “act drift”, we found this not to be a problem in practice; in fact, taking multiple samples substantially improved performance d</context>
</contexts>
<marker>Yao, Mimno, McCallum, 2009</marker>
<rawString>Limin Yao, David Mimno, and Andrew McCallum. 2009. Efficient methods for topic model inference on streaming document collections. In Proceedings of KDD, pages 937–946.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>