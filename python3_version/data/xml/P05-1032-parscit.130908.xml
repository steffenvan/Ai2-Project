<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9982355">
Scaling Phrase-Based Statistical Machine Translation
to Larger Corpora and Longer Phrases
</title>
<author confidence="0.995271">
Chris Callison-Burch Colin Bannard
</author>
<affiliation confidence="0.99906">
University of Edinburgh
</affiliation>
<address confidence="0.7962355">
2 Buccleuch Place
Edinburgh EH8 9LW
</address>
<email confidence="0.999285">
{chris,colin}@linearb.co.uk
</email>
<author confidence="0.655675">
Josh Schroeder
</author>
<affiliation confidence="0.625638">
Linear B Ltd.
</affiliation>
<address confidence="0.870357">
39 B Cumberland Street
Edinburgh EH3 6RA
</address>
<email confidence="0.997748">
josh@linearb.co.uk
</email>
<sectionHeader confidence="0.997377" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999532538461539">
In this paper we describe a novel data
structure for phrase-based statistical ma-
chine translation which allows for the re-
trieval of arbitrarily long phrases while si-
multaneously using less memory than is
required by current decoder implementa-
tions. We detail the computational com-
plexity and average retrieval times for
looking up phrase translations in our suf-
fix array-based data structure. We show
how sampling can be used to reduce the
retrieval time by orders of magnitude with
no loss in translation quality.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999027085714286">
Statistical machine translation (SMT) has an advan-
tage over many other statistical natural language
processing applications in that training data is reg-
ularly produced by other human activity. For some
language pairs very large sets of training data are
now available. The publications of the European
Union and United Nations provide gigbytes of data
between various language pairs which can be eas-
ily mined using a web crawler. The Linguistics
Data Consortium provides an excellent set of off
the shelf Arabic-English and Chinese-English paral-
lel corpora for the annual NIST machine translation
evaluation exercises.
The size of the NIST training data presents a prob-
lem for phrase-based statistical machine translation.
Decoders such as Pharaoh (Koehn, 2004) primarily
use lookup tables for the storage of phrases and their
translations. Since retrieving longer segments of hu-
man translated text generally leads to better trans-
lation quality, participants in the evaluation exer-
cise try to maximize the length of phrases that are
stored in lookup tables. The combination of large
corpora and long phrases means that the table size
can quickly become unwieldy.
A number of groups in the 2004 evaluation exer-
cise indicated problems dealing with the data. Cop-
ing strategies included limiting the length of phrases
to something small, not using the entire training data
set, computing phrases probabilities on disk, and fil-
tering the phrase table down to a manageable size
after the testing set was distributed. We present a
data structure that is easily capable of handling the
largest data sets currently available, and show that it
can be scaled to much larger data sets.
In this paper we:
</bodyText>
<listItem confidence="0.954550923076923">
• Motivate the problem with storing enumerated
phrases in a table by examining the memory re-
quirements of the method for the NIST data set
• Detail the advantages of using long phrases in
SMT, and examine their potential coverage
• Describe a suffix array-based data structure
which allows for the retrieval of translations
of arbitrarily long phrases, and show that it re-
quires far less memory than a table
• Calculate the computational complexity and
average time for retrieving phrases and show
how this can be sped up by orders of magnitude
with no loss in translation accuracy
</listItem>
<sectionHeader confidence="0.999785" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9910215">
Koehn et al. (2003) compare a number of differ-
ent approaches to phrase-based statistical machine
</bodyText>
<page confidence="0.974931">
255
</page>
<note confidence="0.993069">
Proceedings of the 43rd Annual Meeting of the ACL, pages 255–262,
Ann Arbor, June 2005. c�2005 Association for Computational Linguistics
</note>
<table confidence="0.998417833333333">
length num uniq average # avg trans
(mil) translations length
1 .88 8.322 1.37
2 16.5 1.733 2.35
3 42.6 1.182 3.44
4 58.7 1.065 4.58
5 65.0 1.035 5.75
6 66.4 1.022 6.91
7 65.8 1.015 8.07
8 64.3 1.012 9.23
9 62.2 1.010 10.4
10 59.9 1.010 11.6
</table>
<tableCaption confidence="0.968994">
Table 1: Statistics about Arabic phrases in the NIST-
2004 large data track.
</tableCaption>
<table confidence="0.998993416666667">
length entries words memory including
(mil) (mil) (gigs) alignments
1 7.3 10 .1 .11
2 36 111 .68 .82
3 86 412 2.18 2.64
4 149 933 4.59 5.59
5 216 1,645 7.74 9.46
6 284 2,513 11.48 14.07
7 351 3,513 15.70 19.30
8 416 4,628 20.34 25.05
9 479 5,841 25.33 31.26
10 539 7,140 30.62 37.85
</table>
<tableCaption confidence="0.959957">
Table 2: Estimated size of lookup tables for the
NIST-2004 Arabic-English data
</tableCaption>
<bodyText confidence="0.999792">
translation including the joint probability phrase-
based model (Marcu and Wong, 2002) and a vari-
ant on the alignment template approach (Och and
Ney, 2004), and contrast them to the performance of
the word-based IBM Model 4 (Brown et al., 1993).
Most relevant for the work presented in this paper,
they compare the effect on translation quality of us-
ing various lengths of phrases, and the size of the
resulting phrase probability tables.
Tillmann (2003) further examines the relationship
between maximum phrase length, size of the trans-
lation table, and accuracy of translation when in-
ducing block-based phrases from word-level align-
ments. Venugopal et al. (2003) and Vogel et al.
(2003) present methods for achieving better transla-
tion quality by growing incrementally larger phrases
by combining smaller phrases with overlapping seg-
ments.
</bodyText>
<sectionHeader confidence="0.855433" genericHeader="method">
3 Scaling to Long Phrases
</sectionHeader>
<bodyText confidence="0.721370090909091">
Table 1 gives statistics about the Arabic-English par-
allel corpus used in the NIST large data track. The
corpus contains 3.75 million sentence pairs, and has
127 million words in English, and 106 million words
in Arabic. The table shows the number of unique
Arabic phrases, and gives the average number of
translations into English and their average length.
Table 2 gives estimates of the size of the lookup
tables needed to store phrases of various lengths,
based on the statistics in Table 1. The number of
unique entries is calculated as the number unique
</bodyText>
<table confidence="0.9992925">
length coverage length coverage
1 93.5% 6 4.70%
2 73.3% 7 2.95%
3 37.1% 8 2.14%
4 15.5% 9 1.99%
5 8.05% 10 1.49%
</table>
<tableCaption confidence="0.811243">
Table 3: Lengths of phrases from the training data
that occur in the NIST-2004 test set
</tableCaption>
<bodyText confidence="0.99997305">
phrases times the average number of translations.
The number of words in the table is calculated as the
number of unique phrases times the phrase length
plus the number of entries times the average transla-
tion length. The memory is calculated assuming that
each word is represented with a 4 byte integer, that
each entry stores its probability as an 8 byte double
and that each word alignment is stored as a 2 byte
short. Note that the size of the table will vary de-
pending on the phrase extraction technique.
Table 3 gives the percent of the 35,313 word long
test set which can be covered using only phrases of
the specified length or greater. The table shows the
efficacy of using phrases of different lengths. The ta-
ble shows that while the rate of falloff is rapid, there
are still multiple matches of phrases of length 10.
The longest matching phrase was one of length 18.
There is little generalization in current SMT imple-
mentations, and consequently longer phrases gener-
ally lead to better translation quality.
</bodyText>
<page confidence="0.995725">
256
</page>
<subsectionHeader confidence="0.99755">
3.1 Why use phrases?
</subsectionHeader>
<bodyText confidence="0.999886125">
Statistical machine translation made considerable
advances in translation quality with the introduction
of phrase-based translation. By increasing the size
of the basic unit of translation, phrase-based ma-
chine translation does away with many of the prob-
lems associated with the original word-based for-
mulation of statistical machine translation (Brown
et al., 1993), in particular:
</bodyText>
<listItem confidence="0.908698222222222">
• The Brown et al. (1993) formulation doesn’t
have a direct way of translating phrases; instead
they specify a fertility parameter which is used
to replicate words and translate them individu-
ally.
• With units as small as words, a lot of reordering
has to happen between languages with different
word orders. But the distortion parameter is a
poor explanation of word order.
</listItem>
<bodyText confidence="0.999513826086956">
Phrase-based SMT overcomes the first of these
problems by eliminating the fertility parameter
and directly handling word-to-phrase and phrase-to-
phrase mappings. The second problem is alleviated
through the use of multi-word units which reduce
the dependency on the distortion parameter. Less
word re-ordering need occur since local dependen-
cies are frequently captured. For example, common
adjective-noun alternations are memorized. How-
ever, since this linguistic information is not encoded
in the model, unseen adjective noun pairs may still
be handled incorrectly.
By increasing the length of phrases beyond a
few words, we might hope to capture additional
non-local linguistic phenomena. For example, by
memorizing longer phrases we may correctly learn
case information for nouns commonly selected by
frequently occurring verbs; we may properly han-
dle discontinuous phrases (such as French negation,
some German verb forms, and English verb particle
constructions) that are neglected by current phrase-
based models; and we may by chance capture some
agreement information in coordinated structures.
</bodyText>
<subsectionHeader confidence="0.999018">
3.2 Deciding what length of phrase to store
</subsectionHeader>
<bodyText confidence="0.9990695">
Despite the potential gains from memorizing longer
phrases, the fact remains that as phrases get longer
</bodyText>
<table confidence="0.998879333333333">
length coverage length coverage
1 96.3% 6 21.9%
2 94.9% 7 11.2%
3 86.1% 8 6.16%
4 65.6% 9 3.95%
5 40.9% 10 2.90%
</table>
<tableCaption confidence="0.7804405">
Table 4: Coverage using only repeated phrases of
the specified length
</tableCaption>
<bodyText confidence="0.999426454545455">
there is a decreasing likelihood that they will be re-
peated. Because of the amount of memory required
to store a phrase table, in current implementations a
choice is made as to the maximum length of phrase
to store.
Based on their analysis of the relationship be-
tween translation quality and phrase length, Koehn
et al. (2003) suggest limiting phrase length to three
words or less. This is entirely a practical sugges-
tion for keeping the phrase table to a reasonable
size, since they measure minor but incremental im-
provement in translation quality up to their maxi-
mum tested phrase length of seven words.1
Table 4 gives statistics about phrases which oc-
cur more than once in the English section of the Eu-
roparl corpus (Koehn, 2002) which was used in the
Koehn et al. (2003) experiments. It shows that the
percentage of words in the corpus that can be cov-
ered by repeated phrases falls off rapidly at length
6, but that even phrases up to length 10 are able to
cover a non-trivial portion of the corpus. This draws
into question the desirability of limiting phrase re-
trieval to length three.
The decision concerning what length of phrases
to store in the phrase table seems to boil down to
a practical consideration: one must weigh the like-
lihood of retrieval against the memory needed to
store longer phrases. We present a data structure
where this is not a consideration. Our suffix array-
based data structure allows the retrieval of arbitrar-
ily long phrases, while simultaneously requiring far
less memory than the standard table-based represen-
tation.
</bodyText>
<footnote confidence="0.9177404">
1While the improvements to translation quality reported in
Koehn et al. (2003) are minor, their evaluation metric may not
have been especially sensitive to adding longer phrases. They
used the Bleu evaluation metric (Papineni et al., 2002), but
capped the n-gram precision at 4-grams.
</footnote>
<page confidence="0.991649">
257
</page>
<figureCaption confidence="0.995968">
Figure 1: An initialized, unsorted suffix array for a
very small corpus
</figureCaption>
<sectionHeader confidence="0.988475" genericHeader="method">
4 Suffix Arrays
</sectionHeader>
<bodyText confidence="0.9999541">
The suffix array data structure (Manber and Myers,
1990) was introduced as a space-economical way of
creating an index for string searches. The suffix ar-
ray data structure makes it convenient to compute
the frequency and location of any substring or n-
gram in a large corpus. Abstractly, a suffix array is
an alphabetically-sorted list of all suffixes in a cor-
pus, where a suffix is a substring running from each
position in the text to the end. However, rather than
actually storing all suffixes, a suffix array can be
constructed by creating a list of references to each
of the suffixes in a corpus. Figure 1 shows how a
suffix array is initialized for a corpus with one sen-
tence. Each index of a word in the corpus has a cor-
responding place in the suffix array, which is identi-
cal in length to the corpus. Figure 2 shows the final
state of the suffix array, which is as a list of the in-
dices of words in the corpus that corresponds to an
alphabetically sorted list of the suffixes.
The advantages of this representation are that it is
compact and easily searchable. The total size of the
suffix array is a constant amount of memory. Typ-
ically it is stored as an array of integers where the
array is the same length as the corpus. Because it is
organized alphabetically, any phrase can be quickly
located within it using a binary search algorithm.
Yamamoto and Church (2001) show how to use
suffix arrays to calculate a number of statistics that
are interesting in natural language processing appli-
cations. They demonstrate how to calculate term fre-
</bodyText>
<figureCaption confidence="0.9854005">
Figure 2: A sorted suffix array and its corresponding
suffixes
</figureCaption>
<bodyText confidence="0.999733285714286">
quency / inverse document frequency (tf/ idf) for all
n-grams in very large corpora, as well as how to use
these frequencies to calculate n-grams with high mu-
tual information and residual inverse document fre-
quency. Here we show how to apply suffix arrays to
parallel corpora to calculate phrase translation prob-
abilities.
</bodyText>
<subsectionHeader confidence="0.990632">
4.1 Applied to parallel corpora
</subsectionHeader>
<bodyText confidence="0.998848">
In order to adapt suffix arrays to be useful for sta-
tistical machine translation we need a data structure
with the following elements:
</bodyText>
<listItem confidence="0.9843565">
• A suffix array created from the source language
portion of the corpus, and another created from
the target language portion of the corpus,
• An index that tells us the correspondence be-
tween sentence numbers and positions in the
source and target language corpora,
• An alignment a for each sentence pair in the
parallel corpus, where a is defined as a subset
of the Cartesian product of the word positions
in a sentence a of length I and a sentence f of
length J:
a C {(i,j) : i = 1...I;j = 1...J}
• A method for extracting the translationally
equivalent phrase for a subphrase given an
aligned sentence pair containing that sub-
phrase.
</listItem>
<bodyText confidence="0.988394">
The total memory usage of the data structure is
thus the size of the source and target corpora, plus
the size of the suffix arrays (identical in length to the
</bodyText>
<figure confidence="0.993896827586207">
Index of
words:
Corpus
0 1 2 3 4 5 6 7 8 9
Initialized, unsorted
Suffixes denoted by s[i]
Suffix Array
to
to confirm
morocco
spain declined
declined
spain
that
aid
0
s[0]
1
2
3
4
5
6
7
aid morocco
8
9
morocco
to confirm that spain declined to aid morocco
</figure>
<bodyText confidence="0.710752588235294">
that spain declined to aid morocco
spain declined to aid morocco
declined to aid morocco
to aid morocco
confirm that spain declined to aid morocco
declined to confirm that spain declined to aid morocco
spain declined to confirm that spain declined to aid morocco
Sorted
Suffix Array Suffixes denoted by s[i]
confirm that spain declined to aid morocco
declined to aid morocco
declined to confirm that spain declined to aid morocco
aid morocco
morocco
spain declined to aid morocco
spain declined to confirm that spain declined to aid morocco
that spain declined to aid morocco
</bodyText>
<figure confidence="0.954549083333333">
to aid morocco
to confirm that spain declined to aid morocco
s[0]
8
3
6
1
9
5
0
4
7
</figure>
<page confidence="0.828403">
2
258
</page>
<bodyText confidence="0.877279083333333">
corpora), plus the size of the two indexes that cor-
relate sentence positions with word positions, plus
the size of the alignments. Assuming we use ints
to represent words and indices, and shorts to repre-
sent word alignments, we get the following memory
usage:
2 * num words in source corpus * sizeof(int)+
2 * num words in target corpus * sizeof(int)+
2 * number sentence pairs * sizeof(int)+
number of word alignments * sizeof(short)
The total amount of memory required to store the
NIST Arabic-English data using this data structure
</bodyText>
<equation confidence="0.8841802">
is
2 * 105,994,774 * sizeof(int)+
2 * 127,450,473 * sizeof(int)+
2 * 3,758,904 * sizeof(int)+
92,975,229 * sizeof(short)
</equation>
<bodyText confidence="0.725656">
Or just over 2 Gigabytes.
</bodyText>
<subsectionHeader confidence="0.8376435">
4.2 Calculating phrase translation
probabilities
</subsectionHeader>
<bodyText confidence="0.9999815">
In order to produce a set of phrase translation prob-
abilities, we need to examine the ways in which
they are calculated. We consider two common ways
of calculating the translation probability: using the
maximum likelihood estimator (MLE) and smooth-
ing the MLE using lexical weighting.
The maximum likelihood estimator for the proba-
bility of a phrase is defined as
</bodyText>
<equation confidence="0.997533333333333">
p(
�f|�e) =count( f, 6)
E f count(�f, e) (1)
</equation>
<bodyText confidence="0.999696833333333">
Where count( f, e) gives the total number of times
the phrase f was aligned with the phrase e� in the
parallel corpus. We define phrase alignments as fol-
lows. A substring e� consisting of the words at po-
sitions l...m is aligned with the phrase f by way of
the subalignment
</bodyText>
<equation confidence="0.56954">
s = a ft {(i,j) : i = l...m,j = 1...J}
</equation>
<bodyText confidence="0.9998696">
The aligned phrase f is the subphrase in f which
spans from min(j) to max(j) for j|(i, j) E s.
The procedure for generating the counts that are
used to calculate the MLE probability using our suf-
fix array-based data structures is:
</bodyText>
<listItem confidence="0.9380635">
1. Locate all the suffixes in the English suffix ar-
ray which begin with the phrase e. Since the
suffix array is sorted alphabetically we can eas-
ily find the first occurrence s[k] and the last oc-
currence s[l]. The length of the span in the suf-
fix array l−k+1 indicates the number of occur-
rences of e� in the corpus. Thus the denominator
E f count(�f, e) can be calculated as l − k + 1.
2. For each of the matching phrases s[i] in the
span s[k]...s[l], look up the value of s[i] which
is the word index w of the suffix in the English
corpus. Look up the sentence number that in-
cludes w, and retrieve the corresponding sen-
tences a and f, and their alignment a.
3. Use a to extract the target phrase f that aligns
with the phrase e� that we are searching for. In-
crement the count for &lt; f, e�&gt;.
4. Calculate the probability for each unique
matching phrase f using the formula in Equa-
tion 1.
</listItem>
<bodyText confidence="0.999460066666667">
A common alternative formulation of the phrase
translation probability is to lexically weight it as fol-
lows:
Where n is the length of e.
In order to use lexical weighting we would need
to repeat steps 1-4 above for each word ei in e. This
would give us the values for p(fj|ei). We would fur-
ther need to retain the subphrase alignment s in or-
der to know the correspondence between the words
(i, j) E s in the aligned phrases, and the total num-
ber of foreign words that each ei is aligned with
(|{i|(i, j) E s}|). Since a phrase alignment &lt; 1, e� &gt;
may have multiple possible word-level alignments,
we retain a set of alignments 5 and take the maxi-
mum:
</bodyText>
<equation confidence="0.849741777777778">
n
i��
plXf|6, s) =
1
{i|(i, j) E s} |E p(fj|ei)
b(i,j)Es
(2)
259
p( f|�e, S) = p( f|e) * arg S ax pl�( f|�e, s) (3)
</equation>
<bodyText confidence="0.999902">
Thus our suffix array-based data structure can be
used straightforwardly to look up all aligned trans-
lations for a given phrase and calculate the proba-
bilities on-the-fly. In the next section we turn to
the computational complexity of constructing phrase
translation probabilities in this way.
</bodyText>
<sectionHeader confidence="0.96495" genericHeader="method">
5 Computational Complexity
</sectionHeader>
<bodyText confidence="0.999604">
Computational complexity is relevant because there
is a speed-memory tradeoff when adopting our data
structure. What we gained in memory efficiency
may be rendered useless if the time it takes to cal-
culate phrase translation probabilities is unreason-
ably long. The computational complexity of looking
up items in a hash table, as is done in current table-
based data structures, is extremely fast. Looking up
a single phrase can be done in unit time, O(1).
The computational complexity of our method has
the following components:
</bodyText>
<listItem confidence="0.991364">
• The complexity of finding all occurrences of
the phrase in the suffix array
• The complexity of retrieving the associated
aligned sentence pairs given the positions of the
phrase in the corpus
</listItem>
<table confidence="0.964991571428572">
phrase freq O time (ms)
respect for the 3 80 24
dead
since the end of 19 240 136
the cold war
the parliament 1291 4391 1117
of the 290921 682550 218369
</table>
<tableCaption confidence="0.9164685">
Table 5: Examples of O and calculation times for
phrases of different frequencies
</tableCaption>
<bodyText confidence="0.8554242">
that are associated with the position wi in the
corpus of each phrase occurrence ei. The com-
plexity is O(k * 2log(m)) where k is the num-
ber of occurrences of e� and m is the number of
sentence pairs in the parallel corpus.
</bodyText>
<listItem confidence="0.981710222222222">
• The complexity of extracting the aligned phrase
for a single occurrence of ei is O(2 log(|ai|) to
get the subphrase alignment si, since we store
the alignments in a sorted array. The complex-
ity of then getting fi from si is O(length(
fi)).
• The complexity of summing over all aligned
phrases and simultaneously calculating their
probabilities is O(k).
</listItem>
<bodyText confidence="0.951906">
Thus we have a total complexity of:
</bodyText>
<equation confidence="0.992254">
O(2log(n) + k * 2log(m) (4)
</equation>
<bodyText confidence="0.7579415">
el... e�k
• The complexity of extracting all aligned + (2log(|ai|) + length( fi)) + k) (5)
phrases using our phrase extraction algorithm
air fi|�ei
</bodyText>
<listItem confidence="0.7437275">
• The complexity of calculating the probabilities
given the aligned phrases
</listItem>
<bodyText confidence="0.945811038461539">
The methods we use to execute each of these, and
their complexities are as follow:
• Since the array is sorted, finding all occur-
rences of the English phrase is extremely fast.
We can do two binary searches: one to find the
first occurrence of the phrase and a second to
find the last. The computational complexity is
therefore bounded by O(2 log(n)) where n is
the length of the corpus.
• We use a similar method to look up the sen-
tences ei and fi and word-level alignment ai
for the MLE estimation of the translation probabil-
ities for a single phrase. The complexity is domi-
nated by the k terms in the equation, when the num-
ber of occurrences of the phrase in the corpus is
high. Phrases with high frequency may cause exces-
sively long retrieval time. This problem is exacer-
bated when we shift to a lexically weighted calcula-
tion of the phrase translation probability. The com-
plexity will be multiplied across each of the compo-
nent words in the phrase, and the component words
themselves will be more frequent than the phrase.
Table 5 shows example times for calculating the
translation probabilities for a number of phrases. For
frequent phrases like of the these times get unaccept-
ably long. While our data structure is perfect for
</bodyText>
<page confidence="0.977122">
260
</page>
<bodyText confidence="0.999901125">
overcoming the problems associated with storing the
translations of long, infrequently occurring phrases,
it in a way introduces the converse problem. It has
a clear disadvantage in the amount of time it takes
to retrieve commonly occurring phrases. In the next
section we examine the use of sampling to speed up
the calculation of translation probabilities for very
frequent phrases.
</bodyText>
<sectionHeader confidence="0.996174" genericHeader="method">
6 Sampling
</sectionHeader>
<bodyText confidence="0.999973653846154">
Rather than compute the phrase translation proba-
bilities by examining the hundreds of thousands of
occurrences of common phrases, we instead sam-
ple from a small subset of the occurrences. It is
unlikely that we need to extract the translations of
all occurrences of a high frequency phrase in order
to get a good approximation of their probabilities.
We instead cap the number of occurrences that we
consider, and thus give a maximum bound on k in
Equation 5.
In order to determine the effect of different lev-
els of sampling, we compare the translation quality
against cumulative retrieval time for calculating the
phrase translation probabilities for all subphrases in
an evaluation set. We translated a held out set of
430 German sentences with 50 words or less into
English. The test sentences were drawn from the
01/17/00 proceedings of the Europarl corpus. The
remainder of the corpus (1 million sentences) was
used as training data to calculate the phrase trans-
lation probabilities. We calculated the translation
quality using Bleu’s modified n-gram precision met-
ric (Papineni et al., 2002) for n-grams of up to length
four. The framework that we used to calculate the
translation probabilities was similar to that detailed
in Koehn et al. (2003). That is:
</bodyText>
<equation confidence="0.981635">
I
p(fi |&amp;i)d(ai − bi−1)plw( fi |&amp;i, a) (8)
i=1
</equation>
<bodyText confidence="0.801847">
Where pLM is a language model probability and d is
a distortion probability which penalizes movement.
Table 6 gives a comparison of the translation qual-
ity under different levels of sampling. While the ac-
</bodyText>
<table confidence="0.805286">
sample size time quality
unlimited 6279 sec .290
50000 1051 sec .289
10000 336 sec .291
5000 201 sec .289
1000 60 sec .288
500 35 sec .288
100 10 sec .288
</table>
<tableCaption confidence="0.734549571428571">
Table 6: A comparison of retrieval times and trans-
lation quality when the number of translations is
capped at various sample sizes
curacy fluctuates very slightly it essentially remains
uniformly high for all levels of sampling. There are
a number of possible reasons for the fact that the
quality does not decrease:
</tableCaption>
<listItem confidence="0.949778">
• The probability estimates under sampling are
sufficiently good that the most probable trans-
lations remain unchanged,
• The interaction with the language model prob-
</listItem>
<bodyText confidence="0.8549689375">
ability rules out the few misestimated probabil-
ities, or
• The decoder tends to select longer or less fre-
quent phrases which are not affected by the
sampling.
While the translation quality remains essentially
unchanged, the cumulative time that it takes to cal-
culate the translation probabilities for all subphrases
in the 430 sentence test set decreases radically. The
total time drops by orders of magnitude from an hour
and a half without sampling down to a mere 10 sec-
onds with a cavalier amount of sampling. This sug-
gests that the data structure is suitable for deployed
SMT systems and that no additional caching need
be done to compensate for the structure’s computa-
tional complexity.
</bodyText>
<sectionHeader confidence="0.999869" genericHeader="conclusions">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999871166666667">
The paper has presented a super-efficient data struc-
ture for phrase-based statistical machine translation.
We have shown that current table-based methods are
unwieldily when used in conjunction with large data
sets and long phrases. We have contrasted this with
our suffix array-based data structure which provides
</bodyText>
<equation confidence="0.882727714285714">
e� = arg maxp(eI1|fI1) (6)
el
1
= arg max
el
I
pLM(eI1) * (7)
</equation>
<page confidence="0.99116">
261
</page>
<bodyText confidence="0.999905074074074">
a very compact way of storing large data sets while
simultaneously allowing the retrieval of arbitrarily
long phrases.
For the NIST-2004 Arabic-English data set,
which is among the largest currently assembled for
statistical machine translation, our representation
uses a very manageable 2 gigabytes of memory. This
is less than is needed to store a table containing
phrases with a maximum of three words, and is ten
times less than the memory required to store a table
with phrases of length eight.
We have further demonstrated that while compu-
tational complexity can make the retrieval of trans-
lation of frequent phrases slow, the use of sampling
is an extremely effective countermeasure to this.
We demonstrated that calculating phrase translation
probabilities from sets of 100 occurrences or less re-
sults in nearly no decrease in translation quality.
The implications of the data structure presented
in this paper are significant. The compact rep-
resentation will allow us to easily scale to paral-
lel corpora consisting of billions of words of text,
and the retrieval of arbitrarily long phrases will al-
low experiments with alternative decoding strate-
gies. These facts in combination allow for an even
greater exploitation of training data in statistical ma-
chine translation.
</bodyText>
<sectionHeader confidence="0.999553" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999807">
Peter Brown, Stephen Della Pietra, Vincent Della Pietra,
and Robert Mercer. 1993. The mathematics of ma-
chine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263–311, June.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings ofHLT/NAACL.
Philipp Koehn. 2002. Europarl: A multilingual corpus
for evaluation of machine translation. Unpublished
Draft.
Philipp Koehn. 2004. Pharaoh: A beam search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings ofAMTA.
Udi Manber and Gene Myers. 1990. Suffix arrays:
A new method for on-line string searches. In The
First Annual ACM-SIAM Symposium on Dicrete Algo-
rithms, pages 319–327.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In Proceedings ofEMNLP.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417–450, De-
cember.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic evalu-
ation of machine translation. In Proceedings ofACL.
Christoph Tillmann. 2003. A projection extension algo-
rithm for statistical machine translation. In Proceed-
ings of EMNLP.
Ashish Venugopal, Stephan Vogel, and Alex Waibel.
2003. Effective phrase translation extraction from
alignment models. In Proceedings ofACL.
Stephan Vogel, Ying Zhang, Fei Huang, Alicia Trib-
ble, Ashish Venugopal, Bing Zhao, and Alex Waibel.
2003. The CMU statistical machine translation sys-
tem. In Proceedings ofMT Summit 9.
Mikio Yamamoto and Kenneth Church. 2001. Using suf-
fix arrays to compute term frequency and document
frequency for all substrings in a corpus. Compuata-
tional Linguistics, 27(1):1–30.
</reference>
<page confidence="0.997291">
262
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.600064">
<title confidence="0.993483">Scaling Phrase-Based Statistical Machine Translation to Larger Corpora and Longer Phrases</title>
<author confidence="0.999715">Chris Callison-Burch Colin Bannard</author>
<affiliation confidence="0.8941375">University of Edinburgh 2 Buccleuch Place</affiliation>
<address confidence="0.849297">Edinburgh EH8 9LW</address>
<author confidence="0.976082">Josh Schroeder</author>
<affiliation confidence="0.990371">Linear B Ltd.</affiliation>
<address confidence="0.9481025">39 B Cumberland Street Edinburgh EH3 6RA</address>
<email confidence="0.9968">josh@linearb.co.uk</email>
<abstract confidence="0.999636357142857">In this paper we describe a novel data structure for phrase-based statistical machine translation which allows for the retrieval of arbitrarily long phrases while simultaneously using less memory than is required by current decoder implementations. We detail the computational complexity and average retrieval times for looking up phrase translations in our suffix array-based data structure. We show how sampling can be used to reduce the retrieval time by orders of magnitude with no loss in translation quality.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter Brown</author>
<author>Stephen Della Pietra</author>
<author>Vincent Della Pietra</author>
<author>Robert Mercer</author>
</authors>
<title>The mathematics of machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="4289" citStr="Brown et al., 1993" startWordPosition="703" endWordPosition="706">ge data track. length entries words memory including (mil) (mil) (gigs) alignments 1 7.3 10 .1 .11 2 36 111 .68 .82 3 86 412 2.18 2.64 4 149 933 4.59 5.59 5 216 1,645 7.74 9.46 6 284 2,513 11.48 14.07 7 351 3,513 15.70 19.30 8 416 4,628 20.34 25.05 9 479 5,841 25.33 31.26 10 539 7,140 30.62 37.85 Table 2: Estimated size of lookup tables for the NIST-2004 Arabic-English data translation including the joint probability phrasebased model (Marcu and Wong, 2002) and a variant on the alignment template approach (Och and Ney, 2004), and contrast them to the performance of the word-based IBM Model 4 (Brown et al., 1993). Most relevant for the work presented in this paper, they compare the effect on translation quality of using various lengths of phrases, and the size of the resulting phrase probability tables. Tillmann (2003) further examines the relationship between maximum phrase length, size of the translation table, and accuracy of translation when inducing block-based phrases from word-level alignments. Venugopal et al. (2003) and Vogel et al. (2003) present methods for achieving better translation quality by growing incrementally larger phrases by combining smaller phrases with overlapping segments. 3 </context>
<context position="7083" citStr="Brown et al., 1993" startWordPosition="1169" endWordPosition="1172">ple matches of phrases of length 10. The longest matching phrase was one of length 18. There is little generalization in current SMT implementations, and consequently longer phrases generally lead to better translation quality. 256 3.1 Why use phrases? Statistical machine translation made considerable advances in translation quality with the introduction of phrase-based translation. By increasing the size of the basic unit of translation, phrase-based machine translation does away with many of the problems associated with the original word-based formulation of statistical machine translation (Brown et al., 1993), in particular: • The Brown et al. (1993) formulation doesn’t have a direct way of translating phrases; instead they specify a fertility parameter which is used to replicate words and translate them individually. • With units as small as words, a lot of reordering has to happen between languages with different word orders. But the distortion parameter is a poor explanation of word order. Phrase-based SMT overcomes the first of these problems by eliminating the fertility parameter and directly handling word-to-phrase and phrase-tophrase mappings. The second problem is alleviated through the us</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter Brown, Stephen Della Pietra, Vincent Della Pietra, and Robert Mercer. 1993. The mathematics of machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings ofHLT/NAACL.</booktitle>
<contexts>
<context position="3149" citStr="Koehn et al. (2003)" startWordPosition="496" endWordPosition="499">e: • Motivate the problem with storing enumerated phrases in a table by examining the memory requirements of the method for the NIST data set • Detail the advantages of using long phrases in SMT, and examine their potential coverage • Describe a suffix array-based data structure which allows for the retrieval of translations of arbitrarily long phrases, and show that it requires far less memory than a table • Calculate the computational complexity and average time for retrieving phrases and show how this can be sped up by orders of magnitude with no loss in translation accuracy 2 Related Work Koehn et al. (2003) compare a number of different approaches to phrase-based statistical machine 255 Proceedings of the 43rd Annual Meeting of the ACL, pages 255–262, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics length num uniq average # avg trans (mil) translations length 1 .88 8.322 1.37 2 16.5 1.733 2.35 3 42.6 1.182 3.44 4 58.7 1.065 4.58 5 65.0 1.035 5.75 6 66.4 1.022 6.91 7 65.8 1.015 8.07 8 64.3 1.012 9.23 9 62.2 1.010 10.4 10 59.9 1.010 11.6 Table 1: Statistics about Arabic phrases in the NIST2004 large data track. length entries words memory including (mil) (mil) (gigs) alignme</context>
<context position="9233" citStr="Koehn et al. (2003)" startWordPosition="1509" endWordPosition="1512">store Despite the potential gains from memorizing longer phrases, the fact remains that as phrases get longer length coverage length coverage 1 96.3% 6 21.9% 2 94.9% 7 11.2% 3 86.1% 8 6.16% 4 65.6% 9 3.95% 5 40.9% 10 2.90% Table 4: Coverage using only repeated phrases of the specified length there is a decreasing likelihood that they will be repeated. Because of the amount of memory required to store a phrase table, in current implementations a choice is made as to the maximum length of phrase to store. Based on their analysis of the relationship between translation quality and phrase length, Koehn et al. (2003) suggest limiting phrase length to three words or less. This is entirely a practical suggestion for keeping the phrase table to a reasonable size, since they measure minor but incremental improvement in translation quality up to their maximum tested phrase length of seven words.1 Table 4 gives statistics about phrases which occur more than once in the English section of the Europarl corpus (Koehn, 2002) which was used in the Koehn et al. (2003) experiments. It shows that the percentage of words in the corpus that can be covered by repeated phrases falls off rapidly at length 6, but that even p</context>
<context position="10539" citStr="Koehn et al. (2003)" startWordPosition="1731" endWordPosition="1734">s into question the desirability of limiting phrase retrieval to length three. The decision concerning what length of phrases to store in the phrase table seems to boil down to a practical consideration: one must weigh the likelihood of retrieval against the memory needed to store longer phrases. We present a data structure where this is not a consideration. Our suffix arraybased data structure allows the retrieval of arbitrarily long phrases, while simultaneously requiring far less memory than the standard table-based representation. 1While the improvements to translation quality reported in Koehn et al. (2003) are minor, their evaluation metric may not have been especially sensitive to adding longer phrases. They used the Bleu evaluation metric (Papineni et al., 2002), but capped the n-gram precision at 4-grams. 257 Figure 1: An initialized, unsorted suffix array for a very small corpus 4 Suffix Arrays The suffix array data structure (Manber and Myers, 1990) was introduced as a space-economical way of creating an index for string searches. The suffix array data structure makes it convenient to compute the frequency and location of any substring or ngram in a large corpus. Abstractly, a suffix array</context>
<context position="23027" citStr="Koehn et al. (2003)" startWordPosition="3943" endWordPosition="3946"> probabilities for all subphrases in an evaluation set. We translated a held out set of 430 German sentences with 50 words or less into English. The test sentences were drawn from the 01/17/00 proceedings of the Europarl corpus. The remainder of the corpus (1 million sentences) was used as training data to calculate the phrase translation probabilities. We calculated the translation quality using Bleu’s modified n-gram precision metric (Papineni et al., 2002) for n-grams of up to length four. The framework that we used to calculate the translation probabilities was similar to that detailed in Koehn et al. (2003). That is: I p(fi |&amp;i)d(ai − bi−1)plw( fi |&amp;i, a) (8) i=1 Where pLM is a language model probability and d is a distortion probability which penalizes movement. Table 6 gives a comparison of the translation quality under different levels of sampling. While the acsample size time quality unlimited 6279 sec .290 50000 1051 sec .289 10000 336 sec .291 5000 201 sec .289 1000 60 sec .288 500 35 sec .288 100 10 sec .288 Table 6: A comparison of retrieval times and translation quality when the number of translations is capped at various sample sizes curacy fluctuates very slightly it essentially remai</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings ofHLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A multilingual corpus for evaluation of machine translation.</title>
<date>2002</date>
<note>Unpublished Draft.</note>
<contexts>
<context position="9639" citStr="Koehn, 2002" startWordPosition="1581" endWordPosition="1582">table, in current implementations a choice is made as to the maximum length of phrase to store. Based on their analysis of the relationship between translation quality and phrase length, Koehn et al. (2003) suggest limiting phrase length to three words or less. This is entirely a practical suggestion for keeping the phrase table to a reasonable size, since they measure minor but incremental improvement in translation quality up to their maximum tested phrase length of seven words.1 Table 4 gives statistics about phrases which occur more than once in the English section of the Europarl corpus (Koehn, 2002) which was used in the Koehn et al. (2003) experiments. It shows that the percentage of words in the corpus that can be covered by repeated phrases falls off rapidly at length 6, but that even phrases up to length 10 are able to cover a non-trivial portion of the corpus. This draws into question the desirability of limiting phrase retrieval to length three. The decision concerning what length of phrases to store in the phrase table seems to boil down to a practical consideration: one must weigh the likelihood of retrieval against the memory needed to store longer phrases. We present a data str</context>
</contexts>
<marker>Koehn, 2002</marker>
<rawString>Philipp Koehn. 2002. Europarl: A multilingual corpus for evaluation of machine translation. Unpublished Draft.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: A beam search decoder for phrase-based statistical machine translation models.</title>
<date>2004</date>
<booktitle>In Proceedings ofAMTA.</booktitle>
<contexts>
<context position="1602" citStr="Koehn, 2004" startWordPosition="238" endWordPosition="239">regularly produced by other human activity. For some language pairs very large sets of training data are now available. The publications of the European Union and United Nations provide gigbytes of data between various language pairs which can be easily mined using a web crawler. The Linguistics Data Consortium provides an excellent set of off the shelf Arabic-English and Chinese-English parallel corpora for the annual NIST machine translation evaluation exercises. The size of the NIST training data presents a problem for phrase-based statistical machine translation. Decoders such as Pharaoh (Koehn, 2004) primarily use lookup tables for the storage of phrases and their translations. Since retrieving longer segments of human translated text generally leads to better translation quality, participants in the evaluation exercise try to maximize the length of phrases that are stored in lookup tables. The combination of large corpora and long phrases means that the table size can quickly become unwieldy. A number of groups in the 2004 evaluation exercise indicated problems dealing with the data. Coping strategies included limiting the length of phrases to something small, not using the entire traini</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Pharaoh: A beam search decoder for phrase-based statistical machine translation models. In Proceedings ofAMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Udi Manber</author>
<author>Gene Myers</author>
</authors>
<title>Suffix arrays: A new method for on-line string searches.</title>
<date>1990</date>
<booktitle>In The First Annual ACM-SIAM Symposium on Dicrete Algorithms,</booktitle>
<pages>319--327</pages>
<contexts>
<context position="10894" citStr="Manber and Myers, 1990" startWordPosition="1788" endWordPosition="1791">deration. Our suffix arraybased data structure allows the retrieval of arbitrarily long phrases, while simultaneously requiring far less memory than the standard table-based representation. 1While the improvements to translation quality reported in Koehn et al. (2003) are minor, their evaluation metric may not have been especially sensitive to adding longer phrases. They used the Bleu evaluation metric (Papineni et al., 2002), but capped the n-gram precision at 4-grams. 257 Figure 1: An initialized, unsorted suffix array for a very small corpus 4 Suffix Arrays The suffix array data structure (Manber and Myers, 1990) was introduced as a space-economical way of creating an index for string searches. The suffix array data structure makes it convenient to compute the frequency and location of any substring or ngram in a large corpus. Abstractly, a suffix array is an alphabetically-sorted list of all suffixes in a corpus, where a suffix is a substring running from each position in the text to the end. However, rather than actually storing all suffixes, a suffix array can be constructed by creating a list of references to each of the suffixes in a corpus. Figure 1 shows how a suffix array is initialized for a </context>
</contexts>
<marker>Manber, Myers, 1990</marker>
<rawString>Udi Manber and Gene Myers. 1990. Suffix arrays: A new method for on-line string searches. In The First Annual ACM-SIAM Symposium on Dicrete Algorithms, pages 319–327.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>William Wong</author>
</authors>
<title>A phrase-based, joint probability model for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings ofEMNLP.</booktitle>
<contexts>
<context position="4131" citStr="Marcu and Wong, 2002" startWordPosition="674" endWordPosition="677">035 5.75 6 66.4 1.022 6.91 7 65.8 1.015 8.07 8 64.3 1.012 9.23 9 62.2 1.010 10.4 10 59.9 1.010 11.6 Table 1: Statistics about Arabic phrases in the NIST2004 large data track. length entries words memory including (mil) (mil) (gigs) alignments 1 7.3 10 .1 .11 2 36 111 .68 .82 3 86 412 2.18 2.64 4 149 933 4.59 5.59 5 216 1,645 7.74 9.46 6 284 2,513 11.48 14.07 7 351 3,513 15.70 19.30 8 416 4,628 20.34 25.05 9 479 5,841 25.33 31.26 10 539 7,140 30.62 37.85 Table 2: Estimated size of lookup tables for the NIST-2004 Arabic-English data translation including the joint probability phrasebased model (Marcu and Wong, 2002) and a variant on the alignment template approach (Och and Ney, 2004), and contrast them to the performance of the word-based IBM Model 4 (Brown et al., 1993). Most relevant for the work presented in this paper, they compare the effect on translation quality of using various lengths of phrases, and the size of the resulting phrase probability tables. Tillmann (2003) further examines the relationship between maximum phrase length, size of the translation table, and accuracy of translation when inducing block-based phrases from word-level alignments. Venugopal et al. (2003) and Vogel et al. (200</context>
</contexts>
<marker>Marcu, Wong, 2002</marker>
<rawString>Daniel Marcu and William Wong. 2002. A phrase-based, joint probability model for statistical machine translation. In Proceedings ofEMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="4200" citStr="Och and Ney, 2004" startWordPosition="687" endWordPosition="690">010 10.4 10 59.9 1.010 11.6 Table 1: Statistics about Arabic phrases in the NIST2004 large data track. length entries words memory including (mil) (mil) (gigs) alignments 1 7.3 10 .1 .11 2 36 111 .68 .82 3 86 412 2.18 2.64 4 149 933 4.59 5.59 5 216 1,645 7.74 9.46 6 284 2,513 11.48 14.07 7 351 3,513 15.70 19.30 8 416 4,628 20.34 25.05 9 479 5,841 25.33 31.26 10 539 7,140 30.62 37.85 Table 2: Estimated size of lookup tables for the NIST-2004 Arabic-English data translation including the joint probability phrasebased model (Marcu and Wong, 2002) and a variant on the alignment template approach (Och and Ney, 2004), and contrast them to the performance of the word-based IBM Model 4 (Brown et al., 1993). Most relevant for the work presented in this paper, they compare the effect on translation quality of using various lengths of phrases, and the size of the resulting phrase probability tables. Tillmann (2003) further examines the relationship between maximum phrase length, size of the translation table, and accuracy of translation when inducing block-based phrases from word-level alignments. Venugopal et al. (2003) and Vogel et al. (2003) present methods for achieving better translation quality by growin</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Josef Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4):417–450, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="10700" citStr="Papineni et al., 2002" startWordPosition="1756" endWordPosition="1759">ms to boil down to a practical consideration: one must weigh the likelihood of retrieval against the memory needed to store longer phrases. We present a data structure where this is not a consideration. Our suffix arraybased data structure allows the retrieval of arbitrarily long phrases, while simultaneously requiring far less memory than the standard table-based representation. 1While the improvements to translation quality reported in Koehn et al. (2003) are minor, their evaluation metric may not have been especially sensitive to adding longer phrases. They used the Bleu evaluation metric (Papineni et al., 2002), but capped the n-gram precision at 4-grams. 257 Figure 1: An initialized, unsorted suffix array for a very small corpus 4 Suffix Arrays The suffix array data structure (Manber and Myers, 1990) was introduced as a space-economical way of creating an index for string searches. The suffix array data structure makes it convenient to compute the frequency and location of any substring or ngram in a large corpus. Abstractly, a suffix array is an alphabetically-sorted list of all suffixes in a corpus, where a suffix is a substring running from each position in the text to the end. However, rather t</context>
<context position="22871" citStr="Papineni et al., 2002" startWordPosition="3916" endWordPosition="3919">termine the effect of different levels of sampling, we compare the translation quality against cumulative retrieval time for calculating the phrase translation probabilities for all subphrases in an evaluation set. We translated a held out set of 430 German sentences with 50 words or less into English. The test sentences were drawn from the 01/17/00 proceedings of the Europarl corpus. The remainder of the corpus (1 million sentences) was used as training data to calculate the phrase translation probabilities. We calculated the translation quality using Bleu’s modified n-gram precision metric (Papineni et al., 2002) for n-grams of up to length four. The framework that we used to calculate the translation probabilities was similar to that detailed in Koehn et al. (2003). That is: I p(fi |&amp;i)d(ai − bi−1)plw( fi |&amp;i, a) (8) i=1 Where pLM is a language model probability and d is a distortion probability which penalizes movement. Table 6 gives a comparison of the translation quality under different levels of sampling. While the acsample size time quality unlimited 6279 sec .290 50000 1051 sec .289 10000 336 sec .291 5000 201 sec .289 1000 60 sec .288 500 35 sec .288 100 10 sec .288 Table 6: A comparison of re</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: A method for automatic evaluation of machine translation. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
</authors>
<title>A projection extension algorithm for statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="4499" citStr="Tillmann (2003)" startWordPosition="739" endWordPosition="740">5.70 19.30 8 416 4,628 20.34 25.05 9 479 5,841 25.33 31.26 10 539 7,140 30.62 37.85 Table 2: Estimated size of lookup tables for the NIST-2004 Arabic-English data translation including the joint probability phrasebased model (Marcu and Wong, 2002) and a variant on the alignment template approach (Och and Ney, 2004), and contrast them to the performance of the word-based IBM Model 4 (Brown et al., 1993). Most relevant for the work presented in this paper, they compare the effect on translation quality of using various lengths of phrases, and the size of the resulting phrase probability tables. Tillmann (2003) further examines the relationship between maximum phrase length, size of the translation table, and accuracy of translation when inducing block-based phrases from word-level alignments. Venugopal et al. (2003) and Vogel et al. (2003) present methods for achieving better translation quality by growing incrementally larger phrases by combining smaller phrases with overlapping segments. 3 Scaling to Long Phrases Table 1 gives statistics about the Arabic-English parallel corpus used in the NIST large data track. The corpus contains 3.75 million sentence pairs, and has 127 million words in English</context>
</contexts>
<marker>Tillmann, 2003</marker>
<rawString>Christoph Tillmann. 2003. A projection extension algorithm for statistical machine translation. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Venugopal</author>
<author>Stephan Vogel</author>
<author>Alex Waibel</author>
</authors>
<title>Effective phrase translation extraction from alignment models.</title>
<date>2003</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="4709" citStr="Venugopal et al. (2003)" startWordPosition="768" endWordPosition="771">lity phrasebased model (Marcu and Wong, 2002) and a variant on the alignment template approach (Och and Ney, 2004), and contrast them to the performance of the word-based IBM Model 4 (Brown et al., 1993). Most relevant for the work presented in this paper, they compare the effect on translation quality of using various lengths of phrases, and the size of the resulting phrase probability tables. Tillmann (2003) further examines the relationship between maximum phrase length, size of the translation table, and accuracy of translation when inducing block-based phrases from word-level alignments. Venugopal et al. (2003) and Vogel et al. (2003) present methods for achieving better translation quality by growing incrementally larger phrases by combining smaller phrases with overlapping segments. 3 Scaling to Long Phrases Table 1 gives statistics about the Arabic-English parallel corpus used in the NIST large data track. The corpus contains 3.75 million sentence pairs, and has 127 million words in English, and 106 million words in Arabic. The table shows the number of unique Arabic phrases, and gives the average number of translations into English and their average length. Table 2 gives estimates of the size of</context>
</contexts>
<marker>Venugopal, Vogel, Waibel, 2003</marker>
<rawString>Ashish Venugopal, Stephan Vogel, and Alex Waibel. 2003. Effective phrase translation extraction from alignment models. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Ying Zhang</author>
<author>Fei Huang</author>
<author>Alicia Tribble</author>
<author>Ashish Venugopal</author>
<author>Bing Zhao</author>
<author>Alex Waibel</author>
</authors>
<title>The CMU statistical machine translation system.</title>
<date>2003</date>
<booktitle>In Proceedings ofMT Summit 9.</booktitle>
<contexts>
<context position="4733" citStr="Vogel et al. (2003)" startWordPosition="773" endWordPosition="776">u and Wong, 2002) and a variant on the alignment template approach (Och and Ney, 2004), and contrast them to the performance of the word-based IBM Model 4 (Brown et al., 1993). Most relevant for the work presented in this paper, they compare the effect on translation quality of using various lengths of phrases, and the size of the resulting phrase probability tables. Tillmann (2003) further examines the relationship between maximum phrase length, size of the translation table, and accuracy of translation when inducing block-based phrases from word-level alignments. Venugopal et al. (2003) and Vogel et al. (2003) present methods for achieving better translation quality by growing incrementally larger phrases by combining smaller phrases with overlapping segments. 3 Scaling to Long Phrases Table 1 gives statistics about the Arabic-English parallel corpus used in the NIST large data track. The corpus contains 3.75 million sentence pairs, and has 127 million words in English, and 106 million words in Arabic. The table shows the number of unique Arabic phrases, and gives the average number of translations into English and their average length. Table 2 gives estimates of the size of the lookup tables neede</context>
</contexts>
<marker>Vogel, Zhang, Huang, Tribble, Venugopal, Zhao, Waibel, 2003</marker>
<rawString>Stephan Vogel, Ying Zhang, Fei Huang, Alicia Tribble, Ashish Venugopal, Bing Zhao, and Alex Waibel. 2003. The CMU statistical machine translation system. In Proceedings ofMT Summit 9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikio Yamamoto</author>
<author>Kenneth Church</author>
</authors>
<title>Using suffix arrays to compute term frequency and document frequency for all substrings in a corpus.</title>
<date>2001</date>
<journal>Compuatational Linguistics,</journal>
<volume>27</volume>
<issue>1</issue>
<contexts>
<context position="12213" citStr="Yamamoto and Church (2001)" startWordPosition="2028" endWordPosition="2031"> suffix array, which is identical in length to the corpus. Figure 2 shows the final state of the suffix array, which is as a list of the indices of words in the corpus that corresponds to an alphabetically sorted list of the suffixes. The advantages of this representation are that it is compact and easily searchable. The total size of the suffix array is a constant amount of memory. Typically it is stored as an array of integers where the array is the same length as the corpus. Because it is organized alphabetically, any phrase can be quickly located within it using a binary search algorithm. Yamamoto and Church (2001) show how to use suffix arrays to calculate a number of statistics that are interesting in natural language processing applications. They demonstrate how to calculate term freFigure 2: A sorted suffix array and its corresponding suffixes quency / inverse document frequency (tf/ idf) for all n-grams in very large corpora, as well as how to use these frequencies to calculate n-grams with high mutual information and residual inverse document frequency. Here we show how to apply suffix arrays to parallel corpora to calculate phrase translation probabilities. 4.1 Applied to parallel corpora In orde</context>
</contexts>
<marker>Yamamoto, Church, 2001</marker>
<rawString>Mikio Yamamoto and Kenneth Church. 2001. Using suffix arrays to compute term frequency and document frequency for all substrings in a corpus. Compuatational Linguistics, 27(1):1–30.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>