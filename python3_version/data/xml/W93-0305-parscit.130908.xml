<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.855457">
HMM-based Part-of-Speech Tagging for Chinese Corpora
</title>
<note confidence="0.504952">
Chao-Huang Chang and Cheng-Der Chen
E000/ CCL, Building 11, Industrial Technology Research Institute
</note>
<address confidence="0.834575">
Chutung, Hsinchu 31015, Taiwan, R.O.C.
</address>
<email confidence="0.986319">
E-mail: changch0e0sun3.cc1. itri.org.tw
</email>
<sectionHeader confidence="0.990237" genericHeader="abstract">
Abstract
</sectionHeader>
<construct confidence="0.96633355">
Chinese part-of-speech lagging is more difficult than
its English counterpart because it needs to be solved
together with the problem of word identification, in
this paper, we present our work on Chinese part-of-
speech tagging based on a first-order, fully-connected
hidden Markov model. Part of the 1991 United Daily
corpus of approximately 10 million Chinese charac-
ters is used for training and testing. A news arti-
cle is first segmented into clauses, then into words
by a Viterbi-based word identification system. The
(untagged) segmented corpus is then used to train the
HMM for tagging using the Baum-Welch reestimation
procedure. We also adopt Kupiec&apos;s concept of word
equivalence classes in the tagger. Modeling higher or-
der local constraints, a pattern-driven tag corrector is
designed to postprocess the tag output of the Viterbi
decoder based on trained HMM parameters. Experi-
mental results for various testing conditions are re-
ported: The system is able to correctly tag approxi-
mately 95% of all words in the testing data.
</construct>
<sectionHeader confidence="0.998073" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.964267588235294">
Part-of-speech tagged corpora are very useful for nat-
ural language processing (NLP) applications such as
speech recognition, text-to-speech, information re-
trieval, and machine translation systems. Automatic
part-of-speech tagging has been intensively studied
and practiced for European languages [1-4,7,8,10].
However, the technology of automatic Chinese part-
of-speech tagging is still in its infancy, due to the
following reasons:
1. Definition of words in Chinese is not clear; there
are not breaks between two adjacent words. For
example, the string M—tiiit contains four char-
acters, but it can be divided into one, two, three,
or four words by different linguists. Other diffi-
cult cases include compound words (e.g.,
split words (e.g., iii). acronyms (e.g.,JOM
), and literay words.
</bodyText>
<listItem confidence="0.982232454545455">
2. Word segmentation can not be fully automatic.
3. Well-defined tag set for Chinese part-of-speech is
not available.
9. A Chinese lexicon with complete parts-of-speech
is hard to find.
5. Chinese part-of-speech tagging is difficult even
for human, i.e., the parts-of-speech for many
words are either arguable or difficult to decide.
6. Manually tagged Chinese corpora, counterparts
of Brown corpus and LOB corpus in Chinese, are
not, available.
</listItem>
<bodyText confidence="0.998379153846154">
These intertwined problems make Chinese part-of-
speech tagging an especially difficult task.
Lee and Chang Chien [5,6] used a Tri-POS Markov
language model and a bootstrap training process for
Lagging a small Chinese corpus (1714 sentences for
training and 233 sentences for testing). They re-
ported a tagging accuracy 81.13% for all words and
87.60% for known words.
In this paper, we present our work on part-of-
speech tagging a large Chinese corpus based on a. hid-
den Markov model (HMM). This is among the first
reports on automatic Chinese part-of-speech tagging
in the literature [5,6].
</bodyText>
<sectionHeader confidence="0.9337395" genericHeader="method">
2 The HMM-based Part-of-
Speech Tagger
</sectionHeader>
<bodyText confidence="0.982444">
Kupiec [9] describes a HMM-based tagging system
which can be trained with a corpus of untagged text.
</bodyText>
<page confidence="0.99685">
40
</page>
<bodyText confidence="0.99996040625">
There are two new features in Kupiec&apos;s tagger: (1)
word equivalence classes and (2) predefined networks.
Words with the same set of parts-of-speech are de-
fined as an equivalence class. For example, “type&amp;quot;
and &apos;&apos;store&amp;quot; belong to the equivalence class noun-or-
ver. This not only reduces the number of param-
eters effectively and also makes the tagging system
robust. The first-order model is extended with prede-
fined networks based on error analysis and linguistic
considerations. Their experimental results show that
the predefined networks reduced the overall error rate
by only 0.2%. Thus, we adopt the concept of equiv-
alence classes but consider that predefined networks
are not worthwhile.
Let us briefly review the formulation of 11ls,11%1
for part-of-speech tagging: A first-order EIMM of N
states and fs,1 possible observations has three sets of
parameters: state transition probability distribution
A (N by N), observation probability distribution B
(N by M), and initial state distribution P (N). For
an observation sequence 0 of length T, there are al-
gorithms, e.g., Viterbi, to uncover the hidden state
sequence I. For tagging, N is the number of parts-of-
speech in the language, AI can be the number of words
or the number of equivalence classes (as Kupiec de-
fined). In Chinese. the number of words is more than
100,000 while the number of equivalence classes is less
than 1.000. The use of equivalence classes reduces the
size of B by 100 times.
The problem of tagging is: Given a word sequence
(observations), find out the correct part-of-speech se-
quence (states).
</bodyText>
<subsectionHeader confidence="0.997812">
2.1 The Part-of-Speech Tag Set
</subsectionHeader>
<bodyText confidence="0.999836166666667">
The tag set contains 46 regular tags plus 11 spe-
cial tags. Regular tags include AO (adjective), CO-C1
(conjunctions), DO-D2 (pronouns), 10 (interjection),
MO (measure), N0-N9 (nouns), PO (preposition), RO-
R6 (particles), TO (mood), UO-U4 (numbers), VO-
V4 (verbs), XO (onomatope), YO-Y4 (compounds),
Z0.22 (adverbs). Special tags are for punctuations
(PAR, SEN, PCT, DUN, COM, SEM, COL), un-
known words (UNK), foreign words (ABC), and
composed numbers (NUM, ARA). It is simplified
and reorganized from the classification of Chinese
Knowledge Information Processing Group (CKIP),
Academia Sillies, Taipei. The original CKIP clas-
sification is a five-level system, too complicated even
for human to use. Sun 1121 designed a three-level tag
set TUCWS of 120 tags for Chinese word segmenta-
tion. However, they tag the corpus by hand without
an automatic tagger. Thus, it is difficult to decide if
the set is good for automatic tagging. Other Chinese
tag sets can be found in the literature: 33 tags in
Su [11], 30 tags in Lee and Chang Chien [5], and 34
tags in Lee et al. [6]. These three tag sets are of two
origins, CKIP [5] and NTHU [6,11]. The numbers of
tags in them are considered too small.
</bodyText>
<subsectionHeader confidence="0.999558">
2.2 Corpus Preparation
</subsectionHeader>
<bodyText confidence="0.985020242424242">
The 1991 United Daily corpus contains more than
10 million Chinese characters, about twenty days of
news articles published by United Informatics, Inc.
during January through March 1991. Basically, it is
a collection of articles in the form of raw text (i.e.,
character stream). Thus, we have to segment the
character stream into a word stream before it can be
used for training or testing the model. The corpus
preparation process consists of the following steps:
Preprocessing Clean up inappropriate parts, such
as titles, parenthesized texts, reporter informa-
tion, figures, etc., in the input article. Arti-
cles mostly composed of inappropriate parts are
deleted.
Clause identification Divide up the article into
clauses delimited by clause-ending punctuations
such as periods, commas, question marks.
Automatic word segmentation
Segment the characters in a clause into words
using a dictionary-based, Viterbi decoding word
identification system.
Manual correction (optional) Check the seg-
mented text to correct segmentation errors due
to unregistered words or inaccuracy of the seg-
mentation algorithm. This step is optional but
helpful especially for training.
Equivalence class look-up Words in the clause
are then converted to identifiers of equivalence
class (EQC-ids) via dictionary look-up.
After the above steps, an article is converted into
a series of sequences of EQC-ids.
Manual tagging of the whole corpus would take sev-
eral man-years. However, tagged corpus is necessary
</bodyText>
<page confidence="0.774082">
41.
</page>
<bodyText confidence="0.746366">
for evaluation of the model and helpful for initializa-
tion of the HMM parameters as Merialdo [8] pointed
</bodyText>
<listItem confidence="0.7416245">
out. Thus, we also tag part of the corpus by the steps
below: (1) Train the IIMM using the articles to be
tagged; (2) Tag the articles using the trained HMM;
(3) Correct the erroneous tags by hand.
</listItem>
<subsectionHeader confidence="0.999155">
2.3 Training the Model
</subsectionHeader>
<bodyText confidence="0.999085176470588">
The untagged corpus of EQC-ids is then used for
training the HMM for Lagging using the Baum-Welch
reestimation procedure with multiple observation se-
quences [9] Before training, the model parameters,
A, B, P, can be initialized with a tagged corpus.
A The tag bigrains in the tagged corpus are counted
to initialize A, the state transition matrix. All
counts are incremented by one then normalized.
B The EQC-id to tag correspondences are counted
to set up B, the observation matrix. All possible
states for an EQC are then incremented by one.
P The initial state matrix P is initialized by counting
the tags of first words in the clause. All counts
are incremented by one then normalized.
After training, the model parameters are adjusted
to bestly predict the most probable tag sequence for
the training data.
</bodyText>
<subsectionHeader confidence="0.999036">
2.4 Automatic Tagging
</subsectionHeader>
<bodyText confidence="0.982188142857143">
Having the trained model parameters, we can au-
tomatically tag an unseen text based on an HMM
decoding algorithm such as Viterbi&apos;s. For a given
clause, the tagging process is:
Automatic word segmentation Segment
the characters in the clause into words using the
above-mentioned word identification system.
Equivalence class look-up Words in the clause
are then converted to EQC-ids via dictionary
look-up.
Viterbi decoding The sequence of EQC-ids. as ob-
servations, is then fed to the Viterbi decoder in
order to find out the most probable hidden state
sequence, namely, the tag sequence
</bodyText>
<subsectionHeader confidence="0.816391">
Pattern-driven Tag Correction
</subsectionHeader>
<bodyText confidence="0.959362352941177">
First-order models are not enough to describe
lo-
cal constraints for predicting part-of-speech tags.
Higher-order models have much more param-
eters to estimate and need a lot more train-
ing data and resources (memory, CPU time).
Kupiec [4] proposed using networks to model
higher-order context based on error analysis and
linguistic considerations. However, using net-
works is considered not elegant and had only
very limited success. We use a simple pattern-
driven Lag corrector to postprocess the tag out-
put: The EQC-id sequence is matched against
predefined patterns; when a match is found, the
corresponding tag corrections are made. These
patterns are designed according to analysis of er-
ror patterns.
</bodyText>
<subsectionHeader confidence="0.982705">
2.5 The Dictionary
</subsectionHeader>
<bodyText confidence="0.999286285714286">
The general dictionary has some 80,000 lexical en-
tries each of which contains the Chinese characters
and its EQC-id. The original dictionary is a col-
laborated work of CCL/ITRI with Academia Sinica,
Taipei: ITRI collected the words, their pronuncia-
tions and word frequencies, while Academia Sinica
provided syntactic and semantic markers. For our
purpose, only the words and their syntactic infor-
mation (parts-of-speech) are useful. As mentioned,
we restructured the general dictionary based on our
newly designed compact tag set. For purpose of com-
parison, we also constructed a closed dictionary in
which the words and their tags in the training and
testing corpora are collected.
</bodyText>
<subsectionHeader confidence="0.998788">
2.6 An Example
</subsectionHeader>
<bodyText confidence="0.9910275">
In the following, we use a real-world example to illus-
trate the tagging process.
</bodyText>
<listItem confidence="0.882523333333333">
• A News Paragraph
41&amp;quot;.Mtrit +—P-FAHDIlltitl,V.
IllifttflittEMT22=-K17.
• Clause Identification
2.
2. *F.17M—EliA_LIM.
</listItem>
<page confidence="0.980329">
42
</page>
<table confidence="0.5671124">
1. &amp;quot;=&amp;quot;7* ithVg Ei -I-- )4 -I- 7: El
111.t
3. 011 fl5 Nna itgl gag EgM
Cifflitt fk-V- 5-J.t
4RM
</table>
<bodyText confidence="0.997109909090909">
A tagged corpus, called corpus1, was prepared
through the steps described in the subsection Cor-
pus Preparation. The corpus is composed of 1,418
clauses or 12,289 word tokens. A larger corpus, called
RI corpus3, contains 3,784 clauses. corpus3 is seg-
mented but untagged, useful only for training.
There are totally 338 word equivalence classes:
Each of the 100 most frequently used ambiguous
words is assigned a unique EQC-id; the rest 238 EQC-
ids are assigned to sets of words with the same set of
possible tags.
</bodyText>
<figure confidence="0.997938538461538">
3 6&amp;quot;,ValtMiirgIAtlialEnnIfifflitZglit
• Word Segmentation
• EQC-ids
1. 123 0 120 2 134 2 135 115 128
2 124 135 112 152 234 152 116
3. 95 121 42 0 133 158 133 20 0
314 I300
3.1 Inside Test, Uniformly Initialized,
General Dictionary
Con dawn # [Vards # Hits Accuracy
8
8
116 116 269
</figure>
<listItem confidence="0.908125157894737">
• Equivalence Classes
I. NI UNK PO NUM MO NUM MON3 VO N8
COM
2. D1 MON3 ZO POV2 NUM AOMOVO DON8
NO COM
3. POVO N3 MON4 UNK NOVO D220 NOVO
C1N3P0 UNK NO NO D1N5U0 U2 U0 UNK
COM
• Tagging Results
1. Ni UNK PO NUM MO NUM MO VO N8
COM
2. DI MO ZO V2 NUM MO N8 NO COM
3. PO N3 N4 UNK VO D2 NO PO UNK NO NO
UO 1)2 1)0 UNK COM
• Correct Tags
I. N1 NO PO NUM MO NUM MO VO N8 COM
2. DI MO 20 V2 NUM MO N8 NO COM
3 PO N3 N4 N2 VU D2 VO PO NO NO NO UO
U2 110 VO COM
</listItem>
<sectionHeader confidence="0.996457" genericHeader="method">
3 Experimental Results
</sectionHeader>
<bodyText confidence="0.99829325">
The whole tagging system, including word segmenta-
tion module, equivalence class mapper, HMM trainer,
and Viterbi decoder, is implemented in C on a Sun
Sparcstation.
</bodyText>
<table confidence="0.997200666666667">
All 12,289 10,610 86.37%
Known 11,389 10,610 93.16%
Ambiguous 3,906 3,135 80.26%
</table>
<tableCaption confidence="0.999826">
Table 1: Accuracy Rates (inside, uniform, general)
</tableCaption>
<bodyText confidence="0.996663055555555">
Table 1 shows the experimental results for an inside
test on corpusi. The 80,000-word general dictionary
was used and the model parameters are uniformly
initialized, i.e., the tags in the corpus are not used to
initialize the parameters.
The accuracy rate for all words is 86.37% (1,674 er-
rors out of 12,284 words). Excluding unknown words
(words not in the dictionary), the accuracy rate is
93.16% (779 errors). in other words, approximately
half of the errors can be attributed to unknown words.
If we only consider ambiguous (multi-POS) words,
the accuracy is 80.26% (771 errors). We can also ob-
serve that only about 35% of the words are ambigu-
ous. (The difference between the latter two numbers
of error is due to special usage of some registered
words, e.g., &apos;everyday&apos; is ZO (adverb) in the dic-
tionary but is used as a company name N2 in
it &apos;Everyday Department Store&apos;.)
</bodyText>
<subsectionHeader confidence="0.992322">
3.2 Inside Test, Initialized with
Tagged Text, General Dictionary
</subsectionHeader>
<bodyText confidence="0.9992815">
Tagged texts are useful for initializing the model pa-
rameters before training. Table 2 shows that the ac-
curacy for ambiguous words was improved by about
three percent (from 80.26% to 83.21%). The accuracy
</bodyText>
<page confidence="0.999666">
43
</page>
<table confidence="0.9780125">
Condition #Words #Hits Accuracy
All 12,284 10,725 87.31%
Known 11,389 10,725 94.17%
Ambiguous 3,906 3,250 83.21%
</table>
<tableCaption confidence="0.996971">
Table 2: Accuracy Rates (inside, initialized, general)
</tableCaption>
<bodyText confidence="0.925355545454546">
rate for known words was also improved to more than
94 percent.
In the last row, corpus3 (3,784 clauses, 35,899
words, translated AP news) was used for training
while corpusl (1,418 clauses, 12,284 words, domes-
tic news) for testing. Due to difference of text type,
accuracy rates are degraded by about 3 percent for
ambiguous words. However, the system is still able
to assign correct tags to 91.83 percent of all words.
This shows the robustness of the model, due to the
concept of equivalence classes.
</bodyText>
<sectionHeader confidence="0.526102" genericHeader="method">
3.5 Outside Test, Closed Dictionary
</sectionHeader>
<subsectionHeader confidence="0.96091">
3.3 Inside Test, Closed Dictionary
</subsectionHeader>
<table confidence="0.8989602">
Train Test All Known Ambiguous
Condition #Words #llits Accuracy
All 12,284 11,895 96.83%
Known 12,264 11,895 96.83%
Ambiguous 2,432 2,043 84.00%
</table>
<tableCaption confidence="0.999677">
Table 3: Accuracy Rates (inside, closed)
</tableCaption>
<bodyText confidence="0.999881285714286">
All words and their used tags in corpusl are col-
lected to form an ideal dictionary, so-called closed
dictionary, for tagging the corpus. The 11MA1-based
tagger is able to correctly tag 96.83% of all words or
84.00% of ambiguous words (Table 3). The accuracy
rate is comparable to that of Kupiec&apos;s 11MM-based
English tagger for the well-known Brown corpus.
</bodyText>
<subsectionHeader confidence="0.873119">
3.4 Outside Test, General Dictionary
</subsectionHeader>
<table confidence="0.9137154">
Train Test _ All Known Ambiguous
800 618 85.80% 92.37% 78.16%
1,000 918 86.58% 92.83% 79.95%
1,200 218 86.90% 92.16% 79.40%
3,784 1,418 85.14% 91.83% 7640%
</table>
<tableCaption confidence="0.999342">
Table 4: Accuracy Rates (outside, general)
</tableCaption>
<bodyText confidence="0.9685153">
Table 4 shows the results for outside tests. The cor-
pus is divided into two parts: one for training, the
other for testing. The first two columns (Train and
Test) are the numbers of clauses (not words) used
for training and testing, respectively. The accuracy
rates are noi as good as Lhose for inside tests: de-
graded by shout &apos;2 percent for known words, by 5
percent for ambiguous words. In general, the system
is able to tag approximately 80 percent of ambiguous
words correctly.
</bodyText>
<table confidence="0.999609">
800 618 96.01% 96.01% 80.29%
1,000 418 96.20% 96.20% 82.27%
1,200 218 95.41% 95.41% 79.91%
</table>
<tableCaption confidence="0.999299">
Table 5: Accuracy Rates (outside, closed)
</tableCaption>
<bodyText confidence="0.689295666666667">
Table 5 summarizes the results for outside tests on
closed dictionary. Approximately 96% of all words
and 80% of ambiguous words are tagged correctly.
</bodyText>
<sectionHeader confidence="0.999623" genericHeader="method">
4 Error Analysis
</sectionHeader>
<subsectionHeader confidence="0.999803">
4.1 Confusion Matrix
</subsectionHeader>
<bodyText confidence="0.961931894736842">
Table 6 shows part of the confusion matrix for the
test described in subsection 3.2; only the confusing
parts-of-speech are shown.
The ANVZ problem: Due to lack of inflections
in Chinese, a Chinese word can have many different
parts-of-speech, yet only one form. It is sometimes
very difficult even for human to identify the correct
tag. For example, Chinese does not have -ing end-
ing for nominalization of verbs, -ly for adverbs, -Lion
for verbal nouns, -en for past participles. Thus, a
word such as Mit can be a verb (VO) &apos;distribute&apos;, a
noun (NO) &apos;distribution&apos;, an adjective (AO) &apos;distribu-
tive&apos;, &apos;distributing&apos; or &apos;distributed&apos;, and an adverb
(ZO) &apos;distributively&apos; in different contexts. Nouns and
verbs are especially hard to distinguish. That is why
the VO-NO (180), NO-VO (47) confusions are common.
The RP problem: Open classes, such as nouns
and verbs, have large population, while closed
classed, such as prepositions and particles have small
</bodyText>
<page confidence="0.996356">
44
</page>
<table confidence="0.999326166666667">
AO CO CI NO PO RO R5 VO V4 ZO others rate
AO 63 0 0 27 0 0 0 42 0 3 14 42.3%
CO 0 106 0 0 8 0 0 0 0 1 0 92.2%
Cl 0 14 19 0 5 0 0 3 0 2 0 44.2%
NO 0 0 0 281 0 0 0 47 0 1 10 81.2%
PO 0 8 1 0 195 0 154 25 26 I 6 46.8%
RU 0 0 0 0 0 452 0 0 0 0 0 100.0%
1t5 0 0 0 0 0 0 20 0 0 12 0 62.5%
VO 1 0 8 180 10 0 0 461 &apos; 4 6 14 67.4%
V4 0 (1 0 0 0 0 0 3 27 0 3 81.8%
ZO 13 11 5 3 0 0 3 5 0 222 8 82.2%
TO 0 0 0 0 0 10 0 0 0 0 8 5.6%
</table>
<tableCaption confidence="0.994928">
Table 6: Part of Confusion Matrix
</tableCaption>
<figure confidence="0.383857">
A: state transition probabilities
</figure>
<figureCaption confidence="0.988411">
Figure 1: The RP Problem
</figureCaption>
<table confidence="0.946493666666667">
POR5V0 POR5Z0
PO 0.015 0.004
R5 0.683 0.227
VO 0.004 0.000
ZO 0.000 0.009
B: observation probabilities
</table>
<page confidence="0.998344">
45
</page>
<bodyText confidence="0.999803157894737">
population. In general. this is not a problem for tag-
ging. However, in our tag set, R5 (aspect prefix) has
only three members rf (P0 R5 VO), 44, and M. The
former two words are also common prepositions (P0).
From the experiments, we observed that while is
a preposition in most instances, it is always tagged
as R5 (aspect). After studying the trained model pa-
rameters A, B, P, we found (Figure 1) that R5 was
assigned large probabilities in B matrix (0.683 for ti
, 0.227 for n) since R5 has only three words while
PO was assigned much smaller probabilities (Due to
the probabilistic characteristic, sum of the observa-
tion probabilities for a state, such as PO, R5, must
be one.) In addition. R5 and PO have not significant
difference in the incoming or outcoming entries of A
matrix because of the characteristic of unsupervised
learning: all instances of?r, are considered as possible
candidates for R5. We consider this as a weakness of
HMI11 for tagging.
</bodyText>
<subsectionHeader confidence="0.566233">
4.2 Error Patterns
</subsectionHeader>
<bodyText confidence="0.99868008">
Tagging errors usually occur in clusters; that is, an
error may cause further nustagging of its neighbors
if they are also ambiguous Common patterns of
mistagging include VO-VO (as NO-NO), ZO-VO (as AO-
NO), VO-NO (as C1-Z2), VO-PO (as NO-R5), PO-NO
(as R5-V0), P0-N1 (as R5-V4), and NO-VO-NO (as
U 1 -C I- Z2) . They can be classified into three types:
ANVZ type These error patterns are due to the
above-mentioned ANVZ problem. This type of
error is reasonable.
R.? type Those error patterns involving R5 are due
to the RP problem. The type of error should
be eliminated by model improvement or post-
processing.
idiomatic type Some idiomatic expressions are
composed of highly ambiguous words. For ex-
ample, in &amp;quot;V, ... all the three words E.4
(Cl N3 PO), 24 (Cl PO VO), Ett (AO NO Z2), are
3-way ambiguous words. That is why the VO-NO
sequence is frequently mistagged as C1-Z2.
If we consider the mistagging of unknown words,
more long tagging error clusters would appear. Ac-
tually, an unknown word not only causes mistagging
of the word itself but also affects the tagging of its
neighbors.
</bodyText>
<subsectionHeader confidence="0.874958">
4.3 Without Equivalence Classes
</subsectionHeader>
<table confidence="0.9583806">
Train Test w/o EQC EQC
800 618 77.02% 80.24%
1,000 918 76.90% 82.27%
1,200 218 77.68% 79.91%
1,418 inside 83.80% 84.00%
</table>
<tableCaption confidence="0.92722">
Table 7: Accuracy Rates (closed, ambiguous words
only)
</tableCaption>
<bodyText confidence="0.999085166666667">
To verify feasibility of the concept of equivalence
classes, we implemented a version of the IINIM tag-
ger considering each word as a unique observation
(without EQC). Table 7 compares the results for in-
side/outside tests on closed dictionary. To our sur-
prise, the concept of equivalence classes not only has
the advantages of saving space/time and making the
tagger robust but also achieve higher tagging accu-
racy, especially in case of outside tests. This might be
due to insufficient training data for the much larger
number of parameters to estimate. Nevertheless, it
also proves that the concept is valid and useful.
</bodyText>
<sectionHeader confidence="0.993491" genericHeader="conclusions">
5 Concluding Remarks
</sectionHeader>
<bodyText confidence="0.999969117647059">
We have presented our initial effort for Chinese part-
of-speech tagging using a first-order fully-connected
hidden Markov model and Kupiec&apos;s concept or equiv-
alence classes. The experimental results show that
the tagging model is promising. We have also dis-
cussed our observations on some imperfections of the
current model. In the near future, we will (1) use
the whole UD corpus to further validate and ver-
ify the system, (2) try to implement. a second-order
HMM, (3) attempt to solve part of the unknown
word tagging problem, (9) attempt to solve part of
the compound word problem, (5) use heuristic rules
for postprocessing the tagging output, (6) perform
word identification and part-of-speech tagging con-
currently, and (7) integrate the tagging HMM with
the linguistic decoder of a Chinese speech recognition
system.
</bodyText>
<sectionHeader confidence="0.997639" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999497666666667">
This paper is a partial result of the project no.
37112100 conducted by the ITRI under sponsorship
of the Minister of Economic Affairs, R.O.C.
</bodyText>
<page confidence="0.999088">
46
</page>
<sectionHeader confidence="0.995866" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999502066666667">
[II K. Church. A stochastic parts program and noun
phrase parser for unresticted text. In Proc. of
ICASSP-89, pages 695-698, Glasgow, Scotland,
1989
(2] D. Cutting, J. Kupiec, J. Pedersen, and P. Sibun.
A practical part-of-speech tagger. In Proc. of the
Third Conference on Applied Natural Language
Processing, Trento, Italy, April 1992.
f3l S. DeRose. Grammatical category disambigua-
tion by statistical optimization. Computational
Linguistics, 1431 39.1988.
[4] J. Kupiec. Robust part-of-speech tagging using
a hidden Markov model Computer Speech and
Language, 6:225-242,1992.
[6] H.-J. Lee and C.-H. Chang Chien. A Markov lan-
guage model in handwritten Chinese text recog-
nition. In Proc. of Workshop on Corpus-based
Researches and Techniques for Natural Language
Processing, Taipei, Taiwan, September 1992.
[6] H.-J. Lee, C.-II Dung, F.-M. Lai, and CAI.
Chang Chien. Applications of Markov language
models. In Proc. of Workshop on Advanced In-
formation Systems, Hsinchu, Taiwan, May 1993.
(7) Y.-C. Lin, T.-H.Chiang, and K.-Y. Su. Discrim-
ination oriented probabilistic tagging. In Proc.
of ROCLING V, pages 87-96, Taipei, 1992.
[6] Li. Merialdo. Tagging text with a probabilistic
model. In Proc. of It74SSP-9.1, pages 809-812,
Toronto, 1991.
[9] L.R. Rabiner. A tutorial on hidden markov mod-
els and selected applications in speech recogni-
tion. Proceedings of the IEEE, 77(2):257-286,
1989.
110j B. Santorini. Part-of-speech tagging guidelines
for the Penn Treebank project University of
Pennsylvania, Pennsylvania, March 1991.
[11] K.-Y. Su, Y.-L. Hsu, and C. Saillard. Con-
structing a phrase structure grammar by incor-
porating linguistic knowledge and statistical log-
likelihood ratio, In Proc. of ROCLING IV, pages
257-275, Pingtung, Taiwan, 1991.
[12) M.S. Sun, T.B.Y. Lai, S.C. Lun, and C.F. Sun.
The design of a tagset for Chinese word segmen-
tation. In First International Conference on Chi-
nese Linguistics, Singapore, June 1992.
</reference>
<page confidence="0.99949">
47
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.680036">
<title confidence="0.909626">HMM-based Part-of-Speech Tagging for Chinese Corpora</title>
<author confidence="0.803337">Chao-Huang Chang</author>
<author confidence="0.803337">Cheng-Der</author>
<affiliation confidence="0.772306">E000/ CCL, Building 11, Industrial Technology Research</affiliation>
<address confidence="0.953505">Chutung, Hsinchu 31015, Taiwan,</address>
<email confidence="0.993716">itri.org.tw</email>
<abstract confidence="0.997520952380952">Chinese part-of-speech lagging is more difficult than its English counterpart because it needs to be solved together with the problem of word identification, in paper, we present our work part-ofspeech tagging based on a first-order, fully-connected hidden Markov model. Part of the 1991 United Daily approximately 10 million Chinese charactraining and A news is first segmented clauses, then into words a identification system. The (untagged) segmented corpus is then used to train the HMM for tagging using the Baum-Welch reestimation procedure. We also adopt Kupiec&apos;s concept of word equivalence classes in the tagger. Modeling higher orlocal constraints, a pattern-driven tag is to the tag output of the Viterbi decoder based on trained HMM parameters. Experimental results for various testing conditions are reported: The system is able to correctly tag approxi- 95% of all words testing data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>A stochastic parts program and noun phrase parser for unresticted text.</title>
<date>1989</date>
<booktitle>In Proc. of ICASSP-89,</booktitle>
<pages>695--698</pages>
<location>Glasgow, Scotland,</location>
<marker>1989</marker>
<rawString>[II K. Church. A stochastic parts program and noun phrase parser for unresticted text. In Proc. of ICASSP-89, pages 695-698, Glasgow, Scotland, 1989</rawString>
</citation>
<citation valid="true">
<title>A practical part-of-speech tagger.</title>
<date>1992</date>
<booktitle>In Proc. of the Third Conference on Applied Natural Language Processing,</booktitle>
<location>Trento, Italy,</location>
<marker>1992</marker>
<rawString>(2] D. Cutting, J. Kupiec, J. Pedersen, and P. Sibun. A practical part-of-speech tagger. In Proc. of the Third Conference on Applied Natural Language Processing, Trento, Italy, April 1992.</rawString>
</citation>
<citation valid="false">
<authors>
<author>f3l S DeRose</author>
</authors>
<title>Grammatical category disambiguation by statistical optimization.</title>
<journal>Computational Linguistics,</journal>
<volume>1431</volume>
<pages>39--1988</pages>
<marker>DeRose, </marker>
<rawString>f3l S. DeRose. Grammatical category disambiguation by statistical optimization. Computational Linguistics, 1431 39.1988.</rawString>
</citation>
<citation valid="false">
<authors>
<author>J Kupiec</author>
</authors>
<title>Robust part-of-speech tagging using a hidden Markov model Computer Speech and Language,</title>
<pages>6--225</pages>
<marker>Kupiec, </marker>
<rawString>[4] J. Kupiec. Robust part-of-speech tagging using a hidden Markov model Computer Speech and Language, 6:225-242,1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H-J Lee</author>
<author>C-H Chang Chien</author>
</authors>
<title>A Markov language model in handwritten Chinese text recognition.</title>
<date>1992</date>
<booktitle>In Proc. of Workshop on Corpus-based Researches and Techniques for Natural Language Processing,</booktitle>
<location>Taipei, Taiwan,</location>
<marker>Lee, Chien, 1992</marker>
<rawString>[6] H.-J. Lee and C.-H. Chang Chien. A Markov language model in handwritten Chinese text recognition. In Proc. of Workshop on Corpus-based Researches and Techniques for Natural Language Processing, Taipei, Taiwan, September 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chang Chien</author>
</authors>
<title>Applications of Markov language models.</title>
<date>1993</date>
<booktitle>In Proc. of Workshop on Advanced Information Systems,</booktitle>
<location>Hsinchu, Taiwan,</location>
<marker>Chien, 1993</marker>
<rawString>[6] H.-J. Lee, C.-II Dung, F.-M. Lai, and CAI. Chang Chien. Applications of Markov language models. In Proc. of Workshop on Advanced Information Systems, Hsinchu, Taiwan, May 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y-C Lin</author>
<author>T-H Chiang</author>
<author>K-Y Su</author>
</authors>
<title>Discrimination oriented probabilistic tagging.</title>
<date>1992</date>
<booktitle>In Proc. of ROCLING V,</booktitle>
<pages>87--96</pages>
<location>Taipei,</location>
<marker>Lin, Chiang, Su, 1992</marker>
<rawString>(7) Y.-C. Lin, T.-H.Chiang, and K.-Y. Su. Discrimination oriented probabilistic tagging. In Proc. of ROCLING V, pages 87-96, Taipei, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Merialdo</author>
</authors>
<title>Tagging text with a probabilistic model.</title>
<date>1991</date>
<booktitle>In Proc. of It74SSP-9.1,</booktitle>
<pages>809--812</pages>
<location>Toronto,</location>
<marker>Merialdo, 1991</marker>
<rawString>[6] Li. Merialdo. Tagging text with a probabilistic model. In Proc. of It74SSP-9.1, pages 809-812, Toronto, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Rabiner</author>
</authors>
<title>A tutorial on hidden markov models and selected applications in speech recognition.</title>
<date>1989</date>
<booktitle>Proceedings of the IEEE,</booktitle>
<pages>77--2</pages>
<marker>Rabiner, 1989</marker>
<rawString>[9] L.R. Rabiner. A tutorial on hidden markov models and selected applications in speech recognition. Proceedings of the IEEE, 77(2):257-286, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>110j B Santorini</author>
</authors>
<title>Part-of-speech tagging guidelines for the Penn Treebank project</title>
<date>1991</date>
<institution>University of Pennsylvania,</institution>
<location>Pennsylvania,</location>
<marker>Santorini, 1991</marker>
<rawString>110j B. Santorini. Part-of-speech tagging guidelines for the Penn Treebank project University of Pennsylvania, Pennsylvania, March 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K-Y Su</author>
<author>Y-L Hsu</author>
<author>C Saillard</author>
</authors>
<title>Constructing a phrase structure grammar by incorporating linguistic knowledge and statistical loglikelihood ratio,</title>
<date>1991</date>
<booktitle>In Proc. of ROCLING IV,</booktitle>
<pages>257--275</pages>
<location>Pingtung, Taiwan,</location>
<marker>Su, Hsu, Saillard, 1991</marker>
<rawString>[11] K.-Y. Su, Y.-L. Hsu, and C. Saillard. Constructing a phrase structure grammar by incorporating linguistic knowledge and statistical loglikelihood ratio, In Proc. of ROCLING IV, pages 257-275, Pingtung, Taiwan, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M S Sun</author>
<author>T B Y Lai</author>
<author>S C Lun</author>
<author>C F Sun</author>
</authors>
<title>The design of a tagset for Chinese word segmentation.</title>
<date>1992</date>
<booktitle>In First International Conference on Chinese Linguistics,</booktitle>
<location>Singapore,</location>
<marker>Sun, Lai, Lun, Sun, 1992</marker>
<rawString>[12) M.S. Sun, T.B.Y. Lai, S.C. Lun, and C.F. Sun. The design of a tagset for Chinese word segmentation. In First International Conference on Chinese Linguistics, Singapore, June 1992.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>