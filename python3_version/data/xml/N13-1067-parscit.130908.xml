<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001525">
<title confidence="0.987738">
Purpose and Polarity of Citation: Towards NLP-based Bibliometrics
</title>
<author confidence="0.984032">
Amjad Abu-Jbara
</author>
<affiliation confidence="0.997442">
Department of EECS
University of Michigan
</affiliation>
<address confidence="0.704093">
Ann Arbor, MI, USA
</address>
<email confidence="0.987195">
amjbara@umich.edu
</email>
<author confidence="0.995517">
Jefferson Ezra
</author>
<affiliation confidence="0.9982455">
Department of EECS
University of Michigan
</affiliation>
<address confidence="0.704711">
Ann Arbor, MI, USA
</address>
<email confidence="0.989632">
jezra@umich.edu
</email>
<author confidence="0.97192">
Dragomir Radev
</author>
<affiliation confidence="0.995727666666667">
Department of EECS
and School of Information
University of Michigan
</affiliation>
<address confidence="0.707298">
Ann Arbor, MI, USA
</address>
<email confidence="0.998546">
radev@umich.edu
</email>
<sectionHeader confidence="0.994789" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999607">
Bibliometric measures are commonly used to
estimate the popularity and the impact of pub-
lished research. Existing bibliometric mea-
sures provide “quantitative” indicators of how
good a published paper is. This does not nec-
essarily reflect the “quality” of the work pre-
sented in the paper. For example, when h-
index is computed for a researcher, all incom-
ing citations are treated equally, ignoring the
fact that some of these citations might be neg-
ative. In this paper, we propose using NLP
to add a “qualitative” aspect to biblometrics.
We analyze the text that accompanies citations
in scientific articles (which we term citation
context). We propose supervised methods for
identifying citation text and analyzing it to de-
termine the purpose (i.e. author intention) and
the polarity (i.e. author sentiment) of citation.
</bodyText>
<sectionHeader confidence="0.998784" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999971339285714">
An objective and fair evaluation of the impact
of published research requires both quantitative
and qualitative assessment. Existing bibliometric
measures such as H-Index (Hirsch, 2005; Hirsch,
2010), G-index (Egghe, 2006), and Impact Fac-
tor (Garfield, 1994) focus on the quantitative aspect
of this evaluation which dose not always correlate
with the qualitative aspect.
For example, the number of papers published by
a researcher only tells how productive she or he is.
It does not say anything about the quality or the im-
pact of the work. Similarly, the number of citations
that a paper receives should not be used to gauge
the quality of the work as it really only measures
the popularity of the work and the interest of other
researchers in it (Garfield, 1979). Controversial pa-
pers or those based on fabricated data or experiments
may receive a large number of citations. A popular
example of fraudulent research that deceived many
researchers and caught media attention was the case
of a South Korean research scientist, Hwang Woo-
suk, who was found to have faked his research re-
sults in the area of human stem cell cloning. His re-
search was published in Science and received close
to 200 citations after the fraud was discovered. The
vast majority of those citations were negative.
This suggests that the purpose of citation should
be taken into consideration when biblometric mea-
sures are computed. Negative citations should be
weighted less than positive or neutral citations. This
motivates the need to automatically distinguish be-
tween positive, negative, and neutral citations and to
identify the purpose of a citation; i.e. the author’s in-
tention behind choosing a published article and cit-
ing it.
This analysis of citation purpose and polarity can
be useful for many applications. For example, it can
be used to build systems that help funding agencies
and hiring committees at universities and research
institutions evaluate researchers’ work more accu-
rately. It can also be used as a preprocessing step in
systems that process scholarly data. For example,
citation-based summarization systems (Qazvinian
and Radev, 2008; Qazvinian et al., 2010; Abu-
Jbara and Radev, 2011) and survey generation sys-
tems (Mohammad et al., 2009; Qazvinian et al.,
2013) can benefit from citation purpose and polar-
ity analysis to improve paper and content selection.
In this paper, we investigate the use of linguis-
tic analysis techniques to automatically identify the
purpose of citing a paper and the polarity of this cita-
tion. We first present a sequence labeling method for
extracting the text that cites a given target reference;
i.e. the text that appears in a scientific article and
refers to another article and comments on it. We use
the term citation context to refer to this text. Next,
</bodyText>
<page confidence="0.980324">
596
</page>
<note confidence="0.471373">
Proceedings of NAACL-HLT 2013, pages 596–606,
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999030222222222">
we use supervised classification techniques to ana-
lyze this text and identify the purpose and polarity
of citation.
The rest of this paper is organized as follows. Sec-
tion 2 reviews the related work. We present our ap-
proach in Section 3. We then describe the data and
experiments in Section 4. Finally, Section 5 con-
cludes the paper and suggests directions for future
work.
</bodyText>
<sectionHeader confidence="0.999815" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99998125">
Our work is related to a large body of research
on citations. Studying citation patterns and ref-
erencing practices has interested researchers for
many years (Hodges, 1972; Garfield et al., 1984).
White (2004) provides a good survey of the differ-
ent research directions that study or use citations. In
the following subsections, we review three lines of
research that are closely related to our work.
</bodyText>
<subsectionHeader confidence="0.951236">
2.1 Citation Context Identification
</subsectionHeader>
<bodyText confidence="0.999987794871795">
The first line of related research addresses the prob-
lem of identifying citation context. The context of a
citation that cites a given target paper can be a set of
sentences, one sentence, or a fragment of a sentence.
Nanba and Okumura (1999) use the term citing
area to refer to the same concept. They define the
citing area as the succession of sentences that ap-
pear around the location of a given reference in a
scientific paper and have connection to it. Their al-
gorithm starts by adding the sentence that contains
the target reference as the first member sentence in
the citing area. Then, they use a set of cue words
and hand-crafted rules to determine whether the sur-
rounding sentences should be added to the citing
area or not. In (Nanba et al., 2000), they use their
algorithm to improve citation type classification and
automatic survey generation.
Qazvinian and Radev (2010) addressed a simi-
lar problem. They proposed a method based on
probabilistic inference to extract non-explicit cit-
ing sentences; i.e., sentences that appear around
the sentence that contains the target reference and
are related to it. They showed experimentally that
citation-based survey generation produces better re-
sults when using both explicit and non-explicit cit-
ing sentences rather than using the explicit ones
alone.
In previous work, we addressed the issue of iden-
tifying the scope of a given target reference in citing
sentences that contain multiple references (2012).
Our definition of reference scope was limited to
fragments of the explicit citing sentence (i.e. the
sentence in which actual citation appears). That
method does not identify related text in surrounding
sentences.
In this work, we propose a supervised sequence
labeling method for identifying the citation context
of given reference which includes the explicit citing
sentence and the related surrounding sentences.
</bodyText>
<subsectionHeader confidence="0.999687">
2.2 Citation Purpose Classification
</subsectionHeader>
<bodyText confidence="0.99995994117647">
Several research efforts have focused on studying
the different purposes for citing a paper (Garfield,
1964; Weinstock, 1971; Moravcsik and Muruge-
san, 1975; and Moitra, 1975; Bonzi, 1982).
Bonzi (1982) studied the characteristics of citing
and cited works that may aid in determining the re-
latedness between them. Garfield (1964) enumer-
ated several reasons why authors cite other publi-
cations, including “alerting researchers to forthcom-
ing work”, paying homage to the leading scholars
in the area, and citations which provide pointers to
background readings. Weinstock (1971) adopted the
same scheme that Garfield proposed in her study of
citations.
Spiegel-Rosing (1977) proposed 13 categories for
citation purpose based on her analysis of the first
four volumes of Science Studies. Some of them are:
Cited source is the specific point of departure for
the research question investigated, Cited source con-
tains the concepts, definitions, interpretations used,
Cited source contains the data used by the citing pa-
per. Nanba and Okumura (1999) came up with a
simple schema composed of only three categories:
Basis, Comparison, and other Other. They pro-
posed a rule-based method that uses a set of statis-
tically selected cue words to determine the category
of a citation. They used this classification as a first
step for scientific paper summarization. Teufel et
al. (2006), in their work on citation function classifi-
cation, adopted 12 categories from Spiegel-Rosing’s
taxonomy. They trained an SVM classifier and used
it to label each citing sentence with exactly one cat-
egory. Further, they mapped the twelve categories to
four top level categories namely: weakness, contrast
</bodyText>
<page confidence="0.994916">
597
</page>
<bodyText confidence="0.996846666666667">
(4 categories), positive (6 categories) and neutral.
The taxonomy that we use in this work is based
on previous work. We adopt a scheme that contains
six categories. We selected the six categories after
studying all the previously used citation taxonomies.
We included the ones we believed are important for
improving bibliometric measures and for the appli-
cations that we are planning to pursue in the future
(Section 5).
</bodyText>
<subsectionHeader confidence="0.999698">
2.3 Citation Polarity Classification
</subsectionHeader>
<bodyText confidence="0.99986565">
The polarity (or sentiment) of a citation has also
been studied previously. Previous work showed
that positive and negative citations are common, al-
though negative citations might be expressed indi-
rectly or in an implicit way (Ziman, 1968; Mac-
Roberts and MacRoberts, 1984; THOMPSON and
YIYUN, 1991). Athar (2011) addressed the prob-
lem of identifying sentiment in citing sentences. He
used a set of structure-based features to train a ma-
chine learning classifier using annotated data. This
work uses the citing sentence only to predict senti-
ment. Context sentences were ignored. Athar and
Teufel (2012a) observed that taking the context into
consideration when judging sentiment in citations
increases the number of negative citations by a fac-
tor of 3. They proposed two methods for utilizing
the context. In the first method, they treat the citing
sentence and a fixed context (a window of four sen-
tences around the citing sentence) as if they were
a single sentence. They extract features from the
merged text and train a classifier similar to what they
did in their 2011 paper. In the second method, they
use a four-class annotation scheme. Each sentence
in a window of four sentences around the citation
is labeled as positive, negative, neutral, or excluded
(unrelated to the cited work). There experiments
surprisingly gave negative results and showed that
classifying sentiment without considering the con-
text achieves better results. They attributed this to
the small size of their training data and to the noise
that including the context text introduces to the data.
In (Athar and Teufel, 2012b), the authors present a
method for automatically identifying all the men-
tions of the cited paper in the citing paper. They
show that considering all the mentions improves the
performance of detecting sentiment in citations.
In our work, we propose a sequence labeling
method for identifying the citation context first, and
then use a supervised approach to determine the po-
larity of a given citation.
</bodyText>
<sectionHeader confidence="0.990753" genericHeader="method">
3 Approach
</sectionHeader>
<bodyText confidence="0.999798833333333">
In this section, we describe our approach to three
tasks: citation context identification, citation pur-
pose classification, and citation polarity identifica-
tion. We also describe a preprocessing stage that is
applied to the citation text before performing any of
the three tasks.
</bodyText>
<subsectionHeader confidence="0.999149">
3.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.999956588235294">
The goal of the preprocessing stage is to clean and
prepare the citation text for part-of-speech tagging
and parsing. The available POS taggers and parsers
are not trained on citation text. Citation text is dif-
ferent from normal text in that it contains references
written in a special format (e.g., author names and
publication year written in parentheses; or reference
indices written in square brackets). Many citing sen-
tences contain multiple references, some of which
might be grouped together in a pair of parentheses
and separated by a comma or a semi-colons. These
references are usually not syntactic nor semantic
constituents of the sentences they appear in. This
results in many POS tagging and parsing errors. We
address this issue in the pre-processing stage to im-
prove the performance of the feature extraction com-
ponent. We perform three pre-processing steps:
</bodyText>
<listItem confidence="0.975612857142857">
a. Reference Tagging: In the first step, we find
and tag all the references that appear in the text. We
use a regular expression to find references and re-
place each reference with a placeholder. The ref-
erence to the target paper is replaced by the place-
holder TREF. Each other reference is replaced by
REF.
b. Reference Grouping: In this step, we identify
grouped references (i.e. multiple references listed
between one pair of parentheses separated by semi-
colons). Each such group is replaced by a place-
holder, GREF. If the target reference is a member of
the group, we use a different placeholder: GTREF.
c. Non-syntactic Reference Removal: A refer-
</listItem>
<bodyText confidence="0.70884325">
ence or a group of references could either be a syn-
tactic constituent and has a semantic role in the sen-
tence or not (Whidby, 2012; Abu Jbara and Radev,
2012). If the reference is not a syntactic compo-
</bodyText>
<page confidence="0.923402">
598
</page>
<bodyText confidence="0.999236058823529">
Feature Description
Demonstrative determiners Takes a value of 1 if the current sentence contains contains a demonstrative determiner (this, these,
etc.), and 0 otherwise.
Conjunctive adverbs Takes a value of 1 if the current sentence starts with a conjunctive adverb (However, Furthermore,
Accordingly, etc.), and 0 otherwise.
Position Position of the current sentence with respect to the citing sentence. This feature takes one of four
values: -1, 0, 1, and 2.
Contains Closest Noun Phrase Takes a value of 1 if the current sentence contains closest noun phrase (if any) immediately before
the reference position in the citing sentence, and 0 otherwise. This noun phrase often is the name of
a method, a tool, or corpus originating from the cited reference.
2-3 grams The first bigram and trigram in the sentence (This approach, One problem with, etc.).
Contains Other references Takes a value of 1 if the current sentence contains references other than the target, and 0 otherwise.
Contains a Mention of target reference Takes a value of 1 if the current sentence contains a mention (explicit or anaphoric) of the target
reference, and 0 otherwise.
Multiple references Takes a value of 1 if the citing sentence contains multiple references, and 0 otherwise. If the cit-
ing sentence contains multiple references, it becomes less likely that the surrounding sentences are
related.
</bodyText>
<tableCaption confidence="0.996557">
Table 1: Features used for citation context identification
</tableCaption>
<bodyText confidence="0.99997">
nent in the sentence, we remove it to reduce pars-
ing errors. Following our previous work (Abu Jbara
and Radev, 2012), we use a rule-based algorithm to
determine whether a reference should be removed
from the sentence or kept. The algorithm uses stylis-
tic and linguistic features such as the style of the
reference, the position of the reference, and the sur-
rounding words to make the decision. When a ref-
erence is removed, the head of the closest noun
phrase (NP) immediately before the position of the
removed reference is used as a representative of the
reference. This is needed for feature extraction as
shown later in the paper.
</bodyText>
<subsectionHeader confidence="0.999149">
3.2 Citation Context Identification
</subsectionHeader>
<bodyText confidence="0.99995234375">
The task of identifying the citation context of a given
target reference can be formally defined as follows.
Given a scientific article A that cites another article
B, find a set of sentences in A that talk about the
work done in B such that at least one of these sen-
tences contains an explicit reference to B.
We treat this problem as a sequence labeling prob-
lem. The goal is to find the globally best sequence
of labels for all the sentences that appear within a
window around the citing sentence. The citing sen-
tence is the one that contains an explicit reference
to the cited paper. Each sentence within the window
is labeled as INCLUDED or EXCLUDED from the
citation context of the given target paper. To deter-
mine the size of the window, we examined a devel-
opment set of 300 sentences. We noticed that the re-
lated context almost always falls within a window of
four sentences. The window includes the citing sen-
tence, one sentence before the citing sentence, and
two sentences after the citing sentence.
We use Conditional Random Fields (CRFs) for
sequence labeling. In particular, we use a first-order
chain-structured CRF. The chain consists of two sets
of nodes: 1) a set of hidden nodes Y which represent
the context labels of sentences (INCLUDED or EX-
CLUDED), and 2) a set of observed nodes X which
represent the features extracted from the sentences.
The task is to estimate the probability of a sequence
of labels Y given the sequence of observed features
X: P(Y|X)
Lafferty et al. (2001) define this probability to be
a normalized product of potential functions ψ:
</bodyText>
<equation confidence="0.9978886">
P (y|x) = � ψk(yt, yt−1, x) (1)
t
Where ψk(yt, yt−1, x) is defined as
�ψk(yt, yt−1, x) = exp( λkf(yt, yt−1, x)) (2)
k
</equation>
<bodyText confidence="0.999891111111111">
where f(yt, yt−1, x) is a transition feature func-
tion of the label at positions i − 1 and i and the ob-
servation sequence x; and λj is a parameter that the
algorithm estimates from training data.
The features we use to train the CRF model in-
clude structural and lexical features that attempt to
capture indicators of relatedness to the given target
reference. The features that we used and their de-
scriptions are listed in table 1.
</bodyText>
<page confidence="0.995753">
599
</page>
<table confidence="0.99849819047619">
Category Description Example
Criticizing Criticism can be positive or negative. A citing sentence is classi- Chiang (2005) introduced a constituent feature to reward
fied as ”criticizing” when it mentions the weakness/strengths of phrases that match a syntactic tree but did not yield signif-
the cited approach, negatively/positively criticizes the cited ap- icant improvement.
proach, negatively/positively evaluates the cited source.
Comparison A citing sentence is classified as ”comparison” when it compares Our approach permits an alternative to minimum error-rate
or contrasts the work in the cited paper to the author’s work. It training (MERT; Och, 2003);
overlaps with the first category when the citing sentence says one
approach is not as good as the other approach. In this case we use
the first category.
Use A citing sentence is classified as ”use” when the citing paper uses We perform the MERT training (Och, 2003) to tune the
the method, idea or tool of the cited paper. optimal feature weights on the development set.
Substantiating A citing sentence is classified as ”substantiating” when the re- It was found to produce automated scores, which strongly
sults, claims of the citing work substantiate, verify the cited paper correlate with human judgements about translation flu-
and support each other. ency (Papineni et al. , 2002).
Basis A citing sentence is classified as ”basis” when the author uses the Our model is derived from the hidden-markov model for
cited work as starting point or motivation and extends on the cited word alignment (Vogel et al., 1996; Och and Ney, 2000).
work.
Neutral (Other) A citing sentence is classified as ”neutral” when it is a neutral The solutions of these problems depend heavily on the
description of the cited work or if it doesn’t come under any of quality of the word alignment (Och and Ney, 2000).
the above categories.
</table>
<tableCaption confidence="0.983734">
Table 2: Annotation scheme for citation purpose. Motivated by the work of (Spiegel-R¨osing, 1977) and (Teufel et al.,
2006)
</tableCaption>
<subsectionHeader confidence="0.998288">
3.3 Citation Purpose Classification
</subsectionHeader>
<bodyText confidence="0.999987521739131">
In this section, we describe the citation purpose clas-
sification task. Given a target paper B and its cita-
tion context (extracted using the method described
above) in a given article A, we want to determine
the purpose of citing B by A. The purpose is de-
fined as intention behind selecting B and citing it by
the author of A (Garfield, 1964).
We use a taxonomy that consists of six categories.
We designed this taxonomy based on our study of
similar taxonomies proposed in previous work. We
selected the categories that we believe are more im-
portant and useful from a bibliometric point of view,
and the ones that can be detected through citation
text analysis. We also tried to limit the number of
categories by grouping similar categories proposed
in previous work under one category. The six cate-
gories, their descriptions, and an example for each
category are listed in Table 2.
We use a supervised approach whereby a classifi-
cation model is trained on a number of lexical and
structural features extracted from a set of labeled ci-
tation contexts. Some of the features that we use to
train the classifier are listed in table 3.
</bodyText>
<subsectionHeader confidence="0.990242">
3.4 Citation Polarity Identification
</subsectionHeader>
<bodyText confidence="0.999939517241379">
In this section, we describe the citation polarity iden-
tification task. Given a target paper B and its citation
context in a given article A, we want to determine
the polarity of the citation text with respect to B.
The polarity can be: positive, negative, or neutral
(objective). Positive, negative, and neutral in this
context are defined in a slightly different way than
their usual sense. A citation is marked positive if it
either explicitly states a strength of the target paper
or indicates that the work done in the target paper
has been used either by the author or a third-party. It
is also marked as positive if it is compared to another
paper (possibly by the same authors) and deemed
better in some way. A citation is marked negative
if it explicitly points to a weakness of the target pa-
per. It is also marked as negative if it is compared
to another paper and deemed worse in some way. A
citation is marked as neutral if it is only descriptive.
Similar to citation purpose classification, we use
a supervised approach for this problem. We train a
classification model using the same features listed in
Table 3. Due to the high skewness in the data (more
than half of the citations are neutral), we use two
setups for binary classification. In the first setup,
the citation is classified as Polarized (Subjective) or
(Neutral) Objective. In the second one, Subjective
citations are classified as Positive or Negative. We
find that this method gives more intuitive results than
using a 3-way classifier.
</bodyText>
<page confidence="0.968873">
600
</page>
<table confidence="0.999820380952381">
Feature Description
Reference count The number of references that appear in the citation context.
Is Separate Whether the target reference appears within a group of references or separate (i.e. single reference).
Closest Verb / Adjective / Adverb The lemmatized form of the closest verb/adjective/adverb to the target reference or its representative or any mention
of it. Distance is measure based on the shortest path in the dependency tree.
Self Citation Whether the citation from the source paper to the target reference is a self citation.
Contains 1st/3rd PP Whether the citation context contains a first/third person pronoun.
Negation Whether the citation context contains a negation cue. The list of negation cues is taken from the training data of
the *SEM 2012 negation detection shared task (Morante and Blanco, 2012).
Speculation Whether the citation context contains a speculation cue. The list is taken from Quirk et al. (1985)
Closest Subjectivity Cue The closest subjectivity cue to the target reference or its representative or any anaphoric mention of it. The list of
cues is taken from OpinionFinder (Wilson et al., 2005)
Contrary Expressions Whether the citation context contains a contrary expression. The list is taken from Biber (1988)
Section The headline of the section in which the citation appears. We identify five title categorizes: 1) Introduction,
Motivation, etc. 2) Background, Prior Work, Previous Work, etc. 3) Experiments, Data, Results, Evaluation, etc.
4) Discussion, Conclusion, Future work, etc.. 5) All other section headlines. Headlines are identified using regular
expressions.
Dependency Relations All the dependency relations that appear in the citation context. For example, nsubj(outperform, algorithm)
is one of the relations extracted from ”This algorithm outperforms the one proposed by...”. The arguments of the
dependency relation are replaced by their lemmatized forms. This type of features has been shown to give good
results in similar tasks (Athar and Teufel, 2012a).
</table>
<tableCaption confidence="0.999881">
Table 3: The features used for citation purpose and polarity classification
</tableCaption>
<sectionHeader confidence="0.997866" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.9997585">
In this section, we describe the data that we used for
evaluation and the experiments that we conducted.
</bodyText>
<subsectionHeader confidence="0.981753">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.999977461538462">
We use the ACL Anthology Network corpus
(AAN) (Radev et al., 2009; Radev et al., 2013) in
our evaluation. AAN is a publicly available collec-
tion of more than 19,000 NLP papers. It includes
a manually curated citation network of its papers
as well as the full text of the papers and the cit-
ing sentences associated with each edge in the ci-
tation network. From this set, we selected 30 pa-
pers that have different numbers of incoming cita-
tions and that were consistently cited since they were
published. These 30 papers received a total of about
3,500 citations from within AAN (average = 115 ci-
tation/paper, Min = 30, and Max = 338). These ci-
tations come from 1,493 unique papers. For each
of these citations, we extracted a window of 4 sen-
tences around the reference position. This brings
the number of sentences in our dataset to a total of
roughly 14,000 sentences. We refer to this dataset as
training/testing dataset.
In addition to this dataset, we created another
dataset that contains 300 citations that cite 5 papers
from AAN. We refer to this dataset as the develop-
ment dataset. This dataset was used to determine the
size of the citation context window, and to develop
the feature sets used in the three tasks described in
Section 3 above.
</bodyText>
<subsectionHeader confidence="0.992242">
4.2 Annotation
</subsectionHeader>
<bodyText confidence="0.999984777777778">
In this section, we describe the annotation process.
We asked graduate students with good background
in NLP (the topic of the annotated sentences) to pro-
vide three annotations for each citation example (a
window of 4 sentences around the reference anchor)
in the training/testing dataset. We asked them to
mark the sentences that are related to a given tar-
get reference. In addition, we asked them to deter-
mine the purpose of citing the target reference by
choosing from the six purpose categories that we
described earlier. We also asked them to determine
whether the citation is negative, positive, or neutral.
To estimate the inter-annotator agreement, we
picked 400 sentences from the training/testing
dataset and assigned them to two different annota-
tors. We use the Kappa coefficient (Cohen, 1968)
to measure the agreement. The Kappa coefficient is
defined as follows:
</bodyText>
<equation confidence="0.998772333333333">
P(A) − P(E)
K � (3)
1 − P(E)
</equation>
<bodyText confidence="0.9982175">
where P(A) is the relative observed agreement
among annotators and P(E) is the hypothetical prob-
</bodyText>
<page confidence="0.997335">
601
</page>
<bodyText confidence="0.9999394">
ability of chance agreement. The agreement be-
tween the two annotators on the context identifica-
tion task was K = 0.89. On Landis and Kochs (Lan-
dis and Koch, 1977) scale, this value indicates al-
most perfect agreement. The agreement on the pur-
pose and the polarity classification task were K =
0.61 and K = 0.66, respectively; which indicates
substantial agreement on the same scale.
The annotation shows that in 22% of the citation
examples, the citation context consists of 2 or more
sentences. The distribution of the purpose categories
in the data was: 14.7% criticism, 8.5% comparison,
17.7% use, 7% substantiation, 5% basis, and 47%
other. The distribution of the polarity categories
was: 30% positive, 12% negative, and 58% neutral.
</bodyText>
<subsectionHeader confidence="0.989517">
4.3 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.9999932">
We use the CRF++1 toolkit for CRF training and
testing. We use the Stanford parser to parse the ci-
tation text and generate the dependency parse trees
of sentences. We use Weka for classification experi-
ments. We experimented with several classifiers in-
cluding: SVM, Logistic Regression (LR), and Naive
Bayes. All the experiments that we conducted used
the training/testing dataset in a 10-fold cross vali-
dation mode. All the results have been tested for
statistical significance using a 2-tailed paired t-test.
</bodyText>
<subsectionHeader confidence="0.9118745">
4.4 Evaluation of Citation Context
Identification
</subsectionHeader>
<bodyText confidence="0.9999895">
We compare the CRF approach to three baselines.
The first baseline (ALL) labels all the sentences in
the citation window of size 4 as INCLUDED in the
citation context. The second baseline (CS-ONLY)
labels the citing sentence only as INCLUDED in the
citation context. In the third baseline, we use a su-
pervised classification method instead of sequence
labeling. We use Support Vector Machines (SVM)
to train a model using the same set of features as in
the CRF approach.
Table 4 shows the precision, recall, and F1 score
of the CRF approach and the baselines. The re-
sults show that our CRF approach outperforms all
the baselines. It also asserts our expectation that ad-
dressing this problem as a sequence labeling prob-
lem leads to better performance than individual sen-
</bodyText>
<footnote confidence="0.969808">
1http://crfpp.googlecode.com/svn/trunk/doc/index.html
</footnote>
<table confidence="0.9999796">
Precision Recall F1
CRFs 98.5% 82.0% 89.5%
ALL 30.7% 100.0% 46.9%
CS-ONLY 88.0% 74.0% 80.4%
SVM 92.0% 76.4% 83.5%
</table>
<tableCaption confidence="0.9999">
Table 4: Results of citation context identification
</tableCaption>
<bodyText confidence="0.999838818181818">
tence classification, which is also clear from the na-
ture of the task.
Feature Analysis: We evaluated the importance
of the features listed in Table 1 by computing the
chi-squared statistic for every feature with respect to
the class. We found that the lexical features (such as
determiners and conjunction adverbs) are generally
more important than the structural features (such as
position and reference count). The features shown
in Table 1 are listed in the order of their importance
based on this analysis.
</bodyText>
<subsectionHeader confidence="0.9568675">
4.5 Evaluation of Citation Purpose
Classification
</subsectionHeader>
<bodyText confidence="0.9999414">
Our experiments with several classification algo-
rithms showed that the SVM classifier outperforms
Logistic Regression and Naive Bayes classifiers.
Due to space limitations, we only show the results
for SVM. Table 5 shows the precision, recall, and
F1 for each of the six categories. It also shows the
overall accuracy and the Macro-F measure.
Feature Analysis: The chi-squared evaluation of
the features listed in Table 3 shows that both lexical
and structural features are important. It also shows
that among lexical features, the ones that are limited
to the existence of a direct relation to the target ref-
erence (such as closest verb, adjective, adverb, sub-
jective cue, etc.) are most useful. This can be ex-
plained by the fact that the restricting the features to
having direct dependency relation introduces much
less noise than other features (such as Dependency
Triplets). Among the structural features, the num-
ber of references in the citation context showed to
be more useful.
</bodyText>
<subsectionHeader confidence="0.845473">
4.6 Evaluation of Citation Polarity
Identification
</subsectionHeader>
<bodyText confidence="0.99924375">
Similar to the case of citation purpose classification,
our experiments showed that the SVM classifier out-
performs the other classifiers that we experimented
with. Table 6 shows the precision, recall, and F1 for
</bodyText>
<page confidence="0.995402">
602
</page>
<table confidence="0.999307666666667">
Criticism Comparison Use Substantiating Basis Other
Precision 53.0% 55.2% 60.0% 50.1% 47.3% 64.0%
Recall 77.4% 43.1% 73.0% 57.3% 39.1% 85.1%
F1 63.0% 48.4% 66.0% 53.5% 42.1% 73.1%
Accuracy: 70.5%
Macro-F: 58.0%
</table>
<tableCaption confidence="0.999734">
Table 5: Summary of Citation Purpose Classification Results (10-fold cross validation, SVM: Linear Kernel, c = 1.0)
</tableCaption>
<bodyText confidence="0.9999352">
each of the three categories. It also shows the over-
all accuracy and the Macro-F measure. The analysis
of the features used to train this classifier using chi-
squared analysis leads to the same conclusions about
the relative importance of the features as described
in the previous subsection. However, we noticed that
features that are related to subjectivity (Subjectiv-
ity Cues, Negation, Speculation) are ranked higher
which makes sense in the case of polarity classifica-
tion.
</bodyText>
<subsectionHeader confidence="0.9522075">
4.7 Impact of Context on Classification
Accuracy
</subsectionHeader>
<bodyText confidence="0.999991047619048">
To study the impact of using citation context in ad-
dition to the citing sentence on classification per-
formance, we ran two polarity classification exper-
iments. In the first experiment, we used the citing
sentence only to extract the features that are used
to train the classifiers. In the second experiment,
we used the gold context sentences (the ones la-
beled INCLUDED by human annotators). Table 6
shows the results of the first experiment between
rounded parentheses and the results of the second
experiments in square brackets. The results show
that adding citation context improves the classifica-
tion accuracy especially in the subjective categories,
specially in the negative category if we want to be
more specific. This supports our intuition about po-
larized citations that authors start their review of the
cited work with an objective (neutral) sentence and
then follow it with their criticism if they have any.
We also reached to similar conclusions with purpose
classification, but we are not showing the numbers
due to space limitations.
</bodyText>
<subsectionHeader confidence="0.9968405">
4.8 Other Experiments
4.8.1 Can We Do Better?
</subsectionHeader>
<bodyText confidence="0.998801333333333">
In this section, we investigate whether it is possi-
ble to improve the performance in the two classifica-
tion tasks. One factor that we believe could have an
</bodyText>
<table confidence="0.999603857142857">
Negative % Positive % Neutral
%
Precision 68.7 (66.4) [69.8] 54.9 (52.1) [55.4] 83.6 (82.8) [84.2]
Recall 79.2 (71.1) [81.1] 48.1 (45.6) [46.3] 95.5 (95.1) [95.3]
F1 73.6 (68.7) [75.0] 51.3 (48.6) [50.4] 89.1 (88.5) [89.4]
Accuracy: 81.4 (74.2) [84.2] %
Macro-F: 71.3 (62.1) [74.2] %
</table>
<tableCaption confidence="0.980625166666667">
Table 6: Summary of Citation Polarity Classification Re-
sults (10-fold cross validation, SVM: Linear Kernel, c =
1.0). Numbers between rounded parentheses are when
only the explicit citing sentence is used (i.e. no context).
Numbers in square brackets are when the gold standard
context is used.
</tableCaption>
<bodyText confidence="0.999983285714286">
impact on the result is the size of the training data.
To examine this hypothesis, we ran the experiment
on different sizes of data. Figure 1 shows the learn-
ing curve of the two classifiers for different sizes of
training data. The accuracy increases as more train-
ing data is available so we can expect that with even
more data, we can do even better.
</bodyText>
<subsectionHeader confidence="0.559947">
4.8.2 Relation Between Citation
Purpose/Polarity and Citation Count
</subsectionHeader>
<bodyText confidence="0.999953">
The main motivation of this work is our hypothet-
ical assumption that using NLP for analyzing cita-
tions gives a clearer picture of the impact of the cited
work. As a way to check the validity of this assump-
tion, we study the correlation between the counts of
the different purpose and polarity categories. We
also study the correlation between these categories
and the total number of citations that a paper re-
ceived since it was published. We use the train-
ing/testing dataset and the gold annotations for this
study.
We compute the Pearson correlation coefficient
between the counts of citations from the different
categories that a paper received per year since its
publication. We found that, on average, the correla-
tion between positive and negative citations is neg-
ative (AVG P = -0.194) and that the correlation be-
</bodyText>
<page confidence="0.996744">
603
</page>
<figure confidence="0.991173">
0 500 1000 1500 2000 2500 3000
Dataset Size
</figure>
<figureCaption confidence="0.9993505">
Figure 1: The effect of size of the data set size on the
classifiers accuracy.
</figureCaption>
<bodyText confidence="0.999823111111111">
tween the count of positive citations and the total
number of citations is higher than the correlation be-
tween negative citations and total citations (AVG P =
0.531 for positive vs. AVG P = 0.054 for negative).
Similarly, we noticed that there is a higher posi-
tive correlation between Use citations and total ci-
tations than in the case of both Substantiation and
Basis. This can be explained by the intuition that
publications that present new algorithms, tools, or
corpora that are used by the research community be-
come more and more popular with time and thus re-
ceive more and more citations.
Figure 2 shows the result of running our pur-
pose classifier on all the citations to Papineni et
al.’s (2002) paper about Bleu, an automatic metric
for evaluating Machine Translation (MT) systems.
The figure shows that this paper receives a high
number of Use citations. This makes sense for a pa-
per that describes an evaluation metric that has been
widely used in the MT area. The figure also shows
that in the recent years, this metric started to receive
some Criticizing citations that resulted in a slight de-
crease in the number of Use citations. Such a tempo-
ral analysis of citation purpose and polarity is useful
for studying the dynamics of research. It can also
be used to detect the emergence or de-emergence of
research techniques.
</bodyText>
<note confidence="0.535892">
2001 2003 2005 2007 2009 2011
</note>
<figureCaption confidence="0.999097">
Figure 2: Change in the purpose of the citations to Pap-
ineni et al. (2002)
</figureCaption>
<sectionHeader confidence="0.99798" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999814">
In this paper, we presented methods for three tasks:
citation context identification, citation purpose clas-
sification, and citation polarity classification. This
work is motivated by the need for more accurate
bibliometric measures that evaluates the impact of
research both qualitatively and quantitatively. Our
experiments showed that we can classify the pur-
pose and polarity of citation with a good accuracy. It
also showed that using the citation context improves
the classification accuracy and increases the num-
ber of polarized citations detected. For future work,
we plan to use the output of this research in several
applications such as predicting future prominence of
publications, studying the dynamics of research, and
designing more accurate bibliometric measures.
</bodyText>
<sectionHeader confidence="0.936626" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.999223916666667">
This research is supported by the Intelligence Ad-
vanced Research Projects Activity (IARPA) via
Department of Interior National Business Center
(DoI/NBC) contract number D11PC20153. The
U.S. Government is authorized to reproduce and dis-
tribute reprints for Governmental purposes notwith-
standing any copyright annotation thereon. Dis-
claimer: The views and conclusions contained
herein are those of the authors and should not be
interpreted as necessarily representing the official
policies or endorsements, either expressed or im-
plied, of IARPA, DoI/NBC, or the U.S. Government.
</bodyText>
<figure confidence="0.9968516">
Accuracy
45
85
75
65
55
35
25
Purpose Accuracy
Polarity Accuracy
40
80
70
60
50
30
20
10
0
Criticizing
Comparison
Use
Substantiating
Basis
Other
</figure>
<page confidence="0.992932">
604
</page>
<sectionHeader confidence="0.982198" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999499394230769">
Daryl E. and Soumyo D. Moitra. 1975. Content analysis
of references: Adjunct or alternative to citation count-
ing? Social Studies of Science, 5(4):pp. 423–441.
Amjad Abu-Jbara and Dragomir Radev. 2011. Coherent
citation-based summarization of scientific papers. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 500–509, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Amjad Abu Jbara and Dragomir Radev. 2012. Refer-
ence scope identification in citing sentences. In Pro-
ceedings of the 2012 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, pages 80–
90, Montr´eal, Canada, June. Association for Compu-
tational Linguistics.
Awais Athar and Simone Teufel. 2012a. Context-
enhanced citation sentiment detection. In Proceed-
ings of the 2012 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, pages 597–
601, Montr´eal, Canada, June. Association for Compu-
tational Linguistics.
Awais Athar and Simone Teufel. 2012b. Detection of
implicit citations for sentiment detection. In Proceed-
ings of the Workshop on Detecting Structure in Schol-
arly Discourse, pages 18–26, Jeju Island, Korea, July.
Association for Computational Linguistics.
Awais Athar. 2011. Sentiment analysis of citations us-
ing sentence structure-based features. In Proceedings
of the ACL 2011 Student Session, pages 81–87, Port-
land, OR, USA, June. Association for Computational
Linguistics.
Douglas Biber. 1988. Variation across speech and writ-
ing. Cambridge University Press, Cambridge.
Susan Bonzi. 1982. Characteristics of a literature as pre-
dictors of relatedness between cited and citing works.
Journal of the American Society for Information Sci-
ence, 33(4):208–216.
J. Cohen. 1968. Weighted kappa: Nominal scale agree-
ment with provision for scaled disagreement or partial
credit. Psychological Bulletin, 70:213–220.
Leo Egghe. 2006. Theory and practise of the g-index.
Scientometrics, 69:131–152.
E. Garfield, Irving H. Sher, and R. J. Torpie. 1984. The
Use of Citation Data in Writing the History of Science.
Institute for Scientific Information Inc., Philadelphia,
Pennsylvania, USA.
Eugene Garfield. 1964. Can citation indexing be auto-
mated?
E. Garfield. 1979. Is citation analysis a legitimate evalu-
ation tool? Scientometrics, 1(4):359–375.
Eugene Garfield. 1994. The thomson reuters impact fac-
tor.
J. E. Hirsch. 2005. An index to quantify an individual’s
scientific research output. Proceedings of the National
Academy of Sciences, 102(46):16569–16572, Novem-
ber.
J. E. Hirsch. 2010. An index to quantify an individ-
ual’s scientific research output that takes into account
the effect of multiple coauthorship. Scientometrics,
85(3):741–754, December.
T. L. Hodges. 1972. Citation indexing-its theory
and application in science, technology, and humani-
ties. Ph.D. thesis, University of California at Berke-
ley.Ph.D. thesis, University of California at Berkeley.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data. In Proceedings of the Eighteenth International
Conference on Machine Learning, ICML ’01, pages
282–289, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
J. Richard Landis and Gary G. Koch. 1977. The Mea-
surement of Observer Agreement for Categorical Data.
Biometrics, 33(1):159–174, March.
Michael H. MacRoberts and Barbara R. MacRoberts.
1984. The negational reference: Or the art of dissem-
bling. Social Studies of Science, 14(1):pp. 91–94.
Saif Mohammad, Bonnie Dorr, Melissa Egan, Ahmed
Hassan, Pradeep Muthukrishan, Vahed Qazvinian,
Dragomir Radev, and David Zajic. 2009. Using ci-
tations to generate surveys of scientific paradigms. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 584–592, Boulder, Colorado, June. Association
for Computational Linguistics.
Roser Morante and Eduardo Blanco. 2012. *sem 2012
shared task: resolving the scope and focus of nega-
tion. In Proceedings of the First Joint Conference
on Lexical and Computational Semantics - Volume 1:
Proceedings of the main conference and the shared
task, and Volume 2: Proceedings of the Sixth Inter-
national Workshop on Semantic Evaluation, SemEval
’12, pages 265–274, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
M. J. Moravcsik and P. Murugesan. 1975. Some results
on the function and quality of citations. Social Studies
of Science, 5:86–92.
Hidetsugu Nanba and Manabu Okumura. 1999. To-
wards multi-paper summarization using reference in-
formation. In IJCAI ’99: Proceedings of the Six-
</reference>
<page confidence="0.984718">
605
</page>
<reference confidence="0.999522539473684">
teenth International Joint Conference on Artificial In-
telligence, pages 926–931, San Francisco, CA, USA.
Morgan Kaufmann Publishers Inc.
Hidetsugu Nanba, Noriko Kando, Manabu Okumura, and
Of Information Science. 2000. Classification of re-
search papers using citation links and citation types:
Towards automatic review article generation.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311–318, Philadelphia, Pennsylva-
nia, USA, July. Association for Computational Lin-
guistics.
Vahed Qazvinian and Dragomir R. Radev. 2008. Scien-
tific paper summarization using citation summary net-
works. In Proceedings of the 22nd International Con-
ference on Computational Linguistics (Coling 2008),
pages 689–696, Manchester, UK, August. Coling 2008
Organizing Committee.
Vahed Qazvinian and Dragomir R. Radev. 2010. Identi-
fying non-explicit citing sentences for citation-based
summarization. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 555–564, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
Vahed Qazvinian, Dragomir R. Radev, and Arzucan
Ozgur. 2010. Citation summarization through
keyphrase extraction. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics
(Coling 2010), pages 895–903, Beijing, China, Au-
gust. Coling 2010 Organizing Committee.
Vahed Qazvinian, Dragomir R. Radev, Saif Mohammad,
Bonnie Dorr, David Zajic, Michael Whidby, and Tae-
sun Moon. 2013. Generating extractive summaries of
scientific paradigms. Journal of Artificial Intelligence
Research.
Randolph Quirk, Sidney Greenbaum, Geoffrey Leech,
and Jan Svartvik. 1985. A Comprehensive Grammar
of the English Language. Longman, London.
Dragomir R. Radev, Pradeep Muthukrishnan, and Vahed
Qazvinian. 2009. The acl anthology network corpus.
In NLPIR4DL ’09: Proceedings of the 2009 Workshop
on Text and Citation Analysis for Scholarly Digital Li-
braries, pages 54–61, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Dragomir R. Radev, Pradeep Muthukrishnan, Vahed
Qazvinian, and Amjad Abu-Jbara. 2013. The acl
anthology network corpus. Language Resources and
Evaluation, pages 1–26.
Ina Spiegel-R¨osing. 1977. Science Studies: Bibliomet-
ric and Content Analysis. Social Studies of Science,
7(1):97–113, February.
Simone Teufel, Advaith Siddharthan, and Dan Tidhar.
2006. Automatic classification of citation function. In
In Proc. of EMNLP-06.
GEOFF THOMPSON and YE YIYUN. 1991. Evalu-
ation in the reporting verbs used in academic papers.
Applied Linguistics, 12(4):365–382.
Melvin Weinstock. 1971. Citation Indexes. Encyclope-
dia of Library and Information Science.
Michael Alan Whidby. 2012. Citation handling: Pro-
cessing citation text in scientific documents. In Master
Thesis.
Howard D. White. 2004. Citation analysis and discourse
analysis revisited. Applied Linguistics, 25(1):89–116.
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi, Claire
Cardie, Ellen Riloff, and Siddharth Patwardhan. 2005.
Opinionfinder: a system for subjectivity analysis. In
Proceedings of HLT/EMNLP on Interactive Demon-
strations, HLT-Demo ’05, pages 34–35, Stroudsburg,
PA, USA. Association for Computational Linguistics.
J. M. Ziman. 1968. Public knowledge: An essay con-
cerning the social dimension of science. Cambridge
U.P., London.
</reference>
<page confidence="0.99875">
606
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.114370">
<title confidence="0.987964">Purpose and Polarity of Citation: Towards NLP-based Bibliometrics</title>
<author confidence="0.873081">Amjad</author>
<affiliation confidence="0.9873905">Department of University of</affiliation>
<author confidence="0.540654">Ann Arbor</author>
<author confidence="0.540654">MI</author>
<email confidence="0.998911">amjbara@umich.edu</email>
<author confidence="0.999719">Jefferson Ezra</author>
<affiliation confidence="0.9999635">Department of EECS University of Michigan</affiliation>
<address confidence="0.998789">Ann Arbor, MI, USA</address>
<email confidence="0.999821">jezra@umich.edu</email>
<author confidence="0.58813">Dragomir</author>
<affiliation confidence="0.990514">Department of and School of University of</affiliation>
<author confidence="0.447303">Ann Arbor</author>
<author confidence="0.447303">MI</author>
<email confidence="0.999636">radev@umich.edu</email>
<abstract confidence="0.998403">Bibliometric measures are commonly used to estimate the popularity and the impact of published research. Existing bibliometric measures provide “quantitative” indicators of how good a published paper is. This does not necessarily reflect the “quality” of the work prein the paper. For example, when hcomputed for a researcher, all incoming citations are treated equally, ignoring the fact that some of these citations might be negative. In this paper, we propose using NLP to add a “qualitative” aspect to biblometrics. We analyze the text that accompanies citations scientific articles (which we term We propose supervised methods for identifying citation text and analyzing it to determine the purpose (i.e. author intention) and the polarity (i.e. author sentiment) of citation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Daryl</author>
<author>Soumyo D Moitra</author>
</authors>
<title>Content analysis of references: Adjunct or alternative to citation counting?</title>
<date>1975</date>
<journal>Social Studies of Science,</journal>
<volume>5</volume>
<issue>4</issue>
<pages>423--441</pages>
<marker>Daryl, Moitra, 1975</marker>
<rawString>Daryl E. and Soumyo D. Moitra. 1975. Content analysis of references: Adjunct or alternative to citation counting? Social Studies of Science, 5(4):pp. 423–441.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amjad Abu-Jbara</author>
<author>Dragomir Radev</author>
</authors>
<title>Coherent citation-based summarization of scientific papers.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>500--509</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<marker>Abu-Jbara, Radev, 2011</marker>
<rawString>Amjad Abu-Jbara and Dragomir Radev. 2011. Coherent citation-based summarization of scientific papers. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 500–509, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amjad Abu Jbara</author>
<author>Dragomir Radev</author>
</authors>
<title>Reference scope identification in citing sentences.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>80--90</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="13022" citStr="Jbara and Radev, 2012" startWordPosition="2083" endWordPosition="2086">eholder. The reference to the target paper is replaced by the placeholder TREF. Each other reference is replaced by REF. b. Reference Grouping: In this step, we identify grouped references (i.e. multiple references listed between one pair of parentheses separated by semicolons). Each such group is replaced by a placeholder, GREF. If the target reference is a member of the group, we use a different placeholder: GTREF. c. Non-syntactic Reference Removal: A reference or a group of references could either be a syntactic constituent and has a semantic role in the sentence or not (Whidby, 2012; Abu Jbara and Radev, 2012). If the reference is not a syntactic compo598 Feature Description Demonstrative determiners Takes a value of 1 if the current sentence contains contains a demonstrative determiner (this, these, etc.), and 0 otherwise. Conjunctive adverbs Takes a value of 1 if the current sentence starts with a conjunctive adverb (However, Furthermore, Accordingly, etc.), and 0 otherwise. Position Position of the current sentence with respect to the citing sentence. This feature takes one of four values: -1, 0, 1, and 2. Contains Closest Noun Phrase Takes a value of 1 if the current sentence contains closest n</context>
<context position="14625" citStr="Jbara and Radev, 2012" startWordPosition="2342" endWordPosition="2345">references other than the target, and 0 otherwise. Contains a Mention of target reference Takes a value of 1 if the current sentence contains a mention (explicit or anaphoric) of the target reference, and 0 otherwise. Multiple references Takes a value of 1 if the citing sentence contains multiple references, and 0 otherwise. If the citing sentence contains multiple references, it becomes less likely that the surrounding sentences are related. Table 1: Features used for citation context identification nent in the sentence, we remove it to reduce parsing errors. Following our previous work (Abu Jbara and Radev, 2012), we use a rule-based algorithm to determine whether a reference should be removed from the sentence or kept. The algorithm uses stylistic and linguistic features such as the style of the reference, the position of the reference, and the surrounding words to make the decision. When a reference is removed, the head of the closest noun phrase (NP) immediately before the position of the removed reference is used as a representative of the reference. This is needed for feature extraction as shown later in the paper. 3.2 Citation Context Identification The task of identifying the citation context o</context>
</contexts>
<marker>Jbara, Radev, 2012</marker>
<rawString>Amjad Abu Jbara and Dragomir Radev. 2012. Reference scope identification in citing sentences. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 80– 90, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Awais Athar</author>
<author>Simone Teufel</author>
</authors>
<title>Contextenhanced citation sentiment detection.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>597--601</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="9625" citStr="Athar and Teufel (2012" startWordPosition="1526" endWordPosition="1529">larity Classification The polarity (or sentiment) of a citation has also been studied previously. Previous work showed that positive and negative citations are common, although negative citations might be expressed indirectly or in an implicit way (Ziman, 1968; MacRoberts and MacRoberts, 1984; THOMPSON and YIYUN, 1991). Athar (2011) addressed the problem of identifying sentiment in citing sentences. He used a set of structure-based features to train a machine learning classifier using annotated data. This work uses the citing sentence only to predict sentiment. Context sentences were ignored. Athar and Teufel (2012a) observed that taking the context into consideration when judging sentiment in citations increases the number of negative citations by a factor of 3. They proposed two methods for utilizing the context. In the first method, they treat the citing sentence and a fixed context (a window of four sentences around the citing sentence) as if they were a single sentence. They extract features from the merged text and train a classifier similar to what they did in their 2011 paper. In the second method, they use a four-class annotation scheme. Each sentence in a window of four sentences around the ci</context>
<context position="24063" citStr="Athar and Teufel, 2012" startWordPosition="3916" endWordPosition="3919">Background, Prior Work, Previous Work, etc. 3) Experiments, Data, Results, Evaluation, etc. 4) Discussion, Conclusion, Future work, etc.. 5) All other section headlines. Headlines are identified using regular expressions. Dependency Relations All the dependency relations that appear in the citation context. For example, nsubj(outperform, algorithm) is one of the relations extracted from ”This algorithm outperforms the one proposed by...”. The arguments of the dependency relation are replaced by their lemmatized forms. This type of features has been shown to give good results in similar tasks (Athar and Teufel, 2012a). Table 3: The features used for citation purpose and polarity classification 4 Evaluation In this section, we describe the data that we used for evaluation and the experiments that we conducted. 4.1 Data We use the ACL Anthology Network corpus (AAN) (Radev et al., 2009; Radev et al., 2013) in our evaluation. AAN is a publicly available collection of more than 19,000 NLP papers. It includes a manually curated citation network of its papers as well as the full text of the papers and the citing sentences associated with each edge in the citation network. From this set, we selected 30 papers th</context>
</contexts>
<marker>Athar, Teufel, 2012</marker>
<rawString>Awais Athar and Simone Teufel. 2012a. Contextenhanced citation sentiment detection. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 597– 601, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Awais Athar</author>
<author>Simone Teufel</author>
</authors>
<title>Detection of implicit citations for sentiment detection.</title>
<date>2012</date>
<booktitle>In Proceedings of the Workshop on Detecting Structure in Scholarly Discourse,</booktitle>
<pages>18--26</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="9625" citStr="Athar and Teufel (2012" startWordPosition="1526" endWordPosition="1529">larity Classification The polarity (or sentiment) of a citation has also been studied previously. Previous work showed that positive and negative citations are common, although negative citations might be expressed indirectly or in an implicit way (Ziman, 1968; MacRoberts and MacRoberts, 1984; THOMPSON and YIYUN, 1991). Athar (2011) addressed the problem of identifying sentiment in citing sentences. He used a set of structure-based features to train a machine learning classifier using annotated data. This work uses the citing sentence only to predict sentiment. Context sentences were ignored. Athar and Teufel (2012a) observed that taking the context into consideration when judging sentiment in citations increases the number of negative citations by a factor of 3. They proposed two methods for utilizing the context. In the first method, they treat the citing sentence and a fixed context (a window of four sentences around the citing sentence) as if they were a single sentence. They extract features from the merged text and train a classifier similar to what they did in their 2011 paper. In the second method, they use a four-class annotation scheme. Each sentence in a window of four sentences around the ci</context>
<context position="24063" citStr="Athar and Teufel, 2012" startWordPosition="3916" endWordPosition="3919">Background, Prior Work, Previous Work, etc. 3) Experiments, Data, Results, Evaluation, etc. 4) Discussion, Conclusion, Future work, etc.. 5) All other section headlines. Headlines are identified using regular expressions. Dependency Relations All the dependency relations that appear in the citation context. For example, nsubj(outperform, algorithm) is one of the relations extracted from ”This algorithm outperforms the one proposed by...”. The arguments of the dependency relation are replaced by their lemmatized forms. This type of features has been shown to give good results in similar tasks (Athar and Teufel, 2012a). Table 3: The features used for citation purpose and polarity classification 4 Evaluation In this section, we describe the data that we used for evaluation and the experiments that we conducted. 4.1 Data We use the ACL Anthology Network corpus (AAN) (Radev et al., 2009; Radev et al., 2013) in our evaluation. AAN is a publicly available collection of more than 19,000 NLP papers. It includes a manually curated citation network of its papers as well as the full text of the papers and the citing sentences associated with each edge in the citation network. From this set, we selected 30 papers th</context>
</contexts>
<marker>Athar, Teufel, 2012</marker>
<rawString>Awais Athar and Simone Teufel. 2012b. Detection of implicit citations for sentiment detection. In Proceedings of the Workshop on Detecting Structure in Scholarly Discourse, pages 18–26, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Awais Athar</author>
</authors>
<title>Sentiment analysis of citations using sentence structure-based features.</title>
<date>2011</date>
<booktitle>In Proceedings of the ACL 2011 Student Session,</booktitle>
<pages>81--87</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, OR, USA,</location>
<contexts>
<context position="9337" citStr="Athar (2011)" startWordPosition="1482" endWordPosition="1483">ries. We selected the six categories after studying all the previously used citation taxonomies. We included the ones we believed are important for improving bibliometric measures and for the applications that we are planning to pursue in the future (Section 5). 2.3 Citation Polarity Classification The polarity (or sentiment) of a citation has also been studied previously. Previous work showed that positive and negative citations are common, although negative citations might be expressed indirectly or in an implicit way (Ziman, 1968; MacRoberts and MacRoberts, 1984; THOMPSON and YIYUN, 1991). Athar (2011) addressed the problem of identifying sentiment in citing sentences. He used a set of structure-based features to train a machine learning classifier using annotated data. This work uses the citing sentence only to predict sentiment. Context sentences were ignored. Athar and Teufel (2012a) observed that taking the context into consideration when judging sentiment in citations increases the number of negative citations by a factor of 3. They proposed two methods for utilizing the context. In the first method, they treat the citing sentence and a fixed context (a window of four sentences around </context>
</contexts>
<marker>Athar, 2011</marker>
<rawString>Awais Athar. 2011. Sentiment analysis of citations using sentence structure-based features. In Proceedings of the ACL 2011 Student Session, pages 81–87, Portland, OR, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas Biber</author>
</authors>
<title>Variation across speech and writing.</title>
<date>1988</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="23300" citStr="Biber (1988)" startWordPosition="3808" endWordPosition="3809">her the citation context contains a negation cue. The list of negation cues is taken from the training data of the *SEM 2012 negation detection shared task (Morante and Blanco, 2012). Speculation Whether the citation context contains a speculation cue. The list is taken from Quirk et al. (1985) Closest Subjectivity Cue The closest subjectivity cue to the target reference or its representative or any anaphoric mention of it. The list of cues is taken from OpinionFinder (Wilson et al., 2005) Contrary Expressions Whether the citation context contains a contrary expression. The list is taken from Biber (1988) Section The headline of the section in which the citation appears. We identify five title categorizes: 1) Introduction, Motivation, etc. 2) Background, Prior Work, Previous Work, etc. 3) Experiments, Data, Results, Evaluation, etc. 4) Discussion, Conclusion, Future work, etc.. 5) All other section headlines. Headlines are identified using regular expressions. Dependency Relations All the dependency relations that appear in the citation context. For example, nsubj(outperform, algorithm) is one of the relations extracted from ”This algorithm outperforms the one proposed by...”. The arguments of</context>
</contexts>
<marker>Biber, 1988</marker>
<rawString>Douglas Biber. 1988. Variation across speech and writing. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan Bonzi</author>
</authors>
<title>Characteristics of a literature as predictors of relatedness between cited and citing works.</title>
<date>1982</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>33</volume>
<issue>4</issue>
<contexts>
<context position="7069" citStr="Bonzi, 1982" startWordPosition="1127" endWordPosition="1128"> scope was limited to fragments of the explicit citing sentence (i.e. the sentence in which actual citation appears). That method does not identify related text in surrounding sentences. In this work, we propose a supervised sequence labeling method for identifying the citation context of given reference which includes the explicit citing sentence and the related surrounding sentences. 2.2 Citation Purpose Classification Several research efforts have focused on studying the different purposes for citing a paper (Garfield, 1964; Weinstock, 1971; Moravcsik and Murugesan, 1975; and Moitra, 1975; Bonzi, 1982). Bonzi (1982) studied the characteristics of citing and cited works that may aid in determining the relatedness between them. Garfield (1964) enumerated several reasons why authors cite other publications, including “alerting researchers to forthcoming work”, paying homage to the leading scholars in the area, and citations which provide pointers to background readings. Weinstock (1971) adopted the same scheme that Garfield proposed in her study of citations. Spiegel-Rosing (1977) proposed 13 categories for citation purpose based on her analysis of the first four volumes of Science Studies. So</context>
</contexts>
<marker>Bonzi, 1982</marker>
<rawString>Susan Bonzi. 1982. Characteristics of a literature as predictors of relatedness between cited and citing works. Journal of the American Society for Information Science, 33(4):208–216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cohen</author>
</authors>
<title>Weighted kappa: Nominal scale agreement with provision for scaled disagreement or partial credit.</title>
<date>1968</date>
<journal>Psychological Bulletin,</journal>
<pages>70--213</pages>
<contexts>
<context position="26337" citStr="Cohen, 1968" startWordPosition="4304" endWordPosition="4305">on example (a window of 4 sentences around the reference anchor) in the training/testing dataset. We asked them to mark the sentences that are related to a given target reference. In addition, we asked them to determine the purpose of citing the target reference by choosing from the six purpose categories that we described earlier. We also asked them to determine whether the citation is negative, positive, or neutral. To estimate the inter-annotator agreement, we picked 400 sentences from the training/testing dataset and assigned them to two different annotators. We use the Kappa coefficient (Cohen, 1968) to measure the agreement. The Kappa coefficient is defined as follows: P(A) − P(E) K � (3) 1 − P(E) where P(A) is the relative observed agreement among annotators and P(E) is the hypothetical prob601 ability of chance agreement. The agreement between the two annotators on the context identification task was K = 0.89. On Landis and Kochs (Landis and Koch, 1977) scale, this value indicates almost perfect agreement. The agreement on the purpose and the polarity classification task were K = 0.61 and K = 0.66, respectively; which indicates substantial agreement on the same scale. The annotation sh</context>
</contexts>
<marker>Cohen, 1968</marker>
<rawString>J. Cohen. 1968. Weighted kappa: Nominal scale agreement with provision for scaled disagreement or partial credit. Psychological Bulletin, 70:213–220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leo Egghe</author>
</authors>
<title>Theory and practise of the g-index.</title>
<date>2006</date>
<journal>Scientometrics,</journal>
<pages>69--131</pages>
<contexts>
<context position="1437" citStr="Egghe, 2006" startWordPosition="213" endWordPosition="214">gative. In this paper, we propose using NLP to add a “qualitative” aspect to biblometrics. We analyze the text that accompanies citations in scientific articles (which we term citation context). We propose supervised methods for identifying citation text and analyzing it to determine the purpose (i.e. author intention) and the polarity (i.e. author sentiment) of citation. 1 Introduction An objective and fair evaluation of the impact of published research requires both quantitative and qualitative assessment. Existing bibliometric measures such as H-Index (Hirsch, 2005; Hirsch, 2010), G-index (Egghe, 2006), and Impact Factor (Garfield, 1994) focus on the quantitative aspect of this evaluation which dose not always correlate with the qualitative aspect. For example, the number of papers published by a researcher only tells how productive she or he is. It does not say anything about the quality or the impact of the work. Similarly, the number of citations that a paper receives should not be used to gauge the quality of the work as it really only measures the popularity of the work and the interest of other researchers in it (Garfield, 1979). Controversial papers or those based on fabricated data </context>
</contexts>
<marker>Egghe, 2006</marker>
<rawString>Leo Egghe. 2006. Theory and practise of the g-index. Scientometrics, 69:131–152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Garfield</author>
<author>Irving H Sher</author>
<author>R J Torpie</author>
</authors>
<date>1984</date>
<booktitle>The Use of Citation Data in Writing the History of Science. Institute for Scientific Information Inc.,</booktitle>
<location>Philadelphia, Pennsylvania, USA.</location>
<contexts>
<context position="4723" citStr="Garfield et al., 1984" startWordPosition="752" endWordPosition="755">ociation for Computational Linguistics we use supervised classification techniques to analyze this text and identify the purpose and polarity of citation. The rest of this paper is organized as follows. Section 2 reviews the related work. We present our approach in Section 3. We then describe the data and experiments in Section 4. Finally, Section 5 concludes the paper and suggests directions for future work. 2 Related Work Our work is related to a large body of research on citations. Studying citation patterns and referencing practices has interested researchers for many years (Hodges, 1972; Garfield et al., 1984). White (2004) provides a good survey of the different research directions that study or use citations. In the following subsections, we review three lines of research that are closely related to our work. 2.1 Citation Context Identification The first line of related research addresses the problem of identifying citation context. The context of a citation that cites a given target paper can be a set of sentences, one sentence, or a fragment of a sentence. Nanba and Okumura (1999) use the term citing area to refer to the same concept. They define the citing area as the succession of sentences t</context>
</contexts>
<marker>Garfield, Sher, Torpie, 1984</marker>
<rawString>E. Garfield, Irving H. Sher, and R. J. Torpie. 1984. The Use of Citation Data in Writing the History of Science. Institute for Scientific Information Inc., Philadelphia, Pennsylvania, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Garfield</author>
</authors>
<title>Can citation indexing be automated?</title>
<date>1964</date>
<contexts>
<context position="6989" citStr="Garfield, 1964" startWordPosition="1115" endWordPosition="1116">ing sentences that contain multiple references (2012). Our definition of reference scope was limited to fragments of the explicit citing sentence (i.e. the sentence in which actual citation appears). That method does not identify related text in surrounding sentences. In this work, we propose a supervised sequence labeling method for identifying the citation context of given reference which includes the explicit citing sentence and the related surrounding sentences. 2.2 Citation Purpose Classification Several research efforts have focused on studying the different purposes for citing a paper (Garfield, 1964; Weinstock, 1971; Moravcsik and Murugesan, 1975; and Moitra, 1975; Bonzi, 1982). Bonzi (1982) studied the characteristics of citing and cited works that may aid in determining the relatedness between them. Garfield (1964) enumerated several reasons why authors cite other publications, including “alerting researchers to forthcoming work”, paying homage to the leading scholars in the area, and citations which provide pointers to background readings. Weinstock (1971) adopted the same scheme that Garfield proposed in her study of citations. Spiegel-Rosing (1977) proposed 13 categories for citatio</context>
<context position="19694" citStr="Garfield, 1964" startWordPosition="3209" endWordPosition="3210"> work or if it doesn’t come under any of quality of the word alignment (Och and Ney, 2000). the above categories. Table 2: Annotation scheme for citation purpose. Motivated by the work of (Spiegel-R¨osing, 1977) and (Teufel et al., 2006) 3.3 Citation Purpose Classification In this section, we describe the citation purpose classification task. Given a target paper B and its citation context (extracted using the method described above) in a given article A, we want to determine the purpose of citing B by A. The purpose is defined as intention behind selecting B and citing it by the author of A (Garfield, 1964). We use a taxonomy that consists of six categories. We designed this taxonomy based on our study of similar taxonomies proposed in previous work. We selected the categories that we believe are more important and useful from a bibliometric point of view, and the ones that can be detected through citation text analysis. We also tried to limit the number of categories by grouping similar categories proposed in previous work under one category. The six categories, their descriptions, and an example for each category are listed in Table 2. We use a supervised approach whereby a classification mode</context>
</contexts>
<marker>Garfield, 1964</marker>
<rawString>Eugene Garfield. 1964. Can citation indexing be automated?</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Garfield</author>
</authors>
<title>Is citation analysis a legitimate evaluation tool?</title>
<date>1979</date>
<journal>Scientometrics,</journal>
<volume>1</volume>
<issue>4</issue>
<contexts>
<context position="1980" citStr="Garfield, 1979" startWordPosition="309" endWordPosition="310">ures such as H-Index (Hirsch, 2005; Hirsch, 2010), G-index (Egghe, 2006), and Impact Factor (Garfield, 1994) focus on the quantitative aspect of this evaluation which dose not always correlate with the qualitative aspect. For example, the number of papers published by a researcher only tells how productive she or he is. It does not say anything about the quality or the impact of the work. Similarly, the number of citations that a paper receives should not be used to gauge the quality of the work as it really only measures the popularity of the work and the interest of other researchers in it (Garfield, 1979). Controversial papers or those based on fabricated data or experiments may receive a large number of citations. A popular example of fraudulent research that deceived many researchers and caught media attention was the case of a South Korean research scientist, Hwang Woosuk, who was found to have faked his research results in the area of human stem cell cloning. His research was published in Science and received close to 200 citations after the fraud was discovered. The vast majority of those citations were negative. This suggests that the purpose of citation should be taken into consideratio</context>
</contexts>
<marker>Garfield, 1979</marker>
<rawString>E. Garfield. 1979. Is citation analysis a legitimate evaluation tool? Scientometrics, 1(4):359–375.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Garfield</author>
</authors>
<title>The thomson reuters impact factor.</title>
<date>1994</date>
<contexts>
<context position="1473" citStr="Garfield, 1994" startWordPosition="219" endWordPosition="220"> using NLP to add a “qualitative” aspect to biblometrics. We analyze the text that accompanies citations in scientific articles (which we term citation context). We propose supervised methods for identifying citation text and analyzing it to determine the purpose (i.e. author intention) and the polarity (i.e. author sentiment) of citation. 1 Introduction An objective and fair evaluation of the impact of published research requires both quantitative and qualitative assessment. Existing bibliometric measures such as H-Index (Hirsch, 2005; Hirsch, 2010), G-index (Egghe, 2006), and Impact Factor (Garfield, 1994) focus on the quantitative aspect of this evaluation which dose not always correlate with the qualitative aspect. For example, the number of papers published by a researcher only tells how productive she or he is. It does not say anything about the quality or the impact of the work. Similarly, the number of citations that a paper receives should not be used to gauge the quality of the work as it really only measures the popularity of the work and the interest of other researchers in it (Garfield, 1979). Controversial papers or those based on fabricated data or experiments may receive a large n</context>
</contexts>
<marker>Garfield, 1994</marker>
<rawString>Eugene Garfield. 1994. The thomson reuters impact factor.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J E Hirsch</author>
</authors>
<title>An index to quantify an individual’s scientific research output.</title>
<date>2005</date>
<booktitle>Proceedings of the National Academy of Sciences,</booktitle>
<volume>102</volume>
<issue>46</issue>
<contexts>
<context position="1399" citStr="Hirsch, 2005" startWordPosition="208" endWordPosition="209">at some of these citations might be negative. In this paper, we propose using NLP to add a “qualitative” aspect to biblometrics. We analyze the text that accompanies citations in scientific articles (which we term citation context). We propose supervised methods for identifying citation text and analyzing it to determine the purpose (i.e. author intention) and the polarity (i.e. author sentiment) of citation. 1 Introduction An objective and fair evaluation of the impact of published research requires both quantitative and qualitative assessment. Existing bibliometric measures such as H-Index (Hirsch, 2005; Hirsch, 2010), G-index (Egghe, 2006), and Impact Factor (Garfield, 1994) focus on the quantitative aspect of this evaluation which dose not always correlate with the qualitative aspect. For example, the number of papers published by a researcher only tells how productive she or he is. It does not say anything about the quality or the impact of the work. Similarly, the number of citations that a paper receives should not be used to gauge the quality of the work as it really only measures the popularity of the work and the interest of other researchers in it (Garfield, 1979). Controversial pap</context>
</contexts>
<marker>Hirsch, 2005</marker>
<rawString>J. E. Hirsch. 2005. An index to quantify an individual’s scientific research output. Proceedings of the National Academy of Sciences, 102(46):16569–16572, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J E Hirsch</author>
</authors>
<title>An index to quantify an individual’s scientific research output that takes into account the effect of multiple coauthorship.</title>
<date>2010</date>
<journal>Scientometrics,</journal>
<volume>85</volume>
<issue>3</issue>
<contexts>
<context position="1414" citStr="Hirsch, 2010" startWordPosition="210" endWordPosition="211">se citations might be negative. In this paper, we propose using NLP to add a “qualitative” aspect to biblometrics. We analyze the text that accompanies citations in scientific articles (which we term citation context). We propose supervised methods for identifying citation text and analyzing it to determine the purpose (i.e. author intention) and the polarity (i.e. author sentiment) of citation. 1 Introduction An objective and fair evaluation of the impact of published research requires both quantitative and qualitative assessment. Existing bibliometric measures such as H-Index (Hirsch, 2005; Hirsch, 2010), G-index (Egghe, 2006), and Impact Factor (Garfield, 1994) focus on the quantitative aspect of this evaluation which dose not always correlate with the qualitative aspect. For example, the number of papers published by a researcher only tells how productive she or he is. It does not say anything about the quality or the impact of the work. Similarly, the number of citations that a paper receives should not be used to gauge the quality of the work as it really only measures the popularity of the work and the interest of other researchers in it (Garfield, 1979). Controversial papers or those ba</context>
</contexts>
<marker>Hirsch, 2010</marker>
<rawString>J. E. Hirsch. 2010. An index to quantify an individual’s scientific research output that takes into account the effect of multiple coauthorship. Scientometrics, 85(3):741–754, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T L Hodges</author>
</authors>
<title>Citation indexing-its theory and application in science, technology, and humanities.</title>
<date>1972</date>
<tech>Ph.D. thesis,</tech>
<institution>University of California at Berkeley.Ph.D. thesis, University of California at Berkeley.</institution>
<contexts>
<context position="4699" citStr="Hodges, 1972" startWordPosition="750" endWordPosition="751">13. c�2013 Association for Computational Linguistics we use supervised classification techniques to analyze this text and identify the purpose and polarity of citation. The rest of this paper is organized as follows. Section 2 reviews the related work. We present our approach in Section 3. We then describe the data and experiments in Section 4. Finally, Section 5 concludes the paper and suggests directions for future work. 2 Related Work Our work is related to a large body of research on citations. Studying citation patterns and referencing practices has interested researchers for many years (Hodges, 1972; Garfield et al., 1984). White (2004) provides a good survey of the different research directions that study or use citations. In the following subsections, we review three lines of research that are closely related to our work. 2.1 Citation Context Identification The first line of related research addresses the problem of identifying citation context. The context of a citation that cites a given target paper can be a set of sentences, one sentence, or a fragment of a sentence. Nanba and Okumura (1999) use the term citing area to refer to the same concept. They define the citing area as the s</context>
</contexts>
<marker>Hodges, 1972</marker>
<rawString>T. L. Hodges. 1972. Citation indexing-its theory and application in science, technology, and humanities. Ph.D. thesis, University of California at Berkeley.Ph.D. thesis, University of California at Berkeley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the Eighteenth International Conference on Machine Learning, ICML ’01,</booktitle>
<pages>282--289</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="16680" citStr="Lafferty et al. (2001)" startWordPosition="2700" endWordPosition="2703">es. The window includes the citing sentence, one sentence before the citing sentence, and two sentences after the citing sentence. We use Conditional Random Fields (CRFs) for sequence labeling. In particular, we use a first-order chain-structured CRF. The chain consists of two sets of nodes: 1) a set of hidden nodes Y which represent the context labels of sentences (INCLUDED or EXCLUDED), and 2) a set of observed nodes X which represent the features extracted from the sentences. The task is to estimate the probability of a sequence of labels Y given the sequence of observed features X: P(Y|X) Lafferty et al. (2001) define this probability to be a normalized product of potential functions ψ: P (y|x) = � ψk(yt, yt−1, x) (1) t Where ψk(yt, yt−1, x) is defined as �ψk(yt, yt−1, x) = exp( λkf(yt, yt−1, x)) (2) k where f(yt, yt−1, x) is a transition feature function of the label at positions i − 1 and i and the observation sequence x; and λj is a parameter that the algorithm estimates from training data. The features we use to train the CRF model include structural and lexical features that attempt to capture indicators of relatedness to the given target reference. The features that we used and their descripti</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth International Conference on Machine Learning, ICML ’01, pages 282–289, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Richard Landis</author>
<author>Gary G Koch</author>
</authors>
<title>The Measurement of Observer Agreement for Categorical Data.</title>
<date>1977</date>
<journal>Biometrics,</journal>
<volume>33</volume>
<issue>1</issue>
<contexts>
<context position="26700" citStr="Landis and Koch, 1977" startWordPosition="4367" endWordPosition="4371">to determine whether the citation is negative, positive, or neutral. To estimate the inter-annotator agreement, we picked 400 sentences from the training/testing dataset and assigned them to two different annotators. We use the Kappa coefficient (Cohen, 1968) to measure the agreement. The Kappa coefficient is defined as follows: P(A) − P(E) K � (3) 1 − P(E) where P(A) is the relative observed agreement among annotators and P(E) is the hypothetical prob601 ability of chance agreement. The agreement between the two annotators on the context identification task was K = 0.89. On Landis and Kochs (Landis and Koch, 1977) scale, this value indicates almost perfect agreement. The agreement on the purpose and the polarity classification task were K = 0.61 and K = 0.66, respectively; which indicates substantial agreement on the same scale. The annotation shows that in 22% of the citation examples, the citation context consists of 2 or more sentences. The distribution of the purpose categories in the data was: 14.7% criticism, 8.5% comparison, 17.7% use, 7% substantiation, 5% basis, and 47% other. The distribution of the polarity categories was: 30% positive, 12% negative, and 58% neutral. 4.3 Experimental Setup W</context>
</contexts>
<marker>Landis, Koch, 1977</marker>
<rawString>J. Richard Landis and Gary G. Koch. 1977. The Measurement of Observer Agreement for Categorical Data. Biometrics, 33(1):159–174, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael H MacRoberts</author>
<author>Barbara R MacRoberts</author>
</authors>
<title>The negational reference: Or the art of dissembling.</title>
<date>1984</date>
<journal>Social Studies of Science,</journal>
<volume>14</volume>
<issue>1</issue>
<pages>91--94</pages>
<contexts>
<context position="9296" citStr="MacRoberts and MacRoberts, 1984" startWordPosition="1473" endWordPosition="1477">on previous work. We adopt a scheme that contains six categories. We selected the six categories after studying all the previously used citation taxonomies. We included the ones we believed are important for improving bibliometric measures and for the applications that we are planning to pursue in the future (Section 5). 2.3 Citation Polarity Classification The polarity (or sentiment) of a citation has also been studied previously. Previous work showed that positive and negative citations are common, although negative citations might be expressed indirectly or in an implicit way (Ziman, 1968; MacRoberts and MacRoberts, 1984; THOMPSON and YIYUN, 1991). Athar (2011) addressed the problem of identifying sentiment in citing sentences. He used a set of structure-based features to train a machine learning classifier using annotated data. This work uses the citing sentence only to predict sentiment. Context sentences were ignored. Athar and Teufel (2012a) observed that taking the context into consideration when judging sentiment in citations increases the number of negative citations by a factor of 3. They proposed two methods for utilizing the context. In the first method, they treat the citing sentence and a fixed co</context>
</contexts>
<marker>MacRoberts, MacRoberts, 1984</marker>
<rawString>Michael H. MacRoberts and Barbara R. MacRoberts. 1984. The negational reference: Or the art of dissembling. Social Studies of Science, 14(1):pp. 91–94.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Saif Mohammad</author>
<author>Bonnie Dorr</author>
<author>Melissa Egan</author>
<author>Ahmed Hassan</author>
<author>Pradeep Muthukrishan</author>
<author>Vahed Qazvinian</author>
<author>Dragomir Radev</author>
<author>David Zajic</author>
</authors>
<title>Using citations to generate surveys of scientific paradigms.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>584--592</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="3454" citStr="Mohammad et al., 2009" startWordPosition="542" endWordPosition="545">f a citation; i.e. the author’s intention behind choosing a published article and citing it. This analysis of citation purpose and polarity can be useful for many applications. For example, it can be used to build systems that help funding agencies and hiring committees at universities and research institutions evaluate researchers’ work more accurately. It can also be used as a preprocessing step in systems that process scholarly data. For example, citation-based summarization systems (Qazvinian and Radev, 2008; Qazvinian et al., 2010; AbuJbara and Radev, 2011) and survey generation systems (Mohammad et al., 2009; Qazvinian et al., 2013) can benefit from citation purpose and polarity analysis to improve paper and content selection. In this paper, we investigate the use of linguistic analysis techniques to automatically identify the purpose of citing a paper and the polarity of this citation. We first present a sequence labeling method for extracting the text that cites a given target reference; i.e. the text that appears in a scientific article and refers to another article and comments on it. We use the term citation context to refer to this text. Next, 596 Proceedings of NAACL-HLT 2013, pages 596–60</context>
</contexts>
<marker>Mohammad, Dorr, Egan, Hassan, Muthukrishan, Qazvinian, Radev, Zajic, 2009</marker>
<rawString>Saif Mohammad, Bonnie Dorr, Melissa Egan, Ahmed Hassan, Pradeep Muthukrishan, Vahed Qazvinian, Dragomir Radev, and David Zajic. 2009. Using citations to generate surveys of scientific paradigms. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 584–592, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Roser Morante</author>
<author>Eduardo Blanco</author>
</authors>
<title>sem 2012 shared task: resolving the scope and focus of negation.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics - Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation, SemEval ’12,</booktitle>
<pages>265--274</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="22870" citStr="Morante and Blanco, 2012" startWordPosition="3739" endWordPosition="3742">eference). Closest Verb / Adjective / Adverb The lemmatized form of the closest verb/adjective/adverb to the target reference or its representative or any mention of it. Distance is measure based on the shortest path in the dependency tree. Self Citation Whether the citation from the source paper to the target reference is a self citation. Contains 1st/3rd PP Whether the citation context contains a first/third person pronoun. Negation Whether the citation context contains a negation cue. The list of negation cues is taken from the training data of the *SEM 2012 negation detection shared task (Morante and Blanco, 2012). Speculation Whether the citation context contains a speculation cue. The list is taken from Quirk et al. (1985) Closest Subjectivity Cue The closest subjectivity cue to the target reference or its representative or any anaphoric mention of it. The list of cues is taken from OpinionFinder (Wilson et al., 2005) Contrary Expressions Whether the citation context contains a contrary expression. The list is taken from Biber (1988) Section The headline of the section in which the citation appears. We identify five title categorizes: 1) Introduction, Motivation, etc. 2) Background, Prior Work, Previ</context>
</contexts>
<marker>Morante, Blanco, 2012</marker>
<rawString>Roser Morante and Eduardo Blanco. 2012. *sem 2012 shared task: resolving the scope and focus of negation. In Proceedings of the First Joint Conference on Lexical and Computational Semantics - Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation, SemEval ’12, pages 265–274, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Moravcsik</author>
<author>P Murugesan</author>
</authors>
<title>Some results on the function and quality of citations.</title>
<date>1975</date>
<journal>Social Studies of Science,</journal>
<pages>5--86</pages>
<contexts>
<context position="7037" citStr="Moravcsik and Murugesan, 1975" startWordPosition="1119" endWordPosition="1123">le references (2012). Our definition of reference scope was limited to fragments of the explicit citing sentence (i.e. the sentence in which actual citation appears). That method does not identify related text in surrounding sentences. In this work, we propose a supervised sequence labeling method for identifying the citation context of given reference which includes the explicit citing sentence and the related surrounding sentences. 2.2 Citation Purpose Classification Several research efforts have focused on studying the different purposes for citing a paper (Garfield, 1964; Weinstock, 1971; Moravcsik and Murugesan, 1975; and Moitra, 1975; Bonzi, 1982). Bonzi (1982) studied the characteristics of citing and cited works that may aid in determining the relatedness between them. Garfield (1964) enumerated several reasons why authors cite other publications, including “alerting researchers to forthcoming work”, paying homage to the leading scholars in the area, and citations which provide pointers to background readings. Weinstock (1971) adopted the same scheme that Garfield proposed in her study of citations. Spiegel-Rosing (1977) proposed 13 categories for citation purpose based on her analysis of the first fou</context>
</contexts>
<marker>Moravcsik, Murugesan, 1975</marker>
<rawString>M. J. Moravcsik and P. Murugesan. 1975. Some results on the function and quality of citations. Social Studies of Science, 5:86–92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hidetsugu Nanba</author>
<author>Manabu Okumura</author>
</authors>
<title>Towards multi-paper summarization using reference information.</title>
<date>1999</date>
<booktitle>In IJCAI ’99: Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence,</booktitle>
<pages>926--931</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="5207" citStr="Nanba and Okumura (1999)" startWordPosition="833" endWordPosition="836">ations. Studying citation patterns and referencing practices has interested researchers for many years (Hodges, 1972; Garfield et al., 1984). White (2004) provides a good survey of the different research directions that study or use citations. In the following subsections, we review three lines of research that are closely related to our work. 2.1 Citation Context Identification The first line of related research addresses the problem of identifying citation context. The context of a citation that cites a given target paper can be a set of sentences, one sentence, or a fragment of a sentence. Nanba and Okumura (1999) use the term citing area to refer to the same concept. They define the citing area as the succession of sentences that appear around the location of a given reference in a scientific paper and have connection to it. Their algorithm starts by adding the sentence that contains the target reference as the first member sentence in the citing area. Then, they use a set of cue words and hand-crafted rules to determine whether the surrounding sentences should be added to the citing area or not. In (Nanba et al., 2000), they use their algorithm to improve citation type classification and automatic su</context>
<context position="7925" citStr="Nanba and Okumura (1999)" startWordPosition="1256" endWordPosition="1259">rchers to forthcoming work”, paying homage to the leading scholars in the area, and citations which provide pointers to background readings. Weinstock (1971) adopted the same scheme that Garfield proposed in her study of citations. Spiegel-Rosing (1977) proposed 13 categories for citation purpose based on her analysis of the first four volumes of Science Studies. Some of them are: Cited source is the specific point of departure for the research question investigated, Cited source contains the concepts, definitions, interpretations used, Cited source contains the data used by the citing paper. Nanba and Okumura (1999) came up with a simple schema composed of only three categories: Basis, Comparison, and other Other. They proposed a rule-based method that uses a set of statistically selected cue words to determine the category of a citation. They used this classification as a first step for scientific paper summarization. Teufel et al. (2006), in their work on citation function classification, adopted 12 categories from Spiegel-Rosing’s taxonomy. They trained an SVM classifier and used it to label each citing sentence with exactly one category. Further, they mapped the twelve categories to four top level ca</context>
</contexts>
<marker>Nanba, Okumura, 1999</marker>
<rawString>Hidetsugu Nanba and Manabu Okumura. 1999. Towards multi-paper summarization using reference information. In IJCAI ’99: Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence, pages 926–931, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hidetsugu Nanba</author>
</authors>
<title>Noriko Kando, Manabu Okumura, and Of Information Science.</title>
<date>2000</date>
<marker>Nanba, 2000</marker>
<rawString>Hidetsugu Nanba, Noriko Kando, Manabu Okumura, and Of Information Science. 2000. Classification of research papers using citation links and citation types: Towards automatic review article generation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Philadelphia, Pennsylvania, USA,</location>
<contexts>
<context position="36155" citStr="Papineni et al. (2002)" startWordPosition="5912" endWordPosition="5916">paper receives a high number of Use citations. This makes sense for a paper that describes an evaluation metric that has been widely used in the MT area. The figure also shows that in the recent years, this metric started to receive some Criticizing citations that resulted in a slight decrease in the number of Use citations. Such a temporal analysis of citation purpose and polarity is useful for studying the dynamics of research. It can also be used to detect the emergence or de-emergence of research techniques. 2001 2003 2005 2007 2009 2011 Figure 2: Change in the purpose of the citations to Papineni et al. (2002) 5 Conclusion In this paper, we presented methods for three tasks: citation context identification, citation purpose classification, and citation polarity classification. This work is motivated by the need for more accurate bibliometric measures that evaluates the impact of research both qualitatively and quantitatively. Our experiments showed that we can classify the purpose and polarity of citation with a good accuracy. It also showed that using the citation context improves the classification accuracy and increases the number of polarized citations detected. For future work, we plan to use </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, Pennsylvania, USA, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vahed Qazvinian</author>
<author>Dragomir R Radev</author>
</authors>
<title>Scientific paper summarization using citation summary networks.</title>
<date>2008</date>
<journal>Organizing Committee.</journal>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<pages>689--696</pages>
<location>Manchester, UK,</location>
<contexts>
<context position="3350" citStr="Qazvinian and Radev, 2008" startWordPosition="524" endWordPosition="527">to automatically distinguish between positive, negative, and neutral citations and to identify the purpose of a citation; i.e. the author’s intention behind choosing a published article and citing it. This analysis of citation purpose and polarity can be useful for many applications. For example, it can be used to build systems that help funding agencies and hiring committees at universities and research institutions evaluate researchers’ work more accurately. It can also be used as a preprocessing step in systems that process scholarly data. For example, citation-based summarization systems (Qazvinian and Radev, 2008; Qazvinian et al., 2010; AbuJbara and Radev, 2011) and survey generation systems (Mohammad et al., 2009; Qazvinian et al., 2013) can benefit from citation purpose and polarity analysis to improve paper and content selection. In this paper, we investigate the use of linguistic analysis techniques to automatically identify the purpose of citing a paper and the polarity of this citation. We first present a sequence labeling method for extracting the text that cites a given target reference; i.e. the text that appears in a scientific article and refers to another article and comments on it. We us</context>
</contexts>
<marker>Qazvinian, Radev, 2008</marker>
<rawString>Vahed Qazvinian and Dragomir R. Radev. 2008. Scientific paper summarization using citation summary networks. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 689–696, Manchester, UK, August. Coling 2008 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vahed Qazvinian</author>
<author>Dragomir R Radev</author>
</authors>
<title>Identifying non-explicit citing sentences for citation-based summarization.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>555--564</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="5850" citStr="Qazvinian and Radev (2010)" startWordPosition="944" endWordPosition="947">ing area to refer to the same concept. They define the citing area as the succession of sentences that appear around the location of a given reference in a scientific paper and have connection to it. Their algorithm starts by adding the sentence that contains the target reference as the first member sentence in the citing area. Then, they use a set of cue words and hand-crafted rules to determine whether the surrounding sentences should be added to the citing area or not. In (Nanba et al., 2000), they use their algorithm to improve citation type classification and automatic survey generation. Qazvinian and Radev (2010) addressed a similar problem. They proposed a method based on probabilistic inference to extract non-explicit citing sentences; i.e., sentences that appear around the sentence that contains the target reference and are related to it. They showed experimentally that citation-based survey generation produces better results when using both explicit and non-explicit citing sentences rather than using the explicit ones alone. In previous work, we addressed the issue of identifying the scope of a given target reference in citing sentences that contain multiple references (2012). Our definition of re</context>
</contexts>
<marker>Qazvinian, Radev, 2010</marker>
<rawString>Vahed Qazvinian and Dragomir R. Radev. 2010. Identifying non-explicit citing sentences for citation-based summarization. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 555–564, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vahed Qazvinian</author>
<author>Dragomir R Radev</author>
<author>Arzucan Ozgur</author>
</authors>
<title>Citation summarization through keyphrase extraction.</title>
<date>2010</date>
<journal>Organizing Committee.</journal>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (Coling</booktitle>
<pages>895--903</pages>
<location>Beijing, China,</location>
<contexts>
<context position="3374" citStr="Qazvinian et al., 2010" startWordPosition="528" endWordPosition="531">h between positive, negative, and neutral citations and to identify the purpose of a citation; i.e. the author’s intention behind choosing a published article and citing it. This analysis of citation purpose and polarity can be useful for many applications. For example, it can be used to build systems that help funding agencies and hiring committees at universities and research institutions evaluate researchers’ work more accurately. It can also be used as a preprocessing step in systems that process scholarly data. For example, citation-based summarization systems (Qazvinian and Radev, 2008; Qazvinian et al., 2010; AbuJbara and Radev, 2011) and survey generation systems (Mohammad et al., 2009; Qazvinian et al., 2013) can benefit from citation purpose and polarity analysis to improve paper and content selection. In this paper, we investigate the use of linguistic analysis techniques to automatically identify the purpose of citing a paper and the polarity of this citation. We first present a sequence labeling method for extracting the text that cites a given target reference; i.e. the text that appears in a scientific article and refers to another article and comments on it. We use the term citation cont</context>
</contexts>
<marker>Qazvinian, Radev, Ozgur, 2010</marker>
<rawString>Vahed Qazvinian, Dragomir R. Radev, and Arzucan Ozgur. 2010. Citation summarization through keyphrase extraction. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 895–903, Beijing, China, August. Coling 2010 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vahed Qazvinian</author>
<author>Dragomir R Radev</author>
<author>Saif Mohammad</author>
<author>Bonnie Dorr</author>
<author>David Zajic</author>
<author>Michael Whidby</author>
<author>Taesun Moon</author>
</authors>
<title>Generating extractive summaries of scientific paradigms.</title>
<date>2013</date>
<journal>Journal of Artificial Intelligence Research.</journal>
<contexts>
<context position="3479" citStr="Qazvinian et al., 2013" startWordPosition="546" endWordPosition="549">author’s intention behind choosing a published article and citing it. This analysis of citation purpose and polarity can be useful for many applications. For example, it can be used to build systems that help funding agencies and hiring committees at universities and research institutions evaluate researchers’ work more accurately. It can also be used as a preprocessing step in systems that process scholarly data. For example, citation-based summarization systems (Qazvinian and Radev, 2008; Qazvinian et al., 2010; AbuJbara and Radev, 2011) and survey generation systems (Mohammad et al., 2009; Qazvinian et al., 2013) can benefit from citation purpose and polarity analysis to improve paper and content selection. In this paper, we investigate the use of linguistic analysis techniques to automatically identify the purpose of citing a paper and the polarity of this citation. We first present a sequence labeling method for extracting the text that cites a given target reference; i.e. the text that appears in a scientific article and refers to another article and comments on it. We use the term citation context to refer to this text. Next, 596 Proceedings of NAACL-HLT 2013, pages 596–606, Atlanta, Georgia, 9–14</context>
</contexts>
<marker>Qazvinian, Radev, Mohammad, Dorr, Zajic, Whidby, Moon, 2013</marker>
<rawString>Vahed Qazvinian, Dragomir R. Radev, Saif Mohammad, Bonnie Dorr, David Zajic, Michael Whidby, and Taesun Moon. 2013. Generating extractive summaries of scientific paradigms. Journal of Artificial Intelligence Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Randolph Quirk</author>
<author>Sidney Greenbaum</author>
<author>Geoffrey Leech</author>
<author>Jan Svartvik</author>
</authors>
<title>A Comprehensive Grammar of the English Language.</title>
<date>1985</date>
<location>Longman, London.</location>
<contexts>
<context position="22983" citStr="Quirk et al. (1985)" startWordPosition="3757" endWordPosition="3760">erence or its representative or any mention of it. Distance is measure based on the shortest path in the dependency tree. Self Citation Whether the citation from the source paper to the target reference is a self citation. Contains 1st/3rd PP Whether the citation context contains a first/third person pronoun. Negation Whether the citation context contains a negation cue. The list of negation cues is taken from the training data of the *SEM 2012 negation detection shared task (Morante and Blanco, 2012). Speculation Whether the citation context contains a speculation cue. The list is taken from Quirk et al. (1985) Closest Subjectivity Cue The closest subjectivity cue to the target reference or its representative or any anaphoric mention of it. The list of cues is taken from OpinionFinder (Wilson et al., 2005) Contrary Expressions Whether the citation context contains a contrary expression. The list is taken from Biber (1988) Section The headline of the section in which the citation appears. We identify five title categorizes: 1) Introduction, Motivation, etc. 2) Background, Prior Work, Previous Work, etc. 3) Experiments, Data, Results, Evaluation, etc. 4) Discussion, Conclusion, Future work, etc.. 5) A</context>
</contexts>
<marker>Quirk, Greenbaum, Leech, Svartvik, 1985</marker>
<rawString>Randolph Quirk, Sidney Greenbaum, Geoffrey Leech, and Jan Svartvik. 1985. A Comprehensive Grammar of the English Language. Longman, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir R Radev</author>
<author>Pradeep Muthukrishnan</author>
<author>Vahed Qazvinian</author>
</authors>
<title>The acl anthology network corpus.</title>
<date>2009</date>
<booktitle>In NLPIR4DL ’09: Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries,</booktitle>
<pages>54--61</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="24335" citStr="Radev et al., 2009" startWordPosition="3961" endWordPosition="3964"> appear in the citation context. For example, nsubj(outperform, algorithm) is one of the relations extracted from ”This algorithm outperforms the one proposed by...”. The arguments of the dependency relation are replaced by their lemmatized forms. This type of features has been shown to give good results in similar tasks (Athar and Teufel, 2012a). Table 3: The features used for citation purpose and polarity classification 4 Evaluation In this section, we describe the data that we used for evaluation and the experiments that we conducted. 4.1 Data We use the ACL Anthology Network corpus (AAN) (Radev et al., 2009; Radev et al., 2013) in our evaluation. AAN is a publicly available collection of more than 19,000 NLP papers. It includes a manually curated citation network of its papers as well as the full text of the papers and the citing sentences associated with each edge in the citation network. From this set, we selected 30 papers that have different numbers of incoming citations and that were consistently cited since they were published. These 30 papers received a total of about 3,500 citations from within AAN (average = 115 citation/paper, Min = 30, and Max = 338). These citations come from 1,493 u</context>
</contexts>
<marker>Radev, Muthukrishnan, Qazvinian, 2009</marker>
<rawString>Dragomir R. Radev, Pradeep Muthukrishnan, and Vahed Qazvinian. 2009. The acl anthology network corpus. In NLPIR4DL ’09: Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries, pages 54–61, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir R Radev</author>
<author>Pradeep Muthukrishnan</author>
<author>Vahed Qazvinian</author>
<author>Amjad Abu-Jbara</author>
</authors>
<title>The acl anthology network corpus. Language Resources and Evaluation,</title>
<date>2013</date>
<pages>1--26</pages>
<contexts>
<context position="24356" citStr="Radev et al., 2013" startWordPosition="3965" endWordPosition="3968">ion context. For example, nsubj(outperform, algorithm) is one of the relations extracted from ”This algorithm outperforms the one proposed by...”. The arguments of the dependency relation are replaced by their lemmatized forms. This type of features has been shown to give good results in similar tasks (Athar and Teufel, 2012a). Table 3: The features used for citation purpose and polarity classification 4 Evaluation In this section, we describe the data that we used for evaluation and the experiments that we conducted. 4.1 Data We use the ACL Anthology Network corpus (AAN) (Radev et al., 2009; Radev et al., 2013) in our evaluation. AAN is a publicly available collection of more than 19,000 NLP papers. It includes a manually curated citation network of its papers as well as the full text of the papers and the citing sentences associated with each edge in the citation network. From this set, we selected 30 papers that have different numbers of incoming citations and that were consistently cited since they were published. These 30 papers received a total of about 3,500 citations from within AAN (average = 115 citation/paper, Min = 30, and Max = 338). These citations come from 1,493 unique papers. For eac</context>
</contexts>
<marker>Radev, Muthukrishnan, Qazvinian, Abu-Jbara, 2013</marker>
<rawString>Dragomir R. Radev, Pradeep Muthukrishnan, Vahed Qazvinian, and Amjad Abu-Jbara. 2013. The acl anthology network corpus. Language Resources and Evaluation, pages 1–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ina Spiegel-R¨osing</author>
</authors>
<title>Science Studies: Bibliometric and Content Analysis.</title>
<date>1977</date>
<journal>Social Studies of Science,</journal>
<volume>7</volume>
<issue>1</issue>
<marker>Spiegel-R¨osing, 1977</marker>
<rawString>Ina Spiegel-R¨osing. 1977. Science Studies: Bibliometric and Content Analysis. Social Studies of Science, 7(1):97–113, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Teufel</author>
<author>Advaith Siddharthan</author>
<author>Dan Tidhar</author>
</authors>
<title>Automatic classification of citation function. In</title>
<date>2006</date>
<booktitle>In Proc. of EMNLP-06.</booktitle>
<contexts>
<context position="8255" citStr="Teufel et al. (2006)" startWordPosition="1311" endWordPosition="1314">our volumes of Science Studies. Some of them are: Cited source is the specific point of departure for the research question investigated, Cited source contains the concepts, definitions, interpretations used, Cited source contains the data used by the citing paper. Nanba and Okumura (1999) came up with a simple schema composed of only three categories: Basis, Comparison, and other Other. They proposed a rule-based method that uses a set of statistically selected cue words to determine the category of a citation. They used this classification as a first step for scientific paper summarization. Teufel et al. (2006), in their work on citation function classification, adopted 12 categories from Spiegel-Rosing’s taxonomy. They trained an SVM classifier and used it to label each citing sentence with exactly one category. Further, they mapped the twelve categories to four top level categories namely: weakness, contrast 597 (4 categories), positive (6 categories) and neutral. The taxonomy that we use in this work is based on previous work. We adopt a scheme that contains six categories. We selected the six categories after studying all the previously used citation taxonomies. We included the ones we believed </context>
<context position="19316" citStr="Teufel et al., 2006" startWordPosition="3140" endWordPosition="3143">ed as ”basis” when the author uses the Our model is derived from the hidden-markov model for cited work as starting point or motivation and extends on the cited word alignment (Vogel et al., 1996; Och and Ney, 2000). work. Neutral (Other) A citing sentence is classified as ”neutral” when it is a neutral The solutions of these problems depend heavily on the description of the cited work or if it doesn’t come under any of quality of the word alignment (Och and Ney, 2000). the above categories. Table 2: Annotation scheme for citation purpose. Motivated by the work of (Spiegel-R¨osing, 1977) and (Teufel et al., 2006) 3.3 Citation Purpose Classification In this section, we describe the citation purpose classification task. Given a target paper B and its citation context (extracted using the method described above) in a given article A, we want to determine the purpose of citing B by A. The purpose is defined as intention behind selecting B and citing it by the author of A (Garfield, 1964). We use a taxonomy that consists of six categories. We designed this taxonomy based on our study of similar taxonomies proposed in previous work. We selected the categories that we believe are more important and useful fr</context>
</contexts>
<marker>Teufel, Siddharthan, Tidhar, 2006</marker>
<rawString>Simone Teufel, Advaith Siddharthan, and Dan Tidhar. 2006. Automatic classification of citation function. In In Proc. of EMNLP-06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>GEOFF THOMPSON</author>
<author>YE YIYUN</author>
</authors>
<title>Evaluation in the reporting verbs used in academic papers.</title>
<date>1991</date>
<journal>Applied Linguistics,</journal>
<volume>12</volume>
<issue>4</issue>
<contexts>
<context position="9323" citStr="THOMPSON and YIYUN, 1991" startWordPosition="1478" endWordPosition="1481">me that contains six categories. We selected the six categories after studying all the previously used citation taxonomies. We included the ones we believed are important for improving bibliometric measures and for the applications that we are planning to pursue in the future (Section 5). 2.3 Citation Polarity Classification The polarity (or sentiment) of a citation has also been studied previously. Previous work showed that positive and negative citations are common, although negative citations might be expressed indirectly or in an implicit way (Ziman, 1968; MacRoberts and MacRoberts, 1984; THOMPSON and YIYUN, 1991). Athar (2011) addressed the problem of identifying sentiment in citing sentences. He used a set of structure-based features to train a machine learning classifier using annotated data. This work uses the citing sentence only to predict sentiment. Context sentences were ignored. Athar and Teufel (2012a) observed that taking the context into consideration when judging sentiment in citations increases the number of negative citations by a factor of 3. They proposed two methods for utilizing the context. In the first method, they treat the citing sentence and a fixed context (a window of four sen</context>
</contexts>
<marker>THOMPSON, YIYUN, 1991</marker>
<rawString>GEOFF THOMPSON and YE YIYUN. 1991. Evaluation in the reporting verbs used in academic papers. Applied Linguistics, 12(4):365–382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Melvin Weinstock</author>
</authors>
<title>Citation Indexes.</title>
<date>1971</date>
<journal>Encyclopedia of Library and Information Science.</journal>
<contexts>
<context position="7006" citStr="Weinstock, 1971" startWordPosition="1117" endWordPosition="1118">at contain multiple references (2012). Our definition of reference scope was limited to fragments of the explicit citing sentence (i.e. the sentence in which actual citation appears). That method does not identify related text in surrounding sentences. In this work, we propose a supervised sequence labeling method for identifying the citation context of given reference which includes the explicit citing sentence and the related surrounding sentences. 2.2 Citation Purpose Classification Several research efforts have focused on studying the different purposes for citing a paper (Garfield, 1964; Weinstock, 1971; Moravcsik and Murugesan, 1975; and Moitra, 1975; Bonzi, 1982). Bonzi (1982) studied the characteristics of citing and cited works that may aid in determining the relatedness between them. Garfield (1964) enumerated several reasons why authors cite other publications, including “alerting researchers to forthcoming work”, paying homage to the leading scholars in the area, and citations which provide pointers to background readings. Weinstock (1971) adopted the same scheme that Garfield proposed in her study of citations. Spiegel-Rosing (1977) proposed 13 categories for citation purpose based o</context>
</contexts>
<marker>Weinstock, 1971</marker>
<rawString>Melvin Weinstock. 1971. Citation Indexes. Encyclopedia of Library and Information Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Alan Whidby</author>
</authors>
<title>Citation handling: Processing citation text in scientific documents. In Master Thesis.</title>
<date>2012</date>
<contexts>
<context position="12994" citStr="Whidby, 2012" startWordPosition="2080" endWordPosition="2081">erence with a placeholder. The reference to the target paper is replaced by the placeholder TREF. Each other reference is replaced by REF. b. Reference Grouping: In this step, we identify grouped references (i.e. multiple references listed between one pair of parentheses separated by semicolons). Each such group is replaced by a placeholder, GREF. If the target reference is a member of the group, we use a different placeholder: GTREF. c. Non-syntactic Reference Removal: A reference or a group of references could either be a syntactic constituent and has a semantic role in the sentence or not (Whidby, 2012; Abu Jbara and Radev, 2012). If the reference is not a syntactic compo598 Feature Description Demonstrative determiners Takes a value of 1 if the current sentence contains contains a demonstrative determiner (this, these, etc.), and 0 otherwise. Conjunctive adverbs Takes a value of 1 if the current sentence starts with a conjunctive adverb (However, Furthermore, Accordingly, etc.), and 0 otherwise. Position Position of the current sentence with respect to the citing sentence. This feature takes one of four values: -1, 0, 1, and 2. Contains Closest Noun Phrase Takes a value of 1 if the current</context>
</contexts>
<marker>Whidby, 2012</marker>
<rawString>Michael Alan Whidby. 2012. Citation handling: Processing citation text in scientific documents. In Master Thesis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Howard D White</author>
</authors>
<title>Citation analysis and discourse analysis revisited.</title>
<date>2004</date>
<journal>Applied Linguistics,</journal>
<volume>25</volume>
<issue>1</issue>
<contexts>
<context position="4737" citStr="White (2004)" startWordPosition="756" endWordPosition="757">al Linguistics we use supervised classification techniques to analyze this text and identify the purpose and polarity of citation. The rest of this paper is organized as follows. Section 2 reviews the related work. We present our approach in Section 3. We then describe the data and experiments in Section 4. Finally, Section 5 concludes the paper and suggests directions for future work. 2 Related Work Our work is related to a large body of research on citations. Studying citation patterns and referencing practices has interested researchers for many years (Hodges, 1972; Garfield et al., 1984). White (2004) provides a good survey of the different research directions that study or use citations. In the following subsections, we review three lines of research that are closely related to our work. 2.1 Citation Context Identification The first line of related research addresses the problem of identifying citation context. The context of a citation that cites a given target paper can be a set of sentences, one sentence, or a fragment of a sentence. Nanba and Okumura (1999) use the term citing area to refer to the same concept. They define the citing area as the succession of sentences that appear aro</context>
</contexts>
<marker>White, 2004</marker>
<rawString>Howard D. White. 2004. Citation analysis and discourse analysis revisited. Applied Linguistics, 25(1):89–116.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Paul Hoffmann</author>
<author>Swapna Somasundaran</author>
<author>Jason Kessler</author>
<author>Janyce Wiebe</author>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
<author>Ellen Riloff</author>
<author>Siddharth Patwardhan</author>
</authors>
<title>Opinionfinder: a system for subjectivity analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP on Interactive Demonstrations, HLT-Demo ’05,</booktitle>
<pages>34--35</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="23182" citStr="Wilson et al., 2005" startWordPosition="3789" endWordPosition="3792">ence is a self citation. Contains 1st/3rd PP Whether the citation context contains a first/third person pronoun. Negation Whether the citation context contains a negation cue. The list of negation cues is taken from the training data of the *SEM 2012 negation detection shared task (Morante and Blanco, 2012). Speculation Whether the citation context contains a speculation cue. The list is taken from Quirk et al. (1985) Closest Subjectivity Cue The closest subjectivity cue to the target reference or its representative or any anaphoric mention of it. The list of cues is taken from OpinionFinder (Wilson et al., 2005) Contrary Expressions Whether the citation context contains a contrary expression. The list is taken from Biber (1988) Section The headline of the section in which the citation appears. We identify five title categorizes: 1) Introduction, Motivation, etc. 2) Background, Prior Work, Previous Work, etc. 3) Experiments, Data, Results, Evaluation, etc. 4) Discussion, Conclusion, Future work, etc.. 5) All other section headlines. Headlines are identified using regular expressions. Dependency Relations All the dependency relations that appear in the citation context. For example, nsubj(outperform, a</context>
</contexts>
<marker>Wilson, Hoffmann, Somasundaran, Kessler, Wiebe, Choi, Cardie, Riloff, Patwardhan, 2005</marker>
<rawString>Theresa Wilson, Paul Hoffmann, Swapna Somasundaran, Jason Kessler, Janyce Wiebe, Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth Patwardhan. 2005. Opinionfinder: a system for subjectivity analysis. In Proceedings of HLT/EMNLP on Interactive Demonstrations, HLT-Demo ’05, pages 34–35, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Ziman</author>
</authors>
<title>Public knowledge: An essay concerning the social dimension of science.</title>
<date>1968</date>
<location>Cambridge U.P., London.</location>
<contexts>
<context position="9263" citStr="Ziman, 1968" startWordPosition="1471" endWordPosition="1472">ork is based on previous work. We adopt a scheme that contains six categories. We selected the six categories after studying all the previously used citation taxonomies. We included the ones we believed are important for improving bibliometric measures and for the applications that we are planning to pursue in the future (Section 5). 2.3 Citation Polarity Classification The polarity (or sentiment) of a citation has also been studied previously. Previous work showed that positive and negative citations are common, although negative citations might be expressed indirectly or in an implicit way (Ziman, 1968; MacRoberts and MacRoberts, 1984; THOMPSON and YIYUN, 1991). Athar (2011) addressed the problem of identifying sentiment in citing sentences. He used a set of structure-based features to train a machine learning classifier using annotated data. This work uses the citing sentence only to predict sentiment. Context sentences were ignored. Athar and Teufel (2012a) observed that taking the context into consideration when judging sentiment in citations increases the number of negative citations by a factor of 3. They proposed two methods for utilizing the context. In the first method, they treat t</context>
</contexts>
<marker>Ziman, 1968</marker>
<rawString>J. M. Ziman. 1968. Public knowledge: An essay concerning the social dimension of science. Cambridge U.P., London.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>