<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000182">
<title confidence="0.9988495">
An SVM Based Voting Algorithm
with Application to Parse Reranking
</title>
<author confidence="0.992883">
Libin Shen and Aravind K. Joshi
</author>
<affiliation confidence="0.875896">
Department of Computer and Information Science
University of Pennsylvania
Philadelphia, PA 19104, USA
</affiliation>
<email confidence="0.999066">
{libin,joshi}@linc.cis.upenn.edu
</email>
<sectionHeader confidence="0.995647" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9995088">
This paper introduces a novel Support Vec-
tor Machines (SVMs) based voting algorithm
for reranking, which provides a way to solve
the sequential models indirectly. We have
presented a risk formulation under the PAC
framework for this voting algorithm. We have
applied this algorithm to the parse reranking
problem, and achieved labeled recall and pre-
cision of 89.4%/89.8% on WSJ section 23 of
Penn Treebank.
</bodyText>
<sectionHeader confidence="0.998979" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999711272727273">
Support Vector Machines (SVMs) have been successfully
used in many machine learning tasks. Unlike the error-
driven algorithms, SVMs search for the hyperplane that
separates a set of training samples that contain two dis-
tinct classes and maximizes the margin between these
two classes. The ability to maximize the margin is be-
lieved to be the reason for SVMs’ superiority over other
classifiers. In addition, SVMs can achieve high perfor-
mance even with input data of high dimensional feature
space, especially because of the use of the ”kernel trick”.
However, the incorporation of SVMs into sequential
models remains a problem. An obvious reason is that
the output of an SVM is the distance to the separating
hyperplane, but not a probability. A possible solution
to this problem is to map SVMs’ results into probabili-
ties through a Sigmoid function, and use Viterbi search
to combine those probabilities (Platt, 1999). However,
this approach conflicts with SVMs’ purpose of achieving
the so-called global optimization1. First, this approach
may constrain SVMs to local features because of the left-
to-right scanning strategy. Furthermore, like other non-
generative Markov models, it suffers from the so-called
</bodyText>
<footnote confidence="0.9054015">
1By global we mean the use of quadratic optimization in
margin maximization.
</footnote>
<bodyText confidence="0.999774976190476">
label bias problem, which means that the transitions leav-
ing a given state compete only against each other, rather
than against all other transitions in the model (Lafferty et
al., 2001). Intuitively, it is the local normalization that
results in the label bias problem.
One way of using discriminative machine learning al-
gorithms in sequential models is to rerank the n-best out-
puts of a generative system. Reranking uses global fea-
tures as well as local features, and does not make lo-
cal normalization. If the output set is large enough, the
reranking approach may help to alleviate the impact of
the label bias problem, because the victim parses (i.e.
those parses which get penalized due to the label bias
problem) will have a chance to take part in the rerank-
ing.
In recent years, reranking techniques have been suc-
cessfully applied to the so-called history-based models
(Black et al., 1993), especially to parsing (Collins, 2000;
Collins and Duffy, 2002). In a history-based model, the
current decision depends on the decisions made previ-
ously. Therefore, we may regard parsing as a special form
of sequential model without losing generality.
Collins (2000) has proposed two reranking algorithms
to rerank the output of an existing parser (Collins, 1999,
Model 2). One is based on Markov Random Fields, and
the other is based on a boosting approach. In (Collins and
Duffy, 2002), the use of Voted Perceptron (VP) (Freund
and Schapire, 1999) for the parse reranking problem has
been described. In that paper, the tree kernel (Collins
and Duffy, 2001) has been used to efficiently count the
number of common subtrees as described in (Bod, 1998).
In this paper we will follow the reranking approach.
We describe a novel SVM-based voting algorithm for
reranking. It provides an alternative way of using a large
margin classifier for sequential models. Instead of using
the parse tree itself as a training sample, we use a pair of
parse trees as a sample, which is analogous to the pref-
erence relation used in the context of ordinal regression
(Herbrich et al., 2000). Furthermore, we justify the al-
gorithm through a modification of the proof of the large
margin rank boundaries for ordinal regression. We then
apply this algorithm to the parse reranking problem.
</bodyText>
<subsectionHeader confidence="0.994489">
1.1 A Short Introduction of SVMs
</subsectionHeader>
<bodyText confidence="0.988701125">
In this section, we give a short introduction of Support
Vector Machines. We follow (Vapnik, 1998)’s definition
of SVMs. For each training sample (yi, xi), yi repre-
sents its class, and xi represents its input vector defined
on a d-dimensional space. Suppose the training samples
{(y1, x1), ..., (yn, xn)} (xi ∈ Rd, yi ∈ {−1, 1}) can be
separated by a hyperplane H: (x • w) + b = 0, which
means
</bodyText>
<equation confidence="0.993955">
yi((xi • w) + b) ≥ 1, (1)
</equation>
<bodyText confidence="0.9975867">
where w is normal to the hyperplane. To train an SVM is
equivalent to searching for the optimal separating hyper-
plane that separates the training data without error and
maximizes the margin between two classes of samples. It
can be shown that maximizing the margin is equivalent to
minimizing ||w||2.
In order to handle linearly non-separable cases, we
introduce a positive slack variable Si for each sample
(yi, xi). Then training can be reduced to the following
Quadratic Programming (QP) problem.
</bodyText>
<equation confidence="0.923462666666667">
Maximize:
yiyjαiαj(xi • xj) (2)
subject to: 0 ≤ αi ≤ C and Pi αiyi = 0,
</equation>
<bodyText confidence="0.9963286875">
where αi(i = 1...l) are the Lagrange multipliers, l is the
total number of training samples, and C is a weighting
parameter for mis-classification.
Since linearly non-separable samples may become sep-
arable in a high-dimensional space, SVMs employ the
”kernel trick” to implicitly separate training samples in
a high-dimensional feature space. Let Φ : Rd 7→ Rh
be a function that maps a d-dimensional input vector
x to an h-dimensional feature vector Φ(x). In order
to search for the optimal separating hyperplane in the
higher-dimensional feature space, we only need to sub-
stitute Φ(xi) • Φ(xj) with xi • xj in formula (2).
If there is a function K, such that K(xi, xj) = Φ(xi)•
Φ(xj), we don’t need to compute Φ(xi) explicitly. K is
called a kernel function. Thus during the training phase
we need to solve the following QP problem.
</bodyText>
<equation confidence="0.991402285714286">
Maximize:
yiyjαiαjK(xi,xj). (3)
subject to: 0 ≤ αi ≤ C, and Pi αiyi = 0.
Let x be a test vector, the decision function is
Ns
f(x) = sgn( X αjyjK(sj, x) + b) (4)
j=1
</equation>
<bodyText confidence="0.9964746">
where sj is a training vector whose corresponding La-
grange multiplier αj &gt; 0. sj is called a support vector.
Ns is the total number of the support vectors. According
to (4), the decision function only depends on the support
vectors.
It is worth noting that not any function K can be used
as a kernel. We call function K : Rd × Rd 7→ R
a well-defined kernel if and only if there is a mapping
function Φ : Rd 7→ Rh such that, for any xi, xj ∈ Rd,
K(xi, xj) = Φ(xi) • Φ(xj). One way of testing whether
a function is a well-defined kernel is to use the Mer-
cer’s theorem (Vapnik, 1998) by utilizing the positive
semidefinteness property. However, as far as a discrete
kernel is concerned, there is a more convenient way to
show that a function is a well-defined kernel. This is
achieved by showing that a function K is a kernel by find-
ing the corresponding mapping function Φ. This method
was used in the proof of the string subsequence kernel
(Cristianini and Shawe-Tayor, 2000) and the tree kernel
(Collins and Duffy, 2001).
</bodyText>
<subsectionHeader confidence="0.997149">
1.2 Large Margin Classifiers
</subsectionHeader>
<bodyText confidence="0.99989047826087">
SVMs are called large margin classifiers because they
search for the hyperplane that maximizes the margin. The
validity of the large margin method is guaranteed by the
theorems of Structural Risk Minimization (SRM) under
Probably Approximately Correct (PAC) framework2; test
error is related to training data error, number of training
samples and the capacity of the learning machine (Smola
et al., 2000).
Vapnik-Chervonenkis (VC) dimension (Vapnik, 1999),
as well as some other measures, is used to estimate the
complexity of the hypothesis space, or the capacity of the
learning machine. The drawback of VC dimension is that
it ignores the structure of the mapping from training sam-
ples to hypotheses, and concentrates solely on the range
of the possible outputs of the learning machine (Smola
et al., 2000). In this paper we will use another measure,
the so-called Fat Shattering Dimension (Shawe-Taylor et
al., 1998), which is shown to be more accurate than VC
dimension (Smola et al., 2000), to justify our voting al-
gorithm,
Let F be a family of hypothesis functions. The fat
shattering dimension of F is a function from margin p to
the maximum number of samples such that any subset of
</bodyText>
<footnote confidence="0.727814">
2SVM’s theoretical accuracy is much lower than their actual
performance. The ability to maximize the margin is believed to
be the reason for SVMs’ superiority over other classifiers.
</footnote>
<equation confidence="0.866234125">
1
LD(α) ≡
2
αi −
Xt
i=1
Xt
i,j=1
1
LD(α) ≡
2
αi −
Xt
i=1
Xt
i,j=1
</equation>
<bodyText confidence="0.893005166666666">
these samples can be classified with margin ρ by a func-
tion in F. An upper bound of the expected error is given
in Theorem 1 below (Shawe-Taylor et al., 1998). We will
use this theorem to justify the new voting algorithm.
Theorem 1 Consider a real-valued function class F
having fat-shattering function bounded above by the
function afat : R → N which is continuous from the
right. Fix 0 E R. If a learner correctly classifies m
independently generated examples z with h = Tθ(f) E
Tθ(F) such that erz(h) = 0 and ρ = min |f(xi) − 0|,
then with confidence 1 − δ the expected error of h is
boundedfrom above by
</bodyText>
<equation confidence="0.940404">
2 (k log(8em k ) log(32m) + log(8m δ )) (5)
m
</equation>
<bodyText confidence="0.96594">
where k = afat(ρ/8).
</bodyText>
<sectionHeader confidence="0.925425" genericHeader="method">
2 A New SVM-based Voting Algorithm
</sectionHeader>
<bodyText confidence="0.999902848484849">
Let xij be the jth candidate parse for the ith sentence in
training data. Let xi1 is the parse with the highest f-score
among all the parses for the ith sentence.
We may take xi1 as positive samples, and xij(j&gt;1) as
negative samples. However, experiments have shown that
this is not the best way to utilize SVMs in reranking (Di-
jkstra, 2001). A trick to be used here is to take a pair of
parses as a sample: for any i and j &gt; 1, (xi1, xij) is a
positive sample, and (xij, xi1) is a negative sample.
Similar idea was employed in the early works of parse
reranking. In the boosting algorithm of (Collins, 2000),
for each sample (parse) xij, its margin is defined as
F(xi1, ¯α) − F(xij, ¯α), where F is a score function and
α¯ is the parameter vector. In (Collins and Duffy, 2002),
for each offending parse, the parameter vector updating
function is in the form of w = w + h(xi1) − h(xij),
where w is the parameter vector and h returns the feature
vector of a parse. But neither of these two papers used a
pair of parses as a sample and defined functions on pairs
of parses. Furthermore, the advantage of using difference
between parses was not theoretically clarified, which we
will describe in the next section.
As far as SVMs are concerned, the use of parses or
pairs of parses both maximize the margin between xi1
and xij, but the one using a single parse as a sample
needs to satisfy some extra constraints on the selection
of decision function. However these constraints are not
necessary (see section 3.3). Therefore the use of pairs of
parses has both theoretic and practical advantages.
Now we need to define the kernel on pairs of parses.
Let (t1, t2), (v1, v2) are two pairs of parses. Let K is
any kernel function on the space of single parses. The
preference kernel PK is defined on K as follows.
</bodyText>
<equation confidence="0.997968">
PK((t1, t2), (v1, v2)) = K(t1, v1) − K(t1, v2)
−K(t2,v1) + K(t2,v2) (6)
</equation>
<bodyText confidence="0.999983666666667">
The preference kernel of this form was previously used
in the context of ordinal regression in (Herbrich et al.,
2000). Then the decision function is
</bodyText>
<equation confidence="0.990736444444445">
Ng
f((xj, xk)) = αiyiPK((si1, si2), (xj, xk)) + b
i=1
Ng
= b + ( αiyi(K(si1,xj) − K(si2,xj)))
i=1
Ng
−( αiyi(K(si1, xk) − K(si2, xk))),
i=1
</equation>
<bodyText confidence="0.999951166666667">
where xj and xk are two distinct parses of a sentence,
(si1, si2) is the ith support vector, and Ns is the total
number of support vectors.
As we have defined them, the training samples are
symmetric with respect to the origin in the space. There-
fore, for any hyperplane that does not pass through the
origin, we can always find a parallel hyperplane that
crosses the origin and makes the margin larger. Hence,
the outcome separating hyperplane has to pass through
the origin, which means that b = 0.
Therefore, for each test parse x, we only need to com-
pute its score as follows.
</bodyText>
<equation confidence="0.9844552">
Ng
score(x) = αiyi(K(si1, x) − K(si2, x)), (7)
i=1
because
f((xj,xk)) = score(xj) − score(xk). (8)
</equation>
<subsectionHeader confidence="0.743379">
2.1 Kernels
</subsectionHeader>
<bodyText confidence="0.999845538461538">
In (6), the preference kernel PK is defined on kernel K.
K can be any possible kernel. We will show that PK is
well-defined in the next section. In this paper, we con-
sider two kernels for K, the linear kernel and the tree
kernel.
In (Collins, 2000), each parse is associated with a set
of features. Linear combination of the features is used
in the decision function. As far as SVM is concerned,
we may encode the features of each parse with a vec-
tor. Dot product is used as the kernel K. Let u and v
are two parses. The computational complexity of linear
kernel O(|fu |* |fv|), where |fu |and |fv |are the length
of the vectors associated with parse u and v respectively.
The goodness of the linear kernel is that it runs very fast
in the test phase, because coefficients of the support vec-
tors can be combined in advance. For a test parse x, the
computational complexity of test is only O(|fx|), which
is independent with the number of the support vectors.
In (Collins and Duffy, 2002), the tree kernel Tr is used
to count the total number of common sub-trees of two
parse trees. Let u and v be two trees. Because Tr can be
computed by dynamic programming, the computational
complexity of Tr(u, v) is O(|u |∗ |v|), where |u |and |v|
are the tree sizes of u and v respectively. For a test parse
x, the computational complexity of the test is O(S ∗ |x|),
where S is the number of support vectors.
</bodyText>
<sectionHeader confidence="0.875794" genericHeader="method">
3 Justifying the Algorithm
</sectionHeader>
<subsectionHeader confidence="0.923783">
3.1 Justifying the Kernel
</subsectionHeader>
<bodyText confidence="0.986069333333333">
Firstly, we show that the preference kernel PK defined
above is well-defined. Suppose kernel K is defined on
T × T. So there exists Φ : T 7→ H, such that
K(x1, x2) = Φ(x1) • Φ(x2) for any x1, x2 ∈ T.
It suffices to show that there exist space H0 and
mapping function Φ0 : T × T → H0 such that
PK((t1, t2), (v1, v2)) = Φ0(t1, t2) • Φ0(t1, t2), where
t1, t2, v1, v2 ∈ T.
According to the definition of PK, we have
</bodyText>
<equation confidence="0.989166">
PK((t1,t2), (v1, v2))
= K(t1, v1) − K(t1, v2) − K(t2, v1) + K(t2, v2)
= Φ(t1) • Φ(v1) − Φ(t1) • Φ(v2)
−Φ(t2) • Φ(v1) + Φ(t2) • Φ(v2)
= (Φ(t1) − Φ(t2)) • (Φ(v1) − Φ(v2)), (9)
Let H0 = H and Φ0(x1, x2) = Φ(x1) − Φ(x2). Hence
kernel PK is well-defined.
</equation>
<subsectionHeader confidence="0.922714">
3.2 Margin Bound for SVM-based Voting
</subsectionHeader>
<bodyText confidence="0.999381777777778">
We will show that the expected error of voting is bounded
from above in the PAC framework. The approach used
here is analogous to the proof of ordinal regression (Her-
brich et al., 2000). The key idea is to show the equiva-
lence of the voting risk and the classification risk.
Let X be the set of all parse trees. For each x ∈ X,
let x¯ be the best parse for the sentence related to x. Thus
the appropriate loss function for the voting problem is as
follows.
</bodyText>
<equation confidence="0.967519">
~ 1 if f(¯x) &lt; f(x)
lvote(x, f) ≡ (10)
0 otherwise
</equation>
<bodyText confidence="0.999117">
where f is a parse scoring function.
Let E = {(x, ¯x)|x ∈ X} ∪ {(¯x,x)|x ∈ X}. E
is the space of event of the classification problem, and
PrE((x,¯x)) = PrE((¯x,x)) = 12PrX(x). For any parse
scoring function f, let gf(x1, x2) ≡ sgn(f(x1)−f(x2)).
For classifier gf on space E, its loss function is
</bodyText>
<equation confidence="0.8815098">
lclass(x1,x2,gf) ≡ ⎧ 1 if x1 = ¯x2 and
⎨⎪⎪⎪⎪ gf(x1,x2) = −1
⎪⎪⎪⎪⎩ 1 if x2 = ¯x1 and
gf(x1, x2) = +1
0 otherwise
</equation>
<bodyText confidence="0.999618">
Therefore the expected risk Rvote(f) for the voting
problem is equivalent to the expected risk Rclass(gf) for
the classification problem.
</bodyText>
<equation confidence="0.9967486">
Rvote(f)
= Ex∈X(lvote(x,f))
= E(x,¯x)∈E(lvote(x,f)) + E(¯x,x)∈E(lvote(x,f))
= E(x1,x2)∈E(lclass(x1,x2,gf))
= Rclass(gf) (11)
</equation>
<bodyText confidence="0.9919176">
However, the definition of space E violates the inde-
pendently and identically distributed (iid) assumption.
Parses for the same sentence are not independent. If we
suppose that no two pairs of parses come from the same
sentence, then the idd assumption holds. In practice, the
number of sentences is very large, i.e. more than 30000.
So we may use more than one pair of parses of the same
sentence and still assume the idd property roughly, be-
cause for any two arbitrary pairs of parses, 29999 out of
30000, these two samples are independent.
Let ρ ≡ mini=1..n,j=2..mi |f(xi1) − f(xij) |=
mini=1..n,j=2..mi |g(xi1,xij)−0|. According to (11) and
Theorem 1 in section 1.2 we get the following theorem.
Theorem 2 If gf makes no error on the training data,
with confidence 1 − δ
</bodyText>
<equation confidence="0.999198">
Rvote(f) = Rclass(gf)
2 (k log(8em
≤ k ) log(32m)
m
+ log( 8m))
δ
(12)
</equation>
<bodyText confidence="0.999155">
where k = afat(ρ/8), m = Pi=1..n(mi − 1).
</bodyText>
<subsectionHeader confidence="0.999273">
3.3 Justifying Pairwise Samples
</subsectionHeader>
<bodyText confidence="0.997674384615385">
An obvious way to use SVM is to use each single parse,
instead of a pair of parse trees, as a training sample. Only
the best parse of each sentence is regarded as a positive
sample, and all the rest are regarded as negative samples.
Similar to the pairwise system, it also maximizes the mar-
gin between the best parse of a sentence and all incorrect
parses of this sentence. Suppose f is the function result-
ing from the SVM. It requires yijf(xij) &gt; 0 for each
sample (xij, yij). However this constraint is not neces-
sary. We only need to guarantee that f(xi1) &gt; f(xij).
This is the reason for using pairs of parses as training
samples instead of single parses.
We may rewrite the score function (7) as follows.
</bodyText>
<equation confidence="0.8126325">
Xscore(x) = ci,jK(si,j, x), (13)
i,j
</equation>
<bodyText confidence="0.9998308">
where i is the index for sentence, j is the index for parse,
and ∀i Pj ci,j = 0.
The format of score in (13) is the same as the deci-
sion function generated by an SVM trained on the single
parses as samples. However, there is a constraint that
the sum of the coefficients related to parses of the same
sentence is 0. So in this way we decrease the size of hy-
pothesis space based on the prior knowledge that only the
different segments of two distinct parses determine which
parse is better.
</bodyText>
<sectionHeader confidence="0.999948" genericHeader="method">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999757244444444">
The use of pairs of parse trees in our model is analogous
to the preference relation used in the ordinal regression
algorithm (Herbrich et al., 2000). In that paper, pairs of
objects have been used as training samples. For exam-
ple, let (r1, r2, ...rm) be a list of objects in the training
data, where ri ranks ith. Then pairs of objects (ri−1, ri)
are training samples. Preference kernel PK in our paper
is the same as the preference kernel in (Herbrich et al.,
2000) in format.
However, the purpose of our model is different from
that of the ordinal regression algorithm. Ordinal regres-
sion searches for a regression function for ordinal values,
while our algorithm is designed to solve a voting prob-
lem. As a result, the two algorithms differ on the def-
inition of the margin. In ordinal regression, the margin
is min |f(ri) − f(ri−1)|, where f is the regression func-
tion for ordinal values. In our algorithm, the margin is
min|score(xi1) − score(xij)|.
In (Kudo and Matsumoto, 2001), SVMs have been em-
ployed in the NP chunking task, a typical labeling prob-
lem. However, they have used a deterministic algorithm
for decoding.
In (Collins, 2000), two reranking algorithms were pro-
posed. In both of these two models, the loss functions are
computed directly on the feature space. All the features
are manually defined.
In (Collins and Duffy, 2002), the Voted Perceptron al-
gorithm was used to in parse reranking. It was shown
in (Freund and Schapire, 1999; Graepel et al., 2001) that
error bound of (voted) Perceptron is related to margins
existing in the training data, but these algorithm are not
supposed to maximize margins. Variants of the Percep-
tron algorithm, which are known as Approximate Maxi-
mal Margin classifier, such as PAM (Krauth and Mezard,
1987), ALMA (Gentile, 2001) and PAUM (Li et al.,
2002), produce decision hyperplanes within ratio of the
maximal margin. However, almost all these algorithms
are reported to be inferior to SVMs in accuracy, while
more efficient in training.
Furthermore, these variants of the Perceptron algo-
rithm take advantage of the large margin existing in the
training data. However, in NLP applications, samples are
usually inseparable even if the kernel trick is used. SVMs
can still be trained to maximize the margin through the
method of soft margin.
</bodyText>
<sectionHeader confidence="0.978399" genericHeader="evaluation">
5 Experiments and Analysis
</sectionHeader>
<bodyText confidence="0.999989979166667">
We use SVMlight (Joachims, 1998) as the SVM clas-
sifier. The soft margin parameter C is set to its default
value in SV Mlight.
We use the same data set as described in (Collins,
2000; Collins and Duffy, 2002). Section 2-21 of the Penn
WSJ Treebank (Marcus et al., 1994) are used as train-
ing data, and section 23 is used for final test. The train-
ing data contains around 40,000 sentences, each of which
has 27 distinct parses on average. Of the 40,000 training
sentences, the first 36,000 are used to train SVMs. The
remaining 4,000 sentences are used as development data.
The training complexity for SVMlight is roughly
O(n2.1) (Joachims, 1998), where n is the number of the
training samples. One solution to the scaling difficulties
is to use the Kernel Fisher Discriminant as described in
(Salomon et al., 2002). In this paper, we divide train-
ing data into slices to speed up training. Each slice con-
tains two pairs of parses from each sentence. Specifically,
slice i contains positive samples (( ˜pk, pki), +1) and neg-
ative samples ((pki, ˜pk), −1), where ˜pk is the best parse
for sentence k, pki is the parse with the ith highest log-
likelihood in all the parses for sentence k and it is not the
best parse. There are about 60000 parses in each slice in
average. For each slice, we train an SVM. Then results
of SVMs are put together with a simple combination. It
takes about 2 days to train a slice on a P3 1.13GHz pro-
cessor.
As a result of this subdivision of the training data into
slices, we cannot take advantage of SVM’s global opti-
mization ability. This seems to nullify our effort to cre-
ate this new algorithm. However, our new algorithm is
still useful for the following reasons. Firstly, with the im-
provement in the computing resources, we will be able to
use larger slices so as to utilize more global optimization.
SVMs are superior to other linear classifiers in theory. On
the other hand, the current size of the slice is large enough
for other NLP applications like text chunking, although it
is not large enough for parse reranking. The last reason
is that we have achieved state-of-the-art results even with
the sliced data.
We have used both a linear kernel and a tree kernel.
For the linear kernel test, we have used the same dataset
as that in (Collins, 2000). In this experiment, we first train
22 SVMs on 22 distinct slices. In order to combine those
SVMs results, we have tried mapping SVMs’ results to
probabilities via a Sigmoid as described in (Platt, 1999).
We use the development data to estimate parameter A
and B in the Sigmoid
</bodyText>
<equation confidence="0.9483555">
1
Pi(y = 1|fi) = 1 + Ae−f:B , (14)
</equation>
<tableCaption confidence="0.909227">
Table 1: Results on section 23 of the WSJ Treebank.
</tableCaption>
<bodyText confidence="0.9167234">
LR/LP = labeled recall/precision. CBs = average number
of crossing brackets per sentence. 0 CBs, 2 CBs are the
percentage of sentences with 0 or ≤ 2 crossing brackets
respectively. CO99 = Model 2 of (Collins, 1999). CH00
= (Charniak, 2000). CO00 = (Collins, 2000).
</bodyText>
<table confidence="0.99985025">
≤40 Words (2245 sentences)
Model LR LP CBs 0 CBs 2 CBs
CO99 88.5% 88.7% 0.92 66.7% 87.1%
CH00 90.1% 90.1% 0.74 70.1% 89.6%
CO00 90.1% 90.4% 0.73 70.7% 89.6%
SVM 89.9% 90.3% 0.75 71.7% 89.4%
≤100 Words (2416 sentences)
Model LR LP CBs 0 CBs 2 CBs
CO99 88.1% 88.3% 1.06 64.0% 85.1%
CH00 89.6% 89.5% 0.88 67.6% 87.7%
CO00 89.6% 89.9% 0.87 68.3% 87.7%
SVM 89.4% 89.8% 0.89 69.2% 87.6%
</table>
<bodyText confidence="0.9952184">
where fi is the result of the ith SVM. The parse with
maximal value of Qi Pi(y = 1|fi) is chosen as the top-
most parse. Experiments on the development data shows
that the result is better if Ae−fiB is much larger than 1.
Therefore
</bodyText>
<figureCaption confidence="0.963698666666667">
Figure 1: Learning curves on the development dataset of
(Collins, 2000). X-axis stands for the number of slices to
be combined. Y-axis stands for the F-score.
</figureCaption>
<figure confidence="0.765244">
0 5 10 15 20
number of slices
</figure>
<tableCaption confidence="0.962127">
Table 2: Results on section 23 of the WSJ Treebank.
</tableCaption>
<table confidence="0.809181">
LR/LP = labeled recall/precision. CBs = average num-
ber of crossing brackets per sentence. CO99 = Model 2
of (Collins, 1999). CD02 = (Collins and Duffy, 2002)
≤100 Words (2416 sentences)
Model LR LP CBs
CO99 88.1% 88.3% 1.06
CD02 88.6% 88.9% 0.99
SVM(tree) 88.7% 88.8% 0.99
</table>
<figure confidence="0.851144588235294">
0.908
0.906
0.904
0.902
0.898
0.896
0.894
0.892
0.91
0.89
0.9
++++++++++++++++++++
×××××××××××××××××××
✷✷✷✷✷✷✷✷✷✷✷✷✷✷✷✷✷
✷✷✸✸✸✸✸✸✸✸✸✸✸✸✸✸✸✸✸✸✸
baseline
β=0
✸ β=200
β=500
✸ β=1000
+ ✷✷
✷××
+
×
+
✷
×
Yn Pi(y = 1|fi) = Yn 1
i=1 i=1
1 + Ae−fiB
≈ Yn 1
i
Ae−fiB
= A−ne(B Pn i=1 fi) (15)
</figure>
<bodyText confidence="0.999315147058824">
Therefore, we may use Pi fi directly, and there is no
need to estimate A and B in (14). Then we combine
SVMs’ result with the log-likelihood generated by the
parser (Collins, 1999). Parameter β is used as the weight
of the log-likelihood. In addition, we find that our SVM
has greater labeled precision than labeled recall, which
means that the system prefer parses with less brackets.
So we take the number of brackets as another feature to
be considered, with weight α. α and β are estimated on
the development data.
The result is shown in Table 1. The performance of
our system matches the results of (Charniak, 2000), but
is a little lower than the results of the Boosting system
in (Collins, 2000), except that the percentage of sen-
tences with no crossing brackets is 1% higher than that of
(Collins, 2000). Since we have to divide data into slices,
we cannot take full advantage of the margin maximiza-
tion.
Figure 1 shows the learning curves. β is used to con-
trol the weight of log-likelihood given by the parser. The
proper value of β depends on the size of training data.
The best result does not improve much after combining 7
slices of training data. We think this is due the limitation
of local optimization.
Our next experiment is on the tree kernel as it is used
in (Collins and Duffy, 2002). We have only trained 5
slices, since each slice takes about 2 weeks to train on
a P3 1.13GHz processor. In addition, the speed of test
for the tree kernel is much slower than that for the lin-
ear kernel. The experimental result is shown in Table 2
The results of our SVM system match the results of the
Voted Perceptron algorithm in (Collins and Duffy, 2002),
although only 5 slices, amounting to less than one fourth
of the whole training dataset, have been used.
</bodyText>
<sectionHeader confidence="0.998968" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999977181818182">
We have introduced a new approach for applying SVMs
to sequential models indirectly, and described a novel
SVM based voting algorithm inspired by the parse
reranking problem. We have presented a risk formula-
tion under the PAC framework for this voting algorithm,
and applied this algorithm to the parse reranking prob-
lem, and achieved LR/LP of 89.4%/89.8% on WSJ sec-
tion 23.
Experimental results show that the SVM with a linear
kernel is superior to the SVM with tree kernel in both
accuracy and speed. The SVM with tree kernel only
achieves a rather low f-score because it takes too many
unrelated features into account. The linear kernel is de-
fined on the features which are manually selected from a
large set of possible features.
As far as context-free grammars are concerned, it will
be hard to include more features into the current feature
set. If we simply use n-grams on context-free grammars,
it is very possible that we will introduce many useless fea-
tures, which may be harmful as they are in tree kernel sys-
tems. One way to include more useful features is to take
advantage of the derivation tree and the elementary trees
in Lexicalized Tree Adjoining Grammar (LTAG) (Joshi
and Schabes, 1997). The basic idea is that each elemen-
tary tree and every segment in a derivation tree is linguis-
tically meaningful.
We also plan to apply this algorithm to other sequen-
tial models, especially to the Supertagging problem. We
believe it will also be very useful to problems of POS
tagging and NP chunking. Compared to parse rerank-
ing, they have a much smaller training dataset and feature
size, which is more suitable for our SVM-based voting
problem.
</bodyText>
<sectionHeader confidence="0.998227" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99927">
We thank Michael Collins for help concerning the data
set, and Anoop Sarkar for his comments. We also thank
three anonymous reviewers for helpful comments.
</bodyText>
<sectionHeader confidence="0.999275" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99977305882353">
E. Black, F. Jelinek, J. Lafferty, Magerman D. M.,
R. Mercer, and S. Roukos. 1993. Towards history-
based grammars: Using richer models for probabilistic
parsing. In Proceedings of the ACL 1993.
Rens Bod. 1998. Beyond Grammar: An Experience-
Based Theory of Language. CSLI Publica-
tions/Cambridge University Press.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In Proceedings ofNAACL 2000.
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Proceedings ofNeural
Information Processing Systems (NIPS 14).
Michael Collins and Nigel Duffy. 2002. New ranking al-
gorithms for parsing and tagging: Kernels over discrete
structures, and the voted perceptron. In Proceedings of
ACL 2002.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In Proceedings of the 7th Inter-
national Conference on Machine Learning.
N. Cristianini and J. Shawe-Tayor. 2000. An introduction
to Support Vector Machines and other kernel-based
learning methods. Cambridge University Press.
Emma Dijkstra. 2001. Support vector machines for parse
selection. Master’s thesis, Univ. of Edinburgh.
Yoav Freund and Robert E. Schapire. 1999. Large mar-
gin classification using the perceptron algorithm. Ma-
chine Learning, 37(3):277–296.
Claudio Gentile. 2001. A new approximate maximal
margin classification algorithm. Journal of Machine
Learning Research, 2:213–242.
Thore Graepel, Ralf Herbrich, and Robert C. Williamson.
2001. From margin to sparsity. In Advances in Neural
Information Processing Systems 13.
Ralf Herbrich, Thore Graepel, and Klaus Obermayer.
2000. Large margin rank boundaries for ordinal re-
gression. In Advances in Large Margin Classifiers,
pages 115–132. MIT Press.
Thorsten Joachims. 1998. Making large-scale support
vector machine learning practical. In Advances in Ker-
nel Methods: Support Vector Machine. MIT Press.
A. Joshi and Y. Schabes. 1997. Tree-adjoining gram-
mars. In G. Rozenberg and A. Salomaa, editors, Hand-
book ofFormal Languages, volume 3, pages 69 – 124.
Springer.
W. Krauth and M. Mezard. 1987. Learning algorithms
with optimal stability in neural networks. Journal of
Physics A, 20:745–752.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with
support vector machines. In Proceedings of NAACL
2001.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Condi-
tional random fields: Probabilistic models for segmen-
tation and labeling sequence data. In Proceedings of
ICML.
Yaoyong Li, Hugo Zaragoza, Ralf Herbrich, John Shawe-
Taylor, and Jaz Kandola. 2002. The perceptron al-
gorithm with uneven margins. In Proceedings of the
International Conference ofMachine Learning.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313–330.
John Platt. 1999. Probabilistic outputs for support vector
machines and comparisons to regularized likelihood
methods. In Advances in Large Margin Classifiers.
MIT Press.
Jesper Salomon, Simon King, and Miles Osborne. 2002.
Framewise phone classification using support vector
machines. In Proceedings ofICSLP 2002.
John Shawe-Taylor, Peter L. Bartlett, Robert C.
Williamson, and Martin Anthony. 1998. Structural
risk minimization over data-dependent hierarchies.
IEEE Trans. on Information Theory, 44(5):1926–1940.
A.J. Smola, P. Bartlett, B. Sch¨olkopf, and C. Schuurmans.
2000. Introduction to large margin classifiers. In A.J.
Smola, P. Bartlett, B. Sch¨olkopf, and C. Schuurmans,
editors, Advances in Large Margin Classifiers, pages
1–26. MIT Press.
Vladimir N. Vapnik. 1998. Statistical Learning Theory.
John Wiley and Sons, Inc.
Vladimir N. Vapnik. 1999. The Nature of Statistical
Learning Theory. Springer, 2 edition.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.229279">
<title confidence="0.999664">An SVM Based Voting with Application to Parse Reranking</title>
<author confidence="0.999377">K Shen</author>
<affiliation confidence="0.99985">Department of Computer and Information University of</affiliation>
<address confidence="0.984205">Philadelphia, PA 19104,</address>
<abstract confidence="0.9548252">This paper introduces a novel Support Vector Machines (SVMs) based voting algorithm for reranking, which provides a way to solve the sequential models indirectly. We have presented a risk formulation under the PAC framework for this voting algorithm. We have applied this algorithm to the parse reranking problem, and achieved labeled recall and preof WSJ section 23 of</abstract>
<author confidence="0.519159">Penn Treebank</author>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Black</author>
<author>F Jelinek</author>
<author>J Lafferty</author>
<author>D M Magerman</author>
<author>R Mercer</author>
<author>S Roukos</author>
</authors>
<title>Towards historybased grammars: Using richer models for probabilistic parsing.</title>
<date>1993</date>
<booktitle>In Proceedings of the ACL</booktitle>
<contexts>
<context position="2845" citStr="Black et al., 1993" startWordPosition="447" endWordPosition="450">y of using discriminative machine learning algorithms in sequential models is to rerank the n-best outputs of a generative system. Reranking uses global features as well as local features, and does not make local normalization. If the output set is large enough, the reranking approach may help to alleviate the impact of the label bias problem, because the victim parses (i.e. those parses which get penalized due to the label bias problem) will have a chance to take part in the reranking. In recent years, reranking techniques have been successfully applied to the so-called history-based models (Black et al., 1993), especially to parsing (Collins, 2000; Collins and Duffy, 2002). In a history-based model, the current decision depends on the decisions made previously. Therefore, we may regard parsing as a special form of sequential model without losing generality. Collins (2000) has proposed two reranking algorithms to rerank the output of an existing parser (Collins, 1999, Model 2). One is based on Markov Random Fields, and the other is based on a boosting approach. In (Collins and Duffy, 2002), the use of Voted Perceptron (VP) (Freund and Schapire, 1999) for the parse reranking problem has been describe</context>
</contexts>
<marker>Black, Jelinek, Lafferty, Magerman, Mercer, Roukos, 1993</marker>
<rawString>E. Black, F. Jelinek, J. Lafferty, Magerman D. M., R. Mercer, and S. Roukos. 1993. Towards historybased grammars: Using richer models for probabilistic parsing. In Proceedings of the ACL 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rens Bod</author>
</authors>
<title>Beyond Grammar: An ExperienceBased Theory of Language. CSLI</title>
<date>1998</date>
<publisher>Publications/Cambridge University Press.</publisher>
<contexts>
<context position="3597" citStr="Bod, 1998" startWordPosition="571" endWordPosition="572"> previously. Therefore, we may regard parsing as a special form of sequential model without losing generality. Collins (2000) has proposed two reranking algorithms to rerank the output of an existing parser (Collins, 1999, Model 2). One is based on Markov Random Fields, and the other is based on a boosting approach. In (Collins and Duffy, 2002), the use of Voted Perceptron (VP) (Freund and Schapire, 1999) for the parse reranking problem has been described. In that paper, the tree kernel (Collins and Duffy, 2001) has been used to efficiently count the number of common subtrees as described in (Bod, 1998). In this paper we will follow the reranking approach. We describe a novel SVM-based voting algorithm for reranking. It provides an alternative way of using a large margin classifier for sequential models. Instead of using the parse tree itself as a training sample, we use a pair of parse trees as a sample, which is analogous to the preference relation used in the context of ordinal regression (Herbrich et al., 2000). Furthermore, we justify the algorithm through a modification of the proof of the large margin rank boundaries for ordinal regression. We then apply this algorithm to the parse re</context>
</contexts>
<marker>Bod, 1998</marker>
<rawString>Rens Bod. 1998. Beyond Grammar: An ExperienceBased Theory of Language. CSLI Publications/Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings ofNAACL</booktitle>
<contexts>
<context position="23013" citStr="Charniak, 2000" startWordPosition="4082" endWordPosition="4083">ins, 2000). In this experiment, we first train 22 SVMs on 22 distinct slices. In order to combine those SVMs results, we have tried mapping SVMs’ results to probabilities via a Sigmoid as described in (Platt, 1999). We use the development data to estimate parameter A and B in the Sigmoid 1 Pi(y = 1|fi) = 1 + Ae−f:B , (14) Table 1: Results on section 23 of the WSJ Treebank. LR/LP = labeled recall/precision. CBs = average number of crossing brackets per sentence. 0 CBs, 2 CBs are the percentage of sentences with 0 or ≤ 2 crossing brackets respectively. CO99 = Model 2 of (Collins, 1999). CH00 = (Charniak, 2000). CO00 = (Collins, 2000). ≤40 Words (2245 sentences) Model LR LP CBs 0 CBs 2 CBs CO99 88.5% 88.7% 0.92 66.7% 87.1% CH00 90.1% 90.1% 0.74 70.1% 89.6% CO00 90.1% 90.4% 0.73 70.7% 89.6% SVM 89.9% 90.3% 0.75 71.7% 89.4% ≤100 Words (2416 sentences) Model LR LP CBs 0 CBs 2 CBs CO99 88.1% 88.3% 1.06 64.0% 85.1% CH00 89.6% 89.5% 0.88 67.6% 87.7% CO00 89.6% 89.9% 0.87 68.3% 87.7% SVM 89.4% 89.8% 0.89 69.2% 87.6% where fi is the result of the ith SVM. The parse with maximal value of Qi Pi(y = 1|fi) is chosen as the topmost parse. Experiments on the development data shows that the result is better if Ae−</context>
<context position="25065" citStr="Charniak, 2000" startWordPosition="4457" endWordPosition="4458">fore, we may use Pi fi directly, and there is no need to estimate A and B in (14). Then we combine SVMs’ result with the log-likelihood generated by the parser (Collins, 1999). Parameter β is used as the weight of the log-likelihood. In addition, we find that our SVM has greater labeled precision than labeled recall, which means that the system prefer parses with less brackets. So we take the number of brackets as another feature to be considered, with weight α. α and β are estimated on the development data. The result is shown in Table 1. The performance of our system matches the results of (Charniak, 2000), but is a little lower than the results of the Boosting system in (Collins, 2000), except that the percentage of sentences with no crossing brackets is 1% higher than that of (Collins, 2000). Since we have to divide data into slices, we cannot take full advantage of the margin maximization. Figure 1 shows the learning curves. β is used to control the weight of log-likelihood given by the parser. The proper value of β depends on the size of training data. The best result does not improve much after combining 7 slices of training data. We think this is due the limitation of local optimization. </context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>E. Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings ofNAACL 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>Convolution kernels for natural language.</title>
<date>2001</date>
<booktitle>In Proceedings ofNeural Information Processing Systems (NIPS 14).</booktitle>
<contexts>
<context position="3504" citStr="Collins and Duffy, 2001" startWordPosition="553" endWordPosition="556">000; Collins and Duffy, 2002). In a history-based model, the current decision depends on the decisions made previously. Therefore, we may regard parsing as a special form of sequential model without losing generality. Collins (2000) has proposed two reranking algorithms to rerank the output of an existing parser (Collins, 1999, Model 2). One is based on Markov Random Fields, and the other is based on a boosting approach. In (Collins and Duffy, 2002), the use of Voted Perceptron (VP) (Freund and Schapire, 1999) for the parse reranking problem has been described. In that paper, the tree kernel (Collins and Duffy, 2001) has been used to efficiently count the number of common subtrees as described in (Bod, 1998). In this paper we will follow the reranking approach. We describe a novel SVM-based voting algorithm for reranking. It provides an alternative way of using a large margin classifier for sequential models. Instead of using the parse tree itself as a training sample, we use a pair of parse trees as a sample, which is analogous to the preference relation used in the context of ordinal regression (Herbrich et al., 2000). Furthermore, we justify the algorithm through a modification of the proof of the larg</context>
<context position="7258" citStr="Collins and Duffy, 2001" startWordPosition="1226" endWordPosition="1229">ion Φ : Rd 7→ Rh such that, for any xi, xj ∈ Rd, K(xi, xj) = Φ(xi) • Φ(xj). One way of testing whether a function is a well-defined kernel is to use the Mercer’s theorem (Vapnik, 1998) by utilizing the positive semidefinteness property. However, as far as a discrete kernel is concerned, there is a more convenient way to show that a function is a well-defined kernel. This is achieved by showing that a function K is a kernel by finding the corresponding mapping function Φ. This method was used in the proof of the string subsequence kernel (Cristianini and Shawe-Tayor, 2000) and the tree kernel (Collins and Duffy, 2001). 1.2 Large Margin Classifiers SVMs are called large margin classifiers because they search for the hyperplane that maximizes the margin. The validity of the large margin method is guaranteed by the theorems of Structural Risk Minimization (SRM) under Probably Approximately Correct (PAC) framework2; test error is related to training data error, number of training samples and the capacity of the learning machine (Smola et al., 2000). Vapnik-Chervonenkis (VC) dimension (Vapnik, 1999), as well as some other measures, is used to estimate the complexity of the hypothesis space, or the capacity of t</context>
</contexts>
<marker>Collins, Duffy, 2001</marker>
<rawString>Michael Collins and Nigel Duffy. 2001. Convolution kernels for natural language. In Proceedings ofNeural Information Processing Systems (NIPS 14).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="2909" citStr="Collins and Duffy, 2002" startWordPosition="456" endWordPosition="459">quential models is to rerank the n-best outputs of a generative system. Reranking uses global features as well as local features, and does not make local normalization. If the output set is large enough, the reranking approach may help to alleviate the impact of the label bias problem, because the victim parses (i.e. those parses which get penalized due to the label bias problem) will have a chance to take part in the reranking. In recent years, reranking techniques have been successfully applied to the so-called history-based models (Black et al., 1993), especially to parsing (Collins, 2000; Collins and Duffy, 2002). In a history-based model, the current decision depends on the decisions made previously. Therefore, we may regard parsing as a special form of sequential model without losing generality. Collins (2000) has proposed two reranking algorithms to rerank the output of an existing parser (Collins, 1999, Model 2). One is based on Markov Random Fields, and the other is based on a boosting approach. In (Collins and Duffy, 2002), the use of Voted Perceptron (VP) (Freund and Schapire, 1999) for the parse reranking problem has been described. In that paper, the tree kernel (Collins and Duffy, 2001) has </context>
<context position="10217" citStr="Collins and Duffy, 2002" startWordPosition="1760" endWordPosition="1763">sentence. We may take xi1 as positive samples, and xij(j&gt;1) as negative samples. However, experiments have shown that this is not the best way to utilize SVMs in reranking (Dijkstra, 2001). A trick to be used here is to take a pair of parses as a sample: for any i and j &gt; 1, (xi1, xij) is a positive sample, and (xij, xi1) is a negative sample. Similar idea was employed in the early works of parse reranking. In the boosting algorithm of (Collins, 2000), for each sample (parse) xij, its margin is defined as F(xi1, ¯α) − F(xij, ¯α), where F is a score function and α¯ is the parameter vector. In (Collins and Duffy, 2002), for each offending parse, the parameter vector updating function is in the form of w = w + h(xi1) − h(xij), where w is the parameter vector and h returns the feature vector of a parse. But neither of these two papers used a pair of parses as a sample and defined functions on pairs of parses. Furthermore, the advantage of using difference between parses was not theoretically clarified, which we will describe in the next section. As far as SVMs are concerned, the use of parses or pairs of parses both maximize the margin between xi1 and xij, but the one using a single parse as a sample needs to</context>
<context position="13288" citStr="Collins and Duffy, 2002" startWordPosition="2321" endWordPosition="2324">ion. As far as SVM is concerned, we may encode the features of each parse with a vector. Dot product is used as the kernel K. Let u and v are two parses. The computational complexity of linear kernel O(|fu |* |fv|), where |fu |and |fv |are the length of the vectors associated with parse u and v respectively. The goodness of the linear kernel is that it runs very fast in the test phase, because coefficients of the support vectors can be combined in advance. For a test parse x, the computational complexity of test is only O(|fx|), which is independent with the number of the support vectors. In (Collins and Duffy, 2002), the tree kernel Tr is used to count the total number of common sub-trees of two parse trees. Let u and v be two trees. Because Tr can be computed by dynamic programming, the computational complexity of Tr(u, v) is O(|u |∗ |v|), where |u |and |v| are the tree sizes of u and v respectively. For a test parse x, the computational complexity of the test is O(S ∗ |x|), where S is the number of support vectors. 3 Justifying the Algorithm 3.1 Justifying the Kernel Firstly, we show that the preference kernel PK defined above is well-defined. Suppose kernel K is defined on T × T. So there exists Φ : T</context>
<context position="19147" citStr="Collins and Duffy, 2002" startWordPosition="3402" endWordPosition="3405"> two algorithms differ on the definition of the margin. In ordinal regression, the margin is min |f(ri) − f(ri−1)|, where f is the regression function for ordinal values. In our algorithm, the margin is min|score(xi1) − score(xij)|. In (Kudo and Matsumoto, 2001), SVMs have been employed in the NP chunking task, a typical labeling problem. However, they have used a deterministic algorithm for decoding. In (Collins, 2000), two reranking algorithms were proposed. In both of these two models, the loss functions are computed directly on the feature space. All the features are manually defined. In (Collins and Duffy, 2002), the Voted Perceptron algorithm was used to in parse reranking. It was shown in (Freund and Schapire, 1999; Graepel et al., 2001) that error bound of (voted) Perceptron is related to margins existing in the training data, but these algorithm are not supposed to maximize margins. Variants of the Perceptron algorithm, which are known as Approximate Maximal Margin classifier, such as PAM (Krauth and Mezard, 1987), ALMA (Gentile, 2001) and PAUM (Li et al., 2002), produce decision hyperplanes within ratio of the maximal margin. However, almost all these algorithms are reported to be inferior to SV</context>
<context position="24048" citStr="Collins and Duffy, 2002" startWordPosition="4272" endWordPosition="4275">e fi is the result of the ith SVM. The parse with maximal value of Qi Pi(y = 1|fi) is chosen as the topmost parse. Experiments on the development data shows that the result is better if Ae−fiB is much larger than 1. Therefore Figure 1: Learning curves on the development dataset of (Collins, 2000). X-axis stands for the number of slices to be combined. Y-axis stands for the F-score. 0 5 10 15 20 number of slices Table 2: Results on section 23 of the WSJ Treebank. LR/LP = labeled recall/precision. CBs = average number of crossing brackets per sentence. CO99 = Model 2 of (Collins, 1999). CD02 = (Collins and Duffy, 2002) ≤100 Words (2416 sentences) Model LR LP CBs CO99 88.1% 88.3% 1.06 CD02 88.6% 88.9% 0.99 SVM(tree) 88.7% 88.8% 0.99 0.908 0.906 0.904 0.902 0.898 0.896 0.894 0.892 0.91 0.89 0.9 ++++++++++++++++++++ ××××××××××××××××××× ✷✷✷✷✷✷✷✷✷✷✷✷✷✷✷✷✷ ✷✷✸✸✸✸✸✸✸✸✸✸✸✸✸✸✸✸✸✸✸ baseline β=0 ✸ β=200 β=500 ✸ β=1000 + ✷✷ ✷×× + × + ✷ × Yn Pi(y = 1|fi) = Yn 1 i=1 i=1 1 + Ae−fiB ≈ Yn 1 i Ae−fiB = A−ne(B Pn i=1 fi) (15) Therefore, we may use Pi fi directly, and there is no need to estimate A and B in (14). Then we combine SVMs’ result with the log-likelihood generated by the parser (Collins, 1999). Parameter β is used a</context>
<context position="25749" citStr="Collins and Duffy, 2002" startWordPosition="4580" endWordPosition="4583">tem in (Collins, 2000), except that the percentage of sentences with no crossing brackets is 1% higher than that of (Collins, 2000). Since we have to divide data into slices, we cannot take full advantage of the margin maximization. Figure 1 shows the learning curves. β is used to control the weight of log-likelihood given by the parser. The proper value of β depends on the size of training data. The best result does not improve much after combining 7 slices of training data. We think this is due the limitation of local optimization. Our next experiment is on the tree kernel as it is used in (Collins and Duffy, 2002). We have only trained 5 slices, since each slice takes about 2 weeks to train on a P3 1.13GHz processor. In addition, the speed of test for the tree kernel is much slower than that for the linear kernel. The experimental result is shown in Table 2 The results of our SVM system match the results of the Voted Perceptron algorithm in (Collins and Duffy, 2002), although only 5 slices, amounting to less than one fourth of the whole training dataset, have been used. 6 Conclusions and Future Work We have introduced a new approach for applying SVMs to sequential models indirectly, and described a nov</context>
</contexts>
<marker>Collins, Duffy, 2002</marker>
<rawString>Michael Collins and Nigel Duffy. 2002. New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron. In Proceedings of ACL 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="3208" citStr="Collins, 1999" startWordPosition="504" endWordPosition="505">es (i.e. those parses which get penalized due to the label bias problem) will have a chance to take part in the reranking. In recent years, reranking techniques have been successfully applied to the so-called history-based models (Black et al., 1993), especially to parsing (Collins, 2000; Collins and Duffy, 2002). In a history-based model, the current decision depends on the decisions made previously. Therefore, we may regard parsing as a special form of sequential model without losing generality. Collins (2000) has proposed two reranking algorithms to rerank the output of an existing parser (Collins, 1999, Model 2). One is based on Markov Random Fields, and the other is based on a boosting approach. In (Collins and Duffy, 2002), the use of Voted Perceptron (VP) (Freund and Schapire, 1999) for the parse reranking problem has been described. In that paper, the tree kernel (Collins and Duffy, 2001) has been used to efficiently count the number of common subtrees as described in (Bod, 1998). In this paper we will follow the reranking approach. We describe a novel SVM-based voting algorithm for reranking. It provides an alternative way of using a large margin classifier for sequential models. Inste</context>
<context position="22988" citStr="Collins, 1999" startWordPosition="4078" endWordPosition="4079">dataset as that in (Collins, 2000). In this experiment, we first train 22 SVMs on 22 distinct slices. In order to combine those SVMs results, we have tried mapping SVMs’ results to probabilities via a Sigmoid as described in (Platt, 1999). We use the development data to estimate parameter A and B in the Sigmoid 1 Pi(y = 1|fi) = 1 + Ae−f:B , (14) Table 1: Results on section 23 of the WSJ Treebank. LR/LP = labeled recall/precision. CBs = average number of crossing brackets per sentence. 0 CBs, 2 CBs are the percentage of sentences with 0 or ≤ 2 crossing brackets respectively. CO99 = Model 2 of (Collins, 1999). CH00 = (Charniak, 2000). CO00 = (Collins, 2000). ≤40 Words (2245 sentences) Model LR LP CBs 0 CBs 2 CBs CO99 88.5% 88.7% 0.92 66.7% 87.1% CH00 90.1% 90.1% 0.74 70.1% 89.6% CO00 90.1% 90.4% 0.73 70.7% 89.6% SVM 89.9% 90.3% 0.75 71.7% 89.4% ≤100 Words (2416 sentences) Model LR LP CBs 0 CBs 2 CBs CO99 88.1% 88.3% 1.06 64.0% 85.1% CH00 89.6% 89.5% 0.88 67.6% 87.7% CO00 89.6% 89.9% 0.87 68.3% 87.7% SVM 89.4% 89.8% 0.89 69.2% 87.6% where fi is the result of the ith SVM. The parse with maximal value of Qi Pi(y = 1|fi) is chosen as the topmost parse. Experiments on the development data shows that th</context>
<context position="24625" citStr="Collins, 1999" startWordPosition="4380" endWordPosition="4381">99). CD02 = (Collins and Duffy, 2002) ≤100 Words (2416 sentences) Model LR LP CBs CO99 88.1% 88.3% 1.06 CD02 88.6% 88.9% 0.99 SVM(tree) 88.7% 88.8% 0.99 0.908 0.906 0.904 0.902 0.898 0.896 0.894 0.892 0.91 0.89 0.9 ++++++++++++++++++++ ××××××××××××××××××× ✷✷✷✷✷✷✷✷✷✷✷✷✷✷✷✷✷ ✷✷✸✸✸✸✸✸✸✸✸✸✸✸✸✸✸✸✸✸✸ baseline β=0 ✸ β=200 β=500 ✸ β=1000 + ✷✷ ✷×× + × + ✷ × Yn Pi(y = 1|fi) = Yn 1 i=1 i=1 1 + Ae−fiB ≈ Yn 1 i Ae−fiB = A−ne(B Pn i=1 fi) (15) Therefore, we may use Pi fi directly, and there is no need to estimate A and B in (14). Then we combine SVMs’ result with the log-likelihood generated by the parser (Collins, 1999). Parameter β is used as the weight of the log-likelihood. In addition, we find that our SVM has greater labeled precision than labeled recall, which means that the system prefer parses with less brackets. So we take the number of brackets as another feature to be considered, with weight α. α and β are estimated on the development data. The result is shown in Table 1. The performance of our system matches the results of (Charniak, 2000), but is a little lower than the results of the Boosting system in (Collins, 2000), except that the percentage of sentences with no crossing brackets is 1% high</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2000</date>
<booktitle>In Proceedings of the 7th International Conference on Machine Learning.</booktitle>
<contexts>
<context position="2883" citStr="Collins, 2000" startWordPosition="454" endWordPosition="455">lgorithms in sequential models is to rerank the n-best outputs of a generative system. Reranking uses global features as well as local features, and does not make local normalization. If the output set is large enough, the reranking approach may help to alleviate the impact of the label bias problem, because the victim parses (i.e. those parses which get penalized due to the label bias problem) will have a chance to take part in the reranking. In recent years, reranking techniques have been successfully applied to the so-called history-based models (Black et al., 1993), especially to parsing (Collins, 2000; Collins and Duffy, 2002). In a history-based model, the current decision depends on the decisions made previously. Therefore, we may regard parsing as a special form of sequential model without losing generality. Collins (2000) has proposed two reranking algorithms to rerank the output of an existing parser (Collins, 1999, Model 2). One is based on Markov Random Fields, and the other is based on a boosting approach. In (Collins and Duffy, 2002), the use of Voted Perceptron (VP) (Freund and Schapire, 1999) for the parse reranking problem has been described. In that paper, the tree kernel (Col</context>
<context position="10048" citStr="Collins, 2000" startWordPosition="1730" endWordPosition="1731">rithm Let xij be the jth candidate parse for the ith sentence in training data. Let xi1 is the parse with the highest f-score among all the parses for the ith sentence. We may take xi1 as positive samples, and xij(j&gt;1) as negative samples. However, experiments have shown that this is not the best way to utilize SVMs in reranking (Dijkstra, 2001). A trick to be used here is to take a pair of parses as a sample: for any i and j &gt; 1, (xi1, xij) is a positive sample, and (xij, xi1) is a negative sample. Similar idea was employed in the early works of parse reranking. In the boosting algorithm of (Collins, 2000), for each sample (parse) xij, its margin is defined as F(xi1, ¯α) − F(xij, ¯α), where F is a score function and α¯ is the parameter vector. In (Collins and Duffy, 2002), for each offending parse, the parameter vector updating function is in the form of w = w + h(xi1) − h(xij), where w is the parameter vector and h returns the feature vector of a parse. But neither of these two papers used a pair of parses as a sample and defined functions on pairs of parses. Furthermore, the advantage of using difference between parses was not theoretically clarified, which we will describe in the next sectio</context>
<context position="12549" citStr="Collins, 2000" startWordPosition="2188" endWordPosition="2189">nd a parallel hyperplane that crosses the origin and makes the margin larger. Hence, the outcome separating hyperplane has to pass through the origin, which means that b = 0. Therefore, for each test parse x, we only need to compute its score as follows. Ng score(x) = αiyi(K(si1, x) − K(si2, x)), (7) i=1 because f((xj,xk)) = score(xj) − score(xk). (8) 2.1 Kernels In (6), the preference kernel PK is defined on kernel K. K can be any possible kernel. We will show that PK is well-defined in the next section. In this paper, we consider two kernels for K, the linear kernel and the tree kernel. In (Collins, 2000), each parse is associated with a set of features. Linear combination of the features is used in the decision function. As far as SVM is concerned, we may encode the features of each parse with a vector. Dot product is used as the kernel K. Let u and v are two parses. The computational complexity of linear kernel O(|fu |* |fv|), where |fu |and |fv |are the length of the vectors associated with parse u and v respectively. The goodness of the linear kernel is that it runs very fast in the test phase, because coefficients of the support vectors can be combined in advance. For a test parse x, the </context>
<context position="18946" citStr="Collins, 2000" startWordPosition="3371" endWordPosition="3372">that of the ordinal regression algorithm. Ordinal regression searches for a regression function for ordinal values, while our algorithm is designed to solve a voting problem. As a result, the two algorithms differ on the definition of the margin. In ordinal regression, the margin is min |f(ri) − f(ri−1)|, where f is the regression function for ordinal values. In our algorithm, the margin is min|score(xi1) − score(xij)|. In (Kudo and Matsumoto, 2001), SVMs have been employed in the NP chunking task, a typical labeling problem. However, they have used a deterministic algorithm for decoding. In (Collins, 2000), two reranking algorithms were proposed. In both of these two models, the loss functions are computed directly on the feature space. All the features are manually defined. In (Collins and Duffy, 2002), the Voted Perceptron algorithm was used to in parse reranking. It was shown in (Freund and Schapire, 1999; Graepel et al., 2001) that error bound of (voted) Perceptron is related to margins existing in the training data, but these algorithm are not supposed to maximize margins. Variants of the Perceptron algorithm, which are known as Approximate Maximal Margin classifier, such as PAM (Krauth an</context>
<context position="20307" citStr="Collins, 2000" startWordPosition="3598" endWordPosition="3599">ll these algorithms are reported to be inferior to SVMs in accuracy, while more efficient in training. Furthermore, these variants of the Perceptron algorithm take advantage of the large margin existing in the training data. However, in NLP applications, samples are usually inseparable even if the kernel trick is used. SVMs can still be trained to maximize the margin through the method of soft margin. 5 Experiments and Analysis We use SVMlight (Joachims, 1998) as the SVM classifier. The soft margin parameter C is set to its default value in SV Mlight. We use the same data set as described in (Collins, 2000; Collins and Duffy, 2002). Section 2-21 of the Penn WSJ Treebank (Marcus et al., 1994) are used as training data, and section 23 is used for final test. The training data contains around 40,000 sentences, each of which has 27 distinct parses on average. Of the 40,000 training sentences, the first 36,000 are used to train SVMs. The remaining 4,000 sentences are used as development data. The training complexity for SVMlight is roughly O(n2.1) (Joachims, 1998), where n is the number of the training samples. One solution to the scaling difficulties is to use the Kernel Fisher Discriminant as desc</context>
<context position="22408" citStr="Collins, 2000" startWordPosition="3972" endWordPosition="3973"> the following reasons. Firstly, with the improvement in the computing resources, we will be able to use larger slices so as to utilize more global optimization. SVMs are superior to other linear classifiers in theory. On the other hand, the current size of the slice is large enough for other NLP applications like text chunking, although it is not large enough for parse reranking. The last reason is that we have achieved state-of-the-art results even with the sliced data. We have used both a linear kernel and a tree kernel. For the linear kernel test, we have used the same dataset as that in (Collins, 2000). In this experiment, we first train 22 SVMs on 22 distinct slices. In order to combine those SVMs results, we have tried mapping SVMs’ results to probabilities via a Sigmoid as described in (Platt, 1999). We use the development data to estimate parameter A and B in the Sigmoid 1 Pi(y = 1|fi) = 1 + Ae−f:B , (14) Table 1: Results on section 23 of the WSJ Treebank. LR/LP = labeled recall/precision. CBs = average number of crossing brackets per sentence. 0 CBs, 2 CBs are the percentage of sentences with 0 or ≤ 2 crossing brackets respectively. CO99 = Model 2 of (Collins, 1999). CH00 = (Charniak, </context>
<context position="23721" citStr="Collins, 2000" startWordPosition="4214" endWordPosition="4215">.7% 0.92 66.7% 87.1% CH00 90.1% 90.1% 0.74 70.1% 89.6% CO00 90.1% 90.4% 0.73 70.7% 89.6% SVM 89.9% 90.3% 0.75 71.7% 89.4% ≤100 Words (2416 sentences) Model LR LP CBs 0 CBs 2 CBs CO99 88.1% 88.3% 1.06 64.0% 85.1% CH00 89.6% 89.5% 0.88 67.6% 87.7% CO00 89.6% 89.9% 0.87 68.3% 87.7% SVM 89.4% 89.8% 0.89 69.2% 87.6% where fi is the result of the ith SVM. The parse with maximal value of Qi Pi(y = 1|fi) is chosen as the topmost parse. Experiments on the development data shows that the result is better if Ae−fiB is much larger than 1. Therefore Figure 1: Learning curves on the development dataset of (Collins, 2000). X-axis stands for the number of slices to be combined. Y-axis stands for the F-score. 0 5 10 15 20 number of slices Table 2: Results on section 23 of the WSJ Treebank. LR/LP = labeled recall/precision. CBs = average number of crossing brackets per sentence. CO99 = Model 2 of (Collins, 1999). CD02 = (Collins and Duffy, 2002) ≤100 Words (2416 sentences) Model LR LP CBs CO99 88.1% 88.3% 1.06 CD02 88.6% 88.9% 0.99 SVM(tree) 88.7% 88.8% 0.99 0.908 0.906 0.904 0.902 0.898 0.896 0.894 0.892 0.91 0.89 0.9 ++++++++++++++++++++ ××××××××××××××××××× ✷✷✷✷✷✷✷✷✷✷✷✷✷✷✷✷✷ ✷✷✸✸✸✸✸✸✸✸✸✸✸✸✸✸✸✸✸✸✸ baseline β=0 ✸</context>
<context position="25147" citStr="Collins, 2000" startWordPosition="4472" endWordPosition="4473">Then we combine SVMs’ result with the log-likelihood generated by the parser (Collins, 1999). Parameter β is used as the weight of the log-likelihood. In addition, we find that our SVM has greater labeled precision than labeled recall, which means that the system prefer parses with less brackets. So we take the number of brackets as another feature to be considered, with weight α. α and β are estimated on the development data. The result is shown in Table 1. The performance of our system matches the results of (Charniak, 2000), but is a little lower than the results of the Boosting system in (Collins, 2000), except that the percentage of sentences with no crossing brackets is 1% higher than that of (Collins, 2000). Since we have to divide data into slices, we cannot take full advantage of the margin maximization. Figure 1 shows the learning curves. β is used to control the weight of log-likelihood given by the parser. The proper value of β depends on the size of training data. The best result does not improve much after combining 7 slices of training data. We think this is due the limitation of local optimization. Our next experiment is on the tree kernel as it is used in (Collins and Duffy, 200</context>
</contexts>
<marker>Collins, 2000</marker>
<rawString>Michael Collins. 2000. Discriminative reranking for natural language parsing. In Proceedings of the 7th International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Cristianini</author>
<author>J Shawe-Tayor</author>
</authors>
<title>An introduction to Support Vector Machines and other kernel-based learning methods.</title>
<date>2000</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="7212" citStr="Cristianini and Shawe-Tayor, 2000" startWordPosition="1218" endWordPosition="1221">l-defined kernel if and only if there is a mapping function Φ : Rd 7→ Rh such that, for any xi, xj ∈ Rd, K(xi, xj) = Φ(xi) • Φ(xj). One way of testing whether a function is a well-defined kernel is to use the Mercer’s theorem (Vapnik, 1998) by utilizing the positive semidefinteness property. However, as far as a discrete kernel is concerned, there is a more convenient way to show that a function is a well-defined kernel. This is achieved by showing that a function K is a kernel by finding the corresponding mapping function Φ. This method was used in the proof of the string subsequence kernel (Cristianini and Shawe-Tayor, 2000) and the tree kernel (Collins and Duffy, 2001). 1.2 Large Margin Classifiers SVMs are called large margin classifiers because they search for the hyperplane that maximizes the margin. The validity of the large margin method is guaranteed by the theorems of Structural Risk Minimization (SRM) under Probably Approximately Correct (PAC) framework2; test error is related to training data error, number of training samples and the capacity of the learning machine (Smola et al., 2000). Vapnik-Chervonenkis (VC) dimension (Vapnik, 1999), as well as some other measures, is used to estimate the complexity</context>
</contexts>
<marker>Cristianini, Shawe-Tayor, 2000</marker>
<rawString>N. Cristianini and J. Shawe-Tayor. 2000. An introduction to Support Vector Machines and other kernel-based learning methods. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emma Dijkstra</author>
</authors>
<title>Support vector machines for parse selection.</title>
<date>2001</date>
<tech>Master’s thesis,</tech>
<institution>Univ. of Edinburgh.</institution>
<contexts>
<context position="9781" citStr="Dijkstra, 2001" startWordPosition="1675" endWordPosition="1677">dently generated examples z with h = Tθ(f) E Tθ(F) such that erz(h) = 0 and ρ = min |f(xi) − 0|, then with confidence 1 − δ the expected error of h is boundedfrom above by 2 (k log(8em k ) log(32m) + log(8m δ )) (5) m where k = afat(ρ/8). 2 A New SVM-based Voting Algorithm Let xij be the jth candidate parse for the ith sentence in training data. Let xi1 is the parse with the highest f-score among all the parses for the ith sentence. We may take xi1 as positive samples, and xij(j&gt;1) as negative samples. However, experiments have shown that this is not the best way to utilize SVMs in reranking (Dijkstra, 2001). A trick to be used here is to take a pair of parses as a sample: for any i and j &gt; 1, (xi1, xij) is a positive sample, and (xij, xi1) is a negative sample. Similar idea was employed in the early works of parse reranking. In the boosting algorithm of (Collins, 2000), for each sample (parse) xij, its margin is defined as F(xi1, ¯α) − F(xij, ¯α), where F is a score function and α¯ is the parameter vector. In (Collins and Duffy, 2002), for each offending parse, the parameter vector updating function is in the form of w = w + h(xi1) − h(xij), where w is the parameter vector and h returns the feat</context>
</contexts>
<marker>Dijkstra, 2001</marker>
<rawString>Emma Dijkstra. 2001. Support vector machines for parse selection. Master’s thesis, Univ. of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Robert E Schapire</author>
</authors>
<title>Large margin classification using the perceptron algorithm.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>37</volume>
<issue>3</issue>
<contexts>
<context position="3395" citStr="Freund and Schapire, 1999" startWordPosition="535" endWordPosition="538">cessfully applied to the so-called history-based models (Black et al., 1993), especially to parsing (Collins, 2000; Collins and Duffy, 2002). In a history-based model, the current decision depends on the decisions made previously. Therefore, we may regard parsing as a special form of sequential model without losing generality. Collins (2000) has proposed two reranking algorithms to rerank the output of an existing parser (Collins, 1999, Model 2). One is based on Markov Random Fields, and the other is based on a boosting approach. In (Collins and Duffy, 2002), the use of Voted Perceptron (VP) (Freund and Schapire, 1999) for the parse reranking problem has been described. In that paper, the tree kernel (Collins and Duffy, 2001) has been used to efficiently count the number of common subtrees as described in (Bod, 1998). In this paper we will follow the reranking approach. We describe a novel SVM-based voting algorithm for reranking. It provides an alternative way of using a large margin classifier for sequential models. Instead of using the parse tree itself as a training sample, we use a pair of parse trees as a sample, which is analogous to the preference relation used in the context of ordinal regression (</context>
<context position="19254" citStr="Freund and Schapire, 1999" startWordPosition="3421" endWordPosition="3424">f(ri−1)|, where f is the regression function for ordinal values. In our algorithm, the margin is min|score(xi1) − score(xij)|. In (Kudo and Matsumoto, 2001), SVMs have been employed in the NP chunking task, a typical labeling problem. However, they have used a deterministic algorithm for decoding. In (Collins, 2000), two reranking algorithms were proposed. In both of these two models, the loss functions are computed directly on the feature space. All the features are manually defined. In (Collins and Duffy, 2002), the Voted Perceptron algorithm was used to in parse reranking. It was shown in (Freund and Schapire, 1999; Graepel et al., 2001) that error bound of (voted) Perceptron is related to margins existing in the training data, but these algorithm are not supposed to maximize margins. Variants of the Perceptron algorithm, which are known as Approximate Maximal Margin classifier, such as PAM (Krauth and Mezard, 1987), ALMA (Gentile, 2001) and PAUM (Li et al., 2002), produce decision hyperplanes within ratio of the maximal margin. However, almost all these algorithms are reported to be inferior to SVMs in accuracy, while more efficient in training. Furthermore, these variants of the Perceptron algorithm t</context>
</contexts>
<marker>Freund, Schapire, 1999</marker>
<rawString>Yoav Freund and Robert E. Schapire. 1999. Large margin classification using the perceptron algorithm. Machine Learning, 37(3):277–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudio Gentile</author>
</authors>
<title>A new approximate maximal margin classification algorithm.</title>
<date>2001</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>2--213</pages>
<contexts>
<context position="19583" citStr="Gentile, 2001" startWordPosition="3476" endWordPosition="3477">hms were proposed. In both of these two models, the loss functions are computed directly on the feature space. All the features are manually defined. In (Collins and Duffy, 2002), the Voted Perceptron algorithm was used to in parse reranking. It was shown in (Freund and Schapire, 1999; Graepel et al., 2001) that error bound of (voted) Perceptron is related to margins existing in the training data, but these algorithm are not supposed to maximize margins. Variants of the Perceptron algorithm, which are known as Approximate Maximal Margin classifier, such as PAM (Krauth and Mezard, 1987), ALMA (Gentile, 2001) and PAUM (Li et al., 2002), produce decision hyperplanes within ratio of the maximal margin. However, almost all these algorithms are reported to be inferior to SVMs in accuracy, while more efficient in training. Furthermore, these variants of the Perceptron algorithm take advantage of the large margin existing in the training data. However, in NLP applications, samples are usually inseparable even if the kernel trick is used. SVMs can still be trained to maximize the margin through the method of soft margin. 5 Experiments and Analysis We use SVMlight (Joachims, 1998) as the SVM classifier. T</context>
</contexts>
<marker>Gentile, 2001</marker>
<rawString>Claudio Gentile. 2001. A new approximate maximal margin classification algorithm. Journal of Machine Learning Research, 2:213–242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thore Graepel</author>
<author>Ralf Herbrich</author>
<author>Robert C Williamson</author>
</authors>
<title>From margin to sparsity.</title>
<date>2001</date>
<booktitle>In Advances in Neural Information Processing Systems 13.</booktitle>
<contexts>
<context position="19277" citStr="Graepel et al., 2001" startWordPosition="3425" endWordPosition="3428">gression function for ordinal values. In our algorithm, the margin is min|score(xi1) − score(xij)|. In (Kudo and Matsumoto, 2001), SVMs have been employed in the NP chunking task, a typical labeling problem. However, they have used a deterministic algorithm for decoding. In (Collins, 2000), two reranking algorithms were proposed. In both of these two models, the loss functions are computed directly on the feature space. All the features are manually defined. In (Collins and Duffy, 2002), the Voted Perceptron algorithm was used to in parse reranking. It was shown in (Freund and Schapire, 1999; Graepel et al., 2001) that error bound of (voted) Perceptron is related to margins existing in the training data, but these algorithm are not supposed to maximize margins. Variants of the Perceptron algorithm, which are known as Approximate Maximal Margin classifier, such as PAM (Krauth and Mezard, 1987), ALMA (Gentile, 2001) and PAUM (Li et al., 2002), produce decision hyperplanes within ratio of the maximal margin. However, almost all these algorithms are reported to be inferior to SVMs in accuracy, while more efficient in training. Furthermore, these variants of the Perceptron algorithm take advantage of the la</context>
</contexts>
<marker>Graepel, Herbrich, Williamson, 2001</marker>
<rawString>Thore Graepel, Ralf Herbrich, and Robert C. Williamson. 2001. From margin to sparsity. In Advances in Neural Information Processing Systems 13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralf Herbrich</author>
<author>Thore Graepel</author>
<author>Klaus Obermayer</author>
</authors>
<title>Large margin rank boundaries for ordinal regression.</title>
<date>2000</date>
<booktitle>In Advances in Large Margin Classifiers,</booktitle>
<pages>115--132</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="4017" citStr="Herbrich et al., 2000" startWordPosition="641" endWordPosition="644"> for the parse reranking problem has been described. In that paper, the tree kernel (Collins and Duffy, 2001) has been used to efficiently count the number of common subtrees as described in (Bod, 1998). In this paper we will follow the reranking approach. We describe a novel SVM-based voting algorithm for reranking. It provides an alternative way of using a large margin classifier for sequential models. Instead of using the parse tree itself as a training sample, we use a pair of parse trees as a sample, which is analogous to the preference relation used in the context of ordinal regression (Herbrich et al., 2000). Furthermore, we justify the algorithm through a modification of the proof of the large margin rank boundaries for ordinal regression. We then apply this algorithm to the parse reranking problem. 1.1 A Short Introduction of SVMs In this section, we give a short introduction of Support Vector Machines. We follow (Vapnik, 1998)’s definition of SVMs. For each training sample (yi, xi), yi represents its class, and xi represents its input vector defined on a d-dimensional space. Suppose the training samples {(y1, x1), ..., (yn, xn)} (xi ∈ Rd, yi ∈ {−1, 1}) can be separated by a hyperplane H: (x • </context>
<context position="11437" citStr="Herbrich et al., 2000" startWordPosition="1980" endWordPosition="1983">to satisfy some extra constraints on the selection of decision function. However these constraints are not necessary (see section 3.3). Therefore the use of pairs of parses has both theoretic and practical advantages. Now we need to define the kernel on pairs of parses. Let (t1, t2), (v1, v2) are two pairs of parses. Let K is any kernel function on the space of single parses. The preference kernel PK is defined on K as follows. PK((t1, t2), (v1, v2)) = K(t1, v1) − K(t1, v2) −K(t2,v1) + K(t2,v2) (6) The preference kernel of this form was previously used in the context of ordinal regression in (Herbrich et al., 2000). Then the decision function is Ng f((xj, xk)) = αiyiPK((si1, si2), (xj, xk)) + b i=1 Ng = b + ( αiyi(K(si1,xj) − K(si2,xj))) i=1 Ng −( αiyi(K(si1, xk) − K(si2, xk))), i=1 where xj and xk are two distinct parses of a sentence, (si1, si2) is the ith support vector, and Ns is the total number of support vectors. As we have defined them, the training samples are symmetric with respect to the origin in the space. Therefore, for any hyperplane that does not pass through the origin, we can always find a parallel hyperplane that crosses the origin and makes the margin larger. Hence, the outcome separ</context>
<context position="14636" citStr="Herbrich et al., 2000" startWordPosition="2591" endWordPosition="2595">ion Φ0 : T × T → H0 such that PK((t1, t2), (v1, v2)) = Φ0(t1, t2) • Φ0(t1, t2), where t1, t2, v1, v2 ∈ T. According to the definition of PK, we have PK((t1,t2), (v1, v2)) = K(t1, v1) − K(t1, v2) − K(t2, v1) + K(t2, v2) = Φ(t1) • Φ(v1) − Φ(t1) • Φ(v2) −Φ(t2) • Φ(v1) + Φ(t2) • Φ(v2) = (Φ(t1) − Φ(t2)) • (Φ(v1) − Φ(v2)), (9) Let H0 = H and Φ0(x1, x2) = Φ(x1) − Φ(x2). Hence kernel PK is well-defined. 3.2 Margin Bound for SVM-based Voting We will show that the expected error of voting is bounded from above in the PAC framework. The approach used here is analogous to the proof of ordinal regression (Herbrich et al., 2000). The key idea is to show the equivalence of the voting risk and the classification risk. Let X be the set of all parse trees. For each x ∈ X, let x¯ be the best parse for the sentence related to x. Thus the appropriate loss function for the voting problem is as follows. ~ 1 if f(¯x) &lt; f(x) lvote(x, f) ≡ (10) 0 otherwise where f is a parse scoring function. Let E = {(x, ¯x)|x ∈ X} ∪ {(¯x,x)|x ∈ X}. E is the space of event of the classification problem, and PrE((x,¯x)) = PrE((¯x,x)) = 12PrX(x). For any parse scoring function f, let gf(x1, x2) ≡ sgn(f(x1)−f(x2)). For classifier gf on space E, it</context>
<context position="17950" citStr="Herbrich et al., 2000" startWordPosition="3196" endWordPosition="3199">e, j is the index for parse, and ∀i Pj ci,j = 0. The format of score in (13) is the same as the decision function generated by an SVM trained on the single parses as samples. However, there is a constraint that the sum of the coefficients related to parses of the same sentence is 0. So in this way we decrease the size of hypothesis space based on the prior knowledge that only the different segments of two distinct parses determine which parse is better. 4 Related Work The use of pairs of parse trees in our model is analogous to the preference relation used in the ordinal regression algorithm (Herbrich et al., 2000). In that paper, pairs of objects have been used as training samples. For example, let (r1, r2, ...rm) be a list of objects in the training data, where ri ranks ith. Then pairs of objects (ri−1, ri) are training samples. Preference kernel PK in our paper is the same as the preference kernel in (Herbrich et al., 2000) in format. However, the purpose of our model is different from that of the ordinal regression algorithm. Ordinal regression searches for a regression function for ordinal values, while our algorithm is designed to solve a voting problem. As a result, the two algorithms differ on t</context>
</contexts>
<marker>Herbrich, Graepel, Obermayer, 2000</marker>
<rawString>Ralf Herbrich, Thore Graepel, and Klaus Obermayer. 2000. Large margin rank boundaries for ordinal regression. In Advances in Large Margin Classifiers, pages 115–132. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making large-scale support vector machine learning practical.</title>
<date>1998</date>
<booktitle>In Advances in Kernel Methods: Support Vector Machine.</booktitle>
<publisher>MIT Press.</publisher>
<contexts>
<context position="20158" citStr="Joachims, 1998" startWordPosition="3568" endWordPosition="3569"> and Mezard, 1987), ALMA (Gentile, 2001) and PAUM (Li et al., 2002), produce decision hyperplanes within ratio of the maximal margin. However, almost all these algorithms are reported to be inferior to SVMs in accuracy, while more efficient in training. Furthermore, these variants of the Perceptron algorithm take advantage of the large margin existing in the training data. However, in NLP applications, samples are usually inseparable even if the kernel trick is used. SVMs can still be trained to maximize the margin through the method of soft margin. 5 Experiments and Analysis We use SVMlight (Joachims, 1998) as the SVM classifier. The soft margin parameter C is set to its default value in SV Mlight. We use the same data set as described in (Collins, 2000; Collins and Duffy, 2002). Section 2-21 of the Penn WSJ Treebank (Marcus et al., 1994) are used as training data, and section 23 is used for final test. The training data contains around 40,000 sentences, each of which has 27 distinct parses on average. Of the 40,000 training sentences, the first 36,000 are used to train SVMs. The remaining 4,000 sentences are used as development data. The training complexity for SVMlight is roughly O(n2.1) (Joac</context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>Thorsten Joachims. 1998. Making large-scale support vector machine learning practical. In Advances in Kernel Methods: Support Vector Machine. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Joshi</author>
<author>Y Schabes</author>
</authors>
<title>Tree-adjoining grammars.</title>
<date>1997</date>
<booktitle>Handbook ofFormal Languages,</booktitle>
<volume>3</volume>
<pages>69--124</pages>
<editor>In G. Rozenberg and A. Salomaa, editors,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="27450" citStr="Joshi and Schabes, 1997" startWordPosition="4876" endWordPosition="4879">oo many unrelated features into account. The linear kernel is defined on the features which are manually selected from a large set of possible features. As far as context-free grammars are concerned, it will be hard to include more features into the current feature set. If we simply use n-grams on context-free grammars, it is very possible that we will introduce many useless features, which may be harmful as they are in tree kernel systems. One way to include more useful features is to take advantage of the derivation tree and the elementary trees in Lexicalized Tree Adjoining Grammar (LTAG) (Joshi and Schabes, 1997). The basic idea is that each elementary tree and every segment in a derivation tree is linguistically meaningful. We also plan to apply this algorithm to other sequential models, especially to the Supertagging problem. We believe it will also be very useful to problems of POS tagging and NP chunking. Compared to parse reranking, they have a much smaller training dataset and feature size, which is more suitable for our SVM-based voting problem. Acknowledgments We thank Michael Collins for help concerning the data set, and Anoop Sarkar for his comments. We also thank three anonymous reviewers f</context>
</contexts>
<marker>Joshi, Schabes, 1997</marker>
<rawString>A. Joshi and Y. Schabes. 1997. Tree-adjoining grammars. In G. Rozenberg and A. Salomaa, editors, Handbook ofFormal Languages, volume 3, pages 69 – 124. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Krauth</author>
<author>M Mezard</author>
</authors>
<title>Learning algorithms with optimal stability in neural networks.</title>
<date>1987</date>
<journal>Journal of Physics A,</journal>
<pages>20--745</pages>
<contexts>
<context position="19561" citStr="Krauth and Mezard, 1987" startWordPosition="3471" endWordPosition="3474">ns, 2000), two reranking algorithms were proposed. In both of these two models, the loss functions are computed directly on the feature space. All the features are manually defined. In (Collins and Duffy, 2002), the Voted Perceptron algorithm was used to in parse reranking. It was shown in (Freund and Schapire, 1999; Graepel et al., 2001) that error bound of (voted) Perceptron is related to margins existing in the training data, but these algorithm are not supposed to maximize margins. Variants of the Perceptron algorithm, which are known as Approximate Maximal Margin classifier, such as PAM (Krauth and Mezard, 1987), ALMA (Gentile, 2001) and PAUM (Li et al., 2002), produce decision hyperplanes within ratio of the maximal margin. However, almost all these algorithms are reported to be inferior to SVMs in accuracy, while more efficient in training. Furthermore, these variants of the Perceptron algorithm take advantage of the large margin existing in the training data. However, in NLP applications, samples are usually inseparable even if the kernel trick is used. SVMs can still be trained to maximize the margin through the method of soft margin. 5 Experiments and Analysis We use SVMlight (Joachims, 1998) as</context>
</contexts>
<marker>Krauth, Mezard, 1987</marker>
<rawString>W. Krauth and M. Mezard. 1987. Learning algorithms with optimal stability in neural networks. Journal of Physics A, 20:745–752.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Chunking with support vector machines.</title>
<date>2001</date>
<booktitle>In Proceedings of NAACL</booktitle>
<contexts>
<context position="18785" citStr="Kudo and Matsumoto, 2001" startWordPosition="3342" endWordPosition="3345">g samples. Preference kernel PK in our paper is the same as the preference kernel in (Herbrich et al., 2000) in format. However, the purpose of our model is different from that of the ordinal regression algorithm. Ordinal regression searches for a regression function for ordinal values, while our algorithm is designed to solve a voting problem. As a result, the two algorithms differ on the definition of the margin. In ordinal regression, the margin is min |f(ri) − f(ri−1)|, where f is the regression function for ordinal values. In our algorithm, the margin is min|score(xi1) − score(xij)|. In (Kudo and Matsumoto, 2001), SVMs have been employed in the NP chunking task, a typical labeling problem. However, they have used a deterministic algorithm for decoding. In (Collins, 2000), two reranking algorithms were proposed. In both of these two models, the loss functions are computed directly on the feature space. All the features are manually defined. In (Collins and Duffy, 2002), the Voted Perceptron algorithm was used to in parse reranking. It was shown in (Freund and Schapire, 1999; Graepel et al., 2001) that error bound of (voted) Perceptron is related to margins existing in the training data, but these algor</context>
</contexts>
<marker>Kudo, Matsumoto, 2001</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2001. Chunking with support vector machines. In Proceedings of NAACL 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmentation and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="2135" citStr="Lafferty et al., 2001" startWordPosition="327" endWordPosition="330"> and use Viterbi search to combine those probabilities (Platt, 1999). However, this approach conflicts with SVMs’ purpose of achieving the so-called global optimization1. First, this approach may constrain SVMs to local features because of the leftto-right scanning strategy. Furthermore, like other nongenerative Markov models, it suffers from the so-called 1By global we mean the use of quadratic optimization in margin maximization. label bias problem, which means that the transitions leaving a given state compete only against each other, rather than against all other transitions in the model (Lafferty et al., 2001). Intuitively, it is the local normalization that results in the label bias problem. One way of using discriminative machine learning algorithms in sequential models is to rerank the n-best outputs of a generative system. Reranking uses global features as well as local features, and does not make local normalization. If the output set is large enough, the reranking approach may help to alleviate the impact of the label bias problem, because the victim parses (i.e. those parses which get penalized due to the label bias problem) will have a chance to take part in the reranking. In recent years, </context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmentation and labeling sequence data. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yaoyong Li</author>
<author>Hugo Zaragoza</author>
<author>Ralf Herbrich</author>
<author>John ShaweTaylor</author>
<author>Jaz Kandola</author>
</authors>
<title>The perceptron algorithm with uneven margins.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference ofMachine Learning.</booktitle>
<contexts>
<context position="19610" citStr="Li et al., 2002" startWordPosition="3480" endWordPosition="3483">h of these two models, the loss functions are computed directly on the feature space. All the features are manually defined. In (Collins and Duffy, 2002), the Voted Perceptron algorithm was used to in parse reranking. It was shown in (Freund and Schapire, 1999; Graepel et al., 2001) that error bound of (voted) Perceptron is related to margins existing in the training data, but these algorithm are not supposed to maximize margins. Variants of the Perceptron algorithm, which are known as Approximate Maximal Margin classifier, such as PAM (Krauth and Mezard, 1987), ALMA (Gentile, 2001) and PAUM (Li et al., 2002), produce decision hyperplanes within ratio of the maximal margin. However, almost all these algorithms are reported to be inferior to SVMs in accuracy, while more efficient in training. Furthermore, these variants of the Perceptron algorithm take advantage of the large margin existing in the training data. However, in NLP applications, samples are usually inseparable even if the kernel trick is used. SVMs can still be trained to maximize the margin through the method of soft margin. 5 Experiments and Analysis We use SVMlight (Joachims, 1998) as the SVM classifier. The soft margin parameter C </context>
</contexts>
<marker>Li, Zaragoza, Herbrich, ShaweTaylor, Kandola, 2002</marker>
<rawString>Yaoyong Li, Hugo Zaragoza, Ralf Herbrich, John ShaweTaylor, and Jaz Kandola. 2002. The perceptron algorithm with uneven margins. In Proceedings of the International Conference ofMachine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1994</date>
<contexts>
<context position="20394" citStr="Marcus et al., 1994" startWordPosition="3611" endWordPosition="3614">efficient in training. Furthermore, these variants of the Perceptron algorithm take advantage of the large margin existing in the training data. However, in NLP applications, samples are usually inseparable even if the kernel trick is used. SVMs can still be trained to maximize the margin through the method of soft margin. 5 Experiments and Analysis We use SVMlight (Joachims, 1998) as the SVM classifier. The soft margin parameter C is set to its default value in SV Mlight. We use the same data set as described in (Collins, 2000; Collins and Duffy, 2002). Section 2-21 of the Penn WSJ Treebank (Marcus et al., 1994) are used as training data, and section 23 is used for final test. The training data contains around 40,000 sentences, each of which has 27 distinct parses on average. Of the 40,000 training sentences, the first 36,000 are used to train SVMs. The remaining 4,000 sentences are used as development data. The training complexity for SVMlight is roughly O(n2.1) (Joachims, 1998), where n is the number of the training samples. One solution to the scaling difficulties is to use the Kernel Fisher Discriminant as described in (Salomon et al., 2002). In this paper, we divide training data into slices to </context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1994</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1994. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Platt</author>
</authors>
<title>Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods.</title>
<date>1999</date>
<booktitle>In Advances in Large Margin Classifiers.</booktitle>
<publisher>MIT Press.</publisher>
<contexts>
<context position="1581" citStr="Platt, 1999" startWordPosition="245" endWordPosition="246">ze the margin is believed to be the reason for SVMs’ superiority over other classifiers. In addition, SVMs can achieve high performance even with input data of high dimensional feature space, especially because of the use of the ”kernel trick”. However, the incorporation of SVMs into sequential models remains a problem. An obvious reason is that the output of an SVM is the distance to the separating hyperplane, but not a probability. A possible solution to this problem is to map SVMs’ results into probabilities through a Sigmoid function, and use Viterbi search to combine those probabilities (Platt, 1999). However, this approach conflicts with SVMs’ purpose of achieving the so-called global optimization1. First, this approach may constrain SVMs to local features because of the leftto-right scanning strategy. Furthermore, like other nongenerative Markov models, it suffers from the so-called 1By global we mean the use of quadratic optimization in margin maximization. label bias problem, which means that the transitions leaving a given state compete only against each other, rather than against all other transitions in the model (Lafferty et al., 2001). Intuitively, it is the local normalization t</context>
<context position="22612" citStr="Platt, 1999" startWordPosition="4007" endWordPosition="4008">s in theory. On the other hand, the current size of the slice is large enough for other NLP applications like text chunking, although it is not large enough for parse reranking. The last reason is that we have achieved state-of-the-art results even with the sliced data. We have used both a linear kernel and a tree kernel. For the linear kernel test, we have used the same dataset as that in (Collins, 2000). In this experiment, we first train 22 SVMs on 22 distinct slices. In order to combine those SVMs results, we have tried mapping SVMs’ results to probabilities via a Sigmoid as described in (Platt, 1999). We use the development data to estimate parameter A and B in the Sigmoid 1 Pi(y = 1|fi) = 1 + Ae−f:B , (14) Table 1: Results on section 23 of the WSJ Treebank. LR/LP = labeled recall/precision. CBs = average number of crossing brackets per sentence. 0 CBs, 2 CBs are the percentage of sentences with 0 or ≤ 2 crossing brackets respectively. CO99 = Model 2 of (Collins, 1999). CH00 = (Charniak, 2000). CO00 = (Collins, 2000). ≤40 Words (2245 sentences) Model LR LP CBs 0 CBs 2 CBs CO99 88.5% 88.7% 0.92 66.7% 87.1% CH00 90.1% 90.1% 0.74 70.1% 89.6% CO00 90.1% 90.4% 0.73 70.7% 89.6% SVM 89.9% 90.3% </context>
</contexts>
<marker>Platt, 1999</marker>
<rawString>John Platt. 1999. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. In Advances in Large Margin Classifiers. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jesper Salomon</author>
<author>Simon King</author>
<author>Miles Osborne</author>
</authors>
<title>Framewise phone classification using support vector machines.</title>
<date>2002</date>
<booktitle>In Proceedings ofICSLP</booktitle>
<contexts>
<context position="20938" citStr="Salomon et al., 2002" startWordPosition="3703" endWordPosition="3706">and Duffy, 2002). Section 2-21 of the Penn WSJ Treebank (Marcus et al., 1994) are used as training data, and section 23 is used for final test. The training data contains around 40,000 sentences, each of which has 27 distinct parses on average. Of the 40,000 training sentences, the first 36,000 are used to train SVMs. The remaining 4,000 sentences are used as development data. The training complexity for SVMlight is roughly O(n2.1) (Joachims, 1998), where n is the number of the training samples. One solution to the scaling difficulties is to use the Kernel Fisher Discriminant as described in (Salomon et al., 2002). In this paper, we divide training data into slices to speed up training. Each slice contains two pairs of parses from each sentence. Specifically, slice i contains positive samples (( ˜pk, pki), +1) and negative samples ((pki, ˜pk), −1), where ˜pk is the best parse for sentence k, pki is the parse with the ith highest loglikelihood in all the parses for sentence k and it is not the best parse. There are about 60000 parses in each slice in average. For each slice, we train an SVM. Then results of SVMs are put together with a simple combination. It takes about 2 days to train a slice on a P3 1</context>
</contexts>
<marker>Salomon, King, Osborne, 2002</marker>
<rawString>Jesper Salomon, Simon King, and Miles Osborne. 2002. Framewise phone classification using support vector machines. In Proceedings ofICSLP 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Shawe-Taylor</author>
<author>Peter L Bartlett</author>
<author>Robert C Williamson</author>
<author>Martin Anthony</author>
</authors>
<title>Structural risk minimization over data-dependent hierarchies.</title>
<date>1998</date>
<journal>IEEE Trans. on Information Theory,</journal>
<volume>44</volume>
<issue>5</issue>
<contexts>
<context position="8209" citStr="Shawe-Taylor et al., 1998" startWordPosition="1375" endWordPosition="1378"> training data error, number of training samples and the capacity of the learning machine (Smola et al., 2000). Vapnik-Chervonenkis (VC) dimension (Vapnik, 1999), as well as some other measures, is used to estimate the complexity of the hypothesis space, or the capacity of the learning machine. The drawback of VC dimension is that it ignores the structure of the mapping from training samples to hypotheses, and concentrates solely on the range of the possible outputs of the learning machine (Smola et al., 2000). In this paper we will use another measure, the so-called Fat Shattering Dimension (Shawe-Taylor et al., 1998), which is shown to be more accurate than VC dimension (Smola et al., 2000), to justify our voting algorithm, Let F be a family of hypothesis functions. The fat shattering dimension of F is a function from margin p to the maximum number of samples such that any subset of 2SVM’s theoretical accuracy is much lower than their actual performance. The ability to maximize the margin is believed to be the reason for SVMs’ superiority over other classifiers. 1 LD(α) ≡ 2 αi − Xt i=1 Xt i,j=1 1 LD(α) ≡ 2 αi − Xt i=1 Xt i,j=1 these samples can be classified with margin ρ by a function in F. An upper boun</context>
</contexts>
<marker>Shawe-Taylor, Bartlett, Williamson, Anthony, 1998</marker>
<rawString>John Shawe-Taylor, Peter L. Bartlett, Robert C. Williamson, and Martin Anthony. 1998. Structural risk minimization over data-dependent hierarchies. IEEE Trans. on Information Theory, 44(5):1926–1940.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A J Smola</author>
<author>P Bartlett</author>
<author>B Sch¨olkopf</author>
<author>C Schuurmans</author>
</authors>
<title>Introduction to large margin classifiers.</title>
<date>2000</date>
<booktitle>Advances in Large Margin Classifiers,</booktitle>
<pages>1--26</pages>
<editor>In A.J. Smola, P. Bartlett, B. Sch¨olkopf, and C. Schuurmans, editors,</editor>
<publisher>MIT Press.</publisher>
<marker>Smola, Bartlett, Sch¨olkopf, Schuurmans, 2000</marker>
<rawString>A.J. Smola, P. Bartlett, B. Sch¨olkopf, and C. Schuurmans. 2000. Introduction to large margin classifiers. In A.J. Smola, P. Bartlett, B. Sch¨olkopf, and C. Schuurmans, editors, Advances in Large Margin Classifiers, pages 1–26. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir N Vapnik</author>
</authors>
<title>Statistical Learning Theory.</title>
<date>1998</date>
<publisher>John Wiley and Sons, Inc.</publisher>
<contexts>
<context position="4345" citStr="Vapnik, 1998" startWordPosition="696" endWordPosition="697">ernative way of using a large margin classifier for sequential models. Instead of using the parse tree itself as a training sample, we use a pair of parse trees as a sample, which is analogous to the preference relation used in the context of ordinal regression (Herbrich et al., 2000). Furthermore, we justify the algorithm through a modification of the proof of the large margin rank boundaries for ordinal regression. We then apply this algorithm to the parse reranking problem. 1.1 A Short Introduction of SVMs In this section, we give a short introduction of Support Vector Machines. We follow (Vapnik, 1998)’s definition of SVMs. For each training sample (yi, xi), yi represents its class, and xi represents its input vector defined on a d-dimensional space. Suppose the training samples {(y1, x1), ..., (yn, xn)} (xi ∈ Rd, yi ∈ {−1, 1}) can be separated by a hyperplane H: (x • w) + b = 0, which means yi((xi • w) + b) ≥ 1, (1) where w is normal to the hyperplane. To train an SVM is equivalent to searching for the optimal separating hyperplane that separates the training data without error and maximizes the margin between two classes of samples. It can be shown that maximizing the margin is equivalent</context>
<context position="6818" citStr="Vapnik, 1998" startWordPosition="1154" endWordPosition="1155">sgn( X αjyjK(sj, x) + b) (4) j=1 where sj is a training vector whose corresponding Lagrange multiplier αj &gt; 0. sj is called a support vector. Ns is the total number of the support vectors. According to (4), the decision function only depends on the support vectors. It is worth noting that not any function K can be used as a kernel. We call function K : Rd × Rd 7→ R a well-defined kernel if and only if there is a mapping function Φ : Rd 7→ Rh such that, for any xi, xj ∈ Rd, K(xi, xj) = Φ(xi) • Φ(xj). One way of testing whether a function is a well-defined kernel is to use the Mercer’s theorem (Vapnik, 1998) by utilizing the positive semidefinteness property. However, as far as a discrete kernel is concerned, there is a more convenient way to show that a function is a well-defined kernel. This is achieved by showing that a function K is a kernel by finding the corresponding mapping function Φ. This method was used in the proof of the string subsequence kernel (Cristianini and Shawe-Tayor, 2000) and the tree kernel (Collins and Duffy, 2001). 1.2 Large Margin Classifiers SVMs are called large margin classifiers because they search for the hyperplane that maximizes the margin. The validity of the la</context>
</contexts>
<marker>Vapnik, 1998</marker>
<rawString>Vladimir N. Vapnik. 1998. Statistical Learning Theory. John Wiley and Sons, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir N Vapnik</author>
</authors>
<date>1999</date>
<journal>The Nature of Statistical Learning Theory. Springer,</journal>
<volume>2</volume>
<pages>edition.</pages>
<contexts>
<context position="7744" citStr="Vapnik, 1999" startWordPosition="1299" endWordPosition="1300">in the proof of the string subsequence kernel (Cristianini and Shawe-Tayor, 2000) and the tree kernel (Collins and Duffy, 2001). 1.2 Large Margin Classifiers SVMs are called large margin classifiers because they search for the hyperplane that maximizes the margin. The validity of the large margin method is guaranteed by the theorems of Structural Risk Minimization (SRM) under Probably Approximately Correct (PAC) framework2; test error is related to training data error, number of training samples and the capacity of the learning machine (Smola et al., 2000). Vapnik-Chervonenkis (VC) dimension (Vapnik, 1999), as well as some other measures, is used to estimate the complexity of the hypothesis space, or the capacity of the learning machine. The drawback of VC dimension is that it ignores the structure of the mapping from training samples to hypotheses, and concentrates solely on the range of the possible outputs of the learning machine (Smola et al., 2000). In this paper we will use another measure, the so-called Fat Shattering Dimension (Shawe-Taylor et al., 1998), which is shown to be more accurate than VC dimension (Smola et al., 2000), to justify our voting algorithm, Let F be a family of hypo</context>
</contexts>
<marker>Vapnik, 1999</marker>
<rawString>Vladimir N. Vapnik. 1999. The Nature of Statistical Learning Theory. Springer, 2 edition.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>