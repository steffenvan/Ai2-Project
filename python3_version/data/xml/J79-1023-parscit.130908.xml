<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.606275333333333">
American Journal of Computational Linguistic.; Microfiche 23
Copyright 1975 by the Association for.Lomputational Linguistics
AUTONOTE2 : NETWORK MEDIATED
</title>
<sectionHeader confidence="0.936287" genericHeader="method">
NATURAL LANGUAGE COMMUNICATION
IN A PERSONAL
INFORMATION RETRIEVAL SYSTEM
William E. Linn, Jr.2
and
</sectionHeader>
<author confidence="0.506873">
Walter Reitman
</author>
<affiliation confidence="0.658919">
UTliversity of Michigan
</affiliation>
<address confidence="0.213897">
Ann Arbor
</address>
<note confidence="0.8291342">
1Tnis paper is based on a doctoral dissertation by the first author. Support
from the National Science Foundation under Grant No. DCR71-02038 is gratefully
acknowledged. Those wishing more complete details about system commands and
Implementation should,write the second author for a User&apos;s Manual.
2Now at Southern Ratlway System, 125 Spring Street, S.W., Atlanta, Georgia 30303
</note>
<sectionHeader confidence="0.840307" genericHeader="method">
ABSTRACT
</sectionHeader>
<subsectionHeader confidence="0.79302">
Natural language combines nouns and adjectives into noun phrases, and
</subsectionHeader>
<bodyText confidence="0.998675523809524">
links phrases by means of prepositions to form complex descriptions of
objects and topics. AUTONOTE2, a file-oriented retrieval system, allows
the user to employ such descriptions to characterize the items of informa-
tiaa he wiehes to store and retrieve. in addition, the system also con-
structs a network representation of the user&apos;s subject matter, using syntac-
tic analysis to derive dependency structures frikam Us descriptions. The
dependency information, expressed as subordinate and coordinate linkages
among the phrases, is represented by a tree of nodes, with simple phrases
at the terminal branches. The PARSER uses the network to disambiguate des-
criptions, querying the user only abbut residual ambiguities.
Associated with the PARSER is a network LOCATOR, which determines
whether a cUrrent user description refers to an existing topic at some level
in the network. The LOCATOR also builds a table specifying the changes, if
any, to be made in a network in- order to represent the topic inferred from
the current input description. For example, if the user&apos;s description con-
tains one or more simple phrases (thereafter referred to as active) directly
describing at least one existing node in the network, the description as a
whole quite likely references an existing network topic. To locate it, the
PARSER first determines the focus phrase, the active phrase at the highest
dependency level. The nodes directly described by the focus phrase are
used to generate candidate topics. These then are matched against the
</bodyText>
<page confidence="0.790909">
2
</page>
<sectionHeader confidence="0.658154" genericHeader="method">
3
</sectionHeader>
<bodyText confidence="0.929241">
remaining active phrases obtained from the description to determine the
most likely referent.
</bodyText>
<subsectionHeader confidence="0.527781">
Many of the procedures employed in deScription and representation also
</subsectionHeader>
<bodyText confidence="0.999236136363637">
are used in network-mediated&apos; retrieval. The user mar initiate retrieval
with a FIND command, supplying a description as argument. The resultant
phraSe table is passed along to the network LOCATOR, which returns a node
nudber to the FIND processor. The FIND processor constructs a set of item
numbers by extracting the textual references from the node. The system
then checks for upward pointers from the node. If there are. structurally
related topics, the FIND processor so informs the user. Note that by virtue
of network Mediation of retrieval, if a user description Is imprecise or
incorrect, the system may be Able to direct the user to relevant related
topics.
Wheit the system queries the user about a topic, for example to deter-
mine the intent-of a description, the tepic node number is passed to a
SPEAKER component. A phrasal description of the node Is returned. To
minimize redundant communication, a level indicator may be set according to
the level of diatail in the user&apos;s description. For example, if the user
describes an item as RESULTS OF THE EXPERIMENT and the system .must ask it he
is referring to SMITH&apos;S EXPERIMENT ON THE SHORT TERM MEMORY OF WHITE RATS,
the resulting query would be: ARE YOU REFERRING TO SMITH&apos;S EXPERIMENT ON
MEMORY?
Coastruction of a description from the network takes place in two&apos;
stages. The first stage steps thorugh the network recursively, collecting
the simple phrases that directly or indirectly describe the specified node.
</bodyText>
<sectionHeader confidence="0.42869" genericHeader="method">
4
</sectionHeader>
<bodyText confidence="0.9330403125">
The level indicator blocks collection of simple phrases below the specified
level. The second stage is carried out by a recursive algorithm that
operates on the tabled simple phraqcs and their interrelations to construct
the phrasal description.
The last major component of the system handles network modification
and reorganization. This enables the user to add or remove references and
phrases, and to modify,, delete, or reorganize his topic structure.
A detailed ease study comparing AUTONOTE2 with a good keyword-based
retrieval system showed that for a coherent body of material, the communi-
cative efficiency of AUTONOTE2, as measured &apos;by the ratio of the number of
w)rds conveyed to the number of words entered, was more than double that of
the keyword-based system. Retrieval capability was enhanced considerably,
and the representation tetwOrk effectively distinguished among the many
topics partially indexed by the same words. Furthermore, SPEAKER output of
topics from the representational network proved a useful retrieval inter-
mediary, greatly reducing the ne-ed for perusal of item texts.
</bodyText>
<sectionHeader confidence="0.898312" genericHeader="method">
TABLE OF CONTENTS
</sectionHeader>
<table confidence="0.971142739130435">
Page
I. INTRODUCTION .. . 7
II. THE AUTONOTE SYSTEM 9
III. Basic AMONOTE Commands. . . . 10
AUTONOTE System Organization 14
OVERVIEW OF AUTONOTE2 16
The Description Language 17
Representational Framework 19
Criteria for the RepresentatioA 19
Overview of the AUTONOTE2 Implementation 24
IV. THE PARSER ANM THE RgPRESENTATIONAL NETWORK . • . .
26
Overview , 26
Design of the Network Data Structures 32
Storage Implementation of the Network 37
The Representational Network: An Example 40
Parsing of Descriptions 40
Implementation of the Parser 48
V. THE NETWORK LOCATOR 54
VI. NETWORK MEDIATED RETRIEVA1 68
Retrieval via Descriptions, 68
Interrogating the Network 71
The SPEAKER Component 73
</table>
<figure confidence="0.5640555">
5
6
Page
VII. NETWORK MODIFICATION 78
</figure>
<title confidence="0.8855043">
Adding References and Phrases to the Network 79
Moving through the Network I • 81
The Caching Facility 81
Retrieval Commands 82
Removing References and Phrases from the Network 82
Topic Deletion 82
Creating New Topic Representations 86
VIII. A CASE STUDY OF SYSTEM PERFORMANCE 87
The Inapplicability of Recall and Precfsion 87
The Sauvain Data Base 88
</title>
<sectionHeader confidence="0.97352625" genericHeader="method">
Results 88
Conclusion 94
REFERENCES 97
I. INTRODUCTION
</sectionHeader>
<bodyText confidence="0.989933361111111">
When two humans communicate, each party builds up a conceptual represen-
tation of the topics of discussion. Such representations are fundamental to
human communicative efficiency. The listener&apos;s representation of the topics
already discussed facilitates communication in that the speaker is spared
the trouble of describing in complete detail those things to which he refers.
Furthermore, the speaker can proceed to related topics without having to
describe them in full. For example, a speaker who has been talking about the
design of a particular experiment can safely move on to discuss the results
of the experiment without specifying anew the experiment he has in mind.
We use the term referential communication to indicate the process by
which a speaker communicates a reference to some subject or topic to a
listener. If within the environment of a personal information system, we
view the information universe as a collection of textual materials each
pertinent to one or more &amp;quot;topics,&amp;quot; then one can readily construct an analogy,
The user and system take on the roles of speaker and listener, respectively.
The domain of discourse is a set of topic descriptions characterizing the
User&apos;s textual materials. The user enters his materials and describes to
the system the topic or topics to which they pertain. During this process,
the system constructs its own representation for the subjects the user has
described, and associatds each piece of text with its, corresponding topic
representations.
This paper describes the design and implementation of al personal infor-
mation storage and retrieval system based on the foregoing analogy with
8
human referential communication. It presents a hierarchical network data
structure for representing topic descriptions formulated within a phrasal
description language. Called the representational network, this structure
enables the system to move easily from one Subject to other related ones.
It provides a means for representing the user&apos;s working context, thereby
enabling the user to describe his materials much more tersely than is possi-
ble in keyword-based systems. The system makes use of the syntactic depen-
dencies among the words and phrases of descriptions in order to represent
structural relationships among the user&apos;s topics. Consequently, the user
imparts structure to the data base in a particularly natural way, eliminating
much of the organization activity normally associated with keyword-based
systems. Our central thesis is that the network mediated techniques provide
for more effective man-machine communication during the processes of des-
cription, organization, and retrieval within a personally generated informa-
tion universe
The procedures used here differ substantially from the typical keyword
indexing and retrieval mechanisms of other personal retrieval systems. The
central, objective is to provide the user with A framework for defining the
important topics or informational objects he deals with, and to enable him to
easily associate items in his data base with these entities. Rather than
viewing the data base as a collection of items and associated index terms,
the user deals with &amp;quot;objects&amp;quot; that are in some sense meaningful to him.
Whether retrieving information or indexing new material the user conveys
references to the appropriate topics. This shift in the user&apos;s view of his
information universe, coupled with the mechanisms we have developed for
9
building up and referring to the topic framework, oonstitute the substance
of our approach to personal information storage and retrieval.
II. THE AUTONOTE SYSTEM
The system described here uses the AUTONOTE information storage and
retrieval system (Reitman et al., 1969) as a base. AUTONOTE is an on-line
retrieval system that runs a§ a user program under the Michigan Terminal
System (MTS), a time-sharing system implemented on the IBM 370/168. The
basic units of information stored in AUTONOTE are called items. The user
may enter arbitrary textual materials into an item and may assign descripbors
by which these materials can be retrieved. Retrieval requests take the form
of single descriptors or combinations of descriptors connected by AND, OR, or
NOT logical Operators. Facilities are provided for deleting, replacing,
linking, and hierarchically organizing text item.
AUTONOTE makes extensive use of the MTS disk file system. MTS disk
files (line files) may be read or written either sequentially or in an indexed
fashion by specifying a line file number. AUTONOTE maintdins two line files
for each user&apos;s data base; one for storing textual materials and bookkeeping
information, the other for storing a descriptor index. Each text item
occupies a specific region of the line number range of the text file. The
descriptor index, on the other hand, is accessed through an efficient hash
coding algorithm that maps each descriptor into an index file line number.
The descriptor index is organized as an inverted file, that is, each line in
</bodyText>
<page confidence="0.388426">
10
</page>
<bodyText confidence="0.9590285">
the index contains pointers to each of the text items assigned the descriptor
for that line.
</bodyText>
<subsectionHeader confidence="0.882366">
Basic AUTONOTE Commands
</subsectionHeader>
<bodyText confidence="0.994884523809524">
Text entry. To enter a new text item, the user first types the command
ENTER and the system responds with a numerical tag for the new item. The
system then enters a &amp;quot;.ext insertion mode&amp;quot; and indicates its readiness to
accept successive text lines with a question mark. After entering text, the
user may return to &amp;quot;command mode&amp;quot; by entering a null line or an end-of-file
Indication. Should the user at any time wish to continue inserting text into
the current item, he may re-enter text insertion mode via the INSERT command.
Subsequent lines are placed below the most recent line for the current item
in the text file.
In command mode, the system prompts the user for input with a minus sign.
The user may give each command in full or he may abbreviate by giving any
initial substring of the command name.
Descriptor entry. To associate one or more descriptors with the current
text item, the user enters a list of words, beginning the input line with an
at sign (@). Any character string up to 16 characters in length may be used
as a descriptor. In addition to updating the descriptor index, the system
also places the actual &amp;quot;@-line&amp;quot; in the text file in a subregion beneath the
text of the current item.
Retrieval. To display a partciular text item the user may enter the
command PRINT followed by the appropriate item number. Sequential blocks of
items can also be specified in the PRINT command, e.g., PRINT 77...85.
</bodyText>
<page confidence="0.577291">
11
</page>
<bodyText confidence="0.953465092592593">
In most cases, however, the specific item number(s) will not be known.
The LIST command accepts a descriptor or logical combination of descriptors
as its argument and responds with a list of the item numbers that satisfy
the query. The functions of the PRINT and LIST commands are combined in the
RETRIEVE,command. It also takes a descriptor specification as argument and
causes each item in -the resulting list to be PRINTed.
Definitional facility. AUTONOTE also provides a definitional facility
that allows the user to -create sets of items referenced by arbitrary com-
binations of descriptors. For example, the command CREATE $IRS= INFORMATION
AND RETRIEVAL AND SYSTEMS adds a new descriptor, $1RS, to the index that
references each item having the words INFORMATION, RETRIEVAL, and SYSTEMS as
descriptors. Any defined term may be used just as any other descriptor in
retrieval requests; they may also be used to deft wo other new terms (e.g.,
CREATE OTHERSYSTEMS= $1RS NOT AUTONOTE).
The definitional facility is also invoked implicitly each time the user
issues a retrieval query. The set of items referenced by the most recent
LIST or RETRIEVE command, called the active set, is assigned the name $.
Should the user wish to refine the results of the previous query, he has
access to the active set. To facilitate this process, each time a missing
descriptor is noted in a retrieval request the descriptor $ is inserted
automattically by the system. For example, the command LIST NOT FORTRAN is
interpreted as LIST $ NOT FROTRAN, i.e., the old active set of items is
restricted to include only those not referenced by the descriptor FORTRAN.
This operation, of course, redefines the active set.
12
Item-item linkages. The ability to define associative links between
any two text items is provided by the APPEND command. When an item is dis-
played, its associative links to other items may optionally be printed along
with a user-specified comment indicating the nature of the association.
Tutorial feature. Throughout the course of its development, AUTONOTE
has been employed to collect, organize, and maintain up-to-date documentation
of its capabilities, usage strategies, and so on. This information is stored
in a publically available data base. It includes brief descriptions of each
of the commands, announcements of recent developments and system changes, and
other instructive information. The AUTONOTE user may call upon this store of
material by entering a HELP command. The user s data base is temporarily set
aside and the public data base is attadhed to the system. The user may then
retrieve instructive information in the same way that he operates with his
awn data base. To assist novice users, the system will optionally print
instructions for accessing the HELP data base.
Grouping. AUTONOTE provides a grouping facility which permits the user
to organize text items in several useful ways. It enables the user to define
a &amp;quot;grouping item&amp;quot; which references an arbitrarily ordered list of other items
This is done by entering into an item an @-line of the form:
@GROUP N1 N2 N3 &apos;
Since any item can represent a gioup0.it is possible to form a complex hier-
archical structure in this way.
A grouping item can be viewed as a node of an inverted tree structure
with downward branches to those items listed in its &amp;quot;@GROUP&amp;quot; line. A request
to display a grouping item initiates recursive processing of the tree
13
structure to identify the terminal and nonterminal items of the hierarchy.
The user may request that only terminal of ,nonterminal items be displayed,
or tW,,the entire list of materials be printed.
</bodyText>
<subsectionHeader confidence="0.671501">
The organization of the HELP data base described above provides an
</subsectionHeader>
<bodyText confidence="0.981675708333333">
excellent example of the power and flexibility of the grouping facility.
The HELP text file contains at this writing approximately 150 items of
documentation. Using the grouping convention, these are organized into
five subgroups: (1) general information; (2) input and editing facilities;
(3) output (retrieval) facilities; (4) organizational facilities; and (5)
utility commands. There-is one major item which groups all of these sub-
groups into a single tree structure. The top node of the structure is
indexed by the descriptor &apos;USERS-MANUAL. As new facilities are incorporated
into the system, their descriptions are entered into the manual structure,
thus assuring that complete and up-to-date documentation is always avail-
able. At any time, the single command: RETRIEVE USERS-MANUAL causes the
entire updated data base to be displayed in organized form;
Command modifiers. AUTONOTE includes a set of modifiers or option
settings that control the execution of many commands. These include options
that affect the format of displayed items, the expansion of grouping struc-
tures, the nature and extent of system feedback, etc. Each of the modifiers
has a default.value that is chosen to simplify use of the system by a
novice. The more experienced user may alter the modifiers via the SET com-
mand to, tailor the system to his own needs, usage patterns and level of
competence.
14
AUTONOTE also provides a large number of auxiliary commands and facili-
ties. A list of the major AUTONOTE commands, each accompanied by a brief
description, is inclUded in Linn (1972, Appendix A).
</bodyText>
<sectionHeader confidence="0.895444" genericHeader="method">
AUTONOTE System Organization
</sectionHeader>
<bodyText confidence="0.9999318">
AUTONOTE has been designed as a modular system so that as new facili-
ties became available they may be tested and later added with little or no
reprogramming of the misting system. The majority of AUTONOTE commands
are implemented as subroutines, each of which resides permanently in an MTS
disk file. The basic system is organized around a central monitor that
accepts user input and calls upon appropriate modules to service the user&apos;s
requests. In addition to the monitor, the core resident system includes a
dynamic loader, a disk file interface and a set of frequently used utility
routines. A number of primitive commands, text entry, and descriptor assign-
ment are also handled by the resident system. As the user requests more
complex serVices (LIST, RETRIEVE, or PRINT, for example), the monitor calls
upon the dynamic loader to bring the appropriate modules into core storage.
These routines then became a part of the resident system, remaining in core
storage until the user explicitly requests their removal. An organizational
diagram of the AUTONOTE system appears in Fig. 1.
The modular design of AUTONOTE coupled with the dynamic loading facility
offers two important benefits. From the user s viewpoint, he has access to
the complete repertory of AUTONOTE services, yet he pays core storage
charges only for those routines he actually uses during a given session.
To the developers of the system, the modular framework facilitates the
</bodyText>
<figure confidence="0.973841722222222">
DISK
STORAGE
15
INPUT/OUTPUT
ROUTINES
MONITOR
UTILITY
Roirrigg
VIMINIMMINNIMION111116
LIST
COMMAND
PROCESSOR
RETRIEVE
COMMAND
PROCESSOR
PRINT
COMMAND
PROCESSOR
</figure>
<figureCaption confidence="0.968604">
Fig. 1 - AUTONOTE System Organization
</figureCaption>
<bodyText confidence="0.967276333333333">
16
addition of new system components. The latter has been an important factor
in the implementation of the AUTONOTE21wstem.
</bodyText>
<sectionHeader confidence="0.98615" genericHeader="method">
III. OVERVIEW OF AUTONOTE2
</sectionHeader>
<subsectionHeader confidence="0.908581">
The AUTONOTE2 system uses ideas (Reitman, 1965; Reitman et al., 1969)
</subsectionHeader>
<bodyText confidence="0.961943470588235">
concerning the use of our &amp;quot;knowledge of the world&amp;quot; to disambiguate and fill
in implied facts when conversing with one another. In particular, the system
design is based upon the assumption that efficient human communication
&amp;quot;depends upon the listenerts ability to make inferences from prior informa-
tion, from context, and from a knowledge of the speaker and the world. Com-
municating in this way, we risk occasional misunderstanding as the price for
avoiding verbose, redundant messages largely consisting of material the
listener already knows&amp;quot; (Reitman et al., 19691.
In our more restricted domain of discourse, we view the process of human
referential communication as onek,guided by some form of internal representa-
tion of the various topics or referents discussed earlier. When a listener
can be assumed to have such a representation, the speaker is spared the dif-
ficulty of describing in complete detail the things to which he refers. He
need only give enough information to allow the referent to be discerned in
full. Our goal then is to develop a representational scheme for our retrieval
system that allows the user analogous communicative efficiencies.
17
</bodyText>
<subsectionHeader confidence="0.922933">
The Description Language
</subsectionHeader>
<bodyText confidence="0.974762782608696">
The first step in devising a representational framework was the formu-
lation of a language for expressing topic descriptions to the system.
Although anunderlying factor in the design of AUTONOTE2 was to make com-
municatioh with the system more &amp;quot;natural,&amp;quot; it should be noted that the
emphasis of this research is not upon parsing or &amp;quot;understanding&amp;quot; natural
language. Rather, our goal is to investigate the notions of topic repre-
sentation and referential communication as a means for improving the user&apos;s
ability to describe, organize, and retrieve his materials. Consequently,
a minimal subset of noun phrases was chosen--minimal in the sense that it
excludes most of the complexity of natural English, yet still retains a
degree of descriptive richness sufficient to explore the underlying ideas of
this study.
Natural language enables us to combine nouns and adjectives into noun
phrases and to interlink noun phrases via prepositions to form complex des-
criptions of objects in the real world. The AUTONOTE2 description language
provides such a framework for composing topic references. A formal grammar
for the language is given in Fig. 2 along with a few sample descriptions that
illustrate the flexibility of expression achievable with the language. These
grammatical rules are not in fact used explicitly by the system in actually
parsing topic descriptions. The grammar is presented here only to specify
precisely the set of descriptions acceptable to the system. The actual
AUTONOTE2 parser is heuristic-based, making use of previously analyzed
phrases, noun-preposition co-occurrences, and a set of heuristics to guide
</bodyText>
<figure confidence="0.403452">
18
</figure>
<figureCaption confidence="0.964643">
Fig. 2 - The AUTONOTE2 Description Language
</figureCaption>
<bodyText confidence="0.9979662">
aModifiers and nouns are arbitrary character strings not recognized as
articles or prepositions. When a number of consecutive &amp;quot;words&amp;quot; are encountered,
the last is parsed as a noun and the preceding words as modifiers.
bPossessive adjectives are treated as a special case of adjectival modifi-
cation.
</bodyText>
<figure confidence="0.846138291666667">
&lt;description&gt; ::=
&lt;noun-group&gt; ::=
&lt;modifier-group&gt; • • =
• •
&lt;preposition&gt; ::=
&lt;article&gt; ::=
&lt;noun-group&gt;
&lt;noun-group&gt; &lt;preposition&gt; &lt;description&gt;
(&lt;article&gt;) (&lt;modifier-group&gt;) &lt;noun&gt; a
&lt;modifier&gt;a 1
&lt;modifier&gt; &lt;modifier-group&gt;
about 1 to from in I on etc.
a an 1 the
(a) Grammar for the description language.
The paper
The paper about microprogramming in the proceedings of the fall joint computer
conference
Notes on the organization of AUTONOTE2 for use in the presentation of the ACM
paper
The use of recall precision measures in the evaluation of the SMART information
retrieval system
Quotes from Feldman&apos;s 1969 paper for use in the introduction of the second
chap ter&apos;
(b) Sample descriptions.
</figure>
<bodyText confidence="0.9737245">
the parsing process. In some instances, the user may even be asked for
parsing assistance.
Representational Framework
Central to the design of AUTONOTE2 is the idea of viewing the user&apos;s
information universe as a collection of &amp;quot;informational objects&amp;quot; or topics,
each having associated with it a number of text items. When the user wishes
to describe a text item, we assume he has such a topic in mind. Using the
phrasal language specified above, he composes a description of that topic
and presents it to the system. AUTONOTE2 then constructs an internal repre-
sentation of that topic. When a text item is described, the system must
consult the representation to determine if the description (1) references an
existing topic, (2) is related to an existing topic, or (3) defines a new
topic. In any case, the ultimate goal is to associate the text item with a
topic representation, possibly augmenting the representation in the process.
Criteria for the Representation
Efficiency of communication. Efficient man-machine communication im-
plies that the user should not in general have to formulate a complete des-
cription of a particular topic in order to convey a reference to it. The
system should be capable of accepting and correctly interpreting incomplete
references by filling in missing information. As an example, a topic fully
described a:F. THE PAPER BY SALTON ABOUT THE SMART SYSTEM might be referred to
as THE PAPER, THE PAPER BY SALTON, THE PAPER ABOUT THE SMART SYSTEM and so on.
A description in the AUTONOTE2 language consists of a noun modified by
adjectives and prepositional phrases. The words that modify any given term
</bodyText>
<page confidence="0.598246">
19
</page>
<bodyText confidence="0.9345239">
20
may themselves be modified in exactly the same way. In effect, each adjec-
tive and prepositional phrase functions as a phrase component that imparts
greater detail to the overall description. In the example above, BY SALTON
and ABOUT THE SYSTEM provide information about the gaper; SMART specifies
which system is meant.
To facilitate efficient communication we require a representational
framework that makes explicit the component phrases of each topic description.
Given such a framework, we have a basis for comparing incomplete descriptions
with the representation to determine possible topic referents.
</bodyText>
<subsectionHeader confidence="0.759564">
Descriptive power.. A system that makes use of syntax in the user&apos;s
</subsectionHeader>
<bodyText confidence="0.998304052631579">
descriptor entries increases descriptive power in that it permits distinctions
that, in general, will not be made in keyword-based retrieval systems. A des-
cription such as THE ORGANIZATION OF THE PAPER ABOUT MTS is semantically
quite different from THE PAPER ABOUT THE ORGANIZATION OF MTS-, despite the fact
that both contain the same words. A system that takes into consideration
the syntactic relationships that hold among the words ORGANIZATION, PAPER,
and MTS can discriminate between the two.
The considerations outlined thus far lead quite naturally to some form
of dependency representation for the user&apos;s topics. Essentially, a depen-
dency representation for the AUTONOTE2 language would reflect the syntactic
dependence of each adjective and prepositional phrase upon an appropriate
noun. Such a framework provides the essential information for enhancing
descriptive power and communicative efficiency as defined above.
21
Hierarchical representations. We view a topic as a group of intercon-
nected subtopics, each bearing on a central theme yet with varying levels of
generality. To make this notion more concrete, consider a user of AUTONOTE2
putting down his thoughts and ideas for a book he is writing. He begins by
entering some general material which he describes simply as &amp;quot;THE BOOK
ABOUT. • • • At some later time he may enter an outline for the book, a list
of reference materials he will use, publishing arrangements, etc. Still
later, he will enter materials for the chapters of his book and perhaps out-
lines for each chapter. In time he will have defined a host of related des-
criptions. Fig. 3 gives a pictorial representation of the resultant complex
&amp;quot;topic.&amp;quot; The representational si-heme uf AUTONOTE2 was designed with complex
hierarchies such as this one in mind. In other words, we want to represent
related topic descriptions via interconnections in a network.
The essential idea is that such a network corresponds to a map of the
organization of the associated textual materials--a map that should reflect
important structural relationships among the materials from the user&apos;s view-
point. A hierarchical representation of this kind is especially effective
during retrieval. If the user requests materials dealing with his book, for
example, the system can also inform him that he has more specific items deal-
ing with the publishing arrangements, the component chapters, and so on.
The notion of a representational network fits well with the dependency
framework we require. The syntactic dependencies among the words and phrases
of a description may be used to represent structural relationships among the
user&apos;s topics. In the example above, the network connection between the
</bodyText>
<figure confidence="0.9476586">
22
OUTLINE
OF CHAPTER 1
• • •
111■1.0.■ ,.MNOW
REFERENCE
MATERIALS
FOR THE BOOK
Pig. 3 - A Topic Hierarchy
23
</figure>
<bodyText confidence="0.97575532051282">
&amp;quot;outline&amp;quot; and the &amp;quot;book&amp;quot; corresponds to the syntactic dependency of &amp;quot;book&amp;quot;
upon &amp;quot;outline&amp;quot; in the description&apos; THE OUTLINE OF THE BOOK ABOUT....
Augmentation of the representation. In the previous discussion of
communicative efficiency we were concerned with associating an incomplete
description with its corresponding topic. In designing the representational
frameufork we also had to consider the case in which a reference provides a
more detailed description of an existing topic. In such instances we want to
enrich the topic representation to include the additional information.
Whether additional descriptive information is encountered in a subsequent
item description or in a retrieval request, we want the system to incorporate
It into its existing knowledge of the user&apos;s topics. This requires that the
representation be structured in such a way that dynamic augmentation is easily
accomplished.
The representation of context. In providing a framework for interpre-
ting terse, incomplete references we naturally are confronted with the problem
of ambiguity. A description such as THE PAPER, or THE PAPER ABOUT MICROPRO-
GRAMMING may in fact satisfy a large number of distinctly described topics.
To deal with this problem we require some kind of contextual framework that
enables the system to infer, where possible, the intent of a vague or ambigu-
ous reference. A user who has been entering material for a paper he is
writing should be able to describe a subsequent item as, say, THE OUTLINE OF
THE PAPER, and have the system infer which paper he means. In general then,
we want the representational framework to include information that identifies
the &amp;quot;working.context,&amp;quot; i.e., those topics the user has referred to recently.
24
System interrogation of the user. Presented with an ambiguous descrip-
tion &amp;quot;out of context,&amp;quot; the system is faced with much the same dilemma a human
listener would face. In such instances, we want the system to be capable of
asking pertinent questions to resolve the ambiguous reference. This implies,
of course, that the representation preserve sufficient information to enable
it to reconstruct descriptions of the user&apos;s topics.
Overview of the AUTONOTE2 Implementation
Data structures. We have now presented the major design requirements
for the representational framework. These preliminary criteria suggest a
representation organized as a network of (possibly interconnected) dependency
structures obtained from syntactic analysis of topic descriptions. The net-
work data structures are discussed in section IV in terms of the representa-
tional criteria and also the computational requirements--how they are to be
accessed, modified, and so on.
The parser. The parsing of descriptions is guided by the state of the
representation at the point they are entered. For this reason, the parsing
algorithm is treated in section IV in conjunction with the representational
data structures. The,presentation includes detailed discussion of the parsing
problems encountered and the heuristics employed in dealing with them.
Network location. The function of the network locator is to analyze the
parse tree to decide whether the description references an existing topic or
defines a new one. Once this decision is made, it constructs a list of any
network modifications required to represent the topic and its associated item
reference. The network location algorithm is described in section V.
25
Retrieval, The AUTONOTE2 retrieval component is invoked via a FIND
command. The command takes a topic description as its argument. The FIND
processor in turn calls upon both the parser and network locator, regaining
control after the appropriate network topic has been identified. Text items
directly associated with the topic then may be retrieved from the data base.
Alternately, the retrieval component will move to structurally related topics
in the representational network to collect additional item references for
subsequent display.
To reconstruct topic descriptions from the network, AUTaNOTE2 includes
a SPEAKER. module. If the user&apos;s description is ambiguous, for example, the
network locator may call for a display of the alternative topics. The FIND
processor employs the SPEAKER to present descriptions of topics structurally
related to the user&apos;s original query. The user also may invoke the SPEAKER
explicitly, via a DESCRIBE command, to obtain descriptions of some subset of
the topics in the representational network. The retrieval component, the
SPEAKER, and the DESCRIBE command are treated in section VI.
Network modification. The last major component sf AUTONOTE2, the net-
work modification processor, is described in section VII. It allows the
user to delete topic representations, create new ones, and merge multiple
topics into a single representation. It also enables the user to move
through clusters of related topics in order to explore associations in the
network.
Auxiliary commands. Various auxiliary commands and facilities are given
in Linn (1972, Appendix BY This appendix also includes some discussion of
usage strategies for achieving the most effective use of AUTONOTE2.
26
Fig. 4 depicts the organization of the AUTONOTE2 components within the
AUTONOTE system framework.
</bodyText>
<sectionHeader confidence="0.661466" genericHeader="method">
Iv. THE PARSER AND THE REPRESENTATIONAL NETWORK
</sectionHeader>
<subsectionHeader confidence="0.648855">
Overview
</subsectionHeader>
<bodyText confidence="0.998656421052631">
When the user wise t to describe a text item, we assume he has in mind
some subject, topic, or informational object that can be characterized by a
phrasal description. A description may convey a refetence to a topic the user
has dealt with earlier; or it may define a new one. The description is ana-
lyzed to determine a dependency tree--a structure that preserves the original
words and phrases of the description and the syntactic dependencies among
them.
In constructing this tree, the parser incorporates primary units called
simple phrases. A simple phrase may consist of a modifier and a noun (e.g.,
AGM CONFERENCE), or of a noun followed by a preposition and modifier (e.g.,
OUTLINE OF PAPER). The parser extracts these basic phrases from the original
description and records the syntactic dependencies among them. A description
such as THE OUTLINE OF THE PAPER ABOUT AUTONOTE2 FOR THE ACM CONFERENCE will
be analyzed into four simple phrases: (1) THE OUTLINE OF THE PAPER, (2) THE
PAPER ABOUT AUTONOTE2, (3) THE PAPER FOR THE CONFERENCE, and (4) THE ACM CON-
FERENCE. Each simple phrase consists of a subject noun and a modifier word.
When two simple phrases have a common subject noun, we say they are coordin-
ate simple phrases. When a modifier word of one simple phrase subsequently
becomes the subject noun of another, we say the latter phrase is subordinate
</bodyText>
<figure confidence="0.986637">
27
DISK
STORAGE
DYNAMIC
LOADER
AUTONOTE2
UTILITY
ROUTINES
DESCRIPTION
PROCESSOR
DEFINE
404 COMMAND
PROCESSOR
H PROCESS
COMMAND
PROCESSOR
NETWORK
LOCATOR
PARSER
</figure>
<sectionHeader confidence="0.920047909090909" genericHeader="method">
SPEAKER
MISCELLANEOUS
FIND
COMMAND
PROCESSOR
NETWORK
MODIFICATION
PROCESSOR
DESCRIBE
COMMAND
PROCESSOR
</sectionHeader>
<figure confidence="0.987594857142857">
A
INPUT/
OUTPUT
ROUTINES
MONITOR
UTILITY
ROUTINES
</figure>
<figureCaption confidence="0.997235">
Fig. 4 - The Organization of AUTONOTE2 within the AUTONOTE System
</figureCaption>
<bodyText confidence="0.998166461538462">
28
to the former. In the above example, THE PAPER ABOUT AUTONOTE2 and THE PAPER
FOR THE CONFERENCE are coordinate simple phrases--both have PAPER as their
subject noun. Both of these are subordinate to the phrase OUTLINE OF THE
PAPER, in which PAPER appears as a modifier word. Additionally, the simple
phrase THE ACM CONFERENCE is subordinate to THE PAPER FOR THE CONFERENCE.
Subordinate phrases simply qualify the use of their subject words. For ex-
ample, phrases subordinate to THE OUTLINE OF THE PAPER provide a more detailed
description of the paper (&amp;quot;ABOUT AUTONOTE2,&amp;quot; and &amp;quot;FOR THE CONFERENCE&amp;quot;); the
phrase subordinate to THE PAPER FOR THE CONFERENCE further qualifies the con-
ference.
In effect, two kinds of dependency information are extracted by the
parser. The first is the dependency of adjectives and prepositional phrases
upon a noun. This information is reflected in the selection of the simple
phrases themselves. Second, there are the dependency relationships among
the simple phrases of the description. This information, expressed in terms
of subordinate and coordinate linkages, may be represented by a tree structure
consisting of nodes with simple phrases at the terminal branches. Fig. 5 .
gives the tree structure for the example. Simple phrases with an immediate
linkage to a node are said to directly describe that node. Note that the two
coordinate phrases from the example directly describe a common node, node B.
The subordinate relationship of the node B phrases to the node A phrase, and
in turn, that of the node C phrase to the node B phrases is reflected by down-
ward branches connecting those nodes.
The resultant tree structure defines the representation of its correspon-
ding topic. Representations of each of the user&apos;s topics are organized into a
</bodyText>
<figure confidence="0.975653">
29
OUTLINE o
paper
PAPER about
Autonote2
PAPER for
conference
Acm
&apos;CONFERENCE
</figure>
<figureCaption confidence="0.890735">
Fig. 5 - Simple Phrase Dependency Structure
</figureCaption>
<bodyText confidence="0.976051923076923">
30
hierarchical data structure called the representational network. The repre-
sentational network is composed of interconnected nodes, simple phrases, and
words. When description is mapped onto the network, the number of the asso-
ciated text item is stored with the highest order node in the corresponding
topic representation.
Each node in the network may have up to four types of linkages: (1)
pointers down to simple phrases that directly describe the node; (2) pointers
down to subordinate nodes; (3) pointers up to superior nodes; and (4) poin-
ters to textual materials associated with the node. Each simple phrase or
single word is directly accessible as a unit in the network through hash cod-
ing procedures similar to those used in maintaining the AUTONOTE keyword in-
dex. Associated with each simple phrase are the linkages to the node(s) the
phrase directly describes. In turn, each single word has associated pointers
that lead the system to the simple phrases containing the word. Fig. 6 il-
lustrates the network representation of the example.
Once a topic is defined in the network, the user can refer to it using
a word, a simple phrase or composition of simple phrases. For example,
should the user later describe a new text item as say, OUTLINE OF THE PAPER
or OUTLINE OF THE PAPER ABOUT AUTONOTE2, the system will note that it al-
ready has a representation for the topic. The only change to the network in
such cases is the addttion of new item reference linkage to the identified
node (node 1). In general, the system attempts to relate each new item des-
cription to those it already &amp;quot;knows&amp;quot; about. For new topics, new nodes are
allocated in the network. Should some subset of the simple phrases of a new
description refer to an existing topic, the additional simple phrases are
</bodyText>
<page confidence="0.953611">
31
</page>
<figure confidence="0.5727425">
Pointer to text
C PAPER
Fig 6 - Corresponding Representational Network Structure
32
</figure>
<bodyText confidence="0.998930458333333">
linked to that existing representation. For example, if in reference to the
same paper the user describes another item as THE ABSTRACT OF THE PAPER ABOUT
AUTONOTE2, the system would modify the network to that shown in Fig. 7.
Design of the Network Data Structures
List structures. As noted above, the representational criteria dictate
a hierarchical netifork-type organization, based upon dependency analyses of
topic descriptions. List structures are particularly well suited for this
kind of application. They provide a convenient representation for depen-
dency trees and are especially appropriate for dealing with complex, evol-
ving structures.
In designing special purpose list structures for the representational
netT,ork, we first specified the logical components of the structure and de-
fined the interconnections among these primitives. Three logical components
were forumlated--simple phrases, nodes, and words. The following subsections
present the major design considerations for each structural component.
Simple phrases. Given our goal of communicative efficiency, we chose
the simple phrase as a primary unit for the network. By analyzing a topic
description into simple phrases we are in effect isolating possible &amp;quot;short-
hand&amp;quot; references to the given topic. The representational data structures
have been designed to allow a topic to be referenced through any of its com-
ponent simple phrases.
Simple phrases are formed from either adjectival or prepositional modi-
fication of a noun. Very often, an adjectival modification can be equiva-
lently expressed by a prepositional phrase dependent upon the same noun
</bodyText>
<figure confidence="0.950669833333333">
33
Pointer to text Pointer to
text
PAPER about
Autonote2
PAPER for
conference
A
*wirmo.-
( OUTLINE )
OUTLINE of
paper
</figure>
<figureCaption confidence="0.992699">
Fig. 7 - Network Representation of a Topic Hierarchy
</figureCaption>
<bodyText confidence="0.994376944444445">
34
(example: THE PAPER ABOUT AUTONOTE and THE AUTONOTE PAPER). In other in-
stances the adjectival form may have multiple interpretations; THE SMITH
ARTICLE could refer to an article by Smith or possibly an article about
Smith. Some prepositions may be used synonymously in a particular context
(THE PAPER ABOUT (ON) SHORT TERM MEMORY); others convey distinctly different
meanings (THE MEMO TO THE COMMITTEE versus THE MEMO FROM THE COMMITTEE). We
do not deal with these problems to the extent of providing a semantics for
&amp;quot;understanding&amp;quot; natural language. However, the representational structure
makes explicit the various possibilities, so that the system is able to gen-
erate plausible alternatives.
We treat adjectival modification as a special case, as if the modifier
and subject noun were related by an unspecified preposition. In terms of
the data structure design, all simple phrases composed of the same two words
are mapped into a larger unit, each subunit of which represents a particular
instance of a simple phrase in a topic description. This arrangement assures
that all information on simple phrases involving any two words is accessible
collectively. This information will then be at hand to provide a basis for
interpreting the alternative referents of each incoming simple phrase. For
example, should the user make reference to THE SMITH PAPER and the system
finds only PAPER ABOUT SMITH in the network, then that single alternative
is chosen. On the other hand, if PAPER BY SMITH also is present, the system
considers both possibilities.
Network nodes. The next structural component of the representational
network is the node. A node groups together a set of simple phrases that
comprise the description of the node. The node also functions as a collector
35
of item references. Each node in the representational network corresponds
to a topic or concept pertinent to the items of textual material associated
with it. The simple phrases that directly describe a node define the cor-
responding concept. Any given node may be linked to more general (lower)
nodes, or to more specific (higher) nodes. For example, a node that repre-
sents a particular paper may be linked downward to another that describes
a conference at which the paper was presented; it may also be linked to
several higher order nodes corresponding to, say, a summary, an outline, and
a review of the paper. As more and more items are described, additional
topics may be tied into the same conference node. The ultimate result will
be a highly interconnected set of concept nodes, each with its own set of
associated textual materials.
To achieve this kind of structural organization for the network, we
make use of the dependency relationships in the user&apos;s descriptions: each
node level corresponds to a syntactic dependency level. In terms of the
example above, the adjectives and prepositional phrases modifying the noun
&amp;quot;paper&amp;quot; are formed into simple phrases that will directly describe a common
node. Simple phrases identifying the confetence will describe a subordinate
node due to the syntactic dependency of &amp;quot;conference&amp;quot; upon &amp;quot;paper&amp;quot; in a phrase
of the form, PAPER AT THE...CONFERENCE. Superior nodes are assigned to the
outline, the summary, and the review, reflecting the dependence of &amp;quot;paper&amp;quot;
upon those nouns in appropriate descriptions.
A node may be viewed as a collection of pointers to simple phrases,
other nodes, and text items. All node linkages are two—way. Pointers down
from a node to its simple phrases are required in order to reconstruct a
36
description of the node. Pointers down to subordinate nodes are necessary for
the same reason. Both upward and downward pointers to other nodes provide a
means for moving from any topic to structurally related ones. Associated with
each instance of a simple phrase is a pointer to the node where item refer-
ences are stored. Finally, bookkeeping information stored with each text item
includes pointers to each topic node with which the item is associated. Item-
node linkages enable the system to provide the user with topic descriptions of
any text item.
Words. The representational structures considered thus far provide sim-
ple phrases as the sole means for accessing the nodes in the network. A less
restrictive access mechanism also is required, for several important reasons.
First, it would be unrealistic to assume that the user will always phrase ref-
erences to a particular topic in exactly the same way. Second, single word
descriptions play an important role in achieving our goal of communicative
efficiency. Since we anticipate that users will make frequent use of single
word references when working in the context of a particular topic, we want to
provide a natural and convenient treatment of such descriptions. Finally, a
phrasal description can convey a higher order categorization of an existing
topic without containing a simple phrase for that topic. For example, THE
REVIEWER&apos;S COMMENTS ON THE PAPER may reference a paper mentioned earlier; yet
it contains no simple phrases describing that paper.
These considerations lead us to the third logical component of the network
data structures, the single word. Essentially, each component word provides ac-
cess to a series of pointers to simple phrases in which the word occurs.
Word-to-phrase pointers are of two types: those indicating usage
37
as subject noun; and those indicating modifier usage in a particular simple
phrase. As we shall see later, this distinction is required in order to
relate new simple phrases to existing topics at an appropriate node level.
Having specified the three logical components and the linkages in the
representational network, we now turn our attention to the storage implemen-
tation of these structures.
Storage Implementation of the Network Structure
There are three directories needed to maintain the representational net-
work, one for each of the components of the structure. All directory infor-
mation must, of course, be saved in permanent storage between AUTONOTE2 ses-
sions. Two design alternatives were considerea for maintaining the network
during execution of the program. The directories could be accessed and up-
dated on disk, or they could be brought into core storage for the duration
of the session. We adopted the former strategy for a number of reasons.
First, AUTONOTE is highly oriented toward the use of disk file storage.
Several file interface routines were available at the outset for conveniently
storing and accessing information through the MTS file system. Second, as
the network grows in complexity, it becomes increasingly unlikely that the
user will reference the major portion of the network during any given ses-
sion. By maintaining the network in disk files, the amount of core storage
required is substantially reduced. Finally, the file approach greatly
simplified the programming effort, especially in those system components
that operate recursively on the list structured network. We will elaborate
on this point further in section VI, which illustrates the simplification of
recursive processes in AUTONOTE2.
38
Rather than store all the directories in a single disk file, we chose
to maintain each directory separately. This strategy preserves the logical
distinction among the three types of directory information, and has also
simplified the programming of the system. We now describe the organization
of each of the directory files.
The node directory. Each node in the representational network has a
corresponding integral node number which is also the line number in the node
directory file. As new node numbers are needed to represent new topics, the
next sequentially numbered line in the node directory is assigned as the node
number. Each node directory line contains four fields--one for bookkeeping
information and three fields for the upward, downward, and item reference
pointers for the node. The item reference region contains a list of integer
item numbers. The upward pointer region also contains a list of integers
that represent immediate linkages to superior nodes. The two types of down-
ward pointers (to nodes and to phrases) are stored in a common region. Each
node, simple phrase, and single word has a corresponding file line number in
its respective directory file. In thee case of nodes, the line number is
simply the node number. In the case of words and simple phrases, the line
number is the result of A hash coding process on a compact character represen-
tation of the word or phrase. Thus a &amp;quot;pointer&amp;quot; is actually a file line num-
ber. Downward pointers to nodes and phrases are distinguishable in the node
directory on the basis of the magnitude of the line number.
Since each of the three pointer fields is of fixed length, there is a
maximum number of each type of pointer for a given node directory line.
39
Each field consequently has an associated continuation pointer to a line
where additional pointers are stored if necessary.
The phrase directory. To locate the phrase directory line for a par-
ticular simple phrase, a hash coding function is applied to the character
string formed by concatenating the modifier word, a slash, and the subject
word. For example, the directory line for the simple phrase PAPER ABOUT
AUTONOTE is the hashcode for the string &amp;quot;AUTONOTE/PAPER.&amp;quot; Since the hashing
function operates only on the modifier and subject word, simple phrases
formed from the same two words, but with differing (or no) prepositions, are
mapped into the same directory line number.
To distinguish among the various instances of the same two-word combin-
ation, the directory line for simple phrases consists of a series of pointer
blocks. Each pointer block contains a code for the particular preposition
used, some additional bookkeeping information, and a pointer to the node
directly described by that occurrence of the simple phrase.
The word directory. The word directory incorporates the same pointer
block principle as the phrase directory. The pointer field of the block in
this case is a pointer into the phrase directory. The preposition code field
contains a binary flag indicating whether the particular word occurs as the
subject noun or modifier word in the simple phrase specified by the pointer.
Like phrases, each word directory line is accessed through an efficient hash
coding algorithm.
The word directory also maintains preposition usage information for each
word. For example, the entry for MEMO may indicate that the word has
40
occurred with the prepositions ON, ABOUT, TO, FROM, etc. This information
is used to guide the parsing of descriptions.
The organization of the three network directories is depicted in Fig. 8.
he Representational Network: An Example
To help fix ideas, We now present a more detailed example that illus-
trates the structure of the representational netwotk. Suppose the user
describes Items 157, 158, and 159 as THE PAPER ABOUT AUTONOTE FOR THE ACM
CONFERENCE: he enters materials on the organization of that paper into
Items 201 and 202. A summary of the paper is placed in Item 230. The user
also describes Item 270 as SMITH&apos;S PAPER, and enters a summary of that paper
into Item 312. A pictorial representation of the resultant portion of the
network is given in Fig. 9, while the corresponding directory contents appear
it Fig. 10. For simplicity, the simple phrase hash codes are represented by
the alphabetic characters U through Z. (In subsequent diagrams, we also omit
word-to-phrase linkages for simplicity.)
Parsing of Descriptions
This section outlines our general approach to parsing topic descrip-
tions. The parsing of prepositional phrases, consecutive modifiers, and
possessive modifiers is considered.
Prepositional phrases. Despite the apparent simplicity of the descrip-
tion language there are several nontrivial parsing problems. One of these
is the difficulty in,determining the noun referent of prepositional phrases.
The determination of noun referents is partially a semantic problem rather
than a purely syntactic one. Consider the following two descriptions:
41
</bodyText>
<note confidence="0.247873">
Internal Form Pointer to Collisidn Preposi- Number Series of Fixed
of the Word Continua- Pointera tion of Length Blocksb
or Phrase tion Usage Upward (Format Given in
Lines Links (b) Below)
</note>
<figure confidence="0.74928575">
(a) Format for the word and phrase directories.
Upward Access PreposTon Article For Future
Link Recency Code Codes Expansion
(b) Pointer block format in the word and phrase directories.
. ,
Number Number &apos;Number Access Continu- List of Preposi- List of List
of of of Recency ation Downward tion Upward of
Upward Downward Items , Line Pointers Code for Poin- Item
Links Links Pointers Each ters Refer-
Downward ences
Pointer
(c) Format for the node directory.
</figure>
<figureCaption confidence="0.899002">
Fig. 8 - Representational Network Directory Formats
</figureCaption>
<bodyText confidence="0.994671">
aUsed in conjunction with the hash codihg mechanism.
bFor single words, there is one block for each phrase containing the
word. For phrases, there is one block for each node that the phrase directly
describes.
or single words, the preposition code is used to distinguish between
words used as subjects or modifiers.
</bodyText>
<sectionHeader confidence="0.623449" genericHeader="method">
42
r Acm
CONFERENCE
</sectionHeader>
<figure confidence="0.991831833333333">
ORGANIZATION
of paper
PAPER for
conference
SUMMARY of
paper
</figure>
<figureCaption confidence="0.599261">
Fig. 9 - A Complex Representation
</figureCaption>
<table confidence="0.894762228571429">
43
No.
Word Blocks Blocks
,
Organization 1 U (sub)
Paper 5 W (sub) X (sub) U (mod) V (sub) Z (mod)
Autonote 1 X (mod)
Conference 2 W (mod) Y (sub)
Summary 1 Z (sub)
ACM 1 Y (mod)
Smith (poss) 1 V (mod)
I
(a) Word directory.
Line Phrase No. Blocks
No. Blocks
U paper/organization 1 Node 3 (of)
V smith/paper (poss) 1 Node 5 (adj)
W conference/paper 1 Node 1 (for)
X autonote/paper 1 Node 1 (about)
Y , acm/conference 1 Node 2 (adj)
Z paper/summary 2 Node 4 (of) Node 6 (of)
. . ..,_
(b) Phrase directory.
Line Pointers Pointers Item
No. Up Down References
1 3,4 2,W,X #157, #158, #159
2 1 Y ...
3 ... 1,U #201, #202
4 ... Z,1 #203
5 6 V #270
6 ... Z,5 #312
(c) Node directory.
Fig. 10 - Corresponding Directory Contentsa
aSee Fig. 9.
44
</table>
<sectionHeader confidence="0.5257925" genericHeader="method">
THE ORGANIZATION (OF THE PAI) (ABOUT AUTONOTE).
THE MEMO (FROM THE COMMITTEE) (TO THE CHAIRMAN).
</sectionHeader>
<bodyText confidence="0.9998646">
In the first example, both prepositional phrases refer to the immedi-
ately preceding noun. In the second case, both refer back to the noun MEMO
at the beginning of the string. Although neither of these examples is in-
tuitively ambiguous, the parsing algorithm must consider each preceding noun
as a possible referent of any given prepositional phrase.
</bodyText>
<subsectionHeader confidence="0.766032">
The AUTONOTE2 parser deals with this problem to a limited extent, by
</subsectionHeader>
<bodyText confidence="0.996597186046512">
utilizing prepositional clues. For example, if the system finds that the
noun MEMO can form a simple phrase with the prepositions ON, ABOUT, TO, and
FROM, then phrases introduced by these prepositions will be associated with
that noun. Such clues will not always yield a unique parsing, of course, as
in the case of inherently ambiguous descriptions. THE PAPER FOR THE CON-
FERENCE ON GENETICS, for example, could refer to a paper on genetics to be
delivered at a conference, or to a paper which is to be delivered at a con-
f‘Eence on genetics.
In such instances we rely upon the user to supply the referent noun
upon request. In the example above, the system may prompt: DOES &amp;quot;ABOUT
GENETICS&amp;quot; REFER TO PAPER OR CONFERENCE? Should the user reply CONFERENCE,
the simple phrase CONFERENCE ON GENETICS will be added to the network. If
at some later time, the parser is attempting to find a referent for the pre-
positional phrase ON GENETICS where CONFERENCE is one of the alternatives,
it forms that simple phrase directly.
Consecutive modifiers. A parallel problem arises in determining the
noun referents for a string of consecutive modifiers. Descriptions
45
containing at most a single adjective for any particular noun are parsed in
the obvious manner. A simple phrase is formed from each modifier and the
noun following it. In the event a noun is preceded by two or more modifiers,
the parser is confronted with a task similar to that of determining the
referent of a prepositional phrase. The modifier occurring immediately be-
fore the noun is first processed as above. Each of the remaining modifiers,
however, can modify any one of several words depending upon their &amp;quot;distance&amp;quot;
from the head noun. Specifically, any such modifier can refer to either the
head noun or any of the other modifiers following it. Consider the descrip-
tions:
A summary of personal information retrieval systems
ir 41-
personal information retrieval systems
A summary of
In both of the cases above, INFORMATION modifies the modifier RETRIEVAL which
in turn modifies the head noun SYSTEMS. Depending upon the user&apos;s intent,
PERSONAL can modify either INFORMATION or SYSTEMS. The choice of modifier
referents is an especially important problem when there are multiple parsings,
each resulting in a different semantic interpretation. For example, LARGE
COMPUTER CONFERENCE could refer to a conference on large computers, or a
large conference on computers. Another important reason for our emphasis
upon correctly identifying modifier referents concerns the use of para-
phrasing. In the example, PERSONAL INFORMATION RETRIEVAL SYSTEMS, if we
determine that INFORMATION modifies RETRIEVAL and PERSONAL modifies SYSTEMS,
then the resultant topic can be paraphrased as (1) PERSONAL SYSTEMS FOR
</bodyText>
<page confidence="0.782016">
46
</page>
<bodyText confidence="0.830195666666667">
INFORMATION RETRIEVAL, or (2) PERSONAL SYSTEMS FOR THE RETRIEVAL OF INFORMA-
TION. Depending upon context and the nature of other topics in the network,
the following incomplete descriptions will in most cases identify the topic:
</bodyText>
<listItem confidence="0.457877">
1. SYSTEMS
</listItem>
<sectionHeader confidence="0.9560106" genericHeader="method">
2. SYSTEMS FOR RETRIEVAL (or RETRIEVAL SYSTEMS)
3. PERSONAL SYSTEMS
4. PERSONAL SYSTEMS FOR RETRIEVAL (or PERSONAL RETRIEVAL SYSTEMS)
5. SYSTEMS FOR INFORMATION RETRIEVAL
6. SYSTEMS FOR RETRIEVAL OF INFORMATION
</sectionHeader>
<bodyText confidence="0.985628552238806">
A different choice of modifier referents determines a correspondingly dif-
ferent set of paraphrases. If PERSONAL was intended to modify INFORMATION,
we would have the paraphrase SYSTEMS FOR THE gETRIEVAL OF PERSONAL INFORMATION,
with a corresponding list of incomplete references to the topic.
As in the prepositional case, the choice of modifier referents is guided
by the current state of the representational network. After processing the
last modifier in the string, the parser positions itself at the preceding
modifier and moves left in the input string until the first word in the modi-
fier string is processed. In the above example, after associating RETRIEVAL
with SYSTEMS, the parser next examines the modifier INFORMATION. A list of
simple phrase candidates is formed. In this case, the list contains INFORMA-
TION RETRIEVAL and INFORMATION SYSTEMS. If neither of the candidate phrases
has been previously used, the system queries,: WHAT DOES INFORMATION MODIFY?
The user&apos;s reply is matched against the eandidate referents and the appropriate
simple phrase is formed.
47
Possessive adjectives. Possessives are processed in much the same way as
normal modifiers. The system recognizes the &apos;s word stem and marks the root
word as a possessive. The root word is later stored in the network directories
along with a possessive flag. Thus the phrase SMITH&apos;S PAPER is stored intern-
ally as SMITH/PAPER (possessive). The removal of the stem insures that a sub-
sequent simple phrase incorporating a preposition (PAPER BY SMITH) will hash to
the same directory lite thus allowing the use of either prepositional or pos-
sessive forms in referencing topics.
A particularly interesting case arises when a possessive occurs in a string
of consecutive modifiers as in SMITH&apos;S LATEST MEMORY EXPERIMENT. The string is
first processed as described above; that is, a check is made to see if SMITH
has been used in a simple phrase with LATEST, MEMORY, or EXPERIMENT. In the
event that this yields no clues, the system then checks to see if SMITH was
rendered as a possessive. Upon noting that it was, the parser carries out a
heuristic that assumes that the possessive modifies the head noun, EXPERIMENT.
The possessive heuristic can be fully stated as follows. A possessive
occurring in a stting of modifiers will be assumed to modify the head noun un-
less another possessive occurs between it and the head noun. In the latter
case, the first possessive will be assumed to modify the second. This is simi-
lar to the possessive feature employed by the REL parser (Dosert &amp; Thompson,
1971). Thus in SMITH&apos;S RESEARCH GROUP&apos;S MEMORY EXPERIMENT, SMITH&apos;S is assumed
to modify GROUP, and GROUP&apos;S is assumed to modify the head noun EXPERIMENT.
The question now arises, why check the phrase directory first instead of
applying the possessive heuristic immediately? To answer this, suppose a topic
was originally described as THE RESULTS OF THE MEMORY EXPERIMENT BY SMITH and
48
the user now attempts to refer to it as SMITH&apos;S MEMORY EXPERIMENT RESULTS. If
the possessive heuristic were applied immediately, the system would incorrectly
form the simple phrase SMITH&apos;S RESULTS, not SMITH&apos;S EXPERIMENT. By checking
the network first, the simple phrase EXPERIMENT BY SMITH will be detected and
the system will parse the description appropriately.
Implementation of the Parser
The ultimate goal of the parser is to determine the simple phrases of a
topic description. The parsing algorithm is implemented as a two stage pro-
cess. The first stage is a preliminary scan to ascertain that the string is
in a form acceptable for analysis. The description is segmented into an
ordered list of words, each of which is marked as either WORD, POSSESSIVE,
ARTICLE, or PREPOSITION. The parser makes no distinction between nouns and
modifiers until completing the scan. At this point, the last in a series of
consecutive WORDs is marked as a NOUN; the preceding words are marked as MOD-
IFIERs. Possessive modifiers are an exception as they can be recognized ex-
plicitly during the scan. A record of article usage is also kept, but the
articles themselves are not placed on the word list.
The preliminary scan of the description can be viewed as a simple finite
state process. Of course, to be completely formal, the recognizer would have
to examine each input character. For convenience we will assume a five state
automaton with inputs: WORD, POSSESSIVE, PREPOSITION, and ARTICLE. The state
transition graph for the machine is given in Fig. 11. The machine starts in
state S0, examines the next input and moves to a new state. If at the end of
the input string, the machine is in state Sl, called the final state, the in-
put is accepted; otherwise, the user is asked to rephrase. Note that in state
</bodyText>
<figure confidence="0.838211333333333">
49
possessive
article preposition
</figure>
<figureCaption confidence="0.853529">
Fig. 11 - A Finite State Acceptor for the Autonote2 Description Language
</figureCaption>
<bodyText confidence="0.99226275">
50
S2&apos; the machine has just encountered an article and is anticipating a &amp;quot;word.&amp;quot;
If the machine is in state S0 upon completion, it has just recognized a pre-
position and is expecting an object; thus, the string is rejected. The state
S4 is reached whenever a possessive is encountered. Since a possessive must
have an object noun, a &amp;quot;word&amp;quot; input is required to reach state Sl. State S3
is a trapping state; once entered, the machine remains in that state regard-
less of the remaining input and the description is consequently rejected.
State S3 corresponds to various error conditions--two consecutive prepositions
or articles, an article between two words, a phrase beginning with a preposi-
tion, etc.
The state transitions for the description BRUNER&apos;S FIRST EXPERIMENT ON THE
</bodyText>
<sectionHeader confidence="0.9579425" genericHeader="method">
CONSERVATION OF LIQUIDS are given below along with the resultant word list.
INPUT TYPE RESULTANT STATE
</sectionHeader>
<reference confidence="0.9711230625">
Bruner&apos;s Possessive S
First Word S1
Experiment Word Si
On Preposition S0
The Article S2
Conservation Word S1
Of Preposition S 0
Liquids Word S1 Accept
WORD TYPE
Bruner&apos;s Modifier
First Modifier
Experiment Noun
On Preposition
Conservation Noun (the)
Of Preposition
Liquids Noun
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.065869">
<note confidence="0.9889025">Journal of Computational Linguistic.; 23 Copyright 1975 by the Association for.Lomputational Linguistics</note>
<title confidence="0.95785225">MEDIATED NATURAL LANGUAGE COMMUNICATION IN A PERSONAL INFORMATION RETRIEVAL SYSTEM</title>
<author confidence="0.723169">E Linn</author>
<author confidence="0.723169">Walter Reitman Michigan Ann Arbor</author>
<abstract confidence="0.984006">paper is based on a doctoral dissertation by the first author. Support National Science Foundation under Grant No. DCR71-02038 is gratefully acknowledged. Those wishing more complete details about system commands and the second author for a User&apos;s Manual. at Southern Ratlway System, 125 Spring Street, S.W., Atlanta, Georgia 30303 ABSTRACT Natural language combines nouns and adjectives into noun phrases, and links phrases by means of prepositions to form complex descriptions of objects and topics. AUTONOTE2, a file-oriented retrieval system, allows the user to employ such descriptions to characterize the items of informatiaa he wiehes to store and retrieve. in addition, the system also cona network representation of the user&apos;s subject matter, using syntactic analysis to derive dependency structures frikam Us descriptions. The dependency information, expressed as subordinate and coordinate linkages the phrases, is represented by a nodes, with simple phrases at the terminal branches. The PARSER uses the network to disambiguate descriptions, querying the user only abbut residual ambiguities. Associated with the PARSER is a network LOCATOR, which determines whether a cUrrent user description refers to an existing topic at some level in the network. The LOCATOR also builds a table specifying the changes, if to be made in a network inorder to represent the inferred from the current input description. For example, if the user&apos;s description conone or more simple phrases (thereafter to as active) directly describing at least one existing node in the network, the description as a whole quite likely references an existing network topic. To locate it, the PARSER first determines the focus phrase, the active phrase at the highest dependency level. The nodes directly described by the focus phrase are used to generate candidate topics. These then are matched against the 2 3 remaining active phrases obtained from the description to determine the most likely referent. Many of the procedures employed in deScription and representation also are used in network-mediated&apos; retrieval. The user mar initiate retrieval with a FIND command, supplying a description as argument. The resultant table is along to the LOCATOR, which returns a node nudber to the FIND processor. The FIND processor constructs a set of item numbers by extracting the textual references from the node. The system checks pointers from the node. If there are. structurally related topics, the FIND processor so informs the user. Note that by virtue of network Mediation of retrieval, if a user description Is imprecise or incorrect, the system may be Able to direct the user to relevant related topics. the system queries the user about a topic, for example determine the intent-of a description, the tepic node number is passed to a SPEAKER component. A phrasal description of the node Is returned. To minimize redundant communication, a level indicator may be set according to the level of diatail in the user&apos;s description. For example, if the user describes an item as RESULTS OF THE EXPERIMENT and the system .must ask it he is referring to SMITH&apos;S EXPERIMENT ON THE SHORT TERM MEMORY OF WHITE RATS, the resulting query would be: ARE YOU REFERRING TO SMITH&apos;S EXPERIMENT ON MEMORY? Coastruction of a description from the network takes place in two&apos; stages. The first stage steps thorugh the network recursively, collecting simple phrases directly or describe the specified node. 4 The level indicator blocks collection of simple phrases below the specified The second stage is carried by a recursive algorithm that on the tabled simple phraqcs their interrelations to construct the phrasal description. The last major component of the system handles network modification and reorganization. This enables the user to add or remove references and phrases, and to modify,, delete, or reorganize his topic structure. A detailed ease study comparing AUTONOTE2 with a good keyword-based retrieval system showed that for a coherent body of material, the communicative efficiency of AUTONOTE2, as measured &apos;by the ratio of the number of w)rds conveyed to the number of words entered, was more than double that of system. Retrieval capability was enhanced considerably, and the representation tetwOrk effectively distinguished among the many topics partially indexed by the same words. Furthermore, SPEAKER output of topics from the representational network proved a useful retrieval intergreatly reducing the for perusal of item texts.</abstract>
<affiliation confidence="0.531433">TABLE OF CONTENTS</affiliation>
<page confidence="0.695077"></page>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<booktitle>Bruner&apos;s Possessive S First Word S1</booktitle>
<marker></marker>
<rawString>Bruner&apos;s Possessive S First Word S1</rawString>
</citation>
<citation valid="false">
<title>Experiment Word Si On Preposition S0</title>
<marker></marker>
<rawString>Experiment Word Si On Preposition S0</rawString>
</citation>
<citation valid="false">
<booktitle>The Article S2 Conservation Word S1 Of Preposition S 0</booktitle>
<marker></marker>
<rawString>The Article S2 Conservation Word S1 Of Preposition S 0</rawString>
</citation>
<citation valid="false">
<title>Liquids Word S1 Accept WORD TYPE Bruner&apos;s Modifier First Modifier Experiment Noun On Preposition Conservation Noun (the) Of Preposition Liquids Noun</title>
<marker></marker>
<rawString>Liquids Word S1 Accept WORD TYPE Bruner&apos;s Modifier First Modifier Experiment Noun On Preposition Conservation Noun (the) Of Preposition Liquids Noun</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>