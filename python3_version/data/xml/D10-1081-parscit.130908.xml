<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000182">
<title confidence="0.999094">
An Efficient Algorithm for Unsupervised Word Segmentation with
Branching Entropy and MDL
</title>
<author confidence="0.996969">
Valentin Zhikov
</author>
<affiliation confidence="0.997244333333333">
Interdisciplinary Graduate School
of Science and Engineering
Tokyo Institute of Technology
</affiliation>
<email confidence="0.990573">
zhikov@lr.pi.titech.ac.jp
</email>
<author confidence="0.985396">
Hiroya Takamura
</author>
<affiliation confidence="0.992853">
Precision and Intelligence Laboratory
Tokyo Institute of Technology
</affiliation>
<email confidence="0.989923">
takamura@pi.titech.ac.jp
</email>
<author confidence="0.988438">
Manabu Okumura
</author>
<affiliation confidence="0.9937845">
Precision and Intelligence Laboratory
Tokyo Institute of Technology
</affiliation>
<email confidence="0.98462">
oku@pi.titech.ac.jp
</email>
<sectionHeader confidence="0.998537" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9994946875">
This paper proposes a fast and simple unsuper-
vised word segmentation algorithm that uti-
lizes the local predictability of adjacent char-
acter sequences, while searching for a least-
effort representation of the data. The model
uses branching entropy as a means of con-
straining the hypothesis space, in order to ef-
ficiently obtain a solution that minimizes the
length of a two-part MDL code. An evaluation
with corpora in Japanese, Thai, English, and
the ”CHILDES” corpus for research in lan-
guage development reveals that the algorithm
achieves an accuracy, comparable to that of
the state-of-the-art methods in unsupervised
word segmentation, in a significantly reduced
computational time.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99996843902439">
As an inherent preprocessing step to nearly all NLP
tasks for writing systems without orthographical
marking of word boundaries, such as Japanese and
Chinese, the importance of word segmentation has
lead to the emergence of a micro-genre in NLP fo-
cused exclusively on this problem.
Supervised probabilistic models such as Condi-
tional Random Fields (CRF) (Lafferty et al., 2001)
have a wide application to the morphological anal-
ysis of these languages. However, the development
of the annotated training corpora necessary for their
functioning is a labor-intensive task, which involves
multiple stages of manual tagging. Because of the
scarcity of labeled data, the domain adaptation of
morphological analyzers is also problematic, and
semi-supervised algorithms that address this issue
have also been proposed (e.g. Liang, 2005; Tsuboi
et al., 2008).
Recent advances in unsupervised word segmen-
tation have been promoted by human cognition re-
search, where it is involved in the modeling of the
mechanisms that underlie language acquisition. An-
other motivation to study unsupervised approaches
is their potential to support the domain adaptation of
morphological analyzers through the incorporation
of unannotated training data, thus reducing the de-
pendency on costly manual work. Apart from the
considerable difficulties in discovering reliable cri-
teria for word induction, the practical application
of such approaches is impeded by their prohibitive
computational cost.
In this paper, we address the issue of achiev-
ing high accuracy in a practical computational time
through an efficient method that relies on a combina-
tion of evidences: the local predictability of charac-
ter patterns, and the reduction of effort achieved by
a given representation of the language data. Both of
these criteria are assumed to play a key role in native
language acquisition. The proposed model allows
experimentation in a more realistic setting, where
the learner is able to apply them simultaneously. The
</bodyText>
<page confidence="0.953093">
832
</page>
<note confidence="0.8177745">
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 832–842,
MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.99892175">
method shows a high performance in terms of accu-
racy and speed, can be applied to language samples
of substantial length, and generalizes well to corpora
in different languages.
</bodyText>
<sectionHeader confidence="0.999803" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999984246153847">
The principle of least effort (Zipf, 1949) postulates
that the path of minimum resistance underlies all
human behavior. Recent research has recognized
its importance in the process of language acquisi-
tion (Kit, 2003). Compression-based word induc-
tion models comply to this principle, as they reor-
ganize the data into a more compact representation
while identifying the vocabulary of a text. The min-
imum description length framework (MDL) (Ris-
sanen, 1978) is an appealing means of formalizing
such models, as it provides a robust foundation for
learning and inference, based solely on compres-
sion.
The major problem in MDL-based word segmen-
tation is the lack of standardized search algorithms
for the exponential hypothesis space (Goldwater,
2006). The representative MDL models compare
favorably to the current state-of-the-art models in
terms of accuracy. Brent and Cartwright (1996) car-
ried out an exhaustive search through the possible
segmentations of a limited subset of the data. Yu
(2000) proposed an EM optimization routine, which
achieved a high accuracy, in spite of a lower com-
pression than the gold standard segmentation.
As a solution to the aforementioned issue, the pro-
posed method incorporates the local predictability of
character sequences into the inference process. Nu-
merous studies have shown that local distributional
cues can serve well the purpose of inducing word
boundaries. Behavioral science has confirmed that
infants are sensitive to the transitional probabilities
found in speech (Saffran et al., 1996). The increase
in uncertainty following a given word prefix is a
well studied criterion for morpheme boundary pre-
diction (Harris, 1955). A good deal of research has
been conducted on methods through which such lo-
cal statistics can be applied to the word induction
problem (e.g. Kempe, 1999; Huang and Powers,
2003; Jin and Tanaka-Ishii, 2006). Hutchens and
Adler (1998) noticed that entropic chunking has the
effect of reducing the perplexity of a text.
Most methods for unsupervised word segmenta-
tion based solely on local statistics presume a cer-
tain – albeit minimum – level of acquaintance with
the target language. For instance, the model of
Huang and Powers (2003) involves some parame-
ters (Markov chain order, numerous threshold val-
ues) that allow its adaptation to the individuality of
written Chinese. In comparison, the method pro-
posed in this paper generalizes easily to a variety of
languages and domains, and is less dependent on an-
notated development data.
The state-of-the-art in unsupervised word seg-
mentation is represented by Bayesian models. Gold-
water et al. (2006) justified the importance of
context as a means of avoiding undersegmentation,
through a method based on hierarchical Dirichlet
processes. Mochihashi et al. (2009) proposed ex-
tensions to this method, which included a nested
character model and an optimized inference proce-
dure. Johnson and Goldwater (2009) have proposed
a novel method based on adaptor grammars, whose
accuracy surpasses the aforementioned methods by
a large margin, when appropriate assumptions are
made regarding the structural units of a language.
</bodyText>
<sectionHeader confidence="0.998943" genericHeader="method">
3 Proposed Method
</sectionHeader>
<subsectionHeader confidence="0.999085">
3.1 Word segmentation with MDL
</subsectionHeader>
<bodyText confidence="0.999889">
The proposed two-part code incorporates some ex-
tensions of models presented in related work, aimed
at achieving a more precise estimation of the repre-
sentation length. We first introduce the general two-
part code, which consists of:
</bodyText>
<listItem confidence="0.975247">
• the model, embodied by a codebook, i.e., a lexi-
con of unique word types M = {wi, ..., w|m|},
• the source text D, obtained through encoding
the corpus using the lexicon.
</listItem>
<bodyText confidence="0.9953468">
The total description length amounts to the num-
ber of bits necessary for simultaneous transmission
of the codebook and the source text. Therefore, our
objective is to minimize the combined description
length of both terms:
</bodyText>
<equation confidence="0.857838">
L(D, M) = L(M) + L(D|M).
</equation>
<bodyText confidence="0.998154">
The description length of the data given M is cal-
culated using the Shannon-Fano code:
</bodyText>
<page confidence="0.951735">
833
</page>
<equation confidence="0.990517333333333">
|M|
L(D|M) = − E #wj 1o92 P(wj),
j=1
</equation>
<bodyText confidence="0.999852476190476">
where #wj stands for the frequency of the word wj
in the text.
Different strategies have been proposed in the lit-
erature for the calculation of the codebook cost. A
common technique in segmentation and morphology
induction models is to calculate the product of the
total length in characters of the lexicon and an esti-
mate of the per-character entropy. In this way, both
the probabilities and lengths of words are taken into
consideration. The use of a constant value is an ef-
fective and easily computable approach, but it is far
from precise. For instance, in Yu (2000) the average
entropy per character is measured against the orig-
inal corpus, but this model does not capture the ef-
fects of the word distributions on the observed char-
acter probabilities. For this reason, we propose a
different method: the codebook is modeled as a sep-
arate Markov chain of characters.
A lexicon of characters M&apos; is defined. The de-
scription length of the lexicon data D&apos; given M&apos; is
then calculated as:
</bodyText>
<equation confidence="0.9995035">
L(D0|M0) = − E |C |#ci 1o92 P(ci),
i=1
</equation>
<bodyText confidence="0.999989962962963">
where #ci denotes the frequency of a character ci
in the lexicon of hypothesis M. The term L(M&apos;)
is constant for any choice of hypothesis, as is repre-
sents the character set of a corpus.
The total description length under the proposed
model is thus calculated as:
(the total length of the text in characters). The para-
metric complexity term is calculated in the same
way for the lexicon. For a derivation of the above
formula, refer to e.g. Li (1998).
MDL is closely related to Bayesian inference. De-
pending on the choice of a universal code, the two
approaches can overlap, as is the case with the two-
part code discussed in this paper. It can be shown
that the model selection in our method is equiva-
lent to a MAP inference, conducted under the as-
sumption that the prior probability of a model de-
creases exponentially with its length (Goldwater,
2006). Thus, the task that we are trying to accom-
plish is to conduct a focused search through the hy-
pothesis space that will allow us to obtain an approx-
imation of the MAP solution in a reasonable time.
The MDL framework does not provide standard
search algorithms for obtaining the hypotheses that
minimize the description length. In the rest of this
section, we will describe an efficient technique suit-
able for the word segmentation task.
</bodyText>
<subsectionHeader confidence="0.9999">
3.2 Obtaining an initial hypothesis
</subsectionHeader>
<bodyText confidence="0.999821">
First, a rough initial hypothesis is built by an algo-
rithm that combines the branching entropy and MDL
criteria.
Given a set X, comprising all the characters found
in a text, the entropy of branching at position k of the
text is defined as:
</bodyText>
<equation confidence="0.999166833333333">
H(Xk|xk−1, ..., xk−n) =
L(M) + L(D|M) = L(M0) + L(D0|M0) + L(D|M) = E− P(x|xk−1, ..., xk−n) 1o92 P(x|xk−1, ..., xk−n),
x∈X
|C ||M|
− #ci 1o92 P(ci) − E #wj 1o92 P(wj) + O(1).
i=1 j=1
</equation>
<bodyText confidence="0.999937">
A rigorous definition should include two addi-
tional terms, L(θ|M) and L(θ&apos;|M&apos;), which give the
representation cost of the parameters of both mod-
els. The L(θ|M) can be calculated as:
</bodyText>
<equation confidence="0.996664">
L(θ|M) = |M |− 1
2 * 1o92 S,
</equation>
<bodyText confidence="0.999926">
where |M |− 1 gives the number of parameters (de-
grees of freedom), and S is the size of the dataset
where xk represents the character found at position
k, and n is the order of the Markov model over char-
acters. For brevity, hereafter we shall denote the ob-
served sequence {xk−1, ..., xk−n} as {xk−1:k−n} .
The above definition is extended to combine the
entropy estimates in the left-to-right and right-to-
left directions, as this factor has reportedly improved
performance figures for models based on branching
entropy (Jin and Tanaka-Ishii, 2006). The estimates
in both directions are summed up, yielding a single
value per position:
</bodyText>
<page confidence="0.916008">
834
</page>
<equation confidence="0.9794248">
H&apos;(Xk;k−1|xk−1:k− n;xk:k+n−1) _ Algorithm 1 Generates an initial hypothesis.
11 � P(x|xk−1:k −n) log2 P(x|xk−1:k−n)
xEX
11 � P(x|xk:k+n −1) log2 P(x|xk:k+n−1).
xEX
</equation>
<bodyText confidence="0.999967809523809">
Suffix arrays are employed during the collection
of frequency statistics. For a character model of or-
der n over a testing corpus of size t and a training
corpus of size m, suffix arrays allow these to be
acquired in O(tn log m) time. Faster implementa-
tions reduce the complexity to O(t(n+log m)). For
further discussion, see Manber and Myers (1991).
During the experiments, we did not use the caching
functionality provided by the suffix array library, but
instead kept the statistics for the current iterative
pass (n-gram order and direction) in a local table.
The chunking technique we adopt is to insert a
boundary when the branching entropy measured in
sequences of length n exceeds a certain threshold
value (H(X|xk−1:k−n) &gt; 0). Both n and 0 are fixed.
Within the described framework, the increase in
context length n promotes precision and recall at
first, but causes a performance degradation when the
entropy estimates become unreliable due to the re-
duced frequencies of long strings. High threshold
values produce a combination of high precision and
low recall, while low values result in low precision
and high recall.
Since the F-score curve obtained as decreasing
values are assigned to the threshold is typically uni-
modal as in many applications of MDL, we employ
a bisection search routine for the estimation of the
threshold (Algorithm 1).
All positions of the dataset are sorted by their en-
tropy values. At each iteration, at most two new
hypotheses are built, and their description lengths
are calculated in time linear to the data size. The
computational complexity of the described routine
is O(t log t), where t is the corpus length in charac-
ters.
The order of the Markov chain n used during the
entropy calculation is the only input variable of the
proposed model. Since different values perform the
best across the various languages, the most appro-
priate settings can be obtained with the help of a
small annotated corpus. However, the MDL objec-
tive also enables unsupervised optimization against
</bodyText>
<equation confidence="0.996126636363637">
thresholds[] := sorted H(Xk) values;
threshold := median of thresholds[];
step := length of thresholds[]/4;
direction := ascending;
minimum := +oo;
while step &gt; 0 do
nextThreshold := thresholds[] value one step in last
direction;
DL = calculateDL(nextThreshold);
if DL &lt; minimum then
minimum:= DL; threshold := nextThreshold;
step := step/2; continue;
end if
reverse direction;
nextThreshold := thresholds[] value one step in last
direction;
if DL &lt; minimum then
minimum:= DL; threshold := nextThreshold;
step := step/2; continue;
end if
reverse direction;
step := step/2;
</equation>
<table confidence="0.87270375">
end while
Corpus [1] [2] [3] [4]
CHILDES 394655.52 367711.66 368056.10 405264.53
Kyoto 1.291E+07 1.289E+07 1.398E+07 1.837E+07
</table>
<tableCaption confidence="0.9140285">
Table 1: Length in bits of the solutions proposed by Al-
gorithm 1 with respect to the character n-gram order.
</tableCaption>
<bodyText confidence="0.999442153846154">
a sufficiently large unlabeled dataset. The order that
minimizes the description length of the data can be
discovered in a few iterations of Algorithm 1 with
increasing values of n, and it typically matches the
optimal value of the parameter (Table 1).
Although an acceptable initial segmentation can
be built using the described approach, it is possible
to obtain higher accuracy with an extended model
that takes into account the statistics of Markov
chains from several orders during the entropy calcu-
lation. This can be done by summing up the entropy
estimates, in the way introduced earlier for combin-
ing the values in both directions:
</bodyText>
<equation confidence="0.860011">
H&apos;&apos;(Xk;k−1|xk−1:k−n; xk:k+n−1) _
P(x|xk−1:k−n)log2 P(x|xk−1:k−n)
+ 11 P(x|xk:k+n−1)log2 P(x|xk:k+n−1)),
xEX
</equation>
<figure confidence="0.780836166666667">
�
nmax
n=1
11
(
xEX
</figure>
<page confidence="0.991294">
835
</page>
<bodyText confidence="0.9998835">
where nma, is the index of the highest order to be
taken into consideration.
</bodyText>
<subsectionHeader confidence="0.998754">
3.3 Refining the initial hypothesis
</subsectionHeader>
<bodyText confidence="0.999969613636364">
In the second phase of the proposed method, we will
refine the initial hypothesis through the reorganiza-
tion of local co-occurrences which produce redun-
dant description length. We opt for greedy optimiza-
tion, as our primary interest is to further explore the
impact that description length minimization has on
accuracy. Of course, such an approach is unlikely
to obtain global minima, but it is a feasible means of
conducting the optimization process, and guarantees
a certain increase in compression.
Since a preliminary segmentation is available, it
is convenient to proceed by inserting or removing
boundaries in the text, thus splitting or merging the
already discovered tokens. The ranked positions in-
volved in the previous step can be reused here, as
this is a way to bias the search towards areas of
the text where boundaries are more likely to occur.
Boundary insertion should start in regions where the
branching entropy is high, and removal should first
occur in regions where the entropy is close to zero.
A drawback of this approach is that it omits loca-
tions where the gains are not immediately obvious,
as it cannot assess the cumulative gains arising from
the merging or splitting of all occurrences of a cer-
tain pair (Algorithm 2).
A clean-up routine, which compensates for this
shortage, is also implemented (Algorithm 3). It op-
erates directly on the types found in the lexicon pro-
duced by Algorithm 2, and is capable of modify-
ing a large number of occurrences of a given pair
in a single step. The lexicon types are sorted by
their contribution to the total description length of
the corpus. For each word type, splitting or merg-
ing is attempted at every letter, beginning from the
center. The algorithm eliminates unlikely types with
low contribution, which represent mostly noise, and
redistributes their cost among more likely ones. The
design of the merging routine makes it impossible to
produce types longer than the ones already found in
the lexicon, as an exhaustive search would be pro-
hibitive.
The evaluation of each hypothetical change in
the segmentation requires that the description length
of the two-part code is recalculated. In order to
</bodyText>
<table confidence="0.899995611111111">
Algorithm 2 Compresses local token co-occurrences.
path[][]:= positions sorted by H(Xk) values;
minimum := DL of model produced at initialization;
repeat
for i = max H(Xk) to min H(Xk) do
pos:= path[i][k];
if no boundary exists at pos then
leftToken := token to the left;
rightToken := token to the right;
longToken := leftToken + rightToken;
calculate DL after splitting;
if DL &lt; minimum then
accept split, update model, update DP vari-
ables;
end if
end if
end for
for i = min H(Xk) to max H(Xk) do
merge leftToken and rightToken into longToken
if DL will decrease (analogous to splitting)
end for
until no change is evident in model
Algorithm 3 A lexicon clean-up procedure.
types[] := lexicon types sorted by cost;
minimum := DL of model produced by Algorithm 2;
repeat
for i = min cost to max cost do
for pos = middle to both ends of types[i] do
longType := types[i];
leftType := sequence from first character to
pos;
rightType:= sequence from pos to last charac-
ter;
calculate DL after splitting longType into left-
Type and rightType;
if DL &lt; minimum then
</table>
<tableCaption confidence="0.582946">
accept split, update model, update DP vari-
ables;
break out of inner loop;
end if
end for
end for
</tableCaption>
<bodyText confidence="0.7364745">
types[] := lexicon types sorted by cost;
for i = max cost to min cost do
for pos = middle to both ends of types[i] do
merge leftType and rightType into longType if
DL will decrease (analogous to splitting)
break out of inner loop;
</bodyText>
<subsectionHeader confidence="0.5302365">
end for
end for
</subsectionHeader>
<bodyText confidence="0.607425">
until no change is evident in model
</bodyText>
<page confidence="0.994652">
836
</page>
<bodyText confidence="0.999959">
make this optimization phase computationally fea-
sible, dynamic programming is employed in Algo-
rithms 2 and 3. The approach adopted for the re-
calculation of the source text term L(D|M) is ex-
plained below. The estimation of the lexicon cost is
analogous. The term L(D|M) can be rewritten as:
</bodyText>
<equation confidence="0.832349">
#wj
#wj log2 N
#wj log2 #wj + N log2 N = T1 + T2,
</equation>
<bodyText confidence="0.983188941176471">
where #wj is the frequency of wj in the segmented
corpus, and N = ����
j=1 #wj is the cumulative to-
ken count. In order to calculate the new length, we
keep the values of the terms T1 and T2 obtained at
the last change of the model. Their new values are
computed for each hypothetical split or merge on the
basis of the last values, and the expected description
length is calculated as their sum. If the produced es-
timate is lower, the model is modified and the new
values of T1 and T2 are stored for future use.
In order to maintain precise token counts, Algo-
rithms 2 and 3 recognize the fact that recurring se-
quences (”byebye” etc.) appear in the corpora, and
handle them accordingly. Known boundaries, such
as the sentence boundaries in the CHILDES corpus,
are also taken into consideration.
</bodyText>
<sectionHeader confidence="0.998558" genericHeader="method">
4 Experimental Settings
</sectionHeader>
<bodyText confidence="0.999766352941176">
We evaluated the proposed model against four
datasets. The first one is the Bernstein-Ratner cor-
pus for language acquisition based on transcripts
from the CHILDES database (Bernstein-Ratner,
1987). It comprises phonetically transcribed utter-
ances of adult speech directed to 13 through 21-
month-old children. We evaluated the performance
of our learner in the cases when the few boundaries
among the individual sentences are available to it
(B), and when it starts from a blank state (N). The
Kyoto University Corpus (Kurohashi and Nagao,
1998) is a standard dataset for Japanese morpho-
logical and dependency structure analysis, which
comprises newspaper articles and editorials from the
Mainichi Shimbun. The BEST corpus for word seg-
mentation and named entity recognition in Thai lan-
guage combines text from a variety of sources in-
</bodyText>
<table confidence="0.998965461538462">
Corpus Language Size Chars Tokens Types
(MB) (K) (K) (K)
CHILDES- English 0.1 95.8 33.3 1.3
B/N
Kyoto Japanese 5.02 1674.9 972.9 39.5
WSJ English 5.22 5220.0 1174.2 49.1
BEST-E Thai 12.64 4360.2 1163.2 26.2
BEST-N Thai 18.37 6422.7 1659.4 36.3
BEST-A Thai 4.59 1619.9 438.7 13.9
BEST-F Thai 16.18 5568.0 1670.8 22.6
Wikipedia Japanese 425.0 169069.3 / /
Asahi Japanese 337.2 112401.1 / /
BEST-All Thai 51.2 17424.0 4371.8 73.4
</table>
<tableCaption confidence="0.809767666666667">
Table 2: Corpora used during the evaluation. Precise to-
ken and type counts have been omitted for Wikipedia and
Asahi, as no gold standard segmentations are available.
</tableCaption>
<bodyText confidence="0.999842137931035">
cluding encyclopedias (E), newspaper articles (N),
scientific articles (A), and novels (F). The WSJ sub-
set of the Penn Treebank II Corpus incorporates
selected stories from the Wall Street Journal, year
1989 (Marcus et al., 1994). Both the original text
(O), and a version in which all characters were con-
verted to lower case (L) were used.
The datasets listed above were built by remov-
ing the tags and blank spaces found in the corpora,
and concatenating the remaining text. We added
two more training datasets for Japanese, which were
used in a separate experiment solely for the acqui-
sition of frequency statistics. One of them was
created from 200,000 randomly chosen Wikipedia
articles, stripped from structural elements. The
other one contains text from the year 2005 issues of
Asahi Newspaper. Statistics regarding all described
datasets are presented in Table 2.
One whole corpus is segmented in each experi-
ment, in order to avoid the statement of an extended
model that would allow the separation of training
and test data. This setting is also necessary for the
direct comparison between the proposed model and
other recent methods evaluated against the entire
CHILDES corpus.
We report the obtained precision, recall and F-
score values calculated using boundary, token and
type counts. Precision (P) and recall (R) are defined
as:
</bodyText>
<equation confidence="0.9920535">
P = #correct units R = #correct units
# output units, �
</equation>
<bodyText confidence="0.842221666666667">
#gold standard units
Boundary, token and lexicon F-scores, denoted
as B-F and T-F and L-F, are calculated as the
</bodyText>
<equation confidence="0.991499625">
|M|
L(D|M) = − E
j=1
=
−
|M|
E
j=1
</equation>
<page confidence="0.995525">
837
</page>
<table confidence="0.99929425">
Model Corpus &amp; Settings B-Prec B-Rec B-F T-Prec T-Rec T-F DL Ref.DL Time
(bits) (bits) (ms)
1 CHILDES, a = 1.2, n = [1-6] 0.8667 0.8898 0.8781 0.6808 0.6990 0.6898 344781.74 300490.52 1060.2
2a (H&apos;) CHILDES, n = 2 0.7636 0.9109 0.8308 0.5352 0.6384 0.5823 367711.66 753.1
2b (H&apos;&apos;) CHILDES, nmaa = 3 0.8692 0.8865 0.8777 0.6792 0.6927 0.6859 347633.07 885.3
1 Kyoto, a = 0, n = [1-6] 0.8208 0.8208 0.8208 0.5784 0.5784 0.5784 1.325E+07 1.120E+07 54958.8
2a (H&apos;) Kyoto, n = 2 0.8100 0.8621 0.8353 0.5934 0.6316 0.6119 1.289E+07 22909.7
2b (H&apos;&apos;) Kyoto, nmaa = 2 0.8024 0.9177 0.8562 0.6093 0.6969 0.6501 1.248+E07 23212.8
</table>
<tableCaption confidence="0.9715965">
Table 3: Comparison of the proposed method (2a, 2b) with the model of Jin and Tanaka-Ishii (2006) (1). Execution
times include the obtaining of frequency statistics, and are represented by averages over 10 runs.
</tableCaption>
<bodyText confidence="0.999914238095238">
harmonic averages of the corresponding precision
and recall values (F = 2PR/(P + R)). As a
rule, boundary-based evaluation produces the high-
est scores among the three evaluation modes, as it
only considers the correspondence between the pro-
posed and the gold standard boundaries at the indi-
vidual positions of the corpora. Token-based evalua-
tion is more strict – it accepts a word as correct only
if its beginning and end are identified accurately, and
no additional boundaries lie in between. Lexicon-
based evaluation reflects the extent to which the vo-
cabulary of the original text has been recovered.
It provides another useful perspective for the error
analysis, which in combination with token scores
can give a better idea of the relationship between the
accuracy of induction and item frequency.
The system was implemented in Java, however it
handled the suffix arrays through an external C li-
brary called Sary.1 All experiments were conducted
on a 2 GHz Core2Duo T7200 machine with 2 GB
RAM.
</bodyText>
<sectionHeader confidence="0.998555" genericHeader="evaluation">
5 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999794230769231">
The scores we obtained using the described instan-
tiations of the branching entropy criterion at the ini-
tialization phase are presented in Table 3, along with
those generated by our own implementation of the
method presented in Jin and Tanaka-Ishii (2006),
where the threshold parameter α was adjusted man-
ually for optimal performance.
The heuristic of Jin and Tanaka-Ishii takes advan-
tage of the trend that branching entropy decreases
as the observed character sequences become longer;
sudden rises can thus be regarded as an indication of
locations where a boundary is likely to exist. Their
method uses a common value for thresholding the
</bodyText>
<footnote confidence="0.892978">
1http://sary.sourceforge.net
</footnote>
<bodyText confidence="0.999763394736842">
entropy change throughout all n-gram orders, and
combines the boundaries discovered in both direc-
tions in a separate step. These properties of the
method would lead to complications if we tried to
employ it in the first phase of our method (i.e. a step
parameter for iterative adjustment of the threshold
value, rules for combining the boundaries, etc.).
The proposed criterion with an automatically de-
termined threshold value produced slightly worse
results than that of Jin and Tanaka-Ishii at the
CHILDES corpus. However, we found out that our
approach achieves approximately 1% higher score
when the best performing threshold value is selected
from the candidate list. There are two observations
that account for the suboptimal threshold choice by
our algorithm. On one hand, the correspondence
between description length and F-score is not abso-
lutely perfect, and this may pose an obstacle to the
optimization process for relatively small language
samples. Another issue lies in the bisection search
routine, which suggests approximations of the de-
scription length minima. The edge that our method
has on the Kyoto corpus can be attributed to a better
estimation of the optimal treshold value due to the
larger amount of data.
The experimental results obtained at the comple-
tion of Algorithm 3 are summarized in Tables 4 and
5. Presented durations include the obtaining of fre-
quency statistics. The nmax parameter is set to the
value which maximizes the compression during the
initial phase, in order to make the results representa-
tive of the case in which no annotated development
corpora are accessible to the algorithm.
It is evident that after the optimization carried out
in the second phase, the description length is re-
duced to levels significantly lower than the ground
truth. In this aspect, the algorithm outperforms the
EM-based method of Yu (2000).
</bodyText>
<page confidence="0.994228">
838
</page>
<table confidence="0.998064818181818">
Corpus &amp; Settings B-F T-F L-F Time
(ms)
CHILDES-B, nmaa=3 0.9092 0.7542 0.5890 2597.2
CHILDES-N, nmaa=3 0.9070 0.7499 0.5578 2949.3
Kyoto, nmaa=2 0.8855 0.7131 0.3725 70164.6
BEST-E, nmaa=5 0.9081 0.7793 0.3549 738055.0
BEST-N, nmaa=5 0.8811 0.7339 0.2807 505327.0
BEST-A, nmaa=5 0.9045 0.7632 0.4246 250863.0
BEST-F, nmaa=5 0.9343 0.8216 0.4820 305522.0
WSJ-O, nmaa=6 0.8405 0.6059 0.3338 658214.0
WSJ-L, nmaa=6 0.8515 0.6373 0.3233 582382.0
</table>
<tableCaption confidence="0.9925525">
Table 4: Results obtained after the termination of Algo-
rithm 3.
</tableCaption>
<table confidence="0.999963363636364">
Corpus &amp; Settings Description Description
Length (Proposed) Length (Total)
CHILDES-B, nmaa=3 290592.30 300490.52
CHILDES-N, nmaa=3 290666.12 300490.52
Kyoto, nmaa=2 1.078E+07 1.120E+07
BEST-E, nmaa=5 1.180E+07 1.252E+07
BEST-N, nmaa=5 1.670E+07 1.809E+07
BEST-A, nmaa=5 4438600.32 4711363.62
BEST-F, nmaa=5 1.562E+07 1.634E+07
WSJ-O, nmaa=6 1.358E+07 1.460E+07
WSJ-L, nmaa=6 1.317E+07 1.399E+07
</table>
<tableCaption confidence="0.9831905">
Table 5: Description length - proposed versus reference
segmentation.
</tableCaption>
<bodyText confidence="0.999203933333333">
We conducted experiments involving various ini-
tialization strategies: scattering boundaries at ran-
dom throughout the text, starting from entirely un-
segmented state, or considering each symbol of the
text to be a separate token. The results obtained
with random initialization confirm the strong rela-
tionship between compression and segmentation ac-
curacy, evident in the increase of token F-score be-
tween the random initialization and the termination
of the algorithm, where description length is lower
(Table 6). They also reveal the importance of the
branching entropy criterion to the generation of hy-
potheses that maximize the evaluation scores and
compression, as well as the role it plays in the re-
duction of computational time.
</bodyText>
<table confidence="0.996794166666667">
T-F-Score Description Time
Length (ms)
Random Init Refinement
0.0441 (0.25) 0.3833 387603.02 6660.4
0.0713 (0.50) 0.3721 383279.86 4975.1
0.0596 (0.75) 0.2777 412743.67 3753.3
</table>
<tableCaption confidence="0.994814666666667">
Table 6: Experimental results for CHILDES-N with ran-
domized initialization and search path. The numbers in
brackets represent the seed boundaries/character ratios.
</tableCaption>
<bodyText confidence="0.9999093125">
The greedy algorithms fail to suggest any opti-
mizations that improve the compression in the ex-
treme cases when the boundaries/character ratio is
either 0 or 1. When no boundaries are given, split-
ting operations produce unique types with a low
frequency that increase the cost of both parts of
the MDL code, and are rejected. The algorithm
runs slowly, as each evaluation operates on candi-
date strings of enormous length. Similarly, when the
corpus is broken down into single-character tokens,
merging individual pairs does not produce any in-
crease in compression. This could be achieved by an
algorithm that estimates the total effect from merg-
ing all instances of a given pair, but such an algo-
rithm would be computationally infeasible for large
corpora.
Finally, we tried randomizing the search path for
Algorithm 2 after an entropy-guided initialization, to
observe a small deterioration in accuracy in the final
segmentation (less than 1% on average).
Figure 1a illustrates the effect that training data
size has on the accuracy of segmentation for the Ky-
oto corpus. The learning curves are similar through-
out the different corpora. For the CHILDES cor-
pus, which has a rather limited vocabulary, token
F-score above 70% can be achieved for datasets as
small as 5000 characters of training data, provided
that reasonable values are set for the nmax parameter
(we used the values presented in Table 4 throughout
these experiments).
Figure 1b shows the evolution of token F-score by
stage for all corpora. The initialization phase seems
to have the highest contribution to the formation of
the final segmentation, and the refinement phase is
highly dependent on the output it produces. As a
consequence, results improve when a more adequate
language sample is provided during the learning of
local dependencies at initialization. This is evident
in the experiments with the larger unlabeled Thai
and Japanese corpora.
For Japanese language with the setting for the
nmax parameter that maximized compression, we
observed an almost 4% increase in the token F-score
produced at the end of the first phase with the Asahi
corpus as training data. Only a small (less than 1%)
rise was observed in the overall performance. The
quite larger dataset of randomly chosen Wikipedia
articles achieved no improvement. We attributed this
</bodyText>
<page confidence="0.997627">
839
</page>
<figureCaption confidence="0.996156">
Figure 1: a) corpus size / accuracy relationship (Kyoto); b) accuracy levels by phase; c) accuracy levels by phase
with various corpora for frequency statistics (Kyoto); d) accuracy levels by phase with different corpora for frequency
statistics (BEST).
</figureCaption>
<bodyText confidence="0.999901785714286">
to the higher degree of correspondence between the
domains of the Asahi and Kyoto corpora (Figure 1c).
Experiments with the BEST corpus reveal bet-
ter the influence of domain-specific data on the ac-
curacy of segmentation. Performance deteriorates
significantly when out-of-domain training data is
used. In spite of its size, the assorted composite cor-
pus, in which in-domain and out-of-domain training
data are mixed, produces worse results than the cor-
pora which include only domain-specific data (Fig-
ure 1d).
Finally, a comparison of the proposed method
with Bayesian n-gram models is presented in Ta-
ble 7. Through the increase of compression in the
refinement phase of the algorithm, accuracy is im-
proved by around 3%, and the scores approach those
of the explicit probabilistic models of Goldwater et
al. (2009) and Mochihashi et al. (2009). The pro-
posed learner surpasses the other unsupervised word
induction models in terms of processing speed. It
should be noticed that a direct comparison of accu-
racy is not possible with Mochihashi et al. (2009),
as they evaluated their system with separate datasets
for training and testing. Furthermore, different seg-
mentation standards exist for Japanese, and there-
fore the ”ground truth” provided by the Kyoto cor-
pus cannot be considered an ideal measure of accu-
racy.
</bodyText>
<sectionHeader confidence="0.998906" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999876333333333">
This paper has presented an efficient algorithm for
unsupervised word induction, which relies on a
combination of evidences. New instantiations of the
branching entropy and MDL criteria have been pro-
posed and evaluated against corpora in different lan-
guages. The MDL-based optimization eliminates
the discretion in the choice of the context length
and threshold parameters, common in segmenta-
tion models based on local statistics. At the same
time, the branching entropy criterion enables a con-
strained search through the hypothesis space, allow-
ing the proposed method to demonstrate a very high
</bodyText>
<page confidence="0.988692">
840
</page>
<table confidence="0.999691125">
Model Corpus T-Prec T-Rec T-F L-Prec L-Rec L-F Time
NPY(3) CHILDES 0.7480 0.7520 0.7500 0.4780 0.5970 0.5310 17 min
NPY(2) CHILDES 0.7480 0.7670 0.7570 0.5730 0.5660 0.5700 17 min
HDP(2) CHILDES 0.7520 0.6960 0.7230 0.6350 0.5520 0.5910 -
Ent-MDL CHILDES 0.7634 0.7453 0.7542 0.6844 0.5170 0.5890 2.60 sec
NPY(2) Kyoto - - 0.6210 - - - -
NPY(3) Kyoto - - 0.6660 - - - -
Ent-MDL Kyoto 0.6912 0.7365 0.7131 0.5908 0.2720 0.3725 70.16 sec
</table>
<tableCaption confidence="0.985151">
Table 7: Comparison of the proposed method (Ent-MDL) with the methods of Mochihashi et al., 2009 (NPY) and
Goldwater et al., 2009 (HDP).
</tableCaption>
<bodyText confidence="0.9983535">
performance in terms of both accuracy and speed.
Possible improvements of the proposed method
include modeling the dependencies among neigh-
boring tokens, which would allow the evaluation
of the context to be reflected in the cost func-
tion. Mechanisms for stochastic optimization imple-
mented in the place of the greedy algorithms could
provide an additional flexibility of search for such
more complex models. As the proposed approach
provides significant performance improvements, it
could be utilized in the development of more so-
phisticated novel word induction schemes, e.g. en-
semble models trained independently with different
data. Of course, we are also going to explore the
model’s potential in the setting of semi-supervised
morphological analysis.
</bodyText>
<sectionHeader confidence="0.999074" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999303483870968">
Bernstein-Ratner, Nan 1987. The phonology of parent –
child speech. Childrens Language, 6:159–174
Brent, Michael R and Timothy A. Cartwright. 1996. Dis-
tributional Regularity and Phonotactic Constraints are
Useful for Segmentation. Cognition 61: 93–125
Goldwater, Sharon. 2006. Nonparametric Bayesian
Models of Lexical Acquisition. Brown University,
Ph.D. Thesis
Goldwater, Sharon, Thomas L. Griffiths and Mark John-
son. 2006. Contextual dependencies in unsupervised
word segmentation. Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
the 44th annual meeting of the Association for Com-
putational Linguistics, Sydney, 673–680
Goldwater, Sharon, Thomas L. Griffiths and Mark John-
son. 2009. A Bayesian framework for word segmen-
tation: Exploring the effects of context. Cognition,
112:1, 21–54.
Harris, Zellig. 1955. From Phoneme to Morpheme. Lan-
guage, 31(2):190-222.
Huang, Jin H. and David Powers. 2003. Chinese Word
Segmentation Based on Contextual Entropy. Proceed-
ings of 17th Pacific Asia Conference, 152–158
Hutchens, Jason L. and Michael D. Alder. 1998. Finding
structure via compression. Proceedings of the Inter-
national Conference on Computational Natural Lan-
guage Learning, 79–82
Jin, Zhihui and Kumiko Tanaka-Ishii. 2006. Unsuper-
vised Segmentation of Chinese Text by Use of Branch-
ing Entropy. Proceedings of the COLING/ACL on
Main conference poster sessions, 428–435
Johnson, Mark and Sharon Goldwater. 2009. Improving
nonparameteric Bayesian inference: experiments on
unsupervised word segmentation with adaptor gram-
mars. Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Association for Computational Linguistics, 317–325.
Kempe, Andre. 1999. Experiments in Unsupervised
Entropy Based Corpus Segmentation. Proceedings of
CoNLL’99, pp. 371–385
Kit, Chunyu. 2003. How does lexical acquisition begin?
A cognitive perspective. Cognitive Science 1(1): 1–
50.
Kurohashi, Sadao and Makoto Nagao. 1998. Building
a Japanese Parsed Corpus while Improving the Pars-
ing System. Proceedings of the First International
Conference on Language Resources and Evaluation,
Granada, Spain, 719–724
Lafferty, John, Andrew McCallum and Fernando Pereira.
2001. Conditional Random Fields: Probabilistic Mod-
els for Segmenting and Labeling Sequence Data. Pro-
ceedings of the International Conference on Machine
Learning.
Li, Hang. 1998. A Probabilistic Approach to Lexical
Semantic Knowledge Acquisition and Structural Dis-
ambiguation. University of Tokyo, Ph.D. Thesis
Liang, Percy. 2005. Semi-Supervised Learning for Nat-
ural Language. Massachusets Institute of Technology,
Master’s Thesis.
Manber, Udi and Gene Myers. 1991. Suffix arrays: a
new method for on-line string searches. SIAM Journal
on Computing 22:935–948
</reference>
<page confidence="0.978906">
841
</page>
<reference confidence="0.999759821428572">
Marcus, Mitchell, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen
Katz and Britta Schasberger. 1994. The Penn Tree-
bank: Annotating Predicate Argument Structure. Hu-
man Language Technology, 114–119
Mochihashi, Daiichi, Takeshi Yamada and Naonori Ueda.
2009. Bayesian unsupervised word segmentation with
nested Pitman-Yor language modeling. Proceedings
of the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference on
Natural Language Processing of the Asian Federation
of Natural Language Processing, 1: 100–108
Rissanen, Jorma. 1978. Modeling by Shortest Data De-
scription. Aulomatica, 14:465–471.
Saffran, Jenny R., Richard N. Aslin and Elissa L. New-
port. 1996. Statistical learning in 8-month-old infants
Science; 274:1926-1928
Tsuboi, Yuta, Hisashi Kashima., Hiroki Oda, Shinsuke
Mori and Yuji Matsumoto. 2008. Training Condi-
tional Random Fields Using Incomplete Annotations.
Proceedings of the 22nd International Conference on
Computational Linguistics - Volume 1,897–904.
Yu, Hua. 2000. Unsupervised word induction using
MDL criterion. Proceedings of tne International Sym-
posium of Chinese Spoken Language Processing, Bei-
jing.
Zipf, George K. 1949. Human Behavior and the Princi-
ple of Least Effort. Addison-Wesley.
</reference>
<page confidence="0.998025">
842
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.054123">
<title confidence="0.9982735">An Efficient Algorithm for Unsupervised Word Segmentation Branching Entropy and MDL</title>
<author confidence="0.919974">Valentin</author>
<affiliation confidence="0.844133666666667">Interdisciplinary Graduate of Science and Tokyo Institute of</affiliation>
<abstract confidence="0.723653115384615">zhikov@lr.pi.titech.ac.jp Hiroya Precision and Intelligence Tokyo Institute of takamura@pi.titech.ac.jp Manabu Precision and Intelligence Tokyo Institute of oku@pi.titech.ac.jp Abstract This paper proposes a fast and simple unsupervised word segmentation algorithm that utilizes the local predictability of adjacent character sequences, while searching for a leasteffort representation of the data. The model uses branching entropy as a means of constraining the hypothesis space, in order to efficiently obtain a solution that minimizes the length of a two-part MDL code. An evaluation with corpora in Japanese, Thai, English, and the ”CHILDES” corpus for research in language development reveals that the algorithm achieves an accuracy, comparable to that of the state-of-the-art methods in unsupervised word segmentation, in a significantly reduced computational time.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nan Bernstein-Ratner</author>
</authors>
<title>The phonology of parent – child speech. Childrens Language,</title>
<date>1987</date>
<pages>6--159</pages>
<contexts>
<context position="19961" citStr="Bernstein-Ratner, 1987" startWordPosition="3271" endWordPosition="3272">sum. If the produced estimate is lower, the model is modified and the new values of T1 and T2 are stored for future use. In order to maintain precise token counts, Algorithms 2 and 3 recognize the fact that recurring sequences (”byebye” etc.) appear in the corpora, and handle them accordingly. Known boundaries, such as the sentence boundaries in the CHILDES corpus, are also taken into consideration. 4 Experimental Settings We evaluated the proposed model against four datasets. The first one is the Bernstein-Ratner corpus for language acquisition based on transcripts from the CHILDES database (Bernstein-Ratner, 1987). It comprises phonetically transcribed utterances of adult speech directed to 13 through 21- month-old children. We evaluated the performance of our learner in the cases when the few boundaries among the individual sentences are available to it (B), and when it starts from a blank state (N). The Kyoto University Corpus (Kurohashi and Nagao, 1998) is a standard dataset for Japanese morphological and dependency structure analysis, which comprises newspaper articles and editorials from the Mainichi Shimbun. The BEST corpus for word segmentation and named entity recognition in Thai language combi</context>
</contexts>
<marker>Bernstein-Ratner, 1987</marker>
<rawString>Bernstein-Ratner, Nan 1987. The phonology of parent – child speech. Childrens Language, 6:159–174</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael R Brent</author>
<author>Timothy A Cartwright</author>
</authors>
<title>Distributional Regularity and Phonotactic Constraints are Useful for Segmentation.</title>
<date>1996</date>
<journal>Cognition</journal>
<volume>61</volume>
<pages>93--125</pages>
<contexts>
<context position="4409" citStr="Brent and Cartwright (1996)" startWordPosition="647" endWordPosition="650"> comply to this principle, as they reorganize the data into a more compact representation while identifying the vocabulary of a text. The minimum description length framework (MDL) (Rissanen, 1978) is an appealing means of formalizing such models, as it provides a robust foundation for learning and inference, based solely on compression. The major problem in MDL-based word segmentation is the lack of standardized search algorithms for the exponential hypothesis space (Goldwater, 2006). The representative MDL models compare favorably to the current state-of-the-art models in terms of accuracy. Brent and Cartwright (1996) carried out an exhaustive search through the possible segmentations of a limited subset of the data. Yu (2000) proposed an EM optimization routine, which achieved a high accuracy, in spite of a lower compression than the gold standard segmentation. As a solution to the aforementioned issue, the proposed method incorporates the local predictability of character sequences into the inference process. Numerous studies have shown that local distributional cues can serve well the purpose of inducing word boundaries. Behavioral science has confirmed that infants are sensitive to the transitional pro</context>
</contexts>
<marker>Brent, Cartwright, 1996</marker>
<rawString>Brent, Michael R and Timothy A. Cartwright. 1996. Distributional Regularity and Phonotactic Constraints are Useful for Segmentation. Cognition 61: 93–125</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
</authors>
<title>Nonparametric Bayesian Models of Lexical Acquisition.</title>
<date>2006</date>
<tech>Ph.D. Thesis</tech>
<institution>Brown University,</institution>
<contexts>
<context position="4271" citStr="Goldwater, 2006" startWordPosition="630" endWordPosition="631">earch has recognized its importance in the process of language acquisition (Kit, 2003). Compression-based word induction models comply to this principle, as they reorganize the data into a more compact representation while identifying the vocabulary of a text. The minimum description length framework (MDL) (Rissanen, 1978) is an appealing means of formalizing such models, as it provides a robust foundation for learning and inference, based solely on compression. The major problem in MDL-based word segmentation is the lack of standardized search algorithms for the exponential hypothesis space (Goldwater, 2006). The representative MDL models compare favorably to the current state-of-the-art models in terms of accuracy. Brent and Cartwright (1996) carried out an exhaustive search through the possible segmentations of a limited subset of the data. Yu (2000) proposed an EM optimization routine, which achieved a high accuracy, in spite of a lower compression than the gold standard segmentation. As a solution to the aforementioned issue, the proposed method incorporates the local predictability of character sequences into the inference process. Numerous studies have shown that local distributional cues c</context>
<context position="9367" citStr="Goldwater, 2006" startWordPosition="1478" endWordPosition="1479">odel is thus calculated as: (the total length of the text in characters). The parametric complexity term is calculated in the same way for the lexicon. For a derivation of the above formula, refer to e.g. Li (1998). MDL is closely related to Bayesian inference. Depending on the choice of a universal code, the two approaches can overlap, as is the case with the twopart code discussed in this paper. It can be shown that the model selection in our method is equivalent to a MAP inference, conducted under the assumption that the prior probability of a model decreases exponentially with its length (Goldwater, 2006). Thus, the task that we are trying to accomplish is to conduct a focused search through the hypothesis space that will allow us to obtain an approximation of the MAP solution in a reasonable time. The MDL framework does not provide standard search algorithms for obtaining the hypotheses that minimize the description length. In the rest of this section, we will describe an efficient technique suitable for the word segmentation task. 3.2 Obtaining an initial hypothesis First, a rough initial hypothesis is built by an algorithm that combines the branching entropy and MDL criteria. Given a set X,</context>
</contexts>
<marker>Goldwater, 2006</marker>
<rawString>Goldwater, Sharon. 2006. Nonparametric Bayesian Models of Lexical Acquisition. Brown University, Ph.D. Thesis</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>Contextual dependencies in unsupervised word segmentation.</title>
<date>2006</date>
<booktitle>Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>673--680</pages>
<location>Sydney,</location>
<contexts>
<context position="6142" citStr="Goldwater et al. (2006)" startWordPosition="921" endWordPosition="925">thods for unsupervised word segmentation based solely on local statistics presume a certain – albeit minimum – level of acquaintance with the target language. For instance, the model of Huang and Powers (2003) involves some parameters (Markov chain order, numerous threshold values) that allow its adaptation to the individuality of written Chinese. In comparison, the method proposed in this paper generalizes easily to a variety of languages and domains, and is less dependent on annotated development data. The state-of-the-art in unsupervised word segmentation is represented by Bayesian models. Goldwater et al. (2006) justified the importance of context as a means of avoiding undersegmentation, through a method based on hierarchical Dirichlet processes. Mochihashi et al. (2009) proposed extensions to this method, which included a nested character model and an optimized inference procedure. Johnson and Goldwater (2009) have proposed a novel method based on adaptor grammars, whose accuracy surpasses the aforementioned methods by a large margin, when appropriate assumptions are made regarding the structural units of a language. 3 Proposed Method 3.1 Word segmentation with MDL The proposed two-part code incorp</context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2006</marker>
<rawString>Goldwater, Sharon, Thomas L. Griffiths and Mark Johnson. 2006. Contextual dependencies in unsupervised word segmentation. Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, Sydney, 673–680</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>A Bayesian framework for word segmentation: Exploring the effects of context.</title>
<date>2009</date>
<journal>Cognition,</journal>
<volume>112</volume>
<pages>21--54</pages>
<contexts>
<context position="32604" citStr="Goldwater et al. (2009)" startWordPosition="5273" endWordPosition="5276">the accuracy of segmentation. Performance deteriorates significantly when out-of-domain training data is used. In spite of its size, the assorted composite corpus, in which in-domain and out-of-domain training data are mixed, produces worse results than the corpora which include only domain-specific data (Figure 1d). Finally, a comparison of the proposed method with Bayesian n-gram models is presented in Table 7. Through the increase of compression in the refinement phase of the algorithm, accuracy is improved by around 3%, and the scores approach those of the explicit probabilistic models of Goldwater et al. (2009) and Mochihashi et al. (2009). The proposed learner surpasses the other unsupervised word induction models in terms of processing speed. It should be noticed that a direct comparison of accuracy is not possible with Mochihashi et al. (2009), as they evaluated their system with separate datasets for training and testing. Furthermore, different segmentation standards exist for Japanese, and therefore the ”ground truth” provided by the Kyoto corpus cannot be considered an ideal measure of accuracy. 6 Conclusions and Future Work This paper has presented an efficient algorithm for unsupervised word</context>
<context position="34300" citStr="Goldwater et al., 2009" startWordPosition="5549" endWordPosition="5552">owing the proposed method to demonstrate a very high 840 Model Corpus T-Prec T-Rec T-F L-Prec L-Rec L-F Time NPY(3) CHILDES 0.7480 0.7520 0.7500 0.4780 0.5970 0.5310 17 min NPY(2) CHILDES 0.7480 0.7670 0.7570 0.5730 0.5660 0.5700 17 min HDP(2) CHILDES 0.7520 0.6960 0.7230 0.6350 0.5520 0.5910 - Ent-MDL CHILDES 0.7634 0.7453 0.7542 0.6844 0.5170 0.5890 2.60 sec NPY(2) Kyoto - - 0.6210 - - - - NPY(3) Kyoto - - 0.6660 - - - - Ent-MDL Kyoto 0.6912 0.7365 0.7131 0.5908 0.2720 0.3725 70.16 sec Table 7: Comparison of the proposed method (Ent-MDL) with the methods of Mochihashi et al., 2009 (NPY) and Goldwater et al., 2009 (HDP). performance in terms of both accuracy and speed. Possible improvements of the proposed method include modeling the dependencies among neighboring tokens, which would allow the evaluation of the context to be reflected in the cost function. Mechanisms for stochastic optimization implemented in the place of the greedy algorithms could provide an additional flexibility of search for such more complex models. As the proposed approach provides significant performance improvements, it could be utilized in the development of more sophisticated novel word induction schemes, e.g. ensemble model</context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2009</marker>
<rawString>Goldwater, Sharon, Thomas L. Griffiths and Mark Johnson. 2009. A Bayesian framework for word segmentation: Exploring the effects of context. Cognition, 112:1, 21–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig Harris</author>
</authors>
<title>From Phoneme to Morpheme.</title>
<date>1955</date>
<journal>Language,</journal>
<pages>31--2</pages>
<contexts>
<context position="5193" citStr="Harris, 1955" startWordPosition="770" endWordPosition="771">accuracy, in spite of a lower compression than the gold standard segmentation. As a solution to the aforementioned issue, the proposed method incorporates the local predictability of character sequences into the inference process. Numerous studies have shown that local distributional cues can serve well the purpose of inducing word boundaries. Behavioral science has confirmed that infants are sensitive to the transitional probabilities found in speech (Saffran et al., 1996). The increase in uncertainty following a given word prefix is a well studied criterion for morpheme boundary prediction (Harris, 1955). A good deal of research has been conducted on methods through which such local statistics can be applied to the word induction problem (e.g. Kempe, 1999; Huang and Powers, 2003; Jin and Tanaka-Ishii, 2006). Hutchens and Adler (1998) noticed that entropic chunking has the effect of reducing the perplexity of a text. Most methods for unsupervised word segmentation based solely on local statistics presume a certain – albeit minimum – level of acquaintance with the target language. For instance, the model of Huang and Powers (2003) involves some parameters (Markov chain order, numerous threshold</context>
</contexts>
<marker>Harris, 1955</marker>
<rawString>Harris, Zellig. 1955. From Phoneme to Morpheme. Language, 31(2):190-222.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin H Huang</author>
<author>David Powers</author>
</authors>
<title>Chinese Word Segmentation Based on Contextual Entropy.</title>
<date>2003</date>
<booktitle>Proceedings of 17th Pacific Asia Conference,</booktitle>
<pages>152--158</pages>
<contexts>
<context position="5371" citStr="Huang and Powers, 2003" startWordPosition="799" endWordPosition="802">ictability of character sequences into the inference process. Numerous studies have shown that local distributional cues can serve well the purpose of inducing word boundaries. Behavioral science has confirmed that infants are sensitive to the transitional probabilities found in speech (Saffran et al., 1996). The increase in uncertainty following a given word prefix is a well studied criterion for morpheme boundary prediction (Harris, 1955). A good deal of research has been conducted on methods through which such local statistics can be applied to the word induction problem (e.g. Kempe, 1999; Huang and Powers, 2003; Jin and Tanaka-Ishii, 2006). Hutchens and Adler (1998) noticed that entropic chunking has the effect of reducing the perplexity of a text. Most methods for unsupervised word segmentation based solely on local statistics presume a certain – albeit minimum – level of acquaintance with the target language. For instance, the model of Huang and Powers (2003) involves some parameters (Markov chain order, numerous threshold values) that allow its adaptation to the individuality of written Chinese. In comparison, the method proposed in this paper generalizes easily to a variety of languages and doma</context>
</contexts>
<marker>Huang, Powers, 2003</marker>
<rawString>Huang, Jin H. and David Powers. 2003. Chinese Word Segmentation Based on Contextual Entropy. Proceedings of 17th Pacific Asia Conference, 152–158</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason L Hutchens</author>
<author>Michael D Alder</author>
</authors>
<title>Finding structure via compression.</title>
<date>1998</date>
<booktitle>Proceedings of the International Conference on Computational Natural Language Learning,</booktitle>
<pages>79--82</pages>
<marker>Hutchens, Alder, 1998</marker>
<rawString>Hutchens, Jason L. and Michael D. Alder. 1998. Finding structure via compression. Proceedings of the International Conference on Computational Natural Language Learning, 79–82</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhihui Jin</author>
<author>Kumiko Tanaka-Ishii</author>
</authors>
<title>Unsupervised Segmentation of Chinese Text by Use of Branching Entropy.</title>
<date>2006</date>
<booktitle>Proceedings of the COLING/ACL on Main conference poster sessions,</booktitle>
<pages>428--435</pages>
<contexts>
<context position="5400" citStr="Jin and Tanaka-Ishii, 2006" startWordPosition="803" endWordPosition="806">sequences into the inference process. Numerous studies have shown that local distributional cues can serve well the purpose of inducing word boundaries. Behavioral science has confirmed that infants are sensitive to the transitional probabilities found in speech (Saffran et al., 1996). The increase in uncertainty following a given word prefix is a well studied criterion for morpheme boundary prediction (Harris, 1955). A good deal of research has been conducted on methods through which such local statistics can be applied to the word induction problem (e.g. Kempe, 1999; Huang and Powers, 2003; Jin and Tanaka-Ishii, 2006). Hutchens and Adler (1998) noticed that entropic chunking has the effect of reducing the perplexity of a text. Most methods for unsupervised word segmentation based solely on local statistics presume a certain – albeit minimum – level of acquaintance with the target language. For instance, the model of Huang and Powers (2003) involves some parameters (Markov chain order, numerous threshold values) that allow its adaptation to the individuality of written Chinese. In comparison, the method proposed in this paper generalizes easily to a variety of languages and domains, and is less dependent on</context>
<context position="11022" citStr="Jin and Tanaka-Ishii, 2006" startWordPosition="1770" endWordPosition="1773">s of both models. The L(θ|M) can be calculated as: L(θ|M) = |M |− 1 2 * 1o92 S, where |M |− 1 gives the number of parameters (degrees of freedom), and S is the size of the dataset where xk represents the character found at position k, and n is the order of the Markov model over characters. For brevity, hereafter we shall denote the observed sequence {xk−1, ..., xk−n} as {xk−1:k−n} . The above definition is extended to combine the entropy estimates in the left-to-right and right-toleft directions, as this factor has reportedly improved performance figures for models based on branching entropy (Jin and Tanaka-Ishii, 2006). The estimates in both directions are summed up, yielding a single value per position: 834 H&apos;(Xk;k−1|xk−1:k− n;xk:k+n−1) _ Algorithm 1 Generates an initial hypothesis. 11 � P(x|xk−1:k −n) log2 P(x|xk−1:k−n) xEX 11 � P(x|xk:k+n −1) log2 P(x|xk:k+n−1). xEX Suffix arrays are employed during the collection of frequency statistics. For a character model of order n over a testing corpus of size t and a training corpus of size m, suffix arrays allow these to be acquired in O(tn log m) time. Faster implementations reduce the complexity to O(t(n+log m)). For further discussion, see Manber and Myers (1</context>
<context position="23483" citStr="Jin and Tanaka-Ishii (2006)" startWordPosition="3852" endWordPosition="3855"> [1, 2, 3, 4, 5, 6] 0.8667 0.8898 0.8781 0.6808 0.6990 0.6898 344781.74 300490.52 1060.2 2a (H&apos;) CHILDES, n = 2 0.7636 0.9109 0.8308 0.5352 0.6384 0.5823 367711.66 753.1 2b (H&apos;&apos;) CHILDES, nmaa = 3 0.8692 0.8865 0.8777 0.6792 0.6927 0.6859 347633.07 885.3 1 Kyoto, a = 0, n = [1, 2, 3, 4, 5, 6] 0.8208 0.8208 0.8208 0.5784 0.5784 0.5784 1.325E+07 1.120E+07 54958.8 2a (H&apos;) Kyoto, n = 2 0.8100 0.8621 0.8353 0.5934 0.6316 0.6119 1.289E+07 22909.7 2b (H&apos;&apos;) Kyoto, nmaa = 2 0.8024 0.9177 0.8562 0.6093 0.6969 0.6501 1.248+E07 23212.8 Table 3: Comparison of the proposed method (2a, 2b) with the model of Jin and Tanaka-Ishii (2006) (1). Execution times include the obtaining of frequency statistics, and are represented by averages over 10 runs. harmonic averages of the corresponding precision and recall values (F = 2PR/(P + R)). As a rule, boundary-based evaluation produces the highest scores among the three evaluation modes, as it only considers the correspondence between the proposed and the gold standard boundaries at the individual positions of the corpora. Token-based evaluation is more strict – it accepts a word as correct only if its beginning and end are identified accurately, and no additional boundaries lie in </context>
<context position="24876" citStr="Jin and Tanaka-Ishii (2006)" startWordPosition="4079" endWordPosition="4082">rror analysis, which in combination with token scores can give a better idea of the relationship between the accuracy of induction and item frequency. The system was implemented in Java, however it handled the suffix arrays through an external C library called Sary.1 All experiments were conducted on a 2 GHz Core2Duo T7200 machine with 2 GB RAM. 5 Results and Discussion The scores we obtained using the described instantiations of the branching entropy criterion at the initialization phase are presented in Table 3, along with those generated by our own implementation of the method presented in Jin and Tanaka-Ishii (2006), where the threshold parameter α was adjusted manually for optimal performance. The heuristic of Jin and Tanaka-Ishii takes advantage of the trend that branching entropy decreases as the observed character sequences become longer; sudden rises can thus be regarded as an indication of locations where a boundary is likely to exist. Their method uses a common value for thresholding the 1http://sary.sourceforge.net entropy change throughout all n-gram orders, and combines the boundaries discovered in both directions in a separate step. These properties of the method would lead to complications if</context>
</contexts>
<marker>Jin, Tanaka-Ishii, 2006</marker>
<rawString>Jin, Zhihui and Kumiko Tanaka-Ishii. 2006. Unsupervised Segmentation of Chinese Text by Use of Branching Entropy. Proceedings of the COLING/ACL on Main conference poster sessions, 428–435</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Sharon Goldwater</author>
</authors>
<title>Improving nonparameteric Bayesian inference: experiments on unsupervised word segmentation with adaptor grammars.</title>
<date>2009</date>
<booktitle>Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Association for Computational Linguistics,</booktitle>
<pages>317--325</pages>
<contexts>
<context position="6448" citStr="Johnson and Goldwater (2009)" startWordPosition="967" endWordPosition="970">ptation to the individuality of written Chinese. In comparison, the method proposed in this paper generalizes easily to a variety of languages and domains, and is less dependent on annotated development data. The state-of-the-art in unsupervised word segmentation is represented by Bayesian models. Goldwater et al. (2006) justified the importance of context as a means of avoiding undersegmentation, through a method based on hierarchical Dirichlet processes. Mochihashi et al. (2009) proposed extensions to this method, which included a nested character model and an optimized inference procedure. Johnson and Goldwater (2009) have proposed a novel method based on adaptor grammars, whose accuracy surpasses the aforementioned methods by a large margin, when appropriate assumptions are made regarding the structural units of a language. 3 Proposed Method 3.1 Word segmentation with MDL The proposed two-part code incorporates some extensions of models presented in related work, aimed at achieving a more precise estimation of the representation length. We first introduce the general twopart code, which consists of: • the model, embodied by a codebook, i.e., a lexicon of unique word types M = {wi, ..., w|m|}, • the source</context>
</contexts>
<marker>Johnson, Goldwater, 2009</marker>
<rawString>Johnson, Mark and Sharon Goldwater. 2009. Improving nonparameteric Bayesian inference: experiments on unsupervised word segmentation with adaptor grammars. Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Association for Computational Linguistics, 317–325.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andre Kempe</author>
</authors>
<title>Experiments in Unsupervised Entropy Based Corpus Segmentation.</title>
<date>1999</date>
<booktitle>Proceedings of CoNLL’99,</booktitle>
<pages>371--385</pages>
<contexts>
<context position="5347" citStr="Kempe, 1999" startWordPosition="797" endWordPosition="798">he local predictability of character sequences into the inference process. Numerous studies have shown that local distributional cues can serve well the purpose of inducing word boundaries. Behavioral science has confirmed that infants are sensitive to the transitional probabilities found in speech (Saffran et al., 1996). The increase in uncertainty following a given word prefix is a well studied criterion for morpheme boundary prediction (Harris, 1955). A good deal of research has been conducted on methods through which such local statistics can be applied to the word induction problem (e.g. Kempe, 1999; Huang and Powers, 2003; Jin and Tanaka-Ishii, 2006). Hutchens and Adler (1998) noticed that entropic chunking has the effect of reducing the perplexity of a text. Most methods for unsupervised word segmentation based solely on local statistics presume a certain – albeit minimum – level of acquaintance with the target language. For instance, the model of Huang and Powers (2003) involves some parameters (Markov chain order, numerous threshold values) that allow its adaptation to the individuality of written Chinese. In comparison, the method proposed in this paper generalizes easily to a varie</context>
</contexts>
<marker>Kempe, 1999</marker>
<rawString>Kempe, Andre. 1999. Experiments in Unsupervised Entropy Based Corpus Segmentation. Proceedings of CoNLL’99, pp. 371–385</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chunyu Kit</author>
</authors>
<title>How does lexical acquisition begin? A cognitive perspective.</title>
<date>2003</date>
<journal>Cognitive Science</journal>
<volume>1</volume>
<issue>1</issue>
<pages>50</pages>
<contexts>
<context position="3741" citStr="Kit, 2003" startWordPosition="548" endWordPosition="549"> 832 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 832–842, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics method shows a high performance in terms of accuracy and speed, can be applied to language samples of substantial length, and generalizes well to corpora in different languages. 2 Related Work The principle of least effort (Zipf, 1949) postulates that the path of minimum resistance underlies all human behavior. Recent research has recognized its importance in the process of language acquisition (Kit, 2003). Compression-based word induction models comply to this principle, as they reorganize the data into a more compact representation while identifying the vocabulary of a text. The minimum description length framework (MDL) (Rissanen, 1978) is an appealing means of formalizing such models, as it provides a robust foundation for learning and inference, based solely on compression. The major problem in MDL-based word segmentation is the lack of standardized search algorithms for the exponential hypothesis space (Goldwater, 2006). The representative MDL models compare favorably to the current state</context>
</contexts>
<marker>Kit, 2003</marker>
<rawString>Kit, Chunyu. 2003. How does lexical acquisition begin? A cognitive perspective. Cognitive Science 1(1): 1– 50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sadao Kurohashi</author>
<author>Makoto Nagao</author>
</authors>
<title>Building a Japanese Parsed Corpus while Improving the Parsing System.</title>
<date>1998</date>
<booktitle>Proceedings of the First International Conference on Language Resources and Evaluation,</booktitle>
<pages>719--724</pages>
<location>Granada,</location>
<contexts>
<context position="20310" citStr="Kurohashi and Nagao, 1998" startWordPosition="3325" endWordPosition="3328">n the CHILDES corpus, are also taken into consideration. 4 Experimental Settings We evaluated the proposed model against four datasets. The first one is the Bernstein-Ratner corpus for language acquisition based on transcripts from the CHILDES database (Bernstein-Ratner, 1987). It comprises phonetically transcribed utterances of adult speech directed to 13 through 21- month-old children. We evaluated the performance of our learner in the cases when the few boundaries among the individual sentences are available to it (B), and when it starts from a blank state (N). The Kyoto University Corpus (Kurohashi and Nagao, 1998) is a standard dataset for Japanese morphological and dependency structure analysis, which comprises newspaper articles and editorials from the Mainichi Shimbun. The BEST corpus for word segmentation and named entity recognition in Thai language combines text from a variety of sources inCorpus Language Size Chars Tokens Types (MB) (K) (K) (K) CHILDES- English 0.1 95.8 33.3 1.3 B/N Kyoto Japanese 5.02 1674.9 972.9 39.5 WSJ English 5.22 5220.0 1174.2 49.1 BEST-E Thai 12.64 4360.2 1163.2 26.2 BEST-N Thai 18.37 6422.7 1659.4 36.3 BEST-A Thai 4.59 1619.9 438.7 13.9 BEST-F Thai 16.18 5568.0 1670.8 2</context>
</contexts>
<marker>Kurohashi, Nagao, 1998</marker>
<rawString>Kurohashi, Sadao and Makoto Nagao. 1998. Building a Japanese Parsed Corpus while Improving the Parsing System. Proceedings of the First International Conference on Language Resources and Evaluation, Granada, Spain, 719–724</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data.</title>
<date>2001</date>
<booktitle>Proceedings of the International Conference on Machine Learning.</booktitle>
<contexts>
<context position="1521" citStr="Lafferty et al., 2001" startWordPosition="212" endWordPosition="215">ES” corpus for research in language development reveals that the algorithm achieves an accuracy, comparable to that of the state-of-the-art methods in unsupervised word segmentation, in a significantly reduced computational time. 1 Introduction As an inherent preprocessing step to nearly all NLP tasks for writing systems without orthographical marking of word boundaries, such as Japanese and Chinese, the importance of word segmentation has lead to the emergence of a micro-genre in NLP focused exclusively on this problem. Supervised probabilistic models such as Conditional Random Fields (CRF) (Lafferty et al., 2001) have a wide application to the morphological analysis of these languages. However, the development of the annotated training corpora necessary for their functioning is a labor-intensive task, which involves multiple stages of manual tagging. Because of the scarcity of labeled data, the domain adaptation of morphological analyzers is also problematic, and semi-supervised algorithms that address this issue have also been proposed (e.g. Liang, 2005; Tsuboi et al., 2008). Recent advances in unsupervised word segmentation have been promoted by human cognition research, where it is involved in the </context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>Lafferty, John, Andrew McCallum and Fernando Pereira. 2001. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. Proceedings of the International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hang Li</author>
</authors>
<title>A Probabilistic Approach to Lexical Semantic Knowledge Acquisition and Structural Disambiguation.</title>
<date>1998</date>
<tech>Ph.D. Thesis</tech>
<institution>University of Tokyo,</institution>
<contexts>
<context position="8965" citStr="Li (1998)" startWordPosition="1406" endWordPosition="1407">s. A lexicon of characters M&apos; is defined. The description length of the lexicon data D&apos; given M&apos; is then calculated as: L(D0|M0) = − E |C |#ci 1o92 P(ci), i=1 where #ci denotes the frequency of a character ci in the lexicon of hypothesis M. The term L(M&apos;) is constant for any choice of hypothesis, as is represents the character set of a corpus. The total description length under the proposed model is thus calculated as: (the total length of the text in characters). The parametric complexity term is calculated in the same way for the lexicon. For a derivation of the above formula, refer to e.g. Li (1998). MDL is closely related to Bayesian inference. Depending on the choice of a universal code, the two approaches can overlap, as is the case with the twopart code discussed in this paper. It can be shown that the model selection in our method is equivalent to a MAP inference, conducted under the assumption that the prior probability of a model decreases exponentially with its length (Goldwater, 2006). Thus, the task that we are trying to accomplish is to conduct a focused search through the hypothesis space that will allow us to obtain an approximation of the MAP solution in a reasonable time. </context>
</contexts>
<marker>Li, 1998</marker>
<rawString>Li, Hang. 1998. A Probabilistic Approach to Lexical Semantic Knowledge Acquisition and Structural Disambiguation. University of Tokyo, Ph.D. Thesis</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
</authors>
<title>Semi-Supervised Learning for Natural Language. Massachusets Institute of Technology, Master’s Thesis.</title>
<date>2005</date>
<contexts>
<context position="1971" citStr="Liang, 2005" startWordPosition="279" endWordPosition="280">ence of a micro-genre in NLP focused exclusively on this problem. Supervised probabilistic models such as Conditional Random Fields (CRF) (Lafferty et al., 2001) have a wide application to the morphological analysis of these languages. However, the development of the annotated training corpora necessary for their functioning is a labor-intensive task, which involves multiple stages of manual tagging. Because of the scarcity of labeled data, the domain adaptation of morphological analyzers is also problematic, and semi-supervised algorithms that address this issue have also been proposed (e.g. Liang, 2005; Tsuboi et al., 2008). Recent advances in unsupervised word segmentation have been promoted by human cognition research, where it is involved in the modeling of the mechanisms that underlie language acquisition. Another motivation to study unsupervised approaches is their potential to support the domain adaptation of morphological analyzers through the incorporation of unannotated training data, thus reducing the dependency on costly manual work. Apart from the considerable difficulties in discovering reliable criteria for word induction, the practical application of such approaches is impede</context>
</contexts>
<marker>Liang, 2005</marker>
<rawString>Liang, Percy. 2005. Semi-Supervised Learning for Natural Language. Massachusets Institute of Technology, Master’s Thesis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Udi Manber</author>
<author>Gene Myers</author>
</authors>
<title>Suffix arrays: a new method for on-line string searches.</title>
<date>1991</date>
<journal>SIAM Journal on Computing</journal>
<pages>22--935</pages>
<contexts>
<context position="11626" citStr="Manber and Myers (1991)" startWordPosition="1869" endWordPosition="1872">Tanaka-Ishii, 2006). The estimates in both directions are summed up, yielding a single value per position: 834 H&apos;(Xk;k−1|xk−1:k− n;xk:k+n−1) _ Algorithm 1 Generates an initial hypothesis. 11 � P(x|xk−1:k −n) log2 P(x|xk−1:k−n) xEX 11 � P(x|xk:k+n −1) log2 P(x|xk:k+n−1). xEX Suffix arrays are employed during the collection of frequency statistics. For a character model of order n over a testing corpus of size t and a training corpus of size m, suffix arrays allow these to be acquired in O(tn log m) time. Faster implementations reduce the complexity to O(t(n+log m)). For further discussion, see Manber and Myers (1991). During the experiments, we did not use the caching functionality provided by the suffix array library, but instead kept the statistics for the current iterative pass (n-gram order and direction) in a local table. The chunking technique we adopt is to insert a boundary when the branching entropy measured in sequences of length n exceeds a certain threshold value (H(X|xk−1:k−n) &gt; 0). Both n and 0 are fixed. Within the described framework, the increase in context length n promotes precision and recall at first, but causes a performance degradation when the entropy estimates become unreliable du</context>
</contexts>
<marker>Manber, Myers, 1991</marker>
<rawString>Manber, Udi and Gene Myers. 1991. Suffix arrays: a new method for on-line string searches. SIAM Journal on Computing 22:935–948</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Grace Kim</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Robert MacIntyre</author>
<author>Ann Bies</author>
<author>Mark Ferguson</author>
<author>Karen Katz</author>
<author>Britta Schasberger</author>
</authors>
<title>The Penn Treebank: Annotating Predicate Argument Structure. Human Language Technology,</title>
<date>1994</date>
<pages>114--119</pages>
<contexts>
<context position="21421" citStr="Marcus et al., 1994" startWordPosition="3504" endWordPosition="3507">6.2 BEST-N Thai 18.37 6422.7 1659.4 36.3 BEST-A Thai 4.59 1619.9 438.7 13.9 BEST-F Thai 16.18 5568.0 1670.8 22.6 Wikipedia Japanese 425.0 169069.3 / / Asahi Japanese 337.2 112401.1 / / BEST-All Thai 51.2 17424.0 4371.8 73.4 Table 2: Corpora used during the evaluation. Precise token and type counts have been omitted for Wikipedia and Asahi, as no gold standard segmentations are available. cluding encyclopedias (E), newspaper articles (N), scientific articles (A), and novels (F). The WSJ subset of the Penn Treebank II Corpus incorporates selected stories from the Wall Street Journal, year 1989 (Marcus et al., 1994). Both the original text (O), and a version in which all characters were converted to lower case (L) were used. The datasets listed above were built by removing the tags and blank spaces found in the corpora, and concatenating the remaining text. We added two more training datasets for Japanese, which were used in a separate experiment solely for the acquisition of frequency statistics. One of them was created from 200,000 randomly chosen Wikipedia articles, stripped from structural elements. The other one contains text from the year 2005 issues of Asahi Newspaper. Statistics regarding all des</context>
</contexts>
<marker>Marcus, Kim, Marcinkiewicz, MacIntyre, Bies, Ferguson, Katz, Schasberger, 1994</marker>
<rawString>Marcus, Mitchell, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz and Britta Schasberger. 1994. The Penn Treebank: Annotating Predicate Argument Structure. Human Language Technology, 114–119</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daiichi Mochihashi</author>
<author>Takeshi Yamada</author>
<author>Naonori Ueda</author>
</authors>
<title>Bayesian unsupervised word segmentation with nested Pitman-Yor language modeling.</title>
<date>2009</date>
<booktitle>Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing,</booktitle>
<volume>1</volume>
<pages>100--108</pages>
<contexts>
<context position="6305" citStr="Mochihashi et al. (2009)" startWordPosition="945" endWordPosition="948"> instance, the model of Huang and Powers (2003) involves some parameters (Markov chain order, numerous threshold values) that allow its adaptation to the individuality of written Chinese. In comparison, the method proposed in this paper generalizes easily to a variety of languages and domains, and is less dependent on annotated development data. The state-of-the-art in unsupervised word segmentation is represented by Bayesian models. Goldwater et al. (2006) justified the importance of context as a means of avoiding undersegmentation, through a method based on hierarchical Dirichlet processes. Mochihashi et al. (2009) proposed extensions to this method, which included a nested character model and an optimized inference procedure. Johnson and Goldwater (2009) have proposed a novel method based on adaptor grammars, whose accuracy surpasses the aforementioned methods by a large margin, when appropriate assumptions are made regarding the structural units of a language. 3 Proposed Method 3.1 Word segmentation with MDL The proposed two-part code incorporates some extensions of models presented in related work, aimed at achieving a more precise estimation of the representation length. We first introduce the gener</context>
<context position="32633" citStr="Mochihashi et al. (2009)" startWordPosition="5278" endWordPosition="5281">. Performance deteriorates significantly when out-of-domain training data is used. In spite of its size, the assorted composite corpus, in which in-domain and out-of-domain training data are mixed, produces worse results than the corpora which include only domain-specific data (Figure 1d). Finally, a comparison of the proposed method with Bayesian n-gram models is presented in Table 7. Through the increase of compression in the refinement phase of the algorithm, accuracy is improved by around 3%, and the scores approach those of the explicit probabilistic models of Goldwater et al. (2009) and Mochihashi et al. (2009). The proposed learner surpasses the other unsupervised word induction models in terms of processing speed. It should be noticed that a direct comparison of accuracy is not possible with Mochihashi et al. (2009), as they evaluated their system with separate datasets for training and testing. Furthermore, different segmentation standards exist for Japanese, and therefore the ”ground truth” provided by the Kyoto corpus cannot be considered an ideal measure of accuracy. 6 Conclusions and Future Work This paper has presented an efficient algorithm for unsupervised word induction, which relies on a</context>
<context position="34267" citStr="Mochihashi et al., 2009" startWordPosition="5543" endWordPosition="5546"> through the hypothesis space, allowing the proposed method to demonstrate a very high 840 Model Corpus T-Prec T-Rec T-F L-Prec L-Rec L-F Time NPY(3) CHILDES 0.7480 0.7520 0.7500 0.4780 0.5970 0.5310 17 min NPY(2) CHILDES 0.7480 0.7670 0.7570 0.5730 0.5660 0.5700 17 min HDP(2) CHILDES 0.7520 0.6960 0.7230 0.6350 0.5520 0.5910 - Ent-MDL CHILDES 0.7634 0.7453 0.7542 0.6844 0.5170 0.5890 2.60 sec NPY(2) Kyoto - - 0.6210 - - - - NPY(3) Kyoto - - 0.6660 - - - - Ent-MDL Kyoto 0.6912 0.7365 0.7131 0.5908 0.2720 0.3725 70.16 sec Table 7: Comparison of the proposed method (Ent-MDL) with the methods of Mochihashi et al., 2009 (NPY) and Goldwater et al., 2009 (HDP). performance in terms of both accuracy and speed. Possible improvements of the proposed method include modeling the dependencies among neighboring tokens, which would allow the evaluation of the context to be reflected in the cost function. Mechanisms for stochastic optimization implemented in the place of the greedy algorithms could provide an additional flexibility of search for such more complex models. As the proposed approach provides significant performance improvements, it could be utilized in the development of more sophisticated novel word induc</context>
</contexts>
<marker>Mochihashi, Yamada, Ueda, 2009</marker>
<rawString>Mochihashi, Daiichi, Takeshi Yamada and Naonori Ueda. 2009. Bayesian unsupervised word segmentation with nested Pitman-Yor language modeling. Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, 1: 100–108</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorma Rissanen</author>
</authors>
<title>Modeling by Shortest Data Description.</title>
<date>1978</date>
<journal>Aulomatica,</journal>
<pages>14--465</pages>
<contexts>
<context position="3979" citStr="Rissanen, 1978" startWordPosition="584" endWordPosition="586">in terms of accuracy and speed, can be applied to language samples of substantial length, and generalizes well to corpora in different languages. 2 Related Work The principle of least effort (Zipf, 1949) postulates that the path of minimum resistance underlies all human behavior. Recent research has recognized its importance in the process of language acquisition (Kit, 2003). Compression-based word induction models comply to this principle, as they reorganize the data into a more compact representation while identifying the vocabulary of a text. The minimum description length framework (MDL) (Rissanen, 1978) is an appealing means of formalizing such models, as it provides a robust foundation for learning and inference, based solely on compression. The major problem in MDL-based word segmentation is the lack of standardized search algorithms for the exponential hypothesis space (Goldwater, 2006). The representative MDL models compare favorably to the current state-of-the-art models in terms of accuracy. Brent and Cartwright (1996) carried out an exhaustive search through the possible segmentations of a limited subset of the data. Yu (2000) proposed an EM optimization routine, which achieved a high</context>
</contexts>
<marker>Rissanen, 1978</marker>
<rawString>Rissanen, Jorma. 1978. Modeling by Shortest Data Description. Aulomatica, 14:465–471.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny R Saffran</author>
<author>Richard N Aslin</author>
<author>Elissa L Newport</author>
</authors>
<title>Statistical learning in 8-month-old infants Science;</title>
<date>1996</date>
<pages>274--1926</pages>
<contexts>
<context position="5058" citStr="Saffran et al., 1996" startWordPosition="747" endWordPosition="750">earch through the possible segmentations of a limited subset of the data. Yu (2000) proposed an EM optimization routine, which achieved a high accuracy, in spite of a lower compression than the gold standard segmentation. As a solution to the aforementioned issue, the proposed method incorporates the local predictability of character sequences into the inference process. Numerous studies have shown that local distributional cues can serve well the purpose of inducing word boundaries. Behavioral science has confirmed that infants are sensitive to the transitional probabilities found in speech (Saffran et al., 1996). The increase in uncertainty following a given word prefix is a well studied criterion for morpheme boundary prediction (Harris, 1955). A good deal of research has been conducted on methods through which such local statistics can be applied to the word induction problem (e.g. Kempe, 1999; Huang and Powers, 2003; Jin and Tanaka-Ishii, 2006). Hutchens and Adler (1998) noticed that entropic chunking has the effect of reducing the perplexity of a text. Most methods for unsupervised word segmentation based solely on local statistics presume a certain – albeit minimum – level of acquaintance with t</context>
</contexts>
<marker>Saffran, Aslin, Newport, 1996</marker>
<rawString>Saffran, Jenny R., Richard N. Aslin and Elissa L. Newport. 1996. Statistical learning in 8-month-old infants Science; 274:1926-1928</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuta Tsuboi</author>
<author>Hisashi Kashima</author>
<author>Hiroki Oda</author>
<author>Shinsuke Mori</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Training Conditional Random Fields Using Incomplete Annotations.</title>
<date>2008</date>
<booktitle>Proceedings of the 22nd International Conference on Computational Linguistics - Volume</booktitle>
<pages>1--897</pages>
<contexts>
<context position="1993" citStr="Tsuboi et al., 2008" startWordPosition="281" endWordPosition="284">ro-genre in NLP focused exclusively on this problem. Supervised probabilistic models such as Conditional Random Fields (CRF) (Lafferty et al., 2001) have a wide application to the morphological analysis of these languages. However, the development of the annotated training corpora necessary for their functioning is a labor-intensive task, which involves multiple stages of manual tagging. Because of the scarcity of labeled data, the domain adaptation of morphological analyzers is also problematic, and semi-supervised algorithms that address this issue have also been proposed (e.g. Liang, 2005; Tsuboi et al., 2008). Recent advances in unsupervised word segmentation have been promoted by human cognition research, where it is involved in the modeling of the mechanisms that underlie language acquisition. Another motivation to study unsupervised approaches is their potential to support the domain adaptation of morphological analyzers through the incorporation of unannotated training data, thus reducing the dependency on costly manual work. Apart from the considerable difficulties in discovering reliable criteria for word induction, the practical application of such approaches is impeded by their prohibitive</context>
</contexts>
<marker>Tsuboi, Kashima, Oda, Mori, Matsumoto, 2008</marker>
<rawString>Tsuboi, Yuta, Hisashi Kashima., Hiroki Oda, Shinsuke Mori and Yuji Matsumoto. 2008. Training Conditional Random Fields Using Incomplete Annotations. Proceedings of the 22nd International Conference on Computational Linguistics - Volume 1,897–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hua Yu</author>
</authors>
<title>Unsupervised word induction using MDL criterion.</title>
<date>2000</date>
<booktitle>Proceedings of tne International Symposium of Chinese Spoken Language Processing,</booktitle>
<location>Beijing.</location>
<contexts>
<context position="4520" citStr="Yu (2000)" startWordPosition="668" endWordPosition="669">ext. The minimum description length framework (MDL) (Rissanen, 1978) is an appealing means of formalizing such models, as it provides a robust foundation for learning and inference, based solely on compression. The major problem in MDL-based word segmentation is the lack of standardized search algorithms for the exponential hypothesis space (Goldwater, 2006). The representative MDL models compare favorably to the current state-of-the-art models in terms of accuracy. Brent and Cartwright (1996) carried out an exhaustive search through the possible segmentations of a limited subset of the data. Yu (2000) proposed an EM optimization routine, which achieved a high accuracy, in spite of a lower compression than the gold standard segmentation. As a solution to the aforementioned issue, the proposed method incorporates the local predictability of character sequences into the inference process. Numerous studies have shown that local distributional cues can serve well the purpose of inducing word boundaries. Behavioral science has confirmed that infants are sensitive to the transitional probabilities found in speech (Saffran et al., 1996). The increase in uncertainty following a given word prefix is</context>
<context position="8058" citStr="Yu (2000)" startWordPosition="1243" endWordPosition="1244">3 |M| L(D|M) = − E #wj 1o92 P(wj), j=1 where #wj stands for the frequency of the word wj in the text. Different strategies have been proposed in the literature for the calculation of the codebook cost. A common technique in segmentation and morphology induction models is to calculate the product of the total length in characters of the lexicon and an estimate of the per-character entropy. In this way, both the probabilities and lengths of words are taken into consideration. The use of a constant value is an effective and easily computable approach, but it is far from precise. For instance, in Yu (2000) the average entropy per character is measured against the original corpus, but this model does not capture the effects of the word distributions on the observed character probabilities. For this reason, we propose a different method: the codebook is modeled as a separate Markov chain of characters. A lexicon of characters M&apos; is defined. The description length of the lexicon data D&apos; given M&apos; is then calculated as: L(D0|M0) = − E |C |#ci 1o92 P(ci), i=1 where #ci denotes the frequency of a character ci in the lexicon of hypothesis M. The term L(M&apos;) is constant for any choice of hypothesis, as i</context>
<context position="27156" citStr="Yu (2000)" startWordPosition="4444" endWordPosition="4445">s obtained at the completion of Algorithm 3 are summarized in Tables 4 and 5. Presented durations include the obtaining of frequency statistics. The nmax parameter is set to the value which maximizes the compression during the initial phase, in order to make the results representative of the case in which no annotated development corpora are accessible to the algorithm. It is evident that after the optimization carried out in the second phase, the description length is reduced to levels significantly lower than the ground truth. In this aspect, the algorithm outperforms the EM-based method of Yu (2000). 838 Corpus &amp; Settings B-F T-F L-F Time (ms) CHILDES-B, nmaa=3 0.9092 0.7542 0.5890 2597.2 CHILDES-N, nmaa=3 0.9070 0.7499 0.5578 2949.3 Kyoto, nmaa=2 0.8855 0.7131 0.3725 70164.6 BEST-E, nmaa=5 0.9081 0.7793 0.3549 738055.0 BEST-N, nmaa=5 0.8811 0.7339 0.2807 505327.0 BEST-A, nmaa=5 0.9045 0.7632 0.4246 250863.0 BEST-F, nmaa=5 0.9343 0.8216 0.4820 305522.0 WSJ-O, nmaa=6 0.8405 0.6059 0.3338 658214.0 WSJ-L, nmaa=6 0.8515 0.6373 0.3233 582382.0 Table 4: Results obtained after the termination of Algorithm 3. Corpus &amp; Settings Description Description Length (Proposed) Length (Total) CHILDES-B, n</context>
</contexts>
<marker>Yu, 2000</marker>
<rawString>Yu, Hua. 2000. Unsupervised word induction using MDL criterion. Proceedings of tne International Symposium of Chinese Spoken Language Processing, Beijing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George K Zipf</author>
</authors>
<title>Human Behavior and the Principle of Least Effort.</title>
<date>1949</date>
<publisher>Addison-Wesley.</publisher>
<contexts>
<context position="3567" citStr="Zipf, 1949" startWordPosition="522" endWordPosition="523">y a key role in native language acquisition. The proposed model allows experimentation in a more realistic setting, where the learner is able to apply them simultaneously. The 832 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 832–842, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics method shows a high performance in terms of accuracy and speed, can be applied to language samples of substantial length, and generalizes well to corpora in different languages. 2 Related Work The principle of least effort (Zipf, 1949) postulates that the path of minimum resistance underlies all human behavior. Recent research has recognized its importance in the process of language acquisition (Kit, 2003). Compression-based word induction models comply to this principle, as they reorganize the data into a more compact representation while identifying the vocabulary of a text. The minimum description length framework (MDL) (Rissanen, 1978) is an appealing means of formalizing such models, as it provides a robust foundation for learning and inference, based solely on compression. The major problem in MDL-based word segmentat</context>
</contexts>
<marker>Zipf, 1949</marker>
<rawString>Zipf, George K. 1949. Human Behavior and the Principle of Least Effort. Addison-Wesley.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>