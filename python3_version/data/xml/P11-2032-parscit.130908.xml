<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000404">
<title confidence="0.982502">
Bayesian Word Alignment for Statistical Machine Translation
</title>
<author confidence="0.87744">
Cos¸kun Mermeri,&apos;
</author>
<note confidence="0.468413333333333">
iBILGEM
TUBITAK
Gebze 41470 Kocaeli, Turkey
</note>
<email confidence="0.993631">
coskun@uekae.tubitak.gov.tr
</email>
<author confidence="0.94913">
Murat Sarac¸lar&apos;
</author>
<affiliation confidence="0.812109666666667">
&apos;Electrical and Electronics Eng. Dept.
Bogazici University
Bebek 34342 Istanbul, Turkey
</affiliation>
<email confidence="0.99791">
murat.saraclar@boun.edu.tr
</email>
<sectionHeader confidence="0.99563" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9996612">
In this work, we compare the translation
performance of word alignments obtained
via Bayesian inference to those obtained via
expectation-maximization (EM). We propose
a Gibbs sampler for fully Bayesian inference
in IBM Model 1, integrating over all possi-
ble parameter values in finding the alignment
distribution. We show that Bayesian inference
outperforms EM in all of the tested language
pairs, domains and data set sizes, by up to 2.99
BLEU points. We also show that the proposed
method effectively addresses the well-known
rare word problem in EM-estimated models;
and at the same time induces a much smaller
dictionary of bilingual word-pairs.
</bodyText>
<sectionHeader confidence="0.998989" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99995456">
Word alignment is a crucial early step in the training
of most statistical machine translation (SMT) sys-
tems, in which the estimated alignments are used for
constraining the set of candidates in phrase/grammar
extraction (Koehn et al., 2003; Chiang, 2007; Galley
et al., 2006). State-of-the-art word alignment mod-
els, such as IBM Models (Brown et al., 1993), HMM
(Vogel et al., 1996), and the jointly-trained symmet-
ric HMM (Liang et al., 2006), contain a large num-
ber of parameters (e.g., word translation probabili-
ties) that need to be estimated in addition to the de-
sired hidden alignment variables.
The most common method of inference in such
models is expectation-maximization (EM) (Demp-
ster et al., 1977) or an approximation to EM when
exact EM is intractable. However, being a maxi-
mization (e.g., maximum likelihood (ML) or max-
imum a posteriori (MAP)) technique, EM is gen-
erally prone to local optima and overfitting. In
essence, the alignment distribution obtained via EM
takes into account only the most likely point in the
parameter space, but does not consider contributions
from other points.
Problems with the standard EM estimation of
IBM Model 1 was pointed out by Moore (2004) and
a number of heuristic changes to the estimation pro-
cedure, such as smoothing the parameter estimates,
were shown to reduce the alignment error rate, but
the effects on translation performance was not re-
ported. Zhao and Xing (2006) note that the param-
eter estimation (for which they use variational EM)
suffers from data sparsity and use symmetric Dirich-
let priors, but they find the MAP solution.
Bayesian inference, the approach in this paper,
have recently been applied to several unsupervised
learning problems in NLP (Goldwater and Griffiths,
2007; Johnson et al., 2007) as well as to other tasks
in SMT such as synchronous grammar induction
(Blunsom et al., 2009) and learning phrase align-
ments directly (DeNero et al., 2008).
Word alignment learning problem was addressed
jointly with segmentation learning in Xu et al.
(2008), Nguyen et al. (2010), and Chung and Gildea
(2009). The former two works place nonparametric
priors (also known as cache models) on the param-
eters and utilize Gibbs sampling. However, align-
ment inference in neither of these works is exactly
Bayesian since the alignments are updated by run-
ning GIZA++ (Xu et al., 2008) or by local maxi-
mization (Nguyen et al., 2010). On the other hand,
</bodyText>
<page confidence="0.977181">
182
</page>
<note confidence="0.5847845">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 182–187,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.988838048780488">
Chung and Gildea (2009) apply a sparse Dirichlet
prior on the multinomial parameters to prevent over-
fitting. They use variational Bayes for inference, but
they do not investigate the effect of Bayesian infer-
ence to word alignment in isolation. Recently, Zhao
and Gildea (2010) proposed fertility extensions to
IBM Model 1 and HMM, but they do not place any
prior on the parameters and their inference method is
actually stochastic EM (also known as Monte Carlo
EM), a ML technique in which sampling is used to
approximate the expected counts in the E-step. Even
though they report substantial reductions in align-
ment error rate, the translation BLEU scores do not
improve.
Our approach in this paper is fully Bayesian in
which the alignment probabilities are inferred by
integrating over all possible parameter values as-
suming an intuitive, sparse prior. We develop a
Gibbs sampler for alignments under IBM Model 1,
which is relevant for the state-of-the-art SMT sys-
tems since: (1) Model 1 is used in bootstrapping
the parameter settings for EM training of higher-
order alignment models, and (2) many state-of-the-
art SMT systems use Model 1 translation probabil-
ities as features in their log-linear model. We eval-
uate the inferred alignments in terms of the end-to-
end translation performance, where we show the re-
sults with a variety of input data to illustrate the gen-
eral applicability of the proposed technique. To our
knowledge, this is the first work to directly investi-
gate the effects of Bayesian alignment inference on
translation performance.
2 Bayesian Inference with IBM Model 1
Given a sentence-aligned parallel corpus (E, F), let
ei (fj) denote the i-th (j-th) source (target)1 word
in e (f), which in turn consists of I (J) words and
denotes the s-th sentence in E (F).2 Each source
sentence is also hypothesized to have an additional
imaginary “null” word e0. Also let VE (VF) denote
the size of the observed source (target) vocabulary.
In Model 1 (Brown et al., 1993), each target word
</bodyText>
<footnote confidence="0.8278285">
1We use the “source” and “target” labels following the gen-
erative process, in which E generates F (cf. Eq. 1).
2Dependence of the sentence-level variables e, f, I, J (and
a and n, which are introduced later) on the sentence index s
should be understood even though not explicitly indicated for
notational simplicity.
</footnote>
<bodyText confidence="0.999251444444444">
fj is associated with a hidden alignment variable aj
whose value ranges over the word positions in the
corresponding source sentence. The set of align-
ments for a sentence (corpus) is denoted by a (A).
The model parameters consist of a VE x VF ta-
ble T of word translation probabilities such that
te,f = P(f|e).
The joint distribution of the Model-1 variables is
given by the following generative model3:
</bodyText>
<equation confidence="0.9965885">
P(E, F, A; T) = Y P(e)P(a|e)P(f|a, e; T) (1)
s
Y= P(e) J teaj,fj (2)
s Y
j=1
(I + 1)J
</equation>
<bodyText confidence="0.999351666666667">
In the proposed Bayesian setting, we treat T as a
random variable with a prior P(T). To find a suit-
able prior for T, we re-write (2) as:
</bodyText>
<equation confidence="0.9930968">
Y
P(E, F, A|T) =
s
P (e)
(I + 1)J (4)
</equation>
<bodyText confidence="0.999685857142857">
where in (3) the count variable ne,f denotes the
number of times the source word type e is aligned
to the target word type f in the sentence-pair s, and
in (4) Ne f = Ps ne, f. Since the distribution over
N,f I in (4) is in the exponential family, specifically
being a multinomial distribution, we choose the con-
jugate prior, in this case the Dirichlet distribution,
for computational convenience.
For each source word type e, we assume the prior
distribution for te = te,1 · · · te,VF, which is itself
a distribution over the target vocabulary, to be a
Dirichlet distribution (with its own set of hyperpa-
rameters Oe = θe,1 · · · θe,VF ) independent from the
priors of other source word types:
</bodyText>
<equation confidence="0.947445">
te — Dirichlet(te; Oe)
fj|a, e, T — Multinomial(fj; teaj )
</equation>
<bodyText confidence="0.999658666666667">
We choose symmetric Dirichlet priors identically
for all source words e with θe,f = θ = 0.0001 to
obtain a sparse Dirichlet prior. A sparse prior favors
</bodyText>
<footnote confidence="0.83387">
3We omit P(Jle) since both J and e are observed and so
this term does not affect the inference of hidden variables.
</footnote>
<equation confidence="0.988344153846154">
P(e) VE VF (te,f)ne,f (3)
Y Y
e=1 f=1
(I + 1)J
(te,f)Ne,f Y
s
=
VE
Y
e=1
VF
Y
f=1
</equation>
<page confidence="0.989292">
183
</page>
<bodyText confidence="0.9808525">
distributions that peak at a single target word and
penalizes flatter translation distributions, even for
rare words. This choice addresses the well-known
problem in the IBM Models, and more severely in
Model 1, in which rare words act as “garbage col-
lectors” (Och and Ney, 2003) and get assigned ex-
cessively large number of word alignments.
Then we obtain the joint distribution of all (ob-
served + hidden) variables as:
P(E, F, A, T; O) = P(T; O) P(E, F, A|T) (5)
where O = O1 ···OVE.
To infer the posterior distribution of the align-
ments, we use Gibbs sampling (Geman and Ge-
man, 1984). One possible method is to derive the
Gibbs sampler from P(E, F, A, T; O) obtained in
(5) and sample the unknowns A and T in turn, re-
sulting in an explicit Gibbs sampler. In this work,
we marginalize out T by:
</bodyText>
<equation confidence="0.485945">
P(E, F, A; O) = IT P(E, F, A, T; O) (6)
</equation>
<bodyText confidence="0.996849">
and obtain a collapsed Gibbs sampler, which sam-
ples only the alignment variables.
Using P(E, F, A; O) obtained in (6), the Gibbs
sampling formula for the individual alignments is
derived as:4
</bodyText>
<equation confidence="0.9922595">
PfF +E
1Nz&apos;,ffF1 eei,f (7)
</equation>
<bodyText confidence="0.999940538461538">
where the superscript -,j denotes the exclusion of
the current value of aj.
The algorithm is given in Table 1. Initialization
of A in Step 1 can be arbitrary, but for faster conver-
gence special initializations have been used, e.g., us-
ing the output of EM (Chiang et al., 2010). Once the
Gibbs sampler is deemed to have converged after B
burn-in iterations, we collect M samples of A with
L iterations in-between5 to estimate P(A|E, F). To
obtain the Viterbi alignments, which are required for
phrase extraction (Koehn et al., 2003), we select for
each aj the most frequent value in the M collected
samples.
</bodyText>
<footnote confidence="0.99968">
4The derivation is quite standard and similar to other
Dirichlet-multinomial Gibbs sampler derivations, e.g. (Resnik
and Hardisty, 2010).
5A lag is introduced to reduce correlation between samples.
</footnote>
<note confidence="0.375185">
Input: E, F; Output: K samples of A
</note>
<listItem confidence="0.6273735">
1 Initialize A
2 for k = 1 to K do
3 for each sentence-pair s in (E, F) do
4 for j = 1 to J do
5 for i = 0 to I do
6 Calculate P(aj = i |· · · )
according to (7)
7 Sample a new value for aj
</listItem>
<tableCaption confidence="0.9893195">
Table 1: Gibbs sampling algorithm for IBM Model 1 (im-
plemented in the accompanying software).
</tableCaption>
<sectionHeader confidence="0.998169" genericHeader="method">
3 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999950793103449">
For TurkishHEnglish experiments, we used the
20K-sentence travel domain BTEC dataset (Kikui
et al., 2006) from the yearly IWSLT evaluations6
for training, the CSTAR 2003 test set for develop-
ment, and the IWSLT 2004 test set for testing7. For
CzechHEnglish, we used the 95K-sentence news
commentary parallel corpus from the WMT shared
task8 for training, news2008 set for development,
news2009 set for testing, and the 438M-word En-
glish and 81.7M-word Czech monolingual news cor-
pora for additional language model (LM) training.
For ArabicHEnglish, we used the 65K-sentence
LDC2004T18 (news from 2001-2004) for training,
the AFP portion of LDC2004T17 (news from 1998,
single reference) for development and testing (about
875 sentences each), and the 298M-word English
and 215M-word Arabic AFP and Xinhua subsets of
the respective Gigaword corpora (LDC2007T07 and
LDC2007T40) for additional LM training. All lan-
guage models are 4-gram in the travel domain exper-
iments and 5-gram in the news domain experiments.
For each language pair, we trained standard
phrase-based SMT systems in both directions (in-
cluding alignment symmetrization and log-linear
model tuning) using Moses (Koehn et al., 2007),
SRILM (Stolcke, 2002), and ZMERT (Zaidan,
2009) tools and evaluated using BLEU (Papineni et
al., 2002). To obtain word alignments, we used the
accompanying Perl code for Bayesian inference and
</bodyText>
<footnote confidence="0.9988594">
6International Workshop on Spoken Language Translation.
http://iwslt2010.fbk.eu
7Using only the first English reference for symmetry.
8Workshop on Machine Translation.
http://www.statmt.org/wmt10/translation-task.html
</footnote>
<equation confidence="0.934401333333333">
P(aj = i|E, F, A—j; O)
1V i, fj + 0e.f
—a, j
</equation>
<page confidence="0.990811">
184
</page>
<table confidence="0.992464857142857">
Method TE ET CE EC AE EA
EM-5 38.91 26.52 14.62 10.07 15.50 15.17
EM-80 39.19 26.47 14.95 10.69 15.66 15.02
GS-N 41.14 27.55 14.99 10.85 14.64 15.89
GS-5 40.63 27.24 15.45 10.57 16.41 15.82
GS-80 41.78 29.51 15.01 10.68 15.92 16.02
M4 39.94 27.47 15.47 11.15 16.46 15.43
</table>
<tableCaption confidence="0.9981815">
Table 2: BLEU scores in translation experiments. E: En-
glish, T: Turkish, C: Czech, A: Arabic.
</tableCaption>
<bodyText confidence="0.993877666666667">
GIZA++ (Och and Ney, 2003) for EM.
For each translation task, we report two EM es-
timates, obtained after 5 and 80 iterations (EM-5
and EM-80), respectively; and three Gibbs sampling
estimates, two of which were initialized with those
two EM Viterbi alignments (GS-5 and GS-80) and a
third was initialized naively9 (GS-N). Sampling set-
tings were B = 400 for THE, 4000 for CHE and
8000 for AHE; M = 100, and L = 10. For refer-
ence, we also report the results with IBM Model 4
alignments (M4) trained in the standard bootstrap-
ping regimen of 15H53343.
</bodyText>
<sectionHeader confidence="0.999944" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999384833333333">
Table 2 compares the BLEU scores of Bayesian in-
ference and EM estimation. In all translation tasks,
Bayesian inference outperforms EM. The improve-
ment range is from 2.59 (in Turkish-to-English)
up to 2.99 (in English-to-Turkish) BLEU points in
travel domain and from 0.16 (in English-to-Czech)
up to 0.85 (in English-to-Arabic) BLEU points in
news domain. Compared to the state-of-the-art IBM
Model 4, the Bayesian Model 1 is better in all travel
domain tasks and is comparable or better in the news
domain.
Fertility of a source word is defined as the num-
ber of target words aligned to it. Table 3 shows the
distribution of fertilities in alignments obtained from
different methods. Compared to EM estimation, in-
cluding Model 4, the proposed Bayesian inference
dramatically reduces “questionable” high-fertility (4
:5 fertility :5 7) alignments and almost entirely elim-
</bodyText>
<footnote confidence="0.981379">
9Each target word was aligned to the source candidate that
co-occured the most number of times with that target word in
the entire parallel corpus.
</footnote>
<table confidence="0.994567727272727">
Method TE ET CE EC AE EA
All 140K 183K 1.63M 1.78M 1.49M 1.82M
EM-80 5.07K 2.91K 52.9K 45.0K 69.1K 29.4K
M4 5.35K 3.10K 36.8K 36.6K 55.6K 36.5K
GS-80 755 419 14.0K 10.9K 47.6K 18.7K
EM-80 426 227 10.5K 18.6K 21.4K 24.2K
M4 81 163 2.57K 10.6K 9.85K 21.8K
GS-80 1 1 39 110 689 525
EM-80 24 24 28 30 44 46
M4 9 9 9 9 9 9
GS-80 8 8 13 18 20 19
</table>
<tableCaption confidence="0.99117">
Table 3: Distribution of inferred alignment fertilities. The
four blocks of rows from top to bottom correspond to (in
order) the total number of source tokens, source tokens
with fertilities in the range 4–7, source tokens with fertil-
ities higher than 7, and the maximum observed fertility.
The first language listed is the source in alignment (Sec-
tion 2).
</tableCaption>
<table confidence="0.99917425">
Method TE ET CE EC AE EA
EM-80 52.5K 38.5K 440K 461K 383K 388K
M4 57.6K 40.5K 439K 441K 422K 405K
GS-80 23.5K 25.4K 180K 209K 158K 176K
</table>
<tableCaption confidence="0.9881375">
Table 4: Sizes of bilingual dictionaries induced by differ-
ent alignment methods.
</tableCaption>
<bodyText confidence="0.9896585">
inates “excessive” alignments (fertility &gt; 8)10.
The number of distinct word-pairs induced by an
alignment has been recently proposed as an objec-
tive function for word alignment (Bodrumlu et al.,
2009). Small dictionary sizes are preferred over
large ones. Table 4 shows that the proposed in-
ference method substantially reduces the alignment
dictionary size, in most cases by more than 50%.
</bodyText>
<sectionHeader confidence="0.99386" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999938714285714">
We developed a Gibbs sampling-based Bayesian in-
ference method for IBM Model 1 word alignments
and showed that it outperforms EM estimation in
terms of translation BLEU scores across several lan-
guage pairs, data sizes and domains. As a result
of this increase, Bayesian Model 1 alignments per-
form close to or better than the state-of-the-art IBM
</bodyText>
<footnote confidence="0.9675305">
10The GIZA++ implementation of Model 4 artificially limits
fertility parameter values to at most nine.
</footnote>
<page confidence="0.998007">
185
</page>
<bodyText confidence="0.9998165">
Model 4. The proposed method learns a compact,
sparse translation distribution, overcoming the well-
known “garbage collection” problem of rare words
in EM-estimated current models.
</bodyText>
<sectionHeader confidence="0.975009" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.8233065">
Murat Sarac¸lar is supported by the T ¨UBA-GEB˙IP
award.
</bodyText>
<sectionHeader confidence="0.985553" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998399936170213">
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A Gibbs sampler for phrasal syn-
chronous grammar induction. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
782–790, Suntec, Singapore, August.
Tugba Bodrumlu, Kevin Knight, and Sujith Ravi. 2009.
A new objective function for word alignment. In Pro-
ceedings of the NAACL HLT Workshop on Integer Lin-
ear Programming for Natural Language Processing,
pages 28–35, Boulder, Colorado, June. Association for
Computational Linguistics.
Peter F. Brown, Vincent J. Della Pietra, Stephen A.
Della Pietra, and Robert L. Mercer. 1993. The mathe-
matics of statistical machine translation: parameter es-
timation. Computational Linguistics, 19(2):263–311.
David Chiang, Jonathan Graehl, Kevin Knight, Adam
Pauls, and Sujith Ravi. 2010. Bayesian inference
for finite-state transducers. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 447–455, Los Angeles, Cali-
fornia, June.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201–228.
Tagyoung Chung and Daniel Gildea. 2009. Unsuper-
vised tokenization for machine translation. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing, pages 718–726,
Singapore, August.
A.P. Dempster, N.M. Laird, and D.B. Rubin. 1977. Max-
imum likelihood from incomplete data via the EM al-
gorithm. Journal of the Royal Statistical Society, Se-
ries B, 39(1):1–38.
John DeNero, Alexandre Bouchard-Cˆot´e, and Dan Klein.
2008. Sampling alignment structure under a Bayesian
translation model. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 314–323, Honolulu, Hawaii, October.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the As-
sociation for Computational Linguistics, pages 961–
968, Sydney, Australia, July.
Stuart Geman and Donald Geman. 1984. Stochastic re-
laxation, Gibbs distributions, and the Bayesian restora-
tion of images. IEEE Transactions On Pattern Analy-
sis And Machine Intelligence, 6(6):721–741, Novem-
ber.
Sharon Goldwater and Tom Griffiths. 2007. A fully
Bayesian approach to unsupervised part-of-speech tag-
ging. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pages 744–
751, Prague, Czech Republic, June.
Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-
ter. 2007. Bayesian inference for PCFGs via Markov
chain Monte Carlo. In Human Language Technologies
2007: The Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 139–146, Rochester, New York, April.
Genichiro Kikui, Seiichi Yamamoto, Toshiyuki
Takezawa, and Eiichiro Sumita. 2006. Com-
parative study on corpora for speech translation.
IEEE Transactions on Audio, Speech and Language
Processing, 14(5):1674–1682.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT-NAACL 2003, Main Papers, pages 48–54,
Edmonton, May-June.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume Pro-
ceedings of the Demo and Poster Sessions, pages 177–
180, Prague, Czech Republic, June.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the Human
Language Technology Conference of the NAACL, Main
Conference, pages 104–111, New York City, USA,
June.
Robert C. Moore. 2004. Improving IBM word alignment
Model 1. In Proceedings of the 42nd Meeting of the
Association for Computational Linguistics (ACL’04),
Main Volume, pages 518–525, Barcelona, Spain, July.
ThuyLinh Nguyen, Stephan Vogel, and Noah A. Smith.
2010. Nonparametric word segmentation for ma-
</reference>
<page confidence="0.987444">
186
</page>
<reference confidence="0.999737634146342">
chine translation. In Proceedings of the 23rd Interna-
tional Conference on Computational Linguistics (Col-
ing 2010), pages 815–823, Beijing, China, August.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19–51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311–318, Philadelphia, Pennsylva-
nia, USA, July.
Philip Resnik and Eric Hardisty. 2010. Gibbs sampling
for the uninitiated. University of Maryland Computer
Science Department; CS-TR-4956, June.
Andreas Stolcke. 2002. SRILM – an extensible language
modeling toolkit. In Seventh International Conference
on Spoken Language Processing, volume 3.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In COLING, pages 836–841.
Jia Xu, Jianfeng Gao, Kristina Toutanova, and Her-
mann Ney. 2008. Bayesian semi-supervised Chinese
word segmentation for statistical machine translation.
In Proceedings of the 22nd International Conference
on Computational Linguistics (Coling 2008), pages
1017–10124, Manchester, UK, August.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91(1):79–88.
Shaojun Zhao and Daniel Gildea. 2010. A fast fertil-
ity hidden Markov model for word alignment using
MCMC. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
pages 596–605, Cambridge, MA, October.
Bing Zhao and Eric P. Xing. 2006. BiTAM: Bilingual
topic admixture models for word alignment. In Pro-
ceedings of the COLING/ACL 2006 Main Conference
Poster Sessions, pages 969–976, Sydney, Australia,
July. Association for Computational Linguistics.
</reference>
<page confidence="0.997929">
187
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.358382">
<title confidence="0.989911">Bayesian Word Alignment for Statistical Machine Translation</title>
<address confidence="0.820109">Gebze 41470 Kocaeli,</address>
<email confidence="0.984213">coskun@uekae.tubitak.gov.tr</email>
<note confidence="0.536596333333333">apos;Electrical and Electronics Eng. Bogazici Bebek 34342 Istanbul,</note>
<email confidence="0.978213">murat.saraclar@boun.edu.tr</email>
<abstract confidence="0.9993669375">In this work, we compare the translation performance of word alignments obtained via Bayesian inference to those obtained via expectation-maximization (EM). We propose a Gibbs sampler for fully Bayesian inference in IBM Model 1, integrating over all possible parameter values in finding the alignment distribution. We show that Bayesian inference outperforms EM in all of the tested language pairs, domains and data set sizes, by up to 2.99 BLEU points. We also show that the proposed method effectively addresses the well-known rare word problem in EM-estimated models; and at the same time induces a much smaller dictionary of bilingual word-pairs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
<author>Chris Dyer</author>
<author>Miles Osborne</author>
</authors>
<title>A Gibbs sampler for phrasal synchronous grammar induction.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>782--790</pages>
<location>Suntec, Singapore,</location>
<contexts>
<context position="2821" citStr="Blunsom et al., 2009" startWordPosition="435" endWordPosition="438"> procedure, such as smoothing the parameter estimates, were shown to reduce the alignment error rate, but the effects on translation performance was not reported. Zhao and Xing (2006) note that the parameter estimation (for which they use variational EM) suffers from data sparsity and use symmetric Dirichlet priors, but they find the MAP solution. Bayesian inference, the approach in this paper, have recently been applied to several unsupervised learning problems in NLP (Goldwater and Griffiths, 2007; Johnson et al., 2007) as well as to other tasks in SMT such as synchronous grammar induction (Blunsom et al., 2009) and learning phrase alignments directly (DeNero et al., 2008). Word alignment learning problem was addressed jointly with segmentation learning in Xu et al. (2008), Nguyen et al. (2010), and Chung and Gildea (2009). The former two works place nonparametric priors (also known as cache models) on the parameters and utilize Gibbs sampling. However, alignment inference in neither of these works is exactly Bayesian since the alignments are updated by running GIZA++ (Xu et al., 2008) or by local maximization (Nguyen et al., 2010). On the other hand, 182 Proceedings of the 49th Annual Meeting of the</context>
</contexts>
<marker>Blunsom, Cohn, Dyer, Osborne, 2009</marker>
<rawString>Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Osborne. 2009. A Gibbs sampler for phrasal synchronous grammar induction. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 782–790, Suntec, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tugba Bodrumlu</author>
<author>Kevin Knight</author>
<author>Sujith Ravi</author>
</authors>
<title>A new objective function for word alignment.</title>
<date>2009</date>
<booktitle>In Proceedings of the NAACL HLT Workshop on Integer Linear Programming for Natural Language Processing,</booktitle>
<pages>28--35</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="14580" citStr="Bodrumlu et al., 2009" startWordPosition="2472" endWordPosition="2475">okens, source tokens with fertilities in the range 4–7, source tokens with fertilities higher than 7, and the maximum observed fertility. The first language listed is the source in alignment (Section 2). Method TE ET CE EC AE EA EM-80 52.5K 38.5K 440K 461K 383K 388K M4 57.6K 40.5K 439K 441K 422K 405K GS-80 23.5K 25.4K 180K 209K 158K 176K Table 4: Sizes of bilingual dictionaries induced by different alignment methods. inates “excessive” alignments (fertility &gt; 8)10. The number of distinct word-pairs induced by an alignment has been recently proposed as an objective function for word alignment (Bodrumlu et al., 2009). Small dictionary sizes are preferred over large ones. Table 4 shows that the proposed inference method substantially reduces the alignment dictionary size, in most cases by more than 50%. 5 Conclusion We developed a Gibbs sampling-based Bayesian inference method for IBM Model 1 word alignments and showed that it outperforms EM estimation in terms of translation BLEU scores across several language pairs, data sizes and domains. As a result of this increase, Bayesian Model 1 alignments perform close to or better than the state-of-the-art IBM 10The GIZA++ implementation of Model 4 artificially </context>
</contexts>
<marker>Bodrumlu, Knight, Ravi, 2009</marker>
<rawString>Tugba Bodrumlu, Kevin Knight, and Sujith Ravi. 2009. A new objective function for word alignment. In Proceedings of the NAACL HLT Workshop on Integer Linear Programming for Natural Language Processing, pages 28–35, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1314" citStr="Brown et al., 1993" startWordPosition="187" endWordPosition="190">ns and data set sizes, by up to 2.99 BLEU points. We also show that the proposed method effectively addresses the well-known rare word problem in EM-estimated models; and at the same time induces a much smaller dictionary of bilingual word-pairs. 1 Introduction Word alignment is a crucial early step in the training of most statistical machine translation (SMT) systems, in which the estimated alignments are used for constraining the set of candidates in phrase/grammar extraction (Koehn et al., 2003; Chiang, 2007; Galley et al., 2006). State-of-the-art word alignment models, such as IBM Models (Brown et al., 1993), HMM (Vogel et al., 1996), and the jointly-trained symmetric HMM (Liang et al., 2006), contain a large number of parameters (e.g., word translation probabilities) that need to be estimated in addition to the desired hidden alignment variables. The most common method of inference in such models is expectation-maximization (EM) (Dempster et al., 1977) or an approximation to EM when exact EM is intractable. However, being a maximization (e.g., maximum likelihood (ML) or maximum a posteriori (MAP)) technique, EM is generally prone to local optima and overfitting. In essence, the alignment distrib</context>
<context position="5559" citStr="Brown et al., 1993" startWordPosition="883" endWordPosition="886">e general applicability of the proposed technique. To our knowledge, this is the first work to directly investigate the effects of Bayesian alignment inference on translation performance. 2 Bayesian Inference with IBM Model 1 Given a sentence-aligned parallel corpus (E, F), let ei (fj) denote the i-th (j-th) source (target)1 word in e (f), which in turn consists of I (J) words and denotes the s-th sentence in E (F).2 Each source sentence is also hypothesized to have an additional imaginary “null” word e0. Also let VE (VF) denote the size of the observed source (target) vocabulary. In Model 1 (Brown et al., 1993), each target word 1We use the “source” and “target” labels following the generative process, in which E generates F (cf. Eq. 1). 2Dependence of the sentence-level variables e, f, I, J (and a and n, which are introduced later) on the sentence index s should be understood even though not explicitly indicated for notational simplicity. fj is associated with a hidden alignment variable aj whose value ranges over the word positions in the corresponding source sentence. The set of alignments for a sentence (corpus) is denoted by a (A). The model parameters consist of a VE x VF table T of word trans</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Adam Pauls</author>
<author>Sujith Ravi</author>
</authors>
<title>Bayesian inference for finite-state transducers. In Human Language Technologies: The</title>
<date>2010</date>
<booktitle>Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>447--455</pages>
<location>Los Angeles, California,</location>
<contexts>
<context position="8999" citStr="Chiang et al., 2010" startWordPosition="1528" endWordPosition="1531">turn, resulting in an explicit Gibbs sampler. In this work, we marginalize out T by: P(E, F, A; O) = IT P(E, F, A, T; O) (6) and obtain a collapsed Gibbs sampler, which samples only the alignment variables. Using P(E, F, A; O) obtained in (6), the Gibbs sampling formula for the individual alignments is derived as:4 PfF +E 1Nz&apos;,ffF1 eei,f (7) where the superscript -,j denotes the exclusion of the current value of aj. The algorithm is given in Table 1. Initialization of A in Step 1 can be arbitrary, but for faster convergence special initializations have been used, e.g., using the output of EM (Chiang et al., 2010). Once the Gibbs sampler is deemed to have converged after B burn-in iterations, we collect M samples of A with L iterations in-between5 to estimate P(A|E, F). To obtain the Viterbi alignments, which are required for phrase extraction (Koehn et al., 2003), we select for each aj the most frequent value in the M collected samples. 4The derivation is quite standard and similar to other Dirichlet-multinomial Gibbs sampler derivations, e.g. (Resnik and Hardisty, 2010). 5A lag is introduced to reduce correlation between samples. Input: E, F; Output: K samples of A 1 Initialize A 2 for k = 1 to K do </context>
</contexts>
<marker>Chiang, Graehl, Knight, Pauls, Ravi, 2010</marker>
<rawString>David Chiang, Jonathan Graehl, Kevin Knight, Adam Pauls, and Sujith Ravi. 2010. Bayesian inference for finite-state transducers. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 447–455, Los Angeles, California, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="1211" citStr="Chiang, 2007" startWordPosition="172" endWordPosition="173">ution. We show that Bayesian inference outperforms EM in all of the tested language pairs, domains and data set sizes, by up to 2.99 BLEU points. We also show that the proposed method effectively addresses the well-known rare word problem in EM-estimated models; and at the same time induces a much smaller dictionary of bilingual word-pairs. 1 Introduction Word alignment is a crucial early step in the training of most statistical machine translation (SMT) systems, in which the estimated alignments are used for constraining the set of candidates in phrase/grammar extraction (Koehn et al., 2003; Chiang, 2007; Galley et al., 2006). State-of-the-art word alignment models, such as IBM Models (Brown et al., 1993), HMM (Vogel et al., 1996), and the jointly-trained symmetric HMM (Liang et al., 2006), contain a large number of parameters (e.g., word translation probabilities) that need to be estimated in addition to the desired hidden alignment variables. The most common method of inference in such models is expectation-maximization (EM) (Dempster et al., 1977) or an approximation to EM when exact EM is intractable. However, being a maximization (e.g., maximum likelihood (ML) or maximum a posteriori (MA</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tagyoung Chung</author>
<author>Daniel Gildea</author>
</authors>
<title>Unsupervised tokenization for machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>718--726</pages>
<location>Singapore,</location>
<contexts>
<context position="3036" citStr="Chung and Gildea (2009)" startWordPosition="469" endWordPosition="472">ation (for which they use variational EM) suffers from data sparsity and use symmetric Dirichlet priors, but they find the MAP solution. Bayesian inference, the approach in this paper, have recently been applied to several unsupervised learning problems in NLP (Goldwater and Griffiths, 2007; Johnson et al., 2007) as well as to other tasks in SMT such as synchronous grammar induction (Blunsom et al., 2009) and learning phrase alignments directly (DeNero et al., 2008). Word alignment learning problem was addressed jointly with segmentation learning in Xu et al. (2008), Nguyen et al. (2010), and Chung and Gildea (2009). The former two works place nonparametric priors (also known as cache models) on the parameters and utilize Gibbs sampling. However, alignment inference in neither of these works is exactly Bayesian since the alignments are updated by running GIZA++ (Xu et al., 2008) or by local maximization (Nguyen et al., 2010). On the other hand, 182 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 182–187, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics Chung and Gildea (2009) apply a sparse Dirichlet prior on t</context>
</contexts>
<marker>Chung, Gildea, 2009</marker>
<rawString>Tagyoung Chung and Daniel Gildea. 2009. Unsupervised tokenization for machine translation. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 718–726, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society, Series B,</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="1666" citStr="Dempster et al., 1977" startWordPosition="244" endWordPosition="248">slation (SMT) systems, in which the estimated alignments are used for constraining the set of candidates in phrase/grammar extraction (Koehn et al., 2003; Chiang, 2007; Galley et al., 2006). State-of-the-art word alignment models, such as IBM Models (Brown et al., 1993), HMM (Vogel et al., 1996), and the jointly-trained symmetric HMM (Liang et al., 2006), contain a large number of parameters (e.g., word translation probabilities) that need to be estimated in addition to the desired hidden alignment variables. The most common method of inference in such models is expectation-maximization (EM) (Dempster et al., 1977) or an approximation to EM when exact EM is intractable. However, being a maximization (e.g., maximum likelihood (ML) or maximum a posteriori (MAP)) technique, EM is generally prone to local optima and overfitting. In essence, the alignment distribution obtained via EM takes into account only the most likely point in the parameter space, but does not consider contributions from other points. Problems with the standard EM estimation of IBM Model 1 was pointed out by Moore (2004) and a number of heuristic changes to the estimation procedure, such as smoothing the parameter estimates, were shown </context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A.P. Dempster, N.M. Laird, and D.B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, Series B, 39(1):1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Alexandre Bouchard-Cˆot´e</author>
<author>Dan Klein</author>
</authors>
<title>Sampling alignment structure under a Bayesian translation model.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>314--323</pages>
<location>Honolulu, Hawaii,</location>
<marker>DeNero, Bouchard-Cˆot´e, Klein, 2008</marker>
<rawString>John DeNero, Alexandre Bouchard-Cˆot´e, and Dan Klein. 2008. Sampling alignment structure under a Bayesian translation model. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 314–323, Honolulu, Hawaii, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>961--968</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="1233" citStr="Galley et al., 2006" startWordPosition="174" endWordPosition="177"> that Bayesian inference outperforms EM in all of the tested language pairs, domains and data set sizes, by up to 2.99 BLEU points. We also show that the proposed method effectively addresses the well-known rare word problem in EM-estimated models; and at the same time induces a much smaller dictionary of bilingual word-pairs. 1 Introduction Word alignment is a crucial early step in the training of most statistical machine translation (SMT) systems, in which the estimated alignments are used for constraining the set of candidates in phrase/grammar extraction (Koehn et al., 2003; Chiang, 2007; Galley et al., 2006). State-of-the-art word alignment models, such as IBM Models (Brown et al., 1993), HMM (Vogel et al., 1996), and the jointly-trained symmetric HMM (Liang et al., 2006), contain a large number of parameters (e.g., word translation probabilities) that need to be estimated in addition to the desired hidden alignment variables. The most common method of inference in such models is expectation-maximization (EM) (Dempster et al., 1977) or an approximation to EM when exact EM is intractable. However, being a maximization (e.g., maximum likelihood (ML) or maximum a posteriori (MAP)) technique, EM is g</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 961– 968, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Geman</author>
<author>Donald Geman</author>
</authors>
<title>Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images.</title>
<date>1984</date>
<journal>IEEE Transactions On Pattern Analysis And Machine Intelligence,</journal>
<volume>6</volume>
<issue>6</issue>
<contexts>
<context position="8253" citStr="Geman and Geman, 1984" startWordPosition="1386" endWordPosition="1390">,f)Ne,f Y s = VE Y e=1 VF Y f=1 183 distributions that peak at a single target word and penalizes flatter translation distributions, even for rare words. This choice addresses the well-known problem in the IBM Models, and more severely in Model 1, in which rare words act as “garbage collectors” (Och and Ney, 2003) and get assigned excessively large number of word alignments. Then we obtain the joint distribution of all (observed + hidden) variables as: P(E, F, A, T; O) = P(T; O) P(E, F, A|T) (5) where O = O1 ···OVE. To infer the posterior distribution of the alignments, we use Gibbs sampling (Geman and Geman, 1984). One possible method is to derive the Gibbs sampler from P(E, F, A, T; O) obtained in (5) and sample the unknowns A and T in turn, resulting in an explicit Gibbs sampler. In this work, we marginalize out T by: P(E, F, A; O) = IT P(E, F, A, T; O) (6) and obtain a collapsed Gibbs sampler, which samples only the alignment variables. Using P(E, F, A; O) obtained in (6), the Gibbs sampling formula for the individual alignments is derived as:4 PfF +E 1Nz&apos;,ffF1 eei,f (7) where the superscript -,j denotes the exclusion of the current value of aj. The algorithm is given in Table 1. Initialization of A</context>
</contexts>
<marker>Geman, Geman, 1984</marker>
<rawString>Stuart Geman and Donald Geman. 1984. Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. IEEE Transactions On Pattern Analysis And Machine Intelligence, 6(6):721–741, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Tom Griffiths</author>
</authors>
<title>A fully Bayesian approach to unsupervised part-of-speech tagging.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>744--751</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="2704" citStr="Goldwater and Griffiths, 2007" startWordPosition="414" endWordPosition="417">the standard EM estimation of IBM Model 1 was pointed out by Moore (2004) and a number of heuristic changes to the estimation procedure, such as smoothing the parameter estimates, were shown to reduce the alignment error rate, but the effects on translation performance was not reported. Zhao and Xing (2006) note that the parameter estimation (for which they use variational EM) suffers from data sparsity and use symmetric Dirichlet priors, but they find the MAP solution. Bayesian inference, the approach in this paper, have recently been applied to several unsupervised learning problems in NLP (Goldwater and Griffiths, 2007; Johnson et al., 2007) as well as to other tasks in SMT such as synchronous grammar induction (Blunsom et al., 2009) and learning phrase alignments directly (DeNero et al., 2008). Word alignment learning problem was addressed jointly with segmentation learning in Xu et al. (2008), Nguyen et al. (2010), and Chung and Gildea (2009). The former two works place nonparametric priors (also known as cache models) on the parameters and utilize Gibbs sampling. However, alignment inference in neither of these works is exactly Bayesian since the alignments are updated by running GIZA++ (Xu et al., 2008)</context>
</contexts>
<marker>Goldwater, Griffiths, 2007</marker>
<rawString>Sharon Goldwater and Tom Griffiths. 2007. A fully Bayesian approach to unsupervised part-of-speech tagging. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 744– 751, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Thomas L Griffiths</author>
<author>Sharon Goldwater</author>
</authors>
<title>Bayesian inference for PCFGs via Markov chain Monte Carlo.</title>
<date>2007</date>
<booktitle>In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>139--146</pages>
<location>Rochester, New York,</location>
<contexts>
<context position="2727" citStr="Johnson et al., 2007" startWordPosition="418" endWordPosition="421">BM Model 1 was pointed out by Moore (2004) and a number of heuristic changes to the estimation procedure, such as smoothing the parameter estimates, were shown to reduce the alignment error rate, but the effects on translation performance was not reported. Zhao and Xing (2006) note that the parameter estimation (for which they use variational EM) suffers from data sparsity and use symmetric Dirichlet priors, but they find the MAP solution. Bayesian inference, the approach in this paper, have recently been applied to several unsupervised learning problems in NLP (Goldwater and Griffiths, 2007; Johnson et al., 2007) as well as to other tasks in SMT such as synchronous grammar induction (Blunsom et al., 2009) and learning phrase alignments directly (DeNero et al., 2008). Word alignment learning problem was addressed jointly with segmentation learning in Xu et al. (2008), Nguyen et al. (2010), and Chung and Gildea (2009). The former two works place nonparametric priors (also known as cache models) on the parameters and utilize Gibbs sampling. However, alignment inference in neither of these works is exactly Bayesian since the alignments are updated by running GIZA++ (Xu et al., 2008) or by local maximizati</context>
</contexts>
<marker>Johnson, Griffiths, Goldwater, 2007</marker>
<rawString>Mark Johnson, Thomas L. Griffiths, and Sharon Goldwater. 2007. Bayesian inference for PCFGs via Markov chain Monte Carlo. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics, pages 139–146, Rochester, New York, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Genichiro Kikui</author>
<author>Seiichi Yamamoto</author>
<author>Toshiyuki Takezawa</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Comparative study on corpora for speech translation.</title>
<date>2006</date>
<journal>IEEE Transactions on Audio, Speech and Language Processing,</journal>
<volume>14</volume>
<issue>5</issue>
<contexts>
<context position="9974" citStr="Kikui et al., 2006" startWordPosition="1707" endWordPosition="1710">standard and similar to other Dirichlet-multinomial Gibbs sampler derivations, e.g. (Resnik and Hardisty, 2010). 5A lag is introduced to reduce correlation between samples. Input: E, F; Output: K samples of A 1 Initialize A 2 for k = 1 to K do 3 for each sentence-pair s in (E, F) do 4 for j = 1 to J do 5 for i = 0 to I do 6 Calculate P(aj = i |· · · ) according to (7) 7 Sample a new value for aj Table 1: Gibbs sampling algorithm for IBM Model 1 (implemented in the accompanying software). 3 Experimental Setup For TurkishHEnglish experiments, we used the 20K-sentence travel domain BTEC dataset (Kikui et al., 2006) from the yearly IWSLT evaluations6 for training, the CSTAR 2003 test set for development, and the IWSLT 2004 test set for testing7. For CzechHEnglish, we used the 95K-sentence news commentary parallel corpus from the WMT shared task8 for training, news2008 set for development, news2009 set for testing, and the 438M-word English and 81.7M-word Czech monolingual news corpora for additional language model (LM) training. For ArabicHEnglish, we used the 65K-sentence LDC2004T18 (news from 2001-2004) for training, the AFP portion of LDC2004T17 (news from 1998, single reference) for development and t</context>
</contexts>
<marker>Kikui, Yamamoto, Takezawa, Sumita, 2006</marker>
<rawString>Genichiro Kikui, Seiichi Yamamoto, Toshiyuki Takezawa, and Eiichiro Sumita. 2006. Comparative study on corpora for speech translation. IEEE Transactions on Audio, Speech and Language Processing, 14(5):1674–1682.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL 2003, Main Papers,</booktitle>
<pages>48--54</pages>
<location>Edmonton, May-June.</location>
<contexts>
<context position="1197" citStr="Koehn et al., 2003" startWordPosition="168" endWordPosition="171">he alignment distribution. We show that Bayesian inference outperforms EM in all of the tested language pairs, domains and data set sizes, by up to 2.99 BLEU points. We also show that the proposed method effectively addresses the well-known rare word problem in EM-estimated models; and at the same time induces a much smaller dictionary of bilingual word-pairs. 1 Introduction Word alignment is a crucial early step in the training of most statistical machine translation (SMT) systems, in which the estimated alignments are used for constraining the set of candidates in phrase/grammar extraction (Koehn et al., 2003; Chiang, 2007; Galley et al., 2006). State-of-the-art word alignment models, such as IBM Models (Brown et al., 1993), HMM (Vogel et al., 1996), and the jointly-trained symmetric HMM (Liang et al., 2006), contain a large number of parameters (e.g., word translation probabilities) that need to be estimated in addition to the desired hidden alignment variables. The most common method of inference in such models is expectation-maximization (EM) (Dempster et al., 1977) or an approximation to EM when exact EM is intractable. However, being a maximization (e.g., maximum likelihood (ML) or maximum a </context>
<context position="9254" citStr="Koehn et al., 2003" startWordPosition="1570" endWordPosition="1573">mpling formula for the individual alignments is derived as:4 PfF +E 1Nz&apos;,ffF1 eei,f (7) where the superscript -,j denotes the exclusion of the current value of aj. The algorithm is given in Table 1. Initialization of A in Step 1 can be arbitrary, but for faster convergence special initializations have been used, e.g., using the output of EM (Chiang et al., 2010). Once the Gibbs sampler is deemed to have converged after B burn-in iterations, we collect M samples of A with L iterations in-between5 to estimate P(A|E, F). To obtain the Viterbi alignments, which are required for phrase extraction (Koehn et al., 2003), we select for each aj the most frequent value in the M collected samples. 4The derivation is quite standard and similar to other Dirichlet-multinomial Gibbs sampler derivations, e.g. (Resnik and Hardisty, 2010). 5A lag is introduced to reduce correlation between samples. Input: E, F; Output: K samples of A 1 Initialize A 2 for k = 1 to K do 3 for each sentence-pair s in (E, F) do 4 for j = 1 to J do 5 for i = 0 to I do 6 Calculate P(aj = i |· · · ) according to (7) 7 Sample a new value for aj Table 1: Gibbs sampling algorithm for IBM Model 1 (implemented in the accompanying software). 3 Expe</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of HLT-NAACL 2003, Main Papers, pages 48–54, Edmonton, May-June.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,</booktitle>
<pages>177--180</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="11062" citStr="Koehn et al., 2007" startWordPosition="1872" endWordPosition="1875">C2004T18 (news from 2001-2004) for training, the AFP portion of LDC2004T17 (news from 1998, single reference) for development and testing (about 875 sentences each), and the 298M-word English and 215M-word Arabic AFP and Xinhua subsets of the respective Gigaword corpora (LDC2007T07 and LDC2007T40) for additional LM training. All language models are 4-gram in the travel domain experiments and 5-gram in the news domain experiments. For each language pair, we trained standard phrase-based SMT systems in both directions (including alignment symmetrization and log-linear model tuning) using Moses (Koehn et al., 2007), SRILM (Stolcke, 2002), and ZMERT (Zaidan, 2009) tools and evaluated using BLEU (Papineni et al., 2002). To obtain word alignments, we used the accompanying Perl code for Bayesian inference and 6International Workshop on Spoken Language Translation. http://iwslt2010.fbk.eu 7Using only the first English reference for symmetry. 8Workshop on Machine Translation. http://www.statmt.org/wmt10/translation-task.html P(aj = i|E, F, A—j; O) 1V i, fj + 0e.f —a, j 184 Method TE ET CE EC AE EA EM-5 38.91 26.52 14.62 10.07 15.50 15.17 EM-80 39.19 26.47 14.95 10.69 15.66 15.02 GS-N 41.14 27.55 14.99 10.85 1</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 177– 180, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Ben Taskar</author>
<author>Dan Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference,</booktitle>
<pages>104--111</pages>
<location>New York City, USA,</location>
<contexts>
<context position="1400" citStr="Liang et al., 2006" startWordPosition="202" endWordPosition="205">d effectively addresses the well-known rare word problem in EM-estimated models; and at the same time induces a much smaller dictionary of bilingual word-pairs. 1 Introduction Word alignment is a crucial early step in the training of most statistical machine translation (SMT) systems, in which the estimated alignments are used for constraining the set of candidates in phrase/grammar extraction (Koehn et al., 2003; Chiang, 2007; Galley et al., 2006). State-of-the-art word alignment models, such as IBM Models (Brown et al., 1993), HMM (Vogel et al., 1996), and the jointly-trained symmetric HMM (Liang et al., 2006), contain a large number of parameters (e.g., word translation probabilities) that need to be estimated in addition to the desired hidden alignment variables. The most common method of inference in such models is expectation-maximization (EM) (Dempster et al., 1977) or an approximation to EM when exact EM is intractable. However, being a maximization (e.g., maximum likelihood (ML) or maximum a posteriori (MAP)) technique, EM is generally prone to local optima and overfitting. In essence, the alignment distribution obtained via EM takes into account only the most likely point in the parameter s</context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 104–111, New York City, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
</authors>
<title>Improving IBM word alignment Model 1.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume,</booktitle>
<pages>518--525</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="2148" citStr="Moore (2004)" startWordPosition="327" endWordPosition="328">n alignment variables. The most common method of inference in such models is expectation-maximization (EM) (Dempster et al., 1977) or an approximation to EM when exact EM is intractable. However, being a maximization (e.g., maximum likelihood (ML) or maximum a posteriori (MAP)) technique, EM is generally prone to local optima and overfitting. In essence, the alignment distribution obtained via EM takes into account only the most likely point in the parameter space, but does not consider contributions from other points. Problems with the standard EM estimation of IBM Model 1 was pointed out by Moore (2004) and a number of heuristic changes to the estimation procedure, such as smoothing the parameter estimates, were shown to reduce the alignment error rate, but the effects on translation performance was not reported. Zhao and Xing (2006) note that the parameter estimation (for which they use variational EM) suffers from data sparsity and use symmetric Dirichlet priors, but they find the MAP solution. Bayesian inference, the approach in this paper, have recently been applied to several unsupervised learning problems in NLP (Goldwater and Griffiths, 2007; Johnson et al., 2007) as well as to other </context>
</contexts>
<marker>Moore, 2004</marker>
<rawString>Robert C. Moore. 2004. Improving IBM word alignment Model 1. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume, pages 518–525, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>ThuyLinh Nguyen</author>
<author>Stephan Vogel</author>
<author>Noah A Smith</author>
</authors>
<title>Nonparametric word segmentation for machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (Coling</booktitle>
<pages>815--823</pages>
<location>Beijing, China,</location>
<contexts>
<context position="3007" citStr="Nguyen et al. (2010)" startWordPosition="464" endWordPosition="467">e that the parameter estimation (for which they use variational EM) suffers from data sparsity and use symmetric Dirichlet priors, but they find the MAP solution. Bayesian inference, the approach in this paper, have recently been applied to several unsupervised learning problems in NLP (Goldwater and Griffiths, 2007; Johnson et al., 2007) as well as to other tasks in SMT such as synchronous grammar induction (Blunsom et al., 2009) and learning phrase alignments directly (DeNero et al., 2008). Word alignment learning problem was addressed jointly with segmentation learning in Xu et al. (2008), Nguyen et al. (2010), and Chung and Gildea (2009). The former two works place nonparametric priors (also known as cache models) on the parameters and utilize Gibbs sampling. However, alignment inference in neither of these works is exactly Bayesian since the alignments are updated by running GIZA++ (Xu et al., 2008) or by local maximization (Nguyen et al., 2010). On the other hand, 182 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 182–187, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics Chung and Gildea (2009) apply </context>
</contexts>
<marker>Nguyen, Vogel, Smith, 2010</marker>
<rawString>ThuyLinh Nguyen, Stephan Vogel, and Noah A. Smith. 2010. Nonparametric word segmentation for machine translation. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 815–823, Beijing, China, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="7946" citStr="Och and Ney, 2003" startWordPosition="1328" endWordPosition="1331">ric Dirichlet priors identically for all source words e with θe,f = θ = 0.0001 to obtain a sparse Dirichlet prior. A sparse prior favors 3We omit P(Jle) since both J and e are observed and so this term does not affect the inference of hidden variables. P(e) VE VF (te,f)ne,f (3) Y Y e=1 f=1 (I + 1)J (te,f)Ne,f Y s = VE Y e=1 VF Y f=1 183 distributions that peak at a single target word and penalizes flatter translation distributions, even for rare words. This choice addresses the well-known problem in the IBM Models, and more severely in Model 1, in which rare words act as “garbage collectors” (Och and Ney, 2003) and get assigned excessively large number of word alignments. Then we obtain the joint distribution of all (observed + hidden) variables as: P(E, F, A, T; O) = P(T; O) P(E, F, A|T) (5) where O = O1 ···OVE. To infer the posterior distribution of the alignments, we use Gibbs sampling (Geman and Geman, 1984). One possible method is to derive the Gibbs sampler from P(E, F, A, T; O) obtained in (5) and sample the unknowns A and T in turn, resulting in an explicit Gibbs sampler. In this work, we marginalize out T by: P(E, F, A; O) = IT P(E, F, A, T; O) (6) and obtain a collapsed Gibbs sampler, whic</context>
<context position="11915" citStr="Och and Ney, 2003" startWordPosition="2006" endWordPosition="2009">ge Translation. http://iwslt2010.fbk.eu 7Using only the first English reference for symmetry. 8Workshop on Machine Translation. http://www.statmt.org/wmt10/translation-task.html P(aj = i|E, F, A—j; O) 1V i, fj + 0e.f —a, j 184 Method TE ET CE EC AE EA EM-5 38.91 26.52 14.62 10.07 15.50 15.17 EM-80 39.19 26.47 14.95 10.69 15.66 15.02 GS-N 41.14 27.55 14.99 10.85 14.64 15.89 GS-5 40.63 27.24 15.45 10.57 16.41 15.82 GS-80 41.78 29.51 15.01 10.68 15.92 16.02 M4 39.94 27.47 15.47 11.15 16.46 15.43 Table 2: BLEU scores in translation experiments. E: English, T: Turkish, C: Czech, A: Arabic. GIZA++ (Och and Ney, 2003) for EM. For each translation task, we report two EM estimates, obtained after 5 and 80 iterations (EM-5 and EM-80), respectively; and three Gibbs sampling estimates, two of which were initialized with those two EM Viterbi alignments (GS-5 and GS-80) and a third was initialized naively9 (GS-N). Sampling settings were B = 400 for THE, 4000 for CHE and 8000 for AHE; M = 100, and L = 10. For reference, we also report the results with IBM Model 4 alignments (M4) trained in the standard bootstrapping regimen of 15H53343. 4 Results Table 2 compares the BLEU scores of Bayesian inference and EM estima</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, Pennsylvania, USA,</location>
<contexts>
<context position="11166" citStr="Papineni et al., 2002" startWordPosition="1888" endWordPosition="1891">ference) for development and testing (about 875 sentences each), and the 298M-word English and 215M-word Arabic AFP and Xinhua subsets of the respective Gigaword corpora (LDC2007T07 and LDC2007T40) for additional LM training. All language models are 4-gram in the travel domain experiments and 5-gram in the news domain experiments. For each language pair, we trained standard phrase-based SMT systems in both directions (including alignment symmetrization and log-linear model tuning) using Moses (Koehn et al., 2007), SRILM (Stolcke, 2002), and ZMERT (Zaidan, 2009) tools and evaluated using BLEU (Papineni et al., 2002). To obtain word alignments, we used the accompanying Perl code for Bayesian inference and 6International Workshop on Spoken Language Translation. http://iwslt2010.fbk.eu 7Using only the first English reference for symmetry. 8Workshop on Machine Translation. http://www.statmt.org/wmt10/translation-task.html P(aj = i|E, F, A—j; O) 1V i, fj + 0e.f —a, j 184 Method TE ET CE EC AE EA EM-5 38.91 26.52 14.62 10.07 15.50 15.17 EM-80 39.19 26.47 14.95 10.69 15.66 15.02 GS-N 41.14 27.55 14.99 10.85 14.64 15.89 GS-5 40.63 27.24 15.45 10.57 16.41 15.82 GS-80 41.78 29.51 15.01 10.68 15.92 16.02 M4 39.94 2</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, Pennsylvania, USA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
<author>Eric Hardisty</author>
</authors>
<title>Gibbs sampling for the uninitiated.</title>
<date>2010</date>
<tech>CS-TR-4956,</tech>
<institution>University of Maryland Computer Science Department;</institution>
<contexts>
<context position="9466" citStr="Resnik and Hardisty, 2010" startWordPosition="1602" endWordPosition="1605">lization of A in Step 1 can be arbitrary, but for faster convergence special initializations have been used, e.g., using the output of EM (Chiang et al., 2010). Once the Gibbs sampler is deemed to have converged after B burn-in iterations, we collect M samples of A with L iterations in-between5 to estimate P(A|E, F). To obtain the Viterbi alignments, which are required for phrase extraction (Koehn et al., 2003), we select for each aj the most frequent value in the M collected samples. 4The derivation is quite standard and similar to other Dirichlet-multinomial Gibbs sampler derivations, e.g. (Resnik and Hardisty, 2010). 5A lag is introduced to reduce correlation between samples. Input: E, F; Output: K samples of A 1 Initialize A 2 for k = 1 to K do 3 for each sentence-pair s in (E, F) do 4 for j = 1 to J do 5 for i = 0 to I do 6 Calculate P(aj = i |· · · ) according to (7) 7 Sample a new value for aj Table 1: Gibbs sampling algorithm for IBM Model 1 (implemented in the accompanying software). 3 Experimental Setup For TurkishHEnglish experiments, we used the 20K-sentence travel domain BTEC dataset (Kikui et al., 2006) from the yearly IWSLT evaluations6 for training, the CSTAR 2003 test set for development, a</context>
</contexts>
<marker>Resnik, Hardisty, 2010</marker>
<rawString>Philip Resnik and Eric Hardisty. 2010. Gibbs sampling for the uninitiated. University of Maryland Computer Science Department; CS-TR-4956, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Seventh International Conference on Spoken Language Processing,</booktitle>
<volume>3</volume>
<contexts>
<context position="11085" citStr="Stolcke, 2002" startWordPosition="1877" endWordPosition="1878">4) for training, the AFP portion of LDC2004T17 (news from 1998, single reference) for development and testing (about 875 sentences each), and the 298M-word English and 215M-word Arabic AFP and Xinhua subsets of the respective Gigaword corpora (LDC2007T07 and LDC2007T40) for additional LM training. All language models are 4-gram in the travel domain experiments and 5-gram in the news domain experiments. For each language pair, we trained standard phrase-based SMT systems in both directions (including alignment symmetrization and log-linear model tuning) using Moses (Koehn et al., 2007), SRILM (Stolcke, 2002), and ZMERT (Zaidan, 2009) tools and evaluated using BLEU (Papineni et al., 2002). To obtain word alignments, we used the accompanying Perl code for Bayesian inference and 6International Workshop on Spoken Language Translation. http://iwslt2010.fbk.eu 7Using only the first English reference for symmetry. 8Workshop on Machine Translation. http://www.statmt.org/wmt10/translation-task.html P(aj = i|E, F, A—j; O) 1V i, fj + 0e.f —a, j 184 Method TE ET CE EC AE EA EM-5 38.91 26.52 14.62 10.07 15.50 15.17 EM-80 39.19 26.47 14.95 10.69 15.66 15.02 GS-N 41.14 27.55 14.99 10.85 14.64 15.89 GS-5 40.63 2</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM – an extensible language modeling toolkit. In Seventh International Conference on Spoken Language Processing, volume 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>HMM-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In COLING,</booktitle>
<pages>836--841</pages>
<contexts>
<context position="1340" citStr="Vogel et al., 1996" startWordPosition="192" endWordPosition="195">up to 2.99 BLEU points. We also show that the proposed method effectively addresses the well-known rare word problem in EM-estimated models; and at the same time induces a much smaller dictionary of bilingual word-pairs. 1 Introduction Word alignment is a crucial early step in the training of most statistical machine translation (SMT) systems, in which the estimated alignments are used for constraining the set of candidates in phrase/grammar extraction (Koehn et al., 2003; Chiang, 2007; Galley et al., 2006). State-of-the-art word alignment models, such as IBM Models (Brown et al., 1993), HMM (Vogel et al., 1996), and the jointly-trained symmetric HMM (Liang et al., 2006), contain a large number of parameters (e.g., word translation probabilities) that need to be estimated in addition to the desired hidden alignment variables. The most common method of inference in such models is expectation-maximization (EM) (Dempster et al., 1977) or an approximation to EM when exact EM is intractable. However, being a maximization (e.g., maximum likelihood (ML) or maximum a posteriori (MAP)) technique, EM is generally prone to local optima and overfitting. In essence, the alignment distribution obtained via EM take</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1996. HMM-based word alignment in statistical translation. In COLING, pages 836–841.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jia Xu</author>
<author>Jianfeng Gao</author>
<author>Kristina Toutanova</author>
<author>Hermann Ney</author>
</authors>
<title>Bayesian semi-supervised Chinese word segmentation for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<pages>1017--10124</pages>
<location>Manchester, UK,</location>
<contexts>
<context position="2985" citStr="Xu et al. (2008)" startWordPosition="460" endWordPosition="463">nd Xing (2006) note that the parameter estimation (for which they use variational EM) suffers from data sparsity and use symmetric Dirichlet priors, but they find the MAP solution. Bayesian inference, the approach in this paper, have recently been applied to several unsupervised learning problems in NLP (Goldwater and Griffiths, 2007; Johnson et al., 2007) as well as to other tasks in SMT such as synchronous grammar induction (Blunsom et al., 2009) and learning phrase alignments directly (DeNero et al., 2008). Word alignment learning problem was addressed jointly with segmentation learning in Xu et al. (2008), Nguyen et al. (2010), and Chung and Gildea (2009). The former two works place nonparametric priors (also known as cache models) on the parameters and utilize Gibbs sampling. However, alignment inference in neither of these works is exactly Bayesian since the alignments are updated by running GIZA++ (Xu et al., 2008) or by local maximization (Nguyen et al., 2010). On the other hand, 182 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 182–187, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics Chung an</context>
</contexts>
<marker>Xu, Gao, Toutanova, Ney, 2008</marker>
<rawString>Jia Xu, Jianfeng Gao, Kristina Toutanova, and Hermann Ney. 2008. Bayesian semi-supervised Chinese word segmentation for statistical machine translation. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1017–10124, Manchester, UK, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omar F Zaidan</author>
</authors>
<title>Z-MERT: A fully configurable open source tool for minimum error rate training of machine translation systems.</title>
<date>2009</date>
<journal>The Prague Bulletin of Mathematical Linguistics,</journal>
<volume>91</volume>
<issue>1</issue>
<contexts>
<context position="11111" citStr="Zaidan, 2009" startWordPosition="1881" endWordPosition="1882">rtion of LDC2004T17 (news from 1998, single reference) for development and testing (about 875 sentences each), and the 298M-word English and 215M-word Arabic AFP and Xinhua subsets of the respective Gigaword corpora (LDC2007T07 and LDC2007T40) for additional LM training. All language models are 4-gram in the travel domain experiments and 5-gram in the news domain experiments. For each language pair, we trained standard phrase-based SMT systems in both directions (including alignment symmetrization and log-linear model tuning) using Moses (Koehn et al., 2007), SRILM (Stolcke, 2002), and ZMERT (Zaidan, 2009) tools and evaluated using BLEU (Papineni et al., 2002). To obtain word alignments, we used the accompanying Perl code for Bayesian inference and 6International Workshop on Spoken Language Translation. http://iwslt2010.fbk.eu 7Using only the first English reference for symmetry. 8Workshop on Machine Translation. http://www.statmt.org/wmt10/translation-task.html P(aj = i|E, F, A—j; O) 1V i, fj + 0e.f —a, j 184 Method TE ET CE EC AE EA EM-5 38.91 26.52 14.62 10.07 15.50 15.17 EM-80 39.19 26.47 14.95 10.69 15.66 15.02 GS-N 41.14 27.55 14.99 10.85 14.64 15.89 GS-5 40.63 27.24 15.45 10.57 16.41 15.</context>
</contexts>
<marker>Zaidan, 2009</marker>
<rawString>Omar F. Zaidan. 2009. Z-MERT: A fully configurable open source tool for minimum error rate training of machine translation systems. The Prague Bulletin of Mathematical Linguistics, 91(1):79–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shaojun Zhao</author>
<author>Daniel Gildea</author>
</authors>
<title>A fast fertility hidden Markov model for word alignment using MCMC.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>596--605</pages>
<location>Cambridge, MA,</location>
<contexts>
<context position="3853" citStr="Zhao and Gildea (2010)" startWordPosition="596" endWordPosition="599">sian since the alignments are updated by running GIZA++ (Xu et al., 2008) or by local maximization (Nguyen et al., 2010). On the other hand, 182 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 182–187, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics Chung and Gildea (2009) apply a sparse Dirichlet prior on the multinomial parameters to prevent overfitting. They use variational Bayes for inference, but they do not investigate the effect of Bayesian inference to word alignment in isolation. Recently, Zhao and Gildea (2010) proposed fertility extensions to IBM Model 1 and HMM, but they do not place any prior on the parameters and their inference method is actually stochastic EM (also known as Monte Carlo EM), a ML technique in which sampling is used to approximate the expected counts in the E-step. Even though they report substantial reductions in alignment error rate, the translation BLEU scores do not improve. Our approach in this paper is fully Bayesian in which the alignment probabilities are inferred by integrating over all possible parameter values assuming an intuitive, sparse prior. We develop a Gibbs sa</context>
</contexts>
<marker>Zhao, Gildea, 2010</marker>
<rawString>Shaojun Zhao and Daniel Gildea. 2010. A fast fertility hidden Markov model for word alignment using MCMC. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 596–605, Cambridge, MA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Zhao</author>
<author>Eric P Xing</author>
</authors>
<title>BiTAM: Bilingual topic admixture models for word alignment.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions,</booktitle>
<pages>969--976</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="2383" citStr="Zhao and Xing (2006)" startWordPosition="364" endWordPosition="367">imum likelihood (ML) or maximum a posteriori (MAP)) technique, EM is generally prone to local optima and overfitting. In essence, the alignment distribution obtained via EM takes into account only the most likely point in the parameter space, but does not consider contributions from other points. Problems with the standard EM estimation of IBM Model 1 was pointed out by Moore (2004) and a number of heuristic changes to the estimation procedure, such as smoothing the parameter estimates, were shown to reduce the alignment error rate, but the effects on translation performance was not reported. Zhao and Xing (2006) note that the parameter estimation (for which they use variational EM) suffers from data sparsity and use symmetric Dirichlet priors, but they find the MAP solution. Bayesian inference, the approach in this paper, have recently been applied to several unsupervised learning problems in NLP (Goldwater and Griffiths, 2007; Johnson et al., 2007) as well as to other tasks in SMT such as synchronous grammar induction (Blunsom et al., 2009) and learning phrase alignments directly (DeNero et al., 2008). Word alignment learning problem was addressed jointly with segmentation learning in Xu et al. (200</context>
</contexts>
<marker>Zhao, Xing, 2006</marker>
<rawString>Bing Zhao and Eric P. Xing. 2006. BiTAM: Bilingual topic admixture models for word alignment. In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 969–976, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>