<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000103">
<title confidence="0.953442">
Using Grammatical Relations to Compare Parsers
</title>
<author confidence="0.948651">
Judita Preiss*
</author>
<affiliation confidence="0.811837">
Computer Laboratory
University of Cambridge
Judita.PreissAcl.cam.ac.uk
</affiliation>
<sectionHeader confidence="0.969684" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999890947368421">
We use the grammatical re-
lations (GRs) described in
Carroll et al. (1998) to compare
a number of parsing algorithms.
A first ranking of the parsers is
provided by comparing the extracted
GRs to a gold standard GR anno-
tation of 500 Susanne sentences:
this required an implementation of
GR extraction software for Penn
Treebank style parsers. In addition,
we perform an experiment using
the extracted GRs as input to the
Lappin and Leass (1994) anaphora
resolution algorithm. This produces
a second ranking of the parsers, and
we investigate the number of errors
that are caused by the incorrect
GRs.
</bodyText>
<sectionHeader confidence="0.998737" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999741833333333">
We investigate the usefulness of a grammatical
relation (GR) evaluation method by using it to
compare the performance of four full parsers
and a GR finder based on a shallow parser.
It is usually difficult to compare perfor-
mance of different style parsers, as the out-
put trees can vary in structure. In this pa-
per, we use GRs to provide a common basis
for comparing full and shallow parsers, and
Penn Treebank and Susanne structures. To
carry out this comparison, we implemented a
GR extraction mechanism for Penn Treebank
</bodyText>
<note confidence="0.4527195">
This work was supported by UK EPSRC project
GR/N36462/93 &apos;Robust Accurate Statistical Parsing&apos;.
</note>
<bodyText confidence="0.999873918918919">
parses. Evaluating parsers using GRs as op-
posed to crossing brackets or labelled preci-
sion/recall metrics can be argued to give a
more robust measure of performance (Carroll
et al., 1998), (Clark and Hockenmaier, 2002).
The main novelty of this paper is the use of
the Carroll et al&apos;s GR evaluation method to
compare the Collins model 1 and model 2, and
Charniak parsers.
An initial evaluation is provided by compar-
ing the extracted GRs to a gold standard GR
annotation of 500 Susanne sentences due to
Carroll et al. To gain insight into the strengths
and weaknesses of the different parsers, we
present a breakdown of the results for each
type of GR. It is not clear whether the rank-
ing produced from the gold standard evalu-
ation is representative: there may be corpus
effects for parsers not trained on Susanne,
and real life applications may not reflect this
ranking. We therefore perform an experi-
ment using the extracted GRs as input to the
Lappin and Leass (1994) anaphora resolution
algorithm. This produces a second ranking of
the parsers, and we investigate the number of
errors that are caused by incorrect GRs.
We describe the parsers and the GR finder
in Section 2. We introduce GRs in Section 3
and briefly describe our GR extraction soft-
ware for Penn Treebank style parses. The eval-
uation, including a description of the evalua-
tion corpus and performance results, is pre-
sented in Section 4. The results are analyzed
in Section 5 and a performance comparison in
the context of anaphora resolution is presented
in Section 6. We draw our conclusions in Sec-
tion 7.
</bodyText>
<page confidence="0.996049">
291
</page>
<table confidence="0.997439">
Parser Corpus LR LP CB OCB 2CB
sentences &lt; 40 words
CH WSJ 90.1 90.1 0.74 70.1 89.6
Cl WSJ 87.52 87.92 0.96 64.86 86.19
C2 WSJ 88.07 88.35 0.95 65.84 86.64
sentences &lt; 100 words
CH WSJ 89.6 89.5 0.88 67.6 87.7
Cl WSJ 87.01 87.41 1.11 62.17 83.86
C2 WSJ 87.60 87.89 1.09 63.20 84.60
BC evaluation
BC Susanne 74.0 73.0 1.03 59.6 -
</table>
<tableCaption confidence="0.9661095">
Table 1: Summary of Published Results (LR = labelled recall, LP = labelled precision,
CB = crossing brackets)
</tableCaption>
<table confidence="0.9990374">
BC a CHb Cl &amp; C2&apos; BU
Grammar Unification-based, Generative, Generative, N/A
PoS and punct 3rd order 0th order
labels
Algorithm LR parser Chart parser Shallow parser
Tagger Acquilex (CLAWS- own Ratnaparkhi Memory-based (Daele-
II) (Elworthy, 1994) (1996).d mans et al., 1996)
Training Susanne Sections 2-21 of the Wall Street Sections 10-19 of the
(Sampson, 1995)e Journal portion of the Penn WSJ corpus of the
Treebank (Marcus et al., 1993) Penn Treebank II
</table>
<tableCaption confidence="0.965396166666667">
&apos;Available from http: //www. cogs. susx.ac .uk/lab/n1p/rasp/
&apos;Available from ftp://ftp.cs.brown.edu/pub/n1parser/
&apos;Available from ftp: //f tp . cis . upenn . edu/pub/mcollins/misc/
dAvailable from http://www.cis.upenn.edu/-adwait/
&apos;Note that the Briscoe and Carroll grammar is manually created, and Susanne was used for development.
Table 2: Parser Descriptions
</tableCaption>
<sectionHeader confidence="0.927485" genericHeader="introduction">
2 Tools
</sectionHeader>
<bodyText confidence="0.9991809375">
In this work we compare four full parsers
from which GRs are extracted by walk-
ing over the trees. These parsers
are Briscoe and Carroll (1993) (BC),
Charniak (2000) (CH), model 1 and model 2
of Collins (1997) (Cl and C2).1- A summary
of published performance results can be found
in Table 1. We also include in our comparison
a GR finder (Buchholz, 2002) (BU) based on
a shallow parser (Daelemans, 1996), (Buch-
holz et al., 1999). Table 2 summarizes the
&apos;Note that Collins&apos; model 1 and Collins&apos; model 2
are considered as two different parsers.
grammar, the parsing algorithm, the tagger
and the training corpus for all the parsers
that we investigate.
</bodyText>
<sectionHeader confidence="0.993678" genericHeader="method">
3 Grammatical Relations
</sectionHeader>
<bodyText confidence="0.9863051">
Lin (1995) proposed an evaluation based on
grammatical dependencies, in which syntac-
tic dependencies are described between heads
and their dependents. This work was extended
by Carroll et al. (1998), and it is this specifi-
cation called grammatical relations which we
employ in our work. An example, for the sen-
tence John gave Mary the book, can be seen in
Figure 1.
Both the Briscoe and Carroll parser and
</bodyText>
<page confidence="0.993538">
292
</page>
<figure confidence="0.629196833333333">
Sentence: John gave Mary the book.
Grammatical relations:
(ncsubj gave John)
(dobj gave Mary)
(obj2 gave book)
(detmod book the)
</figure>
<figureCaption confidence="0.999858">
Figure 1: Sample GR output
</figureCaption>
<bodyText confidence="0.999944472222222">
Buchholz&apos;s GR finder already output GRs
in the desired format. Although Buchholz&apos;s
work has focused mainly on extracting rela-
tions involving verbs, some non-verb relations
(e.g. detmod) are also produced by the chun-
ker she employs (Veenstra and van den Bosch,
2000).
Therefore, to carry out a GR comparison,
we need to extract GRs from Penn Treebank
style parses. We manually created rules which
find the relevant heads and their dependants
by traversing the parse tree (for example, the
NP in a S NP VP rule gives an instance
of the ncsubj relation). In cases where a dis-
tinction is difficult/impossible to make from a
Penn Treebank tree (e.g. xcomp vs xmod), we
sacrificed recall for precision and only encoded
rules which cause as few misclassifications as
possible.2
Similar work has been carried out by
Blaheta and Charniak (2000) who used statis-
tical methods to add function tags to Penn
Treebank I style parses; however, as well as
converting the tags into a Carroll et al. for-
mat, we would need to add extra rules to ex-
tract other GRs needed for our application de-
scribed in Section 6. E.g. the direct object
is not immediately apparent from Penn Tree-
bank II tags.
We restrict the GRs we extract from the
Penn Treebank to those that are necessary for
the anaphora resolution application (the ob-
ject relations, the complement relations and
the ncmod relation), and those that are a sim-
ple by-product of extracting the necessary re-
lations (e.g. aux).
</bodyText>
<footnote confidence="0.6359665">
2These kind of errors caused by the GR extraction
rules may be responsible for degraded performance.
</footnote>
<sectionHeader confidence="0.974658" genericHeader="method">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999956769230769">
As part of the development of their parser,
Carroll et al. have manually annotated 500
sentences with their GRs.3 The sentences
were selected at random from the Susanne cor-
pus subject to the constraint that they are
within the coverage of the Briscoe and Car-
roll parser.4 We used our own evaluation soft-
ware which only scores correct an exact match
of the output with the gold standard. This
has caused some differences in performance
with previously published results, for example
the Briscoe and Carroll GRs do not produce
the expected conjunction in the conj relation,
causing the system to score zero.
The results of all systems are presented in
Table 3. For each system, we present two fig-
ures: precision (the number of instances of this
GR the system correctly annotated divided by
the number of instances labelled as this GR
by the system), and recall (the number of in-
stances of this GR the system correctly anno-
tated divided by the number of instances of
this GR in the corpus). In the #occs column
of the table, we also present the number of oc-
currences of each GR in the 500 sentence cor-
pus. A dash (â€”) indicates that a certain GR
annotation was not present in the answer cor-
pus at al1.5 We also show the mean it precision
and recall for each system, and the weighted
mean itw, where precision and recall values
are weighted by the number of occurrences of
each GR.
To obtain a ranking of the parsers, we com-
pare F0_1 using the t-test. The 500 sentence
corpus is split into 10 segments and an F-
measure is computed for each algorithm on all
segments. These are then compared using the
t-test. The results are presented in Table 4,
which is to be interpreted as follows:
</bodyText>
<footnote confidence="0.954774111111111">
3Available from http: //www. cogs. susx. ac .uk/
lab/nlp/carroll/greval .html
4The parser has a coverage of about 74% on Su-
sanne.
5Note that we currently do not extract cmod, conj,
csubj, mod, subj, xmod or xsubj from Penn Treebank
parses. We have merged the xcomp, ccomp and clausal
GRs to make the evaluation meaningful (no clausal
tags appear in the gold standard).
</footnote>
<page confidence="0.99278">
293
</page>
<table confidence="0.998792894736842">
GR # occ BC BU CH Cl C2
arg_mod 41 - - 75.68 68.29 78.12 60.98 82.86 70.73 82.86 70.73
aux 381 87.06 84.78 93.70 89.76 89.86 83.73 87.00 86.09 89.89 86.35
clausal 403 43.27 52.61 75.79 71.46 62.19 43.67 50.57 32.75 49.11 27.30
cmod 209 38.28 23.45 55.71 18.66 -
conj 165 0.00 0.00 80.00 24.24 -
csubj 3 0.00 0.00 0.00 0.00 -
detmod 1124 91.15 89.77 92.41 90.93 90.19 87.54 92.09 89.06 92.15 88.79
dobj 409 85.83 78.48 88.42 76.53 84.43 75.55 86.16 74.57 84.85 75.31
iobj 158 31.93 67.09 57.75 51.90 27.53 67.09 27.09 69.62 27.01 70.25
mod 21 1.25 28.57 -
ncmod 2403 69.45 57.72 66.86 51.64 79.84 46.32 81.46 47.36 81.08 47.27
ncsubj 1038 81.99 82.47 85.83 72.93 81.80 70.13 79.19 65.99 81.29 69.46
obj2 19 27.45 73.68 46.15 31.58 61.54 42.11 81.82 47.37 61.54 42.11
subj 1 0.00 0.00 -
xmod 128 13.64 2.34 69.23 7.03 -
xsubj 5 - - 50.00 40.00 -
/-1 407 35.71 40.06 58.60 43.43 40.97 36.07 41.77 36.47 40.61 36.10
Itiv - 69.99 65.86 77.30 64.06 73.86 57.90 73.67 57.42 73.81 57.62
</table>
<tableCaption confidence="0.995049">
Table 3: GR Precisions and Recalls
</tableCaption>
<equation confidence="0.372742">
BC BU
C2 85
</equation>
<bodyText confidence="0.997339777777778">
This example means that the Collins model
2 parser does not outperform the Buchholz GR
finder, but it outperforms the Briscoe parser
with a statistical significance of 85%. Table 4
shows that the Buchholz&apos; GR finder, based
on a shallow parser, outperforms all the other
parsers. This is followed in order by Char-
niak&apos;s, Collins&apos; model 2, Collins&apos; model 1, and
then Briscoe and Carroll&apos;s.
</bodyText>
<table confidence="0.998962333333333">
BC BU CH Cl C2
BC - - - - -
BU 99.5 - 99.5 99.5 99.5
CH 85 - - 75 55
Cl 70 - - - -
C2 85 - - 80 -
</table>
<tableCaption confidence="0.982907">
Table 4: t-tests for F-measure
</tableCaption>
<sectionHeader confidence="0.996337" genericHeader="method">
5 Error Analysis
</sectionHeader>
<bodyText confidence="0.999915166666667">
We investigated the cases where groups of sys-
tems failed to annotate some GRs (missing
GRs) and cases where groups of systems re-
turned the same wrong relation (extra GRs).
The results of this are presented in Tables 5
and 6. In Table 5, we present the percent-
age of wrong cases covered by a particular
combination of systems (i.e. BC represents the
proportion of extra relations which were only
suggested by the Briscoe and Carroll parser,
whereas BC BU CH Cl C2 represents those ex-
tras which were suggested by all parsers.)6 We
present individual percentages, the percentage
covered by the related Collins parsers (Cl C2),
the Penn Treebank parsers (CH Cl C2) and all
systems. The GRs wrongly suggested by all
systems could be used to identify errors in the
gold standard, since these break down into:
</bodyText>
<listItem confidence="0.805816">
â€¢ Extra aux relations, where this is
not marked up in the gold stan-
dard (e.g. . . . receive. . . approval. . . to
be printed. . . is missing the
</listItem>
<bodyText confidence="0.9979514">
Â°Note that we are generating a probability distribu-
tion of same extra GRs over all system combinations.
For example, Cl C2 represents the extra cases sug-
gested by precisely these systems and so is independent
of the percentage covered by CH Cl C2.
</bodyText>
<page confidence="0.984206">
294
</page>
<table confidence="0.9998655">
BC BU CH Cl C2 Cl C2 CH Cl C2 BC BU CH Cl C2
arg_mod 0.00 35.71 21.43 0.00 0.00 7.14 7.14 0.00
aux 27.78 7.78 7.78 14.44 1.11 7.78 2.22 14.44
clausal 48.25 11.87 7.59 5.06 2.92 7.20 7.39 0.00
detmod 22.94 14.22 15.14 1.38 0.92 3.21 12.84 10.55
dobj 24.65 10.56 14.08 4.93 4.23 7.75 9.15 3.52
iobj 14.93 2.99 10.87 0.85 2.13 12.58 17.06 4.26
ncmod 31.51 32.63 6.45 0.23 0.53 4.28 6.83 2.03
ncsubj 25.20 14.43 13.01 7.52 3.05 10.37 7.52 3.46
obj2 66.67 13.73 5.88 1.96 3.92 0.00 0.00 0.00
</table>
<tableCaption confidence="0.958422">
Table 5: Percentage of Extras
</tableCaption>
<table confidence="0.9999593">
BC BU CH Cl C2 Cl C2 CH Cl C2 BC BU CH Cl C2
arg_mod 53.66 0.00 0.00 0.00 0.00 0.00 0.00 19.51
aux 18.00 9.00 8.00 2.00 1.00 4.00 7.00 20.00
clausal 16.71 0.26 0.26 0.00 1.57 13.84 23.76 15.40
detmod 17.75 7.79 11.69 0.87 2.16 3.46 11.26 21.21
dobj 12.09 12.09 4.95 3.30 1.65 4.40 11.54 20.88
iobj 12.15 18.69 1.87 0.93 0.00 2.80 6.54 17.76
ncmod 2.99 13.26 3.10 0.11 0.28 1.75 9.31 29.80
ncsubj 7.24 11.07 3.02 5.03 0.40 3.42 17.51 17.91
obj2 0.00 0.00 6.67 0.00 0.00 0.00 0.00 13.33
</table>
<tableCaption confidence="0.990181">
Table 6: Percentage of Missing
</tableCaption>
<bodyText confidence="0.523984">
(aux printed be) relation).
</bodyText>
<listItem confidence="0.9458026">
â€¢ Extra ncmod relations, due to wrong iden-
tification of the head by the algorithms or
in the gold standard.
â€¢ Extra iobj relations, due to a misclassifi-
cation of an ncmod relation.
</listItem>
<bodyText confidence="0.94201875">
Table 6 classifies the cases of missing GRs
and could therefore be used to discover missing
classes of GRs, as well as mistakes in the gold
standard. The main sources of errors are:
</bodyText>
<listItem confidence="0.971989375">
â€¢ Missing ncmod relations where
the modifier is temporal,
e.g. (ncmod say Friday).
â€¢ Missing detmods, due to certain words
not being assigned a determiner tag by
the taggers. Examples of such words are
many and several. This error creates ex-
tra ncmod relations instead.
</listItem>
<bodyText confidence="0.999975333333333">
The table also shows that the clausal rela-
tion would benefit from improvement since the
clausal relations is frequently omitted from all
the Penn Treebank parsers. However, in the
case of this relation, we have sacrificed recall
for precision.
</bodyText>
<sectionHeader confidence="0.988679" genericHeader="method">
6 Anaphora Resolution
</sectionHeader>
<bodyText confidence="0.9999494">
We investigate the effect of using different
parsers in an anaphora resolution system.
This will indicate the impact of a change in
parser performance on a real task: although
one parser may have a marginally higher pre-
cision than another on a particular evaluation
corpus, it is not clear whether this will be re-
flected by the results of a system which makes
use of this parser, and which may work on a
different corpus.
</bodyText>
<subsectionHeader confidence="0.997464">
6.1 Lappin and Leass
</subsectionHeader>
<bodyText confidence="0.998843">
We choose to re-implement a non-probabilistic
algorithm due to Lappin and Leass (1994),
</bodyText>
<page confidence="0.99592">
295
</page>
<bodyText confidence="0.999027708333333">
because this anaphora resolution algorithm
can be encoded in terms of the GR information
(Preiss and Briscoe, 2003). For each pronoun,
this algorithm uses syntactic criteria to rule
out noun phrases that cannot possibly corefer
with it. An antecedent is then chosen accord-
ing to a ranking based on salience weights.
For all pronouns, noun phrases are ruled
out if they have incompatible agreement fea-
tures. Pronouns are split into two classes, lexi-
cal (reflexives and reciprocals) and non-lexical
anaphors. There are additional syntactic fil-
ters for both of the two types of anaphors.
(ncsubj like she)
(dobj like her)
Secondly, GR information is used for obtain-
ing salience values. In the above sentence, we
would use the ncsubj relation to reward she for
being a subject and the dobj relation to give
her points for accusative emphasis.
The algorithm makes use of the object rela-
tions (ncsubj, dobj, obj2, iobj), the complement
relations (xcomp, ccomp, and clausal), and the
non-clausal modifier ncmod relation.
</bodyText>
<figure confidence="0.5498352">
6.3 Evaluation
Factor Weight
Sentence recency 100
Subject emphasis 80
Existential emphasis 70
Accusative emphasis 50
Indirect object/oblique 40
Head noun emphasis 80
Non-adverbial emphasis 50
Sents Prons
1 754 134
2 785 116
3 318 153
4 268 135
5 271 135
</figure>
<tableCaption confidence="0.9599725">
Table 8: Corpus Information
Table 7: Salience weights
</tableCaption>
<bodyText confidence="0.999811833333333">
Candidates which remain after filtering are
ranked according to their salience. A salience
value corresponding to a weighted sum of the
relevant feature weights (summarized in Ta-
ble 7) is computed. If we consider the sentence
John walks, the salience of John will be:
</bodyText>
<subsectionHeader confidence="0.47411">
sal(John) Wsent Wsubj Whead Wnon-adv
</subsectionHeader>
<bodyText confidence="0.990487285714286">
= 100 + 80 + 80 + 50
= 310
The weights are scaled by a factor of ()
where s is the distance (number of sentences)
of the candidate from the pronoun.
The candidate with the highest salience is
proposed as the antecedent.
</bodyText>
<subsectionHeader confidence="0.999932">
6.2 Using GR Information
</subsectionHeader>
<bodyText confidence="0.999939846153846">
The algorithm uses GR information at two
points: initially, it is used to eliminate certain
intrasentential candidates from the candidates
list. For example, in the sentence She likes her,
she and her cannot corefer, which is expressed
by a shared head in the following GRs:
For this experiment, we use an anaphori-
cally resolved 2400 sentence initial segment of
the BNC (Leech, 1992), which we split into five
segments containing roughly equal numbers of
pronouns. The number of sentences and pro-
nouns in each of the five segments is presented
in Table 8.
</bodyText>
<table confidence="0.999862">
BC BU CH Cl C2
1 60.45 63.43 62.69 62.69 61.19
2 50.86 52.59 54.31 55.17 54.31
3 69.93 69.93 69.28 67.32 69.28
4 67.41 65.19 69.63 63.70 66.67
5 54.81 52.59 50.37 51.85 51.85
tt 60.69 60.75 61.26 60.15 60.66
o-2 52.36 48.87 60.66 32.83 45.73
</table>
<tableCaption confidence="0.998867">
Table 9: Anaphora Results
</tableCaption>
<bodyText confidence="0.99524">
The results of the Lappin and Leass
anaphora resolution algorithm using each of
the parsers are presented in Table 9.7 The
&apos;algorithms&apos; are only evaluated on pronouns
</bodyText>
<footnote confidence="0.991938333333333">
71n this case, Briscoe means the Lappin and Leass
algorithm using the GRs generated by the Briscoe and
Carroll algorithm, etc.
</footnote>
<page confidence="0.989565">
296
</page>
<table confidence="0.986082">
BC BU CH Cl C2
BC â€” â€” â€” 60 0
BU 0 â€” â€” 70 0
CH 60 65 â€” 75 75
Cl â€” â€” â€” â€” â€”
C2 â€” â€” â€” 70 â€”
</table>
<tableCaption confidence="0.755641">
Table 10: t-tests for Anaphora Resolution Per-
formance
</tableCaption>
<bodyText confidence="0.999917416666666">
where all systems suggested an answer, so only
precision is reported.8 The difference between
the &apos;worst&apos; and the &apos;best&apos; systems&apos; mean itt per-
formance is about 1%. However, the variance
o-2 (a measure of robustness of a system) is
lowest for Collins&apos; model 1. We again inves-
tigate the significance of the performance re-
sults using a t-test on our five segments, and
the results can be seen in Table 10. The rank-
ing obtained in this case indicates very small
differences in performance between the algo-
rithms.
</bodyText>
<subsectionHeader confidence="0.969956">
6.4 Error Analysis
</subsectionHeader>
<bodyText confidence="0.997469888888889">
In our error analysis, we found that in 40%
(13) of the errors the anaphora resolution al-
gorithm made a mistake with all the parsers.
This suggests that for a large number of pro-
nouns, the error is with the anaphora resolu-
tion algorithm and not with the parser em-
ployed. The breakdown of the number of sys-
tems that suggested each mistake for each pro-
noun can be seen in Table 11.9
</bodyText>
<table confidence="0.9975465">
# Systems 0 1 2 3 4 5
# Pronouns 288 71 59 54 48 153
</table>
<tableCaption confidence="0.998659">
Table 11: Number of Mistaken Systems
</tableCaption>
<bodyText confidence="0.939139192307692">
It is also interesting to see the number of dif-
ferent antecedents suggested by the anaphora
resolution algorithm using the various parsers
(Table 12). We can see that there is a ten-
8Systems attempt all pronouns which they are
given; pronouns were only removed if the correct an-
tecedent was wrongly tagged. Only about 10 pronouns
were removed in this way.
91n this table, 1 system means only one system chose
the wrong antecedent, etc.
dency to choose the same (potentially wrong)
antecedent, since there are no cases where all
versions of the Lappin and Leass algorithm
chose different antecedents (versus 153 times
all systems chose the wrong antecedent). The
number of times that only one antecedent
exists in the suggested answers is strikingly
high. However, this may be slightly mis-
leading, as a chosen pronominal antecedent
(e.g. in Mary... Shei. She2, She2 will resolve
to She&apos;) counts as identical whether or not
it refers to the same entity. In scoring the
anaphora resolution, if Shei was previously
wrongly resolved, She2 is also treated as an
error. This choice of evaluation method may
be having an impact on our overall accuracy.
</bodyText>
<table confidence="0.9987585">
# Antecedents 0 1 2 3 4 5
# Pronouns â€” 436 203 30 4 0
</table>
<tableCaption confidence="0.998084">
Table 12: Number of Different Antecedents
</tableCaption>
<sectionHeader confidence="0.98567" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999981791666667">
We have presented two evaluations of infor-
mation derived from full and shallow parsers.
The first compares the results of certain GRs
against a gold standard, and the second inves-
tigates the change in accuracy of an anaphora
resolution system when the parser is varied.
When the systems&apos; F-measures were com-
pared, we found that Buchholz&apos; GR finder out-
performed the conventional full parsers. This
is an interesting result, which shows that ac-
curate GRs can be obtained without the ex-
pense of constructing a full parse. The rank-
ing between the Penn Treebank parsers ob-
tained from the GR evaluation reflects the
ranking obtained from a direct parser compar-
ison (from Table 1).
In the task-based evaluation, the perfor-
mance gap between the anaphora resolution
algorithm using the various parsers narrowed.
This may be due to the anaphora resolution al-
gorithm making use of only certain instances
of GRs which are &apos;equally difficult&apos; for all
parsers to extract.
We expect the results of the anaphora res-
</bodyText>
<page confidence="0.986752">
297
</page>
<bodyText confidence="0.999912571428571">
olution experiment to be typical of parser ap-
plications that make use of a large number
of types of GRs. Future work is required to
evaluate parsers on applications that make use
of just a few types of GRs, for example se-
lectional preference based word sense disam-
biguation.
</bodyText>
<sectionHeader confidence="0.989961" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999506">
I would like to thank Sabine Buchholz for pro-
viding me with the output from her system.
My thanks also go to Ted Briscoe for his in-
sight into parsers, Ann Copestake and Joe
Hurd for reading previous drafts of this paper.
</bodyText>
<sectionHeader confidence="0.997812" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9996049">
D. Blaheta and E. Charniak. 2000. Assigning func-
tion tags to parsed text. In Proceedings of the
First Meeting of the North American Chapter of
the Association for Computational Linguistics,
pages 234-240.
E. J. Briscoe and J. Carroll. 1993. Gener-
alised probabilistic LR parsing of natural lan-
guage (corpora) with unification-based gram-
mars. Computational Linguistics, 19(1):25-60.
S. Buchholz, J. Veenstra, and W. Daelemans.
1999. Cascaded grammatical relation assign-
ment. In P. Fung and J. Zhou, editors, Proceed-
ings of the Joint SIGDAT Conference on Em-
pirical Methods in Natural Language Processing
and Very Large Corpora (EMNLP/VLC), pages
239-246.
S. Buchholz. 2002. Memory-Based Grammatical
Relation Finding. Ph.D. thesis, University of
Tilburg.
J. Carroll, E. Briscoe, and A. Sanfilippo. 1998.
Parser evaluation: A survey and a new proposal.
In Proceedings of the International Conference
on Language Resources and Evaluation, pages
447-454.
E. Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of NAACL-2000, pages
132-139.
S. Clark and J. Hockenmaier. 2002. Evaluating
a wide-coverage CCG parser. In Proceedings of
the LREC 2002 Beyond Parseval Workshop.
M. Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings
of the 35th Annual Meeting of the ACL (jointly
with the 8th Conference of the EACL), pages 16-
23.
W. Daelemans, J. Zavrel, P. Berck, and S. Gillis.
1996. MBT: A memory-based part of speech
tagger generator. In E. Ejerhed and I. Dagan,
editors, Proceedings of the 4th ACL/SIGDAT
Workshop on Very Large Corpora, pages 14-27.
W. Daelemans. 1996. Abstraction considered
harmful: Lazy learning of language processing.
In H. J. van den Herik and A. Weijiters, editors,
Proceedings of the Sixth Beligian-Dutch Confer-
ence on Machine Learning, pages 3-12.
D. Elworthy. 1994. Does Baum-Welch re-
estimation help taggers? In Proceedings of the
4th Conference on Applied NLP, pages 53-58.
S. Lappin and H. Leass. 1994. An algorithm
for pronominal anaphora resolution. Computa-
tional Linguistics, 20(4):535-561.
G. Leech. 1992. 100 million words of English: the
British National Corpus. Language Research,
28(1):1-13.
D. Lin. 1995. Dependency-based parser evalua-
tion: a study with a software manual corpus.
In R. Sutcliffe, H-D. Koch, and A. McEllingott,
editors, Industrial Parsing of Software Manuals,
pages 13-24.
M. Marcus, R. Santorini, and M. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn TreeBank. Computational Lin-
guisitcs, 19(2):313-330.
J. Preiss and E. Briscoe. 2003. Shallow or full
parsing for anaphora resolution? An experiment
with the Lappin and Leass algorithm. In Pro-
ceedings of the Workshop on Anaphora Resolu-
tion.
A. Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing, pages 133-142.
G. Sampson. 1995. English for the computer. Ox-
ford University Press.
J. Veenstra and A. van den Bosch. 2000. Single-
classifier memory-based phrase chunking. In
Proceedings of the Fourth Conference on Com-
putational Natural Language Learning (CoNLL)
and the Second Learning Language in Logic
Workshop (LLL), pages 157-159.
</reference>
<page confidence="0.997055">
298
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.836766">
<title confidence="0.999963">Using Grammatical Relations to Compare Parsers</title>
<author confidence="0.99629">Judita Preiss</author>
<affiliation confidence="0.999949">Computer Laboratory University of Cambridge</affiliation>
<email confidence="0.974972">Judita.PreissAcl.cam.ac.uk</email>
<abstract confidence="0.98904635">We use the grammatical relations (GRs) described in Carroll et al. (1998) to compare a number of parsing algorithms. A first ranking of the parsers is provided by comparing the extracted GRs to a gold standard GR annotation of 500 Susanne sentences: this required an implementation of GR extraction software for Penn Treebank style parsers. In addition, we perform an experiment using the extracted GRs as input to the Lappin and Leass (1994) anaphora resolution algorithm. This produces a second ranking of the parsers, and we investigate the number of errors that are caused by the incorrect GRs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Blaheta</author>
<author>E Charniak</author>
</authors>
<title>Assigning function tags to parsed text.</title>
<date>2000</date>
<booktitle>In Proceedings of the First Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>234--240</pages>
<contexts>
<context position="6295" citStr="Blaheta and Charniak (2000)" startWordPosition="1043" endWordPosition="1046">he chunker she employs (Veenstra and van den Bosch, 2000). Therefore, to carry out a GR comparison, we need to extract GRs from Penn Treebank style parses. We manually created rules which find the relevant heads and their dependants by traversing the parse tree (for example, the NP in a S NP VP rule gives an instance of the ncsubj relation). In cases where a distinction is difficult/impossible to make from a Penn Treebank tree (e.g. xcomp vs xmod), we sacrificed recall for precision and only encoded rules which cause as few misclassifications as possible.2 Similar work has been carried out by Blaheta and Charniak (2000) who used statistical methods to add function tags to Penn Treebank I style parses; however, as well as converting the tags into a Carroll et al. format, we would need to add extra rules to extract other GRs needed for our application described in Section 6. E.g. the direct object is not immediately apparent from Penn Treebank II tags. We restrict the GRs we extract from the Penn Treebank to those that are necessary for the anaphora resolution application (the object relations, the complement relations and the ncmod relation), and those that are a simple by-product of extracting the necessary </context>
</contexts>
<marker>Blaheta, Charniak, 2000</marker>
<rawString>D. Blaheta and E. Charniak. 2000. Assigning function tags to parsed text. In Proceedings of the First Meeting of the North American Chapter of the Association for Computational Linguistics, pages 234-240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E J Briscoe</author>
<author>J Carroll</author>
</authors>
<title>Generalised probabilistic LR parsing of natural language (corpora) with unification-based grammars.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--1</pages>
<contexts>
<context position="4361" citStr="Briscoe and Carroll (1993)" startWordPosition="719" endWordPosition="722">he (Sampson, 1995)e Journal portion of the Penn WSJ corpus of the Treebank (Marcus et al., 1993) Penn Treebank II &apos;Available from http: //www. cogs. susx.ac .uk/lab/n1p/rasp/ &apos;Available from ftp://ftp.cs.brown.edu/pub/n1parser/ &apos;Available from ftp: //f tp . cis . upenn . edu/pub/mcollins/misc/ dAvailable from http://www.cis.upenn.edu/-adwait/ &apos;Note that the Briscoe and Carroll grammar is manually created, and Susanne was used for development. Table 2: Parser Descriptions 2 Tools In this work we compare four full parsers from which GRs are extracted by walking over the trees. These parsers are Briscoe and Carroll (1993) (BC), Charniak (2000) (CH), model 1 and model 2 of Collins (1997) (Cl and C2).1- A summary of published performance results can be found in Table 1. We also include in our comparison a GR finder (Buchholz, 2002) (BU) based on a shallow parser (Daelemans, 1996), (Buchholz et al., 1999). Table 2 summarizes the &apos;Note that Collins&apos; model 1 and Collins&apos; model 2 are considered as two different parsers. grammar, the parsing algorithm, the tagger and the training corpus for all the parsers that we investigate. 3 Grammatical Relations Lin (1995) proposed an evaluation based on grammatical dependencies</context>
</contexts>
<marker>Briscoe, Carroll, 1993</marker>
<rawString>E. J. Briscoe and J. Carroll. 1993. Generalised probabilistic LR parsing of natural language (corpora) with unification-based grammars. Computational Linguistics, 19(1):25-60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Buchholz</author>
<author>J Veenstra</author>
<author>W Daelemans</author>
</authors>
<title>Cascaded grammatical relation assignment.</title>
<date>1999</date>
<booktitle>Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC),</booktitle>
<pages>239--246</pages>
<editor>In P. Fung and J. Zhou, editors,</editor>
<contexts>
<context position="4647" citStr="Buchholz et al., 1999" startWordPosition="769" endWordPosition="773">isc/ dAvailable from http://www.cis.upenn.edu/-adwait/ &apos;Note that the Briscoe and Carroll grammar is manually created, and Susanne was used for development. Table 2: Parser Descriptions 2 Tools In this work we compare four full parsers from which GRs are extracted by walking over the trees. These parsers are Briscoe and Carroll (1993) (BC), Charniak (2000) (CH), model 1 and model 2 of Collins (1997) (Cl and C2).1- A summary of published performance results can be found in Table 1. We also include in our comparison a GR finder (Buchholz, 2002) (BU) based on a shallow parser (Daelemans, 1996), (Buchholz et al., 1999). Table 2 summarizes the &apos;Note that Collins&apos; model 1 and Collins&apos; model 2 are considered as two different parsers. grammar, the parsing algorithm, the tagger and the training corpus for all the parsers that we investigate. 3 Grammatical Relations Lin (1995) proposed an evaluation based on grammatical dependencies, in which syntactic dependencies are described between heads and their dependents. This work was extended by Carroll et al. (1998), and it is this specification called grammatical relations which we employ in our work. An example, for the sentence John gave Mary the book, can be seen </context>
</contexts>
<marker>Buchholz, Veenstra, Daelemans, 1999</marker>
<rawString>S. Buchholz, J. Veenstra, and W. Daelemans. 1999. Cascaded grammatical relation assignment. In P. Fung and J. Zhou, editors, Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC), pages 239-246.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Buchholz</author>
</authors>
<title>Memory-Based Grammatical Relation Finding.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Tilburg.</institution>
<contexts>
<context position="4573" citStr="Buchholz, 2002" startWordPosition="759" endWordPosition="760">ser/ &apos;Available from ftp: //f tp . cis . upenn . edu/pub/mcollins/misc/ dAvailable from http://www.cis.upenn.edu/-adwait/ &apos;Note that the Briscoe and Carroll grammar is manually created, and Susanne was used for development. Table 2: Parser Descriptions 2 Tools In this work we compare four full parsers from which GRs are extracted by walking over the trees. These parsers are Briscoe and Carroll (1993) (BC), Charniak (2000) (CH), model 1 and model 2 of Collins (1997) (Cl and C2).1- A summary of published performance results can be found in Table 1. We also include in our comparison a GR finder (Buchholz, 2002) (BU) based on a shallow parser (Daelemans, 1996), (Buchholz et al., 1999). Table 2 summarizes the &apos;Note that Collins&apos; model 1 and Collins&apos; model 2 are considered as two different parsers. grammar, the parsing algorithm, the tagger and the training corpus for all the parsers that we investigate. 3 Grammatical Relations Lin (1995) proposed an evaluation based on grammatical dependencies, in which syntactic dependencies are described between heads and their dependents. This work was extended by Carroll et al. (1998), and it is this specification called grammatical relations which we employ in ou</context>
</contexts>
<marker>Buchholz, 2002</marker>
<rawString>S. Buchholz. 2002. Memory-Based Grammatical Relation Finding. Ph.D. thesis, University of Tilburg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carroll</author>
<author>E Briscoe</author>
<author>A Sanfilippo</author>
</authors>
<title>Parser evaluation: A survey and a new proposal.</title>
<date>1998</date>
<booktitle>In Proceedings of the International Conference on Language Resources and Evaluation,</booktitle>
<pages>447--454</pages>
<contexts>
<context position="1562" citStr="Carroll et al., 1998" startWordPosition="247" endWordPosition="250">is usually difficult to compare performance of different style parsers, as the output trees can vary in structure. In this paper, we use GRs to provide a common basis for comparing full and shallow parsers, and Penn Treebank and Susanne structures. To carry out this comparison, we implemented a GR extraction mechanism for Penn Treebank This work was supported by UK EPSRC project GR/N36462/93 &apos;Robust Accurate Statistical Parsing&apos;. parses. Evaluating parsers using GRs as opposed to crossing brackets or labelled precision/recall metrics can be argued to give a more robust measure of performance (Carroll et al., 1998), (Clark and Hockenmaier, 2002). The main novelty of this paper is the use of the Carroll et al&apos;s GR evaluation method to compare the Collins model 1 and model 2, and Charniak parsers. An initial evaluation is provided by comparing the extracted GRs to a gold standard GR annotation of 500 Susanne sentences due to Carroll et al. To gain insight into the strengths and weaknesses of the different parsers, we present a breakdown of the results for each type of GR. It is not clear whether the ranking produced from the gold standard evaluation is representative: there may be corpus effects for parse</context>
<context position="5092" citStr="Carroll et al. (1998)" startWordPosition="839" endWordPosition="842">rmance results can be found in Table 1. We also include in our comparison a GR finder (Buchholz, 2002) (BU) based on a shallow parser (Daelemans, 1996), (Buchholz et al., 1999). Table 2 summarizes the &apos;Note that Collins&apos; model 1 and Collins&apos; model 2 are considered as two different parsers. grammar, the parsing algorithm, the tagger and the training corpus for all the parsers that we investigate. 3 Grammatical Relations Lin (1995) proposed an evaluation based on grammatical dependencies, in which syntactic dependencies are described between heads and their dependents. This work was extended by Carroll et al. (1998), and it is this specification called grammatical relations which we employ in our work. An example, for the sentence John gave Mary the book, can be seen in Figure 1. Both the Briscoe and Carroll parser and 292 Sentence: John gave Mary the book. Grammatical relations: (ncsubj gave John) (dobj gave Mary) (obj2 gave book) (detmod book the) Figure 1: Sample GR output Buchholz&apos;s GR finder already output GRs in the desired format. Although Buchholz&apos;s work has focused mainly on extracting relations involving verbs, some non-verb relations (e.g. detmod) are also produced by the chunker she employs (</context>
</contexts>
<marker>Carroll, Briscoe, Sanfilippo, 1998</marker>
<rawString>J. Carroll, E. Briscoe, and A. Sanfilippo. 1998. Parser evaluation: A survey and a new proposal. In Proceedings of the International Conference on Language Resources and Evaluation, pages 447-454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of NAACL-2000,</booktitle>
<pages>132--139</pages>
<contexts>
<context position="4383" citStr="Charniak (2000)" startWordPosition="724" endWordPosition="725">on of the Penn WSJ corpus of the Treebank (Marcus et al., 1993) Penn Treebank II &apos;Available from http: //www. cogs. susx.ac .uk/lab/n1p/rasp/ &apos;Available from ftp://ftp.cs.brown.edu/pub/n1parser/ &apos;Available from ftp: //f tp . cis . upenn . edu/pub/mcollins/misc/ dAvailable from http://www.cis.upenn.edu/-adwait/ &apos;Note that the Briscoe and Carroll grammar is manually created, and Susanne was used for development. Table 2: Parser Descriptions 2 Tools In this work we compare four full parsers from which GRs are extracted by walking over the trees. These parsers are Briscoe and Carroll (1993) (BC), Charniak (2000) (CH), model 1 and model 2 of Collins (1997) (Cl and C2).1- A summary of published performance results can be found in Table 1. We also include in our comparison a GR finder (Buchholz, 2002) (BU) based on a shallow parser (Daelemans, 1996), (Buchholz et al., 1999). Table 2 summarizes the &apos;Note that Collins&apos; model 1 and Collins&apos; model 2 are considered as two different parsers. grammar, the parsing algorithm, the tagger and the training corpus for all the parsers that we investigate. 3 Grammatical Relations Lin (1995) proposed an evaluation based on grammatical dependencies, in which syntactic d</context>
<context position="6295" citStr="Charniak (2000)" startWordPosition="1045" endWordPosition="1046">he employs (Veenstra and van den Bosch, 2000). Therefore, to carry out a GR comparison, we need to extract GRs from Penn Treebank style parses. We manually created rules which find the relevant heads and their dependants by traversing the parse tree (for example, the NP in a S NP VP rule gives an instance of the ncsubj relation). In cases where a distinction is difficult/impossible to make from a Penn Treebank tree (e.g. xcomp vs xmod), we sacrificed recall for precision and only encoded rules which cause as few misclassifications as possible.2 Similar work has been carried out by Blaheta and Charniak (2000) who used statistical methods to add function tags to Penn Treebank I style parses; however, as well as converting the tags into a Carroll et al. format, we would need to add extra rules to extract other GRs needed for our application described in Section 6. E.g. the direct object is not immediately apparent from Penn Treebank II tags. We restrict the GRs we extract from the Penn Treebank to those that are necessary for the anaphora resolution application (the object relations, the complement relations and the ncmod relation), and those that are a simple by-product of extracting the necessary </context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>E. Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of NAACL-2000, pages 132-139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Clark</author>
<author>J Hockenmaier</author>
</authors>
<title>Evaluating a wide-coverage CCG parser.</title>
<date>2002</date>
<booktitle>In Proceedings of the LREC 2002 Beyond Parseval Workshop.</booktitle>
<contexts>
<context position="1593" citStr="Clark and Hockenmaier, 2002" startWordPosition="251" endWordPosition="254">compare performance of different style parsers, as the output trees can vary in structure. In this paper, we use GRs to provide a common basis for comparing full and shallow parsers, and Penn Treebank and Susanne structures. To carry out this comparison, we implemented a GR extraction mechanism for Penn Treebank This work was supported by UK EPSRC project GR/N36462/93 &apos;Robust Accurate Statistical Parsing&apos;. parses. Evaluating parsers using GRs as opposed to crossing brackets or labelled precision/recall metrics can be argued to give a more robust measure of performance (Carroll et al., 1998), (Clark and Hockenmaier, 2002). The main novelty of this paper is the use of the Carroll et al&apos;s GR evaluation method to compare the Collins model 1 and model 2, and Charniak parsers. An initial evaluation is provided by comparing the extracted GRs to a gold standard GR annotation of 500 Susanne sentences due to Carroll et al. To gain insight into the strengths and weaknesses of the different parsers, we present a breakdown of the results for each type of GR. It is not clear whether the ranking produced from the gold standard evaluation is representative: there may be corpus effects for parsers not trained on Susanne, and </context>
</contexts>
<marker>Clark, Hockenmaier, 2002</marker>
<rawString>S. Clark and J. Hockenmaier. 2002. Evaluating a wide-coverage CCG parser. In Proceedings of the LREC 2002 Beyond Parseval Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the ACL (jointly with the 8th Conference of the EACL),</booktitle>
<pages>16--23</pages>
<contexts>
<context position="4427" citStr="Collins (1997)" startWordPosition="733" endWordPosition="734">rcus et al., 1993) Penn Treebank II &apos;Available from http: //www. cogs. susx.ac .uk/lab/n1p/rasp/ &apos;Available from ftp://ftp.cs.brown.edu/pub/n1parser/ &apos;Available from ftp: //f tp . cis . upenn . edu/pub/mcollins/misc/ dAvailable from http://www.cis.upenn.edu/-adwait/ &apos;Note that the Briscoe and Carroll grammar is manually created, and Susanne was used for development. Table 2: Parser Descriptions 2 Tools In this work we compare four full parsers from which GRs are extracted by walking over the trees. These parsers are Briscoe and Carroll (1993) (BC), Charniak (2000) (CH), model 1 and model 2 of Collins (1997) (Cl and C2).1- A summary of published performance results can be found in Table 1. We also include in our comparison a GR finder (Buchholz, 2002) (BU) based on a shallow parser (Daelemans, 1996), (Buchholz et al., 1999). Table 2 summarizes the &apos;Note that Collins&apos; model 1 and Collins&apos; model 2 are considered as two different parsers. grammar, the parsing algorithm, the tagger and the training corpus for all the parsers that we investigate. 3 Grammatical Relations Lin (1995) proposed an evaluation based on grammatical dependencies, in which syntactic dependencies are described between heads and </context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>M. Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proceedings of the 35th Annual Meeting of the ACL (jointly with the 8th Conference of the EACL), pages 16-23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>J Zavrel</author>
<author>P Berck</author>
<author>S Gillis</author>
</authors>
<title>MBT: A memory-based part of speech tagger generator.</title>
<date>1996</date>
<booktitle>Proceedings of the 4th ACL/SIGDAT Workshop on Very Large Corpora,</booktitle>
<pages>14--27</pages>
<editor>In E. Ejerhed and I. Dagan, editors,</editor>
<marker>Daelemans, Zavrel, Berck, Gillis, 1996</marker>
<rawString>W. Daelemans, J. Zavrel, P. Berck, and S. Gillis. 1996. MBT: A memory-based part of speech tagger generator. In E. Ejerhed and I. Dagan, editors, Proceedings of the 4th ACL/SIGDAT Workshop on Very Large Corpora, pages 14-27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
</authors>
<title>Abstraction considered harmful: Lazy learning of language processing.</title>
<date>1996</date>
<booktitle>Proceedings of the Sixth Beligian-Dutch Conference on Machine Learning,</booktitle>
<pages>3--12</pages>
<editor>In H. J. van den Herik and A. Weijiters, editors,</editor>
<contexts>
<context position="4622" citStr="Daelemans, 1996" startWordPosition="767" endWordPosition="768"> edu/pub/mcollins/misc/ dAvailable from http://www.cis.upenn.edu/-adwait/ &apos;Note that the Briscoe and Carroll grammar is manually created, and Susanne was used for development. Table 2: Parser Descriptions 2 Tools In this work we compare four full parsers from which GRs are extracted by walking over the trees. These parsers are Briscoe and Carroll (1993) (BC), Charniak (2000) (CH), model 1 and model 2 of Collins (1997) (Cl and C2).1- A summary of published performance results can be found in Table 1. We also include in our comparison a GR finder (Buchholz, 2002) (BU) based on a shallow parser (Daelemans, 1996), (Buchholz et al., 1999). Table 2 summarizes the &apos;Note that Collins&apos; model 1 and Collins&apos; model 2 are considered as two different parsers. grammar, the parsing algorithm, the tagger and the training corpus for all the parsers that we investigate. 3 Grammatical Relations Lin (1995) proposed an evaluation based on grammatical dependencies, in which syntactic dependencies are described between heads and their dependents. This work was extended by Carroll et al. (1998), and it is this specification called grammatical relations which we employ in our work. An example, for the sentence John gave Ma</context>
</contexts>
<marker>Daelemans, 1996</marker>
<rawString>W. Daelemans. 1996. Abstraction considered harmful: Lazy learning of language processing. In H. J. van den Herik and A. Weijiters, editors, Proceedings of the Sixth Beligian-Dutch Conference on Machine Learning, pages 3-12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Elworthy</author>
</authors>
<title>Does Baum-Welch reestimation help taggers?</title>
<date>1994</date>
<booktitle>In Proceedings of the 4th Conference on Applied NLP,</booktitle>
<pages>53--58</pages>
<contexts>
<context position="3637" citStr="Elworthy, 1994" startWordPosition="613" endWordPosition="614"> 0.74 70.1 89.6 Cl WSJ 87.52 87.92 0.96 64.86 86.19 C2 WSJ 88.07 88.35 0.95 65.84 86.64 sentences &lt; 100 words CH WSJ 89.6 89.5 0.88 67.6 87.7 Cl WSJ 87.01 87.41 1.11 62.17 83.86 C2 WSJ 87.60 87.89 1.09 63.20 84.60 BC evaluation BC Susanne 74.0 73.0 1.03 59.6 - Table 1: Summary of Published Results (LR = labelled recall, LP = labelled precision, CB = crossing brackets) BC a CHb Cl &amp; C2&apos; BU Grammar Unification-based, Generative, Generative, N/A PoS and punct 3rd order 0th order labels Algorithm LR parser Chart parser Shallow parser Tagger Acquilex (CLAWS- own Ratnaparkhi Memory-based (DaeleII) (Elworthy, 1994) (1996).d mans et al., 1996) Training Susanne Sections 2-21 of the Wall Street Sections 10-19 of the (Sampson, 1995)e Journal portion of the Penn WSJ corpus of the Treebank (Marcus et al., 1993) Penn Treebank II &apos;Available from http: //www. cogs. susx.ac .uk/lab/n1p/rasp/ &apos;Available from ftp://ftp.cs.brown.edu/pub/n1parser/ &apos;Available from ftp: //f tp . cis . upenn . edu/pub/mcollins/misc/ dAvailable from http://www.cis.upenn.edu/-adwait/ &apos;Note that the Briscoe and Carroll grammar is manually created, and Susanne was used for development. Table 2: Parser Descriptions 2 Tools In this work we co</context>
</contexts>
<marker>Elworthy, 1994</marker>
<rawString>D. Elworthy. 1994. Does Baum-Welch reestimation help taggers? In Proceedings of the 4th Conference on Applied NLP, pages 53-58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Lappin</author>
<author>H Leass</author>
</authors>
<title>An algorithm for pronominal anaphora resolution.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<pages>20--4</pages>
<contexts>
<context position="2344" citStr="Lappin and Leass (1994)" startWordPosition="385" endWordPosition="388"> 2, and Charniak parsers. An initial evaluation is provided by comparing the extracted GRs to a gold standard GR annotation of 500 Susanne sentences due to Carroll et al. To gain insight into the strengths and weaknesses of the different parsers, we present a breakdown of the results for each type of GR. It is not clear whether the ranking produced from the gold standard evaluation is representative: there may be corpus effects for parsers not trained on Susanne, and real life applications may not reflect this ranking. We therefore perform an experiment using the extracted GRs as input to the Lappin and Leass (1994) anaphora resolution algorithm. This produces a second ranking of the parsers, and we investigate the number of errors that are caused by incorrect GRs. We describe the parsers and the GR finder in Section 2. We introduce GRs in Section 3 and briefly describe our GR extraction software for Penn Treebank style parses. The evaluation, including a description of the evaluation corpus and performance results, is presented in Section 4. The results are analyzed in Section 5 and a performance comparison in the context of anaphora resolution is presented in Section 6. We draw our conclusions in Secti</context>
<context position="14333" citStr="Lappin and Leass (1994)" startWordPosition="2508" endWordPosition="2511"> in the case of this relation, we have sacrificed recall for precision. 6 Anaphora Resolution We investigate the effect of using different parsers in an anaphora resolution system. This will indicate the impact of a change in parser performance on a real task: although one parser may have a marginally higher precision than another on a particular evaluation corpus, it is not clear whether this will be reflected by the results of a system which makes use of this parser, and which may work on a different corpus. 6.1 Lappin and Leass We choose to re-implement a non-probabilistic algorithm due to Lappin and Leass (1994), 295 because this anaphora resolution algorithm can be encoded in terms of the GR information (Preiss and Briscoe, 2003). For each pronoun, this algorithm uses syntactic criteria to rule out noun phrases that cannot possibly corefer with it. An antecedent is then chosen according to a ranking based on salience weights. For all pronouns, noun phrases are ruled out if they have incompatible agreement features. Pronouns are split into two classes, lexical (reflexives and reciprocals) and non-lexical anaphors. There are additional syntactic filters for both of the two types of anaphors. (ncsubj l</context>
</contexts>
<marker>Lappin, Leass, 1994</marker>
<rawString>S. Lappin and H. Leass. 1994. An algorithm for pronominal anaphora resolution. Computational Linguistics, 20(4):535-561.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Leech</author>
</authors>
<title>100 million words of English:</title>
<date>1992</date>
<journal>the British National Corpus. Language Research,</journal>
<pages>28--1</pages>
<contexts>
<context position="16594" citStr="Leech, 1992" startWordPosition="2886" endWordPosition="2887">+ 50 = 310 The weights are scaled by a factor of () where s is the distance (number of sentences) of the candidate from the pronoun. The candidate with the highest salience is proposed as the antecedent. 6.2 Using GR Information The algorithm uses GR information at two points: initially, it is used to eliminate certain intrasentential candidates from the candidates list. For example, in the sentence She likes her, she and her cannot corefer, which is expressed by a shared head in the following GRs: For this experiment, we use an anaphorically resolved 2400 sentence initial segment of the BNC (Leech, 1992), which we split into five segments containing roughly equal numbers of pronouns. The number of sentences and pronouns in each of the five segments is presented in Table 8. BC BU CH Cl C2 1 60.45 63.43 62.69 62.69 61.19 2 50.86 52.59 54.31 55.17 54.31 3 69.93 69.93 69.28 67.32 69.28 4 67.41 65.19 69.63 63.70 66.67 5 54.81 52.59 50.37 51.85 51.85 tt 60.69 60.75 61.26 60.15 60.66 o-2 52.36 48.87 60.66 32.83 45.73 Table 9: Anaphora Results The results of the Lappin and Leass anaphora resolution algorithm using each of the parsers are presented in Table 9.7 The &apos;algorithms&apos; are only evaluated on p</context>
</contexts>
<marker>Leech, 1992</marker>
<rawString>G. Leech. 1992. 100 million words of English: the British National Corpus. Language Research, 28(1):1-13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Dependency-based parser evaluation: a study with a software manual corpus. In</title>
<date>1995</date>
<booktitle>Industrial Parsing of Software Manuals,</booktitle>
<pages>13--24</pages>
<editor>R. Sutcliffe, H-D. Koch, and A. McEllingott, editors,</editor>
<contexts>
<context position="4904" citStr="Lin (1995)" startWordPosition="813" endWordPosition="814">alking over the trees. These parsers are Briscoe and Carroll (1993) (BC), Charniak (2000) (CH), model 1 and model 2 of Collins (1997) (Cl and C2).1- A summary of published performance results can be found in Table 1. We also include in our comparison a GR finder (Buchholz, 2002) (BU) based on a shallow parser (Daelemans, 1996), (Buchholz et al., 1999). Table 2 summarizes the &apos;Note that Collins&apos; model 1 and Collins&apos; model 2 are considered as two different parsers. grammar, the parsing algorithm, the tagger and the training corpus for all the parsers that we investigate. 3 Grammatical Relations Lin (1995) proposed an evaluation based on grammatical dependencies, in which syntactic dependencies are described between heads and their dependents. This work was extended by Carroll et al. (1998), and it is this specification called grammatical relations which we employ in our work. An example, for the sentence John gave Mary the book, can be seen in Figure 1. Both the Briscoe and Carroll parser and 292 Sentence: John gave Mary the book. Grammatical relations: (ncsubj gave John) (dobj gave Mary) (obj2 gave book) (detmod book the) Figure 1: Sample GR output Buchholz&apos;s GR finder already output GRs in t</context>
</contexts>
<marker>Lin, 1995</marker>
<rawString>D. Lin. 1995. Dependency-based parser evaluation: a study with a software manual corpus. In R. Sutcliffe, H-D. Koch, and A. McEllingott, editors, Industrial Parsing of Software Manuals, pages 13-24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>R Santorini</author>
<author>M Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn TreeBank. Computational Linguisitcs,</title>
<date>1993</date>
<pages>19--2</pages>
<contexts>
<context position="3831" citStr="Marcus et al., 1993" startWordPosition="644" endWordPosition="647">0 87.89 1.09 63.20 84.60 BC evaluation BC Susanne 74.0 73.0 1.03 59.6 - Table 1: Summary of Published Results (LR = labelled recall, LP = labelled precision, CB = crossing brackets) BC a CHb Cl &amp; C2&apos; BU Grammar Unification-based, Generative, Generative, N/A PoS and punct 3rd order 0th order labels Algorithm LR parser Chart parser Shallow parser Tagger Acquilex (CLAWS- own Ratnaparkhi Memory-based (DaeleII) (Elworthy, 1994) (1996).d mans et al., 1996) Training Susanne Sections 2-21 of the Wall Street Sections 10-19 of the (Sampson, 1995)e Journal portion of the Penn WSJ corpus of the Treebank (Marcus et al., 1993) Penn Treebank II &apos;Available from http: //www. cogs. susx.ac .uk/lab/n1p/rasp/ &apos;Available from ftp://ftp.cs.brown.edu/pub/n1parser/ &apos;Available from ftp: //f tp . cis . upenn . edu/pub/mcollins/misc/ dAvailable from http://www.cis.upenn.edu/-adwait/ &apos;Note that the Briscoe and Carroll grammar is manually created, and Susanne was used for development. Table 2: Parser Descriptions 2 Tools In this work we compare four full parsers from which GRs are extracted by walking over the trees. These parsers are Briscoe and Carroll (1993) (BC), Charniak (2000) (CH), model 1 and model 2 of Collins (1997) (Cl</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. Marcus, R. Santorini, and M. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn TreeBank. Computational Linguisitcs, 19(2):313-330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Preiss</author>
<author>E Briscoe</author>
</authors>
<title>Shallow or full parsing for anaphora resolution? An experiment with the Lappin and Leass algorithm.</title>
<date>2003</date>
<booktitle>In Proceedings of the Workshop on Anaphora Resolution.</booktitle>
<contexts>
<context position="14454" citStr="Preiss and Briscoe, 2003" startWordPosition="2527" endWordPosition="2530"> of using different parsers in an anaphora resolution system. This will indicate the impact of a change in parser performance on a real task: although one parser may have a marginally higher precision than another on a particular evaluation corpus, it is not clear whether this will be reflected by the results of a system which makes use of this parser, and which may work on a different corpus. 6.1 Lappin and Leass We choose to re-implement a non-probabilistic algorithm due to Lappin and Leass (1994), 295 because this anaphora resolution algorithm can be encoded in terms of the GR information (Preiss and Briscoe, 2003). For each pronoun, this algorithm uses syntactic criteria to rule out noun phrases that cannot possibly corefer with it. An antecedent is then chosen according to a ranking based on salience weights. For all pronouns, noun phrases are ruled out if they have incompatible agreement features. Pronouns are split into two classes, lexical (reflexives and reciprocals) and non-lexical anaphors. There are additional syntactic filters for both of the two types of anaphors. (ncsubj like she) (dobj like her) Secondly, GR information is used for obtaining salience values. In the above sentence, we would </context>
</contexts>
<marker>Preiss, Briscoe, 2003</marker>
<rawString>J. Preiss and E. Briscoe. 2003. Shallow or full parsing for anaphora resolution? An experiment with the Lappin and Leass algorithm. In Proceedings of the Workshop on Anaphora Resolution.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A maximum entropy model for part-of-speech tagging.</title>
<date>1996</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>133--142</pages>
<marker>Ratnaparkhi, 1996</marker>
<rawString>A. Ratnaparkhi. 1996. A maximum entropy model for part-of-speech tagging. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 133-142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Sampson</author>
</authors>
<title>English for the computer.</title>
<date>1995</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="3753" citStr="Sampson, 1995" startWordPosition="632" endWordPosition="633">89.6 89.5 0.88 67.6 87.7 Cl WSJ 87.01 87.41 1.11 62.17 83.86 C2 WSJ 87.60 87.89 1.09 63.20 84.60 BC evaluation BC Susanne 74.0 73.0 1.03 59.6 - Table 1: Summary of Published Results (LR = labelled recall, LP = labelled precision, CB = crossing brackets) BC a CHb Cl &amp; C2&apos; BU Grammar Unification-based, Generative, Generative, N/A PoS and punct 3rd order 0th order labels Algorithm LR parser Chart parser Shallow parser Tagger Acquilex (CLAWS- own Ratnaparkhi Memory-based (DaeleII) (Elworthy, 1994) (1996).d mans et al., 1996) Training Susanne Sections 2-21 of the Wall Street Sections 10-19 of the (Sampson, 1995)e Journal portion of the Penn WSJ corpus of the Treebank (Marcus et al., 1993) Penn Treebank II &apos;Available from http: //www. cogs. susx.ac .uk/lab/n1p/rasp/ &apos;Available from ftp://ftp.cs.brown.edu/pub/n1parser/ &apos;Available from ftp: //f tp . cis . upenn . edu/pub/mcollins/misc/ dAvailable from http://www.cis.upenn.edu/-adwait/ &apos;Note that the Briscoe and Carroll grammar is manually created, and Susanne was used for development. Table 2: Parser Descriptions 2 Tools In this work we compare four full parsers from which GRs are extracted by walking over the trees. These parsers are Briscoe and Carrol</context>
</contexts>
<marker>Sampson, 1995</marker>
<rawString>G. Sampson. 1995. English for the computer. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Veenstra</author>
<author>A van den Bosch</author>
</authors>
<title>Singleclassifier memory-based phrase chunking.</title>
<date>2000</date>
<booktitle>In Proceedings of the Fourth Conference on Computational Natural Language Learning (CoNLL) and the Second Learning Language in Logic Workshop (LLL),</booktitle>
<pages>157--159</pages>
<marker>Veenstra, van den Bosch, 2000</marker>
<rawString>J. Veenstra and A. van den Bosch. 2000. Singleclassifier memory-based phrase chunking. In Proceedings of the Fourth Conference on Computational Natural Language Learning (CoNLL) and the Second Learning Language in Logic Workshop (LLL), pages 157-159.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>