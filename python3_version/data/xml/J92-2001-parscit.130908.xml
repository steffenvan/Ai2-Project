<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.98847">
Inheritance in Word Grammar
</title>
<author confidence="0.999421">
Norman M. Fraser Richard A. Hudsont
</author>
<affiliation confidence="0.999116">
University of Surrey University College London
</affiliation>
<bodyText confidence="0.99919">
This paper describes the central role played by default inheritance in Word Grammar, a theory
of language knowledge and processing. A single formalism is used to represent knowledge at the
levels of morphology, syntax, and semantics. A single rule of inference is used to inherit knowledge
at all of these levels. This rule is distinctive in that it requires defaults to be explicitly overridden
in the case of exceptions. The explicit overriding rule is used in syntax to achieve what other
theories achieve by means of transformations, metarules, or lexical rules.
</bodyText>
<sectionHeader confidence="0.99437" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.99995024">
Since the scientific study of language first began, a central concern of linguists has
been the identification of linguistic generalizations and, where necessary, the stating
of exceptions to these generalizations.&apos; However, it is only within the last few years
that linguists have begun to think of this process in terms of the construction of de-
fault inheritance hierarchies. This new way of envisaging old problems is attractive
for at least three reasons. Firstly, it encourages linguists to be explicit not just about
the relations that hold between individuals and classes, but also about the relations
that hold between different classes. For example, where the nouns of a language have
traditionally been assigned to some number of distinct morphological paradigms, the
default inheritance approach encourages the morphologist to pay attention to gener-
alizations that cut across paradigms. If these generalizations are inherited, then there
must be some shared super class to inherit from, and the system of word classes and
paradigms must be designed accordingly.
Secondly, whereas generalizations have traditionally been class-based, in the in-
heritance approach they are based on typical cases and their features, any of which
may be overridden. Thus the shading from core members of a class to peripheral
members can be accommodated—indeed, the existence of peripheral members is pre-
dicted by the mechanism for overriding defaults. The third and more pragmatic reason
why it is useful to recast well-known linguistic problems in terms of default inheri-
tance is that there is a fairly well-developed—though by no means conclusive—body
of knowledge on the subject in the artificial intelligence field of knowledge represen-
tation (e.g. Etherington and Reiter 1983; Brachman 1985; Touretzky 1986; Etherington
1988). Nearer the computer science mainstream, work in object-oriented programming
languages (Cook 1989) offers an interesting range of relevant insights and inheritance-
based tools.
</bodyText>
<footnote confidence="0.90450925">
* Social and Computer Sciences Research Group, University of Surrey, Guildford, Surrey, GU2 5XH,
United Kingdom. E-mail: norman@soc.surrey.ac.uk.
t Department of Phonetics and Linguistics, University College London, Gower Street, London, WC1E
6BT, United Kingdom. E-mail: thudson@ucl.ac.uk.
1 We received very helpful comments on an earlier draft of this paper from three anonymous
Computational Linguistics readers, to whom we are most grateful. We also benefitted from discussions
with participants at the Workshop on Inheritance in Natural Language Processing, Tilburg, August
1990.
</footnote>
<note confidence="0.8433655">
C) 1992 Association for Computational Linguistics
Computational Linguistics Volume 18, Number 2
</note>
<bodyText confidence="0.99988836">
In recent years, linguists and computational linguists in particular have begun
to explore problems at most linguistic levels within inheritance frameworks. For ex-
ample, Gibbon and Reinhard have proposed inheritance-based solutions to problems
of phonology and prosody (Gibbon 1990; Reinhard and Gibbon 1991). Most work to
date has centered on morphology (e.g. De Smedt 1984; Flickinger, Pollard, and Wa-
sow 1985; Daelemans 1987; Calder 1989). A certain amount has also been achieved
in syntax (e.g. De Smedt 1984; Flickinger 1987), where inheritance is used to con-
struct subcategorization frames for words. As for semantics, there has been a great
deal of work on inheritance in so-called &apos;semantic networks,&apos; but much of this work
relates only loosely to the semantics of natural language. The work we present in
this paper differs from all previous work in natural language processing (NLP) in at
least two respects. Firstly, it is distinctive in the extent to which inheritance is used.
Within our framework knowledge at all levels (morphology, syntax, semantics, world
knowledge) is integrated in a single inheritance hierarchy. Indeed, given the extent of
integration, some of these level distinctions must be regarded as arbitrary. Secondly, it
is distinctive in the purposes for which inheritance is used. The canonical application
of inheritance in NLP is lexicon construction. Our system uses inheritance for this
purpose but it also makes inheritance play a vital role in the building of structure
during parsing.
What we describe is part of a theory of language (knowledge and processing)
called Word Grammar (WG) (Hudson 1984; 1990). Section 2 introduces the knowledge
representation language used in WG. Section 3 outlines the use of inheritance in WG
to describe the facts of syntax and semantics. Concluding observations are drawn in
Section 4. An Appendix sets out a fragment of English grammar and a simple sentence
analysis.
</bodyText>
<sectionHeader confidence="0.825182" genericHeader="keywords">
2. Word Grammar
</sectionHeader>
<bodyText confidence="0.999582">
In this section we define the syntax of WG propositions and explain how they can be
interpreted. The Appendix contains a fragment of English grammar from which all
examples are drawn.
</bodyText>
<subsectionHeader confidence="0.993086">
2.1 Propositions
</subsectionHeader>
<bodyText confidence="0.999553375">
One of the central claims of WG is that knowledge of language is a sub-component of
knowledge in general, and in particular that it is a kind of propositional knowledge
(which we assume must be distinguished from other kinds of knowledge, notably per-
ceptual knowledge). This amounts to the rather uncontroversial claim that all linguistic
knowledge may be expressed in terms of propositions (just as it can be expressed in
terms of attribute-value structures). This is uncontroversial because it is obviously
possible to represent any standard linguistic structure or rule as a collection of propo-
sitions, though the same is probably not true for representations of faces, sounds and
so on, which are based more directly on perception. The use of propositions to repre-
sent linguistic knowledge allows us to use standard logical operations as the basis for
parsing. One set of propositions defines the observable properties of the input, and
parsing consists of drawing inferences. These inferences constitute the analysis of the
input, so, as in unification-based systems, the formal properties of sentence structure
are the same as those of a grammar.
Propositions are of two types, positive and negative. A positive proposition con-
sists of a predicate and two arguments. By convention, an infix notation is used:
</bodyText>
<footnote confidence="0.304679">
(1a) noun isa word.
</footnote>
<page confidence="0.994224">
134
</page>
<note confidence="0.776097">
Norman M. Fraser and Richard A. Hudson Inheritance in Word Grammar
(lb) verb has (1 finiteness).
(1c) (stem of JUMP) = &lt;jump&gt;.
</note>
<bodyText confidence="0.987502">
The parentheses are generally redundant, so in later examples we shall omit them.
As we shall see, the arguments are usually somewhat more complex than in these
examples.
A negative proposition consists of &apos;NOT:&apos; followed by another proposition:
</bodyText>
<listItem confidence="0.9374635">
(2a) NOT: tensed verb has 0 subject.
(2b) NOT: position of predependent of word = after it.
A negated proposition may itself be negated:
(3) NOT: NOT: position of subject of v+s verb = after it.
</listItem>
<bodyText confidence="0.9124735">
Negated propositions play a crucial role in the WG system for default inheritance, as
we shall explain below.
</bodyText>
<subsectionHeader confidence="0.998268">
2.2 Predicates
</subsectionHeader>
<bodyText confidence="0.999604666666667">
Three different predicates are recognized:2 &apos;isa,&apos; =,&apos; and &apos;has.&apos;
The &apos;isa&apos; predicate is used to encode the relationship between a subtype and a
supertype in a type hierarchy. This predicate is used to express both the relationship
between instances and types (such as the well-known relationship of &apos;Clyde&apos; to &apos;ele-
phant&apos;) and the relationship between types and supertypes (such as the relationship
of &apos;elephant&apos; to &apos;mammal&apos;). Instances and types are collectively known as concepts.
</bodyText>
<listItem confidence="0.677827">
(4a) Clyde isa elephant.
(4b) elephant isa mammal.
</listItem>
<bodyText confidence="0.994577">
The predicate indicates identity of arguments. The reason it is necessary to include
this predicate is that it is possible to identify the same concept by means of different
kinds of names. For example, assuming that Clyde is grey, we can identify the concept
&apos;grey&apos; either by its atomic name or as a function of Clyde. The &apos;=&apos; predicate shows
this identity. More complex identifications are of course also possible.
</bodyText>
<listItem confidence="0.992776333333333">
(5a) color of Clyde = grey.
(5b) mother of Clyde = sister of father of Babar.
(5c) mother of mother of Clyde = mother of father of Babar.
</listItem>
<footnote confidence="0.919063142857143">
2 During the evolution of WG this figure has varied between five (Hudson 1984) and one (Hudson 1989),
although the expressiveness of the formalism has not changed significantly. There is a balance to be
struck between having a minimal (i.e. one-member) predicate set with necessary distinctions marked in
the arguments, and having a more readable notation that includes extra predicates. The three-member
set has been used in recent WG publications. In at least one computer implementation (described in
Hudson 1989) the &apos;grammarian&apos;s grammar&apos; is written with three predicates and compiled into a
&apos;machine grammar&apos; that uses only one predicate.
</footnote>
<page confidence="0.990921">
135
</page>
<note confidence="0.370438">
Computational Linguistics Volume 18, Number 2
</note>
<bodyText confidence="0.9532462">
Propositions including the predicate can be thought of as path equations of the
sort used to indicate coreference in directed acyclic graphs.
The &apos;has&apos; predicate is used to associate attributes with concepts. A proposition of
the form shown in (6) associates concept Y with concept X in the quantity specified
in Q.
</bodyText>
<equation confidence="0.806073">
(6) X has Q Y.
</equation>
<bodyText confidence="0.761242857142857">
Q is called a quantitator. It signifies how many instances of the specified type should
be associated with X.
(7a) word has 1 head.
(7b) finite verb has 0 head.
In the simplest cases, as in most of the examples in this paper, the quantitator can be
just a single integer, but it is also possible to specify a range of numbers by giving the
minimum and maximum, e.g. [0-11.
</bodyText>
<subsectionHeader confidence="0.998576">
2.3 Arguments
</subsectionHeader>
<bodyText confidence="0.996494913043478">
Arguments fall into nine basic types. It is helpful to be able to describe these types in
respect of their structure without reference to their function as arguments. We shall
therefore say that a well-formed argument must be a name where a name conforms
to one of the following definitions.
Atoms. The atoms of the WG knowledge representation are single words such as
&apos;verb&apos; or hyphenated words such as &apos;proper-noun&apos; that identify single nodes in the
knowledge structure.
Sets. A set of concepts is enclosed in set brackets. The first element inside the brackets
is a sign that identifies whether conjunction or disjunction is intended. &apos;{&amp;: A, B}&apos;
means &apos;A and B.&apos; &apos;{/:A, B1&apos; means &apos;A or B.&apos; Special notations are used for two
particular kinds of ordered &apos;and&apos; set. Strings of orthographic symbols are enclosed
in angle brackets (e.g. &apos;&lt;did&gt;&apos;). Linear constituent structures are formed by linking
constituents by &apos;+&apos; (e.g. &apos;stem of it + mEd&apos;).
Relational names. These consist of an atom that is the name of a relation, followed
by &apos;of&apos;, followed by a name that may be another relational name. Thus, &apos;A of B&apos; and
&apos;A of B of C&apos; are both well formed (e.g. &apos;subject of verb,&apos; position of subject of verb&apos;).
Relational names are right-embedding.
Positional names. These consist of positional atoms (&apos;before,&apos; after,&amp;quot;adjacent-to,&apos;
&apos;next-to&apos;) followed by a name, e.g. &apos;before X.&apos; Positional names identify positions in a
linear sequence in which the named concept is located.
It. Where a proposition refers to the same concept on either side of a path equation,
the second instance of the concept is identified by the name &apos;it.&apos; Example (8) uses &apos;it&apos;
to refer to &apos;word.&apos;
</bodyText>
<listItem confidence="0.714577">
(8) position of dependent of word = after it.
</listItem>
<page confidence="0.998575">
136
</page>
<note confidence="0.571244">
Norman M. Fraser and Richard A. Hudson Inheritance in Word Grammar
</note>
<bodyText confidence="0.9509141875">
The concept identified after the 1=-&apos; by &apos;it&apos; is always identified before the &apos;=&apos; by the
most deeply embedded (i.e. rightmost) name, which in this example is &apos;word.&apos;
Compound names. These consist of two parts, the first of which is the value of a
feature or a set of feature values, and the second of which is an atom. Thus &apos;past verb&apos;
and &apos;{&amp;: past, positive, s+v} polarity-verb&apos; are well-formed compound names.
Temporary names. Stored concepts in the WG knowledge structure are types; e.g.
&apos;noun&apos; means &apos;the typical noun,&apos; and &apos;subject of verb&apos; means &apos;the typical subject of the
typical verb.&apos; To distinguish particular tokens from these types, tokens are assigned
temporary names as they are encountered during processing. These names are tempo-
rary in the sense that they do not belong to the permanent knowledge base, instead
being introduced during processing. Temporary names consist of integers prefixed by
a character or characters. By convention, morpheme instances are prefixed by &apos;m,&apos;
word instances by &apos;w,&apos; and objects in the semantics by &apos;c&apos; (for &apos;concept&apos;). Thus &apos;ml,&apos;
&apos;w12,&apos; and &apos;c6&apos; are all well-formed temporary names. For example, the propositions in
(9) (which refer to the sentence analyzed in the Appendix, Mary jumped) illustrate the
use of temporary names.
</bodyText>
<listItem confidence="0.985020666666667">
(9a) whole of w1 = &lt;Mary&gt;.
(9b) whole of w2 = &lt;jumped&gt;.
(9c) position of wl = before w2.
</listItem>
<bodyText confidence="0.99676725">
The analysis of a sentence involves taking the observable facts, such as the above,
and inferring unobservable ones which are logically consistent with them, with each
other and with the knowledge base. The inferred facts for Mary jumped include the
following:
</bodyText>
<listItem confidence="0.946925">
(10a) w1 isa MARY.
(10b) w2 isa past JUMP.
(10c) subject of w2 = wl.
(10d) w2 has 1 subject.
(10e) position of subject of w2 = before it.
(10f) sense of w2 = c1.
</listItem>
<bodyText confidence="0.943665833333333">
Instance names. These consist of a name preceded by &apos;a&apos; (or &apos;an&apos;). Whereas temporary
names provide a means of identifying specific instances, instance names provide a
means of identifying any single instance of a specified type. If a type &apos;X&apos; exists in the
inheritance hierarchy, then the instance name &apos;a X&apos; refers to any instance that isa X.
For example,
(11) subject of passive verb = a complement of it.
Notice that without this &apos;a,&apos; a name refers to every example of the type concerned.
(This follows from the interpretation of concepts as types; if something is true of
some concept, then it must also be true of every instance of that concept, barring
specified exceptions). Proposition (11) identifies the subject of a passive verb with just
one of its complements; without &apos;a,&apos; it would identify the subject with every one of
the complements, and lead to chaos.
</bodyText>
<page confidence="0.989205">
137
</page>
<note confidence="0.334558">
Computational Linguistics Volume 18, Number 2
</note>
<bodyText confidence="0.932536">
Quantified names. As already explained, these consist of a quantitator followed by
an atom: e.g. &apos;1 head,&apos; 0 complement.&apos;
</bodyText>
<subsectionHeader confidence="0.997387">
2.4 Interpretation
</subsectionHeader>
<bodyText confidence="0.999183933333333">
The WG knowledge representation language keeps close to ordinary English, as can be
seen from the examples given above and from the Appendix. It avoids the ambiguity
of ordinary English, and is much less rich, but the propositions are easy to understand.
A full formal account of the semantics of the language would require a separate paper;
in this section we limit ourselves to a brief discussion of one of the simpler areas, viz
the use of positional names.
Positional names, which it will be recalled consist of a word such as &apos;before&apos; or
&apos;after&apos; followed by another name, are primarily to do with relations in time—the
relations between co-occurring spoken words, between co-occurring phonemes, or
between events or times that are referred to in the semantic structure of a sentence. If
the data to be analyzed are written, then some of these relations are mapped onto the
spatial patterns of writing. With this reservation, then, &apos;before X&apos; is the name of some
time earlier than X, where X is itself either a time (say, last Friday) or an event that
can be located in time (e.g. Mary&apos;s birthday party). The examples in (12) illustrate two
uses of this single general pattern.
</bodyText>
<listItem confidence="0.980839">
(12a) position of dependent of word = after it.
(12b) time of referent of past verb = before it.
</listItem>
<bodyText confidence="0.999889823529412">
The first example refers to the order of words in a sentence, while the second refers
to the deictic relation between the event referred to by a past-tense verb and the time
when that verb itself is uttered. (In (12b), &apos;it&apos; is coreferential with the verb, so when
this proposition is inherited by a token of a past verb, &apos;it&apos; refers to this token, or more
precisely to the time when it is uttered).
The rule for interpreting &apos;after&apos; or &apos;before&apos; must therefore be capable of determining
which of two events occurs before the other. This is straightforward if these events
are themselves given temporary names whose integer rises with time; w2 occurs,
by definition, before w3, and c2 before c3. In this way, all temporary concepts are
effectively time-stamped. All the rule needs to do is compare the integers.
Positional names are noteworthy because they illustrate particularly clearly the
extent to which different kinds of knowledge can be integrated into a single system.
The same formal apparatus, interpreted by the same inference rule, is used in syntax
(regarding word order) and also in semantics (regarding temporal ordering of events).
Moreover, the latter events themselves include not only the events referred to (e.g. the
event of Mary jumping), but also the event of uttering the words concerned (i.e. in
Mary jumped, the utterance of the word jumped).
</bodyText>
<subsectionHeader confidence="0.989106">
2.5 Inheritance
</subsectionHeader>
<bodyText confidence="0.991480714285714">
Inheritance is the rule of inference that derives new propositions from existing ones.
It is sanctioned primarily by any occurrence of the &apos;isa&apos; predicate, but also by various
other formal patterns mentioned below. If A isa B, then A inherits all the properties
of B (except those that are blocked as we explain in the next section). In other words:
Inheritance. If A isa B, then for any true proposition P that refers to B, it is possible
to infer another true proposition Q that is the same as P except that A is substituted
in Q for B in P.
</bodyText>
<page confidence="0.997283">
138
</page>
<note confidence="0.621101">
Norman M. Fraser and Richard A. Hudson Inheritance in Word Grammar
</note>
<listItem confidence="0.916744">
For example:
(13a) Clyde isa elephant.
(13b) color of elephant = gray.
(13c) color of Clyde = gray. [from (13a,b)]
</listItem>
<bodyText confidence="0.999215666666667">
A similar interpretation applies to the &apos;=&apos; predicate, which is in effect a reciprocal &apos;isa&apos;.
If A = B, then any true proposition that contains A can be matched by another in
which B replaces A, and vice versa. Inferentially, then, both &apos;isa&apos; and &apos;=&apos; are extremely
simple, and extremely powerful, allowing the creation of new propositions by simple
substitution operations.
The most noteworthy feature of the WG inheritance system is, once again, that
it applies to all types of knowledge, allowing a single integrated knowledge base
and a single set of inference rules for both linguistic and nonlinguistic knowledge
(cf. the &apos;preference rules&apos; of Jackendoff 1983). The same rule that allows us to inherit
information about Clyde also allows us to inherit information about words within
the grammar and about words in sentences. These similarities can be seen from the
following examples.
</bodyText>
<listItem confidence="0.999138222222222">
(14a) noun isa word.
(14b) word has 1 head.
(14c) so: noun has 1 head.
(15a) MARY isa noun.
(15b) noun has 1 head. [--= 14c1
(15c) so: MARY has 1 head.
(16a) wl isa MARY.
(16b) MARY has 1 head. [= 15c]
(16c) so: w1 has 1 head.
</listItem>
<bodyText confidence="0.997327571428571">
These examples show how inheritance allows information to be inherited both within
the grammar (14, 15) and from the grammar to a particular sentence word, in the pro-
cess of parsing (16). As already explained, the aim in parsing is to link each sentence
word to a word in the grammar from which it can inherit a set of propositions com-
patible with the propositions inherited for all the other words in the same sentence.
Another set of examples applies the same inheritance rule to meanings and con-
cepts:
</bodyText>
<listItem confidence="0.994768">
(17a) jumping isa action.
(17b) action has 1 actor.
(17c) so: jumping has 1 actor.
</listItem>
<page confidence="0.956813">
139
</page>
<figure confidence="0.2988">
Computational Linguistics Volume 18, Number 2
</figure>
<listItem confidence="0.998448083333333">
(18a) sense of JUMP = jumping.
(18b) jumping has 1 actor. [= 17c]
(18c) so: sense of JUMP has 1 actor.
(19a) w2 isa JUMP.
(19b) sense of JUMP has 1 actor. E= 18cl
(19c) so: sense of w2 has 1 actor.
(20a) w2 isa word.
(20b) referent of word isa sense of it.
(20c) referent of w2 isa sense of it.
(21a) referent of w2 isa sense of it. [= 20c]
(21b) sense of w2 has 1 actor. 19cl
(21c) so: referent of w2 has 1 actor.
</listItem>
<bodyText confidence="0.9995406">
If we continue this chain of deductions we eventually find that Mary is the actor of the
event of jumping referred to by w2; in other words, Mary jumped. If the analysis were
embedded in a body of knowledge about the world in which Clyde trod on Mary&apos;s
toes, then we could infer that the person on whose toes Clyde trod jumped; and so
on.
The unified nature of inheritance in WG allows us to recognize, or at least imagine,
a single inheritance hierarchy for the whole of knowledge, within which linguistic
concepts can be located as special cases of more general ones. In particular, words are
a special kind of action, and inherit from &apos;action&apos; properties such as having a time and
an actor:
</bodyText>
<listItem confidence="0.9669192">
(22a) word isa action.
(22b) action has 1 actor.
(22c) so: word has 1 actor.
(22d) action has 1 time.
(22e) so: word has 1 time.
</listItem>
<bodyText confidence="0.999631888888889">
It was this inheritance that allowed us to assume that a word has a time, which can
be referred to not only in the rules for word order but also in those for the semantics
of tense (cf. (12) above).
Another direction in which WG extends the normal scope of inheritance is by
allowing it to apply to relations as well as to the more familiar kind of nonrelational
category, such as elephant, word, etc. (For a similar approach see Thomason and
Touretzky 1991). This allows us to recognize a hierarchy of grammatical relations,
with, for example, &apos;object&apos; as a particular kind of &apos;dependent&apos;; which in turn allows us
to formulate word-order rules that refer to the appropriate point in the hierarchy, and
</bodyText>
<page confidence="0.98021">
140
</page>
<note confidence="0.646703">
Norman M. Fraser and Richard A. Hudson Inheritance in Word Grammar
</note>
<bodyText confidence="0.995537">
then automatically generalize to all relations below this. Here is a simple example of
the inferences that can be drawn.
</bodyText>
<listItem confidence="0.99384125">
(23a) position of dependent of word = after it.
(23b) object isa dependent.
(23c) LIKE isa word.
(23d) so: position of object of LIKE = after it.
</listItem>
<bodyText confidence="0.999912666666667">
To summarize, then, inheritance plays a much larger part in WG than in other theories.
It allows us to locate atomic concepts in inheritance hierarchies, and encourages us to
try to unify them all into a single grand hierarchy that reveals the continuities between
linguistic and other concepts. A fortiori, it integrates linguistic categories of different
levels into a single system, in which the same inheritance rule applies to morphology,
syntax, and semantics. Moreover, since WG uses dependency instead of constituent
structure, all the units of syntax (outside coordination) are single words, so the only
difference between the &apos;rules of grammar&apos; and &apos;lexical entries&apos; is in the generality,
rather than the size, of the units to which they refer.
</bodyText>
<subsectionHeader confidence="0.999399">
2.6 Overriding
</subsectionHeader>
<bodyText confidence="0.997584352941176">
In a default inheritance system, information is inherited only by default, i.e. in the
absence of some exceptional information. One key question is how exceptions should
be handled, and our answer is perhaps the most controversial part of this paper.
The standard answer is, of course, that any more general proposition is overridden
by a more specific one that contradicts it. For example, the past-tense form did takes
precedence over the expected *doed because the former is specified in relation to DO,
whereas the latter is inherited from the general rules for verbs. This principle, which we
call automatic overriding, underlies most discussions of inheritance (e.g. Shieber 1986;
Flickinger 1987), but it is also assumed in a lot of linguistic theory where the notion
of &apos;inheritance&apos; is not recognized as such—e.g. in the &apos;Proper Inclusion Precedence
Principle&apos; governing the ordering of rules in phonology (see, for example, Pullum
1979 for a survey of this literature).
Our answer is quite different, and involves the negative propositions, introduced
by &apos;NOT:&apos;, which we described earlier. In WG, inheritance is not blocked by a more
specific proposition, but by a negative proposition. We know that *doed is not possible
because there is a proposition that tells us so (24a), and not just because (24b) requires
did:
</bodyText>
<listItem confidence="0.948915">
(24a) NOT: whole of past DO = stem of it + whole of mEd.
(24b) whole of past DO = &lt;did&gt;.
</listItem>
<bodyText confidence="0.999630428571429">
Every exceptional fact is paired with a negative fact that blocks inheritance. We call
this stipulated overriding. It remains to be seen which of these approaches—automatic
overriding or stipulated overriding—will be favored by future research in NLP. The
extra cost of exceptional facts in the first system lies in the need to ensure that more
specific facts are accessed before more general ones. In the second system, the cost
lies in the need for a larger database. Our reasons for preferring stipulated overriding
are partly concerned with cognitive modeling (see Hudson 1990: 40ff), but we also
</bodyText>
<page confidence="0.975799">
141
</page>
<note confidence="0.313946">
Computational Linguistics Volume 18, Number 2
</note>
<bodyText confidence="0.9961755">
believe that the syntactic and semantic arguments that we present in the next sections
support this approach. Here, then, is the rule of inference for default inheritance:
</bodyText>
<subsectionHeader confidence="0.757257">
Default inheritance
</subsectionHeader>
<bodyText confidence="0.999582444444445">
If A isa B, then for any true proposition P that refers to B, it is possible to infer
another true proposition Q that is the same as P except that A is substituted in Q for
B in P. unless NOT: Q.
That is, we can apply the inheritance rule defined in the last section, unless there
is a negative proposition that conflicts with the inherited proposition. This negative
proposition has the form NOT: Q, where Q is also a proposition that is available either
by inspection or by inference. This proposition, NOT: Q, must itself pass the same test,
since it may in turn be overridden by NOT: NOT: Q, and so on recursively.
We can now give a more detailed summary of inheritance and blocking in WG.
</bodyText>
<listItem confidence="0.914725">
(26a) A proposition P is valid iff
a. it is contained in the knowledge base or
bi. it may be inherited and
bii. NOT: P cannot be inherited.
(26b) A proposition P may be inherited iff
a. Q is valid and
b. at every point where P differs from Q, by containing Y
instead of X, X subsumes Y.
(26c) A name X subsumes another name Y iff
a. Y isa X, or
b. Y is a compound name (A B), where B is subsumed by X, or
c. X = Y.
</listItem>
<bodyText confidence="0.999796083333333">
(Allowing inheritance to apply to compound names allows multiple inheritance—i.e.,
one concept may inherit down more than one path. For example, dogs is &apos;plural DOG,&apos;
an example of both DOG and &apos;plural noun.&apos; From DOG it inherits its stem and its
sense (inter alia), while &apos;plural noun&apos; provides the suffix, the &apos;set&apos; meaning, and the
ability to occur, for example, after these).
Having introduced our theory of default inheritance, we can now discuss some
linguistic applications in more depth. One of the most distinctive features of our theory
is our claim that default inheritance applies to syntax and compositional semantics,
so we shall concentrate on these areas. The preceding discussion should have made it
clear that we also use default inheritance in morphology, but we will not pursue that
further here. (A brief WG account of English inflectional morphology can be found in
Hudson 1990: 181-90.)
</bodyText>
<sectionHeader confidence="0.98615" genericHeader="introduction">
3. Syntax
</sectionHeader>
<subsectionHeader confidence="0.996719">
3.1 Word Types
</subsectionHeader>
<bodyText confidence="0.997813">
WG syntax is centred on two inheritance hierarchies, one for word types (i.e. word
classes and lexical items) and the other for grammatical relations. In Word Grammar
</bodyText>
<page confidence="0.965155">
142
</page>
<figure confidence="0.9053152">
Inheritance in Word Grammar
Norman M. Fraser and Richard A. Hudson
word
adword noun conjunction verb
adjective adverb proper common pronoun polarity STAND etc
</figure>
<figureCaption confidence="0.891635666666667">
preposition count determiner modal
Figure 1
The word type hierarchy.
</figureCaption>
<bodyText confidence="0.992053666666667">
(as suggested by the name) the category &apos;word&apos; is basic in every sense. Figure 1 shows
the top of the hierarchy of word types assumed in WG for English, and some of the
corresponding WG propositions are given in (27).
</bodyText>
<listItem confidence="0.989717181818181">
(27a) count isa common.
(27b) common isa noun.
(27c) noun isa word.
Three points should be noted about this hierarchy.
1. We assume a hierarchical relation among word types, instead of the
more usual cross-classification based on features. This links to a general
restriction on the use of features in WG, which excludes all features
except those that are morphosyntactic—i.e., reflected in morphology and
relevant to syntax or semantics.
2. Secondly, we assume some nonstandard analyses; in particular, a
preposition is a kind of adverb, and a determiner is a kind of pronoun,
which in turn is a kind of noun.
3. We prefer to keep an open mind on the extent to which our categories
are universal, but we are sure that some are parochial (relevant to
English only). This hierarchy can be continued downward to include
lexical items (such as STAND, shown in the diagram), which may in turn
be further subdivided; e.g., we can distinguish transitive and intransitive
versions of STAND (with, it should be noted, the same irregular
morphology in both cases):
(28a) STAND isa verb.
(28b) STAND/intrans isa STAND.
(28c) STAND/trans isa STAND.
</listItem>
<page confidence="0.992916">
143
</page>
<note confidence="0.527081">
Computational Linguistics Volume 18, Number 2
</note>
<bodyText confidence="0.9993386">
As explained earlier, because lexical items are part of the same hierarchy as general
word classes, there is no formal distinction between the lexicon and the rest of the
grammar. Furthermore, we use the same isa relation to link word tokens to word
types; so if w3 is the name of the word stand in I can&apos;t stand cats, it too will fit into the
same hierarchy:
</bodyText>
<equation confidence="0.659953">
(29) w3 isa STAND/trans.
</equation>
<bodyText confidence="0.9514615">
Word tokens can be thought of as a constantly changing fringe on the bottom of
the (relatively) permanent hierarchy.
</bodyText>
<subsectionHeader confidence="0.999761">
3.2 Grammatical Functions
</subsectionHeader>
<bodyText confidence="0.999975413793103">
We now come to the second hierarchy of syntax, the hierarchy of grammatical rela-
tions. Unlike most other syntactic theories, WG uses constituent structure only for the
purpose of describing coordinate constructions (cf. Hudson 1990: 404ff for details). All
other syntactic structure is expressed in terms of dependencies between pairs of words,
one of which is the head of the other, its dependent. Higher nodes such as phrases
or sentences are not represented explicitly in the grammar. WG is thus a variety of
dependency grammar.
Dependency grammar was first formalized by Tesniere (1959) and refined by
Hays (1964), Gaifman (1965), Robinson (1970) and others. A number of dependency-
based theories have emerged from the linguistic underground during the last thirty
years. These include the Meaning-Text model (Menuk and Zolkovskij 1970; Menuk
1988), Case Grammar (Anderson 1971; 1977), Daughter Dependency Grammar (Hud-
son 1976), WG (Hudson 1984; 1990), Functional Generative Description (Sgall, Hajieova,
and Panevova 1986), and Lexicase (Starosta 1988). While none of these theories has at-
tained widespread popularity, some of their central insights have become increasingly
influential in the phrase structure grammar mainstream. For example, the trend toward
head-driven approaches, the prominence of notions such as &apos;government,&apos; the explicit
use of grammatical relations and case, and the reduced amount of information carried
in phrasal categories all reflect the general migration toward dependency. Increased
interest in categorial grammars, and especially unification categorial grammars (which
are virtually indistinguishable from dependency grammars) provides further evidence
of this tendency.3
The combination of default inheritance with dependency syntax allows an inter-
esting range of generalizations and exceptions. Like other dependency grammars, WG
requires a typical word to have one head, though the same word may act as head to
more than one other word, its dependents. As in other theories, just one word is al-
lowed to be an exception to this rule; we call this word the &apos;root&apos; of the sentence. This
has (by definition) no head, and is generally a finite verb; e.g. in Mary didn&apos;t jump,
the polarity verb (&apos;auxiliary verb&apos;) didn&apos;t is the root, on which both Mary and jump
</bodyText>
<footnote confidence="0.994725125">
3 The last decade has seen increased interest in dependency grammar among computational linguists.
Dependency grammar has been applied in the experimental parsing systems of Hellwig (1986), Sigurd
(1989), and Covington (1991); in the &apos;Kielikone&apos; natural language interface of Jappinen, Lassila, and
Lahtola (1988); in the machine translation systems of EUROTRA (Johnson, King, and des Tombe 1985),
DLT (Schubert 1987), Charles University (Sgall and Panevova 1987), and IBM Tokyo (Maruyama 1990);
and in the speech recognition system of Giachin and Rullent (1989). Parsers based on the theories of
Lexicase (Starosta and Nomura 1986) and Word Grammar (Fraser 1989; Hudson 1989) have also been
developed.
</footnote>
<page confidence="0.990479">
144
</page>
<note confidence="0.847914">
Norman M. Fraser and Richard A. Hudson Inheritance in Word Grammar
</note>
<listItem confidence="0.82734275">
depend. Here, then, we already have a simple example of default inheritance:
(30a) word has 1 head.
(30b) w1 isa word.
(30c) so: wl has 1 head.
</listItem>
<bodyText confidence="0.89745975">
On the other hand, for w2, the finite verb didn&apos;t, this general rule is blocked to allow
it to occur without a head (i.e. to make the head optional, 10-11 head&apos;). This analysis
assumes that obligatory (&apos;1&apos;) and optional C[0-11&apos;) conflict, so the former must be
suppressed by (31d).4
</bodyText>
<listItem confidence="0.9917896">
(31a) finite verb has [0-11 head.
(31b) w2 isa finite verb.
(31c) so: w2 has [0-11 head.
(31d) NOT: finite verb has 1 head.
(31e) so: NOT: w2 has 1 head.
</listItem>
<bodyText confidence="0.995749">
If the rule about having one head per word allows exceptions in one direction, we
may expect exceptions in the other direction as well: words that have more than one
head. This is not allowed in other versions of dependency grammar,&apos; but in WG it
is the basis for our analysis of a range of important constructions: raising, control,
extraction, and passives (not to mention coordination, which is often allowed as an
exception by other theories). For example, in Mary didn&apos;t jump, we recognize Mary as
the subject not only of didn&apos;t but also of jump, so Mary has two heads, contrary to the
general rule.
</bodyText>
<figure confidence="0.78720875">
(32)
subject
subject xcomplement
Mary didn&apos;t jump
</figure>
<footnote confidence="0.846813555555556">
4 This analysis may in fact be more complicated than it needs to be. We could allow finite verbs to
inherit the regular &apos;1 head&apos; simply by not blocking it, and allow for &apos;0 head&apos; by an extra rule, which
provides the other alternative.
5 The notion of a word with two heads is meaningless in theories based on phrase structure, because
&apos;head&apos; is used there in relation to phrases, not words. The X-bar &apos;head&apos; corresponds to our &apos;root,&apos; the
word in a phrase that has no head inside that phrase. It is true that some linguists have suggested that
a phrase might have more than one head (e.g. Warner 1987), and this has been a standard analysis of
coordinate structures since Bloomfield (1933); but this is very different from a single word having more
than one head.
</footnote>
<page confidence="0.984525">
145
</page>
<note confidence="0.518447">
Computational Linguistics Volume 18, Number 2
</note>
<bodyText confidence="0.978958333333333">
(In a dependency diagram, the arrow points towards the dependent.) This is per-
mitted by a proposition which, at least by implication, overrides the general rule, and
which refers to the grammatical function &apos;xcomplement&apos;:6
</bodyText>
<listItem confidence="0.803464">
(33) subject of xcomplement of word = subject of it.
</listItem>
<bodyText confidence="0.99958225">
In other words, a word may have two heads provided that one of them is the xcomple-
ment of the other. (We return below to the relations among the grammatical functions
such as &apos;subject&apos; and &apos;xcomplement&apos;).
The possibility of having more than one head is related to another important
generalization, namely that heads and dependents are usually adjacent. If we think of
each word as defining a &apos;phrase,&apos; made up of that word plus any words subordinate
to it, this is equivalent to the PSG ban on discontinuous phrases. In the simple cases,
then, the following generalization is true:
</bodyText>
<listItem confidence="0.709873">
(34) position of word = adjacent-to head of it.
</listItem>
<bodyText confidence="0.6486575">
An operational definition of &apos;adjacent-to&apos; checks that no word between the words
concerned has a head outside the phrase:
</bodyText>
<listItem confidence="0.9966125">
(35a) A is adjacent-to B iff every word between A and B is a subordinate
of B.
(35b) A is a subordinate of B iff A is B or A is a dependent of a
subordinate of B.
</listItem>
<bodyText confidence="0.982357866666667">
But what if a word has more than one head? This normally leads to a discontinuity;
e.g. in Mary didn&apos;t jump, the phrase rooted in jump consists of Mary jump, but does not
include didn&apos;t. Saying that Mary jump is discontinuous is the same as saying that Mary
is not adjacent to one of its heads, jump. Interestingly, though, Mary does have one
head to which it is adjacent (viz didn&apos;t), and more generally the same is true of all
discontinuities: even if a word has some nonadjacent heads, it also has at least one to
which it is adjacent. We can therefore keep our generalization (34) in a slightly revised
form, with &apos;a head&apos; (one head) rather than &apos;head&apos; (every head):
(36) position of word adjacent-to a head of it.
This generalization is inherited by every word, so every word has to be adjacent to at
least one of its heads. This treatment of discontinuity has many important ramifications
that cannot be explored fully here.
The generalizations discussed in this section have referred crucially to grammatical
functions.7 In some cases these were the functions &apos;dependent&apos; and &apos;head,&apos; but we also
mentioned &apos;subject&apos; and &apos;xcomplement.&apos; The functional categories are arranged in an
</bodyText>
<footnote confidence="0.993879">
6 The name &apos;xcomplement&apos; is borrowed from Lexical Functional Grammar. The term used in earlier WG
literature is &apos;incomplement.&apos;
7 As in LFG, the term &apos;function&apos; is used here in both its mathematical and grammatical senses, but
(unlike LFG) with a single word as the argument; so in expressions such as &apos;head of X&apos; or &apos;subject of
X,&apos; X is always some word or word type rather than a phrase.
</footnote>
<page confidence="0.992322">
146
</page>
<note confidence="0.498831">
Norman M. Fraser and Richard A. Hudson Inheritance in Word Grammar
</note>
<figure confidence="0.825193">
predependent postdependent
visitor subject preadjunct complement postadjunct
object xcomplement etc
</figure>
<figureCaption confidence="0.987443">
Figure 2
</figureCaption>
<bodyText confidence="0.981954529411765">
Hierarchy of dependency types for English.
inheritance hierarchy, and the one for English is shown (in part) in Figure 2. This
hierarchy allows generalizations to be made about different types of dependent at the
most appropriate level. As with the hierarchy of word classes, we are sure that some
of these categories are specific to languages like English, and not universal, but others
seem to be very widespread or universal.
Generalizations about word order are perhaps the clearest examples of general-
izations that take advantage of the hierarchical organization of grammatical functions
in WG. Proposition (37) states the default word order of English (i.e. English is a
head-first language).
(37) position of dependent of word = after it.
Although this generalization has important exceptions, it is clearly true of &apos;typical&apos;
dependencies in English; for example, in a running text we find that between 60% and
70% of dependencies are head-first.
The exceptional order of those dependent types that typically precede their heads
is handled by the propositions shown in (38), referring to the super-category &apos;prede-
pendent.&apos;
</bodyText>
<listItem confidence="0.9762185">
(38a) position of predependent of word = before it.
(38b) NOT: position of predependent of word = after it.
</listItem>
<bodyText confidence="0.874508444444444">
The usual machinery of default inheritance applies, so that (38b) blocks the normal
head-first rule, and (38a) replaces it by the exceptional one. There are just a few con-
structions that allow a dependent to precede its head, one of which is the subject-verb
pairs
8 As one of our readers commented, if pressure toward consistency were the strongest pressure on
language development, we should expect VSO languages to outnumber SVO, but of course they do not
(about 40% of the world&apos;s languages are said to be SVO, compared with only 10% VSO). One
explanation for this is presumably the strong tendency for subjects to be more topical than verbs, but it
remains as a challenging area for research.
</bodyText>
<page confidence="0.990255">
147
</page>
<note confidence="0.531568">
Computational Linguistics Volume 18, Number 2
</note>
<bodyText confidence="0.996687875">
One of the most important applications of default inheritance in WG syntax is
in the distinction of &apos;derived&apos; from &apos;underlying&apos; or &apos;basic&apos; patterns. The general point
is that underlying patterns are allowed by the most general rules, and are therefore
most typical; whereas derived patterns involve rules that override these, so they are
exceptional. In this way we can capture the different statuses of these patterns in
a completely monostratal analysis and without the use of special devices such as
transformations, lexical rules, or metarules.
Take for instance the rules given in the Appendix for inverted subjects.
</bodyText>
<listItem confidence="0.968478714285714">
(39a) tensed polarity-verb has 1 sv-order.
(39b) sv-order of verb = { /: s+v, v+s}.
(39c) position of dependent of word = after it.
(39d) position of predependent of word = before it.
(39e) NOT: position of predependent of word = after it.
(39f) NOT: position of subject of v+s verb = before it.
(39g) NOT: NOT: position of subject of v+s verb = after it.
</listItem>
<bodyText confidence="0.999984785714286">
The first two rules allow us to distinguish tensed polarity-verbs according to whether
their subject precedes (&apos;s+v&apos;) or follows (&apos;v+s&apos;) them.9 This allows us to treat &apos;v+s verb&apos;
as an exception to the general rule that subjects precede their head, which is in turn
an exception to the generalization that words follow their heads. This system allows
us to generate a sentence such as Did Mary jump? with just one syntactic structure,
free of empty positions, while still showine that it is a less normal construction than
a sentence such as Mary did jump. In parsing terms, the only problem is to find and
apply the necessary propositions; there is no need to reconstruct any kind of abstract
structure for the sentence itself.
The use of &apos;NOT&apos; rules for overriding defaults finds support in the fact that the
&apos;NOT&apos; rule in (39e) is also crucial for solving at least two other major problems, namely
passives and extraction. In a passive sentence like (40), WG handles object-promotion
by analyzing the subject as also being the object. This is achieved by means of propo-
sition (41a) (which is slightly simplified).
</bodyText>
<listItem confidence="0.97191">
(40) Mary was kissed by John.
(41a) subject of passive verb = object of it.
(41b) NOT: position of predependent of word = after it.
</listItem>
<bodyText confidence="0.961257333333333">
The problem is that Mary, as the object of kissed, ought to follow it, but since Mary
is also the subject this requirement is overridden by proposition (39e=41b), so Mary
never inherits the need to follow kissed.11
</bodyText>
<footnote confidence="0.56725">
9 See Hudson (1990: 215-6) for a discussion of the apparent lack of morphological consequences of the
subject-position feature.
10 The exceptionality of an inverted subject is shown by the negative proposition &apos;NOT: NOT: position of
subject of v+s verb = after it.&apos; This proposition is inherited by the word tokens concerned—i.e. it is part
of the analysis of the sentence itself, and not just available in the grammar.
lilt may not be obvious exactly how this works. How does (41b) stop the object of a passive from
following the verb, given that it refers to &apos;predependent,&apos; which does not subsume &apos;object&apos;? The answer
lies in (41a): the object of a passive verb is also its subject, so any rule (such as (41b)) that applies to the
subject also, ipso facto, applies to its object.
</footnote>
<page confidence="0.985674">
148
</page>
<note confidence="0.346122">
Norman M. Fraser and Richard A. Hudson Inheritance in Word Grammar
</note>
<bodyText confidence="0.982148666666667">
A similar approach is used to handle extraction. For example, consider sentence (42).
(42) Salesmen I distrust.
Here salesmen must be an object of distrust, so the order should be I distrust salesmen,
but in this case we also recognize a special kind of predependent relation (&apos;visitor,&apos;
roughly equivalent to &apos;Comp&apos;) between distrust and salesmen, so once again the word
order conflict between these two relations can be resolved. The rules are given in (43).
</bodyText>
<listItem confidence="0.989824833333333">
(43a) finite verb has [0-1] visitor.
(43b) visitor isa predependent.
(43c) position of predependent of word = before it.
(43d) NOT: position of predependent of word = after it.
(43e) visitor of word = a postdependent of it.
(43f) visitor of word = a visitor of complement of it.
</listItem>
<bodyText confidence="0.959596571428571">
Proposition (43a) allows distrust to have a visitor, which according to (43b) is a kind of
predependent and therefore, by inheritance from (43c,d), must precede it. The visitor
is also some kind of postdependent, according to (43e), so it may be the verb&apos;s object
as in our example. But equally it may &apos;hop&apos; down the dependency chain thanks to
(430, thereby providing for the analysis of sentences such as (44).
(44) Salesmen I don&apos;t think many people say they trust.
Further details of the WG analysis of passives and extraction can be found in Hudson
(1990).
Both passivization and extraction are standard examples of syntactic problems that
need special machinery. According to WG—and more recently Flickinger, Pollard, and
Wasow (1985) and Flickinger (1987)—all that is needed is default inheritance, which
is available (though generally not explicitly recognized) in every linguistic theory; so
any theory capable of accommodating exceptional morphology already has the power
to deal with subject-inversion, passives and extraction.
</bodyText>
<sectionHeader confidence="0.904961" genericHeader="method">
4. Semantics
</sectionHeader>
<bodyText confidence="0.998560636363636">
Default inheritance also plays a crucial part in the WG treatment of semantics. It is
probably obvious how it applies in the familiar examples of inheritance in semantic
networks--e.g., how one can infer that Clyde has a head from more general propo-
sitions about elephants or animals. Rather than discussing this familiar territory we
shall show how default inheritance helps us to answer a recurrent objection to de-
pendency analysis: the syntactic dependency structure is completely flat, so it does
not provide any units between the individual word and the complete phrase (where
&apos;phrase&apos; means a word and the complete set of all its dependents and their respective
phrases).
For example, the semantic structure for the phrase typical French house has to men-
tion the concept &apos;French house&apos; (a typical French house is a house that is typical as a
</bodyText>
<page confidence="0.992022">
149
</page>
<figure confidence="0.574534">
Computational Linguistics Volume 18, Number 2
French house, and not just as a house); but a flat dependency structure such as (45)
provides no syntactic unit larger than the individual &apos;words (Dahl 1980).
(45)
typical French house
</figure>
<figureCaption confidence="0.127702">
Similarly, how can we handle &apos;VP-anaphora&apos; without a VP node? For example, we
need the concept &apos;adore peanuts&apos; as part of the semantic structure of Fred doesn&apos;t in
(46a), but adores peanuts is not a phrase in the syntactic structure of the first clause:
</figureCaption>
<figure confidence="0.257485">
(46a) Mary adores peanuts but Fred doesn&apos;t.
r
{ [Mary adores peanuts] [but Fred doesn&apos; tll
</figure>
<bodyText confidence="0.988222153846154">
Inheritance is relevant to these questions because notions such as &apos;French-house&apos;12
and &apos;adoring-peanuts&apos; can be located in an inheritance hierarchy between the more
general notions denoted by their heads (&apos;house&apos; or &apos;adoring&apos;) and the more specific ones
denoted by the complete phrase (&apos;typical-French-house,&apos; Mary-adoring-peanuts&apos;):
(47) house adoring
French-house adoring-peanuts
typical-French-house Mary-adoring-peanuts
As usual, each of these concepts inherits all the properties of the concepts above it in
the hierarchy except where these are overridden.&amp;quot;
It is easy to see how a dependency grammar can generate concepts at the top
and bottom of these hierarchies. The top concept comes straight from the lexical entry
for the root word (e.g. HOUSE, ADORE), and the bottom one belongs to the word
token—the word in the sentence concerned (e.g. the word house in the phrase typical
</bodyText>
<footnote confidence="0.754927111111111">
12 The hyphen in &apos;French-house&apos; is needed because this is an atomic name whose internal structure is
irrelevant.
13 Overriding is found in well-known examples such as fake diamond, not to mention ordinary negative
sentences such as Mary didn&apos;t jump. A fake diamond is an object that inherits some of the properties of
diamonds, especially the visible ones, but not all, and in particular not those that are criterial in the
trade. If the sense of Mary jumped (i.e. the kind of situation to which it can refer) is P. then the referent
of Mary didn&apos;t jump (i.e. the actual situation to which it refers) is NOT: P, in which we know nothing
about the situation except that it is not one in which Mary jumped. (The relevant rule is given in the
Appendix).
</footnote>
<page confidence="0.995356">
150
</page>
<note confidence="0.839596">
Norman M. Fraser and Richard A. Hudson Inheritance in Word Grammar
</note>
<bodyText confidence="0.999693666666667">
French house). The problem is how to generate concepts in between the two, and the
WG solution is to recognize the notion head-sense: the head-sense of some word W
is the concept that results from combining W with its head. Thus the head-sense of
French in our example is the result of combining French with house (as adjunct and
head respectively); and that of peanuts in Mary adores peanuts is the result of combining
peanuts with adores. This is how we generate semantic structures that contain &apos;French-
house&apos; and &apos;adoring-peanuts&apos; without recognizing French house or adores peanuts as
units in the syntax.
The rules that allow head-senses include these:
</bodyText>
<listItem confidence="0.9589065">
(48a) dependent of word has 1 head-sense.
(48b) referent of word = a dependent of head-sense of it.
</listItem>
<bodyText confidence="0.9997342">
By the first rule, every dependent of a word has a head-sense, i.e. makes a distinct
contribution, in combination with its head, to the sentence&apos;s meaning. The notion
&apos;head-sense&apos; is thus a functor that maps the senses of the dependent and the head onto
a third concept, applying equally to complements such as peanuts in adores peanuts and
to adjuncts such as French in French houses. There is nothing quite like it in standard
semantic systems such as Montague Grammar, but it applies in conjunction with rather
more standard functors which each pick out one particular (semantic) role as the
one filled by the dependent&apos;s referent (by rule (48b)). Generally speaking, this role
is defined by just one of the words concerned, according to whether the dependent
is an adjunct or a complement. If it is an adjunct, it defines its own semantic role
(e.g. French defines its own semantic role as &apos;nationality&apos; or &apos;location&apos;), but if it is a
complement then it leaves the head to define its role (e.g. adores provides the role
&apos;adoree,&apos; or whatever it should be called, for its complement).
The relevance to inheritance is that all the different head-senses are held together
by inheritance. These are the basic rules:
</bodyText>
<listItem confidence="0.766854">
(49a) head-sense of dependent of word isa sense of it.
(49b) referent of word isa head-sense of dependent of it.
</listItem>
<bodyText confidence="0.994559533333333">
That is, the combination of a word and one of its dependents yields a concept that is
normally a particular case of the concept defined by the word on its own (a French
house is a kind of house; adoring peanuts is a kind of adoring), and whatever a word
refers to must be a particular case of all the concepts defined by it in combination with
its dependents (e.g., the particular situation referred to by Mary adores peanuts must be
one that is both an example of adoring peanuts and of Mary adoring).
We can impose further structure on the semantics; for example, by requiring all
other head-senses to build on that of the object:
(50) head-sense of dependent of word isa head-sense of object of it.
This is equivalent to an ordered procedure that combines the verb with its object be-
fore it takes account of any other dependents. This has the attraction of a Categorial
Grammar approach, in which dependents are added one at a time and subtle distinc-
tions of grouping can be made e.g., among the complements within VP; but it has the
advantage of not requiring a similar binary bracketing in the syntax. Why is this an
advantage? Some of the drawbacks of binary syntactic structures are obvious—e.g.,
</bodyText>
<page confidence="0.993303">
151
</page>
<note confidence="0.635106">
Computational Linguistics Volume 18, Number 2
</note>
<bodyText confidence="0.999869">
the need for far more phrasal nodes each carrying much the same information. How-
ever, the advantage of our system that we should like to draw attention to here is the
possibility of reducing the amount of ambiguity.
For example, in WG the sequence cheap book about linguistics has just one possible
syntactic analysis, the usual flat analysis with both cheap and about as dependents of
book, but its semantics contains the concepts &apos;cheap book&apos; and &apos;book about linguistics&apos;
respectively. These interpretations can be seen from examples such as the following,
where the sense of ONE is based on the semantic structure of the antecedent phrase.
</bodyText>
<listItem confidence="0.60101875">
(51a) I wanted a cheap book about linguistics but I could only find one
about cricket.
(51b) I wanted a cheap book about linguistics but I could only find a
dear one.
</listItem>
<bodyText confidence="0.999959666666667">
In a standard approach, the first clause has to be given two distinct syntactic structures,
one containing the unit cheap book and the other book about linguistics; but this ambiguity
cannot be resolved until the end of the second clause. Our judgment is that the first
clause is not in fact semantically ambiguous in this way; and according to our approach
there is no need to postulate such ambiguity since both concepts &apos;cheap book&apos; and
&apos;book about linguistics&apos; are available there, in addition to the unifying concept &apos;cheap
book about linguistics&apos; (which could be identified in relation to book as its supersense,
the concept that is an instance of the head-sense of every dependent). Here is the
relevant part of the structure of both (51a) and (51b):
</bodyText>
<equation confidence="0.926607333333333">
(55) c6
price subject
c4 c2 c3 c5 c5
</equation>
<bodyText confidence="0.5830115">
matter
adjunct I complement complement
cheap book about linguistics
In this diagram, the concepts are as follows:
</bodyText>
<footnote confidence="0.739746111111111">
c1 &apos;book&apos;
c2 &apos;cheap book&apos;
c3 &apos;book about linguistics&apos;
c4 &apos;cheap&apos;
c5 &apos;about linguistics&apos;
c6 &apos;cheap book about linguistics&apos;
The isa relations in this structure provide the basis for ordinary default inheritance;
so if a book has pages, then so do a cheap book, a book about linguistics, and a
cheap book about linguistics. They also allow defaults to be overridden in the usual
</footnote>
<page confidence="0.991725">
152
</page>
<note confidence="0.84053">
Norman M. Fraser and Richard A. Hudson Inheritance in Word Grammar
</note>
<bodyText confidence="0.99774475">
way; this is precisely the function of ordinary adjectives and adverbs like CHEAP
or QUICKLY, which mean &apos;more cheap/quickly than the default,&apos; where the default
value is the average for the category concerned. The theoretical apparatus that allows
these meanings is precisely the same as the one we apply in morphology and syntax.
</bodyText>
<sectionHeader confidence="0.923299" genericHeader="method">
5. Conclusion
</sectionHeader>
<bodyText confidence="0.999930533333333">
We have shown that it is possible, and fruitful, to develop a general theory of default
inheritance that is equally relevant, and equally important, for all levels of linguistic
analysis. We have demonstrated this in relation to syntax and semantics, and by oc-
casional allusions to morphology, but we assume it is equally true of phonology and
of pragmatics (in every sense).
This conclusion, if true, is important for linguists and psychologists, because it
shows that the structures found within language are formally similar in at least some
respects to those found in general knowledge, and that both kinds of knowledge are
processed in ways that are similar in at least some important respects. And our conclu-
sion is also important for computational linguists because it indicates the possibility
of a single very general inheritance mechanism that can be applied at every stage in
the parsing process, and also in the manipulation of knowledge structures. A second
conclusion, however, is that default inheritance should be based on the principle of
stipulated overriding (by means of &apos;NOT:...&apos; propositions), rather than on automatic
overriding. This conclusion conflicts directly with standard theory and practice.
</bodyText>
<sectionHeader confidence="0.943469" genericHeader="method">
References
</sectionHeader>
<reference confidence="0.999112030769231">
Anderson, John M. (1971). The Grammar of
Case: Towards a Localistic Theory.
Cambridge: Cambridge University Press.
Anderson, John M. (1977). On Case Grammar:
Prolegomena to a Theory of Grammatical
Relations. London: Croom Helm.
Bloomfield, Leonard (1933). Language.
London: Allen and Unwin.
Brachman, Ronald J. (1985). &amp;quot;I lied about
the trees, or defaults and definitions in
knowledge representation.&amp;quot; Al Magazine
6,80-93.
Calder, Jonathan (1989). &amp;quot;Paradigmatic
morphology&amp;quot; In Proceedings, Fourth
Conference of the European Chapter of the
Association for Computational Linguistics,
Manchester, U.K., April, 58-59.
Cook, S., ed. (1989). Proceedings, 1989
European Conference on Object Oriented
Programming. Cambridge: Cambridge
University Press.
Covington, Michael A. (1991). &amp;quot;Parsing
discontinuous constituents in dependency
grammar.&amp;quot; Computational Linguistics 16(4),
234-236.
Daelemans, Walter (1987). Studies in language
technology: An object-oriented computer model
of morphosyntactic aspects of Dutch. Doctoral
dissertation, Katholieke Universiteit
Leuven.
Dahl, Osten (1980). &amp;quot;Some arguments for
higher nodes in syntax: A reply to
Hudson&apos;s &apos;Constituency and
dependency&apos;.&amp;quot; Linguistics 18,485-488.
De Smedt, Koenraad (1984). &amp;quot;Using
object-oriented knowledge representation
techniques in morphology and syntax
programming.&amp;quot; ECAI-84, 181-184.
Etherington, David R., and Reiter, Raymond
(1983). &amp;quot;On inheritance hierarchies with
exceptions.&amp;quot; Proceedings, AAAI-83.
Washington, 104-108.
Etherington, David R. (1988). Reasoning with
Incomplete Information. Los Altos: Morgan
Kaufmann.
Flickinger, Daniel P. (1987). Lexical rules in the
hierarchical lexicon. Doctoral dissertation,
Stanford University, California.
Flickinger, Daniel P.; Pollard, Carl J.; and
Wasow, Thomas (1985). &amp;quot;Structure sharing
in lexical representation.&amp;quot; In Proceedings,
23rd Annual Meeting of the Association for
Computational Linguistics, Chicago,
262-267.
Fraser, Norman M. (1989). &amp;quot;Parsing and
dependency grammar.&amp;quot; In UCL Working
Papers in Linguistics 1, edited by Robyn
Carston, 296-319. London: University
College London.
Gaifman, Haim (1965). &amp;quot;Dependency
systems and phrase-structure systems.&amp;quot;
Information and Control 8,304-337.
Giachin, Egidio R, and Rullent, Claudio
(1989). &amp;quot;A parallel parser for spoken
natural language.&amp;quot; IICAI-89, 1537-1542.
</reference>
<page confidence="0.995753">
153
</page>
<note confidence="0.567211">
Computational Linguistics Volume 18, Number 2
</note>
<reference confidence="0.997253115384615">
Gibbon, Dafydd (1990). &amp;quot;Prosodic
association by template inheritance.&amp;quot; In
Proceedings, Workshop on Inheritance in
Natural Language Processing, edited by
Walter Daelemans and Gerald Gazdar,
65-81. Tilburg: ITK.
Hays, David (1964). &amp;quot;Dependency theory: a
formalism and some observations.&amp;quot;
Language 40,511-525.
Hellwig, Peter (1986). &amp;quot;Dependency
Unification Grammar (DUG).&amp;quot;
COLING-86, 195-198.
Hudson, Richard A. (1976). Arguments for a
Non-Transformational Grammar. Chicago:
University of Chicago Press.
Hudson, Richard A. (1984). Word Grammar.
Oxford: Blackwell.
Hudson, Richard A. (1989). &amp;quot;Towards a
computer testable word grammar of
English.&amp;quot; In UCL Working Papers in
Linguistics 1, edited by Robyn Carston,
321-339. London: University College
London.
Hudson, Richard A. (1990). English Word
Grammar. Oxford: Blackwell.
Jackendoff, Ray (1983). Semantics and
Cognition. Cambridge, MA: The MIT
Press.
Jappinen, Harri; Lassila, Eero; and Lehtola,
Aarno (1988). &amp;quot;Locally governed trees
and dependency parsing.&amp;quot; COLING-88,
275-277.
Johnson, Rod; King, Maghi; and des Tombe,
Louis (1985). &amp;quot;EUROTRA: A multilingual
system under development.&amp;quot;
Computational Linguistics 11,155-169.
Maruyama, Hiroshi (1990). &amp;quot;Structural
disambiguation with constraint
propagation.&amp;quot; In Proceedings, 28th Annual
Meeting of the Association for Computational
Linguistics, 31-38.
Menuk, Igor A. (1988). Dependency Syntax:
Theory and Practice. Albany, NY: State
University of New York Press.
Mel&apos;Cuk, Igor A., and Zolkovslcij, Alexander
K. (1970). &amp;quot;Towards a functioning
&apos;Meaning-Text&apos; model of language.&amp;quot;
Linguistics 57,10-47.
Pullum, Geoffrey K. (1979). Rule Interaction
and the Organization of a Grammar. London:
Garland.
Reinhard, Sabine, and Gibbon, Dafydd
(1991). &amp;quot;Prosodic inheritance and
morphological generalisations.&amp;quot; In
Proceedings, Fifth Conference of the European
Chapter of the Association for Computational
Linguistics. April, Berlin, 131-136.
Robinson, Jane J. (1970). &amp;quot;Dependency
structures and transformational rules.&amp;quot;
Language 46,259-285.
Schubert, Klaus (1987). Metataxis: Contrastive
Dependency Syntax for Machine Translation.
Dordrecht: Foris.
Sgall, Petr; Hajieova, Eva; and Panevova,
Jarmila (1986). The Meaning of the Sentence
in its Semantic and Pragmatic Aspects.
Prague: Academia.
Sgall, Petr, and Panevova, Jarmila (1987).
&amp;quot;Machine translation, linguistics, and
interlingua.&amp;quot; In Proceedings, Third
Conference of the European Chapter of the
Association for Computational Linguistics,
99-108.
Shieber, Stuart M. (1986). An Introduction to
Unification-Based Approaches to Grammar.
Stanford, CA: CSLI.
Sigurd, Bengt (1989). &amp;quot;DEPARSE-An
experimental dependency parser.&amp;quot; In
Praktisk Lingvistik 12, edited by B. Sigurd;
M. Eeg-Olofsson; L. Eriksson;
B. Gawronska-Werngren; K. Holmqvist;
and P. Touati, 30-41. Lunds: Lunds
Universitet.
Starosta, Stanley (1988). The Case for Lexicase:
An Outline of Lexicase Grammatical Theory.
London: Pinter.
Starosta, Stanley, and Nomura, H. (1986).
&amp;quot;Lexicase parsing: A lexicon-driven
approach to syntactic analysis.&amp;quot;
COLING-86, 127-132.
Tesniere, Lucien (1959). Elements de Syntaxe
Struciurale. Paris: Librairie Klincksieck.
Thomason, Richmond H., and Touretzky,
David F. (1991). &amp;quot;Inheritance theory and
networks with roles.&amp;quot; In Principles of
Semantic Networks, edited by John
F. Sowa, 231-266. San Mateo, CA: Morgan
Kaufmann.
Touretzky, David F. (1986). The Mathematics
of Inheritance Systems. Los Altos: Morgan
Kaufmann.
Warner, Anthony (1987). &amp;quot;Multiple heads
and minor categories in GPSG.&amp;quot;
Linguistics 27,179-205.
</reference>
<page confidence="0.999752">
154
</page>
<note confidence="0.919405">
Norman M. Fraser and Richard A. Hudson Inheritance in Word Grammar
</note>
<sectionHeader confidence="0.984657" genericHeader="method">
Appendix
</sectionHeader>
<bodyText confidence="0.99944225">
This appendix contains a selection of WG propositions that are sufficient to generate
simple syntactic, morphological, and semantic structures for the sentences Mary jumped,
Mary didn&apos;t jump, Did Mary jump?, and Didn&apos;t Mary jump? After presenting the grammar,
we give an analysis of Mary jumped.
</bodyText>
<sectionHeader confidence="0.97532" genericHeader="method">
GRAMMAR
</sectionHeader>
<reference confidence="0.745637333333333">
Word classes
noun isa word.
proper isa noun.
verb isa word.
polarity-verb isa verb.
Morphosyntactic features
</reference>
<bodyText confidence="0.9958475">
verb has 1 finiteness.
finiteness of verb = {/: finite, non-finite}.
non-finite verb has 1 aspect.
aspect of verb = {/: infinitive, perfect, participle}.
finite verb has 1 mood.
mood of verb = {/: tensed, imperative}.
tensed verb has 1 tense.
tense of verb = {/: past, present}.
tensed polarity-verb has 1 polarity.
polarity of verb = {/: positive, negative}.
tensed polarity-verb has 1 sv-order.
sv-order of verb = {/: s+v, v+s}.
</bodyText>
<sectionHeader confidence="0.815226" genericHeader="method">
Regular morphology
</sectionHeader>
<bodyText confidence="0.905206916666667">
body isa form.
ed-form isa body.
base isa body.
n&apos;t-form isa form.
past verb has 1 ed-form.
whole of ed-form of verb = stem of it + whole of mEd.
whole of mEd = &lt;ed&gt;.
negative verb has 1 n&apos;t-form.
whole of n&apos;t-form of verb = whole of body of it + whole of mN&apos;t.
whole of mN&apos;t = &lt;n&apos;t&gt;.
infinitive verb has 1 base.
whole of base of word = stem of it.
</bodyText>
<sectionHeader confidence="0.919288" genericHeader="method">
Dependencies
</sectionHeader>
<reference confidence="0.9888542">
predependent isa dependent.
postdependent isa dependent.
subject isa predependent.
complement isa postdependent.
xcomplement isa complement.
</reference>
<page confidence="0.99528">
155
</page>
<note confidence="0.24761">
Computational Linguistics Volume 18, Number 2
</note>
<bodyText confidence="0.976285272727273">
Syntactic valency
word has 1 head.
finite verb has [0-1] head.
word has 0 subject.
verb has 1 subject.
NOT: tensed verb has 0 subject.
subject of verb isa noun.
word has 0 complement.
polarity-verb has 1 xcomplement.
xcomplement of polarity-verb isa infinitive verb.
Raising/control
subject of xcomplement of word = subject of it.
Word order
position of dependent of word = after it.
position of predependent of word = before it.
NOT: position of predependent of word = after it.
NOT: position of subject of v+s verb = before it.
NOT: NOT: position of subject of v+s verb = after it.
Semantics: sense and reference
word has 1 meaning.
referent isa meaning.
sense isa meaning.
proper has 0 sense.
NOT: proper has 1 sense.
referent of word isa sense of it.
NOT: referent of negative verb isa sense of it.
NOT: proper has 1 sense.
Semantics: words and other actions
word isa action.
action has 1 time.
action has 1 actor.
actor of sense of word = referent of subject of it.
time of referent of past verb = before it.
</bodyText>
<reference confidence="0.940018">
&apos;Lexicon&apos;
MARY isa proper.
whole of MARY = &lt;Mary&gt;.
referent of MARY = Mary.
DO isa verb.
DO/dummy isa DO.
DO/dummy isa polarity-verb.
stem of DO = &lt;do&gt;.
whole of past DO = &lt;did&gt;.
NOT: whole of past DO = stem of it + whole of mEd.
meaning of DO/dummy = meaning of xcomplement of it.
</reference>
<page confidence="0.996733">
156
</page>
<note confidence="0.699836">
Norman M. Fraser and Richard A. Hudson Inheritance in Word Grammar
</note>
<bodyText confidence="0.688722">
JUMP isa verb.
stem of JUMP = &lt;jump&gt;.
sense of JUMP = jumping.
jumping isa action.
</bodyText>
<sectionHeader confidence="0.724723" genericHeader="method">
SENTENCE ANALYSIS
</sectionHeader>
<bodyText confidence="0.869392222222222">
The following is an analysis of Mary jumped, in which we present all the propositions
about its constituent words (wl, w2) that can be inherited from the grammar on the
basis of the propositions in B, the &apos;calculated&apos; properties. The latter are of course not
generated by inheritance but by applying some kind of &apos;Best Fit Principle&apos; to the
observable propositions in A plus the already inheritable propositions.
A. Observable propositions
whole of w1 = &lt;Mary&gt;.
whole of w2 = &lt;jumped&gt;.
position of wl = before w2.
</bodyText>
<subsectionHeader confidence="0.589441">
B. Calculated propositions
</subsectionHeader>
<bodyText confidence="0.637260666666667">
w1 isa singular MARY.
w2 isa past tensed finite JUMP.
subject of w2 = wl.
head of wl = w2.
finiteness of w2 = finite.
mood of w2 = tensed.
tense of w2 = past.
sense of w2 = c1.
referent of w2 = c2.
</bodyText>
<reference confidence="0.9819644">
C. Inherited propositions
wl isa proper.
wl isa noun.
w1 isa word.
w1 isa action.
w1 has 1 time.
w1 has 1 actor.
wl has 1 head.
w1 has 1 referent.
referent of w1 = Mary.
w2 isa past tensed finite verb.
w2 isa tensed finite verb.
etc.
w2 isa verb.
w2 has 1 finiteness.
w2 has 1 mood.
w2 has 1 tense.
w2 has [0-1J head.
w2 has 1 subject.
subject of w2 isa noun.
</reference>
<page confidence="0.94042">
157
</page>
<reference confidence="0.878403125">
Computational Linguistics Volume 18, Number 2
position of subject of w2 = before w2.
w2 has 0 complement.
w2 has 1 sense.
w2 has 1 referent.
c1 = jumping.
c1 isa action.
c1 has 1 time.
c1 has 1 actor.
c2 isa jumping.
c2 isa action.
c2 has 1 time.
c2 has 1 actor.
time of c2 = before w2.
actor of cl = Mary.
actor of c2 = Mary.
</reference>
<page confidence="0.996849">
158
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.945011">
<title confidence="0.999213">Inheritance in Word Grammar</title>
<author confidence="0.999977">Norman M Fraser Richard A Hudsont</author>
<affiliation confidence="0.997868">University of Surrey University College London</affiliation>
<abstract confidence="0.990580333333333">This paper describes the central role played by default inheritance in Word Grammar, a theory of language knowledge and processing. A single formalism is used to represent knowledge at the levels of morphology, syntax, and semantics. A single rule of inference is used to inherit knowledge at all of these levels. This rule is distinctive in that it requires defaults to be explicitly overridden in the case of exceptions. The explicit overriding rule is used in syntax to achieve what other theories achieve by means of transformations, metarules, or lexical rules.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>John M Anderson</author>
</authors>
<title>The Grammar of Case: Towards a Localistic Theory. Cambridge:</title>
<date>1971</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="30420" citStr="Anderson 1971" startWordPosition="5054" endWordPosition="5055">actic structure is expressed in terms of dependencies between pairs of words, one of which is the head of the other, its dependent. Higher nodes such as phrases or sentences are not represented explicitly in the grammar. WG is thus a variety of dependency grammar. Dependency grammar was first formalized by Tesniere (1959) and refined by Hays (1964), Gaifman (1965), Robinson (1970) and others. A number of dependencybased theories have emerged from the linguistic underground during the last thirty years. These include the Meaning-Text model (Menuk and Zolkovskij 1970; Menuk 1988), Case Grammar (Anderson 1971; 1977), Daughter Dependency Grammar (Hudson 1976), WG (Hudson 1984; 1990), Functional Generative Description (Sgall, Hajieova, and Panevova 1986), and Lexicase (Starosta 1988). While none of these theories has attained widespread popularity, some of their central insights have become increasingly influential in the phrase structure grammar mainstream. For example, the trend toward head-driven approaches, the prominence of notions such as &apos;government,&apos; the explicit use of grammatical relations and case, and the reduced amount of information carried in phrasal categories all reflect the general</context>
</contexts>
<marker>Anderson, 1971</marker>
<rawString>Anderson, John M. (1971). The Grammar of Case: Towards a Localistic Theory. Cambridge: Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John M Anderson</author>
</authors>
<title>On Case Grammar: Prolegomena to a Theory of Grammatical Relations.</title>
<date>1977</date>
<location>London: Croom Helm.</location>
<marker>Anderson, 1977</marker>
<rawString>Anderson, John M. (1977). On Case Grammar: Prolegomena to a Theory of Grammatical Relations. London: Croom Helm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonard Bloomfield</author>
</authors>
<date>1933</date>
<publisher>Allen</publisher>
<location>Language. London:</location>
<contexts>
<context position="34536" citStr="Bloomfield (1933)" startWordPosition="5728" endWordPosition="5729">could allow finite verbs to inherit the regular &apos;1 head&apos; simply by not blocking it, and allow for &apos;0 head&apos; by an extra rule, which provides the other alternative. 5 The notion of a word with two heads is meaningless in theories based on phrase structure, because &apos;head&apos; is used there in relation to phrases, not words. The X-bar &apos;head&apos; corresponds to our &apos;root,&apos; the word in a phrase that has no head inside that phrase. It is true that some linguists have suggested that a phrase might have more than one head (e.g. Warner 1987), and this has been a standard analysis of coordinate structures since Bloomfield (1933); but this is very different from a single word having more than one head. 145 Computational Linguistics Volume 18, Number 2 (In a dependency diagram, the arrow points towards the dependent.) This is permitted by a proposition which, at least by implication, overrides the general rule, and which refers to the grammatical function &apos;xcomplement&apos;:6 (33) subject of xcomplement of word = subject of it. In other words, a word may have two heads provided that one of them is the xcomplement of the other. (We return below to the relations among the grammatical functions such as &apos;subject&apos; and &apos;xcompleme</context>
</contexts>
<marker>Bloomfield, 1933</marker>
<rawString>Bloomfield, Leonard (1933). Language. London: Allen and Unwin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald J Brachman</author>
</authors>
<title>I lied about the trees, or defaults and definitions in knowledge representation.&amp;quot;</title>
<date>1985</date>
<journal>Al Magazine</journal>
<pages>6--80</pages>
<contexts>
<context position="2480" citStr="Brachman 1985" startWordPosition="380" endWordPosition="381">hey are based on typical cases and their features, any of which may be overridden. Thus the shading from core members of a class to peripheral members can be accommodated—indeed, the existence of peripheral members is predicted by the mechanism for overriding defaults. The third and more pragmatic reason why it is useful to recast well-known linguistic problems in terms of default inheritance is that there is a fairly well-developed—though by no means conclusive—body of knowledge on the subject in the artificial intelligence field of knowledge representation (e.g. Etherington and Reiter 1983; Brachman 1985; Touretzky 1986; Etherington 1988). Nearer the computer science mainstream, work in object-oriented programming languages (Cook 1989) offers an interesting range of relevant insights and inheritancebased tools. * Social and Computer Sciences Research Group, University of Surrey, Guildford, Surrey, GU2 5XH, United Kingdom. E-mail: norman@soc.surrey.ac.uk. t Department of Phonetics and Linguistics, University College London, Gower Street, London, WC1E 6BT, United Kingdom. E-mail: thudson@ucl.ac.uk. 1 We received very helpful comments on an earlier draft of this paper from three anonymous Comput</context>
</contexts>
<marker>Brachman, 1985</marker>
<rawString>Brachman, Ronald J. (1985). &amp;quot;I lied about the trees, or defaults and definitions in knowledge representation.&amp;quot; Al Magazine 6,80-93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Calder</author>
</authors>
<title>Paradigmatic morphology&amp;quot; In</title>
<date>1989</date>
<booktitle>Proceedings, Fourth Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>58--59</pages>
<location>Manchester, U.K.,</location>
<contexts>
<context position="3815" citStr="Calder 1989" startWordPosition="564" endWordPosition="565">kshop on Inheritance in Natural Language Processing, Tilburg, August 1990. C) 1992 Association for Computational Linguistics Computational Linguistics Volume 18, Number 2 In recent years, linguists and computational linguists in particular have begun to explore problems at most linguistic levels within inheritance frameworks. For example, Gibbon and Reinhard have proposed inheritance-based solutions to problems of phonology and prosody (Gibbon 1990; Reinhard and Gibbon 1991). Most work to date has centered on morphology (e.g. De Smedt 1984; Flickinger, Pollard, and Wasow 1985; Daelemans 1987; Calder 1989). A certain amount has also been achieved in syntax (e.g. De Smedt 1984; Flickinger 1987), where inheritance is used to construct subcategorization frames for words. As for semantics, there has been a great deal of work on inheritance in so-called &apos;semantic networks,&apos; but much of this work relates only loosely to the semantics of natural language. The work we present in this paper differs from all previous work in natural language processing (NLP) in at least two respects. Firstly, it is distinctive in the extent to which inheritance is used. Within our framework knowledge at all levels (morph</context>
</contexts>
<marker>Calder, 1989</marker>
<rawString>Calder, Jonathan (1989). &amp;quot;Paradigmatic morphology&amp;quot; In Proceedings, Fourth Conference of the European Chapter of the Association for Computational Linguistics, Manchester, U.K., April, 58-59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Cook</author>
<author>ed</author>
</authors>
<title>Proceedings,</title>
<date>1989</date>
<booktitle>European Conference on Object Oriented Programming. Cambridge:</booktitle>
<publisher>Cambridge University Press.</publisher>
<marker>Cook, ed, 1989</marker>
<rawString>Cook, S., ed. (1989). Proceedings, 1989 European Conference on Object Oriented Programming. Cambridge: Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael A Covington</author>
</authors>
<title>Parsing discontinuous constituents in dependency grammar.&amp;quot;</title>
<date>1991</date>
<journal>Computational Linguistics</journal>
<volume>16</volume>
<issue>4</issue>
<pages>234--236</pages>
<contexts>
<context position="32062" citStr="Covington (1991)" startWordPosition="5300" endWordPosition="5301">head, though the same word may act as head to more than one other word, its dependents. As in other theories, just one word is allowed to be an exception to this rule; we call this word the &apos;root&apos; of the sentence. This has (by definition) no head, and is generally a finite verb; e.g. in Mary didn&apos;t jump, the polarity verb (&apos;auxiliary verb&apos;) didn&apos;t is the root, on which both Mary and jump 3 The last decade has seen increased interest in dependency grammar among computational linguists. Dependency grammar has been applied in the experimental parsing systems of Hellwig (1986), Sigurd (1989), and Covington (1991); in the &apos;Kielikone&apos; natural language interface of Jappinen, Lassila, and Lahtola (1988); in the machine translation systems of EUROTRA (Johnson, King, and des Tombe 1985), DLT (Schubert 1987), Charles University (Sgall and Panevova 1987), and IBM Tokyo (Maruyama 1990); and in the speech recognition system of Giachin and Rullent (1989). Parsers based on the theories of Lexicase (Starosta and Nomura 1986) and Word Grammar (Fraser 1989; Hudson 1989) have also been developed. 144 Norman M. Fraser and Richard A. Hudson Inheritance in Word Grammar depend. Here, then, we already have a simple exampl</context>
</contexts>
<marker>Covington, 1991</marker>
<rawString>Covington, Michael A. (1991). &amp;quot;Parsing discontinuous constituents in dependency grammar.&amp;quot; Computational Linguistics 16(4), 234-236.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
</authors>
<title>Studies in language technology: An object-oriented computer model of morphosyntactic aspects of Dutch. Doctoral dissertation,</title>
<date>1987</date>
<institution>Katholieke Universiteit Leuven.</institution>
<contexts>
<context position="3801" citStr="Daelemans 1987" startWordPosition="562" endWordPosition="563">pants at the Workshop on Inheritance in Natural Language Processing, Tilburg, August 1990. C) 1992 Association for Computational Linguistics Computational Linguistics Volume 18, Number 2 In recent years, linguists and computational linguists in particular have begun to explore problems at most linguistic levels within inheritance frameworks. For example, Gibbon and Reinhard have proposed inheritance-based solutions to problems of phonology and prosody (Gibbon 1990; Reinhard and Gibbon 1991). Most work to date has centered on morphology (e.g. De Smedt 1984; Flickinger, Pollard, and Wasow 1985; Daelemans 1987; Calder 1989). A certain amount has also been achieved in syntax (e.g. De Smedt 1984; Flickinger 1987), where inheritance is used to construct subcategorization frames for words. As for semantics, there has been a great deal of work on inheritance in so-called &apos;semantic networks,&apos; but much of this work relates only loosely to the semantics of natural language. The work we present in this paper differs from all previous work in natural language processing (NLP) in at least two respects. Firstly, it is distinctive in the extent to which inheritance is used. Within our framework knowledge at all</context>
</contexts>
<marker>Daelemans, 1987</marker>
<rawString>Daelemans, Walter (1987). Studies in language technology: An object-oriented computer model of morphosyntactic aspects of Dutch. Doctoral dissertation, Katholieke Universiteit Leuven.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Osten Dahl</author>
</authors>
<title>Some arguments for higher nodes in syntax: A reply to Hudson&apos;s &apos;Constituency and dependency&apos;.&amp;quot;</title>
<date>1980</date>
<journal>Linguistics</journal>
<pages>18--485</pages>
<contexts>
<context position="45738" citStr="Dahl 1980" startWordPosition="7595" endWordPosition="7596">tic dependency structure is completely flat, so it does not provide any units between the individual word and the complete phrase (where &apos;phrase&apos; means a word and the complete set of all its dependents and their respective phrases). For example, the semantic structure for the phrase typical French house has to mention the concept &apos;French house&apos; (a typical French house is a house that is typical as a 149 Computational Linguistics Volume 18, Number 2 French house, and not just as a house); but a flat dependency structure such as (45) provides no syntactic unit larger than the individual &apos;words (Dahl 1980). (45) typical French house Similarly, how can we handle &apos;VP-anaphora&apos; without a VP node? For example, we need the concept &apos;adore peanuts&apos; as part of the semantic structure of Fred doesn&apos;t in (46a), but adores peanuts is not a phrase in the syntactic structure of the first clause: (46a) Mary adores peanuts but Fred doesn&apos;t. r { [Mary adores peanuts] [but Fred doesn&apos; tll Inheritance is relevant to these questions because notions such as &apos;French-house&apos;12 and &apos;adoring-peanuts&apos; can be located in an inheritance hierarchy between the more general notions denoted by their heads (&apos;house&apos; or &apos;adoring&apos;)</context>
</contexts>
<marker>Dahl, 1980</marker>
<rawString>Dahl, Osten (1980). &amp;quot;Some arguments for higher nodes in syntax: A reply to Hudson&apos;s &apos;Constituency and dependency&apos;.&amp;quot; Linguistics 18,485-488.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koenraad De Smedt</author>
</authors>
<title>Using object-oriented knowledge representation techniques in morphology and syntax programming.&amp;quot;</title>
<date>1984</date>
<pages>84--181</pages>
<marker>De Smedt, 1984</marker>
<rawString>De Smedt, Koenraad (1984). &amp;quot;Using object-oriented knowledge representation techniques in morphology and syntax programming.&amp;quot; ECAI-84, 181-184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David R Etherington</author>
<author>Raymond Reiter</author>
</authors>
<title>On inheritance hierarchies with exceptions.&amp;quot;</title>
<date>1983</date>
<booktitle>Proceedings, AAAI-83.</booktitle>
<pages>104--108</pages>
<location>Washington,</location>
<contexts>
<context position="2465" citStr="Etherington and Reiter 1983" startWordPosition="376" endWordPosition="379">in the inheritance approach they are based on typical cases and their features, any of which may be overridden. Thus the shading from core members of a class to peripheral members can be accommodated—indeed, the existence of peripheral members is predicted by the mechanism for overriding defaults. The third and more pragmatic reason why it is useful to recast well-known linguistic problems in terms of default inheritance is that there is a fairly well-developed—though by no means conclusive—body of knowledge on the subject in the artificial intelligence field of knowledge representation (e.g. Etherington and Reiter 1983; Brachman 1985; Touretzky 1986; Etherington 1988). Nearer the computer science mainstream, work in object-oriented programming languages (Cook 1989) offers an interesting range of relevant insights and inheritancebased tools. * Social and Computer Sciences Research Group, University of Surrey, Guildford, Surrey, GU2 5XH, United Kingdom. E-mail: norman@soc.surrey.ac.uk. t Department of Phonetics and Linguistics, University College London, Gower Street, London, WC1E 6BT, United Kingdom. E-mail: thudson@ucl.ac.uk. 1 We received very helpful comments on an earlier draft of this paper from three a</context>
</contexts>
<marker>Etherington, Reiter, 1983</marker>
<rawString>Etherington, David R., and Reiter, Raymond (1983). &amp;quot;On inheritance hierarchies with exceptions.&amp;quot; Proceedings, AAAI-83. Washington, 104-108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David R Etherington</author>
</authors>
<title>Reasoning with Incomplete Information.</title>
<date>1988</date>
<publisher>Morgan Kaufmann.</publisher>
<location>Los Altos:</location>
<contexts>
<context position="2515" citStr="Etherington 1988" startWordPosition="384" endWordPosition="385">and their features, any of which may be overridden. Thus the shading from core members of a class to peripheral members can be accommodated—indeed, the existence of peripheral members is predicted by the mechanism for overriding defaults. The third and more pragmatic reason why it is useful to recast well-known linguistic problems in terms of default inheritance is that there is a fairly well-developed—though by no means conclusive—body of knowledge on the subject in the artificial intelligence field of knowledge representation (e.g. Etherington and Reiter 1983; Brachman 1985; Touretzky 1986; Etherington 1988). Nearer the computer science mainstream, work in object-oriented programming languages (Cook 1989) offers an interesting range of relevant insights and inheritancebased tools. * Social and Computer Sciences Research Group, University of Surrey, Guildford, Surrey, GU2 5XH, United Kingdom. E-mail: norman@soc.surrey.ac.uk. t Department of Phonetics and Linguistics, University College London, Gower Street, London, WC1E 6BT, United Kingdom. E-mail: thudson@ucl.ac.uk. 1 We received very helpful comments on an earlier draft of this paper from three anonymous Computational Linguistics readers, to who</context>
</contexts>
<marker>Etherington, 1988</marker>
<rawString>Etherington, David R. (1988). Reasoning with Incomplete Information. Los Altos: Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel P Flickinger</author>
</authors>
<title>Lexical rules in the hierarchical lexicon. Doctoral dissertation,</title>
<date>1987</date>
<institution>Stanford University,</institution>
<location>California.</location>
<contexts>
<context position="3904" citStr="Flickinger 1987" startWordPosition="579" endWordPosition="580">sociation for Computational Linguistics Computational Linguistics Volume 18, Number 2 In recent years, linguists and computational linguists in particular have begun to explore problems at most linguistic levels within inheritance frameworks. For example, Gibbon and Reinhard have proposed inheritance-based solutions to problems of phonology and prosody (Gibbon 1990; Reinhard and Gibbon 1991). Most work to date has centered on morphology (e.g. De Smedt 1984; Flickinger, Pollard, and Wasow 1985; Daelemans 1987; Calder 1989). A certain amount has also been achieved in syntax (e.g. De Smedt 1984; Flickinger 1987), where inheritance is used to construct subcategorization frames for words. As for semantics, there has been a great deal of work on inheritance in so-called &apos;semantic networks,&apos; but much of this work relates only loosely to the semantics of natural language. The work we present in this paper differs from all previous work in natural language processing (NLP) in at least two respects. Firstly, it is distinctive in the extent to which inheritance is used. Within our framework knowledge at all levels (morphology, syntax, semantics, world knowledge) is integrated in a single inheritance hierarch</context>
<context position="23605" citStr="Flickinger 1987" startWordPosition="3900" endWordPosition="3901">absence of some exceptional information. One key question is how exceptions should be handled, and our answer is perhaps the most controversial part of this paper. The standard answer is, of course, that any more general proposition is overridden by a more specific one that contradicts it. For example, the past-tense form did takes precedence over the expected *doed because the former is specified in relation to DO, whereas the latter is inherited from the general rules for verbs. This principle, which we call automatic overriding, underlies most discussions of inheritance (e.g. Shieber 1986; Flickinger 1987), but it is also assumed in a lot of linguistic theory where the notion of &apos;inheritance&apos; is not recognized as such—e.g. in the &apos;Proper Inclusion Precedence Principle&apos; governing the ordering of rules in phonology (see, for example, Pullum 1979 for a survey of this literature). Our answer is quite different, and involves the negative propositions, introduced by &apos;NOT:&apos;, which we described earlier. In WG, inheritance is not blocked by a more specific proposition, but by a negative proposition. We know that *doed is not possible because there is a proposition that tells us so (24a), and not just be</context>
<context position="44393" citStr="Flickinger (1987)" startWordPosition="7382" endWordPosition="7383">d), must precede it. The visitor is also some kind of postdependent, according to (43e), so it may be the verb&apos;s object as in our example. But equally it may &apos;hop&apos; down the dependency chain thanks to (430, thereby providing for the analysis of sentences such as (44). (44) Salesmen I don&apos;t think many people say they trust. Further details of the WG analysis of passives and extraction can be found in Hudson (1990). Both passivization and extraction are standard examples of syntactic problems that need special machinery. According to WG—and more recently Flickinger, Pollard, and Wasow (1985) and Flickinger (1987)—all that is needed is default inheritance, which is available (though generally not explicitly recognized) in every linguistic theory; so any theory capable of accommodating exceptional morphology already has the power to deal with subject-inversion, passives and extraction. 4. Semantics Default inheritance also plays a crucial part in the WG treatment of semantics. It is probably obvious how it applies in the familiar examples of inheritance in semantic networks--e.g., how one can infer that Clyde has a head from more general propositions about elephants or animals. Rather than discussing th</context>
</contexts>
<marker>Flickinger, 1987</marker>
<rawString>Flickinger, Daniel P. (1987). Lexical rules in the hierarchical lexicon. Doctoral dissertation, Stanford University, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel P Flickinger</author>
<author>Carl J Pollard</author>
<author>Thomas Wasow</author>
</authors>
<title>Structure sharing in lexical representation.&amp;quot;</title>
<date>1985</date>
<booktitle>In Proceedings, 23rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>262--267</pages>
<location>Chicago,</location>
<marker>Flickinger, Pollard, Wasow, 1985</marker>
<rawString>Flickinger, Daniel P.; Pollard, Carl J.; and Wasow, Thomas (1985). &amp;quot;Structure sharing in lexical representation.&amp;quot; In Proceedings, 23rd Annual Meeting of the Association for Computational Linguistics, Chicago, 262-267.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Norman M Fraser</author>
</authors>
<title>Parsing and dependency grammar.&amp;quot;</title>
<date>1989</date>
<booktitle>In UCL Working Papers in Linguistics 1, edited by Robyn Carston,</booktitle>
<pages>296--319</pages>
<institution>London: University College London.</institution>
<contexts>
<context position="32499" citStr="Fraser 1989" startWordPosition="5366" endWordPosition="5367">pendency grammar among computational linguists. Dependency grammar has been applied in the experimental parsing systems of Hellwig (1986), Sigurd (1989), and Covington (1991); in the &apos;Kielikone&apos; natural language interface of Jappinen, Lassila, and Lahtola (1988); in the machine translation systems of EUROTRA (Johnson, King, and des Tombe 1985), DLT (Schubert 1987), Charles University (Sgall and Panevova 1987), and IBM Tokyo (Maruyama 1990); and in the speech recognition system of Giachin and Rullent (1989). Parsers based on the theories of Lexicase (Starosta and Nomura 1986) and Word Grammar (Fraser 1989; Hudson 1989) have also been developed. 144 Norman M. Fraser and Richard A. Hudson Inheritance in Word Grammar depend. Here, then, we already have a simple example of default inheritance: (30a) word has 1 head. (30b) w1 isa word. (30c) so: wl has 1 head. On the other hand, for w2, the finite verb didn&apos;t, this general rule is blocked to allow it to occur without a head (i.e. to make the head optional, 10-11 head&apos;). This analysis assumes that obligatory (&apos;1&apos;) and optional C[0-11&apos;) conflict, so the former must be suppressed by (31d).4 (31a) finite verb has [0-11 head. (31b) w2 isa finite verb. (</context>
</contexts>
<marker>Fraser, 1989</marker>
<rawString>Fraser, Norman M. (1989). &amp;quot;Parsing and dependency grammar.&amp;quot; In UCL Working Papers in Linguistics 1, edited by Robyn Carston, 296-319. London: University College London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haim Gaifman</author>
</authors>
<title>Dependency systems and phrase-structure systems.&amp;quot;</title>
<date>1965</date>
<journal>Information and Control</journal>
<pages>8--304</pages>
<contexts>
<context position="30173" citStr="Gaifman (1965)" startWordPosition="5018" endWordPosition="5019">second hierarchy of syntax, the hierarchy of grammatical relations. Unlike most other syntactic theories, WG uses constituent structure only for the purpose of describing coordinate constructions (cf. Hudson 1990: 404ff for details). All other syntactic structure is expressed in terms of dependencies between pairs of words, one of which is the head of the other, its dependent. Higher nodes such as phrases or sentences are not represented explicitly in the grammar. WG is thus a variety of dependency grammar. Dependency grammar was first formalized by Tesniere (1959) and refined by Hays (1964), Gaifman (1965), Robinson (1970) and others. A number of dependencybased theories have emerged from the linguistic underground during the last thirty years. These include the Meaning-Text model (Menuk and Zolkovskij 1970; Menuk 1988), Case Grammar (Anderson 1971; 1977), Daughter Dependency Grammar (Hudson 1976), WG (Hudson 1984; 1990), Functional Generative Description (Sgall, Hajieova, and Panevova 1986), and Lexicase (Starosta 1988). While none of these theories has attained widespread popularity, some of their central insights have become increasingly influential in the phrase structure grammar mainstream</context>
</contexts>
<marker>Gaifman, 1965</marker>
<rawString>Gaifman, Haim (1965). &amp;quot;Dependency systems and phrase-structure systems.&amp;quot; Information and Control 8,304-337.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Egidio R Giachin</author>
<author>Claudio Rullent</author>
</authors>
<title>A parallel parser for spoken natural language.&amp;quot;</title>
<date>1989</date>
<booktitle>IICAI-89,</booktitle>
<pages>1537--1542</pages>
<contexts>
<context position="32399" citStr="Giachin and Rullent (1989)" startWordPosition="5348" endWordPosition="5351">uxiliary verb&apos;) didn&apos;t is the root, on which both Mary and jump 3 The last decade has seen increased interest in dependency grammar among computational linguists. Dependency grammar has been applied in the experimental parsing systems of Hellwig (1986), Sigurd (1989), and Covington (1991); in the &apos;Kielikone&apos; natural language interface of Jappinen, Lassila, and Lahtola (1988); in the machine translation systems of EUROTRA (Johnson, King, and des Tombe 1985), DLT (Schubert 1987), Charles University (Sgall and Panevova 1987), and IBM Tokyo (Maruyama 1990); and in the speech recognition system of Giachin and Rullent (1989). Parsers based on the theories of Lexicase (Starosta and Nomura 1986) and Word Grammar (Fraser 1989; Hudson 1989) have also been developed. 144 Norman M. Fraser and Richard A. Hudson Inheritance in Word Grammar depend. Here, then, we already have a simple example of default inheritance: (30a) word has 1 head. (30b) w1 isa word. (30c) so: wl has 1 head. On the other hand, for w2, the finite verb didn&apos;t, this general rule is blocked to allow it to occur without a head (i.e. to make the head optional, 10-11 head&apos;). This analysis assumes that obligatory (&apos;1&apos;) and optional C[0-11&apos;) conflict, so th</context>
</contexts>
<marker>Giachin, Rullent, 1989</marker>
<rawString>Giachin, Egidio R, and Rullent, Claudio (1989). &amp;quot;A parallel parser for spoken natural language.&amp;quot; IICAI-89, 1537-1542.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dafydd Gibbon</author>
</authors>
<title>Prosodic association by template inheritance.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings, Workshop on Inheritance in Natural Language Processing, edited by Walter Daelemans and Gerald Gazdar,</booktitle>
<pages>65--81</pages>
<publisher>ITK.</publisher>
<location>Tilburg:</location>
<contexts>
<context position="3655" citStr="Gibbon 1990" startWordPosition="538" endWordPosition="539">is paper from three anonymous Computational Linguistics readers, to whom we are most grateful. We also benefitted from discussions with participants at the Workshop on Inheritance in Natural Language Processing, Tilburg, August 1990. C) 1992 Association for Computational Linguistics Computational Linguistics Volume 18, Number 2 In recent years, linguists and computational linguists in particular have begun to explore problems at most linguistic levels within inheritance frameworks. For example, Gibbon and Reinhard have proposed inheritance-based solutions to problems of phonology and prosody (Gibbon 1990; Reinhard and Gibbon 1991). Most work to date has centered on morphology (e.g. De Smedt 1984; Flickinger, Pollard, and Wasow 1985; Daelemans 1987; Calder 1989). A certain amount has also been achieved in syntax (e.g. De Smedt 1984; Flickinger 1987), where inheritance is used to construct subcategorization frames for words. As for semantics, there has been a great deal of work on inheritance in so-called &apos;semantic networks,&apos; but much of this work relates only loosely to the semantics of natural language. The work we present in this paper differs from all previous work in natural language proce</context>
</contexts>
<marker>Gibbon, 1990</marker>
<rawString>Gibbon, Dafydd (1990). &amp;quot;Prosodic association by template inheritance.&amp;quot; In Proceedings, Workshop on Inheritance in Natural Language Processing, edited by Walter Daelemans and Gerald Gazdar, 65-81. Tilburg: ITK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Hays</author>
</authors>
<title>Dependency theory: a formalism and some observations.&amp;quot;</title>
<date>1964</date>
<journal>Language</journal>
<pages>40--511</pages>
<contexts>
<context position="30157" citStr="Hays (1964)" startWordPosition="5016" endWordPosition="5017"> come to the second hierarchy of syntax, the hierarchy of grammatical relations. Unlike most other syntactic theories, WG uses constituent structure only for the purpose of describing coordinate constructions (cf. Hudson 1990: 404ff for details). All other syntactic structure is expressed in terms of dependencies between pairs of words, one of which is the head of the other, its dependent. Higher nodes such as phrases or sentences are not represented explicitly in the grammar. WG is thus a variety of dependency grammar. Dependency grammar was first formalized by Tesniere (1959) and refined by Hays (1964), Gaifman (1965), Robinson (1970) and others. A number of dependencybased theories have emerged from the linguistic underground during the last thirty years. These include the Meaning-Text model (Menuk and Zolkovskij 1970; Menuk 1988), Case Grammar (Anderson 1971; 1977), Daughter Dependency Grammar (Hudson 1976), WG (Hudson 1984; 1990), Functional Generative Description (Sgall, Hajieova, and Panevova 1986), and Lexicase (Starosta 1988). While none of these theories has attained widespread popularity, some of their central insights have become increasingly influential in the phrase structure gr</context>
</contexts>
<marker>Hays, 1964</marker>
<rawString>Hays, David (1964). &amp;quot;Dependency theory: a formalism and some observations.&amp;quot; Language 40,511-525.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Hellwig</author>
</authors>
<title>Dependency Unification Grammar (DUG).&amp;quot;</title>
<date>1986</date>
<booktitle>COLING-86,</booktitle>
<pages>195--198</pages>
<contexts>
<context position="32025" citStr="Hellwig (1986)" startWordPosition="5295" endWordPosition="5296">equires a typical word to have one head, though the same word may act as head to more than one other word, its dependents. As in other theories, just one word is allowed to be an exception to this rule; we call this word the &apos;root&apos; of the sentence. This has (by definition) no head, and is generally a finite verb; e.g. in Mary didn&apos;t jump, the polarity verb (&apos;auxiliary verb&apos;) didn&apos;t is the root, on which both Mary and jump 3 The last decade has seen increased interest in dependency grammar among computational linguists. Dependency grammar has been applied in the experimental parsing systems of Hellwig (1986), Sigurd (1989), and Covington (1991); in the &apos;Kielikone&apos; natural language interface of Jappinen, Lassila, and Lahtola (1988); in the machine translation systems of EUROTRA (Johnson, King, and des Tombe 1985), DLT (Schubert 1987), Charles University (Sgall and Panevova 1987), and IBM Tokyo (Maruyama 1990); and in the speech recognition system of Giachin and Rullent (1989). Parsers based on the theories of Lexicase (Starosta and Nomura 1986) and Word Grammar (Fraser 1989; Hudson 1989) have also been developed. 144 Norman M. Fraser and Richard A. Hudson Inheritance in Word Grammar depend. Here, </context>
</contexts>
<marker>Hellwig, 1986</marker>
<rawString>Hellwig, Peter (1986). &amp;quot;Dependency Unification Grammar (DUG).&amp;quot; COLING-86, 195-198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard A Hudson</author>
</authors>
<title>Arguments for a Non-Transformational Grammar. Chicago:</title>
<date>1976</date>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="30470" citStr="Hudson 1976" startWordPosition="5060" endWordPosition="5062">es between pairs of words, one of which is the head of the other, its dependent. Higher nodes such as phrases or sentences are not represented explicitly in the grammar. WG is thus a variety of dependency grammar. Dependency grammar was first formalized by Tesniere (1959) and refined by Hays (1964), Gaifman (1965), Robinson (1970) and others. A number of dependencybased theories have emerged from the linguistic underground during the last thirty years. These include the Meaning-Text model (Menuk and Zolkovskij 1970; Menuk 1988), Case Grammar (Anderson 1971; 1977), Daughter Dependency Grammar (Hudson 1976), WG (Hudson 1984; 1990), Functional Generative Description (Sgall, Hajieova, and Panevova 1986), and Lexicase (Starosta 1988). While none of these theories has attained widespread popularity, some of their central insights have become increasingly influential in the phrase structure grammar mainstream. For example, the trend toward head-driven approaches, the prominence of notions such as &apos;government,&apos; the explicit use of grammatical relations and case, and the reduced amount of information carried in phrasal categories all reflect the general migration toward dependency. Increased interest i</context>
</contexts>
<marker>Hudson, 1976</marker>
<rawString>Hudson, Richard A. (1976). Arguments for a Non-Transformational Grammar. Chicago: University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard A Hudson</author>
</authors>
<title>Word Grammar.</title>
<date>1984</date>
<publisher>Blackwell.</publisher>
<location>Oxford:</location>
<contexts>
<context position="5011" citStr="Hudson 1984" startWordPosition="754" endWordPosition="755">l levels (morphology, syntax, semantics, world knowledge) is integrated in a single inheritance hierarchy. Indeed, given the extent of integration, some of these level distinctions must be regarded as arbitrary. Secondly, it is distinctive in the purposes for which inheritance is used. The canonical application of inheritance in NLP is lexicon construction. Our system uses inheritance for this purpose but it also makes inheritance play a vital role in the building of structure during parsing. What we describe is part of a theory of language (knowledge and processing) called Word Grammar (WG) (Hudson 1984; 1990). Section 2 introduces the knowledge representation language used in WG. Section 3 outlines the use of inheritance in WG to describe the facts of syntax and semantics. Concluding observations are drawn in Section 4. An Appendix sets out a fragment of English grammar and a simple sentence analysis. 2. Word Grammar In this section we define the syntax of WG propositions and explain how they can be interpreted. The Appendix contains a fragment of English grammar from which all examples are drawn. 2.1 Propositions One of the central claims of WG is that knowledge of language is a sub-compon</context>
<context position="8733" citStr="Hudson 1984" startWordPosition="1363" endWordPosition="1364">es identity of arguments. The reason it is necessary to include this predicate is that it is possible to identify the same concept by means of different kinds of names. For example, assuming that Clyde is grey, we can identify the concept &apos;grey&apos; either by its atomic name or as a function of Clyde. The &apos;=&apos; predicate shows this identity. More complex identifications are of course also possible. (5a) color of Clyde = grey. (5b) mother of Clyde = sister of father of Babar. (5c) mother of mother of Clyde = mother of father of Babar. 2 During the evolution of WG this figure has varied between five (Hudson 1984) and one (Hudson 1989), although the expressiveness of the formalism has not changed significantly. There is a balance to be struck between having a minimal (i.e. one-member) predicate set with necessary distinctions marked in the arguments, and having a more readable notation that includes extra predicates. The three-member set has been used in recent WG publications. In at least one computer implementation (described in Hudson 1989) the &apos;grammarian&apos;s grammar&apos; is written with three predicates and compiled into a &apos;machine grammar&apos; that uses only one predicate. 135 Computational Linguistics Vol</context>
<context position="30487" citStr="Hudson 1984" startWordPosition="5064" endWordPosition="5065">f words, one of which is the head of the other, its dependent. Higher nodes such as phrases or sentences are not represented explicitly in the grammar. WG is thus a variety of dependency grammar. Dependency grammar was first formalized by Tesniere (1959) and refined by Hays (1964), Gaifman (1965), Robinson (1970) and others. A number of dependencybased theories have emerged from the linguistic underground during the last thirty years. These include the Meaning-Text model (Menuk and Zolkovskij 1970; Menuk 1988), Case Grammar (Anderson 1971; 1977), Daughter Dependency Grammar (Hudson 1976), WG (Hudson 1984; 1990), Functional Generative Description (Sgall, Hajieova, and Panevova 1986), and Lexicase (Starosta 1988). While none of these theories has attained widespread popularity, some of their central insights have become increasingly influential in the phrase structure grammar mainstream. For example, the trend toward head-driven approaches, the prominence of notions such as &apos;government,&apos; the explicit use of grammatical relations and case, and the reduced amount of information carried in phrasal categories all reflect the general migration toward dependency. Increased interest in categorial gram</context>
</contexts>
<marker>Hudson, 1984</marker>
<rawString>Hudson, Richard A. (1984). Word Grammar. Oxford: Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard A Hudson</author>
</authors>
<title>Towards a computer testable word grammar of English.&amp;quot;</title>
<date>1989</date>
<booktitle>In UCL Working Papers in Linguistics 1, edited by Robyn Carston,</booktitle>
<pages>321--339</pages>
<institution>London: University College London.</institution>
<contexts>
<context position="8755" citStr="Hudson 1989" startWordPosition="1367" endWordPosition="1368">ts. The reason it is necessary to include this predicate is that it is possible to identify the same concept by means of different kinds of names. For example, assuming that Clyde is grey, we can identify the concept &apos;grey&apos; either by its atomic name or as a function of Clyde. The &apos;=&apos; predicate shows this identity. More complex identifications are of course also possible. (5a) color of Clyde = grey. (5b) mother of Clyde = sister of father of Babar. (5c) mother of mother of Clyde = mother of father of Babar. 2 During the evolution of WG this figure has varied between five (Hudson 1984) and one (Hudson 1989), although the expressiveness of the formalism has not changed significantly. There is a balance to be struck between having a minimal (i.e. one-member) predicate set with necessary distinctions marked in the arguments, and having a more readable notation that includes extra predicates. The three-member set has been used in recent WG publications. In at least one computer implementation (described in Hudson 1989) the &apos;grammarian&apos;s grammar&apos; is written with three predicates and compiled into a &apos;machine grammar&apos; that uses only one predicate. 135 Computational Linguistics Volume 18, Number 2 Propo</context>
<context position="32513" citStr="Hudson 1989" startWordPosition="5368" endWordPosition="5369">mar among computational linguists. Dependency grammar has been applied in the experimental parsing systems of Hellwig (1986), Sigurd (1989), and Covington (1991); in the &apos;Kielikone&apos; natural language interface of Jappinen, Lassila, and Lahtola (1988); in the machine translation systems of EUROTRA (Johnson, King, and des Tombe 1985), DLT (Schubert 1987), Charles University (Sgall and Panevova 1987), and IBM Tokyo (Maruyama 1990); and in the speech recognition system of Giachin and Rullent (1989). Parsers based on the theories of Lexicase (Starosta and Nomura 1986) and Word Grammar (Fraser 1989; Hudson 1989) have also been developed. 144 Norman M. Fraser and Richard A. Hudson Inheritance in Word Grammar depend. Here, then, we already have a simple example of default inheritance: (30a) word has 1 head. (30b) w1 isa word. (30c) so: wl has 1 head. On the other hand, for w2, the finite verb didn&apos;t, this general rule is blocked to allow it to occur without a head (i.e. to make the head optional, 10-11 head&apos;). This analysis assumes that obligatory (&apos;1&apos;) and optional C[0-11&apos;) conflict, so the former must be suppressed by (31d).4 (31a) finite verb has [0-11 head. (31b) w2 isa finite verb. (31c) so: w2 ha</context>
</contexts>
<marker>Hudson, 1989</marker>
<rawString>Hudson, Richard A. (1989). &amp;quot;Towards a computer testable word grammar of English.&amp;quot; In UCL Working Papers in Linguistics 1, edited by Robyn Carston, 321-339. London: University College London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard A Hudson</author>
</authors>
<title>English Word Grammar.</title>
<date>1990</date>
<publisher>Blackwell.</publisher>
<location>Oxford:</location>
<contexts>
<context position="24900" citStr="Hudson 1990" startWordPosition="4115" endWordPosition="4116">24b) whole of past DO = &lt;did&gt;. Every exceptional fact is paired with a negative fact that blocks inheritance. We call this stipulated overriding. It remains to be seen which of these approaches—automatic overriding or stipulated overriding—will be favored by future research in NLP. The extra cost of exceptional facts in the first system lies in the need to ensure that more specific facts are accessed before more general ones. In the second system, the cost lies in the need for a larger database. Our reasons for preferring stipulated overriding are partly concerned with cognitive modeling (see Hudson 1990: 40ff), but we also 141 Computational Linguistics Volume 18, Number 2 believe that the syntactic and semantic arguments that we present in the next sections support this approach. Here, then, is the rule of inference for default inheritance: Default inheritance If A isa B, then for any true proposition P that refers to B, it is possible to infer another true proposition Q that is the same as P except that A is substituted in Q for B in P. unless NOT: Q. That is, we can apply the inheritance rule defined in the last section, unless there is a negative proposition that conflicts with the inheri</context>
<context position="27171" citStr="Hudson 1990" startWordPosition="4522" endWordPosition="4523">rovides the suffix, the &apos;set&apos; meaning, and the ability to occur, for example, after these). Having introduced our theory of default inheritance, we can now discuss some linguistic applications in more depth. One of the most distinctive features of our theory is our claim that default inheritance applies to syntax and compositional semantics, so we shall concentrate on these areas. The preceding discussion should have made it clear that we also use default inheritance in morphology, but we will not pursue that further here. (A brief WG account of English inflectional morphology can be found in Hudson 1990: 181-90.) 3. Syntax 3.1 Word Types WG syntax is centred on two inheritance hierarchies, one for word types (i.e. word classes and lexical items) and the other for grammatical relations. In Word Grammar 142 Inheritance in Word Grammar Norman M. Fraser and Richard A. Hudson word adword noun conjunction verb adjective adverb proper common pronoun polarity STAND etc preposition count determiner modal Figure 1 The word type hierarchy. (as suggested by the name) the category &apos;word&apos; is basic in every sense. Figure 1 shows the top of the hierarchy of word types assumed in WG for English, and some of </context>
<context position="29771" citStr="Hudson 1990" startWordPosition="4953" endWordPosition="4954">on and the rest of the grammar. Furthermore, we use the same isa relation to link word tokens to word types; so if w3 is the name of the word stand in I can&apos;t stand cats, it too will fit into the same hierarchy: (29) w3 isa STAND/trans. Word tokens can be thought of as a constantly changing fringe on the bottom of the (relatively) permanent hierarchy. 3.2 Grammatical Functions We now come to the second hierarchy of syntax, the hierarchy of grammatical relations. Unlike most other syntactic theories, WG uses constituent structure only for the purpose of describing coordinate constructions (cf. Hudson 1990: 404ff for details). All other syntactic structure is expressed in terms of dependencies between pairs of words, one of which is the head of the other, its dependent. Higher nodes such as phrases or sentences are not represented explicitly in the grammar. WG is thus a variety of dependency grammar. Dependency grammar was first formalized by Tesniere (1959) and refined by Hays (1964), Gaifman (1965), Robinson (1970) and others. A number of dependencybased theories have emerged from the linguistic underground during the last thirty years. These include the Meaning-Text model (Menuk and Zolkovsk</context>
<context position="42055" citStr="Hudson (1990" startWordPosition="6992" endWordPosition="6993">o other major problems, namely passives and extraction. In a passive sentence like (40), WG handles object-promotion by analyzing the subject as also being the object. This is achieved by means of proposition (41a) (which is slightly simplified). (40) Mary was kissed by John. (41a) subject of passive verb = object of it. (41b) NOT: position of predependent of word = after it. The problem is that Mary, as the object of kissed, ought to follow it, but since Mary is also the subject this requirement is overridden by proposition (39e=41b), so Mary never inherits the need to follow kissed.11 9 See Hudson (1990: 215-6) for a discussion of the apparent lack of morphological consequences of the subject-position feature. 10 The exceptionality of an inverted subject is shown by the negative proposition &apos;NOT: NOT: position of subject of v+s verb = after it.&apos; This proposition is inherited by the word tokens concerned—i.e. it is part of the analysis of the sentence itself, and not just available in the grammar. lilt may not be obvious exactly how this works. How does (41b) stop the object of a passive from following the verb, given that it refers to &apos;predependent,&apos; which does not subsume &apos;object&apos;? The answ</context>
<context position="44191" citStr="Hudson (1990)" startWordPosition="7355" endWordPosition="7356">43f) visitor of word = a visitor of complement of it. Proposition (43a) allows distrust to have a visitor, which according to (43b) is a kind of predependent and therefore, by inheritance from (43c,d), must precede it. The visitor is also some kind of postdependent, according to (43e), so it may be the verb&apos;s object as in our example. But equally it may &apos;hop&apos; down the dependency chain thanks to (430, thereby providing for the analysis of sentences such as (44). (44) Salesmen I don&apos;t think many people say they trust. Further details of the WG analysis of passives and extraction can be found in Hudson (1990). Both passivization and extraction are standard examples of syntactic problems that need special machinery. According to WG—and more recently Flickinger, Pollard, and Wasow (1985) and Flickinger (1987)—all that is needed is default inheritance, which is available (though generally not explicitly recognized) in every linguistic theory; so any theory capable of accommodating exceptional morphology already has the power to deal with subject-inversion, passives and extraction. 4. Semantics Default inheritance also plays a crucial part in the WG treatment of semantics. It is probably obvious how i</context>
</contexts>
<marker>Hudson, 1990</marker>
<rawString>Hudson, Richard A. (1990). English Word Grammar. Oxford: Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ray Jackendoff</author>
</authors>
<title>Semantics and Cognition.</title>
<date>1983</date>
<publisher>The MIT Press.</publisher>
<location>Cambridge, MA:</location>
<contexts>
<context position="18763" citStr="Jackendoff 1983" startWordPosition="3052" endWordPosition="3053">cate, which is in effect a reciprocal &apos;isa&apos;. If A = B, then any true proposition that contains A can be matched by another in which B replaces A, and vice versa. Inferentially, then, both &apos;isa&apos; and &apos;=&apos; are extremely simple, and extremely powerful, allowing the creation of new propositions by simple substitution operations. The most noteworthy feature of the WG inheritance system is, once again, that it applies to all types of knowledge, allowing a single integrated knowledge base and a single set of inference rules for both linguistic and nonlinguistic knowledge (cf. the &apos;preference rules&apos; of Jackendoff 1983). The same rule that allows us to inherit information about Clyde also allows us to inherit information about words within the grammar and about words in sentences. These similarities can be seen from the following examples. (14a) noun isa word. (14b) word has 1 head. (14c) so: noun has 1 head. (15a) MARY isa noun. (15b) noun has 1 head. [--= 14c1 (15c) so: MARY has 1 head. (16a) wl isa MARY. (16b) MARY has 1 head. [= 15c] (16c) so: w1 has 1 head. These examples show how inheritance allows information to be inherited both within the grammar (14, 15) and from the grammar to a particular sentenc</context>
</contexts>
<marker>Jackendoff, 1983</marker>
<rawString>Jackendoff, Ray (1983). Semantics and Cognition. Cambridge, MA: The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harri Jappinen</author>
<author>Eero Lassila</author>
<author>Lehtola</author>
</authors>
<title>Locally governed trees and dependency parsing.&amp;quot;</title>
<date>1988</date>
<booktitle>COLING-88,</booktitle>
<pages>275--277</pages>
<location>Aarno</location>
<marker>Jappinen, Lassila, Lehtola, 1988</marker>
<rawString>Jappinen, Harri; Lassila, Eero; and Lehtola, Aarno (1988). &amp;quot;Locally governed trees and dependency parsing.&amp;quot; COLING-88, 275-277.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rod Johnson</author>
<author>Maghi King</author>
<author>des Tombe</author>
</authors>
<title>EUROTRA: A multilingual system under development.&amp;quot;</title>
<date>1985</date>
<journal>Computational Linguistics</journal>
<pages>11--155</pages>
<location>Louis</location>
<marker>Johnson, King, Tombe, 1985</marker>
<rawString>Johnson, Rod; King, Maghi; and des Tombe, Louis (1985). &amp;quot;EUROTRA: A multilingual system under development.&amp;quot; Computational Linguistics 11,155-169.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroshi Maruyama</author>
</authors>
<title>Structural disambiguation with constraint propagation.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings, 28th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>31--38</pages>
<contexts>
<context position="32331" citStr="Maruyama 1990" startWordPosition="5339" endWordPosition="5340">te verb; e.g. in Mary didn&apos;t jump, the polarity verb (&apos;auxiliary verb&apos;) didn&apos;t is the root, on which both Mary and jump 3 The last decade has seen increased interest in dependency grammar among computational linguists. Dependency grammar has been applied in the experimental parsing systems of Hellwig (1986), Sigurd (1989), and Covington (1991); in the &apos;Kielikone&apos; natural language interface of Jappinen, Lassila, and Lahtola (1988); in the machine translation systems of EUROTRA (Johnson, King, and des Tombe 1985), DLT (Schubert 1987), Charles University (Sgall and Panevova 1987), and IBM Tokyo (Maruyama 1990); and in the speech recognition system of Giachin and Rullent (1989). Parsers based on the theories of Lexicase (Starosta and Nomura 1986) and Word Grammar (Fraser 1989; Hudson 1989) have also been developed. 144 Norman M. Fraser and Richard A. Hudson Inheritance in Word Grammar depend. Here, then, we already have a simple example of default inheritance: (30a) word has 1 head. (30b) w1 isa word. (30c) so: wl has 1 head. On the other hand, for w2, the finite verb didn&apos;t, this general rule is blocked to allow it to occur without a head (i.e. to make the head optional, 10-11 head&apos;). This analysis</context>
</contexts>
<marker>Maruyama, 1990</marker>
<rawString>Maruyama, Hiroshi (1990). &amp;quot;Structural disambiguation with constraint propagation.&amp;quot; In Proceedings, 28th Annual Meeting of the Association for Computational Linguistics, 31-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor A Menuk</author>
</authors>
<title>Dependency Syntax: Theory and Practice.</title>
<date>1988</date>
<publisher>State University of New York Press.</publisher>
<location>Albany, NY:</location>
<contexts>
<context position="30391" citStr="Menuk 1988" startWordPosition="5050" endWordPosition="5051">or details). All other syntactic structure is expressed in terms of dependencies between pairs of words, one of which is the head of the other, its dependent. Higher nodes such as phrases or sentences are not represented explicitly in the grammar. WG is thus a variety of dependency grammar. Dependency grammar was first formalized by Tesniere (1959) and refined by Hays (1964), Gaifman (1965), Robinson (1970) and others. A number of dependencybased theories have emerged from the linguistic underground during the last thirty years. These include the Meaning-Text model (Menuk and Zolkovskij 1970; Menuk 1988), Case Grammar (Anderson 1971; 1977), Daughter Dependency Grammar (Hudson 1976), WG (Hudson 1984; 1990), Functional Generative Description (Sgall, Hajieova, and Panevova 1986), and Lexicase (Starosta 1988). While none of these theories has attained widespread popularity, some of their central insights have become increasingly influential in the phrase structure grammar mainstream. For example, the trend toward head-driven approaches, the prominence of notions such as &apos;government,&apos; the explicit use of grammatical relations and case, and the reduced amount of information carried in phrasal categ</context>
</contexts>
<marker>Menuk, 1988</marker>
<rawString>Menuk, Igor A. (1988). Dependency Syntax: Theory and Practice. Albany, NY: State University of New York Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor A Mel&apos;Cuk</author>
<author>Alexander K Zolkovslcij</author>
</authors>
<title>Towards a functioning &apos;Meaning-Text&apos; model of language.&amp;quot;</title>
<date>1970</date>
<journal>Linguistics</journal>
<pages>57--10</pages>
<marker>Mel&apos;Cuk, Zolkovslcij, 1970</marker>
<rawString>Mel&apos;Cuk, Igor A., and Zolkovslcij, Alexander K. (1970). &amp;quot;Towards a functioning &apos;Meaning-Text&apos; model of language.&amp;quot; Linguistics 57,10-47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey K Pullum</author>
</authors>
<title>Rule Interaction and the Organization of a Grammar.</title>
<date>1979</date>
<location>London: Garland.</location>
<contexts>
<context position="23847" citStr="Pullum 1979" startWordPosition="3939" endWordPosition="3940">by a more specific one that contradicts it. For example, the past-tense form did takes precedence over the expected *doed because the former is specified in relation to DO, whereas the latter is inherited from the general rules for verbs. This principle, which we call automatic overriding, underlies most discussions of inheritance (e.g. Shieber 1986; Flickinger 1987), but it is also assumed in a lot of linguistic theory where the notion of &apos;inheritance&apos; is not recognized as such—e.g. in the &apos;Proper Inclusion Precedence Principle&apos; governing the ordering of rules in phonology (see, for example, Pullum 1979 for a survey of this literature). Our answer is quite different, and involves the negative propositions, introduced by &apos;NOT:&apos;, which we described earlier. In WG, inheritance is not blocked by a more specific proposition, but by a negative proposition. We know that *doed is not possible because there is a proposition that tells us so (24a), and not just because (24b) requires did: (24a) NOT: whole of past DO = stem of it + whole of mEd. (24b) whole of past DO = &lt;did&gt;. Every exceptional fact is paired with a negative fact that blocks inheritance. We call this stipulated overriding. It remains t</context>
</contexts>
<marker>Pullum, 1979</marker>
<rawString>Pullum, Geoffrey K. (1979). Rule Interaction and the Organization of a Grammar. London: Garland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Reinhard</author>
<author>Dafydd Gibbon</author>
</authors>
<title>Prosodic inheritance and morphological generalisations.&amp;quot;</title>
<date>1991</date>
<booktitle>In Proceedings, Fifth Conference of the European Chapter of the Association for Computational Linguistics.</booktitle>
<pages>131--136</pages>
<publisher>April,</publisher>
<location>Berlin,</location>
<contexts>
<context position="3682" citStr="Reinhard and Gibbon 1991" startWordPosition="540" endWordPosition="543"> three anonymous Computational Linguistics readers, to whom we are most grateful. We also benefitted from discussions with participants at the Workshop on Inheritance in Natural Language Processing, Tilburg, August 1990. C) 1992 Association for Computational Linguistics Computational Linguistics Volume 18, Number 2 In recent years, linguists and computational linguists in particular have begun to explore problems at most linguistic levels within inheritance frameworks. For example, Gibbon and Reinhard have proposed inheritance-based solutions to problems of phonology and prosody (Gibbon 1990; Reinhard and Gibbon 1991). Most work to date has centered on morphology (e.g. De Smedt 1984; Flickinger, Pollard, and Wasow 1985; Daelemans 1987; Calder 1989). A certain amount has also been achieved in syntax (e.g. De Smedt 1984; Flickinger 1987), where inheritance is used to construct subcategorization frames for words. As for semantics, there has been a great deal of work on inheritance in so-called &apos;semantic networks,&apos; but much of this work relates only loosely to the semantics of natural language. The work we present in this paper differs from all previous work in natural language processing (NLP) in at least two</context>
</contexts>
<marker>Reinhard, Gibbon, 1991</marker>
<rawString>Reinhard, Sabine, and Gibbon, Dafydd (1991). &amp;quot;Prosodic inheritance and morphological generalisations.&amp;quot; In Proceedings, Fifth Conference of the European Chapter of the Association for Computational Linguistics. April, Berlin, 131-136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane J Robinson</author>
</authors>
<title>Dependency structures and transformational rules.&amp;quot;</title>
<date>1970</date>
<journal>Language</journal>
<pages>46--259</pages>
<contexts>
<context position="30190" citStr="Robinson (1970)" startWordPosition="5020" endWordPosition="5021"> of syntax, the hierarchy of grammatical relations. Unlike most other syntactic theories, WG uses constituent structure only for the purpose of describing coordinate constructions (cf. Hudson 1990: 404ff for details). All other syntactic structure is expressed in terms of dependencies between pairs of words, one of which is the head of the other, its dependent. Higher nodes such as phrases or sentences are not represented explicitly in the grammar. WG is thus a variety of dependency grammar. Dependency grammar was first formalized by Tesniere (1959) and refined by Hays (1964), Gaifman (1965), Robinson (1970) and others. A number of dependencybased theories have emerged from the linguistic underground during the last thirty years. These include the Meaning-Text model (Menuk and Zolkovskij 1970; Menuk 1988), Case Grammar (Anderson 1971; 1977), Daughter Dependency Grammar (Hudson 1976), WG (Hudson 1984; 1990), Functional Generative Description (Sgall, Hajieova, and Panevova 1986), and Lexicase (Starosta 1988). While none of these theories has attained widespread popularity, some of their central insights have become increasingly influential in the phrase structure grammar mainstream. For example, th</context>
</contexts>
<marker>Robinson, 1970</marker>
<rawString>Robinson, Jane J. (1970). &amp;quot;Dependency structures and transformational rules.&amp;quot; Language 46,259-285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Schubert</author>
</authors>
<title>Metataxis: Contrastive Dependency Syntax for Machine Translation.</title>
<date>1987</date>
<location>Dordrecht: Foris.</location>
<contexts>
<context position="32254" citStr="Schubert 1987" startWordPosition="5328" endWordPosition="5329">t&apos; of the sentence. This has (by definition) no head, and is generally a finite verb; e.g. in Mary didn&apos;t jump, the polarity verb (&apos;auxiliary verb&apos;) didn&apos;t is the root, on which both Mary and jump 3 The last decade has seen increased interest in dependency grammar among computational linguists. Dependency grammar has been applied in the experimental parsing systems of Hellwig (1986), Sigurd (1989), and Covington (1991); in the &apos;Kielikone&apos; natural language interface of Jappinen, Lassila, and Lahtola (1988); in the machine translation systems of EUROTRA (Johnson, King, and des Tombe 1985), DLT (Schubert 1987), Charles University (Sgall and Panevova 1987), and IBM Tokyo (Maruyama 1990); and in the speech recognition system of Giachin and Rullent (1989). Parsers based on the theories of Lexicase (Starosta and Nomura 1986) and Word Grammar (Fraser 1989; Hudson 1989) have also been developed. 144 Norman M. Fraser and Richard A. Hudson Inheritance in Word Grammar depend. Here, then, we already have a simple example of default inheritance: (30a) word has 1 head. (30b) w1 isa word. (30c) so: wl has 1 head. On the other hand, for w2, the finite verb didn&apos;t, this general rule is blocked to allow it to occu</context>
</contexts>
<marker>Schubert, 1987</marker>
<rawString>Schubert, Klaus (1987). Metataxis: Contrastive Dependency Syntax for Machine Translation. Dordrecht: Foris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Petr Sgall</author>
<author>Eva Hajieova</author>
<author>Jarmila Panevova</author>
</authors>
<title>The Meaning of the Sentence in its Semantic and Pragmatic Aspects.</title>
<date>1986</date>
<publisher>Academia.</publisher>
<location>Prague:</location>
<marker>Sgall, Hajieova, Panevova, 1986</marker>
<rawString>Sgall, Petr; Hajieova, Eva; and Panevova, Jarmila (1986). The Meaning of the Sentence in its Semantic and Pragmatic Aspects. Prague: Academia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Petr Sgall</author>
<author>Jarmila Panevova</author>
</authors>
<title>Machine translation, linguistics, and interlingua.&amp;quot;</title>
<date>1987</date>
<booktitle>In Proceedings, Third Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>99--108</pages>
<contexts>
<context position="32300" citStr="Sgall and Panevova 1987" startWordPosition="5332" endWordPosition="5335">inition) no head, and is generally a finite verb; e.g. in Mary didn&apos;t jump, the polarity verb (&apos;auxiliary verb&apos;) didn&apos;t is the root, on which both Mary and jump 3 The last decade has seen increased interest in dependency grammar among computational linguists. Dependency grammar has been applied in the experimental parsing systems of Hellwig (1986), Sigurd (1989), and Covington (1991); in the &apos;Kielikone&apos; natural language interface of Jappinen, Lassila, and Lahtola (1988); in the machine translation systems of EUROTRA (Johnson, King, and des Tombe 1985), DLT (Schubert 1987), Charles University (Sgall and Panevova 1987), and IBM Tokyo (Maruyama 1990); and in the speech recognition system of Giachin and Rullent (1989). Parsers based on the theories of Lexicase (Starosta and Nomura 1986) and Word Grammar (Fraser 1989; Hudson 1989) have also been developed. 144 Norman M. Fraser and Richard A. Hudson Inheritance in Word Grammar depend. Here, then, we already have a simple example of default inheritance: (30a) word has 1 head. (30b) w1 isa word. (30c) so: wl has 1 head. On the other hand, for w2, the finite verb didn&apos;t, this general rule is blocked to allow it to occur without a head (i.e. to make the head option</context>
</contexts>
<marker>Sgall, Panevova, 1987</marker>
<rawString>Sgall, Petr, and Panevova, Jarmila (1987). &amp;quot;Machine translation, linguistics, and interlingua.&amp;quot; In Proceedings, Third Conference of the European Chapter of the Association for Computational Linguistics, 99-108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
</authors>
<title>An Introduction to Unification-Based Approaches to Grammar.</title>
<date>1986</date>
<publisher>CSLI.</publisher>
<location>Stanford, CA:</location>
<contexts>
<context position="23587" citStr="Shieber 1986" startWordPosition="3898" endWordPosition="3899">, i.e. in the absence of some exceptional information. One key question is how exceptions should be handled, and our answer is perhaps the most controversial part of this paper. The standard answer is, of course, that any more general proposition is overridden by a more specific one that contradicts it. For example, the past-tense form did takes precedence over the expected *doed because the former is specified in relation to DO, whereas the latter is inherited from the general rules for verbs. This principle, which we call automatic overriding, underlies most discussions of inheritance (e.g. Shieber 1986; Flickinger 1987), but it is also assumed in a lot of linguistic theory where the notion of &apos;inheritance&apos; is not recognized as such—e.g. in the &apos;Proper Inclusion Precedence Principle&apos; governing the ordering of rules in phonology (see, for example, Pullum 1979 for a survey of this literature). Our answer is quite different, and involves the negative propositions, introduced by &apos;NOT:&apos;, which we described earlier. In WG, inheritance is not blocked by a more specific proposition, but by a negative proposition. We know that *doed is not possible because there is a proposition that tells us so (24a</context>
</contexts>
<marker>Shieber, 1986</marker>
<rawString>Shieber, Stuart M. (1986). An Introduction to Unification-Based Approaches to Grammar. Stanford, CA: CSLI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bengt Sigurd</author>
</authors>
<title>DEPARSE-An experimental dependency parser.&amp;quot; In Praktisk Lingvistik 12, edited by</title>
<date>1989</date>
<pages>30--41</pages>
<institution>Lunds: Lunds Universitet.</institution>
<contexts>
<context position="32040" citStr="Sigurd (1989)" startWordPosition="5297" endWordPosition="5298">l word to have one head, though the same word may act as head to more than one other word, its dependents. As in other theories, just one word is allowed to be an exception to this rule; we call this word the &apos;root&apos; of the sentence. This has (by definition) no head, and is generally a finite verb; e.g. in Mary didn&apos;t jump, the polarity verb (&apos;auxiliary verb&apos;) didn&apos;t is the root, on which both Mary and jump 3 The last decade has seen increased interest in dependency grammar among computational linguists. Dependency grammar has been applied in the experimental parsing systems of Hellwig (1986), Sigurd (1989), and Covington (1991); in the &apos;Kielikone&apos; natural language interface of Jappinen, Lassila, and Lahtola (1988); in the machine translation systems of EUROTRA (Johnson, King, and des Tombe 1985), DLT (Schubert 1987), Charles University (Sgall and Panevova 1987), and IBM Tokyo (Maruyama 1990); and in the speech recognition system of Giachin and Rullent (1989). Parsers based on the theories of Lexicase (Starosta and Nomura 1986) and Word Grammar (Fraser 1989; Hudson 1989) have also been developed. 144 Norman M. Fraser and Richard A. Hudson Inheritance in Word Grammar depend. Here, then, we alread</context>
</contexts>
<marker>Sigurd, 1989</marker>
<rawString>Sigurd, Bengt (1989). &amp;quot;DEPARSE-An experimental dependency parser.&amp;quot; In Praktisk Lingvistik 12, edited by B. Sigurd; M. Eeg-Olofsson; L. Eriksson; B. Gawronska-Werngren; K. Holmqvist; and P. Touati, 30-41. Lunds: Lunds Universitet.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley Starosta</author>
</authors>
<title>The Case for Lexicase: An Outline of Lexicase Grammatical Theory.</title>
<date>1988</date>
<location>London: Pinter.</location>
<contexts>
<context position="30596" citStr="Starosta 1988" startWordPosition="5077" endWordPosition="5078">re not represented explicitly in the grammar. WG is thus a variety of dependency grammar. Dependency grammar was first formalized by Tesniere (1959) and refined by Hays (1964), Gaifman (1965), Robinson (1970) and others. A number of dependencybased theories have emerged from the linguistic underground during the last thirty years. These include the Meaning-Text model (Menuk and Zolkovskij 1970; Menuk 1988), Case Grammar (Anderson 1971; 1977), Daughter Dependency Grammar (Hudson 1976), WG (Hudson 1984; 1990), Functional Generative Description (Sgall, Hajieova, and Panevova 1986), and Lexicase (Starosta 1988). While none of these theories has attained widespread popularity, some of their central insights have become increasingly influential in the phrase structure grammar mainstream. For example, the trend toward head-driven approaches, the prominence of notions such as &apos;government,&apos; the explicit use of grammatical relations and case, and the reduced amount of information carried in phrasal categories all reflect the general migration toward dependency. Increased interest in categorial grammars, and especially unification categorial grammars (which are virtually indistinguishable from dependency g</context>
</contexts>
<marker>Starosta, 1988</marker>
<rawString>Starosta, Stanley (1988). The Case for Lexicase: An Outline of Lexicase Grammatical Theory. London: Pinter.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley Starosta</author>
<author>H Nomura</author>
</authors>
<title>Lexicase parsing: A lexicon-driven approach to syntactic analysis.&amp;quot;</title>
<date>1986</date>
<booktitle>COLING-86,</booktitle>
<pages>127--132</pages>
<contexts>
<context position="32469" citStr="Starosta and Nomura 1986" startWordPosition="5359" endWordPosition="5362">ast decade has seen increased interest in dependency grammar among computational linguists. Dependency grammar has been applied in the experimental parsing systems of Hellwig (1986), Sigurd (1989), and Covington (1991); in the &apos;Kielikone&apos; natural language interface of Jappinen, Lassila, and Lahtola (1988); in the machine translation systems of EUROTRA (Johnson, King, and des Tombe 1985), DLT (Schubert 1987), Charles University (Sgall and Panevova 1987), and IBM Tokyo (Maruyama 1990); and in the speech recognition system of Giachin and Rullent (1989). Parsers based on the theories of Lexicase (Starosta and Nomura 1986) and Word Grammar (Fraser 1989; Hudson 1989) have also been developed. 144 Norman M. Fraser and Richard A. Hudson Inheritance in Word Grammar depend. Here, then, we already have a simple example of default inheritance: (30a) word has 1 head. (30b) w1 isa word. (30c) so: wl has 1 head. On the other hand, for w2, the finite verb didn&apos;t, this general rule is blocked to allow it to occur without a head (i.e. to make the head optional, 10-11 head&apos;). This analysis assumes that obligatory (&apos;1&apos;) and optional C[0-11&apos;) conflict, so the former must be suppressed by (31d).4 (31a) finite verb has [0-11 hea</context>
</contexts>
<marker>Starosta, Nomura, 1986</marker>
<rawString>Starosta, Stanley, and Nomura, H. (1986). &amp;quot;Lexicase parsing: A lexicon-driven approach to syntactic analysis.&amp;quot; COLING-86, 127-132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucien Tesniere</author>
</authors>
<title>Elements de Syntaxe Struciurale. Paris: Librairie Klincksieck.</title>
<date>1959</date>
<contexts>
<context position="30130" citStr="Tesniere (1959)" startWordPosition="5011" endWordPosition="5012">.2 Grammatical Functions We now come to the second hierarchy of syntax, the hierarchy of grammatical relations. Unlike most other syntactic theories, WG uses constituent structure only for the purpose of describing coordinate constructions (cf. Hudson 1990: 404ff for details). All other syntactic structure is expressed in terms of dependencies between pairs of words, one of which is the head of the other, its dependent. Higher nodes such as phrases or sentences are not represented explicitly in the grammar. WG is thus a variety of dependency grammar. Dependency grammar was first formalized by Tesniere (1959) and refined by Hays (1964), Gaifman (1965), Robinson (1970) and others. A number of dependencybased theories have emerged from the linguistic underground during the last thirty years. These include the Meaning-Text model (Menuk and Zolkovskij 1970; Menuk 1988), Case Grammar (Anderson 1971; 1977), Daughter Dependency Grammar (Hudson 1976), WG (Hudson 1984; 1990), Functional Generative Description (Sgall, Hajieova, and Panevova 1986), and Lexicase (Starosta 1988). While none of these theories has attained widespread popularity, some of their central insights have become increasingly influential</context>
</contexts>
<marker>Tesniere, 1959</marker>
<rawString>Tesniere, Lucien (1959). Elements de Syntaxe Struciurale. Paris: Librairie Klincksieck.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richmond H Thomason</author>
<author>David F Touretzky</author>
</authors>
<title>Inheritance theory and networks with roles.&amp;quot;</title>
<date>1991</date>
<booktitle>In Principles of Semantic Networks,</booktitle>
<pages>231--266</pages>
<publisher>Morgan Kaufmann.</publisher>
<location>San Mateo, CA:</location>
<note>edited by</note>
<contexts>
<context position="21549" citStr="Thomason and Touretzky 1991" startWordPosition="3569" endWordPosition="3572">ties such as having a time and an actor: (22a) word isa action. (22b) action has 1 actor. (22c) so: word has 1 actor. (22d) action has 1 time. (22e) so: word has 1 time. It was this inheritance that allowed us to assume that a word has a time, which can be referred to not only in the rules for word order but also in those for the semantics of tense (cf. (12) above). Another direction in which WG extends the normal scope of inheritance is by allowing it to apply to relations as well as to the more familiar kind of nonrelational category, such as elephant, word, etc. (For a similar approach see Thomason and Touretzky 1991). This allows us to recognize a hierarchy of grammatical relations, with, for example, &apos;object&apos; as a particular kind of &apos;dependent&apos;; which in turn allows us to formulate word-order rules that refer to the appropriate point in the hierarchy, and 140 Norman M. Fraser and Richard A. Hudson Inheritance in Word Grammar then automatically generalize to all relations below this. Here is a simple example of the inferences that can be drawn. (23a) position of dependent of word = after it. (23b) object isa dependent. (23c) LIKE isa word. (23d) so: position of object of LIKE = after it. To summarize, the</context>
</contexts>
<marker>Thomason, Touretzky, 1991</marker>
<rawString>Thomason, Richmond H., and Touretzky, David F. (1991). &amp;quot;Inheritance theory and networks with roles.&amp;quot; In Principles of Semantic Networks, edited by John F. Sowa, 231-266. San Mateo, CA: Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David F Touretzky</author>
</authors>
<title>The Mathematics of Inheritance Systems.</title>
<date>1986</date>
<publisher>Morgan Kaufmann.</publisher>
<location>Los Altos:</location>
<contexts>
<context position="2496" citStr="Touretzky 1986" startWordPosition="382" endWordPosition="383">n typical cases and their features, any of which may be overridden. Thus the shading from core members of a class to peripheral members can be accommodated—indeed, the existence of peripheral members is predicted by the mechanism for overriding defaults. The third and more pragmatic reason why it is useful to recast well-known linguistic problems in terms of default inheritance is that there is a fairly well-developed—though by no means conclusive—body of knowledge on the subject in the artificial intelligence field of knowledge representation (e.g. Etherington and Reiter 1983; Brachman 1985; Touretzky 1986; Etherington 1988). Nearer the computer science mainstream, work in object-oriented programming languages (Cook 1989) offers an interesting range of relevant insights and inheritancebased tools. * Social and Computer Sciences Research Group, University of Surrey, Guildford, Surrey, GU2 5XH, United Kingdom. E-mail: norman@soc.surrey.ac.uk. t Department of Phonetics and Linguistics, University College London, Gower Street, London, WC1E 6BT, United Kingdom. E-mail: thudson@ucl.ac.uk. 1 We received very helpful comments on an earlier draft of this paper from three anonymous Computational Linguist</context>
</contexts>
<marker>Touretzky, 1986</marker>
<rawString>Touretzky, David F. (1986). The Mathematics of Inheritance Systems. Los Altos: Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Warner</author>
</authors>
<title>Multiple heads and minor categories in GPSG.&amp;quot;</title>
<date>1987</date>
<journal>Linguistics</journal>
<pages>27--179</pages>
<contexts>
<context position="34448" citStr="Warner 1987" startWordPosition="5715" endWordPosition="5716">idn&apos;t jump 4 This analysis may in fact be more complicated than it needs to be. We could allow finite verbs to inherit the regular &apos;1 head&apos; simply by not blocking it, and allow for &apos;0 head&apos; by an extra rule, which provides the other alternative. 5 The notion of a word with two heads is meaningless in theories based on phrase structure, because &apos;head&apos; is used there in relation to phrases, not words. The X-bar &apos;head&apos; corresponds to our &apos;root,&apos; the word in a phrase that has no head inside that phrase. It is true that some linguists have suggested that a phrase might have more than one head (e.g. Warner 1987), and this has been a standard analysis of coordinate structures since Bloomfield (1933); but this is very different from a single word having more than one head. 145 Computational Linguistics Volume 18, Number 2 (In a dependency diagram, the arrow points towards the dependent.) This is permitted by a proposition which, at least by implication, overrides the general rule, and which refers to the grammatical function &apos;xcomplement&apos;:6 (33) subject of xcomplement of word = subject of it. In other words, a word may have two heads provided that one of them is the xcomplement of the other. (We return</context>
</contexts>
<marker>Warner, 1987</marker>
<rawString>Warner, Anthony (1987). &amp;quot;Multiple heads and minor categories in GPSG.&amp;quot; Linguistics 27,179-205.</rawString>
</citation>
<citation valid="false">
<title>Word classes noun isa word. proper isa noun. verb isa word. polarity-verb isa verb. Morphosyntactic features predependent isa dependent. postdependent isa dependent. subject isa predependent. complement isa postdependent. xcomplement isa complement. &apos;Lexicon&apos;</title>
<marker></marker>
<rawString>Word classes noun isa word. proper isa noun. verb isa word. polarity-verb isa verb. Morphosyntactic features predependent isa dependent. postdependent isa dependent. subject isa predependent. complement isa postdependent. xcomplement isa complement. &apos;Lexicon&apos;</rawString>
</citation>
<citation valid="false">
<authors>
<author>MARY isa proper</author>
</authors>
<title>whole of MARY = referent of MARY = Mary. DO isa verb. DO/dummy isa DO. DO/dummy isa polarity-verb. stem of DO = whole of past DO = NOT: whole of past DO = stem of it + whole of mEd. meaning of DO/dummy = meaning of xcomplement of it. C. Inherited propositions wl isa proper. wl isa noun. w1 isa word. w1 isa action. w1 has 1 time. w1 has 1 actor. wl has 1 head. w1 has 1 referent. referent of w1 = Mary. w2 isa past tensed finite verb. w2 isa tensed finite verb.</title>
<tech>etc.</tech>
<marker>proper, </marker>
<rawString>MARY isa proper. whole of MARY = &lt;Mary&gt;. referent of MARY = Mary. DO isa verb. DO/dummy isa DO. DO/dummy isa polarity-verb. stem of DO = &lt;do&gt;. whole of past DO = &lt;did&gt;. NOT: whole of past DO = stem of it + whole of mEd. meaning of DO/dummy = meaning of xcomplement of it. C. Inherited propositions wl isa proper. wl isa noun. w1 isa word. w1 isa action. w1 has 1 time. w1 has 1 actor. wl has 1 head. w1 has 1 referent. referent of w1 = Mary. w2 isa past tensed finite verb. w2 isa tensed finite verb. etc.</rawString>
</citation>
<citation valid="false">
<authors>
<author>w2 isa</author>
</authors>
<title>verb. w2 has 1 finiteness. w2 has 1 mood. w2 has 1 tense. w2 has [0-1J head. w2 has 1 subject. subject of w2 isa noun. Computational Linguistics Volume 18, Number 2 position of subject of</title>
<booktitle>w2 = before w2. w2 has 0 complement. w2 has 1 sense. w2 has 1 referent. c1 = jumping. c1 isa action. c1 has 1 time. c1 has 1 actor. c2 isa jumping. c2 isa action. c2 has 1 time. c2 has 1 actor. time of c2 = before w2. actor of cl = Mary. actor of c2 =</booktitle>
<publisher>Mary.</publisher>
<marker>isa, </marker>
<rawString>w2 isa verb. w2 has 1 finiteness. w2 has 1 mood. w2 has 1 tense. w2 has [0-1J head. w2 has 1 subject. subject of w2 isa noun. Computational Linguistics Volume 18, Number 2 position of subject of w2 = before w2. w2 has 0 complement. w2 has 1 sense. w2 has 1 referent. c1 = jumping. c1 isa action. c1 has 1 time. c1 has 1 actor. c2 isa jumping. c2 isa action. c2 has 1 time. c2 has 1 actor. time of c2 = before w2. actor of cl = Mary. actor of c2 = Mary.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>