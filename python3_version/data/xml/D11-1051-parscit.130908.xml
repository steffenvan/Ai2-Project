<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000209">
<title confidence="0.995789">
Semantic Topic Models: Combining Word Distributional Statistics and
Dictionary Definitions
</title>
<author confidence="0.999075">
Weiwei Guo Mona Diab
</author>
<affiliation confidence="0.987222">
Department of Computer Science, Center for Computational Learning Systems,
Columbia University, Columbia University,
</affiliation>
<email confidence="0.998992">
weiwei@cs.columbia.edu mdiab@ccls.columbia.edu
</email>
<sectionHeader confidence="0.997389" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999972222222222">
In this paper, we propose a novel topic
model based on incorporating dictionary
definitions. Traditional topic models treat
words as surface strings without assuming
predefined knowledge about word mean-
ing. They infer topics only by observing
surface word co-occurrence. However, the
co-occurred words may not be semanti-
cally related in a manner that is relevant
for topic coherence. Exploiting dictionary
definitions explicitly in our model yields
a better understanding of word semantics
leading to better text modeling. We exploit
WordNet as a lexical resource for sense
definitions. We show that explicitly mod-
eling word definitions helps improve per-
formance significantly over the baseline
for a text categorization task.
</bodyText>
<sectionHeader confidence="0.999508" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999771566666667">
Latent Dirichlet Allocation (LDA) (Blei et al.,
2003) serves as a data-driven framework in model-
ing text corpora. The statistical model allows vari-
able extensions to integrate linguistic features such
as syntax (Griffiths et al., 2005), and has been ap-
plied in many areas.
In LDA, there are two factors which determine
the topic of a word: the topic distribution of the
document, and the probability of a topic to emit
this word. This information is learned in an unsu-
pervised manner to maximize the likelihood of the
corpus. However, this data-driven approach has
some limitations. If a word is not observed fre-
quently enough in the corpus, then it is likely to
be assigned the dominant topic in this document.
For example, the word grease (a thick fatty oil) in
a political domain document should be assigned
the topic chemicals. However, since it is an in-
frequent word, LDA cannot learn its correct se-
mantics from the observed distribution, the LDA552
model will assign it the dominant document topic
politics. If we look up the semantics of the word
grease in a dictionary, we will not find any of its
meanings indicating the politics topic, yet there is
ample evidence for the chemical topic. Accord-
ingly, we hypothesize that if we know the seman-
tics of words in advance, we can get a better in-
dication of their topics. Therefore, in this paper,
we test our hypothesis by exploring the integration
of word semantics explicitly in the topic modeling
framework.
In order to incorporate word semantics from
dictionaries, we recognize the need to model
sense-topic distribution rather than word-topic dis-
tribution, since dictionaries are constructed at the
sense level. We use WordNet (Fellbaum, 1998)
as our lexical resource of choice. The notion of
a sense in WordNet goes beyond a typical word
sense in a traditional dictionary since a WordNet
sense links senses of different words that have
similar meanings. Accordingly, the sense for the
first verbal entry for buy and for purchase will
have the same sense id (and same definition) in
WordNet, while they could have different mean-
ing definitions in a traditional dictionary such as
the Merriam Webster Dictionary or LDOCE. In
our model, a topic will first emit a WordNet sense,
then the sense will generate a word. This is in-
spired by the intuition that words are instantiations
of concepts.
The paper is organized as follows: In Sections 2
and 3, we describe our models based on WordNet.
In Section 4, experiment results on text catego-
rization are presented. Moreover, we analyze both
qualitatively and quantitatively the contribution of
modeling definitions (by teasing out the contribu-
tion of explicit sense modeling in a word sense dis-
ambiguation task). Related work is introduced in
Section 5. We conclude in Section 6 by discussing
some possible future directions.
</bodyText>
<note confidence="0.9953985">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 552–561,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<figureCaption confidence="0.877397">
Figure 1: (a) LDA: Latent Dirichlet Allocation
(Blei et al., 2003). (b) STM: Semantic topic
model. The dashed arrows indicate the distribu-
tions (φ and q) and nodes (z) are not influenced by
the values of pointed nodes.
</figureCaption>
<sectionHeader confidence="0.996686" genericHeader="method">
2 Semantic Topic Model
</sectionHeader>
<subsectionHeader confidence="0.995774">
2.1 Latent Dirichlet Allocation
</subsectionHeader>
<bodyText confidence="0.999894428571428">
We briefly introduce LDA where Collapsed Gibbs
Sampling (Griffiths and Steyvers, 2004) is used
for inference. In figure 1a, given a corpus with
D documents, LDA will summarize each docu-
ment as a normalized T-dimension topic mixture
θ. Topic mixture θ is drawn from a Dirichlet distri-
bution Dir(α) with a symmetric prior α. φ con-
tains T multinomial distribution, each represent-
ing the probability of a topic z generating word w
p(wjz). φ is drawn from a Dirichlet distribution
Dir(β) with prior β.
In Collapsed Gibbs Sampling, the distribution
of a topic for the word wi = w based on values of
other data is computed as:
</bodyText>
<equation confidence="0.9963045">
nw−i,z + O (1)
n−i,z + WO
</equation>
<bodyText confidence="0.9037076">
In this equation, n(d)
−i,z is a count of how many
words are assigned topic z in document d, exclud-
ing the topic of the ith word; nw−i,z is a count of
how many words = w are assigned topic z, also
</bodyText>
<page confidence="0.995555">
553
</page>
<bodyText confidence="0.999769833333333">
excluding the topic of the ith word. Hence, the
first fraction is the proportion of the topic in this
document p(z|θ). The second fraction is the prob-
ability of topic z emitting word w. After the topics
become stable, all the topics in a document con-
struct the topic mixture θ.
</bodyText>
<subsectionHeader confidence="0.9989395">
2.2 Applying Word Sense Disambiguation
Techniques
</subsectionHeader>
<bodyText confidence="0.995893952380953">
We add a sense node between the topic node and
the word node based on two linguistic observa-
tions: a) Polysemy: many words have more than
one meaning. A topic is more directly relevant to
a word meaning (sense) than to a word due to pol-
ysemy; b) Synonymy: different words may share
the same sense. WordNet explicitly models syn-
onymy by linking synonyms to the same sense. In
WordNet, each sense has an associated definition.
It is worth noting that we model the sense-word
relation differently from (Boyd-Graber and Blei,
2007), where in their model words are generated
from topics, then senses are generated from words.
In our model, we assume that during the genera-
tive process, the author picks a concept relevant to
the topic, then thinks of a best word that represents
that concept. Hence the word choice is dependent
on the relatedness of the sense and its fit to the
document context.
In standard topic models, the topic of a word
is sampled from the document level topic mixture
θ. The underlying assumption is that all words in a
document constitute the context of the target word.
However, it is not the case in real world corpora.
Titov and McDonald (2008) find that using global
topic mixtures can only extract global topics in on-
line reviews (e.g., Creative Labs MP3 players and
iPods) and ignores local topics (product features
such as portability and battery). They design the
Multi-grain LDA where the local topic of a word
is only determined by topics of surrounding sen-
tences. In word sense disambiguation (WSD), an
even narrower context is taken into consideration,
for instance in graph based WSD models (Mihal-
cea, 2005), the choice of a sense for a word only
depends on a local window whose size equals the
length of the sentence. Later in (Sinha and Mihal-
cea, 2007; Guo and Diab, 2010; Li et al., 2010),
people use a fixed window size containing around
12 neighbor words for WSD.
Accordingly, we adopt the WSD inspired local
window strategy in our model. However, we do
</bodyText>
<figure confidence="0.9912418">
d
T
S
d
d
P(zi = z|z−i, w) ∝ n(d)
−i,z + α
×
n−i + Tα
(d)
</figure>
<bodyText confidence="0.99977465">
not employ the complicated schema in (Titov and
McDonald, 2008). We simply hypothesize that the
surrounding E words are semantically related to the
considered word, and they construct a local slid-
ing window for that target word. For a document
d with Nd words, we represent it as Nd local win-
dows – a window is created for each word. The
model is illustrated in the left rectangle in figure
1b. The window size is fixed for each word: it
contains E/2 preceding words, and E/2 following
words. Therefore, a word in the original document
will have E copies, existing in E+1 local windows.
Similarly, there are E + 1 pairs of topics/senses
assigned for each word in the original document.
Each window has a distribution Bi over topics. Bi
will emit the topics of words in the window.
This approach enables us to exploit different
context sizes without restricting it to the sentence
length, and hence spread topic information across
sentence boundaries.
</bodyText>
<subsectionHeader confidence="0.996472">
2.3 Integrating Definitions
</subsectionHeader>
<bodyText confidence="0.999989307692308">
Intuitively, a sense definition reveals some prior
knowledge on the topic domain: the definition of
sense [crime, offense, offence] indicates a legal
topic; the definition of sense [basketball] indicates
a sports topic, etc. Therefore, during inference, we
want to choose a topic/sense pair for each word,
such that the topic is supported by the context B
and the sense definition also matches that topic.
Given that words used in the sense definitions
are strongly relevant to the sense/concept, we set
out to find the topics of those definition words, and
accordingly assign the sense sen itself these top-
ics. We treat a sense definition as a document and
perform Gibbs sampling on it. We normalize def-
inition length by a variable -y. Therefore, before
the topic model sees the actual documents, each
sense s has been sampled -y times. The -y topics
are then used as a “training set”, so that given a
sense, φ has some prior knowledge of which topic
it should be sampled from.
Consider the sense [party, political party] with
a definition “an organization to gain political
power” of length 6 when -y = 12. If topic
model assigns politics topic to the words “orga-
nization political power”, then sense [party, polit-
ical party] will be sampled from politics topic for
</bodyText>
<sectionHeader confidence="0.578023" genericHeader="method">
3 * -y/definitionLength = 6 times.
</sectionHeader>
<bodyText confidence="0.9964645">
We refer to the proposed model as Semantic
Topic Model (figure 1b). For each window vi in
</bodyText>
<page confidence="0.993691">
554
</page>
<bodyText confidence="0.994859863636364">
the document set, the model will generate a distri-
bution of topics Bi. It will emit the topics of E + 1
words in the window. For a word wij in window
vi, a sense sij is drawn from the topic, and then sij
generates the word wi. Sense-topic distribution φ
contains T multinomial distributions over all pos-
sible senses in the corpus drawn from a symmetric
Dirichlet distribution Dir(β). From WordNet we
know the set of words W (s) that have a sense s
as an entry. A sense s can only emit words from
W(s). Hence, for each sense s, there is a multi-
nomial distribution qs over W (s). All q are drawn
from symmetric Dir(A).
On the definition side, we use a different prior
αs to generate a topic mixture B. Aside from gen-
erating si, zi will deterministically generate the
current sense sen for -y/Nsen times (Nsen is the
number of words in the definition of sense sen),
so that sen is sampled -y times in total.
The formal procedure of generative process is
the following:
For the definition of sense sen:
</bodyText>
<listItem confidence="0.7954646">
• choose topic mixture B — Dir(αs).
• for each word wi:
— choose topic zi — Mult(B).
— choose sense si — Mult(φzi).
— deterministically choose sense sen ti
</listItem>
<bodyText confidence="0.891771666666667">
Mult(φzi) for -y/Nsen times.
— choose word wi — Mult(qsi).
For each window vi in a document:
</bodyText>
<listItem confidence="0.794233">
• choose local topic mixture Bi — Dir(αd).
• for each word wij in vi:
— choose topic zij — Mult(Bi).
— choose sense sij — Mult(φzij).
— choose word wij — Mult(qsij).
</listItem>
<subsectionHeader confidence="0.999679">
2.4 Using WordNet
</subsectionHeader>
<bodyText confidence="0.999991619047619">
Since definitions and documents are in different
genre/domains, they have different distributions
on senses and words. Besides, the definition sets
contain topics from all kinds of domains, many of
which are irrelevant to the document set. Hence
we prefer φ and q that are specific for the doc-
ument set, and we do not want them to be “cor-
rupted” by the text in the definition set. There-
fore, as in figure 1b, the dashed lines indicate that
when we estimate φ and q, the topic/sense pair and
sense/word pairs in the definition set are not con-
sidered.
WordNet senses are connected by relations such
as synonymy, hypernymy, similar attributes, etc.
We observe that neighboring sense definitions are
usually similar and are in the same topic domain.
Hence, we represent the definition of a sense as
the union of itself with its neighboring sense def-
initions pertaining to WordNet relations. In this
way, the definition gets richer as it considers more
data for discovering reliable topics.
</bodyText>
<sectionHeader confidence="0.999771" genericHeader="method">
3 Inference
</sectionHeader>
<bodyText confidence="0.996601066666667">
We still use Collapsed Gibbs Sampling to find la-
tent variables. Gibbs Sampling will initialize all
hidden variables randomly. In each iteration, hid-
den variables are sequentially sampled from the
distribution conditioned on all the other variables.
In order to compute the conditional probability
P(zi = z, si = s|z−i, s−i, w) for a topic/sense
pair, we start by computing the joint probability
P(z, s, w) = P(z)P(s|z)P(w|s). Since the gen-
erative processes are not exactly the same for def-
initions and documents, we need to compute the
joint probability differently. We use a type spe-
cific subscript to distinguish them: Ps(·) for sense
definitions and Pd(·) for documents.
Let sen be a sense. Integrating out O we have:
</bodyText>
<equation confidence="0.999378333333333">
~S YS Q
�Γ(Tαs) z Γ(n(sen)
z + αs)
Ps(z) = (2)
Γ(αs)T Γ(n(sen) + Tα)
sen=1
</equation>
<bodyText confidence="0.999587928571429">
where n(sen) zmeans the number of times a word
in the definition of sen is assigned to topic z, and
n(sen) is the length of the definition. S is all the
potential senses in the documents.
We have the same formula of P(s|z) and
P(w|s) for definitions and documents. Similarly,
let nz be the number of words in the documents
assigned to topic z, and nsz be the number of times
sense s assigned to topic z. Note that when s
appears in the superscript surrounded by brackets
such as n(s)
z , it denotes the number of words as-
signed to topics z in the definition of sense s. By
integrating out φ we obtain the second term:
</bodyText>
<equation confidence="0.993676166666667">
~T YT Q
�Γ(Sβ) s Γ(ns z + n(s)
z γ/n(s) + β)
P(s|z) = Γ(β)S Γ(nz + P s0 n(s0)
z γ/n(s0) + Sβ)
z=1
</equation>
<bodyText confidence="0.99732425">
At last, assume ns denotes the number of sense
s in the documents, and nws denotes the number of
sense s to generate the word w, then integrating
out η we have:
</bodyText>
<equation confidence="0.79583925">
QW (s)
w Γ(nws + λ)
555
Γ(ns + |W(s)|λ)
</equation>
<bodyText confidence="0.9994892">
With equation 2-4, we can compute the condi-
tional probability Ps(zi = z, si = s|z−i, s−i, w)
for a sense-topic pair in the sense definition. Let
seni be the sense definition containing word wi,
then we have:
</bodyText>
<equation confidence="0.999969285714286">
Ps(zi = z, si = s|z−c, s−c, w) ∝ n(seni) (5)
nsz + n(s0) −i,z + αs
−i,zγ/n(s0) + β n(seni)
−i + Tαs
nws + λ
nz + Ps0 n(s0)
−i,zγ/n(s0) + Sβ
</equation>
<bodyText confidence="0.999811333333333">
The subscript −i in expression n−i denotes
the number of certain events excluding word wi.
Hence the three fractions in equation 5 correspond
to the probability of choosing z from Osen, choos-
ing s from z and choosing w from s. Also note that
our model defines s that can only generate words
in W (s), therefore for any word w ∈/ W (s), the
third fraction will yield a 0.
The probability for documents is similar to that
for definitions except that there is a topic mixture
for each word, which is estimated by the topics in
the window. Hence Pd(z) is estimated as:
</bodyText>
<equation confidence="0.9965525">
Qz Γ(n(vi)
z + αd)
(6)
Γ(n(vi) + Tαd)
</equation>
<bodyText confidence="0.999463333333333">
Thus, the conditional probability for documents
can be estimated by cancellation terms in equation
6, 3, and 4:
</bodyText>
<equation confidence="0.9994428">
Pd(zij = z, sij = s|z−c�, s−c�, w)
ns−ij,z + n(s0)
z γ/n(s0) + β
z γ/n(s0) + Sβ
n−ij,z + P s0 n(s0)
</equation>
<subsectionHeader confidence="0.995413">
3.1 Approximation
</subsectionHeader>
<bodyText confidence="0.9996779375">
In current model, each word appears in E + 1 win-
dows, and will be generated E + 1 times, so there
will be E + 1 pairs of topics/senses sampled for
each word, which requires a lot of additional com-
putation (proportional to context size E). On the
other hand, it can be imagined that the set of val-
ues {zij, sij|j − E/2 ≤ i ≤ j + E/2} in dif-
ferent windows vi should roughly be the same,
since they are hidden values for the same word wj.
Therefore, to reduce computation complexity dur-
ing Gibbs sampling, we approximate the values of
{zij, sij |i =6 j} by the topic/sense (zjj, sjj) that
are generated from window vj. That is, in Gibbs
sampling, the algorithm does not actually sample
the values of {zij, sij,  |i =6 j}; instead, it directly
assumes the sampled values are zjj, sjj.
</bodyText>
<equation confidence="0.985717769230769">
P(w|s) = YS Γ(|W(s)|λ)
s=1 Γ(λ)|W(s)|
ns + |W(s)|λ
YPd(z) = Γ(Tαd)
i Γ(αd)T
n(vi)
−ij,z + αd
n(vi)
−ij + Tαd
nw−ij,s + λ
n−ij,s + |W(s)|λ
∝
(7)
</equation>
<sectionHeader confidence="0.991643" genericHeader="method">
4 Experiments and Analysis
</sectionHeader>
<bodyText confidence="0.999759510204081">
Data: We experiment with several datasets,
namely, the Brown Corpus (Brown), New York
Times (NYT) from the American National Cor-
pus, Reuters (R20) and WordNet definitions. In a
preprocessing step, we remove all the non-content
words whose part of speech tags are not one of
the following set {noun, adjective, adverb, verb}.
Moreover, words that do not have a valid lemma in
WordNet are removed. For WordNet definitions,
we remove stop words hence focusing on relevant
content words.
Corpora statistics after each step of preprocess-
ing is presented in Table 1. The column WN token
lists the number of word#pos tokens after prepro-
cessing. Note that now we treat word#pos as a
word token. The column word types shows cor-
responding word#pos types, and the total number
of possible sense types is listed in column sense
types. The DOCs size for WordNet is the total
number of senses defined in WordNet.
Experiments: We design two tasks to test our
models: (1) text categorization task for evaluat-
ing the quality of values of topic nodes, and (2) a
WSD task for evaluating the quality of the values
of the sense nodes, mainly as a diagnostic tool tar-
geting the specific aspect of sense definitions in-
corporation and distinguish that component’s con-
tribution to text categorization performance. We
compare the performance of four topic models.
(a) LDA: the traditional topic model proposed in
(Blei et al., 2003) except that it uses Gibbs Sam-
pling for inference. (b) LDA+def: is LDA with
sense definitions. However they are not explic-
itly modeled; rather they are treated as documents
and used as augmented data. (c) STM0: the topic
model with an additional explicit sense node in the
model, but we do not model the sense definitions.
And finally (d) STMn is the full model with defi-
nitions explicitly modeled. In this setting n is the
-y value. We experiment with different -y values
in the STM models, and investigate the semantic
scope of words/senses by choosing different win-
dow size E. We report mean and standard deviation
based on 10 runs.
It is worth noting that a larger window size
E suggests documents have larger impact on the
model (φ,η) than definitions, since each document
word has E copies. This is not a desirable property
when we want to investigate the weight of defi-
</bodyText>
<page confidence="0.984057">
556
</page>
<bodyText confidence="0.999725833333333">
nitions by choosing different -y values. Accord-
ingly, we only use zjj, sjj, wjj to estimate φ, η, so
that the impact of documents is fixed. This makes
more sense, in that after the approximation in sec-
tion 3.1, there is no need to use {zij, sij,  |i =� j}
(they have the same values as zjj, sjj).
</bodyText>
<subsectionHeader confidence="0.981767">
4.1 Text Categorization
</subsectionHeader>
<bodyText confidence="0.99996096969697">
We believe our model can generate more “correct”
topics by looking into dictionaries. In topic mod-
els, each word is generalized as a topic and each
document is summarized as the topic mixture θ,
hence it is natural to evaluate the quality of in-
ferred topics in a text categorization task. We fol-
low the classification framework in (Griffiths et
al., 2005): first run topic models on each dataset
individually without knowing label information
to achieve document level topic mixtures, then we
employ Naive Bayes and SVM (both implemented
in the WEKA Toolkit (Hall et al., 2009)) to per-
form classification on the topic mixtures. For all
document, the features are the percentage of top-
ics. Similar to (Griffiths et al., 2005), we assess in-
ferred topics by the classification accuracy of 10-
fold cross validation on each dataset.
We evaluate our models on three datasets in the
cross validation manner: The Brown corpus which
comprises 500 documents grouped into 15 cate-
gories (same set used in (Griffiths et al., 2005));
NYT comprising 800 documents grouped into the
16 most frequent label categories; Reuters (R20)
comprising 8600 documents labeled with the most
frequent 20 categories. In R20, combination of
categories is treated as separate category labels,
so money, interest and interest are considered
different labels.
For the three datasets, we use the Brown cor-
pus only as a tuning set to decide on the topic
model parameters for all of our experimentation,
and use the optimized parameters directly on NYT
and R20 without further optimization.
</bodyText>
<subsectionHeader confidence="0.693787">
4.1.1 Classification Results
</subsectionHeader>
<bodyText confidence="0.999803">
Searching -y and E on Brown: The classification
accuracy on the Brown corpus with different E and
-y values using Naive Bayes and SVM are pre-
sented in figure 2a and 2b. In this section, the
number of topics T is set to 50. The possible
E values in the horizontal axis are 2, 10, 20, 40,
all. The possible -y values are 0, 1, 2. Note that
E = all means that no local window is used, and
-y = 0 means definitions are not used. The hyper-
</bodyText>
<table confidence="0.997999833333333">
Corpus DOCs size orig tokens content tokens WN tokens word types sense types
Brown 500 1022393 580882 547887 27438 46645
NYT 800 743665 436988 393120 19025 37631
R20 8595 901691 450935 417331 9930 24834
SemCor 352 676546 404460 352563 28925 45973
WordNet 117659 1447779 886923 786679 42080 60567
</table>
<tableCaption confidence="0.9988">
Table 1: Corpus statistics
</tableCaption>
<figureCaption confidence="0.993457">
Figure 2: Classification accuracy at different parameter settings
</figureCaption>
<figure confidence="0.997577608695652">
70
65
60
55
50
45
40
0 10 20 30 40 50 all
window size
(a) Naive Bayes on Brown
65
60
55
50
45
40
0 10 20 30 40 50 all
window size
(b) SVM on Brown
55
0 10 20 30 40 50 all
accuracy%
80
75
70
65
60
STM0
STM1
STM2
window size
(c) SVM on NYT
accuracy%
accuracy%
75
70
LDA
LDA+def
STM0
STM1
STM2
LDA
LDA+def
STM0
STM1
STM2
</figure>
<figureCaption confidence="0.578589">
parameters are tuned as αd = 0.1, αs = 0.01, β =
0.01, A = 0.1.
</figureCaption>
<bodyText confidence="0.999687793103448">
From figure 2, we observe that results using
SVM have the same trend as Naive Bayes except
that the accuracies are roughly 5% higher for SVM
classifier. The results of LDA and LDA+def sug-
gest that simply treating definitions as documents
in an augmented data manner does not help. Com-
paring SMT0 with LDA in the same c values, we
find that explicitly modeling the sense node in the
model greatly improves the classification results.
The reason may be that words in LDA are inde-
pendent isolated strings, while in STM0 they are
connected by senses.
STM2 prefers smaller window sizes (c less than
40). That means two words with a distance larger
than 40 are not necessarily semantically related or
share the same topic. This c number also corre-
lates with the optimal context window size of 12
reported in WSD tasks (Sinha and Mihalcea, 2007;
Guo and Diab, 2010).
Classification results: Table 2 shows the results
of our models using best tuned parameters of c =
10, &apos;y = 2 on 3 datasets. We present three base-
lines in Table 2: (1) WEKA uses WEKA’s classi-
fiers directly on bag-of-words without topic mod-
eling. The values of features are simply term fre-
quency. (2) WEKA+FS performs feature selection
using information gain before applying classifica-
tion. (3) LDA, is the traditional topic model. Note
that Griffiths et al.’s (2005) implementation of
</bodyText>
<page confidence="0.986884">
557
</page>
<bodyText confidence="0.998877">
LDA achieve 51% on Brown corpus using Naive
Bayes . Finally the Table illustrates the results
obtained using our proposed models STM0 (&apos;y=0)
and STM2 (&apos;y = 2).
It is worth noting that R20 (compared to NYT)
is a harder condition for topic models. This is
because fewer words (10000 distinct words ver-
sus 19000 in NYT) are frequently used in a large
training set (8600 documents versus 800 in NYT),
making the surface word feature space no longer
as sparse as in the NYT or Brown corpus, which
implies simply using surface words without con-
sidering the words distributional statistics – topic
modeling – is good enough for classification. In
(Blei et al., 2003) figure 10b they also show worse
text categorization results over the SVM baseline
when more than 15% of the training labels of
Reuters are available for the SVM classifiers, indi-
cating that LDA is less necessary with large train-
ing data. In our investigation, we report results
on SVM classifiers trained on the whole Reuters
training set. In our experiments, LDA fails to cor-
rectly classify nearly 10% of the Reuters docu-
ments compared to the WEKA baseline, however
STM2 can still achieve significantly better accu-
racy (+4%) in the SVM classification condition.
Table 2 illustrates that despite the difference be-
tween NYT, Reuters and Brown (data size, genre,
domains, category labels), exploiting WSD tech-
niques (namely using a local window size cou-
pled with explicitly modeling a sense node) yields
</bodyText>
<table confidence="0.999591857142857">
Brown NYT R20
NB SVM NB SVM NB SVM
WEKA 48 47.8 57 54.1 72.4 82.9
WEKA+FS 50 47.2 56.9 55.1 72.9 83.4
LDA 47.8±4.3 53.9±3.8 48.5±5.5 53.8±3.5 61.0±3.3 72.5±2.5
STM0 68.6±3.5 70.7±3.9 66.7±3.8 74.2±4.0 72.7±3.5 85.2±0.9
STM2 69.3±3.3 75.4±3.7 74.6±3.3 79.3±2.5 73±3.7 86.9±1.2
</table>
<tableCaption confidence="0.971526">
Table 2: Classification results on 3 datasets using hyperparameters tuned on Brown.
</tableCaption>
<footnote confidence="0.473638">
1.Sports 2.Politics 3.National News 4.Entertainment 5.International News
6.Society 7.Business 8.Miscellaneous 9.Finance 10.Culture 11.Science
12.Health 13.Law 14.Technology 15.Religion 16.Environment
</footnote>
<figureCaption confidence="0.999005">
Figure 3: SVM accuracy on each category of NYT
</figureCaption>
<bodyText confidence="0.999938272727273">
significantly better results than all three baselines
including LDA. Furthermore, explicit definition
modeling as used in STM2 yields the best perfor-
mance consistently overall.
Finally, in Figure 2c we show the SVM clas-
sification results on NYT in different parame-
ter settings. We find that the NYT classifica-
tion accuracy trend is consistent with that on the
Brown corpus for each parameter setting of E E
12,10, 20, 40, all} and γ E 10, 1, 21. This further
proves the robustness of STMn.
</bodyText>
<subsectionHeader confidence="0.992375333333333">
4.2 Analysis on the Impact of Modeling
Definitions
4.2.1 Qualitative Analysis
</subsectionHeader>
<bodyText confidence="0.999645692307692">
To understand why definitions are helpful in text
categorization, we analyze the SVM performance
of STM0 and STM2 (E = 10) on each cate-
gory of NYT dataset (figure 3). We find STM2
outperforms STM0 in all categories. However,
the largest gain is observed in Society, Miscel-
laneous, Culture, Technology. For Technology,
we should credit WordNet definitions, since Tech-
nology may contain many infrequent technical
terms, and STM0 cannot generalize the meaning
of words only by distributional information due to
their low frequency usage. However in some other
domains, fewer specialized words are repeatedly
</bodyText>
<page confidence="0.987572">
558
</page>
<bodyText confidence="0.999311357142857">
used, hence STM0 can do as well as STM2.
For the other 3 categories, we hypothesize that
these documents are likely to be a mixture of mul-
tiple topics. For example, a Culture news could
contain topics pertaining to religion, history, art;
while a Society news about crime could relate to
law, family, economics. In this case, it is very
important to sample a true topic for each word,
so that ML algorithms can distinguish the Cul-
ture documents from the Religion ones by the pro-
portion of topics. Accordingly, adding definitions
should be very helpful, since it specifically defines
the topic of a sense, and shields it from the influ-
ence of other “incorrect/irrelevant” topics.
</bodyText>
<subsectionHeader confidence="0.5831765">
4.2.2 Quantitative Analysis with Word Sense
Disambiguation
</subsectionHeader>
<bodyText confidence="0.999020923076923">
A side effect of our model is that it sense disam-
biguates all words. As a means of analyzing and
gaining some insight into the exact contribution of
explicitly incorporating sense definitions (STMn)
versus simply a sense node (STM0) in the model,
we investigate the quality of the sense assignments
in our models. We believe that the choice of the
correct sense is directly correlated with the choice
of a correct topic in our framework. Accord-
ingly, a relative improvement of STMn over STM0
(where the only difference is the explicit sense def-
inition modeling) in WSD task is an indicator of
the impact of using sense definitions in the text
categorization task.
WSD Data: We choose the all-words WSD task in
which an unsupervised WSD system is required to
disambiguate all the content words in documents.
Our models are evaluated against the SemCor
dataset. We prefer SemCor to all-words datasets
available in Senseval-3 (Snyder and Palmer, 2004)
or SemEval-2007 (Pradhan et al., 2007), since
it includes many more documents than either set
(350 versus 3) and therefore allowing more reli-
able results. Moreover, SemCor is also the dataset
used in (Boyd-Graber et al., 2007), where a Word-
Net based topic model for WSD is introduced. The
</bodyText>
<figure confidence="0.935297">
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
accuracy%
100
90
80
70
60
50
STM0
STM2
</figure>
<table confidence="0.993557">
Total Noun Adjective Adverb Verb
sense annotated words 225992 86996 31729 18947 88320
polysemous words 187871 70529 21989 11498 83855
TF-IDF - 0.422 0.300 0.153 0.182
</table>
<tableCaption confidence="0.99981">
Table 3: Statistics of SemCor per POS
</tableCaption>
<bodyText confidence="0.997895222222222">
statistics of SemCor is listed in table 3.
We use hyperparameters tuned from the text cat-
egorization task: αd=0.1, αs=0.01, β=0.01, δ=1,
T=50, and try different values of c E 110, 20, 40}
and -y E 10, 2,10}. The Brown corpus and Word-
Net definitions corpus are used as augmented data,
which means the dashed line in figure 1c will be-
come bold. Finally, we choose the most frequent
answer for each word in the last 10 iterations of a
Gibbs Sampling run as the final sense choice.
WSD Results: Disambiguation per POS results
are presented in table 4. We only report results
on polysemous words. We can see that modeling
definitions (STM2 and STM10) improves perfor-
mance significantly over STM0’s across the board
per POS and overall. The fact that STMn picks
more correct senses helps explain why STMn clas-
sifies more documents correctly than STM0. Also
it is interesting to see that unlike in the text cate-
gorization task, larger values of -y generate better
WSD results. However, the window size c, does
not make a significant difference, yet we note that
c=10 is still the optimal value, similar to our ob-
servation in the text categorization task.
STM10 achieves similar results as in LDAWN
(Boyd-Graber et al., 2007) which was specifically
designed for WSD. LDAWN needs a fine grained
hypernym hierarchy to perform WSD, hence they
can only disambiguate nouns. They report differ-
ent performances under various parameter setting.
We cite their best performance of 38% accuracy
on nouns as a comparison point to our best perfor-
mance for nouns of 38.5%.
An interesting feature of STM10 is that it
performs much better in nouns than adverbs and
verbs, compared to a random baseline in Table
4. This is understandable since topic information
content is mostly borne by nouns and adjectives,
while adverbs and verbs tend to be less informa-
tive about topics (e.g., even, indicate, take), and
used more across different domain documents.
Hence topic models are weaker in their ability
to identify clear cues for senses for verbs and
adverbs. In support of our hypothesis about the
POS distribution, we compute the average TF-IDF
</bodyText>
<page confidence="0.988655">
559
</page>
<bodyText confidence="0.946624782608696">
scores for each POS (shown in Table 3 according
to the equation illustrated below). The average
TF-IDF clearly indicate the positive skewness of
the nouns and adjectives (high TF-IDF) correlates
with the better WSD performance.
� TF-IDF(pos) = i
Ed TF-IDF(wi,d)
# of wi,d
where wi,d E pos.
At last, we notice that the most frequent sense
baseline performs much better than our models.
This is understandable since: (1) most frequent
sense baseline can be treated as a supervised
method in the sense that the sense frequency is
calculated based on the sense choice as present
in sense annotated data; (2) our model is not de-
signed for WSD, therefore it discards a lot of in-
formation when choosing the sense: in our model,
the choice of a sense si is only dependent on two
facts: the corresponding topic zi and word wi,
while in (Li et al., 2010; Banerjee and Pedersen,
2003), they consider all the senses and words in
the context words.
</bodyText>
<sectionHeader confidence="0.999984" genericHeader="method">
5 Related work
</sectionHeader>
<bodyText confidence="0.99920725">
Various topic models have been developed for
many applications. Recently there is a trend
of modeling document dependency (Dietz et al.,
2007; Mei et al., 2008; Daume, 2009). How-
ever, topics are only inferred based on word co-
occurrence, while word semantics are ignored.
Boyd-Graber et al. (2007) are the first to inte-
grate semantics into the topic model framework.
They propose a topic model based on WordNet
noun hierarchy for WSD. A word is assumed to be
generated by first sampling a topic, then choosing
a path from the root node of hierarchy to a sense
node corresponding to that word. However, they
only focus on WSD. They do not exploit word def-
initions, neither do they report results on text cat-
egorization.
Chemudugunta et al. (2008) also incorporate a
sense hierarchy into a topic model. In their frame-
work, a word may be directly generated from a
topic (as in standard topic models), or it can be
</bodyText>
<table confidence="0.999649333333333">
Total Noun Adjective Adverb Verb
random 22.1 26.2 27.9 32.2 15.8
most frequent sense 64.7 74.7 77.5 74.0 59.6
STM0 e = 10 24.1±1.4 29.3±4.3 28.7±1.1 34.1±3.1 17.1±1.6
e = 20 24±1.3 30.2±3.3 29.1±1.4 34.9±3.1 15.9±0.7
e = 40 24±2.4 28.4±4.3 28.7±1.1 36.4±4.7 17.3±2.4
STM2 e = 10 27.5±1.1 36.1±3.8 34.0±1.2 33.4±1.8 17.8±1.4
e = 20 25.7±1.3 32.0±4.2 33.5±0.7 34.2±3.4 17.3±0.7
e = 40 26.1±1.3 32.5±3.9 33.6±0.9 34.2±3.4 17.5±1.4
STM10 e = 10 28.8±1.1 38.5±2.3 34.7±0.8 34.0±3.3 18.4±1.2
e = 20 27.7±1.0 36.8±2.2 34.5±0.7 33.0±3.1 17.6±0.7
e = 40 28.1±1.5 38.4±3.1 34.0±1.0 35.1±5.4 17.0±0.9
</table>
<tableCaption confidence="0.998996">
Table 4: Disambiguation results per POS on polysemous words.
</tableCaption>
<bodyText confidence="0.999932413793104">
generated by choosing a sense path in the hierar-
chy. Note that no topic information is on the sense
path. If a word is generated from the hierarchy,
then it is not assigned a topic. Their models based
on different dictionaries improve perplexity.
Recently, several systems have been proposed
to apply topic models to WSD. Cai et al. (2007)
incorporate topic features into a supervised WSD
framework. Brody and Lapata (2009) place the
sense induction in a Baysian framework by assum-
ing each context word is generated from the target
word’s senses, and a context is modeled as a multi-
nomial distribution over the target word’s senses
rather than topics. Li et al. (2010) design sev-
eral systems that use latent topics to find a most
likely sense based on the sense paraphrases (ex-
tracted from WordNet) and context. Their WSD
models are unsupervised and outperform state-of-
art systems.
Our model borrows the local window idea from
word sense disambiguation community. In graph-
based WSD systems (Mihalcea, 2005; Sinha and
Mihalcea, 2007; Guo and Diab, 2010), a node is
created for each sense. Two nodes will be con-
nected if their distance is less than a predefined
value; the weight on the edge is a value returned
by sense similarity measures, then the PageR-
ank/Indegree algorithm is applied on this graph to
determine the appropriate senses.
</bodyText>
<sectionHeader confidence="0.99914" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999989">
We presented a novel model STM that combines
explicit semantic information and word distribu-
tion information in a unified topic model. STM
is able to capture topics of words more accurately
than traditional LDA topic models. In future work,
we plan to model the WordNet sense network. We
believe that WordNet senses are too fine-grained,
hence we plan to use clustered senses, instead of
</bodyText>
<page confidence="0.961667">
560
</page>
<bodyText confidence="0.957739">
current WN senses, in order to avail the model of
more generalization power.
</bodyText>
<sectionHeader confidence="0.999495" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9998605">
This research was funded by the Ofce of the Direc-
tor of National Intelligence (ODNI), Intelligence
Advanced Research Projects Activity (IARPA),
through the U.S. Army Research Lab. All state-
ments of fact, opinion or conclusions contained
herein are those of the authors and should not be
construed as representing the ofcial views or poli-
cies of IARPA, the ODNI or the U.S. Government.
</bodyText>
<sectionHeader confidence="0.999321" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999425441558441">
Satanjeev Banerjee and Ted Pedersen. 2003. Extended
gloss overlaps as a measure of semantic relatedness.
In Proceedings of the 18th International Joint Con-
ference on Artificial Intelligence, pages 805–810.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. Journal of Ma-
chine Learning Research, 3:993–1022.
Jordan Boyd-Graber and David M. Blei. 2007. Putop:
turning predominant senses into a topic model for
word sense disambiguation. In Proceedings of the
4th International Workshop on Semantic Evalua-
tions, pages 277–281.
Jordan Boyd-Graber, David M. Blei, and Xiaojin Zhu.
2007. A topic model for word sense disambiguation.
In Proceedings of the 2007 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 1024–1033.
Samuel Brody and Mirella Lapata. 2009. Bayesian
word sense induction. In Proceedings of the 12th
Conference of the European Chapter of the ACL,
pages 103–111.
Jun Fu Cai, Wee Sun Lee, and Yee Whye Teh. 2007.
Improving word sense disambiguation using topic
features. In Proceedings of 2007 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 1015–1023.
Chaitanya Chemudugunta, Padhraic Smyth, and Mark
Steyvers. 2008. Combining concept hierarchies
and statistical topic models. In Proceedings of the
17th ACM conference on Information and knowl-
edge management, pages 1469–1470.
Hal Daume. 2009. Markov random topic fields. In
Proceedings of the ACL-IJCNLP Conference, pages
293–296.
Laura Dietz, Steffen Bickel, and Tobias Scheffer. 2007.
Unsupervised prediction of citation influence. In
Proceedings of the 24th international conference on
Machine learning, pages 233–240.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101:5228–5235.
Thomas L. Griffiths, Mark Steyvers, David M. Blei,
and Joshua B. Tenenbaum. 2005. Integrating top-
ics and syntax. In Advances in Neural Information
Processing Systems.
Weiwei Guo and Mona Diab. 2010. Combining or-
thogonal monolingual and multilingual sources of
evidence for all words wsd. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 1542–1551.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11:10–18.
Linlin Li, Benjamin Roth, and Caroline Sporleder.
2010. Topic models for word sense disambiguation
and token-based idiom detection. In Proceedings of
the 48th Annual Meeting of the Association for Com-
putational Linguistics, pages 1138–1147.
Qiaozhu Mei, Deng Cai, Duo Zhang, and Chengxiang
Zhai. 2008. Topic modeling with network regu-
larization. In Proceedings of the 17th international
conference on World Wide Web, pages 101–110.
Rada Mihalcea. 2005. Unsupervised large-vocabulary
word sense disambiguation with graph-based algo-
rithms for sequence data labeling. In Proceedings
of the Joint Conference on Human Language Tech-
nology and Empirical Methods in Natural Language
Processing, pages 411–418.
Sameer S. Pradhan, Edward Loper, Dmitriy Dligach,
and Martha Palmer. 2007. Semeval-2007 task 17:
English lexical sample, srl and all words. In Pro-
ceedings of the 4th International Workshop on Se-
</reference>
<page confidence="0.527674">
561
</page>
<reference confidence="0.996172571428572">
mantic Evaluations, pages 87–92. ACL.
Ravi Sinha and Rada Mihalcea. 2007. Unsupervised
graph-based word sense disambiguation using mea-
sures of word semantic similarity. In Proceedings
of the IEEE International Conference on Semantic
Computing, pages 363–369.
Benjamin Snyder and Martha Palmer. 2004. The en-
glish all-words task. In Senseval-3: Third Interna-
tional Workshop on the Evaluation of Systems for the
Semantic Analysis of Text, pages 41–43. ACL.
Ivan Titov and Ryan McDonald. 2008. Modeling
online reviews with multi-grain topic models. In
Proceedings of the 17th international conference on
World Wide Web, pages 111–120.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.986931">
<title confidence="0.9971745">Semantic Topic Models: Combining Word Distributional Statistics Dictionary Definitions</title>
<author confidence="0.999936">Weiwei Guo Mona Diab</author>
<affiliation confidence="0.9999855">Department of Computer Science, Center for Computational Learning Systems, Columbia University, Columbia University,</affiliation>
<email confidence="0.999878">weiwei@cs.columbia.edumdiab@ccls.columbia.edu</email>
<abstract confidence="0.99959647368421">In this paper, we propose a novel topic model based on incorporating dictionary definitions. Traditional topic models treat words as surface strings without assuming predefined knowledge about word meaning. They infer topics only by observing surface word co-occurrence. However, the co-occurred words may not be semantically related in a manner that is relevant for topic coherence. Exploiting dictionary definitions explicitly in our model yields a better understanding of word semantics leading to better text modeling. We exploit WordNet as a lexical resource for sense definitions. We show that explicitly modeling word definitions helps improve performance significantly over the baseline for a text categorization task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>Extended gloss overlaps as a measure of semantic relatedness.</title>
<date>2003</date>
<booktitle>In Proceedings of the 18th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>805--810</pages>
<contexts>
<context position="31449" citStr="Banerjee and Pedersen, 2003" startWordPosition="5444" endWordPosition="5447"> Ed TF-IDF(wi,d) # of wi,d where wi,d E pos. At last, we notice that the most frequent sense baseline performs much better than our models. This is understandable since: (1) most frequent sense baseline can be treated as a supervised method in the sense that the sense frequency is calculated based on the sense choice as present in sense annotated data; (2) our model is not designed for WSD, therefore it discards a lot of information when choosing the sense: in our model, the choice of a sense si is only dependent on two facts: the corresponding topic zi and word wi, while in (Li et al., 2010; Banerjee and Pedersen, 2003), they consider all the senses and words in the context words. 5 Related work Various topic models have been developed for many applications. Recently there is a trend of modeling document dependency (Dietz et al., 2007; Mei et al., 2008; Daume, 2009). However, topics are only inferred based on word cooccurrence, while word semantics are ignored. Boyd-Graber et al. (2007) are the first to integrate semantics into the topic model framework. They propose a topic model based on WordNet noun hierarchy for WSD. A word is assumed to be generated by first sampling a topic, then choosing a path from t</context>
</contexts>
<marker>Banerjee, Pedersen, 2003</marker>
<rawString>Satanjeev Banerjee and Ted Pedersen. 2003. Extended gloss overlaps as a measure of semantic relatedness. In Proceedings of the 18th International Joint Conference on Artificial Intelligence, pages 805–810.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="1080" citStr="Blei et al., 2003" startWordPosition="144" endWordPosition="147">knowledge about word meaning. They infer topics only by observing surface word co-occurrence. However, the co-occurred words may not be semantically related in a manner that is relevant for topic coherence. Exploiting dictionary definitions explicitly in our model yields a better understanding of word semantics leading to better text modeling. We exploit WordNet as a lexical resource for sense definitions. We show that explicitly modeling word definitions helps improve performance significantly over the baseline for a text categorization task. 1 Introduction Latent Dirichlet Allocation (LDA) (Blei et al., 2003) serves as a data-driven framework in modeling text corpora. The statistical model allows variable extensions to integrate linguistic features such as syntax (Griffiths et al., 2005), and has been applied in many areas. In LDA, there are two factors which determine the topic of a word: the topic distribution of the document, and the probability of a topic to emit this word. This information is learned in an unsupervised manner to maximize the likelihood of the corpus. However, this data-driven approach has some limitations. If a word is not observed frequently enough in the corpus, then it is </context>
<context position="4116" citStr="Blei et al., 2003" startWordPosition="644" endWordPosition="647">results on text categorization are presented. Moreover, we analyze both qualitatively and quantitatively the contribution of modeling definitions (by teasing out the contribution of explicit sense modeling in a word sense disambiguation task). Related work is introduced in Section 5. We conclude in Section 6 by discussing some possible future directions. Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 552–561, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics Figure 1: (a) LDA: Latent Dirichlet Allocation (Blei et al., 2003). (b) STM: Semantic topic model. The dashed arrows indicate the distributions (φ and q) and nodes (z) are not influenced by the values of pointed nodes. 2 Semantic Topic Model 2.1 Latent Dirichlet Allocation We briefly introduce LDA where Collapsed Gibbs Sampling (Griffiths and Steyvers, 2004) is used for inference. In figure 1a, given a corpus with D documents, LDA will summarize each document as a normalized T-dimension topic mixture θ. Topic mixture θ is drawn from a Dirichlet distribution Dir(α) with a symmetric prior α. φ contains T multinomial distribution, each representing the probabil</context>
<context position="17606" citStr="Blei et al., 2003" startWordPosition="3075" endWordPosition="3078">s is listed in column sense types. The DOCs size for WordNet is the total number of senses defined in WordNet. Experiments: We design two tasks to test our models: (1) text categorization task for evaluating the quality of values of topic nodes, and (2) a WSD task for evaluating the quality of the values of the sense nodes, mainly as a diagnostic tool targeting the specific aspect of sense definitions incorporation and distinguish that component’s contribution to text categorization performance. We compare the performance of four topic models. (a) LDA: the traditional topic model proposed in (Blei et al., 2003) except that it uses Gibbs Sampling for inference. (b) LDA+def: is LDA with sense definitions. However they are not explicitly modeled; rather they are treated as documents and used as augmented data. (c) STM0: the topic model with an additional explicit sense node in the model, but we do not model the sense definitions. And finally (d) STMn is the full model with definitions explicitly modeled. In this setting n is the -y value. We experiment with different -y values in the STM models, and investigate the semantic scope of words/senses by choosing different window size E. We report mean and s</context>
<context position="23603" citStr="Blei et al., 2003" startWordPosition="4139" endWordPosition="4142">ve Bayes . Finally the Table illustrates the results obtained using our proposed models STM0 (&apos;y=0) and STM2 (&apos;y = 2). It is worth noting that R20 (compared to NYT) is a harder condition for topic models. This is because fewer words (10000 distinct words versus 19000 in NYT) are frequently used in a large training set (8600 documents versus 800 in NYT), making the surface word feature space no longer as sparse as in the NYT or Brown corpus, which implies simply using surface words without considering the words distributional statistics – topic modeling – is good enough for classification. In (Blei et al., 2003) figure 10b they also show worse text categorization results over the SVM baseline when more than 15% of the training labels of Reuters are available for the SVM classifiers, indicating that LDA is less necessary with large training data. In our investigation, we report results on SVM classifiers trained on the whole Reuters training set. In our experiments, LDA fails to correctly classify nearly 10% of the Reuters documents compared to the WEKA baseline, however STM2 can still achieve significantly better accuracy (+4%) in the SVM classification condition. Table 2 illustrates that despite the</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordan Boyd-Graber</author>
<author>David M Blei</author>
</authors>
<title>Putop: turning predominant senses into a topic model for word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations,</booktitle>
<pages>277--281</pages>
<contexts>
<context position="6026" citStr="Boyd-Graber and Blei, 2007" startWordPosition="986" endWordPosition="989">pics in a document construct the topic mixture θ. 2.2 Applying Word Sense Disambiguation Techniques We add a sense node between the topic node and the word node based on two linguistic observations: a) Polysemy: many words have more than one meaning. A topic is more directly relevant to a word meaning (sense) than to a word due to polysemy; b) Synonymy: different words may share the same sense. WordNet explicitly models synonymy by linking synonyms to the same sense. In WordNet, each sense has an associated definition. It is worth noting that we model the sense-word relation differently from (Boyd-Graber and Blei, 2007), where in their model words are generated from topics, then senses are generated from words. In our model, we assume that during the generative process, the author picks a concept relevant to the topic, then thinks of a best word that represents that concept. Hence the word choice is dependent on the relatedness of the sense and its fit to the document context. In standard topic models, the topic of a word is sampled from the document level topic mixture θ. The underlying assumption is that all words in a document constitute the context of the target word. However, it is not the case in real </context>
</contexts>
<marker>Boyd-Graber, Blei, 2007</marker>
<rawString>Jordan Boyd-Graber and David M. Blei. 2007. Putop: turning predominant senses into a topic model for word sense disambiguation. In Proceedings of the 4th International Workshop on Semantic Evaluations, pages 277–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordan Boyd-Graber</author>
<author>David M Blei</author>
<author>Xiaojin Zhu</author>
</authors>
<title>A topic model for word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>1024--1033</pages>
<contexts>
<context position="28103" citStr="Boyd-Graber et al., 2007" startWordPosition="4860" endWordPosition="4863">n modeling) in WSD task is an indicator of the impact of using sense definitions in the text categorization task. WSD Data: We choose the all-words WSD task in which an unsupervised WSD system is required to disambiguate all the content words in documents. Our models are evaluated against the SemCor dataset. We prefer SemCor to all-words datasets available in Senseval-3 (Snyder and Palmer, 2004) or SemEval-2007 (Pradhan et al., 2007), since it includes many more documents than either set (350 versus 3) and therefore allowing more reliable results. Moreover, SemCor is also the dataset used in (Boyd-Graber et al., 2007), where a WordNet based topic model for WSD is introduced. The 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 accuracy% 100 90 80 70 60 50 STM0 STM2 Total Noun Adjective Adverb Verb sense annotated words 225992 86996 31729 18947 88320 polysemous words 187871 70529 21989 11498 83855 TF-IDF - 0.422 0.300 0.153 0.182 Table 3: Statistics of SemCor per POS statistics of SemCor is listed in table 3. We use hyperparameters tuned from the text categorization task: αd=0.1, αs=0.01, β=0.01, δ=1, T=50, and try different values of c E 110, 20, 40} and -y E 10, 2,10}. The Brown corpus and WordNet definitions cor</context>
<context position="29668" citStr="Boyd-Graber et al., 2007" startWordPosition="5142" endWordPosition="5145">We can see that modeling definitions (STM2 and STM10) improves performance significantly over STM0’s across the board per POS and overall. The fact that STMn picks more correct senses helps explain why STMn classifies more documents correctly than STM0. Also it is interesting to see that unlike in the text categorization task, larger values of -y generate better WSD results. However, the window size c, does not make a significant difference, yet we note that c=10 is still the optimal value, similar to our observation in the text categorization task. STM10 achieves similar results as in LDAWN (Boyd-Graber et al., 2007) which was specifically designed for WSD. LDAWN needs a fine grained hypernym hierarchy to perform WSD, hence they can only disambiguate nouns. They report different performances under various parameter setting. We cite their best performance of 38% accuracy on nouns as a comparison point to our best performance for nouns of 38.5%. An interesting feature of STM10 is that it performs much better in nouns than adverbs and verbs, compared to a random baseline in Table 4. This is understandable since topic information content is mostly borne by nouns and adjectives, while adverbs and verbs tend to</context>
<context position="31823" citStr="Boyd-Graber et al. (2007)" startWordPosition="5506" endWordPosition="5509">esigned for WSD, therefore it discards a lot of information when choosing the sense: in our model, the choice of a sense si is only dependent on two facts: the corresponding topic zi and word wi, while in (Li et al., 2010; Banerjee and Pedersen, 2003), they consider all the senses and words in the context words. 5 Related work Various topic models have been developed for many applications. Recently there is a trend of modeling document dependency (Dietz et al., 2007; Mei et al., 2008; Daume, 2009). However, topics are only inferred based on word cooccurrence, while word semantics are ignored. Boyd-Graber et al. (2007) are the first to integrate semantics into the topic model framework. They propose a topic model based on WordNet noun hierarchy for WSD. A word is assumed to be generated by first sampling a topic, then choosing a path from the root node of hierarchy to a sense node corresponding to that word. However, they only focus on WSD. They do not exploit word definitions, neither do they report results on text categorization. Chemudugunta et al. (2008) also incorporate a sense hierarchy into a topic model. In their framework, a word may be directly generated from a topic (as in standard topic models),</context>
</contexts>
<marker>Boyd-Graber, Blei, Zhu, 2007</marker>
<rawString>Jordan Boyd-Graber, David M. Blei, and Xiaojin Zhu. 2007. A topic model for word sense disambiguation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 1024–1033.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel Brody</author>
<author>Mirella Lapata</author>
</authors>
<title>Bayesian word sense induction.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the ACL,</booktitle>
<pages>103--111</pages>
<contexts>
<context position="33511" citStr="Brody and Lapata (2009)" startWordPosition="5791" endWordPosition="5794">8.5±2.3 34.7±0.8 34.0±3.3 18.4±1.2 e = 20 27.7±1.0 36.8±2.2 34.5±0.7 33.0±3.1 17.6±0.7 e = 40 28.1±1.5 38.4±3.1 34.0±1.0 35.1±5.4 17.0±0.9 Table 4: Disambiguation results per POS on polysemous words. generated by choosing a sense path in the hierarchy. Note that no topic information is on the sense path. If a word is generated from the hierarchy, then it is not assigned a topic. Their models based on different dictionaries improve perplexity. Recently, several systems have been proposed to apply topic models to WSD. Cai et al. (2007) incorporate topic features into a supervised WSD framework. Brody and Lapata (2009) place the sense induction in a Baysian framework by assuming each context word is generated from the target word’s senses, and a context is modeled as a multinomial distribution over the target word’s senses rather than topics. Li et al. (2010) design several systems that use latent topics to find a most likely sense based on the sense paraphrases (extracted from WordNet) and context. Their WSD models are unsupervised and outperform state-ofart systems. Our model borrows the local window idea from word sense disambiguation community. In graphbased WSD systems (Mihalcea, 2005; Sinha and Mihalc</context>
</contexts>
<marker>Brody, Lapata, 2009</marker>
<rawString>Samuel Brody and Mirella Lapata. 2009. Bayesian word sense induction. In Proceedings of the 12th Conference of the European Chapter of the ACL, pages 103–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Fu Cai</author>
<author>Wee Sun Lee</author>
<author>Yee Whye Teh</author>
</authors>
<title>Improving word sense disambiguation using topic features.</title>
<date>2007</date>
<booktitle>In Proceedings of 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1015--1023</pages>
<contexts>
<context position="33427" citStr="Cai et al. (2007)" startWordPosition="5779" endWordPosition="5782">.7 e = 40 26.1±1.3 32.5±3.9 33.6±0.9 34.2±3.4 17.5±1.4 STM10 e = 10 28.8±1.1 38.5±2.3 34.7±0.8 34.0±3.3 18.4±1.2 e = 20 27.7±1.0 36.8±2.2 34.5±0.7 33.0±3.1 17.6±0.7 e = 40 28.1±1.5 38.4±3.1 34.0±1.0 35.1±5.4 17.0±0.9 Table 4: Disambiguation results per POS on polysemous words. generated by choosing a sense path in the hierarchy. Note that no topic information is on the sense path. If a word is generated from the hierarchy, then it is not assigned a topic. Their models based on different dictionaries improve perplexity. Recently, several systems have been proposed to apply topic models to WSD. Cai et al. (2007) incorporate topic features into a supervised WSD framework. Brody and Lapata (2009) place the sense induction in a Baysian framework by assuming each context word is generated from the target word’s senses, and a context is modeled as a multinomial distribution over the target word’s senses rather than topics. Li et al. (2010) design several systems that use latent topics to find a most likely sense based on the sense paraphrases (extracted from WordNet) and context. Their WSD models are unsupervised and outperform state-ofart systems. Our model borrows the local window idea from word sense d</context>
</contexts>
<marker>Cai, Lee, Teh, 2007</marker>
<rawString>Jun Fu Cai, Wee Sun Lee, and Yee Whye Teh. 2007. Improving word sense disambiguation using topic features. In Proceedings of 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1015–1023.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chaitanya Chemudugunta</author>
<author>Padhraic Smyth</author>
<author>Mark Steyvers</author>
</authors>
<title>Combining concept hierarchies and statistical topic models.</title>
<date>2008</date>
<booktitle>In Proceedings of the 17th ACM conference on Information and knowledge management,</booktitle>
<pages>1469--1470</pages>
<contexts>
<context position="32271" citStr="Chemudugunta et al. (2008)" startWordPosition="5586" endWordPosition="5589">ncy (Dietz et al., 2007; Mei et al., 2008; Daume, 2009). However, topics are only inferred based on word cooccurrence, while word semantics are ignored. Boyd-Graber et al. (2007) are the first to integrate semantics into the topic model framework. They propose a topic model based on WordNet noun hierarchy for WSD. A word is assumed to be generated by first sampling a topic, then choosing a path from the root node of hierarchy to a sense node corresponding to that word. However, they only focus on WSD. They do not exploit word definitions, neither do they report results on text categorization. Chemudugunta et al. (2008) also incorporate a sense hierarchy into a topic model. In their framework, a word may be directly generated from a topic (as in standard topic models), or it can be Total Noun Adjective Adverb Verb random 22.1 26.2 27.9 32.2 15.8 most frequent sense 64.7 74.7 77.5 74.0 59.6 STM0 e = 10 24.1±1.4 29.3±4.3 28.7±1.1 34.1±3.1 17.1±1.6 e = 20 24±1.3 30.2±3.3 29.1±1.4 34.9±3.1 15.9±0.7 e = 40 24±2.4 28.4±4.3 28.7±1.1 36.4±4.7 17.3±2.4 STM2 e = 10 27.5±1.1 36.1±3.8 34.0±1.2 33.4±1.8 17.8±1.4 e = 20 25.7±1.3 32.0±4.2 33.5±0.7 34.2±3.4 17.3±0.7 e = 40 26.1±1.3 32.5±3.9 33.6±0.9 34.2±3.4 17.5±1.4 STM10 </context>
</contexts>
<marker>Chemudugunta, Smyth, Steyvers, 2008</marker>
<rawString>Chaitanya Chemudugunta, Padhraic Smyth, and Mark Steyvers. 2008. Combining concept hierarchies and statistical topic models. In Proceedings of the 17th ACM conference on Information and knowledge management, pages 1469–1470.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daume</author>
</authors>
<title>Markov random topic fields.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP Conference,</booktitle>
<pages>293--296</pages>
<contexts>
<context position="31700" citStr="Daume, 2009" startWordPosition="5488" endWordPosition="5489">e frequency is calculated based on the sense choice as present in sense annotated data; (2) our model is not designed for WSD, therefore it discards a lot of information when choosing the sense: in our model, the choice of a sense si is only dependent on two facts: the corresponding topic zi and word wi, while in (Li et al., 2010; Banerjee and Pedersen, 2003), they consider all the senses and words in the context words. 5 Related work Various topic models have been developed for many applications. Recently there is a trend of modeling document dependency (Dietz et al., 2007; Mei et al., 2008; Daume, 2009). However, topics are only inferred based on word cooccurrence, while word semantics are ignored. Boyd-Graber et al. (2007) are the first to integrate semantics into the topic model framework. They propose a topic model based on WordNet noun hierarchy for WSD. A word is assumed to be generated by first sampling a topic, then choosing a path from the root node of hierarchy to a sense node corresponding to that word. However, they only focus on WSD. They do not exploit word definitions, neither do they report results on text categorization. Chemudugunta et al. (2008) also incorporate a sense hie</context>
</contexts>
<marker>Daume, 2009</marker>
<rawString>Hal Daume. 2009. Markov random topic fields. In Proceedings of the ACL-IJCNLP Conference, pages 293–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Dietz</author>
<author>Steffen Bickel</author>
<author>Tobias Scheffer</author>
</authors>
<title>Unsupervised prediction of citation influence.</title>
<date>2007</date>
<booktitle>In Proceedings of the 24th international conference on Machine learning,</booktitle>
<pages>233--240</pages>
<contexts>
<context position="31668" citStr="Dietz et al., 2007" startWordPosition="5480" endWordPosition="5483">ised method in the sense that the sense frequency is calculated based on the sense choice as present in sense annotated data; (2) our model is not designed for WSD, therefore it discards a lot of information when choosing the sense: in our model, the choice of a sense si is only dependent on two facts: the corresponding topic zi and word wi, while in (Li et al., 2010; Banerjee and Pedersen, 2003), they consider all the senses and words in the context words. 5 Related work Various topic models have been developed for many applications. Recently there is a trend of modeling document dependency (Dietz et al., 2007; Mei et al., 2008; Daume, 2009). However, topics are only inferred based on word cooccurrence, while word semantics are ignored. Boyd-Graber et al. (2007) are the first to integrate semantics into the topic model framework. They propose a topic model based on WordNet noun hierarchy for WSD. A word is assumed to be generated by first sampling a topic, then choosing a path from the root node of hierarchy to a sense node corresponding to that word. However, they only focus on WSD. They do not exploit word definitions, neither do they report results on text categorization. Chemudugunta et al. (20</context>
</contexts>
<marker>Dietz, Bickel, Scheffer, 2007</marker>
<rawString>Laura Dietz, Steffen Bickel, and Tobias Scheffer. 2007. Unsupervised prediction of citation influence. In Proceedings of the 24th international conference on Machine learning, pages 233–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="2729" citStr="Fellbaum, 1998" startWordPosition="423" endWordPosition="424">ot find any of its meanings indicating the politics topic, yet there is ample evidence for the chemical topic. Accordingly, we hypothesize that if we know the semantics of words in advance, we can get a better indication of their topics. Therefore, in this paper, we test our hypothesis by exploring the integration of word semantics explicitly in the topic modeling framework. In order to incorporate word semantics from dictionaries, we recognize the need to model sense-topic distribution rather than word-topic distribution, since dictionaries are constructed at the sense level. We use WordNet (Fellbaum, 1998) as our lexical resource of choice. The notion of a sense in WordNet goes beyond a typical word sense in a traditional dictionary since a WordNet sense links senses of different words that have similar meanings. Accordingly, the sense for the first verbal entry for buy and for purchase will have the same sense id (and same definition) in WordNet, while they could have different meaning definitions in a traditional dictionary such as the Merriam Webster Dictionary or LDOCE. In our model, a topic will first emit a WordNet sense, then the sense will generate a word. This is inspired by the intuit</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>Proceedings of the National Academy of Sciences,</booktitle>
<pages>101--5228</pages>
<contexts>
<context position="4410" citStr="Griffiths and Steyvers, 2004" startWordPosition="691" endWordPosition="694">. We conclude in Section 6 by discussing some possible future directions. Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 552–561, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics Figure 1: (a) LDA: Latent Dirichlet Allocation (Blei et al., 2003). (b) STM: Semantic topic model. The dashed arrows indicate the distributions (φ and q) and nodes (z) are not influenced by the values of pointed nodes. 2 Semantic Topic Model 2.1 Latent Dirichlet Allocation We briefly introduce LDA where Collapsed Gibbs Sampling (Griffiths and Steyvers, 2004) is used for inference. In figure 1a, given a corpus with D documents, LDA will summarize each document as a normalized T-dimension topic mixture θ. Topic mixture θ is drawn from a Dirichlet distribution Dir(α) with a symmetric prior α. φ contains T multinomial distribution, each representing the probability of a topic z generating word w p(wjz). φ is drawn from a Dirichlet distribution Dir(β) with prior β. In Collapsed Gibbs Sampling, the distribution of a topic for the word wi = w based on values of other data is computed as: nw−i,z + O (1) n−i,z + WO In this equation, n(d) −i,z is a count o</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>Thomas L. Griffiths and Mark Steyvers. 2004. Finding scientific topics. Proceedings of the National Academy of Sciences, 101:5228–5235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
<author>David M Blei</author>
<author>Joshua B Tenenbaum</author>
</authors>
<title>Integrating topics and syntax.</title>
<date>2005</date>
<booktitle>In Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="1262" citStr="Griffiths et al., 2005" startWordPosition="172" endWordPosition="175">relevant for topic coherence. Exploiting dictionary definitions explicitly in our model yields a better understanding of word semantics leading to better text modeling. We exploit WordNet as a lexical resource for sense definitions. We show that explicitly modeling word definitions helps improve performance significantly over the baseline for a text categorization task. 1 Introduction Latent Dirichlet Allocation (LDA) (Blei et al., 2003) serves as a data-driven framework in modeling text corpora. The statistical model allows variable extensions to integrate linguistic features such as syntax (Griffiths et al., 2005), and has been applied in many areas. In LDA, there are two factors which determine the topic of a word: the topic distribution of the document, and the probability of a topic to emit this word. This information is learned in an unsupervised manner to maximize the likelihood of the corpus. However, this data-driven approach has some limitations. If a word is not observed frequently enough in the corpus, then it is likely to be assigned the dominant topic in this document. For example, the word grease (a thick fatty oil) in a political domain document should be assigned the topic chemicals. How</context>
<context position="19161" citStr="Griffiths et al., 2005" startWordPosition="3353" endWordPosition="3356">, we only use zjj, sjj, wjj to estimate φ, η, so that the impact of documents is fixed. This makes more sense, in that after the approximation in section 3.1, there is no need to use {zij, sij, |i =� j} (they have the same values as zjj, sjj). 4.1 Text Categorization We believe our model can generate more “correct” topics by looking into dictionaries. In topic models, each word is generalized as a topic and each document is summarized as the topic mixture θ, hence it is natural to evaluate the quality of inferred topics in a text categorization task. We follow the classification framework in (Griffiths et al., 2005): first run topic models on each dataset individually without knowing label information to achieve document level topic mixtures, then we employ Naive Bayes and SVM (both implemented in the WEKA Toolkit (Hall et al., 2009)) to perform classification on the topic mixtures. For all document, the features are the percentage of topics. Similar to (Griffiths et al., 2005), we assess inferred topics by the classification accuracy of 10- fold cross validation on each dataset. We evaluate our models on three datasets in the cross validation manner: The Brown corpus which comprises 500 documents groupe</context>
</contexts>
<marker>Griffiths, Steyvers, Blei, Tenenbaum, 2005</marker>
<rawString>Thomas L. Griffiths, Mark Steyvers, David M. Blei, and Joshua B. Tenenbaum. 2005. Integrating topics and syntax. In Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Guo</author>
<author>Mona Diab</author>
</authors>
<title>Combining orthogonal monolingual and multilingual sources of evidence for all words wsd.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1542--1551</pages>
<contexts>
<context position="7305" citStr="Guo and Diab, 2010" startWordPosition="1209" endWordPosition="1212">al topic mixtures can only extract global topics in online reviews (e.g., Creative Labs MP3 players and iPods) and ignores local topics (product features such as portability and battery). They design the Multi-grain LDA where the local topic of a word is only determined by topics of surrounding sentences. In word sense disambiguation (WSD), an even narrower context is taken into consideration, for instance in graph based WSD models (Mihalcea, 2005), the choice of a sense for a word only depends on a local window whose size equals the length of the sentence. Later in (Sinha and Mihalcea, 2007; Guo and Diab, 2010; Li et al., 2010), people use a fixed window size containing around 12 neighbor words for WSD. Accordingly, we adopt the WSD inspired local window strategy in our model. However, we do d T S d d P(zi = z|z−i, w) ∝ n(d) −i,z + α × n−i + Tα (d) not employ the complicated schema in (Titov and McDonald, 2008). We simply hypothesize that the surrounding E words are semantically related to the considered word, and they construct a local sliding window for that target word. For a document d with Nd words, we represent it as Nd local windows – a window is created for each word. The model is illustrat</context>
<context position="22453" citStr="Guo and Diab, 2010" startWordPosition="3942" endWordPosition="3945">n an augmented data manner does not help. Comparing SMT0 with LDA in the same c values, we find that explicitly modeling the sense node in the model greatly improves the classification results. The reason may be that words in LDA are independent isolated strings, while in STM0 they are connected by senses. STM2 prefers smaller window sizes (c less than 40). That means two words with a distance larger than 40 are not necessarily semantically related or share the same topic. This c number also correlates with the optimal context window size of 12 reported in WSD tasks (Sinha and Mihalcea, 2007; Guo and Diab, 2010). Classification results: Table 2 shows the results of our models using best tuned parameters of c = 10, &apos;y = 2 on 3 datasets. We present three baselines in Table 2: (1) WEKA uses WEKA’s classifiers directly on bag-of-words without topic modeling. The values of features are simply term frequency. (2) WEKA+FS performs feature selection using information gain before applying classification. (3) LDA, is the traditional topic model. Note that Griffiths et al.’s (2005) implementation of 557 LDA achieve 51% on Brown corpus using Naive Bayes . Finally the Table illustrates the results obtained using </context>
<context position="34140" citStr="Guo and Diab, 2010" startWordPosition="5896" endWordPosition="5899"> sense induction in a Baysian framework by assuming each context word is generated from the target word’s senses, and a context is modeled as a multinomial distribution over the target word’s senses rather than topics. Li et al. (2010) design several systems that use latent topics to find a most likely sense based on the sense paraphrases (extracted from WordNet) and context. Their WSD models are unsupervised and outperform state-ofart systems. Our model borrows the local window idea from word sense disambiguation community. In graphbased WSD systems (Mihalcea, 2005; Sinha and Mihalcea, 2007; Guo and Diab, 2010), a node is created for each sense. Two nodes will be connected if their distance is less than a predefined value; the weight on the edge is a value returned by sense similarity measures, then the PageRank/Indegree algorithm is applied on this graph to determine the appropriate senses. 6 Conclusion and Future Work We presented a novel model STM that combines explicit semantic information and word distribution information in a unified topic model. STM is able to capture topics of words more accurately than traditional LDA topic models. In future work, we plan to model the WordNet sense network.</context>
</contexts>
<marker>Guo, Diab, 2010</marker>
<rawString>Weiwei Guo and Mona Diab. 2010. Combining orthogonal monolingual and multilingual sources of evidence for all words wsd. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1542–1551.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The weka data mining software: an update.</title>
<date>2009</date>
<journal>SIGKDD Explor. Newsl.,</journal>
<pages>11--10</pages>
<contexts>
<context position="19383" citStr="Hall et al., 2009" startWordPosition="3388" endWordPosition="3391">es as zjj, sjj). 4.1 Text Categorization We believe our model can generate more “correct” topics by looking into dictionaries. In topic models, each word is generalized as a topic and each document is summarized as the topic mixture θ, hence it is natural to evaluate the quality of inferred topics in a text categorization task. We follow the classification framework in (Griffiths et al., 2005): first run topic models on each dataset individually without knowing label information to achieve document level topic mixtures, then we employ Naive Bayes and SVM (both implemented in the WEKA Toolkit (Hall et al., 2009)) to perform classification on the topic mixtures. For all document, the features are the percentage of topics. Similar to (Griffiths et al., 2005), we assess inferred topics by the classification accuracy of 10- fold cross validation on each dataset. We evaluate our models on three datasets in the cross validation manner: The Brown corpus which comprises 500 documents grouped into 15 categories (same set used in (Griffiths et al., 2005)); NYT comprising 800 documents grouped into the 16 most frequent label categories; Reuters (R20) comprising 8600 documents labeled with the most frequent 20 c</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The weka data mining software: an update. SIGKDD Explor. Newsl., 11:10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Linlin Li</author>
<author>Benjamin Roth</author>
<author>Caroline Sporleder</author>
</authors>
<title>Topic models for word sense disambiguation and token-based idiom detection.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1138--1147</pages>
<contexts>
<context position="7323" citStr="Li et al., 2010" startWordPosition="1213" endWordPosition="1216">n only extract global topics in online reviews (e.g., Creative Labs MP3 players and iPods) and ignores local topics (product features such as portability and battery). They design the Multi-grain LDA where the local topic of a word is only determined by topics of surrounding sentences. In word sense disambiguation (WSD), an even narrower context is taken into consideration, for instance in graph based WSD models (Mihalcea, 2005), the choice of a sense for a word only depends on a local window whose size equals the length of the sentence. Later in (Sinha and Mihalcea, 2007; Guo and Diab, 2010; Li et al., 2010), people use a fixed window size containing around 12 neighbor words for WSD. Accordingly, we adopt the WSD inspired local window strategy in our model. However, we do d T S d d P(zi = z|z−i, w) ∝ n(d) −i,z + α × n−i + Tα (d) not employ the complicated schema in (Titov and McDonald, 2008). We simply hypothesize that the surrounding E words are semantically related to the considered word, and they construct a local sliding window for that target word. For a document d with Nd words, we represent it as Nd local windows – a window is created for each word. The model is illustrated in the left rec</context>
<context position="31419" citStr="Li et al., 2010" startWordPosition="5440" endWordPosition="5443">� TF-IDF(pos) = i Ed TF-IDF(wi,d) # of wi,d where wi,d E pos. At last, we notice that the most frequent sense baseline performs much better than our models. This is understandable since: (1) most frequent sense baseline can be treated as a supervised method in the sense that the sense frequency is calculated based on the sense choice as present in sense annotated data; (2) our model is not designed for WSD, therefore it discards a lot of information when choosing the sense: in our model, the choice of a sense si is only dependent on two facts: the corresponding topic zi and word wi, while in (Li et al., 2010; Banerjee and Pedersen, 2003), they consider all the senses and words in the context words. 5 Related work Various topic models have been developed for many applications. Recently there is a trend of modeling document dependency (Dietz et al., 2007; Mei et al., 2008; Daume, 2009). However, topics are only inferred based on word cooccurrence, while word semantics are ignored. Boyd-Graber et al. (2007) are the first to integrate semantics into the topic model framework. They propose a topic model based on WordNet noun hierarchy for WSD. A word is assumed to be generated by first sampling a topi</context>
<context position="33756" citStr="Li et al. (2010)" startWordPosition="5834" endWordPosition="5837"> Note that no topic information is on the sense path. If a word is generated from the hierarchy, then it is not assigned a topic. Their models based on different dictionaries improve perplexity. Recently, several systems have been proposed to apply topic models to WSD. Cai et al. (2007) incorporate topic features into a supervised WSD framework. Brody and Lapata (2009) place the sense induction in a Baysian framework by assuming each context word is generated from the target word’s senses, and a context is modeled as a multinomial distribution over the target word’s senses rather than topics. Li et al. (2010) design several systems that use latent topics to find a most likely sense based on the sense paraphrases (extracted from WordNet) and context. Their WSD models are unsupervised and outperform state-ofart systems. Our model borrows the local window idea from word sense disambiguation community. In graphbased WSD systems (Mihalcea, 2005; Sinha and Mihalcea, 2007; Guo and Diab, 2010), a node is created for each sense. Two nodes will be connected if their distance is less than a predefined value; the weight on the edge is a value returned by sense similarity measures, then the PageRank/Indegree a</context>
</contexts>
<marker>Li, Roth, Sporleder, 2010</marker>
<rawString>Linlin Li, Benjamin Roth, and Caroline Sporleder. 2010. Topic models for word sense disambiguation and token-based idiom detection. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1138–1147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiaozhu Mei</author>
<author>Deng Cai</author>
<author>Duo Zhang</author>
<author>Chengxiang Zhai</author>
</authors>
<title>Topic modeling with network regularization.</title>
<date>2008</date>
<booktitle>In Proceedings of the 17th international conference on World Wide Web,</booktitle>
<pages>101--110</pages>
<contexts>
<context position="31686" citStr="Mei et al., 2008" startWordPosition="5484" endWordPosition="5487">ense that the sense frequency is calculated based on the sense choice as present in sense annotated data; (2) our model is not designed for WSD, therefore it discards a lot of information when choosing the sense: in our model, the choice of a sense si is only dependent on two facts: the corresponding topic zi and word wi, while in (Li et al., 2010; Banerjee and Pedersen, 2003), they consider all the senses and words in the context words. 5 Related work Various topic models have been developed for many applications. Recently there is a trend of modeling document dependency (Dietz et al., 2007; Mei et al., 2008; Daume, 2009). However, topics are only inferred based on word cooccurrence, while word semantics are ignored. Boyd-Graber et al. (2007) are the first to integrate semantics into the topic model framework. They propose a topic model based on WordNet noun hierarchy for WSD. A word is assumed to be generated by first sampling a topic, then choosing a path from the root node of hierarchy to a sense node corresponding to that word. However, they only focus on WSD. They do not exploit word definitions, neither do they report results on text categorization. Chemudugunta et al. (2008) also incorpora</context>
</contexts>
<marker>Mei, Cai, Zhang, Zhai, 2008</marker>
<rawString>Qiaozhu Mei, Deng Cai, Duo Zhang, and Chengxiang Zhai. 2008. Topic modeling with network regularization. In Proceedings of the 17th international conference on World Wide Web, pages 101–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
</authors>
<title>Unsupervised large-vocabulary word sense disambiguation with graph-based algorithms for sequence data labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of the Joint Conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>411--418</pages>
<contexts>
<context position="7139" citStr="Mihalcea, 2005" startWordPosition="1177" endWordPosition="1179">ll words in a document constitute the context of the target word. However, it is not the case in real world corpora. Titov and McDonald (2008) find that using global topic mixtures can only extract global topics in online reviews (e.g., Creative Labs MP3 players and iPods) and ignores local topics (product features such as portability and battery). They design the Multi-grain LDA where the local topic of a word is only determined by topics of surrounding sentences. In word sense disambiguation (WSD), an even narrower context is taken into consideration, for instance in graph based WSD models (Mihalcea, 2005), the choice of a sense for a word only depends on a local window whose size equals the length of the sentence. Later in (Sinha and Mihalcea, 2007; Guo and Diab, 2010; Li et al., 2010), people use a fixed window size containing around 12 neighbor words for WSD. Accordingly, we adopt the WSD inspired local window strategy in our model. However, we do d T S d d P(zi = z|z−i, w) ∝ n(d) −i,z + α × n−i + Tα (d) not employ the complicated schema in (Titov and McDonald, 2008). We simply hypothesize that the surrounding E words are semantically related to the considered word, and they construct a loca</context>
<context position="34093" citStr="Mihalcea, 2005" startWordPosition="5890" endWordPosition="5891">amework. Brody and Lapata (2009) place the sense induction in a Baysian framework by assuming each context word is generated from the target word’s senses, and a context is modeled as a multinomial distribution over the target word’s senses rather than topics. Li et al. (2010) design several systems that use latent topics to find a most likely sense based on the sense paraphrases (extracted from WordNet) and context. Their WSD models are unsupervised and outperform state-ofart systems. Our model borrows the local window idea from word sense disambiguation community. In graphbased WSD systems (Mihalcea, 2005; Sinha and Mihalcea, 2007; Guo and Diab, 2010), a node is created for each sense. Two nodes will be connected if their distance is less than a predefined value; the weight on the edge is a value returned by sense similarity measures, then the PageRank/Indegree algorithm is applied on this graph to determine the appropriate senses. 6 Conclusion and Future Work We presented a novel model STM that combines explicit semantic information and word distribution information in a unified topic model. STM is able to capture topics of words more accurately than traditional LDA topic models. In future wo</context>
</contexts>
<marker>Mihalcea, 2005</marker>
<rawString>Rada Mihalcea. 2005. Unsupervised large-vocabulary word sense disambiguation with graph-based algorithms for sequence data labeling. In Proceedings of the Joint Conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 411–418.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer S Pradhan</author>
<author>Edward Loper</author>
<author>Dmitriy Dligach</author>
<author>Martha Palmer</author>
</authors>
<title>Semeval-2007 task 17: English lexical sample, srl and all words.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations,</booktitle>
<pages>87--92</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="27915" citStr="Pradhan et al., 2007" startWordPosition="4829" endWordPosition="4832">irectly correlated with the choice of a correct topic in our framework. Accordingly, a relative improvement of STMn over STM0 (where the only difference is the explicit sense definition modeling) in WSD task is an indicator of the impact of using sense definitions in the text categorization task. WSD Data: We choose the all-words WSD task in which an unsupervised WSD system is required to disambiguate all the content words in documents. Our models are evaluated against the SemCor dataset. We prefer SemCor to all-words datasets available in Senseval-3 (Snyder and Palmer, 2004) or SemEval-2007 (Pradhan et al., 2007), since it includes many more documents than either set (350 versus 3) and therefore allowing more reliable results. Moreover, SemCor is also the dataset used in (Boyd-Graber et al., 2007), where a WordNet based topic model for WSD is introduced. The 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 accuracy% 100 90 80 70 60 50 STM0 STM2 Total Noun Adjective Adverb Verb sense annotated words 225992 86996 31729 18947 88320 polysemous words 187871 70529 21989 11498 83855 TF-IDF - 0.422 0.300 0.153 0.182 Table 3: Statistics of SemCor per POS statistics of SemCor is listed in table 3. We use hyperparameter</context>
</contexts>
<marker>Pradhan, Loper, Dligach, Palmer, 2007</marker>
<rawString>Sameer S. Pradhan, Edward Loper, Dmitriy Dligach, and Martha Palmer. 2007. Semeval-2007 task 17: English lexical sample, srl and all words. In Proceedings of the 4th International Workshop on Semantic Evaluations, pages 87–92. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ravi Sinha</author>
<author>Rada Mihalcea</author>
</authors>
<title>Unsupervised graph-based word sense disambiguation using measures of word semantic similarity.</title>
<date>2007</date>
<booktitle>In Proceedings of the IEEE International Conference on Semantic Computing,</booktitle>
<pages>363--369</pages>
<contexts>
<context position="7285" citStr="Sinha and Mihalcea, 2007" startWordPosition="1204" endWordPosition="1208">2008) find that using global topic mixtures can only extract global topics in online reviews (e.g., Creative Labs MP3 players and iPods) and ignores local topics (product features such as portability and battery). They design the Multi-grain LDA where the local topic of a word is only determined by topics of surrounding sentences. In word sense disambiguation (WSD), an even narrower context is taken into consideration, for instance in graph based WSD models (Mihalcea, 2005), the choice of a sense for a word only depends on a local window whose size equals the length of the sentence. Later in (Sinha and Mihalcea, 2007; Guo and Diab, 2010; Li et al., 2010), people use a fixed window size containing around 12 neighbor words for WSD. Accordingly, we adopt the WSD inspired local window strategy in our model. However, we do d T S d d P(zi = z|z−i, w) ∝ n(d) −i,z + α × n−i + Tα (d) not employ the complicated schema in (Titov and McDonald, 2008). We simply hypothesize that the surrounding E words are semantically related to the considered word, and they construct a local sliding window for that target word. For a document d with Nd words, we represent it as Nd local windows – a window is created for each word. Th</context>
<context position="22432" citStr="Sinha and Mihalcea, 2007" startWordPosition="3938" endWordPosition="3941">definitions as documents in an augmented data manner does not help. Comparing SMT0 with LDA in the same c values, we find that explicitly modeling the sense node in the model greatly improves the classification results. The reason may be that words in LDA are independent isolated strings, while in STM0 they are connected by senses. STM2 prefers smaller window sizes (c less than 40). That means two words with a distance larger than 40 are not necessarily semantically related or share the same topic. This c number also correlates with the optimal context window size of 12 reported in WSD tasks (Sinha and Mihalcea, 2007; Guo and Diab, 2010). Classification results: Table 2 shows the results of our models using best tuned parameters of c = 10, &apos;y = 2 on 3 datasets. We present three baselines in Table 2: (1) WEKA uses WEKA’s classifiers directly on bag-of-words without topic modeling. The values of features are simply term frequency. (2) WEKA+FS performs feature selection using information gain before applying classification. (3) LDA, is the traditional topic model. Note that Griffiths et al.’s (2005) implementation of 557 LDA achieve 51% on Brown corpus using Naive Bayes . Finally the Table illustrates the re</context>
<context position="34119" citStr="Sinha and Mihalcea, 2007" startWordPosition="5892" endWordPosition="5895">nd Lapata (2009) place the sense induction in a Baysian framework by assuming each context word is generated from the target word’s senses, and a context is modeled as a multinomial distribution over the target word’s senses rather than topics. Li et al. (2010) design several systems that use latent topics to find a most likely sense based on the sense paraphrases (extracted from WordNet) and context. Their WSD models are unsupervised and outperform state-ofart systems. Our model borrows the local window idea from word sense disambiguation community. In graphbased WSD systems (Mihalcea, 2005; Sinha and Mihalcea, 2007; Guo and Diab, 2010), a node is created for each sense. Two nodes will be connected if their distance is less than a predefined value; the weight on the edge is a value returned by sense similarity measures, then the PageRank/Indegree algorithm is applied on this graph to determine the appropriate senses. 6 Conclusion and Future Work We presented a novel model STM that combines explicit semantic information and word distribution information in a unified topic model. STM is able to capture topics of words more accurately than traditional LDA topic models. In future work, we plan to model the W</context>
</contexts>
<marker>Sinha, Mihalcea, 2007</marker>
<rawString>Ravi Sinha and Rada Mihalcea. 2007. Unsupervised graph-based word sense disambiguation using measures of word semantic similarity. In Proceedings of the IEEE International Conference on Semantic Computing, pages 363–369.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Martha Palmer</author>
</authors>
<title>The english all-words task. In</title>
<date>2004</date>
<booktitle>Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text,</booktitle>
<pages>41--43</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="27876" citStr="Snyder and Palmer, 2004" startWordPosition="4823" endWordPosition="4826"> that the choice of the correct sense is directly correlated with the choice of a correct topic in our framework. Accordingly, a relative improvement of STMn over STM0 (where the only difference is the explicit sense definition modeling) in WSD task is an indicator of the impact of using sense definitions in the text categorization task. WSD Data: We choose the all-words WSD task in which an unsupervised WSD system is required to disambiguate all the content words in documents. Our models are evaluated against the SemCor dataset. We prefer SemCor to all-words datasets available in Senseval-3 (Snyder and Palmer, 2004) or SemEval-2007 (Pradhan et al., 2007), since it includes many more documents than either set (350 versus 3) and therefore allowing more reliable results. Moreover, SemCor is also the dataset used in (Boyd-Graber et al., 2007), where a WordNet based topic model for WSD is introduced. The 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 accuracy% 100 90 80 70 60 50 STM0 STM2 Total Noun Adjective Adverb Verb sense annotated words 225992 86996 31729 18947 88320 polysemous words 187871 70529 21989 11498 83855 TF-IDF - 0.422 0.300 0.153 0.182 Table 3: Statistics of SemCor per POS statistics of SemCor is l</context>
</contexts>
<marker>Snyder, Palmer, 2004</marker>
<rawString>Benjamin Snyder and Martha Palmer. 2004. The english all-words task. In Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, pages 41–43. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>Ryan McDonald</author>
</authors>
<title>Modeling online reviews with multi-grain topic models.</title>
<date>2008</date>
<booktitle>In Proceedings of the 17th international conference on World Wide Web,</booktitle>
<pages>111--120</pages>
<contexts>
<context position="6666" citStr="Titov and McDonald (2008)" startWordPosition="1099" endWordPosition="1102">r model words are generated from topics, then senses are generated from words. In our model, we assume that during the generative process, the author picks a concept relevant to the topic, then thinks of a best word that represents that concept. Hence the word choice is dependent on the relatedness of the sense and its fit to the document context. In standard topic models, the topic of a word is sampled from the document level topic mixture θ. The underlying assumption is that all words in a document constitute the context of the target word. However, it is not the case in real world corpora. Titov and McDonald (2008) find that using global topic mixtures can only extract global topics in online reviews (e.g., Creative Labs MP3 players and iPods) and ignores local topics (product features such as portability and battery). They design the Multi-grain LDA where the local topic of a word is only determined by topics of surrounding sentences. In word sense disambiguation (WSD), an even narrower context is taken into consideration, for instance in graph based WSD models (Mihalcea, 2005), the choice of a sense for a word only depends on a local window whose size equals the length of the sentence. Later in (Sinha</context>
</contexts>
<marker>Titov, McDonald, 2008</marker>
<rawString>Ivan Titov and Ryan McDonald. 2008. Modeling online reviews with multi-grain topic models. In Proceedings of the 17th international conference on World Wide Web, pages 111–120.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>