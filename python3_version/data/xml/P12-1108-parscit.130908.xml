<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000085">
<title confidence="0.999519">
A Cost Sensitive Part-of-Speech Tagging:
Differentiating Serious Errors from Minor Errors
</title>
<author confidence="0.999094">
Hyun-Je Song&apos; Jeong-Woo Son&apos; Tae-Gil Noh&apos; Seong-Bae Park&apos; ,3 Sang-Jo Lee&apos;
</author>
<affiliation confidence="0.870867">
&apos;School of Computer Sci. &amp; Eng. &apos;Computational Linguistics 3NLP Lab.
Kyungpook Nat’l Univ. Heidelberg University Dept. of Computer Science
</affiliation>
<address confidence="0.698638">
Daegu, Korea Heidelberg, Germany University of Illinois at Chicago
</address>
<email confidence="0.998875">
{hjsong,jwson,tgnoh}@sejong.knu.ac.kr sbpark@uic.edu sjlee@knu.ac.kr
</email>
<sectionHeader confidence="0.995641" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9996991">
All types of part-of-speech (POS) tagging er-
rors have been equally treated by existing tag-
gers. However, the errors are not equally im-
portant, since some errors affect the perfor-
mance of subsequent natural language pro-
cessing (NLP) tasks seriously while others do
not. This paper aims to minimize these serious
errors while retaining the overall performance
of POS tagging. Two gradient loss functions
are proposed to reflect the different types of er-
rors. They are designed to assign a larger cost
to serious errors and a smaller one to minor
errors. Through a set of POS tagging exper-
iments, it is shown that the classifier trained
with the proposed loss functions reduces se-
rious errors compared to state-of-the-art POS
taggers. In addition, the experimental result
on text chunking shows that fewer serious er-
rors help to improve the performance of sub-
sequent NLP tasks.
</bodyText>
<sectionHeader confidence="0.999126" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9997728">
Part-of-speech (POS) tagging is needed as a pre-
processor for various natural language processing
(NLP) tasks such as parsing, named entity recogni-
tion (NER), and text chunking. Since POS tagging is
normally performed in the early step of NLP tasks,
the errors in POS tagging are critical in that they
affect subsequent steps and often lower the overall
performance of NLP tasks.
Previous studies on POS tagging have shown
high performance with machine learning techniques
(Ratnaparkhi, 1996; Brants, 2000; Lafferty et al.,
2001). Among the types of machine learning ap-
proaches, supervised machine learning techniques
were commonly used in early studies on POS tag-
ging. With the characteristics of a language (Rat-
naparkhi, 1996; Kudo et al., 2004) and informa-
tive features for POS tagging (Toutanova and Man-
ning, 2000), the state-of-the-art supervised POS tag-
ging achieves over 97% of accuracy (Shen et al.,
2007; Manning, 2011). This performance is gen-
erally regarded as the maximum performance that
can be achieved by supervised machine learning
techniques. There have also been many studies on
POS tagging with semi-supervised (Subramanya et
al., 2010; Søgaard, 2011) or unsupervised machine
learning methods (Berg-Kirkpatrick et al., 2010;
Das and Petrov, 2011) recently. However, there still
exists room to improve supervised POS tagging in
terms of error differentiation.
It should be noted that not all errors are equally
important in POS tagging. Let us consider the parse
trees in Figure 1 as an example. In Figure 1(a),
the word “plans” is mistagged as a noun where it
should be a verb. This error results in a wrong parse
tree that is severely different from the correct tree
shown in Figure 1(b). The verb phrase of the verb
“plans” in Figure 1(b) is discarded in Figure 1(a)
and the whole sentence is analyzed as a single noun
phrase. Figure 1(c) and (d) show another tagging er-
ror and its effect. In Figure 1(c), a noun is tagged as
a NNS (plural noun) where its correct tag is NN (sin-
gular or mass noun). However, the error in Figure
1(c) affects only locally the noun phrase to which
“physics” belongs. As a result, the general structure
of the parse tree in Figure 1(c) is nearly the same as
</bodyText>
<page confidence="0.954446">
1025
</page>
<note confidence="0.989437">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1025–1034,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<figure confidence="0.976994444444444">
raise 150 billion in cash.
VB CD CD IN NN
(a) A parse tree with a serious error.
(c) A parse tree with a minor error.
raise 150 billion in cash.
VB CD CD IN NN
(b) The correct parse tree of the sentence“The treasury
plans ...”.
(d) The correct parse tree of the sentence “We altered
S
NP
The treasury
DT NNP
plans
NNS
S
VP
VP
to
TO
We
PRP
NP
altered
VBN
S
NP VP
NP PP
the chemistry and physics
NN CC NNS INDT NN
of the atmosphere
DT
S
NP VP
The treasury
DT NNP
S
VP
to
TO
VP
plans
VBZ
NP
NP VP
We
PRP
altered
VBN
NP PP
the chemistry and physics of the atmosphere
DT NN CC NN INDT NN
S
. . .”.
</figure>
<figureCaption confidence="0.999959">
Figure 1: An example of POS tagging errors
</figureCaption>
<bodyText confidence="0.998206914285714">
the correct one in Figure 1(d). That is, a sentence
analyzed with this type of error would yield a cor-
rect or near-correct result in many NLP tasks such
as machine translation and text chunking.
The goal of this paper is to differentiate the seri-
ous POS tagging errors from the minor errors. POS
tagging is generally regarded as a classification task,
and zero-one loss is commonly used in learning clas-
sifiers (Altun et al., 2003). Since zero-one loss con-
siders all errors equally, it can not distinguish error
types. Therefore, a new loss is required to incorpo-
rate different error types into the learning machines.
This paper proposes two gradient loss functions to
reflect differences among POS tagging errors. The
functions assign relatively small cost to minor er-
rors, while larger cost is given to serious errors.
They are applied to learning multiclass support vec-
tor machines (Tsochantaridis et al., 2004) which is
trained to minimize the serious errors. Overall accu-
racy of this SVM is not improved against the state-
of-the-art POS tagger, but the serious errors are sig-
nificantly reduced with the proposed method. The
effect of the fewer serious errors is shown by apply-
ing it to the well-known NLP task of text chunking.
Experimental results show that the proposed method
achieves a higher F1-score compared to other POS
taggers.
The rest of the paper is organized as follows. Sec-
tion 2 reviews the related studies on POS tagging. In
Section 3, serious and minor errors are defined, and
it is shown that both errors are observable in a gen-
eral corpus. Section 4 proposes two new loss func-
tions for discriminating the error types in POS tag-
ging. Experimental results are presented in Section
5. Finally, Section 6 draws some conclusions.
</bodyText>
<sectionHeader confidence="0.999769" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9956845">
The POS tagging problem has generally been solved
by machine learning methods for sequential label-
</bodyText>
<page confidence="0.985278">
1026
</page>
<table confidence="0.991256">
Tag category POS tags
Substantive NN, NNS, NNP, NNPS, CD, PRP, PRP$
Predicate VB, VBD, VBG, VBN, VBP, VBZ, MD, JJ, JJR, JJS
Adverbial RB, RBR, RBS, RP, UH, EX, WP, WP$, WRB, CC, IN, TO
Determiner DT, PDT, WDT
Etc FW, SYM, POS, LS
</table>
<tableCaption confidence="0.9999">
Table 1: Tag categories and POS tags in Penn Tree Bank tag set
</tableCaption>
<bodyText confidence="0.999788444444444">
ing. In early studies, rich linguistic features and su-
pervised machine learning techniques are applied by
using annotated corpora like the Wall Street Journal
corpus (Marcus et al., 1994). For instance, Ratna-
parkhi (1996) used a maximum entropy model for
POS tagging. In this study, the features for rarely
appearing words in a corpus are expanded to im-
prove the overall performance. Following this direc-
tion, various studies have been proposed to extend
informative features for POS tagging (Toutanova
and Manning, 2000; Toutanova et al., 2003; Man-
ning, 2011). In addition, various supervised meth-
ods such as HMMs and CRFs are widely applied to
POS tagging. Lafferty et al. (2001) adopted CRFs
to predict POS tags. The methods based on CRFs
not only have all the advantages of the maximum
entropy markov models but also resolve the well-
known problem of label bias. Kudo et al. (2004)
modified CRFs for non-segmented languages like
Japanese which have the problem of word boundary
ambiguity.
As a result of these efforts, the performance of
state-of-the-art supervised POS tagging shows over
97% of accuracy (Toutanova et al., 2003; Gim´enez
and M`arquez, 2004; Tsuruoka and Tsujii, 2005;
Shen et al., 2007; Manning, 2011). Due to the high
accuracy of supervised approaches for POS tagging,
it has been deemed that there is no room to im-
prove the performance on POS tagging in supervised
manner. Thus, recent studies on POS tagging focus
on semi-supervised (Spoustov´a et al., 2009; Sub-
ramanya et al., 2010; Søgaard, 2011) or unsuper-
vised approaches (Haghighi and Klein, 2006; Gold-
water and Griffiths, 2007; Johnson, 2007; Graca et
al., 2009; Berg-Kirkpatrick et al., 2010; Das and
Petrov, 2011). Most previous studies on POS tag-
ging have focused on how to extract more linguistic
features or how to adopt supervised or unsupervised
approaches based on a single evaluation measure,
accuracy. However, with a different viewpoint for
errors on POS tagging, there is still some room to
improve the performance of POS tagging for subse-
quent NLP tasks, even though the overall accuracy
can not be much improved.
In ordinary studies on POS tagging, costs of er-
rors are equally assigned. However, with respect
to the performance of NLP tasks relying on the re-
sult of POS tagging, errors should be treated differ-
ently. In the machine learning community, cost sen-
sitive learning has been studied to differentiate costs
among errors. By adopting different misclassifica-
tion costs for each type of errors, a classifier is op-
timized to achieve the lowest expected cost (Elkan,
2001; Cai and Hofmann, 2004; Zhou and Liu, 2006).
</bodyText>
<sectionHeader confidence="0.997219" genericHeader="method">
3 Error Analysis of Existing POS Tagger
</sectionHeader>
<bodyText confidence="0.99993275">
The effects of POS tagging errors to subsequent
NLP tasks vary according to their type. Some errors
are serious, while others are not. In this paper, the
seriousness of tagging errors is determined by cat-
egorical structures of POS tags. Table 1 shows the
Penn tree bank POS tags and their categories. There
are five categories in this table: substantive, pred-
icate, adverbial, determiner, and etc. Serious tag-
ging errors are defined as misclassifications among
the categories, while minor errors are defined as mis-
classifications within a category. This definition fol-
lows the fact that POS tags in the same category
form similar syntax structures in a sentence (Zhao
and Marcus, 2009). That is, inter-category errors are
treated as serious errors, while intra-category errors
are treated as minor errors.
Table 2 shows the distribution of inter-category
and intra-category errors observed in section 22–
24 of the WSJ corpus (Marcus et al., 1994) that is
tagged by the Stanford Log-linear Part-Of-Speech
</bodyText>
<page confidence="0.975438">
1027
</page>
<figure confidence="0.996321307692308">
Predicted category
Substantive Predicate Adverbial Determiner Etc
Substantive
Predicate
Adverbial
Determiner
Etc
614 479 32 10 15
585 743 107 2 14
41 156 500 42 2
13 7 47 24 0
23 11 3 1 0
True category
</figure>
<tableCaption confidence="0.978529">
Table 2: The distribution of tagging errors on WSJ corpus by Stanford Part-Of-Speech Tagger.
</tableCaption>
<bodyText confidence="0.999622157894737">
Tagger (Manning, 2011) (trained with WSJ sections
00–18). In this table, bold numbers denote inter-
category errors while all other numbers show intra-
category errors. The number of total errors is 3,471
out of 129,654 words. Among them, 1,881 errors
(54.19%) are intra-category, while 1,590 of the er-
rors (45.81%) are inter-category. If we can reduce
these inter-category errors under the cost of mini-
mally increasing intra-category errors, the tagging
results would improve in quality.
Generally in POS tagging, all tagging errors are
regarded equally in importance. However, inter-
category and intra-category errors should be distin-
guished. Since a machine learning method is opti-
mized by a loss function, inter-category errors can
be efficiently reduced if a loss function is designed
to handle both types of errors with different cost. We
propose two loss functions for POS tagging and they
are applied to multiclass Support Vector Machines.
</bodyText>
<sectionHeader confidence="0.759455" genericHeader="method">
4 Learning SVMs with Class Similarity
</sectionHeader>
<bodyText confidence="0.999721222222222">
POS tagging has been solved as a sequential labeling
problem which assumes dependency among words.
However, by adopting sequential features such as
POS tags of previous words, the dependency can be
partially resolved. If it is assumed that words are
independent of one another, POS tagging can be re-
garded as a multiclass classification problem. One
of the best solutions for this problem is by using an
SVM.
</bodyText>
<subsectionHeader confidence="0.984917">
4.1 Training SVMs with Loss Function
</subsectionHeader>
<bodyText confidence="0.9275268">
Assume that a training data set D =
{(x1, y1), (x2, y2), . . . , (xl, yl)} is given where
xi ∈ Rd is an instance vector and yi ∈ {+1, −1}
is its class label. SVM finds an optimal hyperplane
satisfying
</bodyText>
<equation confidence="0.5883175">
xi · w + b ≥ +1 for yi = +1,
xi · w + b ≤ −1 for yi = −1,
</equation>
<bodyText confidence="0.999857333333333">
where w and b are parameters to be estimated from
training data D. To estimate the parameters, SVMs
minimizes a hinge loss defined as
</bodyText>
<equation confidence="0.9503565">
ξi = Lhinge(yi, w · xi + b)
= max{0,1 − yi · (w · xi + b)}.
</equation>
<bodyText confidence="0.998932">
With regularizer ||w||2 to control model complexity,
the optimization problem of SVMs is defined as
</bodyText>
<equation confidence="0.99313875">
2||w||2 + C
1
subject to
yi(xi · w + b) ≥ 1 − ξi, and ξi ≥ 0 ∀i,
</equation>
<bodyText confidence="0.9807806">
where C is a user parameter to penalize errors.
Crammer et al. (2002) expanded the binary-class
SVM for multiclass classifications. In multiclass
SVMs, by considering all classes the optimization
of SVM is generalized as
</bodyText>
<equation confidence="0.98862675">
||wk||2 + C
with constraints
(wyi · φ(xi, yi)) − (wk · φ(xi, k)) ≥ 1 − ξi,
ξi ≥ 0 ∀i, ∀k ∈ K \ yi,
</equation>
<bodyText confidence="0.9996485">
where φ(xi, yi) is a combined feature representation
of xi and yi, and K is the set of classes.
</bodyText>
<figure confidence="0.935327174603174">
min
w,C
ξi,
�l
i=1
1
2
kEK
min
w,C
ξi,
�l
i=1
1028
DT
WDT
POS
ADVERBIAL
PREDICATE
FW
POS
SUBSTANTIVE
PDT
OTHERS
DETERMINER
NN
NNS
NOUN
NNP
NNPS
CD
VB
PRP
VBD
PRONOUN
VBG
PRP$
VERB
VBN
VBP
VBZ
JJ
ADJECT
MD
JJR
JJS
RB
RBR
ADVERB
RBS
RP
UH
EX
WP
WP$
CONJUNCTION
WH-
WRB
SYM
CC
IN
TO
LS
</figure>
<figureCaption confidence="0.999692">
Figure 2: A tree structure of POS tags.
</figureCaption>
<bodyText confidence="0.99798875">
Since both binary and multiclass SVMs adopt a
hinge loss, the errors between classes have the same
cost. To assign different cost to different errors,
Tsochantaridis et al. (2004) proposed an efficient
way to adopt arbitrary loss function, L(yi, yj) which
returns zero if yi = yj, otherwise L(yi, yj) &gt; 0.
Then, the hinge loss �i is re-scaled with the inverse
of the additional loss between two classes. By scal-
ing slack variables with the inverse loss, margin vi-
olation with high loss L(yi, yj) is more severely re-
stricted than that with low loss. Thus, the optimiza-
tion problem with L(yi, yj) is given as
</bodyText>
<equation confidence="0.997598714285714">
1 X
2
kEK
with constraints
(wyi - O(xi, yi)) − (wk - O(xi, k)) &gt; 1 − �i
L(yi, k),
�i &gt; 0 di, dkEK\yi,
</equation>
<bodyText confidence="0.999680666666667">
With the Lagrange multiplier α, the optimization
problem in Equation (1) is easily converted to the
following dual quadratic problem.
</bodyText>
<equation confidence="0.1978325">
J(xi, yi, ki)J(xj, yj, kj) − Xl X αi,ki,
i kiEK\yi
</equation>
<table confidence="0.73658">
with constraints αi,ki &lt; C, di = 1,---,1,
Xα &gt; 0 and L(yi, ki)
kiEK\yi
where J(xi, yi, ki) is defined as
J(xi, yi, ki) = O(xi, yi) − O(xi, ki).
</table>
<subsectionHeader confidence="0.965419">
4.2 Loss Functions for POS tagging
</subsectionHeader>
<bodyText confidence="0.999871142857143">
To design a loss function for POS tagging, this paper
adopts categorical structures of POS tags. The sim-
plest way to reflect the structure of POS tags shown
in Table 1 is to assign larger cost to inter-category
errors than to intra-category errors. Thus, the loss
function with the categorical structure in Table 1 is
defined as
</bodyText>
<equation confidence="0.9630735">
0 if yi = yj,
S if yi =� yj but they belong
to the same POS category,
1 otherwise,
</equation>
<bodyText confidence="0.999935">
where 0 &lt; S &lt; 1 is a constant to reduce the value of
L,(yi, yj) when yi and yj are similar. As shown in
this equation, inter-category errors have larger cost
than intra-category errors. This loss L,(yi, yj) is
named as category loss.
The loss function L,(yi, yj) is designed to reflect
the categories in Table 1. However, the structure
of POS tags can be represented as a more complex
structure. Let us consider the category, predicate.
</bodyText>
<equation confidence="0.8178565">
X
kiEK\yi
X αi,kiαj,kj X
kjEK\yj
min
α
1 l
X
i,j
2
min
w,C
l
||wk||2 + C X �i, (1)
i=1



L,(yi, yj) =
(2)
</equation>
<page confidence="0.894822">
1029
</page>
<figure confidence="0.999831545454546">
(a) Multiclass SVMs with hinge loss
k
(b) Multiclass SVMs with the proposed loss
function
Class VB
k
Class NN Class NNS
L(NN, VB)
Class VB
Class NN k Class NNS
L(NN, NNS)
</figure>
<figureCaption confidence="0.999988">
Figure 3: Effect of the proposed loss function in multiclass SVMs
</figureCaption>
<bodyText confidence="0.999572714285714">
This category has ten POS tags, and can be further
categorized into two sub-categories: verb and ad-
ject. Figure 2 represents a categorical structure of
POS tags as a tree with five categories of POS tags
and their seven sub-categories.
To express the tree structure of Figure 2 as a loss,
another loss function Lt(yi, yj) is defined as
</bodyText>
<equation confidence="0.9996375">
Lt(yi,yj) �
1�[Dist(Pi�j, yi) + Dist(Pi�j, yj)] × -y, (3)
</equation>
<bodyText confidence="0.999972142857143">
where Pij denotes the nearest common parent of
both yi and yj, and the function Dist(Pij,yi) re-
turns the number of steps from Pij to yi. The user
parameter -y is a scaling factor of a unit loss for a
single step. This loss Lt(yi, yj) returns large value
if the distance between yi and yj is far in the tree
structure, and it is named as tree loss.
As shown in Equation (1), two proposed loss
functions adjust margin violation between classes.
They basically assign less value for intra-category
errors than inter-category errors. Thus, a classi-
fier is optimized to strictly keep inter-category er-
rors within a smaller boundary. Figure 3 shows a
simple example. In this figure, there are three POS
tags and two categories. NN (singular or mass noun)
and NNS (plural noun) belong to the same cate-
gory, while VB (verb, base form) is in another cat-
egory. Figure 3(a) shows the decision boundary of
NN based on hinge loss. As shown in this figure, a
single � is applied for the margin violation among
all classes. Figure 3(b) also presents the decision
boundary of NN, but it is determined with the pro-
posed loss function. In this figure, the margin vio-
lation is applied differently to inter-category (NN to
VB) and intra-category (NN to NNS) errors. It re-
sults in reducing errors between NN and VB even if
the errors between NN and NNS could be slightly
increased.
</bodyText>
<sectionHeader confidence="0.999839" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.996397">
5.1 Experimental Setting
</subsectionHeader>
<bodyText confidence="0.999662647058823">
Experiments are performed with a well-known stan-
dard data set, the Wall Street Journal (WSJ) corpus.
The data is divided into training, development and
test sets as in (Toutanova et al., 2003; Tsuruoka and
Tsujii, 2005; Shen et al., 2007). Table 3 shows some
simple statistics of these data sets. As shown in
this table, training data contains 38,219 sentences
with 912,344 words. In the development data set,
there are 5,527 sentences with about 131,768 words,
those in the test set are 5,462 sentences and 129,654
words. The development data set is used only to se-
lect S in Equation (2) and -y in Equation (3).
Table 4 shows the feature set for our experiments.
In this table, wi and ti denote the lexicon and POS
tag for the i-th word in a sentence respectively. We
use almost the same feature set as used in (Tsuruoka
and Tsujii, 2005) including word features, tag fea-
</bodyText>
<page confidence="0.967883">
1030
</page>
<table confidence="0.99978825">
Training Develop Test
Section 0–18 19–21 22–24
# of sentences 38,219 5,527 5,462
# of terms 912,344 131,768 129,654
</table>
<tableCaption confidence="0.99105">
Table 3: Simple statistics of experimental data
</tableCaption>
<table confidence="0.9894537">
Feature Name Description
Word features wi−2, wi−1, wi, wi+1, wi+2
wi−1 · wi, wi · wi+1
Tag features ti−2, ti−1, ti+1, ti+2
ti−2 · ti−1, ti+1 · ti+2
ti−2 · ti−1 · ti+1, ti−1 · ti+1 · ti+2
ti−2 · ti−1 · ti+1 · ti+2
Tag/Word ti−2·wi, ti−1·wi, ti+1·wi, ti+2·wi
combination ti−1 · ti+1 · wi
Prefix features prefixes of wi (up to length 9)
</table>
<bodyText confidence="0.862167714285714">
Suffix features suffixes of wi (up to length 9)
Lexical features whether wi contains capitals
whether wi has a number
whether wi has a hyphen
whether wi is all capital
whether wi starts with capital and
locates at the middle of sentence
</bodyText>
<tableCaption confidence="0.987021">
Table 4: Feature template for experiments
</tableCaption>
<bodyText confidence="0.995732">
tures, word/tag combination features, prefix and suf-
fix features as well as lexical features. The POS tags
for words are obtained from a two-pass approach
proposed by Nakagawa et al. (2001).
In the experiments, two multiclass SVMs with the
proposed loss functions are used. One is CL-MSVM
with category loss and the other is TL-MSVM with
tree loss. A linear kernel is used for both SVMs.
</bodyText>
<subsectionHeader confidence="0.999142">
5.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.99099025">
CL-MSVM with δ = 0.4 shows the best overall per-
formance on the development data where its error
rate is as low as 2.71%. δ = 0.4 implies that the
cost of intra-category errors is set to 40% of that of
inter-category errors. The error rate of TL-MSVM
is 2.69% when γ is 0.6. δ = 0.4 and γ = 0.6 are set
in the all experiments below.
Table 5 gives the comparison with the previous
work and proposed methods on the test data. As can
be seen from this table, the best performing algo-
rithms achieve near 2.67% error rate (Shen et al.,
2007; Manning, 2011). CL-MSVM and TL-MSVM
</bodyText>
<table confidence="0.991918857142857">
Error # of Intra # of Inter
(%) error error
(Gim´enez and M`arquez, 1,995 1,692
2004) 2.84 (54.11%) (45.89%)
(Tsuruoka and Tsujii, 2.85 - -
2005)
(Shen et al., 2007) 2.67 1,856 1,612
(53.52%) (46.48%)
(Manning, 2011) 2.68 1,881 1,590
(54.19%) (45.81%)
CL-MSVM (δ = 0.4) 2.69 1,916 1,567
(55.01%) (44.99%)
TL-MSVM (γ = 0.6) 2.68 1,904 1,574
(54.74%) (45.26%)
</table>
<tableCaption confidence="0.999988">
Table 5: Comparison with the previous works
</tableCaption>
<bodyText confidence="0.999967681818182">
achieve an error rate of 2.69% and 2.68% respec-
tively. Although overall error rates of CL-MSVM
and TL-MSVM are not improved compared to the
previous state-of-the-art methods, they show reason-
able performance.
For inter-category error, CL-MSVM achieves the
best performance. The number of inter-category er-
ror is 1,567, which shows 23 errors reduction com-
pared to previous best inter-category result by (Man-
ning, 2011). TL-MSVM also makes 16 less inter-
category errors than Manning’s tagger. When com-
pared with Shen’s tagger, both CL-MSVM and TL-
MSVM make far less inter-category errors even if
their overall performance is slightly lower than that
of Shen’s tagger. However, the intra-category er-
ror rate of the proposed methods has some slight
increases. The purpose of proposed methods is to
minimize inter-category errors but preserving over-
all performance. From these results, it can be found
that the proposed methods which are trained with the
proposed loss functions do differentiate serious and
minor POS tagging errors.
</bodyText>
<subsectionHeader confidence="0.999766">
5.3 Chunking Experiments
</subsectionHeader>
<bodyText confidence="0.999786375">
The task of chunking is to identify the non-recursive
cores for various types of phrases. In chunking, the
POS information is one of the most crucial aspects in
identifying chunks. Especially inter-category POS
errors seriously affect the performance of chunking
because they are more likely to mislead the chunk
compared to intra-category errors.
Here, chunking experiments are performed with
</bodyText>
<page confidence="0.957962">
1031
</page>
<table confidence="0.9946584">
POS tagger Accuracy (%) Precision Recall F1-score
(Shen et al., 2007) 96.08 94.03 93.75 93.89
(Manning, 2011) 96.08 94 93.8 93.9
CL-MSVM (S = 0.4) 96.13 94.1 93.9 94.00
TL-MSVM (-y = 0.6) 96.12 94.1 93.9 94.00
</table>
<tableCaption confidence="0.999794">
Table 6: The experimental results for chunking
</tableCaption>
<bodyText confidence="0.999239743589744">
a data set provided for the CoNLL-2000 shared
task. The training data contains 8,936 sentences
with 211,727 words obtained from sections 15–18
of the WSJ. The test data consists of 2,012 sentences
and 47,377 words in section 20 of the WSJ. In order
to represent chunks, an IOB model is used, where
every word is tagged with a chunk label extended
with B (the beginning of a chunk), I (inside a chunk),
and O (outside a chunk). First, the POS informa-
tion in test data are replaced to the result of our POS
tagger. Then it is evaluated using trained chunking
model. Since CRFs (Conditional Random Fields)
has been shown near state-of-the-art performance in
text chunking (Fei Sha and Fernando Pereira, 2003;
Sun et al., 2008), we use CRF++, an open source
CRF implementation by Kudo (2005), with default
feature template and parameter settings of the pack-
age. For simplicity in the experiments, the values
of S in Equation (2) and -y in Equation (3) are set
to be 0.4 and 0.6 respectively which are same as the
previous section.
Table 6 gives the experimental results of text
chunking according to the kinds of POS taggers in-
cluding two previous works, CL-MSVM, and TL-
MSVM. Shen’s tagger and Manning’s tagger show
nearly the same performance. They achieve an ac-
curacy of 96.08% and around 93.9 F1-score. On the
other hand, CL-MSVM achieves 96.13% accuracy
and 94.00 F1-score. The accuracy and F1-score of
TL-MSVM are 96.12% and 94.00. Both CL-MSVM
and TL-MSVM show slightly better performances
than other POS taggers. As shown in Table 5, both
CL-MSVM and TL-MSVM achieve lower accura-
cies than other methods, while their inter-category
errors are less than that of other experimental meth-
ods. Thus, the improvement of CL-MSVM and TL-
MSVM implies that, for the subsequent natural lan-
guage processing, a POS tagger should considers
different cost of tagging errors.
</bodyText>
<sectionHeader confidence="0.998618" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99998725">
In this paper, we have shown that supervised POS
tagging can be improved by discriminating inter-
category errors from intra-category ones. An inter-
category error occurs by mislabeling a word with
a totally different tag, while an intra-category error
is caused by a similar POS tag. Therefore, inter-
category errors affect the performances of subse-
quent NLP tasks far more than intra-category errors.
This implies that different costs should be consid-
ered in training POS tagger according to error types.
As a solution to this problem, we have proposed
two gradient loss functions which reflect different
costs for two error types. The cost of an error type is
set according to (i) categorical difference or (ii) dis-
tance in the tree structure of POS tags. Our POS
experiment has shown that if these loss functions
are applied to multiclass SVMs, they could signif-
icantly reduce inter-category errors. Through the
text chunking experiment, it is shown that the multi-
class SVMs trained with the proposed loss functions
which generate fewer inter-category errors achieve
higher performance than existing POS taggers.
We have shown that cost sensitive learning can be
applied to POS tagging only with multiclass SVMs.
However, the proposed loss functions are general
enough to be applied to other existing POS taggers.
Most supervised machine learning techniques are
optimized on their loss functions. Therefore, the
performance of POS taggers based on supervised
machine learning techniques can be improved by ap-
plying the proposed loss functions to learn their clas-
sifiers.
</bodyText>
<sectionHeader confidence="0.997496" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9927965">
This research was supported by the Converg-
ing Research Center Program funded by the
Ministry of Education, Science and Technology
(2011K000659).
</bodyText>
<sectionHeader confidence="0.998774" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.988467833333333">
Yasemin Altun, Mark Johnson, and Thomas Hofmann.
2003. Investigating Loss Functions and Optimiza-
tion Methods for Discriminative Learning of Label Se-
quences. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing. pp.
145–152.
</reference>
<page confidence="0.935337">
1032
</page>
<reference confidence="0.999678944444445">
Talyor Berg-Kirkpatrick, Alexandre Bouchard-Cˆot´e,
John DeNero, and Dan Klein. 2010. Painless Un-
supervised Learning with Features. In Proceedings
of the North American Chapter of the Association for
Computational Linguistics. pp. 582–590.
Thorsten Brants. 2000. TnT-A Statistical Part-of-Speech
Tagger. In Proceedings of the Sixth Applied Natural
Language Processing Conference. pp. 224–231.
Lijuan Cai and Thomas Hofmann. 2004. Hierarchi-
cal Document Categorization with Support Vector Ma-
chines. In Proceedings of the Thirteenth ACM Inter-
national Conference on Information and Knowledge
Management. pp. 78–87.
Koby Crammer, Yoram Singer. 2002. On the Algorith-
mic Implementation of Multiclass Kernel-based Vec-
tor Machines. Journal ofMachine Learning Research,
Vol. 2. pp. 265–292.
Dipanjan Das and Slav Petrov. 2011. Unsupervised Part-
of-Speech Tagging with Bilingual Graph-Based Pro-
jections. In Proceedings of the 49th Annual Meeting
of the Association of Computational Linguistics. pp.
600–609.
Charles Elkan. 2001. The Foundations of Cost-Sensitive
Learning. In Proceedings of the Seventeenth Interna-
tional Joint Conference on Artificial Intelligence. pp.
973–978.
Jes´us Gim´enez and Lluis M`arquez. 2004. SVMTool: A
general POS tagger generator based on Support Vector
Machines. In Proceedings of the Fourth International
Conference on Language Resources and Evaluation.
pp. 43–46.
Sharon Goldwater and Thomas T. Griffiths. 2007. A
fully Bayesian Approach to Unsupervised Part-of-
Speech Tagging. In Proceedings of the 45th Annual
Meeting of the Association of Computational Linguis-
tics. pp. 744–751.
Joao Graca, Kuzman Ganchev, Ben Taskar, and Fernando
Pereira. 2009. Posterior vs Parameter Sparsity in La-
tent Variable Models. In Advances in Neural Informa-
tion Processing Systems 22. pp. 664–672.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
Learning for Sequence Models. In Proceedings of the
North American Chapter of the Association for Com-
putational Linguistics. pp. 320–327.
Mark Johnson. 2007. Why doesn’t EM find good HMM
POS-taggers? In Proceedings of the 2007 Joint Meet-
ing of the Conference on Empirical Methods in Natu-
ral Language Processing and the Conference on Com-
putational Natural Language Learning. pp. 296–305.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying Conditional Random Fields to
Japanese Morphological Analysis. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing. pp. 230–237.
Taku Kudo. 2005. CRF++: Yet another CRF toolkit.
http://crfpp.sourceforge.net.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional Random Fields: Probabilistic Mod-
els for Segmenting and Labeling Sequence Data. In
Proceedings of the Eighteenth International Confer-
ence on Machine Learning. pp. 282–289.
Christopher D. Manning. 2011. Part-of-Speech Tagging
from 97% to 100%: Is It Time for Some Linguistics?.
In Proceedings of the 12th International Conference
on Intelligent Text Processing and Computational Lin-
guistics. pp. 171–189.
Tetsuji Nakagawa, Taku Kudo, and Yuji Matsumoto.
2001. Unknown Word Guessing and Part-of-Speech
Tagging Using Support Vector Machines. In Proceed-
ings of the Sixth Natural Language Processing Pacific
Rim Symposium. pp. 325–331.
Adwait Ratnaparkhi. 1996. A Maximum Entropy Model
for Part-Of-Speech Tagging. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing. pp. 133–142.
Fei Sha and Fernando Pereira. 2003. Shallow Parsing
with Conditional Random Fields. In Proceedings of
the Human Language Technology and North American
Chapter ofthe Association for Computational Linguis-
tics. pp. 213–220.
Libin Shen, Giorgio Satta, and Aravind K. Joshi 2007.
Guided Learning for Bidirectional Sequence Classifi-
cation. In Proceedings of the 45th Annual Meeting
of the Association of Computational Linguistics. pp.
760–767.
Anders Søgaard 2011. Semisupervised condensed near-
est neighbor for part-of-speech tagging. In Proceed-
ings of the 49th Annual Meeting of the Association of
Computational Linguistics. pp. 48–52.
Drahomira “johanka” Spoustov`a, Jan Hajiˇc, Jan Raab,
and Miroslav Spousta 2009. Semi-supervised training
for the averaged perceptron POS tagger. In Proceed-
ings of the European Chapter of the Association for
Computational Linguistics. pp. 763–771.
Amarnag Subramanya, Slav Petrov and Fernando Pereira
2010. Efficient Graph-Based Semi-Supervised Learn-
ing of Structured Tagging Models. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing. pp. 167–176.
Xu Sun, Louis-Philippe Morency, Daisuke Okanohara
and Jun’ichi Tsujii 2008. Modeling Latent-Dynamic
in Shallow Parsing: A Latent Conditional Model with
Improved Inference. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics.
pp. 841–848.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-Rich Part-of-
Speech Tagging with a Cyclic Dependency Network.
</reference>
<page confidence="0.55107">
1033
</page>
<reference confidence="0.999669633333333">
In Proceedings of the Human Language Technology
and North American Chapter of the Association for
Computational Linguistics. pp. 252–259.
Kristina Toutanova and Christopher D. Manning. 2000.
Enriching the Knowledge Sources Used in a Maxi-
mum Entropy Part-of-Speech Tagger. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing. pp. 63–70.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemi Altun. 2004. Support Vec-
tor Learning for Interdependent and Structured Output
Spaces. In Proceedings of the 21st International Con-
ference on Machine Learning. pp. 104–111.
Yoshimasa Tsuruoka and Jun’ichi Tsujii. 2005. Bidi-
rectional Inference with the Easiest-First Strategy for
Tagging Sequence Data. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing. pp. 467–474.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a Large Annotated
Corpus of English: The Penn Treebank. Computa-
tional Linguistics, Vol. 19, No.2 . pp. 313–330.
Qiuye Zhao and Mitch Marcus. 2009. A Simple Un-
supervised Learner for POS Disambiguation Rules
Given Only a Minimal Lexicon. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing. pp. 688–697.
Zhi-Hua Zhou and Xu-Ying Liu 2006. On Multi-Class
Cost-Sensitive Learning. In Proceedings of the AAAI
Conference on Artificial Intelligence. pp. 567–572.
</reference>
<page confidence="0.995229">
1034
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.346671">
<title confidence="0.999361">A Cost Sensitive Part-of-Speech</title>
<author confidence="0.822365">Differentiating Serious Errors from Minor Errors</author>
<affiliation confidence="0.861273">of Computer Sci. &amp; Eng. Linguistics Lab. Kyungpook Nat’l Univ. Heidelberg University Dept. of Computer Science Daegu, Korea Heidelberg, Germany University of Illinois at</affiliation>
<email confidence="0.679559">sbpark@uic.edusjlee@knu.ac.kr</email>
<abstract confidence="0.994610047619047">All types of part-of-speech (POS) tagging errors have been equally treated by existing taggers. However, the errors are not equally important, since some errors affect the performance of subsequent natural language processing (NLP) tasks seriously while others do not. This paper aims to minimize these serious errors while retaining the overall performance of POS tagging. Two gradient loss functions are proposed to reflect the different types of errors. They are designed to assign a larger cost to serious errors and a smaller one to minor errors. Through a set of POS tagging experiments, it is shown that the classifier trained with the proposed loss functions reduces serious errors compared to state-of-the-art POS taggers. In addition, the experimental result on text chunking shows that fewer serious errors help to improve the performance of subsequent NLP tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yasemin Altun</author>
<author>Mark Johnson</author>
<author>Thomas Hofmann</author>
</authors>
<title>Investigating Loss Functions and Optimization Methods for Discriminative Learning of Label Sequences.</title>
<date>2003</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<pages>145--152</pages>
<contexts>
<context position="4814" citStr="Altun et al., 2003" startWordPosition="811" endWordPosition="814">asury DT NNP S VP to TO VP plans VBZ NP NP VP We PRP altered VBN NP PP the chemistry and physics of the atmosphere DT NN CC NN INDT NN S . . .”. Figure 1: An example of POS tagging errors the correct one in Figure 1(d). That is, a sentence analyzed with this type of error would yield a correct or near-correct result in many NLP tasks such as machine translation and text chunking. The goal of this paper is to differentiate the serious POS tagging errors from the minor errors. POS tagging is generally regarded as a classification task, and zero-one loss is commonly used in learning classifiers (Altun et al., 2003). Since zero-one loss considers all errors equally, it can not distinguish error types. Therefore, a new loss is required to incorporate different error types into the learning machines. This paper proposes two gradient loss functions to reflect differences among POS tagging errors. The functions assign relatively small cost to minor errors, while larger cost is given to serious errors. They are applied to learning multiclass support vector machines (Tsochantaridis et al., 2004) which is trained to minimize the serious errors. Overall accuracy of this SVM is not improved against the stateof-th</context>
</contexts>
<marker>Altun, Johnson, Hofmann, 2003</marker>
<rawString>Yasemin Altun, Mark Johnson, and Thomas Hofmann. 2003. Investigating Loss Functions and Optimization Methods for Discriminative Learning of Label Sequences. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. pp. 145–152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Talyor Berg-Kirkpatrick</author>
<author>Alexandre Bouchard-Cˆot´e</author>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>Painless Unsupervised Learning with Features.</title>
<date>2010</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<pages>582--590</pages>
<marker>Berg-Kirkpatrick, Bouchard-Cˆot´e, DeNero, Klein, 2010</marker>
<rawString>Talyor Berg-Kirkpatrick, Alexandre Bouchard-Cˆot´e, John DeNero, and Dan Klein. 2010. Painless Unsupervised Learning with Features. In Proceedings of the North American Chapter of the Association for Computational Linguistics. pp. 582–590.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
</authors>
<title>TnT-A Statistical Part-of-Speech Tagger.</title>
<date>2000</date>
<booktitle>In Proceedings of the Sixth Applied Natural Language Processing Conference.</booktitle>
<pages>224--231</pages>
<contexts>
<context position="1842" citStr="Brants, 2000" startWordPosition="278" endWordPosition="279">ing shows that fewer serious errors help to improve the performance of subsequent NLP tasks. 1 Introduction Part-of-speech (POS) tagging is needed as a preprocessor for various natural language processing (NLP) tasks such as parsing, named entity recognition (NER), and text chunking. Since POS tagging is normally performed in the early step of NLP tasks, the errors in POS tagging are critical in that they affect subsequent steps and often lower the overall performance of NLP tasks. Previous studies on POS tagging have shown high performance with machine learning techniques (Ratnaparkhi, 1996; Brants, 2000; Lafferty et al., 2001). Among the types of machine learning approaches, supervised machine learning techniques were commonly used in early studies on POS tagging. With the characteristics of a language (Ratnaparkhi, 1996; Kudo et al., 2004) and informative features for POS tagging (Toutanova and Manning, 2000), the state-of-the-art supervised POS tagging achieves over 97% of accuracy (Shen et al., 2007; Manning, 2011). This performance is generally regarded as the maximum performance that can be achieved by supervised machine learning techniques. There have also been many studies on POS tagg</context>
</contexts>
<marker>Brants, 2000</marker>
<rawString>Thorsten Brants. 2000. TnT-A Statistical Part-of-Speech Tagger. In Proceedings of the Sixth Applied Natural Language Processing Conference. pp. 224–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lijuan Cai</author>
<author>Thomas Hofmann</author>
</authors>
<title>Hierarchical Document Categorization with Support Vector Machines.</title>
<date>2004</date>
<booktitle>In Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management.</booktitle>
<pages>78--87</pages>
<contexts>
<context position="9134" citStr="Cai and Hofmann, 2004" startWordPosition="1533" endWordPosition="1536">ome room to improve the performance of POS tagging for subsequent NLP tasks, even though the overall accuracy can not be much improved. In ordinary studies on POS tagging, costs of errors are equally assigned. However, with respect to the performance of NLP tasks relying on the result of POS tagging, errors should be treated differently. In the machine learning community, cost sensitive learning has been studied to differentiate costs among errors. By adopting different misclassification costs for each type of errors, a classifier is optimized to achieve the lowest expected cost (Elkan, 2001; Cai and Hofmann, 2004; Zhou and Liu, 2006). 3 Error Analysis of Existing POS Tagger The effects of POS tagging errors to subsequent NLP tasks vary according to their type. Some errors are serious, while others are not. In this paper, the seriousness of tagging errors is determined by categorical structures of POS tags. Table 1 shows the Penn tree bank POS tags and their categories. There are five categories in this table: substantive, predicate, adverbial, determiner, and etc. Serious tagging errors are defined as misclassifications among the categories, while minor errors are defined as misclassifications within </context>
</contexts>
<marker>Cai, Hofmann, 2004</marker>
<rawString>Lijuan Cai and Thomas Hofmann. 2004. Hierarchical Document Categorization with Support Vector Machines. In Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management. pp. 78–87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Yoram Singer</author>
</authors>
<title>On the Algorithmic Implementation of Multiclass Kernel-based Vector Machines.</title>
<date>2002</date>
<journal>Journal ofMachine Learning Research,</journal>
<volume>2</volume>
<pages>265--292</pages>
<marker>Crammer, Singer, 2002</marker>
<rawString>Koby Crammer, Yoram Singer. 2002. On the Algorithmic Implementation of Multiclass Kernel-based Vector Machines. Journal ofMachine Learning Research, Vol. 2. pp. 265–292.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Slav Petrov</author>
</authors>
<title>Unsupervised Partof-Speech Tagging with Bilingual Graph-Based Projections.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association of Computational Linguistics.</booktitle>
<pages>600--609</pages>
<contexts>
<context position="2602" citStr="Das and Petrov, 2011" startWordPosition="393" endWordPosition="396">y studies on POS tagging. With the characteristics of a language (Ratnaparkhi, 1996; Kudo et al., 2004) and informative features for POS tagging (Toutanova and Manning, 2000), the state-of-the-art supervised POS tagging achieves over 97% of accuracy (Shen et al., 2007; Manning, 2011). This performance is generally regarded as the maximum performance that can be achieved by supervised machine learning techniques. There have also been many studies on POS tagging with semi-supervised (Subramanya et al., 2010; Søgaard, 2011) or unsupervised machine learning methods (Berg-Kirkpatrick et al., 2010; Das and Petrov, 2011) recently. However, there still exists room to improve supervised POS tagging in terms of error differentiation. It should be noted that not all errors are equally important in POS tagging. Let us consider the parse trees in Figure 1 as an example. In Figure 1(a), the word “plans” is mistagged as a noun where it should be a verb. This error results in a wrong parse tree that is severely different from the correct tree shown in Figure 1(b). The verb phrase of the verb “plans” in Figure 1(b) is discarded in Figure 1(a) and the whole sentence is analyzed as a single noun phrase. Figure 1(c) and (</context>
<context position="8237" citStr="Das and Petrov, 2011" startWordPosition="1384" endWordPosition="1387">agging shows over 97% of accuracy (Toutanova et al., 2003; Gim´enez and M`arquez, 2004; Tsuruoka and Tsujii, 2005; Shen et al., 2007; Manning, 2011). Due to the high accuracy of supervised approaches for POS tagging, it has been deemed that there is no room to improve the performance on POS tagging in supervised manner. Thus, recent studies on POS tagging focus on semi-supervised (Spoustov´a et al., 2009; Subramanya et al., 2010; Søgaard, 2011) or unsupervised approaches (Haghighi and Klein, 2006; Goldwater and Griffiths, 2007; Johnson, 2007; Graca et al., 2009; Berg-Kirkpatrick et al., 2010; Das and Petrov, 2011). Most previous studies on POS tagging have focused on how to extract more linguistic features or how to adopt supervised or unsupervised approaches based on a single evaluation measure, accuracy. However, with a different viewpoint for errors on POS tagging, there is still some room to improve the performance of POS tagging for subsequent NLP tasks, even though the overall accuracy can not be much improved. In ordinary studies on POS tagging, costs of errors are equally assigned. However, with respect to the performance of NLP tasks relying on the result of POS tagging, errors should be treat</context>
</contexts>
<marker>Das, Petrov, 2011</marker>
<rawString>Dipanjan Das and Slav Petrov. 2011. Unsupervised Partof-Speech Tagging with Bilingual Graph-Based Projections. In Proceedings of the 49th Annual Meeting of the Association of Computational Linguistics. pp. 600–609.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Elkan</author>
</authors>
<title>The Foundations of Cost-Sensitive Learning.</title>
<date>2001</date>
<booktitle>In Proceedings of the Seventeenth International Joint Conference on Artificial Intelligence.</booktitle>
<pages>973--978</pages>
<contexts>
<context position="9111" citStr="Elkan, 2001" startWordPosition="1531" endWordPosition="1532">re is still some room to improve the performance of POS tagging for subsequent NLP tasks, even though the overall accuracy can not be much improved. In ordinary studies on POS tagging, costs of errors are equally assigned. However, with respect to the performance of NLP tasks relying on the result of POS tagging, errors should be treated differently. In the machine learning community, cost sensitive learning has been studied to differentiate costs among errors. By adopting different misclassification costs for each type of errors, a classifier is optimized to achieve the lowest expected cost (Elkan, 2001; Cai and Hofmann, 2004; Zhou and Liu, 2006). 3 Error Analysis of Existing POS Tagger The effects of POS tagging errors to subsequent NLP tasks vary according to their type. Some errors are serious, while others are not. In this paper, the seriousness of tagging errors is determined by categorical structures of POS tags. Table 1 shows the Penn tree bank POS tags and their categories. There are five categories in this table: substantive, predicate, adverbial, determiner, and etc. Serious tagging errors are defined as misclassifications among the categories, while minor errors are defined as mis</context>
</contexts>
<marker>Elkan, 2001</marker>
<rawString>Charles Elkan. 2001. The Foundations of Cost-Sensitive Learning. In Proceedings of the Seventeenth International Joint Conference on Artificial Intelligence. pp. 973–978.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Lluis M`arquez</author>
</authors>
<title>SVMTool: A general POS tagger generator based on Support Vector Machines.</title>
<date>2004</date>
<booktitle>In Proceedings of the Fourth International Conference on Language Resources and Evaluation.</booktitle>
<pages>43--46</pages>
<marker>Gim´enez, M`arquez, 2004</marker>
<rawString>Jes´us Gim´enez and Lluis M`arquez. 2004. SVMTool: A general POS tagger generator based on Support Vector Machines. In Proceedings of the Fourth International Conference on Language Resources and Evaluation. pp. 43–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas T Griffiths</author>
</authors>
<title>A fully Bayesian Approach to Unsupervised Part-ofSpeech Tagging.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics.</booktitle>
<pages>744--751</pages>
<contexts>
<context position="8148" citStr="Goldwater and Griffiths, 2007" startWordPosition="1369" endWordPosition="1373">ary ambiguity. As a result of these efforts, the performance of state-of-the-art supervised POS tagging shows over 97% of accuracy (Toutanova et al., 2003; Gim´enez and M`arquez, 2004; Tsuruoka and Tsujii, 2005; Shen et al., 2007; Manning, 2011). Due to the high accuracy of supervised approaches for POS tagging, it has been deemed that there is no room to improve the performance on POS tagging in supervised manner. Thus, recent studies on POS tagging focus on semi-supervised (Spoustov´a et al., 2009; Subramanya et al., 2010; Søgaard, 2011) or unsupervised approaches (Haghighi and Klein, 2006; Goldwater and Griffiths, 2007; Johnson, 2007; Graca et al., 2009; Berg-Kirkpatrick et al., 2010; Das and Petrov, 2011). Most previous studies on POS tagging have focused on how to extract more linguistic features or how to adopt supervised or unsupervised approaches based on a single evaluation measure, accuracy. However, with a different viewpoint for errors on POS tagging, there is still some room to improve the performance of POS tagging for subsequent NLP tasks, even though the overall accuracy can not be much improved. In ordinary studies on POS tagging, costs of errors are equally assigned. However, with respect to </context>
</contexts>
<marker>Goldwater, Griffiths, 2007</marker>
<rawString>Sharon Goldwater and Thomas T. Griffiths. 2007. A fully Bayesian Approach to Unsupervised Part-ofSpeech Tagging. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics. pp. 744–751.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joao Graca</author>
<author>Kuzman Ganchev</author>
<author>Ben Taskar</author>
<author>Fernando Pereira</author>
</authors>
<title>Posterior vs Parameter Sparsity in Latent Variable Models.</title>
<date>2009</date>
<booktitle>In Advances in Neural Information Processing Systems 22.</booktitle>
<pages>664--672</pages>
<contexts>
<context position="8183" citStr="Graca et al., 2009" startWordPosition="1376" endWordPosition="1379">he performance of state-of-the-art supervised POS tagging shows over 97% of accuracy (Toutanova et al., 2003; Gim´enez and M`arquez, 2004; Tsuruoka and Tsujii, 2005; Shen et al., 2007; Manning, 2011). Due to the high accuracy of supervised approaches for POS tagging, it has been deemed that there is no room to improve the performance on POS tagging in supervised manner. Thus, recent studies on POS tagging focus on semi-supervised (Spoustov´a et al., 2009; Subramanya et al., 2010; Søgaard, 2011) or unsupervised approaches (Haghighi and Klein, 2006; Goldwater and Griffiths, 2007; Johnson, 2007; Graca et al., 2009; Berg-Kirkpatrick et al., 2010; Das and Petrov, 2011). Most previous studies on POS tagging have focused on how to extract more linguistic features or how to adopt supervised or unsupervised approaches based on a single evaluation measure, accuracy. However, with a different viewpoint for errors on POS tagging, there is still some room to improve the performance of POS tagging for subsequent NLP tasks, even though the overall accuracy can not be much improved. In ordinary studies on POS tagging, costs of errors are equally assigned. However, with respect to the performance of NLP tasks relyin</context>
</contexts>
<marker>Graca, Ganchev, Taskar, Pereira, 2009</marker>
<rawString>Joao Graca, Kuzman Ganchev, Ben Taskar, and Fernando Pereira. 2009. Posterior vs Parameter Sparsity in Latent Variable Models. In Advances in Neural Information Processing Systems 22. pp. 664–672.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Prototype-driven Learning for Sequence Models.</title>
<date>2006</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<pages>320--327</pages>
<contexts>
<context position="8117" citStr="Haghighi and Klein, 2006" startWordPosition="1365" endWordPosition="1368"> the problem of word boundary ambiguity. As a result of these efforts, the performance of state-of-the-art supervised POS tagging shows over 97% of accuracy (Toutanova et al., 2003; Gim´enez and M`arquez, 2004; Tsuruoka and Tsujii, 2005; Shen et al., 2007; Manning, 2011). Due to the high accuracy of supervised approaches for POS tagging, it has been deemed that there is no room to improve the performance on POS tagging in supervised manner. Thus, recent studies on POS tagging focus on semi-supervised (Spoustov´a et al., 2009; Subramanya et al., 2010; Søgaard, 2011) or unsupervised approaches (Haghighi and Klein, 2006; Goldwater and Griffiths, 2007; Johnson, 2007; Graca et al., 2009; Berg-Kirkpatrick et al., 2010; Das and Petrov, 2011). Most previous studies on POS tagging have focused on how to extract more linguistic features or how to adopt supervised or unsupervised approaches based on a single evaluation measure, accuracy. However, with a different viewpoint for errors on POS tagging, there is still some room to improve the performance of POS tagging for subsequent NLP tasks, even though the overall accuracy can not be much improved. In ordinary studies on POS tagging, costs of errors are equally assi</context>
</contexts>
<marker>Haghighi, Klein, 2006</marker>
<rawString>Aria Haghighi and Dan Klein. 2006. Prototype-driven Learning for Sequence Models. In Proceedings of the North American Chapter of the Association for Computational Linguistics. pp. 320–327.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Why doesn’t EM find good HMM POS-taggers?</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Meeting of the Conference on Empirical Methods in Natural Language Processing and the Conference on Computational Natural Language Learning.</booktitle>
<pages>296--305</pages>
<contexts>
<context position="8163" citStr="Johnson, 2007" startWordPosition="1374" endWordPosition="1375">hese efforts, the performance of state-of-the-art supervised POS tagging shows over 97% of accuracy (Toutanova et al., 2003; Gim´enez and M`arquez, 2004; Tsuruoka and Tsujii, 2005; Shen et al., 2007; Manning, 2011). Due to the high accuracy of supervised approaches for POS tagging, it has been deemed that there is no room to improve the performance on POS tagging in supervised manner. Thus, recent studies on POS tagging focus on semi-supervised (Spoustov´a et al., 2009; Subramanya et al., 2010; Søgaard, 2011) or unsupervised approaches (Haghighi and Klein, 2006; Goldwater and Griffiths, 2007; Johnson, 2007; Graca et al., 2009; Berg-Kirkpatrick et al., 2010; Das and Petrov, 2011). Most previous studies on POS tagging have focused on how to extract more linguistic features or how to adopt supervised or unsupervised approaches based on a single evaluation measure, accuracy. However, with a different viewpoint for errors on POS tagging, there is still some room to improve the performance of POS tagging for subsequent NLP tasks, even though the overall accuracy can not be much improved. In ordinary studies on POS tagging, costs of errors are equally assigned. However, with respect to the performance</context>
</contexts>
<marker>Johnson, 2007</marker>
<rawString>Mark Johnson. 2007. Why doesn’t EM find good HMM POS-taggers? In Proceedings of the 2007 Joint Meeting of the Conference on Empirical Methods in Natural Language Processing and the Conference on Computational Natural Language Learning. pp. 296–305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Kaoru Yamamoto</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Applying Conditional Random Fields to Japanese Morphological Analysis.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<pages>230--237</pages>
<contexts>
<context position="2084" citStr="Kudo et al., 2004" startWordPosition="315" endWordPosition="318">med entity recognition (NER), and text chunking. Since POS tagging is normally performed in the early step of NLP tasks, the errors in POS tagging are critical in that they affect subsequent steps and often lower the overall performance of NLP tasks. Previous studies on POS tagging have shown high performance with machine learning techniques (Ratnaparkhi, 1996; Brants, 2000; Lafferty et al., 2001). Among the types of machine learning approaches, supervised machine learning techniques were commonly used in early studies on POS tagging. With the characteristics of a language (Ratnaparkhi, 1996; Kudo et al., 2004) and informative features for POS tagging (Toutanova and Manning, 2000), the state-of-the-art supervised POS tagging achieves over 97% of accuracy (Shen et al., 2007; Manning, 2011). This performance is generally regarded as the maximum performance that can be achieved by supervised machine learning techniques. There have also been many studies on POS tagging with semi-supervised (Subramanya et al., 2010; Søgaard, 2011) or unsupervised machine learning methods (Berg-Kirkpatrick et al., 2010; Das and Petrov, 2011) recently. However, there still exists room to improve supervised POS tagging in t</context>
<context position="7426" citStr="Kudo et al. (2004)" startWordPosition="1255" endWordPosition="1258">. In this study, the features for rarely appearing words in a corpus are expanded to improve the overall performance. Following this direction, various studies have been proposed to extend informative features for POS tagging (Toutanova and Manning, 2000; Toutanova et al., 2003; Manning, 2011). In addition, various supervised methods such as HMMs and CRFs are widely applied to POS tagging. Lafferty et al. (2001) adopted CRFs to predict POS tags. The methods based on CRFs not only have all the advantages of the maximum entropy markov models but also resolve the wellknown problem of label bias. Kudo et al. (2004) modified CRFs for non-segmented languages like Japanese which have the problem of word boundary ambiguity. As a result of these efforts, the performance of state-of-the-art supervised POS tagging shows over 97% of accuracy (Toutanova et al., 2003; Gim´enez and M`arquez, 2004; Tsuruoka and Tsujii, 2005; Shen et al., 2007; Manning, 2011). Due to the high accuracy of supervised approaches for POS tagging, it has been deemed that there is no room to improve the performance on POS tagging in supervised manner. Thus, recent studies on POS tagging focus on semi-supervised (Spoustov´a et al., 2009; S</context>
</contexts>
<marker>Kudo, Yamamoto, Matsumoto, 2004</marker>
<rawString>Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto. 2004. Applying Conditional Random Fields to Japanese Morphological Analysis. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. pp. 230–237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
</authors>
<date>2005</date>
<note>CRF++: Yet another CRF toolkit. http://crfpp.sourceforge.net.</note>
<contexts>
<context position="22892" citStr="Kudo (2005)" startWordPosition="3972" endWordPosition="3973">s of 2,012 sentences and 47,377 words in section 20 of the WSJ. In order to represent chunks, an IOB model is used, where every word is tagged with a chunk label extended with B (the beginning of a chunk), I (inside a chunk), and O (outside a chunk). First, the POS information in test data are replaced to the result of our POS tagger. Then it is evaluated using trained chunking model. Since CRFs (Conditional Random Fields) has been shown near state-of-the-art performance in text chunking (Fei Sha and Fernando Pereira, 2003; Sun et al., 2008), we use CRF++, an open source CRF implementation by Kudo (2005), with default feature template and parameter settings of the package. For simplicity in the experiments, the values of S in Equation (2) and -y in Equation (3) are set to be 0.4 and 0.6 respectively which are same as the previous section. Table 6 gives the experimental results of text chunking according to the kinds of POS taggers including two previous works, CL-MSVM, and TLMSVM. Shen’s tagger and Manning’s tagger show nearly the same performance. They achieve an accuracy of 96.08% and around 93.9 F1-score. On the other hand, CL-MSVM achieves 96.13% accuracy and 94.00 F1-score. The accuracy </context>
</contexts>
<marker>Kudo, 2005</marker>
<rawString>Taku Kudo. 2005. CRF++: Yet another CRF toolkit. http://crfpp.sourceforge.net.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data.</title>
<date>2001</date>
<booktitle>In Proceedings of the Eighteenth International Conference on Machine Learning.</booktitle>
<pages>282--289</pages>
<contexts>
<context position="1866" citStr="Lafferty et al., 2001" startWordPosition="280" endWordPosition="283"> fewer serious errors help to improve the performance of subsequent NLP tasks. 1 Introduction Part-of-speech (POS) tagging is needed as a preprocessor for various natural language processing (NLP) tasks such as parsing, named entity recognition (NER), and text chunking. Since POS tagging is normally performed in the early step of NLP tasks, the errors in POS tagging are critical in that they affect subsequent steps and often lower the overall performance of NLP tasks. Previous studies on POS tagging have shown high performance with machine learning techniques (Ratnaparkhi, 1996; Brants, 2000; Lafferty et al., 2001). Among the types of machine learning approaches, supervised machine learning techniques were commonly used in early studies on POS tagging. With the characteristics of a language (Ratnaparkhi, 1996; Kudo et al., 2004) and informative features for POS tagging (Toutanova and Manning, 2000), the state-of-the-art supervised POS tagging achieves over 97% of accuracy (Shen et al., 2007; Manning, 2011). This performance is generally regarded as the maximum performance that can be achieved by supervised machine learning techniques. There have also been many studies on POS tagging with semi-supervised</context>
<context position="7223" citStr="Lafferty et al. (2001)" startWordPosition="1218" endWordPosition="1221">sed machine learning techniques are applied by using annotated corpora like the Wall Street Journal corpus (Marcus et al., 1994). For instance, Ratnaparkhi (1996) used a maximum entropy model for POS tagging. In this study, the features for rarely appearing words in a corpus are expanded to improve the overall performance. Following this direction, various studies have been proposed to extend informative features for POS tagging (Toutanova and Manning, 2000; Toutanova et al., 2003; Manning, 2011). In addition, various supervised methods such as HMMs and CRFs are widely applied to POS tagging. Lafferty et al. (2001) adopted CRFs to predict POS tags. The methods based on CRFs not only have all the advantages of the maximum entropy markov models but also resolve the wellknown problem of label bias. Kudo et al. (2004) modified CRFs for non-segmented languages like Japanese which have the problem of word boundary ambiguity. As a result of these efforts, the performance of state-of-the-art supervised POS tagging shows over 97% of accuracy (Toutanova et al., 2003; Gim´enez and M`arquez, 2004; Tsuruoka and Tsujii, 2005; Shen et al., 2007; Manning, 2011). Due to the high accuracy of supervised approaches for POS</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. In Proceedings of the Eighteenth International Conference on Machine Learning. pp. 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
</authors>
<title>Part-of-Speech Tagging from 97% to 100%: Is It Time for Some Linguistics?.</title>
<date>2011</date>
<booktitle>In Proceedings of the 12th International Conference on Intelligent Text Processing and Computational Linguistics.</booktitle>
<pages>171--189</pages>
<contexts>
<context position="2265" citStr="Manning, 2011" startWordPosition="346" endWordPosition="347">quent steps and often lower the overall performance of NLP tasks. Previous studies on POS tagging have shown high performance with machine learning techniques (Ratnaparkhi, 1996; Brants, 2000; Lafferty et al., 2001). Among the types of machine learning approaches, supervised machine learning techniques were commonly used in early studies on POS tagging. With the characteristics of a language (Ratnaparkhi, 1996; Kudo et al., 2004) and informative features for POS tagging (Toutanova and Manning, 2000), the state-of-the-art supervised POS tagging achieves over 97% of accuracy (Shen et al., 2007; Manning, 2011). This performance is generally regarded as the maximum performance that can be achieved by supervised machine learning techniques. There have also been many studies on POS tagging with semi-supervised (Subramanya et al., 2010; Søgaard, 2011) or unsupervised machine learning methods (Berg-Kirkpatrick et al., 2010; Das and Petrov, 2011) recently. However, there still exists room to improve supervised POS tagging in terms of error differentiation. It should be noted that not all errors are equally important in POS tagging. Let us consider the parse trees in Figure 1 as an example. In Figure 1(a)</context>
<context position="7102" citStr="Manning, 2011" startWordPosition="1198" endWordPosition="1200">Tag categories and POS tags in Penn Tree Bank tag set ing. In early studies, rich linguistic features and supervised machine learning techniques are applied by using annotated corpora like the Wall Street Journal corpus (Marcus et al., 1994). For instance, Ratnaparkhi (1996) used a maximum entropy model for POS tagging. In this study, the features for rarely appearing words in a corpus are expanded to improve the overall performance. Following this direction, various studies have been proposed to extend informative features for POS tagging (Toutanova and Manning, 2000; Toutanova et al., 2003; Manning, 2011). In addition, various supervised methods such as HMMs and CRFs are widely applied to POS tagging. Lafferty et al. (2001) adopted CRFs to predict POS tags. The methods based on CRFs not only have all the advantages of the maximum entropy markov models but also resolve the wellknown problem of label bias. Kudo et al. (2004) modified CRFs for non-segmented languages like Japanese which have the problem of word boundary ambiguity. As a result of these efforts, the performance of state-of-the-art supervised POS tagging shows over 97% of accuracy (Toutanova et al., 2003; Gim´enez and M`arquez, 2004</context>
<context position="10524" citStr="Manning, 2011" startWordPosition="1763" endWordPosition="1764">rors are treated as serious errors, while intra-category errors are treated as minor errors. Table 2 shows the distribution of inter-category and intra-category errors observed in section 22– 24 of the WSJ corpus (Marcus et al., 1994) that is tagged by the Stanford Log-linear Part-Of-Speech 1027 Predicted category Substantive Predicate Adverbial Determiner Etc Substantive Predicate Adverbial Determiner Etc 614 479 32 10 15 585 743 107 2 14 41 156 500 42 2 13 7 47 24 0 23 11 3 1 0 True category Table 2: The distribution of tagging errors on WSJ corpus by Stanford Part-Of-Speech Tagger. Tagger (Manning, 2011) (trained with WSJ sections 00–18). In this table, bold numbers denote intercategory errors while all other numbers show intracategory errors. The number of total errors is 3,471 out of 129,654 words. Among them, 1,881 errors (54.19%) are intra-category, while 1,590 of the errors (45.81%) are inter-category. If we can reduce these inter-category errors under the cost of minimally increasing intra-category errors, the tagging results would improve in quality. Generally in POS tagging, all tagging errors are regarded equally in importance. However, intercategory and intra-category errors should </context>
<context position="19973" citStr="Manning, 2011" startWordPosition="3498" endWordPosition="3499">A linear kernel is used for both SVMs. 5.2 Experimental Results CL-MSVM with δ = 0.4 shows the best overall performance on the development data where its error rate is as low as 2.71%. δ = 0.4 implies that the cost of intra-category errors is set to 40% of that of inter-category errors. The error rate of TL-MSVM is 2.69% when γ is 0.6. δ = 0.4 and γ = 0.6 are set in the all experiments below. Table 5 gives the comparison with the previous work and proposed methods on the test data. As can be seen from this table, the best performing algorithms achieve near 2.67% error rate (Shen et al., 2007; Manning, 2011). CL-MSVM and TL-MSVM Error # of Intra # of Inter (%) error error (Gim´enez and M`arquez, 1,995 1,692 2004) 2.84 (54.11%) (45.89%) (Tsuruoka and Tsujii, 2.85 - - 2005) (Shen et al., 2007) 2.67 1,856 1,612 (53.52%) (46.48%) (Manning, 2011) 2.68 1,881 1,590 (54.19%) (45.81%) CL-MSVM (δ = 0.4) 2.69 1,916 1,567 (55.01%) (44.99%) TL-MSVM (γ = 0.6) 2.68 1,904 1,574 (54.74%) (45.26%) Table 5: Comparison with the previous works achieve an error rate of 2.69% and 2.68% respectively. Although overall error rates of CL-MSVM and TL-MSVM are not improved compared to the previous state-of-the-art methods, t</context>
<context position="21957" citStr="Manning, 2011" startWordPosition="3808" endWordPosition="3809">e proposed loss functions do differentiate serious and minor POS tagging errors. 5.3 Chunking Experiments The task of chunking is to identify the non-recursive cores for various types of phrases. In chunking, the POS information is one of the most crucial aspects in identifying chunks. Especially inter-category POS errors seriously affect the performance of chunking because they are more likely to mislead the chunk compared to intra-category errors. Here, chunking experiments are performed with 1031 POS tagger Accuracy (%) Precision Recall F1-score (Shen et al., 2007) 96.08 94.03 93.75 93.89 (Manning, 2011) 96.08 94 93.8 93.9 CL-MSVM (S = 0.4) 96.13 94.1 93.9 94.00 TL-MSVM (-y = 0.6) 96.12 94.1 93.9 94.00 Table 6: The experimental results for chunking a data set provided for the CoNLL-2000 shared task. The training data contains 8,936 sentences with 211,727 words obtained from sections 15–18 of the WSJ. The test data consists of 2,012 sentences and 47,377 words in section 20 of the WSJ. In order to represent chunks, an IOB model is used, where every word is tagged with a chunk label extended with B (the beginning of a chunk), I (inside a chunk), and O (outside a chunk). First, the POS informatio</context>
</contexts>
<marker>Manning, 2011</marker>
<rawString>Christopher D. Manning. 2011. Part-of-Speech Tagging from 97% to 100%: Is It Time for Some Linguistics?. In Proceedings of the 12th International Conference on Intelligent Text Processing and Computational Linguistics. pp. 171–189.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuji Nakagawa</author>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Unknown Word Guessing and Part-of-Speech Tagging Using Support Vector Machines.</title>
<date>2001</date>
<booktitle>In Proceedings of the Sixth Natural Language Processing Pacific Rim Symposium.</booktitle>
<pages>325--331</pages>
<contexts>
<context position="19199" citStr="Nakagawa et al. (2001)" startWordPosition="3351" endWordPosition="3354">· ti+2 ti−2 · ti−1 · ti+1 · ti+2 Tag/Word ti−2·wi, ti−1·wi, ti+1·wi, ti+2·wi combination ti−1 · ti+1 · wi Prefix features prefixes of wi (up to length 9) Suffix features suffixes of wi (up to length 9) Lexical features whether wi contains capitals whether wi has a number whether wi has a hyphen whether wi is all capital whether wi starts with capital and locates at the middle of sentence Table 4: Feature template for experiments tures, word/tag combination features, prefix and suffix features as well as lexical features. The POS tags for words are obtained from a two-pass approach proposed by Nakagawa et al. (2001). In the experiments, two multiclass SVMs with the proposed loss functions are used. One is CL-MSVM with category loss and the other is TL-MSVM with tree loss. A linear kernel is used for both SVMs. 5.2 Experimental Results CL-MSVM with δ = 0.4 shows the best overall performance on the development data where its error rate is as low as 2.71%. δ = 0.4 implies that the cost of intra-category errors is set to 40% of that of inter-category errors. The error rate of TL-MSVM is 2.69% when γ is 0.6. δ = 0.4 and γ = 0.6 are set in the all experiments below. Table 5 gives the comparison with the previo</context>
</contexts>
<marker>Nakagawa, Kudo, Matsumoto, 2001</marker>
<rawString>Tetsuji Nakagawa, Taku Kudo, and Yuji Matsumoto. 2001. Unknown Word Guessing and Part-of-Speech Tagging Using Support Vector Machines. In Proceedings of the Sixth Natural Language Processing Pacific Rim Symposium. pp. 325–331.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A Maximum Entropy Model for Part-Of-Speech Tagging.</title>
<date>1996</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<pages>133--142</pages>
<contexts>
<context position="1828" citStr="Ratnaparkhi, 1996" startWordPosition="276" endWordPosition="277">esult on text chunking shows that fewer serious errors help to improve the performance of subsequent NLP tasks. 1 Introduction Part-of-speech (POS) tagging is needed as a preprocessor for various natural language processing (NLP) tasks such as parsing, named entity recognition (NER), and text chunking. Since POS tagging is normally performed in the early step of NLP tasks, the errors in POS tagging are critical in that they affect subsequent steps and often lower the overall performance of NLP tasks. Previous studies on POS tagging have shown high performance with machine learning techniques (Ratnaparkhi, 1996; Brants, 2000; Lafferty et al., 2001). Among the types of machine learning approaches, supervised machine learning techniques were commonly used in early studies on POS tagging. With the characteristics of a language (Ratnaparkhi, 1996; Kudo et al., 2004) and informative features for POS tagging (Toutanova and Manning, 2000), the state-of-the-art supervised POS tagging achieves over 97% of accuracy (Shen et al., 2007; Manning, 2011). This performance is generally regarded as the maximum performance that can be achieved by supervised machine learning techniques. There have also been many studi</context>
<context position="6763" citStr="Ratnaparkhi (1996)" startWordPosition="1143" endWordPosition="1145"> The POS tagging problem has generally been solved by machine learning methods for sequential label1026 Tag category POS tags Substantive NN, NNS, NNP, NNPS, CD, PRP, PRP$ Predicate VB, VBD, VBG, VBN, VBP, VBZ, MD, JJ, JJR, JJS Adverbial RB, RBR, RBS, RP, UH, EX, WP, WP$, WRB, CC, IN, TO Determiner DT, PDT, WDT Etc FW, SYM, POS, LS Table 1: Tag categories and POS tags in Penn Tree Bank tag set ing. In early studies, rich linguistic features and supervised machine learning techniques are applied by using annotated corpora like the Wall Street Journal corpus (Marcus et al., 1994). For instance, Ratnaparkhi (1996) used a maximum entropy model for POS tagging. In this study, the features for rarely appearing words in a corpus are expanded to improve the overall performance. Following this direction, various studies have been proposed to extend informative features for POS tagging (Toutanova and Manning, 2000; Toutanova et al., 2003; Manning, 2011). In addition, various supervised methods such as HMMs and CRFs are widely applied to POS tagging. Lafferty et al. (2001) adopted CRFs to predict POS tags. The methods based on CRFs not only have all the advantages of the maximum entropy markov models but also </context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A Maximum Entropy Model for Part-Of-Speech Tagging. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. pp. 133–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Sha</author>
<author>Fernando Pereira</author>
</authors>
<title>Shallow Parsing with Conditional Random Fields.</title>
<date>2003</date>
<booktitle>In Proceedings of the Human Language Technology and North American Chapter ofthe Association for Computational Linguistics.</booktitle>
<pages>213--220</pages>
<marker>Sha, Pereira, 2003</marker>
<rawString>Fei Sha and Fernando Pereira. 2003. Shallow Parsing with Conditional Random Fields. In Proceedings of the Human Language Technology and North American Chapter ofthe Association for Computational Linguistics. pp. 213–220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Giorgio Satta</author>
<author>Aravind K Joshi</author>
</authors>
<title>Guided Learning for Bidirectional Sequence Classification.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics.</booktitle>
<pages>760--767</pages>
<contexts>
<context position="2249" citStr="Shen et al., 2007" startWordPosition="342" endWordPosition="345">t they affect subsequent steps and often lower the overall performance of NLP tasks. Previous studies on POS tagging have shown high performance with machine learning techniques (Ratnaparkhi, 1996; Brants, 2000; Lafferty et al., 2001). Among the types of machine learning approaches, supervised machine learning techniques were commonly used in early studies on POS tagging. With the characteristics of a language (Ratnaparkhi, 1996; Kudo et al., 2004) and informative features for POS tagging (Toutanova and Manning, 2000), the state-of-the-art supervised POS tagging achieves over 97% of accuracy (Shen et al., 2007; Manning, 2011). This performance is generally regarded as the maximum performance that can be achieved by supervised machine learning techniques. There have also been many studies on POS tagging with semi-supervised (Subramanya et al., 2010; Søgaard, 2011) or unsupervised machine learning methods (Berg-Kirkpatrick et al., 2010; Das and Petrov, 2011) recently. However, there still exists room to improve supervised POS tagging in terms of error differentiation. It should be noted that not all errors are equally important in POS tagging. Let us consider the parse trees in Figure 1 as an example</context>
<context position="7748" citStr="Shen et al., 2007" startWordPosition="1304" endWordPosition="1307">ised methods such as HMMs and CRFs are widely applied to POS tagging. Lafferty et al. (2001) adopted CRFs to predict POS tags. The methods based on CRFs not only have all the advantages of the maximum entropy markov models but also resolve the wellknown problem of label bias. Kudo et al. (2004) modified CRFs for non-segmented languages like Japanese which have the problem of word boundary ambiguity. As a result of these efforts, the performance of state-of-the-art supervised POS tagging shows over 97% of accuracy (Toutanova et al., 2003; Gim´enez and M`arquez, 2004; Tsuruoka and Tsujii, 2005; Shen et al., 2007; Manning, 2011). Due to the high accuracy of supervised approaches for POS tagging, it has been deemed that there is no room to improve the performance on POS tagging in supervised manner. Thus, recent studies on POS tagging focus on semi-supervised (Spoustov´a et al., 2009; Subramanya et al., 2010; Søgaard, 2011) or unsupervised approaches (Haghighi and Klein, 2006; Goldwater and Griffiths, 2007; Johnson, 2007; Graca et al., 2009; Berg-Kirkpatrick et al., 2010; Das and Petrov, 2011). Most previous studies on POS tagging have focused on how to extract more linguistic features or how to adopt </context>
<context position="17594" citStr="Shen et al., 2007" startWordPosition="3066" endWordPosition="3069"> also presents the decision boundary of NN, but it is determined with the proposed loss function. In this figure, the margin violation is applied differently to inter-category (NN to VB) and intra-category (NN to NNS) errors. It results in reducing errors between NN and VB even if the errors between NN and NNS could be slightly increased. 5 Experiments 5.1 Experimental Setting Experiments are performed with a well-known standard data set, the Wall Street Journal (WSJ) corpus. The data is divided into training, development and test sets as in (Toutanova et al., 2003; Tsuruoka and Tsujii, 2005; Shen et al., 2007). Table 3 shows some simple statistics of these data sets. As shown in this table, training data contains 38,219 sentences with 912,344 words. In the development data set, there are 5,527 sentences with about 131,768 words, those in the test set are 5,462 sentences and 129,654 words. The development data set is used only to select S in Equation (2) and -y in Equation (3). Table 4 shows the feature set for our experiments. In this table, wi and ti denote the lexicon and POS tag for the i-th word in a sentence respectively. We use almost the same feature set as used in (Tsuruoka and Tsujii, 2005</context>
<context position="19957" citStr="Shen et al., 2007" startWordPosition="3494" endWordPosition="3497">VM with tree loss. A linear kernel is used for both SVMs. 5.2 Experimental Results CL-MSVM with δ = 0.4 shows the best overall performance on the development data where its error rate is as low as 2.71%. δ = 0.4 implies that the cost of intra-category errors is set to 40% of that of inter-category errors. The error rate of TL-MSVM is 2.69% when γ is 0.6. δ = 0.4 and γ = 0.6 are set in the all experiments below. Table 5 gives the comparison with the previous work and proposed methods on the test data. As can be seen from this table, the best performing algorithms achieve near 2.67% error rate (Shen et al., 2007; Manning, 2011). CL-MSVM and TL-MSVM Error # of Intra # of Inter (%) error error (Gim´enez and M`arquez, 1,995 1,692 2004) 2.84 (54.11%) (45.89%) (Tsuruoka and Tsujii, 2.85 - - 2005) (Shen et al., 2007) 2.67 1,856 1,612 (53.52%) (46.48%) (Manning, 2011) 2.68 1,881 1,590 (54.19%) (45.81%) CL-MSVM (δ = 0.4) 2.69 1,916 1,567 (55.01%) (44.99%) TL-MSVM (γ = 0.6) 2.68 1,904 1,574 (54.74%) (45.26%) Table 5: Comparison with the previous works achieve an error rate of 2.69% and 2.68% respectively. Although overall error rates of CL-MSVM and TL-MSVM are not improved compared to the previous state-of-th</context>
<context position="21917" citStr="Shen et al., 2007" startWordPosition="3800" endWordPosition="3803">e proposed methods which are trained with the proposed loss functions do differentiate serious and minor POS tagging errors. 5.3 Chunking Experiments The task of chunking is to identify the non-recursive cores for various types of phrases. In chunking, the POS information is one of the most crucial aspects in identifying chunks. Especially inter-category POS errors seriously affect the performance of chunking because they are more likely to mislead the chunk compared to intra-category errors. Here, chunking experiments are performed with 1031 POS tagger Accuracy (%) Precision Recall F1-score (Shen et al., 2007) 96.08 94.03 93.75 93.89 (Manning, 2011) 96.08 94 93.8 93.9 CL-MSVM (S = 0.4) 96.13 94.1 93.9 94.00 TL-MSVM (-y = 0.6) 96.12 94.1 93.9 94.00 Table 6: The experimental results for chunking a data set provided for the CoNLL-2000 shared task. The training data contains 8,936 sentences with 211,727 words obtained from sections 15–18 of the WSJ. The test data consists of 2,012 sentences and 47,377 words in section 20 of the WSJ. In order to represent chunks, an IOB model is used, where every word is tagged with a chunk label extended with B (the beginning of a chunk), I (inside a chunk), and O (out</context>
</contexts>
<marker>Shen, Satta, Joshi, 2007</marker>
<rawString>Libin Shen, Giorgio Satta, and Aravind K. Joshi 2007. Guided Learning for Bidirectional Sequence Classification. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics. pp. 760–767.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Søgaard</author>
</authors>
<title>Semisupervised condensed nearest neighbor for part-of-speech tagging.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association of Computational Linguistics.</booktitle>
<pages>48--52</pages>
<contexts>
<context position="2507" citStr="Søgaard, 2011" startWordPosition="382" endWordPosition="383">e learning approaches, supervised machine learning techniques were commonly used in early studies on POS tagging. With the characteristics of a language (Ratnaparkhi, 1996; Kudo et al., 2004) and informative features for POS tagging (Toutanova and Manning, 2000), the state-of-the-art supervised POS tagging achieves over 97% of accuracy (Shen et al., 2007; Manning, 2011). This performance is generally regarded as the maximum performance that can be achieved by supervised machine learning techniques. There have also been many studies on POS tagging with semi-supervised (Subramanya et al., 2010; Søgaard, 2011) or unsupervised machine learning methods (Berg-Kirkpatrick et al., 2010; Das and Petrov, 2011) recently. However, there still exists room to improve supervised POS tagging in terms of error differentiation. It should be noted that not all errors are equally important in POS tagging. Let us consider the parse trees in Figure 1 as an example. In Figure 1(a), the word “plans” is mistagged as a noun where it should be a verb. This error results in a wrong parse tree that is severely different from the correct tree shown in Figure 1(b). The verb phrase of the verb “plans” in Figure 1(b) is discard</context>
<context position="8064" citStr="Søgaard, 2011" startWordPosition="1359" endWordPosition="1360">egmented languages like Japanese which have the problem of word boundary ambiguity. As a result of these efforts, the performance of state-of-the-art supervised POS tagging shows over 97% of accuracy (Toutanova et al., 2003; Gim´enez and M`arquez, 2004; Tsuruoka and Tsujii, 2005; Shen et al., 2007; Manning, 2011). Due to the high accuracy of supervised approaches for POS tagging, it has been deemed that there is no room to improve the performance on POS tagging in supervised manner. Thus, recent studies on POS tagging focus on semi-supervised (Spoustov´a et al., 2009; Subramanya et al., 2010; Søgaard, 2011) or unsupervised approaches (Haghighi and Klein, 2006; Goldwater and Griffiths, 2007; Johnson, 2007; Graca et al., 2009; Berg-Kirkpatrick et al., 2010; Das and Petrov, 2011). Most previous studies on POS tagging have focused on how to extract more linguistic features or how to adopt supervised or unsupervised approaches based on a single evaluation measure, accuracy. However, with a different viewpoint for errors on POS tagging, there is still some room to improve the performance of POS tagging for subsequent NLP tasks, even though the overall accuracy can not be much improved. In ordinary stu</context>
</contexts>
<marker>Søgaard, 2011</marker>
<rawString>Anders Søgaard 2011. Semisupervised condensed nearest neighbor for part-of-speech tagging. In Proceedings of the 49th Annual Meeting of the Association of Computational Linguistics. pp. 48–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Drahomira “johanka” Spoustov`a</author>
<author>Jan Hajiˇc</author>
<author>Jan Raab</author>
<author>Miroslav Spousta</author>
</authors>
<title>Semi-supervised training for the averaged perceptron POS tagger.</title>
<date>2009</date>
<booktitle>In Proceedings of the European Chapter of the Association for Computational Linguistics.</booktitle>
<pages>763--771</pages>
<marker>Spoustov`a, Hajiˇc, Raab, Spousta, 2009</marker>
<rawString>Drahomira “johanka” Spoustov`a, Jan Hajiˇc, Jan Raab, and Miroslav Spousta 2009. Semi-supervised training for the averaged perceptron POS tagger. In Proceedings of the European Chapter of the Association for Computational Linguistics. pp. 763–771.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Amarnag Subramanya</author>
</authors>
<title>Slav Petrov and Fernando Pereira 2010. Efficient Graph-Based Semi-Supervised Learning of Structured Tagging Models.</title>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<pages>167--176</pages>
<marker>Subramanya, </marker>
<rawString>Amarnag Subramanya, Slav Petrov and Fernando Pereira 2010. Efficient Graph-Based Semi-Supervised Learning of Structured Tagging Models. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. pp. 167–176.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Xu Sun</author>
</authors>
<title>Louis-Philippe Morency, Daisuke Okanohara and Jun’ichi Tsujii 2008. Modeling Latent-Dynamic in Shallow Parsing: A Latent Conditional Model with Improved Inference.</title>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics.</booktitle>
<pages>841--848</pages>
<marker>Sun, </marker>
<rawString>Xu Sun, Louis-Philippe Morency, Daisuke Okanohara and Jun’ichi Tsujii 2008. Modeling Latent-Dynamic in Shallow Parsing: A Latent Conditional Model with Improved Inference. In Proceedings of the 22nd International Conference on Computational Linguistics. pp. 841–848.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-Rich Part-ofSpeech Tagging with a Cyclic Dependency Network.</title>
<date>2003</date>
<contexts>
<context position="7086" citStr="Toutanova et al., 2003" startWordPosition="1194" endWordPosition="1197">, SYM, POS, LS Table 1: Tag categories and POS tags in Penn Tree Bank tag set ing. In early studies, rich linguistic features and supervised machine learning techniques are applied by using annotated corpora like the Wall Street Journal corpus (Marcus et al., 1994). For instance, Ratnaparkhi (1996) used a maximum entropy model for POS tagging. In this study, the features for rarely appearing words in a corpus are expanded to improve the overall performance. Following this direction, various studies have been proposed to extend informative features for POS tagging (Toutanova and Manning, 2000; Toutanova et al., 2003; Manning, 2011). In addition, various supervised methods such as HMMs and CRFs are widely applied to POS tagging. Lafferty et al. (2001) adopted CRFs to predict POS tags. The methods based on CRFs not only have all the advantages of the maximum entropy markov models but also resolve the wellknown problem of label bias. Kudo et al. (2004) modified CRFs for non-segmented languages like Japanese which have the problem of word boundary ambiguity. As a result of these efforts, the performance of state-of-the-art supervised POS tagging shows over 97% of accuracy (Toutanova et al., 2003; Gim´enez an</context>
<context position="17547" citStr="Toutanova et al., 2003" startWordPosition="3058" endWordPosition="3061">the margin violation among all classes. Figure 3(b) also presents the decision boundary of NN, but it is determined with the proposed loss function. In this figure, the margin violation is applied differently to inter-category (NN to VB) and intra-category (NN to NNS) errors. It results in reducing errors between NN and VB even if the errors between NN and NNS could be slightly increased. 5 Experiments 5.1 Experimental Setting Experiments are performed with a well-known standard data set, the Wall Street Journal (WSJ) corpus. The data is divided into training, development and test sets as in (Toutanova et al., 2003; Tsuruoka and Tsujii, 2005; Shen et al., 2007). Table 3 shows some simple statistics of these data sets. As shown in this table, training data contains 38,219 sentences with 912,344 words. In the development data set, there are 5,527 sentences with about 131,768 words, those in the test set are 5,462 sentences and 129,654 words. The development data set is used only to select S in Equation (2) and -y in Equation (3). Table 4 shows the feature set for our experiments. In this table, wi and ti denote the lexicon and POS tag for the i-th word in a sentence respectively. We use almost the same fe</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-Rich Part-ofSpeech Tagging with a Cyclic Dependency Network.</rawString>
</citation>
<citation valid="false">
<booktitle>In Proceedings of the Human Language Technology and North American Chapter of the Association for Computational Linguistics.</booktitle>
<pages>252--259</pages>
<marker></marker>
<rawString>In Proceedings of the Human Language Technology and North American Chapter of the Association for Computational Linguistics. pp. 252–259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Christopher D Manning</author>
</authors>
<title>Enriching the Knowledge Sources Used in a Maximum Entropy Part-of-Speech Tagger.</title>
<date>2000</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<pages>63--70</pages>
<contexts>
<context position="2155" citStr="Toutanova and Manning, 2000" startWordPosition="326" endWordPosition="330">gging is normally performed in the early step of NLP tasks, the errors in POS tagging are critical in that they affect subsequent steps and often lower the overall performance of NLP tasks. Previous studies on POS tagging have shown high performance with machine learning techniques (Ratnaparkhi, 1996; Brants, 2000; Lafferty et al., 2001). Among the types of machine learning approaches, supervised machine learning techniques were commonly used in early studies on POS tagging. With the characteristics of a language (Ratnaparkhi, 1996; Kudo et al., 2004) and informative features for POS tagging (Toutanova and Manning, 2000), the state-of-the-art supervised POS tagging achieves over 97% of accuracy (Shen et al., 2007; Manning, 2011). This performance is generally regarded as the maximum performance that can be achieved by supervised machine learning techniques. There have also been many studies on POS tagging with semi-supervised (Subramanya et al., 2010; Søgaard, 2011) or unsupervised machine learning methods (Berg-Kirkpatrick et al., 2010; Das and Petrov, 2011) recently. However, there still exists room to improve supervised POS tagging in terms of error differentiation. It should be noted that not all errors a</context>
<context position="7062" citStr="Toutanova and Manning, 2000" startWordPosition="1190" endWordPosition="1193">eterminer DT, PDT, WDT Etc FW, SYM, POS, LS Table 1: Tag categories and POS tags in Penn Tree Bank tag set ing. In early studies, rich linguistic features and supervised machine learning techniques are applied by using annotated corpora like the Wall Street Journal corpus (Marcus et al., 1994). For instance, Ratnaparkhi (1996) used a maximum entropy model for POS tagging. In this study, the features for rarely appearing words in a corpus are expanded to improve the overall performance. Following this direction, various studies have been proposed to extend informative features for POS tagging (Toutanova and Manning, 2000; Toutanova et al., 2003; Manning, 2011). In addition, various supervised methods such as HMMs and CRFs are widely applied to POS tagging. Lafferty et al. (2001) adopted CRFs to predict POS tags. The methods based on CRFs not only have all the advantages of the maximum entropy markov models but also resolve the wellknown problem of label bias. Kudo et al. (2004) modified CRFs for non-segmented languages like Japanese which have the problem of word boundary ambiguity. As a result of these efforts, the performance of state-of-the-art supervised POS tagging shows over 97% of accuracy (Toutanova e</context>
</contexts>
<marker>Toutanova, Manning, 2000</marker>
<rawString>Kristina Toutanova and Christopher D. Manning. 2000. Enriching the Knowledge Sources Used in a Maximum Entropy Part-of-Speech Tagger. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. pp. 63–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ioannis Tsochantaridis</author>
<author>Thomas Hofmann</author>
<author>Thorsten Joachims</author>
<author>Yasemi Altun</author>
</authors>
<title>Support Vector Learning for Interdependent and Structured Output Spaces.</title>
<date>2004</date>
<booktitle>In Proceedings of the 21st International Conference on Machine Learning.</booktitle>
<pages>104--111</pages>
<contexts>
<context position="5297" citStr="Tsochantaridis et al., 2004" startWordPosition="887" endWordPosition="890">rors. POS tagging is generally regarded as a classification task, and zero-one loss is commonly used in learning classifiers (Altun et al., 2003). Since zero-one loss considers all errors equally, it can not distinguish error types. Therefore, a new loss is required to incorporate different error types into the learning machines. This paper proposes two gradient loss functions to reflect differences among POS tagging errors. The functions assign relatively small cost to minor errors, while larger cost is given to serious errors. They are applied to learning multiclass support vector machines (Tsochantaridis et al., 2004) which is trained to minimize the serious errors. Overall accuracy of this SVM is not improved against the stateof-the-art POS tagger, but the serious errors are significantly reduced with the proposed method. The effect of the fewer serious errors is shown by applying it to the well-known NLP task of text chunking. Experimental results show that the proposed method achieves a higher F1-score compared to other POS taggers. The rest of the paper is organized as follows. Section 2 reviews the related studies on POS tagging. In Section 3, serious and minor errors are defined, and it is shown that</context>
<context position="13463" citStr="Tsochantaridis et al. (2004)" startWordPosition="2307" endWordPosition="2310">1 − ξi, ξi ≥ 0 ∀i, ∀k ∈ K \ yi, where φ(xi, yi) is a combined feature representation of xi and yi, and K is the set of classes. min w,C ξi, �l i=1 1 2 kEK min w,C ξi, �l i=1 1028 DT WDT POS ADVERBIAL PREDICATE FW POS SUBSTANTIVE PDT OTHERS DETERMINER NN NNS NOUN NNP NNPS CD VB PRP VBD PRONOUN VBG PRP$ VERB VBN VBP VBZ JJ ADJECT MD JJR JJS RB RBR ADVERB RBS RP UH EX WP WP$ CONJUNCTION WHWRB SYM CC IN TO LS Figure 2: A tree structure of POS tags. Since both binary and multiclass SVMs adopt a hinge loss, the errors between classes have the same cost. To assign different cost to different errors, Tsochantaridis et al. (2004) proposed an efficient way to adopt arbitrary loss function, L(yi, yj) which returns zero if yi = yj, otherwise L(yi, yj) &gt; 0. Then, the hinge loss �i is re-scaled with the inverse of the additional loss between two classes. By scaling slack variables with the inverse loss, margin violation with high loss L(yi, yj) is more severely restricted than that with low loss. Thus, the optimization problem with L(yi, yj) is given as 1 X 2 kEK with constraints (wyi - O(xi, yi)) − (wk - O(xi, k)) &gt; 1 − �i L(yi, k), �i &gt; 0 di, dkEK\yi, With the Lagrange multiplier α, the optimization problem in Equation (</context>
</contexts>
<marker>Tsochantaridis, Hofmann, Joachims, Altun, 2004</marker>
<rawString>Ioannis Tsochantaridis, Thomas Hofmann, Thorsten Joachims, and Yasemi Altun. 2004. Support Vector Learning for Interdependent and Structured Output Spaces. In Proceedings of the 21st International Conference on Machine Learning. pp. 104–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshimasa Tsuruoka</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Bidirectional Inference with the Easiest-First Strategy for Tagging Sequence Data.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<pages>467--474</pages>
<contexts>
<context position="7729" citStr="Tsuruoka and Tsujii, 2005" startWordPosition="1300" endWordPosition="1303">In addition, various supervised methods such as HMMs and CRFs are widely applied to POS tagging. Lafferty et al. (2001) adopted CRFs to predict POS tags. The methods based on CRFs not only have all the advantages of the maximum entropy markov models but also resolve the wellknown problem of label bias. Kudo et al. (2004) modified CRFs for non-segmented languages like Japanese which have the problem of word boundary ambiguity. As a result of these efforts, the performance of state-of-the-art supervised POS tagging shows over 97% of accuracy (Toutanova et al., 2003; Gim´enez and M`arquez, 2004; Tsuruoka and Tsujii, 2005; Shen et al., 2007; Manning, 2011). Due to the high accuracy of supervised approaches for POS tagging, it has been deemed that there is no room to improve the performance on POS tagging in supervised manner. Thus, recent studies on POS tagging focus on semi-supervised (Spoustov´a et al., 2009; Subramanya et al., 2010; Søgaard, 2011) or unsupervised approaches (Haghighi and Klein, 2006; Goldwater and Griffiths, 2007; Johnson, 2007; Graca et al., 2009; Berg-Kirkpatrick et al., 2010; Das and Petrov, 2011). Most previous studies on POS tagging have focused on how to extract more linguistic featur</context>
<context position="17574" citStr="Tsuruoka and Tsujii, 2005" startWordPosition="3062" endWordPosition="3065">ng all classes. Figure 3(b) also presents the decision boundary of NN, but it is determined with the proposed loss function. In this figure, the margin violation is applied differently to inter-category (NN to VB) and intra-category (NN to NNS) errors. It results in reducing errors between NN and VB even if the errors between NN and NNS could be slightly increased. 5 Experiments 5.1 Experimental Setting Experiments are performed with a well-known standard data set, the Wall Street Journal (WSJ) corpus. The data is divided into training, development and test sets as in (Toutanova et al., 2003; Tsuruoka and Tsujii, 2005; Shen et al., 2007). Table 3 shows some simple statistics of these data sets. As shown in this table, training data contains 38,219 sentences with 912,344 words. In the development data set, there are 5,527 sentences with about 131,768 words, those in the test set are 5,462 sentences and 129,654 words. The development data set is used only to select S in Equation (2) and -y in Equation (3). Table 4 shows the feature set for our experiments. In this table, wi and ti denote the lexicon and POS tag for the i-th word in a sentence respectively. We use almost the same feature set as used in (Tsuru</context>
</contexts>
<marker>Tsuruoka, Tsujii, 2005</marker>
<rawString>Yoshimasa Tsuruoka and Jun’ichi Tsujii. 2005. Bidirectional Inference with the Easiest-First Strategy for Tagging Sequence Data. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. pp. 467–474.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a Large Annotated Corpus of English: The Penn Treebank.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<pages>313--330</pages>
<contexts>
<context position="6729" citStr="Marcus et al., 1994" startWordPosition="1137" endWordPosition="1140">aws some conclusions. 2 Related Work The POS tagging problem has generally been solved by machine learning methods for sequential label1026 Tag category POS tags Substantive NN, NNS, NNP, NNPS, CD, PRP, PRP$ Predicate VB, VBD, VBG, VBN, VBP, VBZ, MD, JJ, JJR, JJS Adverbial RB, RBR, RBS, RP, UH, EX, WP, WP$, WRB, CC, IN, TO Determiner DT, PDT, WDT Etc FW, SYM, POS, LS Table 1: Tag categories and POS tags in Penn Tree Bank tag set ing. In early studies, rich linguistic features and supervised machine learning techniques are applied by using annotated corpora like the Wall Street Journal corpus (Marcus et al., 1994). For instance, Ratnaparkhi (1996) used a maximum entropy model for POS tagging. In this study, the features for rarely appearing words in a corpus are expanded to improve the overall performance. Following this direction, various studies have been proposed to extend informative features for POS tagging (Toutanova and Manning, 2000; Toutanova et al., 2003; Manning, 2011). In addition, various supervised methods such as HMMs and CRFs are widely applied to POS tagging. Lafferty et al. (2001) adopted CRFs to predict POS tags. The methods based on CRFs not only have all the advantages of the maxim</context>
<context position="10144" citStr="Marcus et al., 1994" startWordPosition="1696" endWordPosition="1699">in this table: substantive, predicate, adverbial, determiner, and etc. Serious tagging errors are defined as misclassifications among the categories, while minor errors are defined as misclassifications within a category. This definition follows the fact that POS tags in the same category form similar syntax structures in a sentence (Zhao and Marcus, 2009). That is, inter-category errors are treated as serious errors, while intra-category errors are treated as minor errors. Table 2 shows the distribution of inter-category and intra-category errors observed in section 22– 24 of the WSJ corpus (Marcus et al., 1994) that is tagged by the Stanford Log-linear Part-Of-Speech 1027 Predicted category Substantive Predicate Adverbial Determiner Etc Substantive Predicate Adverbial Determiner Etc 614 479 32 10 15 585 743 107 2 14 41 156 500 42 2 13 7 47 24 0 23 11 3 1 0 True category Table 2: The distribution of tagging errors on WSJ corpus by Stanford Part-Of-Speech Tagger. Tagger (Manning, 2011) (trained with WSJ sections 00–18). In this table, bold numbers denote intercategory errors while all other numbers show intracategory errors. The number of total errors is 3,471 out of 129,654 words. Among them, 1,881 e</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1994</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1994. Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics, Vol. 19, No.2 . pp. 313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiuye Zhao</author>
<author>Mitch Marcus</author>
</authors>
<title>A Simple Unsupervised Learner for POS Disambiguation Rules Given Only a Minimal Lexicon.</title>
<date>2009</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<pages>688--697</pages>
<contexts>
<context position="9882" citStr="Zhao and Marcus, 2009" startWordPosition="1656" endWordPosition="1659">according to their type. Some errors are serious, while others are not. In this paper, the seriousness of tagging errors is determined by categorical structures of POS tags. Table 1 shows the Penn tree bank POS tags and their categories. There are five categories in this table: substantive, predicate, adverbial, determiner, and etc. Serious tagging errors are defined as misclassifications among the categories, while minor errors are defined as misclassifications within a category. This definition follows the fact that POS tags in the same category form similar syntax structures in a sentence (Zhao and Marcus, 2009). That is, inter-category errors are treated as serious errors, while intra-category errors are treated as minor errors. Table 2 shows the distribution of inter-category and intra-category errors observed in section 22– 24 of the WSJ corpus (Marcus et al., 1994) that is tagged by the Stanford Log-linear Part-Of-Speech 1027 Predicted category Substantive Predicate Adverbial Determiner Etc Substantive Predicate Adverbial Determiner Etc 614 479 32 10 15 585 743 107 2 14 41 156 500 42 2 13 7 47 24 0 23 11 3 1 0 True category Table 2: The distribution of tagging errors on WSJ corpus by Stanford Par</context>
</contexts>
<marker>Zhao, Marcus, 2009</marker>
<rawString>Qiuye Zhao and Mitch Marcus. 2009. A Simple Unsupervised Learner for POS Disambiguation Rules Given Only a Minimal Lexicon. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. pp. 688–697.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhi-Hua Zhou</author>
<author>Xu-Ying Liu</author>
</authors>
<title>On Multi-Class Cost-Sensitive Learning.</title>
<date>2006</date>
<booktitle>In Proceedings of the AAAI Conference on Artificial Intelligence.</booktitle>
<pages>567--572</pages>
<contexts>
<context position="9155" citStr="Zhou and Liu, 2006" startWordPosition="1537" endWordPosition="1540"> performance of POS tagging for subsequent NLP tasks, even though the overall accuracy can not be much improved. In ordinary studies on POS tagging, costs of errors are equally assigned. However, with respect to the performance of NLP tasks relying on the result of POS tagging, errors should be treated differently. In the machine learning community, cost sensitive learning has been studied to differentiate costs among errors. By adopting different misclassification costs for each type of errors, a classifier is optimized to achieve the lowest expected cost (Elkan, 2001; Cai and Hofmann, 2004; Zhou and Liu, 2006). 3 Error Analysis of Existing POS Tagger The effects of POS tagging errors to subsequent NLP tasks vary according to their type. Some errors are serious, while others are not. In this paper, the seriousness of tagging errors is determined by categorical structures of POS tags. Table 1 shows the Penn tree bank POS tags and their categories. There are five categories in this table: substantive, predicate, adverbial, determiner, and etc. Serious tagging errors are defined as misclassifications among the categories, while minor errors are defined as misclassifications within a category. This defi</context>
</contexts>
<marker>Zhou, Liu, 2006</marker>
<rawString>Zhi-Hua Zhou and Xu-Ying Liu 2006. On Multi-Class Cost-Sensitive Learning. In Proceedings of the AAAI Conference on Artificial Intelligence. pp. 567–572.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>