<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001985">
<title confidence="0.997007">
Comparing and Combining Generative and Posterior Probability Models:
Some Advances in Sentence Boundary Detection in Speech
</title>
<note confidence="0.5151725">
Yang Liu Andreas Stolcke Elizabeth Shriberg Mary Harper
ICSI and Purdue University SRI and ICSI Purdue University
</note>
<email confidence="0.941047">
yangl@icsi.berkeley.edu stolcke,ees@speech.sri.com harper@ecn.purdue.edu
</email>
<sectionHeader confidence="0.996809" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999989611111111">
We compare and contrast two different models for
detecting sentence-like units in continuous speech.
The first approach uses hidden Markov sequence
models based on N-grams and maximum likeli-
hood estimation, and employs model interpolation
to combine different representations of the data.
The second approach models the posterior proba-
bilities of the target classes; it is discriminative and
integrates multiple knowledge sources in the max-
imum entropy (maxent) framework. Both models
combine lexical, syntactic, and prosodic informa-
tion. We develop a technique for integrating pre-
trained probability models into the maxent frame-
work, and show that this approach can improve
on an HMM-based state-of-the-art system for the
sentence-boundary detection task. An even more
substantial improvement is obtained by combining
the posterior probabilities of the two systems.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999874359375">
Sentence boundary detection is a problem that has
received limited attention in the text-based com-
putational linguistics community (Schmid, 2000;
Palmer and Hearst, 1994; Reynar and Ratnaparkhi,
1997), but which has recently acquired renewed im-
portance through an effort by the DARPA EARS
program (DARPA Information Processing Technol-
ogy Office, 2003) to improve automatic speech tran-
scription technology. Since standard speech recog-
nizers output an unstructured stream of words, im-
proving transcription means not only that word ac-
curacy must be improved, but also that commonly
used structural features such as sentence boundaries
need to be recognized. The task is thus fundamen-
tally based on both acoustic and textual (via auto-
matic word recognition) information. From a com-
putational linguistics point of view, sentence units
are crucial and assumed in most of the further pro-
cessing steps that one would want to apply to such
output: tagging and parsing, information extraction,
and summarization, among others.
Sentence segmentation from speech is a difficult
problem. The best systems benchmarked in a re-
cent government-administered evaluation yield er-
ror rates between 30% and 50%, depending on the
genre of speech processed (measured as the num-
ber of missed and inserted sentence boundaries as
a percentage of true sentence boundaries). Because
of the difficulty of the task, which leaves plenty of
room for improvement, its relevance to real-world
applications, and the range of potential knowledge
sources to be modeled (acoustics and text-based,
lower- and higher-level), this is an interesting chal-
lenge problem for statistical and computational ap-
proaches.
All of the systems participating in the recent
DARPA RT-03F Metadata Extraction evaluation
(National Institute of Standards and Technology,
2003) were based on a hidden Markov model frame-
work, in which word/tag sequences are modeled by
N-gram language models (LMs). Additional fea-
tures (mostly reflecting speech prosody) are mod-
eled as observation likelihoods attached to the N-
gram states of the HMM (Shriberg et al., 2000). The
HMM is a generative modeling approach, since it
describes a stochastic process with hidden variables
(the locations of sentence boundaries) that produces
the observable data. The segmentation is inferred
by comparing the likelihoods of different boundary
hypotheses.
While the HMM approach is computationally ef-
ficient and (as described later) provides a convenient
way for modularizing the knowledge sources, it has
two main drawbacks: First, the standard training
methods for HMMs maximize the joint probability
of observed and hidden events, as opposed to the
posterior probability of the correct hidden variable
assignment given the observations. The latter is a
criterion more closely related to classification error.
Second, the N-gram LM underlying the HMM tran-
sition model makes it difficult to use features that
are highly correlated (such as word and POS labels)
without greatly increasing the number of model pa-
rameters; this in turn would make robust estimation
</bodyText>
<figureCaption confidence="0.999764">
Figure 1: Diagram of the sentence segmentation task.
</figureCaption>
<bodyText confidence="0.997968285714286">
difficult.
In this paper, we describe our effort to overcome
these shortcomings by 1) replacing the generative
model with one that estimates the posterior proba-
bilities directly, and 2) using the maximum entropy
(maxent) framework to estimate conditional distri-
butions, giving us a more principled way to com-
bine a large number of overlapping features. Both
techniques have been used previously for traditional
NLP tasks, but they are not straightforward to ap-
ply in our case because of the diverse nature of the
knowledge sources used in sentence segmentation.
We describe the techniques we developed to work
around these difficulties, and compare classification
accuracy of the old and new approach on different
genres of speech. We also investigate how word
recognition error affects that comparison. Finally,
we show that a simple combination of the two ap-
proaches turns out to be highly effective in improv-
ing the best previous results obtained on a bench-
mark task.
</bodyText>
<sectionHeader confidence="0.919387" genericHeader="method">
2 The Sentence Segmentation Task
</sectionHeader>
<bodyText confidence="0.999960058823529">
The sentence boundary detection problem is de-
picted in Figure 1 in the source-channel framework.
The speaker intends to say something, chooses the
word string, and imposes prosodic cues (duration,
emphasis, intonation, etc). This signal goes through
the speech production channel to generate an acous-
tic signal. A speech recognizer determines the most
likely word string given this signal. To detect pos-
sible sentence boundaries in the recognized word
string, prosodic features are extracted from the sig-
nal, and combined with textual cues obtained from
the word string. At issue in this paper is the final
box in the diagram: how to model and combine the
available knowledge sources to find the most accu-
rate hypotheses.
Note that this problem differs from the sen-
tence boundary detection problem for written text in
the natural language processing literature (Schmid,
2000; Palmer and Hearst, 1994; Reynar and Rat-
naparkhi, 1997). Here we are dealing with spo-
ken language, therefore there is no punctuation in-
formation, the words are not capitalized, and the
transcripts from the recognition output are errorful.
This lack of textual cues is partly compensated by
prosodic information (timing, pitch, and energy pat-
terns) conveyed by speech. Also note that in spon-
taneous conversational speech “sentence” is not al-
ways a straightforward notion. For our purposes we
use the definition of a “sentence-like unit”, or SU,
as defined by the LDC for labeling and evaluation
purposes (Strassel, 2003).
The training data has SU boundaries marked by
annotators, based on both the recorded speech and
its transcription. In testing, a system has to recover
both the words and the locations of sentence bound-
aries, denoted by (W, E) = w1 e1 w2 ... wiei ... wn
where W represents the strings of word tokens and
Ethe inter-word boundary events (sentence bound-
ary or no boundary) .
The system output is scored by first finding a min-
imum edit distance alignment between the hypothe-
sized word string and the reference, and then com-
paring the aligned event labels. The SU error rate is
defined as the total number of deleted or inserted SU
boundary events, divided by the number of true SU
boundaries.1 For diagnostic purposes a secondary
evaluation condition allows use of the correct word
transcripts. This condition allows us to study the
segmentation task without the confounding effect of
speech recognition errors, using perfect lexical in-
formation.
</bodyText>
<sectionHeader confidence="0.997624" genericHeader="method">
3 Features and Knowledge Sources
</sectionHeader>
<bodyText confidence="0.9959634375">
Words and sentence boundaries are mutually con-
strained via syntactic structure. Therefore, the word
identities themselves (from automatic recognition
or human transcripts) constitute a primary knowl-
edge source for the sentence segmentation task. We
also make use of various automatic taggers that map
the word sequence to other representations. The
TnT tagger (Brants, 2000) is used to obtain part-of-
speech (POS) tags. A TBL chunker trained on Wall
Street Journal corpus (Ngai and Florian, 2001) maps
each word to an associated chunk tag, encoding
chunk type and relative word position (beginning of
an NP, inside a VP, etc.). The tagged versions of
the word stream are provided to allow generaliza-
tions based on syntactic structure and to smooth out
possibly undertrained word-based probability esti-
</bodyText>
<footnote confidence="0.86644775">
1This is the same as simple per-event classification accu-
racy, except that the denominator counts only the “marked”
events, thereby yielding error rates that are much higher than
if one uses all potential boundary locations.
</footnote>
<figure confidence="0.9970975">
fusion of[]
knowledge[]
souces[]
sentence boundary[]
hypothesis[]
word string[]
prosody❑
prosodic[]
features[]
channel❑
speech[]
recognizero
word string[]
syntax, semantics,[]
word selection,[]
puntuation[]
impose prosody❑
idea❑
signal[]
prosodic feature[]
extraction[]
textual feature[]
word, POS,[]
classes[]
</figure>
<bodyText confidence="0.99998165">
mates. For the same reasons we also generate word
class labels that are automatically induced from bi-
gram word distributions (Brown et al., 1992).
To model the prosodic structure of sentence
boundaries, we extract several hundred features
around each word boundary. These are based on the
acoustic alignments produced by a speech recog-
nizer (or forced alignments of the true words when
given). The features capture duration, pitch, and
energy patterns associated with the word bound-
aries. Informative features include the pause du-
ration at the boundary, the difference in pitch be-
fore and after the boundary, and so on. A cru-
cial aspect of many of these features is that they
are highly correlated (e.g., by being derived from
the same raw measurements via different normaliza-
tions), real-valued (not discrete), and possibly unde-
fined (e.g., unvoiced speech regions have no pitch).
These properties make prosodic features difficult to
model directly in either of the approaches we are ex-
amining in the paper. Hence, we have resorted to a
modular approach: the information from prosodic
features is modeled separately by a decision tree
classifier that outputs posterior probability estimates
P(eijfi), whereeiis the boundary event afterwi,
andfiis the prosodic feature vector associated with
the word boundary. Conveniently, this approach
also permits us to include some non-prosodic fea-
tures that are highly relevant for the task, but not
otherwise represented, such as whether a speaker
(turn) change occurred at the location in question.2
A practical issue that greatly influences model de-
sign is that not all information sources are avail-
able uniformly for all training data. For example,
prosodic modeling assumes acoustic data; whereas,
word-based models can be trained on text-only data,
which is usually available in much larger quantities.
This poses a problem for approaches that model all
relevant information jointly and is another strong
motivation for modular approaches.
</bodyText>
<sectionHeader confidence="0.999687" genericHeader="method">
4 The Models
</sectionHeader>
<subsectionHeader confidence="0.998402">
4.1 Hidden Markov Model for Segmentation
</subsectionHeader>
<bodyText confidence="0.933885181818182">
Our baseline model, and the one that forms the ba-
sis of much of the prior work on acoustic sentence
segmentation (Shriberg et al., 2000; Gotoh and Re-
nals, 2000; Christensen, 2001; Kim and Woodland,
2001), is a hidden Markov model. The states of
the model correspond to wordswiand following
2Here we are glossing over some details on prosodic mod-
eling that are orthogonal to the discussion in this paper. For
example, instead of simple decision trees we actually use en-
semble bagging to reduce the variance of the classifier (Liu et
al., 2004).
</bodyText>
<figureCaption confidence="0.99076475">
Figure 2: The graphical model for the SU detection
problem. Only one word+event is depicted in each state,
but in a model based on N-grams the previousN—1
tokens would condition the transition to the next state.
</figureCaption>
<bodyText confidence="0.999915555555556">
event labels ei. The observations associated with
the states are the words, as well as other (mainly
prosodic) features fi. Figure 2 shows a graphi-
cal model representation of the variables involved.
Note that the words appear in both the states and the
observations, such that the word stream constrains
the possible hidden states to matching words; the
ambiguity in the task stems entirely from the choice
of events.
</bodyText>
<subsectionHeader confidence="0.691545">
4.1.1 Classification
</subsectionHeader>
<bodyText confidence="0.999986571428571">
Standard algorithms are available to extract the most
probable state (and thus event) sequence given a set
of observations. The error metric is based on clas-
sification of individual word boundaries. Therefore,
rather than finding the highest probability sequence
of events, we identify the events with highest poste-
rior individually at each boundaryi:
</bodyText>
<equation confidence="0.9584605">
ei=argmaxP(ei jW, F) (1)
ei
</equation>
<bodyText confidence="0.9999172">
whereWandFare the words and features for
the entire test sequence, respectively. The individ-
ual event posteriors are obtained by applying the
forward-backward algorithm for HMMs (Rabiner
and Juang, 1986).
</bodyText>
<subsectionHeader confidence="0.62252">
4.1.2 Model Estimation
</subsectionHeader>
<bodyText confidence="0.996559333333333">
Training of the HMM is supervised since event-
labeled data is available. There are two sets of pa-
rameters to estimate. The state transition proba-
bilities are estimated using a hidden event N-gram
LM (Stolcke and Shriberg, 1996). The LM is
obtained with standard N-gram estimation meth-
ods from data that contains the word+event tags in
sequence:w1,e1,w2,...en—1,wn. The resulting
LM can then compute the required HMM transition
</bodyText>
<equation confidence="0.998677583333333">
Oi Oi+1
W F
i i W F
i+1 i+1
W E
i i
W E
i+1 i+1
probabilities as3
P(wieilw1e1:::wi—1ei-1)=
P(wilw1e1 ::: wi—1ei-1) x
P(eilw1e1 ::: wi—1ei-1wi)
</equation>
<bodyText confidence="0.999982882352941">
The N-gram estimator maximizes the joint
word+event sequence likelihoodP(W;E)on the
training data (modulo smoothing), and does not
guarantee that the correct event posteriors needed
for classification according to Equation (1) are
maximized.
The second set of HMM parameters are the ob-
servation likelihoods P (fi l ei; wi) . Instead of train-
ing a likelihood model we make use of the prosodic
classifiers described in Section 3. We have at our
disposal decision trees that estimate P(eil fi). If
we further assume that prosodic features are inde-
pendent of words given the event type (a reasonable
simplification if features are chosen appropriately),
observation likelihoods may be obtained by
SinceP(fi)is constant we can ignore it when car-
rying out the maximization (1).
</bodyText>
<subsubsectionHeader confidence="0.720885">
4.1.3 Knowledge Combination
</subsubsectionHeader>
<bodyText confidence="0.999894681818182">
The HMM structure makes strong independence as-
sumptions: (1) that features depend only on the cur-
rent state (and in practice, as we saw, only on the
event label) and (2) that each word+event label de-
pends only on the lastN—1tokens. In return, we
get a computationally efficient structure that allows
information from the entire sequenceW;Fto in-
form the posterior probabilities needed for classifi-
cation, via the forward-backward algorithm.
More problematic in practice is the integration
of multiple word-level features, such as POS tags
and chunker output. Theoretically, all tags could
simply be included in the hidden state representa-
tion to allow joint modeling of words, tags, and
events. However, this would drastically increase the
size of the state space, making robust model estima-
tion with standard N-gram techniques difficult. A
method that works well in practice is linear inter-
polation, whereby the conditional probability esti-
mates of various models are simply averaged, thus
reducing variance. In our case, we obtain good re-
sults by interpolating a word-N-gram model with
</bodyText>
<footnote confidence="0.941488">
3To utilize word+event contexts of length greater than one
we have to employ HMMs of order 2 or greater, or equivalently,
make the entire word+event N-gram be the state.
</footnote>
<bodyText confidence="0.998327875">
one based on automatically induced word classes
(Brown et al., 1992).
Similarly, we can interpolate LMs trained from
different corpora. This is usually more effective
than pooling the training data because it allows con-
trol over the contributions of the different sources.
For example, we have a small corpus of training data
labeled precisely to the LDC’s SU specifications,
but a much larger (130M word) corpus of standard
broadcast new transcripts with punctuation, from
which an approximate version of SUs could be in-
ferred. The larger corpus should get a larger weight
on account of its size, but a lower weight given the
mismatch of the SU labels. By tuning the interpola-
tion weight of the two LMs empirically (using held-
out data) the right compromise was found.
</bodyText>
<subsectionHeader confidence="0.985654">
4.2 Maxent Posterior Probability Model
</subsectionHeader>
<bodyText confidence="0.999979157894737">
As observed, HMM training does not maximize the
posterior probabilities of the correct labels. This
mismatch between training and use of the model
as a classifier would not arise if the model directly
estimated the posterior boundary label probabilities
P(eilW;F). A second problem with HMMs is that
the underlying N-gram sequence model does not
cope well with multiple representations (features) of
the word sequence (words, POS, etc.) short ofbuild-
ing a joint model of all variables. This type of sit-
uation is well-suited to a maximum entropy formu-
lation (Berger et al., 1996), which allows condition-
ing features to apply simultaneously, and therefore
gives greater freedom in choosing representations.
Another desirable characteristic of maxent models
is that they do not split the data recursively to condi-
tion their probability estimates, which makes them
more robust than decision trees when training data
is limited.
</bodyText>
<subsectionHeader confidence="0.774102">
4.2.1 Model Formulation and Training
</subsectionHeader>
<bodyText confidence="0.999832666666667">
We built a posterior probability model for sentence
boundary classification in the maxent framework.
Such a model takes the familiar exponential form4
</bodyText>
<equation confidence="0.9360465">
P(elW; Za( F) = 1 F) ePk Akgk(e;W;F) (3)
W;
whereZ,\(W;F)is the normalization term:
Za(W;F)=Xe&apos; ePkAkgk(e&apos;;W;F) (4)
</equation>
<bodyText confidence="0.999758">
The functionsgk(e;W;F)are indicator functions
corresponding to (complex) features defined over
</bodyText>
<footnote confidence="0.860945">
4We omit the indexifromehere since the “current” event
is meant in all cases.
</footnote>
<equation confidence="0.9992545">
P(filwi;ei)=P(eilfi)
P(ei)P(fi) (2)
</equation>
<bodyText confidence="0.990777947368421">
events, words, and prosodic features. For example,
one such feature function might be:
J 1 : if wi = uhhuh and e = SU
g(e, W, F)=l 0:otherwise
The maxent model is estimated by finding the pa-
rametersAksuch that the expected values of the var-
ious feature functionsEP[gk(e&apos;,W,F)]match the
empirical averages in the training data. It can be
shown that the resulting model has maximal entropy
among all the distributions satisfying these expec-
tation constraints. At the same time, the parame-
ters so chosen maximize the conditional likelihood
f1iP(eiIW,F)over the training data, subject to the
constraints of the exponential form given by Equa-
tion (3).5 The conditional likelihood is closely re-
lated to the individual event posteriors used for clas-
sification, meaning that this type of model explicitly
optimizes discrimination of correct from incorrect
labels.
</bodyText>
<subsectionHeader confidence="0.964813">
4.2.2 Choice of Features
</subsectionHeader>
<bodyText confidence="0.994370380952381">
Even though the mathematical formulation gives us
the freedom to use features that are overlapping or
otherwise dependent, we still have to choose a sub-
set that is informative and parsimonious, so as to
give good generalization and robust parameter es-
timates. Various feature selection algorithms for
maxent models have been proposed, e.g., (Berger et
al., 1996). However, since computational efficiency
was not an issue in our experiments, we included all
features that corresponded to information available
to our baseline approach, as listed below. We did
eliminate features that were triggered only once in
the training set to improve robustness and to avoid
overconstraining the model.
•Word N-grams. We use combinations
of preceding and following words to en-
code the word context of the event, e.g.,
&lt;wi&gt;, &lt;wi+1&gt;, &lt;wi, wi+1&gt;, &lt;wi-1, wi&gt;,
&lt;wi-2, wi-1, wi&gt;, and &lt;wi, wi+1, wi+2&gt;,
where wi refers to the word before the bound-
ary of interest.
</bodyText>
<listItem confidence="0.979261333333333">
• POS N-grams. POS tags are the same as used
for the HMM approach. The features capturing
POS context are similar to those based on word
tokens.
•Chunker tags. These are used similarly to POS
and word features, except we use tags encoding
</listItem>
<footnote confidence="0.760776333333333">
5In our experiments we used the L-BFGS parameter estima-
tion method, with Gaussian-prior smoothing (Chen and Rosen-
feld, 1999) to avoid overfitting.
</footnote>
<bodyText confidence="0.985111527272727">
chunk type (NP, VP, etc.) and word position
within the chunk (beginning versus inside).6
•Word classes. These are similar to N-gram pat-
terns but over automatically induced classes.
•Turn flags. Since speaker change often marks
an SU boundary, we use this binary feature.
Note that in the HMM approach this feature
had to be grouped with the prosodic features
and handled by the decision tree. In the max-
ent approach we can use it separately.
•Prosody. As we described earlier, decision tree
classifiers are used to generate the posterior
probabilitiesp(eiIfi). Since the maxent classi-
fier is most conveniently used with binary fea-
tures, we encode the prosodic posteriors into
several binary features via thresholding. Equa-
tion (3) shows that the presence of each fea-
ture in a maxent model has a monotonic effect
on the final probability (raising or lowering it
by a constant factor eAk sk ). This suggests en-
coding the decision tree posteriors in a cumu-
lative fashion through a series of binary fea-
tures, for example, p &gt; 0. 1, p &gt; 0. 3, p &gt; 0. 5,
p &gt; 0.7, p &gt; 0.9. This representation is also
more robust to mismatch between the posterior
probability in training and test set, since small
changes in the posterior value affect at most
one feature.
Note that the maxent framework does allow the
use of real-valued feature functions, but pre-
liminary experiments have shown no gain com-
pared to the binary features constructed as de-
scribed above. Still, this is a topic for future
research.
•Auxiliary LM. As mentioned earlier, additional
text-only language model training data is of-
ten available. In the HMM model we incor-
porated auxiliary LMs by interpolation, which
is not possible here since there is no LM per
se, but rather N-gram features. However, we
can use the same trick as we used for prosodic
features. A word-only HMM is used to esti-
mate posterior event probabilities according to
the auxiliary LM, and these posteriors are then
thresholded to yield binary features.
•Combined features. To date we have not fully
investigated compound features that combine
different knowledge sources and are able to
model the interaction between them explicitly.
6Chunker features were only used for broadcast news
data, due to the poor chunking performance on conversational
speech.
We only included a limited set of such features,
for example, a combination of the decision tree
hypothesis and POS contexts.
</bodyText>
<subsectionHeader confidence="0.997914">
4.3 Differences Between IIMM and Maxent
</subsectionHeader>
<bodyText confidence="0.999957642857143">
We have already discussed the differences between
the two approaches regarding the training objective
function (joint likelihood versus conditional likeli-
hood) and with respect to the handling of depen-
dent word features (model interpolation versus in-
tegrated modeling via maxent). On both counts the
maxent classifier should be superior to the HMM.
However, the maxent approach also has some the-
oretical disadvantages compared to the HMM by
design. One obvious shortcoming is that some in-
formation gets lost in the thresholding that converts
posterior probabilities from the prosodic model and
the auxiliary LM into binary features.
A more qualitative limitation of the maxent
model is that it only uses local evidence (the sur-
rounding word context and the local prosodic fea-
tures). In that respect, the maxent model resem-
bles the conditional probability model at the in-
dividual HMM states. The HMM as a whole,
however, through the forward-backward procedure,
propagates evidence from all parts of the observa-
tion sequence to any given decision point. Variants
such as the conditional Markov model (CMM) com-
bine sequence modeling with posterior probability
(e.g., maxent) modeling, but it has been shown that
CMM’s are still structurally inferior to HMMs be-
cause they only propagate evidence forward in time,
not backwards (Klein and Manning, 2002).
</bodyText>
<sectionHeader confidence="0.998963" genericHeader="evaluation">
5 Results and Discussion
</sectionHeader>
<subsectionHeader confidence="0.978443">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.9991056">
Experiments comparing the two modeling ap-
proaches were conducted on two corpora: broad-
cast news (BN) and conversational telephone speech
(CTS). BN and CTS differ in genre and speaking
style. These differences are reflected in the fre-
quency of SU boundaries: about 14% of inter-word
boundaries are SUs in CTS, compared to roughly
8% in BN.
The corpora are annotated by LDC according to
the guidelines of (Strassel, 2003). Training and test
data are those used in the DARPA Rich Transcrip-
tion Fall 2003 evaluation.7 For CTS, there is about
40 hours of conversational data from the Switch-
board corpus for training and 6 hours (72 conversa-
tions) for testing. The BN data has about 20 hours
</bodyText>
<footnote confidence="0.994666">
7We used both the development set and the evaluation set
as the test set in this paper, in order to have a larger test set to
make the results more meaningful.
</footnote>
<table confidence="0.9986448">
HMM Maxent Combined
BN REF 48.72 48.61 46.79
STT 55.37 56.51 54.35
CTS REF 31.51 30.66 29.30
STT 42.97 43.02 41.88
</table>
<tableCaption confidence="0.992514333333333">
Table 1: SU detection results (error rate in %) using
maxent and HMM individually and in combination on
BN and CTS.
</tableCaption>
<bodyText confidence="0.999960421052632">
of broadcast news shows in the training set and 3
hours (6 shows) in the test set. The SU detection
task is evaluated on both the reference transcriptions
(REF) and speech recognition outputs (STT). The
speech recognition output is obtained from the SRI
recognizer (Stolcke et al., 2003).
System performance is evaluated using the offi-
cial NIST evaluation tools,8 which implement the
metric described earlier. In our experiments, we
compare how the two approaches perform individ-
ually and in combination. The combined classifier
is obtained by simply averaging the posterior esti-
mates from the two models, and then picking the
event type with the highest probability at each posi-
tion.
We also investigate other experimental factors,
such as the impact of the speech recognition errors,
the impact of genre, and the contribution of text ver-
sus prosodic information in each model.
</bodyText>
<subsectionHeader confidence="0.99872">
5.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999935272727273">
Table 1 shows SU detection results for BN and
CTS, using both reference transcriptions and speech
recognition output, using the HMM and the max-
ent approach individually and in combination. The
maxent approach slightly outperforms the HMM ap-
proach when evaluating on the reference transcripts,
and the combination of the two approaches achieves
the best performance for all tasks (significant at
p&lt;0.05using the sign test on the reference tran-
scription condition, mixed results on using recogni-
tion output).
</bodyText>
<subsectionHeader confidence="0.687297">
5.2.1 BN vs. CTS
</subsectionHeader>
<bodyText confidence="0.9993569">
The detection error rate on CTS is lower than on
BN. This may be due to the metric used for per-
formance. Detection error rate is measured as the
percentage of errors per reference SU. The number
of SUs in CTS is much larger than for BN, making
the relative error rate lower for the conversational
speech task. Notice also from Table 1 that maxent
yields more gain on CTS than on BN (for the refer-
ence transcription condition on both corpora). One
possible reason for this is that we have more train-
</bodyText>
<footnote confidence="0.963221">
8http://www.nist.gov/speech/tests/rt/rt2003/fall/
</footnote>
<table confidence="0.9995508">
Del Ins Total
BN HMM 28.48 20.24 48.72
Maxent 32.06 16.54 48.61
CTS HMM 17.19 14.32 31.51
Maxent 19.97 10.69 30.66
</table>
<tableCaption confidence="0.991443333333333">
Table 2: Error rates for the two approaches on reference
transcriptions. Performance is shown in deletion, inser-
tion, and total error rate (%).
</tableCaption>
<table confidence="0.9996082">
BN CTS
HMM Textual 67.48 38.92
Textual + prosody 48.72 31.51
Maxent Textual 63.56 36.32
Textual + prosody 48.61 30.66
</table>
<tableCaption confidence="0.992205">
Table 3: SU detection error rate (%) using different
knowledge sources, for BN and CTS, evaluated on the
reference transcription.
</tableCaption>
<bodyText confidence="0.9538915">
ing data and thus less of a sparse data problem for
CTS.
</bodyText>
<subsubsectionHeader confidence="0.482229">
5.2.2 Error Type Analysis
</subsubsectionHeader>
<bodyText confidence="0.999842333333333">
Table 2 shows error rates for the HMM and the max-
ent approaches in the reference condition. Due to
the reduced dependence on the prosody model, the
errors made in the maxent approach are different
from the HMM approach. There are more deletion
errors and fewer insertion errors, since the prosody
model tends to overgenerate SU hypotheses. The
different error patterns suggest that we can effec-
tively combine the system output from the two ap-
proaches. As shown in the Table 1, the combination
of maxent and HMM consistently yields the best
performance.
</bodyText>
<subsectionHeader confidence="0.972649">
5.2.3 Contribution of Knowledge Sources
</subsectionHeader>
<bodyText confidence="0.999981944444444">
Table 3 shows SU detection results for the two ap-
proaches, using textual information only, as well as
in combination with the prosody model (which are
the same results as shown in Table 1). We only re-
port the results on the reference transcription con-
dition, in order to not confound the comparison by
word recognition errors.
The superior results for text-only classification
are consistent with the maxent model’s ability to
combine overlapping word-level features in a prin-
cipled way. However, the HMM largely catches
up once prosodic information is added. This can
be attributed to the loss-less integration of prosodic
posteriors in the HMM, as well as the fact that in
the HMM, each boundary decision is affected by
prosodic information throughout the data; whereas,
the maxent model only uses the prosodic features at
the boundary to be classified.
</bodyText>
<subsectionHeader confidence="0.895992">
5.2.4 Effect of Recognition Errors
</subsectionHeader>
<bodyText confidence="0.999972142857143">
We observe in Table 1 that there is a large increase in
error rate when evaluating on the speech recognition
output. This happens in part because word informa-
tion is inaccurate in the recognition output, thus im-
pacting the LMs and lexical features. The prosody
model is also affected, since the alignment of incor-
rect words to the speech is imperfect, thereby affect-
ing the prosodic feature extraction. However, the
prosody model is more robust to recognition errors
than the LMs, due to its lesser dependence on word
identity. The degradation on CTS is larger than on
BN. This can easily be explained by the difference
in word error rates, 22.9% on CTS and 12.1% on
BN.
The maxent system degrades more than then
HMM system when errorful recognition output is
used. In light of the previous section, this makes
sense: most of the improvement of the maxent
model comes from better lexical feature modeling.
But these are exactly the features that are most de-
teriorated by faulty recognition output.
</bodyText>
<sectionHeader confidence="0.998933" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.99999215">
We have described two different approaches for
modeling and integration of diverse knowledge
sources for automatic sentence segmentation from
speech: a state-of-the-art approach based on
HMMs, and an alternative approach based on pos-
terior probability estimation via maximum entropy.
To achieve competitive performance with the max-
ent model we devised a cumulative binary coding
scheme to map posterior estimates from auxiliary
submodels into features for the maxent model.
The two approaches have complementary
strengths and weaknesses that were reflected in the
results, consistent with the findings for text-based
NLP tasks (Klein and Manning, 2002). The maxent
model showed much better accuracy than the HMM
with lexical information, and a smaller win after
combination with prosodic features. The HMM
made more effective use of prosodic information
and degraded less with errorful word recognition.
A interpolation of posterior probabilities from the
two systems achieved 2-7% relative error reduction
compared to the baseline (significant atp&lt;0.05
for the reference transcription condition). The
results were consistent for two different genres of
speech.
In future work we hope to determine how the in-
dividual qualitative differences of the two models
(estimation methods, model structure, etc.) con-
tribute to the observed differences in results. To
improve results overall, we plan to explore features
that combine multiple knowledge sources, as well
as approaches that model recognition uncertainty in
order to mitigate the effects of word errors. We also
plan to investigate using a conditional random field
(CRF) model. CRFs combine the advantages of
both the HMM and the maxent approaches, being
a discriminatively trained model that can incorpo-
rate overlapping features (the maxent advantages),
while also modeling sequence dependencies (an ad-
vantage of HMMs) (Lafferty et al., 2001).
</bodyText>
<sectionHeader confidence="0.999392" genericHeader="acknowledgments">
7 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999985363636364">
The authors gratefully thank Le Zhang for his guid-
ance in applying the maximum entropy approach
to this task. This research has been supported
by DARPA under contract MDA972-02-C-0038,
NSF-STIMULATE under IRI-9619921, NSF BCS-
9980054, and NASA under NCC 2-1256. Distri-
bution is unlimited. Any opinions expressed in this
paper are those of the authors and do not necessarily
reflect the views of DARPA, NSF, or NASA. Part of
this work was carried out while the last author was
on leave from Purdue University and at NSF.
</bodyText>
<sectionHeader confidence="0.999449" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999760105263158">
A. L. Berger, S. A. Della Pietra, and V. J. Della
Pietra. 1996. A maximum entropy approach to
natural language processing. Computational Lin-
guistics, 22:39–72.
T. Brants. 2000. TnT—a statistical part-of-speech
tagger. In Proc. of the Sixth Applied NLP, pages
224–231.
P. F. Brown, V. J. Della Pietra, P. V. DeSouza, J. C.
Lai, and R. L. Mercer. 1992. Class-based n-gram
models of natural language. Computational Lin-
guistics, 18:467–479.
S. Chen and R. Rosenfeld. 1999. A Gaussian prior
for smoothing maximum entropy models. Tech-
nical report, Carnegie Mellon University.
H. Christensen. 2001. Punctuation annotation us-
ing statistical prosody models. In ISCA Work-
shop on Prosody in Speech Recognition and Un-
derstanding.
DARPA Information Processing Technol-
ogy Office. 2003. Effective, afford-
able, reusable speech-to-text (EARS).
http://www.darpa.mil/ipto/programs/ears/.
Y. Gotoh and S. Renals. 2000. Sentence bound-
ary detection in broadcast speech transcripts. In
ISCA Workshop: Automatic Speech Recognition:
Challenges for the new Millennium ASR-2000,
pages 228–235.
J. Kim and P. C. Woodland. 2001. The use of
prosody in a combined system for punctuation
generation and speech recognition. In Proc. of
Eurospeech 2001, pages 2757–2760.
D. Klein and C. Manning. 2002. Conditional struc-
ture versus conditional estimation in NLP mod-
els. In Proc. ofEMNLP 2002, pages 9–16.
J. Lafferty, A. McCallum, and F. Pereira. 2001.
Conditional random field: Probabilistic models
for segmenting and labeling sequence data. In
Prof. ofICML 2001, pages 282–289.
Y. Liu, E. Shriberg, A. Stolcke, and M. Harper.
2004. Using machine learning to cope with im-
balanced classes in natural speech: Evidence
from sentence boundary and disfluency detection.
In Proc. of ICSLP 2004 (To Appear).
National Institute of Standards and Technol-
ogy. 2003. RT-03F workshop agenda and
presentations. http://www.nist.gov/speech/tests/
rt/rt2003/fall/presentations/, November.
G. Ngai and R. Florian. 2001. Transformation-
based learning in the fast lane. In Proc. ofNAACL
2001, pages 40–47, June.
D. D. Palmer and M. A. Hearst. 1994. Adaptive
sentence boundary disambiguation. In Proc. of
the Fourth Applied NLP, pages 78–83.
L. R. Rabiner and B. H. Juang. 1986. An introduc-
tion to hidden Markov models. IEEE ASSP Mag-
azine, 3(1):4–16, January.
J. Reynar and A. Ratnaparkhi. 1997. A maximum
entropy approach to identifying sentence bound-
aries. In Proc. of the Fifth Applied NLP, pages
16–19.
H. Schmid. 2000. Unsupervised learning of pe-
riod disambiguation for tokenisation. University
of Stuttgart, Internal Report.
E. Shriberg, A. Stolcke, D. H. Tur, and G. Tur.
2000. Prosody-based automatic segmentation of
speech into sentences and topics. Speech Com-
munication, 32(1-2):127–154.
A. Stolcke and E. Shriberg. 1996. Automatic lin-
guistic segmentation of conversational speech. In
Proc. ofICSLP 1996, pages 1005–1008.
A. Stolcke, H. Franco, and R. Gadde et al.
2003. Speech-to-text research at SRI-ICSI-UW.
http://www.nist.gov/speech/tests/rt/rt2003/spring/
presentations/index.htm.
S. Strassel, 2003. Simple Metadata Annotation
Specification V5.0. Linguistic Data Consortium.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.656817">
<title confidence="0.9850425">Comparing and Combining Generative and Posterior Probability Some Advances in Sentence Boundary Detection in Speech</title>
<author confidence="0.998113">Yang Liu Andreas Stolcke Elizabeth Shriberg Mary Harper</author>
<affiliation confidence="0.686783">and Purdue University and ICSI</affiliation>
<email confidence="0.98937">yangl@icsi.berkeley.edustolcke,ees@speech.sri.comharper@ecn.purdue.edu</email>
<abstract confidence="0.999727578947368">We compare and contrast two different models for detecting sentence-like units in continuous speech. The first approach uses hidden Markov sequence models based on N-grams and maximum likelihood estimation, and employs model interpolation to combine different representations of the data. The second approach models the posterior probabilities of the target classes; it is discriminative and integrates multiple knowledge sources in the maximum entropy (maxent) framework. Both models combine lexical, syntactic, and prosodic information. We develop a technique for integrating pretrained probability models into the maxent framework, and show that this approach can improve on an HMM-based state-of-the-art system for the sentence-boundary detection task. An even more substantial improvement is obtained by combining the posterior probabilities of the two systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A L Berger</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<pages>22--39</pages>
<contexts>
<context position="16996" citStr="Berger et al., 1996" startWordPosition="2644" endWordPosition="2647">d. 4.2 Maxent Posterior Probability Model As observed, HMM training does not maximize the posterior probabilities of the correct labels. This mismatch between training and use of the model as a classifier would not arise if the model directly estimated the posterior boundary label probabilities P(eilW;F). A second problem with HMMs is that the underlying N-gram sequence model does not cope well with multiple representations (features) of the word sequence (words, POS, etc.) short ofbuilding a joint model of all variables. This type of situation is well-suited to a maximum entropy formulation (Berger et al., 1996), which allows conditioning features to apply simultaneously, and therefore gives greater freedom in choosing representations. Another desirable characteristic of maxent models is that they do not split the data recursively to condition their probability estimates, which makes them more robust than decision trees when training data is limited. 4.2.1 Model Formulation and Training We built a posterior probability model for sentence boundary classification in the maxent framework. Such a model takes the familiar exponential form4 P(elW; Za( F) = 1 F) ePk Akgk(e;W;F) (3) W; whereZ,\(W;F)is the no</context>
<context position="19098" citStr="Berger et al., 1996" startWordPosition="2960" endWordPosition="2963"> given by Equation (3).5 The conditional likelihood is closely related to the individual event posteriors used for classification, meaning that this type of model explicitly optimizes discrimination of correct from incorrect labels. 4.2.2 Choice of Features Even though the mathematical formulation gives us the freedom to use features that are overlapping or otherwise dependent, we still have to choose a subset that is informative and parsimonious, so as to give good generalization and robust parameter estimates. Various feature selection algorithms for maxent models have been proposed, e.g., (Berger et al., 1996). However, since computational efficiency was not an issue in our experiments, we included all features that corresponded to information available to our baseline approach, as listed below. We did eliminate features that were triggered only once in the training set to improve robustness and to avoid overconstraining the model. •Word N-grams. We use combinations of preceding and following words to encode the word context of the event, e.g., &lt;wi&gt;, &lt;wi+1&gt;, &lt;wi, wi+1&gt;, &lt;wi-1, wi&gt;, &lt;wi-2, wi-1, wi&gt;, and &lt;wi, wi+1, wi+2&gt;, where wi refers to the word before the boundary of interest. • POS N-grams. PO</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22:39–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Brants</author>
</authors>
<title>TnT—a statistical part-of-speech tagger.</title>
<date>2000</date>
<booktitle>In Proc. of the Sixth Applied NLP,</booktitle>
<pages>224--231</pages>
<contexts>
<context position="8179" citStr="Brants, 2000" startWordPosition="1255" endWordPosition="1256">condition allows use of the correct word transcripts. This condition allows us to study the segmentation task without the confounding effect of speech recognition errors, using perfect lexical information. 3 Features and Knowledge Sources Words and sentence boundaries are mutually constrained via syntactic structure. Therefore, the word identities themselves (from automatic recognition or human transcripts) constitute a primary knowledge source for the sentence segmentation task. We also make use of various automatic taggers that map the word sequence to other representations. The TnT tagger (Brants, 2000) is used to obtain part-ofspeech (POS) tags. A TBL chunker trained on Wall Street Journal corpus (Ngai and Florian, 2001) maps each word to an associated chunk tag, encoding chunk type and relative word position (beginning of an NP, inside a VP, etc.). The tagged versions of the word stream are provided to allow generalizations based on syntactic structure and to smooth out possibly undertrained word-based probability esti1This is the same as simple per-event classification accuracy, except that the denominator counts only the “marked” events, thereby yielding error rates that are much higher </context>
</contexts>
<marker>Brants, 2000</marker>
<rawString>T. Brants. 2000. TnT—a statistical part-of-speech tagger. In Proc. of the Sixth Applied NLP, pages 224–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>V J Della Pietra</author>
<author>P V DeSouza</author>
<author>J C Lai</author>
<author>R L Mercer</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<pages>18--467</pages>
<contexts>
<context position="9286" citStr="Brown et al., 1992" startWordPosition="1416" endWordPosition="1419"> except that the denominator counts only the “marked” events, thereby yielding error rates that are much higher than if one uses all potential boundary locations. fusion of[] knowledge[] souces[] sentence boundary[] hypothesis[] word string[] prosody❑ prosodic[] features[] channel❑ speech[] recognizero word string[] syntax, semantics,[] word selection,[] puntuation[] impose prosody❑ idea❑ signal[] prosodic feature[] extraction[] textual feature[] word, POS,[] classes[] mates. For the same reasons we also generate word class labels that are automatically induced from bigram word distributions (Brown et al., 1992). To model the prosodic structure of sentence boundaries, we extract several hundred features around each word boundary. These are based on the acoustic alignments produced by a speech recognizer (or forced alignments of the true words when given). The features capture duration, pitch, and energy patterns associated with the word boundaries. Informative features include the pause duration at the boundary, the difference in pitch before and after the boundary, and so on. A crucial aspect of many of these features is that they are highly correlated (e.g., by being derived from the same raw measu</context>
<context position="15678" citStr="Brown et al., 1992" startWordPosition="2429" endWordPosition="2432">s would drastically increase the size of the state space, making robust model estimation with standard N-gram techniques difficult. A method that works well in practice is linear interpolation, whereby the conditional probability estimates of various models are simply averaged, thus reducing variance. In our case, we obtain good results by interpolating a word-N-gram model with 3To utilize word+event contexts of length greater than one we have to employ HMMs of order 2 or greater, or equivalently, make the entire word+event N-gram be the state. one based on automatically induced word classes (Brown et al., 1992). Similarly, we can interpolate LMs trained from different corpora. This is usually more effective than pooling the training data because it allows control over the contributions of the different sources. For example, we have a small corpus of training data labeled precisely to the LDC’s SU specifications, but a much larger (130M word) corpus of standard broadcast new transcripts with punctuation, from which an approximate version of SUs could be inferred. The larger corpus should get a larger weight on account of its size, but a lower weight given the mismatch of the SU labels. By tuning the </context>
</contexts>
<marker>Brown, Pietra, DeSouza, Lai, Mercer, 1992</marker>
<rawString>P. F. Brown, V. J. Della Pietra, P. V. DeSouza, J. C. Lai, and R. L. Mercer. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18:467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Chen</author>
<author>R Rosenfeld</author>
</authors>
<title>A Gaussian prior for smoothing maximum entropy models.</title>
<date>1999</date>
<tech>Technical report,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="20044" citStr="Chen and Rosenfeld, 1999" startWordPosition="3115" endWordPosition="3119"> model. •Word N-grams. We use combinations of preceding and following words to encode the word context of the event, e.g., &lt;wi&gt;, &lt;wi+1&gt;, &lt;wi, wi+1&gt;, &lt;wi-1, wi&gt;, &lt;wi-2, wi-1, wi&gt;, and &lt;wi, wi+1, wi+2&gt;, where wi refers to the word before the boundary of interest. • POS N-grams. POS tags are the same as used for the HMM approach. The features capturing POS context are similar to those based on word tokens. •Chunker tags. These are used similarly to POS and word features, except we use tags encoding 5In our experiments we used the L-BFGS parameter estimation method, with Gaussian-prior smoothing (Chen and Rosenfeld, 1999) to avoid overfitting. chunk type (NP, VP, etc.) and word position within the chunk (beginning versus inside).6 •Word classes. These are similar to N-gram patterns but over automatically induced classes. •Turn flags. Since speaker change often marks an SU boundary, we use this binary feature. Note that in the HMM approach this feature had to be grouped with the prosodic features and handled by the decision tree. In the maxent approach we can use it separately. •Prosody. As we described earlier, decision tree classifiers are used to generate the posterior probabilitiesp(eiIfi). Since the maxent</context>
</contexts>
<marker>Chen, Rosenfeld, 1999</marker>
<rawString>S. Chen and R. Rosenfeld. 1999. A Gaussian prior for smoothing maximum entropy models. Technical report, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Christensen</author>
</authors>
<title>Punctuation annotation using statistical prosody models.</title>
<date>2001</date>
<booktitle>In ISCA Workshop on Prosody in Speech Recognition and Understanding.</booktitle>
<contexts>
<context position="11359" citStr="Christensen, 2001" startWordPosition="1743" endWordPosition="1744">hat not all information sources are available uniformly for all training data. For example, prosodic modeling assumes acoustic data; whereas, word-based models can be trained on text-only data, which is usually available in much larger quantities. This poses a problem for approaches that model all relevant information jointly and is another strong motivation for modular approaches. 4 The Models 4.1 Hidden Markov Model for Segmentation Our baseline model, and the one that forms the basis of much of the prior work on acoustic sentence segmentation (Shriberg et al., 2000; Gotoh and Renals, 2000; Christensen, 2001; Kim and Woodland, 2001), is a hidden Markov model. The states of the model correspond to wordswiand following 2Here we are glossing over some details on prosodic modeling that are orthogonal to the discussion in this paper. For example, instead of simple decision trees we actually use ensemble bagging to reduce the variance of the classifier (Liu et al., 2004). Figure 2: The graphical model for the SU detection problem. Only one word+event is depicted in each state, but in a model based on N-grams the previousN—1 tokens would condition the transition to the next state. event labels ei. The o</context>
</contexts>
<marker>Christensen, 2001</marker>
<rawString>H. Christensen. 2001. Punctuation annotation using statistical prosody models. In ISCA Workshop on Prosody in Speech Recognition and Understanding.</rawString>
</citation>
<citation valid="true">
<date>2003</date>
<booktitle>DARPA Information Processing Technology Office.</booktitle>
<note>Effective, affordable, reusable speech-to-text (EARS). http://www.darpa.mil/ipto/programs/ears/.</note>
<marker>2003</marker>
<rawString>DARPA Information Processing Technology Office. 2003. Effective, affordable, reusable speech-to-text (EARS). http://www.darpa.mil/ipto/programs/ears/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Gotoh</author>
<author>S Renals</author>
</authors>
<title>Sentence boundary detection in broadcast speech transcripts. In ISCA Workshop: Automatic Speech Recognition: Challenges for the new Millennium ASR-2000,</title>
<date>2000</date>
<pages>228--235</pages>
<contexts>
<context position="11340" citStr="Gotoh and Renals, 2000" startWordPosition="1738" endWordPosition="1742">uences model design is that not all information sources are available uniformly for all training data. For example, prosodic modeling assumes acoustic data; whereas, word-based models can be trained on text-only data, which is usually available in much larger quantities. This poses a problem for approaches that model all relevant information jointly and is another strong motivation for modular approaches. 4 The Models 4.1 Hidden Markov Model for Segmentation Our baseline model, and the one that forms the basis of much of the prior work on acoustic sentence segmentation (Shriberg et al., 2000; Gotoh and Renals, 2000; Christensen, 2001; Kim and Woodland, 2001), is a hidden Markov model. The states of the model correspond to wordswiand following 2Here we are glossing over some details on prosodic modeling that are orthogonal to the discussion in this paper. For example, instead of simple decision trees we actually use ensemble bagging to reduce the variance of the classifier (Liu et al., 2004). Figure 2: The graphical model for the SU detection problem. Only one word+event is depicted in each state, but in a model based on N-grams the previousN—1 tokens would condition the transition to the next state. eve</context>
</contexts>
<marker>Gotoh, Renals, 2000</marker>
<rawString>Y. Gotoh and S. Renals. 2000. Sentence boundary detection in broadcast speech transcripts. In ISCA Workshop: Automatic Speech Recognition: Challenges for the new Millennium ASR-2000, pages 228–235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kim</author>
<author>P C Woodland</author>
</authors>
<title>The use of prosody in a combined system for punctuation generation and speech recognition.</title>
<date>2001</date>
<booktitle>In Proc. of Eurospeech</booktitle>
<pages>2757--2760</pages>
<contexts>
<context position="11384" citStr="Kim and Woodland, 2001" startWordPosition="1745" endWordPosition="1748">tion sources are available uniformly for all training data. For example, prosodic modeling assumes acoustic data; whereas, word-based models can be trained on text-only data, which is usually available in much larger quantities. This poses a problem for approaches that model all relevant information jointly and is another strong motivation for modular approaches. 4 The Models 4.1 Hidden Markov Model for Segmentation Our baseline model, and the one that forms the basis of much of the prior work on acoustic sentence segmentation (Shriberg et al., 2000; Gotoh and Renals, 2000; Christensen, 2001; Kim and Woodland, 2001), is a hidden Markov model. The states of the model correspond to wordswiand following 2Here we are glossing over some details on prosodic modeling that are orthogonal to the discussion in this paper. For example, instead of simple decision trees we actually use ensemble bagging to reduce the variance of the classifier (Liu et al., 2004). Figure 2: The graphical model for the SU detection problem. Only one word+event is depicted in each state, but in a model based on N-grams the previousN—1 tokens would condition the transition to the next state. event labels ei. The observations associated wi</context>
</contexts>
<marker>Kim, Woodland, 2001</marker>
<rawString>J. Kim and P. C. Woodland. 2001. The use of prosody in a combined system for punctuation generation and speech recognition. In Proc. of Eurospeech 2001, pages 2757–2760.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C Manning</author>
</authors>
<title>Conditional structure versus conditional estimation in NLP models.</title>
<date>2002</date>
<booktitle>In Proc. ofEMNLP</booktitle>
<pages>9--16</pages>
<contexts>
<context position="23861" citStr="Klein and Manning, 2002" startWordPosition="3739" endWordPosition="3742">surrounding word context and the local prosodic features). In that respect, the maxent model resembles the conditional probability model at the individual HMM states. The HMM as a whole, however, through the forward-backward procedure, propagates evidence from all parts of the observation sequence to any given decision point. Variants such as the conditional Markov model (CMM) combine sequence modeling with posterior probability (e.g., maxent) modeling, but it has been shown that CMM’s are still structurally inferior to HMMs because they only propagate evidence forward in time, not backwards (Klein and Manning, 2002). 5 Results and Discussion 5.1 Experimental Setup Experiments comparing the two modeling approaches were conducted on two corpora: broadcast news (BN) and conversational telephone speech (CTS). BN and CTS differ in genre and speaking style. These differences are reflected in the frequency of SU boundaries: about 14% of inter-word boundaries are SUs in CTS, compared to roughly 8% in BN. The corpora are annotated by LDC according to the guidelines of (Strassel, 2003). Training and test data are those used in the DARPA Rich Transcription Fall 2003 evaluation.7 For CTS, there is about 40 hours of </context>
<context position="30718" citStr="Klein and Manning, 2002" startWordPosition="4868" endWordPosition="4871">ent approaches for modeling and integration of diverse knowledge sources for automatic sentence segmentation from speech: a state-of-the-art approach based on HMMs, and an alternative approach based on posterior probability estimation via maximum entropy. To achieve competitive performance with the maxent model we devised a cumulative binary coding scheme to map posterior estimates from auxiliary submodels into features for the maxent model. The two approaches have complementary strengths and weaknesses that were reflected in the results, consistent with the findings for text-based NLP tasks (Klein and Manning, 2002). The maxent model showed much better accuracy than the HMM with lexical information, and a smaller win after combination with prosodic features. The HMM made more effective use of prosodic information and degraded less with errorful word recognition. A interpolation of posterior probabilities from the two systems achieved 2-7% relative error reduction compared to the baseline (significant atp&lt;0.05 for the reference transcription condition). The results were consistent for two different genres of speech. In future work we hope to determine how the individual qualitative differences of the two </context>
</contexts>
<marker>Klein, Manning, 2002</marker>
<rawString>D. Klein and C. Manning. 2002. Conditional structure versus conditional estimation in NLP models. In Proc. ofEMNLP 2002, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random field: Probabilistic models for segmenting and labeling sequence data. In Prof. ofICML</title>
<date>2001</date>
<pages>282--289</pages>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random field: Probabilistic models for segmenting and labeling sequence data. In Prof. ofICML 2001, pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Liu</author>
<author>E Shriberg</author>
<author>A Stolcke</author>
<author>M Harper</author>
</authors>
<title>Using machine learning to cope with imbalanced classes in natural speech: Evidence from sentence boundary and disfluency detection.</title>
<date>2004</date>
<booktitle>In Proc. of ICSLP</booktitle>
<note>(To Appear).</note>
<contexts>
<context position="11723" citStr="Liu et al., 2004" startWordPosition="1803" endWordPosition="1806"> modular approaches. 4 The Models 4.1 Hidden Markov Model for Segmentation Our baseline model, and the one that forms the basis of much of the prior work on acoustic sentence segmentation (Shriberg et al., 2000; Gotoh and Renals, 2000; Christensen, 2001; Kim and Woodland, 2001), is a hidden Markov model. The states of the model correspond to wordswiand following 2Here we are glossing over some details on prosodic modeling that are orthogonal to the discussion in this paper. For example, instead of simple decision trees we actually use ensemble bagging to reduce the variance of the classifier (Liu et al., 2004). Figure 2: The graphical model for the SU detection problem. Only one word+event is depicted in each state, but in a model based on N-grams the previousN—1 tokens would condition the transition to the next state. event labels ei. The observations associated with the states are the words, as well as other (mainly prosodic) features fi. Figure 2 shows a graphical model representation of the variables involved. Note that the words appear in both the states and the observations, such that the word stream constrains the possible hidden states to matching words; the ambiguity in the task stems enti</context>
</contexts>
<marker>Liu, Shriberg, Stolcke, Harper, 2004</marker>
<rawString>Y. Liu, E. Shriberg, A. Stolcke, and M. Harper. 2004. Using machine learning to cope with imbalanced classes in natural speech: Evidence from sentence boundary and disfluency detection. In Proc. of ICSLP 2004 (To Appear).</rawString>
</citation>
<citation valid="true">
<title>RT-03F workshop agenda and presentations. http://www.nist.gov/speech/tests/</title>
<date>2003</date>
<booktitle>rt/rt2003/fall/presentations/,</booktitle>
<institution>National Institute of Standards and Technology.</institution>
<marker>2003</marker>
<rawString>National Institute of Standards and Technology. 2003. RT-03F workshop agenda and presentations. http://www.nist.gov/speech/tests/ rt/rt2003/fall/presentations/, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Ngai</author>
<author>R Florian</author>
</authors>
<title>Transformationbased learning in the fast lane.</title>
<date>2001</date>
<booktitle>In Proc. ofNAACL</booktitle>
<pages>40--47</pages>
<contexts>
<context position="8300" citStr="Ngai and Florian, 2001" startWordPosition="1274" endWordPosition="1277">ithout the confounding effect of speech recognition errors, using perfect lexical information. 3 Features and Knowledge Sources Words and sentence boundaries are mutually constrained via syntactic structure. Therefore, the word identities themselves (from automatic recognition or human transcripts) constitute a primary knowledge source for the sentence segmentation task. We also make use of various automatic taggers that map the word sequence to other representations. The TnT tagger (Brants, 2000) is used to obtain part-ofspeech (POS) tags. A TBL chunker trained on Wall Street Journal corpus (Ngai and Florian, 2001) maps each word to an associated chunk tag, encoding chunk type and relative word position (beginning of an NP, inside a VP, etc.). The tagged versions of the word stream are provided to allow generalizations based on syntactic structure and to smooth out possibly undertrained word-based probability esti1This is the same as simple per-event classification accuracy, except that the denominator counts only the “marked” events, thereby yielding error rates that are much higher than if one uses all potential boundary locations. fusion of[] knowledge[] souces[] sentence boundary[] hypothesis[] word</context>
</contexts>
<marker>Ngai, Florian, 2001</marker>
<rawString>G. Ngai and R. Florian. 2001. Transformationbased learning in the fast lane. In Proc. ofNAACL 2001, pages 40–47, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D D Palmer</author>
<author>M A Hearst</author>
</authors>
<title>Adaptive sentence boundary disambiguation.</title>
<date>1994</date>
<booktitle>In Proc. of the Fourth Applied NLP,</booktitle>
<pages>78--83</pages>
<contexts>
<context position="1371" citStr="Palmer and Hearst, 1994" startWordPosition="184" endWordPosition="187">s in the maximum entropy (maxent) framework. Both models combine lexical, syntactic, and prosodic information. We develop a technique for integrating pretrained probability models into the maxent framework, and show that this approach can improve on an HMM-based state-of-the-art system for the sentence-boundary detection task. An even more substantial improvement is obtained by combining the posterior probabilities of the two systems. 1 Introduction Sentence boundary detection is a problem that has received limited attention in the text-based computational linguistics community (Schmid, 2000; Palmer and Hearst, 1994; Reynar and Ratnaparkhi, 1997), but which has recently acquired renewed importance through an effort by the DARPA EARS program (DARPA Information Processing Technology Office, 2003) to improve automatic speech transcription technology. Since standard speech recognizers output an unstructured stream of words, improving transcription means not only that word accuracy must be improved, but also that commonly used structural features such as sentence boundaries need to be recognized. The task is thus fundamentally based on both acoustic and textual (via automatic word recognition) information. Fr</context>
<context position="6232" citStr="Palmer and Hearst, 1994" startWordPosition="940" endWordPosition="943">nel to generate an acoustic signal. A speech recognizer determines the most likely word string given this signal. To detect possible sentence boundaries in the recognized word string, prosodic features are extracted from the signal, and combined with textual cues obtained from the word string. At issue in this paper is the final box in the diagram: how to model and combine the available knowledge sources to find the most accurate hypotheses. Note that this problem differs from the sentence boundary detection problem for written text in the natural language processing literature (Schmid, 2000; Palmer and Hearst, 1994; Reynar and Ratnaparkhi, 1997). Here we are dealing with spoken language, therefore there is no punctuation information, the words are not capitalized, and the transcripts from the recognition output are errorful. This lack of textual cues is partly compensated by prosodic information (timing, pitch, and energy patterns) conveyed by speech. Also note that in spontaneous conversational speech “sentence” is not always a straightforward notion. For our purposes we use the definition of a “sentence-like unit”, or SU, as defined by the LDC for labeling and evaluation purposes (Strassel, 2003). The</context>
</contexts>
<marker>Palmer, Hearst, 1994</marker>
<rawString>D. D. Palmer and M. A. Hearst. 1994. Adaptive sentence boundary disambiguation. In Proc. of the Fourth Applied NLP, pages 78–83.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Rabiner</author>
<author>B H Juang</author>
</authors>
<title>An introduction to hidden Markov models.</title>
<date>1986</date>
<journal>IEEE ASSP Magazine,</journal>
<volume>3</volume>
<issue>1</issue>
<contexts>
<context position="12959" citStr="Rabiner and Juang, 1986" startWordPosition="1997" endWordPosition="2000">e choice of events. 4.1.1 Classification Standard algorithms are available to extract the most probable state (and thus event) sequence given a set of observations. The error metric is based on classification of individual word boundaries. Therefore, rather than finding the highest probability sequence of events, we identify the events with highest posterior individually at each boundaryi: ei=argmaxP(ei jW, F) (1) ei whereWandFare the words and features for the entire test sequence, respectively. The individual event posteriors are obtained by applying the forward-backward algorithm for HMMs (Rabiner and Juang, 1986). 4.1.2 Model Estimation Training of the HMM is supervised since eventlabeled data is available. There are two sets of parameters to estimate. The state transition probabilities are estimated using a hidden event N-gram LM (Stolcke and Shriberg, 1996). The LM is obtained with standard N-gram estimation methods from data that contains the word+event tags in sequence:w1,e1,w2,...en—1,wn. The resulting LM can then compute the required HMM transition Oi Oi+1 W F i i W F i+1 i+1 W E i i W E i+1 i+1 probabilities as3 P(wieilw1e1:::wi—1ei-1)= P(wilw1e1 ::: wi—1ei-1) x P(eilw1e1 ::: wi—1ei-1wi) The N-</context>
</contexts>
<marker>Rabiner, Juang, 1986</marker>
<rawString>L. R. Rabiner and B. H. Juang. 1986. An introduction to hidden Markov models. IEEE ASSP Magazine, 3(1):4–16, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Reynar</author>
<author>A Ratnaparkhi</author>
</authors>
<title>A maximum entropy approach to identifying sentence boundaries.</title>
<date>1997</date>
<booktitle>In Proc. of the Fifth Applied NLP,</booktitle>
<pages>16--19</pages>
<contexts>
<context position="1402" citStr="Reynar and Ratnaparkhi, 1997" startWordPosition="188" endWordPosition="191">(maxent) framework. Both models combine lexical, syntactic, and prosodic information. We develop a technique for integrating pretrained probability models into the maxent framework, and show that this approach can improve on an HMM-based state-of-the-art system for the sentence-boundary detection task. An even more substantial improvement is obtained by combining the posterior probabilities of the two systems. 1 Introduction Sentence boundary detection is a problem that has received limited attention in the text-based computational linguistics community (Schmid, 2000; Palmer and Hearst, 1994; Reynar and Ratnaparkhi, 1997), but which has recently acquired renewed importance through an effort by the DARPA EARS program (DARPA Information Processing Technology Office, 2003) to improve automatic speech transcription technology. Since standard speech recognizers output an unstructured stream of words, improving transcription means not only that word accuracy must be improved, but also that commonly used structural features such as sentence boundaries need to be recognized. The task is thus fundamentally based on both acoustic and textual (via automatic word recognition) information. From a computational linguistics </context>
<context position="6263" citStr="Reynar and Ratnaparkhi, 1997" startWordPosition="944" endWordPosition="948">ic signal. A speech recognizer determines the most likely word string given this signal. To detect possible sentence boundaries in the recognized word string, prosodic features are extracted from the signal, and combined with textual cues obtained from the word string. At issue in this paper is the final box in the diagram: how to model and combine the available knowledge sources to find the most accurate hypotheses. Note that this problem differs from the sentence boundary detection problem for written text in the natural language processing literature (Schmid, 2000; Palmer and Hearst, 1994; Reynar and Ratnaparkhi, 1997). Here we are dealing with spoken language, therefore there is no punctuation information, the words are not capitalized, and the transcripts from the recognition output are errorful. This lack of textual cues is partly compensated by prosodic information (timing, pitch, and energy patterns) conveyed by speech. Also note that in spontaneous conversational speech “sentence” is not always a straightforward notion. For our purposes we use the definition of a “sentence-like unit”, or SU, as defined by the LDC for labeling and evaluation purposes (Strassel, 2003). The training data has SU boundarie</context>
</contexts>
<marker>Reynar, Ratnaparkhi, 1997</marker>
<rawString>J. Reynar and A. Ratnaparkhi. 1997. A maximum entropy approach to identifying sentence boundaries. In Proc. of the Fifth Applied NLP, pages 16–19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schmid</author>
</authors>
<title>Unsupervised learning of period disambiguation for tokenisation.</title>
<date>2000</date>
<institution>University of Stuttgart, Internal Report.</institution>
<contexts>
<context position="1346" citStr="Schmid, 2000" startWordPosition="182" endWordPosition="183">owledge sources in the maximum entropy (maxent) framework. Both models combine lexical, syntactic, and prosodic information. We develop a technique for integrating pretrained probability models into the maxent framework, and show that this approach can improve on an HMM-based state-of-the-art system for the sentence-boundary detection task. An even more substantial improvement is obtained by combining the posterior probabilities of the two systems. 1 Introduction Sentence boundary detection is a problem that has received limited attention in the text-based computational linguistics community (Schmid, 2000; Palmer and Hearst, 1994; Reynar and Ratnaparkhi, 1997), but which has recently acquired renewed importance through an effort by the DARPA EARS program (DARPA Information Processing Technology Office, 2003) to improve automatic speech transcription technology. Since standard speech recognizers output an unstructured stream of words, improving transcription means not only that word accuracy must be improved, but also that commonly used structural features such as sentence boundaries need to be recognized. The task is thus fundamentally based on both acoustic and textual (via automatic word rec</context>
<context position="6207" citStr="Schmid, 2000" startWordPosition="938" endWordPosition="939">roduction channel to generate an acoustic signal. A speech recognizer determines the most likely word string given this signal. To detect possible sentence boundaries in the recognized word string, prosodic features are extracted from the signal, and combined with textual cues obtained from the word string. At issue in this paper is the final box in the diagram: how to model and combine the available knowledge sources to find the most accurate hypotheses. Note that this problem differs from the sentence boundary detection problem for written text in the natural language processing literature (Schmid, 2000; Palmer and Hearst, 1994; Reynar and Ratnaparkhi, 1997). Here we are dealing with spoken language, therefore there is no punctuation information, the words are not capitalized, and the transcripts from the recognition output are errorful. This lack of textual cues is partly compensated by prosodic information (timing, pitch, and energy patterns) conveyed by speech. Also note that in spontaneous conversational speech “sentence” is not always a straightforward notion. For our purposes we use the definition of a “sentence-like unit”, or SU, as defined by the LDC for labeling and evaluation purpo</context>
</contexts>
<marker>Schmid, 2000</marker>
<rawString>H. Schmid. 2000. Unsupervised learning of period disambiguation for tokenisation. University of Stuttgart, Internal Report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Shriberg</author>
<author>A Stolcke</author>
<author>D H Tur</author>
<author>G Tur</author>
</authors>
<title>Prosody-based automatic segmentation of speech into sentences and topics.</title>
<date>2000</date>
<journal>Speech Communication,</journal>
<pages>32--1</pages>
<contexts>
<context position="3296" citStr="Shriberg et al., 2000" startWordPosition="480" endWordPosition="483">and the range of potential knowledge sources to be modeled (acoustics and text-based, lower- and higher-level), this is an interesting challenge problem for statistical and computational approaches. All of the systems participating in the recent DARPA RT-03F Metadata Extraction evaluation (National Institute of Standards and Technology, 2003) were based on a hidden Markov model framework, in which word/tag sequences are modeled by N-gram language models (LMs). Additional features (mostly reflecting speech prosody) are modeled as observation likelihoods attached to the Ngram states of the HMM (Shriberg et al., 2000). The HMM is a generative modeling approach, since it describes a stochastic process with hidden variables (the locations of sentence boundaries) that produces the observable data. The segmentation is inferred by comparing the likelihoods of different boundary hypotheses. While the HMM approach is computationally efficient and (as described later) provides a convenient way for modularizing the knowledge sources, it has two main drawbacks: First, the standard training methods for HMMs maximize the joint probability of observed and hidden events, as opposed to the posterior probability of the co</context>
<context position="11316" citStr="Shriberg et al., 2000" startWordPosition="1734" endWordPosition="1737">issue that greatly influences model design is that not all information sources are available uniformly for all training data. For example, prosodic modeling assumes acoustic data; whereas, word-based models can be trained on text-only data, which is usually available in much larger quantities. This poses a problem for approaches that model all relevant information jointly and is another strong motivation for modular approaches. 4 The Models 4.1 Hidden Markov Model for Segmentation Our baseline model, and the one that forms the basis of much of the prior work on acoustic sentence segmentation (Shriberg et al., 2000; Gotoh and Renals, 2000; Christensen, 2001; Kim and Woodland, 2001), is a hidden Markov model. The states of the model correspond to wordswiand following 2Here we are glossing over some details on prosodic modeling that are orthogonal to the discussion in this paper. For example, instead of simple decision trees we actually use ensemble bagging to reduce the variance of the classifier (Liu et al., 2004). Figure 2: The graphical model for the SU detection problem. Only one word+event is depicted in each state, but in a model based on N-grams the previousN—1 tokens would condition the transitio</context>
</contexts>
<marker>Shriberg, Stolcke, Tur, Tur, 2000</marker>
<rawString>E. Shriberg, A. Stolcke, D. H. Tur, and G. Tur. 2000. Prosody-based automatic segmentation of speech into sentences and topics. Speech Communication, 32(1-2):127–154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
<author>E Shriberg</author>
</authors>
<title>Automatic linguistic segmentation of conversational speech.</title>
<date>1996</date>
<booktitle>In Proc. ofICSLP</booktitle>
<pages>1005--1008</pages>
<contexts>
<context position="13210" citStr="Stolcke and Shriberg, 1996" startWordPosition="2038" endWordPosition="2041">e, rather than finding the highest probability sequence of events, we identify the events with highest posterior individually at each boundaryi: ei=argmaxP(ei jW, F) (1) ei whereWandFare the words and features for the entire test sequence, respectively. The individual event posteriors are obtained by applying the forward-backward algorithm for HMMs (Rabiner and Juang, 1986). 4.1.2 Model Estimation Training of the HMM is supervised since eventlabeled data is available. There are two sets of parameters to estimate. The state transition probabilities are estimated using a hidden event N-gram LM (Stolcke and Shriberg, 1996). The LM is obtained with standard N-gram estimation methods from data that contains the word+event tags in sequence:w1,e1,w2,...en—1,wn. The resulting LM can then compute the required HMM transition Oi Oi+1 W F i i W F i+1 i+1 W E i i W E i+1 i+1 probabilities as3 P(wieilw1e1:::wi—1ei-1)= P(wilw1e1 ::: wi—1ei-1) x P(eilw1e1 ::: wi—1ei-1wi) The N-gram estimator maximizes the joint word+event sequence likelihoodP(W;E)on the training data (modulo smoothing), and does not guarantee that the correct event posteriors needed for classification according to Equation (1) are maximized. The second set </context>
</contexts>
<marker>Stolcke, Shriberg, 1996</marker>
<rawString>A. Stolcke and E. Shriberg. 1996. Automatic linguistic segmentation of conversational speech. In Proc. ofICSLP 1996, pages 1005–1008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
<author>H Franco</author>
<author>R Gadde</author>
</authors>
<date>2003</date>
<note>Speech-to-text research at SRI-ICSI-UW. http://www.nist.gov/speech/tests/rt/rt2003/spring/ presentations/index.htm.</note>
<contexts>
<context position="25275" citStr="Stolcke et al., 2003" startWordPosition="3984" endWordPosition="3987">as the test set in this paper, in order to have a larger test set to make the results more meaningful. HMM Maxent Combined BN REF 48.72 48.61 46.79 STT 55.37 56.51 54.35 CTS REF 31.51 30.66 29.30 STT 42.97 43.02 41.88 Table 1: SU detection results (error rate in %) using maxent and HMM individually and in combination on BN and CTS. of broadcast news shows in the training set and 3 hours (6 shows) in the test set. The SU detection task is evaluated on both the reference transcriptions (REF) and speech recognition outputs (STT). The speech recognition output is obtained from the SRI recognizer (Stolcke et al., 2003). System performance is evaluated using the official NIST evaluation tools,8 which implement the metric described earlier. In our experiments, we compare how the two approaches perform individually and in combination. The combined classifier is obtained by simply averaging the posterior estimates from the two models, and then picking the event type with the highest probability at each position. We also investigate other experimental factors, such as the impact of the speech recognition errors, the impact of genre, and the contribution of text versus prosodic information in each model. 5.2 Expe</context>
</contexts>
<marker>Stolcke, Franco, Gadde, 2003</marker>
<rawString>A. Stolcke, H. Franco, and R. Gadde et al. 2003. Speech-to-text research at SRI-ICSI-UW. http://www.nist.gov/speech/tests/rt/rt2003/spring/ presentations/index.htm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Strassel</author>
</authors>
<title>Simple Metadata Annotation Specification V5.0. Linguistic Data Consortium.</title>
<date>2003</date>
<contexts>
<context position="6827" citStr="Strassel, 2003" startWordPosition="1037" endWordPosition="1038">mer and Hearst, 1994; Reynar and Ratnaparkhi, 1997). Here we are dealing with spoken language, therefore there is no punctuation information, the words are not capitalized, and the transcripts from the recognition output are errorful. This lack of textual cues is partly compensated by prosodic information (timing, pitch, and energy patterns) conveyed by speech. Also note that in spontaneous conversational speech “sentence” is not always a straightforward notion. For our purposes we use the definition of a “sentence-like unit”, or SU, as defined by the LDC for labeling and evaluation purposes (Strassel, 2003). The training data has SU boundaries marked by annotators, based on both the recorded speech and its transcription. In testing, a system has to recover both the words and the locations of sentence boundaries, denoted by (W, E) = w1 e1 w2 ... wiei ... wn where W represents the strings of word tokens and Ethe inter-word boundary events (sentence boundary or no boundary) . The system output is scored by first finding a minimum edit distance alignment between the hypothesized word string and the reference, and then comparing the aligned event labels. The SU error rate is defined as the total numb</context>
<context position="24330" citStr="Strassel, 2003" startWordPosition="3817" endWordPosition="3818">n that CMM’s are still structurally inferior to HMMs because they only propagate evidence forward in time, not backwards (Klein and Manning, 2002). 5 Results and Discussion 5.1 Experimental Setup Experiments comparing the two modeling approaches were conducted on two corpora: broadcast news (BN) and conversational telephone speech (CTS). BN and CTS differ in genre and speaking style. These differences are reflected in the frequency of SU boundaries: about 14% of inter-word boundaries are SUs in CTS, compared to roughly 8% in BN. The corpora are annotated by LDC according to the guidelines of (Strassel, 2003). Training and test data are those used in the DARPA Rich Transcription Fall 2003 evaluation.7 For CTS, there is about 40 hours of conversational data from the Switchboard corpus for training and 6 hours (72 conversations) for testing. The BN data has about 20 hours 7We used both the development set and the evaluation set as the test set in this paper, in order to have a larger test set to make the results more meaningful. HMM Maxent Combined BN REF 48.72 48.61 46.79 STT 55.37 56.51 54.35 CTS REF 31.51 30.66 29.30 STT 42.97 43.02 41.88 Table 1: SU detection results (error rate in %) using maxe</context>
</contexts>
<marker>Strassel, 2003</marker>
<rawString>S. Strassel, 2003. Simple Metadata Annotation Specification V5.0. Linguistic Data Consortium.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>