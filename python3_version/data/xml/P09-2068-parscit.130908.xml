<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.092382">
<title confidence="0.991814">
Investigating Pitch Accent Recognition in Non-native Speech
</title>
<author confidence="0.988379">
Gina-Anne Levow
</author>
<affiliation confidence="0.9967365">
Computer Science Department
University of Chicago
</affiliation>
<email confidence="0.989704">
ginalevow@gmail.com
</email>
<sectionHeader confidence="0.993725" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999837736842105">
Acquisition of prosody, in addition to vo-
cabulary and grammar, is essential for lan-
guage learners. However, it has received
less attention in instruction. To enable
automatic identification and feedback on
learners’ prosodic errors, we investigate
automatic pitch accent labeling for non-
native speech. We demonstrate that an
acoustic-based context model can achieve
accuracies over 79% on binary pitch ac-
cent recognition when trained on within-
group data. Furthermore, we demonstrate
that good accuracies are achieved in cross-
group training, where native and near-
native training data result in no significant
loss of accuracy on non-native test speech.
These findings illustrate the potential for
automatic feedback in computer-assisted
prosody learning.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99995485">
Acquisition of prosody, in addition to vocabulary
and grammar, is essential for language learners.
However, intonation has been less-emphasized
both in classroom and computer-assisted language
instruction (Chun, 1998). Outside of tone lan-
guages, it can be difficult to characterize the fac-
tors that lead to non-native prosody in learner
speech, and it is difficult for instructors to find time
for the one-on-one interaction that is required to
provide feedback and instruction in prosody.
To address these problems and enable automatic
feedback to learners in a computer-assisted lan-
guage learning setting, we investigate automatic
prosodic labelling of non-native speech. While
many prior systems (Teixeia et al., 2000; Tep-
perman and Narayanan, 2008) aim to assign a
score to the learner speech, we hope to provide
more focused feedback by automatically identify-
ing prosodic units, such as pitch accents in English
or tone in Mandarin, to enable direct comparison
with gold-standard native utterances.
There has been substantial progress in auto-
matic pitch accent recognition for native speech,
achieving accuracies above 80% for acoustic-
feature based recognition in multi-speaker cor-
pora (Sridhar et al., 2007; Levow, 2008). How-
ever, there has been little study of pitch accent
recognition in non-native speech. Given the chal-
lenges posed for automatic speech recognition of
non-native speech, we ask whether recognition of
intonational categories is practical for non-native
speech. To lay the foundations for computer-
assisted intonation tutoring, we ask whether com-
petitive accuracies can be achieved on non-native
speech. We further investigate whether good
recognition accuracy can be achieved using rel-
atively available labeled native or near-native
speech, or whether it will be necessary to col-
lect larger amounts of training or adaptation data
matched for speaker, language background, or lan-
guage proficiency.
We employ a pitch accent recognition approach
that exploits local and coarticulatory context to
achieve competitive pitch accent recognition accu-
racy on native speech. Using a corpus of prosod-
ically labelled native and non-native speech, we
illustrate that similar acoustic contrasts hold for
pitch accents in both native and non-native speech.
These contrasts yield competitive accuracies on
binary pitch accent recognition using within-group
training data. Furthermore, there is no significant
drop in accuracy when models trained on native or
near-native speech are employed for classification
of non-native speech.
The remainder of the paper is organized as fol-
lows. We present the LeaP Corpus used for our
experiments in Section 2. We next describe the
feature sets employed for classification (Section 3)
and contrastive acoustic analysis for these features
in native and non-native speech (Section 4). We
</bodyText>
<page confidence="0.978829">
269
</page>
<note confidence="0.9719405">
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 269–272,
Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP
</note>
<table confidence="0.999057">
ID Description
c1 non-native, before prosody training
c2 non-native, after first prosody training
c3 non-native, after second prosody training
e1 non-native, before going abroad
e2 non-native, after going abroad
sl ’super-learner’, near-native
na native
</table>
<tableCaption confidence="0.812543">
Table 1: Speaker groups, with ID and description
in the LeaP Corpus
</tableCaption>
<bodyText confidence="0.9998675">
then describe the classifier setting and experimen-
tal results in Section 5 as well as discussion. Fi-
nally, we present some conclusions and plans for
future work.
</bodyText>
<sectionHeader confidence="0.686537" genericHeader="introduction">
2 LeaP Corpus and the Dataset
</sectionHeader>
<bodyText confidence="0.999963384615384">
We employ data from the LeaP Corpus (Milde and
Gut, 2002), collected at the University of Biele-
feld as part of the “Learning Prosody in a For-
eign Language” project. Details of the corpus
(Milde and Gut, 2002), inter-rater reliability mea-
sures (Gut and Bayerl, 2004), and other research
findings (Gut, 2009) have been reported.
Here we focus on the read English segment of
the corpus that has been labelled with modified
EToBI tags1 , to enable better comparison with
prior results of prosodic labelling accuracy and
also to better model a typical language laboratory
setting where students read or repeat. This yields
a total of 37 recordings of just over 300 syllables
each, from 26 speakers, as in Table 1.2 This set
allows the evaluation of prosodic labelling across
a range of native and non-native proficiency lev-
els. The modified version of ETobi employed by
the LeaP annotators allows transcription of 14 cat-
egories of pitch accent and 14 categories of bound-
ary tone. However, in our experiments, we will fo-
cus only on pitch accent recognition and will col-
lapse the inventory to the relatively standard, and
more reliably annotated, four-way (high, down-
stepped high, low, and unaccented) and binary (ac-
cented, unaccented) label sets.
</bodyText>
<footnote confidence="0.666519666666667">
1While the full corpus includes speakers from a range of
languages, the EToBI labels were applied primarily to data
from German speakers.
2Length of recordings varies due to differences in syllab-
ification and cliticization, as well as disfluencies and reading
errors.
</footnote>
<sectionHeader confidence="0.992335" genericHeader="method">
3 Acoustic-Prosodic Features
</sectionHeader>
<bodyText confidence="0.999981935483871">
Recent research has highlighted the importance
of context for both tone and intonation. The
role of context can be seen in the characteriza-
tion of pitch accents such as down-stepped high
and in phenomena such as downdrift across a
phrase. Further, local coarticulation with neigh-
boring tones has been shown to have a signif-
icant impact on the realization of prosodic ele-
ments, due to articulatory constraints (Xu and
Sun, 2002). The use of prosodic and coarticu-
latory context has improved the effectiveness of
tone and pitch accent recognition in a range of lan-
guages (Mandarin (Wang and Seneff, 2000), En-
glish (Sun, 2002)) and learning frameworks (deci-
sion trees (Sun, 2002), HMMs (Wang and Seneff,
2000), and CRFs (Levow, 2008)).
Thus, in this work, we employ a rich contextual
feature set, based on that in (Levow, 2008). We
build on the pitch target approximation model, tak-
ing the syllable as the domain of tone prediction
with a pitch height and contour target approached
exponentially over the course of the syllable, con-
sistent with (Sun, 2002). We employ an acoustic
model at the syllable level, employing pitch, in-
tensity and duration measures. The acoustic mea-
sures are computed using Praat’s (Boersma, 2001)
”To pitch” and ”To intensity.” We log-scaled and
speaker-normalized all pitch and intensity values.
We compute two sets of features: one set de-
scribing features local to the syllable and one set
capturing contextual information.
</bodyText>
<subsectionHeader confidence="0.996356">
3.1 Local features
</subsectionHeader>
<bodyText confidence="0.999942777777778">
We extract features to represent the pitch height
and pitch contour of the syllable. For pitch fea-
tures, we extract the following information: (a)
pitch values for five evenly spaced points in the
voiced region of the syllable, (b) pitch maximum,
mean, minimum, and range, and (c) pitch slope,
from midpoint to end of syllable. We also ob-
tain the following non-pitch features: (a) intensity
maximum and mean and (b) syllable duration.
</bodyText>
<subsectionHeader confidence="0.998131">
3.2 Context Modeling
</subsectionHeader>
<bodyText confidence="0.999972166666667">
To capture local contextual influences and cues,
we employ two sets of features. The first set of fea-
tures includes differences between pitch maxima,
pitch means, pitches at the midpoint of the sylla-
bles, pitch slopes, intensity maxima, and intensity
means, between the current and preceding or fol-
</bodyText>
<page confidence="0.985096">
270
</page>
<bodyText confidence="0.999683">
lowing syllable. The second set of features adds
the last pitch values from the end of the preceding
syllable and the first from the beginning of the fol-
lowing syllable. These features capture both the
relative differences in pitch associated with pitch
accent as well as phenomena such as pitch peak
delay in which the actual pitch target may not be
reached until the following syllable.
</bodyText>
<sectionHeader confidence="0.9183265" genericHeader="method">
4 Acoustic Analysis of Native and
Non-native Tone
</sectionHeader>
<bodyText confidence="0.999979653061224">
To assess the potential effectiveness of tone recog-
nition for non-native speech, we analyze and com-
pare native and non-native speech with respect to
features used for classification that have shown
utility in prior work. Pitch accents are charac-
terized not only by their absolute pitch height,
but also by contrast with neighboring syllables.
Thus, we compare the values for pitch and delta
pitch, the difference between the current and pre-
ceding syllable, both with log-scaled measures for
high-accented and unaccented syllables. We con-
trast these values within speaker group (native: na;
non-native: e1, c1). We also compare the delta
pitch measures between speaker groups (na versus
e1 or c1).
Not only do we find significant differences for
delta pitch between accented and unaccented syl-
lables for native speakers as we expect, but we
find that non-native speakers also exhibit signif-
icant differences for this measure (t-test, two-
tailed,p &lt; 0.001). Accented syllables are reli-
ably higher in pitch than immediately preceding
syllables, while unaccented syllables show no con-
trast. Importantly, we further observe a significant
difference in delta pitch for high accented sylla-
bles between native and non-native speech. Na-
tive speakers employ a markedly larger change in
pitch to indicate accent than do non-native speak-
ers, a fine-grained view consistent with findings
that non-native speakers employ a relatively com-
pressed pitch range (Gut, 2009).
For one non-native group (e1), we find that al-
though these speakers produce reliable contrasts
in delta pitch between neighboring syllables, the
overall pitch height of high accented syllables is
not significantly different from that of unaccented
syllables. For native speakers and the ’c1’ non-
native group, though, overall pitch height does
differ significantly between accented and unac-
cented syllables. This finding suggests that while
all speakers in this data set understand the locally
contrastive role of pitch accent, some non-native
speaker groups do not have as reliable global con-
trol of pitch.
The presence of these reliable contrasts between
accented and unaccented syllables in both na-
tive and non-native speech suggests that automatic
pitch accent recognition in learner speech could be
successful.
</bodyText>
<sectionHeader confidence="0.938142" genericHeader="method">
5 Pitch Accent Recognition Experiments
</sectionHeader>
<bodyText confidence="0.99994635">
We assess the effectiveness of pitch accent recog-
nition on the LeaP Corpus speech. We hope to
understand whether pitch accent can be accurately
recognized in non-native speech and whether ac-
curacy rates would be competitive with those on
native speech. In addition, we aim to compare the
impact of different sources of training data. We
assess whether non-native prosody can be recog-
nized using native or near-native training speech or
whether it will be necessary to use matched train-
ing data from non-natives of similar skill level or
language background.
Thus we perform experiments on matched train-
ing and test data, training and testing within
groups of speakers. We also evaluate cross-group
training and testing, training on one group of
speakers (native and near-native) and testing on
another (non-native). We contrast all these results
with assignment of the dominant ’unaccented’ la-
bel to all instances (common class).
</bodyText>
<subsectionHeader confidence="0.999068">
5.1 Support Vector Machine Classifier
</subsectionHeader>
<bodyText confidence="0.9999095">
For all supervised experiments reported in this pa-
per, we employ a Support Vector machine (SVM)
with a linear kernel. Support Vector Machines pro-
vide a fast, easily trainable classification frame-
work that has proven effective in a wide range of
application tasks. For example, in the binary clas-
sification case, given a set of training examples
presented as feature vectors of length D, the lin-
ear SVM algorithm learns a vector of weights of
length D which is a linear combination of a sub-
set of the input vectors and performs classification
based on the function f(x) = sign(wT x − b). We
employ the publicly available implementation of
SVMs, LIBSVM (C-C.Cheng and Lin, 2001).
</bodyText>
<sectionHeader confidence="0.633591" genericHeader="evaluation">
5.2 Results
</sectionHeader>
<bodyText confidence="0.9969315">
We see that, for within group training, on the
binary pitch accent recognition task, accuracies
</bodyText>
<page confidence="0.990423">
271
</page>
<table confidence="0.9988326">
c1 c2 c3 e1 e2 sl na
Within-group Accuracy 79.1 80.9 80.6 81 82.5 82.4 81.2
Cross-group Accuracy (na) 77.2 79 81.4 80.3 82.5 83.2
Cross-group Accuracy (sl) 77.3 79.9 82 80.5 82.9 81.6
Common Class 56.9 59.6 56.2 70.2 64 65.5 63.6
</table>
<tableCaption confidence="0.929142">
Table 2: Pitch accent recognition, within-group, cross-group with native and near-native training, and
most common class baseline: Non-native (plain), ’Super-learner’ (underline sl), Native (bold na)
</tableCaption>
<bodyText confidence="0.999713363636364">
range from approximately 79% to 82.5%. These
levels are consistent with syllable-, acoustic-
feature-based prosodic recognition reported in the
literature (Levow, 2008). A summary of these re-
sults appears in Table 2. In the cross-group train-
ing and testing condition, we observe some vari-
ations in accuracy, for some training sets. How-
ever, crucially none of the differences between
native-based or near-native training and within-
group training reach significance for the binary
pitch accent recognition task.
</bodyText>
<sectionHeader confidence="0.999234" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999974352941177">
We have demonstrated the effectiveness of pitch
accent recognition on both native and non-native
data from the LeaP corpus, based on significant
differences between accented and unaccented syl-
lables in both native and non-native speech. Al-
though these differences are significantly larger in
native speech, recognition remains robust to train-
ing with native speech and testing on non-native
speech, without significant drops in accuracy. This
result argues that binary pitch accent recognition
using native training data may be sufficiently ac-
curate that to avoid collection and labeling of large
amounts of training data matched by speaker or
fluency-level to support prosodic annotation and
feedback. In future work, we plan to incorporate
prosodic recognition and synthesized feedback to
support computer-assisted prosody learning.
</bodyText>
<sectionHeader confidence="0.997487" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999975333333333">
We thank the creators of the LeaP Corpus as well
as C-C. Cheng and C-J. Lin for LibSVM. This
work was supported by NSF IIS: 0414919.
</bodyText>
<sectionHeader confidence="0.99948" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999089166666667">
P. Boersma. 2001. Praat, a system for doing phonetics
by computer. Glot International, 5(9–10):341–345.
C-C.Cheng and C-J. Lin. 2001. LIBSVM:a library
for support vector machines. Software available at:
http://www.csie.ntu.edu.tw/ cjlin/libsvm.
Dorothy M. Chun. 1998. Signal analysis software for
teaching discourse intonation. Language Learning
&amp; Technology, 2(1):61–77.
U. Gut and P. S. Bayerl. 2004. Measuring the relia-
bility of manual annotations of speech corpora. In
Proceedings of Speech Prosody 2004.
U. Gut. 2009. Non-native speech. A corpus-based
analysis of phonological and phonetic properties of
L2 English and German. Peter Lang, Frankfurt.
G.-A. Levow. 2008. Automatic prosodic labeling with
conditional random fields and rich acoustic features.
In Proceedings of the IJCNLP 2008.
J.-T. Milde and U. Gut. 2002. A prosodic corpus of
non-native speech. In Proceedings of the Speech
Prosody 2002 Conference, pages 503–506.
V. Rangarajan Sridhar, S. Bangalore, and S. Narayanan.
2007. Exploiting acoustic and syntactic features for
prosody labeling in a maximum entropy framework.
In Proceedings of HLT NAACL 2007, pages 1–8.
Xuejing Sun. 2002. Pitch accent prediction using en-
semble machine learning. In Proceedings of ICSLP-
2002.
C. Teixeia, H. Franco, E. Shriberg, K. Precoda, and
K. Somnez. 2000. Prosodic features for automatic
text-independent evaluation of degree of nativeness
for language learners. In Proceedings of ICSLP
2000.
J. Tepperman and S. Narayanan. 2008. Better non-
native intonation scores through prosodic theory. In
Proceedings of Interspeech 2008.
C. Wang and S. Seneff. 2000. Improved tone recogni-
tion by normalizing for coarticulation and intonation
effects. In Proceedings of 6th International Confer-
ence on Spoken Language Processing.
Yi Xu and X. Sun. 2002. Maximum speed of pitch
change and how it may relate to speech. Journal of
the Acoustical Society ofAmerica, 111.
</reference>
<page confidence="0.997253">
272
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.848901">
<title confidence="0.998397">Investigating Pitch Accent Recognition in Non-native Speech</title>
<author confidence="0.865889">Gina-Anne Levow</author>
<affiliation confidence="0.9998445">Computer Science Department University of Chicago</affiliation>
<email confidence="0.999762">ginalevow@gmail.com</email>
<abstract confidence="0.99898265">Acquisition of prosody, in addition to vocabulary and grammar, is essential for language learners. However, it has received less attention in instruction. To enable automatic identification and feedback on learners’ prosodic errors, we investigate automatic pitch accent labeling for nonnative speech. We demonstrate that an acoustic-based context model can achieve accuracies over 79% on binary pitch accent recognition when trained on withingroup data. Furthermore, we demonstrate that good accuracies are achieved in crossgroup training, where native and nearnative training data result in no significant loss of accuracy on non-native test speech. These findings illustrate the potential for automatic feedback in computer-assisted prosody learning.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P Boersma</author>
</authors>
<title>Praat, a system for doing phonetics by computer.</title>
<date>2001</date>
<journal>Glot International,</journal>
<volume>5</volume>
<issue>9</issue>
<contexts>
<context position="7163" citStr="Boersma, 2001" startWordPosition="1102" endWordPosition="1103">glish (Sun, 2002)) and learning frameworks (decision trees (Sun, 2002), HMMs (Wang and Seneff, 2000), and CRFs (Levow, 2008)). Thus, in this work, we employ a rich contextual feature set, based on that in (Levow, 2008). We build on the pitch target approximation model, taking the syllable as the domain of tone prediction with a pitch height and contour target approached exponentially over the course of the syllable, consistent with (Sun, 2002). We employ an acoustic model at the syllable level, employing pitch, intensity and duration measures. The acoustic measures are computed using Praat’s (Boersma, 2001) ”To pitch” and ”To intensity.” We log-scaled and speaker-normalized all pitch and intensity values. We compute two sets of features: one set describing features local to the syllable and one set capturing contextual information. 3.1 Local features We extract features to represent the pitch height and pitch contour of the syllable. For pitch features, we extract the following information: (a) pitch values for five evenly spaced points in the voiced region of the syllable, (b) pitch maximum, mean, minimum, and range, and (c) pitch slope, from midpoint to end of syllable. We also obtain the foll</context>
</contexts>
<marker>Boersma, 2001</marker>
<rawString>P. Boersma. 2001. Praat, a system for doing phonetics by computer. Glot International, 5(9–10):341–345. C-C.Cheng and C-J. Lin. 2001. LIBSVM:a library for support vector machines. Software available at: http://www.csie.ntu.edu.tw/ cjlin/libsvm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dorothy M Chun</author>
</authors>
<title>Signal analysis software for teaching discourse intonation.</title>
<date>1998</date>
<journal>Language Learning &amp; Technology,</journal>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="1141" citStr="Chun, 1998" startWordPosition="155" endWordPosition="156">s over 79% on binary pitch accent recognition when trained on withingroup data. Furthermore, we demonstrate that good accuracies are achieved in crossgroup training, where native and nearnative training data result in no significant loss of accuracy on non-native test speech. These findings illustrate the potential for automatic feedback in computer-assisted prosody learning. 1 Introduction Acquisition of prosody, in addition to vocabulary and grammar, is essential for language learners. However, intonation has been less-emphasized both in classroom and computer-assisted language instruction (Chun, 1998). Outside of tone languages, it can be difficult to characterize the factors that lead to non-native prosody in learner speech, and it is difficult for instructors to find time for the one-on-one interaction that is required to provide feedback and instruction in prosody. To address these problems and enable automatic feedback to learners in a computer-assisted language learning setting, we investigate automatic prosodic labelling of non-native speech. While many prior systems (Teixeia et al., 2000; Tepperman and Narayanan, 2008) aim to assign a score to the learner speech, we hope to provide </context>
</contexts>
<marker>Chun, 1998</marker>
<rawString>Dorothy M. Chun. 1998. Signal analysis software for teaching discourse intonation. Language Learning &amp; Technology, 2(1):61–77.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Gut</author>
<author>P S Bayerl</author>
</authors>
<title>Measuring the reliability of manual annotations of speech corpora.</title>
<date>2004</date>
<booktitle>In Proceedings of Speech Prosody</booktitle>
<contexts>
<context position="4671" citStr="Gut and Bayerl, 2004" startWordPosition="693" endWordPosition="696">before going abroad e2 non-native, after going abroad sl ’super-learner’, near-native na native Table 1: Speaker groups, with ID and description in the LeaP Corpus then describe the classifier setting and experimental results in Section 5 as well as discussion. Finally, we present some conclusions and plans for future work. 2 LeaP Corpus and the Dataset We employ data from the LeaP Corpus (Milde and Gut, 2002), collected at the University of Bielefeld as part of the “Learning Prosody in a Foreign Language” project. Details of the corpus (Milde and Gut, 2002), inter-rater reliability measures (Gut and Bayerl, 2004), and other research findings (Gut, 2009) have been reported. Here we focus on the read English segment of the corpus that has been labelled with modified EToBI tags1 , to enable better comparison with prior results of prosodic labelling accuracy and also to better model a typical language laboratory setting where students read or repeat. This yields a total of 37 recordings of just over 300 syllables each, from 26 speakers, as in Table 1.2 This set allows the evaluation of prosodic labelling across a range of native and non-native proficiency levels. The modified version of ETobi employed by </context>
</contexts>
<marker>Gut, Bayerl, 2004</marker>
<rawString>U. Gut and P. S. Bayerl. 2004. Measuring the reliability of manual annotations of speech corpora. In Proceedings of Speech Prosody 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Gut</author>
</authors>
<title>Non-native speech. A corpus-based analysis of phonological and phonetic properties of L2</title>
<date>2009</date>
<journal>English</journal>
<location>Frankfurt.</location>
<contexts>
<context position="4712" citStr="Gut, 2009" startWordPosition="701" endWordPosition="702">d sl ’super-learner’, near-native na native Table 1: Speaker groups, with ID and description in the LeaP Corpus then describe the classifier setting and experimental results in Section 5 as well as discussion. Finally, we present some conclusions and plans for future work. 2 LeaP Corpus and the Dataset We employ data from the LeaP Corpus (Milde and Gut, 2002), collected at the University of Bielefeld as part of the “Learning Prosody in a Foreign Language” project. Details of the corpus (Milde and Gut, 2002), inter-rater reliability measures (Gut and Bayerl, 2004), and other research findings (Gut, 2009) have been reported. Here we focus on the read English segment of the corpus that has been labelled with modified EToBI tags1 , to enable better comparison with prior results of prosodic labelling accuracy and also to better model a typical language laboratory setting where students read or repeat. This yields a total of 37 recordings of just over 300 syllables each, from 26 speakers, as in Table 1.2 This set allows the evaluation of prosodic labelling across a range of native and non-native proficiency levels. The modified version of ETobi employed by the LeaP annotators allows transcription </context>
<context position="10062" citStr="Gut, 2009" startWordPosition="1564" endWordPosition="1565">d that non-native speakers also exhibit significant differences for this measure (t-test, twotailed,p &lt; 0.001). Accented syllables are reliably higher in pitch than immediately preceding syllables, while unaccented syllables show no contrast. Importantly, we further observe a significant difference in delta pitch for high accented syllables between native and non-native speech. Native speakers employ a markedly larger change in pitch to indicate accent than do non-native speakers, a fine-grained view consistent with findings that non-native speakers employ a relatively compressed pitch range (Gut, 2009). For one non-native group (e1), we find that although these speakers produce reliable contrasts in delta pitch between neighboring syllables, the overall pitch height of high accented syllables is not significantly different from that of unaccented syllables. For native speakers and the ’c1’ nonnative group, though, overall pitch height does differ significantly between accented and unaccented syllables. This finding suggests that while all speakers in this data set understand the locally contrastive role of pitch accent, some non-native speaker groups do not have as reliable global control o</context>
</contexts>
<marker>Gut, 2009</marker>
<rawString>U. Gut. 2009. Non-native speech. A corpus-based analysis of phonological and phonetic properties of L2 English and German. Peter Lang, Frankfurt.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G-A Levow</author>
</authors>
<title>Automatic prosodic labeling with conditional random fields and rich acoustic features.</title>
<date>2008</date>
<booktitle>In Proceedings of the IJCNLP</booktitle>
<contexts>
<context position="2150" citStr="Levow, 2008" startWordPosition="311" endWordPosition="312">estigate automatic prosodic labelling of non-native speech. While many prior systems (Teixeia et al., 2000; Tepperman and Narayanan, 2008) aim to assign a score to the learner speech, we hope to provide more focused feedback by automatically identifying prosodic units, such as pitch accents in English or tone in Mandarin, to enable direct comparison with gold-standard native utterances. There has been substantial progress in automatic pitch accent recognition for native speech, achieving accuracies above 80% for acousticfeature based recognition in multi-speaker corpora (Sridhar et al., 2007; Levow, 2008). However, there has been little study of pitch accent recognition in non-native speech. Given the challenges posed for automatic speech recognition of non-native speech, we ask whether recognition of intonational categories is practical for non-native speech. To lay the foundations for computerassisted intonation tutoring, we ask whether competitive accuracies can be achieved on non-native speech. We further investigate whether good recognition accuracy can be achieved using relatively available labeled native or near-native speech, or whether it will be necessary to collect larger amounts of</context>
<context position="6673" citStr="Levow, 2008" startWordPosition="1021" endWordPosition="1022">en in the characterization of pitch accents such as down-stepped high and in phenomena such as downdrift across a phrase. Further, local coarticulation with neighboring tones has been shown to have a significant impact on the realization of prosodic elements, due to articulatory constraints (Xu and Sun, 2002). The use of prosodic and coarticulatory context has improved the effectiveness of tone and pitch accent recognition in a range of languages (Mandarin (Wang and Seneff, 2000), English (Sun, 2002)) and learning frameworks (decision trees (Sun, 2002), HMMs (Wang and Seneff, 2000), and CRFs (Levow, 2008)). Thus, in this work, we employ a rich contextual feature set, based on that in (Levow, 2008). We build on the pitch target approximation model, taking the syllable as the domain of tone prediction with a pitch height and contour target approached exponentially over the course of the syllable, consistent with (Sun, 2002). We employ an acoustic model at the syllable level, employing pitch, intensity and duration measures. The acoustic measures are computed using Praat’s (Boersma, 2001) ”To pitch” and ”To intensity.” We log-scaled and speaker-normalized all pitch and intensity values. We comput</context>
<context position="13272" citStr="Levow, 2008" startWordPosition="2069" endWordPosition="2070"> accuracies 271 c1 c2 c3 e1 e2 sl na Within-group Accuracy 79.1 80.9 80.6 81 82.5 82.4 81.2 Cross-group Accuracy (na) 77.2 79 81.4 80.3 82.5 83.2 Cross-group Accuracy (sl) 77.3 79.9 82 80.5 82.9 81.6 Common Class 56.9 59.6 56.2 70.2 64 65.5 63.6 Table 2: Pitch accent recognition, within-group, cross-group with native and near-native training, and most common class baseline: Non-native (plain), ’Super-learner’ (underline sl), Native (bold na) range from approximately 79% to 82.5%. These levels are consistent with syllable-, acousticfeature-based prosodic recognition reported in the literature (Levow, 2008). A summary of these results appears in Table 2. In the cross-group training and testing condition, we observe some variations in accuracy, for some training sets. However, crucially none of the differences between native-based or near-native training and withingroup training reach significance for the binary pitch accent recognition task. 6 Conclusion We have demonstrated the effectiveness of pitch accent recognition on both native and non-native data from the LeaP corpus, based on significant differences between accented and unaccented syllables in both native and non-native speech. Although</context>
</contexts>
<marker>Levow, 2008</marker>
<rawString>G.-A. Levow. 2008. Automatic prosodic labeling with conditional random fields and rich acoustic features. In Proceedings of the IJCNLP 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-T Milde</author>
<author>U Gut</author>
</authors>
<title>A prosodic corpus of non-native speech.</title>
<date>2002</date>
<booktitle>In Proceedings of the Speech Prosody 2002 Conference,</booktitle>
<pages>503--506</pages>
<contexts>
<context position="4463" citStr="Milde and Gut, 2002" startWordPosition="658" endWordPosition="661">, Singapore, 4 August 2009. c�2009 ACL and AFNLP ID Description c1 non-native, before prosody training c2 non-native, after first prosody training c3 non-native, after second prosody training e1 non-native, before going abroad e2 non-native, after going abroad sl ’super-learner’, near-native na native Table 1: Speaker groups, with ID and description in the LeaP Corpus then describe the classifier setting and experimental results in Section 5 as well as discussion. Finally, we present some conclusions and plans for future work. 2 LeaP Corpus and the Dataset We employ data from the LeaP Corpus (Milde and Gut, 2002), collected at the University of Bielefeld as part of the “Learning Prosody in a Foreign Language” project. Details of the corpus (Milde and Gut, 2002), inter-rater reliability measures (Gut and Bayerl, 2004), and other research findings (Gut, 2009) have been reported. Here we focus on the read English segment of the corpus that has been labelled with modified EToBI tags1 , to enable better comparison with prior results of prosodic labelling accuracy and also to better model a typical language laboratory setting where students read or repeat. This yields a total of 37 recordings of just over 3</context>
</contexts>
<marker>Milde, Gut, 2002</marker>
<rawString>J.-T. Milde and U. Gut. 2002. A prosodic corpus of non-native speech. In Proceedings of the Speech Prosody 2002 Conference, pages 503–506.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Rangarajan Sridhar</author>
<author>S Bangalore</author>
<author>S Narayanan</author>
</authors>
<title>Exploiting acoustic and syntactic features for prosody labeling in a maximum entropy framework.</title>
<date>2007</date>
<booktitle>In Proceedings of HLT NAACL</booktitle>
<pages>1--8</pages>
<contexts>
<context position="2136" citStr="Sridhar et al., 2007" startWordPosition="307" endWordPosition="310">arning setting, we investigate automatic prosodic labelling of non-native speech. While many prior systems (Teixeia et al., 2000; Tepperman and Narayanan, 2008) aim to assign a score to the learner speech, we hope to provide more focused feedback by automatically identifying prosodic units, such as pitch accents in English or tone in Mandarin, to enable direct comparison with gold-standard native utterances. There has been substantial progress in automatic pitch accent recognition for native speech, achieving accuracies above 80% for acousticfeature based recognition in multi-speaker corpora (Sridhar et al., 2007; Levow, 2008). However, there has been little study of pitch accent recognition in non-native speech. Given the challenges posed for automatic speech recognition of non-native speech, we ask whether recognition of intonational categories is practical for non-native speech. To lay the foundations for computerassisted intonation tutoring, we ask whether competitive accuracies can be achieved on non-native speech. We further investigate whether good recognition accuracy can be achieved using relatively available labeled native or near-native speech, or whether it will be necessary to collect lar</context>
</contexts>
<marker>Sridhar, Bangalore, Narayanan, 2007</marker>
<rawString>V. Rangarajan Sridhar, S. Bangalore, and S. Narayanan. 2007. Exploiting acoustic and syntactic features for prosody labeling in a maximum entropy framework. In Proceedings of HLT NAACL 2007, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuejing Sun</author>
</authors>
<title>Pitch accent prediction using ensemble machine learning.</title>
<date>2002</date>
<booktitle>In Proceedings of ICSLP2002.</booktitle>
<contexts>
<context position="6371" citStr="Sun, 2002" startWordPosition="971" endWordPosition="972">a from German speakers. 2Length of recordings varies due to differences in syllabification and cliticization, as well as disfluencies and reading errors. 3 Acoustic-Prosodic Features Recent research has highlighted the importance of context for both tone and intonation. The role of context can be seen in the characterization of pitch accents such as down-stepped high and in phenomena such as downdrift across a phrase. Further, local coarticulation with neighboring tones has been shown to have a significant impact on the realization of prosodic elements, due to articulatory constraints (Xu and Sun, 2002). The use of prosodic and coarticulatory context has improved the effectiveness of tone and pitch accent recognition in a range of languages (Mandarin (Wang and Seneff, 2000), English (Sun, 2002)) and learning frameworks (decision trees (Sun, 2002), HMMs (Wang and Seneff, 2000), and CRFs (Levow, 2008)). Thus, in this work, we employ a rich contextual feature set, based on that in (Levow, 2008). We build on the pitch target approximation model, taking the syllable as the domain of tone prediction with a pitch height and contour target approached exponentially over the course of the syllable, co</context>
</contexts>
<marker>Sun, 2002</marker>
<rawString>Xuejing Sun. 2002. Pitch accent prediction using ensemble machine learning. In Proceedings of ICSLP2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Teixeia</author>
<author>H Franco</author>
<author>E Shriberg</author>
<author>K Precoda</author>
<author>K Somnez</author>
</authors>
<title>Prosodic features for automatic text-independent evaluation of degree of nativeness for language learners.</title>
<date>2000</date>
<booktitle>In Proceedings of ICSLP</booktitle>
<contexts>
<context position="1644" citStr="Teixeia et al., 2000" startWordPosition="231" endWordPosition="234">owever, intonation has been less-emphasized both in classroom and computer-assisted language instruction (Chun, 1998). Outside of tone languages, it can be difficult to characterize the factors that lead to non-native prosody in learner speech, and it is difficult for instructors to find time for the one-on-one interaction that is required to provide feedback and instruction in prosody. To address these problems and enable automatic feedback to learners in a computer-assisted language learning setting, we investigate automatic prosodic labelling of non-native speech. While many prior systems (Teixeia et al., 2000; Tepperman and Narayanan, 2008) aim to assign a score to the learner speech, we hope to provide more focused feedback by automatically identifying prosodic units, such as pitch accents in English or tone in Mandarin, to enable direct comparison with gold-standard native utterances. There has been substantial progress in automatic pitch accent recognition for native speech, achieving accuracies above 80% for acousticfeature based recognition in multi-speaker corpora (Sridhar et al., 2007; Levow, 2008). However, there has been little study of pitch accent recognition in non-native speech. Given</context>
</contexts>
<marker>Teixeia, Franco, Shriberg, Precoda, Somnez, 2000</marker>
<rawString>C. Teixeia, H. Franco, E. Shriberg, K. Precoda, and K. Somnez. 2000. Prosodic features for automatic text-independent evaluation of degree of nativeness for language learners. In Proceedings of ICSLP 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Tepperman</author>
<author>S Narayanan</author>
</authors>
<title>Better nonnative intonation scores through prosodic theory.</title>
<date>2008</date>
<booktitle>In Proceedings of Interspeech</booktitle>
<contexts>
<context position="1676" citStr="Tepperman and Narayanan, 2008" startWordPosition="235" endWordPosition="239"> been less-emphasized both in classroom and computer-assisted language instruction (Chun, 1998). Outside of tone languages, it can be difficult to characterize the factors that lead to non-native prosody in learner speech, and it is difficult for instructors to find time for the one-on-one interaction that is required to provide feedback and instruction in prosody. To address these problems and enable automatic feedback to learners in a computer-assisted language learning setting, we investigate automatic prosodic labelling of non-native speech. While many prior systems (Teixeia et al., 2000; Tepperman and Narayanan, 2008) aim to assign a score to the learner speech, we hope to provide more focused feedback by automatically identifying prosodic units, such as pitch accents in English or tone in Mandarin, to enable direct comparison with gold-standard native utterances. There has been substantial progress in automatic pitch accent recognition for native speech, achieving accuracies above 80% for acousticfeature based recognition in multi-speaker corpora (Sridhar et al., 2007; Levow, 2008). However, there has been little study of pitch accent recognition in non-native speech. Given the challenges posed for automa</context>
</contexts>
<marker>Tepperman, Narayanan, 2008</marker>
<rawString>J. Tepperman and S. Narayanan. 2008. Better nonnative intonation scores through prosodic theory. In Proceedings of Interspeech 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Wang</author>
<author>S Seneff</author>
</authors>
<title>Improved tone recognition by normalizing for coarticulation and intonation effects.</title>
<date>2000</date>
<booktitle>In Proceedings of 6th International Conference on Spoken Language Processing.</booktitle>
<contexts>
<context position="6545" citStr="Wang and Seneff, 2000" startWordPosition="998" endWordPosition="1001">ic-Prosodic Features Recent research has highlighted the importance of context for both tone and intonation. The role of context can be seen in the characterization of pitch accents such as down-stepped high and in phenomena such as downdrift across a phrase. Further, local coarticulation with neighboring tones has been shown to have a significant impact on the realization of prosodic elements, due to articulatory constraints (Xu and Sun, 2002). The use of prosodic and coarticulatory context has improved the effectiveness of tone and pitch accent recognition in a range of languages (Mandarin (Wang and Seneff, 2000), English (Sun, 2002)) and learning frameworks (decision trees (Sun, 2002), HMMs (Wang and Seneff, 2000), and CRFs (Levow, 2008)). Thus, in this work, we employ a rich contextual feature set, based on that in (Levow, 2008). We build on the pitch target approximation model, taking the syllable as the domain of tone prediction with a pitch height and contour target approached exponentially over the course of the syllable, consistent with (Sun, 2002). We employ an acoustic model at the syllable level, employing pitch, intensity and duration measures. The acoustic measures are computed using Praat</context>
</contexts>
<marker>Wang, Seneff, 2000</marker>
<rawString>C. Wang and S. Seneff. 2000. Improved tone recognition by normalizing for coarticulation and intonation effects. In Proceedings of 6th International Conference on Spoken Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi Xu</author>
<author>X Sun</author>
</authors>
<title>Maximum speed of pitch change and how it may relate to speech.</title>
<date>2002</date>
<journal>Journal of the Acoustical Society ofAmerica,</journal>
<pages>111</pages>
<contexts>
<context position="6371" citStr="Xu and Sun, 2002" startWordPosition="969" endWordPosition="972"> to data from German speakers. 2Length of recordings varies due to differences in syllabification and cliticization, as well as disfluencies and reading errors. 3 Acoustic-Prosodic Features Recent research has highlighted the importance of context for both tone and intonation. The role of context can be seen in the characterization of pitch accents such as down-stepped high and in phenomena such as downdrift across a phrase. Further, local coarticulation with neighboring tones has been shown to have a significant impact on the realization of prosodic elements, due to articulatory constraints (Xu and Sun, 2002). The use of prosodic and coarticulatory context has improved the effectiveness of tone and pitch accent recognition in a range of languages (Mandarin (Wang and Seneff, 2000), English (Sun, 2002)) and learning frameworks (decision trees (Sun, 2002), HMMs (Wang and Seneff, 2000), and CRFs (Levow, 2008)). Thus, in this work, we employ a rich contextual feature set, based on that in (Levow, 2008). We build on the pitch target approximation model, taking the syllable as the domain of tone prediction with a pitch height and contour target approached exponentially over the course of the syllable, co</context>
</contexts>
<marker>Xu, Sun, 2002</marker>
<rawString>Yi Xu and X. Sun. 2002. Maximum speed of pitch change and how it may relate to speech. Journal of the Acoustical Society ofAmerica, 111.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>