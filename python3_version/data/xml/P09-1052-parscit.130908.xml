<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000076">
<title confidence="0.995142">
Employing Topic Models for Pattern-based Semantic Class Discovery
</title>
<author confidence="0.999298">
Huibin Zhang1* Mingjie Zhu2* Shuming Shi3 Ji-Rong Wen3
</author>
<affiliation confidence="0.988749">
1Nankai University
2University of Science and Technology of China
3Microsoft Research Asia
</affiliation>
<email confidence="0.976522">
{v-huibzh, v-mingjz, shumings, jrwen}@microsoft.com
</email>
<sectionHeader confidence="0.99856" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999950217391304">
A semantic class is a collection of items
(words or phrases) which have semantically
peer or sibling relationship. This paper studies
the employment of topic models to automati-
cally construct semantic classes, taking as the
source data a collection of raw semantic
classes (RASCs), which were extracted by ap-
plying predefined patterns to web pages. The
primary requirement (and challenge) here is
dealing with multi-membership: An item may
belong to multiple semantic classes; and we
need to discover as many as possible the dif-
ferent semantic classes the item belongs to. To
adopt topic models, we treat RASCs as “doc-
uments”, items as “words”, and the final se-
mantic classes as “topics”. Appropriate
preprocessing and postprocessing are per-
formed to improve results quality, to reduce
computation cost, and to tackle the fixed-k
constraint of a typical topic model. Experi-
ments conducted on 40 million web pages
show that our approach could yield better re-
sults than alternative approaches.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9955164">
Semantic class construction (Lin and Pantel,
2001; Pantel and Lin, 2002; Pasca, 2004; Shinza-
to and Torisawa, 2005; Ohshima et al., 2006)
tries to discover the peer or sibling relationship
among terms or phrases by organizing them into
semantic classes. For example, {red, white,
black...} is a semantic class consisting of color
instances. A popular way for semantic class dis-
covery is pattern-based approach, where prede-
fined patterns (Table 1) are applied to a
</bodyText>
<footnote confidence="0.947652">
 This work was performed when the authors were interns at
Microsoft Research Asia
</footnote>
<bodyText confidence="0.99975325">
collection of web pages or an online web search
engine to produce some raw semantic classes
(abbreviated as RASCs, Table 2). RASCs cannot
be treated as the ultimate semantic classes, be-
cause they are typically noisy and incomplete, as
shown in Table 2. In addition, the information of
one real semantic class may be distributed in lots
of RASCs (R2 and R3 in Table 2).
</bodyText>
<table confidence="0.9780622">
Type Pattern
SENT NP {, NP}*{,} (and|or) {other} NP
TAG &lt;UL&gt; &lt;LI&gt;item&lt;/LI&gt; ... &lt;LI&gt;item&lt;/LI&gt; &lt;/UL&gt;
TAG &lt;SELECT&gt; &lt;OPTION&gt;item...&lt;OPTION&gt;item &lt;/SELECT&gt;
* SENT: Sentence structure patterns; TAG: HTML Tag patterns
</table>
<tableCaption confidence="0.999761">
Table 1. Sample patterns
</tableCaption>
<note confidence="0.986738">
R1: {gold, silver, copper, coal, iron, uranium}
R2: {red, yellow, color, gold, silver, copper}
R3: {red, green, blue, yellow}
R4: {HTML, Text, PDF, MS Word, Any file type}
R5: {Today, Tomorrow, Wednesday, Thursday, Friday,
Saturday, Sunday}
R6: {Bush, Iraq, Photos, USA, War}
</note>
<tableCaption confidence="0.997686">
Table 2. Sample raw semantic classes (RASCs)
</tableCaption>
<bodyText confidence="0.991129842105263">
This paper aims to discover high-quality se-
mantic classes from a large collection of noisy
RASCs. The primary requirement (and chal-
lenge) here is to deal with multi-membership, i.e.,
one item may belong to multiple different seman-
tic classes. For example, the term “Lincoln” can
simultaneously represent a person, a place, or a
car brand name. Multi-membership is more pop-
ular than at a first glance, because quite a lot of
English common words have also been borrowed
as company names, places, or product names.
For a given item (as a query) which belongs to
multiple semantic classes, we intend to return the
semantic classes separately, rather than mixing
all their items together.
Existing pattern-based approaches only pro-
vide very limited support to multi-membership.
For example, RASCs with the same labels (or
hypernyms) are merged in (Pasca, 2004) to gen-
</bodyText>
<page confidence="0.986387">
459
</page>
<note confidence="0.999612">
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 459–467,
Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999932054545455">
erate the ultimate semantic classes. This is prob-
lematic, because RASCs may not have (accurate)
hypernyms with them.
In this paper, we propose to use topic models
to address the problem. In some topic models, a
document is modeled as a mixture of hidden top-
ics. The words of a document are generated ac-
cording to the word distribution over the topics
corresponding to the document (see Section 2 for
details). Given a corpus, the latent topics can be
obtained by a parameter estimation procedure.
Topic modeling provides a formal and conve-
nient way of dealing with multi-membership,
which is our primary motivation of adopting top-
ic models here. To employ topic models, we treat
RASCs as “documents”, items as “words”, and
the final semantic classes as “topics”.
There are, however, several challenges in ap-
plying topic models to our problem. To begin
with, the computation is intractable for
processing a large collection of RASCs (our da-
taset for experiments contains 2.7 million unique
RASCs extracted from 40 million web pages).
Second, typical topic models require the number
of topics (k) to be given. But it lacks an easy way
of acquiring the ideal number of semantic classes
from the source RASC collection. For the first
challenge, we choose to apply topic models to
the RASCs containing an item q, rather than the
whole RASC collection. In addition, we also per-
form some preprocessing operations in which
some items are discarded to further improve effi-
ciency. For the second challenge, considering
that most items only belong to a small number of
semantic classes, we fix (for all items q) a topic
number which is slightly larger than the number
of classes an item could belong to. And then a
postprocessing operation is performed to merge
the results of topic models to generate the ulti-
mate semantic classes.
Experimental results show that, our topic
model approach is able to generate higher-quality
semantic classes than popular clustering algo-
rithms (e.g., K-Medoids and DBSCAN).
We make two contributions in the paper: On
one hand, we find an effective way of construct-
ing high-quality semantic classes in the pattern-
based category which deals with multi-
membership. On the other hand, we demonstrate,
for the first time, that topic modeling can be uti-
lized to help mining the peer relationship among
words. In contrast, the general related relation-
ship between words is extracted in existing topic
modeling applications. Thus we expand the ap-
plication scope of topic modeling.
</bodyText>
<sectionHeader confidence="0.993331" genericHeader="method">
2 Topic Models
</sectionHeader>
<bodyText confidence="0.9997081875">
In this section we briefly introduce the two wide-
ly used topic models which are adopted in our
paper. Both of them model a document as a mix-
ture of hidden topics. The words of every docu-
ment are assumed to be generated via a
generative probability process. The parameters of
the model are estimated from a training process
over a given corpus, by maximizing the likelih-
ood of generating the corpus. Then the model can
be utilized to inference a new document.
pLSI: The probabilistic Latent Semantic In-
dexing Model (pLSI) was introduced in Hof-
mann (1999), arose from Latent Semantic
Indexing (Deerwester et al., 1990). The follow-
ing process illustrates how to generate a docu-
ment d in pLSI:
</bodyText>
<listItem confidence="0.853705142857143">
1. Pick a topic mixture distribution 𝑝(∙ |𝑑).
2. For each word wi in d
a. Pick a latent topic z with the probabil-
ity 𝑝 (𝑧 |𝑑) for wi
b. Generate wi with probability 𝑝 (𝑤𝑖 |𝑧)
So with k latent topics, the likelihood of gene-
rating a document d is
</listItem>
<equation confidence="0.9881025">
𝑝(𝑑) = 𝑝 𝑤𝑖 𝑧 𝑝(𝑧|𝑑) (2.1)
𝑖 𝑧
</equation>
<bodyText confidence="0.9794314">
LDA (Blei et al., 2003): In LDA, the topic
mixture is drawn from a conjugate Dirichlet prior
that remains the same for all documents (Figure
1). The generative process for each document in
the corpus is,
</bodyText>
<listItem confidence="0.995631222222222">
1. Choose document length N from a Pois-
son distribution Poisson(𝜉).
2. Choose 𝜃 from a Dirichlet distribution
with parameter α.
3. For each of the N words wi.
a. Choose a topic z from a Multinomial
distribution with parameter 𝜃.
b. Pick a word wi from 𝑝 𝑤𝑖 𝑧, 𝛽 .
So the likelihood of generating a document is
</listItem>
<equation confidence="0.974358">
𝑝(𝑑) = 𝑝(𝜃|𝛼)
𝜃
</equation>
<figureCaption confidence="0.9947065">
Figure 1. Graphical model representation of LDA,
from Blei et al. (2003)
</figureCaption>
<figure confidence="0.885840444444445">
α
θ
z
w
β
N
M
𝑝(𝑧|𝜃)𝑝 𝑤𝑖 𝑧, 𝛽 𝑑𝜃 (2.2)
𝑖 𝑧
</figure>
<page confidence="0.998839">
460
</page>
<sectionHeader confidence="0.99063" genericHeader="method">
3 Our Approach
</sectionHeader>
<bodyText confidence="0.99933675">
The source data of our approach is a collection
(denoted as CR) of RASCs extracted via applying
patterns to a large collection of web pages. Given
an item as an input query, the output of our ap-
proach is one or multiple semantic classes for the
item. To be applicable in real-world dataset, our
approach needs to be able to process at least mil-
lions of RASCs.
</bodyText>
<subsectionHeader confidence="0.999371">
3.1 Main Idea
</subsectionHeader>
<bodyText confidence="0.9991246">
As reviewed in Section 2, topic modeling pro-
vides a formal and convenient way of grouping
documents and words to topics. In order to apply
topic models to our problem, we map RASCs to
documents, items to words, and treat the output
topics yielded from topic modeling as our seman-
tic classes (Table 3). The motivation of utilizing
topic modeling to solve our problem and building
the above mapping comes from the following
observations.
</bodyText>
<listItem confidence="0.952219">
1) In our problem, one item may belong to
multiple semantic classes; similarly in topic
modeling, a word can appear in multiple top-
ics.
2) We observe from our source data that
some RASCs are comprised of items in mul-
tiple semantic classes. And at the same time,
one document could be related to multiple
topics in some topic models (e.g., pLSI and
LDA).
</listItem>
<tableCaption confidence="0.9643335">
Topic modeling Semantic class construction
word item (word or phrase)
document RASC
topic semantic class
Table 3. The mapping from the concepts in topic
modeling to those in semantic class construction
</tableCaption>
<bodyText confidence="0.986168086956522">
Due to the above observations, we hope topic
modeling can be employed to construct semantic
classes from RASCs, just as it has been used in
assigning documents and words to topics.
There are some critical challenges and issues
which should be properly addressed when topic
models are adopted here.
Efficiency: Our RASC collection CR contains
about 2.7 million unique RASCs and 26 million
(1 million unique) items. Building topic models
directly for such a large dataset may be computa-
tionally intractable. To overcome this challenge,
we choose to apply topic models to the RASCs
containing a specific item rather than the whole
RASC collection. Please keep in mind that our
goal in this paper is to construct the semantic
classes for an item when the item is given as a
query. For one item q, we denote CR(q) to be all
the RASCs in CR containing the item. We believe
building a topic model over CR(q) is much more
effective because it contains significantly fewer
“documents”, “words”, and “topics”. To further
improve efficiency, we also perform preprocess-
ing (refer to Section 3.4 for details) before build-
ing topic models for CR(q), where some low-
frequency items are removed.
Determine the number of topics: Most topic
models require the number of topics to be known
beforehand1. However, it is not an easy task to
automatically determine the exact number of se-
mantic classes an item q should belong to. Ac-
tually the number may vary for different q. Our
solution is to set (for all items q) the topic num-
ber to be a fixed value (k=5 in our experiments)
which is slightly larger than the number of se-
mantic classes most items could belong to. Then
we perform postprocessing for the k topics to
produce the final properly semantic classes.
In summary, our approach contains three
phases (Figure 2). We build topic models for
every CR(q), rather than the whole collection CR.
A preprocessing phase and a postprocessing
phase are added before and after the topic model-
ing phase to improve efficiency and to overcome
the fixed-k problem. The details of each phase
are presented in the following subsections.
</bodyText>
<figureCaption confidence="0.990474">
Figure 2. Main phases of our approach
</figureCaption>
<subsectionHeader confidence="0.999833">
3.2 Adopting Topic Models
</subsectionHeader>
<bodyText confidence="0.97819875">
For an item q, topic modeling is adopted to
process the RASCs in CR(q) to generate k seman-
tic classes. Here we use LDA as an example to
1 Although there is study of non-parametric Bayesian mod-
els (Li et al., 2007) which need no prior knowledge of topic
number, the computational complexity seems to exceed our
efficiency requirement and we shall leave this to future
work.
</bodyText>
<figure confidence="0.998197227272728">
Item q
CR
Preprocessing
CR(q)
R580
R1
R2
R400
�
R2�
R1�
Topic
modeling
T1
T2
T3
T4
T5
Postprocessing
C1
C2
C3
</figure>
<page confidence="0.998832">
461
</page>
<bodyText confidence="0.827179">
illustrate the process. The case of other genera-
tive topic models (e.g., pLSI) is very similar.
According to the assumption of LDA and our
concept mapping in Table 3, a RASC (“docu-
ment”) is viewed as a mixture of hidden semantic
classes (“topics”). The generative process for a
RASC R in the “corpus” CR(q) is as follows,
</bodyText>
<listItem confidence="0.995502375">
1) Choose a RASC size (i.e., the number of
items in R): NR ~ Poisson().
2) Choose a k-dimensional vector 9R from a
Dirichlet distribution with parameter a.
3) For each of the NR items an:
a) Pick a semantic class zn from a mul-
tinomial distribution with parameter
9R.
</listItem>
<bodyText confidence="0.99339514">
b) Pick an item an from p(an |zn, fl) ,
where the item probabilities are pa-
rameterized by the matrix fl.
There are three parameters in the model: � (a
scalar), a (a k-dimensional vector), and fl (a
k x V matrix where V is the number of distinct
items in CR(q)). The parameter values can be ob-
tained from a training (or called parameter esti-
mation) process over CR(q), by maximizing the
likelihood of generating the corpus. Once fl is
determined, we are able to compute p(a|z,fl),
the probability of item a belonging to semantic
class z. Therefore we can determine the members
of a semantic class z by selecting those items
with high p(a1z,fl) values.
The number of topics k is assumed known and
fixed in LDA. As has been discussed in Section
3.1, we set a constant k value for all different
CR(q). And we rely on the postprocessing phase
to merge the semantic classes produced by the
topic model to generate the ultimate semantic
classes.
When topic modeling is used in document
classification, an inference procedure is required
to determine the topics for a new document.
Please note that inference is not needed in our
problem.
One natural question here is: Considering that
in most topic modeling applications, the words
within a resultant topic are typically semantically
related but may not be in peer relationship, then
what is the intuition that the resultant topics here
are semantic classes rather than lists of generally
related words? The magic lies in the “docu-
ments” we used in employing topic models.
Words co-occurred in real documents tend to be
semantically related; while items co-occurred in
RASCs tend to be peers. Experimental results
show that most items in the same output seman-
tic class have peer relationship.
It might be noteworthy to mention the exchan-
geability or “bag-of-words” assumption in most
topic models. Although the order of words in a
document may be important, standard topic mod-
els neglect the order for simplicity and other rea-
sons2. The order of items in a RASC is clearly
much weaker than the order of words in an ordi-
nary document. In some sense, topic models are
more suitable to be used here than in processing
an ordinary document corpus.
</bodyText>
<subsectionHeader confidence="0.999292">
3.3 Preprocessing and Postprocessing
</subsectionHeader>
<bodyText confidence="0.999407764705882">
Preprocessing is applied to CR(q) before we build
topic models for it. In this phase, we discard
from all RASCs the items with frequency (i.e.,
the number of RASCs containing the item) less
than a threshold h. A RASC itself is discarded
from CR(q) if it contains less than two items after
the item-removal operations. We choose to re-
move low-frequency items, because we found
that low-frequency items are seldom important
members of any semantic class for q. So the goal
is to reduce the topic model training time (by
reducing the training data) without sacrificing
results quality too much. In the experiments sec-
tion, we compare the approaches with and with-
out preprocessing in terms of results quality and
efficiency. Interestingly, experimental results
show that, for some small threshold values, the
results quality becomes higher after preprocess-
ing is performed. We will give more discussions
in Section 4.
In the postprocessing phase, the output seman-
tic classes (“topics”) of topic modeling are
merged to generate the ultimate semantic classes.
As indicated in Sections 3.1 and 3.2, we fix the
number of topics (k=5) for different corpus CR(q)
in employing topic models. For most items q,
this is a larger value than the real number of se-
mantic classes the item belongs to. As a result,
one real semantic class may be divided into mul-
tiple topics. Therefore one core operation in this
phase is to merge those topics into one semantic
class. In addition, the items in each semantic
class need to be properly ordered. Thus main
operations include,
</bodyText>
<listItem confidence="0.8452585">
1) Merge semantic classes
2) Sort the items in each semantic class
</listItem>
<bodyText confidence="0.72796">
Now we illustrate how to perform the opera-
tions.
Merge semantic classes: The merge process
is performed by repeatedly calculating the simi-
2 There are topic model extensions considering word order
in documents, such as Griffiths et al. (2005).
</bodyText>
<page confidence="0.997394">
462
</page>
<bodyText confidence="0.9992542">
larity between two semantic classes and merging
the two ones with the highest similarity until the
similarity is under a threshold. One simple and
straightforward similarity measure is the Jaccard
coefficient,
</bodyText>
<equation confidence="0.467992">
C1 U C21
</equation>
<bodyText confidence="0.999876">
where C1 n C2 and C1 U C2 are respectively the
intersection and union of semantic classes C1 and
C2. This formula might be over-simple, because
the similarity between two different items is not
exploited. So we propose the following measure,
</bodyText>
<equation confidence="0.969936">
(3.2)
C1 1• 1C21
</equation>
<bodyText confidence="0.998902714285714">
where |C |is the number of items in semantic
class C, and sim(a,b) is the similarity between
items a and b, which will be discussed shortly. In
Section 4, we compare the performance of the
above two formulas by experiments.
Sort items: We assign an importance score to
every item in a semantic class and sort them ac-
cording to the importance scores. Intuitively, an
item should get a high rank if the average simi-
larity between the item and the other items in the
semantic class is high, and if it has high similari-
ty to the query item q. Thus we calculate the im-
portance of item a in a semantic class C as
follows,
</bodyText>
<equation confidence="0.99768">
g(a|C) = A•sim(a,C)+(1-A) •sim(a,q) (3.3)
</equation>
<bodyText confidence="0.999974846153846">
where A is a parameter in [0,1], sim(a,q) is the
similarity between a and the query item q, and
sim(a,C) is the similarity between a and C, calcu-
lated as,
Item similarity calculation: Formulas 3.2,
3.3, and 3.4 rely on the calculation of the similar-
ity between two items.
One simple way of estimating item similarity
is to count the number of RASCs containing both
of them. We extend such an idea by distinguish-
ing the reliability of different patterns and pu-
nishing term similarity contributions from the
same site. The resultant similarity formula is,
</bodyText>
<equation confidence="0.993867333333333">
m ki
sim(a, b) = I log(1 + I w(P(Ci,j ))
i=1 j=1
</equation>
<bodyText confidence="0.999976642857143">
where Ci,j is a RASC containing both a and b,
P(Ci,j) is the pattern via which the RASC is ex-
tracted, and w(P) is the weight of pattern P. As-
sume all these RASCs belong to m sites with Ci,j
extracted from a page in site i, and ki being the
number of RASCs corresponding to site i. To
determine the weight of every type of pattern, we
randomly selected 50 RASCs for each pattern
and labeled their quality. The weight of each
kind of pattern is then determined by the average
quality of all labeled RASCs corresponding to it.
The efficiency of postprocessing is not a prob-
lem, because the time cost of postprocessing is
much less than that of the topic modeling phase.
</bodyText>
<subsectionHeader confidence="0.828197">
3.4 Discussion
</subsectionHeader>
<subsubsectionHeader confidence="0.841438">
3.4.1 Efficiency of processing popular items
</subsubsectionHeader>
<bodyText confidence="0.999994235294118">
Our approach receives a query item q from users
and returns the semantic classes containing the
query. The maximal query processing time
should not be larger than several seconds, be-
cause users would not like to wait more time.
Although the average query processing time of
our approach is much shorter than 1 second (see
Table 4 in Section 4), it takes several minutes to
process a popular item such as “Washington”,
because it is contained in a lot of RASCs. In or-
der to reduce the maximal online processing
time, our solution is offline processing popular
items and storing the resultant semantic classes
on disk. The time cost of offline processing is
feasible, because we spent about 15 hours on a 4-
core machine to complete the offline processing
for all the items in our RASC collection.
</bodyText>
<subsectionHeader confidence="0.685949">
3.4.2 Alternative approaches
</subsectionHeader>
<bodyText confidence="0.999930153846154">
One may be able to easily think of other ap-
proaches to address our problem. Here we dis-
cuss some alternative approaches which are
treated as our baseline in experiments.
RASC clustering: Given a query item q, run a
clustering algorithm over CR(q) and merge all
RASCs in the same cluster as one semantic class.
Formula 3.1 or 3.2 can be used to compute the
similarity between RASCs in performing cluster-
ing. We try two clustering algorithms in experi-
ments: K-Medoids and DBSCAN. Please note k-
means cannot be utilized here because coordi-
nates are not available for RASCs. One draw-
back of RASC clustering is that it cannot deal
with the case of one RASC containing the items
from multiple semantic classes.
Item clustering: By Formula 3.5, we are able
to construct an item graph GI to record the
neighbors (in terms of similarity) of each item.
Given a query item q, we first retrieve its neigh-
bors from GI, and then run a clustering algorithm
over the neighbors. As in the case of RASC clus-
tering, we try two clustering algorithms in expe-
riments: K-Medoids and DBSCAN. The primary
disadvantage of item clustering is that it cannot
assign an item (except for the query item q) to
</bodyText>
<equation confidence="0.999121777777778">
sim(a, C) =
bEC sim(a, b)
(3.4)
1C1
sim(C1, C2 ) =
C1 n C21 (3.1)
sim(C1, C2 ) =
aEC1 LbEC2 sim(a, b)
) (3.5)
</equation>
<page confidence="0.997712">
463
</page>
<bodyText confidence="0.9990782">
multiple semantic classes. As a result, when we
input “gold” as the query, the item “silver” can
only be assigned to one semantic class, although
the term can simultaneously represents a color
and a chemical element.
</bodyText>
<sectionHeader confidence="0.999854" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.997034">
4.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.981189380952381">
Datasets: By using the Open Directory Project
(ODP3) URLs as seeds, we crawled about 40 mil-
lion English web pages in a breadth-first way.
RASCs are extracted via applying a list of sen-
tence structure patterns and HTML tag patterns
(see Table 1 for some examples). Our RASC col-
lection CR contains about 2.7 million unique
RASCs and 1 million distinct items.
Query set and labeling: We have volunteers
to try Google Sets4, record their queries being
used, and select overall 55 queries to form our
query set. For each query, the results of all ap-
proaches are mixed together and labeled by fol-
lowing two steps. In the first step, the standard
(or ideal) semantic classes (SSCs) for the query
are manually determined. For example, the ideal
semantic classes for item “Georgia” may include
Countries, and U.S. states. In the second step,
each item is assigned a label of “Good”, “Fair”,
or “Bad” with respect to each SSC. For example,
“silver” is labeled “Good” with respect to “col-
ors” and “chemical elements”. We adopt metric
MnDCG (Section 4.2) as our evaluation metric.
Approaches for comparison: We compare
our approach with the alternative approaches dis-
cussed in Section 3.4.2.
LDA: Our approach with LDA as the topic
model. The implementation of LDA is based
on Blei’s code of variational EM for LDA5.
pLSI: Our approach with pLSI as the topic
model. The implementation of pLSI is based
on Schein, et al. (2002).
KMedoids-RASC: The RASC clustering ap-
proach illustrated in Section 3.4.2, with the
K-Medoids clustering algorithm utilized.
DBSCAN-RASC: The RASC clustering ap-
proach with DBSCAN utilized.
KMedoids-Item: The item clustering ap-
proach with the K-Medoids utilized.
DBSCAN-Item: The item clustering ap-
proach with the DBSCAN clustering algo-
rithm utilized.
</bodyText>
<footnote confidence="0.997707333333333">
3 http://www.dmoz.org
4 http://labs.google.com/sets
5 http://www.cs.princeton.edu/—blei/lda-c/
</footnote>
<bodyText confidence="0.9998865">
K-Medoids clustering needs to predefine the
cluster number k. We fix the k value for all dif-
ferent query item q, as has been done for the top-
ic model approach. For fair comparison, the same
postprocessing is made for all the approaches.
And the same preprocessing is made for all the
approaches except for the item clustering ones
(to which the preprocessing is not applicable).
</bodyText>
<subsectionHeader confidence="0.994747">
4.2 Evaluation Methodology
</subsectionHeader>
<bodyText confidence="0.9994676">
Each produced semantic class is an ordered list
of items. A couple of metrics in the information
retrieval (IR) community like Precision@10,
MAP (mean average precision), and nDCG
(normalized discounted cumulative gain) are
available for evaluating a single ranked list of
items per query (Croft et al., 2009). Among the
metrics, nDCG (Jarvelin and Kekalainen, 2000)
can handle our three-level judgments (“Good”,
“Fair”, and “Bad”, refer to Section 4.1),
</bodyText>
<equation confidence="0.9998375">
k i=1 G(i)/log(i + 1)
k i=1 G*(i)/log(i + 1)
</equation>
<bodyText confidence="0.999095875">
where G(i) is the gain value assigned to the i’th
item, and G*(i) is the gain value assigned to the
i’th item of an ideal (or perfect) ranking list.
Here we extend the IR metrics to the evalua-
tion of multiple ordered lists per query. We use
nDCG as the basic metric and extend it to
MnDCG.
Assume labelers have determined m SSCs
(SSC1—SSCm, refer to Section 4.1) for query q
and the weight (or importance) of SSCi is wi. As-
sume n semantic classes are generated by an ap-
proach and n1 of them have corresponding SSCs
(i.e., no appropriate SSC can be found for the
remaining n-n1 semantic classes). We define the
MnDCG score of an approach (with respect to
query q) as,
</bodyText>
<equation confidence="0.8919964">
MnDCG(q) =
where
10 if ki= 0
Score(SSCi) = 1 max (nDCG(GiJ) if ki 0 (4.3)
ki i E[1, ki]
</equation>
<bodyText confidence="0.999819571428572">
In the above formula, nDCG(Gi,j) is the nDCG
score of semantic class Gi,j; and ki denotes the
number of semantic classes assigned to SSCi. For
a list of queries, the MnDCG score of an algo-
rithm is the average of all scores for the queries.
The metric is designed to properly deal with
the following cases,
</bodyText>
<equation confidence="0.992736444444444">
nDCG@k =
(4.1)
in1 Wi • Score(SSCi)
m
i=1
n1 •
n
Wi
(4.2)
</equation>
<page confidence="0.995915">
464
</page>
<listItem confidence="0.823553666666667">
i). One semantic class is wrongly split into
multiple ones: Punished by dividing 𝑘𝑖 in
Formula 4.3;
</listItem>
<bodyText confidence="0.917085285714286">
ii). A semantic class is too noisy to be as-
signed to any SSC: Processed by the
“n1/n” in Formula 4.2;
iii). Fewer semantic classes (than the number
of SSCs) are produced: Punished in For-
mula 4.3 by assigning a zero value.
iv). Wrongly merge multiple semantic
classes into one: The nDCG score of the
merged one will be small because it is
computed with respect to only one single
SSC.
The gain values of nDCG for the three relev-
ance levels (“Bad”, “Fair”, and “Good”) are re-
spectively -1, 1, and 2 in experiments.
</bodyText>
<subsectionHeader confidence="0.998789">
4.3 Experimental Results
</subsectionHeader>
<subsubsectionHeader confidence="0.980433">
4.3.1 Overall performance comparison
</subsubsectionHeader>
<bodyText confidence="0.9994986">
Figure 3 shows the performance comparison be-
tween the approaches listed in Section 4.1, using
metrics MnDCG@n (n=1...10). Postprocessing
is performed for all the approaches, where For-
mula 3.2 is adopted to compute the similarity
between semantic classes. The results show that
that the topic modeling approaches produce
higher-quality semantic classes than the other
approaches. It indicates that the topic mixture
assumption of topic modeling can handle the
multi-membership problem very well here.
Among the alternative approaches, RASC clus-
tering behaves better than item clustering. The
reason might be that an item cannot belong to
multiple clusters in the two item clustering ap-
proaches, while RASC clustering allows this. For
the RASC clustering approaches, although one
item has the chance to belong to different seman-
tic classes, one RASC can only belong to one
semantic class.
</bodyText>
<figureCaption confidence="0.929943333333333">
Figure 3. Quality comparison (MnDCG@n) among
approaches (frequency threshold h = 4 in preprocess-
ing; k = 5 in topic models)
</figureCaption>
<subsubsectionHeader confidence="0.705152">
4.3.2 Preprocessing experiments
</subsubsectionHeader>
<bodyText confidence="0.99976">
Table 4 shows the average query processing time
and results quality of the LDA approach, by va-
rying frequency threshold h. Similar results are
observed for the pLSI approach. In the table, h=1
means no preprocessing is performed. The aver-
age query processing time is calculated over all
items in our dataset. As the threshold h increases,
the processing time decreases as expected, be-
cause the input of topic modeling gets smaller.
The second column lists the results quality
(measured by MnDCG@10). Interestingly, we
get the best results quality when h=4 (i.e., the
items with frequency less than 4 are discarded).
The reason may be that most low-frequency
items are noisy ones. As a result, preprocessing
can improve both results quality and processing
efficiency; and h=4 seems a good choice in pre-
processing for our dataset.
</bodyText>
<table confidence="0.971396181818182">
h Avg. Query Proc. Quality
Time (seconds) (MnDCG@10)
1 0.414 0.281
2 0.375 0.294
3 0.320 0.322
4 0.268 0.331
5 0.232 0.328
6 0.210 0.315
7 0.197 0.315
8 0.184 0.313
9 0.173 0.288
</table>
<tableCaption confidence="0.9822225">
Table 4. Time complexity and quality comparison
among LDA approaches of different thresholds
</tableCaption>
<subsubsectionHeader confidence="0.6215">
4.3.3 Postprocessing experiments
</subsubsectionHeader>
<figureCaption confidence="0.967393">
Figure 4. Results quality comparison among topic
modeling approaches with and without postprocessing
(metric: MnDCG@10)
</figureCaption>
<bodyText confidence="0.9964958">
The effect of postprocessing is shown in Figure
4. In the figure, NP means no postprocessing is
performed. Sim1 and Sim2 respectively mean
Formula 3.1 and Formula 3.2 are used in post-
processing as the similarity measure between
</bodyText>
<figure confidence="0.999085681818182">
0.45
0.35
0.25
0.15
0.05
0.4
0.3
0.2
0.1
0
1 2 3 4 5 6 7 8 9 10
pLSI LDA KMedoids-RASC
DBSCAN-RASC KMedoids-Item DBSCAN-Item
0.34 NP
0.33 Sim1
0.32 Sim2
0.31
0.3
0.29
0.28
0.27
LDA pLSI
</figure>
<page confidence="0.999497">
465
</page>
<bodyText confidence="0.999815833333333">
semantic classes. The same preprocessing (h=4)
is performed in generating the data. It can be
seen that postprocessing improves results quality.
Sim2 achieves more performance improvement
than Sim1, which demonstrates the effectiveness
of the similarity measure in Formula 3.2.
</bodyText>
<subsubsectionHeader confidence="0.448429">
4.3.4 Sample results
</subsubsectionHeader>
<bodyText confidence="0.991037">
Table 5 shows the semantic classes generated by
our LDA approach for some sample queries in
which the bad classes or bad members are hig-
hlighted (to save space, 10 items are listed here,
and the query itself is omitted in the resultant
semantic classes).
Query Semantic Classes
apple C1: ibm, microsoft, sony, dell, toshiba, sam-
sung, panasonic, canon, nec, sharp ...
C2: peach, strawberry, cherry, orange, bana-
na, lemon, pineapple, raspberry, pear, grape
...
gold C1: silver, copper, platinum, zinc, lead, iron,
nickel, tin, aluminum, manganese ...
C2: silver, red, black, white, blue, purple,
orange, pink, brown, navy ...
C3: silver, platinum, earrings, diamonds,
rings, bracelets, necklaces, pendants, jewelry,
watches ...
C4: silver, home, money, business, metal,
furniture, shoes, gypsum, hematite, fluorite
...
lincoln C1: ford, mazda, toyota, dodge, nissan, hon-
da, bmw, chrysler, mitsubishi, audi ...
C2: bristol, manchester, birmingham, leeds,
london, cardiff, nottingham, newcastle, shef-
field, southampton ...
C3: jefferson, jackson, washington, madison,
franklin, sacramento, new york city, monroe,
Louisville, marion ...
computer C1: chemistry, mathematics, physics, biolo-
science gy, psychology, education, history, music,
business, economics ...
</bodyText>
<tableCaption confidence="0.7999875">
Table 5. Semantic classes generated by our approach
for some sample queries (topic model = LDA)
</tableCaption>
<sectionHeader confidence="0.999964" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999877261904762">
Several categories of work are related to ours.
The first category is about set expansion (i.e.,
retrieving one semantic class given one term or a
couple of terms). Syntactic context information is
used (Hindle, 1990; Ruge, 1992; Lin, 1998) to
compute term similarities, based on which simi-
lar words to a particular word can directly be
returned. Google sets is an online service which,
given one to five items, predicts other items in
the set. Ghahramani and Heller (2005) introduce
a Bayesian Sets algorithm for set expansion. Set
expansion is performed by feeding queries to
web search engines in Wang and Cohen (2007)
and Kozareva (2008). All of the above work only
yields one semantic class for a given query.
Second, there are pattern-based approaches in the
literature which only do limited integration of
RASCs (Shinzato and Torisawa, 2004; Shinzato
and Torisawa, 2005; Pasca, 2004), as discussed
in the introduction section. In Shi et al. (2008),
an ad-hoc approach was proposed to discover the
multiple semantic classes for one item. The third
category is distributional similarity approaches
which provide multi-membership support (Har-
ris, 1985; Lin and Pantel, 2001; Pantel and Lin,
2002). Among them, the CBC algorithm (Pantel
and Lin, 2002) addresses the multi-membership
problem. But it relies on term vectors and centro-
ids which are not available in pattern-based ap-
proaches. It is therefore not clear whether it can
be borrowed to deal with multi-membership here.
Among the various applications of topic
modeling, maybe the efforts of using topic model
for Word Sense Disambiguation (WSD) are most
relevant to our work. In Cai et al (2007), LDA is
utilized to capture the global context information
as the topic features for better performing the
WSD task. In Boyd-Graber et al. (2007), Latent
Dirichlet with WordNet (LDAWN) is developed
for simultaneously disambiguating a corpus and
learning the domains in which to consider each
word. They do not generate semantic classes.
</bodyText>
<sectionHeader confidence="0.999778" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.99876">
We presented an approach that employs topic
modeling for semantic class construction. Given
an item q, we first retrieve all RASCs containing
the item to form a collection CR(q). Then we per-
form some preprocessing to CR(q) and build a
topic model for it. Finally, the output semantic
classes of topic modeling are post-processed to
generate the final semantic classes. For the CR(q)
which contains a lot of RASCs, we perform of-
fline processing according to the above process
and store the results on disk, in order to reduce
the online query processing time.
We also proposed an evaluation methodology
for measuring the quality of semantic classes.
We show by experiments that our topic modeling
approach outperforms the item clustering and
RASC clustering approaches.
</bodyText>
<sectionHeader confidence="0.998613" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.997287666666667">
We wish to acknowledge help from Xiaokang
Liu for mining RASCs from web pages, Chan-
gliang Wang and Zhongkai Fu for data process.
</bodyText>
<page confidence="0.999477">
466
</page>
<sectionHeader confidence="0.998345" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999563909090909">
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. J. Mach. Learn.
Res., 3:993–1022.
Bruce Croft, Donald Metzler, and Trevor Strohman.
2009. Search Engines: Information Retrieval in
Practice. Addison Wesley.
Jordan Boyd-Graber, David Blei, and Xiaojin
Zhu.2007. A topic model for word sense disambig-
uation. In Proceedings EMNLP-CoNLL 2007, pag-
es 1024–1033, Prague, Czech Republic, June.
Association for Computational Linguistics.
Jun Fu Cai, Wee Sun Lee, and Yee Whye Teh. 2007.
NUS-ML: Improving word sense disambiguation
using topic features. In Proceedings of the Interna-
tional Workshop on Semantic Evaluations, volume
4.
Scott Deerwester, Susan T. Dumais, GeorgeW. Fur-
nas, Thomas K. Landauer, and Richard Harshman.
1990. Indexing by latent semantic analysis. Journal
of the American Society for Information Science,
41:391–407.
Zoubin Ghahramani and Katherine A. Heller. 2005.
Bayesian Sets. In Advances in Neural Information
Processing Systems (NIPS05).
Thomas L. Griffiths, Mark Steyvers, David M.
Blei,and Joshua B. Tenenbaum. 2005. Integrating
topics and syntax. In Advances in Neural Informa-
tion Processing Systems 17, pages 537–544. MIT
Press
Zellig Harris. Distributional Structure. The Philoso-
phy of Linguistics. New York: Oxford University
Press. 1985.
Donald Hindle. 1990. Noun Classification from Pre-
dicate-Argument Structures. In Proceedings of
ACL90, pages 268–275.
Thomas Hofmann. 1999. Probabilistic latent semantic
indexing. In Proceedings of the 22nd annual inter-
national ACM SIGIR99, pages 50–57, New York,
NY, USA. ACM.
Kalervo Jarvelin, and Jaana Kekalainen. 2000. IR
Evaluation Methods for Retrieving Highly Rele-
vant Documents. In Proceedings of the 23rd An-
nual International ACM SIGIR Conference on
Research and Development in Information Retriev-
al (SIGIR2000).
Zornitsa Kozareva, Ellen Riloff and Eduard Hovy.
2008. Semantic Class Learning from the Web with
Hyponym Pattern Linkage Graphs, In Proceedings
of ACL-08.
Wei Li, David M. Blei, and Andrew McCallum. Non-
parametric Bayes Pachinko Allocation. In Proceed-
ings of Conference on Uncertainty in Artificial In-
telligence (UAI), 2007.
Dekang Lin. 1998. Automatic Retrieval and Cluster-
ing of Similar Words. In Proceedings of COLING-
ACL98, pages 768-774.
Dekang Lin and Patrick Pantel. 2001. Induction of
Semantic Classes from Natural Language Text. In
Proceedings of SIGKDD01, pages 317-322.
Hiroaki Ohshima, Satoshi Oyama, and Katsumi Tana-
ka. 2006. Searching coordinate terms with their
context from the web. In WISE06, pages 40–47.
Patrick Pantel and Dekang Lin. 2002. Discovering
Word Senses from Text. In Proceedings of
SIGKDD02.
Marius Pasca. 2004. Acquisition of Categorized
Named Entities for Web Search. In Proc. of 2004
CIKM.
Gerda Ruge. 1992. Experiments on Linguistically-
Based Term Associations. In Information
Processing &amp; Management, 28(3), pages 317-32.
Andrew I. Schein, Alexandrin Popescul, Lyle H.
Ungar and David M. Pennock. 2002. Methods and
metrics for cold-start recommendations. In Pro-
ceedings of SIGIR02, pages 253-260.
Shuming Shi, Xiaokang Liu and Ji-Rong Wen. 2008.
Pattern-based Semantic Class Discovery with Mul-
ti-Membership Support. In CIKM2008, pages
1453-1454.
Keiji Shinzato and Kentaro Torisawa. 2004. Acquir-
ing Hyponymy Relations from Web Documents. In
HLT/NAACL04, pages 73–80.
Keiji Shinzato and Kentaro Torisawa. 2005. A Simple
WWW-based Method for Semantic Word Class
Acquisition. In RANLP05.
Richard C. Wang and William W. Cohen. 2007. Lan-
gusage-Independent Set Expansion of Named Enti-
ties Using the Web. In ICDM2007.
</reference>
<page confidence="0.999284">
467
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.881086">
<title confidence="0.999667">Employing Topic Models for Pattern-based Semantic Class Discovery</title>
<author confidence="0.965478">Mingjie Shuming Ji-Rong</author>
<affiliation confidence="0.973881333333333">University of Science and Technology of China Research Asia</affiliation>
<email confidence="0.998681">v-huibzh@microsoft.com</email>
<email confidence="0.998681">v-mingjz@microsoft.com</email>
<email confidence="0.998681">shumings@microsoft.com</email>
<email confidence="0.998681">jrwen@microsoft.com</email>
<abstract confidence="0.99959025">A semantic class is a collection of items (words or phrases) which have semantically peer or sibling relationship. This paper studies the employment of topic models to automatically construct semantic classes, taking as the data a collection of semantic which were extracted by applying predefined patterns to web pages. The primary requirement (and challenge) here is dealing with multi-membership: An item may belong to multiple semantic classes; and we need to discover as many as possible the different semantic classes the item belongs to. To topic models, we treat RASCs as items as and the final seclasses as Appropriate preprocessing and postprocessing are performed to improve results quality, to reduce cost, and to tackle the constraint of a typical topic model. Experiments conducted on 40 million web pages show that our approach could yield better results than alternative approaches.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>3--993</pages>
<contexts>
<context position="7271" citStr="Blei et al., 2003" startWordPosition="1194" endWordPosition="1197">d of generating the corpus. Then the model can be utilized to inference a new document. pLSI: The probabilistic Latent Semantic Indexing Model (pLSI) was introduced in Hofmann (1999), arose from Latent Semantic Indexing (Deerwester et al., 1990). The following process illustrates how to generate a document d in pLSI: 1. Pick a topic mixture distribution 𝑝(∙ |𝑑). 2. For each word wi in d a. Pick a latent topic z with the probability 𝑝 (𝑧 |𝑑) for wi b. Generate wi with probability 𝑝 (𝑤𝑖 |𝑧) So with k latent topics, the likelihood of generating a document d is 𝑝(𝑑) = 𝑝 𝑤𝑖 𝑧 𝑝(𝑧|𝑑) (2.1) 𝑖 𝑧 LDA (Blei et al., 2003): In LDA, the topic mixture is drawn from a conjugate Dirichlet prior that remains the same for all documents (Figure 1). The generative process for each document in the corpus is, 1. Choose document length N from a Poisson distribution Poisson(𝜉). 2. Choose 𝜃 from a Dirichlet distribution with parameter α. 3. For each of the N words wi. a. Choose a topic z from a Multinomial distribution with parameter 𝜃. b. Pick a word wi from 𝑝 𝑤𝑖 𝑧, 𝛽 . So the likelihood of generating a document is 𝑝(𝑑) = 𝑝(𝜃|𝛼) 𝜃 Figure 1. Graphical model representation of LDA, from Blei et al. (2003) α θ z w β N M 𝑝(𝑧|𝜃)</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. J. Mach. Learn. Res., 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bruce Croft</author>
<author>Donald Metzler</author>
<author>Trevor Strohman</author>
</authors>
<title>Search Engines: Information Retrieval in Practice.</title>
<date>2009</date>
<publisher>Addison Wesley.</publisher>
<contexts>
<context position="24030" citStr="Croft et al., 2009" startWordPosition="4093" endWordPosition="4096">nt query item q, as has been done for the topic model approach. For fair comparison, the same postprocessing is made for all the approaches. And the same preprocessing is made for all the approaches except for the item clustering ones (to which the preprocessing is not applicable). 4.2 Evaluation Methodology Each produced semantic class is an ordered list of items. A couple of metrics in the information retrieval (IR) community like Precision@10, MAP (mean average precision), and nDCG (normalized discounted cumulative gain) are available for evaluating a single ranked list of items per query (Croft et al., 2009). Among the metrics, nDCG (Jarvelin and Kekalainen, 2000) can handle our three-level judgments (“Good”, “Fair”, and “Bad”, refer to Section 4.1), k i=1 G(i)/log(i + 1) k i=1 G*(i)/log(i + 1) where G(i) is the gain value assigned to the i’th item, and G*(i) is the gain value assigned to the i’th item of an ideal (or perfect) ranking list. Here we extend the IR metrics to the evaluation of multiple ordered lists per query. We use nDCG as the basic metric and extend it to MnDCG. Assume labelers have determined m SSCs (SSC1—SSCm, refer to Section 4.1) for query q and the weight (or importance) of </context>
</contexts>
<marker>Croft, Metzler, Strohman, 2009</marker>
<rawString>Bruce Croft, Donald Metzler, and Trevor Strohman. 2009. Search Engines: Information Retrieval in Practice. Addison Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordan Boyd-Graber</author>
<author>David Blei</author>
<author>Xiaojin Zhu 2007</author>
</authors>
<title>A topic model for word sense disambiguation.</title>
<date></date>
<booktitle>In Proceedings EMNLP-CoNLL 2007,</booktitle>
<pages>1024--1033</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<marker>Boyd-Graber, Blei, 2007, </marker>
<rawString>Jordan Boyd-Graber, David Blei, and Xiaojin Zhu.2007. A topic model for word sense disambiguation. In Proceedings EMNLP-CoNLL 2007, pages 1024–1033, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Fu Cai</author>
<author>Wee Sun Lee</author>
<author>Yee Whye Teh</author>
</authors>
<title>NUS-ML: Improving word sense disambiguation using topic features.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Workshop on Semantic Evaluations,</booktitle>
<volume>4</volume>
<contexts>
<context position="32058" citStr="Cai et al (2007)" startWordPosition="5407" endWordPosition="5410">em. The third category is distributional similarity approaches which provide multi-membership support (Harris, 1985; Lin and Pantel, 2001; Pantel and Lin, 2002). Among them, the CBC algorithm (Pantel and Lin, 2002) addresses the multi-membership problem. But it relies on term vectors and centroids which are not available in pattern-based approaches. It is therefore not clear whether it can be borrowed to deal with multi-membership here. Among the various applications of topic modeling, maybe the efforts of using topic model for Word Sense Disambiguation (WSD) are most relevant to our work. In Cai et al (2007), LDA is utilized to capture the global context information as the topic features for better performing the WSD task. In Boyd-Graber et al. (2007), Latent Dirichlet with WordNet (LDAWN) is developed for simultaneously disambiguating a corpus and learning the domains in which to consider each word. They do not generate semantic classes. 6 Conclusions We presented an approach that employs topic modeling for semantic class construction. Given an item q, we first retrieve all RASCs containing the item to form a collection CR(q). Then we perform some preprocessing to CR(q) and build a topic model f</context>
</contexts>
<marker>Cai, Lee, Teh, 2007</marker>
<rawString>Jun Fu Cai, Wee Sun Lee, and Yee Whye Teh. 2007. NUS-ML: Improving word sense disambiguation using topic features. In Proceedings of the International Workshop on Semantic Evaluations, volume 4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer Furnas</author>
<author>Richard Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society for Information Science,</journal>
<pages>41--391</pages>
<marker>Furnas, Harshman, 1990</marker>
<rawString>Scott Deerwester, Susan T. Dumais, GeorgeW. Furnas, Thomas K. Landauer, and Richard Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society for Information Science, 41:391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zoubin Ghahramani</author>
<author>Katherine A Heller</author>
</authors>
<title>Bayesian Sets.</title>
<date>2005</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS05).</booktitle>
<contexts>
<context position="30875" citStr="Ghahramani and Heller (2005)" startWordPosition="5219" endWordPosition="5222">on, history, music, business, economics ... Table 5. Semantic classes generated by our approach for some sample queries (topic model = LDA) 5 Related Work Several categories of work are related to ours. The first category is about set expansion (i.e., retrieving one semantic class given one term or a couple of terms). Syntactic context information is used (Hindle, 1990; Ruge, 1992; Lin, 1998) to compute term similarities, based on which similar words to a particular word can directly be returned. Google sets is an online service which, given one to five items, predicts other items in the set. Ghahramani and Heller (2005) introduce a Bayesian Sets algorithm for set expansion. Set expansion is performed by feeding queries to web search engines in Wang and Cohen (2007) and Kozareva (2008). All of the above work only yields one semantic class for a given query. Second, there are pattern-based approaches in the literature which only do limited integration of RASCs (Shinzato and Torisawa, 2004; Shinzato and Torisawa, 2005; Pasca, 2004), as discussed in the introduction section. In Shi et al. (2008), an ad-hoc approach was proposed to discover the multiple semantic classes for one item. The third category is distrib</context>
</contexts>
<marker>Ghahramani, Heller, 2005</marker>
<rawString>Zoubin Ghahramani and Katherine A. Heller. 2005. Bayesian Sets. In Advances in Neural Information Processing Systems (NIPS05).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
<author>David M Blei</author>
<author>Joshua B Tenenbaum</author>
</authors>
<title>Integrating topics and syntax.</title>
<date>2005</date>
<booktitle>In Advances in Neural Information Processing Systems 17,</booktitle>
<pages>537--544</pages>
<publisher>MIT Press</publisher>
<contexts>
<context position="16612" citStr="Griffiths et al. (2005)" startWordPosition="2819" endWordPosition="2822">r of semantic classes the item belongs to. As a result, one real semantic class may be divided into multiple topics. Therefore one core operation in this phase is to merge those topics into one semantic class. In addition, the items in each semantic class need to be properly ordered. Thus main operations include, 1) Merge semantic classes 2) Sort the items in each semantic class Now we illustrate how to perform the operations. Merge semantic classes: The merge process is performed by repeatedly calculating the simi2 There are topic model extensions considering word order in documents, such as Griffiths et al. (2005). 462 larity between two semantic classes and merging the two ones with the highest similarity until the similarity is under a threshold. One simple and straightforward similarity measure is the Jaccard coefficient, C1 U C21 where C1 n C2 and C1 U C2 are respectively the intersection and union of semantic classes C1 and C2. This formula might be over-simple, because the similarity between two different items is not exploited. So we propose the following measure, (3.2) C1 1• 1C21 where |C |is the number of items in semantic class C, and sim(a,b) is the similarity between items a and b, which wi</context>
</contexts>
<marker>Griffiths, Steyvers, Blei, Tenenbaum, 2005</marker>
<rawString>Thomas L. Griffiths, Mark Steyvers, David M. Blei,and Joshua B. Tenenbaum. 2005. Integrating topics and syntax. In Advances in Neural Information Processing Systems 17, pages 537–544. MIT Press</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig Harris</author>
</authors>
<title>Distributional Structure. The Philosophy of Linguistics.</title>
<date>1985</date>
<publisher>University Press.</publisher>
<location>New York: Oxford</location>
<contexts>
<context position="31557" citStr="Harris, 1985" startWordPosition="5325" endWordPosition="5327">is performed by feeding queries to web search engines in Wang and Cohen (2007) and Kozareva (2008). All of the above work only yields one semantic class for a given query. Second, there are pattern-based approaches in the literature which only do limited integration of RASCs (Shinzato and Torisawa, 2004; Shinzato and Torisawa, 2005; Pasca, 2004), as discussed in the introduction section. In Shi et al. (2008), an ad-hoc approach was proposed to discover the multiple semantic classes for one item. The third category is distributional similarity approaches which provide multi-membership support (Harris, 1985; Lin and Pantel, 2001; Pantel and Lin, 2002). Among them, the CBC algorithm (Pantel and Lin, 2002) addresses the multi-membership problem. But it relies on term vectors and centroids which are not available in pattern-based approaches. It is therefore not clear whether it can be borrowed to deal with multi-membership here. Among the various applications of topic modeling, maybe the efforts of using topic model for Word Sense Disambiguation (WSD) are most relevant to our work. In Cai et al (2007), LDA is utilized to capture the global context information as the topic features for better perfor</context>
</contexts>
<marker>Harris, 1985</marker>
<rawString>Zellig Harris. Distributional Structure. The Philosophy of Linguistics. New York: Oxford University Press. 1985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Hindle</author>
</authors>
<title>Noun Classification from Predicate-Argument Structures.</title>
<date>1990</date>
<booktitle>In Proceedings of ACL90,</booktitle>
<pages>268--275</pages>
<contexts>
<context position="30618" citStr="Hindle, 1990" startWordPosition="5177" endWordPosition="5178">ingham, newcastle, sheffield, southampton ... C3: jefferson, jackson, washington, madison, franklin, sacramento, new york city, monroe, Louisville, marion ... computer C1: chemistry, mathematics, physics, bioloscience gy, psychology, education, history, music, business, economics ... Table 5. Semantic classes generated by our approach for some sample queries (topic model = LDA) 5 Related Work Several categories of work are related to ours. The first category is about set expansion (i.e., retrieving one semantic class given one term or a couple of terms). Syntactic context information is used (Hindle, 1990; Ruge, 1992; Lin, 1998) to compute term similarities, based on which similar words to a particular word can directly be returned. Google sets is an online service which, given one to five items, predicts other items in the set. Ghahramani and Heller (2005) introduce a Bayesian Sets algorithm for set expansion. Set expansion is performed by feeding queries to web search engines in Wang and Cohen (2007) and Kozareva (2008). All of the above work only yields one semantic class for a given query. Second, there are pattern-based approaches in the literature which only do limited integration of RAS</context>
</contexts>
<marker>Hindle, 1990</marker>
<rawString>Donald Hindle. 1990. Noun Classification from Predicate-Argument Structures. In Proceedings of ACL90, pages 268–275.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>Probabilistic latent semantic indexing.</title>
<date>1999</date>
<booktitle>In Proceedings of the 22nd annual international ACM SIGIR99,</booktitle>
<pages>50--57</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="6835" citStr="Hofmann (1999)" startWordPosition="1106" endWordPosition="1108">pand the application scope of topic modeling. 2 Topic Models In this section we briefly introduce the two widely used topic models which are adopted in our paper. Both of them model a document as a mixture of hidden topics. The words of every document are assumed to be generated via a generative probability process. The parameters of the model are estimated from a training process over a given corpus, by maximizing the likelihood of generating the corpus. Then the model can be utilized to inference a new document. pLSI: The probabilistic Latent Semantic Indexing Model (pLSI) was introduced in Hofmann (1999), arose from Latent Semantic Indexing (Deerwester et al., 1990). The following process illustrates how to generate a document d in pLSI: 1. Pick a topic mixture distribution 𝑝(∙ |𝑑). 2. For each word wi in d a. Pick a latent topic z with the probability 𝑝 (𝑧 |𝑑) for wi b. Generate wi with probability 𝑝 (𝑤𝑖 |𝑧) So with k latent topics, the likelihood of generating a document d is 𝑝(𝑑) = 𝑝 𝑤𝑖 𝑧 𝑝(𝑧|𝑑) (2.1) 𝑖 𝑧 LDA (Blei et al., 2003): In LDA, the topic mixture is drawn from a conjugate Dirichlet prior that remains the same for all documents (Figure 1). The generative process for each document i</context>
</contexts>
<marker>Hofmann, 1999</marker>
<rawString>Thomas Hofmann. 1999. Probabilistic latent semantic indexing. In Proceedings of the 22nd annual international ACM SIGIR99, pages 50–57, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kalervo Jarvelin</author>
<author>Jaana Kekalainen</author>
</authors>
<title>IR Evaluation Methods for Retrieving Highly Relevant Documents.</title>
<date>2000</date>
<booktitle>In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR2000).</booktitle>
<contexts>
<context position="24087" citStr="Jarvelin and Kekalainen, 2000" startWordPosition="4101" endWordPosition="4104">c model approach. For fair comparison, the same postprocessing is made for all the approaches. And the same preprocessing is made for all the approaches except for the item clustering ones (to which the preprocessing is not applicable). 4.2 Evaluation Methodology Each produced semantic class is an ordered list of items. A couple of metrics in the information retrieval (IR) community like Precision@10, MAP (mean average precision), and nDCG (normalized discounted cumulative gain) are available for evaluating a single ranked list of items per query (Croft et al., 2009). Among the metrics, nDCG (Jarvelin and Kekalainen, 2000) can handle our three-level judgments (“Good”, “Fair”, and “Bad”, refer to Section 4.1), k i=1 G(i)/log(i + 1) k i=1 G*(i)/log(i + 1) where G(i) is the gain value assigned to the i’th item, and G*(i) is the gain value assigned to the i’th item of an ideal (or perfect) ranking list. Here we extend the IR metrics to the evaluation of multiple ordered lists per query. We use nDCG as the basic metric and extend it to MnDCG. Assume labelers have determined m SSCs (SSC1—SSCm, refer to Section 4.1) for query q and the weight (or importance) of SSCi is wi. Assume n semantic classes are generated by an</context>
</contexts>
<marker>Jarvelin, Kekalainen, 2000</marker>
<rawString>Kalervo Jarvelin, and Jaana Kekalainen. 2000. IR Evaluation Methods for Retrieving Highly Relevant Documents. In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR2000).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zornitsa Kozareva</author>
<author>Ellen Riloff</author>
<author>Eduard Hovy</author>
</authors>
<title>Semantic Class Learning from the Web with Hyponym Pattern Linkage Graphs,</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08.</booktitle>
<marker>Kozareva, Riloff, Hovy, 2008</marker>
<rawString>Zornitsa Kozareva, Ellen Riloff and Eduard Hovy. 2008. Semantic Class Learning from the Web with Hyponym Pattern Linkage Graphs, In Proceedings of ACL-08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Li</author>
<author>David M Blei</author>
<author>Andrew McCallum</author>
</authors>
<title>Nonparametric Bayes Pachinko Allocation.</title>
<date>2007</date>
<booktitle>In Proceedings of Conference on Uncertainty in Artificial Intelligence (UAI),</booktitle>
<contexts>
<context position="11662" citStr="Li et al., 2007" startWordPosition="1971" endWordPosition="1974">ontains three phases (Figure 2). We build topic models for every CR(q), rather than the whole collection CR. A preprocessing phase and a postprocessing phase are added before and after the topic modeling phase to improve efficiency and to overcome the fixed-k problem. The details of each phase are presented in the following subsections. Figure 2. Main phases of our approach 3.2 Adopting Topic Models For an item q, topic modeling is adopted to process the RASCs in CR(q) to generate k semantic classes. Here we use LDA as an example to 1 Although there is study of non-parametric Bayesian models (Li et al., 2007) which need no prior knowledge of topic number, the computational complexity seems to exceed our efficiency requirement and we shall leave this to future work. Item q CR Preprocessing CR(q) R580 R1 R2 R400 � R2� R1� Topic modeling T1 T2 T3 T4 T5 Postprocessing C1 C2 C3 461 illustrate the process. The case of other generative topic models (e.g., pLSI) is very similar. According to the assumption of LDA and our concept mapping in Table 3, a RASC (“document”) is viewed as a mixture of hidden semantic classes (“topics”). The generative process for a RASC R in the “corpus” CR(q) is as follows, 1) C</context>
</contexts>
<marker>Li, Blei, McCallum, 2007</marker>
<rawString>Wei Li, David M. Blei, and Andrew McCallum. Nonparametric Bayes Pachinko Allocation. In Proceedings of Conference on Uncertainty in Artificial Intelligence (UAI), 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic Retrieval and Clustering of Similar Words.</title>
<date>1998</date>
<booktitle>In Proceedings of COLINGACL98,</booktitle>
<pages>768--774</pages>
<contexts>
<context position="30642" citStr="Lin, 1998" startWordPosition="5181" endWordPosition="5182">ld, southampton ... C3: jefferson, jackson, washington, madison, franklin, sacramento, new york city, monroe, Louisville, marion ... computer C1: chemistry, mathematics, physics, bioloscience gy, psychology, education, history, music, business, economics ... Table 5. Semantic classes generated by our approach for some sample queries (topic model = LDA) 5 Related Work Several categories of work are related to ours. The first category is about set expansion (i.e., retrieving one semantic class given one term or a couple of terms). Syntactic context information is used (Hindle, 1990; Ruge, 1992; Lin, 1998) to compute term similarities, based on which similar words to a particular word can directly be returned. Google sets is an online service which, given one to five items, predicts other items in the set. Ghahramani and Heller (2005) introduce a Bayesian Sets algorithm for set expansion. Set expansion is performed by feeding queries to web search engines in Wang and Cohen (2007) and Kozareva (2008). All of the above work only yields one semantic class for a given query. Second, there are pattern-based approaches in the literature which only do limited integration of RASCs (Shinzato and Torisaw</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic Retrieval and Clustering of Similar Words. In Proceedings of COLINGACL98, pages 768-774.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>Induction of Semantic Classes from Natural Language Text.</title>
<date>2001</date>
<booktitle>In Proceedings of SIGKDD01,</booktitle>
<pages>317--322</pages>
<contexts>
<context position="1329" citStr="Lin and Pantel, 2001" startWordPosition="196" endWordPosition="199">may belong to multiple semantic classes; and we need to discover as many as possible the different semantic classes the item belongs to. To adopt topic models, we treat RASCs as “documents”, items as “words”, and the final semantic classes as “topics”. Appropriate preprocessing and postprocessing are performed to improve results quality, to reduce computation cost, and to tackle the fixed-k constraint of a typical topic model. Experiments conducted on 40 million web pages show that our approach could yield better results than alternative approaches. 1 Introduction Semantic class construction (Lin and Pantel, 2001; Pantel and Lin, 2002; Pasca, 2004; Shinzato and Torisawa, 2005; Ohshima et al., 2006) tries to discover the peer or sibling relationship among terms or phrases by organizing them into semantic classes. For example, {red, white, black...} is a semantic class consisting of color instances. A popular way for semantic class discovery is pattern-based approach, where predefined patterns (Table 1) are applied to a  This work was performed when the authors were interns at Microsoft Research Asia collection of web pages or an online web search engine to produce some raw semantic classes (abbreviate</context>
<context position="31579" citStr="Lin and Pantel, 2001" startWordPosition="5328" endWordPosition="5331">y feeding queries to web search engines in Wang and Cohen (2007) and Kozareva (2008). All of the above work only yields one semantic class for a given query. Second, there are pattern-based approaches in the literature which only do limited integration of RASCs (Shinzato and Torisawa, 2004; Shinzato and Torisawa, 2005; Pasca, 2004), as discussed in the introduction section. In Shi et al. (2008), an ad-hoc approach was proposed to discover the multiple semantic classes for one item. The third category is distributional similarity approaches which provide multi-membership support (Harris, 1985; Lin and Pantel, 2001; Pantel and Lin, 2002). Among them, the CBC algorithm (Pantel and Lin, 2002) addresses the multi-membership problem. But it relies on term vectors and centroids which are not available in pattern-based approaches. It is therefore not clear whether it can be borrowed to deal with multi-membership here. Among the various applications of topic modeling, maybe the efforts of using topic model for Word Sense Disambiguation (WSD) are most relevant to our work. In Cai et al (2007), LDA is utilized to capture the global context information as the topic features for better performing the WSD task. In </context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. Induction of Semantic Classes from Natural Language Text. In Proceedings of SIGKDD01, pages 317-322.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroaki Ohshima</author>
<author>Satoshi Oyama</author>
<author>Katsumi Tanaka</author>
</authors>
<title>Searching coordinate terms with their context from the web. In</title>
<date>2006</date>
<booktitle>WISE06,</booktitle>
<pages>40--47</pages>
<contexts>
<context position="1416" citStr="Ohshima et al., 2006" startWordPosition="211" endWordPosition="214">he different semantic classes the item belongs to. To adopt topic models, we treat RASCs as “documents”, items as “words”, and the final semantic classes as “topics”. Appropriate preprocessing and postprocessing are performed to improve results quality, to reduce computation cost, and to tackle the fixed-k constraint of a typical topic model. Experiments conducted on 40 million web pages show that our approach could yield better results than alternative approaches. 1 Introduction Semantic class construction (Lin and Pantel, 2001; Pantel and Lin, 2002; Pasca, 2004; Shinzato and Torisawa, 2005; Ohshima et al., 2006) tries to discover the peer or sibling relationship among terms or phrases by organizing them into semantic classes. For example, {red, white, black...} is a semantic class consisting of color instances. A popular way for semantic class discovery is pattern-based approach, where predefined patterns (Table 1) are applied to a  This work was performed when the authors were interns at Microsoft Research Asia collection of web pages or an online web search engine to produce some raw semantic classes (abbreviated as RASCs, Table 2). RASCs cannot be treated as the ultimate semantic classes, because</context>
</contexts>
<marker>Ohshima, Oyama, Tanaka, 2006</marker>
<rawString>Hiroaki Ohshima, Satoshi Oyama, and Katsumi Tanaka. 2006. Searching coordinate terms with their context from the web. In WISE06, pages 40–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Dekang Lin</author>
</authors>
<title>Discovering Word Senses from Text.</title>
<date>2002</date>
<booktitle>In Proceedings of SIGKDD02.</booktitle>
<contexts>
<context position="1351" citStr="Pantel and Lin, 2002" startWordPosition="200" endWordPosition="203"> semantic classes; and we need to discover as many as possible the different semantic classes the item belongs to. To adopt topic models, we treat RASCs as “documents”, items as “words”, and the final semantic classes as “topics”. Appropriate preprocessing and postprocessing are performed to improve results quality, to reduce computation cost, and to tackle the fixed-k constraint of a typical topic model. Experiments conducted on 40 million web pages show that our approach could yield better results than alternative approaches. 1 Introduction Semantic class construction (Lin and Pantel, 2001; Pantel and Lin, 2002; Pasca, 2004; Shinzato and Torisawa, 2005; Ohshima et al., 2006) tries to discover the peer or sibling relationship among terms or phrases by organizing them into semantic classes. For example, {red, white, black...} is a semantic class consisting of color instances. A popular way for semantic class discovery is pattern-based approach, where predefined patterns (Table 1) are applied to a  This work was performed when the authors were interns at Microsoft Research Asia collection of web pages or an online web search engine to produce some raw semantic classes (abbreviated as RASCs, Table 2). </context>
<context position="31602" citStr="Pantel and Lin, 2002" startWordPosition="5332" endWordPosition="5335">eb search engines in Wang and Cohen (2007) and Kozareva (2008). All of the above work only yields one semantic class for a given query. Second, there are pattern-based approaches in the literature which only do limited integration of RASCs (Shinzato and Torisawa, 2004; Shinzato and Torisawa, 2005; Pasca, 2004), as discussed in the introduction section. In Shi et al. (2008), an ad-hoc approach was proposed to discover the multiple semantic classes for one item. The third category is distributional similarity approaches which provide multi-membership support (Harris, 1985; Lin and Pantel, 2001; Pantel and Lin, 2002). Among them, the CBC algorithm (Pantel and Lin, 2002) addresses the multi-membership problem. But it relies on term vectors and centroids which are not available in pattern-based approaches. It is therefore not clear whether it can be borrowed to deal with multi-membership here. Among the various applications of topic modeling, maybe the efforts of using topic model for Word Sense Disambiguation (WSD) are most relevant to our work. In Cai et al (2007), LDA is utilized to capture the global context information as the topic features for better performing the WSD task. In Boyd-Graber et al. (200</context>
</contexts>
<marker>Pantel, Lin, 2002</marker>
<rawString>Patrick Pantel and Dekang Lin. 2002. Discovering Word Senses from Text. In Proceedings of SIGKDD02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marius Pasca</author>
</authors>
<title>Acquisition of Categorized Named Entities for Web Search.</title>
<date>2004</date>
<booktitle>In Proc. of</booktitle>
<publisher>CIKM.</publisher>
<contexts>
<context position="1364" citStr="Pasca, 2004" startWordPosition="204" endWordPosition="205"> we need to discover as many as possible the different semantic classes the item belongs to. To adopt topic models, we treat RASCs as “documents”, items as “words”, and the final semantic classes as “topics”. Appropriate preprocessing and postprocessing are performed to improve results quality, to reduce computation cost, and to tackle the fixed-k constraint of a typical topic model. Experiments conducted on 40 million web pages show that our approach could yield better results than alternative approaches. 1 Introduction Semantic class construction (Lin and Pantel, 2001; Pantel and Lin, 2002; Pasca, 2004; Shinzato and Torisawa, 2005; Ohshima et al., 2006) tries to discover the peer or sibling relationship among terms or phrases by organizing them into semantic classes. For example, {red, white, black...} is a semantic class consisting of color instances. A popular way for semantic class discovery is pattern-based approach, where predefined patterns (Table 1) are applied to a  This work was performed when the authors were interns at Microsoft Research Asia collection of web pages or an online web search engine to produce some raw semantic classes (abbreviated as RASCs, Table 2). RASCs cannot </context>
<context position="3607" citStr="Pasca, 2004" startWordPosition="563" endWordPosition="564">or example, the term “Lincoln” can simultaneously represent a person, a place, or a car brand name. Multi-membership is more popular than at a first glance, because quite a lot of English common words have also been borrowed as company names, places, or product names. For a given item (as a query) which belongs to multiple semantic classes, we intend to return the semantic classes separately, rather than mixing all their items together. Existing pattern-based approaches only provide very limited support to multi-membership. For example, RASCs with the same labels (or hypernyms) are merged in (Pasca, 2004) to gen459 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 459–467, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP erate the ultimate semantic classes. This is problematic, because RASCs may not have (accurate) hypernyms with them. In this paper, we propose to use topic models to address the problem. In some topic models, a document is modeled as a mixture of hidden topics. The words of a document are generated according to the word distribution over the topics corresponding to the document (see Section 2 for details). Given a corpus, the late</context>
<context position="31292" citStr="Pasca, 2004" startWordPosition="5287" endWordPosition="5288">n which similar words to a particular word can directly be returned. Google sets is an online service which, given one to five items, predicts other items in the set. Ghahramani and Heller (2005) introduce a Bayesian Sets algorithm for set expansion. Set expansion is performed by feeding queries to web search engines in Wang and Cohen (2007) and Kozareva (2008). All of the above work only yields one semantic class for a given query. Second, there are pattern-based approaches in the literature which only do limited integration of RASCs (Shinzato and Torisawa, 2004; Shinzato and Torisawa, 2005; Pasca, 2004), as discussed in the introduction section. In Shi et al. (2008), an ad-hoc approach was proposed to discover the multiple semantic classes for one item. The third category is distributional similarity approaches which provide multi-membership support (Harris, 1985; Lin and Pantel, 2001; Pantel and Lin, 2002). Among them, the CBC algorithm (Pantel and Lin, 2002) addresses the multi-membership problem. But it relies on term vectors and centroids which are not available in pattern-based approaches. It is therefore not clear whether it can be borrowed to deal with multi-membership here. Among the</context>
</contexts>
<marker>Pasca, 2004</marker>
<rawString>Marius Pasca. 2004. Acquisition of Categorized Named Entities for Web Search. In Proc. of 2004 CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerda Ruge</author>
</authors>
<title>Experiments on LinguisticallyBased Term Associations.</title>
<date>1992</date>
<booktitle>In Information Processing &amp; Management,</booktitle>
<volume>28</volume>
<issue>3</issue>
<pages>317--32</pages>
<contexts>
<context position="30630" citStr="Ruge, 1992" startWordPosition="5179" endWordPosition="5180">tle, sheffield, southampton ... C3: jefferson, jackson, washington, madison, franklin, sacramento, new york city, monroe, Louisville, marion ... computer C1: chemistry, mathematics, physics, bioloscience gy, psychology, education, history, music, business, economics ... Table 5. Semantic classes generated by our approach for some sample queries (topic model = LDA) 5 Related Work Several categories of work are related to ours. The first category is about set expansion (i.e., retrieving one semantic class given one term or a couple of terms). Syntactic context information is used (Hindle, 1990; Ruge, 1992; Lin, 1998) to compute term similarities, based on which similar words to a particular word can directly be returned. Google sets is an online service which, given one to five items, predicts other items in the set. Ghahramani and Heller (2005) introduce a Bayesian Sets algorithm for set expansion. Set expansion is performed by feeding queries to web search engines in Wang and Cohen (2007) and Kozareva (2008). All of the above work only yields one semantic class for a given query. Second, there are pattern-based approaches in the literature which only do limited integration of RASCs (Shinzato</context>
</contexts>
<marker>Ruge, 1992</marker>
<rawString>Gerda Ruge. 1992. Experiments on LinguisticallyBased Term Associations. In Information Processing &amp; Management, 28(3), pages 317-32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew I Schein</author>
<author>Alexandrin Popescul</author>
<author>Lyle H Ungar</author>
<author>David M Pennock</author>
</authors>
<title>Methods and metrics for cold-start recommendations.</title>
<date>2002</date>
<booktitle>In Proceedings of SIGIR02,</booktitle>
<pages>253--260</pages>
<contexts>
<context position="22868" citStr="Schein, et al. (2002)" startWordPosition="3922" endWordPosition="3925">s, and U.S. states. In the second step, each item is assigned a label of “Good”, “Fair”, or “Bad” with respect to each SSC. For example, “silver” is labeled “Good” with respect to “colors” and “chemical elements”. We adopt metric MnDCG (Section 4.2) as our evaluation metric. Approaches for comparison: We compare our approach with the alternative approaches discussed in Section 3.4.2. LDA: Our approach with LDA as the topic model. The implementation of LDA is based on Blei’s code of variational EM for LDA5. pLSI: Our approach with pLSI as the topic model. The implementation of pLSI is based on Schein, et al. (2002). KMedoids-RASC: The RASC clustering approach illustrated in Section 3.4.2, with the K-Medoids clustering algorithm utilized. DBSCAN-RASC: The RASC clustering approach with DBSCAN utilized. KMedoids-Item: The item clustering approach with the K-Medoids utilized. DBSCAN-Item: The item clustering approach with the DBSCAN clustering algorithm utilized. 3 http://www.dmoz.org 4 http://labs.google.com/sets 5 http://www.cs.princeton.edu/—blei/lda-c/ K-Medoids clustering needs to predefine the cluster number k. We fix the k value for all different query item q, as has been done for the topic model app</context>
</contexts>
<marker>Schein, Popescul, Ungar, Pennock, 2002</marker>
<rawString>Andrew I. Schein, Alexandrin Popescul, Lyle H. Ungar and David M. Pennock. 2002. Methods and metrics for cold-start recommendations. In Proceedings of SIGIR02, pages 253-260.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shuming Shi</author>
<author>Xiaokang Liu</author>
<author>Ji-Rong Wen</author>
</authors>
<title>Pattern-based Semantic Class Discovery with Multi-Membership Support.</title>
<date>2008</date>
<booktitle>In CIKM2008,</booktitle>
<pages>1453--1454</pages>
<contexts>
<context position="31356" citStr="Shi et al. (2008)" startWordPosition="5296" endWordPosition="5299">returned. Google sets is an online service which, given one to five items, predicts other items in the set. Ghahramani and Heller (2005) introduce a Bayesian Sets algorithm for set expansion. Set expansion is performed by feeding queries to web search engines in Wang and Cohen (2007) and Kozareva (2008). All of the above work only yields one semantic class for a given query. Second, there are pattern-based approaches in the literature which only do limited integration of RASCs (Shinzato and Torisawa, 2004; Shinzato and Torisawa, 2005; Pasca, 2004), as discussed in the introduction section. In Shi et al. (2008), an ad-hoc approach was proposed to discover the multiple semantic classes for one item. The third category is distributional similarity approaches which provide multi-membership support (Harris, 1985; Lin and Pantel, 2001; Pantel and Lin, 2002). Among them, the CBC algorithm (Pantel and Lin, 2002) addresses the multi-membership problem. But it relies on term vectors and centroids which are not available in pattern-based approaches. It is therefore not clear whether it can be borrowed to deal with multi-membership here. Among the various applications of topic modeling, maybe the efforts of us</context>
</contexts>
<marker>Shi, Liu, Wen, 2008</marker>
<rawString>Shuming Shi, Xiaokang Liu and Ji-Rong Wen. 2008. Pattern-based Semantic Class Discovery with Multi-Membership Support. In CIKM2008, pages 1453-1454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keiji Shinzato</author>
<author>Kentaro Torisawa</author>
</authors>
<title>Acquiring Hyponymy Relations from Web Documents. In</title>
<date>2004</date>
<booktitle>HLT/NAACL04,</booktitle>
<pages>73--80</pages>
<contexts>
<context position="31249" citStr="Shinzato and Torisawa, 2004" startWordPosition="5279" endWordPosition="5282">ge, 1992; Lin, 1998) to compute term similarities, based on which similar words to a particular word can directly be returned. Google sets is an online service which, given one to five items, predicts other items in the set. Ghahramani and Heller (2005) introduce a Bayesian Sets algorithm for set expansion. Set expansion is performed by feeding queries to web search engines in Wang and Cohen (2007) and Kozareva (2008). All of the above work only yields one semantic class for a given query. Second, there are pattern-based approaches in the literature which only do limited integration of RASCs (Shinzato and Torisawa, 2004; Shinzato and Torisawa, 2005; Pasca, 2004), as discussed in the introduction section. In Shi et al. (2008), an ad-hoc approach was proposed to discover the multiple semantic classes for one item. The third category is distributional similarity approaches which provide multi-membership support (Harris, 1985; Lin and Pantel, 2001; Pantel and Lin, 2002). Among them, the CBC algorithm (Pantel and Lin, 2002) addresses the multi-membership problem. But it relies on term vectors and centroids which are not available in pattern-based approaches. It is therefore not clear whether it can be borrowed to</context>
</contexts>
<marker>Shinzato, Torisawa, 2004</marker>
<rawString>Keiji Shinzato and Kentaro Torisawa. 2004. Acquiring Hyponymy Relations from Web Documents. In HLT/NAACL04, pages 73–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keiji Shinzato</author>
<author>Kentaro Torisawa</author>
</authors>
<title>A Simple WWW-based Method for Semantic Word Class Acquisition.</title>
<date>2005</date>
<booktitle>In RANLP05.</booktitle>
<contexts>
<context position="1393" citStr="Shinzato and Torisawa, 2005" startWordPosition="206" endWordPosition="210">iscover as many as possible the different semantic classes the item belongs to. To adopt topic models, we treat RASCs as “documents”, items as “words”, and the final semantic classes as “topics”. Appropriate preprocessing and postprocessing are performed to improve results quality, to reduce computation cost, and to tackle the fixed-k constraint of a typical topic model. Experiments conducted on 40 million web pages show that our approach could yield better results than alternative approaches. 1 Introduction Semantic class construction (Lin and Pantel, 2001; Pantel and Lin, 2002; Pasca, 2004; Shinzato and Torisawa, 2005; Ohshima et al., 2006) tries to discover the peer or sibling relationship among terms or phrases by organizing them into semantic classes. For example, {red, white, black...} is a semantic class consisting of color instances. A popular way for semantic class discovery is pattern-based approach, where predefined patterns (Table 1) are applied to a  This work was performed when the authors were interns at Microsoft Research Asia collection of web pages or an online web search engine to produce some raw semantic classes (abbreviated as RASCs, Table 2). RASCs cannot be treated as the ultimate se</context>
<context position="31278" citStr="Shinzato and Torisawa, 2005" startWordPosition="5283" endWordPosition="5286">te term similarities, based on which similar words to a particular word can directly be returned. Google sets is an online service which, given one to five items, predicts other items in the set. Ghahramani and Heller (2005) introduce a Bayesian Sets algorithm for set expansion. Set expansion is performed by feeding queries to web search engines in Wang and Cohen (2007) and Kozareva (2008). All of the above work only yields one semantic class for a given query. Second, there are pattern-based approaches in the literature which only do limited integration of RASCs (Shinzato and Torisawa, 2004; Shinzato and Torisawa, 2005; Pasca, 2004), as discussed in the introduction section. In Shi et al. (2008), an ad-hoc approach was proposed to discover the multiple semantic classes for one item. The third category is distributional similarity approaches which provide multi-membership support (Harris, 1985; Lin and Pantel, 2001; Pantel and Lin, 2002). Among them, the CBC algorithm (Pantel and Lin, 2002) addresses the multi-membership problem. But it relies on term vectors and centroids which are not available in pattern-based approaches. It is therefore not clear whether it can be borrowed to deal with multi-membership h</context>
</contexts>
<marker>Shinzato, Torisawa, 2005</marker>
<rawString>Keiji Shinzato and Kentaro Torisawa. 2005. A Simple WWW-based Method for Semantic Word Class Acquisition. In RANLP05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard C Wang</author>
<author>William W Cohen</author>
</authors>
<title>Langusage-Independent Set Expansion of Named Entities Using the Web. In</title>
<date>2007</date>
<booktitle>ICDM2007.</booktitle>
<contexts>
<context position="31023" citStr="Wang and Cohen (2007)" startWordPosition="5243" endWordPosition="5246">Several categories of work are related to ours. The first category is about set expansion (i.e., retrieving one semantic class given one term or a couple of terms). Syntactic context information is used (Hindle, 1990; Ruge, 1992; Lin, 1998) to compute term similarities, based on which similar words to a particular word can directly be returned. Google sets is an online service which, given one to five items, predicts other items in the set. Ghahramani and Heller (2005) introduce a Bayesian Sets algorithm for set expansion. Set expansion is performed by feeding queries to web search engines in Wang and Cohen (2007) and Kozareva (2008). All of the above work only yields one semantic class for a given query. Second, there are pattern-based approaches in the literature which only do limited integration of RASCs (Shinzato and Torisawa, 2004; Shinzato and Torisawa, 2005; Pasca, 2004), as discussed in the introduction section. In Shi et al. (2008), an ad-hoc approach was proposed to discover the multiple semantic classes for one item. The third category is distributional similarity approaches which provide multi-membership support (Harris, 1985; Lin and Pantel, 2001; Pantel and Lin, 2002). Among them, the CBC</context>
</contexts>
<marker>Wang, Cohen, 2007</marker>
<rawString>Richard C. Wang and William W. Cohen. 2007. Langusage-Independent Set Expansion of Named Entities Using the Web. In ICDM2007.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>