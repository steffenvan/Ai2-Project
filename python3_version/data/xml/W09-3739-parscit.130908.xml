<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.028826">
<title confidence="0.9952725">
An Application of Lexical Semantics Annotation to
Question-Answering in e-Farming
</title>
<author confidence="0.746556">
Mukda Suktarachan (1), Patrick Saint-Dizier (2)
</author>
<affiliation confidence="0.621358">
(1) NAIST, Kasetsart University, Bangkok, Thailand
</affiliation>
<address confidence="0.865997">
(2) IRIT, Toulouse, France
</address>
<email confidence="0.958986">
naist da da@yahoo.com, stdizier@irit.fr
</email>
<sectionHeader confidence="0.962559" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999414222222222">
In this poster we present an approach to responding to complex
questions in the agriculture domain, from specifications given by ex-
perts. We present in particular a semantic annotation procedure that
would allow us to define accurate and domain dedicated forms of lexical
semantics inference, in order to be able to match non factoid questions
(i.e. questions whose response is a significant text portion) with doc-
uments on a large scale. This project is designed to help farmers to
get advices via question answering on SMS in order to improve rice
farming.
</bodyText>
<sectionHeader confidence="0.763755" genericHeader="categories and subject descriptors">
1 Challenges and Goals
</sectionHeader>
<bodyText confidence="0.999423357142857">
Question answering (Moldovan 2000, Maybury 2004) operates on top of
search engines of classical textual database querying tools, by providing a
layer that has natural language understanding and generation as well some
reasoning capabilities in order to provide users with responses which are
much more accurate and cooperative than what search engines provide in
general. This is particularly crucial when responses are not straightforward,
e.g. when they require some form of elaboration (synthesis of data, consis-
tency checking, etc.), reasoning or when the response is not a simple item,
but a well-formed fragment of text, e.g. a chain of events leading to a con-
sequence, a procedure, etc.
The project we present here emerged from a need from the Thai Ministry
of Agriculture. The main goal is to develop tools for e-Farming, in particular
rice farming, so that farmers can easily get information on farming rice and
rice diseases, for example via SMS. The Thai Ministry of Agriculture has
</bodyText>
<page confidence="0.981607">
338
</page>
<bodyText confidence="0.9788698">
Proceedings of the 8th International Conference on Computational Semantics, pages 338–341,
Tilburg, January 2009. c�2009 International Conference on Computational Semantics
large text databases on the way rice can be planted, on how to prepare and
fertilize soils and on the numerous diseases rice may be subject to, the ef-
fects, the treatments, etc. Question answering is a particularly well-adapted
approach to allow farmers to query in Thai (via SMS short messages) such
databases.
The NAIST lab at the university of Kasetsart has basic tools to parse
Thai (stemmer, morphological analysis, part of speech recognition, and sim-
ple syntactic analysis). These tools were designed for machine translation
purposes, but they turn out to be appropriate to parse queries and to retreive
information in technical texts.
2 Outline of the Project and Methodology
The needs of the Thai Ministry of Agriculture have been specified in a sim-
ple way via a corpus composed of (1) questions raised in real life by farmers
(about 1000 questions), (2) the responses which have been provided by ex-
perts based on existing documents (possibly several responses per question)
and (3) the texts they originate from. In general the response is found in
a unique text: there are no multiple answers since most texts are not re-
dundant, although some responses, in particular complex or indirect ones,
may involve the taking into account of several independent texts. We will
not address here the problem of message length reduction so that it fits into
SMS format (althought this is also an important semantic problem).
A few examples of question-answer pairs are (glosses from Thai, but
structures are in fact quite similar to English):
</bodyText>
<listItem confidence="0.93416675">
Q: How to prevent the Weedy rice?
R1: Skip some seasons when growing rice,
R2: Grow hydrotonics plants.
Q: How to control the Bacterial Leaf Streak Disease?
R: Do not put too much Nitrogen.
Q: How to eradicate the rice thrips?
R: Spray with Malathion or Carbaryl every week, add fertilizer and water
every two days.
</listItem>
<bodyText confidence="0.999054">
Questions are essentially factoid questions (e.g. products to use, best pe-
riods for plantations, varieties to use, symptoms of a desease), why questions
where responses are chains of events (reasons for something to happen) and
a large number of procedural questions (Delpech et al. 2008) in particular
for treating deseases. There are no comparative or evaluative questions as
in other domains.
</bodyText>
<page confidence="0.996233">
339
</page>
<bodyText confidence="0.9999287">
In most cases, questions do not have responses which can be immediately
found in the texts. For example: How does the Sheath Blight affects the rice
growth? has the following response: Plants heavily infected at these stages
produce poorly filled grain, particularly in the lower portion of the panicle.
Additional losses result from .... Therefore some lexical semantics devices
are needed to allow appropriate question-text matching. The second aspect
of this problem is to be able to extract the complete text portion that
responds to the question. For that purpose we are developing an annotation
methodology whose goal is to identify the different processes at stake and
the needed resources.
</bodyText>
<sectionHeader confidence="0.973314" genericHeader="general terms">
3 The Question-Answering Process Annotation
</sectionHeader>
<bodyText confidence="0.949646961538462">
Since the task is quite large (a large group of students are annotating the set
of 1000 questions and related texts), we need to establish norms and anno-
tation guidelines. Based on the research conducted at IRIT on annotating
procedural questions and instructions based on semantic roles (TextCoop
project), we first annotate the questions and their corresponding responses
as provided by the ministry of agriculture. There are many attempts to
annotate arguments by means of primitives, our approach is here oriented
towards the task and the specific actions. Therefore roles are not as stan-
dard as they are in general (see IWCS 2009 Dautriche et al. this volume).
An earlier attempt for language generation is e.g. (Delin 1994). Seman-
tic tags are either close to thematic roles (instrument, location, etc.) or
borrowed from the primitive systems of the Lexical Conceptual Structure
(LCS), in particular to establish links between arguments, which thematic
roles cannot do. For example, in the first Thai university we have a link
between ’first’ and ’Thai university’ which is either loctemp or loc+char+Zdent
depending on the interpretation of first (oldest or the best). Then, the text
in which the response occurs is annotated (so that the boundaries and sur-
rounding elements of the response can be characterized) and the underlying
lexical semantics mechanisms at stake are given.
Let us present here an illustrative example:
Q: How can thrips destroy the rice ?
annotation:
&lt;question type=”manner” &gt; How can &lt;agent&gt; thrips &lt;/agent&gt; &lt;action&gt;
destroy &lt;theme&gt; the rice &lt;/theme&gt; &lt;/action&gt; ? &lt;/question&gt;
The response is annotated as follows:
&lt;response&gt; &lt;agent&gt; The rice thrips &lt;action&gt; sucks the sap &lt;source&gt;
</bodyText>
<page confidence="0.993751">
340
</page>
<bodyText confidence="0.9868804">
from the young plant. &lt;/source&gt; &lt;/action&gt; &lt;/response&gt;
To match the action ’destroy’ in the question with the text portion from
which the response is extracted, it is then necessary to identify the inference:
&lt;lex inference&gt; &lt;action&gt; Suck sap of X &lt;/action&gt; &lt;entail&gt; &lt;modality&gt;
probably &lt;/modality&gt; &lt;action&gt; destroy X &lt;/action&gt; &lt;/entail&gt;,
</bodyText>
<equation confidence="0.9158385">
&lt;type&gt; X : plant &lt;/type&gt;
&lt;part-of&gt; sap : X &lt;/part-of &gt; &lt;/lex inference&gt;
</equation>
<bodyText confidence="0.999942681818182">
This example shows that (1) in the question and in the answer, anno-
tations are used to identify the different components, arguments, adjuncts,
but also some other components (e.g. temporal adverbs), and (2) the an-
notation developed to characterize the matching between the question and
the answer is used to induce and develop forms of lexical inference (or other
phenomena like synonymy, lexical equivalence, etc.). The types and lexi-
cal functions which are introduced contribute to the process of induction of
generalizations over some semantic categories (plants, products, etc.), and
verb classes.
At this level, the inferences which may be drawn are directly attached
to the terms which are tagged. This is obviously too limited. We are now
exprimenting different generalization levels in order to tune lexical inference
rules. This process involves (1) generalization principles over different types
and categories (via a domain ontology), and (2) a set of principles that
limit these generalizations via, for example, the taking into account of the
semantics restrictions imposed by lexical items, in particular verbs. This
task needs to be developed and evaluated gradually,; so far it is too early to
evaluate the quality of the rules we get, but we believe this is a simple and
close to the data approach and it should be reproducible to other areas.
This approach, and the principles we have briefly outlined, allow us
to introduce a working method for the development of question-answering
systems for concrete applications, for non-factoid questions.
</bodyText>
<sectionHeader confidence="0.998826" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9993434">
[1] Delpech, E., Saint-Dizier, P., 2008, Investigating the Structure of Pro-
cedural Texts for Answering How-to Questions, LREC 2008, Marrakech.
[2] Delin, J., Hartley, A., Paris, C., Scott, D., Vander Linden, K., Express-
ing Procedural Relationships in Multilingual Instructions, Proc. 7th Int.
Workshop on Natural Language Generation, pp. 61-70, Maine, USA, 1994.
</reference>
<page confidence="0.981538">
341
</page>
<reference confidence="0.9999667">
[3] Maybury, M., New Directions in Question Answering, The MIT Press,
Menlo Park, 2004.
[4] Moldovan, D., Harabagiu, S., Pasca, M., Milhacea, R., Goodrum, R.,
Grju, R., Rus, V., The Structure and Performance of an Open-Domain
Question Answering System, Proc. 38th Meeting of the ACL, Hong Kong,
2000.
[5] Takechi, M., Tokunaga, T., Matsumoto, Y., Tanaka, H., Feature Se-
lection in Categorizing Procedural Expressions, The Sixth International
Workshop on Information Retrieval with Asian Languages (IRAL2003),
pp.49-56, 2003.
</reference>
<page confidence="0.998264">
342
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.196915">
<title confidence="0.996017">An Application of Lexical Semantics Annotation Question-Answering in e-Farming</title>
<author confidence="0.998852">Mukda Suktarachan</author>
<affiliation confidence="0.947603">(1) NAIST, Kasetsart University, Bangkok,</affiliation>
<address confidence="0.960288">(2) IRIT, Toulouse,</address>
<email confidence="0.987716">naistdada@yahoo.com,stdizier@irit.fr</email>
<abstract confidence="0.996673661654136">In this poster we present an approach to responding to complex questions in the agriculture domain, from specifications given by experts. We present in particular a semantic annotation procedure that would allow us to define accurate and domain dedicated forms of lexical semantics inference, in order to be able to match non factoid questions (i.e. questions whose response is a significant text portion) with documents on a large scale. This project is designed to help farmers to get advices via question answering on SMS in order to improve rice farming. 1 Challenges and Goals Question answering (Moldovan 2000, Maybury 2004) operates on top of search engines of classical textual database querying tools, by providing a layer that has natural language understanding and generation as well some reasoning capabilities in order to provide users with responses which are much more accurate and cooperative than what search engines provide in general. This is particularly crucial when responses are not straightforward, e.g. when they require some form of elaboration (synthesis of data, consistency checking, etc.), reasoning or when the response is not a simple item, but a well-formed fragment of text, e.g. a chain of events leading to a consequence, a procedure, etc. The project we present here emerged from a need from the Thai Ministry of Agriculture. The main goal is to develop tools for e-Farming, in particular rice farming, so that farmers can easily get information on farming rice and rice diseases, for example via SMS. The Thai Ministry of Agriculture has 338 of the 8th International Conference on Computational pages 338–341, January 2009. International Conference on Computational Semantics large text databases on the way rice can be planted, on how to prepare and fertilize soils and on the numerous diseases rice may be subject to, the effects, the treatments, etc. Question answering is a particularly well-adapted approach to allow farmers to query in Thai (via SMS short messages) such databases. The NAIST lab at the university of Kasetsart has basic tools to parse Thai (stemmer, morphological analysis, part of speech recognition, and simple syntactic analysis). These tools were designed for machine translation purposes, but they turn out to be appropriate to parse queries and to retreive information in technical texts. 2 Outline of the Project and Methodology The needs of the Thai Ministry of Agriculture have been specified in a simple way via a corpus composed of (1) questions raised in real life by farmers (about 1000 questions), (2) the responses which have been provided by experts based on existing documents (possibly several responses per question) and (3) the texts they originate from. In general the response is found in a unique text: there are no multiple answers since most texts are not redundant, although some responses, in particular complex or indirect ones, may involve the taking into account of several independent texts. We will not address here the problem of message length reduction so that it fits into SMS format (althought this is also an important semantic problem). A few examples of question-answer pairs are (glosses from Thai, but structures are in fact quite similar to English): Q: How to prevent the Weedy rice? R1: Skip some seasons when growing rice, R2: Grow hydrotonics plants. Q: How to control the Bacterial Leaf Streak Disease? R: Do not put too much Nitrogen. Q: How to eradicate the rice thrips? R: Spray with Malathion or Carbaryl every week, add fertilizer and water every two days. Questions are essentially factoid questions (e.g. products to use, best periods for plantations, varieties to use, symptoms of a desease), why questions where responses are chains of events (reasons for something to happen) and a large number of procedural questions (Delpech et al. 2008) in particular for treating deseases. There are no comparative or evaluative questions as in other domains. 339 In most cases, questions do not have responses which can be immediately in the texts. For example: does the Sheath Blight affects the rice the following response: heavily infected at these stages produce poorly filled grain, particularly in the lower portion of the panicle. losses result from Therefore some lexical semantics devices are needed to allow appropriate question-text matching. The second aspect of this problem is to be able to extract the complete text portion that responds to the question. For that purpose we are developing an annotation methodology whose goal is to identify the different processes at stake and the needed resources. 3 The Question-Answering Process Annotation Since the task is quite large (a large group of students are annotating the set of 1000 questions and related texts), we need to establish norms and annotation guidelines. Based on the research conducted at IRIT on annotating procedural questions and instructions based on semantic roles (TextCoop project), we first annotate the questions and their corresponding responses as provided by the ministry of agriculture. There are many attempts to annotate arguments by means of primitives, our approach is here oriented towards the task and the specific actions. Therefore roles are not as standard as they are in general (see IWCS 2009 Dautriche et al. this volume). An earlier attempt for language generation is e.g. (Delin 1994). Semantic tags are either close to thematic roles (instrument, location, etc.) or borrowed from the primitive systems of the Lexical Conceptual Structure (LCS), in particular to establish links between arguments, which thematic cannot do. For example, in first Thai university have a link ’first’ and ’Thai university’ which is either or depending on the interpretation of first (oldest or the best). Then, the text in which the response occurs is annotated (so that the boundaries and surrounding elements of the response can be characterized) and the underlying lexical semantics mechanisms at stake are given. Let us present here an illustrative example: can thrips destroy the rice ? annotation: type=”manner” can rice The response is annotated as follows: rice thrips the sap 340 the young plant. To match the action ’destroy’ in the question with the text portion from which the response is extracted, it is then necessary to identify the inference: sap of X X : plant : X This example shows that (1) in the question and in the answer, annotations are used to identify the different components, arguments, adjuncts, but also some other components (e.g. temporal adverbs), and (2) the annotation developed to characterize the matching between the question and the answer is used to induce and develop forms of lexical inference (or other phenomena like synonymy, lexical equivalence, etc.). The types and lexical functions which are introduced contribute to the process of induction of generalizations over some semantic categories (plants, products, etc.), and verb classes. At this level, the inferences which may be drawn are directly attached to the terms which are tagged. This is obviously too limited. We are now exprimenting different generalization levels in order to tune lexical inference rules. This process involves (1) generalization principles over different types and categories (via a domain ontology), and (2) a set of principles that limit these generalizations via, for example, the taking into account of the semantics restrictions imposed by lexical items, in particular verbs. This task needs to be developed and evaluated gradually,; so far it is too early to evaluate the quality of the rules we get, but we believe this is a simple and close to the data approach and it should be reproducible to other areas. This approach, and the principles we have briefly outlined, allow us to introduce a working method for the development of question-answering systems for concrete applications, for non-factoid questions.</abstract>
<note confidence="0.898462111111111">References [1] Delpech, E., Saint-Dizier, P., 2008, Investigating the Structure of Procedural Texts for Answering How-to Questions, LREC 2008, Marrakech. Delin, J., Hartley, A., Paris, C., Scott, D., Vander Linden, K., Express- Procedural Relationships in Multilingual Proc. 7th Int. Workshop on Natural Language Generation, pp. 61-70, Maine, USA, 1994. 341 Maybury, M., Directions in Question The MIT Press, Menlo Park, 2004. [4] Moldovan, D., Harabagiu, S., Pasca, M., Milhacea, R., Goodrum, R., R., Rus, V., Structure and Performance of an Open-Domain Answering Proc. 38th Meeting of the ACL, Hong Kong, 2000. Takechi, M., Tokunaga, T., Matsumoto, Y., Tanaka, H., Sein Categorizing Procedural The Sixth International Workshop on Information Retrieval with Asian Languages (IRAL2003), pp.49-56, 2003. 342</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Delpech</author>
<author>P Saint-Dizier</author>
</authors>
<title>Investigating the Structure of Procedural Texts for Answering How-to Questions,</title>
<date>2008</date>
<location>LREC</location>
<marker>[1]</marker>
<rawString>Delpech, E., Saint-Dizier, P., 2008, Investigating the Structure of Procedural Texts for Answering How-to Questions, LREC 2008, Marrakech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Delin</author>
<author>A Hartley</author>
<author>C Paris</author>
<author>D Scott</author>
<author>Vander Linden</author>
<author>K</author>
</authors>
<title>Expressing Procedural Relationships in Multilingual Instructions,</title>
<date>1994</date>
<booktitle>Proc. 7th Int. Workshop on Natural Language Generation,</booktitle>
<pages>61--70</pages>
<location>Maine, USA,</location>
<marker>[2]</marker>
<rawString>Delin, J., Hartley, A., Paris, C., Scott, D., Vander Linden, K., Expressing Procedural Relationships in Multilingual Instructions, Proc. 7th Int. Workshop on Natural Language Generation, pp. 61-70, Maine, USA, 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Maybury</author>
</authors>
<title>New Directions in Question Answering,</title>
<date>2004</date>
<publisher>The MIT Press,</publisher>
<location>Menlo Park,</location>
<marker>[3]</marker>
<rawString>Maybury, M., New Directions in Question Answering, The MIT Press, Menlo Park, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Moldovan</author>
<author>S Harabagiu</author>
<author>M Pasca</author>
<author>R Milhacea</author>
<author>R Goodrum</author>
<author>R Grju</author>
<author>V Rus</author>
</authors>
<title>The Structure and Performance of an Open-Domain Question Answering System,</title>
<date>2000</date>
<booktitle>Proc. 38th Meeting of the ACL,</booktitle>
<location>Hong Kong,</location>
<marker>[4]</marker>
<rawString>Moldovan, D., Harabagiu, S., Pasca, M., Milhacea, R., Goodrum, R., Grju, R., Rus, V., The Structure and Performance of an Open-Domain Question Answering System, Proc. 38th Meeting of the ACL, Hong Kong, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Takechi</author>
<author>T Tokunaga</author>
<author>Y Matsumoto</author>
<author>H Tanaka</author>
</authors>
<title>Feature Selection in Categorizing Procedural Expressions,</title>
<date>2003</date>
<booktitle>The Sixth International Workshop on Information Retrieval with Asian Languages (IRAL2003),</booktitle>
<pages>49--56</pages>
<marker>[5]</marker>
<rawString>Takechi, M., Tokunaga, T., Matsumoto, Y., Tanaka, H., Feature Selection in Categorizing Procedural Expressions, The Sixth International Workshop on Information Retrieval with Asian Languages (IRAL2003), pp.49-56, 2003.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>