<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001939">
<note confidence="0.55310725">
Subcategorisation Acquisition from Raw Text for a Free Word-Order
Language
Will Roberts and Markus Egg and Valia Kordoni
Institute f¨ur Anglistik und Amerikanistik, Humboldt University
</note>
<address confidence="0.838036">
10099 Berlin, Germany
</address>
<email confidence="0.998061">
{will.roberts,markus.egg,evangelia.kordoni}@anglistik.hu-berlin.de
</email>
<sectionHeader confidence="0.994775" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999992272727273">
We describe a state-of-the-art automatic
system that can acquire subcategorisation
frames from raw text for a free word-order
language. We use it to construct a subcate-
gorisation lexicon of German verbs from a
large Web page corpus. With an automatic
verb classification paradigm we evaluate
our subcategorisation lexicon against a pre-
vious classification of German verbs; the
lexicon produced by our system performs
better than the best previous results.
</bodyText>
<sectionHeader confidence="0.99843" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99995">
We introduce a state-of-the-art system for the ac-
quisition of subcategorisation frames (SCFs) from
large corpora, which can deal with languages with
very free word order. The concrete language we
treat is German; its word order variability is illus-
trated in (1)–(4), all of which express the sentence
The man gave the old dog a chop:
</bodyText>
<listItem confidence="0.99977375">
(1) Dem alten Hund gab der Mann ein Schnitzel.
(2) Ein Schnitzel gab dem alten Hund der Mann.
(3) Ein Schnitzel gab der Mann dem alten Hund.
(4) Der Mann gab dem alten Hund ein Schnitzel.
</listItem>
<bodyText confidence="0.999634615384615">
On the basis of raw text, the system can be
used to build extensive SCF lexicons for German
verbs. Subcategorisation means that lexical items
require specific obligatory concomitants or argu-
ments; we focus on verb subcategorisation. E.g.,
the verb geben ‘give’ requires three arguments, the
nominative subject der Mann ‘the man’, the dative
indirect object dem alten Hund ‘the old dog’, and
the accusative direct object ein Schnitzel ‘a chop’.
Other syntactic items may be subcategorised for,
too, e.g. both stellen and its English translation
put subcategorise for subject, direct object, and a
prepositional phrase (PP) like on the shelf:
</bodyText>
<listItem confidence="0.863706">
(5) [NP Al] put [NP the book] [PP on the shelf].
</listItem>
<bodyText confidence="0.999842870967742">
Subcategorisation frames describe a combina-
tion of arguments required by a specific verb. The
set of SCFs for a verb is called its subcategori-
sation preference. Our system follows much pre-
vious work by counting PPs that accompany the
verb among its complements, even though they are
not obligatory (so-called ‘adjuncts’), because PP
adjuncts are excellent clues to a verb’s semantics
(Sun et al., 2008). However, nominal and clausal
adjuncts do not count as verbal complements.
SCF information can benefit all applications
that need information on predicate-argument struc-
ture, e.g., parsing, verb clustering, semantic role la-
belling, or machine translation. Automatic acquisi-
tion of SCF information with minimal supervision
is also crucial to construct useful resources quickly.
The main innovation of the presented new sys-
tem is to address two challenges simultaneously,
viz., SCF acquisition from raw text and the focus
on languages with a very free word order. With
this system, we create an SCF lexicon for German
verbs and evaluate this lexicon against a previously
published manual verb classification, showing bet-
ter performance than has been reported until now.
After an overview of previous work on SCF ac-
quisition in Section 2, Section 3 describes our sub-
categorisation acquisition system, and Section 4
the SCF lexicon that we build using it. In Sec-
tions 5 and 6 we evaluate the SCF lexicon on a verb
classification task and discuss our results; Section 7
then concludes with directions for future work.
</bodyText>
<page confidence="0.961681">
298
</page>
<note confidence="0.9973785">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 298–307,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.967802" genericHeader="method">
2 Previous work
</sectionHeader>
<bodyText confidence="0.999967">
To date, research on SCF acquisition from corpora
has mostly targeted English. Brent and Berwick
(1991) detect five SCFs by looking for attested
contexts where argument slots are filled by closed-
class lexical items (pronouns or proper names).
Briscoe and Carroll (1997) detect 163 SCFs with
a system that builds an SCF lexicon whose en-
tries include the relative frequency of SCF classes.
Potential SCF patterns are extracted from a cor-
pus parsed with a dependency-based parser, and
then filtered by hypothesis testing on binomial fre-
quency data. Korhonen (2002) refines Briscoe and
Carroll (1997)’s system using back-off estimates
on the WordNet semantic class of the verb’s pre-
dominant sense, assuming that semantically similar
verbs have similar SCFs, following Levin (1993).
Some current statistical methods for Semantic Role
Labelling build models that also capture subcat-
egorisation information, e.g., Grenager and Man-
ning (2006). Schulte im Walde (2009) offers a re-
cent survey of the SCF acquisition literature.
SCF acquisition is also an important step in the
automatic semantic role labelling (Grenager and
Manning, 2006; Lang and Lapata, 2010; Titov and
Klementiev, 2012). Semantic roles of a verb de-
scribe the kind of involvement of entities in the
event introduced by the verb, e.g., as agent (active,
often not affected by the event) or patient (passive,
often affected). On the basis of these SCFs, se-
mantic roles can be assigned due to the interdepen-
dence between semantic roles and their syntactic
realisations, called Argument Linking (Levin, 1993;
Levin and Rappaport Hovav, 2005).
Acquiring SCFs for languages with a very fixed
word order like English needs only a simple syn-
tactic analysis, which mainly relies on the prede-
termined sequencing of arguments in the sentence,
e.g., Grenager and Manning (2006). When word
order is freer, the analysis gets more complicated,
and must include a full syntactic parse.
What is more, German is a counterexample to
Manning’s (1993) expectation that freedom of
word order should be matched by an increase in
case and/or agreement marking. This is due to a
very high degree of syncretism (identity of word
forms) in German paradigms for nouns, adjectives,
and determiners. E.g., the noun Auto ‘car’ has only
two forms, Auto for nominative, dative, and ac-
cusative singular, and Autos for genitive singular
and all four plural forms. This is in contrast to some
other free word order languages for which SCF
acquisition has been studied, like Modern Greek
(Maragoudakis et al., 2000) and Czech (Sarkar and
Zeman, 2000). A one-many relation between word
forms and case is also one of the problems for SCF
acquisition in Urdu (Ghulam, 2011).
For German, initial studies used semi-automatic
techniques and manual evaluation (Eckle-Kohler,
1999; Wauschkuhn, 1999). The first automatic sub-
categorisation acquisition system for German is de-
scribed by Schulte im Walde (2002a), who defined
an SCF inventory and manually wrote a grammar
to analyse verb constructions according to these
frames. A lexicalised PCFG parser using this gram-
mar was trained on 18.7 million words of German
newspaper text; the trained parser model contained
explicit subcategorisation frequencies, which could
then be extracted to construct a subcategorisation
lexicon for 14,229 German verbs. This work was
evaluated against a German dictionary, the Duden
Stilw¨orterbuch (Schulte im Walde, 2002b).
Schulte im Walde and Brew (2002) used the sub-
categorisation lexicon created by the system to au-
tomatically induce a set of semantic verb classes
with an unsupervised clustering algorithm. This
clustering was evaluated against a small manually
created semantic verb classification. Schulte im
Walde (2006) continues this work using a larger
manual verb classification. The SCFs used in this
study are defined at three levels of granularity. The
first level (38 different SCFs) lists only the comple-
ments in the frame; the second one adds head and
case information for PP complements (183 SCFs).
The third level examined the effect of adding selec-
tional preferences, but results were inconclusive.
A recent paper (Scheible et al., 2013) describes a
system similar to ours, built on a statistical depen-
dency parser, and using some of the same kinds
of rules as we describe in Section 3.1; this system
is evaluated in a task-based way (e.g., to improve
the performance of a SMT system) and cannot be
directly compared to our system in this paper.
</bodyText>
<sectionHeader confidence="0.989132" genericHeader="method">
3 The SCF acquisition system
</sectionHeader>
<bodyText confidence="0.999912285714286">
This section describes the first contribution of this
paper, a state-of-the-art subcategorisation acquisi-
tion system for German. Its core component is a
rule-based SCF tagger which operates on phrase
structure analyses, as delivered by a statistical
parser. Given a parse of a sentence, the tagger as-
signs each finite verb in the sentence an SCF type.
</bodyText>
<page confidence="0.996852">
299
</page>
<bodyText confidence="0.999989208333333">
We use the SCF inventory of Schulte im Walde
(2002a), which includes complements like n for
nominative subject, a for accusative direct object,
d for dative indirect object, r for reflexive pronoun,
and x for expletive es (‘it’) subject. Clausal com-
plements can be infinite (i); finite ones can have
the verb in second position (S-2) or include the
complementiser dass ‘that’ (S-dass). Comple-
ments can be combined as in na (transitive verb);
for PPs in SCFs, the head is specified, e.g., p:f¨ur
for PP complements headed by f¨ur ‘for’1.
Due to the free word order, simple phrase struc-
ture like that used for analysis of English is not
enough to specify the syntax of German sentences.
Therefore we use the annotation scheme in the
manually constructed German treebanks NEGRA
and TIGER (Skut et al., 1997; Brants et al., 2002),
which decorate parse trees with edge labels specify-
ing the syntactic roles of constituents. We automat-
ically annotate the parse trees from our statistical
parser using a simple machine learning model.
In the next section, we illustrate the operation of
the SCF tagger with reference to examples; then in
Section 3.2 we describe our edge labeller.
</bodyText>
<subsectionHeader confidence="0.998003">
3.1 The SCF tagger
</subsectionHeader>
<bodyText confidence="0.999243875">
The SCF tagger begins by collecting complements
co-occurring with a verb instance using the phrase
structure of the sentence. In our system, we obtain
phrase structure information for unannotated text
using the Berkeley Parser (Petrov et al., 2006), a
statistical unlexicalised parser trained on TIGER.
Fig. 1 illustrates the phrase structure analysis and
edge labels in the TIGER corpus for (6):
</bodyText>
<listItem confidence="0.853593">
(6) Das hielte ich f¨ur moralisch außerordentlich
fragw¨urdig.
‘I’d consider that morally extremely
questionable’.
</listItem>
<bodyText confidence="0.999651111111111">
Its finite verb hielte (from halten ‘hold’) has
three complements, the subject ich ‘I’, edge-
labelled with SB, the direct object das ‘that’, la-
belled with OA, and a PP headed by f¨ur ‘for’ (MO
stands for ‘modifier’). After collecting comple-
ments, the SCF tagger uses this edge label infor-
mation to determine the complements’ syntactic
roles, and assigns the verb the corresponding SCF;
in the case of halten above, the SCF is nap:f¨ur.
</bodyText>
<footnote confidence="0.938455">
1We digress from Schulte im Walde’s original SCF inven-
tory in that we do not indicate case information in PPs.
</footnote>
<bodyText confidence="0.975787636363636">
The rule-based SCF tagger handles auxiliary and
modal verb constructions, passive alternations, sep-
arable verb prefixes, and raising and control con-
structions. E.g., the subject sie ‘they’ of anfangen
‘begin’ in (7) doubles as the subject of its infinite
clausal complement; hence, it shows up in the SCF
of the complement’s head geben ‘give’, too:
(7) Sie fingen an, mir Stromschl¨age zu geben.
‘They started to give me electric shocks.’
The tagger also handles involved cases with
many complements, including PPs and clauses as
in (8). As the SCF inventory allows at most three
complements in an SCF, such cases call for pri-
oritising of verbal complements (e.g., subjects, ob-
jects, and clausal complements are preferred over
PP complements). Consequently, the main verb
empfehlen ‘recommend’ in (8), which has a subject,
a dative object, a PP, and an infinitival clausal com-
plement, is assigned the SCF ndi. Another chal-
lenging task which relies on edge label information
is filtering out clausal adjuncts (relative clauses and
parentheticals) so as not to include them in SCFs.
</bodyText>
<listItem confidence="0.909613666666667">
(8) [PP Am Freitag] empfahl [NP:Nom der
Aufsichtsrat] [NP:Dat den Aktion¨aren], [S das
Angebot abzulehnen].
</listItem>
<bodyText confidence="0.9875595">
‘On Friday the board of directors advised
shareholders to turn down the offer.’
The 17 rules of the SCF tagger are simple; most
of them categorise the complements of a specific
verb instance; e.g., if a nominal complement to the
verb is edge-labelled as a nominative subject, add n
to the verb’s SCF, unless the verb is in the passive,
in which case add a to the SCF.
Our system was optimised by progressively re-
fining the SCF tagger’s rules through manual error
analysis on sentences from TIGER. The result is
an automatic SCF tagger that is resilient to varia-
tions in sentence structure and is firmly based on
linguistically motivated knowledge. As a test case
for its linguistic soundness, we chose the perfect
parses in the TIGER treebank and found that the
tagger is very accurate in capturing subcategorisa-
tion information inherent in these data.
</bodyText>
<subsectionHeader confidence="0.999314">
3.2 The edge labeller
</subsectionHeader>
<bodyText confidence="0.998887333333333">
To obtain edge label information for the parses de-
livered by the Berkeley Parser, we built a novel
machine learning classifier to annotate parse trees
</bodyText>
<page confidence="0.994173">
300
</page>
<figureCaption confidence="0.999598">
Figure 1: Edge labels in the TIGER corpus.
</figureCaption>
<figure confidence="0.996819352941176">
hielte
ich
f¨ur
moralisch außerordentlich fragw¨urdig
S
MO
PP
OA HD SB
AC NK
VVFIN
PPER
APPR
AP
PDS
Das
$.
.
</figure>
<bodyText confidence="0.995189">
with TIGER edge label information. This edge la-
beller is a maximum entropy (multiclass logistic
regression) model built using the Stanford Classi-
fier package2. We include features such as:
</bodyText>
<listItem confidence="0.956542235294118">
• The part of speech of the complement;
• The first word of the complement;
• The lexical head of the complement;
• N-grams on the end of the lexical head of the
complement;
• The kind of article of a complement;
• The presence or absence of specific article
forms in other complements to the same verb;
• Position of the complement with respect to a
reflexive pronoun in the sentence;
• The lemmatised form of the verb governing
the complement (i.e., the verb on which the
complement depends syntactically);
• The clause type of the governing verb; and,
• Active or passive voice of the governing verb.
We do no tuning and use the software’s default
hyperparameters (L2 regularisation with σ = 3).
</listItem>
<bodyText confidence="0.999869882352941">
This classifier was trained from edge label data
extracted from the NEGRA and TIGER corpora;
our training set contained 300,000 samples (ap-
proximately 25% from NEGRA and 75% from
TIGER). On a held-out test set of 10% (contain-
ing 34,000 samples), the classifier achieves a final
F-score of 95.5% on the edge labelling task.
The edge labeller makes the simplifying assump-
tion that verbal complements can be labelled inde-
pendently. Consequently, it tends to annotate multi-
ple complements as subject for each verb. This has
to do with the numerical dominance of subjects,
which make up about 40% of all verb complements,
more than three times the number of the next most
common complement type (direct object).
Therefore we first collect all possible labels with
associated probabilities that the edge labeller as-
</bodyText>
<footnote confidence="0.8555925">
2http://nlp.stanford.edu/software/
classifier.shtml
</footnote>
<bodyText confidence="0.999921416666667">
signs to each complement of a verb. We then
choose the set of labels with the highest probability
that includes at most one subject and at most one
accusative direct object for the verb, assuming that
the joint probability of a set of labels is the product
of the individual label probabilities.
We use our edge labeller in this work for mor-
phological disambiguation of nominals and for
identifying clausal adjuncts, but the edge labeller
is a standalone reusable component, which might
be equally well be used to mark up parse trees for,
e.g., a semantic role labelling system.
</bodyText>
<sectionHeader confidence="0.979702" genericHeader="method">
4 The subcategorisation lexicon
</sectionHeader>
<bodyText confidence="0.999880615384615">
With the system described in Sec. 3, we build a Ger-
man subcategorisation lexicon that collects counts
of (lemma, SCF) on deWaC (Baroni et al., 2009),
a corpus of text extracted from Web search re-
sults, with 109 words automatically POS-tagged
and lemmatised by the TreeTagger (Schmid, 1994).
A subset of this corpus, SdeWaC (Faaß and Eckart,
2013), has been preprocessed to include only sen-
tences which are maximally parsable; this smaller
corpus includes 880 million words in 45 million
sentences. We parsed 3 million sentences (80 mil-
lion words) of SdeWaC; after filtering out those
verb lemmas seen only five times or fewer in the
corpus, we are left with statistics on 8 million verb
instances, representing 9,825 verb lemmas.
As a concrete example for the resulting SCF lexi-
con, consider the entry for sprechen ‘talk’ in Fig. 2,
which occurs 16,254 times in our SCF lexicon.
Sprechen refers to a conversation with speaker,
hearer, topic, message, and code: Speakers are ex-
pressed by nominative NPs, hearers, by mit-, bei-
or zu-PPs, topics, by von- and fuber-PPs. The code
is expressed in in-PPs, and the message, by ac-
cusative NPs (einige Worte sprechen ‘to say a few
words’), main-clause complements or subordinate
dass (‘that’) sentences. Other uses of the verb are
</bodyText>
<page confidence="0.993588">
301
</page>
<table confidence="0.683633285714286">
np:von (2715), n (2696), na (1380), np:mit
(1247), np:in (1132), nS-2 (1064), np:luber
(853), np:flur (695), nS-dass (491), np:zu
(307), nap:in (280), nap:von (275), ni (261),
np:bei (212), np:gegen (192), np:an (186),
naS-2 (172), np:aus (168), np:auf (112),
nap:luber (112)
</table>
<figureCaption confidence="0.990595">
Figure 2: SCF lexicon for sprechen
</figureCaption>
<bodyText confidence="0.999949882352941">
figurative , e.g., sprechen gegen ‘be a counterar-
gument to’. As the distinction between arguments
and adjuncts is gradual in our system, some adjunct
patterns appear in the lexicon, too, but only with
low frequency, e.g., np:auf, in which the auf-PP
expresses the setting of the conversation, as in auf
der Tagung sprechen ‘speak at the convention’.
For reference, we also constructed an SCF lexi-
con from the NEGRA and TIGER corpora, which
together comprise about 1.2 million words. This
SCF lexicon contains statistics on 133,897 verb
instances (5,316 verb lemmas). While the manual
annotations in NEGRA and TIGER mean that this
SCF lexicon has virtually no noise, the small size
of the corpora results in problems with data spar-
sity and negatively impacts the utility of this re-
source (see discussion in Section 6.2).
</bodyText>
<sectionHeader confidence="0.968273" genericHeader="method">
5 Automatic verb classification
</sectionHeader>
<bodyText confidence="0.999969926829268">
The remainder of the paper sets out to establish the
relevance of our SCF acquisition system by com-
parison to previous work. As stated in Sec. 2, the
only prior automatic German SCF acquisition sys-
tem is that of Schulte im Walde (2002a), which was
evaluated directly against an electronic version of
a large dictionary; as this is not an open access
resource, we cannot perform a similar evaluation.
We opt therefore to use a task-based evaluation
to compare our system directly with Schulte im
Walde’s, and leave manual evaluation for future
work. We refer back to the experiment set up by
Schulte im Walde (2006) to automatically induce
classifications of German verbs by clustering them
on the basis of their SCF preferences as listed in
her SCF lexicon. By casting this experiment as a
fixed task, we can compare our system directly to
hers. The link between subcategorisation and verb
semantics is linguistically sound, due to the inter-
dependence between verb meanings and the num-
ber and kinds of their syntactic arguments (Levin,
1993; Levin and Rappaport Hovav, 2005). E.g.,
only transitive verbs that denote a change of state
like cut and break enter in the middle construction
(The bread cuts easily.), with the patient or theme
argument appearing as the syntactic subject. Thus,
verbs whose SCF preferences show such an alter-
nation can be predicted to denote a change of state.
We adopt the automatic verb classification
paradigm to evaluate our system, replicating
Schulte im Walde’s (2006) experiment to the best
of our ability. We argue that by evaluating our
SdeWaC SCF lexicon described in the previous
section, we simultaneously evaluate our subcate-
gorisation acquisition system; this technique also
allows us to demonstrate the semantic relevance of
our SCF lexicon. Section 5.1 introduces the man-
ual verb classification we use as a gold standard
and Section 5.2 describes our unsupervised clus-
tering technique. Our evaluation of the clustering
against the gold standard then follows in Section 6.
</bodyText>
<subsectionHeader confidence="0.984369">
5.1 Manual verb classifications
</subsectionHeader>
<bodyText confidence="0.999983">
The semantic verb classification proposed by
Schulte im Walde (2006, page 162ff.), hereafter
SiW2006, comprises 168 high- and low-frequency
verbs grouped into 43 semantic classes, with be-
tween 2 and 7 verbs per class. Examples of these
classes are Aspect (e.g., anfangen ‘begin’), Propo-
sitional Attitude (e.g., denken ‘think’), Transfer of
Possession (Obtaining) (e.g., bekommen ‘get’), and
Weather (e.g., regnen ‘rain’). Some of the classes
are subclassified3, e.g., Manner of Motion, with
the subclasses Locomotion (klettern ‘climb’), Ro-
tation (rotieren ‘rotate’), Rush (eilen ‘hurry’), Ve-
hicle (fliegen ‘fly’), and Flotation (gleiten ‘glide’).
These classes are related to Levin classes in that
some are roughly equivalent to a Levin class (e.g.,
Aspect and Levin’s Begin class), others are sub-
groups of Levin classes, e.g., Position is a sub-
group of Levin’s Dangle class; finally, some classes
lump together Levin classes, e.g., Transfer of Pos-
session (Obtaining) combines Levin’s Get and Ob-
tain classes. This shows that these classes could be
integrated into a large-scale classification of Ger-
man verbs in the style of Levin (1993).
</bodyText>
<subsectionHeader confidence="0.997448">
5.2 Clustering
</subsectionHeader>
<bodyText confidence="0.999098666666667">
From the counts of (lemma, SCF) in the SCF lexi-
con, we can estimate the conditional probability
that a particular verb v appears with an SCF f:
</bodyText>
<footnote confidence="0.995044">
3For the purpose of our evaluation, we disregard class-
subclass relations and consider subclasses as separate entities.
</footnote>
<page confidence="0.997095">
302
</page>
<bodyText confidence="0.994733714285714">
P(scf = f|lemma = v). We smooth these con-
ditional probability distributions by backing off to
the prior probability P(scf) (Katz, 1987).
With these smoothed conditional probabilities,
we cluster verbs with k-means clustering (Forgy,
1965), a hard clustering technique, which partitions
a set of objects into k clusters. The algorithm is ini-
tialised with a starting set of k cluster centroids; it
then proceeds iteratively, first assigning each ob-
ject to the cluster whose centroid is closest under
some distance measure, and then calculating new
centroids to represent the centres of the updated
clusters. The algorithm terminates when the assign-
ment of objects to clusters no longer changes.
</bodyText>
<equation confidence="0.987543857142857">
�
D(pllq) =
i
irad(p, q) = D(pllp +q
2 ) + D(qllp +q
2 ) (10)
skew(p, q) = D(pllαq + (1 − α)p) (11)
</equation>
<bodyText confidence="0.999978714285714">
In our experiments, verbs are represented by
their conditional probability distributions over
SCFs. As distance measures, we use two variants
of the Kullback-Leibler divergence (9), a measure
of the dissimilarity of two probability distributions.
The KL divergence from p to q is undefined if at
some point q but not p is zero, so we use measures
based on KL without this problem, viz., the in-
formation radius (aka Jensen-Shannon divergence,
a symmetric metric, (10)), as well as skew diver-
gence (an asymmetric dissimilarity measure which
smoothes q by interpolating it to a small degree
with p, (11)), where we set the interpolation param-
eter to be α = 0.9, to make our results comparable
to Schulte im Walde’s (2006)4.
As mentioned, the k-means algorithm is ini-
tialised with a set of cluster centroids; in this study,
we initialise the centroids by random partitions
(each of the n objects is randomly assigned to one
of k clusters, and the centroids are then computed
as the means of these random partitions). Because
the random initial centroids influence the final clus-
tering, we repeat the clustering a number of times.
We also initialise the k-means cluster centroids
using agglomerative hierarchical clustering, a de-
terministic iterative bottom-up process. Hierarchi-
cal clustering initially assigns verbs to singleton
clusters; the two clusters which are “nearest” to
</bodyText>
<footnote confidence="0.459585333333333">
4Schulte im Walde (2006) takes α = 0.9 although Lee
(1999) recommends α = 0.99 or higher values in her original
description of skew divergence.
</footnote>
<bodyText confidence="0.999955166666667">
each other are then joined together, and this pro-
cess is repeated until the desired number of clusters
is obtained. Hierarchical clustering is performed
to group the verbs into k clusters; the centroids
of these clusters are then used to initialise the k-
means algorithm. While there exist several variants
of hierarchical clustering, we use Ward’s method
(Ward, Jr, 1963) for merging clusters, which at-
tempts to minimise the variance inside clusters;
Ward’s criterion was previously found to be the
most effective hierarchical clustering technique for
verb classification (Schulte im Walde, 2006).
</bodyText>
<sectionHeader confidence="0.998178" genericHeader="evaluation">
6 Evaluation
</sectionHeader>
<bodyText confidence="0.999976666666667">
This section presents the results of evaluating the
unsupervised verb clustering based on our SCF lex-
ica against the gold standard described in Sec. 5.1.
</bodyText>
<subsectionHeader confidence="0.636644">
6.1 Results
</subsectionHeader>
<bodyText confidence="0.99992734375">
We use two cluster purity measures, defined in
Fig. 3; we intentionally target our numerical eval-
uations to be directly comparable with previous
results in the literature. As k-means is a hard clus-
tering algorithm, we consider a clustering C to be
an equivalence relation that partitions n verbs into
k disjoint subsets C = {C1, ... , Ck}.
The first of these purity measures, adjusted Rand
index (Randa in Eq. (12)) judges clustering simi-
larity using the notion of the overlap between a
cluster Ci in a given clustering C and a cluster Gj
in a gold standard clustering g, this value being
denoted by Cgij = |Ci n Gj|; values of Randa
range between 0 for chance and 1 for perfect cor-
relation. The other metric, the pairwise F-score
(PairF, Eq. (13)), operates by constructing a con-
tingency table on the (2) pairs of verbs, the idea
being that the gold standard provides binary judge-
ments about whether two verbs should be clustered
together or not. If a clustering agrees with the gold
standard in clustering a pair of verbs together or
separately, this is a “correct” answer; by extension,
information retrieval measures such as precision
(P) and recall (R) can be computed.
Table 1 shows the performance of our SCF lex-
ica, evaluated against the SiW2006 gold standard.
The random baseline is given by PairF = 2.08 and
Randa = −0.004 (calculated as the average of 50
random partitions). The optimal baseline is PairF
= 95.81 and Randa = 0.909, calculated by evalu-
ating the gold standard against itself. As the gold
standard includes polysemous verbs, which belong
</bodyText>
<equation confidence="0.948774733333333">
pi log pi
qi
(9)
303
Randa(C, G) =
P �CGij� − hP �|Ci|� P �|Gj|�i /�n �
i,j 2 i 2 j 2 2
hP �|Ci |�|Gj |�|Ci|� P ~i (12)
� + P hP
~i �|Gj |/�n �
1 −
2 i 2 j 2 i 2 j 2 2
2P(C, G)R(C, G)
PairF(C, G) = (13)
P(C, G) + R(C, G)
</equation>
<figureCaption confidence="0.995802">
Figure 3: Evaluation metrics used to compare clusterings to gold standards.
</figureCaption>
<table confidence="0.999800307692308">
Data Set Eval Distance Manual Random Best Random Mean Ward
Schulte im Walde PairF IRad 40.23 1.34 → 16.15 13.37 17.86 → 17.49
Skew 47.28 2.41 → 18.01 14.07 15.86 → 15.23
Randa IRad 0.358 0.001 → 0.118 0.093 0.145 → 0.142
Skew 0.429 −0.002 → 0.142 0.102 0.158 → 0.158
NEGRA/TIGER PairF IRad 30.77 2.06 → 14.67 12.39 16.13 → 15.52
Skew 40.19 3.47 → 12.95 11.48 14.05 → 14.31
Randa IRad 0.281 0.000 → 0.122 0.094 0.134 → 0.129
Skew 0.382 −0.015 → 0.102 0.089 0.112 → 0.114
SdeWaC PairF IRad 42.66 1.62 → 20.36 18.26 26.94 → 27.50
Skew 50.38 2.99 → 20.75 17.80 24.60 → 24.94
Randa IRad 0.387 −0.006 → 0.167 0.146 0.232 → 0.238
Skew 0.465 0.008 → 0.170 0.143 0.208 → 0.211
</table>
<tableCaption confidence="0.999923">
Table 1: Evaluation of the NEGRA/TIGER and SdeWaC SCF lexica using the SiW2006 gold standard.
</tableCaption>
<bodyText confidence="0.9997299">
to more than one cluster, the optimal baseline is
calculated by randomly picking one of their senses;
the average is then taken over 50 such trials.
We cluster using k = 43, matching the number
of clusters in the gold standard. Of the 168 verbs in
SiW2006, 159 are attested in NEGRA and TIGER
(17,285 instances), and 167 are found in SdeWaC
(1,047,042 instances)5.
We report the results using k-means clustering
initialised under a variety of conditions. “Manual”
shows the quality of the clustering achieved when
initialising k-means with the gold standard classes.
We also initialise clustering 10 times using ran-
dom partitions. For the best clustering6 in these
10, “Random Best” shows the evaluation of both
the starting random partition and the final cluster-
ing found by k-means; “Random Mean” shows the
average cluster purity of the 10 final clusterings.
“Ward” shows the evaluation of the clustering ini-
tialised with centroids found by hierarchical clus-
</bodyText>
<footnote confidence="0.9851106">
5Verbs missing from the clustering reduce the maximum
achievable cluster purity score.
6Specifically, we take the clustering result with the mini-
mum intra-cluster distance (not the clustering result with the
best performance on the gold standard).
</footnote>
<bodyText confidence="0.999961210526316">
tering of the verbs using Ward’s method. Again,
both the initial partition found by Ward’s method
and the k-means solution based on it are shown.
For comparison, we list the results of Schulte
im Walde (2006, p. 174, Table 7) for the second
level of SCF granularity, with PP head and case
information (see Sec. 2 for Schulte im Walde’s
analysis). While this seems the most appropriate
comparison to draw, since we also collect statis-
tics about PPs, it is ambitious because, as noted
in Section 3, our SCF lexica lack case informa-
tion about PPs.7 Compared to Schulte im Walde’s
numbers, the NEGRA/TIGER SCF lexicon scores
significantly worse on the PairF evaluation metric
under all conditions, and also on the Randa metric
using the skew divergence measure (Randa/IRad
is not significantly different). The SdeWaC SCF
lexicon scores better on all metrics and conditions;
these results are significant at the p &lt; 0.001 level8.
</bodyText>
<footnote confidence="0.979372">
7PP case information is relevant for prepositions that can
take both locative and directional readings, as in in der Stadt
(dative) ‘in town’ und in die Stadt (accusative) ‘to town’.
8Statistical significance is calculated by running repeated
k-means clusterings with random partition initialisation and
evaluating the results using the relevant purity metrics. These
repeated clustering scores represent a random variable (a func-
</footnote>
<page confidence="0.99643">
304
</page>
<subsectionHeader confidence="0.991913">
6.2 Discussion
</subsectionHeader>
<bodyText confidence="0.9964032">
Sec. 6.1 compared the SCF lexicon created us-
ing SdeWaC with the lexicon built by Schulte im
Walde (2002a), showing that our lexicon achieves
significantly better results on the verb clustering
task. We interpret this to be indicative of a more
accurate subcategorisation lexicon, and, by exten-
sion, of a more accurate SCF acquisition system.
We attribute this superior performance primar-
ily to our use of a statistical parser as opposed to
a hand-written grammar. This design choice has
several advantages. First, the parser delivers robust
syntactic analyses, which we can expect to be rel-
atively domain-independent. Second, we make no
prior assumptions about the variety of subcategori-
sation phenomena that might appear in text, decou-
pling the identification of SCFs from the ability
to parse natural language. Third, the fact that our
parser and edge labeller are trained on the 800,000
word NEGRA/TIGER corpus means that we bene-
fit from the linguistic expertise that went into build-
ing that treebank. Our use of off-the-shelf tools
(the parser and our simple yet effective machine
learning model describing edge label information)
makes our system considerably simpler and easier
to implement than Schulte im Walde’s. We see our
system as more easily extensible to other languages
for which there is a parser and an initial syntacti-
cally annotated corpus to train the edge labeller on.
The NEGRA/TIGER SCF lexicon performs not
as well on the verb clustering evaluations, as fewer
verbs are attested in NEGRA/TIGER compared to
the SdeWaC SCF lexicon and gold standard clus-
terings. Data sparsity can be a problem in SCF ac-
quisition; all other factors being equal, using more
data to construct an SCF lexicon should make pat-
terns in the language more readily visible and re-
duce the chance of missing a particular lemma-
SCF combination accidentally. A secondary ef-
fect is that models of verb subcategorisation prefer-
ences like the ones used here can be more precisely
estimated as the counts of observed verb instances
increase, particularly for low-frequency verbs.
Error analysis of our SCF lexicon reveals low
counts of expletive subjects. The edge labeller is
supposed to annotate semantically empty subjects
(es, ‘it’) as expletive; for clusterings examined in
Sec. 5.1, this would affect weather verbs (e.g., es
tion of the random cluster centroids used to initialise the k-
means clustering). These samples are normally distributed, so
we determine statistical significance using a t-test against the
“Random Mean” results reported by Schulte im Walde (2006).
regnet, ‘it’s raining’). However, in our SdeWaC
SCF lexicon, expletive subjects are clearly under-
represented. Our SCF lexicon built on TIGER,
where expletive subjects are systematically la-
belled, has the SCF xa as the most common SCF
for the verb geben (in es gibt ‘there is’). In con-
trast, in our SdeWaC SCF lexicon, the most com-
mon SCF is the transitive na, with xa in seventh
place. I.e., the edge labeller does not identify all
expletive subjects, which is due to the fact that ex-
pletive subjects are syntactically indistinguishable
from neuter pronominal subjects, so the edge la-
beller does not have a rich feature set to inform it
about this category. But since, statistically, exple-
tive pronouns make up less than 1% of subjects
in TIGER, the prior probability of labelling a con-
stituent as expletive is very low. Due to these fig-
ures, we do not expect this issue to seriously impact
the quality of our verb classification evaluations.
</bodyText>
<sectionHeader confidence="0.999697" genericHeader="discussions">
7 Future work
</sectionHeader>
<bodyText confidence="0.9995244">
In this paper we have presented a state-of-
the-art subcategorisation acquisition system for
free-word order languages, and used it to cre-
ate a large subcategorisation frame lexicon for
German verbs. Our SCF lexicon resource is
available at http://amor.cms.hu-berlin.
de/˜robertsw/scflex.html. We are per-
forming a manual evaluation of the output of our
system, which we will report soon.
We plan to continue this work first by expanding
our SCF lexicon with case information and selec-
tional preferences, second by using our SCF clas-
sifier and lexicon for verbal Multiword Expression
identification in German, and last by comparing
it to existing verb classifications, either by using
available resources for German like the SALSA
corpus (Burchardt et al., 2006), or by translating
parts of VerbNet into German to create a more
extensive gold standard for verb clustering in the
spirit of Sun et al. (2010) who found that Levin’s
verb classification can be translated to French and
still usefully allow generalisation over verb classes.
Finally, we plan to perform in vivo evaluation
of our SCF lexicon, to determine what benefit
it can deliver for NLP applications such as Se-
mantic Role Labelling and Word Sense Disam-
biguation. Recent research has found that even
automatically-acquired verb classifications can be
useful for NLP applications (Shutova et al., 2010;
Guo et al., 2011).
</bodyText>
<page confidence="0.998599">
305
</page>
<sectionHeader confidence="0.983299" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999719735294118">
Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The WaCky wide web: A
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209–226.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
treebank. In TLT, pages 24–41.
Michael R. Brent and Robert C. Berwick. 1991. Auto-
matic acquisition of subcategorization frames from
tagged text. In HLT, pages 342–345. Morgan Kauf-
mann.
Ted Briscoe and John Carroll. 1997. Automatic ex-
traction of subcategorization from corpora. CoRR,
cmp-lg/9702002.
Aljoscha Burchardt, Katrin Erk, Anette Frank, An-
drea Kowalski, Sebastian Pado, and Manfred Pinkal.
2006. The SALSA corpus: A German corpus re-
source for lexical semantics. In LREC.
Judith Eckle-Kohler. 1999. Linguistic knowledge for
automatic lexicon acquisition from German text cor-
pora. Ph.D. thesis, Universit¨at Stuttgart.
Gertrud Faaß and Kerstin Eckart. 2013. SdeWaC - A
corpus of parsable sentences from the Web. In Lan-
guage processing and knowledge in the Web, pages
61–68. Springer, Berlin, Heidelberg.
Edward W. Forgy. 1965. Cluster analysis of multivari-
ate data: Efficiency versus interpretability of classifi-
cations. Biometrics, 21:768–769.
Raza Ghulam. 2011. Subcategorization acquisition
and classes of predication in Urdu. Ph.D. thesis,
Universit¨at Konstanz.
Trond Grenager and Christopher D. Manning. 2006.
Unsupervised discovery of a statistical verb lexicon.
In EMNLP, pages 1–8.
Yufan Guo, Anna Korhonen, and Thierry Poibeau.
2011. A weakly-supervised approach to argumen-
tative zoning of scientific documents. In EMNLP,
pages 273–283.
Slava Katz. 1987. Estimation of probabilities from
sparse data for the language model component of a
speech recognizer. IEEE Transactions on Acoustics,
Speech and Signal Processing, 35(3):400–401.
Anna Korhonen. 2002. Subcategorization acquisi-
tion. Technical report, University of Cambridge,
Computer Laboratory.
Joel Lang and Mirella Lapata. 2010. Unsupervised
induction of semantic roles. In HLT, pages 939–947.
Lillian Lee. 1999. Measures of distributional similar-
ity. In ACL, pages 25–32.
Beth Levin and Malka Rappaport Hovav. 2005. Argu-
ment realization. Cambridge University Press, Cam-
bridge.
Beth Levin. 1993. English verb classes and alter-
nations: A preliminary investigation. University of
Chicago Press, Chicago.
Christopher D. Manning. 1993. Automatic acquisition
of a large subcategorization dictionary from corpora.
In ACL, pages 235–242.
Manolis Maragoudakis, Katia Lida Kermanidis, and
George Kokkinakis. 2000. Learning subcategoriza-
tion frames from corpora: A case study for modern
Greek. In Proceedings of COMLEX 2000, Work-
shop on Computational Lexicography and Multime-
dia Dictionaries, pages 19–22.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In ACL, pages 433–440.
Anoop Sarkar and Daniel Zeman. 2000. Automatic
extraction of subcategorization frames for Czech. In
COLING, pages 691–697.
Silke Scheible, Sabine Schulte im Walde, Marion
Weller, and Max Kisselew. 2013. A compact but lin-
guistically detailed database for German verb subcat-
egorisation relying on dependency parses from Web
corpora: Tool, guidelines and resource. In Web as
Corpus Workshop.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In NeMLaP, vol-
ume 12, pages 44–49.
Sabine Schulte im Walde and Chris Brew. 2002. Induc-
ing German semantic verb classes from purely syn-
tactic subcategorisation information. In ACL, pages
223–230.
Sabine Schulte im Walde. 2002a. A subcategorisation
lexicon for German verbs induced from a lexicalised
PCFG. In LREC, pages 1351–1357.
Sabine Schulte im Walde. 2002b. Evaluating verb sub-
categorisation frames learned by a German statisti-
cal grammar against manual definitions in the Duden
Dictionary. In EURALEX, pages 187–197.
Sabine Schulte im Walde. 2006. Experiments on
the automatic induction of German semantic verb
classes. Computational Linguistics, 32(2):159–194.
Sabine Schulte im Walde. 2009. The induction of
verb frames and verb classes from corpora. In Anke
L¨udeling and Merja Kyt¨o, editors, Corpus linguis-
tics: An international handbook, volume 2, chap-
ter 44, pages 952–971. Mouton de Gruyter, Berlin.
Ekaterina Shutova, Lin Sun, and Anna Korhonen.
2010. Metaphor identification using verb and noun
clustering. In COLING, pages 1002–1010.
</reference>
<page confidence="0.990155">
306
</page>
<reference confidence="0.999231631578948">
Wojciech Skut, Brigitte Krenn, Thorsten Brants, and
Hans Uszkoreit. 1997. An annotation scheme for
free word order languages. In ANLP, pages 88–95.
Lin Sun, Anna Korhonen, and Yuval Krymolowski.
2008. Verb class discovery from rich syntactic data.
In CICLing, pages 16–27, Haifa, Israel.
Lin Sun, Anna Korhonen, Thierry Poibeau, and C´edric
Messiant. 2010. Investigating the cross-linguistic
potential of VerbNet-style classification. In COL-
ING, pages 1056–1064, Beijing, China.
Ivan Titov and Alexandre Klementiev. 2012. A
Bayesian approach to unsupervised semantic role in-
duction. In EACL, pages 12–22.
Joe H. Ward, Jr. 1963. Hierarchical grouping to opti-
mize an objective function. Journal of the American
Statistical Association, 58(301):236–244.
Oliver Wauschkuhn. 1999. Automatische Extrak-
tion von Verbvalenzen aus deutschen Textkorpora.
Shaker Verlag.
</reference>
<page confidence="0.998579">
307
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.388972">
<title confidence="0.963794">Subcategorisation Acquisition from Raw Text for a Free Word-Order Language</title>
<author confidence="0.527741">Roberts Egg</author>
<affiliation confidence="0.537724">Institute f¨ur Anglistik und Amerikanistik, Humboldt</affiliation>
<address confidence="0.976701">10099 Berlin,</address>
<abstract confidence="0.999800666666666">We describe a state-of-the-art automatic system that can acquire subcategorisation frames from raw text for a free word-order language. We use it to construct a subcategorisation lexicon of German verbs from a large Web page corpus. With an automatic verb classification paradigm we evaluate our subcategorisation lexicon against a previous classification of German verbs; the lexicon produced by our system performs better than the best previous results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
<author>Adriano Ferraresi</author>
<author>Eros Zanchetta</author>
</authors>
<title>The WaCky wide web: A collection of very large linguistically processed webcrawled corpora. Language Resources and Evaluation,</title>
<date>2009</date>
<pages>43--3</pages>
<contexts>
<context position="15694" citStr="Baroni et al., 2009" startWordPosition="2526" endWordPosition="2529">most one accusative direct object for the verb, assuming that the joint probability of a set of labels is the product of the individual label probabilities. We use our edge labeller in this work for morphological disambiguation of nominals and for identifying clausal adjuncts, but the edge labeller is a standalone reusable component, which might be equally well be used to mark up parse trees for, e.g., a semantic role labelling system. 4 The subcategorisation lexicon With the system described in Sec. 3, we build a German subcategorisation lexicon that collects counts of (lemma, SCF) on deWaC (Baroni et al., 2009), a corpus of text extracted from Web search results, with 109 words automatically POS-tagged and lemmatised by the TreeTagger (Schmid, 1994). A subset of this corpus, SdeWaC (Faaß and Eckart, 2013), has been preprocessed to include only sentences which are maximally parsable; this smaller corpus includes 880 million words in 45 million sentences. We parsed 3 million sentences (80 million words) of SdeWaC; after filtering out those verb lemmas seen only five times or fewer in the corpus, we are left with statistics on 8 million verb instances, representing 9,825 verb lemmas. As a concrete exam</context>
</contexts>
<marker>Baroni, Bernardini, Ferraresi, Zanchetta, 2009</marker>
<rawString>Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The WaCky wide web: A collection of very large linguistically processed webcrawled corpora. Language Resources and Evaluation, 43(3):209–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Brants</author>
<author>Stefanie Dipper</author>
<author>Silvia Hansen</author>
<author>Wolfgang Lezius</author>
<author>George Smith</author>
</authors>
<title>The TIGER treebank.</title>
<date>2002</date>
<booktitle>In TLT,</booktitle>
<pages>24--41</pages>
<contexts>
<context position="9389" citStr="Brants et al., 2002" startWordPosition="1485" endWordPosition="1488">for expletive es (‘it’) subject. Clausal complements can be infinite (i); finite ones can have the verb in second position (S-2) or include the complementiser dass ‘that’ (S-dass). Complements can be combined as in na (transitive verb); for PPs in SCFs, the head is specified, e.g., p:f¨ur for PP complements headed by f¨ur ‘for’1. Due to the free word order, simple phrase structure like that used for analysis of English is not enough to specify the syntax of German sentences. Therefore we use the annotation scheme in the manually constructed German treebanks NEGRA and TIGER (Skut et al., 1997; Brants et al., 2002), which decorate parse trees with edge labels specifying the syntactic roles of constituents. We automatically annotate the parse trees from our statistical parser using a simple machine learning model. In the next section, we illustrate the operation of the SCF tagger with reference to examples; then in Section 3.2 we describe our edge labeller. 3.1 The SCF tagger The SCF tagger begins by collecting complements co-occurring with a verb instance using the phrase structure of the sentence. In our system, we obtain phrase structure information for unannotated text using the Berkeley Parser (Petr</context>
</contexts>
<marker>Brants, Dipper, Hansen, Lezius, Smith, 2002</marker>
<rawString>Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang Lezius, and George Smith. 2002. The TIGER treebank. In TLT, pages 24–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael R Brent</author>
<author>Robert C Berwick</author>
</authors>
<title>Automatic acquisition of subcategorization frames from tagged text.</title>
<date>1991</date>
<booktitle>In HLT,</booktitle>
<pages>342--345</pages>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="3816" citStr="Brent and Berwick (1991)" startWordPosition="589" endWordPosition="592">n Section 2, Section 3 describes our subcategorisation acquisition system, and Section 4 the SCF lexicon that we build using it. In Sections 5 and 6 we evaluate the SCF lexicon on a verb classification task and discuss our results; Section 7 then concludes with directions for future work. 298 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 298–307, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics 2 Previous work To date, research on SCF acquisition from corpora has mostly targeted English. Brent and Berwick (1991) detect five SCFs by looking for attested contexts where argument slots are filled by closedclass lexical items (pronouns or proper names). Briscoe and Carroll (1997) detect 163 SCFs with a system that builds an SCF lexicon whose entries include the relative frequency of SCF classes. Potential SCF patterns are extracted from a corpus parsed with a dependency-based parser, and then filtered by hypothesis testing on binomial frequency data. Korhonen (2002) refines Briscoe and Carroll (1997)’s system using back-off estimates on the WordNet semantic class of the verb’s predominant sense, assuming </context>
</contexts>
<marker>Brent, Berwick, 1991</marker>
<rawString>Michael R. Brent and Robert C. Berwick. 1991. Automatic acquisition of subcategorization frames from tagged text. In HLT, pages 342–345. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
</authors>
<title>Automatic extraction of subcategorization from corpora.</title>
<date>1997</date>
<location>CoRR, cmp-lg/9702002.</location>
<contexts>
<context position="3982" citStr="Briscoe and Carroll (1997)" startWordPosition="615" endWordPosition="618"> SCF lexicon on a verb classification task and discuss our results; Section 7 then concludes with directions for future work. 298 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 298–307, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics 2 Previous work To date, research on SCF acquisition from corpora has mostly targeted English. Brent and Berwick (1991) detect five SCFs by looking for attested contexts where argument slots are filled by closedclass lexical items (pronouns or proper names). Briscoe and Carroll (1997) detect 163 SCFs with a system that builds an SCF lexicon whose entries include the relative frequency of SCF classes. Potential SCF patterns are extracted from a corpus parsed with a dependency-based parser, and then filtered by hypothesis testing on binomial frequency data. Korhonen (2002) refines Briscoe and Carroll (1997)’s system using back-off estimates on the WordNet semantic class of the verb’s predominant sense, assuming that semantically similar verbs have similar SCFs, following Levin (1993). Some current statistical methods for Semantic Role Labelling build models that also capture</context>
</contexts>
<marker>Briscoe, Carroll, 1997</marker>
<rawString>Ted Briscoe and John Carroll. 1997. Automatic extraction of subcategorization from corpora. CoRR, cmp-lg/9702002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aljoscha Burchardt</author>
<author>Katrin Erk</author>
<author>Anette Frank</author>
<author>Andrea Kowalski</author>
<author>Sebastian Pado</author>
<author>Manfred Pinkal</author>
</authors>
<title>The SALSA corpus: A German corpus resource for lexical semantics.</title>
<date>2006</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="34026" citStr="Burchardt et al., 2006" startWordPosition="5545" endWordPosition="5548">ate a large subcategorisation frame lexicon for German verbs. Our SCF lexicon resource is available at http://amor.cms.hu-berlin. de/˜robertsw/scflex.html. We are performing a manual evaluation of the output of our system, which we will report soon. We plan to continue this work first by expanding our SCF lexicon with case information and selectional preferences, second by using our SCF classifier and lexicon for verbal Multiword Expression identification in German, and last by comparing it to existing verb classifications, either by using available resources for German like the SALSA corpus (Burchardt et al., 2006), or by translating parts of VerbNet into German to create a more extensive gold standard for verb clustering in the spirit of Sun et al. (2010) who found that Levin’s verb classification can be translated to French and still usefully allow generalisation over verb classes. Finally, we plan to perform in vivo evaluation of our SCF lexicon, to determine what benefit it can deliver for NLP applications such as Semantic Role Labelling and Word Sense Disambiguation. Recent research has found that even automatically-acquired verb classifications can be useful for NLP applications (Shutova et al., 2</context>
</contexts>
<marker>Burchardt, Erk, Frank, Kowalski, Pado, Pinkal, 2006</marker>
<rawString>Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea Kowalski, Sebastian Pado, and Manfred Pinkal. 2006. The SALSA corpus: A German corpus resource for lexical semantics. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judith Eckle-Kohler</author>
</authors>
<title>Linguistic knowledge for automatic lexicon acquisition from German text corpora.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>Universit¨at Stuttgart.</institution>
<contexts>
<context position="6508" citStr="Eckle-Kohler, 1999" startWordPosition="1020" endWordPosition="1021">an paradigms for nouns, adjectives, and determiners. E.g., the noun Auto ‘car’ has only two forms, Auto for nominative, dative, and accusative singular, and Autos for genitive singular and all four plural forms. This is in contrast to some other free word order languages for which SCF acquisition has been studied, like Modern Greek (Maragoudakis et al., 2000) and Czech (Sarkar and Zeman, 2000). A one-many relation between word forms and case is also one of the problems for SCF acquisition in Urdu (Ghulam, 2011). For German, initial studies used semi-automatic techniques and manual evaluation (Eckle-Kohler, 1999; Wauschkuhn, 1999). The first automatic subcategorisation acquisition system for German is described by Schulte im Walde (2002a), who defined an SCF inventory and manually wrote a grammar to analyse verb constructions according to these frames. A lexicalised PCFG parser using this grammar was trained on 18.7 million words of German newspaper text; the trained parser model contained explicit subcategorisation frequencies, which could then be extracted to construct a subcategorisation lexicon for 14,229 German verbs. This work was evaluated against a German dictionary, the Duden Stilw¨orterbuch</context>
</contexts>
<marker>Eckle-Kohler, 1999</marker>
<rawString>Judith Eckle-Kohler. 1999. Linguistic knowledge for automatic lexicon acquisition from German text corpora. Ph.D. thesis, Universit¨at Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gertrud Faaß</author>
<author>Kerstin Eckart</author>
</authors>
<title>SdeWaC - A corpus of parsable sentences from the Web. In Language processing and knowledge in the Web,</title>
<date>2013</date>
<pages>61--68</pages>
<publisher>Springer,</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="15892" citStr="Faaß and Eckart, 2013" startWordPosition="2558" endWordPosition="2561">or morphological disambiguation of nominals and for identifying clausal adjuncts, but the edge labeller is a standalone reusable component, which might be equally well be used to mark up parse trees for, e.g., a semantic role labelling system. 4 The subcategorisation lexicon With the system described in Sec. 3, we build a German subcategorisation lexicon that collects counts of (lemma, SCF) on deWaC (Baroni et al., 2009), a corpus of text extracted from Web search results, with 109 words automatically POS-tagged and lemmatised by the TreeTagger (Schmid, 1994). A subset of this corpus, SdeWaC (Faaß and Eckart, 2013), has been preprocessed to include only sentences which are maximally parsable; this smaller corpus includes 880 million words in 45 million sentences. We parsed 3 million sentences (80 million words) of SdeWaC; after filtering out those verb lemmas seen only five times or fewer in the corpus, we are left with statistics on 8 million verb instances, representing 9,825 verb lemmas. As a concrete example for the resulting SCF lexicon, consider the entry for sprechen ‘talk’ in Fig. 2, which occurs 16,254 times in our SCF lexicon. Sprechen refers to a conversation with speaker, hearer, topic, mess</context>
</contexts>
<marker>Faaß, Eckart, 2013</marker>
<rawString>Gertrud Faaß and Kerstin Eckart. 2013. SdeWaC - A corpus of parsable sentences from the Web. In Language processing and knowledge in the Web, pages 61–68. Springer, Berlin, Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward W Forgy</author>
</authors>
<title>Cluster analysis of multivariate data: Efficiency versus interpretability of classifications.</title>
<date>1965</date>
<journal>Biometrics,</journal>
<pages>21--768</pages>
<contexts>
<context position="21690" citStr="Forgy, 1965" startWordPosition="3489" endWordPosition="3490"> integrated into a large-scale classification of German verbs in the style of Levin (1993). 5.2 Clustering From the counts of (lemma, SCF) in the SCF lexicon, we can estimate the conditional probability that a particular verb v appears with an SCF f: 3For the purpose of our evaluation, we disregard classsubclass relations and consider subclasses as separate entities. 302 P(scf = f|lemma = v). We smooth these conditional probability distributions by backing off to the prior probability P(scf) (Katz, 1987). With these smoothed conditional probabilities, we cluster verbs with k-means clustering (Forgy, 1965), a hard clustering technique, which partitions a set of objects into k clusters. The algorithm is initialised with a starting set of k cluster centroids; it then proceeds iteratively, first assigning each object to the cluster whose centroid is closest under some distance measure, and then calculating new centroids to represent the centres of the updated clusters. The algorithm terminates when the assignment of objects to clusters no longer changes. � D(pllq) = i irad(p, q) = D(pllp +q 2 ) + D(qllp +q 2 ) (10) skew(p, q) = D(pllαq + (1 − α)p) (11) In our experiments, verbs are represented by </context>
</contexts>
<marker>Forgy, 1965</marker>
<rawString>Edward W. Forgy. 1965. Cluster analysis of multivariate data: Efficiency versus interpretability of classifications. Biometrics, 21:768–769.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raza Ghulam</author>
</authors>
<title>Subcategorization acquisition and classes of predication in Urdu.</title>
<date>2011</date>
<tech>Ph.D. thesis,</tech>
<institution>Universit¨at Konstanz.</institution>
<contexts>
<context position="6406" citStr="Ghulam, 2011" startWordPosition="1008" endWordPosition="1009">reement marking. This is due to a very high degree of syncretism (identity of word forms) in German paradigms for nouns, adjectives, and determiners. E.g., the noun Auto ‘car’ has only two forms, Auto for nominative, dative, and accusative singular, and Autos for genitive singular and all four plural forms. This is in contrast to some other free word order languages for which SCF acquisition has been studied, like Modern Greek (Maragoudakis et al., 2000) and Czech (Sarkar and Zeman, 2000). A one-many relation between word forms and case is also one of the problems for SCF acquisition in Urdu (Ghulam, 2011). For German, initial studies used semi-automatic techniques and manual evaluation (Eckle-Kohler, 1999; Wauschkuhn, 1999). The first automatic subcategorisation acquisition system for German is described by Schulte im Walde (2002a), who defined an SCF inventory and manually wrote a grammar to analyse verb constructions according to these frames. A lexicalised PCFG parser using this grammar was trained on 18.7 million words of German newspaper text; the trained parser model contained explicit subcategorisation frequencies, which could then be extracted to construct a subcategorisation lexicon f</context>
</contexts>
<marker>Ghulam, 2011</marker>
<rawString>Raza Ghulam. 2011. Subcategorization acquisition and classes of predication in Urdu. Ph.D. thesis, Universit¨at Konstanz.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trond Grenager</author>
<author>Christopher D Manning</author>
</authors>
<title>Unsupervised discovery of a statistical verb lexicon.</title>
<date>2006</date>
<booktitle>In EMNLP,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="4647" citStr="Grenager and Manning (2006)" startWordPosition="715" endWordPosition="719">ilds an SCF lexicon whose entries include the relative frequency of SCF classes. Potential SCF patterns are extracted from a corpus parsed with a dependency-based parser, and then filtered by hypothesis testing on binomial frequency data. Korhonen (2002) refines Briscoe and Carroll (1997)’s system using back-off estimates on the WordNet semantic class of the verb’s predominant sense, assuming that semantically similar verbs have similar SCFs, following Levin (1993). Some current statistical methods for Semantic Role Labelling build models that also capture subcategorisation information, e.g., Grenager and Manning (2006). Schulte im Walde (2009) offers a recent survey of the SCF acquisition literature. SCF acquisition is also an important step in the automatic semantic role labelling (Grenager and Manning, 2006; Lang and Lapata, 2010; Titov and Klementiev, 2012). Semantic roles of a verb describe the kind of involvement of entities in the event introduced by the verb, e.g., as agent (active, often not affected by the event) or patient (passive, often affected). On the basis of these SCFs, semantic roles can be assigned due to the interdependence between semantic roles and their syntactic realisations, called </context>
</contexts>
<marker>Grenager, Manning, 2006</marker>
<rawString>Trond Grenager and Christopher D. Manning. 2006. Unsupervised discovery of a statistical verb lexicon. In EMNLP, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yufan Guo</author>
<author>Anna Korhonen</author>
<author>Thierry Poibeau</author>
</authors>
<title>A weakly-supervised approach to argumentative zoning of scientific documents.</title>
<date>2011</date>
<booktitle>In EMNLP,</booktitle>
<pages>273--283</pages>
<marker>Guo, Korhonen, Poibeau, 2011</marker>
<rawString>Yufan Guo, Anna Korhonen, and Thierry Poibeau. 2011. A weakly-supervised approach to argumentative zoning of scientific documents. In EMNLP, pages 273–283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slava Katz</author>
</authors>
<title>Estimation of probabilities from sparse data for the language model component of a speech recognizer.</title>
<date>1987</date>
<journal>IEEE Transactions on Acoustics, Speech and Signal Processing,</journal>
<volume>35</volume>
<issue>3</issue>
<contexts>
<context position="21587" citStr="Katz, 1987" startWordPosition="3476" endWordPosition="3477">Possession (Obtaining) combines Levin’s Get and Obtain classes. This shows that these classes could be integrated into a large-scale classification of German verbs in the style of Levin (1993). 5.2 Clustering From the counts of (lemma, SCF) in the SCF lexicon, we can estimate the conditional probability that a particular verb v appears with an SCF f: 3For the purpose of our evaluation, we disregard classsubclass relations and consider subclasses as separate entities. 302 P(scf = f|lemma = v). We smooth these conditional probability distributions by backing off to the prior probability P(scf) (Katz, 1987). With these smoothed conditional probabilities, we cluster verbs with k-means clustering (Forgy, 1965), a hard clustering technique, which partitions a set of objects into k clusters. The algorithm is initialised with a starting set of k cluster centroids; it then proceeds iteratively, first assigning each object to the cluster whose centroid is closest under some distance measure, and then calculating new centroids to represent the centres of the updated clusters. The algorithm terminates when the assignment of objects to clusters no longer changes. � D(pllq) = i irad(p, q) = D(pllp +q 2 ) +</context>
</contexts>
<marker>Katz, 1987</marker>
<rawString>Slava Katz. 1987. Estimation of probabilities from sparse data for the language model component of a speech recognizer. IEEE Transactions on Acoustics, Speech and Signal Processing, 35(3):400–401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Korhonen</author>
</authors>
<title>Subcategorization acquisition.</title>
<date>2002</date>
<tech>Technical report,</tech>
<institution>University of Cambridge, Computer Laboratory.</institution>
<contexts>
<context position="4274" citStr="Korhonen (2002)" startWordPosition="665" endWordPosition="666">ciation for Computational Linguistics 2 Previous work To date, research on SCF acquisition from corpora has mostly targeted English. Brent and Berwick (1991) detect five SCFs by looking for attested contexts where argument slots are filled by closedclass lexical items (pronouns or proper names). Briscoe and Carroll (1997) detect 163 SCFs with a system that builds an SCF lexicon whose entries include the relative frequency of SCF classes. Potential SCF patterns are extracted from a corpus parsed with a dependency-based parser, and then filtered by hypothesis testing on binomial frequency data. Korhonen (2002) refines Briscoe and Carroll (1997)’s system using back-off estimates on the WordNet semantic class of the verb’s predominant sense, assuming that semantically similar verbs have similar SCFs, following Levin (1993). Some current statistical methods for Semantic Role Labelling build models that also capture subcategorisation information, e.g., Grenager and Manning (2006). Schulte im Walde (2009) offers a recent survey of the SCF acquisition literature. SCF acquisition is also an important step in the automatic semantic role labelling (Grenager and Manning, 2006; Lang and Lapata, 2010; Titov an</context>
</contexts>
<marker>Korhonen, 2002</marker>
<rawString>Anna Korhonen. 2002. Subcategorization acquisition. Technical report, University of Cambridge, Computer Laboratory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Lang</author>
<author>Mirella Lapata</author>
</authors>
<title>Unsupervised induction of semantic roles.</title>
<date>2010</date>
<booktitle>In HLT,</booktitle>
<pages>939--947</pages>
<contexts>
<context position="4864" citStr="Lang and Lapata, 2010" startWordPosition="751" endWordPosition="754">frequency data. Korhonen (2002) refines Briscoe and Carroll (1997)’s system using back-off estimates on the WordNet semantic class of the verb’s predominant sense, assuming that semantically similar verbs have similar SCFs, following Levin (1993). Some current statistical methods for Semantic Role Labelling build models that also capture subcategorisation information, e.g., Grenager and Manning (2006). Schulte im Walde (2009) offers a recent survey of the SCF acquisition literature. SCF acquisition is also an important step in the automatic semantic role labelling (Grenager and Manning, 2006; Lang and Lapata, 2010; Titov and Klementiev, 2012). Semantic roles of a verb describe the kind of involvement of entities in the event introduced by the verb, e.g., as agent (active, often not affected by the event) or patient (passive, often affected). On the basis of these SCFs, semantic roles can be assigned due to the interdependence between semantic roles and their syntactic realisations, called Argument Linking (Levin, 1993; Levin and Rappaport Hovav, 2005). Acquiring SCFs for languages with a very fixed word order like English needs only a simple syntactic analysis, which mainly relies on the predetermined </context>
</contexts>
<marker>Lang, Lapata, 2010</marker>
<rawString>Joel Lang and Mirella Lapata. 2010. Unsupervised induction of semantic roles. In HLT, pages 939–947.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lillian Lee</author>
</authors>
<title>Measures of distributional similarity.</title>
<date>1999</date>
<booktitle>In ACL,</booktitle>
<pages>25--32</pages>
<contexts>
<context position="23679" citStr="Lee (1999)" startWordPosition="3820" endWordPosition="3821">nitialise the centroids by random partitions (each of the n objects is randomly assigned to one of k clusters, and the centroids are then computed as the means of these random partitions). Because the random initial centroids influence the final clustering, we repeat the clustering a number of times. We also initialise the k-means cluster centroids using agglomerative hierarchical clustering, a deterministic iterative bottom-up process. Hierarchical clustering initially assigns verbs to singleton clusters; the two clusters which are “nearest” to 4Schulte im Walde (2006) takes α = 0.9 although Lee (1999) recommends α = 0.99 or higher values in her original description of skew divergence. each other are then joined together, and this process is repeated until the desired number of clusters is obtained. Hierarchical clustering is performed to group the verbs into k clusters; the centroids of these clusters are then used to initialise the kmeans algorithm. While there exist several variants of hierarchical clustering, we use Ward’s method (Ward, Jr, 1963) for merging clusters, which attempts to minimise the variance inside clusters; Ward’s criterion was previously found to be the most effective </context>
</contexts>
<marker>Lee, 1999</marker>
<rawString>Lillian Lee. 1999. Measures of distributional similarity. In ACL, pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Levin</author>
<author>Malka Rappaport Hovav</author>
</authors>
<title>Argument realization.</title>
<date>2005</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<marker>Levin, Hovav, 2005</marker>
<rawString>Beth Levin and Malka Rappaport Hovav. 2005. Argument realization. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Levin</author>
</authors>
<title>English verb classes and alternations: A preliminary investigation.</title>
<date>1993</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago.</location>
<contexts>
<context position="4489" citStr="Levin (1993)" startWordPosition="696" endWordPosition="697">ment slots are filled by closedclass lexical items (pronouns or proper names). Briscoe and Carroll (1997) detect 163 SCFs with a system that builds an SCF lexicon whose entries include the relative frequency of SCF classes. Potential SCF patterns are extracted from a corpus parsed with a dependency-based parser, and then filtered by hypothesis testing on binomial frequency data. Korhonen (2002) refines Briscoe and Carroll (1997)’s system using back-off estimates on the WordNet semantic class of the verb’s predominant sense, assuming that semantically similar verbs have similar SCFs, following Levin (1993). Some current statistical methods for Semantic Role Labelling build models that also capture subcategorisation information, e.g., Grenager and Manning (2006). Schulte im Walde (2009) offers a recent survey of the SCF acquisition literature. SCF acquisition is also an important step in the automatic semantic role labelling (Grenager and Manning, 2006; Lang and Lapata, 2010; Titov and Klementiev, 2012). Semantic roles of a verb describe the kind of involvement of entities in the event introduced by the verb, e.g., as agent (active, often not affected by the event) or patient (passive, often aff</context>
<context position="19024" citStr="Levin, 1993" startWordPosition="3075" endWordPosition="3076">k-based evaluation to compare our system directly with Schulte im Walde’s, and leave manual evaluation for future work. We refer back to the experiment set up by Schulte im Walde (2006) to automatically induce classifications of German verbs by clustering them on the basis of their SCF preferences as listed in her SCF lexicon. By casting this experiment as a fixed task, we can compare our system directly to hers. The link between subcategorisation and verb semantics is linguistically sound, due to the interdependence between verb meanings and the number and kinds of their syntactic arguments (Levin, 1993; Levin and Rappaport Hovav, 2005). E.g., only transitive verbs that denote a change of state like cut and break enter in the middle construction (The bread cuts easily.), with the patient or theme argument appearing as the syntactic subject. Thus, verbs whose SCF preferences show such an alternation can be predicted to denote a change of state. We adopt the automatic verb classification paradigm to evaluate our system, replicating Schulte im Walde’s (2006) experiment to the best of our ability. We argue that by evaluating our SdeWaC SCF lexicon described in the previous section, we simultaneo</context>
<context position="21168" citStr="Levin (1993)" startWordPosition="3407" endWordPosition="3408">n ‘climb’), Rotation (rotieren ‘rotate’), Rush (eilen ‘hurry’), Vehicle (fliegen ‘fly’), and Flotation (gleiten ‘glide’). These classes are related to Levin classes in that some are roughly equivalent to a Levin class (e.g., Aspect and Levin’s Begin class), others are subgroups of Levin classes, e.g., Position is a subgroup of Levin’s Dangle class; finally, some classes lump together Levin classes, e.g., Transfer of Possession (Obtaining) combines Levin’s Get and Obtain classes. This shows that these classes could be integrated into a large-scale classification of German verbs in the style of Levin (1993). 5.2 Clustering From the counts of (lemma, SCF) in the SCF lexicon, we can estimate the conditional probability that a particular verb v appears with an SCF f: 3For the purpose of our evaluation, we disregard classsubclass relations and consider subclasses as separate entities. 302 P(scf = f|lemma = v). We smooth these conditional probability distributions by backing off to the prior probability P(scf) (Katz, 1987). With these smoothed conditional probabilities, we cluster verbs with k-means clustering (Forgy, 1965), a hard clustering technique, which partitions a set of objects into k cluste</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>Beth Levin. 1993. English verb classes and alternations: A preliminary investigation. University of Chicago Press, Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
</authors>
<title>Automatic acquisition of a large subcategorization dictionary from corpora.</title>
<date>1993</date>
<booktitle>In ACL,</booktitle>
<pages>235--242</pages>
<marker>Manning, 1993</marker>
<rawString>Christopher D. Manning. 1993. Automatic acquisition of a large subcategorization dictionary from corpora. In ACL, pages 235–242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manolis Maragoudakis</author>
<author>Katia Lida Kermanidis</author>
<author>George Kokkinakis</author>
</authors>
<title>Learning subcategorization frames from corpora: A case study for modern Greek.</title>
<date>2000</date>
<booktitle>In Proceedings of COMLEX 2000, Workshop on Computational Lexicography and Multimedia Dictionaries,</booktitle>
<pages>pages</pages>
<contexts>
<context position="6251" citStr="Maragoudakis et al., 2000" startWordPosition="979" endWordPosition="982"> syntactic parse. What is more, German is a counterexample to Manning’s (1993) expectation that freedom of word order should be matched by an increase in case and/or agreement marking. This is due to a very high degree of syncretism (identity of word forms) in German paradigms for nouns, adjectives, and determiners. E.g., the noun Auto ‘car’ has only two forms, Auto for nominative, dative, and accusative singular, and Autos for genitive singular and all four plural forms. This is in contrast to some other free word order languages for which SCF acquisition has been studied, like Modern Greek (Maragoudakis et al., 2000) and Czech (Sarkar and Zeman, 2000). A one-many relation between word forms and case is also one of the problems for SCF acquisition in Urdu (Ghulam, 2011). For German, initial studies used semi-automatic techniques and manual evaluation (Eckle-Kohler, 1999; Wauschkuhn, 1999). The first automatic subcategorisation acquisition system for German is described by Schulte im Walde (2002a), who defined an SCF inventory and manually wrote a grammar to analyse verb constructions according to these frames. A lexicalised PCFG parser using this grammar was trained on 18.7 million words of German newspape</context>
</contexts>
<marker>Maragoudakis, Kermanidis, Kokkinakis, 2000</marker>
<rawString>Manolis Maragoudakis, Katia Lida Kermanidis, and George Kokkinakis. 2000. Learning subcategorization frames from corpora: A case study for modern Greek. In Proceedings of COMLEX 2000, Workshop on Computational Lexicography and Multimedia Dictionaries, pages 19–22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In ACL,</booktitle>
<pages>433--440</pages>
<contexts>
<context position="10005" citStr="Petrov et al., 2006" startWordPosition="1583" endWordPosition="1586">002), which decorate parse trees with edge labels specifying the syntactic roles of constituents. We automatically annotate the parse trees from our statistical parser using a simple machine learning model. In the next section, we illustrate the operation of the SCF tagger with reference to examples; then in Section 3.2 we describe our edge labeller. 3.1 The SCF tagger The SCF tagger begins by collecting complements co-occurring with a verb instance using the phrase structure of the sentence. In our system, we obtain phrase structure information for unannotated text using the Berkeley Parser (Petrov et al., 2006), a statistical unlexicalised parser trained on TIGER. Fig. 1 illustrates the phrase structure analysis and edge labels in the TIGER corpus for (6): (6) Das hielte ich f¨ur moralisch außerordentlich fragw¨urdig. ‘I’d consider that morally extremely questionable’. Its finite verb hielte (from halten ‘hold’) has three complements, the subject ich ‘I’, edgelabelled with SB, the direct object das ‘that’, labelled with OA, and a PP headed by f¨ur ‘for’ (MO stands for ‘modifier’). After collecting complements, the SCF tagger uses this edge label information to determine the complements’ syntactic ro</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In ACL, pages 433–440.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anoop Sarkar</author>
<author>Daniel Zeman</author>
</authors>
<title>Automatic extraction of subcategorization frames for Czech. In</title>
<date>2000</date>
<booktitle>COLING,</booktitle>
<pages>691--697</pages>
<contexts>
<context position="6286" citStr="Sarkar and Zeman, 2000" startWordPosition="985" endWordPosition="988"> is a counterexample to Manning’s (1993) expectation that freedom of word order should be matched by an increase in case and/or agreement marking. This is due to a very high degree of syncretism (identity of word forms) in German paradigms for nouns, adjectives, and determiners. E.g., the noun Auto ‘car’ has only two forms, Auto for nominative, dative, and accusative singular, and Autos for genitive singular and all four plural forms. This is in contrast to some other free word order languages for which SCF acquisition has been studied, like Modern Greek (Maragoudakis et al., 2000) and Czech (Sarkar and Zeman, 2000). A one-many relation between word forms and case is also one of the problems for SCF acquisition in Urdu (Ghulam, 2011). For German, initial studies used semi-automatic techniques and manual evaluation (Eckle-Kohler, 1999; Wauschkuhn, 1999). The first automatic subcategorisation acquisition system for German is described by Schulte im Walde (2002a), who defined an SCF inventory and manually wrote a grammar to analyse verb constructions according to these frames. A lexicalised PCFG parser using this grammar was trained on 18.7 million words of German newspaper text; the trained parser model co</context>
</contexts>
<marker>Sarkar, Zeman, 2000</marker>
<rawString>Anoop Sarkar and Daniel Zeman. 2000. Automatic extraction of subcategorization frames for Czech. In COLING, pages 691–697.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silke Scheible</author>
<author>Sabine Schulte im Walde</author>
<author>Marion Weller</author>
<author>Max Kisselew</author>
</authors>
<title>A compact but linguistically detailed database for German verb subcategorisation relying on dependency parses from Web corpora: Tool, guidelines and resource.</title>
<date>2013</date>
<booktitle>In Web as Corpus Workshop.</booktitle>
<contexts>
<context position="7870" citStr="Scheible et al., 2013" startWordPosition="1227" endWordPosition="1230">e a set of semantic verb classes with an unsupervised clustering algorithm. This clustering was evaluated against a small manually created semantic verb classification. Schulte im Walde (2006) continues this work using a larger manual verb classification. The SCFs used in this study are defined at three levels of granularity. The first level (38 different SCFs) lists only the complements in the frame; the second one adds head and case information for PP complements (183 SCFs). The third level examined the effect of adding selectional preferences, but results were inconclusive. A recent paper (Scheible et al., 2013) describes a system similar to ours, built on a statistical dependency parser, and using some of the same kinds of rules as we describe in Section 3.1; this system is evaluated in a task-based way (e.g., to improve the performance of a SMT system) and cannot be directly compared to our system in this paper. 3 The SCF acquisition system This section describes the first contribution of this paper, a state-of-the-art subcategorisation acquisition system for German. Its core component is a rule-based SCF tagger which operates on phrase structure analyses, as delivered by a statistical parser. Give</context>
</contexts>
<marker>Scheible, Walde, Weller, Kisselew, 2013</marker>
<rawString>Silke Scheible, Sabine Schulte im Walde, Marion Weller, and Max Kisselew. 2013. A compact but linguistically detailed database for German verb subcategorisation relying on dependency parses from Web corpora: Tool, guidelines and resource. In Web as Corpus Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In NeMLaP,</booktitle>
<volume>12</volume>
<pages>44--49</pages>
<contexts>
<context position="15835" citStr="Schmid, 1994" startWordPosition="2550" endWordPosition="2551">ilities. We use our edge labeller in this work for morphological disambiguation of nominals and for identifying clausal adjuncts, but the edge labeller is a standalone reusable component, which might be equally well be used to mark up parse trees for, e.g., a semantic role labelling system. 4 The subcategorisation lexicon With the system described in Sec. 3, we build a German subcategorisation lexicon that collects counts of (lemma, SCF) on deWaC (Baroni et al., 2009), a corpus of text extracted from Web search results, with 109 words automatically POS-tagged and lemmatised by the TreeTagger (Schmid, 1994). A subset of this corpus, SdeWaC (Faaß and Eckart, 2013), has been preprocessed to include only sentences which are maximally parsable; this smaller corpus includes 880 million words in 45 million sentences. We parsed 3 million sentences (80 million words) of SdeWaC; after filtering out those verb lemmas seen only five times or fewer in the corpus, we are left with statistics on 8 million verb instances, representing 9,825 verb lemmas. As a concrete example for the resulting SCF lexicon, consider the entry for sprechen ‘talk’ in Fig. 2, which occurs 16,254 times in our SCF lexicon. Sprechen r</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In NeMLaP, volume 12, pages 44–49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Schulte im Walde</author>
<author>Chris Brew</author>
</authors>
<title>Inducing German semantic verb classes from purely syntactic subcategorisation information.</title>
<date>2002</date>
<booktitle>In ACL,</booktitle>
<pages>223--230</pages>
<contexts>
<context position="7168" citStr="Walde and Brew (2002)" startWordPosition="1116" endWordPosition="1119">c subcategorisation acquisition system for German is described by Schulte im Walde (2002a), who defined an SCF inventory and manually wrote a grammar to analyse verb constructions according to these frames. A lexicalised PCFG parser using this grammar was trained on 18.7 million words of German newspaper text; the trained parser model contained explicit subcategorisation frequencies, which could then be extracted to construct a subcategorisation lexicon for 14,229 German verbs. This work was evaluated against a German dictionary, the Duden Stilw¨orterbuch (Schulte im Walde, 2002b). Schulte im Walde and Brew (2002) used the subcategorisation lexicon created by the system to automatically induce a set of semantic verb classes with an unsupervised clustering algorithm. This clustering was evaluated against a small manually created semantic verb classification. Schulte im Walde (2006) continues this work using a larger manual verb classification. The SCFs used in this study are defined at three levels of granularity. The first level (38 different SCFs) lists only the complements in the frame; the second one adds head and case information for PP complements (183 SCFs). The third level examined the effect of</context>
</contexts>
<marker>Walde, Brew, 2002</marker>
<rawString>Sabine Schulte im Walde and Chris Brew. 2002. Inducing German semantic verb classes from purely syntactic subcategorisation information. In ACL, pages 223–230.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Schulte im Walde</author>
</authors>
<title>A subcategorisation lexicon for German verbs induced from a lexicalised PCFG. In</title>
<date>2002</date>
<booktitle>LREC,</booktitle>
<pages>1351--1357</pages>
<contexts>
<context position="6635" citStr="Walde (2002" startWordPosition="1039" endWordPosition="1040">cusative singular, and Autos for genitive singular and all four plural forms. This is in contrast to some other free word order languages for which SCF acquisition has been studied, like Modern Greek (Maragoudakis et al., 2000) and Czech (Sarkar and Zeman, 2000). A one-many relation between word forms and case is also one of the problems for SCF acquisition in Urdu (Ghulam, 2011). For German, initial studies used semi-automatic techniques and manual evaluation (Eckle-Kohler, 1999; Wauschkuhn, 1999). The first automatic subcategorisation acquisition system for German is described by Schulte im Walde (2002a), who defined an SCF inventory and manually wrote a grammar to analyse verb constructions according to these frames. A lexicalised PCFG parser using this grammar was trained on 18.7 million words of German newspaper text; the trained parser model contained explicit subcategorisation frequencies, which could then be extracted to construct a subcategorisation lexicon for 14,229 German verbs. This work was evaluated against a German dictionary, the Duden Stilw¨orterbuch (Schulte im Walde, 2002b). Schulte im Walde and Brew (2002) used the subcategorisation lexicon created by the system to automa</context>
<context position="8614" citStr="Walde (2002" startWordPosition="1357" endWordPosition="1358">be in Section 3.1; this system is evaluated in a task-based way (e.g., to improve the performance of a SMT system) and cannot be directly compared to our system in this paper. 3 The SCF acquisition system This section describes the first contribution of this paper, a state-of-the-art subcategorisation acquisition system for German. Its core component is a rule-based SCF tagger which operates on phrase structure analyses, as delivered by a statistical parser. Given a parse of a sentence, the tagger assigns each finite verb in the sentence an SCF type. 299 We use the SCF inventory of Schulte im Walde (2002a), which includes complements like n for nominative subject, a for accusative direct object, d for dative indirect object, r for reflexive pronoun, and x for expletive es (‘it’) subject. Clausal complements can be infinite (i); finite ones can have the verb in second position (S-2) or include the complementiser dass ‘that’ (S-dass). Complements can be combined as in na (transitive verb); for PPs in SCFs, the head is specified, e.g., p:f¨ur for PP complements headed by f¨ur ‘for’1. Due to the free word order, simple phrase structure like that used for analysis of English is not enough to speci</context>
<context position="18218" citStr="Walde (2002" startWordPosition="2943" endWordPosition="2944"> words. This SCF lexicon contains statistics on 133,897 verb instances (5,316 verb lemmas). While the manual annotations in NEGRA and TIGER mean that this SCF lexicon has virtually no noise, the small size of the corpora results in problems with data sparsity and negatively impacts the utility of this resource (see discussion in Section 6.2). 5 Automatic verb classification The remainder of the paper sets out to establish the relevance of our SCF acquisition system by comparison to previous work. As stated in Sec. 2, the only prior automatic German SCF acquisition system is that of Schulte im Walde (2002a), which was evaluated directly against an electronic version of a large dictionary; as this is not an open access resource, we cannot perform a similar evaluation. We opt therefore to use a task-based evaluation to compare our system directly with Schulte im Walde’s, and leave manual evaluation for future work. We refer back to the experiment set up by Schulte im Walde (2006) to automatically induce classifications of German verbs by clustering them on the basis of their SCF preferences as listed in her SCF lexicon. By casting this experiment as a fixed task, we can compare our system direct</context>
<context position="29849" citStr="Walde (2002" startWordPosition="4875" endWordPosition="4876">ns; these results are significant at the p &lt; 0.001 level8. 7PP case information is relevant for prepositions that can take both locative and directional readings, as in in der Stadt (dative) ‘in town’ und in die Stadt (accusative) ‘to town’. 8Statistical significance is calculated by running repeated k-means clusterings with random partition initialisation and evaluating the results using the relevant purity metrics. These repeated clustering scores represent a random variable (a func304 6.2 Discussion Sec. 6.1 compared the SCF lexicon created using SdeWaC with the lexicon built by Schulte im Walde (2002a), showing that our lexicon achieves significantly better results on the verb clustering task. We interpret this to be indicative of a more accurate subcategorisation lexicon, and, by extension, of a more accurate SCF acquisition system. We attribute this superior performance primarily to our use of a statistical parser as opposed to a hand-written grammar. This design choice has several advantages. First, the parser delivers robust syntactic analyses, which we can expect to be relatively domain-independent. Second, we make no prior assumptions about the variety of subcategorisation phenomena</context>
</contexts>
<marker>Walde, 2002</marker>
<rawString>Sabine Schulte im Walde. 2002a. A subcategorisation lexicon for German verbs induced from a lexicalised PCFG. In LREC, pages 1351–1357.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Schulte im Walde</author>
</authors>
<title>Evaluating verb subcategorisation frames learned by a German statistical grammar against manual definitions in the Duden Dictionary. In</title>
<date>2002</date>
<booktitle>EURALEX,</booktitle>
<pages>187--197</pages>
<contexts>
<context position="6635" citStr="Walde (2002" startWordPosition="1039" endWordPosition="1040">cusative singular, and Autos for genitive singular and all four plural forms. This is in contrast to some other free word order languages for which SCF acquisition has been studied, like Modern Greek (Maragoudakis et al., 2000) and Czech (Sarkar and Zeman, 2000). A one-many relation between word forms and case is also one of the problems for SCF acquisition in Urdu (Ghulam, 2011). For German, initial studies used semi-automatic techniques and manual evaluation (Eckle-Kohler, 1999; Wauschkuhn, 1999). The first automatic subcategorisation acquisition system for German is described by Schulte im Walde (2002a), who defined an SCF inventory and manually wrote a grammar to analyse verb constructions according to these frames. A lexicalised PCFG parser using this grammar was trained on 18.7 million words of German newspaper text; the trained parser model contained explicit subcategorisation frequencies, which could then be extracted to construct a subcategorisation lexicon for 14,229 German verbs. This work was evaluated against a German dictionary, the Duden Stilw¨orterbuch (Schulte im Walde, 2002b). Schulte im Walde and Brew (2002) used the subcategorisation lexicon created by the system to automa</context>
<context position="8614" citStr="Walde (2002" startWordPosition="1357" endWordPosition="1358">be in Section 3.1; this system is evaluated in a task-based way (e.g., to improve the performance of a SMT system) and cannot be directly compared to our system in this paper. 3 The SCF acquisition system This section describes the first contribution of this paper, a state-of-the-art subcategorisation acquisition system for German. Its core component is a rule-based SCF tagger which operates on phrase structure analyses, as delivered by a statistical parser. Given a parse of a sentence, the tagger assigns each finite verb in the sentence an SCF type. 299 We use the SCF inventory of Schulte im Walde (2002a), which includes complements like n for nominative subject, a for accusative direct object, d for dative indirect object, r for reflexive pronoun, and x for expletive es (‘it’) subject. Clausal complements can be infinite (i); finite ones can have the verb in second position (S-2) or include the complementiser dass ‘that’ (S-dass). Complements can be combined as in na (transitive verb); for PPs in SCFs, the head is specified, e.g., p:f¨ur for PP complements headed by f¨ur ‘for’1. Due to the free word order, simple phrase structure like that used for analysis of English is not enough to speci</context>
<context position="18218" citStr="Walde (2002" startWordPosition="2943" endWordPosition="2944"> words. This SCF lexicon contains statistics on 133,897 verb instances (5,316 verb lemmas). While the manual annotations in NEGRA and TIGER mean that this SCF lexicon has virtually no noise, the small size of the corpora results in problems with data sparsity and negatively impacts the utility of this resource (see discussion in Section 6.2). 5 Automatic verb classification The remainder of the paper sets out to establish the relevance of our SCF acquisition system by comparison to previous work. As stated in Sec. 2, the only prior automatic German SCF acquisition system is that of Schulte im Walde (2002a), which was evaluated directly against an electronic version of a large dictionary; as this is not an open access resource, we cannot perform a similar evaluation. We opt therefore to use a task-based evaluation to compare our system directly with Schulte im Walde’s, and leave manual evaluation for future work. We refer back to the experiment set up by Schulte im Walde (2006) to automatically induce classifications of German verbs by clustering them on the basis of their SCF preferences as listed in her SCF lexicon. By casting this experiment as a fixed task, we can compare our system direct</context>
<context position="29849" citStr="Walde (2002" startWordPosition="4875" endWordPosition="4876">ns; these results are significant at the p &lt; 0.001 level8. 7PP case information is relevant for prepositions that can take both locative and directional readings, as in in der Stadt (dative) ‘in town’ und in die Stadt (accusative) ‘to town’. 8Statistical significance is calculated by running repeated k-means clusterings with random partition initialisation and evaluating the results using the relevant purity metrics. These repeated clustering scores represent a random variable (a func304 6.2 Discussion Sec. 6.1 compared the SCF lexicon created using SdeWaC with the lexicon built by Schulte im Walde (2002a), showing that our lexicon achieves significantly better results on the verb clustering task. We interpret this to be indicative of a more accurate subcategorisation lexicon, and, by extension, of a more accurate SCF acquisition system. We attribute this superior performance primarily to our use of a statistical parser as opposed to a hand-written grammar. This design choice has several advantages. First, the parser delivers robust syntactic analyses, which we can expect to be relatively domain-independent. Second, we make no prior assumptions about the variety of subcategorisation phenomena</context>
</contexts>
<marker>Walde, 2002</marker>
<rawString>Sabine Schulte im Walde. 2002b. Evaluating verb subcategorisation frames learned by a German statistical grammar against manual definitions in the Duden Dictionary. In EURALEX, pages 187–197.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Schulte im Walde</author>
</authors>
<title>Experiments on the automatic induction of German semantic verb classes.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>2</issue>
<contexts>
<context position="7440" citStr="Walde (2006)" startWordPosition="1158" endWordPosition="1159">ords of German newspaper text; the trained parser model contained explicit subcategorisation frequencies, which could then be extracted to construct a subcategorisation lexicon for 14,229 German verbs. This work was evaluated against a German dictionary, the Duden Stilw¨orterbuch (Schulte im Walde, 2002b). Schulte im Walde and Brew (2002) used the subcategorisation lexicon created by the system to automatically induce a set of semantic verb classes with an unsupervised clustering algorithm. This clustering was evaluated against a small manually created semantic verb classification. Schulte im Walde (2006) continues this work using a larger manual verb classification. The SCFs used in this study are defined at three levels of granularity. The first level (38 different SCFs) lists only the complements in the frame; the second one adds head and case information for PP complements (183 SCFs). The third level examined the effect of adding selectional preferences, but results were inconclusive. A recent paper (Scheible et al., 2013) describes a system similar to ours, built on a statistical dependency parser, and using some of the same kinds of rules as we describe in Section 3.1; this system is eva</context>
<context position="18598" citStr="Walde (2006)" startWordPosition="3006" endWordPosition="3007">e remainder of the paper sets out to establish the relevance of our SCF acquisition system by comparison to previous work. As stated in Sec. 2, the only prior automatic German SCF acquisition system is that of Schulte im Walde (2002a), which was evaluated directly against an electronic version of a large dictionary; as this is not an open access resource, we cannot perform a similar evaluation. We opt therefore to use a task-based evaluation to compare our system directly with Schulte im Walde’s, and leave manual evaluation for future work. We refer back to the experiment set up by Schulte im Walde (2006) to automatically induce classifications of German verbs by clustering them on the basis of their SCF preferences as listed in her SCF lexicon. By casting this experiment as a fixed task, we can compare our system directly to hers. The link between subcategorisation and verb semantics is linguistically sound, due to the interdependence between verb meanings and the number and kinds of their syntactic arguments (Levin, 1993; Levin and Rappaport Hovav, 2005). E.g., only transitive verbs that denote a change of state like cut and break enter in the middle construction (The bread cuts easily.), wi</context>
<context position="20098" citStr="Walde (2006" startWordPosition="3243" endWordPosition="3244">xperiment to the best of our ability. We argue that by evaluating our SdeWaC SCF lexicon described in the previous section, we simultaneously evaluate our subcategorisation acquisition system; this technique also allows us to demonstrate the semantic relevance of our SCF lexicon. Section 5.1 introduces the manual verb classification we use as a gold standard and Section 5.2 describes our unsupervised clustering technique. Our evaluation of the clustering against the gold standard then follows in Section 6. 5.1 Manual verb classifications The semantic verb classification proposed by Schulte im Walde (2006, page 162ff.), hereafter SiW2006, comprises 168 high- and low-frequency verbs grouped into 43 semantic classes, with between 2 and 7 verbs per class. Examples of these classes are Aspect (e.g., anfangen ‘begin’), Propositional Attitude (e.g., denken ‘think’), Transfer of Possession (Obtaining) (e.g., bekommen ‘get’), and Weather (e.g., regnen ‘rain’). Some of the classes are subclassified3, e.g., Manner of Motion, with the subclasses Locomotion (klettern ‘climb’), Rotation (rotieren ‘rotate’), Rush (eilen ‘hurry’), Vehicle (fliegen ‘fly’), and Flotation (gleiten ‘glide’). These classes are re</context>
<context position="23645" citStr="Walde (2006)" startWordPosition="3813" endWordPosition="3814">uster centroids; in this study, we initialise the centroids by random partitions (each of the n objects is randomly assigned to one of k clusters, and the centroids are then computed as the means of these random partitions). Because the random initial centroids influence the final clustering, we repeat the clustering a number of times. We also initialise the k-means cluster centroids using agglomerative hierarchical clustering, a deterministic iterative bottom-up process. Hierarchical clustering initially assigns verbs to singleton clusters; the two clusters which are “nearest” to 4Schulte im Walde (2006) takes α = 0.9 although Lee (1999) recommends α = 0.99 or higher values in her original description of skew divergence. each other are then joined together, and this process is repeated until the desired number of clusters is obtained. Hierarchical clustering is performed to group the verbs into k clusters; the centroids of these clusters are then used to initialise the kmeans algorithm. While there exist several variants of hierarchical clustering, we use Ward’s method (Ward, Jr, 1963) for merging clusters, which attempts to minimise the variance inside clusters; Ward’s criterion was previous</context>
<context position="28579" citStr="Walde (2006" startWordPosition="4673" endWordPosition="4674"> the average cluster purity of the 10 final clusterings. “Ward” shows the evaluation of the clustering initialised with centroids found by hierarchical clus5Verbs missing from the clustering reduce the maximum achievable cluster purity score. 6Specifically, we take the clustering result with the minimum intra-cluster distance (not the clustering result with the best performance on the gold standard). tering of the verbs using Ward’s method. Again, both the initial partition found by Ward’s method and the k-means solution based on it are shown. For comparison, we list the results of Schulte im Walde (2006, p. 174, Table 7) for the second level of SCF granularity, with PP head and case information (see Sec. 2 for Schulte im Walde’s analysis). While this seems the most appropriate comparison to draw, since we also collect statistics about PPs, it is ambitious because, as noted in Section 3, our SCF lexica lack case information about PPs.7 Compared to Schulte im Walde’s numbers, the NEGRA/TIGER SCF lexicon scores significantly worse on the PairF evaluation metric under all conditions, and also on the Randa metric using the skew divergence measure (Randa/IRad is not significantly different). The S</context>
<context position="32308" citStr="Walde (2006)" startWordPosition="5266" endWordPosition="5267">e more precisely estimated as the counts of observed verb instances increase, particularly for low-frequency verbs. Error analysis of our SCF lexicon reveals low counts of expletive subjects. The edge labeller is supposed to annotate semantically empty subjects (es, ‘it’) as expletive; for clusterings examined in Sec. 5.1, this would affect weather verbs (e.g., es tion of the random cluster centroids used to initialise the kmeans clustering). These samples are normally distributed, so we determine statistical significance using a t-test against the “Random Mean” results reported by Schulte im Walde (2006). regnet, ‘it’s raining’). However, in our SdeWaC SCF lexicon, expletive subjects are clearly underrepresented. Our SCF lexicon built on TIGER, where expletive subjects are systematically labelled, has the SCF xa as the most common SCF for the verb geben (in es gibt ‘there is’). In contrast, in our SdeWaC SCF lexicon, the most common SCF is the transitive na, with xa in seventh place. I.e., the edge labeller does not identify all expletive subjects, which is due to the fact that expletive subjects are syntactically indistinguishable from neuter pronominal subjects, so the edge labeller does no</context>
</contexts>
<marker>Walde, 2006</marker>
<rawString>Sabine Schulte im Walde. 2006. Experiments on the automatic induction of German semantic verb classes. Computational Linguistics, 32(2):159–194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Schulte im Walde</author>
</authors>
<title>The induction of verb frames and verb classes from corpora.</title>
<date>2009</date>
<booktitle>In Anke L¨udeling and Merja Kyt¨o, editors, Corpus linguistics: An international handbook,</booktitle>
<volume>2</volume>
<pages>952--971</pages>
<location>Berlin.</location>
<contexts>
<context position="4672" citStr="Walde (2009)" startWordPosition="722" endWordPosition="723">e the relative frequency of SCF classes. Potential SCF patterns are extracted from a corpus parsed with a dependency-based parser, and then filtered by hypothesis testing on binomial frequency data. Korhonen (2002) refines Briscoe and Carroll (1997)’s system using back-off estimates on the WordNet semantic class of the verb’s predominant sense, assuming that semantically similar verbs have similar SCFs, following Levin (1993). Some current statistical methods for Semantic Role Labelling build models that also capture subcategorisation information, e.g., Grenager and Manning (2006). Schulte im Walde (2009) offers a recent survey of the SCF acquisition literature. SCF acquisition is also an important step in the automatic semantic role labelling (Grenager and Manning, 2006; Lang and Lapata, 2010; Titov and Klementiev, 2012). Semantic roles of a verb describe the kind of involvement of entities in the event introduced by the verb, e.g., as agent (active, often not affected by the event) or patient (passive, often affected). On the basis of these SCFs, semantic roles can be assigned due to the interdependence between semantic roles and their syntactic realisations, called Argument Linking (Levin, </context>
</contexts>
<marker>Walde, 2009</marker>
<rawString>Sabine Schulte im Walde. 2009. The induction of verb frames and verb classes from corpora. In Anke L¨udeling and Merja Kyt¨o, editors, Corpus linguistics: An international handbook, volume 2, chapter 44, pages 952–971. Mouton de Gruyter, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ekaterina Shutova</author>
<author>Lin Sun</author>
<author>Anna Korhonen</author>
</authors>
<title>Metaphor identification using verb and noun clustering.</title>
<date>2010</date>
<booktitle>In COLING,</booktitle>
<pages>1002--1010</pages>
<marker>Shutova, Sun, Korhonen, 2010</marker>
<rawString>Ekaterina Shutova, Lin Sun, and Anna Korhonen. 2010. Metaphor identification using verb and noun clustering. In COLING, pages 1002–1010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wojciech Skut</author>
<author>Brigitte Krenn</author>
<author>Thorsten Brants</author>
<author>Hans Uszkoreit</author>
</authors>
<title>An annotation scheme for free word order languages. In</title>
<date>1997</date>
<booktitle>ANLP,</booktitle>
<pages>88--95</pages>
<contexts>
<context position="9367" citStr="Skut et al., 1997" startWordPosition="1481" endWordPosition="1484">ive pronoun, and x for expletive es (‘it’) subject. Clausal complements can be infinite (i); finite ones can have the verb in second position (S-2) or include the complementiser dass ‘that’ (S-dass). Complements can be combined as in na (transitive verb); for PPs in SCFs, the head is specified, e.g., p:f¨ur for PP complements headed by f¨ur ‘for’1. Due to the free word order, simple phrase structure like that used for analysis of English is not enough to specify the syntax of German sentences. Therefore we use the annotation scheme in the manually constructed German treebanks NEGRA and TIGER (Skut et al., 1997; Brants et al., 2002), which decorate parse trees with edge labels specifying the syntactic roles of constituents. We automatically annotate the parse trees from our statistical parser using a simple machine learning model. In the next section, we illustrate the operation of the SCF tagger with reference to examples; then in Section 3.2 we describe our edge labeller. 3.1 The SCF tagger The SCF tagger begins by collecting complements co-occurring with a verb instance using the phrase structure of the sentence. In our system, we obtain phrase structure information for unannotated text using the</context>
</contexts>
<marker>Skut, Krenn, Brants, Uszkoreit, 1997</marker>
<rawString>Wojciech Skut, Brigitte Krenn, Thorsten Brants, and Hans Uszkoreit. 1997. An annotation scheme for free word order languages. In ANLP, pages 88–95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lin Sun</author>
<author>Anna Korhonen</author>
<author>Yuval Krymolowski</author>
</authors>
<title>Verb class discovery from rich syntactic data. In</title>
<date>2008</date>
<booktitle>CICLing,</booktitle>
<pages>16--27</pages>
<location>Haifa,</location>
<contexts>
<context position="2368" citStr="Sun et al., 2008" startWordPosition="365" endWordPosition="368">bcategorised for, too, e.g. both stellen and its English translation put subcategorise for subject, direct object, and a prepositional phrase (PP) like on the shelf: (5) [NP Al] put [NP the book] [PP on the shelf]. Subcategorisation frames describe a combination of arguments required by a specific verb. The set of SCFs for a verb is called its subcategorisation preference. Our system follows much previous work by counting PPs that accompany the verb among its complements, even though they are not obligatory (so-called ‘adjuncts’), because PP adjuncts are excellent clues to a verb’s semantics (Sun et al., 2008). However, nominal and clausal adjuncts do not count as verbal complements. SCF information can benefit all applications that need information on predicate-argument structure, e.g., parsing, verb clustering, semantic role labelling, or machine translation. Automatic acquisition of SCF information with minimal supervision is also crucial to construct useful resources quickly. The main innovation of the presented new system is to address two challenges simultaneously, viz., SCF acquisition from raw text and the focus on languages with a very free word order. With this system, we create an SCF le</context>
</contexts>
<marker>Sun, Korhonen, Krymolowski, 2008</marker>
<rawString>Lin Sun, Anna Korhonen, and Yuval Krymolowski. 2008. Verb class discovery from rich syntactic data. In CICLing, pages 16–27, Haifa, Israel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lin Sun</author>
<author>Anna Korhonen</author>
<author>Thierry Poibeau</author>
<author>C´edric Messiant</author>
</authors>
<title>Investigating the cross-linguistic potential of VerbNet-style classification.</title>
<date>2010</date>
<booktitle>In COLING,</booktitle>
<pages>1056--1064</pages>
<location>Beijing, China.</location>
<marker>Sun, Korhonen, Poibeau, Messiant, 2010</marker>
<rawString>Lin Sun, Anna Korhonen, Thierry Poibeau, and C´edric Messiant. 2010. Investigating the cross-linguistic potential of VerbNet-style classification. In COLING, pages 1056–1064, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>Alexandre Klementiev</author>
</authors>
<title>A Bayesian approach to unsupervised semantic role induction.</title>
<date>2012</date>
<booktitle>In EACL,</booktitle>
<pages>12--22</pages>
<contexts>
<context position="4893" citStr="Titov and Klementiev, 2012" startWordPosition="755" endWordPosition="758">n (2002) refines Briscoe and Carroll (1997)’s system using back-off estimates on the WordNet semantic class of the verb’s predominant sense, assuming that semantically similar verbs have similar SCFs, following Levin (1993). Some current statistical methods for Semantic Role Labelling build models that also capture subcategorisation information, e.g., Grenager and Manning (2006). Schulte im Walde (2009) offers a recent survey of the SCF acquisition literature. SCF acquisition is also an important step in the automatic semantic role labelling (Grenager and Manning, 2006; Lang and Lapata, 2010; Titov and Klementiev, 2012). Semantic roles of a verb describe the kind of involvement of entities in the event introduced by the verb, e.g., as agent (active, often not affected by the event) or patient (passive, often affected). On the basis of these SCFs, semantic roles can be assigned due to the interdependence between semantic roles and their syntactic realisations, called Argument Linking (Levin, 1993; Levin and Rappaport Hovav, 2005). Acquiring SCFs for languages with a very fixed word order like English needs only a simple syntactic analysis, which mainly relies on the predetermined sequencing of arguments in th</context>
</contexts>
<marker>Titov, Klementiev, 2012</marker>
<rawString>Ivan Titov and Alexandre Klementiev. 2012. A Bayesian approach to unsupervised semantic role induction. In EACL, pages 12–22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joe H Ward</author>
</authors>
<title>Hierarchical grouping to optimize an objective function.</title>
<date>1963</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>58</volume>
<issue>301</issue>
<marker>Ward, 1963</marker>
<rawString>Joe H. Ward, Jr. 1963. Hierarchical grouping to optimize an objective function. Journal of the American Statistical Association, 58(301):236–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oliver Wauschkuhn</author>
</authors>
<title>Automatische Extraktion von Verbvalenzen aus deutschen Textkorpora.</title>
<date>1999</date>
<publisher>Shaker Verlag.</publisher>
<contexts>
<context position="6527" citStr="Wauschkuhn, 1999" startWordPosition="1022" endWordPosition="1023">ns, adjectives, and determiners. E.g., the noun Auto ‘car’ has only two forms, Auto for nominative, dative, and accusative singular, and Autos for genitive singular and all four plural forms. This is in contrast to some other free word order languages for which SCF acquisition has been studied, like Modern Greek (Maragoudakis et al., 2000) and Czech (Sarkar and Zeman, 2000). A one-many relation between word forms and case is also one of the problems for SCF acquisition in Urdu (Ghulam, 2011). For German, initial studies used semi-automatic techniques and manual evaluation (Eckle-Kohler, 1999; Wauschkuhn, 1999). The first automatic subcategorisation acquisition system for German is described by Schulte im Walde (2002a), who defined an SCF inventory and manually wrote a grammar to analyse verb constructions according to these frames. A lexicalised PCFG parser using this grammar was trained on 18.7 million words of German newspaper text; the trained parser model contained explicit subcategorisation frequencies, which could then be extracted to construct a subcategorisation lexicon for 14,229 German verbs. This work was evaluated against a German dictionary, the Duden Stilw¨orterbuch (Schulte im Walde,</context>
</contexts>
<marker>Wauschkuhn, 1999</marker>
<rawString>Oliver Wauschkuhn. 1999. Automatische Extraktion von Verbvalenzen aus deutschen Textkorpora. Shaker Verlag.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>