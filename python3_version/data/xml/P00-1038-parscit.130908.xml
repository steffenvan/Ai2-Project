<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000007">
<title confidence="0.97499">
Query-Relevant Summarization using FAQs
</title>
<author confidence="0.997211">
Adam Berger Vibhu O. Mittal
</author>
<affiliation confidence="0.963838">
School of Computer Science Just Research
Carnegie Mellon University 4616 Henry Street
</affiliation>
<address confidence="0.609189">
Pittsburgh, PA 15213 Pittsburgh, PA 15213
</address>
<email confidence="0.998706">
aberger@cs.cmu.edu mittal@justresearch.com
</email>
<sectionHeader confidence="0.993894" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999843">
This paper introduces a statistical model for
query-relevant summarization: succinctly
characterizing the relevance of a document
to a query. Learning parameter values for
the proposed model requires a large collec-
tion of summarized documents, which we
do not have, but as a proxy, we use a col-
lection of FAQ (frequently-asked question)
documents. Taking a learning approach en-
ables a principled, quantitative evaluation
of the proposed system, and the results of
some initial experiments—on a collection
of Usenet FAQs and on a FAQ-like set
of customer-submitted questions to several
large retail companies—suggest the plausi-
bility of learning for summarization.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999469444444445">
An important distinction in document summarization
is between generic summaries, which capture the cen-
tral ideas of the document in much the same way that
the abstract of this paper was designed to distill its
salient points, and query-relevant summaries, which
reflect the relevance of a document to a user-specified
query. This paper discusses query-relevant summa-
rization, sometimes also called “user-focused summa-
rization” (Mani and Bloedorn, 1998).
Query-relevant summaries are especially important
in the “needle(s) in a haystack” document retrieval
problem: a user has an information need expressed
as a query (What countries export smoked
salmon?), and a retrieval system must locate within
a large collection of documents those documents most
likely to fulfill this need. Many interactive retrieval
systems—web search engines like Altavista, for
instance—present the user with a small set of candi-
date relevant documents, each summarized; the user
must then perform a kind of triage to identify likely
relevant documents from this set. The web page sum-
maries presented by most search engines are generic,
not query-relevant, and thus provide very little guid-
ance to the user in assessing relevance. Query-relevant
summarization (QRS) aims to provide a more effective
characterization of a document by accounting for the
user’s information need when generating a summary.
</bodyText>
<figureCaption confidence="0.9263325">
Figure 1: One promising setting for query-relevant sum-
marization is large-scale document retrieval. Given a user
</figureCaption>
<bodyText confidence="0.888019636363636">
query , search engines typically first (a) identify a set of
documents which appear potentially relevant to the query,
and then (b) produce a short characterization of each
document’s relevance to . The purpose of is to as-
sist the user in finding documents that merit a more detailed
inspection.
As with almost all previous work on summarization,
this paper focuses on the task of extractive summariza-
tion: selecting as summaries text spans—either com-
plete sentences or paragraphs—from the original doc-
ument.
</bodyText>
<subsectionHeader confidence="0.990201">
1.1 Statistical models for summarization
</subsectionHeader>
<bodyText confidence="0.999969333333333">
From a document and query , the task of query-
relevant summarization is to extract a portion from
which best reveals how the document relates to
the query. To begin, we start with a collection of
triplets, where is a human-constructed sum-
mary of relative to the query . From such a collec-
</bodyText>
<figure confidence="0.99705474137931">
(a) (b)
Search for
relevant
documents
Summarize
documents
relative to
Q
6
6
6
6
What is amniocentesis?
Amniocenteses, or amnio, is
a prenatal test in which...
What can it detect?
One of the main uses of
amniocentesis is to detect
chromosomal abnormalities...
What are the risks of amnio?
The main risk of amnio is
that it may increase the
chance of miscarriage...
. . .
A single
FAQ
document
S1
D2
D3
S2
Some parents
elect to teach
their children
at home...
...
...
S3
Good Will
Hunting is
about...
...
=Weather in Paris
Q1 in December
Q= Home
2 schooling
Q3 = Academy award
winners in 1998
D1
Snow is
not unusual
in France...
...
...
...
Summary of
document with
respect to Q2
</figure>
<figureCaption confidence="0.980427">
Figure 2: Learning to perform query-relevant summariza-
tion requires a set of documents summarized with respect to
</figureCaption>
<bodyText confidence="0.974943342857143">
queries. Here we show three imaginary triplets ,
but the statistical learning techniques described in Section 2
require thousands of examples.
tion of data, we fit the best function
mapping document/query pairs to summaries.
The mapping we use is a probabilistic one, meaning
the system assigns a value to every possible
summary of . The QRS system will summarize
a pair by selecting
There are at least two ways to interpret .
First, one could view as a “degree of be-
lief” that the correct summary of relative to is .
Of course, what constitutes a good summary in any
setting is subjective: any two people performing the
same summarization task will likely disagree on which
part of the document to extract. We could, in principle,
ask a large number of people to perform the same task.
Doing so would impose a distribution over
candidate summaries. Under the second, or “frequen-
tist” interpretation, is the fraction of people
who would select —equivalently, the probability that
a person selected at random would prefer as the sum-
mary.
The statistical model is parametric, the
values of which are learned by inspection of the
triplets. The learning process involves
maximum-likelihood estimation of probabilistic lan-
guage models and the statistical technique of shrink-
age (Stein, 1955).
This probabilistic approach easily generalizes to
the generic summarization setting, where there is no
query. In that case, the training data consists of
pairs, where is a summary of the document . The
goal, in this case, is to learn and apply a mapping
from documents to summaries. That is,
</bodyText>
<figureCaption confidence="0.986151833333333">
Figure 3: FAQs consist of a list of questions and answers
on a single topic; the FAQ depicted here is part of an in-
formational document on amniocentesis. This paper views
answers in a FAQ as different summaries of the FAQ: the an-
swer to the th question is a summary of the FAQ relative to
that question.
</figureCaption>
<bodyText confidence="0.6602225">
find
def
</bodyText>
<subsectionHeader confidence="0.99874">
1.2 Using FAQ data for summarization
</subsectionHeader>
<bodyText confidence="0.994669690909091">
We have proposed using statistical learning to con-
struct a summarization system, but have not yet dis-
cussed the one crucial ingredient of any learning pro-
cedure: training data. The ideal training data would
contain a large number of heterogeneous documents, a
large number of queries, and summaries of each doc-
ument relative to each query. We know of no such
publicly-available collection. Many studies on text
summarization have focused on the task of summariz-
ing newswire text, but there is no obvious way to use
news articles for query-relevant summarization within
our proposed framework.
In this paper, we propose a novel data collection
for training a QRS model: frequently-asked question
documents. Each frequently-asked question document
(FAQ) is comprised of questions and answers about
a specific topic. We view each answer in a FAQ as
a summary of the document relative to the question
which preceded it. That is, an FAQ with ques-
tion/answer pairs comes equipped with different
queries and summaries: the answer to the th ques-
tion is a summary of the document relative to the th
question. While a somewhat unorthodox perspective,
this insight allows us to enlist FAQs as labeled train-
ing data for the purpose of learning the parameters of
a statistical QRS model.
FAQ data has some properties that make it particu-
larly attractive for text learning:
def
There exist a large number of Usenet FAQs—
several thousand documents—publicly available
on the Web1. Moreover, many large compa-
nies maintain their own FAQs to streamline the
customer-response process.
FAQs are generally well-structured documents,
so the task of extracting the constituent parts
(queries and answers) is amenable to automation.
There have even been proposals for standardized
FAQ formats, such as RFC1153 and the Minimal
Digest Format (Wancho, 1990).
Usenet FAQs cover an astonishingly wide variety
of topics, ranging from extraterrestrial visitors to
mutual-fund investing. If there’s an online com-
munity of people with a common interest, there’s
likely to be a Usenet FAQ on that subject.
There has been a small amount of published work
involving question/answer data, including (Sato and
Sato, 1998) and (Lin, 1999). Sato and Sato used FAQs
as a source of summarization corpora, although in
quite a different context than that presented here. Lin
used the datasets from a question/answer task within
the Tipster project, a dataset of considerably smaller
size than the FAQs we employ. Neither of these paper
focused on a statistical machine learning approach to
summarization.
</bodyText>
<sectionHeader confidence="0.749526" genericHeader="method">
2 A probabilistic model of
summarization
</sectionHeader>
<bodyText confidence="0.936785642857143">
Given a query and document , the query-relevant
summarization task is to find
the a posteriori most probable summary for .
Using Bayes’ rule, we can rewrite this expression as
relevance fidelity
where the last line follows by dropping the dependence
on in .
Equation (1) is a search problem: find the summary
which maximizes the product of two factors:
1. The relevance of the query to the sum-
mary: A document may contain some portions
directly relevant to the query, and other sections
bearing little or no relation to the query. Con-
sider, for instance, the problem of summarizing a
</bodyText>
<footnote confidence="0.94741">
1Two online sources for FAQ data are www.faqs.org
and rtfm.mit.edu.
</footnote>
<bodyText confidence="0.9900231">
survey on the history of organized sports relative
to the query “Who was Lou Gehrig?” A summary
mentioning Lou Gehrig is probably more relevant
to this query than one describing the rules of vol-
leyball, even if two-thirds of the survey happens
to be about volleyball.
2. The fidelity of the summary to the
document: Among a set of candidate sum-
maries whose relevance scores are comparable,
we should prefer that summary which is most
representative of the document as a whole. Sum-
maries of documents relative to a query can of-
ten mislead a reader into overestimating the rel-
evance of an unrelated document. In particular,
very long documents are likely (by sheer luck)
to contain some portion which appears related to
the query. A document having nothing to do with
Lou Gehrig may include a mention of his name
in passing, perhaps in the context of amyotropic
lateral sclerosis, the disease from which he suf-
fered. The fidelity term guards against this occur-
rence by rewarding or penalizing candidate sum-
maries, depending on whether they are germane
to the main theme of the document.
More generally, the fidelity term represents a
prior, query-independent distribution over candi-
date summaries. In addition to enforcing fidelity,
this term could serve to distinguish between more
and less fluent candidate summaries, in much the
same way that traditional language models steer a
speech dictation system towards more fluent hy-
pothesized transcriptions.
In words, (1) says that the best summary of a doc-
ument relative to a query is relevant to the query (ex-
hibits a large value) and also representative of
the document from which it was extracted (exhibits a
large value). We now describe the paramet-
ric form of these models, and how one can determine
optimal values for these parameters using maximum-
likelihood estimation.
</bodyText>
<subsectionHeader confidence="0.990139">
2.1 Language modeling
</subsectionHeader>
<bodyText confidence="0.9844525">
The type of statistical model we employ for both
and is a unigram probability distri-
bution over words; in other words, a language model.
Stochastic models of language have been used exten-
sively in speech recognition, optical character recogni-
tion, and machine translation (Jelinek, 1997; Berger et
al., 1994). Language models have also started to find
their way into document retrieval (Ponte and Croft,
1998; Ponte, 1998).
The fidelity model
One simple statistical characterization of an -word
document is the frequency of
</bodyText>
<equation confidence="0.962988">
(1)
</equation>
<bodyText confidence="0.981361285714286">
each word in —in other words, a marginal distribu-
tion over words. That is, if word appears times in
, then . This is not only intuitive, but
also the maximum-likelihood estimate for .
Now imagine that, when asked to summarize rel-
ative to , a person generates a summary from in the
following way:
</bodyText>
<figure confidence="0.7156265">
Select a length for the summary according to
some distribution .
</figure>
<figureCaption confidence="0.897499333333333">
Figure 4: The relevance of a query to the th an-
swer in document is a convex combination of five distribu-
tions: (1) a uniform model . (2) a corpus-wide model ;
</figureCaption>
<figure confidence="0.8289507">
(3) a model constructed from the document containing
; (4) a model constructed from and the neigh-
boring sentences in ; (5) a model constructed from
alone. (The distribution is omitted for clarity.)
Do for :
– Select a word at random according to the
distribution . (That is, throw all the words
in into a bag, pull one out, and then re-
place it.)
– Set .
</figure>
<bodyText confidence="0.975814984126984">
In following this procedure, the person will generate
the summary with probability
Denoting by the set of all known words, and by
the number of times that word appears in
, one can also write (2) as a multinomial distribution:
In the text classification literature, this characteriza-
tion of is known as a “bag of words” model, since the
distribution does not take account of the order of
the words within the document , but rather views as
an unordered set (“bag”) of words. Of course, ignoring
word order amounts to discarding potentially valuable
information. In Figure 3, for instance, the second ques-
tion contains an anaphoric reference to the preceding
question: a sophisticated context-sensitive model of
language might be able to detect that it in this context
refers to amniocentesis, but a context-free model
will not.
The relevance model
In principle, one could proceed analogously to (2),
and take
for a length- query . But this strat-
egy suffers from a sparse estimation problem. In con-
trast to a document, which we expect will typically
contain a few hundred words, a normal-sized summary
contains just a handful of words. What this means is
that will assign zero probability to most words, and
any query containing a word not in the summary will
receive a relevance score of zero.
(The fidelity model doesn’t suffer from zero-
probabilities, at least not in the extractive summariza-
tion setting. Since a summary is part of its contain-
ing document , every word in also appears in ,
and therefore for every word . But
we have no guarantee, for the relevance model, that a
summary contains all the words in the query.)
We address this zero-probability problem by inter-
polating or “smoothing” the model with four more
robustly estimated unigram word models. Listed in
order of decreasing variance but increasing bias away
from , they are:
: a probability distribution constructed using
not only , but also all words within the six sum-
maries (answers) surrounding in . Since
is calculated using more text than just alone, its
parameter estimates should be more robust that
those of . On the other hand, the model is,
by construction, biased away from , and there-
fore provides only indirect evidence for the rela-
tion between and .
: a probability distribution constructed over the
entire document containing . This model has
even less variance than , but is even more bi-
ased away from .
: a probability distribution constructed over all
documents .
: the uniform distribution over all words.
Figure 4 is a hierarchical depiction of the various
language models which come into play in calculating
. Each summary model lives at a leaf node,
and the relevance of a query to that summary is
a convex combination of the distributions at each node
Algorithm: Shrinkage for estimation
along a path from the leaf to the root2:
</bodyText>
<equation confidence="0.368364">
(5)
</equation>
<bodyText confidence="0.999970363636364">
We calculate the weighting coefficients
using the statistical technique
known as shrinkage (Stein, 1955), a simple form of
the EM algorithm (Dempster et al., 1977).
As a practical matter, if one assumes the model
assigns probabilities independently of , then we can
drop the term when ranking candidate summaries,
since the score of all candidate summaries will re-
ceive an identical contribution from the term. We
make this simplifying assumption in the experiments
reported in the following section.
</bodyText>
<sectionHeader confidence="0.999952" genericHeader="method">
3 Results
</sectionHeader>
<bodyText confidence="0.999902666666667">
To gauge how well our proposed summarization tech-
nique performs, we applied it to two different real-
world collections of answered questions:
</bodyText>
<listItem confidence="0.417082">
Usenet FAQs: A collection of frequently-
</listItem>
<bodyText confidence="0.949182428571429">
asked question documents from the comp.*
Usenet hierarchy. The documents contained
questions/answer pairs in total.
Call-center data: A collection of questions
submitted by customers to the companies Air
Canada, Ben and Jerry, Iomagic, and Mylex,
along with the answers supplied by company
</bodyText>
<footnote confidence="0.753656">
2By incorporating a model into the relevance model,
equation (6) has implicitly resurrected the dependence on
which we dropped, for the sake of simplicity, in deriving (1).
</footnote>
<bodyText confidence="0.999803218181818">
representatives. These four documents contain
question/answer pairs.
We conducted an identical, parallel set of experi-
ments on both. First, we used a randomly-selected
subset of 70% of the question/answer pairs to calcu-
late the language models —a simple
matter of counting word frequencies. Then, we used
this same set of data to estimate the model weights
using shrinkage. We re-
served the remaining 30% of the question/answer pairs
to evaluate the performance of the system, in a manner
described below.
Figure 5 shows the progress of the EM algo-
rithm in calculating maximum-likelihood values for
the smoothing coefficients , for the first of the three
runs on the Usenet data. The quick convergence and
the final values were essentially identical for the
other partitions of this dataset.
The call-center data’s convergence behavior was
similar, although the final values were quite differ-
ent. Figure 6 shows the final model weights for the
first of the three experiments on both datasets. For
the Usenet FAQ data, the corpus language model is the
best predictor of the query and thus receives the high-
est weight. This may seem counterintuitive; one might
suspect that answer to the query ( , that is) would be
most similar to, and therefore the best predictor of,
the query. But the corpus model, while certainly bi-
ased away from the distribution of words found in the
query, contains (by construction) no zeros, whereas
each summary model is typically very sparse.
In the call-center data, the corpus model weight
is lower at the expense of a higher document model
weight. We suspect this arises from the fact that the
documents in the Usenet data were all quite similar to
one another in lexical content, in contrast to the call-
center documents. As a result, in the call-center data
the document containing will appear much more rel-
evant than the corpus as a whole.
To evaluate the performance of the trained QRS
model, we used the previously-unseen portion of the
FAQ data in the following way. For each test
pair, we recorded how highly the system ranked the
correct summary —the answer to in —relative
to the other answers in . We repeated this entire se-
quence three times for both the Usenet and the call-
center data.
For these datasets, we discovered that using a uni-
form fidelity term in place of the model de-
scribed above yields essentially the same result. This
is not surprising: while the fidelity term is an important
component of a real summarization system, our evalu-
ation was conducted in an answer-locating framework,
and in this context the fidelity term—enforcing that the
summary be similar to the entire document from which
</bodyText>
<figure confidence="0.998335727272727">
Input: Distributions ,
(not used to
estimate )
Output Model weights
1. Set
2. Repeat until converges:
3. Set for
5. (E-step)
(similarly for )
6. (M-step)
(similarly for )
</figure>
<figureCaption confidence="0.98809875">
Figure 5: Estimating the weights of the five constituent models in (6) using the EM algorithm. The values here were computed
using a single, randomly-selected 70% portion of the Usenet FAQ dataset. Left: The weights for the models are initialized to
, but within a few iterations settle to their final values. Right: The progression of the likelihood of the training data during
the execution of the EM algorithm; almost all of the improvement comes in the first five iterations.
</figureCaption>
<figure confidence="0.999694043478261">
Log-likelihood
0.5
0.4
model weight
0.3
0.2
0.1
0
-6.9
1 2 3 4 5 6 7 8 9 10
iteration
Test Training
uniform corpus FAQ nearby answers answer
-6.6
-6.7
-6.8
-6.3
-6.4
-6.5
1 2 3 4 5 6 7 8 9 10
Iteration
Document
14%
</figure>
<figureCaption confidence="0.999895">
Figure 6: Maximum-likelihood weights for the various
</figureCaption>
<bodyText confidence="0.8824412">
components of the relevance model . Left: Weights
assigned to the constituent models from the Usenet FAQ
data. Right: Corresponding breakdown for the call-center
data. These weights were calculated using shrinkage.
it was drawn—is not so important.
From a set of rankings , one can
measure the the quality of a ranking algorithm using
the harmonic mean rank:
A lower number indicates better performance; ,
which is optimal, means that the algorithm consis-
tently assigns the first rank to the correct answer. Ta-
ble 1 shows the harmonic mean rank on the two col-
lections. The third column of Table 1 shows the result
of a QRS system using a uniform fidelity model, the
fourth corresponds to a standard tfidf-based ranking
method (Ponte, 1998), and the last column reflects the
performance of randomly guessing the correct sum-
mary from all answers in the document.
trial # trials LM tfidf random
Usenet 1 554 1.41 2.29 4.20
FAQ 2 549 1.38 2.42 4.25
data 3 535 1.40 2.30 4.19
Call 1 1020 4.8 38.7 1335
center 2 1055 4.0 22.6 1335
data 3 1037 4.2 26.0 1321
</bodyText>
<tableCaption confidence="0.9590305">
Table 1: Performance of query-relevant extractive summa-
rization on the Usenet and call-center datasets. The numbers
reported in the three rightmost columns are harmonic mean
ranks: lower is better.
</tableCaption>
<sectionHeader confidence="0.998546" genericHeader="evaluation">
4 Extensions
</sectionHeader>
<subsectionHeader confidence="0.999951">
4.1 Question-answering
</subsectionHeader>
<bodyText confidence="0.9998008">
The reader may by now have realized that our approach
to the QRS problem may be portable to the problem of
question-answering. By question-answering, we mean
a system which automatically extracts from a poten-
tially lengthy document (or set of documents) the an-
swer to a user-specified question. Devising a high-
quality question-answering system would be of great
service to anyone lacking the inclination to read an
entire user’s manual just to find the answer to a sin-
gle question. The success of the various automated
</bodyText>
<figure confidence="0.998548611111111">
Usenet FAQ
call-center
Summary
11%
Neighbors
0%
Corpus
47%
Uniform Uniform
0% 7%
Summary
29%
Corpus
42%
Document
40%
Neighbors
10%
</figure>
<page confidence="0.547524">
def
</page>
<bodyText confidence="0.999958205882353">
question-answering services on the Internet (such as
AskJeeves) underscores the commercial importance
of this task.
One can cast answer-finding as a traditional docu-
ment retrieval problem by considering each candidate
answer as an isolated document and ranking each can-
didate answer by relevance to the query. Traditional
tfidf-based ranking of answers will reward candidate
answers with many words in common with the query.
Employing traditional vector-space retrieval to find an-
swers seems attractive, since tfidf is a standard, time-
tested algorithm in the toolbox of any IR professional.
What this paper has described is a first step towards
more sophisticated models of question-answering.
First, we have dispensed with the simplifying assump-
tion that the candidate answers are independent of one
another by using a model which explicitly accounts
for the correlation between text blocks—candidate
answers—within a single document. Second, we have
put forward a principled statistical model for answer-
ranking; has a probabilistic in-
terpretation as the best answer to within is .
Question-answering and query-relevant summariza-
tion are of course not one and the same. For one, the
criterion of containing an answer to a question is rather
stricter than mere relevance. Put another way, only a
small number of documents actually contain the an-
swer to a given query, while every document can in
principle be summarized with respect to that query.
Second, it would seem that the term, which
acts as a prior on summaries in (1), is less appropriate
in a question-answering setting, where it is less impor-
tant that a candidate answer to a query bears resem-
blance to the document containing it.
</bodyText>
<subsectionHeader confidence="0.94753">
4.2 Generic summarization
</subsectionHeader>
<bodyText confidence="0.993106125">
Although this paper focuses on the task of query-
relevant summarization, the core ideas—formulating
a probabilistic model of the problem and learning
the values of this model automatically from FAQ-like
data—are equally applicable to generic summariza-
tion. In this case, one seeks the summary which best
typifies the document. Applying Bayes’ rule as in (1),
generative prior
The first term on the right is a generative model of doc-
uments from summaries, and the second is a prior dis-
tribution over summaries. One can think of this factor-
ization in terms of a dialogue. Alice, a newspaper edi-
tor, has an idea for a story, which she relates to Bob.
Bob researches and writes the story , which we can
view as a “corruption” of Alice’s original idea . The
task of generic summarization is to recover , given
only the generated document , a model of
how the Alice generates summaries from documents,
and a prior distribution on ideas .
The central problem in information theory is reliable
communication through an unreliable channel. We can
interpret Alice’s idea as the original signal, and the
process by which Bob turns this idea into a document
as the channel, which corrupts the original message.
The summarizer’s task is to “decode” the original, con-
densed message from the document.
We point out this source-channel perspective be-
cause of the increasing influence that information the-
ory has exerted on language and information-related
applications. For instance, the source-channel model
has been used for non-extractive summarization, gen-
erating titles automatically from news articles (Wit-
brock and Mittal, 1999).
The factorization in (6) is superficially similar to (1),
but there is an important difference: is a gener-
ative, from a summary to a larger document, whereas
is compressive, from a summary to a smaller
query. This distinction is likely to translate in prac-
tice into quite different statistical models and training
procedures in the two cases.
</bodyText>
<sectionHeader confidence="0.998561" genericHeader="conclusions">
5 Summary
</sectionHeader>
<bodyText confidence="0.999964">
The task of summarization is difficult to define and
even more difficult to automate. Historically, a re-
warding line of attack for automating language-related
problems has been to take a machine learning perspec-
tive: let a computer learn how to perform the task by
“watching” a human perform it many times. This is
the strategy we have pursued here.
There has been some work on learning a probabilis-
tic model of summarization from text; some of the ear-
liest work on this was due to Kupiec et al. (1995),
who used a collection of manually-summarized text
to learn the weights for a set of features used in a
generic summarization system. Hovy and Lin (1997)
present another system that learned how the position
of a sentence affects its suitability for inclusion in
a summary of the document. More recently, there
has been work on building more complex, structured
models—probabilistic syntax trees—to compress sin-
gle sentences (Knight and Marcu, 2000). Mani and
Bloedorn (1998) have recently proposed a method for
automatically constructing decision trees to predict
whether a sentence should or should not be included
in a document’s summary. These previous approaches
focus mainly on the generic summarization task, not
query relevant summarization.
The language modelling approach described here
does suffer from a common flaw within text processing
systems: the problem of synonymy. A candidate an-
</bodyText>
<equation confidence="0.410444">
(6)
</equation>
<bodyText confidence="0.999983774193548">
swer containing the term Constantinople is likely
to be relevant to a question about Istanbul, but rec-
ognizing this correspondence requires a step beyond
word frequency histograms. Synonymy has received
much attention within the document retrieval com-
munity recently, and researchers have applied a vari-
ety of heuristic and statistical techniques—including
pseudo-relevance feedback and local context analy-
sis (Efthimiadis and Biron, 1994; Xu and Croft, 1996).
Some recent work in statistical IR has extended the ba-
sic language modelling approaches to account for word
synonymy (Berger and Lafferty, 1999).
This paper has proposed the use of two novel
datasets for summarization: the frequently-asked
questions (FAQs) from Usenet archives and ques-
tion/answer pairs from the call centers of retail compa-
nies. Clearly this data isn’t a perfect fit for the task of
building a QRS system: after all, answers are not sum-
maries. However, we believe that the FAQs represent a
reasonable source of query-related document conden-
sations. Furthermore, using FAQs allows us to assess
the effectiveness of applying standard statistical learn-
ing machinery—maximum-likelihood estimation, the
EM algorithm, and so on—to the QRS problem. More
importantly, it allows us to evaluate our results in a rig-
orous, non-heuristic way. Although this work is meant
as an opening salvo in the battle to conquer summa-
rization with quantitative, statistical weapons, we ex-
pect in the future to enlist linguistic, semantic, and
other non-statistical tools which have shown promise
in condensing text.
</bodyText>
<sectionHeader confidence="0.998226" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9998268">
This research was supported in part by an IBM Univer-
sity Partnership Award and by Claritech Corporation.
The authors thank Right Now Tech for the use of the
call-center question database. We also acknowledge
thoughtful comments on this paper by Inderjeet Mani.
</bodyText>
<sectionHeader confidence="0.997731" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.993695571428572">
A. Berger and J. Lafferty. 1999. Information retrieval
as statistical translation. In Proc. ofACMSIGIR-99.
A. Berger, P. Brown, S. Della Pietra, V. Della Pietra,
J. Gillett, J. Lafferty, H. Printz, and L. Ures. 1994.
The CANDIDE system for machine translation. In
Proc. of the ARPA Human Language Technology
Workshop.
Y. Chali, S. Matwin, and S. Szpakowicz. 1999. Query-
biased text summarization as a question-answering
technique. In Proc. of the AAAI Fall Symp. on Ques-
tion Answering Systems, pages 52–56.
A. Dempster, N. Laird, and D. Rubin. 1977. Max-
imum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society,
39B:1–38.
E. Efthimiadis and P. Biron. 1994. UCLA-Okapi at
TREC-2: Query expansion experiments. In Proc. of
the Text Retrieval Conference (TREC-2).
E. Hovy and C. Lin. 1997. Automated text summa-
rization in SUMMARIST. In Proc. of the ACL Wkshp
on Intelligent Text Summarization, pages 18–24.
F. Jelinek. 1997. Statistical methods for speech recog-
nition. MIT Press.
K. Knight and D. Marcu. 2000. Statistics-based
summarization—Step one: Sentence compression.
In Proc. ofAAAI-00. AAAI.
J. Kupiec, J. Pedersen, and F. Chen. 1995. A trainable
document summarizer. In Proc. SIGIR-95, pages
68–73, July.
Chin-Yew Lin. 1999. Training a selection function
for extraction. In Proc. of the Eighth ACM CIKM
Conference, Kansas City, MO.
I. Mani and E. Bloedorn. 1998. Machine learning of
generic and user-focused summarization. In Proc.
ofAAAI-98, pages 821–826.
J. Ponte and W. Croft. 1998. A language modeling ap-
proach to information retrieval. In Proc. of SIGIR-
98, pages 275–281.
J. Ponte. 1998. A language modelling approach to
information retrieval. Ph.D. thesis, University of
Massachusetts at Amherst.
S. Sato and M. Sato. 1998. Rewriting saves extracted
summaries. In Proc. of the AAAI Intelligent Text
Summarization Workshop, pages 76–83.
C. Stein. 1955. Inadmissibility of the usual estimator
for the mean of a multivariate normal distribution.
In Proc. of the Third Berkeley symposium on mathe-
matical statistics and probability, pages 197–206.
F. Wancho. 1990. RFC 1153: Digest message format.
M. Witbrock and V. Mittal. 1999. Headline Genera-
tion: A framework for generating highly-condensed
non-extractive summaries. In Proc. ofACMSIGIR-
99, pages 315–316.
J. Xu and B. Croft. 1996. Query expansion using lo-
cal and global document analysis. In Proc. ofACM
SIGIR-96.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.737997">
<title confidence="0.999921">Query-Relevant Summarization using FAQs</title>
<author confidence="0.999979">Adam Berger Vibhu O Mittal</author>
<affiliation confidence="0.879347">School of Computer Science Just Research Carnegie Mellon University 4616 Henry Street</affiliation>
<address confidence="0.998891">Pittsburgh, PA 15213 Pittsburgh, PA 15213</address>
<email confidence="0.996298">aberger@cs.cmu.edumittal@justresearch.com</email>
<abstract confidence="0.998678882352941">This paper introduces a statistical model for succinctly characterizing the relevance of a document to a query. Learning parameter values for the proposed model requires a large collection of summarized documents, which we do not have, but as a proxy, we use a collection of FAQ (frequently-asked question) documents. Taking a learning approach enables a principled, quantitative evaluation of the proposed system, and the results of some initial experiments—on a collection of Usenet FAQs and on a FAQ-like set of customer-submitted questions to several large retail companies—suggest the plausibility of learning for summarization.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Berger</author>
<author>J Lafferty</author>
</authors>
<title>Information retrieval as statistical translation.</title>
<date>1999</date>
<booktitle>In Proc. ofACMSIGIR-99.</booktitle>
<contexts>
<context position="27668" citStr="Berger and Lafferty, 1999" startWordPosition="4549" endWordPosition="4552">y. A candidate an(6) swer containing the term Constantinople is likely to be relevant to a question about Istanbul, but recognizing this correspondence requires a step beyond word frequency histograms. Synonymy has received much attention within the document retrieval community recently, and researchers have applied a variety of heuristic and statistical techniques—including pseudo-relevance feedback and local context analysis (Efthimiadis and Biron, 1994; Xu and Croft, 1996). Some recent work in statistical IR has extended the basic language modelling approaches to account for word synonymy (Berger and Lafferty, 1999). This paper has proposed the use of two novel datasets for summarization: the frequently-asked questions (FAQs) from Usenet archives and question/answer pairs from the call centers of retail companies. Clearly this data isn’t a perfect fit for the task of building a QRS system: after all, answers are not summaries. However, we believe that the FAQs represent a reasonable source of query-related document condensations. Furthermore, using FAQs allows us to assess the effectiveness of applying standard statistical learning machinery—maximum-likelihood estimation, the EM algorithm, and so on—to t</context>
</contexts>
<marker>Berger, Lafferty, 1999</marker>
<rawString>A. Berger and J. Lafferty. 1999. Information retrieval as statistical translation. In Proc. ofACMSIGIR-99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Berger</author>
<author>P Brown</author>
<author>S Della Pietra</author>
<author>V Della Pietra</author>
<author>J Gillett</author>
<author>J Lafferty</author>
<author>H Printz</author>
<author>L Ures</author>
</authors>
<title>The CANDIDE system for machine translation.</title>
<date>1994</date>
<booktitle>In Proc. of the ARPA Human Language Technology Workshop.</booktitle>
<contexts>
<context position="11386" citStr="Berger et al., 1994" startWordPosition="1840" endWordPosition="1843">levant to the query (exhibits a large value) and also representative of the document from which it was extracted (exhibits a large value). We now describe the parametric form of these models, and how one can determine optimal values for these parameters using maximumlikelihood estimation. 2.1 Language modeling The type of statistical model we employ for both and is a unigram probability distribution over words; in other words, a language model. Stochastic models of language have been used extensively in speech recognition, optical character recognition, and machine translation (Jelinek, 1997; Berger et al., 1994). Language models have also started to find their way into document retrieval (Ponte and Croft, 1998; Ponte, 1998). The fidelity model One simple statistical characterization of an -word document is the frequency of (1) each word in —in other words, a marginal distribution over words. That is, if word appears times in , then . This is not only intuitive, but also the maximum-likelihood estimate for . Now imagine that, when asked to summarize relative to , a person generates a summary from in the following way: Select a length for the summary according to some distribution . Figure 4: The relev</context>
</contexts>
<marker>Berger, Brown, Pietra, Pietra, Gillett, Lafferty, Printz, Ures, 1994</marker>
<rawString>A. Berger, P. Brown, S. Della Pietra, V. Della Pietra, J. Gillett, J. Lafferty, H. Printz, and L. Ures. 1994. The CANDIDE system for machine translation. In Proc. of the ARPA Human Language Technology Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Chali</author>
<author>S Matwin</author>
<author>S Szpakowicz</author>
</authors>
<title>Querybiased text summarization as a question-answering technique.</title>
<date>1999</date>
<booktitle>In Proc. of the AAAI Fall Symp. on Question Answering Systems,</booktitle>
<pages>52--56</pages>
<marker>Chali, Matwin, Szpakowicz, 1999</marker>
<rawString>Y. Chali, S. Matwin, and S. Szpakowicz. 1999. Querybiased text summarization as a question-answering technique. In Proc. of the AAAI Fall Symp. on Question Answering Systems, pages 52–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Dempster</author>
<author>N Laird</author>
<author>D Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society,</journal>
<pages>39--1</pages>
<contexts>
<context position="15471" citStr="Dempster et al., 1977" startWordPosition="2550" endWordPosition="2553">ore biased away from . : a probability distribution constructed over all documents . : the uniform distribution over all words. Figure 4 is a hierarchical depiction of the various language models which come into play in calculating . Each summary model lives at a leaf node, and the relevance of a query to that summary is a convex combination of the distributions at each node Algorithm: Shrinkage for estimation along a path from the leaf to the root2: (5) We calculate the weighting coefficients using the statistical technique known as shrinkage (Stein, 1955), a simple form of the EM algorithm (Dempster et al., 1977). As a practical matter, if one assumes the model assigns probabilities independently of , then we can drop the term when ranking candidate summaries, since the score of all candidate summaries will receive an identical contribution from the term. We make this simplifying assumption in the experiments reported in the following section. 3 Results To gauge how well our proposed summarization technique performs, we applied it to two different realworld collections of answered questions: Usenet FAQs: A collection of frequentlyasked question documents from the comp.* Usenet hierarchy. The documents</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. Dempster, N. Laird, and D. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, 39B:1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Efthimiadis</author>
<author>P Biron</author>
</authors>
<title>UCLA-Okapi at TREC-2: Query expansion experiments.</title>
<date>1994</date>
<booktitle>In Proc. of the Text Retrieval Conference (TREC-2).</booktitle>
<contexts>
<context position="27501" citStr="Efthimiadis and Biron, 1994" startWordPosition="4522" endWordPosition="4525">, not query relevant summarization. The language modelling approach described here does suffer from a common flaw within text processing systems: the problem of synonymy. A candidate an(6) swer containing the term Constantinople is likely to be relevant to a question about Istanbul, but recognizing this correspondence requires a step beyond word frequency histograms. Synonymy has received much attention within the document retrieval community recently, and researchers have applied a variety of heuristic and statistical techniques—including pseudo-relevance feedback and local context analysis (Efthimiadis and Biron, 1994; Xu and Croft, 1996). Some recent work in statistical IR has extended the basic language modelling approaches to account for word synonymy (Berger and Lafferty, 1999). This paper has proposed the use of two novel datasets for summarization: the frequently-asked questions (FAQs) from Usenet archives and question/answer pairs from the call centers of retail companies. Clearly this data isn’t a perfect fit for the task of building a QRS system: after all, answers are not summaries. However, we believe that the FAQs represent a reasonable source of query-related document condensations. Furthermor</context>
</contexts>
<marker>Efthimiadis, Biron, 1994</marker>
<rawString>E. Efthimiadis and P. Biron. 1994. UCLA-Okapi at TREC-2: Query expansion experiments. In Proc. of the Text Retrieval Conference (TREC-2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hovy</author>
<author>C Lin</author>
</authors>
<title>Automated text summarization in SUMMARIST.</title>
<date>1997</date>
<booktitle>In Proc. of the ACL Wkshp on Intelligent Text Summarization,</booktitle>
<pages>18--24</pages>
<contexts>
<context position="26314" citStr="Hovy and Lin (1997)" startWordPosition="4349" endWordPosition="4352">t to define and even more difficult to automate. Historically, a rewarding line of attack for automating language-related problems has been to take a machine learning perspective: let a computer learn how to perform the task by “watching” a human perform it many times. This is the strategy we have pursued here. There has been some work on learning a probabilistic model of summarization from text; some of the earliest work on this was due to Kupiec et al. (1995), who used a collection of manually-summarized text to learn the weights for a set of features used in a generic summarization system. Hovy and Lin (1997) present another system that learned how the position of a sentence affects its suitability for inclusion in a summary of the document. More recently, there has been work on building more complex, structured models—probabilistic syntax trees—to compress single sentences (Knight and Marcu, 2000). Mani and Bloedorn (1998) have recently proposed a method for automatically constructing decision trees to predict whether a sentence should or should not be included in a document’s summary. These previous approaches focus mainly on the generic summarization task, not query relevant summarization. The </context>
</contexts>
<marker>Hovy, Lin, 1997</marker>
<rawString>E. Hovy and C. Lin. 1997. Automated text summarization in SUMMARIST. In Proc. of the ACL Wkshp on Intelligent Text Summarization, pages 18–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
</authors>
<title>Statistical methods for speech recognition.</title>
<date>1997</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="11364" citStr="Jelinek, 1997" startWordPosition="1838" endWordPosition="1839">o a query is relevant to the query (exhibits a large value) and also representative of the document from which it was extracted (exhibits a large value). We now describe the parametric form of these models, and how one can determine optimal values for these parameters using maximumlikelihood estimation. 2.1 Language modeling The type of statistical model we employ for both and is a unigram probability distribution over words; in other words, a language model. Stochastic models of language have been used extensively in speech recognition, optical character recognition, and machine translation (Jelinek, 1997; Berger et al., 1994). Language models have also started to find their way into document retrieval (Ponte and Croft, 1998; Ponte, 1998). The fidelity model One simple statistical characterization of an -word document is the frequency of (1) each word in —in other words, a marginal distribution over words. That is, if word appears times in , then . This is not only intuitive, but also the maximum-likelihood estimate for . Now imagine that, when asked to summarize relative to , a person generates a summary from in the following way: Select a length for the summary according to some distribution</context>
</contexts>
<marker>Jelinek, 1997</marker>
<rawString>F. Jelinek. 1997. Statistical methods for speech recognition. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
<author>D Marcu</author>
</authors>
<title>Statistics-based summarization—Step one: Sentence compression.</title>
<date>2000</date>
<booktitle>In Proc. ofAAAI-00.</booktitle>
<publisher>AAAI.</publisher>
<contexts>
<context position="26609" citStr="Knight and Marcu, 2000" startWordPosition="4393" endWordPosition="4396">we have pursued here. There has been some work on learning a probabilistic model of summarization from text; some of the earliest work on this was due to Kupiec et al. (1995), who used a collection of manually-summarized text to learn the weights for a set of features used in a generic summarization system. Hovy and Lin (1997) present another system that learned how the position of a sentence affects its suitability for inclusion in a summary of the document. More recently, there has been work on building more complex, structured models—probabilistic syntax trees—to compress single sentences (Knight and Marcu, 2000). Mani and Bloedorn (1998) have recently proposed a method for automatically constructing decision trees to predict whether a sentence should or should not be included in a document’s summary. These previous approaches focus mainly on the generic summarization task, not query relevant summarization. The language modelling approach described here does suffer from a common flaw within text processing systems: the problem of synonymy. A candidate an(6) swer containing the term Constantinople is likely to be relevant to a question about Istanbul, but recognizing this correspondence requires a step</context>
</contexts>
<marker>Knight, Marcu, 2000</marker>
<rawString>K. Knight and D. Marcu. 2000. Statistics-based summarization—Step one: Sentence compression. In Proc. ofAAAI-00. AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kupiec</author>
<author>J Pedersen</author>
<author>F Chen</author>
</authors>
<title>A trainable document summarizer.</title>
<date>1995</date>
<booktitle>In Proc. SIGIR-95,</booktitle>
<pages>68--73</pages>
<contexts>
<context position="26160" citStr="Kupiec et al. (1995)" startWordPosition="4323" endWordPosition="4326"> to translate in practice into quite different statistical models and training procedures in the two cases. 5 Summary The task of summarization is difficult to define and even more difficult to automate. Historically, a rewarding line of attack for automating language-related problems has been to take a machine learning perspective: let a computer learn how to perform the task by “watching” a human perform it many times. This is the strategy we have pursued here. There has been some work on learning a probabilistic model of summarization from text; some of the earliest work on this was due to Kupiec et al. (1995), who used a collection of manually-summarized text to learn the weights for a set of features used in a generic summarization system. Hovy and Lin (1997) present another system that learned how the position of a sentence affects its suitability for inclusion in a summary of the document. More recently, there has been work on building more complex, structured models—probabilistic syntax trees—to compress single sentences (Knight and Marcu, 2000). Mani and Bloedorn (1998) have recently proposed a method for automatically constructing decision trees to predict whether a sentence should or should</context>
</contexts>
<marker>Kupiec, Pedersen, Chen, 1995</marker>
<rawString>J. Kupiec, J. Pedersen, and F. Chen. 1995. A trainable document summarizer. In Proc. SIGIR-95, pages 68–73, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>Training a selection function for extraction.</title>
<date>1999</date>
<booktitle>In Proc. of the Eighth ACM CIKM Conference,</booktitle>
<location>Kansas City, MO.</location>
<contexts>
<context position="8178" citStr="Lin, 1999" startWordPosition="1313" endWordPosition="1314">-structured documents, so the task of extracting the constituent parts (queries and answers) is amenable to automation. There have even been proposals for standardized FAQ formats, such as RFC1153 and the Minimal Digest Format (Wancho, 1990). Usenet FAQs cover an astonishingly wide variety of topics, ranging from extraterrestrial visitors to mutual-fund investing. If there’s an online community of people with a common interest, there’s likely to be a Usenet FAQ on that subject. There has been a small amount of published work involving question/answer data, including (Sato and Sato, 1998) and (Lin, 1999). Sato and Sato used FAQs as a source of summarization corpora, although in quite a different context than that presented here. Lin used the datasets from a question/answer task within the Tipster project, a dataset of considerably smaller size than the FAQs we employ. Neither of these paper focused on a statistical machine learning approach to summarization. 2 A probabilistic model of summarization Given a query and document , the query-relevant summarization task is to find the a posteriori most probable summary for . Using Bayes’ rule, we can rewrite this expression as relevance fidelity wh</context>
</contexts>
<marker>Lin, 1999</marker>
<rawString>Chin-Yew Lin. 1999. Training a selection function for extraction. In Proc. of the Eighth ACM CIKM Conference, Kansas City, MO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mani</author>
<author>E Bloedorn</author>
</authors>
<title>Machine learning of generic and user-focused summarization.</title>
<date>1998</date>
<booktitle>In Proc. ofAAAI-98,</booktitle>
<pages>821--826</pages>
<contexts>
<context position="1379" citStr="Mani and Bloedorn, 1998" startWordPosition="194" endWordPosition="197"> Usenet FAQs and on a FAQ-like set of customer-submitted questions to several large retail companies—suggest the plausibility of learning for summarization. 1 Introduction An important distinction in document summarization is between generic summaries, which capture the central ideas of the document in much the same way that the abstract of this paper was designed to distill its salient points, and query-relevant summaries, which reflect the relevance of a document to a user-specified query. This paper discusses query-relevant summarization, sometimes also called “user-focused summarization” (Mani and Bloedorn, 1998). Query-relevant summaries are especially important in the “needle(s) in a haystack” document retrieval problem: a user has an information need expressed as a query (What countries export smoked salmon?), and a retrieval system must locate within a large collection of documents those documents most likely to fulfill this need. Many interactive retrieval systems—web search engines like Altavista, for instance—present the user with a small set of candidate relevant documents, each summarized; the user must then perform a kind of triage to identify likely relevant documents from this set. The web</context>
<context position="26635" citStr="Mani and Bloedorn (1998)" startWordPosition="4397" endWordPosition="4400">re has been some work on learning a probabilistic model of summarization from text; some of the earliest work on this was due to Kupiec et al. (1995), who used a collection of manually-summarized text to learn the weights for a set of features used in a generic summarization system. Hovy and Lin (1997) present another system that learned how the position of a sentence affects its suitability for inclusion in a summary of the document. More recently, there has been work on building more complex, structured models—probabilistic syntax trees—to compress single sentences (Knight and Marcu, 2000). Mani and Bloedorn (1998) have recently proposed a method for automatically constructing decision trees to predict whether a sentence should or should not be included in a document’s summary. These previous approaches focus mainly on the generic summarization task, not query relevant summarization. The language modelling approach described here does suffer from a common flaw within text processing systems: the problem of synonymy. A candidate an(6) swer containing the term Constantinople is likely to be relevant to a question about Istanbul, but recognizing this correspondence requires a step beyond word frequency his</context>
</contexts>
<marker>Mani, Bloedorn, 1998</marker>
<rawString>I. Mani and E. Bloedorn. 1998. Machine learning of generic and user-focused summarization. In Proc. ofAAAI-98, pages 821–826.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ponte</author>
<author>W Croft</author>
</authors>
<title>A language modeling approach to information retrieval.</title>
<date>1998</date>
<booktitle>In Proc. of SIGIR98,</booktitle>
<pages>275--281</pages>
<contexts>
<context position="11486" citStr="Ponte and Croft, 1998" startWordPosition="1856" endWordPosition="1859">was extracted (exhibits a large value). We now describe the parametric form of these models, and how one can determine optimal values for these parameters using maximumlikelihood estimation. 2.1 Language modeling The type of statistical model we employ for both and is a unigram probability distribution over words; in other words, a language model. Stochastic models of language have been used extensively in speech recognition, optical character recognition, and machine translation (Jelinek, 1997; Berger et al., 1994). Language models have also started to find their way into document retrieval (Ponte and Croft, 1998; Ponte, 1998). The fidelity model One simple statistical characterization of an -word document is the frequency of (1) each word in —in other words, a marginal distribution over words. That is, if word appears times in , then . This is not only intuitive, but also the maximum-likelihood estimate for . Now imagine that, when asked to summarize relative to , a person generates a summary from in the following way: Select a length for the summary according to some distribution . Figure 4: The relevance of a query to the th answer in document is a convex combination of five distributions: (1) a un</context>
</contexts>
<marker>Ponte, Croft, 1998</marker>
<rawString>J. Ponte and W. Croft. 1998. A language modeling approach to information retrieval. In Proc. of SIGIR98, pages 275–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ponte</author>
</authors>
<title>A language modelling approach to information retrieval.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Massachusetts at Amherst.</institution>
<contexts>
<context position="11500" citStr="Ponte, 1998" startWordPosition="1860" endWordPosition="1861"> a large value). We now describe the parametric form of these models, and how one can determine optimal values for these parameters using maximumlikelihood estimation. 2.1 Language modeling The type of statistical model we employ for both and is a unigram probability distribution over words; in other words, a language model. Stochastic models of language have been used extensively in speech recognition, optical character recognition, and machine translation (Jelinek, 1997; Berger et al., 1994). Language models have also started to find their way into document retrieval (Ponte and Croft, 1998; Ponte, 1998). The fidelity model One simple statistical characterization of an -word document is the frequency of (1) each word in —in other words, a marginal distribution over words. That is, if word appears times in , then . This is not only intuitive, but also the maximum-likelihood estimate for . Now imagine that, when asked to summarize relative to , a person generates a summary from in the following way: Select a length for the summary according to some distribution . Figure 4: The relevance of a query to the th answer in document is a convex combination of five distributions: (1) a uniform model . </context>
<context position="20767" citStr="Ponte, 1998" startWordPosition="3440" endWordPosition="3441">ding breakdown for the call-center data. These weights were calculated using shrinkage. it was drawn—is not so important. From a set of rankings , one can measure the the quality of a ranking algorithm using the harmonic mean rank: A lower number indicates better performance; , which is optimal, means that the algorithm consistently assigns the first rank to the correct answer. Table 1 shows the harmonic mean rank on the two collections. The third column of Table 1 shows the result of a QRS system using a uniform fidelity model, the fourth corresponds to a standard tfidf-based ranking method (Ponte, 1998), and the last column reflects the performance of randomly guessing the correct summary from all answers in the document. trial # trials LM tfidf random Usenet 1 554 1.41 2.29 4.20 FAQ 2 549 1.38 2.42 4.25 data 3 535 1.40 2.30 4.19 Call 1 1020 4.8 38.7 1335 center 2 1055 4.0 22.6 1335 data 3 1037 4.2 26.0 1321 Table 1: Performance of query-relevant extractive summarization on the Usenet and call-center datasets. The numbers reported in the three rightmost columns are harmonic mean ranks: lower is better. 4 Extensions 4.1 Question-answering The reader may by now have realized that our approach </context>
</contexts>
<marker>Ponte, 1998</marker>
<rawString>J. Ponte. 1998. A language modelling approach to information retrieval. Ph.D. thesis, University of Massachusetts at Amherst.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sato</author>
<author>M Sato</author>
</authors>
<title>Rewriting saves extracted summaries.</title>
<date>1998</date>
<booktitle>In Proc. of the AAAI Intelligent Text Summarization Workshop,</booktitle>
<pages>76--83</pages>
<contexts>
<context position="8162" citStr="Sato and Sato, 1998" startWordPosition="1308" endWordPosition="1311">s. FAQs are generally well-structured documents, so the task of extracting the constituent parts (queries and answers) is amenable to automation. There have even been proposals for standardized FAQ formats, such as RFC1153 and the Minimal Digest Format (Wancho, 1990). Usenet FAQs cover an astonishingly wide variety of topics, ranging from extraterrestrial visitors to mutual-fund investing. If there’s an online community of people with a common interest, there’s likely to be a Usenet FAQ on that subject. There has been a small amount of published work involving question/answer data, including (Sato and Sato, 1998) and (Lin, 1999). Sato and Sato used FAQs as a source of summarization corpora, although in quite a different context than that presented here. Lin used the datasets from a question/answer task within the Tipster project, a dataset of considerably smaller size than the FAQs we employ. Neither of these paper focused on a statistical machine learning approach to summarization. 2 A probabilistic model of summarization Given a query and document , the query-relevant summarization task is to find the a posteriori most probable summary for . Using Bayes’ rule, we can rewrite this expression as relev</context>
</contexts>
<marker>Sato, Sato, 1998</marker>
<rawString>S. Sato and M. Sato. 1998. Rewriting saves extracted summaries. In Proc. of the AAAI Intelligent Text Summarization Workshop, pages 76–83.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Stein</author>
</authors>
<title>Inadmissibility of the usual estimator for the mean of a multivariate normal distribution.</title>
<date>1955</date>
<booktitle>In Proc. of the Third Berkeley symposium on mathematical statistics and probability,</booktitle>
<pages>197--206</pages>
<contexts>
<context position="5335" citStr="Stein, 1955" startWordPosition="847" endWordPosition="848"> part of the document to extract. We could, in principle, ask a large number of people to perform the same task. Doing so would impose a distribution over candidate summaries. Under the second, or “frequentist” interpretation, is the fraction of people who would select —equivalently, the probability that a person selected at random would prefer as the summary. The statistical model is parametric, the values of which are learned by inspection of the triplets. The learning process involves maximum-likelihood estimation of probabilistic language models and the statistical technique of shrinkage (Stein, 1955). This probabilistic approach easily generalizes to the generic summarization setting, where there is no query. In that case, the training data consists of pairs, where is a summary of the document . The goal, in this case, is to learn and apply a mapping from documents to summaries. That is, Figure 3: FAQs consist of a list of questions and answers on a single topic; the FAQ depicted here is part of an informational document on amniocentesis. This paper views answers in a FAQ as different summaries of the FAQ: the answer to the th question is a summary of the FAQ relative to that question. fi</context>
<context position="15412" citStr="Stein, 1955" startWordPosition="2541" endWordPosition="2542">model has even less variance than , but is even more biased away from . : a probability distribution constructed over all documents . : the uniform distribution over all words. Figure 4 is a hierarchical depiction of the various language models which come into play in calculating . Each summary model lives at a leaf node, and the relevance of a query to that summary is a convex combination of the distributions at each node Algorithm: Shrinkage for estimation along a path from the leaf to the root2: (5) We calculate the weighting coefficients using the statistical technique known as shrinkage (Stein, 1955), a simple form of the EM algorithm (Dempster et al., 1977). As a practical matter, if one assumes the model assigns probabilities independently of , then we can drop the term when ranking candidate summaries, since the score of all candidate summaries will receive an identical contribution from the term. We make this simplifying assumption in the experiments reported in the following section. 3 Results To gauge how well our proposed summarization technique performs, we applied it to two different realworld collections of answered questions: Usenet FAQs: A collection of frequentlyasked questio</context>
</contexts>
<marker>Stein, 1955</marker>
<rawString>C. Stein. 1955. Inadmissibility of the usual estimator for the mean of a multivariate normal distribution. In Proc. of the Third Berkeley symposium on mathematical statistics and probability, pages 197–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Wancho</author>
</authors>
<title>RFC 1153: Digest message</title>
<date>1990</date>
<booktitle>In Proc. ofACMSIGIR99,</booktitle>
<pages>315--316</pages>
<contexts>
<context position="7809" citStr="Wancho, 1990" startWordPosition="1255" endWordPosition="1256">se of learning the parameters of a statistical QRS model. FAQ data has some properties that make it particularly attractive for text learning: def There exist a large number of Usenet FAQs— several thousand documents—publicly available on the Web1. Moreover, many large companies maintain their own FAQs to streamline the customer-response process. FAQs are generally well-structured documents, so the task of extracting the constituent parts (queries and answers) is amenable to automation. There have even been proposals for standardized FAQ formats, such as RFC1153 and the Minimal Digest Format (Wancho, 1990). Usenet FAQs cover an astonishingly wide variety of topics, ranging from extraterrestrial visitors to mutual-fund investing. If there’s an online community of people with a common interest, there’s likely to be a Usenet FAQ on that subject. There has been a small amount of published work involving question/answer data, including (Sato and Sato, 1998) and (Lin, 1999). Sato and Sato used FAQs as a source of summarization corpora, although in quite a different context than that presented here. Lin used the datasets from a question/answer task within the Tipster project, a dataset of considerably</context>
</contexts>
<marker>Wancho, 1990</marker>
<rawString>F. Wancho. 1990. RFC 1153: Digest message format. M. Witbrock and V. Mittal. 1999. Headline Generation: A framework for generating highly-condensed non-extractive summaries. In Proc. ofACMSIGIR99, pages 315–316.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Xu</author>
<author>B Croft</author>
</authors>
<title>Query expansion using local and global document analysis.</title>
<date>1996</date>
<booktitle>In Proc. ofACM SIGIR-96.</booktitle>
<contexts>
<context position="27522" citStr="Xu and Croft, 1996" startWordPosition="4526" endWordPosition="4529">ation. The language modelling approach described here does suffer from a common flaw within text processing systems: the problem of synonymy. A candidate an(6) swer containing the term Constantinople is likely to be relevant to a question about Istanbul, but recognizing this correspondence requires a step beyond word frequency histograms. Synonymy has received much attention within the document retrieval community recently, and researchers have applied a variety of heuristic and statistical techniques—including pseudo-relevance feedback and local context analysis (Efthimiadis and Biron, 1994; Xu and Croft, 1996). Some recent work in statistical IR has extended the basic language modelling approaches to account for word synonymy (Berger and Lafferty, 1999). This paper has proposed the use of two novel datasets for summarization: the frequently-asked questions (FAQs) from Usenet archives and question/answer pairs from the call centers of retail companies. Clearly this data isn’t a perfect fit for the task of building a QRS system: after all, answers are not summaries. However, we believe that the FAQs represent a reasonable source of query-related document condensations. Furthermore, using FAQs allows </context>
</contexts>
<marker>Xu, Croft, 1996</marker>
<rawString>J. Xu and B. Croft. 1996. Query expansion using local and global document analysis. In Proc. ofACM SIGIR-96.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>