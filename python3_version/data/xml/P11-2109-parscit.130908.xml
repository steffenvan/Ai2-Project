<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.994963">
Generalized Interpolation in Decision Tree LM
</title>
<author confidence="0.978567">
Denis Filimonov†$
</author>
<affiliation confidence="0.93535">
$Human Language Technology
Center of Excellence
Johns Hopkins University
</affiliation>
<email confidence="0.676532">
den@cs.umd.edu
</email>
<affiliation confidence="0.9821255">
Mary Harper††Department of Computer Science
University of Maryland, College Park
</affiliation>
<email confidence="0.997345">
mharper@umd.edu
</email>
<sectionHeader confidence="0.996651" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9990456">
In the face of sparsity, statistical models are
often interpolated with lower order (backoff)
models, particularly in Language Modeling.
In this paper, we argue that there is a rela-
tion between the higher order and the backoff
model that must be satisfied in order for the
interpolation to be effective. We show that in
n-gram models, the relation is trivially held,
but in models that allow arbitrary clustering
of context (such as decision tree models), this
relation is generally not satisfied. Based on
this insight, we also propose a generalization
of linear interpolation which significantly im-
proves the performance of a decision tree lan-
guage model.
</bodyText>
<sectionHeader confidence="0.998779" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999912">
A prominent use case for Language Models (LMs)
in NLP applications such as Automatic Speech
Recognition (ASR) and Machine Translation (MT)
is selection of the most fluent word sequence among
multiple hypotheses. Statistical LMs formulate the
problem as the computation of the model’s proba-
bility to generate the word sequence w1w2 ... wm �
wm1 , assuming that higher probability corresponds to
more fluent hypotheses. LMs are often represented
in the following generative form:
</bodyText>
<footnote confidence="0.5945906">
p(wi|wi−1
1 )
In the following discussion, we will refer to the func-
tion p(wi|wi−1
1 ) as a language model.
</footnote>
<page confidence="0.931644">
620
</page>
<bodyText confidence="0.669824">
Note the context space for this function, wi−1
</bodyText>
<equation confidence="0.497925">
1
</equation>
<bodyText confidence="0.99146575">
is arbitrarily long, necessitating some independence
assumption, which usually consists of reducing the
relevant context to n − 1 immediately preceding to-
kens:
</bodyText>
<equation confidence="0.979875666666667">
p(wi|wi−1
1 ) p(wi|wi−1
i−n+1)
</equation>
<bodyText confidence="0.999897714285714">
These distributions are typically estimated from ob-
served counts of n-grams wii−n+1 in the training
data. The context space is still far too large; there-
fore, the models are recursively smoothed using
lower order distributions. For instance, in a widely
used n-gram LM, the probabilities are estimated as
follows:
</bodyText>
<equation confidence="0.9978815">
˜p(wi|wi−1
i−n+1) = �(wi|wi−1
i−n+1) + (1)
&apos;Y(wi−n+1) - ˜p(wi|wi−n+2)
</equation>
<bodyText confidence="0.989191">
where p is a discounted probability1.
In addition to n-gram models, there are many
other ways to estimate probability distributions
p(wi|wi−1
i−n+1); in this work, we are particularly in-
terested in models involving decision trees (DTs).
As in n-gram models, DT models also often uti-
lize interpolation with lower order models; however,
there are issues concerning the interpolation which
arise from the fact that decision trees permit arbi-
trary clustering of context, and these issues are the
main subject of this paper.
</bodyText>
<footnote confidence="0.56871675">
1We refer the reader to (Chen and Goodman, 1999) for a
survey of the discounting methods for n-gram models.
p(wm1 ) = �m
i=1
</footnote>
<note confidence="0.670969">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 620–624,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.978755" genericHeader="method">
2 Decision Trees
</sectionHeader>
<bodyText confidence="0.998585285714286">
The vast context space in a language model man-
dates the use of context clustering in some form. In
n-gram models, the clustering can be represented as
a k-ary decision tree of depth n − 1, where k is the
size of the vocabulary. Note that this is a very con-
strained form of a decision tree, and is probably sub-
optimal. Indeed, it is likely that some of the clusters
predict very similar distributions of words, and the
model would benefit from merging them. Therefore,
it is reasonable to believe that arbitrary (i.e., uncon-
strained) context clustering such as a decision tree
should be able to outperform the n-gram model.
A decision tree provides us with a clustering func-
tion �(wi−1
</bodyText>
<equation confidence="0.714964142857143">
i−n+1) _* {b1, ... , .b�&apos;}, where N is the
number of clusters (leaves in the DT), and clusters
4bk are disjoint subsets of the context space; the
probability estimation is approximated as follows:
p(wi|wi−1
i−n+1) �p(wi|�(wi−1
i−n+1)) (2)
</equation>
<bodyText confidence="0.9530389375">
Methods of DT construction and probability estima-
tion used in this work are based on (Filimonov and
Harper, 2009); therefore, we refer the reader to that
paper for details.
Another advantage of using decision trees is the
ease of adding parameters such as syntactic tags:
In this case, the decision tree would cluster the con-
text space wi−1
i−n+1ti−1
i−n+1 based on information the-
oretic metrics, without utilizing heuristics for which
order the context attributes are to be backed off (cf.
Eq. 1). In subsequent discussion, we will write
equations for word models (Eq. 2), but they are
equally applicable to joint models (Eq. 3) with trivial
transformations.
</bodyText>
<sectionHeader confidence="0.97813" genericHeader="method">
3 Backoff Property
</sectionHeader>
<bodyText confidence="0.9882125">
Let us rewrite the interpolation Eq. 1 in a more
generic way:
</bodyText>
<equation confidence="0.993992">
�p(wi|wi−1
1 ) = �n(wi|�n(wi−1
1 )) + (4)
�(�n(wi−1
1 )) &apos; �p(wi|BOn−1(wi−1
1 ))
</equation>
<bodyText confidence="0.982367714285714">
where, Pn is a discounted distribution, 4bn is a clus-
tering function of order n, and �(�n(wi−1
1 )) is the
backoff weight chosen to normalize the distribution.
BOn−1 is the backoff clustering function of order
n − 1, representing a reduction of context size. In
the case of an n-gram model, �n(wi−1
</bodyText>
<equation confidence="0.8403892">
1 ) is the set
of word sequences where the last n − 1 words are
wi−1
i−n+1, similarly, BOn−1(wi−1
1 ) is the set of se-
</equation>
<bodyText confidence="0.9272352">
quences ending with wi−1
i−n+2. In the case of a de-
cision tree model, the same backoff function is typ-
ically used, but the clustering function can be arbi-
trary.
The intuition behind Eq. 4 is that the backoff con-
text BOn−1(wi−1
1 ) allows for more robust (but less
informed) probability estimation than the context
cluster �n(wi−1
</bodyText>
<equation confidence="0.878053666666667">
1 ). More precisely:
bwi−1 1�, : W E �n(wi−1
1 ) � W E BOn−1(wi−1
</equation>
<listItem confidence="0.672572333333333">
1 )
(5)
that is, every word sequence W that belongs to a
context cluster �n(wi−1
1 ), belongs to the same back-
off cluster BOn−1(wi−1
</listItem>
<bodyText confidence="0.973985827586207">
1 ) (hence has the same back-
off distribution). For n-gram models, Property 5
trivially holds since BOn−1(wi−1
1 ) and 4bn(wi−1
1 )
are defined as sets of sequences ending with wi−1
i−n+2
and wi−1
i−n+1 with the former clearly being a superset
of the latter. However, when 4b can be arbitrary, e.g.,
a decision tree, that is not necessarily so.
Let us consider what happens when we have
two context sequences W and W&apos; that belong to
the same cluster 4bn(W) = 4bn(W&apos;) but differ-
ent backoff clusters BOn−1(W) =� BOn−1(W&apos;).
For example: suppose we have 4b(wi−2wi−1) =
({on}, {may,june}) and two corresponding backoff
clusters: BO&apos; = ({may}) and BO&apos;&apos; = ({june}).
Following on, the word may is likely to be a month
rather than a modal verb, although the latter is
more frequent and will dominate in BO&apos;. There-
fore we have much less faith in p(wi|BO&apos;) than in
p(wi|BO&apos;&apos;) and would like a much smaller weight -y
assigned to BO&apos;, but it is not possible in the back-
off scheme in Eq. 4, thus we will have to settle on a
compromise value of &apos;y, resulting in suboptimal per-
formance.
We would expect this effect to be more pro-
nounced in higher order models, because viola-
</bodyText>
<equation confidence="0.993609590909091">
p(w1m tm1 ) = E
m
ri
t1...t- i=1
m
ri
E�
t1 ... t-
i−1 i−1
p(witi|�(wi−n+1ti−n+1)) (3)
E
p(wm 1 ) =
t1...t-
p(witi|wi−1
1 ti−1
1 )
621
˜pn(wi|wi−1
i−n+1) =
n
E
m=1
</equation>
<bodyText confidence="0.999928611111111">
tions of Property 5 are less frequent in lower or-
der models. Indeed, in a 2-gram model, the
property is never violated since its backoff, un-
igram, contains the entire context in one clus-
ter. The 3-gram example above, Φ(wi−2wi−1) =
({on}, {may,june}), although illustrative, is not
likely to occur because may in wi−1 position will
likely be split from june very early on, since it is
very informative about the following word. How-
ever, in a 4-gram model, Φ(wi−3wi−2wi−1) =
({on}, {may,june}, {&lt;unk&gt;}) is quite plausible.
Thus, arbitrary clustering (an advantage of DTs)
leads to violation of Property 5, which, we argue,
may lead to a degradation of performance if back-
off interpolation Eq. 4 is used. In the next section,
we generalize the interpolation scheme which, as we
show in Section 6, allows us to find a better solution
in the face of the violation of Property 5.
</bodyText>
<sectionHeader confidence="0.991702" genericHeader="method">
4 Linear Interpolation
</sectionHeader>
<bodyText confidence="0.988939">
We use linear interpolation as the baseline, rep-
resented recursively, which is similar to Jelinek-
Mercer smoothing for n-gram models (Jelinek and
Mercer, 1980):
</bodyText>
<equation confidence="0.99806">
�pn(wi|wi−1
i−n+1) = An(On) &apos; pn(wi|On) + (6)
(1 − An(On)) &apos; �pn−1(wi|wi−1
i−n+2)
</equation>
<bodyText confidence="0.996184444444445">
where φn = Φn(wi−1
i−n+1), and λn(φn) E [0, 1] are
assigned to each cluster and are optimized on a held-
out set using EM. pn(wi|φn) is the probability dis-
tribution at the cluster φn in the tree of order n. This
interpolation method is particularly useful as, un-
like count-based discounting methods (e.g., Kneser-
Ney), it can be applied to already smooth distribu-
tions pn2.
</bodyText>
<sectionHeader confidence="0.995754" genericHeader="method">
5 Generalized Interpolation
</sectionHeader>
<bodyText confidence="0.994858">
We can unwind the recursion in Eq. 6 and make sub-
stitutions:
</bodyText>
<equation confidence="0.9995215">
λn(φn) — ˆλn(φn)
(1 — λn(φn)) &apos; λn−1(φn−1) — ˆλn−1(φn−1)
</equation>
<bodyText confidence="0.494652">
...
</bodyText>
<footnote confidence="0.941714666666667">
2In decision trees, the distribution at a cluster (leaf) is often
recursively interpolated with its parent node, e.g. (Bahl et al.,
1990; Heeman, 1999; Filimonov and Harper, 2009).
</footnote>
<equation confidence="0.997064333333333">
ˆλm(φm) &apos; pm(wi|φm) (7)
En ˆλm(φm) = 1
m=1
</equation>
<bodyText confidence="0.999879727272727">
Note that in this parameterization, the weight as-
signed to pn−1(wi|φn−1) is limited by (1—λn(φn)),
i.e., the weight assigned to the higher order model.
Ideally we should be able to assign a different set
of interpolation weights for every eligible combina-
tion of clusters φn, φn−1, . . . , φ1. However, not only
is the number of such combinations extremely large,
but many of them will not be observed in the train-
ing data, making parameter estimation cumbersome.
Therefore, we propose the following parameteriza-
tion for the interpolation of decision tree models:
</bodyText>
<equation confidence="0.981834">
n
pn(wi |wi−n+1) Em=1E m( 1 λ (φm(wi|φm) (8)
</equation>
<bodyText confidence="0.999976866666667">
Note that this parameterization has the same num-
ber of parameters as in Eq. 7 (one per cluster in ev-
ery tree), but the number of degrees of freedom is
larger because the the parameters are not constrained
to sum to 1, hence the denominator.
In Eq. 8, there is no explicit distinction between
higher order and backoff models. Indeed, it ac-
knowledges that lower order models are not backoff
models when Property 5 is not satisfied. However,
it can be shown that Eq. 8 reduces to Eq. 6 if Prop-
erty 5 holds. Therefore, the new parameterization
can be thought of as a generalization of linear inter-
polation. Indeed, suppose we have the parameteri-
zation in Eq. 8 and Property 5. Let us transform this
parameterization into Eq. 7 by induction. We define:
</bodyText>
<equation confidence="0.9953005">
Λm = �m λk ; Λm = λm + Λm−1
k=1
</equation>
<bodyText confidence="0.918028">
where, due to space limitation, we redefine λm =
</bodyText>
<equation confidence="0.627611">
λm(φm) and Λm = Λm(φm); φm = Φm(wi−1
1 ),
i.e., the cluster of model order m, to which the se-
quence wi−1
1 belongs. The lowest order distribution
p1 is not interpolated with anything, hence:
Λ1˜p1(wi|φ1) = λ1p1(wi|φ1)
</equation>
<bodyText confidence="0.879049">
Now the induction step. From Property 5, it follows
that φm C φm−1, thus, for all sequences in bwn1 E
</bodyText>
<page confidence="0.993051">
622
</page>
<table confidence="0.9516384">
n-gram DT: Eq. 6 (baseline) DT: Eq. 8 (generalized)
order Jelinek-Mercer Mod KN word-tree syntactic word-tree syntactic
2-gram 270.2 261.0 257.8 214.3 258.1 214.6
3-gram 186.5 (31.0%) 174.3 (33.2%) 168.7 (34.6%) 156.8 (26.8%) 168.4 (34.8%) 155.3 (27.6%)
4-gram 177.1 (5.0%) 161.7 (7.2%) 164.0 (2.8%) 156.5 (0.2%) 155.7 (7.5%) 147.1 (5.3%)
</table>
<tableCaption confidence="0.992824666666667">
Table 1: Perplexity results on PTB WSJ section 23. Percentage numbers in parentheses denote the reduction of per-
plexity relative to the lower order model of the same type. “Word-tree” and “syntactic” refer to DT models estimated
using words only (Eq. 2) and words and tags jointly (Eq. 3).
</tableCaption>
<bodyText confidence="0.838521">
φm, we have the same distribution:
</bodyText>
<equation confidence="0.989932">
λmpm(wi|φm) + Am−1�pm−1(wi|φm−1) =
= Am 1 Ampm(wi |φm) + Am-,
-,Pm−1(wi |φm−1)/
( �
= Am �λmpm(wi|φm) + (1 − �λm)�pm−1(wi|φm−1)
λm
m Am
</equation>
<bodyText confidence="0.9980628">
Note that the last transformation is because φm C
φm�1; had it not been the case, �pm would depend on
the combination of φm and φm_1 and require multi-
ple parameters to be represented on its entire domain
wn1 E φm. After n iterations, we have:
</bodyText>
<equation confidence="0.959732">
n
E λm(φm)pm(wi|φm) = An�pn(wi|φn); (cf. Eq. 8)
m=1
</equation>
<bodyText confidence="0.9999906">
Thus, we have constructed �pn(wi|φn) using the
same recursive representation as in Eq. 6, which
proves that the standard linear interpolation is a spe-
cial case of the new interpolation scheme, which oc-
curs when the backoff Property 5 holds.
</bodyText>
<sectionHeader confidence="0.999788" genericHeader="evaluation">
6 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999800085714286">
Models are trained on 35M words of WSJ 94-96
from LDC2008T13. The text was converted into
speech-like form, namely numbers and abbrevia-
tions were verbalized, text was downcased, punc-
tuation was removed, and contractions and posses-
sives were joined with the previous word (i.e., they
’ll becomes they’ll). For syntactic modeling, we
used tags comprised of POS tags of the word and its
head, as in (Filimonov and Harper, 2009). Parsing
of the text for tag extraction occurred after verbal-
ization of numbers and abbreviations but before any
further processing; we used an appropriately trained
latent variable PCFG parser (Huang and Harper,
2009). For reference, we include n-gram models
with Jelinek-Mercer and modified interpolated KN
discounting. All models use the same vocabulary of
approximately 50k words.
We implemented four decision tree models3: two
using the interpolation method of (Eq. 6) and two
based on the generalized interpolation (Eq. 8). Pa-
rameters λ were estimated using the L-BFGS to
minimize the entropy on a heldout set. In order to
eliminate the influence of all factors other than the
interpolation, we used the same decision trees. The
perplexity results on WSJ section 23 are presented in
Table 1. As we have predicted, the effect of the new
interpolation becomes apparent at the 4-gram order,
when Property 5 is most frequently violated. Note
that we observe similar patterns for both word-tree
and syntactic models, with syntactic models outper-
forming their word-tree counterparts.
We believe that (Xu and Jelinek, 2004) also suf-
fers from violation of Property 5, however, since
they use a heuristic method4 to set backoff weights,
it is difficult to ascertain the extent.
</bodyText>
<sectionHeader confidence="0.998986" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999811333333333">
The main contribution of this paper is the insight
that in the standard recursive backoff there is an im-
plied relation between the backoff and the higher or-
der models, which is essential for adequate perfor-
mance. When this relation is not satisfied other in-
terpolation methods should be employed; hence, we
propose a generalization of linear interpolation that
significantly outperforms the standard form in such
a scenario.
</bodyText>
<footnote confidence="0.956135">
3We refer the reader to (Filimonov and Harper, 2009) for
details on the tree construction algorithm.
4The higher order model was discounted according to KN
discounting, while the lower order model could be either a lower
order DT (forest) model, or a standard n-gram model, with the
former performing slightly better.
</footnote>
<equation confidence="0.911272">
= Ampm(wi|φm) ;
</equation>
<page confidence="0.998761">
623
</page>
<sectionHeader confidence="0.993874" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999885041666667">
Lalit R. Bahl, Peter F. Brown, Peter V. de Souza, and
Robert L. Mercer. 1990. A tree-based statistical lan-
guage model for natural language speech recognition.
Readings in speech recognition, pages 507–514.
Stanley F. Chen and Joshua Goodman. 1999. An empir-
ical study of smoothing techniques for language mod-
eling. Computer Speech &amp;Language, 13(4):359–393.
Denis Filimonov and Mary Harper. 2009. A joint lan-
guage model with fine-grain syntactic tags. In Pro-
ceedings of the EMNLP.
Peter A. Heeman. 1999. POS tags and decision trees
for language modeling. In Proceedings of the Joint
SIGDAT Conference on Empirical Methods in Natural
Language Processing and Very Large Corpora, pages
129–137.
Zhongqiang Huang and Mary Harper. 2009. Self-
Training PCFG grammars with latent annotations
across languages. In Proceedings of the EMNLP 2009.
Frederick Jelinek and Robert L. Mercer. 1980. Inter-
polated estimation of markov source parameters from
sparse data. In Proceedings of the Workshop on Pat-
tern Recognition in Practice, pages 381–397.
Peng Xu and Frederick Jelinek. 2004. Random forests in
language modeling. In Proceedings of the EMNLP.
</reference>
<page confidence="0.998647">
624
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.256613">
<title confidence="0.871987">Generalized Interpolation in Decision Tree LM Language</title>
<affiliation confidence="0.849608">Center of</affiliation>
<author confidence="0.492021">Johns Hopkins</author>
<email confidence="0.996932">den@cs.umd.edu</email>
<affiliation confidence="0.997622">of Computer University of Maryland, College</affiliation>
<email confidence="0.99981">mharper@umd.edu</email>
<abstract confidence="0.9979825625">In the face of sparsity, statistical models are often interpolated with lower order (backoff) models, particularly in Language Modeling. In this paper, we argue that there is a relation between the higher order and the backoff model that must be satisfied in order for the interpolation to be effective. We show that in n-gram models, the relation is trivially held, but in models that allow arbitrary clustering of context (such as decision tree models), this relation is generally not satisfied. Based on this insight, we also propose a generalization of linear interpolation which significantly improves the performance of a decision tree language model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Lalit R Bahl</author>
<author>Peter F Brown</author>
<author>Peter V de Souza</author>
<author>Robert L Mercer</author>
</authors>
<title>A tree-based statistical language model for natural language speech recognition. Readings in speech recognition,</title>
<date>1990</date>
<pages>507--514</pages>
<marker>Bahl, Brown, de Souza, Mercer, 1990</marker>
<rawString>Lalit R. Bahl, Peter F. Brown, Peter V. de Souza, and Robert L. Mercer. 1990. A tree-based statistical language model for natural language speech recognition. Readings in speech recognition, pages 507–514.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1999</date>
<journal>Computer Speech &amp;Language,</journal>
<volume>13</volume>
<issue>4</issue>
<contexts>
<context position="2713" citStr="Chen and Goodman, 1999" startWordPosition="417" endWordPosition="420">1) = �(wi|wi−1 i−n+1) + (1) &apos;Y(wi−n+1) - ˜p(wi|wi−n+2) where p is a discounted probability1. In addition to n-gram models, there are many other ways to estimate probability distributions p(wi|wi−1 i−n+1); in this work, we are particularly interested in models involving decision trees (DTs). As in n-gram models, DT models also often utilize interpolation with lower order models; however, there are issues concerning the interpolation which arise from the fact that decision trees permit arbitrary clustering of context, and these issues are the main subject of this paper. 1We refer the reader to (Chen and Goodman, 1999) for a survey of the discounting methods for n-gram models. p(wm1 ) = �m i=1 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 620–624, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics 2 Decision Trees The vast context space in a language model mandates the use of context clustering in some form. In n-gram models, the clustering can be represented as a k-ary decision tree of depth n − 1, where k is the size of the vocabulary. Note that this is a very constrained form of a decision tree, and is probably</context>
</contexts>
<marker>Chen, Goodman, 1999</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1999. An empirical study of smoothing techniques for language modeling. Computer Speech &amp;Language, 13(4):359–393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Denis Filimonov</author>
<author>Mary Harper</author>
</authors>
<title>A joint language model with fine-grain syntactic tags.</title>
<date>2009</date>
<booktitle>In Proceedings of the EMNLP.</booktitle>
<contexts>
<context position="4045" citStr="Filimonov and Harper, 2009" startWordPosition="639" endWordPosition="642">and the model would benefit from merging them. Therefore, it is reasonable to believe that arbitrary (i.e., unconstrained) context clustering such as a decision tree should be able to outperform the n-gram model. A decision tree provides us with a clustering function �(wi−1 i−n+1) _* {b1, ... , .b�&apos;}, where N is the number of clusters (leaves in the DT), and clusters 4bk are disjoint subsets of the context space; the probability estimation is approximated as follows: p(wi|wi−1 i−n+1) �p(wi|�(wi−1 i−n+1)) (2) Methods of DT construction and probability estimation used in this work are based on (Filimonov and Harper, 2009); therefore, we refer the reader to that paper for details. Another advantage of using decision trees is the ease of adding parameters such as syntactic tags: In this case, the decision tree would cluster the context space wi−1 i−n+1ti−1 i−n+1 based on information theoretic metrics, without utilizing heuristics for which order the context attributes are to be backed off (cf. Eq. 1). In subsequent discussion, we will write equations for word models (Eq. 2), but they are equally applicable to joint models (Eq. 3) with trivial transformations. 3 Backoff Property Let us rewrite the interpolation E</context>
<context position="8856" citStr="Filimonov and Harper, 2009" startWordPosition="1497" endWordPosition="1500">er and are optimized on a heldout set using EM. pn(wi|φn) is the probability distribution at the cluster φn in the tree of order n. This interpolation method is particularly useful as, unlike count-based discounting methods (e.g., KneserNey), it can be applied to already smooth distributions pn2. 5 Generalized Interpolation We can unwind the recursion in Eq. 6 and make substitutions: λn(φn) — ˆλn(φn) (1 — λn(φn)) &apos; λn−1(φn−1) — ˆλn−1(φn−1) ... 2In decision trees, the distribution at a cluster (leaf) is often recursively interpolated with its parent node, e.g. (Bahl et al., 1990; Heeman, 1999; Filimonov and Harper, 2009). ˆλm(φm) &apos; pm(wi|φm) (7) En ˆλm(φm) = 1 m=1 Note that in this parameterization, the weight assigned to pn−1(wi|φn−1) is limited by (1—λn(φn)), i.e., the weight assigned to the higher order model. Ideally we should be able to assign a different set of interpolation weights for every eligible combination of clusters φn, φn−1, . . . , φ1. However, not only is the number of such combinations extremely large, but many of them will not be observed in the training data, making parameter estimation cumbersome. Therefore, we propose the following parameterization for the interpolation of decision tree</context>
<context position="12448" citStr="Filimonov and Harper, 2009" startWordPosition="2121" endWordPosition="2124">epresentation as in Eq. 6, which proves that the standard linear interpolation is a special case of the new interpolation scheme, which occurs when the backoff Property 5 holds. 6 Results and Discussion Models are trained on 35M words of WSJ 94-96 from LDC2008T13. The text was converted into speech-like form, namely numbers and abbreviations were verbalized, text was downcased, punctuation was removed, and contractions and possessives were joined with the previous word (i.e., they ’ll becomes they’ll). For syntactic modeling, we used tags comprised of POS tags of the word and its head, as in (Filimonov and Harper, 2009). Parsing of the text for tag extraction occurred after verbalization of numbers and abbreviations but before any further processing; we used an appropriately trained latent variable PCFG parser (Huang and Harper, 2009). For reference, we include n-gram models with Jelinek-Mercer and modified interpolated KN discounting. All models use the same vocabulary of approximately 50k words. We implemented four decision tree models3: two using the interpolation method of (Eq. 6) and two based on the generalized interpolation (Eq. 8). Parameters λ were estimated using the L-BFGS to minimize the entropy </context>
</contexts>
<marker>Filimonov, Harper, 2009</marker>
<rawString>Denis Filimonov and Mary Harper. 2009. A joint language model with fine-grain syntactic tags. In Proceedings of the EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter A Heeman</author>
</authors>
<title>POS tags and decision trees for language modeling.</title>
<date>1999</date>
<booktitle>In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,</booktitle>
<pages>129--137</pages>
<contexts>
<context position="8827" citStr="Heeman, 1999" startWordPosition="1495" endWordPosition="1496"> to each cluster and are optimized on a heldout set using EM. pn(wi|φn) is the probability distribution at the cluster φn in the tree of order n. This interpolation method is particularly useful as, unlike count-based discounting methods (e.g., KneserNey), it can be applied to already smooth distributions pn2. 5 Generalized Interpolation We can unwind the recursion in Eq. 6 and make substitutions: λn(φn) — ˆλn(φn) (1 — λn(φn)) &apos; λn−1(φn−1) — ˆλn−1(φn−1) ... 2In decision trees, the distribution at a cluster (leaf) is often recursively interpolated with its parent node, e.g. (Bahl et al., 1990; Heeman, 1999; Filimonov and Harper, 2009). ˆλm(φm) &apos; pm(wi|φm) (7) En ˆλm(φm) = 1 m=1 Note that in this parameterization, the weight assigned to pn−1(wi|φn−1) is limited by (1—λn(φn)), i.e., the weight assigned to the higher order model. Ideally we should be able to assign a different set of interpolation weights for every eligible combination of clusters φn, φn−1, . . . , φ1. However, not only is the number of such combinations extremely large, but many of them will not be observed in the training data, making parameter estimation cumbersome. Therefore, we propose the following parameterization for the i</context>
</contexts>
<marker>Heeman, 1999</marker>
<rawString>Peter A. Heeman. 1999. POS tags and decision trees for language modeling. In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, pages 129–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongqiang Huang</author>
<author>Mary Harper</author>
</authors>
<title>SelfTraining PCFG grammars with latent annotations across languages.</title>
<date>2009</date>
<booktitle>In Proceedings of the EMNLP</booktitle>
<contexts>
<context position="12667" citStr="Huang and Harper, 2009" startWordPosition="2154" endWordPosition="2157">n 35M words of WSJ 94-96 from LDC2008T13. The text was converted into speech-like form, namely numbers and abbreviations were verbalized, text was downcased, punctuation was removed, and contractions and possessives were joined with the previous word (i.e., they ’ll becomes they’ll). For syntactic modeling, we used tags comprised of POS tags of the word and its head, as in (Filimonov and Harper, 2009). Parsing of the text for tag extraction occurred after verbalization of numbers and abbreviations but before any further processing; we used an appropriately trained latent variable PCFG parser (Huang and Harper, 2009). For reference, we include n-gram models with Jelinek-Mercer and modified interpolated KN discounting. All models use the same vocabulary of approximately 50k words. We implemented four decision tree models3: two using the interpolation method of (Eq. 6) and two based on the generalized interpolation (Eq. 8). Parameters λ were estimated using the L-BFGS to minimize the entropy on a heldout set. In order to eliminate the influence of all factors other than the interpolation, we used the same decision trees. The perplexity results on WSJ section 23 are presented in Table 1. As we have predicted</context>
</contexts>
<marker>Huang, Harper, 2009</marker>
<rawString>Zhongqiang Huang and Mary Harper. 2009. SelfTraining PCFG grammars with latent annotations across languages. In Proceedings of the EMNLP 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
<author>Robert L Mercer</author>
</authors>
<title>Interpolated estimation of markov source parameters from sparse data.</title>
<date>1980</date>
<booktitle>In Proceedings of the Workshop on Pattern Recognition in Practice,</booktitle>
<pages>381--397</pages>
<contexts>
<context position="8072" citStr="Jelinek and Mercer, 1980" startWordPosition="1363" endWordPosition="1366">ever, in a 4-gram model, Φ(wi−3wi−2wi−1) = ({on}, {may,june}, {&lt;unk&gt;}) is quite plausible. Thus, arbitrary clustering (an advantage of DTs) leads to violation of Property 5, which, we argue, may lead to a degradation of performance if backoff interpolation Eq. 4 is used. In the next section, we generalize the interpolation scheme which, as we show in Section 6, allows us to find a better solution in the face of the violation of Property 5. 4 Linear Interpolation We use linear interpolation as the baseline, represented recursively, which is similar to JelinekMercer smoothing for n-gram models (Jelinek and Mercer, 1980): �pn(wi|wi−1 i−n+1) = An(On) &apos; pn(wi|On) + (6) (1 − An(On)) &apos; �pn−1(wi|wi−1 i−n+2) where φn = Φn(wi−1 i−n+1), and λn(φn) E [0, 1] are assigned to each cluster and are optimized on a heldout set using EM. pn(wi|φn) is the probability distribution at the cluster φn in the tree of order n. This interpolation method is particularly useful as, unlike count-based discounting methods (e.g., KneserNey), it can be applied to already smooth distributions pn2. 5 Generalized Interpolation We can unwind the recursion in Eq. 6 and make substitutions: λn(φn) — ˆλn(φn) (1 — λn(φn)) &apos; λn−1(φn−1) — ˆλn−1(φn−1)</context>
</contexts>
<marker>Jelinek, Mercer, 1980</marker>
<rawString>Frederick Jelinek and Robert L. Mercer. 1980. Interpolated estimation of markov source parameters from sparse data. In Proceedings of the Workshop on Pattern Recognition in Practice, pages 381–397.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peng Xu</author>
<author>Frederick Jelinek</author>
</authors>
<title>Random forests in language modeling.</title>
<date>2004</date>
<booktitle>In Proceedings of the EMNLP.</booktitle>
<contexts>
<context position="13571" citStr="Xu and Jelinek, 2004" startWordPosition="2297" endWordPosition="2300">lized interpolation (Eq. 8). Parameters λ were estimated using the L-BFGS to minimize the entropy on a heldout set. In order to eliminate the influence of all factors other than the interpolation, we used the same decision trees. The perplexity results on WSJ section 23 are presented in Table 1. As we have predicted, the effect of the new interpolation becomes apparent at the 4-gram order, when Property 5 is most frequently violated. Note that we observe similar patterns for both word-tree and syntactic models, with syntactic models outperforming their word-tree counterparts. We believe that (Xu and Jelinek, 2004) also suffers from violation of Property 5, however, since they use a heuristic method4 to set backoff weights, it is difficult to ascertain the extent. 7 Conclusion The main contribution of this paper is the insight that in the standard recursive backoff there is an implied relation between the backoff and the higher order models, which is essential for adequate performance. When this relation is not satisfied other interpolation methods should be employed; hence, we propose a generalization of linear interpolation that significantly outperforms the standard form in such a scenario. 3We refer</context>
</contexts>
<marker>Xu, Jelinek, 2004</marker>
<rawString>Peng Xu and Frederick Jelinek. 2004. Random forests in language modeling. In Proceedings of the EMNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>