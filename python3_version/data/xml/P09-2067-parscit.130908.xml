<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000005">
<title confidence="0.9995">
Automatic Story Segmentation using a Bayesian Decision Framework
for Statistical Models of Lexical Chain Features
</title>
<author confidence="0.996523">
Wai-Kit Lo Wenying Xiong Helen Meng
</author>
<affiliation confidence="0.9849725">
The Chinese University The Chinese University The Chinese University
of Hong Kong, of Hong Kong, of Hong Kong,
</affiliation>
<address confidence="0.515869">
Hong Kong, China Hong Kong, China Hong Kong, China
</address>
<email confidence="0.991944">
wklo@se.cuhk.edu.hk wyxiong@se.cuhk.edu.hk hmmeng@se.cuhk.edu.hk
</email>
<sectionHeader confidence="0.993776" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99992737037037">
This paper presents a Bayesian decision
framework that performs automatic story
segmentation based on statistical model-
ing of one or more lexical chain features.
Automatic story segmentation aims to lo-
cate the instances in time where a story
ends and another begins. A lexical chain
is formed by linking coherent lexical
items chronologically. A story boundary
is often associated with a significant
number of lexical chains ending before it,
starting after it, as well as a low count of
chains continuing through it. We devise a
Bayesian framework to capture such be-
havior, using the lexical chain features of
start, continuation and end. In the scoring
criteria, lexical chain starts/ends are
modeled statistically with the Weibull
and uniform distributions at story boun-
daries and non-boundaries respectively.
The normal distribution is used for lexi-
cal chain continuations. Full combination
of all lexical chain features gave the best
performance (F1=0.6356). We found that
modeling chain continuations contributes
significantly towards segmentation per-
formance.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999975666666666">
Automatic story segmentation is an important
precursor in processing audio or video streams in
large information repositories. Very often, these
continuous streams of data do not come with
boundaries that segment them into semantically
coherent units, or stories. The story unit is
needed for a wide range of spoken language in-
formation retrieval tasks, such as topic tracking,
clustering, indexing and retrieval. To perform
automatic story segmentation, there are three
categories of cues available: lexical cues from
transcriptions, prosodic cues from the audio
stream and video cues such as anchor face and
color histograms. Among the three types of cues,
lexical cues are the most generic since they can
work on text and multimedia sources. Previous
approaches include TextTiling (Hearst 1997) that
monitors changes in sentence similarity, use of
cue phrases (Reynar 1999) and Hidden Markov
Models (Yamron 1998). In addition, the ap-
proach based on lexical chaining captures the
content coherence by linking coherent lexical
items (Morris and Hirst 1991, Hirst and St-Onge
1998). Stokes (2004) discovers boundaries by
chaining up terms and locating instances of time
where the count of chain starts and ends (boun-
dary strength) achieves local maxima. Chan et al.
(2007) enhanced this approach through statistical
modeling of lexical chain starts and ends. We
further extend this approach in two aspects: 1) a
Bayesian decision framework is used; 2) chain
continuations straddling across boundaries are
taken into consideration and statistically modeled.
</bodyText>
<sectionHeader confidence="0.992355" genericHeader="introduction">
2 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999861142857143">
Experiments are conducted using data from the
TDT-2 Voice of America Mandarin broadcast.
In particular, we only use the data from the long
programs (40 programs, 1458 stories in total),
each of which is about one hour in duration. The
average number of words per story is 297. The
news programs are further divided chronologi-
cally into training (for parameter estimation of
the statistical models), development (for tuning
decision thresholds) and test (for performance
evaluation) sets, as shown in Figure 1. Automatic
speech recognition (ASR) outputs that are pro-
vided in the TDT-2 corpus are used for lexical
chain formation.
</bodyText>
<page confidence="0.977242">
265
</page>
<note confidence="0.92546">
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 265–268,
Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.9987325">
The story segmentation task in this work is to
decide whether a hypothesized utterance boun-
dary (provided in the TDT-2 data based on the
speech recognition result) is a story boundary.
Segmentation performance is evaluated using the
F1-measure.
</bodyText>
<subsectionHeader confidence="0.370427">
Training Set Development Set Test Set
</subsectionHeader>
<figure confidence="0.815663">
697 stories 385 stories 376 stories
20 hour 10 hour 10 hour
Feb.20th,1998 Mar.4th,1998 Mar.17th,1998 Apr.4th,1998
</figure>
<figureCaption confidence="0.937161">
Figure 1: Organization of the long programs in TDT-2
VOA Mandarin for our experiments.
</figureCaption>
<sectionHeader confidence="0.99113" genericHeader="method">
3 Approach
</sectionHeader>
<bodyText confidence="0.99996968">
Our approach considers utterance boundaries that
are labeled in the TDT-2 corpus and classifies
them either as a story boundary or non-boundary.
We form lexical chains from the TDT-2 ASR
outputs by linking repeated words. Since words
may also repeat across different stories, we limit
the maximum distance between consecutive
words within the lexical chain. This limit is op-
timized according to the approach in (Chan et al.
2007) based on the training data. The optimal
value is found to be 130.9sec for long programs.
We make use of three lexical chain features:
chain starts, continuations and ends. At the be-
ginning of a story, new words are introduced
more frequently and hence we observe many lex-
ical chain starts. There is also tendency of many
lexical chains ending before a story ends. As a
result, there is a higher density of chain starts and
ends in the proximity of a story boundary. Fur-
thermore, there tends to be fewer chains strad-
dling across a story boundary. Based on these
characteristics of lexical chains, we devise a sta-
tistical framework for story segmentation by
modeling the distribution of these lexical chain
features near the story boundaries.
</bodyText>
<subsectionHeader confidence="0.9738365">
3*1 Story Segmentation based on a Single
Lexical Chain Feature
</subsectionHeader>
<bodyText confidence="0.9720005">
Given an utterance boundary with the lexical
chain feature, X, we compare the conditional
probabilities of observing a boundary, B, or non-
boundary, B, as
</bodyText>
<sectionHeader confidence="0.618165" genericHeader="method">
P(B I X)&lt; P(B I x). (1)
</sectionHeader>
<bodyText confidence="0.991419777777778">
where X is a single chain feature, which may be
the chain start (S), chain continuation (C), or
chain end (E).
By applying the Bayes&apos; theorem, this can be
rewritten as a likelihood ratio test,
for which the decision threshold
is θx = P(B)/P(B) , dependent on the a priori
probability of observing boundary or a non-
boundary.
</bodyText>
<subsectionHeader confidence="0.643145">
3*2 Story Segmentation based on Combined
Chain Features
</subsectionHeader>
<bodyText confidence="0.9738975">
When multiple features are used in combination,
we formulate the problem as
</bodyText>
<equation confidence="0.6583665">
P(B I S, E, �)&lt; �(� I �, �, �)
&gt; . (3)
</equation>
<bodyText confidence="0.8809215">
By assuming that the chain features are condi-
tionally independent of one another (i.e.,
</bodyText>
<equation confidence="0.886727166666667">
P(S,C,EIB) = P(SIB) P(CIB) P(EIB)), the formu-
lation can be rewritten as a likelihood ratio test
� � � � � � � � �
( I ) ( I ) ( I )
� � � � � � � � �
( I ) ( I ) ( I )
</equation>
<sectionHeader confidence="0.614036" genericHeader="method">
4 Modeling of Lexical Chain Features
</sectionHeader>
<subsectionHeader confidence="0.97662">
4*1 Chain starts and ends
</subsectionHeader>
<bodyText confidence="0.9999826">
We follow (Chan et al. 2007) to model the lexi-
cal chain starts and ends at a story boundary with
a statistical distribution. We apply a window
around the candidate boundaries (same window
size for both chain starts and ends) in our work.
Chain features falling outside the window are
excluded from the model. Figure 2 shows the
distribution when a window size of 20 seconds is
used. This is the optimal window size when
chain start and end features are combined.
</bodyText>
<figure confidence="0.7426865">
-20-1a-16-14-12-10 -a -6 -4 -2 0 2 4 6 a 10 12 14 16 1a 20
Offset from story boundary in second
</figure>
<figureCaption confidence="0.998700333333333">
Figure 2: Distribution of chain starts and ends at
known story boundaries. The Weibull distribution is
used to model these distributions.
</figureCaption>
<bodyText confidence="0.9991465">
We also assume that the probability of seeing
a lexical chain start / end at a particular instance
is independent of the starts / ends of other chains.
As a result, the probability of seeing a sequence
of chain starts at a story boundary is given by the
product of a sequence of Weibull distributions
</bodyText>
<table confidence="0.7285438">
N k-1tj \ JJIkrik(ti
) J
50
40
Number of lexical chain features
30
20
10
Frequency of lexical
chain features
Fitted Weibull dist. for
lexical chain starts
Fitted Weibull dist. for
lexical chain ends
x
</table>
<equation confidence="0.967376444444444">
� � � θ
( I ) &lt; &gt;
P(X I B) x
(2)
&lt; &gt;��� . (4)
θ
� � �
( I ) =
, (5)
</equation>
<page confidence="0.976973">
266
</page>
<bodyText confidence="0.9995325">
where S is the sequence of time with chain starts
(S=[ti, t2, ... t;, ... tNs]), ks is the shape, λs is the
scale for the fitted Weibull distribution for chain
starts, Ns is the number of chain starts. The same
formulation is similarly applied to chain ends.
Figure 3 shows the frequency of raw feature
points for lexical chain starts and ends near utter-
ance boundaries that are non-story boundaries.
Since there is no obvious distribution pattern for
these lexical chain features near a non-story
boundary, we model these characteristics with a
uniform distribution.
</bodyText>
<figure confidence="0.8306825">
Relative frequency of chain starts I ends
0.1
Offset from utterance boundary in seconds
(non-story boundaries only)
</figure>
<figureCaption confidence="0.9953925">
Figure 3: Distribution of chain starts and ends at ut-
terance boundaries that are non-story boundaries.
</figureCaption>
<subsectionHeader confidence="0.692531">
4*2 Chain continuations
</subsectionHeader>
<bodyText confidence="0.9636345">
Figure 4 shows the distributions of chain contin-
uations near story boundary and non-story boun-
dary. As one may expect, there are fewer lexical
chains that straddle across a story boundary (the
</bodyText>
<sectionHeader confidence="0.485971" genericHeader="method">
P(C I B))
</sectionHeader>
<bodyText confidence="0.991373166666667">
curve of when compared to a non-story
boundary (the curve of P(C IB)). Based on the
observations, we model the probability of occur-
rence of lexical chains straddling across a given
story boundary or non-story boundary by a nor-
mal distribution.
</bodyText>
<figure confidence="0.982206333333333">
0 5 10 15 20 25
Number of chain continuations straddling across an
utterance boundary
</figure>
<figureCaption confidence="0.995397">
Figure 4: Distributions of chain continuations at story
boundaries and non-story boundaries.
</figureCaption>
<sectionHeader confidence="0.563256" genericHeader="method">
5 Story Segmentation based on Combi-
</sectionHeader>
<subsectionHeader confidence="0.396253">
nation of Lexical Chain Features
</subsectionHeader>
<bodyText confidence="0.9999836">
We trained the parameters of the Weibull distri-
bution for lexical chain starts and ends at story
boundaries, the uniform distribution for lexical
chain start / end at non-story boundary, and the
normal distribution for lexical chain continua-
tions. Instead of directly using a threshold as
shown in Equation (2), we optimize on the para-
meter n, which is the optimal number of top scor-
ing utterance boundaries that are classified as
story boundaries in the development set.
</bodyText>
<subsectionHeader confidence="0.95698">
5*1 Using Bayesian decision framework
</subsectionHeader>
<bodyText confidence="0.9996744">
We compare the performance of the Bayesian
decision framework to the use of likelihood only
P(��B) as shown in Figure 5. The results demon-
strate consistent improvement in F1-measure
when using the Bayesian decision framework.
</bodyText>
<figure confidence="0.969577714285714">
0.6
0.4
0.2
P(S/B) P(E/B) P(S / B) P(E/B)
P S B
( / ) P E B
( / )
</figure>
<figureCaption confidence="0.99691">
Figure 5: Story segmentation performance in F1-
measure when using single lexical chain features.
</figureCaption>
<figure confidence="0.690782">
5*2 Modeling multiple features jointly
</figure>
<figureCaption confidence="0.998532">
Figure 6: Results of F1-measure comparing the seg-
mentation results using different statistical models of
lexical chain features.
</figureCaption>
<bodyText confidence="0.999273166666667">
We further compare the performance of various
scoring methods including single and combined
lexical chain features. The baseline result is ob-
tained using a scoring function based on the like-
lihoods of seeing a chain start or end at a story
boundary (Chan et al. 2007) which is denoted as
Score(S, E). Performance from other methods
based on the same dataset can be referenced from
Chan et al. 2007 and will not be repeated here.
The best story segmentation performance is
achieved by combining all lexical chain features
which achieves an F1-measure of 0.6356. All
improvements have been verified to be statisti-
cally significant (α=0.05). By comparing the re-
sults of (e) to (h), (c) to (g), and (b) to (f), we can
see that lexical chain continuation feature contri-
butes significantly and consistently towards story
segmentation performance.
</bodyText>
<figure confidence="0.997808454545454">
-16-14-12-10 -8 -6 -4 -2 0 2 4 6 8 10 12 14 16
0.08
0.06
0.04
0.02
x
Lexical chain starts I ends
Fitted uniform dist. for
lexical chain starts
Fitted uniform dist. for
lexical chain ends
Probability
0.16
0.14
0.12
0.08
0.06
0.04
0.02
0.1
0
x
</figure>
<figureCaption confidence="0.74612">
Relative frequency of lexical chain
continuation at an utterance boundary
Fitted distribution at story boundary
Fitted distribution at non-story boundary
</figureCaption>
<figure confidence="0.991376794871795">
Story boundary, P(C I B)
Non-story boundary,P(C I B)
)
(a) Score(S, E) [Chan 2007
(a) (b) (c) (d) (e) (f) (g) (h)
F1- measure
0.8
0.6
0.4
0.2
0
(b) P(S I B)
P(SIB)
(c) P(E I B)
P E B
( I )
(d) P(C I B)
P C B
( I )
(e) P(S I B)P(E I B)
P S B P E B
( I ) ( I )
(f)P(SIB)P(CIB)
P(SIB)P(CIB)
(g) P(E I B)P(C I B)
P E B P C B
( I ) ( I )
(h) P(S I B)P(E I B)P(C I B)
P(SIB)P(EIB)P(CIB)
F1- measure
0
267
5*3 Analysis
11 $hain $ontinuations7
(1811191,+W22q[alffl91 ,►,3},[W*91 (48;1*1�91 r(58&apos;*44
(681RA91 (&amp;PIA91 (88 &apos;PA91 (98-tfg91 (1084A]1 (118 01*9
2tteran$e &amp;quot;oun#ar!
-o$$urs at 664 se$on# in #o$ument 3O41996031750900510001
6hi$h is not a stor! &amp;quot;oun#ar!.
</figure>
<figureCaption confidence="0.8991878">
Figure 7: Lexical chain starts, ends and continuations
in the proximity of a non-story boundary. Wi[xxxx]
denotes the i-th Chinese word &amp;quot;xxxx&amp;quot;.
Figure 7 shows an utterance boundary that is a
non-story boundary. There is a high concentra-
</figureCaption>
<bodyText confidence="0.992878142857143">
tion of chain starts and ends near the boundary
which leads to a misclassification if we only
combine chain starts and ends for segmentation.
However, there are also a large number of chain
continuations across the utterance boundary,
which implies that a story boundary is less likely.
The full combination gives the correct decision.
</bodyText>
<figure confidence="0.906454428571428">
6 $hain $ontinuations7
(189eA91 (28 *p*�F], (38 H;91
(48#±�91 (58 1361 (68 01W9
`e6 `e5 `e4 `e3 `e2 `e1 ts1 ts2 ts3
2tteran$e &amp;quot;oun#ar!
-o$$urs at 2014 se$on# in #o$ument
3O41998031950900510001 6hi$h is a stor! &amp;quot;oun#ar!.
</figure>
<figureCaption confidence="0.9946645">
Figure 8: Lexical chain starts, ends and continuations
in the proximity of a story boundary.
</figureCaption>
<bodyText confidence="0.992770846153846">
Figure 8 shows another example where an ut-
terance boundary is misclassified as a non-story
boundary when only the combination of lexical
chain starts and ends are used. Incorporation of
the chain continuation feature helps rectify the
classification.
From these two examples, we can see that the
incorporation of chain continuation in our story
segmentation framework can complement the
features of chain starts and ends. In both exam-
ples above, the number of chain continuations
plays a crucial role in correct identification of a
story boundary.
</bodyText>
<sectionHeader confidence="0.999285" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.9999324">
We have presented a Bayesian decision frame-
work that performs automatic story segmentation
based on statistical modeling of one or more lex-
ical chain features, including lexical chain starts,
continuations and ends. Experimentation shows
that the Bayesian decision framework is superior
to the use of likelihoods for segmentation. We
also experimented with a variety of scoring crite-
ria, involving likelihood ratio tests of a single
feature (i.e. lexical chain starts, continuations or
ends), their pair-wise combinations, as well as
the full combination of all three features. Lexical
chain starts/ends are modeled statistically with
the Weibull and normal distributions for story
boundaries and non-boundaries. The normal dis-
tribution is used for lexical chain continuations.
Full combination of all lexical chain features
gave the best performance (F1=0.6356). Model-
ing chain continuations contribute significantly
towards segmentation performance.
</bodyText>
<sectionHeader confidence="0.998205" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9969415">
This work is affiliated with the CUHK MoE-
Microsoft Key Laboratory of Human-centric Compu-
ting and Interface Technologies. We would also like
to thank Professor Mari Ostendorf for suggesting the
use of continuing chains and Mr. Kelvin Chan for
providing information about his previous work.
</bodyText>
<sectionHeader confidence="0.999082" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999249041666667">
Chan, S. K. et al. 2007. &amp;quot;Modeling the Statistical Be-
haviour of Lexical Chains to Capture Word Cohe-
siveness for Automatic Story Segmentation&amp;quot;, Proc.
ofINTERSPEECH-2007.
Hearst, M. A. 1997. &amp;quot;TextTiling: Segmenting Text
into Multiparagraph Subtopic Passages&amp;quot;, Computa-
tional Linguistics, 23(1), pp. 33-64.
Hirst, G. and St-Onge, D. 1998. &amp;quot;Lexical chains as
representations of context for the detection and
correction of malapropisms&amp;quot;, WordNet. An Elec-
tronic Lexical Database, pp. 305-332.
Morris, J. and Hirst, G. 1991. &amp;quot;Lexical cohesion com-
puted by thesaural relations as an indicator of the
structure of text&amp;quot;, Computational Linguistics,
17(1), pp. 21-48.
Reynar, J.C. 1999, &amp;quot;Statistical models for topic seg-
mentation&amp;quot;, Proc. 37th annual meeting of the ACL,
pp. 357-364.
Stokes, N. 2004. Applications of Lexical Cohesion
Analysis in the Topic Detection and Tracking Do-
main, PhD thesis, University College Dublin.
Yamron, J.P. et al. 1998, &amp;quot;A hidden Markov model
approach to text segmentation and event tracking&amp;quot;,
Proc. ICASSP 1998, pp. 333-336.
</reference>
<figure confidence="0.9881644">
t.3
te2
to
ts1 ts2 tea ts4 ts5 ts6 ts7
-15 -10 -5 5 10 15
time
10
20
time
10 20
</figure>
<page confidence="0.950861">
268
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.377217">
<title confidence="0.9988855">Automatic Story Segmentation using a Bayesian Decision Framework for Statistical Models of Lexical Chain Features</title>
<author confidence="0.999167">Wai-Kit Lo Wenying Xiong Helen Meng</author>
<affiliation confidence="0.998385">The Chinese University The Chinese University The Chinese University</affiliation>
<address confidence="0.6204835">of Hong Kong, of Hong Kong, of Hong Kong, Hong Kong, China Hong Kong, China Hong Kong, China</address>
<email confidence="0.966134">wklo@se.cuhk.edu.hkwyxiong@se.cuhk.edu.hkhmmeng@se.cuhk.edu.hk</email>
<abstract confidence="0.992852428571429">This paper presents a Bayesian decision framework that performs automatic story segmentation based on statistical modeling of one or more lexical chain features. Automatic story segmentation aims to locate the instances in time where a story ends and another begins. A lexical chain is formed by linking coherent lexical items chronologically. A story boundary is often associated with a significant number of lexical chains ending before it, starting after it, as well as a low count of chains continuing through it. We devise a Bayesian framework to capture such behavior, using the lexical chain features of start, continuation and end. In the scoring criteria, lexical chain starts/ends are modeled statistically with the Weibull and uniform distributions at story boundaries and non-boundaries respectively. The normal distribution is used for lexical chain continuations. Full combination of all lexical chain features gave the best performance (F1=0.6356). We found that modeling chain continuations contributes significantly towards segmentation performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S K Chan</author>
</authors>
<title>Modeling the Statistical Behaviour of Lexical Chains to Capture Word Cohesiveness for Automatic Story Segmentation&amp;quot;,</title>
<date>2007</date>
<booktitle>Proc. ofINTERSPEECH-2007.</booktitle>
<contexts>
<context position="11693" citStr="Chan 2007" startWordPosition="1941" endWordPosition="1942"> can see that lexical chain continuation feature contributes significantly and consistently towards story segmentation performance. -16-14-12-10 -8 -6 -4 -2 0 2 4 6 8 10 12 14 16 0.08 0.06 0.04 0.02 x Lexical chain starts I ends Fitted uniform dist. for lexical chain starts Fitted uniform dist. for lexical chain ends Probability 0.16 0.14 0.12 0.08 0.06 0.04 0.02 0.1 0 x Relative frequency of lexical chain continuation at an utterance boundary Fitted distribution at story boundary Fitted distribution at non-story boundary Story boundary, P(C I B) Non-story boundary,P(C I B) ) (a) Score(S, E) [Chan 2007 (a) (b) (c) (d) (e) (f) (g) (h) F1- measure 0.8 0.6 0.4 0.2 0 (b) P(S I B) P(SIB) (c) P(E I B) P E B ( I ) (d) P(C I B) P C B ( I ) (e) P(S I B)P(E I B) P S B P E B ( I ) ( I ) (f)P(SIB)P(CIB) P(SIB)P(CIB) (g) P(E I B)P(C I B) P E B P C B ( I ) ( I ) (h) P(S I B)P(E I B)P(C I B) P(SIB)P(EIB)P(CIB) F1- measure 0 267 5*3 Analysis 11 $hain $ontinuations7 (1811191,+W22q[alffl91 ,►,3},[W*91 (48;1*1�91 r(58&apos;*44 (681RA91 (&amp;PIA91 (88 &apos;PA91 (98-tfg91 (1084A]1 (118 01*9 2tteran$e &amp;quot;oun#ar! -o$$urs at 664 se$on# in #o$ument 3O41996031750900510001 6hi$h is not a stor! &amp;quot;oun#ar!. Figure 7: Lexical chain sta</context>
</contexts>
<marker>Chan, 2007</marker>
<rawString>Chan, S. K. et al. 2007. &amp;quot;Modeling the Statistical Behaviour of Lexical Chains to Capture Word Cohesiveness for Automatic Story Segmentation&amp;quot;, Proc. ofINTERSPEECH-2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Hearst</author>
</authors>
<title>TextTiling: Segmenting Text into Multiparagraph Subtopic Passages&amp;quot;,</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>1</issue>
<pages>33--64</pages>
<contexts>
<context position="2265" citStr="Hearst 1997" startWordPosition="333" endWordPosition="334">ith boundaries that segment them into semantically coherent units, or stories. The story unit is needed for a wide range of spoken language information retrieval tasks, such as topic tracking, clustering, indexing and retrieval. To perform automatic story segmentation, there are three categories of cues available: lexical cues from transcriptions, prosodic cues from the audio stream and video cues such as anchor face and color histograms. Among the three types of cues, lexical cues are the most generic since they can work on text and multimedia sources. Previous approaches include TextTiling (Hearst 1997) that monitors changes in sentence similarity, use of cue phrases (Reynar 1999) and Hidden Markov Models (Yamron 1998). In addition, the approach based on lexical chaining captures the content coherence by linking coherent lexical items (Morris and Hirst 1991, Hirst and St-Onge 1998). Stokes (2004) discovers boundaries by chaining up terms and locating instances of time where the count of chain starts and ends (boundary strength) achieves local maxima. Chan et al. (2007) enhanced this approach through statistical modeling of lexical chain starts and ends. We further extend this approach in two</context>
</contexts>
<marker>Hearst, 1997</marker>
<rawString>Hearst, M. A. 1997. &amp;quot;TextTiling: Segmenting Text into Multiparagraph Subtopic Passages&amp;quot;, Computational Linguistics, 23(1), pp. 33-64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Hirst</author>
<author>D St-Onge</author>
</authors>
<title>Lexical chains as representations of context for the detection and correction of malapropisms&amp;quot;, WordNet. An Electronic Lexical Database,</title>
<date>1998</date>
<pages>305--332</pages>
<contexts>
<context position="2549" citStr="Hirst and St-Onge 1998" startWordPosition="375" endWordPosition="378">re are three categories of cues available: lexical cues from transcriptions, prosodic cues from the audio stream and video cues such as anchor face and color histograms. Among the three types of cues, lexical cues are the most generic since they can work on text and multimedia sources. Previous approaches include TextTiling (Hearst 1997) that monitors changes in sentence similarity, use of cue phrases (Reynar 1999) and Hidden Markov Models (Yamron 1998). In addition, the approach based on lexical chaining captures the content coherence by linking coherent lexical items (Morris and Hirst 1991, Hirst and St-Onge 1998). Stokes (2004) discovers boundaries by chaining up terms and locating instances of time where the count of chain starts and ends (boundary strength) achieves local maxima. Chan et al. (2007) enhanced this approach through statistical modeling of lexical chain starts and ends. We further extend this approach in two aspects: 1) a Bayesian decision framework is used; 2) chain continuations straddling across boundaries are taken into consideration and statistically modeled. 2 Experimental Setup Experiments are conducted using data from the TDT-2 Voice of America Mandarin broadcast. In particular,</context>
</contexts>
<marker>Hirst, St-Onge, 1998</marker>
<rawString>Hirst, G. and St-Onge, D. 1998. &amp;quot;Lexical chains as representations of context for the detection and correction of malapropisms&amp;quot;, WordNet. An Electronic Lexical Database, pp. 305-332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Morris</author>
<author>G Hirst</author>
</authors>
<title>Lexical cohesion computed by thesaural relations as an indicator of the structure of text&amp;quot;,</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<volume>17</volume>
<issue>1</issue>
<pages>21--48</pages>
<contexts>
<context position="2524" citStr="Morris and Hirst 1991" startWordPosition="371" endWordPosition="374">story segmentation, there are three categories of cues available: lexical cues from transcriptions, prosodic cues from the audio stream and video cues such as anchor face and color histograms. Among the three types of cues, lexical cues are the most generic since they can work on text and multimedia sources. Previous approaches include TextTiling (Hearst 1997) that monitors changes in sentence similarity, use of cue phrases (Reynar 1999) and Hidden Markov Models (Yamron 1998). In addition, the approach based on lexical chaining captures the content coherence by linking coherent lexical items (Morris and Hirst 1991, Hirst and St-Onge 1998). Stokes (2004) discovers boundaries by chaining up terms and locating instances of time where the count of chain starts and ends (boundary strength) achieves local maxima. Chan et al. (2007) enhanced this approach through statistical modeling of lexical chain starts and ends. We further extend this approach in two aspects: 1) a Bayesian decision framework is used; 2) chain continuations straddling across boundaries are taken into consideration and statistically modeled. 2 Experimental Setup Experiments are conducted using data from the TDT-2 Voice of America Mandarin </context>
</contexts>
<marker>Morris, Hirst, 1991</marker>
<rawString>Morris, J. and Hirst, G. 1991. &amp;quot;Lexical cohesion computed by thesaural relations as an indicator of the structure of text&amp;quot;, Computational Linguistics, 17(1), pp. 21-48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J C Reynar</author>
</authors>
<title>Statistical models for topic segmentation&amp;quot;,</title>
<date>1999</date>
<booktitle>Proc. 37th annual meeting of the ACL,</booktitle>
<pages>357--364</pages>
<contexts>
<context position="2344" citStr="Reynar 1999" startWordPosition="345" endWordPosition="346">The story unit is needed for a wide range of spoken language information retrieval tasks, such as topic tracking, clustering, indexing and retrieval. To perform automatic story segmentation, there are three categories of cues available: lexical cues from transcriptions, prosodic cues from the audio stream and video cues such as anchor face and color histograms. Among the three types of cues, lexical cues are the most generic since they can work on text and multimedia sources. Previous approaches include TextTiling (Hearst 1997) that monitors changes in sentence similarity, use of cue phrases (Reynar 1999) and Hidden Markov Models (Yamron 1998). In addition, the approach based on lexical chaining captures the content coherence by linking coherent lexical items (Morris and Hirst 1991, Hirst and St-Onge 1998). Stokes (2004) discovers boundaries by chaining up terms and locating instances of time where the count of chain starts and ends (boundary strength) achieves local maxima. Chan et al. (2007) enhanced this approach through statistical modeling of lexical chain starts and ends. We further extend this approach in two aspects: 1) a Bayesian decision framework is used; 2) chain continuations stra</context>
</contexts>
<marker>Reynar, 1999</marker>
<rawString>Reynar, J.C. 1999, &amp;quot;Statistical models for topic segmentation&amp;quot;, Proc. 37th annual meeting of the ACL, pp. 357-364.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Stokes</author>
</authors>
<title>Applications of Lexical Cohesion Analysis in the Topic Detection and Tracking Domain, PhD thesis,</title>
<date>2004</date>
<institution>University College Dublin.</institution>
<contexts>
<context position="2564" citStr="Stokes (2004)" startWordPosition="379" endWordPosition="380">f cues available: lexical cues from transcriptions, prosodic cues from the audio stream and video cues such as anchor face and color histograms. Among the three types of cues, lexical cues are the most generic since they can work on text and multimedia sources. Previous approaches include TextTiling (Hearst 1997) that monitors changes in sentence similarity, use of cue phrases (Reynar 1999) and Hidden Markov Models (Yamron 1998). In addition, the approach based on lexical chaining captures the content coherence by linking coherent lexical items (Morris and Hirst 1991, Hirst and St-Onge 1998). Stokes (2004) discovers boundaries by chaining up terms and locating instances of time where the count of chain starts and ends (boundary strength) achieves local maxima. Chan et al. (2007) enhanced this approach through statistical modeling of lexical chain starts and ends. We further extend this approach in two aspects: 1) a Bayesian decision framework is used; 2) chain continuations straddling across boundaries are taken into consideration and statistically modeled. 2 Experimental Setup Experiments are conducted using data from the TDT-2 Voice of America Mandarin broadcast. In particular, we only use th</context>
</contexts>
<marker>Stokes, 2004</marker>
<rawString>Stokes, N. 2004. Applications of Lexical Cohesion Analysis in the Topic Detection and Tracking Domain, PhD thesis, University College Dublin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J P Yamron</author>
</authors>
<title>A hidden Markov model approach to text segmentation and event tracking&amp;quot;,</title>
<date>1998</date>
<booktitle>Proc. ICASSP</booktitle>
<pages>333--336</pages>
<contexts>
<context position="2383" citStr="Yamron 1998" startWordPosition="351" endWordPosition="352">ge of spoken language information retrieval tasks, such as topic tracking, clustering, indexing and retrieval. To perform automatic story segmentation, there are three categories of cues available: lexical cues from transcriptions, prosodic cues from the audio stream and video cues such as anchor face and color histograms. Among the three types of cues, lexical cues are the most generic since they can work on text and multimedia sources. Previous approaches include TextTiling (Hearst 1997) that monitors changes in sentence similarity, use of cue phrases (Reynar 1999) and Hidden Markov Models (Yamron 1998). In addition, the approach based on lexical chaining captures the content coherence by linking coherent lexical items (Morris and Hirst 1991, Hirst and St-Onge 1998). Stokes (2004) discovers boundaries by chaining up terms and locating instances of time where the count of chain starts and ends (boundary strength) achieves local maxima. Chan et al. (2007) enhanced this approach through statistical modeling of lexical chain starts and ends. We further extend this approach in two aspects: 1) a Bayesian decision framework is used; 2) chain continuations straddling across boundaries are taken into</context>
</contexts>
<marker>Yamron, 1998</marker>
<rawString>Yamron, J.P. et al. 1998, &amp;quot;A hidden Markov model approach to text segmentation and event tracking&amp;quot;, Proc. ICASSP 1998, pp. 333-336.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>