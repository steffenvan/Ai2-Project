<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004038">
<title confidence="0.9994265">
Graph-based Ranking Algorithms for Sentence Extraction,
Applied to Text Summarization
</title>
<author confidence="0.998245">
Rada Mihalcea
</author>
<affiliation confidence="0.9996465">
Department of Computer Science
University of North Texas
</affiliation>
<email confidence="0.995926">
rada@cs.unt.edu
</email>
<sectionHeader confidence="0.98294" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999961666666667">
This paper presents an innovative unsupervised
method for automatic sentence extraction using graph-
based ranking algorithms. We evaluate the method in
the context of a text summarization task, and show
that the results obtained compare favorably with pre-
viously published results on established benchmarks.
</bodyText>
<sectionHeader confidence="0.995146" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999920620689655">
Graph-based ranking algorithms, such as Klein-
berg’s HITS algorithm (Kleinberg, 1999) or Google’s
PageRank (Brin and Page, 1998), have been tradition-
ally and successfully used in citation analysis, social
networks, and the analysis of the link-structure of the
World Wide Web. In short, a graph-based ranking al-
gorithm is a way of deciding on the importance of a
vertex within a graph, by taking into account global in-
formation recursively computed from the entire graph,
rather than relying only on local vertex-specific infor-
mation.
A similar line of thinking can be applied to lexical
or semantic graphs extracted from natural language
documents, resulting in a graph-based ranking model
called TextRank (Mihalcea and Tarau, 2004), which
can be used for a variety of natural language process-
ing applications where knowledge drawn from an en-
tire text is used in making local ranking/selection de-
cisions. Such text-oriented ranking methods can be
applied to tasks ranging from automated extraction
of keyphrases, to extractive summarization and word
sense disambiguation (Mihalcea et al., 2004).
In this paper, we investigate a range of graph-
based ranking algorithms, and evaluate their applica-
tion to automatic unsupervised sentence extraction in
the context of a text summarization task. We show
that the results obtained with this new unsupervised
method are competitive with previously developed
state-of-the-art systems.
</bodyText>
<sectionHeader confidence="0.939909" genericHeader="method">
2 Graph-Based Ranking Algorithms
</sectionHeader>
<bodyText confidence="0.9998592">
Graph-based ranking algorithms are essentially a way
of deciding the importance of a vertex within a graph,
based on information drawn from the graph structure.
In this section, we present three graph-based ranking
algorithms – previously found to be successful on a
range of ranking problems. We also show how these
algorithms can be adapted to undirected or weighted
graphs, which are particularly useful in the context of
text-based ranking applications.
Let G = (V, E) be a directed graph with the set of
vertices V and set of edges E, where E is a subset
of V × V . For a given vertex Vi, let In(Vi) be the
set of vertices that point to it (predecessors), and let
Out(Vi) be the set of vertices that vertex Vi points to
(successors).
</bodyText>
<sectionHeader confidence="0.728603" genericHeader="method">
2.1 HITS
</sectionHeader>
<bodyText confidence="0.998331888888889">
HITS (Hyperlinked Induced Topic Search) (Klein-
berg, 1999) is an iterative algorithm that was designed
for ranking Web pages according to their degree of
“authority”. The HITS algorithm makes a distinction
between “authorities” (pages with a large number of
incoming links) and “hubs” (pages with a large num-
ber of outgoing links). For each vertex, HITS pro-
duces two sets of scores – an “authority” score, and a
“hub” score:
</bodyText>
<equation confidence="0.99392525">
HITSA(Vi) = E HITSH(Vj) (1)
VjEIn(Vi)
HITSH(Vi) = E HITSA(Vj) (2)
Vj EOut(Vi)
</equation>
<subsectionHeader confidence="0.998749">
2.2 Positional Power Function
</subsectionHeader>
<bodyText confidence="0.999195">
Introduced by (Herings et al., 2001), the positional
power function is a ranking algorithm that determines
the score of a vertex as a function that combines both
the number of its successors, and the score of its suc-
cessors.
</bodyText>
<equation confidence="0.998821666666667">
POSP (Vi) = V
1 E (1+POSP(Vj)) (3)
Vj EOut(Vi)
</equation>
<bodyText confidence="0.9990215">
The counterpart of thepositional power function is
the positional weakness function, defined as:
</bodyText>
<equation confidence="0.999893">
P OSW (Vi) = |V
1
Vj EIn(Vi)
E (1 + POSW (Vj)) (4)
</equation>
<subsectionHeader confidence="0.98075">
2.3 PageRank
</subsectionHeader>
<bodyText confidence="0.936346571428571">
PageRank (Brin and Page, 1998) is perhaps one of the
most popular ranking algorithms, and was designed as
a method for Web link analysis. Unlike other ranking
algorithms, PageRank integrates the impact of both in-
coming and outgoing links into one single model, and
therefore it produces only one set of scores:

</bodyText>
<equation confidence="0.99800652173913">
HITSWA
(Vi) =
Vj EIn(Vi)
wjiHITSWH (Vj) (6)
wijHITSWA (Vj) (7)

HITSW H (Vi) =
Vj EOut(Vi)

POSWP (Vi) = 1
|V |
(1 + wijPOSWP (Vj)) (8)
Vj EOut(Vi)

PR(Vi) = (1 − d) + d *
VjEIn(Vi)
PR(Vj) (5)
|Out(Vj )|

POSWW (Vi) = 1
|V |
(1 + wjiPOSWW (Vj)) (9)
Vj EIn(Vi)
</equation>
<bodyText confidence="0.99764">
where d is a parameter that is set between 0 and 1 1.
For each of these algorithms, starting from arbitrary
values assigned to each node in the graph, the compu-
tation iterates until convergence below a given thresh-
old is achieved. After running the algorithm, a score is
associated with each vertex, which represents the “im-
portance” or “power” of that vertex within the graph.
Notice that the final values are not affected by the
choice of the initial value, only the number of itera-
tions to convergence may be different.
</bodyText>
<subsectionHeader confidence="0.997091">
2.4 Undirected Graphs
</subsectionHeader>
<bodyText confidence="0.999966166666666">
Although traditionally applied on directed graphs, re-
cursive graph-based ranking algorithms can be also
applied to undirected graphs, in which case the out-
degree of a vertex is equal to the in-degree of the ver-
tex. For loosely connected graphs, with the number of
edges proportional with the number of vertices, undi-
rected graphs tend to have more gradual convergence
curves. As the connectivity of the graph increases
(i.e. larger number of edges), convergence is usually
achieved after fewer iterations, and the convergence
curves for directed and undirected graphs practically
overlap.
</bodyText>
<subsectionHeader confidence="0.993006">
2.5 Weighted Graphs
</subsectionHeader>
<bodyText confidence="0.9776516">
In the context of Web surfing or citation analysis, it
is unusual for a vertex to include multiple or partial
links to another vertex, and hence the original defini-
tion for graph-based ranking algorithms is assuming
unweighted graphs.
However, in our TextRank model the graphs are
build from natural language texts, and may include
multiple or partial links between the units (vertices)
that are extracted from text. It may be therefore use-
ful to indicate and incorporate into the model the
“strength” of the connection between two vertices Vi
and Vj as a weight wij added to the corresponding
edge that connects the two vertices.
Consequently, we introduce new formulae for
graph-based ranking that take into account edge
weights when computing the score associated with a
vertex in the graph.
&apos;The factor d is usually set at 0.85 (Brin and Page, 1998), and
this is the value we are also using in our implementation.

</bodyText>
<equation confidence="0.993013">
PRW(Vi) = (1 − d) + d *
Vj EIn(Vi)
</equation>
<bodyText confidence="0.999909666666667">
While the final vertex scores (and therefore rank-
ings) for weighted graphs differ significantly as com-
pared to their unweighted alternatives, the number of
iterations to convergence and the shape of the conver-
gence curves is almost identical for weighted and un-
weighted graphs.
</bodyText>
<sectionHeader confidence="0.949192" genericHeader="method">
3 Sentence Extraction
</sectionHeader>
<bodyText confidence="0.999924103448276">
To enable the application of graph-based ranking al-
gorithms to natural language texts, TextRank starts by
building a graph that represents the text, and intercon-
nects words or other text entities with meaningful re-
lations. For the task of sentence extraction, the goal
is to rank entire sentences, and therefore a vertex is
added to the graph for each sentence in the text.
To establish connections (edges) between sen-
tences, we are defining a “similarity” relation, where
“similarity” is measured as a function of content over-
lap. Such a relation between two sentences can be
seen as a process of “recommendation”: a sentence
that addresses certain concepts in a text, gives the
reader a “recommendation” to refer to other sentences
in the text that address the same concepts, and there-
fore a link can be drawn between any two such sen-
tences that share common content.
The overlap of two sentences can be determined
simply as the number of common tokens between
the lexical representations of the two sentences, or it
can be run through syntactic filters, which only count
words of a certain syntactic category. Moreover,
to avoid promoting long sentences, we are using a
normalization factor, and divide the content overlap
of two sentences with the length of each sentence.
Formally, given two sentences Si and Sj, with a
sentence being represented by the set of Ni words
that appear in the sentence: Si = Wi1, Wi2, ..., WiNi,
the similarity of Si and Sj is defined as:
</bodyText>
<equation confidence="0.999189">
Similarity(Si,Sj) = |Wk|WkSi&amp;WkSj|
log(|Si|)+log(|Sj|)
</equation>
<bodyText confidence="0.9838975">
The resulting graph is highly connected, with a
weight associated with each edge, indicating the
</bodyText>
<equation confidence="0.8300036">
PRW (Vj)
 wkj
VkEOut(Vj)
wji
(10)
</equation>
<bodyText confidence="0.999956962962963">
strength of the connections between various sentence
pairs in the text2. The text is therefore represented as
a weighted graph, and consequently we are using the
weighted graph-based ranking formulae introduced in
Section 2.5. The graph can be represented as: (a) sim-
ple undirected graph; (b) directed weighted graph with
the orientation of edges set from a sentence to sen-
tences that follow in the text (directed forward); or (c)
directed weighted graph with the orientation of edges
set from a sentence to previous sentences in the text
(directed backward).
After the ranking algorithm is run on the graph, sen-
tences are sorted in reversed order of their score, and
the top ranked sentences are selected for inclusion in
the summary.
Figure 1 shows a text sample, and the associated
weighted graph constructed for this text. The figure
also shows sample weights attached to the edges con-
nected to vertex 93, and the final score computed for
each vertex, using the PR formula, applied on an undi-
rected graph. The sentences with the highest rank are
selected for inclusion in the abstract. For this sample
article, sentences with id-s 9, 15, 16, 18 are extracted,
resulting in a summary of about 100 words, which ac-
cording to automatic evaluation measures, is ranked
the second among summaries produced by 15 other
systems (see Section 4 for evaluation methodology).
</bodyText>
<sectionHeader confidence="0.99823" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.986536464285714">
The TextRank sentence extraction algorithm is eval-
uated in the context of a single-document summa-
rization task, using 567 news articles provided dur-
ing the Document Understanding Evaluations 2002
(DUC, 2002). For each article, TextRank generates
a 100-words summary — the task undertaken by other
systems participating in this single document summa-
rization task.
For evaluation, we are using the ROUGE evaluation
toolkit, which is a method based on Ngram statistics,
found to be highly correlated with human evaluations
(Lin and Hovy, 2003a). Two manually produced ref-
erence summaries are provided, and used in the eval-
uation process4.
2In single documents, sentences with highly similar content
are very rarely if at all encountered, and therefore sentence redun-
dancy does not have a significant impact on the summarization of
individual texts. This may not be however the case with multiple
document summarization, where a redundancy removal technique
– such as a maximum threshold imposed on the sentence similar-
ity – needs to be implemented.
3Weights are listed to the right or above the edge they cor-
respond to. Similar weights are computed for each edge in the
graph, but are not displayed due to space restrictions.
4The evaluation is done using the Ngram(1,1) setting of
ROUGE, which was found to have the highest correlation with hu-
man judgments, at a confidence level of 95%. Only the first 100
words in each summary are considered.
</bodyText>
<figureCaption confidence="0.983167">
Figure 1: Sample graph build for sentence extraction
from a newspaper article.
</figureCaption>
<bodyText confidence="0.9984965">
We evaluate the summaries produced by TextRank
using each of the three graph-based ranking algo-
rithms described in Section 2. Table 1 shows the re-
sults obtained with each algorithm, when using graphs
that are: (a) undirected, (b) directed forward, or (c) di-
rected backward.
For a comparative evaluation, Table 2 shows the re-
sults obtained on this data set by the top 5 (out of 15)
performing systems participating in the single docu-
ment summarization task at DUC 2002 (DUC, 2002).
It also lists the baseline performance, computed for
100-word summaries generated by taking the first sen-
tences in each article.
Discussion. The TextRank approach to sentence ex-
traction succeeds in identifying the most important
sentences in a text based on information exclusively
</bodyText>
<figure confidence="0.996218341176471">
3: BC−HurricaineGilbert, 09−11 339
4: BC−Hurricaine Gilbert, 0348
5: Hurricaine Gilbert heads toward Dominican Coast
6: By Ruddy Gonzalez
7: Associated Press Writer
8: Santo Domingo, Dominican Republic (AP)
9: Hurricaine Gilbert Swept towrd the Dominican Republic Sunday, and the Civil Defense
alerted its heavily populated south coast to prepare for high winds, heavy rains, and high seas.
10: The storm was approaching from the southeast with sustained winds of 75 mph gusting
to 92 mph.
11: &amp;quot;There is no need for alarm,&amp;quot; Civil Defense Director Eugenio Cabral said in a television
alert shortly after midnight Saturday.
12: Cabral said residents of the province of Barahona should closely follow Gilbert’s movement.
13: An estimated 100,000 people live in the province, including 70,000 in the city of Barahona,
about 125 miles west of Santo Domingo.
14. Tropical storm Gilbert formed in the eastern Carribean and strenghtened into a hurricaine
Saturday night.
15: The National Hurricaine Center in Miami reported its position at 2 a.m. Sunday at latitude
16.1 north, longitude 67.5 west, about 140 miles south of Ponce, Puerto Rico, and 200 miles
southeast of Santo Domingo.
16: The National Weather Service in San Juan, Puerto Rico, said Gilbert was moving westard
at 15 mph with a &amp;quot;broad area of cloudiness and heavy weather&amp;quot; rotating around the center
of the storm.
17. The weather service issued a flash flood watch for Puerto Rico and the Virgin Islands until
at least 6 p.m. Sunday.
18: Strong winds associated with the Gilbert brought coastal flooding, strong southeast winds,
and up to 12 feet to Puerto Rico’s south coast.
19: There were no reports on casualties.
20: San Juan, on the north coast, had heavy rains and gusts Saturday, but they subsided during
the night.
21: On Saturday, Hurricane Florence was downgraded to a tropical storm, and its remnants
pushed inland from the U.S. Gulf Coast.
22: Residents returned home, happy to find little damage from 90 mph winds and sheets of rain.
23: Florence, the sixth named storm of the 1988 Atlantic storm season, was the second hurricane.
24: The first, Debby, reached minimal hurricane strength briefly before hitting the Mexican coast
last month.
20
[0.84]
[0.50]24 4 [0.71]
0.15
[1.02]
21
[0.15]
0.19
7
0.15
0.55
[0.15]19
8
0.35
9
0.15
[0.70]
[1.83]
0.59
0.30
[1.58]
18
0.29
10
[0.99]
0.15
0.27
[0.70]
17
11
[0.56]
0.14
0.15
0.16
15 14 13
[1.36] [1.09]
16
[1.65]
23
[0.80]
5
[1.20]
22
[0.70]
6
[0.15]
12
[0.93]
[0.76]
</figure>
<table confidence="0.951549727272727">
Algorithm Graph
Undirected Dir. forward Dir. backward
HITS� 0.4912 0.4584 0.5023
� 0.4912 0.5023 0.4584
HITS� 0.4878 0.4538 0.3910
� 0.4878 0.3910 0.4538
POS� 0.4904 0.4202 0.5008
�
POS�
�
PageRank
</table>
<tableCaption confidence="0.914908">
Table 1: Results for text summarization using Text-
</tableCaption>
<bodyText confidence="0.847216411764706">
Rank sentence extraction. Graph-based ranking al-
gorithms: HITS, Positional Function, PageRank.
Graphs: undirected, directed forward, directed back-
ward.
Top 5 systems (DUC, 2002) Baseline
S27 S31 S28 S21 S29
0.5011 0.4914 0.4890 0.4869 0.4681 0.4799
Table 2: Results for single document summarization
for top 5 (out of 15) DUC 2002 systems, and baseline.
drawn from the text itself. Unlike other supervised
systems, which attempt to learn what makes a good
summary by training on collections of summaries built
for other articles, TextRank is fully unsupervised, and
relies only on the given text to derive an extractive
summary.
Among all algorithms, the HITSA and PageRank
algorithms provide the best performance, at par with
the best performing system from DUC 20025. This
proves that graph-based ranking algorithms, previ-
ously found successful in Web link analysis, can be
turned into a state-of-the-art tool for sentence extrac-
tion when applied to graphs extracted from texts.
Notice that TextRank goes beyond the sentence
“connectivity” in a text. For instance, sentence 15 in
the example provided in Figure 1 would not be iden-
tified as “important” based on the number of connec-
tions it has with other vertices in the graph6, but it is
identified as “important” by TextRank (and by humans
– according to the reference summaries for this text).
Another important advantage of TextRank is that it
gives a ranking over all sentences in a text – which
means that it can be easily adapted to extracting very
short summaries, or longer more explicative sum-
maries, consisting of more than 100 words.
</bodyText>
<sectionHeader confidence="0.999898" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.975530571428571">
Sentence extraction is considered to be an important
first step for automatic text summarization. As a con-
sequence, there is a large body of work on algorithms
5Notice that rows two and four in Table 1 are in fact redundant,
since the `hub” (`weakness”) variations of the HITS (Positional)
algorithms can be derived from their `authority” (`power”) coun-
terparts by reversing the edge orientation in the graphs.
</bodyText>
<tableCaption confidence="0.524166666666667">
6Only seven edges are incident with vertex 15, less than e.g.
eleven edges incident with vertex 14 – not selected as `important”
by TextRank.
</tableCaption>
<bodyText confidence="0.999990636363636">
for sentence extraction undertaken as part of the DUC
evaluation exercises. Previous approaches include su-
pervised learning (Teufel and Moens, 1997), vectorial
similarity computed between an initial abstract and
sentences in the given document, or intra-document
similarities (Salton et al., 1997). It is also notable the
study reported in (Lin and Hovy, 2003b) discussing
the usefulness and limitations of automatic sentence
extraction for summarization, which emphasizes the
need of accurate tools for sentence extraction, as an
integral part of automatic summarization systems.
</bodyText>
<sectionHeader confidence="0.99959" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999969">
Intuitively, TextRank works well because it does not
only rely on the local context of a text unit (ver-
tex), but rather it takes into account information re-
cursively drawn from the entire text (graph). Through
the graphs it builds on texts, TextRank identifies con-
nections between various entities in a text, and im-
plements the concept of recommendation. A text unit
recommends other related text units, and the strength
of the recommendation is recursively computed based
on the importance of the units making the recommen-
dation. In the process of identifying important sen-
tences in a text, a sentence recommends another sen-
tence that addresses similar concepts as being useful
for the overall understanding of the text. Sentences
that are highly recommended by other sentences are
likely to be more informative for the given text, and
will be therefore given a higher score.
An important aspect of TextRank is that it does
not require deep linguistic knowledge, nor domain
or language specific annotated corpora, which makes
it highly portable to other domains, genres, or lan-
guages.
</bodyText>
<sectionHeader confidence="0.996457" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99991456">
S. Brin and L. Page. 1998. The anatomy of a large-scale hypertextual Web
search engine. Computer Networks and ISDN Systems, 30(1–7).
DUC. 2002. Document understanding conference 2002. http://www-
nlpir.nist.gov/projects/duc/.
P.J. Herings, G. van der Laan, and D. Talman. 2001. Measuring the power
of nodes in digraphs. Technical report, Tinbergen Institute.
J.M. Kleinberg. 1999. Authoritative sources in a hyperlinked environ-
ment. Journal of the ACM, 46(5):604–632.
C.Y. Lin and E.H. Hovy. 2003a. Automatic evaluation of summaries using
n-gram co-occurrence statistics. In Proceedings of Human Language
Technology Conference (HLT-NAACL 2003), Edmonton, Canada, May.
C.Y. Lin and E.H. Hovy. 2003b. The potential and limitations of sentence
extraction for summarization. In Proceedings of the HLT/NAACL
Workshop on Automatic Summarization, Edmonton, Canada, May.
R. Mihalcea and P. Tarau. 2004. TextRank – bringing order into texts.
R. Mihalcea, P. Tarau, and E. Figa. 2004. PageRank on semantic net-
works, with application to word sense disambiguation. In Proceed-
ings of the 20st International Conference on Computational Linguis-
tics (COLING 2004), Geneva, Switzerland, August.
G. Salton, A. Singhal, M. Mitra, and C. Buckley. 1997. Automatic text
structuring and summarization. Information Processing and Manage-
ment, 2(32).
S. Teufel and M. Moens. 1997. Sentence extraction as a classification
task. In ACL/EACL workshop on ”Intelligent and scalable Text sum-
marization”, pages 58–65, Madrid, Spain.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.925295">
<title confidence="0.987115">Graph-based Ranking Algorithms for Sentence Extraction, Applied to Text Summarization</title>
<author confidence="0.99936">Rada Mihalcea</author>
<affiliation confidence="0.99944">Department of Computer Science University of North Texas</affiliation>
<email confidence="0.999816">rada@cs.unt.edu</email>
<abstract confidence="0.992527714285714">This paper presents an innovative unsupervised method for automatic sentence extraction using graphbased ranking algorithms. We evaluate the method in the context of a text summarization task, and show that the results obtained compare favorably with previously published results on established benchmarks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Brin</author>
<author>L Page</author>
</authors>
<title>The anatomy of a large-scale hypertextual Web search engine.</title>
<date>1998</date>
<journal>Computer Networks and ISDN Systems,</journal>
<pages>30--1</pages>
<contexts>
<context position="631" citStr="Brin and Page, 1998" startWordPosition="82" endWordPosition="85"> Ranking Algorithms for Sentence Extraction, Applied to Text Summarization Rada Mihalcea Department of Computer Science University of North Texas rada@cs.unt.edu Abstract This paper presents an innovative unsupervised method for automatic sentence extraction using graphbased ranking algorithms. We evaluate the method in the context of a text summarization task, and show that the results obtained compare favorably with previously published results on established benchmarks. 1 Introduction Graph-based ranking algorithms, such as Kleinberg’s HITS algorithm (Kleinberg, 1999) or Google’s PageRank (Brin and Page, 1998), have been traditionally and successfully used in citation analysis, social networks, and the analysis of the link-structure of the World Wide Web. In short, a graph-based ranking algorithm is a way of deciding on the importance of a vertex within a graph, by taking into account global information recursively computed from the entire graph, rather than relying only on local vertex-specific information. A similar line of thinking can be applied to lexical or semantic graphs extracted from natural language documents, resulting in a graph-based ranking model called TextRank (Mihalcea and Tarau, </context>
<context position="3706" citStr="Brin and Page, 1998" startWordPosition="589" endWordPosition="592">cores – an “authority” score, and a “hub” score: HITSA(Vi) = E HITSH(Vj) (1) VjEIn(Vi) HITSH(Vi) = E HITSA(Vj) (2) Vj EOut(Vi) 2.2 Positional Power Function Introduced by (Herings et al., 2001), the positional power function is a ranking algorithm that determines the score of a vertex as a function that combines both the number of its successors, and the score of its successors. POSP (Vi) = V 1 E (1+POSP(Vj)) (3) Vj EOut(Vi) The counterpart of thepositional power function is the positional weakness function, defined as: P OSW (Vi) = |V 1 Vj EIn(Vi) E (1 + POSW (Vj)) (4) 2.3 PageRank PageRank (Brin and Page, 1998) is perhaps one of the most popular ranking algorithms, and was designed as a method for Web link analysis. Unlike other ranking algorithms, PageRank integrates the impact of both incoming and outgoing links into one single model, and therefore it produces only one set of scores:  HITSWA (Vi) = Vj EIn(Vi) wjiHITSWH (Vj) (6) wijHITSWA (Vj) (7)  HITSW H (Vi) = Vj EOut(Vi)  POSWP (Vi) = 1 |V | (1 + wijPOSWP (Vj)) (8) Vj EOut(Vi)  PR(Vi) = (1 − d) + d * VjEIn(Vi) PR(Vj) (5) |Out(Vj )|  POSWW (Vi) = 1 |V | (1 + wjiPOSWW (Vj)) (9) Vj EIn(Vi) where d is a parameter that is set between 0 and 1 1.</context>
<context position="6260" citStr="Brin and Page, 1998" startWordPosition="1028" endWordPosition="1031">. However, in our TextRank model the graphs are build from natural language texts, and may include multiple or partial links between the units (vertices) that are extracted from text. It may be therefore useful to indicate and incorporate into the model the “strength” of the connection between two vertices Vi and Vj as a weight wij added to the corresponding edge that connects the two vertices. Consequently, we introduce new formulae for graph-based ranking that take into account edge weights when computing the score associated with a vertex in the graph. &apos;The factor d is usually set at 0.85 (Brin and Page, 1998), and this is the value we are also using in our implementation.  PRW(Vi) = (1 − d) + d * Vj EIn(Vi) While the final vertex scores (and therefore rankings) for weighted graphs differ significantly as compared to their unweighted alternatives, the number of iterations to convergence and the shape of the convergence curves is almost identical for weighted and unweighted graphs. 3 Sentence Extraction To enable the application of graph-based ranking algorithms to natural language texts, TextRank starts by building a graph that represents the text, and interconnects words or other text entities wi</context>
</contexts>
<marker>Brin, Page, 1998</marker>
<rawString>S. Brin and L. Page. 1998. The anatomy of a large-scale hypertextual Web search engine. Computer Networks and ISDN Systems, 30(1–7).</rawString>
</citation>
<citation valid="true">
<authors>
<author>DUC</author>
</authors>
<title>Document understanding conference</title>
<date>2002</date>
<note>http://wwwnlpir.nist.gov/projects/duc/.</note>
<contexts>
<context position="9911" citStr="DUC, 2002" startWordPosition="1636" endWordPosition="1637"> applied on an undirected graph. The sentences with the highest rank are selected for inclusion in the abstract. For this sample article, sentences with id-s 9, 15, 16, 18 are extracted, resulting in a summary of about 100 words, which according to automatic evaluation measures, is ranked the second among summaries produced by 15 other systems (see Section 4 for evaluation methodology). 4 Evaluation The TextRank sentence extraction algorithm is evaluated in the context of a single-document summarization task, using 567 news articles provided during the Document Understanding Evaluations 2002 (DUC, 2002). For each article, TextRank generates a 100-words summary — the task undertaken by other systems participating in this single document summarization task. For evaluation, we are using the ROUGE evaluation toolkit, which is a method based on Ngram statistics, found to be highly correlated with human evaluations (Lin and Hovy, 2003a). Two manually produced reference summaries are provided, and used in the evaluation process4. 2In single documents, sentences with highly similar content are very rarely if at all encountered, and therefore sentence redundancy does not have a significant impact on </context>
<context position="11693" citStr="DUC 2002" startWordPosition="1930" endWordPosition="1931">95%. Only the first 100 words in each summary are considered. Figure 1: Sample graph build for sentence extraction from a newspaper article. We evaluate the summaries produced by TextRank using each of the three graph-based ranking algorithms described in Section 2. Table 1 shows the results obtained with each algorithm, when using graphs that are: (a) undirected, (b) directed forward, or (c) directed backward. For a comparative evaluation, Table 2 shows the results obtained on this data set by the top 5 (out of 15) performing systems participating in the single document summarization task at DUC 2002 (DUC, 2002). It also lists the baseline performance, computed for 100-word summaries generated by taking the first sentences in each article. Discussion. The TextRank approach to sentence extraction succeeds in identifying the most important sentences in a text based on information exclusively 3: BC−HurricaineGilbert, 09−11 339 4: BC−Hurricaine Gilbert, 0348 5: Hurricaine Gilbert heads toward Dominican Coast 6: By Ruddy Gonzalez 7: Associated Press Writer 8: Santo Domingo, Dominican Republic (AP) 9: Hurricaine Gilbert Swept towrd the Dominican Republic Sunday, and the Civil Defense alerted it</context>
<context position="14899" citStr="DUC, 2002" startWordPosition="2451" endWordPosition="2452">15 [0.70] [1.83] 0.59 0.30 [1.58] 18 0.29 10 [0.99] 0.15 0.27 [0.70] 17 11 [0.56] 0.14 0.15 0.16 15 14 13 [1.36] [1.09] 16 [1.65] 23 [0.80] 5 [1.20] 22 [0.70] 6 [0.15] 12 [0.93] [0.76] Algorithm Graph Undirected Dir. forward Dir. backward HITS� 0.4912 0.4584 0.5023 � 0.4912 0.5023 0.4584 HITS� 0.4878 0.4538 0.3910 � 0.4878 0.3910 0.4538 POS� 0.4904 0.4202 0.5008 � POS� � PageRank Table 1: Results for text summarization using TextRank sentence extraction. Graph-based ranking algorithms: HITS, Positional Function, PageRank. Graphs: undirected, directed forward, directed backward. Top 5 systems (DUC, 2002) Baseline S27 S31 S28 S21 S29 0.5011 0.4914 0.4890 0.4869 0.4681 0.4799 Table 2: Results for single document summarization for top 5 (out of 15) DUC 2002 systems, and baseline. drawn from the text itself. Unlike other supervised systems, which attempt to learn what makes a good summary by training on collections of summaries built for other articles, TextRank is fully unsupervised, and relies only on the given text to derive an extractive summary. Among all algorithms, the HITSA and PageRank algorithms provide the best performance, at par with the best performing system from DUC 20025. This pr</context>
</contexts>
<marker>DUC, 2002</marker>
<rawString>DUC. 2002. Document understanding conference 2002. http://wwwnlpir.nist.gov/projects/duc/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P J Herings</author>
<author>G van der Laan</author>
<author>D Talman</author>
</authors>
<title>Measuring the power of nodes in digraphs.</title>
<date>2001</date>
<tech>Technical report,</tech>
<institution>Tinbergen Institute.</institution>
<marker>Herings, van der Laan, Talman, 2001</marker>
<rawString>P.J. Herings, G. van der Laan, and D. Talman. 2001. Measuring the power of nodes in digraphs. Technical report, Tinbergen Institute.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Kleinberg</author>
</authors>
<title>Authoritative sources in a hyperlinked environment.</title>
<date>1999</date>
<journal>Journal of the ACM,</journal>
<volume>46</volume>
<issue>5</issue>
<contexts>
<context position="2768" citStr="Kleinberg, 1999" startWordPosition="428" endWordPosition="430">three graph-based ranking algorithms – previously found to be successful on a range of ranking problems. We also show how these algorithms can be adapted to undirected or weighted graphs, which are particularly useful in the context of text-based ranking applications. Let G = (V, E) be a directed graph with the set of vertices V and set of edges E, where E is a subset of V × V . For a given vertex Vi, let In(Vi) be the set of vertices that point to it (predecessors), and let Out(Vi) be the set of vertices that vertex Vi points to (successors). 2.1 HITS HITS (Hyperlinked Induced Topic Search) (Kleinberg, 1999) is an iterative algorithm that was designed for ranking Web pages according to their degree of “authority”. The HITS algorithm makes a distinction between “authorities” (pages with a large number of incoming links) and “hubs” (pages with a large number of outgoing links). For each vertex, HITS produces two sets of scores – an “authority” score, and a “hub” score: HITSA(Vi) = E HITSH(Vj) (1) VjEIn(Vi) HITSH(Vi) = E HITSA(Vj) (2) Vj EOut(Vi) 2.2 Positional Power Function Introduced by (Herings et al., 2001), the positional power function is a ranking algorithm that determines the score of a ver</context>
</contexts>
<marker>Kleinberg, 1999</marker>
<rawString>J.M. Kleinberg. 1999. Authoritative sources in a hyperlinked environment. Journal of the ACM, 46(5):604–632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Y Lin</author>
<author>E H Hovy</author>
</authors>
<title>Automatic evaluation of summaries using n-gram co-occurrence statistics.</title>
<date>2003</date>
<booktitle>In Proceedings of Human Language Technology Conference (HLT-NAACL 2003),</booktitle>
<location>Edmonton, Canada,</location>
<contexts>
<context position="10243" citStr="Lin and Hovy, 2003" startWordPosition="1686" endWordPosition="1689">d by 15 other systems (see Section 4 for evaluation methodology). 4 Evaluation The TextRank sentence extraction algorithm is evaluated in the context of a single-document summarization task, using 567 news articles provided during the Document Understanding Evaluations 2002 (DUC, 2002). For each article, TextRank generates a 100-words summary — the task undertaken by other systems participating in this single document summarization task. For evaluation, we are using the ROUGE evaluation toolkit, which is a method based on Ngram statistics, found to be highly correlated with human evaluations (Lin and Hovy, 2003a). Two manually produced reference summaries are provided, and used in the evaluation process4. 2In single documents, sentences with highly similar content are very rarely if at all encountered, and therefore sentence redundancy does not have a significant impact on the summarization of individual texts. This may not be however the case with multiple document summarization, where a redundancy removal technique – such as a maximum threshold imposed on the sentence similarity – needs to be implemented. 3Weights are listed to the right or above the edge they correspond to. Similar weights are co</context>
<context position="17247" citStr="Lin and Hovy, 2003" startWordPosition="2831" endWordPosition="2834">rithms can be derived from their `authority” (`power”) counterparts by reversing the edge orientation in the graphs. 6Only seven edges are incident with vertex 15, less than e.g. eleven edges incident with vertex 14 – not selected as `important” by TextRank. for sentence extraction undertaken as part of the DUC evaluation exercises. Previous approaches include supervised learning (Teufel and Moens, 1997), vectorial similarity computed between an initial abstract and sentences in the given document, or intra-document similarities (Salton et al., 1997). It is also notable the study reported in (Lin and Hovy, 2003b) discussing the usefulness and limitations of automatic sentence extraction for summarization, which emphasizes the need of accurate tools for sentence extraction, as an integral part of automatic summarization systems. 6 Conclusions Intuitively, TextRank works well because it does not only rely on the local context of a text unit (vertex), but rather it takes into account information recursively drawn from the entire text (graph). Through the graphs it builds on texts, TextRank identifies connections between various entities in a text, and implements the concept of recommendation. A text un</context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>C.Y. Lin and E.H. Hovy. 2003a. Automatic evaluation of summaries using n-gram co-occurrence statistics. In Proceedings of Human Language Technology Conference (HLT-NAACL 2003), Edmonton, Canada, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Y Lin</author>
<author>E H Hovy</author>
</authors>
<title>The potential and limitations of sentence extraction for summarization.</title>
<date>2003</date>
<booktitle>In Proceedings of the HLT/NAACL Workshop on Automatic Summarization,</booktitle>
<location>Edmonton, Canada,</location>
<contexts>
<context position="10243" citStr="Lin and Hovy, 2003" startWordPosition="1686" endWordPosition="1689">d by 15 other systems (see Section 4 for evaluation methodology). 4 Evaluation The TextRank sentence extraction algorithm is evaluated in the context of a single-document summarization task, using 567 news articles provided during the Document Understanding Evaluations 2002 (DUC, 2002). For each article, TextRank generates a 100-words summary — the task undertaken by other systems participating in this single document summarization task. For evaluation, we are using the ROUGE evaluation toolkit, which is a method based on Ngram statistics, found to be highly correlated with human evaluations (Lin and Hovy, 2003a). Two manually produced reference summaries are provided, and used in the evaluation process4. 2In single documents, sentences with highly similar content are very rarely if at all encountered, and therefore sentence redundancy does not have a significant impact on the summarization of individual texts. This may not be however the case with multiple document summarization, where a redundancy removal technique – such as a maximum threshold imposed on the sentence similarity – needs to be implemented. 3Weights are listed to the right or above the edge they correspond to. Similar weights are co</context>
<context position="17247" citStr="Lin and Hovy, 2003" startWordPosition="2831" endWordPosition="2834">rithms can be derived from their `authority” (`power”) counterparts by reversing the edge orientation in the graphs. 6Only seven edges are incident with vertex 15, less than e.g. eleven edges incident with vertex 14 – not selected as `important” by TextRank. for sentence extraction undertaken as part of the DUC evaluation exercises. Previous approaches include supervised learning (Teufel and Moens, 1997), vectorial similarity computed between an initial abstract and sentences in the given document, or intra-document similarities (Salton et al., 1997). It is also notable the study reported in (Lin and Hovy, 2003b) discussing the usefulness and limitations of automatic sentence extraction for summarization, which emphasizes the need of accurate tools for sentence extraction, as an integral part of automatic summarization systems. 6 Conclusions Intuitively, TextRank works well because it does not only rely on the local context of a text unit (vertex), but rather it takes into account information recursively drawn from the entire text (graph). Through the graphs it builds on texts, TextRank identifies connections between various entities in a text, and implements the concept of recommendation. A text un</context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>C.Y. Lin and E.H. Hovy. 2003b. The potential and limitations of sentence extraction for summarization. In Proceedings of the HLT/NAACL Workshop on Automatic Summarization, Edmonton, Canada, May. R. Mihalcea and P. Tarau. 2004. TextRank – bringing order into texts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>P Tarau</author>
<author>E Figa</author>
</authors>
<title>PageRank on semantic networks, with application to word sense disambiguation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20st International Conference on Computational Linguistics (COLING 2004),</booktitle>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="1598" citStr="Mihalcea et al., 2004" startWordPosition="233" endWordPosition="236">ather than relying only on local vertex-specific information. A similar line of thinking can be applied to lexical or semantic graphs extracted from natural language documents, resulting in a graph-based ranking model called TextRank (Mihalcea and Tarau, 2004), which can be used for a variety of natural language processing applications where knowledge drawn from an entire text is used in making local ranking/selection decisions. Such text-oriented ranking methods can be applied to tasks ranging from automated extraction of keyphrases, to extractive summarization and word sense disambiguation (Mihalcea et al., 2004). In this paper, we investigate a range of graphbased ranking algorithms, and evaluate their application to automatic unsupervised sentence extraction in the context of a text summarization task. We show that the results obtained with this new unsupervised method are competitive with previously developed state-of-the-art systems. 2 Graph-Based Ranking Algorithms Graph-based ranking algorithms are essentially a way of deciding the importance of a vertex within a graph, based on information drawn from the graph structure. In this section, we present three graph-based ranking algorithms – previou</context>
</contexts>
<marker>Mihalcea, Tarau, Figa, 2004</marker>
<rawString>R. Mihalcea, P. Tarau, and E. Figa. 2004. PageRank on semantic networks, with application to word sense disambiguation. In Proceedings of the 20st International Conference on Computational Linguistics (COLING 2004), Geneva, Switzerland, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>A Singhal</author>
<author>M Mitra</author>
<author>C Buckley</author>
</authors>
<title>Automatic text structuring and summarization.</title>
<date>1997</date>
<booktitle>Information Processing and Management,</booktitle>
<volume>2</volume>
<issue>32</issue>
<contexts>
<context position="17185" citStr="Salton et al., 1997" startWordPosition="2819" endWordPosition="2822"> the `hub” (`weakness”) variations of the HITS (Positional) algorithms can be derived from their `authority” (`power”) counterparts by reversing the edge orientation in the graphs. 6Only seven edges are incident with vertex 15, less than e.g. eleven edges incident with vertex 14 – not selected as `important” by TextRank. for sentence extraction undertaken as part of the DUC evaluation exercises. Previous approaches include supervised learning (Teufel and Moens, 1997), vectorial similarity computed between an initial abstract and sentences in the given document, or intra-document similarities (Salton et al., 1997). It is also notable the study reported in (Lin and Hovy, 2003b) discussing the usefulness and limitations of automatic sentence extraction for summarization, which emphasizes the need of accurate tools for sentence extraction, as an integral part of automatic summarization systems. 6 Conclusions Intuitively, TextRank works well because it does not only rely on the local context of a text unit (vertex), but rather it takes into account information recursively drawn from the entire text (graph). Through the graphs it builds on texts, TextRank identifies connections between various entities in a</context>
</contexts>
<marker>Salton, Singhal, Mitra, Buckley, 1997</marker>
<rawString>G. Salton, A. Singhal, M. Mitra, and C. Buckley. 1997. Automatic text structuring and summarization. Information Processing and Management, 2(32).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Teufel</author>
<author>M Moens</author>
</authors>
<title>Sentence extraction as a classification task.</title>
<date>1997</date>
<booktitle>In ACL/EACL workshop on ”Intelligent and scalable Text summarization”,</booktitle>
<pages>58--65</pages>
<location>Madrid,</location>
<contexts>
<context position="17036" citStr="Teufel and Moens, 1997" startWordPosition="2799" endWordPosition="2802">text summarization. As a consequence, there is a large body of work on algorithms 5Notice that rows two and four in Table 1 are in fact redundant, since the `hub” (`weakness”) variations of the HITS (Positional) algorithms can be derived from their `authority” (`power”) counterparts by reversing the edge orientation in the graphs. 6Only seven edges are incident with vertex 15, less than e.g. eleven edges incident with vertex 14 – not selected as `important” by TextRank. for sentence extraction undertaken as part of the DUC evaluation exercises. Previous approaches include supervised learning (Teufel and Moens, 1997), vectorial similarity computed between an initial abstract and sentences in the given document, or intra-document similarities (Salton et al., 1997). It is also notable the study reported in (Lin and Hovy, 2003b) discussing the usefulness and limitations of automatic sentence extraction for summarization, which emphasizes the need of accurate tools for sentence extraction, as an integral part of automatic summarization systems. 6 Conclusions Intuitively, TextRank works well because it does not only rely on the local context of a text unit (vertex), but rather it takes into account information</context>
</contexts>
<marker>Teufel, Moens, 1997</marker>
<rawString>S. Teufel and M. Moens. 1997. Sentence extraction as a classification task. In ACL/EACL workshop on ”Intelligent and scalable Text summarization”, pages 58–65, Madrid, Spain.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>