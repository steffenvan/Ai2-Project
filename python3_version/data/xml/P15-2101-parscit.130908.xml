<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001744">
<title confidence="0.984796">
Reducing infrequent-token perplexity via variational corpora
</title>
<author confidence="0.997896">
Yusheng Xie1,# Pranjal Daga1 Yu Cheng2 Kunpeng Zhang3 Ankit Agrawal1 Alok Choudhary1
</author>
<affiliation confidence="0.8862105">
1 Northwestern University 2 IBM Research 3 University of Maryland
Evanston, IL USA Yorktown Heights, NY USA College Park, MD USA
</affiliation>
<email confidence="0.9961">
# yxi389@eecs.northwestern.edu
</email>
<sectionHeader confidence="0.993748" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997836">
Recurrent neural network (RNN) is recog-
nized as a powerful language model (LM).
We investigate deeper into its performance
portfolio, which performs well on frequent
grammatical patterns but much less so on
less frequent terms. Such portfolio is ex-
pected and desirable in applications like
autocomplete, but is less useful in social
content analysis where many creative, un-
expected usages occur (e.g., URL inser-
tion). We adapt a generic RNN model and
show that, with variational training cor-
pora and epoch unfolding, the model im-
proves its performance for the task of URL
insertion suggestions.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999950722222222">
Just 135 most frequent words account for 50% text
of the entire Brown corpus (Francis and Kucera,
1979). But over 44% (22,010 out of 49,815) of
Brown’s vocabulary are hapax legomena1. The in-
tricate relationship between vocabulary words and
their utterance frequency results in some impor-
tant advancements in natural language process-
ing (NLP). For example, tf-idf results from rules
applied to word frequencies in global and local
context (Manning and Sch¨utze, 1999). A com-
mon preprocessing step for tf-idf is filtering rare
words, which is usually justified for two reasons.
First, low frequency cutoff promises computa-
tional speedup due to Zipf’s law (1935). Second,
many believe that most NLP and machine learning
algorithms demand repetitive patterns and reoc-
currences, which are by definition missing in low
frequency words.
</bodyText>
<subsectionHeader confidence="0.979836">
1.1 Should infrequent words be filtered?
</subsectionHeader>
<bodyText confidence="0.9346485">
Infrequent words have high probability of becom-
ing frequent as we consider them in a larger con-
</bodyText>
<footnote confidence="0.648472">
1Words appear only once in corpus.
</footnote>
<bodyText confidence="0.99951775">
text (e.g., Ishmael, the protagonist name in Moby-
Dick, appears merely once in the novel’s dialogues
but is a highly referenced word in the discus-
sions/critiques around the novel). In many modern
NLP applications, context grows constantly: fresh
news articles come out on CNN and New York
Times everyday; conversations on Twitter are up-
dated in real time. In processing online social me-
dia text, it would seem premature to filter words
simply due to infrequency, the kind of infrequency
that can be eliminated by taking a larger corpus
available from the same source.
To further undermine the conventional justifica-
tion, computational speedup is attenuated in RNN-
based LMs (compared to n-gram LMs), thanks to
modern GPU architecture. We train a large RNN-
LSTM (long short-term memory unit) (Hochreiter
and Schmidhuber, 1997) model as our LM on two
versions of Jane Austen’s complete works. Deal-
ing with 33% less vocabulary in the filtered ver-
sion, the model only gains marginally on running
time or memory usage. In Table 1.1, “Filtered cor-
pus” filters out all the hapax legomena in “Full cor-
pus”.
</bodyText>
<table confidence="0.9946698">
Full corpus Filtered corpus
corpus length 756,273 751,325
vocab. size 15,125 10,177
running time 1,446 sec 1,224 sec
GPU memory 959 MB 804 MB
</table>
<tableCaption confidence="0.98694">
Table 1: Filtered corpus gains little in running time
or memory usage when using a RNN LM.
</tableCaption>
<bodyText confidence="0.997552666666667">
Since RNN LMs suffer only small penalty in
keeping the full corpus, can we take advantage of
this situation to improve the LM?
</bodyText>
<subsectionHeader confidence="0.996913">
1.2 Improving performance portfolio of LM
</subsectionHeader>
<bodyText confidence="0.8262205">
One improvement is LM’s performance portfo-
lio. A LM’s performance is usually quantified as
</bodyText>
<page confidence="0.911769">
609
</page>
<bodyText confidence="0.953222684210526">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 609–615,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
perplexity, which is exponentialized negative log-
likelihood in predictions.
For our notation, let VX denote the vocabu-
lary of words that appear in a text corpus X =
{x1, x2,...}. Given a sequence x1, x2, ... , xm−1,
where each x E VX, the LM predicts the next
in sequence, xm E VX, as a probability distribu-
tion over the entire vocabulary V (its prediction
denoted as p). If vm E VX is the true token at
position m, the model’s perplexity at index m is
quantified as exp(− ln(p[vm])). The training goal
is to minimize average perplexity across X.
However, a deeper look into perplexity beyond
corpus-wide average reveals interesting findings.
Using the same model setting as for Table 1.1,
Figure 1 illustrates the relationship between word-
level perplexity and its frequency in corpus. In
general, the less frequent a word appears, the
more unpredictable it becomes. In Table 1.2, the
trained model achieves an average perplexity of
78 on filtered corpus. But also shown in Table
1.2, many common words register with perplexity
over 1,000, which means they are practically un-
predictable. More details are summarized in Table
1.2. The LM achieves exceptionally low perplex-
ity on words such as &lt;apostr.&gt;s (’s, the posses-
sive case), &lt;comma&gt; (, the comma). And these
tokens’ high frequencies in corpus have promised
the model’s average performance. Meanwhile, the
LM has bafflingly high perplexity on common-
place words such as read and considering.
Figure 1: (best viewed in color) We look at word
level perplexity with respect to the word frequency
in corpus. The less frequent a word appears, the
more unpredictable it becomes.
</bodyText>
<sectionHeader confidence="0.995554" genericHeader="introduction">
2 Methodology
</sectionHeader>
<bodyText confidence="0.99905375">
We describe a novel approach of constructing and
utilizing pre-training corpus that eventually reduce
LMs’s high perplexity on rare tokens. The stan-
dard way to utilize a pre-training corpus W is to
</bodyText>
<table confidence="0.9993038">
Token Freq. Perplexity 1 Perplexity 2
corpus avg. N/A 78 82
&lt;apostr.&gt;s 4,443 1.1 1.1
of 23,046 4.9 5.0
&lt;comma&gt; 57,552 5.2 5.1
been 3,452 5.4 5.7
read 224 3,658 3,999
quiet 108 6,807 6,090
returning 89 7,764 6,268
considering 80 9,573 8,451
</table>
<tableCaption confidence="0.93491875">
Table 2: A close look at RNN-LSTM’s perplexity
at word level. “Perplexity 1” is model perplexity
based on filtered corpus (c.f., Table 1.1) and “Per-
plexity 2” is based on full corpus.
</tableCaption>
<bodyText confidence="0.999855272727272">
first train the model on W then fine-tune it on tar-
get corpus X. Thanks to availability of text, W
can be orders of magnitude larger than X, which
makes pre-training on W challenging.
A more efficient way to utilize W is to construct
variational corpora based on X and W. In the fol-
lowing subsections, we first describe how replace-
ment tokens are selected from a probability mass
function (pmf), which is built from W; then ex-
plain how the variational corpora variates with re-
placement tokens through epochs.
</bodyText>
<subsectionHeader confidence="0.994909">
2.1 Learn from pre-training corpus
</subsectionHeader>
<bodyText confidence="0.998649304347826">
One way to alleviate the impact from infrequent
vocabulary is to expose the model to a larger
and overarching pre-training corpus (Erhan et al.,
2010), if available. Let W be a larger corpus
than X and assume that VX C_ VW. For exam-
ple, if X is Herman Melville’s Moby-Dick, W
can be Melville’s complete works. Further, we
use VX,1 to denote the subset of VX that are ha-
pax legonema in corpus X; similarly, VX,n (for
n = 2,3, ...) denotes the subset of VX that occur
n times in X. Many hapax legomena in VX,1 are
likely to become more frequent tokens in VW.
Suppose that x E VX,1. Denoted by
ReplacePMF(W, VW, x) in Algorithm 1, we rep-
resent x as a probability mass function (pmf) over
{xi, x2, ...}, where each xz is selected from VW n
VX,n for n &gt; 1 using one of the two methods be-
low. For illustration purpose, suppose the hapax
legomenon, x, in question is matrimonial:
1) e.g., matrimony. Words that have very high
literal similarity with x. We measure literal sim-
ilarity using Jaro-Winkler measure, which is an
empirical, weighted measure based on string edit
</bodyText>
<figure confidence="0.987461703703703">
10000
Word level average perplexity
4
9000
8000
7000
6000
5000
4000
3000
2000
1000
0
word perplexity
word frequency (log scale)
65536
32768
Word frequency in corpus
16384
8192
4096
2048
1024
512
256
128
64
</figure>
<page confidence="0.983542">
610
</page>
<bodyText confidence="0.998156105263158">
distance. We set the measure threshold very high
(&gt; 0.93), which minimizes false positives as well
as captures many hapax legonema due to adv./adj.,
pl./singular (e.g, -y/-ily and -y/-ies).
2) e.g., marital Words that are direct syno/hypo-
nyms to x in the WordNet (Miller, 1995).
getContextAround(x0) function in Algorithm 1
simply extracts symmetric context words from
both left and right sides of x0. Although the in-
vestigated LM only uses left context in predicting
word x0, context right of x0 is still useful informa-
tion in general. Given a context word c right of x0,
the LM can learn x0’s predictability over c, which
is beneficial to the corpus-wide perplexity reduc-
tion.
In practice, we select no more than 5 substitu-
tion words from each method above. The prob-
ability mass on each x0i is proportional to its fre-
quency in W and then normalized by softmax:
</bodyText>
<equation confidence="0.9985605">
i)/ E5
pmf(x0 i) = freq(x0 k=1 freq(x0 k). This sub-
</equation>
<bodyText confidence="0.999572333333333">
stitution can help LMs learn better because we re-
place the un-trainable VX,1 tokens with tokens that
can be trained from the larger corpus W. In con-
cept, it is like explaining a new word to school kids
by defining it using vocabulary words in their ex-
isting knowledge.
</bodyText>
<subsectionHeader confidence="0.998286">
2.2 Unfold training epochs
</subsectionHeader>
<bodyText confidence="0.997415217391304">
Epoch in machine learning terminology usually
means a complete pass of the training dataset.
many iterative algorithms take dozens of epochs
on the same training data as they update the
model’s weights with smaller and smaller adjust-
ments through the epochs.
We refer to the the training process proposed
in Figure 2 (b) as “variational corpora”. Com-
pared to the traditional structure in Figure 2 (a),
the main advantage of using variational corpora is
the ability to freely adjust the corpus at each ver-
sion. Effectively, we unfold the training into sep-
arate epochs. This allows us to gradually incorpo-
rate the replacement tokens without severely dis-
torting the target corpus X, which is the learning
goal. In addition, variational corpora can further
regularize the training of LM in batch mode (Sri-
vastava et al., 2014).
Algorithm 1 constructs variational corpora
X(s) at epoch s. Assuming X(s + 1) being avail-
able, Algorithm 1 appends snippets, which are
sampled from W, into X(s) for the sth epoch. For
the last epoch s = 5, X(5) = X. As the epoch
</bodyText>
<figureCaption confidence="0.971297428571429">
Figure 2: Unfold the training process in units of
epochs. (a) Typical flow where model parses the
same corpus at each epoch. (b) The proposed
training architecture with variational corpora to in-
corporate the substitution algorithm.
Algorithm 1: Randomly constructs varia-
tional corpus at epoch s.
</figureCaption>
<bodyText confidence="0.9700185">
Input: W, X, VW, VX, VX,n, n, as defined in
Section 1.2&amp;2.1,
s, 5, current and max epoch number.
Output: X(s), variational corpus at epoch s
</bodyText>
<equation confidence="0.9982369">
1 X(s) +– X(s + 1)
2 for each x E VX,n do
p +– ReplacePMF(W, VW, x)
i +– Dirichlet(p).generate()
while i +– X.getNextIdxOf(x) do
x0 +– i.draw()
c +– W.getContextAround(x0)
c.substr([0, uniformRnd (0, S−s
S JcJ)1)
X(s).append(c)
</equation>
<sectionHeader confidence="0.521339" genericHeader="method">
10 return X(s)
</sectionHeader>
<bodyText confidence="0.99987">
number increases, fewer and shorter snippets are
appended, which alleviates training stress. By fix-
ing an n value, the algorithm applies to all words
in VX,n.
In addition, as a regularization trick (Mikolov
et al., 2013; Pascanu et al., 2013) , we use a uni-
form random context window (line 8) when inject-
ing snippets from W into X(s).
</bodyText>
<figure confidence="0.992252583333333">
epoch 1
update
parameters
model
epoch 2
update
parameters
model
Load in batch
Load in batch
same text
corpus
same text
corpus
same text
corpus
Load in batch
epoch...
.......
update
parameters
epoch...
epoch 1
epoch 2
model
model
overwrite
VX,n weights
overwrite
VX,n weights
randomized batch
randomized batch
text
corpus S1
text
corpus S2
text
corpus S3
update
parameters
randomized batch
3
4
5
6
7
8
9
</figure>
<page confidence="0.986092">
611
</page>
<table confidence="0.9985492">
Freq. nofilter 3filter ptw vc
10 28,542 (668.1) 23,649 (641.2) 27,986 (1,067.2) 20,994 (950.9)
100 1,180.3 (21.7) 1,158.2 (19.2) 735.8 (29.8) 755.8 (31.5)
1K 163.2 (12.9) 163.9 (12.2) 138.5 (14.1) 137.7 (15.7)
5K 47.5 (3.3) 47.2 (3.1) 40.2 (3.2) 40.2 (3.3)
10K 16.4 (0.31) 16.7 (0.29) 14.4 (0.42) 14.1 (0.41)
40K 7.6 (0.09) 7.6 (0.09) 7.0 (0.09) 7.0 (0.10)
all tokens 82.1 (2.0) 77.9 (1.9) 68.6 (2.1) 68.9 (2.1)
GPU memory 959MB 783MB 1.8GB 971MB
running time 1,446 sec 1,181 sec 9,061 sec 6,960 sec
</table>
<tableCaption confidence="0.997733">
Table 3: Experiments compare average perplexity produced by the proposed variational corpora approach
</tableCaption>
<bodyText confidence="0.718221461538462">
and other methods on a same test corpus. Bold fonts indicate best. “Freq.” indicates the average corpus-
frequency (e.g., Freq.=1K means that words in this group, on average, appear 1,000 times in corpus).
Perplexity numbers are averaged over 5 runs with standard deviation reported in parentheses. GPU
memory usage and running time are also reported for each method.
Err. type Context before True token LM prediction
False neg. &lt;unk&gt;, via, &lt;unk&gt;, banana, muffin, chocolate, URL to a cooking blog recipe
False neg. sewing, ideas, &lt;unk&gt;, inspiring, picture, on, URL to favim.com esty
False neg. nike, sports, fashion, &lt;unk&gt;, women, &lt;unk&gt;, URL to nelly.com macy
False pos. new, york, yankees, endless, summer, tee, &lt;unk&gt;, shop &lt;url&gt;
False pos. take, a, rest, from, your, #harrodssale, shopping &lt;url&gt;
Table 4: False positives and false negatives predicted by the model in the Pinterest application. The
context words preceding to token in questions are provided for easier analysis3.
Accuracy
</bodyText>
<sectionHeader confidence="0.997931" genericHeader="method">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999683">
3.1 Perplexity reduction
</subsectionHeader>
<bodyText confidence="0.999902058823529">
We validate our method in Table 3 by showing per-
plexity reduction on infrequent words. We split
Jane Austen’s novels (0.7 million words) as tar-
get corpus X and test corpus, and her contem-
poraries’ novels4 as pre-training corpus W (2.7
million words). In Table 3, nofilter is the unfil-
tered corpus; 3filter replaces all tokens in VX,3
by &lt;unk&gt;; ptw performs naive pre-training on W
then on X; vc performs training with the proposed
variational corpora. Our LM implements the RNN
training as described in (Zaremba et al., 2014). Ta-
ble 3 also illustrates the GPU memory usage and
running time of the compared methods and shows
that vc is more efficient than simply ptw.
vc has the best performance on low-frequency
words by some margin. ptw is the best on frequent
words because of its access to a large pre-training
</bodyText>
<footnote confidence="0.861533833333333">
3Favim.com is a website for sharing crafts, creativity
ideas. Esty.com is a e-commerce website for trading hand-
made crafts. Nelly.com is Scandinavia’s largest online fash-
ion store. Macy’s a US-based department store. Harrod’s is a
luxury department store in London.
4Dickens and the Bronte sisters
</footnote>
<bodyText confidence="0.999556666666667">
corpus. But somewhat to our surprise, ptw per-
forms badly on low-frequency words, which we
reckon is due to the rare words introduced in W:
while pre-training on W helps reduce perplexity
of words in VX,1 but also introduces additional ha-
pax legomena in VW,1 \ VX,1.
</bodyText>
<figure confidence="0.981178375">
0.5
nofilter 3filter
0.4 ptw vc
0.3
0.2
0.1
0
tech animals travel food home fashion
</figure>
<figureCaption confidence="0.981106">
Figure 3: Accuracy of suggested URL positions
across different categories of Pinterest captions.
</figureCaption>
<subsectionHeader confidence="0.999953">
3.2 Locating URLs in Pinterest captions
</subsectionHeader>
<bodyText confidence="0.99559475">
Beyond evaluations in Table 3. We apply our
method to locate URLs in over 400,000 Pinterest
captions. Unlike Facebook, Twitter, Pinterest is
not a “social hub” but rather an interest-discovery
</bodyText>
<page confidence="0.996045">
612
</page>
<bodyText confidence="0.999923103448276">
site (Linder et al., 2014; Zhong et al., 2014). To
maximally preserve user experience, postings on
Pinterest embed URLs in a natural, nonintrusive
manner and a very small portion of the posts con-
tain URLs.
In Figure 3, we ask the LM to suggest a po-
sition for the URL in the context and verify the
suggest with test data in each category. For ex-
ample, the model is presented with a sequence
of tokens: find, more, top, dresses, at, afford-
able, prices, &lt;punctuation&gt;, visit, and is asked
to predict if the next token is an URL link. In
the given example, plausible tokens after visit can
be either &lt;http://macys.com&gt; or nearest, Macy,
&lt;apostr.&gt;s, store. The proposed vc mechanism
outperforms others in 5 of the 6 categories. In
Figure 3, accuracy is measured as the percentage
of correctly suggested positions. Any prediction
next to or close to the correct position is counted
as incorrect.
In Table 4, we list some of the false nega-
tive and false positive errors made by the LM.
Many URLs on Pinterest are e-commerce URLs
and the vendors often also have physical stores. So
in predicting such e-commerce URLs, some mis-
takes are “excusable” because the LM is confused
whether the upcoming token should be an URL
(web store) or the brand name (physical store)
(e.g, http://macys.com vs. Macy’s).
</bodyText>
<sectionHeader confidence="0.999881" genericHeader="method">
4 Related work
</sectionHeader>
<bodyText confidence="0.999937543478261">
Recurrent neural network (RNN) is a type of neu-
ral sequence model that have high capacity across
various sequence tasks such as language model-
ing (Bengio et al., 2000), machine translation (Liu
et al., 2014), speech recognition (Graves et al.,
2013). Like other neural network models (e.g.,
feed-forward), RNNs can be trained using back-
propogation algorithm (Sutskever et al., 2011).
Recently, the authors in (Zaremba et al., 2014)
successfully apply dropout, an effective regular-
ization method for feed-forward neural networks,
to RNNs and achieve strong empirical improve-
ments.
Reducing perplexity on text corpus is proba-
bly the most demonstrated benchmark for mod-
ern language models (n-gram based and neural
models alike) (Chelba et al., 2013; Church et al.,
2007; Goodman and Gao, 2000; Gao and Zhang,
2002). Based on Zipf’s law (Zipf, 1935), a fil-
tered corpus greatly reduces the vocabulary size
and computation complexity. Recently, a rigor-
ous study (Kobayashi, 2014) looks at how per-
plexity can be manipulated by simply supplying
the model with the same corpus reduced to vary-
ing degrees. Kobayashi (2014) describes his study
from a macro point of view (i.e., the overall corpus
level perplexity). In this work, we present, at word
level, the correlation between perplexity and word
frequency.
Token rarity is a long-standing issue with n-
gram language models (Manning and Sch¨utze,
1999). Katz smoothing (Katz, 1987) and Kneser-
Ney based smoothing methods (Teh, 2006) are
well known techniques for addressing sparsity in
n-gram models. However, they are not directly
used to resolve unigram sparsity.
Using word morphology information is another
way of dealing with rare tokens (Botha and Blun-
som, 2014). By decomposing words into mor-
phemes, the authors in (Botha and Blunsom, 2014)
are able to learn representations on the morpheme
level and therefore scale the language modeling to
unseen words as long as they are made of previ-
ously seen morphemes. Shown in their work, this
technique works with character-based language in
addition to English.
</bodyText>
<sectionHeader confidence="0.998883" genericHeader="method">
5 Acknowledgements
</sectionHeader>
<bodyText confidence="0.9997928">
This work is supported in part by the following
grants: NSF awards CCF-1029166, IIS-1343639,
and CCF-1409601; DOE award DESC0007456;
AFOSR award FA9550-12-1-0458; NIST award
70NANB14H012.
</bodyText>
<sectionHeader confidence="0.998164" genericHeader="conclusions">
6 Conclusions &amp; future work
</sectionHeader>
<bodyText confidence="0.9999797">
This paper investigates the performance portfolio
of popular neural language models. We propose
a variational training scheme that has the advan-
tage of a large pre-training corpus but without us-
ing as much computing resources. On low fre-
quency words, our proposed scheme also outper-
forms naive pre-training.
In the future, we want to incorporate WordNet
knowledge to further reduce perplexity on infre-
quent words.
</bodyText>
<sectionHeader confidence="0.996116" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.748492333333333">
Yoshua Bengio, Rjean Ducharme, Pascal Vincent,
Departement D’informatique Et Recherche Opera-
tionnelle, and Centre De Recherche. 2000. A neural
</reference>
<page confidence="0.996079">
613
</page>
<reference confidence="0.997499620370371">
probabilistic language model. Journal of Machine
Learning Research, 3:1137–1155.
Jan A. Botha and Phil Blunsom. 2014. Compositional
morphology for word representations and language
modelling. In Proceedings of the 31th International
Conference on Machine Learning, ICML 2014, Bei-
jing, China, 21-26 June 2014, pages 1899–1907.
Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,
Thorsten Brants, Phillipp Koehn, and Tony Robin-
son. 2013. One billion word benchmark for measur-
ing progress in statistical language modeling. Tech-
nical report, Google.
Kenneth Church, Ted Hart, and Jianfeng Gao. 2007.
Compressing trigram language models with golomb
coding. In EMNLP-CoNLL 2007, Proceedings
of the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Compu-
tational Natural Language Learning, June 28-30,
2007, Prague, Czech Republic, pages 199–207.
Dumitru Erhan, Yoshua Bengio, Aaron Courville,
Pierre-Antoine Manzagol, Pascal Vincent, and Samy
Bengio. 2010. Why does unsupervised pre-training
help deep learning? J. Mach. Learn. Res., 11:625–
660, March.
Nelson Francis and Henry Kucera. 1979. Brown cor-
pus manual. Technical report, Department of Lin-
guistics, Brown University, Providence, Rhode Is-
land, US.
Jianfeng Gao and Min Zhang. 2002. Improving lan-
guage model size reduction using better pruning cri-
teria. In Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics, ACL
’02, pages 176–182, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Joshua Goodman and Jianfeng Gao. 2000. Language
model size reduction by pruning and clustering. In
Sixth International Conference on Spoken Language
Processing, ICSLP 2000 / INTERSPEECH 2000,
Beijing, China, October 16-20, 2000, pages 110–
113.
Alex Graves, Abdel-rahman Mohamed, and Geof-
frey E. Hinton. 2013. Speech recognition with
deep recurrent neural networks. In IEEE Interna-
tional Conference on Acoustics, Speech and Signal
Processing, ICASSP 2013, Vancouver, BC, Canada,
May 26-31, 2013, pages 6645–6649.
Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long
short-term memory. Neural Comput., 9(8):1735–
1780, November.
S. Katz. 1987. Estimation of probabilities from sparse
data for the language model component of a speech
recognizer. Acoustics, Speech and Signal Process-
ing, IEEE Transactions on, 35(3):400–401, Mar.
Hayato Kobayashi. 2014. Perplexity on reduced cor-
pora. In Proceedings of the 52nd Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 797–806. Association
for Computational Linguistics.
Rhema Linder, Clair Snodgrass, and Andruid Kerne.
2014. Everyday ideation: all of my ideas are on
pinterest. In CHI Conference on Human Factors in
Computing Systems, CHI’14, Toronto, ON, Canada
- April 26 - May 01, 2014, pages 2411–2420.
Shujie Liu, Nan Yang, Mu Li, and Ming Zhou. 2014.
A recursive recurrent neural network for statistical
machine translation. In Proceedings of the 52nd An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1491–
1500, Baltimore, Maryland, June. Association for
Computational Linguistics.
Christopher D. Manning and Hinrich Sch¨utze. 1999.
Foundations of Statistical Natural Language Pro-
cessing. MIT Press.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013. Distributed rep-
resentations of words and phrases and their compo-
sitionality. In NIPS, pages 3111–3119.
George A. Miller. 1995. Wordnet: A lexical database
for english. Commun. ACM, 38(11):39–41, Novem-
ber.
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
2013. On the difficulty of training recurrent neural
networks. In Proceedings of the 30th International
Conference on Machine Learning, ICML 2013, At-
lanta, GA, USA, 16-21 June 2013, pages 1310–1318.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overfitting. Journal of Machine Learning Re-
search, 15:1929–1958.
Ilya Sutskever, James Martens, and Geoffrey Hinton.
2011. Generating text with recurrent neural net-
works. In Lise Getoor and Tobias Scheffer, editors,
Proceedings of the 28th International Conference
on Machine Learning (ICML-11), ICML ’11, pages
1017–1024, New York, NY, USA, June. ACM.
Yee Whye Teh. 2006. A bayesian interpretation of
interpolated kneserney. Technical report.
Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.
2014. Recurrent neural network regularization.
arXiv preprint arXiv:1409.2329.
Changtao Zhong, Mostafa Salehi, Sunil Shah, Mar-
ius Cobzarenco, Nishanth Sastry, and Meeyoung
Cha. 2014. Social bootstrapping: how pinterest
and last.fm social communities benefit by borrowing
links from facebook. In 23rd International World
Wide Web Conference, WWW ’14, Seoul, Republic
of Korea, April 7-11, 2014, pages 305–314.
</reference>
<page confidence="0.98502">
614
</page>
<reference confidence="0.930087666666667">
G.K. Zipf. 1935. The Psycho-biology of Language:
An Introduction to Dynamic Philology. The MIT
paperback series. Houghton Mifflin.
</reference>
<page confidence="0.998523">
615
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.851793">
<title confidence="0.986361">Reducing infrequent-token perplexity via variational corpora</title>
<author confidence="0.918292">Alok</author>
<affiliation confidence="0.996747">1Northwestern University 2IBM Research 3University of Maryland</affiliation>
<address confidence="0.943964">Evanston, IL USA Yorktown Heights, NY USA College Park, MD USA</address>
<abstract confidence="0.9990739375">Recurrent neural network (RNN) is recognized as a powerful language model (LM). We investigate deeper into its performance portfolio, which performs well on frequent grammatical patterns but much less so on less frequent terms. Such portfolio is expected and desirable in applications like autocomplete, but is less useful in social content analysis where many creative, unexpected usages occur (e.g., URL insertion). We adapt a generic RNN model and show that, with variational training corpora and epoch unfolding, the model improves its performance for the task of URL insertion suggestions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Rjean Ducharme</author>
<author>Pascal Vincent</author>
</authors>
<title>Departement D’informatique Et Recherche Operationnelle, and Centre De Recherche.</title>
<date>2000</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="16462" citStr="Bengio et al., 2000" startWordPosition="2753" endWordPosition="2756">position is counted as incorrect. In Table 4, we list some of the false negative and false positive errors made by the LM. Many URLs on Pinterest are e-commerce URLs and the vendors often also have physical stores. So in predicting such e-commerce URLs, some mistakes are “excusable” because the LM is confused whether the upcoming token should be an URL (web store) or the brand name (physical store) (e.g, http://macys.com vs. Macy’s). 4 Related work Recurrent neural network (RNN) is a type of neural sequence model that have high capacity across various sequence tasks such as language modeling (Bengio et al., 2000), machine translation (Liu et al., 2014), speech recognition (Graves et al., 2013). Like other neural network models (e.g., feed-forward), RNNs can be trained using backpropogation algorithm (Sutskever et al., 2011). Recently, the authors in (Zaremba et al., 2014) successfully apply dropout, an effective regularization method for feed-forward neural networks, to RNNs and achieve strong empirical improvements. Reducing perplexity on text corpus is probably the most demonstrated benchmark for modern language models (n-gram based and neural models alike) (Chelba et al., 2013; Church et al., 2007;</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, 2000</marker>
<rawString>Yoshua Bengio, Rjean Ducharme, Pascal Vincent, Departement D’informatique Et Recherche Operationnelle, and Centre De Recherche. 2000. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan A Botha</author>
<author>Phil Blunsom</author>
</authors>
<title>Compositional morphology for word representations and language modelling.</title>
<date>2014</date>
<booktitle>In Proceedings of the 31th International Conference on Machine Learning, ICML 2014,</booktitle>
<pages>1899--1907</pages>
<location>Beijing,</location>
<contexts>
<context position="18004" citStr="Botha and Blunsom, 2014" startWordPosition="2993" endWordPosition="2997">g degrees. Kobayashi (2014) describes his study from a macro point of view (i.e., the overall corpus level perplexity). In this work, we present, at word level, the correlation between perplexity and word frequency. Token rarity is a long-standing issue with ngram language models (Manning and Sch¨utze, 1999). Katz smoothing (Katz, 1987) and KneserNey based smoothing methods (Teh, 2006) are well known techniques for addressing sparsity in n-gram models. However, they are not directly used to resolve unigram sparsity. Using word morphology information is another way of dealing with rare tokens (Botha and Blunsom, 2014). By decomposing words into morphemes, the authors in (Botha and Blunsom, 2014) are able to learn representations on the morpheme level and therefore scale the language modeling to unseen words as long as they are made of previously seen morphemes. Shown in their work, this technique works with character-based language in addition to English. 5 Acknowledgements This work is supported in part by the following grants: NSF awards CCF-1029166, IIS-1343639, and CCF-1409601; DOE award DESC0007456; AFOSR award FA9550-12-1-0458; NIST award 70NANB14H012. 6 Conclusions &amp; future work This paper investiga</context>
</contexts>
<marker>Botha, Blunsom, 2014</marker>
<rawString>Jan A. Botha and Phil Blunsom. 2014. Compositional morphology for word representations and language modelling. In Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014, pages 1899–1907.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>Tomas Mikolov</author>
<author>Mike Schuster</author>
<author>Qi Ge</author>
<author>Thorsten Brants</author>
<author>Phillipp Koehn</author>
<author>Tony Robinson</author>
</authors>
<title>One billion word benchmark for measuring progress in statistical language modeling.</title>
<date>2013</date>
<tech>Technical report, Google.</tech>
<contexts>
<context position="17040" citStr="Chelba et al., 2013" startWordPosition="2839" endWordPosition="2842">s language modeling (Bengio et al., 2000), machine translation (Liu et al., 2014), speech recognition (Graves et al., 2013). Like other neural network models (e.g., feed-forward), RNNs can be trained using backpropogation algorithm (Sutskever et al., 2011). Recently, the authors in (Zaremba et al., 2014) successfully apply dropout, an effective regularization method for feed-forward neural networks, to RNNs and achieve strong empirical improvements. Reducing perplexity on text corpus is probably the most demonstrated benchmark for modern language models (n-gram based and neural models alike) (Chelba et al., 2013; Church et al., 2007; Goodman and Gao, 2000; Gao and Zhang, 2002). Based on Zipf’s law (Zipf, 1935), a filtered corpus greatly reduces the vocabulary size and computation complexity. Recently, a rigorous study (Kobayashi, 2014) looks at how perplexity can be manipulated by simply supplying the model with the same corpus reduced to varying degrees. Kobayashi (2014) describes his study from a macro point of view (i.e., the overall corpus level perplexity). In this work, we present, at word level, the correlation between perplexity and word frequency. Token rarity is a long-standing issue with n</context>
</contexts>
<marker>Chelba, Mikolov, Schuster, Ge, Brants, Koehn, Robinson, 2013</marker>
<rawString>Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson. 2013. One billion word benchmark for measuring progress in statistical language modeling. Technical report, Google.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Church</author>
<author>Ted Hart</author>
<author>Jianfeng Gao</author>
</authors>
<title>Compressing trigram language models with golomb coding.</title>
<date>2007</date>
<booktitle>In EMNLP-CoNLL 2007, Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>199--207</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="17061" citStr="Church et al., 2007" startWordPosition="2843" endWordPosition="2846">Bengio et al., 2000), machine translation (Liu et al., 2014), speech recognition (Graves et al., 2013). Like other neural network models (e.g., feed-forward), RNNs can be trained using backpropogation algorithm (Sutskever et al., 2011). Recently, the authors in (Zaremba et al., 2014) successfully apply dropout, an effective regularization method for feed-forward neural networks, to RNNs and achieve strong empirical improvements. Reducing perplexity on text corpus is probably the most demonstrated benchmark for modern language models (n-gram based and neural models alike) (Chelba et al., 2013; Church et al., 2007; Goodman and Gao, 2000; Gao and Zhang, 2002). Based on Zipf’s law (Zipf, 1935), a filtered corpus greatly reduces the vocabulary size and computation complexity. Recently, a rigorous study (Kobayashi, 2014) looks at how perplexity can be manipulated by simply supplying the model with the same corpus reduced to varying degrees. Kobayashi (2014) describes his study from a macro point of view (i.e., the overall corpus level perplexity). In this work, we present, at word level, the correlation between perplexity and word frequency. Token rarity is a long-standing issue with ngram language models </context>
</contexts>
<marker>Church, Hart, Gao, 2007</marker>
<rawString>Kenneth Church, Ted Hart, and Jianfeng Gao. 2007. Compressing trigram language models with golomb coding. In EMNLP-CoNLL 2007, Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, June 28-30, 2007, Prague, Czech Republic, pages 199–207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dumitru Erhan</author>
<author>Yoshua Bengio</author>
<author>Aaron Courville</author>
<author>Pierre-Antoine Manzagol</author>
<author>Pascal Vincent</author>
<author>Samy Bengio</author>
</authors>
<title>Why does unsupervised pre-training help deep learning?</title>
<date>2010</date>
<journal>J. Mach. Learn. Res.,</journal>
<volume>11</volume>
<pages>660</pages>
<contexts>
<context position="6751" citStr="Erhan et al., 2010" startWordPosition="1097" endWordPosition="1100">availability of text, W can be orders of magnitude larger than X, which makes pre-training on W challenging. A more efficient way to utilize W is to construct variational corpora based on X and W. In the following subsections, we first describe how replacement tokens are selected from a probability mass function (pmf), which is built from W; then explain how the variational corpora variates with replacement tokens through epochs. 2.1 Learn from pre-training corpus One way to alleviate the impact from infrequent vocabulary is to expose the model to a larger and overarching pre-training corpus (Erhan et al., 2010), if available. Let W be a larger corpus than X and assume that VX C_ VW. For example, if X is Herman Melville’s Moby-Dick, W can be Melville’s complete works. Further, we use VX,1 to denote the subset of VX that are hapax legonema in corpus X; similarly, VX,n (for n = 2,3, ...) denotes the subset of VX that occur n times in X. Many hapax legomena in VX,1 are likely to become more frequent tokens in VW. Suppose that x E VX,1. Denoted by ReplacePMF(W, VW, x) in Algorithm 1, we represent x as a probability mass function (pmf) over {xi, x2, ...}, where each xz is selected from VW n VX,n for n &gt; 1</context>
</contexts>
<marker>Erhan, Bengio, Courville, Manzagol, Vincent, Bengio, 2010</marker>
<rawString>Dumitru Erhan, Yoshua Bengio, Aaron Courville, Pierre-Antoine Manzagol, Pascal Vincent, and Samy Bengio. 2010. Why does unsupervised pre-training help deep learning? J. Mach. Learn. Res., 11:625– 660, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nelson Francis</author>
<author>Henry Kucera</author>
</authors>
<title>Brown corpus manual.</title>
<date>1979</date>
<tech>Technical report,</tech>
<institution>Department of Linguistics, Brown University,</institution>
<location>Providence, Rhode Island, US.</location>
<contexts>
<context position="1028" citStr="Francis and Kucera, 1979" startWordPosition="153" endWordPosition="156">tigate deeper into its performance portfolio, which performs well on frequent grammatical patterns but much less so on less frequent terms. Such portfolio is expected and desirable in applications like autocomplete, but is less useful in social content analysis where many creative, unexpected usages occur (e.g., URL insertion). We adapt a generic RNN model and show that, with variational training corpora and epoch unfolding, the model improves its performance for the task of URL insertion suggestions. 1 Introduction Just 135 most frequent words account for 50% text of the entire Brown corpus (Francis and Kucera, 1979). But over 44% (22,010 out of 49,815) of Brown’s vocabulary are hapax legomena1. The intricate relationship between vocabulary words and their utterance frequency results in some important advancements in natural language processing (NLP). For example, tf-idf results from rules applied to word frequencies in global and local context (Manning and Sch¨utze, 1999). A common preprocessing step for tf-idf is filtering rare words, which is usually justified for two reasons. First, low frequency cutoff promises computational speedup due to Zipf’s law (1935). Second, many believe that most NLP and mac</context>
</contexts>
<marker>Francis, Kucera, 1979</marker>
<rawString>Nelson Francis and Henry Kucera. 1979. Brown corpus manual. Technical report, Department of Linguistics, Brown University, Providence, Rhode Island, US.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Min Zhang</author>
</authors>
<title>Improving language model size reduction using better pruning criteria.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02,</booktitle>
<pages>176--182</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="17106" citStr="Gao and Zhang, 2002" startWordPosition="2851" endWordPosition="2854">iu et al., 2014), speech recognition (Graves et al., 2013). Like other neural network models (e.g., feed-forward), RNNs can be trained using backpropogation algorithm (Sutskever et al., 2011). Recently, the authors in (Zaremba et al., 2014) successfully apply dropout, an effective regularization method for feed-forward neural networks, to RNNs and achieve strong empirical improvements. Reducing perplexity on text corpus is probably the most demonstrated benchmark for modern language models (n-gram based and neural models alike) (Chelba et al., 2013; Church et al., 2007; Goodman and Gao, 2000; Gao and Zhang, 2002). Based on Zipf’s law (Zipf, 1935), a filtered corpus greatly reduces the vocabulary size and computation complexity. Recently, a rigorous study (Kobayashi, 2014) looks at how perplexity can be manipulated by simply supplying the model with the same corpus reduced to varying degrees. Kobayashi (2014) describes his study from a macro point of view (i.e., the overall corpus level perplexity). In this work, we present, at word level, the correlation between perplexity and word frequency. Token rarity is a long-standing issue with ngram language models (Manning and Sch¨utze, 1999). Katz smoothing </context>
</contexts>
<marker>Gao, Zhang, 2002</marker>
<rawString>Jianfeng Gao and Min Zhang. 2002. Improving language model size reduction using better pruning criteria. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02, pages 176–182, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
<author>Jianfeng Gao</author>
</authors>
<title>Language model size reduction by pruning and clustering.</title>
<date>2000</date>
<booktitle>In Sixth International Conference on Spoken Language Processing, ICSLP 2000 / INTERSPEECH 2000,</booktitle>
<pages>110--113</pages>
<location>Beijing, China,</location>
<contexts>
<context position="17084" citStr="Goodman and Gao, 2000" startWordPosition="2847" endWordPosition="2850"> machine translation (Liu et al., 2014), speech recognition (Graves et al., 2013). Like other neural network models (e.g., feed-forward), RNNs can be trained using backpropogation algorithm (Sutskever et al., 2011). Recently, the authors in (Zaremba et al., 2014) successfully apply dropout, an effective regularization method for feed-forward neural networks, to RNNs and achieve strong empirical improvements. Reducing perplexity on text corpus is probably the most demonstrated benchmark for modern language models (n-gram based and neural models alike) (Chelba et al., 2013; Church et al., 2007; Goodman and Gao, 2000; Gao and Zhang, 2002). Based on Zipf’s law (Zipf, 1935), a filtered corpus greatly reduces the vocabulary size and computation complexity. Recently, a rigorous study (Kobayashi, 2014) looks at how perplexity can be manipulated by simply supplying the model with the same corpus reduced to varying degrees. Kobayashi (2014) describes his study from a macro point of view (i.e., the overall corpus level perplexity). In this work, we present, at word level, the correlation between perplexity and word frequency. Token rarity is a long-standing issue with ngram language models (Manning and Sch¨utze, </context>
</contexts>
<marker>Goodman, Gao, 2000</marker>
<rawString>Joshua Goodman and Jianfeng Gao. 2000. Language model size reduction by pruning and clustering. In Sixth International Conference on Spoken Language Processing, ICSLP 2000 / INTERSPEECH 2000, Beijing, China, October 16-20, 2000, pages 110– 113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Graves</author>
<author>Abdel-rahman Mohamed</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>Speech recognition with deep recurrent neural networks.</title>
<date>2013</date>
<booktitle>In IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2013,</booktitle>
<pages>6645--6649</pages>
<location>Vancouver, BC, Canada,</location>
<contexts>
<context position="16544" citStr="Graves et al., 2013" startWordPosition="2765" endWordPosition="2768">nd false positive errors made by the LM. Many URLs on Pinterest are e-commerce URLs and the vendors often also have physical stores. So in predicting such e-commerce URLs, some mistakes are “excusable” because the LM is confused whether the upcoming token should be an URL (web store) or the brand name (physical store) (e.g, http://macys.com vs. Macy’s). 4 Related work Recurrent neural network (RNN) is a type of neural sequence model that have high capacity across various sequence tasks such as language modeling (Bengio et al., 2000), machine translation (Liu et al., 2014), speech recognition (Graves et al., 2013). Like other neural network models (e.g., feed-forward), RNNs can be trained using backpropogation algorithm (Sutskever et al., 2011). Recently, the authors in (Zaremba et al., 2014) successfully apply dropout, an effective regularization method for feed-forward neural networks, to RNNs and achieve strong empirical improvements. Reducing perplexity on text corpus is probably the most demonstrated benchmark for modern language models (n-gram based and neural models alike) (Chelba et al., 2013; Church et al., 2007; Goodman and Gao, 2000; Gao and Zhang, 2002). Based on Zipf’s law (Zipf, 1935), a </context>
</contexts>
<marker>Graves, Mohamed, Hinton, 2013</marker>
<rawString>Alex Graves, Abdel-rahman Mohamed, and Geoffrey E. Hinton. 2013. Speech recognition with deep recurrent neural networks. In IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2013, Vancouver, BC, Canada, May 26-31, 2013, pages 6645–6649.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sepp Hochreiter</author>
<author>J¨urgen Schmidhuber</author>
</authors>
<title>Long short-term memory.</title>
<date>1997</date>
<journal>Neural Comput.,</journal>
<volume>9</volume>
<issue>8</issue>
<pages>1780</pages>
<contexts>
<context position="2748" citStr="Hochreiter and Schmidhuber, 1997" startWordPosition="425" endWordPosition="428">odern NLP applications, context grows constantly: fresh news articles come out on CNN and New York Times everyday; conversations on Twitter are updated in real time. In processing online social media text, it would seem premature to filter words simply due to infrequency, the kind of infrequency that can be eliminated by taking a larger corpus available from the same source. To further undermine the conventional justification, computational speedup is attenuated in RNNbased LMs (compared to n-gram LMs), thanks to modern GPU architecture. We train a large RNNLSTM (long short-term memory unit) (Hochreiter and Schmidhuber, 1997) model as our LM on two versions of Jane Austen’s complete works. Dealing with 33% less vocabulary in the filtered version, the model only gains marginally on running time or memory usage. In Table 1.1, “Filtered corpus” filters out all the hapax legomena in “Full corpus”. Full corpus Filtered corpus corpus length 756,273 751,325 vocab. size 15,125 10,177 running time 1,446 sec 1,224 sec GPU memory 959 MB 804 MB Table 1: Filtered corpus gains little in running time or memory usage when using a RNN LM. Since RNN LMs suffer only small penalty in keeping the full corpus, can we take advantage of </context>
</contexts>
<marker>Hochreiter, Schmidhuber, 1997</marker>
<rawString>Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long short-term memory. Neural Comput., 9(8):1735– 1780, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Katz</author>
</authors>
<title>Estimation of probabilities from sparse data for the language model component of a speech recognizer. Acoustics, Speech and Signal Processing,</title>
<date>1987</date>
<journal>IEEE Transactions on,</journal>
<volume>35</volume>
<issue>3</issue>
<contexts>
<context position="17718" citStr="Katz, 1987" startWordPosition="2951" endWordPosition="2952"> Based on Zipf’s law (Zipf, 1935), a filtered corpus greatly reduces the vocabulary size and computation complexity. Recently, a rigorous study (Kobayashi, 2014) looks at how perplexity can be manipulated by simply supplying the model with the same corpus reduced to varying degrees. Kobayashi (2014) describes his study from a macro point of view (i.e., the overall corpus level perplexity). In this work, we present, at word level, the correlation between perplexity and word frequency. Token rarity is a long-standing issue with ngram language models (Manning and Sch¨utze, 1999). Katz smoothing (Katz, 1987) and KneserNey based smoothing methods (Teh, 2006) are well known techniques for addressing sparsity in n-gram models. However, they are not directly used to resolve unigram sparsity. Using word morphology information is another way of dealing with rare tokens (Botha and Blunsom, 2014). By decomposing words into morphemes, the authors in (Botha and Blunsom, 2014) are able to learn representations on the morpheme level and therefore scale the language modeling to unseen words as long as they are made of previously seen morphemes. Shown in their work, this technique works with character-based la</context>
</contexts>
<marker>Katz, 1987</marker>
<rawString>S. Katz. 1987. Estimation of probabilities from sparse data for the language model component of a speech recognizer. Acoustics, Speech and Signal Processing, IEEE Transactions on, 35(3):400–401, Mar.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hayato Kobayashi</author>
</authors>
<title>Perplexity on reduced corpora.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>797--806</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="17268" citStr="Kobayashi, 2014" startWordPosition="2878" endWordPosition="2879"> (Sutskever et al., 2011). Recently, the authors in (Zaremba et al., 2014) successfully apply dropout, an effective regularization method for feed-forward neural networks, to RNNs and achieve strong empirical improvements. Reducing perplexity on text corpus is probably the most demonstrated benchmark for modern language models (n-gram based and neural models alike) (Chelba et al., 2013; Church et al., 2007; Goodman and Gao, 2000; Gao and Zhang, 2002). Based on Zipf’s law (Zipf, 1935), a filtered corpus greatly reduces the vocabulary size and computation complexity. Recently, a rigorous study (Kobayashi, 2014) looks at how perplexity can be manipulated by simply supplying the model with the same corpus reduced to varying degrees. Kobayashi (2014) describes his study from a macro point of view (i.e., the overall corpus level perplexity). In this work, we present, at word level, the correlation between perplexity and word frequency. Token rarity is a long-standing issue with ngram language models (Manning and Sch¨utze, 1999). Katz smoothing (Katz, 1987) and KneserNey based smoothing methods (Teh, 2006) are well known techniques for addressing sparsity in n-gram models. However, they are not directly </context>
</contexts>
<marker>Kobayashi, 2014</marker>
<rawString>Hayato Kobayashi. 2014. Perplexity on reduced corpora. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 797–806. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rhema Linder</author>
<author>Clair Snodgrass</author>
<author>Andruid Kerne</author>
</authors>
<title>Everyday ideation: all of my ideas are on pinterest.</title>
<date>2014</date>
<booktitle>In CHI Conference on Human Factors in Computing Systems, CHI’14,</booktitle>
<pages>2411--2420</pages>
<location>Toronto, ON, Canada -</location>
<contexts>
<context position="15011" citStr="Linder et al., 2014" startWordPosition="2502" endWordPosition="2505">reckon is due to the rare words introduced in W: while pre-training on W helps reduce perplexity of words in VX,1 but also introduces additional hapax legomena in VW,1 \ VX,1. 0.5 nofilter 3filter 0.4 ptw vc 0.3 0.2 0.1 0 tech animals travel food home fashion Figure 3: Accuracy of suggested URL positions across different categories of Pinterest captions. 3.2 Locating URLs in Pinterest captions Beyond evaluations in Table 3. We apply our method to locate URLs in over 400,000 Pinterest captions. Unlike Facebook, Twitter, Pinterest is not a “social hub” but rather an interest-discovery 612 site (Linder et al., 2014; Zhong et al., 2014). To maximally preserve user experience, postings on Pinterest embed URLs in a natural, nonintrusive manner and a very small portion of the posts contain URLs. In Figure 3, we ask the LM to suggest a position for the URL in the context and verify the suggest with test data in each category. For example, the model is presented with a sequence of tokens: find, more, top, dresses, at, affordable, prices, &lt;punctuation&gt;, visit, and is asked to predict if the next token is an URL link. In the given example, plausible tokens after visit can be either &lt;http://macys.com&gt; or nearest</context>
</contexts>
<marker>Linder, Snodgrass, Kerne, 2014</marker>
<rawString>Rhema Linder, Clair Snodgrass, and Andruid Kerne. 2014. Everyday ideation: all of my ideas are on pinterest. In CHI Conference on Human Factors in Computing Systems, CHI’14, Toronto, ON, Canada - April 26 - May 01, 2014, pages 2411–2420.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shujie Liu</author>
<author>Nan Yang</author>
<author>Mu Li</author>
<author>Ming Zhou</author>
</authors>
<title>A recursive recurrent neural network for statistical machine translation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>1491--1500</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland,</location>
<contexts>
<context position="16502" citStr="Liu et al., 2014" startWordPosition="2759" endWordPosition="2762">4, we list some of the false negative and false positive errors made by the LM. Many URLs on Pinterest are e-commerce URLs and the vendors often also have physical stores. So in predicting such e-commerce URLs, some mistakes are “excusable” because the LM is confused whether the upcoming token should be an URL (web store) or the brand name (physical store) (e.g, http://macys.com vs. Macy’s). 4 Related work Recurrent neural network (RNN) is a type of neural sequence model that have high capacity across various sequence tasks such as language modeling (Bengio et al., 2000), machine translation (Liu et al., 2014), speech recognition (Graves et al., 2013). Like other neural network models (e.g., feed-forward), RNNs can be trained using backpropogation algorithm (Sutskever et al., 2011). Recently, the authors in (Zaremba et al., 2014) successfully apply dropout, an effective regularization method for feed-forward neural networks, to RNNs and achieve strong empirical improvements. Reducing perplexity on text corpus is probably the most demonstrated benchmark for modern language models (n-gram based and neural models alike) (Chelba et al., 2013; Church et al., 2007; Goodman and Gao, 2000; Gao and Zhang, 2</context>
</contexts>
<marker>Liu, Yang, Li, Zhou, 2014</marker>
<rawString>Shujie Liu, Nan Yang, Mu Li, and Ming Zhou. 2014. A recursive recurrent neural network for statistical machine translation. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1491– 1500, Baltimore, Maryland, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Hinrich Sch¨utze</author>
</authors>
<date>1999</date>
<booktitle>Foundations of Statistical Natural Language Processing.</booktitle>
<publisher>MIT Press.</publisher>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>Christopher D. Manning and Hinrich Sch¨utze. 1999. Foundations of Statistical Natural Language Processing. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Gregory S Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In NIPS,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="11038" citStr="Mikolov et al., 2013" startWordPosition="1847" endWordPosition="1850">rpus at epoch s. Input: W, X, VW, VX, VX,n, n, as defined in Section 1.2&amp;2.1, s, 5, current and max epoch number. Output: X(s), variational corpus at epoch s 1 X(s) +– X(s + 1) 2 for each x E VX,n do p +– ReplacePMF(W, VW, x) i +– Dirichlet(p).generate() while i +– X.getNextIdxOf(x) do x0 +– i.draw() c +– W.getContextAround(x0) c.substr([0, uniformRnd (0, S−s S JcJ)1) X(s).append(c) 10 return X(s) number increases, fewer and shorter snippets are appended, which alleviates training stress. By fixing an n value, the algorithm applies to all words in VX,n. In addition, as a regularization trick (Mikolov et al., 2013; Pascanu et al., 2013) , we use a uniform random context window (line 8) when injecting snippets from W into X(s). epoch 1 update parameters model epoch 2 update parameters model Load in batch Load in batch same text corpus same text corpus same text corpus Load in batch epoch... ....... update parameters epoch... epoch 1 epoch 2 model model overwrite VX,n weights overwrite VX,n weights randomized batch randomized batch text corpus S1 text corpus S2 text corpus S3 update parameters randomized batch 3 4 5 6 7 8 9 611 Freq. nofilter 3filter ptw vc 10 28,542 (668.1) 23,649 (641.2) 27,986 (1,067.</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. 2013. Distributed representations of words and phrases and their compositionality. In NIPS, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>Wordnet: A lexical database for english.</title>
<date>1995</date>
<journal>Commun. ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="8153" citStr="Miller, 1995" startWordPosition="1353" endWordPosition="1354">ith x. We measure literal similarity using Jaro-Winkler measure, which is an empirical, weighted measure based on string edit 10000 Word level average perplexity 4 9000 8000 7000 6000 5000 4000 3000 2000 1000 0 word perplexity word frequency (log scale) 65536 32768 Word frequency in corpus 16384 8192 4096 2048 1024 512 256 128 64 610 distance. We set the measure threshold very high (&gt; 0.93), which minimizes false positives as well as captures many hapax legonema due to adv./adj., pl./singular (e.g, -y/-ily and -y/-ies). 2) e.g., marital Words that are direct syno/hyponyms to x in the WordNet (Miller, 1995). getContextAround(x0) function in Algorithm 1 simply extracts symmetric context words from both left and right sides of x0. Although the investigated LM only uses left context in predicting word x0, context right of x0 is still useful information in general. Given a context word c right of x0, the LM can learn x0’s predictability over c, which is beneficial to the corpus-wide perplexity reduction. In practice, we select no more than 5 substitution words from each method above. The probability mass on each x0i is proportional to its frequency in W and then normalized by softmax: i)/ E5 pmf(x0 </context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. Wordnet: A lexical database for english. Commun. ACM, 38(11):39–41, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan Pascanu</author>
<author>Tomas Mikolov</author>
<author>Yoshua Bengio</author>
</authors>
<title>On the difficulty of training recurrent neural networks.</title>
<date>2013</date>
<booktitle>In Proceedings of the 30th International Conference on Machine Learning, ICML 2013,</booktitle>
<pages>16--21</pages>
<location>Atlanta, GA, USA,</location>
<contexts>
<context position="11061" citStr="Pascanu et al., 2013" startWordPosition="1851" endWordPosition="1854">: W, X, VW, VX, VX,n, n, as defined in Section 1.2&amp;2.1, s, 5, current and max epoch number. Output: X(s), variational corpus at epoch s 1 X(s) +– X(s + 1) 2 for each x E VX,n do p +– ReplacePMF(W, VW, x) i +– Dirichlet(p).generate() while i +– X.getNextIdxOf(x) do x0 +– i.draw() c +– W.getContextAround(x0) c.substr([0, uniformRnd (0, S−s S JcJ)1) X(s).append(c) 10 return X(s) number increases, fewer and shorter snippets are appended, which alleviates training stress. By fixing an n value, the algorithm applies to all words in VX,n. In addition, as a regularization trick (Mikolov et al., 2013; Pascanu et al., 2013) , we use a uniform random context window (line 8) when injecting snippets from W into X(s). epoch 1 update parameters model epoch 2 update parameters model Load in batch Load in batch same text corpus same text corpus same text corpus Load in batch epoch... ....... update parameters epoch... epoch 1 epoch 2 model model overwrite VX,n weights overwrite VX,n weights randomized batch randomized batch text corpus S1 text corpus S2 text corpus S3 update parameters randomized batch 3 4 5 6 7 8 9 611 Freq. nofilter 3filter ptw vc 10 28,542 (668.1) 23,649 (641.2) 27,986 (1,067.2) 20,994 (950.9) 100 1</context>
</contexts>
<marker>Pascanu, Mikolov, Bengio, 2013</marker>
<rawString>Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. 2013. On the difficulty of training recurrent neural networks. In Proceedings of the 30th International Conference on Machine Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013, pages 1310–1318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitish Srivastava</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.</title>
<date>2014</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>15--1929</pages>
<marker>Srivastava, Hinton, 2014</marker>
<rawString>Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15:1929–1958.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilya Sutskever</author>
<author>James Martens</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Generating text with recurrent neural networks.</title>
<date>2011</date>
<booktitle>In Lise Getoor and Tobias Scheffer, editors, Proceedings of the 28th International Conference on Machine Learning (ICML-11), ICML ’11,</booktitle>
<pages>1017--1024</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA,</location>
<contexts>
<context position="16677" citStr="Sutskever et al., 2011" startWordPosition="2784" endWordPosition="2787">s. So in predicting such e-commerce URLs, some mistakes are “excusable” because the LM is confused whether the upcoming token should be an URL (web store) or the brand name (physical store) (e.g, http://macys.com vs. Macy’s). 4 Related work Recurrent neural network (RNN) is a type of neural sequence model that have high capacity across various sequence tasks such as language modeling (Bengio et al., 2000), machine translation (Liu et al., 2014), speech recognition (Graves et al., 2013). Like other neural network models (e.g., feed-forward), RNNs can be trained using backpropogation algorithm (Sutskever et al., 2011). Recently, the authors in (Zaremba et al., 2014) successfully apply dropout, an effective regularization method for feed-forward neural networks, to RNNs and achieve strong empirical improvements. Reducing perplexity on text corpus is probably the most demonstrated benchmark for modern language models (n-gram based and neural models alike) (Chelba et al., 2013; Church et al., 2007; Goodman and Gao, 2000; Gao and Zhang, 2002). Based on Zipf’s law (Zipf, 1935), a filtered corpus greatly reduces the vocabulary size and computation complexity. Recently, a rigorous study (Kobayashi, 2014) looks at</context>
</contexts>
<marker>Sutskever, Martens, Hinton, 2011</marker>
<rawString>Ilya Sutskever, James Martens, and Geoffrey Hinton. 2011. Generating text with recurrent neural networks. In Lise Getoor and Tobias Scheffer, editors, Proceedings of the 28th International Conference on Machine Learning (ICML-11), ICML ’11, pages 1017–1024, New York, NY, USA, June. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
</authors>
<title>A bayesian interpretation of interpolated kneserney.</title>
<date>2006</date>
<tech>Technical report.</tech>
<contexts>
<context position="17768" citStr="Teh, 2006" startWordPosition="2959" endWordPosition="2960">s greatly reduces the vocabulary size and computation complexity. Recently, a rigorous study (Kobayashi, 2014) looks at how perplexity can be manipulated by simply supplying the model with the same corpus reduced to varying degrees. Kobayashi (2014) describes his study from a macro point of view (i.e., the overall corpus level perplexity). In this work, we present, at word level, the correlation between perplexity and word frequency. Token rarity is a long-standing issue with ngram language models (Manning and Sch¨utze, 1999). Katz smoothing (Katz, 1987) and KneserNey based smoothing methods (Teh, 2006) are well known techniques for addressing sparsity in n-gram models. However, they are not directly used to resolve unigram sparsity. Using word morphology information is another way of dealing with rare tokens (Botha and Blunsom, 2014). By decomposing words into morphemes, the authors in (Botha and Blunsom, 2014) are able to learn representations on the morpheme level and therefore scale the language modeling to unseen words as long as they are made of previously seen morphemes. Shown in their work, this technique works with character-based language in addition to English. 5 Acknowledgements </context>
</contexts>
<marker>Teh, 2006</marker>
<rawString>Yee Whye Teh. 2006. A bayesian interpretation of interpolated kneserney. Technical report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wojciech Zaremba</author>
</authors>
<title>Ilya Sutskever, and Oriol Vinyals.</title>
<date>2014</date>
<marker>Zaremba, 2014</marker>
<rawString>Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. 2014. Recurrent neural network regularization. arXiv preprint arXiv:1409.2329.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Changtao Zhong</author>
<author>Mostafa Salehi</author>
<author>Sunil Shah</author>
<author>Marius Cobzarenco</author>
<author>Nishanth Sastry</author>
<author>Meeyoung Cha</author>
</authors>
<title>Social bootstrapping: how pinterest and last.fm social communities benefit by borrowing links from facebook.</title>
<date>2014</date>
<booktitle>In 23rd International World Wide Web Conference, WWW ’14, Seoul, Republic of Korea,</booktitle>
<pages>305--314</pages>
<contexts>
<context position="15032" citStr="Zhong et al., 2014" startWordPosition="2506" endWordPosition="2509">rare words introduced in W: while pre-training on W helps reduce perplexity of words in VX,1 but also introduces additional hapax legomena in VW,1 \ VX,1. 0.5 nofilter 3filter 0.4 ptw vc 0.3 0.2 0.1 0 tech animals travel food home fashion Figure 3: Accuracy of suggested URL positions across different categories of Pinterest captions. 3.2 Locating URLs in Pinterest captions Beyond evaluations in Table 3. We apply our method to locate URLs in over 400,000 Pinterest captions. Unlike Facebook, Twitter, Pinterest is not a “social hub” but rather an interest-discovery 612 site (Linder et al., 2014; Zhong et al., 2014). To maximally preserve user experience, postings on Pinterest embed URLs in a natural, nonintrusive manner and a very small portion of the posts contain URLs. In Figure 3, we ask the LM to suggest a position for the URL in the context and verify the suggest with test data in each category. For example, the model is presented with a sequence of tokens: find, more, top, dresses, at, affordable, prices, &lt;punctuation&gt;, visit, and is asked to predict if the next token is an URL link. In the given example, plausible tokens after visit can be either &lt;http://macys.com&gt; or nearest, Macy, &lt;apostr.&gt;s, s</context>
</contexts>
<marker>Zhong, Salehi, Shah, Cobzarenco, Sastry, Cha, 2014</marker>
<rawString>Changtao Zhong, Mostafa Salehi, Sunil Shah, Marius Cobzarenco, Nishanth Sastry, and Meeyoung Cha. 2014. Social bootstrapping: how pinterest and last.fm social communities benefit by borrowing links from facebook. In 23rd International World Wide Web Conference, WWW ’14, Seoul, Republic of Korea, April 7-11, 2014, pages 305–314.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G K Zipf</author>
</authors>
<title>The Psycho-biology of Language: An Introduction to Dynamic Philology. The MIT paperback series.</title>
<date>1935</date>
<publisher>Houghton Mifflin.</publisher>
<contexts>
<context position="17140" citStr="Zipf, 1935" startWordPosition="2859" endWordPosition="2860">s et al., 2013). Like other neural network models (e.g., feed-forward), RNNs can be trained using backpropogation algorithm (Sutskever et al., 2011). Recently, the authors in (Zaremba et al., 2014) successfully apply dropout, an effective regularization method for feed-forward neural networks, to RNNs and achieve strong empirical improvements. Reducing perplexity on text corpus is probably the most demonstrated benchmark for modern language models (n-gram based and neural models alike) (Chelba et al., 2013; Church et al., 2007; Goodman and Gao, 2000; Gao and Zhang, 2002). Based on Zipf’s law (Zipf, 1935), a filtered corpus greatly reduces the vocabulary size and computation complexity. Recently, a rigorous study (Kobayashi, 2014) looks at how perplexity can be manipulated by simply supplying the model with the same corpus reduced to varying degrees. Kobayashi (2014) describes his study from a macro point of view (i.e., the overall corpus level perplexity). In this work, we present, at word level, the correlation between perplexity and word frequency. Token rarity is a long-standing issue with ngram language models (Manning and Sch¨utze, 1999). Katz smoothing (Katz, 1987) and KneserNey based s</context>
</contexts>
<marker>Zipf, 1935</marker>
<rawString>G.K. Zipf. 1935. The Psycho-biology of Language: An Introduction to Dynamic Philology. The MIT paperback series. Houghton Mifflin.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>