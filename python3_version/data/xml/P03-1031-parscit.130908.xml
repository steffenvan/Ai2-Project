<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000463">
<title confidence="0.850113">
Corpus-based Discourse Understanding in Spoken Dialogue Systems
</title>
<note confidence="0.514171">
Ryuichiro Higashinaka and Mikio Nakano and Kiyoaki Aikawa†
NTT Communication Science Laboratories
Nippon Telegraph and Telephone Corporation
</note>
<address confidence="0.4478315">
3-1 Morinosato Wakamiya
Atsugi, Kanagawa 243-0198, Japan
</address>
<email confidence="0.998226">
{rh,nakano}@atom.brl.ntt.co.jp, aik@idea.brl.ntt.co.jp
</email>
<sectionHeader confidence="0.995631" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.937546068965517">
This paper concerns the discourse under-
standing process in spoken dialogue sys-
tems. This process enables the system to
understand user utterances based on the
context of a dialogue. Since multiple can-
didates for the understanding result can
be obtained for a user utterance due to
the ambiguity of speech understanding, it
is not appropriate to decide on a single
understanding result after each user ut-
terance. By holding multiple candidates
for understanding results and resolving the
ambiguity as the dialogue progresses, the
discourse understanding accuracy can be
improved. This paper proposes a method
for resolving this ambiguity based on sta-
tistical information obtained from dia-
logue corpora. Unlike conventional meth-
ods that use hand-crafted rules, the pro-
posed method enables easy design of the
discourse understanding process. Experi-
ment results have shown that a system that
exploits the proposed method performs
sufficiently and that holding multiple can-
didates for understanding results is effec-
tive.
†Currently with the School of Media Science, Tokyo Uni-
versity of Technology, 1404-1 Katakuracho, Hachioji, Tokyo
192-0982, Japan.
</bodyText>
<sectionHeader confidence="0.997328" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999974034482759">
For spoken dialogue systems to correctly understand
user intentions to achieve certain tasks while con-
versing with users, the dialogue state has to be ap-
propriately updated (Zue and Glass, 2000) after each
user utterance. Here, a dialogue state means all
the information that the system possesses concern-
ing the dialogue. For example, a dialogue state in-
cludes intention recognition results after each user
utterance, the user utterance history, the system ut-
terance history, and so forth. Obtaining the user in-
tention and the content of an utterance using only the
single utterance is called speech understanding, and
updating the dialogue state based on both the previ-
ous utterance and the current dialogue state is called
discourse understanding. In general, the result of
speech understanding can be ambiguous, because it
is currently difficult to uniquely decide on a single
speech recognition result out of the many recogni-
tion candidates available, and because the syntac-
tic and semantic analysis process normally produce
multiple hypotheses. The system, however, has to be
able to uniquely determine the understanding result
after each user utterance in order to respond to the
user. The system therefore must be able to choose
the appropriate speech understanding result by re-
ferring to the dialogue state.
Most conventional systems uniquely determine
the result of the discourse understanding, i.e., the
dialogue state, after each user utterance. However,
multiple dialogue states are created from the current
dialogue state and the speech understanding results
corresponding to the user utterance, which leads to
ambiguity. When this ambiguity is ignored, the dis-
course understanding accuracy is likely to decrease.
Our idea for improving the discourse understanding
accuracy is to make the system hold multiple dia-
logue states after a user utterance and use succeed-
ing utterances to resolve the ambiguity among di-
alogue states. Although the concept of combining
multiple dialogue states and speech understanding
results has already been reported (Miyazaki et al.,
2002), they use intuition-based hand-crafted rules
for the disambiguation of dialogue states, which are
costly and sometimes lead to inaccuracy. To resolve
the ambiguity of dialogue states and reduce the cost
of rule making, we propose using statistical infor-
mation obtained from dialogue corpora, which com-
prise dialogues conducted between the system and
users.
The next section briefly illustrates the basic ar-
chitecture of a spoken dialogue system. Section 3
describes the problem to be solved in detail. Then
after introducing related work, our approach is de-
scribed with an example dialogue. After that, we
describe the experiments we performed to verify our
approach, and discuss the results. The last section
summarizes the main points and mentions future
work.
</bodyText>
<sectionHeader confidence="0.974865" genericHeader="introduction">
2 Discourse Understanding
</sectionHeader>
<bodyText confidence="0.999807">
Here, we describe the basic architecture of a spoken
dialogue system (Figure 1). When receiving a user
utterance, the system behaves as follows.
</bodyText>
<listItem confidence="0.972749833333333">
1. The speech recognizer receives a user utterance
and outputs a speech recognition hypothesis.
2. The language understanding component re-
ceives the speech recognition hypothesis. The
syntactic and semantic analysis is performed
to convert it into a form called a dialogue
act. Table 1 shows an example of a dialogue
act. In the example, “refer-start-and-end-time”
is called the dialogue act type, which briefly
describes the meaning of a dialogue act, and
“start=14:00” and “end=15:00” are add-on in-
formation.1
</listItem>
<footnote confidence="0.959843666666667">
1In general, a dialogue act corresponds to one sentence.
However, in dialogues where user utterances are unrestricted,
smaller units, such as phrases, can be regarded as dialogue acts.
</footnote>
<figureCaption confidence="0.998832">
Figure 1: Architecture of a spoken dialogue system.
</figureCaption>
<listItem confidence="0.993635818181818">
3. The discourse understanding component re-
ceives the dialogue act, refers to the current di-
alogue state, and updates the dialogue state.
4. The dialogue manager receives the current dia-
logue state, decides the next utterance, and out-
puts the next words to speak. The dialogue state
is updated at the same time so that it contains
the content of system utterances.
5. The speech synthesizer receives the output of
the dialogue manager and responds to the user
by speech.
</listItem>
<bodyText confidence="0.955033375">
This paper deals with the discourse understand-
ing component. Since we are resolving the ambi-
guity of speech understanding from the discourse
point of view and not within the speech understand-
ing candidates, we assume that a dialogue state is
uniquely determined given a dialogue state and the
next dialogue act, which means that a dialogue act
is a command to change a dialogue state. We also
assume that the relationship between the dialogue
act and the way to update the dialogue state can be
easily described without expertise in dialogue sys-
tem research. We found that these assumptions are
reasonable from our experience in system develop-
ment. Note also that this paper does not separately
deal with reference resolution; we assume that it is
performed by a command. A speech understanding
result is considered to be equal to a dialogue act in
this article.
In this paper, we consider frames as representa-
tions of dialogue states. To represent dialogue states,
plans have often been used (Allen and Perrault,
1980; Carberry, 1990). Traditionally, plan-based
discourse understanding methods have been imple-
mented mostly in keyboard-based dialogue systems,
</bodyText>
<table confidence="0.995263">
User Utterance “from two p.m. to three p.m.”
Dialogue Act [act-type=refer-start-and-end-
time, start=14:00, end=15:00]
</table>
<tableCaption confidence="0.9442195">
Table 1: A user utterance and the corresponding di-
alogue act.
</tableCaption>
<bodyText confidence="0.978901">
although there are some recent attempts to apply
them to spoken dialogue systems as well (Allen et
al., 2001; Rich et al., 2001); however, considering
the current performance of speech recognizers and
the limitations in task domains, we believe frame-
based discourse understanding and dialogue man-
agement are sufficient (Chu-Carroll, 2000; Seneff,
2002; Bobrow et al., 1977).
</bodyText>
<sectionHeader confidence="0.997662" genericHeader="method">
3 Problem
</sectionHeader>
<bodyText confidence="0.997902">
Most conventional spoken dialogue systems
uniquely determine the dialogue state after a user
utterance. Normally, however, there are multiple
candidates for the result of speech understanding,
which leads to the creation of multiple dialogue
state candidates. We believe that there are cases
where it is better to hold more than one dialogue
state and resolve the ambiguity as the dialogue
progresses rather than to decide on a single dialogue
state after each user utterance.
As an example, consider a piece of dialogue in
which the user utterance “from two p.m.” has been
misrecognized as “uh two p.m.” (Figure 2). Fig-
ure 3 shows the description of the example dia-
logue in detail including the system’s inner states,
such as dialogue acts corresponding to the speech
recognition hypotheses2 and the intention recogni-
tion results.3 After receiving the speech recogni-
tion hypothesis “uh two p.m.,” the system cannot
tell whether the user utterance corresponds to a dia-
logue act specifying the start time or the end time
(da1,da2). Therefore, the system tries to obtain
further information about the time. In this case,
the system utters a backchannel to prompt the next
user utterance to resolve the ambiguity from the dis-
course.4 At this stage, the system holds two dialogue
</bodyText>
<footnote confidence="0.87057">
2In this example, for convenience of explanation, the n-best
speech recognition input is not considered.
3An intention recognition result is one of the elements of a
dialogue state.
4A yes/no question may be an appropriate choice as well.
</footnote>
<equation confidence="0.987705888888889">
✓ ✏
S1 : what time would you like to reserve a
meeting room?
U1 : from two p.m. [uh two p.m.]
S2 : uh-huh
U2 : to three p.m. [to three p.m.]
S3 : from two p.m. to three p.m.?
U3 : yes [yes]
✒ ✑
</equation>
<figureCaption confidence="0.99817">
Figure 2: Example dialogue.
</figureCaption>
<bodyText confidence="0.996538695652174">
(S means a system utterance and U a user utterance.
Recognition results are enclosed in square brackets.)
states having different intention recognition results
(ds1,ds2). The next utterance, “to three p.m.,” is
one that uniquely corresponds to a dialogue act spec-
ifying the end time (da3), and thus updates the two
current dialogue states. As a result, two dialogue
states still remain (ds3,ds4). If the system can tell
that the previous dialogue act was about the start
time at this moment, it can understand the user in-
tention correctly. The correct understanding result,
ds3, is derived from the combination of ds1 and
da3, where ds1 is induced by ds0 and da1. As
shown here, holding multiple understanding results
can be better than just deciding on the best speech
understanding hypothesis and discarding other pos-
sibilities.
In this paper, we consider a discourse understand-
ing component that deals with multiple dialogue
states. Such a component must choose the best com-
bination of a dialogue state and a dialogue act out of
all possibilities. An appropriate scoring method for
the dialogue states is therefore required.
</bodyText>
<sectionHeader confidence="0.999942" genericHeader="method">
4 Related Work
</sectionHeader>
<bodyText confidence="0.9999649">
Nakano et al. (1999) proposed a method that holds
multiple dialogue states ordered by priority to deal
with the problem that some utterances convey mean-
ing over several speech intervals and that the under-
standing result cannot be determined at each inter-
val end. Miyazaki et al. (2002) proposed a method
combining Nakano et al.’s (1999) method and n-best
recognition hypotheses, and reported improvement
in discourse understanding accuracy. They used a
metric similar to the concept error rate for the evalu-
</bodyText>
<figure confidence="0.995216166666666">
[System utterance (S1)]
“What time would you like to reserve a meeting
room?”
[Dialogue act] [act-type=ask-time]
[Intention recognition result candidates]
1. [room=nil, start=nil, end=nil] (ds0)
1
[User utterance (U1)]
“From two p.m.”
[Speech recognition hypotheses]
1. “uh two p.m.”
[Dialogue act candidates]
1. [act-type=refer-start-time,time=14:00] (da1)
2. [act-type=refer-end-time,time=15:00] (da2)
[Intention recognition result candidates]
1. [room=nil, start=14:00, end=nil]
(ds1, induced from ds0 and da1)
2. [room=nil, start=nil, end=14:00]
(ds2, induced from ds0 and da2)
1
[System utterance (S2)] “uh-huh”
[Dialogue act] [act-type=backchannel]
1
[User utterance (U2)]
“To three p.m.”
[Speech recognition hypotheses]
1. “to three p.m.”
[Dialogue act candidates]
1. [act-type=refer-end-time, time=15:00] (da3)
[Intention recognition result candidates]
1. [room=nil, start=14:00, end=15:00]
(ds3, induced from ds1 and da3)
2. [room=nil, start=nil, end=15:00]
(ds4, induced from ds2 and da3)
1
[System utterance (S3)]
“from two p.m. to three p.m.?”
[Dialogue act]
[act-type=confirm-time,start=14:00, end=15:00]
1
[User utterance (U3)] “yes”
[Speech recognition hypotheses]
1. “yes”
[Dialogue act candidates]
1. [act-type=acknowledge]
[Intention recognition result candidates]
1. [room=nil, start=14:00, end=15:00]
2. [room=nil, start=nil, end=15:00]
</figure>
<figureCaption confidence="0.975695">
Figure 3: Detailed description of the understanding
of the example dialogue.
</figureCaption>
<bodyText confidence="0.998811038461538">
ation of discourse accuracy, comparing reference di-
alogue states with hypothesis dialogue states. Both
these methods employ hand-crafted rules to score
the dialogue states to decide the best dialogue state.
Creating such rules requires expert knowledge, and
is also time consuming.
There are approaches that propose statistically es-
timating the dialogue act type from several previous
dialogue act types using N-gram probability (Nagata
and Morimoto, 1994; Reithinger and Maier, 1995).
Although their approaches can be used for disam-
biguating user utterance using discourse informa-
tion, they do not consider holding multiple dialogue
states.
In the context of plan-based utterance understand-
ing (Allen and Perrault, 1980; Carberry, 1990),
when there is ambiguity in the understanding re-
sult of a user utterance, an interpretation best suited
to the estimated plan should be selected. In ad-
dition, the system must choose the most plausible
plans from multiple possible candidates. Although
we do not adopt plan-based representation of dia-
logue states as noted before, this problem is close to
what we are dealing with. Unfortunately, however,
it seems that no systematic ways to score the candi-
dates for disambiguation have been proposed.
</bodyText>
<sectionHeader confidence="0.997359" genericHeader="method">
5 Approach
</sectionHeader>
<bodyText confidence="0.9915032">
The discourse understanding method that we pro-
pose takes the same approach as Miyazaki et al.
(2002). However, our method is different in that,
when ordering the multiple dialogue states, the sta-
tistical information derived from the dialogue cor-
</bodyText>
<listItem confidence="0.861079862068966">
pora is used. We propose using two kinds of statisti-
cal information:
1. the probability of a dialogue act type sequence,
and
2. the collocation probability of a dialogue state
and the next dialogue act.
5.1 Statistical Information
Probability of a dialogue act type sequence
Based on the same idea as Nagata and Morimoto
(1994) and Reithinger and Maier (1995), we use the
probability of a dialogue act type sequence, namely,
the N-gram probability of dialogue act types. Sys-
tem utterances and the transcription of user utter-
ances are both converted to dialogue acts using a di-
alogue act conversion parser, then the N-gram prob-
ability of the dialogue act types is calculated.
# explanation
1. whether slots asked previously by the system
are changed
2. whether slots being confirmed are changed
3. whether slots already confirmed are changed
4. whether the dialogue act fills slots that do not
have values
5. whether the dialogue act tries changing slots
that have values
6. when 5 is true, whether slot values are not
changed as a result
7. whether the dialogue act updates the initial
dialogue state 5
</listItem>
<tableCaption confidence="0.957920666666667">
Table 2: Seven binary attributes to classify collo-
cation patterns of a dialogue state and the next dia-
logue act.
</tableCaption>
<bodyText confidence="0.999138291666667">
Collocation probability of a dialogue state and
the next dialogue act From the dialogue corpora,
dialogue states and the succeeding user utterances
are extracted. Then, pairs comprising a dialogue
state and a dialogue act are created after convert-
ing user utterances into dialogue acts. Contrary to
the probability of sequential patterns of dialogue act
types that represents a brief flow of a dialogue, this
collocation information expresses a local detailed
flow of a dialogue, such as dialogue state changes
caused by the dialogue act. The simple bigram of
dialogue states and dialogue acts is not sufficient
due to the complexity of the data that a dialogue
state possesses, which can cause data sparseness
problems. Therefore, we classify the ways that di-
alogue states are changed by dialogue acts into 64
classes characterized by seven binary attributes (Ta-
ble 2) and compute the occurrence probability of
each class in the corpora. We assume that the un-
derstanding result of the user intention contained in
a dialogue state is expressed as a frame, which is
common in many systems (Bobrow et al., 1977). A
frame is a bundle of slots that consist of attribute-
value pairs concerning a certain domain.
</bodyText>
<footnote confidence="0.995030666666667">
5The first user utterance should be treated separately, be-
cause the system’s initial utterance is an open question leading
to an unrestricted utterance of a user.
</footnote>
<subsectionHeader confidence="0.99647">
5.2 Scoring of Dialogue Acts
</subsectionHeader>
<bodyText confidence="0.999987882352941">
Each speech recognition hypothesis is converted to
a dialogue act or acts. When there are several di-
alogue acts corresponding to a speech recognition
hypothesis, all possible dialogue acts are created as
in Figure 3, where the utterance “uh two p.m.” pro-
duces two dialogue act candidates. Each dialogue
act is given a score using its linguistic and acous-
tic scores. The linguistic score represents the gram-
matical adequacy of a speech recognition hypothe-
sis from which the dialogue act originates, and the
acoustic score the acoustic reliability of a dialogue
act. Sometimes, there is a case that a dialogue act
has such a low acoustic or linguistic score and that
it is better to ignore the act. We therefore create a
dialogue act called null act, and add this null act to
our list of dialogue acts. A null act is a dialogue act
that does not change the dialogue state at all.
</bodyText>
<subsectionHeader confidence="0.999798">
5.3 Scoring of Dialogue States
</subsectionHeader>
<bodyText confidence="0.9999916">
Since the dialogue state is uniquely updated by a di-
alogue act, if there are l dialogue acts derived from
speech understanding and m dialogue states, m x l
new dialogue states are created. In this case, we de-
fine the score of a dialogue state St+1 as
</bodyText>
<equation confidence="0.839829">
St+1 = St + α · sact + β · sngram + γ · scol
</equation>
<bodyText confidence="0.999960666666667">
where St is the score of a dialogue state just before
the update, sact the score of a dialogue act, sngram
the score concerning the probability of a dialogue
act type sequence, scol the score concerning the col-
location probability of dialogue states and dialogue
acts, and α, β, and γ are the weighting factors.
</bodyText>
<subsectionHeader confidence="0.999185">
5.4 Ordering of Dialogue States
</subsectionHeader>
<bodyText confidence="0.999647909090909">
The newly created dialogue states are ordered based
on the score. The dialogue state that has the best
score is regarded as the most probable one, and the
system responds to the user by referring to it. The
maximum number of dialogue states is needed in
order to drop low-score dialogue states and thereby
perform the operation in real time. This dropping
process can be considered as a beam search in view
of the entire discourse process, thus we name the
maximum number of dialogue states the dialogue
state beam width.
</bodyText>
<sectionHeader confidence="0.999359" genericHeader="method">
6 Experiment
</sectionHeader>
<subsectionHeader confidence="0.9957575">
6.1 Extracting Statistical Information from Di-
alogue Corpus
</subsectionHeader>
<bodyText confidence="0.996509840909091">
Dialogue Corpus We analyzed a corpus of dia-
logues between naive users and a Japanese spoken
dialogue system, which were collected in acousti-
cally insulated booths. The task domain was meet-
ing room reservation. Subjects were instructed to
reserve a meeting room on a certain date from a cer-
tain time to a certain time. As a speech recognition
engine, Julius3.1p1 (Lee et al., 2001) was used with
its attached acoustic model. For the language model,
we used a trigram trained from randomly generated
texts of acceptable phrases. For system response,
NTT’s speech synthesis engine FinalFluet (Takano
et al., 2001) was used. The system had a vocabulary
of 168 words, each registered with a category and
a semantic feature in its lexicon. The system used
hand-crafted rules for discourse understanding. The
corpus consists of 240 dialogues from 15 subjects
(10 males and 5 females), each one performing 16
dialogues. Dialogues that took more than three min-
utes were regarded as failures. The task completion
rate was 78.3% (188/240).
Extraction of Statistical Information From the
transcription, we created a trigram of dialogue act
types using the CMU-Cambridge Toolkit (Clarkson
and Rosenfeld, 1997). Figure 3 shows an example
of the trigram information starting from {refer-start-
time backchannel}. The bigram information used
for smoothing is also shown. The collocation proba-
bility was obtained from the recorded dialogue states
and the transcription following them. Out of 64 pos-
sible patterns, we found 17 in the corpus as shown in
Figure 4. Taking the case of the example dialogue in
Figure 3, it happened that the sequence {refer-start-
time backchannel refer-end-time} does not appear in
the corpus; thus, the probability is calculated based
on the bigram probability using the backoff weight,
which is 0.006. The trigram probability for {refer-
end-time backchannel refer-end-time} is 0.031.
The collocation probability of the sequence ds1
+ da3 → ds3 fits collocation pattern 12, where a
slot having no value was changed. The sequence
ds2 + da3 → ds4 fits collocation pattern 17, where
a slot having a value was changed to have a differ-
ent value. The probabilities were 0.155 and 0.009,
</bodyText>
<table confidence="0.875525933333334">
dialogue act type sequence (trigram) probability
score
refer-start-time backchannel backchannel -1.0852
refer-start-time backchannel ask-date -2.0445
refer-start-time backchannel ask-start-time -0.8633
refer-start-time backchannel request -2.0445
refer-start-time backchannel refer-day -1.7790
refer-start-time backchannel refer-month -0.4009
refer-start-time backchannel refer-room -0.8633
refer-start-time backchannel refer-start-time -0.7172
dialogue act type sequence backoff probability
(bigram) weight score
refer-start-time backchannel
refer-end-time backchannel
backchannel refer-end-time
</table>
<tableCaption confidence="0.985924666666667">
Table 3: An example of bigram and trigram of dia-
logue act types with their probability score in com-
mon logarithm.
</tableCaption>
<table confidence="0.99653547368421">
# collocation occurrence
pattern probability
0 1 1 1 0 0 1 0.001
0 1 1 0 0 1 0 0.053
0 0 0 0 0 0 0 0.273
1 0 0 0 1 0 0 0.001
1 0 1 1 0 0 0 0.005
0 0 1 1 0 0 0 0.036
0 0 0 0 1 0 0 0.047
0 1 1 0 1 0 0 0.041
0 0 1 1 0 0 1 0.010
0 0 1 0 0 1 0 0.016
0 0 0 0 0 0 1 0.064
0 0 0 1 0 0 0 0.155
1 0 0 1 0 0 0 0.043
0 0 1 0 1 0 0 0.061
1 0 0 1 0 0 1 0.001
0 0 0 1 0 0 1 0.186
0 0 0 0 0 1 0 0.009
</table>
<tableCaption confidence="0.956183">
Table 4: The 17 collocation patterns and their oc-
</tableCaption>
<bodyText confidence="0.9606135">
currence probabilities. See Figure 2 for the detail
of binary attributes. Attributes 1-7 are ordered from
left to right.
respectively. By the simple adding of the two proba-
bilities in common logarithms in each case, ds3 has
the probability score -3.015 and ds4 -3.549, sug-
gesting that the sequence ds3 is the most probable
discourse understanding result after U2.
</bodyText>
<subsectionHeader confidence="0.999612">
6.2 Verification of our approach
</subsectionHeader>
<bodyText confidence="0.999727333333333">
To verify the effectiveness of the proposed ap-
proach, we built a Japanese spoken dialogue system
in the meeting reservation domain that employs the
</bodyText>
<figure confidence="0.731865666666667">
-1.1337 -0.7928
0.4570 -0.6450
-0.5567 -1.0716
</figure>
<bodyText confidence="0.996353583333333">
proposed discourse understanding method and per-
formed dialogue experiments.
The speech recognition engine was Julius3.3p1
(Lee et al., 2001) with its attached acoustic models.
For the language model, we made a trigram from
the transcription obtained from the corpora. The
system had a vocabulary of 243. The recognition
engine outputs 5-best recognition hypotheses. This
time, values for sact, sngram, scol are the logarithm
of the inverse number of n-best ranks,6 the log like-
lihood of dialogue act type trigram probability, and
the common logarithm of the collocation probabil-
ity, respectively. For the experiment, weighting fac-
tors are all set to one (α = Q = γ = 1). The di-
alogue state beam width was 15. We collected 256
dialogues from 16 subjects (7 males and 9 females).
The speech recognition accuracy (word error rate)
was 65.18%. Dialogues that took more than five
minutes were regarded as failures. The task com-
pletion rate was 88.3% (226/256).7
From all user speech intervals, the number of
times that dialogue states below second place be-
came first place was 120 (7.68%), showing a relative
frequency of shuffling within the dialogue states.
</bodyText>
<subsectionHeader confidence="0.993327">
6.3 Effectiveness of Holding Multiple Dialogue
States
</subsectionHeader>
<bodyText confidence="0.9997418125">
The main reason that we developed the proposed
corpus-based discourse understanding method was
that it is difficult to manually create rules to deal
with multiple dialogue states. It is yet to be exam-
ined, however, whether holding multiple dialogue
states is really effective for accurate discourse un-
derstanding.
To verify that holding multiple dialogue states is
effective, we fixed the speech recognizer’s output to
1-best, and studied the system performance changes
when the dialogue state beam width was changed
from 1 to 30. When the dialogue state beam width is
too large, the computational cost becomes high and
the system cannot respond in real time. We therefore
selected 30 for empirical reasons.
The task domain and other settings were the same
</bodyText>
<footnote confidence="0.986144333333333">
6In this experiment, only the acoustic score of a dialogue act
was considered.
7It should be noted that due to the creation of an enormous
number of dialogue states in discourse understanding, the pro-
posed system takes a few seconds to respond after the user in-
put.
</footnote>
<bodyText confidence="0.999994757575758">
as in the previous experiment except for the dialogue
state beam width changes. We collected 448 dia-
logues from 28 subjects (4 males and 24 females),
each one performing 16 dialogues. Each subject was
instructed to reserve the same meeting room twice,
once with the 1-beam-width system and again with
30-beam-width system. The order of what room to
reserve and what system to use was randomized.
The speech recognition accuracy was 69.17%. Di-
alogues that took more than five minutes were re-
garded as failures. The task completion rates for the
1-beam-width system and the 30-beam-width sys-
tem were 88.3% and 91.0%, and the average task
completion times were 107.66 seconds and 95.86
seconds, respectively. A statistical hypothesis test
showed that times taken to carry out a task with the
30-beam-width system are significantly shorter than
those with the 1-beam-width system (Z = −2.01,
p &lt; .05). In this test, we used a kind of censored
mean computed by taking the mean of the times
only for subjects that completed the tasks with both
systems. The population distribution was estimated
by the bootstrap method (Cohen, 1995). It may be
possible to evaluate the discourse understanding by
comparing the best dialogue state with the reference
dialogue state, and calculate a metric such as the
CER (concept error rate) as Miyazaki et al. (2002)
do; however it is not clear whether the discourse
understanding can be evaluated this way, since it is
not certain whether the CER correlates closely with
the system’s performance (Higashinaka et al., 2002).
Therefore, this time, we used the task completion
time and the task completion rate for comparison.
</bodyText>
<sectionHeader confidence="0.999539" genericHeader="method">
7 Discussion
</sectionHeader>
<bodyText confidence="0.996564523809524">
Cost of creating the discourse understanding
component The best task completion rate in the ex-
periments was 91.0% (the case of 1-best recognition
input and a 30 dialogue state beam width). This high
rate suggests that the proposed approach is effective
in reducing the cost of creating the discourse un-
derstanding component in that no hand-crafted rules
are necessary. For statistical discourse understand-
ing, an initial system, e.g., a system that employs
the proposed approach with only sact for scoring the
dialogue states, is needed in order to create the di-
alogue corpus; however, once it has been made, the
creation of the discourse understanding component
requires no expert knowledge.
Effectiveness of holding multiple dialogue states
The result of the examination of dialogue state beam
width changes suggests that holding multiple dia-
logue states shortens the task completion time. As
far as task-oriented spoken dialogue systems are
concerned, holding multiple dialogue states con-
tributes to the accuracy of discourse understanding.
</bodyText>
<sectionHeader confidence="0.982779" genericHeader="conclusions">
8 Summary and Future Work
</sectionHeader>
<bodyText confidence="0.999996777777778">
We proposed a new discourse understanding method
that orders multiple dialogue states created from
multiple dialogue states and the succeeding speech
understanding results based on statistical informa-
tion obtained from dialogue corpora. The results of
the experiments show that our approach is effective
in reducing the cost of creating the discourse under-
standing component, and the advantage of keeping
multiple dialogue states was also shown.
There still remain several issues that we need to
explore. These include the use of statistical informa-
tion other than the probability of a dialogue act type
sequence and the collocation probability of dialogue
states and dialogue acts, the optimization of weight-
ing factors α, β, γ, other default parameters that we
used in the experiments, and more experiments in
larger domains. Despite these issues, the present re-
sults have shown that our approach is promising.
</bodyText>
<sectionHeader confidence="0.996004" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9998445">
We thank Dr. Hiroshi Murase and all members of the
Dialogue Understanding Research Group for useful
discussions. Thanks also go to the anonymous re-
viewers for their helpful comments.
</bodyText>
<sectionHeader confidence="0.999469" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999731666666667">
James F. Allen and C. Raymond Perrault. 1980. Analyz-
ing intention in utterances. Artif. Intel., 15:143–178.
James Allen, George Ferguson, and Amanda Stent. 2001.
An architecture for more realistic conversational sys-
tems. In Proc. IUI, pages 1–8.
Daniel G. Bobrow, Ronald M. Kaplan, Martin Kay, Don-
ald A. Norman, Henry Thompson, and Terry Wino-
grad. 1977. GUS, a frame driven dialog system. Artif.
Intel., 8:155–173.
Sandra Carberry. 1990. Plan Recognition in Natural
Language Dialogue. MIT Press, Cambridge, Mass.
Junnifer Chu-Carroll. 2000. MIMIC: An adaptive
mixed initiative spoken dialogue system for informa-
tion queries. In Proc. 6th Applied NLP, pages 97–104.
P.R. Clarkson and R. Rosenfeld. 1997. Statistical lan-
guage modeling using the CMU-Cambridge toolkit. In
Proc. Eurospeech, pages 2707–2710.
Paul R. Cohen. 1995. Empirical Methods for Artificial
Intelligence. MIT Press.
Ryuichiro Higashinaka, Noboru Miyazaki, Mikio
Nakano, and Kiyoaki Aikawa. 2002. A method
for evaluating incremental utterance understanding
in spoken dialogue systems. In Proc. ICSLP, pages
829–832.
Akinobu Lee, Tatsuya Kawahara, and Kiyohiro Shikano.
2001. Julius – an open source real-time large vocab-
ulary recognition engine. In Proc. Eurospeech, pages
1691–1694.
Noboru Miyazaki, Mikio Nakano, and Kiyoaki Aikawa.
2002. Robust speech understanding using incremen-
tal understanding with n-best recognition hypothe-
ses. In SIG-SLP-40, Information Processing Society
of Japan., pages 121–126. (in Japanese).
Masaaki Nagata and Tsuyoshi Morimoto. 1994. First
steps toward statistical modeling of dialogue to predict
the speech act type of the next utterance. Speech Com-
munication, 15:193–203.
Mikio Nakano, Noboru Miyazaki, Jun-ichi Hirasawa,
Kohji Dohsaka, and Takeshi Kawabata. 1999. Un-
derstanding unsegmented user utterances in real-time
spoken dialogue systems. In Proc. 37th ACL, pages
200–207.
Norbert Reithinger and Elisabeth Maier. 1995. Utiliz-
ing statistical dialogue act processing in Verbmobil. In
Proc. 33th ACL, pages 116–121.
Charles Rich, Candace Sidner, and Neal Lesh. 2001.
COLLAGEN: Applying collaborative discourse the-
ory. AI Magazine, 22(4):15–25.
Stephanie Seneff. 2002. Response planning and genera-
tion in the MERCURY flight reservation system. Com-
puter Speech and Language, 16(3–4):283–312.
Satoshi Takano, Kimihito Tanaka, Hideyuki Mizuno,
Masanobu Abe, and ShiN’ya Nakajima. 2001. A
Japanese TTS system based on multi-form units and a
speech modification algorithm with harmonics recon-
struction. IEEE Transactions on Speech and Process-
ing, 9(1):3–10.
Victor W. Zue and James R. Glass. 2000. Conversational
interfaces: Advances and challenges. Proceedings of
IEEE, 88(8):1166–1180.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.288675">
<title confidence="0.999107">Corpus-based Discourse Understanding in Spoken Dialogue Systems</title>
<author confidence="0.960938">Higashinaka Nakano</author>
<affiliation confidence="0.9944385">NTT Communication Science Laboratories Nippon Telegraph and Telephone Corporation</affiliation>
<address confidence="0.983099">3-1 Morinosato Wakamiya Atsugi, Kanagawa 243-0198, Japan</address>
<email confidence="0.978299">aik@idea.brl.ntt.co.jp</email>
<abstract confidence="0.9861455">This paper concerns the discourse understanding process in spoken dialogue systems. This process enables the system to understand user utterances based on the context of a dialogue. Since multiple candidates for the understanding result can be obtained for a user utterance due to the ambiguity of speech understanding, it is not appropriate to decide on a single understanding result after each user utterance. By holding multiple candidates for understanding results and resolving the ambiguity as the dialogue progresses, the discourse understanding accuracy can be improved. This paper proposes a method for resolving this ambiguity based on statistical information obtained from dialogue corpora. Unlike conventional methods that use hand-crafted rules, the proposed method enables easy design of the discourse understanding process. Experiment results have shown that a system that exploits the proposed method performs sufficiently and that holding multiple candidates for understanding results is effective. with the School of Media Science, Tokyo Uni-</abstract>
<address confidence="0.7519335">versity of Technology, 1404-1 Katakuracho, Hachioji, Tokyo 192-0982, Japan.</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James F Allen</author>
<author>C Raymond Perrault</author>
</authors>
<title>Analyzing intention in utterances.</title>
<date>1980</date>
<journal>Artif. Intel.,</journal>
<pages>15--143</pages>
<contexts>
<context position="6741" citStr="Allen and Perrault, 1980" startWordPosition="1033" endWordPosition="1036">ssume that the relationship between the dialogue act and the way to update the dialogue state can be easily described without expertise in dialogue system research. We found that these assumptions are reasonable from our experience in system development. Note also that this paper does not separately deal with reference resolution; we assume that it is performed by a command. A speech understanding result is considered to be equal to a dialogue act in this article. In this paper, we consider frames as representations of dialogue states. To represent dialogue states, plans have often been used (Allen and Perrault, 1980; Carberry, 1990). Traditionally, plan-based discourse understanding methods have been implemented mostly in keyboard-based dialogue systems, User Utterance “from two p.m. to three p.m.” Dialogue Act [act-type=refer-start-and-endtime, start=14:00, end=15:00] Table 1: A user utterance and the corresponding dialogue act. although there are some recent attempts to apply them to spoken dialogue systems as well (Allen et al., 2001; Rich et al., 2001); however, considering the current performance of speech recognizers and the limitations in task domains, we believe framebased discourse understanding</context>
<context position="12988" citStr="Allen and Perrault, 1980" startWordPosition="1977" endWordPosition="1980">. Both these methods employ hand-crafted rules to score the dialogue states to decide the best dialogue state. Creating such rules requires expert knowledge, and is also time consuming. There are approaches that propose statistically estimating the dialogue act type from several previous dialogue act types using N-gram probability (Nagata and Morimoto, 1994; Reithinger and Maier, 1995). Although their approaches can be used for disambiguating user utterance using discourse information, they do not consider holding multiple dialogue states. In the context of plan-based utterance understanding (Allen and Perrault, 1980; Carberry, 1990), when there is ambiguity in the understanding result of a user utterance, an interpretation best suited to the estimated plan should be selected. In addition, the system must choose the most plausible plans from multiple possible candidates. Although we do not adopt plan-based representation of dialogue states as noted before, this problem is close to what we are dealing with. Unfortunately, however, it seems that no systematic ways to score the candidates for disambiguation have been proposed. 5 Approach The discourse understanding method that we propose takes the same appro</context>
</contexts>
<marker>Allen, Perrault, 1980</marker>
<rawString>James F. Allen and C. Raymond Perrault. 1980. Analyzing intention in utterances. Artif. Intel., 15:143–178.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Allen</author>
<author>George Ferguson</author>
<author>Amanda Stent</author>
</authors>
<title>An architecture for more realistic conversational systems.</title>
<date>2001</date>
<booktitle>In Proc. IUI,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="7170" citStr="Allen et al., 2001" startWordPosition="1093" endWordPosition="1096">o a dialogue act in this article. In this paper, we consider frames as representations of dialogue states. To represent dialogue states, plans have often been used (Allen and Perrault, 1980; Carberry, 1990). Traditionally, plan-based discourse understanding methods have been implemented mostly in keyboard-based dialogue systems, User Utterance “from two p.m. to three p.m.” Dialogue Act [act-type=refer-start-and-endtime, start=14:00, end=15:00] Table 1: A user utterance and the corresponding dialogue act. although there are some recent attempts to apply them to spoken dialogue systems as well (Allen et al., 2001; Rich et al., 2001); however, considering the current performance of speech recognizers and the limitations in task domains, we believe framebased discourse understanding and dialogue management are sufficient (Chu-Carroll, 2000; Seneff, 2002; Bobrow et al., 1977). 3 Problem Most conventional spoken dialogue systems uniquely determine the dialogue state after a user utterance. Normally, however, there are multiple candidates for the result of speech understanding, which leads to the creation of multiple dialogue state candidates. We believe that there are cases where it is better to hold more</context>
</contexts>
<marker>Allen, Ferguson, Stent, 2001</marker>
<rawString>James Allen, George Ferguson, and Amanda Stent. 2001. An architecture for more realistic conversational systems. In Proc. IUI, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel G Bobrow</author>
<author>Ronald M Kaplan</author>
<author>Martin Kay</author>
<author>Donald A Norman</author>
<author>Henry Thompson</author>
<author>Terry Winograd</author>
</authors>
<title>GUS, a frame driven dialog system.</title>
<date>1977</date>
<journal>Artif. Intel.,</journal>
<pages>8--155</pages>
<contexts>
<context position="7435" citStr="Bobrow et al., 1977" startWordPosition="1131" endWordPosition="1134">ds have been implemented mostly in keyboard-based dialogue systems, User Utterance “from two p.m. to three p.m.” Dialogue Act [act-type=refer-start-and-endtime, start=14:00, end=15:00] Table 1: A user utterance and the corresponding dialogue act. although there are some recent attempts to apply them to spoken dialogue systems as well (Allen et al., 2001; Rich et al., 2001); however, considering the current performance of speech recognizers and the limitations in task domains, we believe framebased discourse understanding and dialogue management are sufficient (Chu-Carroll, 2000; Seneff, 2002; Bobrow et al., 1977). 3 Problem Most conventional spoken dialogue systems uniquely determine the dialogue state after a user utterance. Normally, however, there are multiple candidates for the result of speech understanding, which leads to the creation of multiple dialogue state candidates. We believe that there are cases where it is better to hold more than one dialogue state and resolve the ambiguity as the dialogue progresses rather than to decide on a single dialogue state after each user utterance. As an example, consider a piece of dialogue in which the user utterance “from two p.m.” has been misrecognized </context>
<context position="16087" citStr="Bobrow et al., 1977" startWordPosition="2489" endWordPosition="2492">ialogue state changes caused by the dialogue act. The simple bigram of dialogue states and dialogue acts is not sufficient due to the complexity of the data that a dialogue state possesses, which can cause data sparseness problems. Therefore, we classify the ways that dialogue states are changed by dialogue acts into 64 classes characterized by seven binary attributes (Table 2) and compute the occurrence probability of each class in the corpora. We assume that the understanding result of the user intention contained in a dialogue state is expressed as a frame, which is common in many systems (Bobrow et al., 1977). A frame is a bundle of slots that consist of attributevalue pairs concerning a certain domain. 5The first user utterance should be treated separately, because the system’s initial utterance is an open question leading to an unrestricted utterance of a user. 5.2 Scoring of Dialogue Acts Each speech recognition hypothesis is converted to a dialogue act or acts. When there are several dialogue acts corresponding to a speech recognition hypothesis, all possible dialogue acts are created as in Figure 3, where the utterance “uh two p.m.” produces two dialogue act candidates. Each dialogue act is g</context>
</contexts>
<marker>Bobrow, Kaplan, Kay, Norman, Thompson, Winograd, 1977</marker>
<rawString>Daniel G. Bobrow, Ronald M. Kaplan, Martin Kay, Donald A. Norman, Henry Thompson, and Terry Winograd. 1977. GUS, a frame driven dialog system. Artif. Intel., 8:155–173.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandra Carberry</author>
</authors>
<title>Plan Recognition in Natural Language Dialogue.</title>
<date>1990</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, Mass.</location>
<contexts>
<context position="6758" citStr="Carberry, 1990" startWordPosition="1037" endWordPosition="1038">p between the dialogue act and the way to update the dialogue state can be easily described without expertise in dialogue system research. We found that these assumptions are reasonable from our experience in system development. Note also that this paper does not separately deal with reference resolution; we assume that it is performed by a command. A speech understanding result is considered to be equal to a dialogue act in this article. In this paper, we consider frames as representations of dialogue states. To represent dialogue states, plans have often been used (Allen and Perrault, 1980; Carberry, 1990). Traditionally, plan-based discourse understanding methods have been implemented mostly in keyboard-based dialogue systems, User Utterance “from two p.m. to three p.m.” Dialogue Act [act-type=refer-start-and-endtime, start=14:00, end=15:00] Table 1: A user utterance and the corresponding dialogue act. although there are some recent attempts to apply them to spoken dialogue systems as well (Allen et al., 2001; Rich et al., 2001); however, considering the current performance of speech recognizers and the limitations in task domains, we believe framebased discourse understanding and dialogue man</context>
<context position="13005" citStr="Carberry, 1990" startWordPosition="1981" endWordPosition="1982">y hand-crafted rules to score the dialogue states to decide the best dialogue state. Creating such rules requires expert knowledge, and is also time consuming. There are approaches that propose statistically estimating the dialogue act type from several previous dialogue act types using N-gram probability (Nagata and Morimoto, 1994; Reithinger and Maier, 1995). Although their approaches can be used for disambiguating user utterance using discourse information, they do not consider holding multiple dialogue states. In the context of plan-based utterance understanding (Allen and Perrault, 1980; Carberry, 1990), when there is ambiguity in the understanding result of a user utterance, an interpretation best suited to the estimated plan should be selected. In addition, the system must choose the most plausible plans from multiple possible candidates. Although we do not adopt plan-based representation of dialogue states as noted before, this problem is close to what we are dealing with. Unfortunately, however, it seems that no systematic ways to score the candidates for disambiguation have been proposed. 5 Approach The discourse understanding method that we propose takes the same approach as Miyazaki e</context>
</contexts>
<marker>Carberry, 1990</marker>
<rawString>Sandra Carberry. 1990. Plan Recognition in Natural Language Dialogue. MIT Press, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Junnifer Chu-Carroll</author>
</authors>
<title>MIMIC: An adaptive mixed initiative spoken dialogue system for information queries.</title>
<date>2000</date>
<booktitle>In Proc. 6th Applied NLP,</booktitle>
<pages>97--104</pages>
<contexts>
<context position="7399" citStr="Chu-Carroll, 2000" startWordPosition="1127" endWordPosition="1128">sed discourse understanding methods have been implemented mostly in keyboard-based dialogue systems, User Utterance “from two p.m. to three p.m.” Dialogue Act [act-type=refer-start-and-endtime, start=14:00, end=15:00] Table 1: A user utterance and the corresponding dialogue act. although there are some recent attempts to apply them to spoken dialogue systems as well (Allen et al., 2001; Rich et al., 2001); however, considering the current performance of speech recognizers and the limitations in task domains, we believe framebased discourse understanding and dialogue management are sufficient (Chu-Carroll, 2000; Seneff, 2002; Bobrow et al., 1977). 3 Problem Most conventional spoken dialogue systems uniquely determine the dialogue state after a user utterance. Normally, however, there are multiple candidates for the result of speech understanding, which leads to the creation of multiple dialogue state candidates. We believe that there are cases where it is better to hold more than one dialogue state and resolve the ambiguity as the dialogue progresses rather than to decide on a single dialogue state after each user utterance. As an example, consider a piece of dialogue in which the user utterance “fr</context>
</contexts>
<marker>Chu-Carroll, 2000</marker>
<rawString>Junnifer Chu-Carroll. 2000. MIMIC: An adaptive mixed initiative spoken dialogue system for information queries. In Proc. 6th Applied NLP, pages 97–104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P R Clarkson</author>
<author>R Rosenfeld</author>
</authors>
<title>Statistical language modeling using the CMU-Cambridge toolkit.</title>
<date>1997</date>
<booktitle>In Proc. Eurospeech,</booktitle>
<pages>2707--2710</pages>
<contexts>
<context position="19716" citStr="Clarkson and Rosenfeld, 1997" startWordPosition="3111" endWordPosition="3114">sis engine FinalFluet (Takano et al., 2001) was used. The system had a vocabulary of 168 words, each registered with a category and a semantic feature in its lexicon. The system used hand-crafted rules for discourse understanding. The corpus consists of 240 dialogues from 15 subjects (10 males and 5 females), each one performing 16 dialogues. Dialogues that took more than three minutes were regarded as failures. The task completion rate was 78.3% (188/240). Extraction of Statistical Information From the transcription, we created a trigram of dialogue act types using the CMU-Cambridge Toolkit (Clarkson and Rosenfeld, 1997). Figure 3 shows an example of the trigram information starting from {refer-starttime backchannel}. The bigram information used for smoothing is also shown. The collocation probability was obtained from the recorded dialogue states and the transcription following them. Out of 64 possible patterns, we found 17 in the corpus as shown in Figure 4. Taking the case of the example dialogue in Figure 3, it happened that the sequence {refer-starttime backchannel refer-end-time} does not appear in the corpus; thus, the probability is calculated based on the bigram probability using the backoff weight, </context>
</contexts>
<marker>Clarkson, Rosenfeld, 1997</marker>
<rawString>P.R. Clarkson and R. Rosenfeld. 1997. Statistical language modeling using the CMU-Cambridge toolkit. In Proc. Eurospeech, pages 2707–2710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul R Cohen</author>
</authors>
<title>Empirical Methods for Artificial Intelligence.</title>
<date>1995</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="25804" citStr="Cohen, 1995" startWordPosition="4143" endWordPosition="4144">task completion rates for the 1-beam-width system and the 30-beam-width system were 88.3% and 91.0%, and the average task completion times were 107.66 seconds and 95.86 seconds, respectively. A statistical hypothesis test showed that times taken to carry out a task with the 30-beam-width system are significantly shorter than those with the 1-beam-width system (Z = −2.01, p &lt; .05). In this test, we used a kind of censored mean computed by taking the mean of the times only for subjects that completed the tasks with both systems. The population distribution was estimated by the bootstrap method (Cohen, 1995). It may be possible to evaluate the discourse understanding by comparing the best dialogue state with the reference dialogue state, and calculate a metric such as the CER (concept error rate) as Miyazaki et al. (2002) do; however it is not clear whether the discourse understanding can be evaluated this way, since it is not certain whether the CER correlates closely with the system’s performance (Higashinaka et al., 2002). Therefore, this time, we used the task completion time and the task completion rate for comparison. 7 Discussion Cost of creating the discourse understanding component The b</context>
</contexts>
<marker>Cohen, 1995</marker>
<rawString>Paul R. Cohen. 1995. Empirical Methods for Artificial Intelligence. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryuichiro Higashinaka</author>
<author>Noboru Miyazaki</author>
<author>Mikio Nakano</author>
<author>Kiyoaki Aikawa</author>
</authors>
<title>A method for evaluating incremental utterance understanding in spoken dialogue systems.</title>
<date>2002</date>
<booktitle>In Proc. ICSLP,</booktitle>
<pages>829--832</pages>
<contexts>
<context position="26229" citStr="Higashinaka et al., 2002" startWordPosition="4210" endWordPosition="4213"> of censored mean computed by taking the mean of the times only for subjects that completed the tasks with both systems. The population distribution was estimated by the bootstrap method (Cohen, 1995). It may be possible to evaluate the discourse understanding by comparing the best dialogue state with the reference dialogue state, and calculate a metric such as the CER (concept error rate) as Miyazaki et al. (2002) do; however it is not clear whether the discourse understanding can be evaluated this way, since it is not certain whether the CER correlates closely with the system’s performance (Higashinaka et al., 2002). Therefore, this time, we used the task completion time and the task completion rate for comparison. 7 Discussion Cost of creating the discourse understanding component The best task completion rate in the experiments was 91.0% (the case of 1-best recognition input and a 30 dialogue state beam width). This high rate suggests that the proposed approach is effective in reducing the cost of creating the discourse understanding component in that no hand-crafted rules are necessary. For statistical discourse understanding, an initial system, e.g., a system that employs the proposed approach with o</context>
</contexts>
<marker>Higashinaka, Miyazaki, Nakano, Aikawa, 2002</marker>
<rawString>Ryuichiro Higashinaka, Noboru Miyazaki, Mikio Nakano, and Kiyoaki Aikawa. 2002. A method for evaluating incremental utterance understanding in spoken dialogue systems. In Proc. ICSLP, pages 829–832.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akinobu Lee</author>
<author>Tatsuya Kawahara</author>
<author>Kiyohiro Shikano</author>
</authors>
<title>Julius – an open source real-time large vocabulary recognition engine.</title>
<date>2001</date>
<booktitle>In Proc. Eurospeech,</booktitle>
<pages>1691--1694</pages>
<contexts>
<context position="18900" citStr="Lee et al., 2001" startWordPosition="2985" endWordPosition="2988">ocess can be considered as a beam search in view of the entire discourse process, thus we name the maximum number of dialogue states the dialogue state beam width. 6 Experiment 6.1 Extracting Statistical Information from Dialogue Corpus Dialogue Corpus We analyzed a corpus of dialogues between naive users and a Japanese spoken dialogue system, which were collected in acoustically insulated booths. The task domain was meeting room reservation. Subjects were instructed to reserve a meeting room on a certain date from a certain time to a certain time. As a speech recognition engine, Julius3.1p1 (Lee et al., 2001) was used with its attached acoustic model. For the language model, we used a trigram trained from randomly generated texts of acceptable phrases. For system response, NTT’s speech synthesis engine FinalFluet (Takano et al., 2001) was used. The system had a vocabulary of 168 words, each registered with a category and a semantic feature in its lexicon. The system used hand-crafted rules for discourse understanding. The corpus consists of 240 dialogues from 15 subjects (10 males and 5 females), each one performing 16 dialogues. Dialogues that took more than three minutes were regarded as failure</context>
<context position="22586" citStr="Lee et al., 2001" startWordPosition="3611" endWordPosition="3614">ight. respectively. By the simple adding of the two probabilities in common logarithms in each case, ds3 has the probability score -3.015 and ds4 -3.549, suggesting that the sequence ds3 is the most probable discourse understanding result after U2. 6.2 Verification of our approach To verify the effectiveness of the proposed approach, we built a Japanese spoken dialogue system in the meeting reservation domain that employs the -1.1337 -0.7928 0.4570 -0.6450 -0.5567 -1.0716 proposed discourse understanding method and performed dialogue experiments. The speech recognition engine was Julius3.3p1 (Lee et al., 2001) with its attached acoustic models. For the language model, we made a trigram from the transcription obtained from the corpora. The system had a vocabulary of 243. The recognition engine outputs 5-best recognition hypotheses. This time, values for sact, sngram, scol are the logarithm of the inverse number of n-best ranks,6 the log likelihood of dialogue act type trigram probability, and the common logarithm of the collocation probability, respectively. For the experiment, weighting factors are all set to one (α = Q = γ = 1). The dialogue state beam width was 15. We collected 256 dialogues from</context>
</contexts>
<marker>Lee, Kawahara, Shikano, 2001</marker>
<rawString>Akinobu Lee, Tatsuya Kawahara, and Kiyohiro Shikano. 2001. Julius – an open source real-time large vocabulary recognition engine. In Proc. Eurospeech, pages 1691–1694.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noboru Miyazaki</author>
<author>Mikio Nakano</author>
<author>Kiyoaki Aikawa</author>
</authors>
<title>Robust speech understanding using incremental understanding with n-best recognition hypotheses.</title>
<date>2002</date>
<booktitle>In SIG-SLP-40, Information Processing Society of Japan.,</booktitle>
<pages>121--126</pages>
<note>(in Japanese).</note>
<contexts>
<context position="3568" citStr="Miyazaki et al., 2002" startWordPosition="525" endWordPosition="528">wever, multiple dialogue states are created from the current dialogue state and the speech understanding results corresponding to the user utterance, which leads to ambiguity. When this ambiguity is ignored, the discourse understanding accuracy is likely to decrease. Our idea for improving the discourse understanding accuracy is to make the system hold multiple dialogue states after a user utterance and use succeeding utterances to resolve the ambiguity among dialogue states. Although the concept of combining multiple dialogue states and speech understanding results has already been reported (Miyazaki et al., 2002), they use intuition-based hand-crafted rules for the disambiguation of dialogue states, which are costly and sometimes lead to inaccuracy. To resolve the ambiguity of dialogue states and reduce the cost of rule making, we propose using statistical information obtained from dialogue corpora, which comprise dialogues conducted between the system and users. The next section briefly illustrates the basic architecture of a spoken dialogue system. Section 3 describes the problem to be solved in detail. Then after introducing related work, our approach is described with an example dialogue. After th</context>
<context position="10612" citStr="Miyazaki et al. (2002)" startWordPosition="1662" endWordPosition="1665">ing other possibilities. In this paper, we consider a discourse understanding component that deals with multiple dialogue states. Such a component must choose the best combination of a dialogue state and a dialogue act out of all possibilities. An appropriate scoring method for the dialogue states is therefore required. 4 Related Work Nakano et al. (1999) proposed a method that holds multiple dialogue states ordered by priority to deal with the problem that some utterances convey meaning over several speech intervals and that the understanding result cannot be determined at each interval end. Miyazaki et al. (2002) proposed a method combining Nakano et al.’s (1999) method and n-best recognition hypotheses, and reported improvement in discourse understanding accuracy. They used a metric similar to the concept error rate for the evalu[System utterance (S1)] “What time would you like to reserve a meeting room?” [Dialogue act] [act-type=ask-time] [Intention recognition result candidates] 1. [room=nil, start=nil, end=nil] (ds0) 1 [User utterance (U1)] “From two p.m.” [Speech recognition hypotheses] 1. “uh two p.m.” [Dialogue act candidates] 1. [act-type=refer-start-time,time=14:00] (da1) 2. [act-type=refer-e</context>
<context position="13617" citStr="Miyazaki et al. (2002)" startWordPosition="2079" endWordPosition="2082">rry, 1990), when there is ambiguity in the understanding result of a user utterance, an interpretation best suited to the estimated plan should be selected. In addition, the system must choose the most plausible plans from multiple possible candidates. Although we do not adopt plan-based representation of dialogue states as noted before, this problem is close to what we are dealing with. Unfortunately, however, it seems that no systematic ways to score the candidates for disambiguation have been proposed. 5 Approach The discourse understanding method that we propose takes the same approach as Miyazaki et al. (2002). However, our method is different in that, when ordering the multiple dialogue states, the statistical information derived from the dialogue corpora is used. We propose using two kinds of statistical information: 1. the probability of a dialogue act type sequence, and 2. the collocation probability of a dialogue state and the next dialogue act. 5.1 Statistical Information Probability of a dialogue act type sequence Based on the same idea as Nagata and Morimoto (1994) and Reithinger and Maier (1995), we use the probability of a dialogue act type sequence, namely, the N-gram probability of dial</context>
<context position="26022" citStr="Miyazaki et al. (2002)" startWordPosition="4177" endWordPosition="4180">pothesis test showed that times taken to carry out a task with the 30-beam-width system are significantly shorter than those with the 1-beam-width system (Z = −2.01, p &lt; .05). In this test, we used a kind of censored mean computed by taking the mean of the times only for subjects that completed the tasks with both systems. The population distribution was estimated by the bootstrap method (Cohen, 1995). It may be possible to evaluate the discourse understanding by comparing the best dialogue state with the reference dialogue state, and calculate a metric such as the CER (concept error rate) as Miyazaki et al. (2002) do; however it is not clear whether the discourse understanding can be evaluated this way, since it is not certain whether the CER correlates closely with the system’s performance (Higashinaka et al., 2002). Therefore, this time, we used the task completion time and the task completion rate for comparison. 7 Discussion Cost of creating the discourse understanding component The best task completion rate in the experiments was 91.0% (the case of 1-best recognition input and a 30 dialogue state beam width). This high rate suggests that the proposed approach is effective in reducing the cost of c</context>
</contexts>
<marker>Miyazaki, Nakano, Aikawa, 2002</marker>
<rawString>Noboru Miyazaki, Mikio Nakano, and Kiyoaki Aikawa. 2002. Robust speech understanding using incremental understanding with n-best recognition hypotheses. In SIG-SLP-40, Information Processing Society of Japan., pages 121–126. (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaaki Nagata</author>
<author>Tsuyoshi Morimoto</author>
</authors>
<title>First steps toward statistical modeling of dialogue to predict the speech act type of the next utterance.</title>
<date>1994</date>
<journal>Speech Communication,</journal>
<pages>15--193</pages>
<contexts>
<context position="12723" citStr="Nagata and Morimoto, 1994" startWordPosition="1938" endWordPosition="1941">result candidates] 1. [room=nil, start=14:00, end=15:00] 2. [room=nil, start=nil, end=15:00] Figure 3: Detailed description of the understanding of the example dialogue. ation of discourse accuracy, comparing reference dialogue states with hypothesis dialogue states. Both these methods employ hand-crafted rules to score the dialogue states to decide the best dialogue state. Creating such rules requires expert knowledge, and is also time consuming. There are approaches that propose statistically estimating the dialogue act type from several previous dialogue act types using N-gram probability (Nagata and Morimoto, 1994; Reithinger and Maier, 1995). Although their approaches can be used for disambiguating user utterance using discourse information, they do not consider holding multiple dialogue states. In the context of plan-based utterance understanding (Allen and Perrault, 1980; Carberry, 1990), when there is ambiguity in the understanding result of a user utterance, an interpretation best suited to the estimated plan should be selected. In addition, the system must choose the most plausible plans from multiple possible candidates. Although we do not adopt plan-based representation of dialogue states as no</context>
<context position="14089" citStr="Nagata and Morimoto (1994)" startWordPosition="2156" endWordPosition="2159">tes for disambiguation have been proposed. 5 Approach The discourse understanding method that we propose takes the same approach as Miyazaki et al. (2002). However, our method is different in that, when ordering the multiple dialogue states, the statistical information derived from the dialogue corpora is used. We propose using two kinds of statistical information: 1. the probability of a dialogue act type sequence, and 2. the collocation probability of a dialogue state and the next dialogue act. 5.1 Statistical Information Probability of a dialogue act type sequence Based on the same idea as Nagata and Morimoto (1994) and Reithinger and Maier (1995), we use the probability of a dialogue act type sequence, namely, the N-gram probability of dialogue act types. System utterances and the transcription of user utterances are both converted to dialogue acts using a dialogue act conversion parser, then the N-gram probability of the dialogue act types is calculated. # explanation 1. whether slots asked previously by the system are changed 2. whether slots being confirmed are changed 3. whether slots already confirmed are changed 4. whether the dialogue act fills slots that do not have values 5. whether the dialogu</context>
</contexts>
<marker>Nagata, Morimoto, 1994</marker>
<rawString>Masaaki Nagata and Tsuyoshi Morimoto. 1994. First steps toward statistical modeling of dialogue to predict the speech act type of the next utterance. Speech Communication, 15:193–203.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikio Nakano</author>
<author>Noboru Miyazaki</author>
<author>Jun-ichi Hirasawa</author>
<author>Kohji Dohsaka</author>
<author>Takeshi Kawabata</author>
</authors>
<title>Understanding unsegmented user utterances in real-time spoken dialogue systems.</title>
<date>1999</date>
<booktitle>In Proc. 37th ACL,</booktitle>
<pages>200--207</pages>
<contexts>
<context position="10347" citStr="Nakano et al. (1999)" startWordPosition="1618" endWordPosition="1621"> correct understanding result, ds3, is derived from the combination of ds1 and da3, where ds1 is induced by ds0 and da1. As shown here, holding multiple understanding results can be better than just deciding on the best speech understanding hypothesis and discarding other possibilities. In this paper, we consider a discourse understanding component that deals with multiple dialogue states. Such a component must choose the best combination of a dialogue state and a dialogue act out of all possibilities. An appropriate scoring method for the dialogue states is therefore required. 4 Related Work Nakano et al. (1999) proposed a method that holds multiple dialogue states ordered by priority to deal with the problem that some utterances convey meaning over several speech intervals and that the understanding result cannot be determined at each interval end. Miyazaki et al. (2002) proposed a method combining Nakano et al.’s (1999) method and n-best recognition hypotheses, and reported improvement in discourse understanding accuracy. They used a metric similar to the concept error rate for the evalu[System utterance (S1)] “What time would you like to reserve a meeting room?” [Dialogue act] [act-type=ask-time] </context>
</contexts>
<marker>Nakano, Miyazaki, Hirasawa, Dohsaka, Kawabata, 1999</marker>
<rawString>Mikio Nakano, Noboru Miyazaki, Jun-ichi Hirasawa, Kohji Dohsaka, and Takeshi Kawabata. 1999. Understanding unsegmented user utterances in real-time spoken dialogue systems. In Proc. 37th ACL, pages 200–207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Norbert Reithinger</author>
<author>Elisabeth Maier</author>
</authors>
<title>Utilizing statistical dialogue act processing in Verbmobil.</title>
<date>1995</date>
<booktitle>In Proc. 33th ACL,</booktitle>
<pages>116--121</pages>
<contexts>
<context position="12752" citStr="Reithinger and Maier, 1995" startWordPosition="1942" endWordPosition="1945">=nil, start=14:00, end=15:00] 2. [room=nil, start=nil, end=15:00] Figure 3: Detailed description of the understanding of the example dialogue. ation of discourse accuracy, comparing reference dialogue states with hypothesis dialogue states. Both these methods employ hand-crafted rules to score the dialogue states to decide the best dialogue state. Creating such rules requires expert knowledge, and is also time consuming. There are approaches that propose statistically estimating the dialogue act type from several previous dialogue act types using N-gram probability (Nagata and Morimoto, 1994; Reithinger and Maier, 1995). Although their approaches can be used for disambiguating user utterance using discourse information, they do not consider holding multiple dialogue states. In the context of plan-based utterance understanding (Allen and Perrault, 1980; Carberry, 1990), when there is ambiguity in the understanding result of a user utterance, an interpretation best suited to the estimated plan should be selected. In addition, the system must choose the most plausible plans from multiple possible candidates. Although we do not adopt plan-based representation of dialogue states as noted before, this problem is c</context>
<context position="14121" citStr="Reithinger and Maier (1995)" startWordPosition="2161" endWordPosition="2164">n proposed. 5 Approach The discourse understanding method that we propose takes the same approach as Miyazaki et al. (2002). However, our method is different in that, when ordering the multiple dialogue states, the statistical information derived from the dialogue corpora is used. We propose using two kinds of statistical information: 1. the probability of a dialogue act type sequence, and 2. the collocation probability of a dialogue state and the next dialogue act. 5.1 Statistical Information Probability of a dialogue act type sequence Based on the same idea as Nagata and Morimoto (1994) and Reithinger and Maier (1995), we use the probability of a dialogue act type sequence, namely, the N-gram probability of dialogue act types. System utterances and the transcription of user utterances are both converted to dialogue acts using a dialogue act conversion parser, then the N-gram probability of the dialogue act types is calculated. # explanation 1. whether slots asked previously by the system are changed 2. whether slots being confirmed are changed 3. whether slots already confirmed are changed 4. whether the dialogue act fills slots that do not have values 5. whether the dialogue act tries changing slots that </context>
</contexts>
<marker>Reithinger, Maier, 1995</marker>
<rawString>Norbert Reithinger and Elisabeth Maier. 1995. Utilizing statistical dialogue act processing in Verbmobil. In Proc. 33th ACL, pages 116–121.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Rich</author>
<author>Candace Sidner</author>
<author>Neal Lesh</author>
</authors>
<title>COLLAGEN: Applying collaborative discourse theory.</title>
<date>2001</date>
<journal>AI Magazine,</journal>
<volume>22</volume>
<issue>4</issue>
<contexts>
<context position="7190" citStr="Rich et al., 2001" startWordPosition="1097" endWordPosition="1100">this article. In this paper, we consider frames as representations of dialogue states. To represent dialogue states, plans have often been used (Allen and Perrault, 1980; Carberry, 1990). Traditionally, plan-based discourse understanding methods have been implemented mostly in keyboard-based dialogue systems, User Utterance “from two p.m. to three p.m.” Dialogue Act [act-type=refer-start-and-endtime, start=14:00, end=15:00] Table 1: A user utterance and the corresponding dialogue act. although there are some recent attempts to apply them to spoken dialogue systems as well (Allen et al., 2001; Rich et al., 2001); however, considering the current performance of speech recognizers and the limitations in task domains, we believe framebased discourse understanding and dialogue management are sufficient (Chu-Carroll, 2000; Seneff, 2002; Bobrow et al., 1977). 3 Problem Most conventional spoken dialogue systems uniquely determine the dialogue state after a user utterance. Normally, however, there are multiple candidates for the result of speech understanding, which leads to the creation of multiple dialogue state candidates. We believe that there are cases where it is better to hold more than one dialogue s</context>
</contexts>
<marker>Rich, Sidner, Lesh, 2001</marker>
<rawString>Charles Rich, Candace Sidner, and Neal Lesh. 2001. COLLAGEN: Applying collaborative discourse theory. AI Magazine, 22(4):15–25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephanie Seneff</author>
</authors>
<title>Response planning and generation in the MERCURY flight reservation system. Computer Speech and Language,</title>
<date>2002</date>
<pages>16--3</pages>
<contexts>
<context position="7413" citStr="Seneff, 2002" startWordPosition="1129" endWordPosition="1130">standing methods have been implemented mostly in keyboard-based dialogue systems, User Utterance “from two p.m. to three p.m.” Dialogue Act [act-type=refer-start-and-endtime, start=14:00, end=15:00] Table 1: A user utterance and the corresponding dialogue act. although there are some recent attempts to apply them to spoken dialogue systems as well (Allen et al., 2001; Rich et al., 2001); however, considering the current performance of speech recognizers and the limitations in task domains, we believe framebased discourse understanding and dialogue management are sufficient (Chu-Carroll, 2000; Seneff, 2002; Bobrow et al., 1977). 3 Problem Most conventional spoken dialogue systems uniquely determine the dialogue state after a user utterance. Normally, however, there are multiple candidates for the result of speech understanding, which leads to the creation of multiple dialogue state candidates. We believe that there are cases where it is better to hold more than one dialogue state and resolve the ambiguity as the dialogue progresses rather than to decide on a single dialogue state after each user utterance. As an example, consider a piece of dialogue in which the user utterance “from two p.m.” h</context>
</contexts>
<marker>Seneff, 2002</marker>
<rawString>Stephanie Seneff. 2002. Response planning and generation in the MERCURY flight reservation system. Computer Speech and Language, 16(3–4):283–312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Takano</author>
<author>Kimihito Tanaka</author>
<author>Hideyuki Mizuno</author>
<author>Masanobu Abe</author>
<author>ShiN’ya Nakajima</author>
</authors>
<title>A Japanese TTS system based on multi-form units and a speech modification algorithm with harmonics reconstruction.</title>
<date>2001</date>
<journal>IEEE Transactions on Speech and Processing,</journal>
<volume>9</volume>
<issue>1</issue>
<contexts>
<context position="19130" citStr="Takano et al., 2001" startWordPosition="3020" endWordPosition="3023">ue Corpus Dialogue Corpus We analyzed a corpus of dialogues between naive users and a Japanese spoken dialogue system, which were collected in acoustically insulated booths. The task domain was meeting room reservation. Subjects were instructed to reserve a meeting room on a certain date from a certain time to a certain time. As a speech recognition engine, Julius3.1p1 (Lee et al., 2001) was used with its attached acoustic model. For the language model, we used a trigram trained from randomly generated texts of acceptable phrases. For system response, NTT’s speech synthesis engine FinalFluet (Takano et al., 2001) was used. The system had a vocabulary of 168 words, each registered with a category and a semantic feature in its lexicon. The system used hand-crafted rules for discourse understanding. The corpus consists of 240 dialogues from 15 subjects (10 males and 5 females), each one performing 16 dialogues. Dialogues that took more than three minutes were regarded as failures. The task completion rate was 78.3% (188/240). Extraction of Statistical Information From the transcription, we created a trigram of dialogue act types using the CMU-Cambridge Toolkit (Clarkson and Rosenfeld, 1997). Figure 3 sho</context>
</contexts>
<marker>Takano, Tanaka, Mizuno, Abe, Nakajima, 2001</marker>
<rawString>Satoshi Takano, Kimihito Tanaka, Hideyuki Mizuno, Masanobu Abe, and ShiN’ya Nakajima. 2001. A Japanese TTS system based on multi-form units and a speech modification algorithm with harmonics reconstruction. IEEE Transactions on Speech and Processing, 9(1):3–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor W Zue</author>
<author>James R Glass</author>
</authors>
<title>Conversational interfaces: Advances and challenges.</title>
<date>2000</date>
<booktitle>Proceedings of IEEE,</booktitle>
<pages>88--8</pages>
<contexts>
<context position="1681" citStr="Zue and Glass, 2000" startWordPosition="236" endWordPosition="239">use hand-crafted rules, the proposed method enables easy design of the discourse understanding process. Experiment results have shown that a system that exploits the proposed method performs sufficiently and that holding multiple candidates for understanding results is effective. †Currently with the School of Media Science, Tokyo University of Technology, 1404-1 Katakuracho, Hachioji, Tokyo 192-0982, Japan. 1 Introduction For spoken dialogue systems to correctly understand user intentions to achieve certain tasks while conversing with users, the dialogue state has to be appropriately updated (Zue and Glass, 2000) after each user utterance. Here, a dialogue state means all the information that the system possesses concerning the dialogue. For example, a dialogue state includes intention recognition results after each user utterance, the user utterance history, the system utterance history, and so forth. Obtaining the user intention and the content of an utterance using only the single utterance is called speech understanding, and updating the dialogue state based on both the previous utterance and the current dialogue state is called discourse understanding. In general, the result of speech understandi</context>
</contexts>
<marker>Zue, Glass, 2000</marker>
<rawString>Victor W. Zue and James R. Glass. 2000. Conversational interfaces: Advances and challenges. Proceedings of IEEE, 88(8):1166–1180.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>