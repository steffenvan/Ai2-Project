<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003738">
<title confidence="0.981358">
Large-Scale Cognate Recovery
</title>
<author confidence="0.995155">
David Hall and Dan Klein
</author>
<affiliation confidence="0.996857">
Computer Science Division
University of California at Berkeley
</affiliation>
<email confidence="0.999374">
{dlwh,klein}@cs.berkeley.edu
</email>
<sectionHeader confidence="0.994813" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9985236">
We present a system for the large scale in-
duction of cognate groups. Our model ex-
plains the evolution of cognates as a sequence
of mutations and innovations along a phy-
logeny. On the task of identifying cognates
from over 21,000 words in 218 different lan-
guages from the Oceanic language family, our
model achieves a cluster purity score over
91%, while maintaining pairwise recall over
62%.
</bodyText>
<sectionHeader confidence="0.998415" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999790229508197">
The critical first step in the reconstruction of an
ancient language is the recovery of related cog-
nate words in its descendants. Unfortunately, this
process has largely been a manual, linguistically-
intensive undertaking for any sizable number of de-
scendant languages. The traditional approach used
by linguists—the comparative method—iterates be-
tween positing putative cognates and then identify-
ing regular sound laws that explain correspondences
between those words (Bloomfield, 1938).
Successful computational approaches have been
developed for large-scale reconstruction of phyloge-
nies (Ringe et al., 2002; Daum´e III and Campbell,
2007; Daum´e III, 2009; Nerbonne, 2010) and an-
cestral word forms of known cognate sets (Oakes,
2000; Bouchard-Cˆot´e et al., 2007; Bouchard-Cˆot´e
et al., 2009), enabling linguists to explore deep his-
torical relationships in an automated fashion. How-
ever, computational approaches thus far have not
been able to offer the same kind of scale for iden-
tifying cognates. Previous work in cognate identi-
fication has largely focused on identifying cognates
in pairs of languages (Mann and Yarowsky, 2001;
Lowe and Mazaudon, 1994; Oakes, 2000; Kondrak,
2001; Mulloni, 2007), with a few recent exceptions
that can find sets in a handful of languages (Bergsma
and Kondrak, 2007; Hall and Klein, 2010).
While it may seem surprising that cognate de-
tection has not successfully scaled to large num-
bers of languages, the task poses challenges not
seen in reconstruction and phylogeny inference. For
instance, morphological innovations and irregular
sound changes can completely obscure relationships
between words in different languages. However, in
the case of reconstruction, an unexplainable word is
simply that: one can still correctly reconstruct its an-
cestor using words from related languages.
In this paper, we present a system that uses two
generative models for large-scale cognate identi-
fication. Both models describe the evolution of
words along a phylogeny according to automatically
learned sound laws in the form of parametric edit
distances. The first is an adaptation of the genera-
tive model of Hall and Klein (2010), and the other
is a new generative model called PARSIM with con-
nections to parsimony methods in computational bi-
ology (Cavalli-Sforza and Edwards, 1965; Fitch,
1971). Our model supports simple, tractable infer-
ence via message passing, at the expense of being
unable to model some cognacy relationships. To
help correct this deficiency, we also describe an ag-
glomerative inference procedure for the model of
Hall and Klein (2010). By using the output of our
system as input to this system, we can find cognate
groups that PARSIM alone cannot recover.
We apply these models to identifying cognate
groups from two language families using the Aus-
tronesian Basic Vocabulary Database (Greenhill et
al., 2008), a catalog of words from about 40% of
the Austronesian languages. We focus on data from
two subfamilies of Austronesian: Formosan and
</bodyText>
<page confidence="0.980322">
344
</page>
<note confidence="0.9580025">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 344–354,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999551454545454">
Oceanic. The datasets are by far the largest on
which automated cognate recovery has ever been at-
tempted, with 18 and 271 languages respectively.
On the larger Oceanic data, our model can achieve
cluster purity scores of 91.8%, while maintaining
pairwise recall of 62.1%. We also analyze the mis-
takes of our system, where we find that some of the
erroneous cognate groups our system finds may not
be errors at all. Instead, they may be previously
unknown cognacy relationships that were not anno-
tated in the data.
</bodyText>
<sectionHeader confidence="0.989582" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999807875">
Before we present our model, we first describe ba-
sic facts of the Austronesian language family, along
with a description of the Austronesian Basic Vocab-
ulary Database, which forms the dataset that we use
for our experiments. For far more detailed coverage
of the Austronesian languages, we direct the inter-
ested reader to Blust (2009)’s comprehensive mono-
graph.
</bodyText>
<subsectionHeader confidence="0.960147">
2.1 The Austronesian Language Family
</subsectionHeader>
<bodyText confidence="0.999967105263158">
The Austronesian language family is one of the
largest in the world, comprising about one-fifth of
the world’s languages. Geographically, it stretches
from its homeland on Formosa (Taiwan) to Mada-
gascar in the west, and as far as Hawai’i and (at one
point) the Easter Islands to the east. Until the ad-
vent of European colonialism spread Indo-European
languages to every continent, Austronesian was the
most widespread of all language families.
Linguistically, the language family is as diverse
as it is large, but a few regularities hold. From
a phonological perspective, two features stand out.
First, the phoneme inventories of these languages
are typically small. For example, it is well-known
that Hawaiian has only 13 phonemes. Moreover, the
phonotactics of these languages are often restrictive.
Sticking with the same example, Hawaiian only al-
lows (C)V syllables: consonants clusters are forbid-
den, and no syllable may end with a consonant.
</bodyText>
<subsectionHeader confidence="0.9835535">
2.2 The Austronesian Basic Vocabulary
Database
</subsectionHeader>
<bodyText confidence="0.999808451612903">
The Austronesian Basic Vocabulary Database
(ABVD) (Greenhill et al., 2008) is an ambitious, on-
going effort to catalog the lexicons and basic facts
about all of the languages in the Austronesian lan-
guage family. It also contains manual reconstruc-
tions for select ancestor languages produced by lin-
guists.
The sample we use—from Bouchard-Cˆot´e et al.
(2009)—contains about 50,000 words across 471
languages spanning all the major divisions of Aus-
tronesian. These words are grouped into cognate
groups and arranged by gloss. For instance, there are
37 distinct cognate groups for the gloss “tail.” One
of these groups includes the words /ekor/, /ingko/,
/iykot/, /kiiki?u/, and /?i?ina/, among others. Most
of these words have been transcribed into the Inter-
national Phonetic Alphabet, though it appears that
some words are transcribed using the Roman alpha-
bet. For instance, the second word in the example is
likely /iyko/, which is a much more likely sequence
than what is transcribed.
In this sample, there are 6307 such cognate
groups and 210 distinct glosses. The data is
somewhat sparse: fewer than 50% of the possible
gloss/language pairs are present. Moreover, there is
some amount of homoplasy—that is, languages with
a word from more than one cognate group for a given
gloss.
Finally, it is important to note that the ABVD is
still a work in progress: they have data from only
50% of extant Austronesian languages.
</bodyText>
<subsectionHeader confidence="0.999875">
2.3 Subfamilies of Austronesian
</subsectionHeader>
<bodyText confidence="0.999729833333333">
In this paper we focus on two branches of the Aus-
tronesian language family, one as a development set
and one as a test set. For our development set, we
use the Formosan branch. The languages in this
group are exclusively found on the Austronesian
homeland of Formosa. The family encompasses a
substantial portion of the linguistic diversity of Aus-
tronesian: Blust (2009) argues that Formosan con-
tains 9 of the 10 first-order splits of the Austrone-
sian family. Formosan’s diversity is surprising since
it contains a mere 18 languages. Thus, Formosan is
a smaller development set that nevertheless is repre-
sentative of larger families.
For our final test set, we use the Oceanic sub-
family, which includes almost 50% of the languages
in the Austronesian family, meaning that it repre-
sents around 10% of all languages in the world.
Oceanic also represents a large fraction of the ge-
</bodyText>
<page confidence="0.998472">
345
</page>
<bodyText confidence="0.9996315">
ographic diversity of Austronesian, stretching from
New Zealand in the south to Hawai’i in the north.
Our sample includes 21863 words from 218 lan-
guages in the Oceanic family.
</bodyText>
<sectionHeader confidence="0.987664" genericHeader="method">
3 Models
</sectionHeader>
<bodyText confidence="0.999544117647059">
In this section we describe two models, one based on
Hall and Klein (2010)—which we call HK10—and
another new model that shares some connection to
parsimony methods in computational biology, which
we call PARSIM. Both are generative models that
describe the evolution of words wt from a set of lan-
guages {E} in a cognate group g along a fixed phy-
logeny T.1 Each cognate group and word is also
associated with a gloss or meaning m, which we as-
sume to be fixed.2 In both models, words evolve
according to regular sound laws cot, which are spe-
cific to each language. Also, both models will make
use of a language model A, which is used for gen-
erating words that are not dependent on the word in
the parent language. (We leave cot and A as abstract
parameters for now. We will describe them in sub-
sequent sections.)
</bodyText>
<subsectionHeader confidence="0.999509">
3.1 HK10
</subsectionHeader>
<bodyText confidence="0.999927357142857">
The first model we describe is a small modification
of the phylogenetic model of Hall and Klein (2010).
In HK10, there is an unknown number of cognate
groups G where each cognate group g consists of a
set of words {wg,t}. In each cognate group, words
evolve along a phylogeny, where each word in a lan-
guage is the result of that word evolving from its
parent according to regular sound laws. To model
the fact that not all languages have a cognate in
each group, each language in the tree has an asso-
ciated “survival” variable Sg,t, where a word may
be lost on that branch (and its descendants) instead
of evolving. Once the words are generated, they are
then “permuted” so that the cognacy relationships
</bodyText>
<footnote confidence="0.9922748">
1Both of these models therefore are insensitive to geo-
graphic and historical factors that cannot be easily approxi-
mated by this tree. See Nichols (1992) for an excellent dis-
cussion of these factors.
2One could easily envision allowing the meaning of a word
to change as well. Modeling this semantic drift has been consid-
ered by Kondrak (2001). In the ABVD, however, any semantic
drift has already been elided, since the database has coarsened
glosses to the extent that there is no meaningful way to model
semantic drift given our data.
</footnote>
<figureCaption confidence="0.899600909090909">
Figure 1: Plate diagrams for (a) HK10 (Hall and Klein,
2010) and (b) PARSIM, our new parsimony model, for
a small set of languages. In HK10, words are generated
following a phylogenetic tree according to sound laws co,
and then “scrambled” with a permutation π so that the
original cognate groups are lost. In PARSIM, all words
for each of the M glosses are generated in a single tree,
with innovations I starting new cognate groups. The
languages depicted are Formosan (For), Paiwan (Pai),
Atayalic (Ata), Ciuli Atayalic (Ciu), and Squliq Atayalic
(Squ).
</figureCaption>
<figure confidence="0.999346413793103">
(b)
(a)
Survival
SPai
IPai ICiu ISqu
Permutation
SFor
SCiu
WPai WCiu
SAta
WPaiWPaiwPai waiwPaiwPaiwPai wPai
V
SSqu
WFor
V
?C
V V
V
WAta
WFor
V
IAta
WSqu
WAta
L
V
M
Evolution
G
</figure>
<page confidence="0.995223">
346
</page>
<bodyText confidence="0.999427285714286">
are obscured. The task of inference then is to re-
cover the original cognate groups.
The generative process for their model is as fol-
lows:
in likelihood. That is, for all pairs of cognate groups
ga with words wa and gb with words wb, we com-
pute the score:
</bodyText>
<listItem confidence="0.922338928571429">
• For each cognate group g, choose a root word
Wroot ∼ p(W |A), a language model over
words.
• For each language E in a pre-order traversal of
the phylogeny:
1. Choose S` ∼ Bernoulli(β`), indicating
whether or not the word survives.
2. If the word survives, choose W` ∼
p(W |ϕ`, Wpar(`)).
3. Otherwise, stop generating words in that
language and its descendants.
• For each language, choose a random permuta-
tion 7r of the observed data, and rearrange the
cognates according to this permutation.
</listItem>
<bodyText confidence="0.999472846153846">
We reproduce the graphical model for HK10 for a
small phylogeny in Figure 1a.
Inference in this model is intractable; to perform
inference exactly, one has to reason over all parti-
tions of the data into cognate groups. To address
this problem, Hall and Klein (2010) propose an it-
erative bipartite matching scheme where one lan-
guage is held out from the others, and then words
are assigned to the remaining groups to maximize
the probability of the attachment. That is, for some
language E and fixed assignments 7r_` for the other
languages, they seek an assignment 7r` that maxi-
mizes:
</bodyText>
<equation confidence="0.824231">
log p(w(`,πe(g))|ϕ, 7r, w_`)
</equation>
<bodyText confidence="0.9999347">
Unfortunately, while this approach was effective
with only a few languages (they tested on three), this
algorithm cannot scale to the eighteen languages in
Formosan, let alone the hundreds of languages in
Oceanic. Therefore, we make two simple modifi-
cations. First, we restrict the cognate assignments
to stay within a gloss. Thus, there are many fewer
potential matchings to consider. Second, we use an
agglomerative inference procedure, which greedily
merges cognate groups that result in the greatest gain
</bodyText>
<equation confidence="0.807431">
log p(waub|ϕ) − log p(wa|ϕ) − log p(wb|ϕ)
</equation>
<bodyText confidence="0.9996788">
This score is the difference between the log proba-
bility of generating two cognate groups jointly and
generating them separately. We then merge the two
that generate the highest gain in likelihood. Like
the iterative bipartite matching algorithm described
above, this algorithm is not exact. However, it is
O(n2 log n) (where n is the size of the largest gloss,
which for Oceanic is 153), while the bipartite match-
ing algorithm is O(n3) (Kuhn, 1955).
Actually, the original HK10 is doubly intractable.
They use weighted automata to represent distribu-
tions over strings, but these automata—particularly
if they are non-deterministic—make inference in
any non-trivial graphical model intractable. We dis-
cuss this issue in more detail in Section 6.
</bodyText>
<subsectionHeader confidence="0.999076">
3.2 A Parsimony-Inspired Model
</subsectionHeader>
<bodyText confidence="0.991018736842106">
We now describe a new model called PARSIM that
supports exact inference tractably, though it sacri-
fices some of the expressive power of HK10. In
our model, each language has at most one word for
each gloss, and this one word changes from one
language to its children according to some edge-
specific Markov process. These changes may either
be mutations, which merely change the surface form
of the word, or innovations, which start a new word
in a new cognate group that is unrelated to the previ-
ous word. Mutations take the form of a conditional
edit operation that models insertions, substitutions,
and deletions that correspond to regular (and, with
lower probability, irregular) sound changes that are
likely to occur between a language and its parent.
Innovations, on the other hand, are generated from a
language model independent of the parent’s word.
Specifically, our generative process takes the fol-
lowing form:
</bodyText>
<listItem confidence="0.98815925">
• For each gloss m, choose a root word Wroot ∼
A, a language model over words.
• For each language E in a pre-order traversal of
the phylogeny:
</listItem>
<equation confidence="0.94432325">
7r∗ = argmax
π
�
g
</equation>
<page confidence="0.939342">
347
</page>
<table confidence="0.769012">
Rukai Paiwan Ciuli Squliq
pouroukou purrok malapzo mappo
</table>
<figureCaption confidence="0.9990294">
Figure 2: A small example of how PARSIM works.
Listed here are the words for “ten” in four languages
from the Formosan family, along with the tree that ex-
plains them. The dashed line indicates an innovation on
the branch.
</figureCaption>
<listItem confidence="0.999855833333333">
1. Choose It — Bernoulli(βt), indicating
whether or not the word is an innovation
or a mutation.
2. If it is a mutation, choose Wt —
p(Wkot, Wpar(t)).
3. Otherwise, choose Wt — A.
</listItem>
<bodyText confidence="0.9991644">
We also depict our model as a plate diagram for a
small phylogeny in Figure 1b.
Because there is only one tree per gloss, there
is no assignment problem to consider, which is the
main source of the intractability of HK10. Instead,
pieces of the phylogeny are simply “cut” into sub-
trees whenever an innovation occurs. Thus, message
passing can be used to perform inference.
As an example of how our process works, con-
sider Figure 2. The Formosan word for “ten”
probably resembled either /purrok/ or /pouroukou/.
There was an innovation in Ciuli and Squliq’s an-
cestor Atayalic that produced a new word for ten.
This word then mutated separately into the words
/malapzo/ and /mappo/, respectively.
</bodyText>
<sectionHeader confidence="0.993307" genericHeader="method">
4 Relation to Parsimony
</sectionHeader>
<bodyText confidence="0.999970510638298">
PARSIM is related to the parsimony principle
from computational biology (Cavalli-Sforza and Ed-
wards, 1965; Fitch, 1971), where it is used to search
for phylogenies. When using parsimony, a phy-
logeny is scored according to the derivation that re-
quires the fewest number of changes of state, where
a state is typically thought of as a gene or some other
trait in a species. These genes are typically called
“characters” in the computational biology literature,
and two species would have the same value for a
character if they share the same property that that
state represents.
When inducing phylogenies of languages, a natu-
ral choice for characters are glosses from a restricted
vocabulary like a Swadesh list, and two words are
represented as the same value for a character if they
are cognate (Ringe et al., 2002). Other features can
be used (Daum´e III and Campbell, 2007; Daum´e III,
2009), but they are not relevant to our discussion.
Consider the small example in Figure 3a with just
four languages. Here, cognacy is encoded using
characters. In this example, at least two changes of
state are required to explain the data: both C and B
must have evolved from A. Therefore, the parsimony
score for this tree is two.
Of course, there is no reason why all changes
should be equally likely. For instance, it might be
extremely likely that B changes into both A and
C, but that A never changes into B or C, and so
weighted variants of parsimony might be neces-
sary (Sankoff and Cedergren, 1983).
With this in mind, PARSIM can be thought of a
weighted variant of parsimony, with two differences.
First, the characters do not indicate ahead of time
which words are related. Instead, the characters are
the words themselves. Second, the transitions be-
tween different states (words) are not uniform. In-
stead, they are weighted by the log probability of
one word changing into another, including both mu-
tations and innovations.
Thus, the task of inference in PARSIM is to find
the most “parsimonious” explanation for the words
we have observed, which is the same as finding the
most likely derivation. Because the distances be-
tween words (that is, the transition probabilities)
are not known ahead of time, they must instead be
learned, which we discuss in Section 7.3
</bodyText>
<sectionHeader confidence="0.761786" genericHeader="method">
5 Limitations of the Parsimony Model
</sectionHeader>
<bodyText confidence="0.99990325">
Potentially, our parsimony model sacrifices a cer-
tain amount of power to make inference tractable.
Specifically, it cannot model homoplasy, the pres-
ence of more than one word in a language for a given
</bodyText>
<footnote confidence="0.9930655">
3It is worth noting that we are not the first to point out a
connection between parsimony and likelihood. Indeed, many
authors in the computational biology literature have formally
demonstrated a connection (Farris, 1973; Felsenstein, 1973).
</footnote>
<page confidence="0.984559">
348
</page>
<figure confidence="0.999988434782609">
(b)
(c)
{A,B}
A
A
A
A
A
A B C A
{A,B} {A,B}
A B B A
A B B A
A A
(a)
(d)
A
B
B
B
B B
A
A
A
</figure>
<figureCaption confidence="0.9969028">
Figure 3: Trees illustrating parsimony and its limitations. In these trees, there are four languages, with words A, B, and
C in various configurations. (a) The most parsimonious derivation for this tree has all intermediate states as A. There
are thus two changes. (b) An example of homoplasy. Here, given this tree, it seems likely that the ancestral languages
contained both A and B. (c) PARSIM cannot recover the example from (b), and so it encodes two innovations (shown
as dashed lines). (d) The HK10 model can recover this relationship, but this power makes the model intractable.
</figureCaption>
<bodyText confidence="0.996200771428571">
gloss. Homoplasy can arise for a variety of reasons
in phylogenetic models of cognates, and we describe
some in this section.
Consider the example illustrated in Figure 3b,
where the two central languages share a cognate, as
do the two outer languages. This is the canonical ex-
ample of homoplasy, and PARSIM cannot correctly
recover this grouping. Instead, it can at best only se-
lect group A or group B as the value for the parent,
and leave the other group fragmented as two innova-
tions, as in Figure 3c. On the other hand, HK10 can
recover this relationship (Figure 3d), but this power
is precisely what makes it intractable.
There are two reasons this kind of homoplasy
could arise. The first is that there were indeed two
words in the parent language for this gloss, or that
there were two words with similar meanings and
the two meanings drifted together. Second, the tree
could be an inadequate model of the evolution in
this case. For instance, there could have been a cer-
tain amount of borrowing between two of these lan-
guages, or there was not a single coherent parent lan-
guage, but rather a language continuum that cannot
be explained by any tree.
However, homoplasy seems to be relatively un-
common (though not unheard of) in the Oceanic and
Formosan families. Where it does appear, our model
should simply fail to get one of the cognate groups,
instead explaining all of them via innovation. To
repair this shortcoming, we can simply run the ag-
glomerative clustering procedure for the model of
Hall and Klein (2010), starting from the groups that
PARSIM has recovered. Using this procedure, we
can hopefully recover many of the under-groupings
caused by homoplasy.
</bodyText>
<sectionHeader confidence="0.997565" genericHeader="method">
6 Inference and Scale
</sectionHeader>
<subsectionHeader confidence="0.667455">
6.1 Inference
</subsectionHeader>
<bodyText confidence="0.999990545454546">
In this section we describe the basics of infer-
ence in the PARSIM model. We have a nearly
tree-structured graphical model (Figure 1); it is
not a tree only because of the innovation param-
eters. Therefore, we apply the common trick of
grouping variables to form a tree. Specifically, we
group each word variable W` with its innovation
parameter I`. The distribution of interest is then
p(W`, I`|Wpar(`), φ`, β`), and the primary operation
is summing out messages µ from the children of a
language and sending a new message to its parent:
</bodyText>
<equation confidence="0.9989215">
Y: �
µ`(wpar(`)) = p(w`|·)
w$ k&apos; ∈ child(C)
p(w`|·) = p(w`|I` = 0, wpar(`), φ`)p(I` = 0|β`)
+ p(w`|I` = 1,φ`)P(I` = 1|β`)
(1)
</equation>
<bodyText confidence="0.9999568">
The first term involves computing the probability of
the word mutating from its parent, and the second
involves the probability of the child word from a lan-
guage model. We describe the parameters and pro-
cedures for these operations in 7.1.
</bodyText>
<subsectionHeader confidence="0.998316">
6.2 Scale
</subsectionHeader>
<bodyText confidence="0.9998122">
Even though inference by message-passing in our
model is tractable, we needed to make certain con-
cessions to make inference acceptably fast. These
choices mainly affect how we represent distributions
over strings.
</bodyText>
<equation confidence="0.471981">
µ`�(w`)
</equation>
<page confidence="0.944316">
349
</page>
<bodyText confidence="0.999208845070423">
First, we need to model distributions and mes- probabilities β`, and the global language model A
sages over words on the internal nodes of a phy- for generating new words. We learn these parame-
logeny. The natural choice in this scenario is to use ters via Expectation Maximization (Dempster et al.,
weighted finite automata (Mohri et al., 1996). Au- 1977), iterating between computing expected counts
tomata have been used to successfully model distri- and adjusting parameters to maximize the posterior
butions of strings for inferring morphology (Dreyer probability of the parameters. In this section, we de-
and Eisner, 2009) as well as cognate detection (Hall scribe those parameters.
and Klein, 2010). Even in models that would be 7.1 Sound Laws
tractable with “ordinary” messages, inference with The core piece of our system is learning the sound
automata quickly becomes intractable, because the laws associated with each edge. Since the founda-
size of the automata grow exponentially with the tion of historical linguists with the neogrammari-
number of messages passed. Therefore, approxima- ans, linguists have argued for the regularity of sound
tions must be used. Dreyer and Eisner (2009) used change at the phonemic level (Schleicher, 1861;
a mixture of a k-best list and a unigram language Bloomfield, 1938). That is to say, if in some lan-
model, while Hall and Klein (2010) used an approx- guage a /t/ changes to a /d/ in some word, it is al-
imation procedure that projected complex automata most certain that it will change in every other place
to simple, tractable automata using a modified KL that has the same surrounding context.
divergence. In practice, of course, sound change is not entirely
While either approach could be used here in prin- regular, and complex extralinguistic events can lead
ciple, we found that automata machinery was simply to sound changes that are irregular. For example,
too slow for our application. Instead, we exploit the in some cultures in which Oceanic languages are
intuition that we do not need to accurately recon- spoken, the name of the chief is taboo: one cannot
struct the word for any ancestral language. More- speak his name, nor say any word that sounds too
over, it is inefficient to keep track of probabilities for much like his name. Speakers of these languages
all strings. Therefore, we only track scores for words do find ways around this prohibition, often resulting
that actually exist in a given gloss, which means that in sound changes that cannot be explained by sound
internal nodes only have mass on those words. That laws alone (Keesing and Fifi’i, 1969).
is, if a gloss has 10 distinct words across all the lan- Nevertheless, we find it useful to model sound
guages in our dataset, we pass messages that only change as a largely regular if stochastic process.
contain information about those 10 words. We employ a sound change model whose expressive
Now, this representation—while more efficient power is equivalent to that of Hall and Klein (2010),
than the automata representations—results in infer- though with a different parameterization. We model
ence that is still quadratic in the number of words the evolution of a word w` to its child w`, as a
in a gloss, since we have distributions of the form sequence of unigram edits that include insertions,
p(w`|wpar(`)Iφ`). Intuitively, it is unlikely that a deletions, and substitutions. Specifically, we use a
word from one distant branch of tree resembles a standard three-state pair hidden Markov model that
word in another branch. Therefore, rather than score is closely related to the classic alignment algorithm
all of these unlikely words, we use a beam where we of Needleman and Wunsch (1970) (Durbin et al.,
only factor in words whose score is at most a fac- 2006).
tor of a−10 less than the maximum score. Our initial The three states in this HMM correspond to
experiments found that using a beam provides large matches/substitutions, insertions, and deletions. The
savings in time with little impact on prediction qual- transitions are set up such that insertions and dele-
ity. tions cannot be interleaved. This prevents spurious
7 Learning equivalent alignments, which would cause the model
PARSIM has three kinds of parameters that we need to assign unnecessarily higher probability to transi-
to learn: the mutation parameters co`, the innovation tions with many insertions and deletions.
350 Actually learning these parameters involves learn-
ing the transition probabilities of this HMM (which
model the overall probability of insertion and dele-
tion) as well as the emission probabilities (which
model the particular edits). Because there are rel-
atively few words for each language (96 on average
in Oceanic), we found it important to tie together
the parameters for the various languages, in contrast
to Hall and Klein (2010) who did not. In our maxi-
mization step, we fit a joint log-linear model for each
language, using features that are both specific to a
language and shared across languages. Our features
included indicators on each substitution, insertion,
and deletion operation, along with an indicator for
the outcome of each edit operation. This last fea-
ture reflects the propensity of a particular phoneme
to appear in a given language at all, no matter what
its ancestral phoneme was. This parameterization
is similar to the one used in the reconstruction sys-
tem of Bouchard-Cˆot´e et al. (2009), except that they
used edit operations that conditioned on the context
of the surrounding word, which is crucial when try-
ing to accurately reconstruct ancestral word forms.
To encourage parameter sharing, we used an E2 reg-
ularization penalty.
</bodyText>
<subsectionHeader confidence="0.997882">
7.2 Innovation Parameters
</subsectionHeader>
<bodyText confidence="0.999944833333334">
The innovation parameters Ot are parameters for
simple Bernoulli distribution that govern the propen-
sity for a language to start a new word. These pa-
rameters can be learned separately, though due to
data sparsity, we found it better to use a tied param-
eterization as with the sound laws. Specifically, we
fit a log linear model whose features are indicators
on the specific language, as well as a global inno-
vation parameter that is shared across all languages.
As with the sound laws, we used an E2 regularization
penalty to encourage the use of the global innovation
parameter.
</bodyText>
<subsectionHeader confidence="0.991813">
7.3 Language Model
</subsectionHeader>
<bodyText confidence="0.99988525">
Finally, we have a single language model A that is
also shared across all languages. A is a simple bi-
gram language model over characters in the Interna-
tional Phonetic Alphabet. A is used when generating
new words either via innovation or from the root of
the tree.
In principle, we could of course have language
models specific to each language, but because there
</bodyText>
<table confidence="0.999193888888889">
Formosan
System Prec Recall F1 Purity
Agg. HK10 77.6 83.2 80.0 84.7
PARSIM 87.8 71.0 78.5 94.6
Combination 85.2 81.3 83.2 92.3
Oceanic
System Prec Recall F1 Purity
PARSIM 84.4 62.1 71.5 91.8
Combination 76.0 73.8 74.9 85.5
</table>
<tableCaption confidence="0.998797">
Table 1: Results on the Formosan and Oceanic fami-
</tableCaption>
<bodyText confidence="0.877631363636364">
lies. PARSIM is the new parsimony model in this pa-
per, Agg. HK10 is our agglomerative variant of Hall and
Klein (2010) and Combination uses PARSIM’s output to
seed the agglomerative matcher. For the agglomerative
systems, we report the point with maximal F1 score, but
we also show precision/recall curves. (See Figure 4.)
are so few words per language, we found that
branch-specific language models caused the model
to prefer to innovate at almost every node since the
language models could essentially memorize the rel-
atively small vocabularies of these languages.
</bodyText>
<sectionHeader confidence="0.999193" genericHeader="method">
8 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999965">
8.1 Cognate Recovery
</subsectionHeader>
<bodyText confidence="0.999587157894737">
We ran both PARSIM and our agglomerative ver-
sion of HK10 on the Formosan datasets. For PAR-
SIM, we initialized the mutation parameters cp to a
model that preferred matches to insertions, substi-
tutions and deletions by a factor of e3, innovation
parameters to 0.5, and the language model to a uni-
form distribution over characters. For the agglomer-
ative HK10, we initialized its parameters to the val-
ues found by our model.4
Based on our observations about homoplasy, we
also considered a combined system where we ran
PARSIM, and then seeded the agglomerative cluster-
ing algorithm with the clusters found by PARSIM.
For evaluation, we report a few metrics. First,
we report cluster purity, which is a kind of pre-
cision measure for clusterings. Specifically, each
cluster is assigned to the cognate group that is the
most common cognate word in that group, and then
purity is computed as the fraction of words that
</bodyText>
<footnote confidence="0.980292">
4Attempts to learn parameters directly with the agglomera-
tive clustering algorithm were not effective.
</footnote>
<page confidence="0.996462">
351
</page>
<figure confidence="0.9479925">
0.4 0.5 0.6 0.7 0.8 0.9 1
Recall
</figure>
<figureCaption confidence="0.9741174">
Figure 4: Precision/Recall curves for our systems. The
Combined System starts from PARSIM’s output, so it
has fewer points to plot, and starts from a point with
lower precision. As PARSIM outputs only one result, it
is starred.
</figureCaption>
<bodyText confidence="0.998066666666667">
are in a cluster whose gold cognate group matches
the cognate group of the cluster. For gold parti-
tions G = {G1, G2, ... , Gg} and found partitions
</bodyText>
<equation confidence="0.99117475">
F = {F1, F2,..., Ff}, we have: purity(G, F) =
�
1 f maxg |Gg ∩Ff|. We also report pairwise pre-
N
</equation>
<bodyText confidence="0.999982545454545">
cision and recall computed over pairs of words.5 Fi-
nally, because agglomerative clustering does not de-
fine a natural “stopping point” other than when the
likelihood gain decreases to 0—which did not per-
form well in our initial tests—we will report both
a precision/recall curve, as well the maximum pair-
wise F1 obtained by the agglomerative HK10 and
the combined system.
The results are in Table 1. On Formosan, PAR-
SIM has much higher precision and purity than our
agglomerative version of HK10 at its highest point,
though its recall and F1 suffer somewhat. Of course,
the comparison is not quite fair, since we have se-
lected the best possible point for HK10.
However, our combination of the two systems
does even better. By feeding our high-precision re-
sults into the agglomerative system and sacrificing
just a little precision, our combined system achieves
much higher F1 scores than either of the systems
alone.
Next, we also examined precision and recall
curves for the two agglomerative systems on For-
</bodyText>
<footnote confidence="0.8230985">
5The main difference between precision and purity is that
pairwise precision is inherently quadratic, meaning that it pe-
nalizes mistakes in large groups much more heavily than mis-
takes in small groups.
</footnote>
<bodyText confidence="0.999957375">
mosan, which we have plotted in Figure 4, along
with the one point output by PARSIM.
We then ran PARSIM and the combined system
on the much larger Oceanic dataset. Performance
on all metrics decreased somewhat, but this is to be
expected since there is so much more data. As with
Formosan, PARSIM has higher precision than the
combined system, but it has much lower recall.
</bodyText>
<subsectionHeader confidence="0.995597">
8.2 Reconstruction
</subsectionHeader>
<bodyText confidence="0.999988925925926">
We also wanted to see how well our cognates could
be used to actually reconstruct the ancestral forms of
words. To do so, we ran a version of Bouchard-Cˆot´e
et al. (2009)’s reconstruction system using both the
cognate groups PARSIM found in the Oceanic lan-
guage family and the gold cognate groups provided
by the ABVD. We then evaluated the average Leven-
shtein distance of the reconstruction for each word
to the reconstruction of that word’s Proto-Oceanic
ancestor provided by linguists. Our evaluation dif-
fers from Bouchard-Cˆot´e et al. (2009) in that they
averaged over cognate groups, which does not make
sense for our task because there are different cognate
groups. Instead, we average over per-modern-word
reconstruction error.
Using this metric, reconstructions using our sys-
tem’s cognates are an average of 2.47 edit opera-
tions from the gold reconstruction, while with gold
cognates the error is 2.19 on average. This repre-
sents an error increase of 12.8%. To see if there
was some pattern to these errors, we also plotted the
fraction of words with each Levenshtein distance for
these reconstructions in Figure 5. While the plots are
similar, the automatic cognates exhibit a longer tail.
Thus, even with automatic cognates, the reconstruc-
tion system can reconstruct words faithfully in many
cases, but in a few instances our system fails.
</bodyText>
<sectionHeader confidence="0.973346" genericHeader="evaluation">
9 Analysis
</sectionHeader>
<bodyText confidence="0.99991275">
We now consider some of the errors made by our
system. Broadly, there are two kinds of mistakes
in a model like ours: those affecting precision and
those affecting recall.
</bodyText>
<subsectionHeader confidence="0.959489">
9.1 Precision
</subsectionHeader>
<bodyText confidence="0.999899666666667">
Many of our precision errors seem to be due to
our somewhat limited model of sound change. For
instance, the language Pazeh has two words for
</bodyText>
<figure confidence="0.989071857142857">
1
0.9
Combined System
PARSIM
Agg. HK10
0.7
0.6
Precision
0.8
352
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
0
0 1 2 3 4 5 6 7 8 9
Levenshtein Distance
</figure>
<figureCaption confidence="0.99097175">
Figure 5: Percentage of words with varying levels of
Levenshtein distance from the gold reconstruction. Gold
Cognates were hand-annotated by linguists, while Auto-
matic Cognates were found by our system.
</figureCaption>
<bodyText confidence="0.999993833333333">
“to sleep:” /mudamai/ and /mid@m/. Somewhat
surprisingly the former word is cognate with Pai-
wan /qmereN/ and Saisiat /ma?r@m/ while the lat-
ter is not. Our system, however, makes the mistake
of grouping /mid@m/ with the Paiwan and Saisiat
words. Our system has inferred that the insertions of
/u/ and /ai/ (which are required to bring /mudamai/
into alignment with the Saisiat and Paiwan words)
are less likely than substituting a few vowels and the
consonant /r/ for /d/ (which are required to align
/mid@m/). Perhaps a more sophisticated model of
sound change could correctly learn this relationship.
However, a preliminary inspection of the data
seems to indicate that not all of our precision errors
are actually errors, but rather places where the data
is insufficiently annotated (and indeed, the ABVD is
still a work in progress). For instance, consider the
words for “meat/flesh” in the Formosan languages:
Squliq /hi?/, Bunun /titi?/, Paiwan /seti/, Kavalan
/?isi?/, CentralAmi /titi/, Our system groups all of
these words except for Squliq /hi?/. However, de-
spite these words’ similarity, there are actually three
cognate groups here. One includes Squliq /hi?/ and
Kavalan /?isi?/, another includes just Paiwan /seti/,
and the third includes Bunun /titi?/ and CentralAmi
/titi/. Crucially, these cognate groups do not fol-
low the phylogeny closely. Thus, either there was a
significant amount of borrowing between these lan-
guages, or there was a striking amount of homoplasy
in Proto-Formosan, or these words are in fact mostly
cognate. While a more thorough, linguistically-
informed analysis is needed to ensure that these are
actually cognates, we believe that our system, in
conjunction with a trained Austronesian specialist,
could potentially find many more cognate groups,
speeding up the process of completing the ABVD.
</bodyText>
<subsectionHeader confidence="0.946461">
9.2 Recall
</subsectionHeader>
<bodyText confidence="0.999979692307692">
Our system can also fail to group words that should
be grouped. One recurring problem seems to
be reduplication, which is a fairly common phe-
nomenon in Austronesian languages. For instance,
there is a cognate group for “to eat” that includes
Bunun /maun/, Thao /kman/, Favorlang /man/, and
Sediq /manakamakan/, among others. Our system
correctly finds this group, with the exception of
/manakamakan/, which is clearly the result of redu-
plication. Reduplication cannot be modeled using
mere sound laws, and so a more complex transition
model is needed to correctly identify these kinds of
changes.
</bodyText>
<sectionHeader confidence="0.997508" genericHeader="conclusions">
10 Conclusion
</sectionHeader>
<bodyText confidence="0.9999767">
We have presented a new system for automatically
finding cognates across many languages. Our sys-
tem is comprised of two parts. The first, PAR-
SIM, is a new high-precision generative model with
tractable inference. The second, HK10, is a mod-
ification of Hall and Klein (2010) that makes their
approximate inference more efficient. We discuss
certain trade-offs needed to make both models scale,
and demonstrated its performance on the Formosan
and Oceanic language families.
</bodyText>
<sectionHeader confidence="0.998947" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99919725">
Shane Bergsma and Greg Kondrak. 2007. Multilingual
cognate identification using integer linear program-
ming. In RANLP Workshop on Acquisition and Man-
agement of Multilingual Lexicons, Borovets, Bulgaria,
September.
Leonard Bloomfield. 1938. Language. Holt, New York.
R. A. Blust. 2009. The Austronesian languages. Aus-
tralian National University.
Alexandre Bouchard-Cˆot´e, Percy Liang, Thomas Grif-
fiths, and Dan Klein. 2007. A probabilistic approach
to diachronic phonology. In EMNLP.
Alexandre Bouchard-Cˆot´e, Thomas L. Griffiths, and Dan
Klein. 2009. Improved reconstruction of protolan-
guage word forms. In NAACL, pages 65–73.
L. L. Cavalli-Sforza and A. W. F. Edwards. 1965. Analy-
sis of human evolution. In S. J. Geerts Genetics Today,
</reference>
<figure confidence="0.920313333333333">
Fraction of Words
Automatic Cognates
Gold Cognates
</figure>
<page confidence="0.991291">
353
</page>
<reference confidence="0.999619105263158">
editor, Proceedings of XIth International Congress of
Genetics, 1963, Vol, page 923–933. 3, 3.
Hal Daum´e III and Lyle Campbell. 2007. A Bayesian
model for discovering typological implications. In
Conference of the Association for Computational Lin-
guistics (ACL).
Hal Daum´e III. 2009. Non-parametric Bayesian areal
linguistics. In NAACL.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum Likelihood from Incomplete Data via the
EM Algorithm. Journal of the Royal Statistical Soci-
ety. Series B (Methodological), 39(1):1–38.
Markus Dreyer and Jason Eisner. 2009. Graphical mod-
els over multiple strings. In EMNLP, Singapore, Au-
gust.
R. Durbin, S. Eddy, A. Krogh, and G. Mitchison. 2006.
Biological sequence analysis. eleventh edition.
James S. Farris. 1973. On Comparing the Shapes of
Taxonomic Trees. Systematic Zoology, 22(1):50–54,
March.
J. Felsenstein. 1973. Maximum likelihood and mini-
mum steps methods for estimating evolutionnary trees
from data on discrete characters. Systematic Zoology,
23:240–249.
W. M. Fitch. 1971. Toward defining the course of evo-
lution: minimal change for a specific tree topology.
Systematic Zoology, 20:406–416.
S.J. Greenhill, R. Blust, and R.D. Gray. 2008. The
Austronesian basic vocabulary database: from bioin-
formatics to lexomics. Evolutionary Bioinformatics,
4:271–283.
David Hall and Dan Klein. 2010. Finding cognates using
phylogenies. In Association for Computational Lin-
guistics (ACL).
Robert M. Keesing and Jonathan Fifi’i. 1969. Kwaio
word tabooing in its cultural context. Journal of the
Polynesian Society, 78(2):154–177.
Grzegorz Kondrak. 2001. Identifying cognates by pho-
netic and semantic similarity. In NAACL.
Harold W. Kuhn. 1955. The Hungarian method for the
assignment problem. Naval Research Logistics Quar-
terly, 2:83–97.
John B. Lowe and Martine Mazaudon. 1994. The re-
construction engine: a computer implementation of
the comparative method. Computational Linguistics,
20(3):381–417.
Gideon S. Mann and David Yarowsky. 2001. Multipath
translation lexicon induction via bridge languages. In
NAACL.
Mehryar Mohri, Fernando Pereira, and Michael Riley.
1996. Weighted automata in text and speech process-
ing. In ECAI-96 Workshop. John Wiley and Sons.
Andrea Mulloni. 2007. Automatic prediction of cognate
orthography using support vector machines. In ACL,
pages 25–30.
Saul B. Needleman and Christian D. Wunsch. 1970. A
general method applicable to the search for similarities
in the amino acid sequence of two proteins. Journal of
Molecular Biology, 48(3):443 – 453.
John Nerbonne. 2010. Measuring the diffusion of lin-
guistic change. Philosophical Transactions of the
Royal Society B: Biological Sciences.
J. Nichols. 1992. Linguistic diversity in space and time.
University of Chicago Press.
Michael P. Oakes. 2000. Computer estimation of vocab-
ulary in a protolanguage from word lists in four daugh-
ter languages. Quantitative Linguistics, 7(3):233–243.
Don Ringe, Tandy Warnow, and Ann Taylor. 2002. Indo-
european and computational cladistics. Transactions
of the Philological Society, 100(1):59–129.
D. Sankoff and R. J. Cedergren, 1983. Simultaneuous
comparison of three or more sequences related by a
tree, page 253–263. Addison-Wesley, Reading, MA.
August Schleicher. 1861. A Compendium of the Com-
parative Grammar of the Indo-European, Sanskrit,
Greek and Latin Languages.
</reference>
<page confidence="0.999145">
354
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.823018">
<title confidence="0.99985">Large-Scale Cognate Recovery</title>
<author confidence="0.999101">David Hall</author>
<author confidence="0.999101">Dan</author>
<affiliation confidence="0.992151">Computer Science University of California at</affiliation>
<abstract confidence="0.984292636363636">We present a system for the large scale induction of cognate groups. Our model explains the evolution of cognates as a sequence of mutations and innovations along a phylogeny. On the task of identifying cognates from over 21,000 words in 218 different languages from the Oceanic language family, our model achieves a cluster purity score over 91%, while maintaining pairwise recall over 62%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Greg Kondrak</author>
</authors>
<title>Multilingual cognate identification using integer linear programming.</title>
<date>2007</date>
<booktitle>In RANLP Workshop on Acquisition and Management of Multilingual Lexicons,</booktitle>
<location>Borovets, Bulgaria,</location>
<contexts>
<context position="1866" citStr="Bergsma and Kondrak, 2007" startWordPosition="278" endWordPosition="281"> ancestral word forms of known cognate sets (Oakes, 2000; Bouchard-Cˆot´e et al., 2007; Bouchard-Cˆot´e et al., 2009), enabling linguists to explore deep historical relationships in an automated fashion. However, computational approaches thus far have not been able to offer the same kind of scale for identifying cognates. Previous work in cognate identification has largely focused on identifying cognates in pairs of languages (Mann and Yarowsky, 2001; Lowe and Mazaudon, 1994; Oakes, 2000; Kondrak, 2001; Mulloni, 2007), with a few recent exceptions that can find sets in a handful of languages (Bergsma and Kondrak, 2007; Hall and Klein, 2010). While it may seem surprising that cognate detection has not successfully scaled to large numbers of languages, the task poses challenges not seen in reconstruction and phylogeny inference. For instance, morphological innovations and irregular sound changes can completely obscure relationships between words in different languages. However, in the case of reconstruction, an unexplainable word is simply that: one can still correctly reconstruct its ancestor using words from related languages. In this paper, we present a system that uses two generative models for large-sca</context>
</contexts>
<marker>Bergsma, Kondrak, 2007</marker>
<rawString>Shane Bergsma and Greg Kondrak. 2007. Multilingual cognate identification using integer linear programming. In RANLP Workshop on Acquisition and Management of Multilingual Lexicons, Borovets, Bulgaria, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonard Bloomfield</author>
</authors>
<title>The Austronesian languages. Australian</title>
<date>1938</date>
<institution>National University.</institution>
<location>Language. Holt, New</location>
<contexts>
<context position="1048" citStr="Bloomfield, 1938" startWordPosition="154" endWordPosition="155">ic language family, our model achieves a cluster purity score over 91%, while maintaining pairwise recall over 62%. 1 Introduction The critical first step in the reconstruction of an ancient language is the recovery of related cognate words in its descendants. Unfortunately, this process has largely been a manual, linguisticallyintensive undertaking for any sizable number of descendant languages. The traditional approach used by linguists—the comparative method—iterates between positing putative cognates and then identifying regular sound laws that explain correspondences between those words (Bloomfield, 1938). Successful computational approaches have been developed for large-scale reconstruction of phylogenies (Ringe et al., 2002; Daum´e III and Campbell, 2007; Daum´e III, 2009; Nerbonne, 2010) and ancestral word forms of known cognate sets (Oakes, 2000; Bouchard-Cˆot´e et al., 2007; Bouchard-Cˆot´e et al., 2009), enabling linguists to explore deep historical relationships in an automated fashion. However, computational approaches thus far have not been able to offer the same kind of scale for identifying cognates. Previous work in cognate identification has largely focused on identifying cognates</context>
<context position="23548" citStr="Bloomfield, 1938" startWordPosition="3927" endWordPosition="3928">010). Even in models that would be 7.1 Sound Laws tractable with “ordinary” messages, inference with The core piece of our system is learning the sound automata quickly becomes intractable, because the laws associated with each edge. Since the foundasize of the automata grow exponentially with the tion of historical linguists with the neogrammarinumber of messages passed. Therefore, approxima- ans, linguists have argued for the regularity of sound tions must be used. Dreyer and Eisner (2009) used change at the phonemic level (Schleicher, 1861; a mixture of a k-best list and a unigram language Bloomfield, 1938). That is to say, if in some lanmodel, while Hall and Klein (2010) used an approx- guage a /t/ changes to a /d/ in some word, it is alimation procedure that projected complex automata most certain that it will change in every other place to simple, tractable automata using a modified KL that has the same surrounding context. divergence. In practice, of course, sound change is not entirely While either approach could be used here in prin- regular, and complex extralinguistic events can lead ciple, we found that automata machinery was simply to sound changes that are irregular. For example, too </context>
</contexts>
<marker>Bloomfield, 1938</marker>
<rawString>Leonard Bloomfield. 1938. Language. Holt, New York. R. A. Blust. 2009. The Austronesian languages. Australian National University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandre Bouchard-Cˆot´e</author>
<author>Percy Liang</author>
<author>Thomas Griffiths</author>
<author>Dan Klein</author>
</authors>
<title>A probabilistic approach to diachronic phonology.</title>
<date>2007</date>
<booktitle>In EMNLP.</booktitle>
<marker>Bouchard-Cˆot´e, Liang, Griffiths, Klein, 2007</marker>
<rawString>Alexandre Bouchard-Cˆot´e, Percy Liang, Thomas Griffiths, and Dan Klein. 2007. A probabilistic approach to diachronic phonology. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandre Bouchard-Cˆot´e</author>
<author>Thomas L Griffiths</author>
<author>Dan Klein</author>
</authors>
<title>Improved reconstruction of protolanguage word forms.</title>
<date>2009</date>
<booktitle>In NAACL,</booktitle>
<pages>65--73</pages>
<marker>Bouchard-Cˆot´e, Griffiths, Klein, 2009</marker>
<rawString>Alexandre Bouchard-Cˆot´e, Thomas L. Griffiths, and Dan Klein. 2009. Improved reconstruction of protolanguage word forms. In NAACL, pages 65–73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L L Cavalli-Sforza</author>
<author>A W F Edwards</author>
</authors>
<title>Analysis of human evolution.</title>
<date>1965</date>
<booktitle>Proceedings of XIth International Congress of Genetics, 1963, Vol,</booktitle>
<pages>923--933</pages>
<editor>In S. J. Geerts Genetics Today, editor,</editor>
<contexts>
<context position="2870" citStr="Cavalli-Sforza and Edwards, 1965" startWordPosition="433" endWordPosition="436"> case of reconstruction, an unexplainable word is simply that: one can still correctly reconstruct its ancestor using words from related languages. In this paper, we present a system that uses two generative models for large-scale cognate identification. Both models describe the evolution of words along a phylogeny according to automatically learned sound laws in the form of parametric edit distances. The first is an adaptation of the generative model of Hall and Klein (2010), and the other is a new generative model called PARSIM with connections to parsimony methods in computational biology (Cavalli-Sforza and Edwards, 1965; Fitch, 1971). Our model supports simple, tractable inference via message passing, at the expense of being unable to model some cognacy relationships. To help correct this deficiency, we also describe an agglomerative inference procedure for the model of Hall and Klein (2010). By using the output of our system as input to this system, we can find cognate groups that PARSIM alone cannot recover. We apply these models to identifying cognate groups from two language families using the Austronesian Basic Vocabulary Database (Greenhill et al., 2008), a catalog of words from about 40% of the Austro</context>
<context position="16060" citStr="Cavalli-Sforza and Edwards, 1965" startWordPosition="2646" endWordPosition="2650">f the intractability of HK10. Instead, pieces of the phylogeny are simply “cut” into subtrees whenever an innovation occurs. Thus, message passing can be used to perform inference. As an example of how our process works, consider Figure 2. The Formosan word for “ten” probably resembled either /purrok/ or /pouroukou/. There was an innovation in Ciuli and Squliq’s ancestor Atayalic that produced a new word for ten. This word then mutated separately into the words /malapzo/ and /mappo/, respectively. 4 Relation to Parsimony PARSIM is related to the parsimony principle from computational biology (Cavalli-Sforza and Edwards, 1965; Fitch, 1971), where it is used to search for phylogenies. When using parsimony, a phylogeny is scored according to the derivation that requires the fewest number of changes of state, where a state is typically thought of as a gene or some other trait in a species. These genes are typically called “characters” in the computational biology literature, and two species would have the same value for a character if they share the same property that that state represents. When inducing phylogenies of languages, a natural choice for characters are glosses from a restricted vocabulary like a Swadesh </context>
</contexts>
<marker>Cavalli-Sforza, Edwards, 1965</marker>
<rawString>L. L. Cavalli-Sforza and A. W. F. Edwards. 1965. Analysis of human evolution. In S. J. Geerts Genetics Today, editor, Proceedings of XIth International Congress of Genetics, 1963, Vol, page 923–933. 3, 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Lyle Campbell</author>
</authors>
<title>A Bayesian model for discovering typological implications.</title>
<date>2007</date>
<booktitle>In Conference of the Association for Computational Linguistics (ACL).</booktitle>
<marker>Daum´e, Campbell, 2007</marker>
<rawString>Hal Daum´e III and Lyle Campbell. 2007. A Bayesian model for discovering typological implications. In Conference of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
</authors>
<title>Non-parametric Bayesian areal linguistics.</title>
<date>2009</date>
<booktitle>In NAACL.</booktitle>
<marker>Daum´e, 2009</marker>
<rawString>Hal Daum´e III. 2009. Non-parametric Bayesian areal linguistics. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum Likelihood from Incomplete Data via the EM Algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society. Series B (Methodological),</journal>
<volume>39</volume>
<issue>1</issue>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum Likelihood from Incomplete Data via the EM Algorithm. Journal of the Royal Statistical Society. Series B (Methodological), 39(1):1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Dreyer</author>
<author>Jason Eisner</author>
</authors>
<title>Graphical models over multiple strings.</title>
<date>2009</date>
<booktitle>In EMNLP, Singapore,</booktitle>
<contexts>
<context position="23427" citStr="Dreyer and Eisner (2009)" startWordPosition="3905" endWordPosition="3908">the parameters. In this section, we deand Eisner, 2009) as well as cognate detection (Hall scribe those parameters. and Klein, 2010). Even in models that would be 7.1 Sound Laws tractable with “ordinary” messages, inference with The core piece of our system is learning the sound automata quickly becomes intractable, because the laws associated with each edge. Since the foundasize of the automata grow exponentially with the tion of historical linguists with the neogrammarinumber of messages passed. Therefore, approxima- ans, linguists have argued for the regularity of sound tions must be used. Dreyer and Eisner (2009) used change at the phonemic level (Schleicher, 1861; a mixture of a k-best list and a unigram language Bloomfield, 1938). That is to say, if in some lanmodel, while Hall and Klein (2010) used an approx- guage a /t/ changes to a /d/ in some word, it is alimation procedure that projected complex automata most certain that it will change in every other place to simple, tractable automata using a modified KL that has the same surrounding context. divergence. In practice, of course, sound change is not entirely While either approach could be used here in prin- regular, and complex extralinguistic </context>
</contexts>
<marker>Dreyer, Eisner, 2009</marker>
<rawString>Markus Dreyer and Jason Eisner. 2009. Graphical models over multiple strings. In EMNLP, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Durbin</author>
<author>S Eddy</author>
<author>A Krogh</author>
<author>G Mitchison</author>
</authors>
<title>Biological sequence analysis. eleventh edition.</title>
<date>2006</date>
<marker>Durbin, Eddy, Krogh, Mitchison, 2006</marker>
<rawString>R. Durbin, S. Eddy, A. Krogh, and G. Mitchison. 2006. Biological sequence analysis. eleventh edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James S Farris</author>
</authors>
<title>On Comparing the Shapes of Taxonomic Trees. Systematic Zoology,</title>
<date>1973</date>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="18678" citStr="Farris, 1973" startWordPosition="3094" endWordPosition="3095">istances between words (that is, the transition probabilities) are not known ahead of time, they must instead be learned, which we discuss in Section 7.3 5 Limitations of the Parsimony Model Potentially, our parsimony model sacrifices a certain amount of power to make inference tractable. Specifically, it cannot model homoplasy, the presence of more than one word in a language for a given 3It is worth noting that we are not the first to point out a connection between parsimony and likelihood. Indeed, many authors in the computational biology literature have formally demonstrated a connection (Farris, 1973; Felsenstein, 1973). 348 (b) (c) {A,B} A A A A A A B C A {A,B} {A,B} A B B A A B B A A A (a) (d) A B B B B B A A A Figure 3: Trees illustrating parsimony and its limitations. In these trees, there are four languages, with words A, B, and C in various configurations. (a) The most parsimonious derivation for this tree has all intermediate states as A. There are thus two changes. (b) An example of homoplasy. Here, given this tree, it seems likely that the ancestral languages contained both A and B. (c) PARSIM cannot recover the example from (b), and so it encodes two innovations (shown as dashed</context>
</contexts>
<marker>Farris, 1973</marker>
<rawString>James S. Farris. 1973. On Comparing the Shapes of Taxonomic Trees. Systematic Zoology, 22(1):50–54, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Felsenstein</author>
</authors>
<title>Maximum likelihood and minimum steps methods for estimating evolutionnary trees from data on discrete characters. Systematic Zoology,</title>
<date>1973</date>
<pages>23--240</pages>
<contexts>
<context position="18698" citStr="Felsenstein, 1973" startWordPosition="3096" endWordPosition="3097">en words (that is, the transition probabilities) are not known ahead of time, they must instead be learned, which we discuss in Section 7.3 5 Limitations of the Parsimony Model Potentially, our parsimony model sacrifices a certain amount of power to make inference tractable. Specifically, it cannot model homoplasy, the presence of more than one word in a language for a given 3It is worth noting that we are not the first to point out a connection between parsimony and likelihood. Indeed, many authors in the computational biology literature have formally demonstrated a connection (Farris, 1973; Felsenstein, 1973). 348 (b) (c) {A,B} A A A A A A B C A {A,B} {A,B} A B B A A B B A A A (a) (d) A B B B B B A A A Figure 3: Trees illustrating parsimony and its limitations. In these trees, there are four languages, with words A, B, and C in various configurations. (a) The most parsimonious derivation for this tree has all intermediate states as A. There are thus two changes. (b) An example of homoplasy. Here, given this tree, it seems likely that the ancestral languages contained both A and B. (c) PARSIM cannot recover the example from (b), and so it encodes two innovations (shown as dashed lines). (d) The HK1</context>
</contexts>
<marker>Felsenstein, 1973</marker>
<rawString>J. Felsenstein. 1973. Maximum likelihood and minimum steps methods for estimating evolutionnary trees from data on discrete characters. Systematic Zoology, 23:240–249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W M Fitch</author>
</authors>
<title>Toward defining the course of evolution: minimal change for a specific tree topology. Systematic Zoology,</title>
<date>1971</date>
<pages>20--406</pages>
<contexts>
<context position="2884" citStr="Fitch, 1971" startWordPosition="437" endWordPosition="438">ainable word is simply that: one can still correctly reconstruct its ancestor using words from related languages. In this paper, we present a system that uses two generative models for large-scale cognate identification. Both models describe the evolution of words along a phylogeny according to automatically learned sound laws in the form of parametric edit distances. The first is an adaptation of the generative model of Hall and Klein (2010), and the other is a new generative model called PARSIM with connections to parsimony methods in computational biology (Cavalli-Sforza and Edwards, 1965; Fitch, 1971). Our model supports simple, tractable inference via message passing, at the expense of being unable to model some cognacy relationships. To help correct this deficiency, we also describe an agglomerative inference procedure for the model of Hall and Klein (2010). By using the output of our system as input to this system, we can find cognate groups that PARSIM alone cannot recover. We apply these models to identifying cognate groups from two language families using the Austronesian Basic Vocabulary Database (Greenhill et al., 2008), a catalog of words from about 40% of the Austronesian languag</context>
<context position="16074" citStr="Fitch, 1971" startWordPosition="2651" endWordPosition="2652">ead, pieces of the phylogeny are simply “cut” into subtrees whenever an innovation occurs. Thus, message passing can be used to perform inference. As an example of how our process works, consider Figure 2. The Formosan word for “ten” probably resembled either /purrok/ or /pouroukou/. There was an innovation in Ciuli and Squliq’s ancestor Atayalic that produced a new word for ten. This word then mutated separately into the words /malapzo/ and /mappo/, respectively. 4 Relation to Parsimony PARSIM is related to the parsimony principle from computational biology (Cavalli-Sforza and Edwards, 1965; Fitch, 1971), where it is used to search for phylogenies. When using parsimony, a phylogeny is scored according to the derivation that requires the fewest number of changes of state, where a state is typically thought of as a gene or some other trait in a species. These genes are typically called “characters” in the computational biology literature, and two species would have the same value for a character if they share the same property that that state represents. When inducing phylogenies of languages, a natural choice for characters are glosses from a restricted vocabulary like a Swadesh list, and two </context>
</contexts>
<marker>Fitch, 1971</marker>
<rawString>W. M. Fitch. 1971. Toward defining the course of evolution: minimal change for a specific tree topology. Systematic Zoology, 20:406–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S J Greenhill</author>
<author>R Blust</author>
<author>R D Gray</author>
</authors>
<title>The Austronesian basic vocabulary database: from bioinformatics to lexomics. Evolutionary Bioinformatics,</title>
<date>2008</date>
<pages>4--271</pages>
<contexts>
<context position="3421" citStr="Greenhill et al., 2008" startWordPosition="522" endWordPosition="525">imony methods in computational biology (Cavalli-Sforza and Edwards, 1965; Fitch, 1971). Our model supports simple, tractable inference via message passing, at the expense of being unable to model some cognacy relationships. To help correct this deficiency, we also describe an agglomerative inference procedure for the model of Hall and Klein (2010). By using the output of our system as input to this system, we can find cognate groups that PARSIM alone cannot recover. We apply these models to identifying cognate groups from two language families using the Austronesian Basic Vocabulary Database (Greenhill et al., 2008), a catalog of words from about 40% of the Austronesian languages. We focus on data from two subfamilies of Austronesian: Formosan and 344 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 344–354, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics Oceanic. The datasets are by far the largest on which automated cognate recovery has ever been attempted, with 18 and 271 languages respectively. On the larger Oceanic data, our model can achieve cluster purity scores of 91.8%, while maintaining pairwise recall of 6</context>
<context position="5750" citStr="Greenhill et al., 2008" startWordPosition="888" endWordPosition="891">nguistically, the language family is as diverse as it is large, but a few regularities hold. From a phonological perspective, two features stand out. First, the phoneme inventories of these languages are typically small. For example, it is well-known that Hawaiian has only 13 phonemes. Moreover, the phonotactics of these languages are often restrictive. Sticking with the same example, Hawaiian only allows (C)V syllables: consonants clusters are forbidden, and no syllable may end with a consonant. 2.2 The Austronesian Basic Vocabulary Database The Austronesian Basic Vocabulary Database (ABVD) (Greenhill et al., 2008) is an ambitious, ongoing effort to catalog the lexicons and basic facts about all of the languages in the Austronesian language family. It also contains manual reconstructions for select ancestor languages produced by linguists. The sample we use—from Bouchard-Cˆot´e et al. (2009)—contains about 50,000 words across 471 languages spanning all the major divisions of Austronesian. These words are grouped into cognate groups and arranged by gloss. For instance, there are 37 distinct cognate groups for the gloss “tail.” One of these groups includes the words /ekor/, /ingko/, /iykot/, /kiiki?u/, an</context>
</contexts>
<marker>Greenhill, Blust, Gray, 2008</marker>
<rawString>S.J. Greenhill, R. Blust, and R.D. Gray. 2008. The Austronesian basic vocabulary database: from bioinformatics to lexomics. Evolutionary Bioinformatics, 4:271–283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Hall</author>
<author>Dan Klein</author>
</authors>
<title>Finding cognates using phylogenies.</title>
<date>2010</date>
<booktitle>In Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="1889" citStr="Hall and Klein, 2010" startWordPosition="282" endWordPosition="285">own cognate sets (Oakes, 2000; Bouchard-Cˆot´e et al., 2007; Bouchard-Cˆot´e et al., 2009), enabling linguists to explore deep historical relationships in an automated fashion. However, computational approaches thus far have not been able to offer the same kind of scale for identifying cognates. Previous work in cognate identification has largely focused on identifying cognates in pairs of languages (Mann and Yarowsky, 2001; Lowe and Mazaudon, 1994; Oakes, 2000; Kondrak, 2001; Mulloni, 2007), with a few recent exceptions that can find sets in a handful of languages (Bergsma and Kondrak, 2007; Hall and Klein, 2010). While it may seem surprising that cognate detection has not successfully scaled to large numbers of languages, the task poses challenges not seen in reconstruction and phylogeny inference. For instance, morphological innovations and irregular sound changes can completely obscure relationships between words in different languages. However, in the case of reconstruction, an unexplainable word is simply that: one can still correctly reconstruct its ancestor using words from related languages. In this paper, we present a system that uses two generative models for large-scale cognate identificati</context>
<context position="3147" citStr="Hall and Klein (2010)" startWordPosition="477" endWordPosition="480">n of words along a phylogeny according to automatically learned sound laws in the form of parametric edit distances. The first is an adaptation of the generative model of Hall and Klein (2010), and the other is a new generative model called PARSIM with connections to parsimony methods in computational biology (Cavalli-Sforza and Edwards, 1965; Fitch, 1971). Our model supports simple, tractable inference via message passing, at the expense of being unable to model some cognacy relationships. To help correct this deficiency, we also describe an agglomerative inference procedure for the model of Hall and Klein (2010). By using the output of our system as input to this system, we can find cognate groups that PARSIM alone cannot recover. We apply these models to identifying cognate groups from two language families using the Austronesian Basic Vocabulary Database (Greenhill et al., 2008), a catalog of words from about 40% of the Austronesian languages. We focus on data from two subfamilies of Austronesian: Formosan and 344 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 344–354, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Ling</context>
<context position="8272" citStr="Hall and Klein (2010)" startWordPosition="1308" endWordPosition="1311">18 languages. Thus, Formosan is a smaller development set that nevertheless is representative of larger families. For our final test set, we use the Oceanic subfamily, which includes almost 50% of the languages in the Austronesian family, meaning that it represents around 10% of all languages in the world. Oceanic also represents a large fraction of the ge345 ographic diversity of Austronesian, stretching from New Zealand in the south to Hawai’i in the north. Our sample includes 21863 words from 218 languages in the Oceanic family. 3 Models In this section we describe two models, one based on Hall and Klein (2010)—which we call HK10—and another new model that shares some connection to parsimony methods in computational biology, which we call PARSIM. Both are generative models that describe the evolution of words wt from a set of languages {E} in a cognate group g along a fixed phylogeny T.1 Each cognate group and word is also associated with a gloss or meaning m, which we assume to be fixed.2 In both models, words evolve according to regular sound laws cot, which are specific to each language. Also, both models will make use of a language model A, which is used for generating words that are not depende</context>
<context position="10321" citStr="Hall and Klein, 2010" startWordPosition="1674" endWordPosition="1677">e cognacy relationships 1Both of these models therefore are insensitive to geographic and historical factors that cannot be easily approximated by this tree. See Nichols (1992) for an excellent discussion of these factors. 2One could easily envision allowing the meaning of a word to change as well. Modeling this semantic drift has been considered by Kondrak (2001). In the ABVD, however, any semantic drift has already been elided, since the database has coarsened glosses to the extent that there is no meaningful way to model semantic drift given our data. Figure 1: Plate diagrams for (a) HK10 (Hall and Klein, 2010) and (b) PARSIM, our new parsimony model, for a small set of languages. In HK10, words are generated following a phylogenetic tree according to sound laws co, and then “scrambled” with a permutation π so that the original cognate groups are lost. In PARSIM, all words for each of the M glosses are generated in a single tree, with innovations I starting new cognate groups. The languages depicted are Formosan (For), Paiwan (Pai), Atayalic (Ata), Ciuli Atayalic (Ciu), and Squliq Atayalic (Squ). (b) (a) Survival SPai IPai ICiu ISqu Permutation SFor SCiu WPai WCiu SAta WPaiWPaiwPai waiwPaiwPaiwPai w</context>
<context position="12012" citStr="Hall and Klein (2010)" startWordPosition="1974" endWordPosition="1977"> of the phylogeny: 1. Choose S` ∼ Bernoulli(β`), indicating whether or not the word survives. 2. If the word survives, choose W` ∼ p(W |ϕ`, Wpar(`)). 3. Otherwise, stop generating words in that language and its descendants. • For each language, choose a random permutation 7r of the observed data, and rearrange the cognates according to this permutation. We reproduce the graphical model for HK10 for a small phylogeny in Figure 1a. Inference in this model is intractable; to perform inference exactly, one has to reason over all partitions of the data into cognate groups. To address this problem, Hall and Klein (2010) propose an iterative bipartite matching scheme where one language is held out from the others, and then words are assigned to the remaining groups to maximize the probability of the attachment. That is, for some language E and fixed assignments 7r_` for the other languages, they seek an assignment 7r` that maximizes: log p(w(`,πe(g))|ϕ, 7r, w_`) Unfortunately, while this approach was effective with only a few languages (they tested on three), this algorithm cannot scale to the eighteen languages in Formosan, let alone the hundreds of languages in Oceanic. Therefore, we make two simple modific</context>
<context position="20907" citStr="Hall and Klein (2010)" startWordPosition="3494" endWordPosition="3497">l of the evolution in this case. For instance, there could have been a certain amount of borrowing between two of these languages, or there was not a single coherent parent language, but rather a language continuum that cannot be explained by any tree. However, homoplasy seems to be relatively uncommon (though not unheard of) in the Oceanic and Formosan families. Where it does appear, our model should simply fail to get one of the cognate groups, instead explaining all of them via innovation. To repair this shortcoming, we can simply run the agglomerative clustering procedure for the model of Hall and Klein (2010), starting from the groups that PARSIM has recovered. Using this procedure, we can hopefully recover many of the under-groupings caused by homoplasy. 6 Inference and Scale 6.1 Inference In this section we describe the basics of inference in the PARSIM model. We have a nearly tree-structured graphical model (Figure 1); it is not a tree only because of the innovation parameters. Therefore, we apply the common trick of grouping variables to form a tree. Specifically, we group each word variable W` with its innovation parameter I`. The distribution of interest is then p(W`, I`|Wpar(`), φ`, β`), an</context>
<context position="23614" citStr="Hall and Klein (2010)" startWordPosition="3939" endWordPosition="3942">th “ordinary” messages, inference with The core piece of our system is learning the sound automata quickly becomes intractable, because the laws associated with each edge. Since the foundasize of the automata grow exponentially with the tion of historical linguists with the neogrammarinumber of messages passed. Therefore, approxima- ans, linguists have argued for the regularity of sound tions must be used. Dreyer and Eisner (2009) used change at the phonemic level (Schleicher, 1861; a mixture of a k-best list and a unigram language Bloomfield, 1938). That is to say, if in some lanmodel, while Hall and Klein (2010) used an approx- guage a /t/ changes to a /d/ in some word, it is alimation procedure that projected complex automata most certain that it will change in every other place to simple, tractable automata using a modified KL that has the same surrounding context. divergence. In practice, of course, sound change is not entirely While either approach could be used here in prin- regular, and complex extralinguistic events can lead ciple, we found that automata machinery was simply to sound changes that are irregular. For example, too slow for our application. Instead, we exploit the in some cultures</context>
<context position="25252" citStr="Hall and Klein (2010)" startWordPosition="4216" endWordPosition="4219">is prohibition, often resulting that actually exist in a given gloss, which means that in sound changes that cannot be explained by sound internal nodes only have mass on those words. That laws alone (Keesing and Fifi’i, 1969). is, if a gloss has 10 distinct words across all the lan- Nevertheless, we find it useful to model sound guages in our dataset, we pass messages that only change as a largely regular if stochastic process. contain information about those 10 words. We employ a sound change model whose expressive Now, this representation—while more efficient power is equivalent to that of Hall and Klein (2010), than the automata representations—results in infer- though with a different parameterization. We model ence that is still quadratic in the number of words the evolution of a word w` to its child w`, as a in a gloss, since we have distributions of the form sequence of unigram edits that include insertions, p(w`|wpar(`)Iφ`). Intuitively, it is unlikely that a deletions, and substitutions. Specifically, we use a word from one distant branch of tree resembles a standard three-state pair hidden Markov model that word in another branch. Therefore, rather than score is closely related to the classi</context>
<context position="27093" citStr="Hall and Klein (2010)" startWordPosition="4511" endWordPosition="4514">ee kinds of parameters that we need to assign unnecessarily higher probability to transito learn: the mutation parameters co`, the innovation tions with many insertions and deletions. 350 Actually learning these parameters involves learning the transition probabilities of this HMM (which model the overall probability of insertion and deletion) as well as the emission probabilities (which model the particular edits). Because there are relatively few words for each language (96 on average in Oceanic), we found it important to tie together the parameters for the various languages, in contrast to Hall and Klein (2010) who did not. In our maximization step, we fit a joint log-linear model for each language, using features that are both specific to a language and shared across languages. Our features included indicators on each substitution, insertion, and deletion operation, along with an indicator for the outcome of each edit operation. This last feature reflects the propensity of a particular phoneme to appear in a given language at all, no matter what its ancestral phoneme was. This parameterization is similar to the one used in the reconstruction system of Bouchard-Cˆot´e et al. (2009), except that they</context>
<context position="29298" citStr="Hall and Klein (2010)" startWordPosition="4883" endWordPosition="4886">ers in the International Phonetic Alphabet. A is used when generating new words either via innovation or from the root of the tree. In principle, we could of course have language models specific to each language, but because there Formosan System Prec Recall F1 Purity Agg. HK10 77.6 83.2 80.0 84.7 PARSIM 87.8 71.0 78.5 94.6 Combination 85.2 81.3 83.2 92.3 Oceanic System Prec Recall F1 Purity PARSIM 84.4 62.1 71.5 91.8 Combination 76.0 73.8 74.9 85.5 Table 1: Results on the Formosan and Oceanic families. PARSIM is the new parsimony model in this paper, Agg. HK10 is our agglomerative variant of Hall and Klein (2010) and Combination uses PARSIM’s output to seed the agglomerative matcher. For the agglomerative systems, we report the point with maximal F1 score, but we also show precision/recall curves. (See Figure 4.) are so few words per language, we found that branch-specific language models caused the model to prefer to innovate at almost every node since the language models could essentially memorize the relatively small vocabularies of these languages. 8 Experiments 8.1 Cognate Recovery We ran both PARSIM and our agglomerative version of HK10 on the Formosan datasets. For PARSIM, we initialized the mu</context>
</contexts>
<marker>Hall, Klein, 2010</marker>
<rawString>David Hall and Dan Klein. 2010. Finding cognates using phylogenies. In Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert M Keesing</author>
<author>Jonathan Fifi’i</author>
</authors>
<title>Kwaio word tabooing in its cultural context.</title>
<date>1969</date>
<journal>Journal of the Polynesian Society,</journal>
<volume>78</volume>
<issue>2</issue>
<marker>Keesing, Fifi’i, 1969</marker>
<rawString>Robert M. Keesing and Jonathan Fifi’i. 1969. Kwaio word tabooing in its cultural context. Journal of the Polynesian Society, 78(2):154–177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grzegorz Kondrak</author>
</authors>
<title>Identifying cognates by phonetic and semantic similarity.</title>
<date>2001</date>
<booktitle>In NAACL.</booktitle>
<marker>Kondrak, 2001</marker>
<rawString>Grzegorz Kondrak. 2001. Identifying cognates by phonetic and semantic similarity. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harold W Kuhn</author>
</authors>
<title>The Hungarian method for the assignment problem.</title>
<date>1955</date>
<journal>Naval Research Logistics Quarterly,</journal>
<pages>2--83</pages>
<contexts>
<context position="13361" citStr="Kuhn, 1955" startWordPosition="2196" endWordPosition="2197">ond, we use an agglomerative inference procedure, which greedily merges cognate groups that result in the greatest gain log p(waub|ϕ) − log p(wa|ϕ) − log p(wb|ϕ) This score is the difference between the log probability of generating two cognate groups jointly and generating them separately. We then merge the two that generate the highest gain in likelihood. Like the iterative bipartite matching algorithm described above, this algorithm is not exact. However, it is O(n2 log n) (where n is the size of the largest gloss, which for Oceanic is 153), while the bipartite matching algorithm is O(n3) (Kuhn, 1955). Actually, the original HK10 is doubly intractable. They use weighted automata to represent distributions over strings, but these automata—particularly if they are non-deterministic—make inference in any non-trivial graphical model intractable. We discuss this issue in more detail in Section 6. 3.2 A Parsimony-Inspired Model We now describe a new model called PARSIM that supports exact inference tractably, though it sacrifices some of the expressive power of HK10. In our model, each language has at most one word for each gloss, and this one word changes from one language to its children accor</context>
</contexts>
<marker>Kuhn, 1955</marker>
<rawString>Harold W. Kuhn. 1955. The Hungarian method for the assignment problem. Naval Research Logistics Quarterly, 2:83–97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John B Lowe</author>
<author>Martine Mazaudon</author>
</authors>
<title>The reconstruction engine: a computer implementation of the comparative method.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>3</issue>
<contexts>
<context position="1720" citStr="Lowe and Mazaudon, 1994" startWordPosition="254" endWordPosition="257">eveloped for large-scale reconstruction of phylogenies (Ringe et al., 2002; Daum´e III and Campbell, 2007; Daum´e III, 2009; Nerbonne, 2010) and ancestral word forms of known cognate sets (Oakes, 2000; Bouchard-Cˆot´e et al., 2007; Bouchard-Cˆot´e et al., 2009), enabling linguists to explore deep historical relationships in an automated fashion. However, computational approaches thus far have not been able to offer the same kind of scale for identifying cognates. Previous work in cognate identification has largely focused on identifying cognates in pairs of languages (Mann and Yarowsky, 2001; Lowe and Mazaudon, 1994; Oakes, 2000; Kondrak, 2001; Mulloni, 2007), with a few recent exceptions that can find sets in a handful of languages (Bergsma and Kondrak, 2007; Hall and Klein, 2010). While it may seem surprising that cognate detection has not successfully scaled to large numbers of languages, the task poses challenges not seen in reconstruction and phylogeny inference. For instance, morphological innovations and irregular sound changes can completely obscure relationships between words in different languages. However, in the case of reconstruction, an unexplainable word is simply that: one can still corre</context>
</contexts>
<marker>Lowe, Mazaudon, 1994</marker>
<rawString>John B. Lowe and Martine Mazaudon. 1994. The reconstruction engine: a computer implementation of the comparative method. Computational Linguistics, 20(3):381–417.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gideon S Mann</author>
<author>David Yarowsky</author>
</authors>
<title>Multipath translation lexicon induction via bridge languages.</title>
<date>2001</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="1695" citStr="Mann and Yarowsky, 2001" startWordPosition="250" endWordPosition="253">al approaches have been developed for large-scale reconstruction of phylogenies (Ringe et al., 2002; Daum´e III and Campbell, 2007; Daum´e III, 2009; Nerbonne, 2010) and ancestral word forms of known cognate sets (Oakes, 2000; Bouchard-Cˆot´e et al., 2007; Bouchard-Cˆot´e et al., 2009), enabling linguists to explore deep historical relationships in an automated fashion. However, computational approaches thus far have not been able to offer the same kind of scale for identifying cognates. Previous work in cognate identification has largely focused on identifying cognates in pairs of languages (Mann and Yarowsky, 2001; Lowe and Mazaudon, 1994; Oakes, 2000; Kondrak, 2001; Mulloni, 2007), with a few recent exceptions that can find sets in a handful of languages (Bergsma and Kondrak, 2007; Hall and Klein, 2010). While it may seem surprising that cognate detection has not successfully scaled to large numbers of languages, the task poses challenges not seen in reconstruction and phylogeny inference. For instance, morphological innovations and irregular sound changes can completely obscure relationships between words in different languages. However, in the case of reconstruction, an unexplainable word is simply </context>
</contexts>
<marker>Mann, Yarowsky, 2001</marker>
<rawString>Gideon S. Mann and David Yarowsky. 2001. Multipath translation lexicon induction via bridge languages. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
<author>Fernando Pereira</author>
<author>Michael Riley</author>
</authors>
<title>Weighted automata in text and speech processing.</title>
<date>1996</date>
<booktitle>In ECAI-96 Workshop.</booktitle>
<publisher>John Wiley and Sons.</publisher>
<contexts>
<context position="22576" citStr="Mohri et al., 1996" startWordPosition="3775" endWordPosition="3778">rocedures for these operations in 7.1. 6.2 Scale Even though inference by message-passing in our model is tractable, we needed to make certain concessions to make inference acceptably fast. These choices mainly affect how we represent distributions over strings. µ`�(w`) 349 First, we need to model distributions and mes- probabilities β`, and the global language model A sages over words on the internal nodes of a phy- for generating new words. We learn these paramelogeny. The natural choice in this scenario is to use ters via Expectation Maximization (Dempster et al., weighted finite automata (Mohri et al., 1996). Au- 1977), iterating between computing expected counts tomata have been used to successfully model distri- and adjusting parameters to maximize the posterior butions of strings for inferring morphology (Dreyer probability of the parameters. In this section, we deand Eisner, 2009) as well as cognate detection (Hall scribe those parameters. and Klein, 2010). Even in models that would be 7.1 Sound Laws tractable with “ordinary” messages, inference with The core piece of our system is learning the sound automata quickly becomes intractable, because the laws associated with each edge. Since the f</context>
</contexts>
<marker>Mohri, Pereira, Riley, 1996</marker>
<rawString>Mehryar Mohri, Fernando Pereira, and Michael Riley. 1996. Weighted automata in text and speech processing. In ECAI-96 Workshop. John Wiley and Sons.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Mulloni</author>
</authors>
<title>Automatic prediction of cognate orthography using support vector machines.</title>
<date>2007</date>
<booktitle>In ACL,</booktitle>
<pages>25--30</pages>
<contexts>
<context position="1764" citStr="Mulloni, 2007" startWordPosition="262" endWordPosition="263">s (Ringe et al., 2002; Daum´e III and Campbell, 2007; Daum´e III, 2009; Nerbonne, 2010) and ancestral word forms of known cognate sets (Oakes, 2000; Bouchard-Cˆot´e et al., 2007; Bouchard-Cˆot´e et al., 2009), enabling linguists to explore deep historical relationships in an automated fashion. However, computational approaches thus far have not been able to offer the same kind of scale for identifying cognates. Previous work in cognate identification has largely focused on identifying cognates in pairs of languages (Mann and Yarowsky, 2001; Lowe and Mazaudon, 1994; Oakes, 2000; Kondrak, 2001; Mulloni, 2007), with a few recent exceptions that can find sets in a handful of languages (Bergsma and Kondrak, 2007; Hall and Klein, 2010). While it may seem surprising that cognate detection has not successfully scaled to large numbers of languages, the task poses challenges not seen in reconstruction and phylogeny inference. For instance, morphological innovations and irregular sound changes can completely obscure relationships between words in different languages. However, in the case of reconstruction, an unexplainable word is simply that: one can still correctly reconstruct its ancestor using words fr</context>
</contexts>
<marker>Mulloni, 2007</marker>
<rawString>Andrea Mulloni. 2007. Automatic prediction of cognate orthography using support vector machines. In ACL, pages 25–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saul B Needleman</author>
<author>Christian D Wunsch</author>
</authors>
<title>A general method applicable to the search for similarities in the amino acid sequence of two proteins.</title>
<date>1970</date>
<journal>Journal of Molecular Biology,</journal>
<volume>48</volume>
<issue>3</issue>
<pages>453</pages>
<contexts>
<context position="25956" citStr="Needleman and Wunsch (1970)" startWordPosition="4330" endWordPosition="4333"> parameterization. We model ence that is still quadratic in the number of words the evolution of a word w` to its child w`, as a in a gloss, since we have distributions of the form sequence of unigram edits that include insertions, p(w`|wpar(`)Iφ`). Intuitively, it is unlikely that a deletions, and substitutions. Specifically, we use a word from one distant branch of tree resembles a standard three-state pair hidden Markov model that word in another branch. Therefore, rather than score is closely related to the classic alignment algorithm all of these unlikely words, we use a beam where we of Needleman and Wunsch (1970) (Durbin et al., only factor in words whose score is at most a fac- 2006). tor of a−10 less than the maximum score. Our initial The three states in this HMM correspond to experiments found that using a beam provides large matches/substitutions, insertions, and deletions. The savings in time with little impact on prediction qual- transitions are set up such that insertions and deleity. tions cannot be interleaved. This prevents spurious 7 Learning equivalent alignments, which would cause the model PARSIM has three kinds of parameters that we need to assign unnecessarily higher probability to tr</context>
</contexts>
<marker>Needleman, Wunsch, 1970</marker>
<rawString>Saul B. Needleman and Christian D. Wunsch. 1970. A general method applicable to the search for similarities in the amino acid sequence of two proteins. Journal of Molecular Biology, 48(3):443 – 453.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Nerbonne</author>
</authors>
<title>Measuring the diffusion of linguistic change.</title>
<date>2010</date>
<journal>Philosophical Transactions of the Royal Society B: Biological Sciences.</journal>
<contexts>
<context position="1237" citStr="Nerbonne, 2010" startWordPosition="180" endWordPosition="181">language is the recovery of related cognate words in its descendants. Unfortunately, this process has largely been a manual, linguisticallyintensive undertaking for any sizable number of descendant languages. The traditional approach used by linguists—the comparative method—iterates between positing putative cognates and then identifying regular sound laws that explain correspondences between those words (Bloomfield, 1938). Successful computational approaches have been developed for large-scale reconstruction of phylogenies (Ringe et al., 2002; Daum´e III and Campbell, 2007; Daum´e III, 2009; Nerbonne, 2010) and ancestral word forms of known cognate sets (Oakes, 2000; Bouchard-Cˆot´e et al., 2007; Bouchard-Cˆot´e et al., 2009), enabling linguists to explore deep historical relationships in an automated fashion. However, computational approaches thus far have not been able to offer the same kind of scale for identifying cognates. Previous work in cognate identification has largely focused on identifying cognates in pairs of languages (Mann and Yarowsky, 2001; Lowe and Mazaudon, 1994; Oakes, 2000; Kondrak, 2001; Mulloni, 2007), with a few recent exceptions that can find sets in a handful of languag</context>
</contexts>
<marker>Nerbonne, 2010</marker>
<rawString>John Nerbonne. 2010. Measuring the diffusion of linguistic change. Philosophical Transactions of the Royal Society B: Biological Sciences.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nichols</author>
</authors>
<title>Linguistic diversity in space and time.</title>
<date>1992</date>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="9876" citStr="Nichols (1992)" startWordPosition="1599" endWordPosition="1600">, words evolve along a phylogeny, where each word in a language is the result of that word evolving from its parent according to regular sound laws. To model the fact that not all languages have a cognate in each group, each language in the tree has an associated “survival” variable Sg,t, where a word may be lost on that branch (and its descendants) instead of evolving. Once the words are generated, they are then “permuted” so that the cognacy relationships 1Both of these models therefore are insensitive to geographic and historical factors that cannot be easily approximated by this tree. See Nichols (1992) for an excellent discussion of these factors. 2One could easily envision allowing the meaning of a word to change as well. Modeling this semantic drift has been considered by Kondrak (2001). In the ABVD, however, any semantic drift has already been elided, since the database has coarsened glosses to the extent that there is no meaningful way to model semantic drift given our data. Figure 1: Plate diagrams for (a) HK10 (Hall and Klein, 2010) and (b) PARSIM, our new parsimony model, for a small set of languages. In HK10, words are generated following a phylogenetic tree according to sound laws </context>
</contexts>
<marker>Nichols, 1992</marker>
<rawString>J. Nichols. 1992. Linguistic diversity in space and time. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael P Oakes</author>
</authors>
<title>Computer estimation of vocabulary in a protolanguage from word lists in four daughter languages.</title>
<date>2000</date>
<journal>Quantitative Linguistics,</journal>
<volume>7</volume>
<issue>3</issue>
<contexts>
<context position="1297" citStr="Oakes, 2000" startWordPosition="191" endWordPosition="192">ants. Unfortunately, this process has largely been a manual, linguisticallyintensive undertaking for any sizable number of descendant languages. The traditional approach used by linguists—the comparative method—iterates between positing putative cognates and then identifying regular sound laws that explain correspondences between those words (Bloomfield, 1938). Successful computational approaches have been developed for large-scale reconstruction of phylogenies (Ringe et al., 2002; Daum´e III and Campbell, 2007; Daum´e III, 2009; Nerbonne, 2010) and ancestral word forms of known cognate sets (Oakes, 2000; Bouchard-Cˆot´e et al., 2007; Bouchard-Cˆot´e et al., 2009), enabling linguists to explore deep historical relationships in an automated fashion. However, computational approaches thus far have not been able to offer the same kind of scale for identifying cognates. Previous work in cognate identification has largely focused on identifying cognates in pairs of languages (Mann and Yarowsky, 2001; Lowe and Mazaudon, 1994; Oakes, 2000; Kondrak, 2001; Mulloni, 2007), with a few recent exceptions that can find sets in a handful of languages (Bergsma and Kondrak, 2007; Hall and Klein, 2010). While </context>
</contexts>
<marker>Oakes, 2000</marker>
<rawString>Michael P. Oakes. 2000. Computer estimation of vocabulary in a protolanguage from word lists in four daughter languages. Quantitative Linguistics, 7(3):233–243.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Don Ringe</author>
<author>Tandy Warnow</author>
<author>Ann Taylor</author>
</authors>
<title>Indoeuropean and computational cladistics.</title>
<date>2002</date>
<journal>Transactions of the Philological Society,</journal>
<volume>100</volume>
<issue>1</issue>
<contexts>
<context position="1171" citStr="Ringe et al., 2002" startWordPosition="168" endWordPosition="171">oduction The critical first step in the reconstruction of an ancient language is the recovery of related cognate words in its descendants. Unfortunately, this process has largely been a manual, linguisticallyintensive undertaking for any sizable number of descendant languages. The traditional approach used by linguists—the comparative method—iterates between positing putative cognates and then identifying regular sound laws that explain correspondences between those words (Bloomfield, 1938). Successful computational approaches have been developed for large-scale reconstruction of phylogenies (Ringe et al., 2002; Daum´e III and Campbell, 2007; Daum´e III, 2009; Nerbonne, 2010) and ancestral word forms of known cognate sets (Oakes, 2000; Bouchard-Cˆot´e et al., 2007; Bouchard-Cˆot´e et al., 2009), enabling linguists to explore deep historical relationships in an automated fashion. However, computational approaches thus far have not been able to offer the same kind of scale for identifying cognates. Previous work in cognate identification has largely focused on identifying cognates in pairs of languages (Mann and Yarowsky, 2001; Lowe and Mazaudon, 1994; Oakes, 2000; Kondrak, 2001; Mulloni, 2007), with </context>
<context position="16770" citStr="Ringe et al., 2002" startWordPosition="2769" endWordPosition="2772">ny is scored according to the derivation that requires the fewest number of changes of state, where a state is typically thought of as a gene or some other trait in a species. These genes are typically called “characters” in the computational biology literature, and two species would have the same value for a character if they share the same property that that state represents. When inducing phylogenies of languages, a natural choice for characters are glosses from a restricted vocabulary like a Swadesh list, and two words are represented as the same value for a character if they are cognate (Ringe et al., 2002). Other features can be used (Daum´e III and Campbell, 2007; Daum´e III, 2009), but they are not relevant to our discussion. Consider the small example in Figure 3a with just four languages. Here, cognacy is encoded using characters. In this example, at least two changes of state are required to explain the data: both C and B must have evolved from A. Therefore, the parsimony score for this tree is two. Of course, there is no reason why all changes should be equally likely. For instance, it might be extremely likely that B changes into both A and C, but that A never changes into B or C, and so</context>
</contexts>
<marker>Ringe, Warnow, Taylor, 2002</marker>
<rawString>Don Ringe, Tandy Warnow, and Ann Taylor. 2002. Indoeuropean and computational cladistics. Transactions of the Philological Society, 100(1):59–129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Sankoff</author>
<author>R J Cedergren</author>
</authors>
<title>Simultaneuous comparison of three or more sequences related by a tree,</title>
<date>1983</date>
<pages>253--263</pages>
<publisher>Addison-Wesley,</publisher>
<location>Reading, MA.</location>
<contexts>
<context position="17450" citStr="Sankoff and Cedergren, 1983" startWordPosition="2891" endWordPosition="2894">l, 2007; Daum´e III, 2009), but they are not relevant to our discussion. Consider the small example in Figure 3a with just four languages. Here, cognacy is encoded using characters. In this example, at least two changes of state are required to explain the data: both C and B must have evolved from A. Therefore, the parsimony score for this tree is two. Of course, there is no reason why all changes should be equally likely. For instance, it might be extremely likely that B changes into both A and C, but that A never changes into B or C, and so weighted variants of parsimony might be necessary (Sankoff and Cedergren, 1983). With this in mind, PARSIM can be thought of a weighted variant of parsimony, with two differences. First, the characters do not indicate ahead of time which words are related. Instead, the characters are the words themselves. Second, the transitions between different states (words) are not uniform. Instead, they are weighted by the log probability of one word changing into another, including both mutations and innovations. Thus, the task of inference in PARSIM is to find the most “parsimonious” explanation for the words we have observed, which is the same as finding the most likely derivatio</context>
</contexts>
<marker>Sankoff, Cedergren, 1983</marker>
<rawString>D. Sankoff and R. J. Cedergren, 1983. Simultaneuous comparison of three or more sequences related by a tree, page 253–263. Addison-Wesley, Reading, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>August Schleicher</author>
</authors>
<date>1861</date>
<journal>A Compendium of the Comparative Grammar of the Indo-European, Sanskrit, Greek and Latin Languages.</journal>
<contexts>
<context position="23479" citStr="Schleicher, 1861" startWordPosition="3915" endWordPosition="3916">ell as cognate detection (Hall scribe those parameters. and Klein, 2010). Even in models that would be 7.1 Sound Laws tractable with “ordinary” messages, inference with The core piece of our system is learning the sound automata quickly becomes intractable, because the laws associated with each edge. Since the foundasize of the automata grow exponentially with the tion of historical linguists with the neogrammarinumber of messages passed. Therefore, approxima- ans, linguists have argued for the regularity of sound tions must be used. Dreyer and Eisner (2009) used change at the phonemic level (Schleicher, 1861; a mixture of a k-best list and a unigram language Bloomfield, 1938). That is to say, if in some lanmodel, while Hall and Klein (2010) used an approx- guage a /t/ changes to a /d/ in some word, it is alimation procedure that projected complex automata most certain that it will change in every other place to simple, tractable automata using a modified KL that has the same surrounding context. divergence. In practice, of course, sound change is not entirely While either approach could be used here in prin- regular, and complex extralinguistic events can lead ciple, we found that automata machin</context>
</contexts>
<marker>Schleicher, 1861</marker>
<rawString>August Schleicher. 1861. A Compendium of the Comparative Grammar of the Indo-European, Sanskrit, Greek and Latin Languages.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>