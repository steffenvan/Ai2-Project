<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000131">
<title confidence="0.999536">
Syntactic Kernels for Natural Language Learning:
the Semantic Role Labeling Case
</title>
<author confidence="0.998758">
Alessandro Moschitti
</author>
<affiliation confidence="0.9973005">
Department of Computer Science
University of Rome ”Tor Vergata”
</affiliation>
<address confidence="0.677894">
Rome, Italy
</address>
<email confidence="0.99376">
moschitti@info.uniroma2.it
</email>
<sectionHeader confidence="0.995549" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999035">
In this paper, we use tree kernels to exploit
deep syntactic parsing information for nat-
ural language applications. We study the
properties of different kernels and we pro-
vide algorithms for their computation in
linear average time. The experiments with
SVMs on the task of predicate argument
classification provide empirical data that
validates our methods.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997326380952381">
Recently, several tree kernels have been applied to
natural language learning, e.g. (Collins and Duffy,
2002; Zelenko et al., 2003; Cumby and Roth, 2003;
Culotta and Sorensen, 2004; Moschitti, 2004). De-
spite their promising results, three general objec-
tions against kernel methods are raised: (1) only a
subset of the dual space features are relevant, thus,
it may be possible to design features in the primal
space that produce the same accuracy with a faster
computation time; (2) in some cases the high num-
ber of features (substructures) of the dual space can
produce overfitting with a consequent accuracy de-
crease (Cumby and Roth, 2003); and (3) the compu-
tation time of kernel functions may be too high and
prevent their application in real scenarios.
In this paper, we study the impact of the sub-
tree (ST) (Vishwanathan and Smola, 2002), subset
tree (SST) (Collins and Duffy, 2002) and partial tree
(PT) kernels on Semantic Role Labeling (SRL). The
PT kernel is a new function that we have designed
to generate larger substructure spaces. Moreover,
</bodyText>
<page confidence="0.993612">
97
</page>
<bodyText confidence="0.999921368421053">
to solve the computation problems, we propose al-
gorithms which evaluate the above kernels in linear
average running time.
We experimented such kernels with Support Vec-
tor Machines (SVMs) on the classification of seman-
tic roles of PropBank (Kingsbury and Palmer, 2002)
and FrameNet (Fillmore, 1982) data sets. The re-
sults show that: (1) the kernel approach provides the
same accuracy of the manually designed features.
(2) The overfitting problem does not occur although
the richer space of PTs does not provide better ac-
curacy than the one based on SST. (3) The average
running time of our tree kernel computation is linear.
In the remainder of this paper, Section 2 intro-
duces the different tree kernel spaces. Section 3 de-
scribes the kernel functions and our fast algorithms
for their evaluation. Section 4 shows the compara-
tive performance in terms of execution time and ac-
curacy.
</bodyText>
<sectionHeader confidence="0.881679" genericHeader="method">
2 Tree kernel Spaces
</sectionHeader>
<bodyText confidence="0.999967571428572">
We consider three different tree kernel spaces: the
subtrees (STs), the subset trees (SSTs) and the novel
partial trees (PTs).
An ST of a tree is rooted in any node and includes
all its descendants. For example, Figure 1 shows the
parse tree of the sentence &amp;quot;Mary brought a cat&amp;quot;
together with its 6 STs. An SST is a more general
structure since its leaves can be associated with non-
terminal symbols. The SSTs satisfy the constraint
that grammatical rules cannot be broken. For exam-
ple, Figure 2 shows 10 SSTs out of 17 of the sub-
tree of Figure 1 rooted in VP. If we relax the non-
breaking rule constraint we obtain a more general
form of substructures, i.e. the PTs. For example,
</bodyText>
<note confidence="0.820393">
Proceedings of the Human Language TechnoloConference of the North American Chapter of the ACL, pages 97–100,
New York, June 2006. E2006Association for Computational Linguistics
</note>
<figure confidence="0.938968">
a
</figure>
<figureCaption confidence="0.88788">
Figure 2: A tree with some of its subset trees (SSTs).
</figureCaption>
<figure confidence="0.99782725">
VP
D
NP
a cat a cat a cat
</figure>
<figureCaption confidence="0.962274">
Figure 3: A tree with some of its partial trees (PTs).
</figureCaption>
<figure confidence="0.9893285">
V
V
...
a
a
D N N D
</figure>
<sectionHeader confidence="0.760363" genericHeader="method">
3 Fast Tree Kernel Functions
</sectionHeader>
<bodyText confidence="0.9998883">
The main idea of tree kernels is to compute the
number of common substructures between two trees
T1 and T2 without explicitly considering the whole
fragment space. We designed a general function
to compute the ST, SST and PT kernels. Our fast
algorithm is inspired by the efficient evaluation of
non-continuous subsequences (described in (Shawe-
Taylor and Cristianini, 2004)). To further increase
the computation speed, we also applied the pre-
selection of node pairs which have non-null kernel.
</bodyText>
<subsectionHeader confidence="0.993794">
3.1 Generalized Tree Kernel function
</subsectionHeader>
<bodyText confidence="0.99996625">
Given a tree fragment space F = ff1, f2,.., fF}, we
use the indicator function Ii(n) which is equal to 1 if
the target fi is rooted at node n and 0 otherwise. We
define the general kernel as:
</bodyText>
<equation confidence="0.99473">
K(T1,T2) =
</equation>
<bodyText confidence="0.9946245">
where NT1 and NT2 are the sets of nodes in T1 and
T2, respectively and O(n1, n2)= P|F|
</bodyText>
<equation confidence="0.857242">
i=1 Ii(n1)Ii(n2),
</equation>
<bodyText confidence="0.941638666666667">
i.e. the number of common fragments rooted at the
n1 and n2 nodes. We can compute it as follows:
- if the node labels of n1 and n2 are different then
</bodyText>
<equation confidence="0.769186833333333">
O(n1, n2) = 0;
- else:
98
XO(n1, n2) = 1 + =l( ~J2) l( ~J1) O(cn1[ ~J1i], cn2[ ~J2i])
~J1, ~J2,l( ~J1) Y
i=1
</equation>
<bodyText confidence="0.9795924">
where ~J1 = (J11, J12, J13, ..) and ~J2 = (J21, J22, J23, ..)
are index sequences associated with the ordered
child sequences cn1 of n1 and cn2 of n2, respectively,
~J1i and ~J2i point to the i-th children in the two se-
quences, and l(·) returns the sequence length. We
note that (1) Eq. 2 is a convolution kernel accord-
ing to the definition and the proof given in (Haus-
sler, 1999). (2) Such kernel generates a feature
space richer than those defined in (Vishwanathan
and Smola, 2002; Collins and Duffy, 2002; Zelenko
et al., 2003; Culotta and Sorensen, 2004; Shawe-
Taylor and Cristianini, 2004). Additionally, we add
the decay factor as follows: O(n1, n2) =
JO(cn1[ ~J1i], cn2[ ~J2i])
where d(~J1) = ~J1l( ~J1) − ~J11 and d(~J2) = ~J2l( ~J2) − ~J21.
In this way, we penalize subtrees built on child
subsequences that contain gaps. Moreover, to
have a similarity score between 0 and 1, we also
apply the normalization in the kernel space, i.e.
K0(T1, T2) = ✓K(T1,T1)×K(T2,T2). As the summation
</bodyText>
<equation confidence="0.646546">
K(T1,T2)
</equation>
<bodyText confidence="0.996027333333333">
in Eq. 3 can be distributed with respect to different
types of sequences, e.g. those composed by p
children, it follows that
</bodyText>
<equation confidence="0.911911">
O(n1, n2) = µ�λ2 + Plm p=1 Op(n1,n2)�, (4)
</equation>
<bodyText confidence="0.968678666666667">
where Op evaluates the number of common subtrees
rooted in subsequences of exactly p children (of n1
and n2) and lm = minfl(cn1), l(cn2)}. Note also that if
we consider only the contribution of the longest se-
quence of node pairs that have the same children, we
implement the SST kernel. For the STs computation
we need also to remove the A2 term from Eq. 4.
Given the two child sequences c1a = cn1 and
c2b = cn2 (a and b are the last children), Op(c1a, c2b) =
</bodyText>
<equation confidence="0.999617">
O(a, b) X |c1 ||c2 |X OP_1 (c1 [1 : 2], c2[1 : r]),
X X λ|c1|−i+|c2|−r
i=1 r=1
</equation>
<bodyText confidence="0.9996805">
where c1[1 : i] and c2[1 : r] are the child subse-
quences from 1 to i and from 1 to r of c1 and c2. If
we name the double summation term as Dp, we can
rewrite the relation as:
</bodyText>
<figure confidence="0.97815276">
cat
Mary
brought
V
NP
D N
brought
NP
D N
Figure 3 shows 10 out of the total 30 PTs, derived
from the same tree as before.
VP
S
a cat
D
N
V
N
N
VP
NP
Mary
brought
D N
a cat
</figure>
<figureCaption confidence="0.940448">
Figure 1: A syntactic parse tree with its subtrees (STs).
</figureCaption>
<figure confidence="0.98884237037037">
cat
a
VP
V
NP
D N
a cat
VP
V
NP
D N
a cat
VP
NP
D N
NP
D N
NP
D N
a cat
N
NP
D N
V
brought
NP
D N
cat
a cat
N
Mary ...
V
D
brought
V
VP VP VP VP VP VP VP
brought
NP
D N
NP
D N
NP
D N
NP
D N
NP
a
D
NP
NP
N
NP
NP
NP
</figure>
<equation confidence="0.893300272727273">
X X O(n1, n2), (1)
n1∈NT1 n2∈NT2
� X λd(~J1)+d(~J2) l(~J1)
µ λ2+ Y
~J1, ~J2,l( ~J1)=l( ~J2) i=1
Ap(c1a, c2b) A(a, b)Dp(|c1|, |c2|) if a = b;
_
0 otherwise.
Note that Dp satisfies the recursive relation:
Dp(k, l) = Ap_1(s[1 : k], t[1 : l]) + ADp(k, l — 1)
+ADp(k — 1, l) + A2Dp(k — 1, l — 1).
</equation>
<bodyText confidence="0.999814">
By means of the above relation, we can compute
the child subsequences of two sets c1 and c2 in
O(p|c1||c2|). This means that the worst case com-
plexity of the PT kernel is O(pp2|NT1||NT2|), where
p is the maximum branching factor of the two trees.
Note that the average p in natural language parse
trees is very small and the overall complexity can be
reduced by avoiding the computation of node pairs
with different labels. The next section shows our fast
algorithm to find non-null node pairs.
</bodyText>
<subsectionHeader confidence="0.999344">
3.2 Fast non-null node pair computation
</subsectionHeader>
<bodyText confidence="0.999899">
To compute the kernels defined in the previous sec-
tion, we sum the A function for each pair (n1, n2)E
NT1 x NT2 (Eq. 1). When the labels associated
with n1 and n2 are different, we can avoid evaluating
A(n1, n2) since it is 0. Thus, we look for a node pair
set Np ={(n1, n2)E NT1 x NT2 : label(n1) = label(n2)}.
To efficiently build Np, we (i) extract the L1 and
L2 lists of nodes from T1 and T2, (ii) sort them in
alphanumeric order and (iii) scan them to find Np.
Step (iii) may require only O ( |NT1  |+  |NT2|) time, but,
if label(n1) appears r1 times in T1 and label(n2) is re-
peated r2 times in T2, we need to consider r1 x r2
pairs. The formal can be found in (Moschitti, 2006).
</bodyText>
<sectionHeader confidence="0.980157" genericHeader="method">
4 The Experiments
</sectionHeader>
<bodyText confidence="0.992583666666667">
In these experiments, we study tree kernel perfor-
mance in terms of average running time and accu-
racy on the classification of predicate arguments. As
shown in (Moschitti, 2004), we can label seman-
tic roles by classifying the smallest subtree that in-
cludes the predicate with one of its arguments, i.e.
the so called PAF structure.
The experiments were carried out with
the SVM-light-TK software available at
</bodyText>
<footnote confidence="0.393003">
http://ai-nlp.info.uniroma2.it/moschitti/
</footnote>
<bodyText confidence="0.9989687">
which encodes the fast tree kernels in the SVM-light
software (Joachims, 1999). The multiclassifiers
were obtained by training an SVM for each class
in the ONE-vs.-ALL fashion. In the testing phase,
we selected the class associated with the maximum
SVM score.
For the ST, SST and PT kernels, we found that the
best A values (see Section 3) on the development set
were 1, 0.4 and 0.8, respectively, whereas the best p
was 0.4.
</bodyText>
<subsectionHeader confidence="0.997562">
4.1 Kernel running time experiments
</subsectionHeader>
<bodyText confidence="0.9984191">
To study the FTK running time, we extracted from
the Penn Treebank several samples of 500 trees con-
taining exactly n nodes. Each point of Figure 4
shows the average computation time1 of the kernel
function applied to the 250,000 pairs of trees of size
n. It clearly appears that the FTK-SST and FTK-PT
(i.e. FTK applied to the SST and PT kernels) av-
erage running time has linear behavior whereas, as
expected, the naive SST algorithm shows a quadratic
curve.
</bodyText>
<figure confidence="0.884839">
5 10 15 20 25 30 35 40 45 50 55
Number of Tree Nodes
</figure>
<figureCaption confidence="0.995516">
Figure 4: Average time in µseconds for the naive SST kernel,
FTK-SST and FTK-PT evaluations.
</figureCaption>
<subsectionHeader confidence="0.990779">
4.2 Experiments on SRL dataset
</subsectionHeader>
<bodyText confidence="0.989928571428571">
We used two different corpora: PropBank
(www.cis.upenn.edu/—ace) along with Penn
Treebank 2 (Marcus et al., 1993) and FrameNet.
PropBank contains about 53,700 sentences and
a fixed split between training and testing used in
other researches. In this split, sections from 02 to
21 are used for training, section 23 for testing and
section 22 as development set. We considered a
total of 122,774 and 7,359 arguments (from Arg0
to Argy, ArgA and ArgAl) in training and testing,
respectively. The tree structures were extracted
from the Penn Treebank.
From the FrameNet corpus (www.icsi.
berkeley.edu/—framenet) we extracted all
</bodyText>
<footnote confidence="0.951007">
1We run the experiments on a Pentium 4, 2GHz, with 1 Gb
ram.
</footnote>
<figure confidence="0.956670416666667">
120
100
80
60
40
20
0
FTK-SST
naive-SST
FTK-PT
99
0 10 20 30 40 50 60 70 80 90 100
</figure>
<subsectionHeader confidence="0.516113">
% Training Data
</subsectionHeader>
<bodyText confidence="0.99509">
manually designed features by 2/3 percent points,
thus they can be seen as a useful tactic to boost sys-
tem accuracy.
</bodyText>
<note confidence="0.530222">
Args Linear ST SST PT
Acc. 87.6 84.6 87.7 86.7
</note>
<tableCaption confidence="0.963461">
Table 1: Evaluation of kernels on PropBank data and gold
parse trees.
</tableCaption>
<figure confidence="0.99916325">
0.88
0.85
0.83
0.80
0.78
0.75
ST SST
Linear PT
</figure>
<figureCaption confidence="0.998405">
Figure 5: Multiclassifier accuracy according to different train- Roles Linear ST SST PT
ing set percentage. Acc. 82.3 80.0 81.2 79.9
</figureCaption>
<bodyText confidence="0.999207722222222">
24,558 sentences of the 40 Frames selected for
the Automatic Labeling of Semantic Roles task of
Senseval 3 (www.senseval.org). We considered
the 18 most frequent roles, for a total of 37,948
examples (30% of the sentences for testing and
70% for training/validation). The sentences were
processed with the Collins’ parser (Collins, 1997)
to generate automatic parse trees.
We run ST, SST and PT kernels along with
the linear kernel of standard features (Carreras and
M`arquez, 2005) on PropBank training sets of dif-
ferent size. Figure 5 illustrates the learning curves
associated with the above kernels for the SVM mul-
ticlassifiers.
The tables 1 and 2 report the results, using all
available training data, on PropBank and FrameNet
test sets, respectively. We note that: (1) the accu-
racy of PTs is almost equal to the one produced by
SSTs as the PT space is a hyperset of SSTs. The
small difference is due to the poor relevance of the
substructures in the PT − SST set, which degrade
the PT space. (2) The high F1 measures of tree ker-
nels on FrameNet suggest that they are robust with
respect to automatic parse trees.
Moreover, the learning time of SVMs using FTK
for the classification of one large argument (Arg 0)
is much lower than the one required by naive algo-
rithm. With all the training data FTK terminated in
6 hours whereas the naive approach required more
than 1 week. However, the complexity burden of
working in the dual space can be alleviated with re-
cent approaches proposed in (Kudo and Matsumoto,
2003; Suzuki et al., 2004).
Finally, we carried out some experiments with the
combination between linear and tree kernels and we
found that tree kernels improve the models based on
</bodyText>
<tableCaption confidence="0.9967095">
Table 2: Evaluation of kernels on FrameNet data encoded in
automatic parse trees.
</tableCaption>
<sectionHeader confidence="0.997398" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99987805">
Xavier Carreras and Lluis M`arquez. 2005. Introduction to the
CoNLL-2005 shared task: Semantic role labeling. In Pro-
ceedings of CoNLL05.
Michael Collins and Nigel Duffy. 2002. New ranking algo-
rithms for parsing and tagging: Kernels over discrete struc-
tures, and the voted perceptron. In ACL02.
Michael Collins. 1997. Three generative, lexicalized models
for statistical parsing. In Proceedings of the ACL97.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency tree
kernels for relation extraction. In Proceedings ofACL04.
Chad Cumby and Dan Roth. 2003. Kernel methods for rela-
tional learning. In Proceedings ofICML03.
Charles J. Fillmore. 1982. Frame semantics. In Linguistics in
the Morning Calm.
D. Haussler. 1999. Convolution kernels on discrete struc-
tures. Technical report ucs-crl-99-10, University of Califor-
nia Santa Cruz.
T. Joachims. 1999. Making large-scale SVM learning practical.
In B. Scholkopf, C. Burges, and A. Smola, editors, Advances
in Kernel Methods - Support Vector Learning.
Paul Kingsbury and Martha Palmer. 2002. From Treebank to
PropBank. In Proceedings ofLREC02.
Taku Kudo and Yuji Matsumoto. 2003. Fast methods for
kernel-based text analysis. In Proceedings ofACL03.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993.
Building a large annotated corpus of english: The Penn Tree-
bank. Computational Linguistics.
Alessandro Moschitti. 2004. A study on convolution kernels
for shallow semantic parsing. In proceedings ofACL04.
Alessandro Moschitti. 2006. Making tree kernels practical for
natural language learning. In Proceedings of EACL06.
John Shawe-Taylor and Nello Cristianini. 2004. Kernel Meth-
ods for Pattern Analysis. Cambridge University Press.
Jun Suzuki, Hideki Isozaki, and Eisaku Maeda. 2004. Con-
volution kernels with feature selection for natural language
processing tasks. In Proceedings ofACL04.
S.V.N. Vishwanathan and A.J. Smola. 2002. Fast kernels on
strings and trees. In Proceedings ofNIPS02.
D. Zelenko, C. Aone, and A. Richardella. 2003. Kernel meth-
ods for relation extraction. JMLR.
</reference>
<page confidence="0.990742">
100
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.643195">
<title confidence="0.996488">Syntactic Kernels for Natural Language the Semantic Role Labeling Case</title>
<author confidence="0.998022">Alessandro</author>
<affiliation confidence="0.999148">Department of Computer University of Rome ”Tor</affiliation>
<address confidence="0.65428">Rome,</address>
<email confidence="0.995642">moschitti@info.uniroma2.it</email>
<abstract confidence="0.9996652">In this paper, we use tree kernels to exploit deep syntactic parsing information for natural language applications. We study the properties of different kernels and we provide algorithms for their computation in linear average time. The experiments with SVMs on the task of predicate argument classification provide empirical data that validates our methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Lluis M`arquez</author>
</authors>
<title>Introduction to the CoNLL-2005 shared task: Semantic role labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of CoNLL05.</booktitle>
<marker>Carreras, M`arquez, 2005</marker>
<rawString>Xavier Carreras and Lluis M`arquez. 2005. Introduction to the CoNLL-2005 shared task: Semantic role labeling. In Proceedings of CoNLL05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron.</title>
<date>2002</date>
<booktitle>In ACL02.</booktitle>
<contexts>
<context position="696" citStr="Collins and Duffy, 2002" startWordPosition="95" endWordPosition="98">ng Case Alessandro Moschitti Department of Computer Science University of Rome ”Tor Vergata” Rome, Italy moschitti@info.uniroma2.it Abstract In this paper, we use tree kernels to exploit deep syntactic parsing information for natural language applications. We study the properties of different kernels and we provide algorithms for their computation in linear average time. The experiments with SVMs on the task of predicate argument classification provide empirical data that validates our methods. 1 Introduction Recently, several tree kernels have been applied to natural language learning, e.g. (Collins and Duffy, 2002; Zelenko et al., 2003; Cumby and Roth, 2003; Culotta and Sorensen, 2004; Moschitti, 2004). Despite their promising results, three general objections against kernel methods are raised: (1) only a subset of the dual space features are relevant, thus, it may be possible to design features in the primal space that produce the same accuracy with a faster computation time; (2) in some cases the high number of features (substructures) of the dual space can produce overfitting with a consequent accuracy decrease (Cumby and Roth, 2003); and (3) the computation time of kernel functions may be too high </context>
<context position="5199" citStr="Collins and Duffy, 2002" startWordPosition="894" endWordPosition="897">t then O(n1, n2) = 0; - else: 98 XO(n1, n2) = 1 + =l( ~J2) l( ~J1) O(cn1[ ~J1i], cn2[ ~J2i]) ~J1, ~J2,l( ~J1) Y i=1 where ~J1 = (J11, J12, J13, ..) and ~J2 = (J21, J22, J23, ..) are index sequences associated with the ordered child sequences cn1 of n1 and cn2 of n2, respectively, ~J1i and ~J2i point to the i-th children in the two sequences, and l(·) returns the sequence length. We note that (1) Eq. 2 is a convolution kernel according to the definition and the proof given in (Haussler, 1999). (2) Such kernel generates a feature space richer than those defined in (Vishwanathan and Smola, 2002; Collins and Duffy, 2002; Zelenko et al., 2003; Culotta and Sorensen, 2004; ShaweTaylor and Cristianini, 2004). Additionally, we add the decay factor as follows: O(n1, n2) = JO(cn1[ ~J1i], cn2[ ~J2i]) where d(~J1) = ~J1l( ~J1) − ~J11 and d(~J2) = ~J2l( ~J2) − ~J21. In this way, we penalize subtrees built on child subsequences that contain gaps. Moreover, to have a similarity score between 0 and 1, we also apply the normalization in the kernel space, i.e. K0(T1, T2) = ✓K(T1,T1)×K(T2,T2). As the summation K(T1,T2) in Eq. 3 can be distributed with respect to different types of sequences, e.g. those composed by p childre</context>
</contexts>
<marker>Collins, Duffy, 2002</marker>
<rawString>Michael Collins and Nigel Duffy. 2002. New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron. In ACL02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalized models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the ACL97.</booktitle>
<contexts>
<context position="11631" citStr="Collins, 1997" startWordPosition="2109" endWordPosition="2110">T Acc. 87.6 84.6 87.7 86.7 Table 1: Evaluation of kernels on PropBank data and gold parse trees. 0.88 0.85 0.83 0.80 0.78 0.75 ST SST Linear PT Figure 5: Multiclassifier accuracy according to different train- Roles Linear ST SST PT ing set percentage. Acc. 82.3 80.0 81.2 79.9 24,558 sentences of the 40 Frames selected for the Automatic Labeling of Semantic Roles task of Senseval 3 (www.senseval.org). We considered the 18 most frequent roles, for a total of 37,948 examples (30% of the sentences for testing and 70% for training/validation). The sentences were processed with the Collins’ parser (Collins, 1997) to generate automatic parse trees. We run ST, SST and PT kernels along with the linear kernel of standard features (Carreras and M`arquez, 2005) on PropBank training sets of different size. Figure 5 illustrates the learning curves associated with the above kernels for the SVM multiclassifiers. The tables 1 and 2 report the results, using all available training data, on PropBank and FrameNet test sets, respectively. We note that: (1) the accuracy of PTs is almost equal to the one produced by SSTs as the PT space is a hyperset of SSTs. The small difference is due to the poor relevance of the su</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three generative, lexicalized models for statistical parsing. In Proceedings of the ACL97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Jeffrey Sorensen</author>
</authors>
<title>Dependency tree kernels for relation extraction.</title>
<date>2004</date>
<booktitle>In Proceedings ofACL04.</booktitle>
<contexts>
<context position="768" citStr="Culotta and Sorensen, 2004" startWordPosition="107" endWordPosition="110">y of Rome ”Tor Vergata” Rome, Italy moschitti@info.uniroma2.it Abstract In this paper, we use tree kernels to exploit deep syntactic parsing information for natural language applications. We study the properties of different kernels and we provide algorithms for their computation in linear average time. The experiments with SVMs on the task of predicate argument classification provide empirical data that validates our methods. 1 Introduction Recently, several tree kernels have been applied to natural language learning, e.g. (Collins and Duffy, 2002; Zelenko et al., 2003; Cumby and Roth, 2003; Culotta and Sorensen, 2004; Moschitti, 2004). Despite their promising results, three general objections against kernel methods are raised: (1) only a subset of the dual space features are relevant, thus, it may be possible to design features in the primal space that produce the same accuracy with a faster computation time; (2) in some cases the high number of features (substructures) of the dual space can produce overfitting with a consequent accuracy decrease (Cumby and Roth, 2003); and (3) the computation time of kernel functions may be too high and prevent their application in real scenarios. In this paper, we study</context>
<context position="5249" citStr="Culotta and Sorensen, 2004" startWordPosition="902" endWordPosition="905"> + =l( ~J2) l( ~J1) O(cn1[ ~J1i], cn2[ ~J2i]) ~J1, ~J2,l( ~J1) Y i=1 where ~J1 = (J11, J12, J13, ..) and ~J2 = (J21, J22, J23, ..) are index sequences associated with the ordered child sequences cn1 of n1 and cn2 of n2, respectively, ~J1i and ~J2i point to the i-th children in the two sequences, and l(·) returns the sequence length. We note that (1) Eq. 2 is a convolution kernel according to the definition and the proof given in (Haussler, 1999). (2) Such kernel generates a feature space richer than those defined in (Vishwanathan and Smola, 2002; Collins and Duffy, 2002; Zelenko et al., 2003; Culotta and Sorensen, 2004; ShaweTaylor and Cristianini, 2004). Additionally, we add the decay factor as follows: O(n1, n2) = JO(cn1[ ~J1i], cn2[ ~J2i]) where d(~J1) = ~J1l( ~J1) − ~J11 and d(~J2) = ~J2l( ~J2) − ~J21. In this way, we penalize subtrees built on child subsequences that contain gaps. Moreover, to have a similarity score between 0 and 1, we also apply the normalization in the kernel space, i.e. K0(T1, T2) = ✓K(T1,T1)×K(T2,T2). As the summation K(T1,T2) in Eq. 3 can be distributed with respect to different types of sequences, e.g. those composed by p children, it follows that O(n1, n2) = µ�λ2 + Plm p=1 Op(n</context>
</contexts>
<marker>Culotta, Sorensen, 2004</marker>
<rawString>Aron Culotta and Jeffrey Sorensen. 2004. Dependency tree kernels for relation extraction. In Proceedings ofACL04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chad Cumby</author>
<author>Dan Roth</author>
</authors>
<title>Kernel methods for relational learning.</title>
<date>2003</date>
<booktitle>In Proceedings ofICML03.</booktitle>
<contexts>
<context position="740" citStr="Cumby and Roth, 2003" startWordPosition="103" endWordPosition="106">uter Science University of Rome ”Tor Vergata” Rome, Italy moschitti@info.uniroma2.it Abstract In this paper, we use tree kernels to exploit deep syntactic parsing information for natural language applications. We study the properties of different kernels and we provide algorithms for their computation in linear average time. The experiments with SVMs on the task of predicate argument classification provide empirical data that validates our methods. 1 Introduction Recently, several tree kernels have been applied to natural language learning, e.g. (Collins and Duffy, 2002; Zelenko et al., 2003; Cumby and Roth, 2003; Culotta and Sorensen, 2004; Moschitti, 2004). Despite their promising results, three general objections against kernel methods are raised: (1) only a subset of the dual space features are relevant, thus, it may be possible to design features in the primal space that produce the same accuracy with a faster computation time; (2) in some cases the high number of features (substructures) of the dual space can produce overfitting with a consequent accuracy decrease (Cumby and Roth, 2003); and (3) the computation time of kernel functions may be too high and prevent their application in real scenar</context>
</contexts>
<marker>Cumby, Roth, 2003</marker>
<rawString>Chad Cumby and Dan Roth. 2003. Kernel methods for relational learning. In Proceedings ofICML03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles J Fillmore</author>
</authors>
<title>Frame semantics.</title>
<date>1982</date>
<booktitle>In Linguistics in the Morning Calm.</booktitle>
<contexts>
<context position="1943" citStr="Fillmore, 1982" startWordPosition="303" endWordPosition="304"> real scenarios. In this paper, we study the impact of the subtree (ST) (Vishwanathan and Smola, 2002), subset tree (SST) (Collins and Duffy, 2002) and partial tree (PT) kernels on Semantic Role Labeling (SRL). The PT kernel is a new function that we have designed to generate larger substructure spaces. Moreover, 97 to solve the computation problems, we propose algorithms which evaluate the above kernels in linear average running time. We experimented such kernels with Support Vector Machines (SVMs) on the classification of semantic roles of PropBank (Kingsbury and Palmer, 2002) and FrameNet (Fillmore, 1982) data sets. The results show that: (1) the kernel approach provides the same accuracy of the manually designed features. (2) The overfitting problem does not occur although the richer space of PTs does not provide better accuracy than the one based on SST. (3) The average running time of our tree kernel computation is linear. In the remainder of this paper, Section 2 introduces the different tree kernel spaces. Section 3 describes the kernel functions and our fast algorithms for their evaluation. Section 4 shows the comparative performance in terms of execution time and accuracy. 2 Tree kernel</context>
</contexts>
<marker>Fillmore, 1982</marker>
<rawString>Charles J. Fillmore. 1982. Frame semantics. In Linguistics in the Morning Calm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Haussler</author>
</authors>
<title>Convolution kernels on discrete structures.</title>
<date>1999</date>
<tech>Technical report ucs-crl-99-10,</tech>
<institution>University of California Santa Cruz.</institution>
<contexts>
<context position="5072" citStr="Haussler, 1999" startWordPosition="875" endWordPosition="877">n fragments rooted at the n1 and n2 nodes. We can compute it as follows: - if the node labels of n1 and n2 are different then O(n1, n2) = 0; - else: 98 XO(n1, n2) = 1 + =l( ~J2) l( ~J1) O(cn1[ ~J1i], cn2[ ~J2i]) ~J1, ~J2,l( ~J1) Y i=1 where ~J1 = (J11, J12, J13, ..) and ~J2 = (J21, J22, J23, ..) are index sequences associated with the ordered child sequences cn1 of n1 and cn2 of n2, respectively, ~J1i and ~J2i point to the i-th children in the two sequences, and l(·) returns the sequence length. We note that (1) Eq. 2 is a convolution kernel according to the definition and the proof given in (Haussler, 1999). (2) Such kernel generates a feature space richer than those defined in (Vishwanathan and Smola, 2002; Collins and Duffy, 2002; Zelenko et al., 2003; Culotta and Sorensen, 2004; ShaweTaylor and Cristianini, 2004). Additionally, we add the decay factor as follows: O(n1, n2) = JO(cn1[ ~J1i], cn2[ ~J2i]) where d(~J1) = ~J1l( ~J1) − ~J11 and d(~J2) = ~J2l( ~J2) − ~J21. In this way, we penalize subtrees built on child subsequences that contain gaps. Moreover, to have a similarity score between 0 and 1, we also apply the normalization in the kernel space, i.e. K0(T1, T2) = ✓K(T1,T1)×K(T2,T2). As th</context>
</contexts>
<marker>Haussler, 1999</marker>
<rawString>D. Haussler. 1999. Convolution kernels on discrete structures. Technical report ucs-crl-99-10, University of California Santa Cruz.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Making large-scale SVM learning practical.</title>
<date>1999</date>
<booktitle>Advances in Kernel Methods - Support Vector Learning.</booktitle>
<editor>In B. Scholkopf, C. Burges, and A. Smola, editors,</editor>
<contexts>
<context position="9075" citStr="Joachims, 1999" startWordPosition="1666" endWordPosition="1667">onsider r1 x r2 pairs. The formal can be found in (Moschitti, 2006). 4 The Experiments In these experiments, we study tree kernel performance in terms of average running time and accuracy on the classification of predicate arguments. As shown in (Moschitti, 2004), we can label semantic roles by classifying the smallest subtree that includes the predicate with one of its arguments, i.e. the so called PAF structure. The experiments were carried out with the SVM-light-TK software available at http://ai-nlp.info.uniroma2.it/moschitti/ which encodes the fast tree kernels in the SVM-light software (Joachims, 1999). The multiclassifiers were obtained by training an SVM for each class in the ONE-vs.-ALL fashion. In the testing phase, we selected the class associated with the maximum SVM score. For the ST, SST and PT kernels, we found that the best A values (see Section 3) on the development set were 1, 0.4 and 0.8, respectively, whereas the best p was 0.4. 4.1 Kernel running time experiments To study the FTK running time, we extracted from the Penn Treebank several samples of 500 trees containing exactly n nodes. Each point of Figure 4 shows the average computation time1 of the kernel function applied to</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>T. Joachims. 1999. Making large-scale SVM learning practical. In B. Scholkopf, C. Burges, and A. Smola, editors, Advances in Kernel Methods - Support Vector Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Kingsbury</author>
<author>Martha Palmer</author>
</authors>
<title>From Treebank to PropBank.</title>
<date>2002</date>
<booktitle>In Proceedings ofLREC02.</booktitle>
<contexts>
<context position="1913" citStr="Kingsbury and Palmer, 2002" startWordPosition="297" endWordPosition="300"> too high and prevent their application in real scenarios. In this paper, we study the impact of the subtree (ST) (Vishwanathan and Smola, 2002), subset tree (SST) (Collins and Duffy, 2002) and partial tree (PT) kernels on Semantic Role Labeling (SRL). The PT kernel is a new function that we have designed to generate larger substructure spaces. Moreover, 97 to solve the computation problems, we propose algorithms which evaluate the above kernels in linear average running time. We experimented such kernels with Support Vector Machines (SVMs) on the classification of semantic roles of PropBank (Kingsbury and Palmer, 2002) and FrameNet (Fillmore, 1982) data sets. The results show that: (1) the kernel approach provides the same accuracy of the manually designed features. (2) The overfitting problem does not occur although the richer space of PTs does not provide better accuracy than the one based on SST. (3) The average running time of our tree kernel computation is linear. In the remainder of this paper, Section 2 introduces the different tree kernel spaces. Section 3 describes the kernel functions and our fast algorithms for their evaluation. Section 4 shows the comparative performance in terms of execution ti</context>
</contexts>
<marker>Kingsbury, Palmer, 2002</marker>
<rawString>Paul Kingsbury and Martha Palmer. 2002. From Treebank to PropBank. In Proceedings ofLREC02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Fast methods for kernel-based text analysis.</title>
<date>2003</date>
<booktitle>In Proceedings ofACL03.</booktitle>
<marker>Kudo, Matsumoto, 2003</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2003. Fast methods for kernel-based text analysis. In Proceedings ofACL03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of english: The Penn Treebank. Computational Linguistics.</title>
<date>1993</date>
<contexts>
<context position="10208" citStr="Marcus et al., 1993" startWordPosition="1863" endWordPosition="1866">point of Figure 4 shows the average computation time1 of the kernel function applied to the 250,000 pairs of trees of size n. It clearly appears that the FTK-SST and FTK-PT (i.e. FTK applied to the SST and PT kernels) average running time has linear behavior whereas, as expected, the naive SST algorithm shows a quadratic curve. 5 10 15 20 25 30 35 40 45 50 55 Number of Tree Nodes Figure 4: Average time in µseconds for the naive SST kernel, FTK-SST and FTK-PT evaluations. 4.2 Experiments on SRL dataset We used two different corpora: PropBank (www.cis.upenn.edu/—ace) along with Penn Treebank 2 (Marcus et al., 1993) and FrameNet. PropBank contains about 53,700 sentences and a fixed split between training and testing used in other researches. In this split, sections from 02 to 21 are used for training, section 23 for testing and section 22 as development set. We considered a total of 122,774 and 7,359 arguments (from Arg0 to Argy, ArgA and ArgAl) in training and testing, respectively. The tree structures were extracted from the Penn Treebank. From the FrameNet corpus (www.icsi. berkeley.edu/—framenet) we extracted all 1We run the experiments on a Pentium 4, 2GHz, with 1 Gb ram. 120 100 80 60 40 20 0 FTK-S</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993. Building a large annotated corpus of english: The Penn Treebank. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>A study on convolution kernels for shallow semantic parsing.</title>
<date>2004</date>
<booktitle>In proceedings ofACL04.</booktitle>
<contexts>
<context position="786" citStr="Moschitti, 2004" startWordPosition="111" endWordPosition="112">, Italy moschitti@info.uniroma2.it Abstract In this paper, we use tree kernels to exploit deep syntactic parsing information for natural language applications. We study the properties of different kernels and we provide algorithms for their computation in linear average time. The experiments with SVMs on the task of predicate argument classification provide empirical data that validates our methods. 1 Introduction Recently, several tree kernels have been applied to natural language learning, e.g. (Collins and Duffy, 2002; Zelenko et al., 2003; Cumby and Roth, 2003; Culotta and Sorensen, 2004; Moschitti, 2004). Despite their promising results, three general objections against kernel methods are raised: (1) only a subset of the dual space features are relevant, thus, it may be possible to design features in the primal space that produce the same accuracy with a faster computation time; (2) in some cases the high number of features (substructures) of the dual space can produce overfitting with a consequent accuracy decrease (Cumby and Roth, 2003); and (3) the computation time of kernel functions may be too high and prevent their application in real scenarios. In this paper, we study the impact of the</context>
<context position="8723" citStr="Moschitti, 2004" startWordPosition="1615" endWordPosition="1616">Np ={(n1, n2)E NT1 x NT2 : label(n1) = label(n2)}. To efficiently build Np, we (i) extract the L1 and L2 lists of nodes from T1 and T2, (ii) sort them in alphanumeric order and (iii) scan them to find Np. Step (iii) may require only O ( |NT1 |+ |NT2|) time, but, if label(n1) appears r1 times in T1 and label(n2) is repeated r2 times in T2, we need to consider r1 x r2 pairs. The formal can be found in (Moschitti, 2006). 4 The Experiments In these experiments, we study tree kernel performance in terms of average running time and accuracy on the classification of predicate arguments. As shown in (Moschitti, 2004), we can label semantic roles by classifying the smallest subtree that includes the predicate with one of its arguments, i.e. the so called PAF structure. The experiments were carried out with the SVM-light-TK software available at http://ai-nlp.info.uniroma2.it/moschitti/ which encodes the fast tree kernels in the SVM-light software (Joachims, 1999). The multiclassifiers were obtained by training an SVM for each class in the ONE-vs.-ALL fashion. In the testing phase, we selected the class associated with the maximum SVM score. For the ST, SST and PT kernels, we found that the best A values (s</context>
</contexts>
<marker>Moschitti, 2004</marker>
<rawString>Alessandro Moschitti. 2004. A study on convolution kernels for shallow semantic parsing. In proceedings ofACL04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Making tree kernels practical for natural language learning.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL06.</booktitle>
<contexts>
<context position="8527" citStr="Moschitti, 2006" startWordPosition="1583" endWordPosition="1584">e A function for each pair (n1, n2)E NT1 x NT2 (Eq. 1). When the labels associated with n1 and n2 are different, we can avoid evaluating A(n1, n2) since it is 0. Thus, we look for a node pair set Np ={(n1, n2)E NT1 x NT2 : label(n1) = label(n2)}. To efficiently build Np, we (i) extract the L1 and L2 lists of nodes from T1 and T2, (ii) sort them in alphanumeric order and (iii) scan them to find Np. Step (iii) may require only O ( |NT1 |+ |NT2|) time, but, if label(n1) appears r1 times in T1 and label(n2) is repeated r2 times in T2, we need to consider r1 x r2 pairs. The formal can be found in (Moschitti, 2006). 4 The Experiments In these experiments, we study tree kernel performance in terms of average running time and accuracy on the classification of predicate arguments. As shown in (Moschitti, 2004), we can label semantic roles by classifying the smallest subtree that includes the predicate with one of its arguments, i.e. the so called PAF structure. The experiments were carried out with the SVM-light-TK software available at http://ai-nlp.info.uniroma2.it/moschitti/ which encodes the fast tree kernels in the SVM-light software (Joachims, 1999). The multiclassifiers were obtained by training an </context>
</contexts>
<marker>Moschitti, 2006</marker>
<rawString>Alessandro Moschitti. 2006. Making tree kernels practical for natural language learning. In Proceedings of EACL06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Shawe-Taylor</author>
<author>Nello Cristianini</author>
</authors>
<title>Kernel Methods for Pattern Analysis.</title>
<date>2004</date>
<publisher>Cambridge University Press.</publisher>
<marker>Shawe-Taylor, Cristianini, 2004</marker>
<rawString>John Shawe-Taylor and Nello Cristianini. 2004. Kernel Methods for Pattern Analysis. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Suzuki</author>
<author>Hideki Isozaki</author>
<author>Eisaku Maeda</author>
</authors>
<title>Convolution kernels with feature selection for natural language processing tasks.</title>
<date>2004</date>
<booktitle>In Proceedings ofACL04.</booktitle>
<marker>Suzuki, Isozaki, Maeda, 2004</marker>
<rawString>Jun Suzuki, Hideki Isozaki, and Eisaku Maeda. 2004. Convolution kernels with feature selection for natural language processing tasks. In Proceedings ofACL04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S V N Vishwanathan</author>
<author>A J Smola</author>
</authors>
<title>Fast kernels on strings and trees.</title>
<date>2002</date>
<booktitle>In Proceedings ofNIPS02.</booktitle>
<contexts>
<context position="1430" citStr="Vishwanathan and Smola, 2002" startWordPosition="220" endWordPosition="223"> promising results, three general objections against kernel methods are raised: (1) only a subset of the dual space features are relevant, thus, it may be possible to design features in the primal space that produce the same accuracy with a faster computation time; (2) in some cases the high number of features (substructures) of the dual space can produce overfitting with a consequent accuracy decrease (Cumby and Roth, 2003); and (3) the computation time of kernel functions may be too high and prevent their application in real scenarios. In this paper, we study the impact of the subtree (ST) (Vishwanathan and Smola, 2002), subset tree (SST) (Collins and Duffy, 2002) and partial tree (PT) kernels on Semantic Role Labeling (SRL). The PT kernel is a new function that we have designed to generate larger substructure spaces. Moreover, 97 to solve the computation problems, we propose algorithms which evaluate the above kernels in linear average running time. We experimented such kernels with Support Vector Machines (SVMs) on the classification of semantic roles of PropBank (Kingsbury and Palmer, 2002) and FrameNet (Fillmore, 1982) data sets. The results show that: (1) the kernel approach provides the same accuracy o</context>
<context position="5174" citStr="Vishwanathan and Smola, 2002" startWordPosition="890" endWordPosition="893">bels of n1 and n2 are different then O(n1, n2) = 0; - else: 98 XO(n1, n2) = 1 + =l( ~J2) l( ~J1) O(cn1[ ~J1i], cn2[ ~J2i]) ~J1, ~J2,l( ~J1) Y i=1 where ~J1 = (J11, J12, J13, ..) and ~J2 = (J21, J22, J23, ..) are index sequences associated with the ordered child sequences cn1 of n1 and cn2 of n2, respectively, ~J1i and ~J2i point to the i-th children in the two sequences, and l(·) returns the sequence length. We note that (1) Eq. 2 is a convolution kernel according to the definition and the proof given in (Haussler, 1999). (2) Such kernel generates a feature space richer than those defined in (Vishwanathan and Smola, 2002; Collins and Duffy, 2002; Zelenko et al., 2003; Culotta and Sorensen, 2004; ShaweTaylor and Cristianini, 2004). Additionally, we add the decay factor as follows: O(n1, n2) = JO(cn1[ ~J1i], cn2[ ~J2i]) where d(~J1) = ~J1l( ~J1) − ~J11 and d(~J2) = ~J2l( ~J2) − ~J21. In this way, we penalize subtrees built on child subsequences that contain gaps. Moreover, to have a similarity score between 0 and 1, we also apply the normalization in the kernel space, i.e. K0(T1, T2) = ✓K(T1,T1)×K(T2,T2). As the summation K(T1,T2) in Eq. 3 can be distributed with respect to different types of sequences, e.g. th</context>
</contexts>
<marker>Vishwanathan, Smola, 2002</marker>
<rawString>S.V.N. Vishwanathan and A.J. Smola. 2002. Fast kernels on strings and trees. In Proceedings ofNIPS02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Zelenko</author>
<author>C Aone</author>
<author>A Richardella</author>
</authors>
<title>Kernel methods for relation extraction.</title>
<date>2003</date>
<publisher>JMLR.</publisher>
<contexts>
<context position="718" citStr="Zelenko et al., 2003" startWordPosition="99" endWordPosition="102">tti Department of Computer Science University of Rome ”Tor Vergata” Rome, Italy moschitti@info.uniroma2.it Abstract In this paper, we use tree kernels to exploit deep syntactic parsing information for natural language applications. We study the properties of different kernels and we provide algorithms for their computation in linear average time. The experiments with SVMs on the task of predicate argument classification provide empirical data that validates our methods. 1 Introduction Recently, several tree kernels have been applied to natural language learning, e.g. (Collins and Duffy, 2002; Zelenko et al., 2003; Cumby and Roth, 2003; Culotta and Sorensen, 2004; Moschitti, 2004). Despite their promising results, three general objections against kernel methods are raised: (1) only a subset of the dual space features are relevant, thus, it may be possible to design features in the primal space that produce the same accuracy with a faster computation time; (2) in some cases the high number of features (substructures) of the dual space can produce overfitting with a consequent accuracy decrease (Cumby and Roth, 2003); and (3) the computation time of kernel functions may be too high and prevent their appl</context>
<context position="5221" citStr="Zelenko et al., 2003" startWordPosition="898" endWordPosition="901">lse: 98 XO(n1, n2) = 1 + =l( ~J2) l( ~J1) O(cn1[ ~J1i], cn2[ ~J2i]) ~J1, ~J2,l( ~J1) Y i=1 where ~J1 = (J11, J12, J13, ..) and ~J2 = (J21, J22, J23, ..) are index sequences associated with the ordered child sequences cn1 of n1 and cn2 of n2, respectively, ~J1i and ~J2i point to the i-th children in the two sequences, and l(·) returns the sequence length. We note that (1) Eq. 2 is a convolution kernel according to the definition and the proof given in (Haussler, 1999). (2) Such kernel generates a feature space richer than those defined in (Vishwanathan and Smola, 2002; Collins and Duffy, 2002; Zelenko et al., 2003; Culotta and Sorensen, 2004; ShaweTaylor and Cristianini, 2004). Additionally, we add the decay factor as follows: O(n1, n2) = JO(cn1[ ~J1i], cn2[ ~J2i]) where d(~J1) = ~J1l( ~J1) − ~J11 and d(~J2) = ~J2l( ~J2) − ~J21. In this way, we penalize subtrees built on child subsequences that contain gaps. Moreover, to have a similarity score between 0 and 1, we also apply the normalization in the kernel space, i.e. K0(T1, T2) = ✓K(T1,T1)×K(T2,T2). As the summation K(T1,T2) in Eq. 3 can be distributed with respect to different types of sequences, e.g. those composed by p children, it follows that O(n</context>
</contexts>
<marker>Zelenko, Aone, Richardella, 2003</marker>
<rawString>D. Zelenko, C. Aone, and A. Richardella. 2003. Kernel methods for relation extraction. JMLR.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>