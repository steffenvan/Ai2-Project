<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002186">
<title confidence="0.993869">
Literature-based discovery for Oceanographic climate science
</title>
<author confidence="0.998383">
Elias Aamot
</author>
<affiliation confidence="0.942321666666667">
Department of Informatics and Computer Science
Norwegian University of Science and Technology
Trondheim, Norway
</affiliation>
<email confidence="0.996087">
eliasaa@stud.ntnu.no
</email>
<sectionHeader confidence="0.993847" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999477111111111">
This paper presents an overview of the
field of literature-based discovery, as orig-
inally applied in biomedicine. Further-
more it identifies some of the challenges
to employing the results of the field in a
new domain, namely oceanographic cli-
mate science, and elaborates on some of
the research that needs to be conducted to
overcome these challenges.
</bodyText>
<sectionHeader confidence="0.998798" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999774925925926">
The increase in growth rate of the scientific litera-
ture over the past decades has forced researchers to
become increasingly specialized in order to keep
up with the state of the art. This inevitably leads
to the fragmentation of science as researchers from
different (sub-)disciplines rarely have time to read
each other’s papers. Swanson (1986) claimed that
this fragmentation of science can lead to undiscov-
ered public knowledge: Conclusions that can be
made from existing literature, but have never been
made because the knowledge fragments have been
discovered in separate (sub-)disciplines. Adopt-
ing the terminology of Swanson (1991), a litera-
ture can be informally defined as a collection of
papers with a significant amount of cross-citation
related to a single topic. Two literatures are com-
plementary if they contain knowledge fragments
which can be combined to form new knowledge,
and disjoint if they have no articles in common,
and exhibit little or no cross-citation. The implicit
hypothesis is that such complementary but disjoint
(CBD) literatures are common, giving rise to sig-
nificant amounts of undiscovered public knowl-
edge. The field of Literature-based Discovery
(LBD)1 focuses on the development and applica-
tion of computational tools to discover undiscov-
ered public knowledge in scientific literature.
</bodyText>
<footnote confidence="0.908774">
1Also called Literature-based knowledge discovery
(LBKD).
</footnote>
<bodyText confidence="0.9990724">
Most work in LBD has been conducted in sub-
fields of the biomedical literature, frequently em-
ploying knowledge resources specific to that do-
main. This paper will present an overview of some
of the research in LBD, and discuss some of the
challenges in reproducing the results made in the
LBD field in a different domain, namely oceano-
graphic climate science. The structure of this pa-
per is as follows: Section 2 will give an overview
of the LBD field, section 3 will discuss differ-
ences between the biomedical domain and that
of oceanographic climate science, and section 4
will discuss directions for research that will be
conducted in order to adapt LBD methods to the
oceanographic climate science domain.
</bodyText>
<sectionHeader confidence="0.970033" genericHeader="method">
2 Literature-based discovery
</sectionHeader>
<bodyText confidence="0.999908117647059">
Swanson (1986) observed that if a literature L1 as-
serted a —* b, and a disjoint literature L2 asserted
b —* c, then the concept denoted by b could func-
tion as a bridge between L1 and L2, leading to the
discovery of the hypothesis a —* c2. One example
given by Swanson showed that fish oils reduced
blood viscosity (fish oil —* blood viscosity),
and that patients of Raynaud’s disease tend to ex-
hibit high blood viscosity (blood viscosity —*
Raynaud). These two facts led to the hypothe-
sis that fish oils can be used in the treatment of
Raynaud’s disease (fish oil —* Raynaud) when
combined. This hypothesis was subsequently con-
firmed experimentally (Digiacomo et al., 1989).
Although the inference steps are not logically
sound, the procedure is able to produce interest-
ing results. The general approach of bridging dis-
</bodyText>
<footnote confidence="0.99753975">
2A note on terminology: In the LBD literature, capital
letters are normally used for the A, B and C concepts. In
this paper, minuscules will be used to represent individual
concepts, while capital letters represent sets.
Also, some authors use A to denote the the goal concept,
and C for the starting concept. This paper follows the most
commonly used terminology, in which a always denotes the
starting concept, and c denotes the goal concept.
</footnote>
<page confidence="0.496532">
1
</page>
<note confidence="0.996601">
Proceedings of the Student Research Workshop at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 1–10,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999932666666667">
joint literatures by means of intermediary terms
has been dubbed Swanson linking, and is also re-
ferred to as the ABC model.
Swanson and Smalheiser (1997) explain that the
discovery of the ABC structure and the fish oil-
Raynaud’s disease connection happened acciden-
tally. This discovery led Swanson to conduct lit-
erature searches aided by existing information re-
trieval tools to search for more undiscovered pub-
lic knowledge using the ABC model, resulting in
the discovery of eleven connections between mi-
graine and magnesium (Swanson, 1988). As the
discovery process was extremely time consum-
ing, requiring the researcher to read hundreds of
papers, Swanson later developed a computational
tool, Arrowsmith, to streamline the discovery pro-
cess.
There are two modes of discovery in the ABC
model: Open discovery and closed discovery. In
open discovery, the researcher only knows the
starting concept a, and is interested in uncov-
ering undiscovered public knowledge related to
a. A researcher who looks for consequences of
ocean acidification might conduct an open dis-
covery search with a = ocean acidification.
In closed-discovery, the researcher knows both
the starting concept a and the goal concept c,
and is interested in finding concepts B that prove
an explanation of the relationship between the
two terms. A researcher who hypothesizes that
ocean acidification might cause a reduction in phy-
toplankton population and tries to discover the
causality chain might conduct a closed discov-
ery search with a = ocean acidification, c =
phytoplankton population.
This section will present an overview of the
state-of-the-art of the LBD field. As this paper
discusses the adaptation of LBD to new domains,
approaches will be grouped into of three groups
according to their dependence on domain specific
tools and resources, because reliance on these is
likely to hinder cross-domain adaptation3.
</bodyText>
<subsectionHeader confidence="0.9388425">
2.1 Group 1: Domain-independent
approaches
</subsectionHeader>
<bodyText confidence="0.998698333333333">
In the general Swanson linking paradigm, open
discovery is conducted by extracting all relations
a → bi from the literature of a, written L(a). For
</bodyText>
<footnote confidence="0.9615025">
3Some of the papers are presented as domain independent,
even though they employ domain specific resources, because
their main research contributions can be adapted in a domain-
independent manner.
</footnote>
<bodyText confidence="0.999068565217392">
every bi, all relations bi → cj are then extracted
from L(bi). The set of all a → bi → cj relations,
dubbed discovery candidates is then are presented
to the user as potential discoveries, sorted accord-
ing to some ranking metric.
In most LBD approaches L(x) is defined
as the set of documents returned when search-
ing for x in a literature database. The litera-
ture database most commonly used in LBD is
Pubmed/Medline4, maintained by the US National
Library of Medicine. The original Arrowsmith
system considered only paper titles, as Swanson
considered these to hold the most compact knowl-
edge, but it has become the standard approach in
LBD to use abstracts and possibly index terms in
addition to the titles. The motivation for this is that
abstracts and index terms contain more knowledge
than only titles.
Somewhat surprisingly, few LBD systems use
full paper texts. Schuemie et al. (2004) show that
30-40% of all information contained in a section
is new to that section, meaning that significant
amounts of knowledge is lost when only looking
at abstracts and index terms of a paper. The need
for full text data is also pointed out by Cameron
et al. (2013). The reason for not using full text
seems to be that paper abstracts and index terms
are available in xml format through the Pubmed
API, while full paper texts require accessing rights
and are normally stored as pdf.
In co-occurrence based systems, a relation x →
y is postulated if x and y exhibit a high degree
of co-occurrence in L(x), either in terms of abso-
lute frequency of co-occurrence, or in terms of sta-
tistical unlikelihood given the statistical promis-
cuity of the two concepts. While a few systems
use the sentence as the domain for counting co-
occurrences, most systems count co-occurrences
across entire abstracts.
To present the user with only potential new dis-
coveries, most LBD systems remove from C all
terms that are already known to be in a relation
with a. In co-occurrence based methods, this is
done by removing any (a, c) pairs that exhibit
higher degrees if co-occurrence than a predefined
threshold (normally 1 co-occurrence) in L(a).
</bodyText>
<subsectionHeader confidence="0.90045">
2.1.1 Arrowsmith
</subsectionHeader>
<bodyText confidence="0.996485">
The original Arrowsmith system works as follows
(Swanson and Smalheiser, 1997): L(a) is fetched
</bodyText>
<footnote confidence="0.979269">
4http://www.ncbi.nlm.nih.gov/pubmed/
</footnote>
<page confidence="0.995417">
2
</page>
<bodyText confidence="0.999959266666667">
by conducting a Medline search to retrieve the ti-
tles of papers containing a in the title. The set
of potential B concepts is extracted as the list of
unique words in L(a), after a stop list of approxi-
mately 5000 words has been applied. The B-term
set is further pruned by removing all the words
that have lesser relative frequency in L(a) than in
Medline. The potential B terms are subsequently
presented to the user, who can then remove words
that are thought to be unsuitable. For each bz ∈ B,
L(bz) is retrieved and a set CZ is generated, subject
to the same stopword and frequency restrictions as
before. The terms in the union of the CZ sets are
then ranked according to the number of b-terms
that connect them to the a-term.
</bodyText>
<subsectionHeader confidence="0.616841">
2.1.2 Information retrieval-based methods
</subsectionHeader>
<bodyText confidence="0.999943">
Gordon and Lindsay (1996) (Lindsay and Gordon,
1999) developed a system in parallel, which dif-
fered from Arrowsmith in several ways: Firstly,
while Arrowsmith was word-based, their system
used n-grams as the unit of analysis. A stop list
was applied by removing all n-grams that con-
tained any stop word occurrence. Secondly, their
system used entire Medline records, comprising
of keywords, abstracts and titles, whereas Arrow-
smith only used paper titles. Thirdly, their sys-
tem employed information retrieval metrics such
as tf*idf to find b-terms among the generated can-
didates, whereas Arrowsmith was based on rela-
tive frequencies.
The lexical statistical approach is so generic that
it lends itself directly to application in different do-
mains. In a later paper, Gordon et al. (2001) em-
ploy this approach to conduct LBD searches di-
rectly on the World Wide Web, searching for ap-
plication areas for genetic algorithms. It should
however be noted that the goal of this experiment
was not LBD in the sense of uncovering undiscov-
ered public knowledge, instead focusing in discov-
ering something that might be “publicly known”
but novel to the user.
</bodyText>
<subsectionHeader confidence="0.963803">
2.1.3 Ranking metrics
</subsectionHeader>
<bodyText confidence="0.999975260869565">
Wren et al. (2004) pointed out that the structure
of concept co-occurrence relationships is such that
most concepts are connected to any other concept
within few steps. This small world phenomenon
implies that research focus should be shifted away
from retrieving discovery candidates to ranking
them, because a significant portion of the con-
cept space will be retrieved even within two co-
occurrence relation steps. The paper proposes
ranking implicit relationships by comparing the
number of observed indirect connections between
a and c to the number of expected connections in a
random network model, given the relative promis-
cuity of the intermediary terms.
In another paper, Wren (2004) emphasizes the
importance of using a statistically sound method
of ranking relationship strengths, such as “chi-
square tests, log-likelihood ratios, z-scores or t-
scores”, because co-occurrence based measures
bias towards more general, and thus less inter-
esting relationships. The paper further proposes
an extension to the mutual information measure
(MIM) as a ranking measure.
</bodyText>
<subsectionHeader confidence="0.833742">
2.1.4 Latent semantic indexing
</subsectionHeader>
<bodyText confidence="0.999990173913044">
Gordon and Dumais (1998) propose exploiting
the ability of certain vector-based semantic mod-
els such as Latent semantic indexing (LSI) to
discover implicit relationships between terms for
LBD. They first train the semantic model on L(a),
and let the user choose as b one of the terms most
similar to a. A new semantic model is built from
L(b), and discovery candidates are ranked accord-
ing to their similarity to a in the L(b)-model. Their
experiments showed that the resulting b- and c-
term candidate lists closely resemble the lists pro-
duced by the information retrieval inspired lexical
statistics.
In another experiment they built a semantic
model from a random sample of all of Medline,
and looked directly for c-terms in the semantic
model by considering the terms most similar to a.
This “zoomed-out” approach produced different
results than the previous Swanson linking inspired
approach, which the authors claimed meant that
the two methods are complementary and could
therefore be used in parallel, but no in-depth eval-
uation was conducted on the quality of the results.
</bodyText>
<subsectionHeader confidence="0.862584">
2.1.5 Evaluation efforts
</subsectionHeader>
<bodyText confidence="0.9991868">
LBD has a tradition for questionable evaluation ef-
fort. The original discoveries in LBD were made
manually by Swanson, and most computational
systems are evaluated solely according to their
ability to replicate one or more of Swanson’s dis-
coveries. This is problematic for several reasons:
First of all, Swanson’s discoveries were never in-
tended as a gold standard, and being able to ac-
complish a single task that is known in advance
does not mean that the results are generalizable.
</bodyText>
<page confidence="0.994962">
3
</page>
<bodyText confidence="0.999964310344827">
Secondly, there is no quantitative basis for com-
paring different approaches or metrics.
Yetisgen-Yildiz and Pratt (2009) conducted the
first systematic quantitative evaluation of discov-
ery candidate ranking metrics and relation rank-
ing/generation techniques. They partitioned Med-
line into two parts, according to a cut-off date.
LBD was conducted on the pre-cut-off set, and
the post-cut-off set was used as a gold standard
to compute precision and recall. In the post-cut-
off set, a connection was considered to exist if
two terms co-occurred in any document. The
ranking metrics that were evaluated were Linking
term count (LTC), that is the number of b-terms
connecting a and c, Average minimum weight
(AMW), that is the average weight of the a →
b → c connections, and Literature cohesiveness
(COH), a measure developed by Swanson but not
widely adopted. Experiments showed that LTC
gave better precision at all levels of recall. The re-
lation generation techniques that were considered
were association rules, tf-idf, z-score and MIM.
The experiment showed that association rules give
the best precision score (8.8%) but the worst recall
score (53.76%), while tf-idf gave the best recall
(88.0%) but a rather low precision (2.29%).
While the evaluation effort was an important
contribution to the LBD field, more quantitative
evaluation is required. First of all, all candidate
ranking/generation techniques and ranking met-
rics were tested with only one value of the pa-
rameters (for instance the cut-off score for tf-idf,
and the cut-off probability for z-score). Compar-
ing the performance of different settings for the
parameters would yield a better understanding of
each of the metrics, and could lead to results com-
pletely different than those reported. Secondly,
only a small subset of possible relation genera-
tion/ranking techniques and discovery candidate
ranking metrics were tested. For example, no re-
lation extraction-based methods (see section 2.3)
were included in the evaluation.
The evaluation methodology can be critiqued in
several ways. Firstly, building the gold standard
from the post-cut-off set is problematic for several
reasons: A co-occurrence can exist in the post-cut-
off set without necessarily corresponding to a new
discovery. Also, as pointed out in Kostoff (2007),
it is very difficult to verify that a discovery has
not been made before the cut-off date. Another
problem is that the post-cut-off set only contains
discoveries that have been made in the present,
all future discoveries are therefore excluded from
the gold standard. Secondly, it is not obvious that
quantitative measures reflect the usefulness of the
LBD system: When at all is said and done, the
usefulness of a LBD system equates to its ability
to support user in discovering knowledge.
</bodyText>
<subsectionHeader confidence="0.99955">
2.2 Group 2: Concept-based approaches
</subsectionHeader>
<bodyText confidence="0.999929666666667">
Several researchers advocate using domain spe-
cific concepts taken from an ontology or con-
trolled vocabularies instead of n-gram tokens. Us-
ing concepts provides three benefits over n-gram
models: Firstly, synonyms and spelling variants
are mapped to the same semantic concept. Sec-
ondly, using concepts allows for ranking and fil-
tering according to semantic categories. Finally, it
becomes easier to constrain the search space by re-
moving spurious or irrelevant n-grams at an early
stage, as they don’t map to any concept in the do-
main. On the other hand, concept extraction from
raw text is a non-trivial operation.
In LBD concept extraction is conducted in one
of two ways: One option is to use NLP tools
designed for entity recognition. The most com-
monly used in the biomedical domain is MetaMap
(Aronson and Lang, 2010), which extracts con-
cepts from the Unified Medical Language System
(UMLS) meta-thesaurus5. The other option is to
use Medical Subject Headings (MeSH)6. MeSH
is a controlled vocabulary for indexing biomedical
papers, with which all Medline papers have been
manually tagged. MeSH keywords can be queried
directly from the Medline API. Both MeSH and
UMLS terms are organized hierarchically accord-
ing to semantic categories.
</bodyText>
<sectionHeader confidence="0.545456" genericHeader="method">
2.2.1 DAD
</sectionHeader>
<bodyText confidence="0.999939">
In their system, DAD (Disease-Adverse reaction-
Drug), Weeber et al. (2001) use MetaMap. They
showed in an experiment that the number of con-
cepts extracted is significantly lower than the num-
ber of n-grams, even after stop lists are applied
(8,362 n-grams vs. 5,998 concepts). DAD also al-
lows the user to specify which semantic categories
to consider, by for instance only allowing concepts
of the type pharmacological substance as c con-
cepts, reducing the number of search paths signif-
icantly.
</bodyText>
<footnote confidence="0.999959">
5http://www.nlm.nih.gov/research/umls/
6http://www.nlm.nih.gov/mesh/
</footnote>
<page confidence="0.994151">
4
</page>
<bodyText confidence="0.9999658">
Their approach was able to replicate both Swan-
son’s Raynaud’s-fish oil and migraine-magnesium
discoveries, but it was discovered that MetaMap
maps both mg (milligram) and Mg (magnesium) to
the concept magnesium, giving optimistic results
for the migraine-magnesium experiment. This is
but one example showing that one of the problems
with employing NLP tools in an LBD system is
that system performance becomes closely tied to
the performance of the tools it employs.
</bodyText>
<subsectionHeader confidence="0.62169">
2.2.2 LitLinker
</subsectionHeader>
<bodyText confidence="0.999896766666666">
Pratt and Yetisgen-Yildiz (2003) developed a
system, LitLinker, which originally also used
MetaMap, but they later found it too computation-
ally expensive for practical use (Yetisgen-Yildiz
and Pratt, 2006). MeSH terms are therefore em-
ployed instead.
In a preprocessing step, LitLinker calculates the
co-occurrence patterns of every MeSH term across
the literatures of every other MeSh term. For ev-
ery MeSH term, the mean and standard deviation
of co-occurrence counts across the literatures is
calculated. In the discovery process, a term is
considered to be related to another term if their
co-occurrence is higher than statistically expected,
based on its z-score.
Yetisgen-Yildiz and Pratt identified three
classes of uninteresting links and terms that
should be pruned automatically by system: (1)
too broad terms (giving the examples medicine,
disease and human), (2) too closely related terms
(giving the example migraine and headache), and
(3) semantically nonsensical connections. The
first class is handled by removing any concept if it
is strictly more specific in the MeSH ontology hi-
erarchy than any included term. The second class
is handled by pruning all links between terms
that are closely related (grandparents, parents,
siblings and children) in the ontology. The third
class is handled by letting the user specify which
semantic classes of concepts are allowed to link.
</bodyText>
<subsectionHeader confidence="0.837961">
2.2.3 Bitola
</subsectionHeader>
<bodyText confidence="0.998704428571429">
Hristovski et al. (2001) originally developed a
system called Bitola7 that discovered association
rules between MeSH terms. Association rules
mining is a common data mining method for dis-
covering relations between variables in a database.
Association rules are traditionally used for mar-
ket basket analysis, in which rules of the type
</bodyText>
<footnote confidence="0.842816">
7http://ibmi3.mf.uni-lj.si/bitola/
</footnote>
<bodyText confidence="0.995271666666667">
{pizza, steak} → {coca cola} are inferred, stat-
ing that if somebody buys pizza and steak, he/she
is likely to buy coca cola as well. In Bitola’s dis-
covery step, basic associations are first mined from
the co-occurrence patterns of MeSH terms. Sub-
sequently, indirect associations a → c are inferred
by combining association rules on the form a → bz
and bz → c, and ranked according to the sum of
strengths of the connecting association rules.
</bodyText>
<subsectionHeader confidence="0.996879">
2.3 Group 3: Relation extraction-based
approaches
</subsectionHeader>
<bodyText confidence="0.999952820512821">
Hristovski et al. (2006) point out two prob-
lems with the co-occurrence based LBD systems:
Firstly, no explicit explanation of the relation be-
tween the a and c terms is given. Secondly, a
large number of spurious relations are discovered,
as demonstrated by the low precision values wit-
nessed during system evaluation. Both aspects
increase the time needed to examine the output
of the system by the human user. They suggest
that employing natural language processing (NLP)
techniques to extract explicit relations from the pa-
pers can improve performance on both points.
The biomedical information extraction tool
most commonly used in LBD is SemRep (Rind-
flesch and Fiszman, 2003), which uses lin-
guistically motived rules on top of the ouput
from MetaMap and the Xerox POS Tag-
ger to extract knowledge in the form of &lt;
subject, predicate, object &gt; relation triplets. Al-
though the knowledge expressed in natural lan-
guage is more complex than what can be rep-
resented in simple relation triplets, SemRep is
able to provide a better approximation to the
knowledge content of scientific papers than do co-
occurrence based methods.
While most LBD research employs the same
NLP tool, systems differ as to how the extracted
relations are represented and how reasoning is
conducted in the relation space. Some researchers
closely follow the Swanson linking paradigm, and
use relation extraction based method instead of
or in addition to co-occurrence based methods
for candidate generation and ranking. Other re-
searchers take an approach motivated by Wren’s
observation that a small-world property holds in
the network of concept relations in literature. As
significant portions of the concept-relation space
will have to be explored in a two-step search any-
way, it might be better to extract all relations from
</bodyText>
<page confidence="0.965746">
5
</page>
<bodyText confidence="0.999940705882353">
the entire literature collection or from a random
sample thereof, and rather focus on valid and effi-
cient reasoning within the entire concept-relation
space.
Smalheiser (2012) critiques the usage of rela-
tion extraction in LBD and claims that while rea-
soning over explicit relations may lead to so-called
incremental discoveries, that is, discoveries that
lie close to the existing knowledge and therefore
are less interesting, they are not able to lead to any
radical discoveries, that is discoveries that seem
unlikely at time of discovery. He also claims that
human discoveries, both incremental and radical,
tend to be on a higher level, using analogies and
abstract similarities rather than explicit relations,
and that the benefit from using relation extraction
therefore is minimal8.
</bodyText>
<subsectionHeader confidence="0.68527">
2.3.1 Augmented Bitola
</subsectionHeader>
<bodyText confidence="0.999951413793103">
In two papers, Hristovski et al. (2006; 2008) ex-
periment with augmenting the Bitola system by
using relation extraction tools. In addition to Sem-
Rep, they also use another tool, BioMedLee, be-
cause each of the tools exhibits better performance
than the other on certain types of relations.
To guide search through the concept-relation
space, they introduce the notion of a discovery pat-
tern. A discovery pattern is a set of concept types
and relations between them that could imply an in-
teresting relationship in the domain. One discov-
ery pattern, maybe treats can informally be stated
as: If a disease leads to a biological change, and
a drug leads to the opposite change, then the drug
may be able to treat the disease.
The integration between Bitola and the NLP
components presented in the system is rather
crude; for a given query term, Bitola outputs a set
of related terms and the set of papers connecting
each related term to the query term. The connect-
ing paper must then be manually input into the
NLP components to extract the relation between
the query term and any related term. Following a
discovery pattern requires extracting relations be-
tween several concepts until a chain of the correct
relations has been found. The possibility to in-
tegrate Bitola and the NLP tools more tightly has
been raised as possible future work, but it has been
noted a concern that the computational load in-
</bodyText>
<footnote confidence="0.8798595">
8Smalheiser’s critique also extends to many of the widely
employed co-occurrence based methods. The argument is
that research should focus on developing methods that rank
interesting relations highly.
</footnote>
<bodyText confidence="0.971933">
creases as the NLP component becomes less con-
strained by the co-occurrence based components.
</bodyText>
<subsectionHeader confidence="0.644405">
2.3.2 Graph-based reasoning
</subsectionHeader>
<bodyText confidence="0.999975714285715">
The extracted relations can be represented as a
Predications Graph in which each concept is rep-
resented by a node and each relation is a labelled,
directed edge from the subject concept to the ob-
ject concept. Representing the concept-relation
space as a graph provides two benefits: As a visual
tool, a graph can display the knowledge extracted
by the system in a way that is easily understood by
the user and can be navigated/explored easily. As
a mathematical object, one can employ graph the-
oretic results when developing algorithms for the
reasoning process.
In the work of Wilkowski et al. (2011) an initial
graph is constructed by querying a pre-compiled
database of predications extracted by SemRep
from Medline for all relations containing the a
concept. The user then incrementally expands the
graph by selecting which terms to query relations
for from a list of concepts ranked by their degree
centrality (i.e. their degree of connectivity in the
graph). After graph construction, potential discov-
ery paths are ranked according to summed degree
centrality.
Although some work has been conducted in
graph-based LBD, seemingly no research has been
conducted on LBD in a global, large-scale pred-
ications graph derived from all of Medline, or a
sample of it.
</bodyText>
<subsectionHeader confidence="0.660636">
2.3.3 Predication-based semantic indexing
</subsectionHeader>
<bodyText confidence="0.999966055555556">
Cohen et al. (2012a) propose a hyperdimensional
computing technique they call predication-based
semantic indexing (PSI) for efficient representa-
tion and reasoning in the concept-relation space.
In PSI, concepts and relations are represented
as high-dimensional vectors, where the semantic
content of a concept’s vector is a combination of
all the relations it occurs in and all the concepts
it is related to, weighted by the frequency of the
relation. The system uses SemRep to extract rela-
tions from a sample of 8,182,882 Medline records
as input to the training process. Inference in this
hyperdimensional space can be performed by ordi-
nary vector operations. The paper shows how PSI
enables analogical reasoning along the lines of “x
is to what as y is to z?” without explicitly travers-
ing the intermediary relation paths between y and
z, leading to efficient inference.
</bodyText>
<page confidence="0.998301">
6
</page>
<bodyText confidence="0.999879666666667">
The system could originally only infer analo-
gies along a single one of the pathways connect-
ing two concepts x and y. In a later paper Co-
hen et al. (2012b) expanded the PSI to allow for
analogies along multiple pathways, by introducing
a vector operation simulating quantum superposi-
tion, efficiently reasoning over the entire subgraph
connecting x and y. The paper claims that because
real world concepts tend to interact through sev-
eral pathways, literature-based discovery should
strive to be able to reason following a similar pat-
tern.
</bodyText>
<subsectionHeader confidence="0.99599">
2.4 Approach type hierarchy
</subsectionHeader>
<bodyText confidence="0.999972407407408">
From the previous section, it is easy to see the
LBD approaches can be divided into a three-level
hierarchy according to their dependence on knowl-
edge resources and NLP tools:
Type 1 approaches do not require any knowl-
edge resources: Terms are extracted directly
from text, and relations are hypothesized ac-
cording to co-occurrence patterns. Because
all knowledge is extracted directly from text
they are completely domain-independent.
Type 2 approaches choose terms from a prede-
fined set of concepts. Co-occurrence patterns
are still used to determine relations. The pre-
defined concepts are normally gathered from
a domain-specific ontology or vocabulary.
Type 3 approaches use relation extraction tools
to extract concepts and relations from text.
Because the relations of interest vary widely
between domains, domain-specific NLP tools
are normally used.
It is evident from the description above that
there is a trade-off between reliance on knowledge
resources and system performance, as well as a
strong correlation between reliance on knowledge
resources and domain-dependence. This poses a
challenge when adapting LBD approaches to new
domains.
</bodyText>
<sectionHeader confidence="0.986837" genericHeader="method">
3 Domain differences
</sectionHeader>
<bodyText confidence="0.97976403508772">
The current work is a part of a project researching
the effects of climate change on the oceanic food
web (i.e. who eats who, and how the relative pop-
ulation sizes affect each other) and the biological
pump (roughly the ocean’s ability to absorb and
retain excess atmospheric CO2). The following
section will discuss some of the research issues
related to adapting the LBD techniques from the
biomedical domain to that of the target domain.
Oceanographic climate science is a cross-
disciplinary domain, bringing together researchers
from fields such as biology, chemistry, earth sci-
ence, climate science and oceanography. The
cross-disciplinary nature gives rise to an abun-
dance of disjoint literatures, providing strong
incentives for LBD. Unfortunately, in a cross-
disciplinary domain, scientists from different
fields bring their own terminologies and scientific
assumptions, creating challenges for LBD work.
While substantial research and engineering ef-
fort has gone into the development of NLP
tools and computational knowledge sources in the
biomedical domain, oceanographic climate sci-
ence is in this respect under-resourced. To the best
of my knowledge, no domain specific NLP tools
exist for any sufficiently closely related domain,
and although ontologies and controlled vocabular-
ies exist for some of the related disciplines, such
as for biology and chemistry, substantial effort is
required to identify and combine the desired re-
sources. As a result, it seems unlikely that any of
the knowledge intensive (type 2 and 3) LBD meth-
ods can be directly applied to oceanographic cli-
mate science. Oceanographic climate science also
lacks an indexed literature database that covers the
entire field, akin to Medline.
Epistemologically there might be a significant
difference between the fields: The objects of study
(the ocean in oceanographic climate science and
the human body in biomedicine) and their pro-
cesses are quite different, requiring different types
of scientific experiments. It therefore seems likely
that the structure of the knowledge produced in the
different fields might be different. In medicine,
experiments can be conducted in a large popula-
tion of complete systems (human bodies), while in
oceanographic experiments must be conducted by
sampling subsystems of a single complete system
(the ocean). It is therefore not surprising that pre-
liminary observations seem to imply that the re-
sults found in oceanographic climate science do
not lend themselves to generalization as easily as
do those in biomedicine, and that the former have
a stronger context dependence (Compare Eicos-
apentaenoic acid AFFECTS Vascular constriction
to Increased labile dissolved organic carbon RE-
DUCES carbon accumulation GIVEN THAT bac-
</bodyText>
<page confidence="0.999089">
7
</page>
<bodyText confidence="0.999720685714286">
teria growth rate is limited). To account for this,
text mining tools must be able to extract precondi-
tions as well as relations, or the user must be in-
volved more closely during discovery pattern ap-
plication to verify that the extracted relations in-
deed hold true in the same context.
Example discovery patterns for oceanographic
climate science have been developed in coopera-
tion with a domain expert, shedding light on some
differences between the domains. One research
goal of biomedicine is to understand the interac-
tions between domain concepts in order to treat
diseases, which is reflected in discovery patterns
such as maybe treats (as mentioned in 2.3.1). The
discovery patterns developed for oceanographic
climate science target the interactions between di-
rectional change events (increase or reduce) in
quantitative variables, such as An increase in CO2
causes a decrease in ocean pH. The types of inter-
actions targeted by these discovery patterns have
a more complex structure than the binary relations
that define maybe treats. Because most relation
extraction tools extract only binary relations, it
seems that simply adapting existing relation ex-
traction tools to the domain will not be sufficient.
Ganiz et al. (2006) discusses that LBD lacks a
solid theoretic foundation, as most research is ap-
plied, rather than theoretical in nature. Although
some inquiry has been conducted into the nature
of discoveries (Smalheiser, 2012), there is little
knowledge about which properties are required to
hold in the domain for the LBD methods to be ap-
plicable, but the current work assumes that all sci-
entific disciplines are sufficiently similar for LBD
methods to be useful.
</bodyText>
<sectionHeader confidence="0.986089" genericHeader="method">
4 Research directions
</sectionHeader>
<bodyText confidence="0.999983516666667">
The lack of available knowledge resources and
NLP tools for the domain makes it hard to di-
rectly employ any of the knowledge intensive
LBD methods. The development of relation ex-
traction tools for the domain falls outside the scope
of the current thesis, and therefore so does the ap-
plication of type 3 approaches. Instead, the current
thesis will focus on bridging the gap between the
different terminologies and writing styles caused
by different backgrounds in the cross-disciplinary
field. To this end, I propose using an unsuper-
vised approach to jointly learn a semantic parser
and an ontology from the literature, following the
approach of Poon and Domingos (2010).
Poon and Domingos (2009) show that a seman-
tic parser that is able to make non-trivial abstrac-
tions from syntactic structure and word usage can
be successfully learned in an unsupervised fash-
ion. The system they describe is for instance able
to map passive and active form into the same se-
mantic representation and build realistic synonym
hierarchies. One challenge that must be addressed
is that the current state-of-the-art clusters words
based on their argument frames, leading to highly
accurate hierarchical clustering of verbs, but lower
performance for nouns as these have less diverse
argument frames. One research question that will
be addressed is how a larger context can be ex-
ploited to yield higher performance for nouns.
In an LBD context, the learning process can be
seen as bootstrapping a set of concepts for the do-
main. The resulting system can be considered a
hybrid between a type 1 and type 2 approach in
terms of the hierarchy defined in 2.4, as it does
not use any domain knowledge, but still proposes
a set of concepts. A hypothesis that will be evalu-
ated empirically is whether this will provide better
results than a pure type 1 system.
The ontology learned by the system can be
edited by a domain expert, or combined with on-
tologies of related fields as they become available,
thus providing an elegant interface for integration
with domain knowledge in an incremental fashion.
The proposed approach will use Markov Logic, a
probabilistic extension to first-order logic (FOL),
as a knowledge representation language. Back-
ground knowledge can therefore easily be incor-
porated by formulating it as FOL, and the proba-
bilistic aspect enables the system handle contra-
dictions that may occur when combining back-
ground knowledge from multiple sources.
The training data set will consist of paper ab-
stracts collected by querying the Mendeley API9
with a set of keywords that represent the most in-
teresting topics in the domain. The keywords will
be developed with the help of a domain expert. As
a pre-processing step, the training sentences will
be dependency parsed using the Stanford Parser10.
The proposed LBD system, Houyi11, will use syn-
onym clusters as concepts, and generate a → bi
</bodyText>
<footnote confidence="0.918687142857143">
9Mendeley is a web-based reference manager and aca-
demic social network that has a large crowd-sourced database
of meta-data, such as abstracts, on scientific papers.
10nlp.stanford.edu/software/lex-parser.
shtml
11The system is named after a legendary archer in Chinese
mythology.
</footnote>
<page confidence="0.997562">
8
</page>
<bodyText confidence="0.999116473684211">
and bi → cj relation candidates based on td-idf
scores. The choice of tf-idf as relation genera-
tion/ranking mechanism is motivated by experi-
ments showing that tf-idf gives high recall at the
cost of mediocre precision (see section 2.1.5). Be-
cause the system is intended to be augmented by
relation extraction tools in the future, recall is
favoured over precision, as precision is expected
to increase in the final version. The discovery can-
didates are ranked by the number of paths connect-
ing them to a, also motivated by the quantitative
experiments described in section 2.1.5.
Houyi will be evaluated quantitatively by com-
paring performance on a data set divided into
training and test data by a cut-off date, follow-
ing the approach taken by Yetisgen-Yildiz and
Pratt (2009). As discussed in section 2.1.5, this
is not a perfect evaluation procedure, but it will
at least give an indication as to whether unsuper-
vised semantic parsing and ontology building con-
tributes to LBD performance. The baseline sys-
tem, Sheshou12, will use the same ranking met-
ric and candidate generation mechanism as Houyi,
and uses the NPs extracted by the Stanford Parser
as terms.
Development of domain specific ontologies and
relation extraction tools is required to apply type
3 LBD methods in the domain. Although outside
the scope of the current thesis, it is expected that
the resulting semantic parser and ontology can be
useful for the development of more sophisticated
tools: The semantic parser can function as a pre-
processing step for the relation extraction tool by
resolving syntactic and synonymic variations. The
ontology can be iteratively improved by integrat-
ing existing ontologies and human editing, thus
providing a point of origin for domain knowledge
engineering.
</bodyText>
<sectionHeader confidence="0.994943" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99991925">
This paper is a part of an ongoing master’s thesis
under the supervision of Pinar ¨Ozt¨urk, with Erwin
Marsi as co-supervisor. I am extremely grateful
for their feedback and support.
</bodyText>
<sectionHeader confidence="0.99797" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995927049180328">
Alan R. Aronson and Franc¸ois-Michel M. Lang. 2010.
An overview of MetaMap: historical perspective and
12Mandarin Chinese for “archer”, a reference to Arrow-
smith.
recent advances. Journal of the American Medical
Informatics Association : JAMIA, 17(3):229–236,
May.
Delroy Cameron, Olivier Bodenreider, Hima Yala-
manchili, Tu Danh, Sreeram Vallabhaneni, Kr-
ishnaprasad Thirunarayan, Amit P. Sheth, and
Thomas C. Rindflesch. 2013. A graph-based re-
covery and decomposition of swanson’s hypothesis
using semantic predications. J. of Biomedical Infor-
matics, 46(2):238–251, April.
Trevor Cohen, Dominic Widdows, Roger W. Schvan-
eveldt, Peter Davies, and Thomas C. Rindflesch.
2012a. Discovering discovery patterns with
predication-based Semantic Indexing. Journal of
Biomedical Informatics, 45(6):1049–1065, Decem-
ber.
Trevor Cohen, Dominic Widdows, Lance Vine, Roger
Schvaneveldt, and Thomas C. Rindflesch. 2012b.
Many Paths Lead to Discovery: Analogical Re-
trieval of Cancer Therapies. In Quantum Interac-
tion, volume 7620 of Lecture Notes in Computer Sci-
ence, pages 90–101. Springer Berlin Heidelberg.
Ralph A. Digiacomo, Joel M. Kremer, and Dhiraj M.
Shah. 1989. Fish-oil dietary supplementation in pa-
tients with Raynaud’s phenomenon: A double-blind,
controlled, prospective study. The American Jour-
nal of Medicine, 86(2):158–164, January.
Murat C. Ganiz, William M. Pottenger, and Christo-
pher D. Janneck. 2006. Recent Advances in Liter-
ature Based Discovery. In Journal of the American
Society for Information Science and Technology.
Michael D. Gordon and Susan Dumais. 1998. Using
latent semantic indexing for literature based discov-
ery. Journal of theAmerican Society for Information
Science and Technology, 49(8):674–685, June.
Michael D. Gordon and Robert K. Lindsay. 1996.
Toward discovery support systems: a replication,
re-examination, and extension of Swanson’s work
on literature-based discovery of a connection be-
tween Raynaud’s and fish oil. Journal of the Ameri-
can Society for Information Science and Technology,
47(2):116–128, February.
Michael Gordon, Robert K. Lindsay, and Weiguo Fan.
2001. Literature Based Discovery on the World
Wide Web. In ACM Transactions on Internet Tech-
nology, pages 261–275, New York, USA. ACM
Press.
Dimitar Hristovski, J. Stare, B. Peterlin, and S. Dze-
roski. 2001. Supporting discovery in medicine
by association rule mining in Medline and UMLS.
Studies in health technology and informatics, 84(Pt
2):1344–1348.
Dimitar Hristovski, Carol Friedman, Thomas C. Rind-
flesch, and Borut Peterlin. 2006. Exploiting seman-
tic relations for literature-based discovery. AMIA ...
Annual Symposium proceedings /AMIA Symposium.
AMIA Symposium, pages 349–353.
</reference>
<page confidence="0.944578">
9
</page>
<reference confidence="0.999801155555556">
Dimitar Hristovski, C. Friedman, T. C. Rindflesch, and
B. Peterlin. 2008. Literature-Based Knowledge
Discovery using Natural Language Processing. In
Peter Bruza and Marc Weeber, editors, Literature-
based Discovery, volume 15 of Information Science
and Knowledge Management, chapter 9, pages 133–
152. Springer, Heidelberg, Germany.
Ronald N. Kostoff. 2007. Validating discovery in
literature-based discovery. Journal of Biomedical
Informatics, 40(4):448–450, August.
Robert K. Lindsay and Michael D. Gordon. 1999.
Literature-based discovery by lexical statistics.
Journal of the American Society for Information Sci-
ence, pages 574–587.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing: Volume 1 - Volume 1, EMNLP
’09, pages 1–10, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Hoifung Poon and Pedro Domingos. 2010. Unsuper-
vised ontology induction from text. In Proceedings
of the 48th Annual Meeting of the Association for
Computational Linguistics, ACL ’10, pages 296–
305, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Wanda Pratt and Meliha Yetisgen-Yildiz. 2003.
LitLinker: capturing connections across the biomed-
ical literature. In Proceedings of the 2nd interna-
tional conference on Knowledge capture, K-CAP
’03, pages 105–112, New York, NY, USA. ACM.
Thomas C. Rindflesch and Marcelo Fiszman. 2003.
The interaction of domain knowledge and linguis-
tic structure in natural language processing: inter-
preting hypernymic propositions in biomedical text.
Journal of Biomedical Informatics, 36(6):462–477,
December.
M. J. Schuemie, M. Weeber, B. J. Schijvenaars, E. M.
van Mulligen, C. C. van der Eijk, R. Jelier, B. Mons,
and J. A. Kors. 2004. Distribution of information
in biomedical abstracts and full-text publications.
Bioinformatics, 20(16):2597–2604, November.
Neil R. Smalheiser. 2012. Literature-based discovery:
Beyond the ABCs. Journal of the American Society
for Information Science and Technology, 63(2):218–
224, February.
Don R. Swanson and Neil R. Smalheiser. 1997. An
interactive system for finding complementary liter-
atures: a stimulus to scientific discovery. Artificial
Intelligence, 91(2):183–203, April.
Don R. Swanson. 1986. Undiscovered public knowl-
edge. The Library Quarterly, 56(2):pp. 103–118.
Don R. Swanson. 1988. Migraine and magnesium:
eleven neglected connections. Perspectives in Biol-
ogy and Medicine, 31(4):526–557.
Don R. Swanson. 1991. Complementary structures
in disjoint science literatures. In SIGIR ’91: Pro-
ceedings of the 14th annual international ACM SI-
GIR conference on Research and development in in-
formation retrieval, pages 280–289, New York, NY,
USA. ACM.
Marc Weeber, Henny Klein, Lolkje T. de Jong van den
Berg, and Rein Vos. 2001. Using concepts in
literature-based discovery: Simulating Swanson’s
Raynaud-fish oil and migraine-magnesium discover-
ies. Journal of the American Society for Information
Science and Technology, 52(7):548–557.
Bartłomiej Wilkowski, Marcelo Fiszman, Christo-
pher M. Miller, Dimitar Hristovski, Sivaram Ara-
bandi, Graciela Rosemblat, and Thomas C. Rind-
flesch. 2011. Graph-based methods for discovery
browsing with semantic predications. AMIA ... An-
nual Symposium proceedings / AMIA Symposium.
AMIA Symposium, 2011:1514–1523.
Jonathan D. Wren, Raffi Bekeredjian, Jelena A. Stew-
art, Ralph V. Shohet, and Harold R. Garner. 2004.
Knowledge discovery by automated identification
and ranking of implicit relationships. Bioinformat-
ics (Oxford, England), 20(3):389–398, February.
Jonathan D. Wren. 2004. Extending the mutual infor-
mation measure to rank inferred literature relation-
ships. BMC bioinformatics, 5, October.
Meliha Yetisgen-Yildiz and Wanda Pratt. 2006. Us-
ing statistical and knowledge-based approaches for
literature-based discovery. Journal of Biomedical
Informatics, 39(6):600–611, December.
Meliha Yetisgen-Yildiz and Wanda Pratt. 2009. A new
evaluation methodology for literature-based discov-
ery systems. Journal of biomedical informatics,
42(4):633–643, August.
</reference>
<page confidence="0.997786">
10
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.585345">
<title confidence="0.987107">Literature-based discovery for Oceanographic climate science</title>
<author confidence="0.997529">Elias</author>
<affiliation confidence="0.999858">Department of Informatics and Computer Norwegian University of Science and</affiliation>
<address confidence="0.597342">Trondheim,</address>
<email confidence="0.9969">eliasaa@stud.ntnu.no</email>
<abstract confidence="0.9995932">This paper presents an overview of the field of literature-based discovery, as originally applied in biomedicine. Furthermore it identifies some of the challenges to employing the results of the field in a new domain, namely oceanographic climate science, and elaborates on some of the research that needs to be conducted to overcome these challenges.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alan R Aronson</author>
<author>Franc¸ois-Michel M Lang</author>
</authors>
<title>An overview of MetaMap: historical perspective and 12Mandarin Chinese for “archer”, a reference to Arrowsmith. recent advances.</title>
<date>2010</date>
<journal>Journal of the American Medical Informatics Association : JAMIA,</journal>
<volume>17</volume>
<issue>3</issue>
<contexts>
<context position="17046" citStr="Aronson and Lang, 2010" startWordPosition="2734" endWordPosition="2737">stly, synonyms and spelling variants are mapped to the same semantic concept. Secondly, using concepts allows for ranking and filtering according to semantic categories. Finally, it becomes easier to constrain the search space by removing spurious or irrelevant n-grams at an early stage, as they don’t map to any concept in the domain. On the other hand, concept extraction from raw text is a non-trivial operation. In LBD concept extraction is conducted in one of two ways: One option is to use NLP tools designed for entity recognition. The most commonly used in the biomedical domain is MetaMap (Aronson and Lang, 2010), which extracts concepts from the Unified Medical Language System (UMLS) meta-thesaurus5. The other option is to use Medical Subject Headings (MeSH)6. MeSH is a controlled vocabulary for indexing biomedical papers, with which all Medline papers have been manually tagged. MeSH keywords can be queried directly from the Medline API. Both MeSH and UMLS terms are organized hierarchically according to semantic categories. 2.2.1 DAD In their system, DAD (Disease-Adverse reactionDrug), Weeber et al. (2001) use MetaMap. They showed in an experiment that the number of concepts extracted is significantl</context>
</contexts>
<marker>Aronson, Lang, 2010</marker>
<rawString>Alan R. Aronson and Franc¸ois-Michel M. Lang. 2010. An overview of MetaMap: historical perspective and 12Mandarin Chinese for “archer”, a reference to Arrowsmith. recent advances. Journal of the American Medical Informatics Association : JAMIA, 17(3):229–236, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Delroy Cameron</author>
<author>Olivier Bodenreider</author>
<author>Hima Yalamanchili</author>
<author>Tu Danh</author>
<author>Sreeram Vallabhaneni</author>
<author>Krishnaprasad Thirunarayan</author>
<author>Amit P Sheth</author>
<author>Thomas C Rindflesch</author>
</authors>
<title>A graph-based recovery and decomposition of swanson’s hypothesis using semantic predications.</title>
<date>2013</date>
<journal>J. of Biomedical Informatics,</journal>
<volume>46</volume>
<issue>2</issue>
<contexts>
<context position="7652" citStr="Cameron et al. (2013)" startWordPosition="1219" endWordPosition="1222"> considered these to hold the most compact knowledge, but it has become the standard approach in LBD to use abstracts and possibly index terms in addition to the titles. The motivation for this is that abstracts and index terms contain more knowledge than only titles. Somewhat surprisingly, few LBD systems use full paper texts. Schuemie et al. (2004) show that 30-40% of all information contained in a section is new to that section, meaning that significant amounts of knowledge is lost when only looking at abstracts and index terms of a paper. The need for full text data is also pointed out by Cameron et al. (2013). The reason for not using full text seems to be that paper abstracts and index terms are available in xml format through the Pubmed API, while full paper texts require accessing rights and are normally stored as pdf. In co-occurrence based systems, a relation x → y is postulated if x and y exhibit a high degree of co-occurrence in L(x), either in terms of absolute frequency of co-occurrence, or in terms of statistical unlikelihood given the statistical promiscuity of the two concepts. While a few systems use the sentence as the domain for counting cooccurrences, most systems count co-occurren</context>
</contexts>
<marker>Cameron, Bodenreider, Yalamanchili, Danh, Vallabhaneni, Thirunarayan, Sheth, Rindflesch, 2013</marker>
<rawString>Delroy Cameron, Olivier Bodenreider, Hima Yalamanchili, Tu Danh, Sreeram Vallabhaneni, Krishnaprasad Thirunarayan, Amit P. Sheth, and Thomas C. Rindflesch. 2013. A graph-based recovery and decomposition of swanson’s hypothesis using semantic predications. J. of Biomedical Informatics, 46(2):238–251, April.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Trevor Cohen</author>
<author>Dominic Widdows</author>
<author>Roger W Schvaneveldt</author>
<author>Peter Davies</author>
<author>Thomas C Rindflesch</author>
</authors>
<marker>Cohen, Widdows, Schvaneveldt, Davies, Rindflesch, </marker>
<rawString>Trevor Cohen, Dominic Widdows, Roger W. Schvaneveldt, Peter Davies, and Thomas C. Rindflesch.</rawString>
</citation>
<citation valid="true">
<authors>
<author>2012a</author>
</authors>
<title>Discovering discovery patterns with predication-based Semantic Indexing.</title>
<date></date>
<journal>Journal of Biomedical Informatics,</journal>
<volume>45</volume>
<issue>6</issue>
<contexts>
<context position="22767" citStr="(2012)" startWordPosition="3629" endWordPosition="3629"> extraction based method instead of or in addition to co-occurrence based methods for candidate generation and ranking. Other researchers take an approach motivated by Wren’s observation that a small-world property holds in the network of concept relations in literature. As significant portions of the concept-relation space will have to be explored in a two-step search anyway, it might be better to extract all relations from 5 the entire literature collection or from a random sample thereof, and rather focus on valid and efficient reasoning within the entire concept-relation space. Smalheiser (2012) critiques the usage of relation extraction in LBD and claims that while reasoning over explicit relations may lead to so-called incremental discoveries, that is, discoveries that lie close to the existing knowledge and therefore are less interesting, they are not able to lead to any radical discoveries, that is discoveries that seem unlikely at time of discovery. He also claims that human discoveries, both incremental and radical, tend to be on a higher level, using analogies and abstract similarities rather than explicit relations, and that the benefit from using relation extraction therefor</context>
</contexts>
<marker>2012a, </marker>
<rawString>2012a. Discovering discovery patterns with predication-based Semantic Indexing. Journal of Biomedical Informatics, 45(6):1049–1065, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohen</author>
<author>Dominic Widdows</author>
<author>Lance Vine</author>
<author>Roger Schvaneveldt</author>
<author>Thomas C Rindflesch</author>
</authors>
<title>Many Paths Lead to Discovery: Analogical Retrieval of Cancer Therapies.</title>
<date>2012</date>
<booktitle>In Quantum Interaction,</booktitle>
<volume>7620</volume>
<pages>90--101</pages>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="26452" citStr="Cohen et al. (2012" startWordPosition="4226" endWordPosition="4229">rom Medline for all relations containing the a concept. The user then incrementally expands the graph by selecting which terms to query relations for from a list of concepts ranked by their degree centrality (i.e. their degree of connectivity in the graph). After graph construction, potential discovery paths are ranked according to summed degree centrality. Although some work has been conducted in graph-based LBD, seemingly no research has been conducted on LBD in a global, large-scale predications graph derived from all of Medline, or a sample of it. 2.3.3 Predication-based semantic indexing Cohen et al. (2012a) propose a hyperdimensional computing technique they call predication-based semantic indexing (PSI) for efficient representation and reasoning in the concept-relation space. In PSI, concepts and relations are represented as high-dimensional vectors, where the semantic content of a concept’s vector is a combination of all the relations it occurs in and all the concepts it is related to, weighted by the frequency of the relation. The system uses SemRep to extract relations from a sample of 8,182,882 Medline records as input to the training process. Inference in this hyperdimensional space can </context>
</contexts>
<marker>Cohen, Widdows, Vine, Schvaneveldt, Rindflesch, 2012</marker>
<rawString>Trevor Cohen, Dominic Widdows, Lance Vine, Roger Schvaneveldt, and Thomas C. Rindflesch. 2012b. Many Paths Lead to Discovery: Analogical Retrieval of Cancer Therapies. In Quantum Interaction, volume 7620 of Lecture Notes in Computer Science, pages 90–101. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph A Digiacomo</author>
<author>Joel M Kremer</author>
<author>Dhiraj M Shah</author>
</authors>
<title>Fish-oil dietary supplementation in patients with Raynaud’s phenomenon: A double-blind, controlled, prospective study.</title>
<date>1989</date>
<journal>The American Journal of Medicine,</journal>
<volume>86</volume>
<issue>2</issue>
<contexts>
<context position="3376" citStr="Digiacomo et al., 1989" startWordPosition="530" endWordPosition="533">serted a —* b, and a disjoint literature L2 asserted b —* c, then the concept denoted by b could function as a bridge between L1 and L2, leading to the discovery of the hypothesis a —* c2. One example given by Swanson showed that fish oils reduced blood viscosity (fish oil —* blood viscosity), and that patients of Raynaud’s disease tend to exhibit high blood viscosity (blood viscosity —* Raynaud). These two facts led to the hypothesis that fish oils can be used in the treatment of Raynaud’s disease (fish oil —* Raynaud) when combined. This hypothesis was subsequently confirmed experimentally (Digiacomo et al., 1989). Although the inference steps are not logically sound, the procedure is able to produce interesting results. The general approach of bridging dis2A note on terminology: In the LBD literature, capital letters are normally used for the A, B and C concepts. In this paper, minuscules will be used to represent individual concepts, while capital letters represent sets. Also, some authors use A to denote the the goal concept, and C for the starting concept. This paper follows the most commonly used terminology, in which a always denotes the starting concept, and c denotes the goal concept. 1 Proceed</context>
</contexts>
<marker>Digiacomo, Kremer, Shah, 1989</marker>
<rawString>Ralph A. Digiacomo, Joel M. Kremer, and Dhiraj M. Shah. 1989. Fish-oil dietary supplementation in patients with Raynaud’s phenomenon: A double-blind, controlled, prospective study. The American Journal of Medicine, 86(2):158–164, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Murat C Ganiz</author>
<author>William M Pottenger</author>
<author>Christopher D Janneck</author>
</authors>
<title>Recent Advances in Literature Based Discovery.</title>
<date>2006</date>
<journal>In Journal of the American Society for Information Science and Technology.</journal>
<contexts>
<context position="33023" citStr="Ganiz et al. (2006)" startWordPosition="5246" endWordPosition="5249">erns such as maybe treats (as mentioned in 2.3.1). The discovery patterns developed for oceanographic climate science target the interactions between directional change events (increase or reduce) in quantitative variables, such as An increase in CO2 causes a decrease in ocean pH. The types of interactions targeted by these discovery patterns have a more complex structure than the binary relations that define maybe treats. Because most relation extraction tools extract only binary relations, it seems that simply adapting existing relation extraction tools to the domain will not be sufficient. Ganiz et al. (2006) discusses that LBD lacks a solid theoretic foundation, as most research is applied, rather than theoretical in nature. Although some inquiry has been conducted into the nature of discoveries (Smalheiser, 2012), there is little knowledge about which properties are required to hold in the domain for the LBD methods to be applicable, but the current work assumes that all scientific disciplines are sufficiently similar for LBD methods to be useful. 4 Research directions The lack of available knowledge resources and NLP tools for the domain makes it hard to directly employ any of the knowledge int</context>
</contexts>
<marker>Ganiz, Pottenger, Janneck, 2006</marker>
<rawString>Murat C. Ganiz, William M. Pottenger, and Christopher D. Janneck. 2006. Recent Advances in Literature Based Discovery. In Journal of the American Society for Information Science and Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael D Gordon</author>
<author>Susan Dumais</author>
</authors>
<title>Using latent semantic indexing for literature based discovery.</title>
<date>1998</date>
<journal>Journal of theAmerican Society for Information Science and Technology,</journal>
<volume>49</volume>
<issue>8</issue>
<contexts>
<context position="11831" citStr="Gordon and Dumais (1998)" startWordPosition="1899" endWordPosition="1902">d indirect connections between a and c to the number of expected connections in a random network model, given the relative promiscuity of the intermediary terms. In another paper, Wren (2004) emphasizes the importance of using a statistically sound method of ranking relationship strengths, such as “chisquare tests, log-likelihood ratios, z-scores or tscores”, because co-occurrence based measures bias towards more general, and thus less interesting relationships. The paper further proposes an extension to the mutual information measure (MIM) as a ranking measure. 2.1.4 Latent semantic indexing Gordon and Dumais (1998) propose exploiting the ability of certain vector-based semantic models such as Latent semantic indexing (LSI) to discover implicit relationships between terms for LBD. They first train the semantic model on L(a), and let the user choose as b one of the terms most similar to a. A new semantic model is built from L(b), and discovery candidates are ranked according to their similarity to a in the L(b)-model. Their experiments showed that the resulting b- and cterm candidate lists closely resemble the lists produced by the information retrieval inspired lexical statistics. In another experiment t</context>
</contexts>
<marker>Gordon, Dumais, 1998</marker>
<rawString>Michael D. Gordon and Susan Dumais. 1998. Using latent semantic indexing for literature based discovery. Journal of theAmerican Society for Information Science and Technology, 49(8):674–685, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael D Gordon</author>
<author>Robert K Lindsay</author>
</authors>
<title>Toward discovery support systems: a replication, re-examination, and extension of Swanson’s work on literature-based discovery of a connection between Raynaud’s and fish oil.</title>
<date>1996</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>47</volume>
<issue>2</issue>
<contexts>
<context position="9564" citStr="Gordon and Lindsay (1996)" startWordPosition="1544" endWordPosition="1547">fter a stop list of approximately 5000 words has been applied. The B-term set is further pruned by removing all the words that have lesser relative frequency in L(a) than in Medline. The potential B terms are subsequently presented to the user, who can then remove words that are thought to be unsuitable. For each bz ∈ B, L(bz) is retrieved and a set CZ is generated, subject to the same stopword and frequency restrictions as before. The terms in the union of the CZ sets are then ranked according to the number of b-terms that connect them to the a-term. 2.1.2 Information retrieval-based methods Gordon and Lindsay (1996) (Lindsay and Gordon, 1999) developed a system in parallel, which differed from Arrowsmith in several ways: Firstly, while Arrowsmith was word-based, their system used n-grams as the unit of analysis. A stop list was applied by removing all n-grams that contained any stop word occurrence. Secondly, their system used entire Medline records, comprising of keywords, abstracts and titles, whereas Arrowsmith only used paper titles. Thirdly, their system employed information retrieval metrics such as tf*idf to find b-terms among the generated candidates, whereas Arrowsmith was based on relative freq</context>
</contexts>
<marker>Gordon, Lindsay, 1996</marker>
<rawString>Michael D. Gordon and Robert K. Lindsay. 1996. Toward discovery support systems: a replication, re-examination, and extension of Swanson’s work on literature-based discovery of a connection between Raynaud’s and fish oil. Journal of the American Society for Information Science and Technology, 47(2):116–128, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Gordon</author>
<author>Robert K Lindsay</author>
<author>Weiguo Fan</author>
</authors>
<title>Literature Based Discovery on the World Wide Web. In</title>
<date>2001</date>
<journal>ACM Transactions on Internet Technology,</journal>
<pages>261--275</pages>
<publisher>ACM Press.</publisher>
<location>New York, USA.</location>
<contexts>
<context position="10325" citStr="Gordon et al. (2001)" startWordPosition="1664" endWordPosition="1667">d-based, their system used n-grams as the unit of analysis. A stop list was applied by removing all n-grams that contained any stop word occurrence. Secondly, their system used entire Medline records, comprising of keywords, abstracts and titles, whereas Arrowsmith only used paper titles. Thirdly, their system employed information retrieval metrics such as tf*idf to find b-terms among the generated candidates, whereas Arrowsmith was based on relative frequencies. The lexical statistical approach is so generic that it lends itself directly to application in different domains. In a later paper, Gordon et al. (2001) employ this approach to conduct LBD searches directly on the World Wide Web, searching for application areas for genetic algorithms. It should however be noted that the goal of this experiment was not LBD in the sense of uncovering undiscovered public knowledge, instead focusing in discovering something that might be “publicly known” but novel to the user. 2.1.3 Ranking metrics Wren et al. (2004) pointed out that the structure of concept co-occurrence relationships is such that most concepts are connected to any other concept within few steps. This small world phenomenon implies that research</context>
</contexts>
<marker>Gordon, Lindsay, Fan, 2001</marker>
<rawString>Michael Gordon, Robert K. Lindsay, and Weiguo Fan. 2001. Literature Based Discovery on the World Wide Web. In ACM Transactions on Internet Technology, pages 261–275, New York, USA. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dimitar Hristovski</author>
<author>J Stare</author>
<author>B Peterlin</author>
<author>S Dzeroski</author>
</authors>
<title>Supporting discovery in medicine by association rule mining</title>
<date>2001</date>
<booktitle>in Medline and UMLS. Studies in health technology and informatics, 84(Pt</booktitle>
<pages>2--1344</pages>
<contexts>
<context position="19952" citStr="Hristovski et al. (2001)" startWordPosition="3176" endWordPosition="3179">oo broad terms (giving the examples medicine, disease and human), (2) too closely related terms (giving the example migraine and headache), and (3) semantically nonsensical connections. The first class is handled by removing any concept if it is strictly more specific in the MeSH ontology hierarchy than any included term. The second class is handled by pruning all links between terms that are closely related (grandparents, parents, siblings and children) in the ontology. The third class is handled by letting the user specify which semantic classes of concepts are allowed to link. 2.2.3 Bitola Hristovski et al. (2001) originally developed a system called Bitola7 that discovered association rules between MeSH terms. Association rules mining is a common data mining method for discovering relations between variables in a database. Association rules are traditionally used for market basket analysis, in which rules of the type 7http://ibmi3.mf.uni-lj.si/bitola/ {pizza, steak} → {coca cola} are inferred, stating that if somebody buys pizza and steak, he/she is likely to buy coca cola as well. In Bitola’s discovery step, basic associations are first mined from the co-occurrence patterns of MeSH terms. Subsequentl</context>
</contexts>
<marker>Hristovski, Stare, Peterlin, Dzeroski, 2001</marker>
<rawString>Dimitar Hristovski, J. Stare, B. Peterlin, and S. Dzeroski. 2001. Supporting discovery in medicine by association rule mining in Medline and UMLS. Studies in health technology and informatics, 84(Pt 2):1344–1348.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dimitar Hristovski</author>
<author>Carol Friedman</author>
<author>Thomas C Rindflesch</author>
<author>Borut Peterlin</author>
</authors>
<title>Exploiting semantic relations for literature-based discovery. AMIA ...</title>
<date>2006</date>
<booktitle>Annual Symposium proceedings /AMIA Symposium. AMIA Symposium,</booktitle>
<pages>349--353</pages>
<contexts>
<context position="20814" citStr="Hristovski et al. (2006)" startWordPosition="3312" endWordPosition="3315">raditionally used for market basket analysis, in which rules of the type 7http://ibmi3.mf.uni-lj.si/bitola/ {pizza, steak} → {coca cola} are inferred, stating that if somebody buys pizza and steak, he/she is likely to buy coca cola as well. In Bitola’s discovery step, basic associations are first mined from the co-occurrence patterns of MeSH terms. Subsequently, indirect associations a → c are inferred by combining association rules on the form a → bz and bz → c, and ranked according to the sum of strengths of the connecting association rules. 2.3 Group 3: Relation extraction-based approaches Hristovski et al. (2006) point out two problems with the co-occurrence based LBD systems: Firstly, no explicit explanation of the relation between the a and c terms is given. Secondly, a large number of spurious relations are discovered, as demonstrated by the low precision values witnessed during system evaluation. Both aspects increase the time needed to examine the output of the system by the human user. They suggest that employing natural language processing (NLP) techniques to extract explicit relations from the papers can improve performance on both points. The biomedical information extraction tool most common</context>
<context position="23443" citStr="Hristovski et al. (2006" startWordPosition="3732" endWordPosition="3735">d claims that while reasoning over explicit relations may lead to so-called incremental discoveries, that is, discoveries that lie close to the existing knowledge and therefore are less interesting, they are not able to lead to any radical discoveries, that is discoveries that seem unlikely at time of discovery. He also claims that human discoveries, both incremental and radical, tend to be on a higher level, using analogies and abstract similarities rather than explicit relations, and that the benefit from using relation extraction therefore is minimal8. 2.3.1 Augmented Bitola In two papers, Hristovski et al. (2006; 2008) experiment with augmenting the Bitola system by using relation extraction tools. In addition to SemRep, they also use another tool, BioMedLee, because each of the tools exhibits better performance than the other on certain types of relations. To guide search through the concept-relation space, they introduce the notion of a discovery pattern. A discovery pattern is a set of concept types and relations between them that could imply an interesting relationship in the domain. One discovery pattern, maybe treats can informally be stated as: If a disease leads to a biological change, and a </context>
</contexts>
<marker>Hristovski, Friedman, Rindflesch, Peterlin, 2006</marker>
<rawString>Dimitar Hristovski, Carol Friedman, Thomas C. Rindflesch, and Borut Peterlin. 2006. Exploiting semantic relations for literature-based discovery. AMIA ... Annual Symposium proceedings /AMIA Symposium. AMIA Symposium, pages 349–353.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dimitar Hristovski</author>
<author>C Friedman</author>
<author>T C Rindflesch</author>
<author>B Peterlin</author>
</authors>
<title>Literature-Based Knowledge Discovery using Natural Language Processing.</title>
<date>2008</date>
<booktitle>of Information Science and Knowledge Management, chapter 9,</booktitle>
<volume>15</volume>
<pages>133--152</pages>
<editor>In Peter Bruza and Marc Weeber, editors, Literaturebased Discovery,</editor>
<publisher>Springer,</publisher>
<location>Heidelberg, Germany.</location>
<marker>Hristovski, Friedman, Rindflesch, Peterlin, 2008</marker>
<rawString>Dimitar Hristovski, C. Friedman, T. C. Rindflesch, and B. Peterlin. 2008. Literature-Based Knowledge Discovery using Natural Language Processing. In Peter Bruza and Marc Weeber, editors, Literaturebased Discovery, volume 15 of Information Science and Knowledge Management, chapter 9, pages 133– 152. Springer, Heidelberg, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald N Kostoff</author>
</authors>
<title>Validating discovery in literature-based discovery.</title>
<date>2007</date>
<journal>Journal of Biomedical Informatics,</journal>
<volume>40</volume>
<issue>4</issue>
<contexts>
<context position="15694" citStr="Kostoff (2007)" startWordPosition="2513" endWordPosition="2514">e metrics, and could lead to results completely different than those reported. Secondly, only a small subset of possible relation generation/ranking techniques and discovery candidate ranking metrics were tested. For example, no relation extraction-based methods (see section 2.3) were included in the evaluation. The evaluation methodology can be critiqued in several ways. Firstly, building the gold standard from the post-cut-off set is problematic for several reasons: A co-occurrence can exist in the post-cutoff set without necessarily corresponding to a new discovery. Also, as pointed out in Kostoff (2007), it is very difficult to verify that a discovery has not been made before the cut-off date. Another problem is that the post-cut-off set only contains discoveries that have been made in the present, all future discoveries are therefore excluded from the gold standard. Secondly, it is not obvious that quantitative measures reflect the usefulness of the LBD system: When at all is said and done, the usefulness of a LBD system equates to its ability to support user in discovering knowledge. 2.2 Group 2: Concept-based approaches Several researchers advocate using domain specific concepts taken fro</context>
</contexts>
<marker>Kostoff, 2007</marker>
<rawString>Ronald N. Kostoff. 2007. Validating discovery in literature-based discovery. Journal of Biomedical Informatics, 40(4):448–450, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert K Lindsay</author>
<author>Michael D Gordon</author>
</authors>
<title>Literature-based discovery by lexical statistics.</title>
<date>1999</date>
<journal>Journal of the American Society for Information Science,</journal>
<pages>574--587</pages>
<contexts>
<context position="9591" citStr="Lindsay and Gordon, 1999" startWordPosition="1548" endWordPosition="1551">mately 5000 words has been applied. The B-term set is further pruned by removing all the words that have lesser relative frequency in L(a) than in Medline. The potential B terms are subsequently presented to the user, who can then remove words that are thought to be unsuitable. For each bz ∈ B, L(bz) is retrieved and a set CZ is generated, subject to the same stopword and frequency restrictions as before. The terms in the union of the CZ sets are then ranked according to the number of b-terms that connect them to the a-term. 2.1.2 Information retrieval-based methods Gordon and Lindsay (1996) (Lindsay and Gordon, 1999) developed a system in parallel, which differed from Arrowsmith in several ways: Firstly, while Arrowsmith was word-based, their system used n-grams as the unit of analysis. A stop list was applied by removing all n-grams that contained any stop word occurrence. Secondly, their system used entire Medline records, comprising of keywords, abstracts and titles, whereas Arrowsmith only used paper titles. Thirdly, their system employed information retrieval metrics such as tf*idf to find b-terms among the generated candidates, whereas Arrowsmith was based on relative frequencies. The lexical statis</context>
</contexts>
<marker>Lindsay, Gordon, 1999</marker>
<rawString>Robert K. Lindsay and Michael D. Gordon. 1999. Literature-based discovery by lexical statistics. Journal of the American Society for Information Science, pages 574–587.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Pedro Domingos</author>
</authors>
<title>Unsupervised semantic parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1 - Volume 1, EMNLP ’09,</booktitle>
<pages>1--10</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="34191" citStr="Poon and Domingos (2009)" startWordPosition="5436" endWordPosition="5439">akes it hard to directly employ any of the knowledge intensive LBD methods. The development of relation extraction tools for the domain falls outside the scope of the current thesis, and therefore so does the application of type 3 approaches. Instead, the current thesis will focus on bridging the gap between the different terminologies and writing styles caused by different backgrounds in the cross-disciplinary field. To this end, I propose using an unsupervised approach to jointly learn a semantic parser and an ontology from the literature, following the approach of Poon and Domingos (2010). Poon and Domingos (2009) show that a semantic parser that is able to make non-trivial abstractions from syntactic structure and word usage can be successfully learned in an unsupervised fashion. The system they describe is for instance able to map passive and active form into the same semantic representation and build realistic synonym hierarchies. One challenge that must be addressed is that the current state-of-the-art clusters words based on their argument frames, leading to highly accurate hierarchical clustering of verbs, but lower performance for nouns as these have less diverse argument frames. One research qu</context>
</contexts>
<marker>Poon, Domingos, 2009</marker>
<rawString>Hoifung Poon and Pedro Domingos. 2009. Unsupervised semantic parsing. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1 - Volume 1, EMNLP ’09, pages 1–10, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Pedro Domingos</author>
</authors>
<title>Unsupervised ontology induction from text.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10,</booktitle>
<pages>296--305</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="34165" citStr="Poon and Domingos (2010)" startWordPosition="5432" endWordPosition="5435">NLP tools for the domain makes it hard to directly employ any of the knowledge intensive LBD methods. The development of relation extraction tools for the domain falls outside the scope of the current thesis, and therefore so does the application of type 3 approaches. Instead, the current thesis will focus on bridging the gap between the different terminologies and writing styles caused by different backgrounds in the cross-disciplinary field. To this end, I propose using an unsupervised approach to jointly learn a semantic parser and an ontology from the literature, following the approach of Poon and Domingos (2010). Poon and Domingos (2009) show that a semantic parser that is able to make non-trivial abstractions from syntactic structure and word usage can be successfully learned in an unsupervised fashion. The system they describe is for instance able to map passive and active form into the same semantic representation and build realistic synonym hierarchies. One challenge that must be addressed is that the current state-of-the-art clusters words based on their argument frames, leading to highly accurate hierarchical clustering of verbs, but lower performance for nouns as these have less diverse argume</context>
</contexts>
<marker>Poon, Domingos, 2010</marker>
<rawString>Hoifung Poon and Pedro Domingos. 2010. Unsupervised ontology induction from text. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 296– 305, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wanda Pratt</author>
<author>Meliha Yetisgen-Yildiz</author>
</authors>
<title>LitLinker: capturing connections across the biomedical literature.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2nd international conference on Knowledge capture, K-CAP ’03,</booktitle>
<pages>105--112</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="18557" citStr="Pratt and Yetisgen-Yildiz (2003)" startWordPosition="2961" endWordPosition="2964">ng the number of search paths significantly. 5http://www.nlm.nih.gov/research/umls/ 6http://www.nlm.nih.gov/mesh/ 4 Their approach was able to replicate both Swanson’s Raynaud’s-fish oil and migraine-magnesium discoveries, but it was discovered that MetaMap maps both mg (milligram) and Mg (magnesium) to the concept magnesium, giving optimistic results for the migraine-magnesium experiment. This is but one example showing that one of the problems with employing NLP tools in an LBD system is that system performance becomes closely tied to the performance of the tools it employs. 2.2.2 LitLinker Pratt and Yetisgen-Yildiz (2003) developed a system, LitLinker, which originally also used MetaMap, but they later found it too computationally expensive for practical use (Yetisgen-Yildiz and Pratt, 2006). MeSH terms are therefore employed instead. In a preprocessing step, LitLinker calculates the co-occurrence patterns of every MeSH term across the literatures of every other MeSh term. For every MeSH term, the mean and standard deviation of co-occurrence counts across the literatures is calculated. In the discovery process, a term is considered to be related to another term if their co-occurrence is higher than statistical</context>
</contexts>
<marker>Pratt, Yetisgen-Yildiz, 2003</marker>
<rawString>Wanda Pratt and Meliha Yetisgen-Yildiz. 2003. LitLinker: capturing connections across the biomedical literature. In Proceedings of the 2nd international conference on Knowledge capture, K-CAP ’03, pages 105–112, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas C Rindflesch</author>
<author>Marcelo Fiszman</author>
</authors>
<title>The interaction of domain knowledge and linguistic structure in natural language processing: interpreting hypernymic propositions in biomedical text.</title>
<date>2003</date>
<journal>Journal of Biomedical Informatics,</journal>
<volume>36</volume>
<issue>6</issue>
<contexts>
<context position="21469" citStr="Rindflesch and Fiszman, 2003" startWordPosition="3416" endWordPosition="3420">th the co-occurrence based LBD systems: Firstly, no explicit explanation of the relation between the a and c terms is given. Secondly, a large number of spurious relations are discovered, as demonstrated by the low precision values witnessed during system evaluation. Both aspects increase the time needed to examine the output of the system by the human user. They suggest that employing natural language processing (NLP) techniques to extract explicit relations from the papers can improve performance on both points. The biomedical information extraction tool most commonly used in LBD is SemRep (Rindflesch and Fiszman, 2003), which uses linguistically motived rules on top of the ouput from MetaMap and the Xerox POS Tagger to extract knowledge in the form of &lt; subject, predicate, object &gt; relation triplets. Although the knowledge expressed in natural language is more complex than what can be represented in simple relation triplets, SemRep is able to provide a better approximation to the knowledge content of scientific papers than do cooccurrence based methods. While most LBD research employs the same NLP tool, systems differ as to how the extracted relations are represented and how reasoning is conducted in the re</context>
</contexts>
<marker>Rindflesch, Fiszman, 2003</marker>
<rawString>Thomas C. Rindflesch and Marcelo Fiszman. 2003. The interaction of domain knowledge and linguistic structure in natural language processing: interpreting hypernymic propositions in biomedical text. Journal of Biomedical Informatics, 36(6):462–477, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Schuemie</author>
<author>M Weeber</author>
<author>B J Schijvenaars</author>
<author>E M van Mulligen</author>
<author>C C van der Eijk</author>
<author>R Jelier</author>
<author>B Mons</author>
<author>J A Kors</author>
</authors>
<title>Distribution of information in biomedical abstracts and full-text publications.</title>
<date>2004</date>
<journal>Bioinformatics,</journal>
<volume>20</volume>
<issue>16</issue>
<marker>Schuemie, Weeber, Schijvenaars, van Mulligen, van der Eijk, Jelier, Mons, Kors, 2004</marker>
<rawString>M. J. Schuemie, M. Weeber, B. J. Schijvenaars, E. M. van Mulligen, C. C. van der Eijk, R. Jelier, B. Mons, and J. A. Kors. 2004. Distribution of information in biomedical abstracts and full-text publications. Bioinformatics, 20(16):2597–2604, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Neil R Smalheiser</author>
</authors>
<title>Literature-based discovery: Beyond the ABCs.</title>
<date>2012</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>63</volume>
<issue>2</issue>
<pages>224</pages>
<contexts>
<context position="22767" citStr="Smalheiser (2012)" startWordPosition="3628" endWordPosition="3629">se relation extraction based method instead of or in addition to co-occurrence based methods for candidate generation and ranking. Other researchers take an approach motivated by Wren’s observation that a small-world property holds in the network of concept relations in literature. As significant portions of the concept-relation space will have to be explored in a two-step search anyway, it might be better to extract all relations from 5 the entire literature collection or from a random sample thereof, and rather focus on valid and efficient reasoning within the entire concept-relation space. Smalheiser (2012) critiques the usage of relation extraction in LBD and claims that while reasoning over explicit relations may lead to so-called incremental discoveries, that is, discoveries that lie close to the existing knowledge and therefore are less interesting, they are not able to lead to any radical discoveries, that is discoveries that seem unlikely at time of discovery. He also claims that human discoveries, both incremental and radical, tend to be on a higher level, using analogies and abstract similarities rather than explicit relations, and that the benefit from using relation extraction therefor</context>
<context position="33233" citStr="Smalheiser, 2012" startWordPosition="5280" endWordPosition="5281"> variables, such as An increase in CO2 causes a decrease in ocean pH. The types of interactions targeted by these discovery patterns have a more complex structure than the binary relations that define maybe treats. Because most relation extraction tools extract only binary relations, it seems that simply adapting existing relation extraction tools to the domain will not be sufficient. Ganiz et al. (2006) discusses that LBD lacks a solid theoretic foundation, as most research is applied, rather than theoretical in nature. Although some inquiry has been conducted into the nature of discoveries (Smalheiser, 2012), there is little knowledge about which properties are required to hold in the domain for the LBD methods to be applicable, but the current work assumes that all scientific disciplines are sufficiently similar for LBD methods to be useful. 4 Research directions The lack of available knowledge resources and NLP tools for the domain makes it hard to directly employ any of the knowledge intensive LBD methods. The development of relation extraction tools for the domain falls outside the scope of the current thesis, and therefore so does the application of type 3 approaches. Instead, the current th</context>
</contexts>
<marker>Smalheiser, 2012</marker>
<rawString>Neil R. Smalheiser. 2012. Literature-based discovery: Beyond the ABCs. Journal of the American Society for Information Science and Technology, 63(2):218– 224, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Don R Swanson</author>
<author>Neil R Smalheiser</author>
</authors>
<title>An interactive system for finding complementary literatures: a stimulus to scientific discovery.</title>
<date>1997</date>
<journal>Artificial Intelligence,</journal>
<volume>91</volume>
<issue>2</issue>
<contexts>
<context position="4363" citStr="Swanson and Smalheiser (1997)" startWordPosition="687" endWordPosition="690">nt sets. Also, some authors use A to denote the the goal concept, and C for the starting concept. This paper follows the most commonly used terminology, in which a always denotes the starting concept, and c denotes the goal concept. 1 Proceedings of the Student Research Workshop at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 1–10, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics joint literatures by means of intermediary terms has been dubbed Swanson linking, and is also referred to as the ABC model. Swanson and Smalheiser (1997) explain that the discovery of the ABC structure and the fish oilRaynaud’s disease connection happened accidentally. This discovery led Swanson to conduct literature searches aided by existing information retrieval tools to search for more undiscovered public knowledge using the ABC model, resulting in the discovery of eleven connections between migraine and magnesium (Swanson, 1988). As the discovery process was extremely time consuming, requiring the researcher to read hundreds of papers, Swanson later developed a computational tool, Arrowsmith, to streamline the discovery process. There are</context>
<context position="8708" citStr="Swanson and Smalheiser, 1997" startWordPosition="1396" endWordPosition="1399">kelihood given the statistical promiscuity of the two concepts. While a few systems use the sentence as the domain for counting cooccurrences, most systems count co-occurrences across entire abstracts. To present the user with only potential new discoveries, most LBD systems remove from C all terms that are already known to be in a relation with a. In co-occurrence based methods, this is done by removing any (a, c) pairs that exhibit higher degrees if co-occurrence than a predefined threshold (normally 1 co-occurrence) in L(a). 2.1.1 Arrowsmith The original Arrowsmith system works as follows (Swanson and Smalheiser, 1997): L(a) is fetched 4http://www.ncbi.nlm.nih.gov/pubmed/ 2 by conducting a Medline search to retrieve the titles of papers containing a in the title. The set of potential B concepts is extracted as the list of unique words in L(a), after a stop list of approximately 5000 words has been applied. The B-term set is further pruned by removing all the words that have lesser relative frequency in L(a) than in Medline. The potential B terms are subsequently presented to the user, who can then remove words that are thought to be unsuitable. For each bz ∈ B, L(bz) is retrieved and a set CZ is generated, </context>
</contexts>
<marker>Swanson, Smalheiser, 1997</marker>
<rawString>Don R. Swanson and Neil R. Smalheiser. 1997. An interactive system for finding complementary literatures: a stimulus to scientific discovery. Artificial Intelligence, 91(2):183–203, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Don R Swanson</author>
</authors>
<title>Undiscovered public knowledge.</title>
<date>1986</date>
<journal>The Library Quarterly,</journal>
<volume>56</volume>
<issue>2</issue>
<pages>103--118</pages>
<contexts>
<context position="928" citStr="Swanson (1986)" startWordPosition="136" endWordPosition="137">n biomedicine. Furthermore it identifies some of the challenges to employing the results of the field in a new domain, namely oceanographic climate science, and elaborates on some of the research that needs to be conducted to overcome these challenges. 1 Introduction The increase in growth rate of the scientific literature over the past decades has forced researchers to become increasingly specialized in order to keep up with the state of the art. This inevitably leads to the fragmentation of science as researchers from different (sub-)disciplines rarely have time to read each other’s papers. Swanson (1986) claimed that this fragmentation of science can lead to undiscovered public knowledge: Conclusions that can be made from existing literature, but have never been made because the knowledge fragments have been discovered in separate (sub-)disciplines. Adopting the terminology of Swanson (1991), a literature can be informally defined as a collection of papers with a significant amount of cross-citation related to a single topic. Two literatures are complementary if they contain knowledge fragments which can be combined to form new knowledge, and disjoint if they have no articles in common, and e</context>
<context position="2717" citStr="Swanson (1986)" startWordPosition="415" endWordPosition="416">domain. This paper will present an overview of some of the research in LBD, and discuss some of the challenges in reproducing the results made in the LBD field in a different domain, namely oceanographic climate science. The structure of this paper is as follows: Section 2 will give an overview of the LBD field, section 3 will discuss differences between the biomedical domain and that of oceanographic climate science, and section 4 will discuss directions for research that will be conducted in order to adapt LBD methods to the oceanographic climate science domain. 2 Literature-based discovery Swanson (1986) observed that if a literature L1 asserted a —* b, and a disjoint literature L2 asserted b —* c, then the concept denoted by b could function as a bridge between L1 and L2, leading to the discovery of the hypothesis a —* c2. One example given by Swanson showed that fish oils reduced blood viscosity (fish oil —* blood viscosity), and that patients of Raynaud’s disease tend to exhibit high blood viscosity (blood viscosity —* Raynaud). These two facts led to the hypothesis that fish oils can be used in the treatment of Raynaud’s disease (fish oil —* Raynaud) when combined. This hypothesis was sub</context>
</contexts>
<marker>Swanson, 1986</marker>
<rawString>Don R. Swanson. 1986. Undiscovered public knowledge. The Library Quarterly, 56(2):pp. 103–118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Don R Swanson</author>
</authors>
<title>Migraine and magnesium: eleven neglected connections.</title>
<date>1988</date>
<booktitle>Perspectives in Biology and Medicine,</booktitle>
<pages>31--4</pages>
<contexts>
<context position="4749" citStr="Swanson, 1988" startWordPosition="749" endWordPosition="750">Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics joint literatures by means of intermediary terms has been dubbed Swanson linking, and is also referred to as the ABC model. Swanson and Smalheiser (1997) explain that the discovery of the ABC structure and the fish oilRaynaud’s disease connection happened accidentally. This discovery led Swanson to conduct literature searches aided by existing information retrieval tools to search for more undiscovered public knowledge using the ABC model, resulting in the discovery of eleven connections between migraine and magnesium (Swanson, 1988). As the discovery process was extremely time consuming, requiring the researcher to read hundreds of papers, Swanson later developed a computational tool, Arrowsmith, to streamline the discovery process. There are two modes of discovery in the ABC model: Open discovery and closed discovery. In open discovery, the researcher only knows the starting concept a, and is interested in uncovering undiscovered public knowledge related to a. A researcher who looks for consequences of ocean acidification might conduct an open discovery search with a = ocean acidification. In closed-discovery, the resea</context>
</contexts>
<marker>Swanson, 1988</marker>
<rawString>Don R. Swanson. 1988. Migraine and magnesium: eleven neglected connections. Perspectives in Biology and Medicine, 31(4):526–557.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Don R Swanson</author>
</authors>
<title>Complementary structures in disjoint science literatures.</title>
<date>1991</date>
<booktitle>In SIGIR ’91: Proceedings of the 14th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>280--289</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1221" citStr="Swanson (1991)" startWordPosition="179" endWordPosition="180">te of the scientific literature over the past decades has forced researchers to become increasingly specialized in order to keep up with the state of the art. This inevitably leads to the fragmentation of science as researchers from different (sub-)disciplines rarely have time to read each other’s papers. Swanson (1986) claimed that this fragmentation of science can lead to undiscovered public knowledge: Conclusions that can be made from existing literature, but have never been made because the knowledge fragments have been discovered in separate (sub-)disciplines. Adopting the terminology of Swanson (1991), a literature can be informally defined as a collection of papers with a significant amount of cross-citation related to a single topic. Two literatures are complementary if they contain knowledge fragments which can be combined to form new knowledge, and disjoint if they have no articles in common, and exhibit little or no cross-citation. The implicit hypothesis is that such complementary but disjoint (CBD) literatures are common, giving rise to significant amounts of undiscovered public knowledge. The field of Literature-based Discovery (LBD)1 focuses on the development and application of c</context>
</contexts>
<marker>Swanson, 1991</marker>
<rawString>Don R. Swanson. 1991. Complementary structures in disjoint science literatures. In SIGIR ’91: Proceedings of the 14th annual international ACM SIGIR conference on Research and development in information retrieval, pages 280–289, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Weeber</author>
<author>Henny Klein</author>
<author>Lolkje T de Jong van den Berg</author>
<author>Rein Vos</author>
</authors>
<title>Using concepts in literature-based discovery: Simulating Swanson’s Raynaud-fish oil and migraine-magnesium discoveries.</title>
<date>2001</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>52</volume>
<issue>7</issue>
<marker>Weeber, Klein, van den Berg, Vos, 2001</marker>
<rawString>Marc Weeber, Henny Klein, Lolkje T. de Jong van den Berg, and Rein Vos. 2001. Using concepts in literature-based discovery: Simulating Swanson’s Raynaud-fish oil and migraine-magnesium discoveries. Journal of the American Society for Information Science and Technology, 52(7):548–557.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bartłomiej Wilkowski</author>
<author>Marcelo Fiszman</author>
<author>Christopher M Miller</author>
<author>Dimitar Hristovski</author>
<author>Sivaram Arabandi</author>
<author>Graciela Rosemblat</author>
<author>Thomas C Rindflesch</author>
</authors>
<title>Graph-based methods for discovery browsing with semantic predications.</title>
<date>2011</date>
<booktitle>AMIA ... Annual Symposium proceedings / AMIA Symposium. AMIA Symposium,</booktitle>
<pages>2011--1514</pages>
<contexts>
<context position="25728" citStr="Wilkowski et al. (2011)" startWordPosition="4113" endWordPosition="4116">Graph-based reasoning The extracted relations can be represented as a Predications Graph in which each concept is represented by a node and each relation is a labelled, directed edge from the subject concept to the object concept. Representing the concept-relation space as a graph provides two benefits: As a visual tool, a graph can display the knowledge extracted by the system in a way that is easily understood by the user and can be navigated/explored easily. As a mathematical object, one can employ graph theoretic results when developing algorithms for the reasoning process. In the work of Wilkowski et al. (2011) an initial graph is constructed by querying a pre-compiled database of predications extracted by SemRep from Medline for all relations containing the a concept. The user then incrementally expands the graph by selecting which terms to query relations for from a list of concepts ranked by their degree centrality (i.e. their degree of connectivity in the graph). After graph construction, potential discovery paths are ranked according to summed degree centrality. Although some work has been conducted in graph-based LBD, seemingly no research has been conducted on LBD in a global, large-scale pre</context>
</contexts>
<marker>Wilkowski, Fiszman, Miller, Hristovski, Arabandi, Rosemblat, Rindflesch, 2011</marker>
<rawString>Bartłomiej Wilkowski, Marcelo Fiszman, Christopher M. Miller, Dimitar Hristovski, Sivaram Arabandi, Graciela Rosemblat, and Thomas C. Rindflesch. 2011. Graph-based methods for discovery browsing with semantic predications. AMIA ... Annual Symposium proceedings / AMIA Symposium. AMIA Symposium, 2011:1514–1523.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan D Wren</author>
<author>Raffi Bekeredjian</author>
<author>Jelena A Stewart</author>
<author>Ralph V Shohet</author>
<author>Harold R Garner</author>
</authors>
<title>Knowledge discovery by automated identification and ranking of implicit relationships. Bioinformatics</title>
<date>2004</date>
<location>(Oxford, England), 20(3):389–398,</location>
<contexts>
<context position="10725" citStr="Wren et al. (2004)" startWordPosition="1733" endWordPosition="1736">candidates, whereas Arrowsmith was based on relative frequencies. The lexical statistical approach is so generic that it lends itself directly to application in different domains. In a later paper, Gordon et al. (2001) employ this approach to conduct LBD searches directly on the World Wide Web, searching for application areas for genetic algorithms. It should however be noted that the goal of this experiment was not LBD in the sense of uncovering undiscovered public knowledge, instead focusing in discovering something that might be “publicly known” but novel to the user. 2.1.3 Ranking metrics Wren et al. (2004) pointed out that the structure of concept co-occurrence relationships is such that most concepts are connected to any other concept within few steps. This small world phenomenon implies that research focus should be shifted away from retrieving discovery candidates to ranking them, because a significant portion of the concept space will be retrieved even within two cooccurrence relation steps. The paper proposes ranking implicit relationships by comparing the number of observed indirect connections between a and c to the number of expected connections in a random network model, given the rela</context>
</contexts>
<marker>Wren, Bekeredjian, Stewart, Shohet, Garner, 2004</marker>
<rawString>Jonathan D. Wren, Raffi Bekeredjian, Jelena A. Stewart, Ralph V. Shohet, and Harold R. Garner. 2004. Knowledge discovery by automated identification and ranking of implicit relationships. Bioinformatics (Oxford, England), 20(3):389–398, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan D Wren</author>
</authors>
<title>Extending the mutual information measure to rank inferred literature relationships.</title>
<date>2004</date>
<journal>BMC bioinformatics,</journal>
<volume>5</volume>
<contexts>
<context position="11398" citStr="Wren (2004)" startWordPosition="1839" endWordPosition="1840">ionships is such that most concepts are connected to any other concept within few steps. This small world phenomenon implies that research focus should be shifted away from retrieving discovery candidates to ranking them, because a significant portion of the concept space will be retrieved even within two cooccurrence relation steps. The paper proposes ranking implicit relationships by comparing the number of observed indirect connections between a and c to the number of expected connections in a random network model, given the relative promiscuity of the intermediary terms. In another paper, Wren (2004) emphasizes the importance of using a statistically sound method of ranking relationship strengths, such as “chisquare tests, log-likelihood ratios, z-scores or tscores”, because co-occurrence based measures bias towards more general, and thus less interesting relationships. The paper further proposes an extension to the mutual information measure (MIM) as a ranking measure. 2.1.4 Latent semantic indexing Gordon and Dumais (1998) propose exploiting the ability of certain vector-based semantic models such as Latent semantic indexing (LSI) to discover implicit relationships between terms for LBD</context>
</contexts>
<marker>Wren, 2004</marker>
<rawString>Jonathan D. Wren. 2004. Extending the mutual information measure to rank inferred literature relationships. BMC bioinformatics, 5, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Meliha Yetisgen-Yildiz</author>
<author>Wanda Pratt</author>
</authors>
<title>Using statistical and knowledge-based approaches for literature-based discovery.</title>
<date>2006</date>
<journal>Journal of Biomedical Informatics,</journal>
<volume>39</volume>
<issue>6</issue>
<contexts>
<context position="18730" citStr="Yetisgen-Yildiz and Pratt, 2006" startWordPosition="2986" endWordPosition="2989">ud’s-fish oil and migraine-magnesium discoveries, but it was discovered that MetaMap maps both mg (milligram) and Mg (magnesium) to the concept magnesium, giving optimistic results for the migraine-magnesium experiment. This is but one example showing that one of the problems with employing NLP tools in an LBD system is that system performance becomes closely tied to the performance of the tools it employs. 2.2.2 LitLinker Pratt and Yetisgen-Yildiz (2003) developed a system, LitLinker, which originally also used MetaMap, but they later found it too computationally expensive for practical use (Yetisgen-Yildiz and Pratt, 2006). MeSH terms are therefore employed instead. In a preprocessing step, LitLinker calculates the co-occurrence patterns of every MeSH term across the literatures of every other MeSh term. For every MeSH term, the mean and standard deviation of co-occurrence counts across the literatures is calculated. In the discovery process, a term is considered to be related to another term if their co-occurrence is higher than statistically expected, based on its z-score. Yetisgen-Yildiz and Pratt identified three classes of uninteresting links and terms that should be pruned automatically by system: (1) too</context>
</contexts>
<marker>Yetisgen-Yildiz, Pratt, 2006</marker>
<rawString>Meliha Yetisgen-Yildiz and Wanda Pratt. 2006. Using statistical and knowledge-based approaches for literature-based discovery. Journal of Biomedical Informatics, 39(6):600–611, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Meliha Yetisgen-Yildiz</author>
<author>Wanda Pratt</author>
</authors>
<title>A new evaluation methodology for literature-based discovery systems.</title>
<date>2009</date>
<journal>Journal of biomedical informatics,</journal>
<volume>42</volume>
<issue>4</issue>
<contexts>
<context position="13518" citStr="Yetisgen-Yildiz and Pratt (2009)" startWordPosition="2172" endWordPosition="2175"> 2.1.5 Evaluation efforts LBD has a tradition for questionable evaluation effort. The original discoveries in LBD were made manually by Swanson, and most computational systems are evaluated solely according to their ability to replicate one or more of Swanson’s discoveries. This is problematic for several reasons: First of all, Swanson’s discoveries were never intended as a gold standard, and being able to accomplish a single task that is known in advance does not mean that the results are generalizable. 3 Secondly, there is no quantitative basis for comparing different approaches or metrics. Yetisgen-Yildiz and Pratt (2009) conducted the first systematic quantitative evaluation of discovery candidate ranking metrics and relation ranking/generation techniques. They partitioned Medline into two parts, according to a cut-off date. LBD was conducted on the pre-cut-off set, and the post-cut-off set was used as a gold standard to compute precision and recall. In the post-cutoff set, a connection was considered to exist if two terms co-occurred in any document. The ranking metrics that were evaluated were Linking term count (LTC), that is the number of b-terms connecting a and c, Average minimum weight (AMW), that is t</context>
<context position="37435" citStr="Yetisgen-Yildiz and Pratt (2009)" startWordPosition="5967" endWordPosition="5970"> showing that tf-idf gives high recall at the cost of mediocre precision (see section 2.1.5). Because the system is intended to be augmented by relation extraction tools in the future, recall is favoured over precision, as precision is expected to increase in the final version. The discovery candidates are ranked by the number of paths connecting them to a, also motivated by the quantitative experiments described in section 2.1.5. Houyi will be evaluated quantitatively by comparing performance on a data set divided into training and test data by a cut-off date, following the approach taken by Yetisgen-Yildiz and Pratt (2009). As discussed in section 2.1.5, this is not a perfect evaluation procedure, but it will at least give an indication as to whether unsupervised semantic parsing and ontology building contributes to LBD performance. The baseline system, Sheshou12, will use the same ranking metric and candidate generation mechanism as Houyi, and uses the NPs extracted by the Stanford Parser as terms. Development of domain specific ontologies and relation extraction tools is required to apply type 3 LBD methods in the domain. Although outside the scope of the current thesis, it is expected that the resulting sema</context>
</contexts>
<marker>Yetisgen-Yildiz, Pratt, 2009</marker>
<rawString>Meliha Yetisgen-Yildiz and Wanda Pratt. 2009. A new evaluation methodology for literature-based discovery systems. Journal of biomedical informatics, 42(4):633–643, August.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>