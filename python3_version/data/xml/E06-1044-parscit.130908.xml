<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000547">
<title confidence="0.983525">
Modelling Semantic Role Plausibility in Human Sentence Processing
</title>
<author confidence="0.549034">
Ulrike Padó and Matthew Crocker
</author>
<affiliation confidence="0.42643">
Computational Linguistics
Saarland University
</affiliation>
<address confidence="0.554735">
66041 Saarbrücken
Germany
</address>
<email confidence="0.991314">
{ulrike,crocker}@coli.uni-sb.de
</email>
<author confidence="0.923434">
Frank Keller
</author>
<affiliation confidence="0.9713725">
School of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.7942715">
2 Buccleuch Place
Edinburgh EH8 9LW, UK
</address>
<email confidence="0.998756">
keller@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.998599" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999925">
We present the psycholinguistically moti-
vated task of predicting human plausibility
judgements for verb-role-argument triples
and introduce a probabilistic model that
solves it. We also evaluate our model on
the related role-labelling task, and com-
pare it with a standard role labeller. For
both tasks, our model benefits from class-
based smoothing, which allows it to make
correct argument-specific predictions de-
spite a severe sparse data problem. The
standard labeller suffers from sparse data
and a strong reliance on syntactic cues, es-
pecially in the prediction task.
</bodyText>
<sectionHeader confidence="0.999515" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998998389830509">
Computational psycholinguistics is concerned
with modelling human language processing.
Much work has gone into the exploration of sen-
tence comprehension. Syntactic preferences that
unfold during the course of the sentence have been
successfully modelled using incremental proba-
bilistic context-free parsing models (e.g., Jurafsky,
1996; Crocker and Brants, 2000). These models
assume that humans prefer the most likely struc-
tural alternative at each point in the sentence. If
the preferred structure changes during processing,
such models correctly predict processing difficulty
for a range of experimentally investigated con-
structions. They do not, however, incorporate an
explicit notion of semantic processing, while there
are many phenomena in human sentence process-
ing that demonstrate a non-trivial interaction of
syntactic preferences and semantic plausibility.
Consider, for example, the well-studied case of
reduced relative clause constructions. When incre-
mentally processing the sentence The deer shot by
the hunter was used as a trophy, there is a local
ambiguity at shot between continuation as a main
clause (as in The deer shot the hunter) or as a re-
duced relative clause modifying deer (equivalent
to The deer which was shot ... ). The main clause
continuation is syntactically more likely.
However, there is a second, semantic clue pro-
vided by the high plausibility of deer being shot
and the low plausibility of them shooting. This
influences readers to choose the syntactically dis-
preferred reduced relative reading which interprets
the deer as an object of shot (McRae et al., 1998).
Plausibility has overridden the syntactic default.
On the other hand, for a sentence like The hunter
shot by the teenager was only 30 years old, se-
mantic plausibility initially reinforces the syntac-
tic main clause preference and readers show diffi-
culty accommodating the subsequent disambigua-
tion towards the reduced relative.
In order to model effects like these, we need
to extend existing models of sentence process-
ing by introducing a semantic dimension. Pos-
sible ways of integrating different sources of in-
formation have been presented e.g. by McRae
et al. (1998) and Narayanan and Jurafsky (2002).
Our aim is to formulate a model that reliably pre-
dicts human plausibility judgements from corpus
resources, in parallel to the standard practice of
basing the syntax component of psycholinguistic
models on corpus probabilities or even probabilis-
tic treebank grammars. We can then use both the
syntactic likelihood and the semantic plausibility
score to predict the preferred syntactic alterna-
tive, thus accounting for the effects shown e.g. by
McRae et al. (1998).
Independent of a syntactic model, we want any
semantic model we define to satisfy two criteria:
First, it needs to be able to make predictions in-
</bodyText>
<page confidence="0.99802">
345
</page>
<bodyText confidence="0.999992833333333">
crementally, in parallel with the syntactic model.
This entails dealing with incomplete or unspeci-
fied (syntactic) information. Second, we want to
extend to semantics the assumption made in syn-
tactic models that the most probable alternative is
the one preferred by humans. The model therefore
must be probabilistic.
We present such a probabilistic model that can
assign roles incrementally as soon as a predicate-
argument pair is seen. It uses the likelihood of the-
matic role assignments to model human interpre-
tation of verb-argument relations. Thematic roles
are a description of the link between verb and ar-
gument at the interface between syntax and se-
mantics. Thus, they provide a shallow level of
sentence semantics which can be learnt from an-
notated corpora.
We evaluate our model by verifying that it in-
deed correctly predicts human judgements, and by
comparing its performance with that of a standard
role labeller in terms of both judgement prediction
and role assignment. Our model has two advan-
tages over the standard labeller: It does not rely
on syntactic features (which can be hard to come
by in an incremental task) and our smoothing ap-
proach allows it to make argument-specific role
predictions in spite of extremely sparse training
data. We conclude that (a) our model solves the
task we set, and (b) our model is better equipped
for our task than a standard role labeller.
The outline of the paper is as follows: After
defining the prediction task more concretely (Sec-
tion 2), we present our simple probabilistic model
that is tailoured to the task (Section 3). We in-
troduce our test and training data in Section 4. It
becomes evident immediately that we face a se-
vere sparse data problem, which we tackle on two
levels: By smoothing the distribution and by ac-
quiring additional counts for sparse cases. The
smoothed model succeeds on the prediction task
(Section 5). Finally, in Section 6, we compare our
model to a standard role labeller.
</bodyText>
<sectionHeader confidence="0.949231" genericHeader="method">
2 The Judgement Prediction Task
</sectionHeader>
<bodyText confidence="0.998711285714286">
We can measure our intuitions about the plau-
sibility of hunters shooting and deer being shot
in terms of plausibility judgements for verb-role-
argument triples. Two example items from McRae
et al. (1998) are presented in Table 1. The judge-
ments were gathered by asking raters to assign a
value on a scale from 1 (not plausible) to 7 (very
</bodyText>
<table confidence="0.9552916">
Verb Noun Role Rating
shoot hunter agent 6.9
shoot hunter patient 2.8
shoot deer agent 1.0
shoot deer patient 6.4
</table>
<tableCaption confidence="0.962566">
Table 1: Test items: Verb-noun pairs with ratings
on a 7 point scale from McRae et al. (1998).
</tableCaption>
<bodyText confidence="0.995111352941177">
plausible) to questions like How common is it for
a hunter to shoot something? (subject reading:
hunter must be agent) or How common is it for a
hunter to be shot? (object reading: hunter must be
patient). The number of ratings available in each
of our three sets of ratings is given in Table 2 (see
also Section 4).
The task for our model is to correctly predict the
plausibility of each verb-role-argument triple. We
evaluate this by correlating the model’s predicted
values and the judgements. The judgement data
is not normally distributed, so we correlate using
Spearman’s ρ (a non-parametric rank-order test).
The ρ value ranges between 0 and 1 and indicates
the strength of association between the two vari-
ables. A significant positive value indicates that
the model’s predictions are accurate.
</bodyText>
<sectionHeader confidence="0.9888335" genericHeader="method">
3 A Model of Human Plausibility
Judgements
</sectionHeader>
<bodyText confidence="0.9999658">
We can formulate a model to solve the prediction
task if we equate the plausibility of a role assign-
ment to a verb-argument pair with its probability,
as suggested above. This value is influenced as
well by the verb’s semantic class and the grammat-
ical function of the argument. The plausibility for
a verb-role-argument triple can thus be estimated
as the joint probability of the argument head a, the
role r, the verb v, the verb’s semantic class c and
the grammatical function gf of a:
</bodyText>
<subsubsectionHeader confidence="0.53607">
Plausibilityv,r,a = P(r,a,v,c,g f )
</subsubsectionHeader>
<bodyText confidence="0.995332">
This joint probability cannot be easily estimated
from co-occurrence counts due to lack of data.
But we can decompose this term into a number
of subterms that approximate intuitively impor-
tant information such as syntactic subcategorisa-
tion (P(g f |v,c)), the syntactic realisation of a se-
mantic role (P(r|v,c,gf)) and selectional prefer-
</bodyText>
<equation confidence="0.87210325">
ences (P(a|v,c,gf,r)):
Plausibilityv,r,a = P(r,a,v,c,gf) =
P(v) · P(c|v) · P(gf |v,c) ·
P(r|v,c,gf) · P(a|v,c,gf,r)
</equation>
<page confidence="0.994389">
346
</page>
<note confidence="0.697551">
shoot.02: [The hunter Arg0] shot [the deer Arg1].
Killing: [The hunter Killer] shot [the deer Victim].
</note>
<figureCaption confidence="0.865501">
Figure 1: Example annotation: PropBank (above)
and FrameNet (below).
</figureCaption>
<bodyText confidence="0.999954117647059">
Each of these subterms can be estimated more eas-
ily from the semantically annotated training data
simply using the maximum likelihood estimate.
However, we still need to smooth our estimates,
especially as the P(a|v,c,gf,r) term remains very
sparse. We describe our use of two complemen-
tary smoothing methods in Section 5.
Our model fulfils the requirements we have
specified: It is probabilistic, able to work incre-
mentally as soon as a single verb-argument pair
is available, and can make predictions even if the
input information is incomplete. The model gen-
erates the missing values if, e.g., the grammatical
function or the verb’s semantic class are not spec-
ified. This means that we can immediately evalu-
ate on the judgement data without needing further
verb sense or syntactic information.
</bodyText>
<sectionHeader confidence="0.865917" genericHeader="method">
4 Test and Training data
</sectionHeader>
<bodyText confidence="0.999748866666667">
Training Data To date, there are two main
annotation efforts that have produced semanti-
cally annotated corpora: PropBank (PB) and
FrameNet (FN). Their approaches to annotation
differ enough to warrant a comparison of the cor-
pora as training resources. Figure 1 gives an exam-
ple sentence annotated in PropBank and FrameNet
style. The PropBank corpus (c. 120,000 propo-
sitions, c. 3,000 verbs) adds semantic annotation
to the Wall Street Journal part of the Penn Tree-
bank. Arguments and adjuncts are annotated for
every verbal proposition in the corpus. A common
set of argument labels Arg0 to Argy and ArgM
(adjuncts) is interpreted in a verb-specific way.
Some consistency in mapping has been achieved,
so that Arg0 generally denotes agents and Arg1
patients/themes.
The FrameNet corpus (c. 58,000 verbal propo-
sitions, c. 1,500 verbs in release 1.1) groups verbs
with similar meanings together into frames (i.e.
descriptions of situations) with a set of frame-
specific roles for participants and items involved
(e.g. a killer, instrument and victim in the Killing
frame). Both the definition of frames as semantic
verb classes and the semantic characterisation of
frame-specific roles introduces a level of informa-
tion that is not present in PropBank. Since corpus
annotation is frame-driven, only some senses of a
verb may be present and word frequencies may not
be representative of English.
Test Data Our main data set consists of 160 data
points from McRae et al. (1998) that were split
randomly into a 60 data point development set and
a 100 data point test set. The data is made up of
two arguments per verb and two ratings for each
verb-argument pair, one for the subject and one
for the object reading of the argument (see Section
2). Each argument is highly plausible in one of
the readings, but implausible in the other (recall
Table 1). Human ratings are on a 7-point scale.
In order to further test the coverage of our
model, we also include 76 items from Trueswell
et al. (1994) with one highly plausible object per
verb and a rating each for the subject and object
reading of the argument. The data were gath-
ered in the same rating study as the McRae et
al. data, so we can assume consistency of the rat-
ings. However, in comparison to the McRae data
set, the data is impoverished as it lacks ratings for
plausible agents (in terms of the example in Ta-
ble 1, this means there are no ratings for hunter).
Lastly, we use 180 items from Keller and Lapata
(2003). In contrast with the previous two studies,
the verbs and nouns for these data were not hand-
selected for the plausibility of their combination.
Rather, they were extracted from the BNC corpus
by frequency criteria: Half the verb-noun combi-
nations are seen in the BNC with high, medium
and low frequency, half are unseen combinations
of the verb set with nouns from the BNC. The
data consists of ratings for 30 verbs and 6 argu-
ments each, interpreted as objects. The human
ratings were gathered using the Magnitude Esti-
mation technique (Bard et al., 1996). This data
set allows us to test on items that were not hand-
selected for a psycholinguistic study, even though
the data lacks agenthood ratings and the items are
poorly covered by the FrameNet corpus.
All test pairs were hand-annotated with
FrameNet and PropBank roles following the
specifications in the FrameNet on-line database
and the PropBank frames files.1
The judgement prediction task is very hard to
solve if the verb is unseen during training. Back-
ing off to syntactic information or a frequency
</bodyText>
<footnote confidence="0.709457">
1Although a single annotator assigned the roles, the anno-
tation should be reliable as roles were mostly unambiguous
and the annotated corpora were used for reference.
</footnote>
<page confidence="0.983105">
347
</page>
<table confidence="0.976205">
Total Revised
Source FN PB
McRae et al. (1998) 100 64 (64%) 92 (92%)
Trueswell et al. (1994) 76 52 (68.4%) 72 (94.7%)
Keller and Lapata (2003) 180 – 162 (90%)
</table>
<tableCaption confidence="0.9790815">
Table 2: Test sets: Total number of ratings and size of revised test sets containing only ratings for seen
verbs (% of total ratings). –: Coverage too low (26.7%).
</tableCaption>
<bodyText confidence="0.999632428571429">
baseline only works if the role set is small and syn-
tactically motivated, which is the case for Prop-
Bank, but not FrameNet. We present results both
for the complete test sets and and for revised sets
containing only items with seen verbs. Exclud-
ing unseen verbs seems justified for FrameNet and
has little effect for the PropBank corpus, since its
coverage is generally much better. Table 2 shows
the total number of ratings for each test set and
the sizes of the revised test sets containing only
items with seen verbs. FrameNet always has sub-
stantially lower coverage. Since only 27% of the
verbs in the Keller &amp; Lapata items are covered in
FrameNet, we do not test this combination.
</bodyText>
<sectionHeader confidence="0.984542" genericHeader="method">
5 Experiment 1: Smoothing Methods
</sectionHeader>
<bodyText confidence="0.999496785714286">
We now turn to evaluating our model. It is im-
mediately clear that we have a severe sparse data
problem. Even if all the verbs are seen, the com-
binations of verbs and arguments are still mostly
unseen in training for all data sets.
We describe two complementary approaches
to smoothing sparse training data. One, Good-
Turing smoothing, approaches the problem of un-
seen data points by assigning them a small proba-
bility. The other, class-based smoothing, attempts
to arrive at semantic generalisations for words.
These serve to identify equivalent verb-argument
pairs that furnish additional counts for the estima-
tion of P(a|v,c,g f ,r).
</bodyText>
<subsectionHeader confidence="0.94204">
5.1 Good-Turing Smoothing and Linear
Interpolation
</subsectionHeader>
<bodyText confidence="0.981403357142857">
We first tackle the sparse data problem by smooth-
ing the distribution of co-occurrence counts. We
use the Good-Turing re-estimate on zero and one
counts to assign a small probability to unseen
events. This method relies on re-estimating the
probability of seen and unseen events based on
knowledge about more frequent events.
Adding Linear Interpolation We also exper-
imented with the linear interpolation method,
which is typically used for smoothing n-gram
models. It re-estimates the probability of the n-
gram in question as a weighted combination of the
n-gram, the n-1-gram and the n-2-gram. For ex-
ample, P(a|v,c,gf,r) is interpolated as
</bodyText>
<equation confidence="0.999816">
P(a|v,c,gf,r) = λ1P(a|v,c,gf,r)+
λ2P(a|v,c,r) +λ3P(a|v,c)
</equation>
<bodyText confidence="0.999952058823529">
The λ values were estimated on the training
data, separately for each of the model’s four con-
ditional probability terms, by maximising five-fold
cross-validation likelihood to avoid overfitting.
We smoothed all model terms using the Good-
Turing method and then interpolated the smoothed
terms. Table 3 lists the test results for both train-
ing corpora and all test sets when Good-Turing
smoothing (GT) is used alone and with linear in-
terpolation (GT/LI). We also give the unsmoothed
coverage and correlation. The need for smoothing
is obvious: Coverage is so low that we can only
compute correlations in two cases, and even for
those, less than 20% of the data are covered.
GT smoothing alone always outperforms the
combination of GT and LI smoothing, especially
for the FrameNet training set. Maximising the
data likelihood during λ estimation does not ap-
proximate our final task well enough: The log
likelihood of the test data is duly improved from
−797.1 to −772.2 for the PropBank data and from
−501.9 to −446.3 for the FrameNet data. How-
ever, especially for the FrameNet training data,
performance on the correlation task diminishes as
data probability rises. A better solution might be
to use the correlation task directly as a λ estima-
tion criterion, but this is much more complex, re-
quiring us to estimate all λ terms simultaneously.
Also, the main problem seems to be that the λ in-
terpolation smoothes by de-emphasising the most
specific (and sparsest) term, so that, on our final
task, the all-important argument-specific informa-
tion is not used efficiently when it is available. We
therefore restrict ourselves to GT smoothing.
</bodyText>
<page confidence="0.991824">
348
</page>
<table confidence="0.966729125">
Smoothed Unsmoothed
Train Smoothing Test Coverage ρ Coverage ρ
PB GT McRae 93.5% (86%) 0.112, ns 2% (2%) –
Trueswell 100% (94.7%) 0.454, ** 17% (16%) ns
Keller&amp;Lapata 100% (90%) 0.285, ** 5% (4%) 0.727, *
GT/LI McRae 93.5% (86%) 0.110, ns 2% (2%) –
Trueswell 100% (94.7%) 0.404, ** 17% (16%) ns
Keller&amp;Lapata 100% (90%) 0.284, ** 5% (4%) 0.727, *
</table>
<tableCaption confidence="0.963276">
Table 3: Experiment 1, GT and Interpolation smoothing. Coverage on seen verbs (and on all items) and
correlation strength (Spearman’s ρ for PB and FN data on all test sets. –: too few data points, ns: not
significant, *: p &lt; 0.05, **: p &lt; 0.01.
</tableCaption>
<figure confidence="0.998701608695652">
FN
GT/LI
GT
Trueswell
Trueswell
McRae
McRae
76.9% (52.6%)
76.9% (52.6%)
87.5% (56%)
87.5% (56%)
0.164, ns
0.046, ns
0.042, ns
0.009, ns
6% (4%)
6% (4%)
6% (4%)
6% (4%)
–
–
–
–
</figure>
<bodyText confidence="0.997943518518518">
Model Performance Both versions of the
smoothed model make predictions for all seen
verbs; the remaining uncovered data points are
those where the correct role is not accounted for
in the training data (the verb may be very sparse
or only seen in a different FrameNet frame). For
the FrameNet training data, there are no significant
correlations, but for the PropBank data, we see
correlations for the Trueswell and Keller&amp;Lapata
sets. One reason for the good performance of
the PB-Trueswell and PB-Keller&amp;Lapata combi-
nations is that in the PropBank training data, the
object role generally seems to be the most likely
one. If the most specific probability term is sparse
and expresses no role preference (which is the case
for most items: see Unsmoothed Coverage), our
model is biased towards the most likely role given
the verb, semantic class and grammatical function.
Recall that the Trueswell and Keller&amp;Lapata data
contain ratings for (plausible) objects only, so that
preferring the patient role is a good strategy. This
also explains why the model performs worse for
the McRae et al. data, which also has ratings for
good agents (and bad patients). On FrameNet, this
preference for “patient” roles is not as marked, so
the FN-Trueswell case does not behave like the
PB-Trueswell case.
</bodyText>
<subsectionHeader confidence="0.991975">
5.2 Class-Based Smoothing
</subsectionHeader>
<bodyText confidence="0.998003947368421">
In addition to smoothing the training distribution,
we also attempt to acquire more counts to es-
timate each P(aJv,c,gf,r) by generalising from
tokens to word classes. The term we estimate
becomes P(classa|classv,g f ,r). This allows us
to make argument-specific predictions as we do
not rely on a uniform smoothed term for unseen
P(a|v,c,gf,r) terms. We use lexicographic noun
classes from WordNet and verb classes induced
by soft unsupervised clustering, which outperform
lexicographic verb classes.
Noun Classes We tested both the coarsest and
the finest noun classification available in Word-
Net, namely the top-level ontology and the noun
synsets which contain only synonyms of the target
word.2 The top-level ontology proved to overgen-
erate alternative nouns, which raises coverage but
does not produce meaningful role predictions. We
therefore use the noun synsets below.
Verb Classes Verbs are clustered according to
linguistic context information, namely argument
head lemmas, the syntactic configuration of verb
and argument, the verb’s semantic class, the gold
role information and a combined feature of gold
role and syntactic configuration. The evaluation of
the clustering task itself is task-based: We choose
the clustering configuration that produces optimal
results in the the prediction task on the McRae de-
velopment set. The base corpus for clustering was
always used for frequency estimation.
We used an implementation of two soft clus-
tering algorithms derived from information the-
ory (Marx, 2004): the Information Distortion (ID)
(Gedeon et al., 2003) and Information Bottleneck
(IB) (Tishby et al., 1999) methods. Soft cluster-
ing allows us to take verb polysemy into account
that is often characterised by different patterns of
syntactic behaviour for each verb meaning.
</bodyText>
<footnote confidence="0.9988515">
2For ambiguous nouns, we chose the sense that led to the
highest probability for the current role assignment.
</footnote>
<page confidence="0.998496">
349
</page>
<bodyText confidence="0.999905095238095">
A number of parameters were set on the devel-
opment set, namely the clustering algorithm, the
smoothing method within the algorithms and the
number of clusters within each run. For our task,
the IB algorithm generally yielded better results.
We decided which clustering parametrisations
should be tried on the test sets based on the notion
of stability: Both algorithms increase the number
of clusters by one at each iteration. Thus, each
parametrisation yields a series of cluster configu-
rations as the number of iterations increases. We
chose those parametrisations where a series of at
least three consecutive cluster configurations re-
turned significant correlations on the development
set. This should be an indication of a generalisable
success, rather than a fluke caused by peculiarities
of the data. On the test sets, results are reported
for the configuration (characterised by the itera-
tion number) that returned the first significant re-
sult in such a series on the development set, as this
is the most general grouping.
</bodyText>
<subsectionHeader confidence="0.999557">
5.3 Combining the Smoothing Methods
</subsectionHeader>
<bodyText confidence="0.99997">
We now present results for combining the GT
and class-based smoothing methods. We use in-
duced verb classes and WordNet noun synsets for
class-based smoothing of P(a|v,c,gf,r), and rely
on GT smoothing if the counts for this term are
still sparse. All other model terms are always
smoothed using the GT method. Table 4 contains
results for three clustering configurations each for
the PropBank and FrameNet data that have proven
stable on the development set. We characterise
them by the clustering algorithm (IB or ID) and
number of clusters. Note that the upper bound for
our ρ values, human agreement or inter-rater cor-
relation, is below 1 (as indicated by a correlation
of Pearson’s r = .640 for the seen pairs from the
Keller and Lapata (2003) data).
For the FrameNet data, there is a marked in-
crease in performance for both test sets. The hu-
man judgements are now reliably predicted with
good coverage in five out of six cases. Clearly,
equivalent verb-argument counts have furnished
accurate item-specific estimates. On the PropBank
data set, class-based smoothing is less helpful: ρ
values generally drop slightly. Apparently, the
FrameNet style of annotation allows us to induce
informative verb classes, whereas the PropBank
classes introduce noise at most.
</bodyText>
<sectionHeader confidence="0.992444" genericHeader="method">
6 Experiment 2: Role Labelling
</sectionHeader>
<bodyText confidence="0.99997228">
We have shown that our model performs well on
its intended task of predicting plausibility judge-
ments, once we have proper smoothing methods
in place. But since this task has some similarity
to role labelling, we can also compare the model
to a standard role labeller on both the prediction
and role labelling tasks. The questions are: How
well do we do labelling, and does a standard role
labeller also predict human judgements?
Beginning with work by Gildea and Jurafsky
(2002), there has been a large interest in se-
mantic role labelling, as evidenced by its adop-
tion as a shared task in the Senseval-III compe-
tition (FrameNet data, Litkowski, 2004) and at
the CoNLL-2004 and 2005 conference (PropBank
data, Carreras and Márquez, 2005). As our model
currently focuses on noun phrase arguments only,
we do not adopt these test sets but continue to use
ours, defining the correct role label to be the one
with the higher probability judgement. We evalu-
ate the model on the McRae test set (recall that the
other sets only contain good patients/themes and
are therefore susceptible to labeller biases).
We formulate frequency baselines for our train-
ing data. For PropBank, always assigning Arg1
results in F = 45.7 (43.8 on the full test set). For
FrameNet, we assign the most frequent role given
the verb, so the baseline is F = 34.4 (26.8).
We base our standard role labelling system on
the SVM labeller described in Giuglea and Mos-
chitti (2004), although without integrating infor-
mation from PropBank and VerbNet for FrameNet
classification as presented in their paper. Thus, we
are left with a set of fairly standard features, such
as phrase type, voice, governing category or path
through parse tree from predicate. These are used
to train two classifiers, one which decides which
phrases should be considered arguments and one
which assigns role labels to these arguments. The
SVM labeller’s F score on an unseen test set is
F = 80.5 for FrameNet data when using gold ar-
gument boundaries. We also trained the labeller
on the PropBank data, resulting in an F score of
F = 98.6 on Section 23, again on gold boundaries.
We also evaluate the SVM labeller on the cor-
relation task by normalising the scores that the la-
beller assigns to each role and then correlating the
normalised scores to the human ratings.
In order to extract features for the SVM labeller,
we had to present the verb-noun pairs in full sen-
</bodyText>
<page confidence="0.987066">
350
</page>
<table confidence="0.9999173125">
Train Test Verb Clusters Coverage ρ
ID 4 93.5% (86%) 0.097, ns
McRae IB 10 93.5% (86%) 0.104, ns
IB 5 93.5% (86%) 0.107, ns
ID 4 100% (94.7%) 0.419, **
PB Trueswell IB 10 100% (94.7%) 0.366, **
IB 5 100% (94.7%) 0.439, **
ID 4 100% (90%) 0.300, **
Keller&amp;Lapata IB 10 100% (90%) 0.255, **
IB 5 100% (90%) 0.297, **
ID 4 87.5% (56%) 0.304, *
McRae IB 9 87.5% (56%) 0.275, *
FN IB 10 87.5% (56%) 0.267, *
ID 4 76.9% (52.6%) 0.256, ns
Trueswell IB 9 76.9% (52.6%) 0.342, *
IB 10 76.9% (52.6%) 0.365, *
</table>
<tableCaption confidence="0.997333">
Table 4: Experiment 1: Combining the smoothing methods. Coverage on seen verbs (and on all items)
</tableCaption>
<bodyText confidence="0.990180765625">
and correlation strength (Spearman’s ρ) for PB and FN data. WN synsets as noun classes. Verb classes:
IB/ID: smoothing algorithm, followed by number of clusters. ns: not significant, *: p&lt;0.05, **: p&lt;0.01
tences, as the labeller relies on a number of fea-
tures from parse trees. We used the experimental
items from the McRae et al. study, which are all
disambiguated towards a reduced relative reading
(object interpretation: The hunter shot by the ...)
of the argument. In doing this, we are potentially
biasing the SVM labeller towards one label, de-
pending on the influence of syntactic features on
role assignment. We therefore also created a main
clause reading of the verb-argument pairs (sub-
ject interpretation: The hunter shot the ...) and
present the results for comparison. For our model,
we have previously not specified the grammatical
function of the argument, but in order to put both
models on a level playing field, we now supply the
grammatical function of Ext (external argument),
which applies for both formulations of the items.
Table 5 shows that for the labelling task, our
model outperforms the labelling baseline and the
SVM labeller on the FrameNet data by at least
16 points F score while the correlation with hu-
man data remains significant. For the PropBank
data, labelling performance is on baseline level,
below the better of the two SVM labeller condi-
tions. This result underscores the usefulness of
argument-specific plausibility estimates furnished
by class-based smoothing for the FrameNet data.
For the PropBank data, our model essentially as-
signs the most frequent role for the verb.
The performance of the SVM labeller suggests
a strong influence of syntactic features: On the
PropBank data set, it always assigns the Arg0 la-
bel if the argument was presented as a subject
(this is correct in 50% of cases) and mostly the
appropriate ArgN label if the argument was pre-
sented as an object. On FrameNet, performance
again is above baseline only for the subject condi-
tion, where there is also a clear trend for assign-
ing agent-style roles. (The object condition is less
clear-cut.) This strong reliance on syntactic cues,
which may be misleading for our data, makes the
labeller perform much worse than on the standard
test sets. For both training corpora, it does not
take word-specific plausibility into account due to
data sparseness and usually assigns the same role
to both arguments of a verb. This precludes a sig-
nificant correlation with the human ratings.
Comparing the training corpora, we find that
both models perform better on the FrameNet data
even though there are many more role labels in
FrameNet, and the SVM labeller does not profit
from the greater smoothing power of FrameNet
verb clusters. Overall, FrameNet has proven more
useful to us, despite its smaller size.
In sum, our model does about as well (PB data)
or better (FN data) on the labelling task as the
SVM labeller, while the labeller does not solve
the prediction task. The success of our model, es-
pecially on the prediction task, stems partly from
the absence of global syntactic features that bias
the standard labeller strongly. This also makes our
model suited for an incremental task. Instead of
</bodyText>
<page confidence="0.996639">
351
</page>
<table confidence="0.999618333333333">
Train Model Coverage ρ Labelling F Labelling Cov.
PB Baseline – – 45.7 (43.8%) 100%
SVM Labeller (subj) 100% (92%) ns 50 (47.9%) 100%
SVM Labeller (obj) 100% (92%) ns 45.7 (43.8%) 100%
IB 5 (subj/obj) 93.5% (86%) ns 45.7 (43.8%) 100%
FN Baseline – – 34.4 (26.8%) 100%
SVM Labeller (subj) 87.5% (56%) ns * 40.6 (31.7%) 100%
SVM Labeller (obj) 87.5% (56%) ns 34.4 (26.8%) 100%
ID 4 (subj/obj) 87.5% (56%) 0.271, 56.3 (43.9%) 100%
</table>
<tableCaption confidence="0.99687">
Table 5: Experiment 2: Standard SVM labeller vs our model. Coverage on seen verbs (and on all items),
</tableCaption>
<bodyText confidence="0.9235255">
correlation strength (Spearman’s ρ), labelling F score and labelling coverage on seen verbs (and on all
items, if different) for PB and FN data on the McRae test set. ns: not significant, *: p&lt;0.05.
syntactic cues, we successfully rely on argument-
specific plausibility estimates furnished by class-
based smoothing. Our joint probability model has
the further advantage of being conceptually much
simpler than the SVM labeller, which relies on a
sophisticated machine learning paradigm. Also,
we need to compute only about one-fifth of the
number of SVM features.
</bodyText>
<sectionHeader confidence="0.99954" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.996825714285714">
We have defined the psycholinguistically moti-
vated task of predicting human plausibility ratings
for verb-role-argument triples. To solve it, we
have presented an incremental probabilistic model
of human plausibility judgements. When we em-
ploy two complementary smoothing methods, the
model achieves both good coverage and reliable
correlations with human data. Our model per-
forms as well as or better than a standard role la-
beller on the task of assigning the preferred role to
each item in our test set. Further, the standard la-
beller does not succeed on the prediction task, as it
cannot overcome the extreme sparse data problem.
Acknowledgements Ulrike Padó acknowledges
a DFG studentship in the International Post-
Graduate College “Language Technology and
Cognitive Systems”. We thank Ana-Maria Giu-
glea, Alessandro Moschitti and Zvika Marx for
making their software available and are grateful to
Amit Dubey, Katrin Erk, Mirella Lapata and Se-
bastian Padó for comments and discussions.
</bodyText>
<sectionHeader confidence="0.999253" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999878857142857">
Bard, E. G., Robertson, D., and Sorace, A. (1996). Magnitude
estimation of linguistic acceptability. Language, 72(1),
32–68.
Carreras, X. and Márquez, L. (2005). Introduction to the
CoNLL-2005 shared task: Semantic role labeling. In Pro-
ceedings of CoNLL-2005.
Crocker, M. and Brants, T. (2000). Wide-coverage proba-
bilistic sentence processing. Journal of Psycholinguistic
Research, 29(6), 647–669.
Gedeon, T., Parker, A., and Dimitrov, A. (2003). Information
distortion and neural coding. Canadian Applied Mathe-
matics Quarterly, 10(1), 33–70.
Gildea, D. and Jurafsky, D. (2002). Automatic labeling of
semantic roles. Computational Linguistics, 28(3), 245–
288.
Giuglea, A.-M. and Moschitti, A. (2004). Knowledge discov-
ery using FrameNet, VerbNet and PropBank. In Proceed-
ings of the Workshop on Ontology and Knowledge Discov-
ering at ECML 2004.
Jurafsky, D. (1996). A probabilistic model of lexical and syn-
tactic access and disambiguation. Cognitive Science, 20,
137–194.
Keller, F. and Lapata, M. (2003). Using the web to obtain fre-
quencies for unseen bigrams. Computational Linguistics,
29(3), 459–484.
Litkowski, K. (2004). Senseval-3 task: Automatic labeling of
semantic roles. In Proceedings of Senseval-3: The Third
International Workshop on the Evaluation of Systems for
the Semantic Analysis of Text.
Marx, Z. (2004). Structure-Based computational aspects of
similarity and analogy in natural language. Ph.D. thesis,
Hebrew University, Jerusalem.
McRae, K., Spivey-Knowlton, M., and Tanenhaus, M.
(1998). Modeling the influence of thematic fit (and other
constraints) in on-line sentence comprehension. Journal
of Memory and Language, 38, 283–312.
Narayanan, S. and Jurafsky, D. (2002). A Bayesian model
predicts human parse preference and reading time in sen-
tence processing. In S. B. T. G. Dietterich and Z. Ghahra-
mani, editors, Advances in Neural Information Processing
Systems 14, pages 59–65. MIT Press.
Tishby, N., Pereira, F. C., and Bialek, W. (1999). The in-
formation bottleneck method. In Proc. of the 37-th An-
nual Allerton Conference on Communication, Control and
Computing, pages 368–377.
Trueswell, J., Tanenhaus, M., and Garnsey, S. (1994). Seman-
tic influences on parsing: Use of thematic role information
in syntactic ambiguity resolution. Journal of Memory and
Language, 33, 285–318.
</reference>
<page confidence="0.998256">
352
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.769755">
<title confidence="0.999771">Modelling Semantic Role Plausibility in Human Sentence Processing</title>
<author confidence="0.996958">Ulrike Padó</author>
<author confidence="0.996958">Matthew Crocker</author>
<affiliation confidence="0.9990895">Computational Linguistics Saarland University</affiliation>
<address confidence="0.992486">66041 Saarbrücken Germany</address>
<email confidence="0.997884">ulrike@coli.uni-sb.de</email>
<email confidence="0.997884">crocker@coli.uni-sb.de</email>
<author confidence="0.999307">Frank Keller</author>
<affiliation confidence="0.939096333333333">School of Informatics University of Edinburgh 2 Buccleuch Place</affiliation>
<address confidence="0.997916">Edinburgh EH8 9LW, UK</address>
<email confidence="0.998106">keller@inf.ed.ac.uk</email>
<abstract confidence="0.997794933333333">We present the psycholinguistically motivated task of predicting human plausibility judgements for verb-role-argument triples and introduce a probabilistic model that solves it. We also evaluate our model on the related role-labelling task, and compare it with a standard role labeller. For both tasks, our model benefits from classbased smoothing, which allows it to make correct argument-specific predictions despite a severe sparse data problem. The standard labeller suffers from sparse data and a strong reliance on syntactic cues, especially in the prediction task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E G Bard</author>
<author>D Robertson</author>
<author>A Sorace</author>
</authors>
<title>Magnitude estimation of linguistic acceptability.</title>
<date>1996</date>
<journal>Language,</journal>
<volume>72</volume>
<issue>1</issue>
<pages>32--68</pages>
<contexts>
<context position="12123" citStr="Bard et al., 1996" startWordPosition="1969" endWordPosition="1972">hunter). Lastly, we use 180 items from Keller and Lapata (2003). In contrast with the previous two studies, the verbs and nouns for these data were not handselected for the plausibility of their combination. Rather, they were extracted from the BNC corpus by frequency criteria: Half the verb-noun combinations are seen in the BNC with high, medium and low frequency, half are unseen combinations of the verb set with nouns from the BNC. The data consists of ratings for 30 verbs and 6 arguments each, interpreted as objects. The human ratings were gathered using the Magnitude Estimation technique (Bard et al., 1996). This data set allows us to test on items that were not handselected for a psycholinguistic study, even though the data lacks agenthood ratings and the items are poorly covered by the FrameNet corpus. All test pairs were hand-annotated with FrameNet and PropBank roles following the specifications in the FrameNet on-line database and the PropBank frames files.1 The judgement prediction task is very hard to solve if the verb is unseen during training. Backing off to syntactic information or a frequency 1Although a single annotator assigned the roles, the annotation should be reliable as roles w</context>
</contexts>
<marker>Bard, Robertson, Sorace, 1996</marker>
<rawString>Bard, E. G., Robertson, D., and Sorace, A. (1996). Magnitude estimation of linguistic acceptability. Language, 72(1), 32–68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Carreras</author>
<author>L Márquez</author>
</authors>
<title>Introduction to the CoNLL-2005 shared task: Semantic role labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of CoNLL-2005.</booktitle>
<contexts>
<context position="23964" citStr="Carreras and Márquez, 2005" startWordPosition="3902" endWordPosition="3905"> have proper smoothing methods in place. But since this task has some similarity to role labelling, we can also compare the model to a standard role labeller on both the prediction and role labelling tasks. The questions are: How well do we do labelling, and does a standard role labeller also predict human judgements? Beginning with work by Gildea and Jurafsky (2002), there has been a large interest in semantic role labelling, as evidenced by its adoption as a shared task in the Senseval-III competition (FrameNet data, Litkowski, 2004) and at the CoNLL-2004 and 2005 conference (PropBank data, Carreras and Márquez, 2005). As our model currently focuses on noun phrase arguments only, we do not adopt these test sets but continue to use ours, defining the correct role label to be the one with the higher probability judgement. We evaluate the model on the McRae test set (recall that the other sets only contain good patients/themes and are therefore susceptible to labeller biases). We formulate frequency baselines for our training data. For PropBank, always assigning Arg1 results in F = 45.7 (43.8 on the full test set). For FrameNet, we assign the most frequent role given the verb, so the baseline is F = 34.4 (26.</context>
</contexts>
<marker>Carreras, Márquez, 2005</marker>
<rawString>Carreras, X. and Márquez, L. (2005). Introduction to the CoNLL-2005 shared task: Semantic role labeling. In Proceedings of CoNLL-2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Crocker</author>
<author>T Brants</author>
</authors>
<title>Wide-coverage probabilistic sentence processing.</title>
<date>2000</date>
<journal>Journal of Psycholinguistic Research,</journal>
<volume>29</volume>
<issue>6</issue>
<pages>647--669</pages>
<contexts>
<context position="1279" citStr="Crocker and Brants, 2000" startWordPosition="171" endWordPosition="174">om classbased smoothing, which allows it to make correct argument-specific predictions despite a severe sparse data problem. The standard labeller suffers from sparse data and a strong reliance on syntactic cues, especially in the prediction task. 1 Introduction Computational psycholinguistics is concerned with modelling human language processing. Much work has gone into the exploration of sentence comprehension. Syntactic preferences that unfold during the course of the sentence have been successfully modelled using incremental probabilistic context-free parsing models (e.g., Jurafsky, 1996; Crocker and Brants, 2000). These models assume that humans prefer the most likely structural alternative at each point in the sentence. If the preferred structure changes during processing, such models correctly predict processing difficulty for a range of experimentally investigated constructions. They do not, however, incorporate an explicit notion of semantic processing, while there are many phenomena in human sentence processing that demonstrate a non-trivial interaction of syntactic preferences and semantic plausibility. Consider, for example, the well-studied case of reduced relative clause constructions. When i</context>
</contexts>
<marker>Crocker, Brants, 2000</marker>
<rawString>Crocker, M. and Brants, T. (2000). Wide-coverage probabilistic sentence processing. Journal of Psycholinguistic Research, 29(6), 647–669.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Gedeon</author>
<author>A Parker</author>
<author>A Dimitrov</author>
</authors>
<title>Information distortion and neural coding.</title>
<date>2003</date>
<journal>Canadian Applied Mathematics Quarterly,</journal>
<volume>10</volume>
<issue>1</issue>
<pages>33--70</pages>
<contexts>
<context position="20528" citStr="Gedeon et al., 2003" startWordPosition="3345" endWordPosition="3348">ion, namely argument head lemmas, the syntactic configuration of verb and argument, the verb’s semantic class, the gold role information and a combined feature of gold role and syntactic configuration. The evaluation of the clustering task itself is task-based: We choose the clustering configuration that produces optimal results in the the prediction task on the McRae development set. The base corpus for clustering was always used for frequency estimation. We used an implementation of two soft clustering algorithms derived from information theory (Marx, 2004): the Information Distortion (ID) (Gedeon et al., 2003) and Information Bottleneck (IB) (Tishby et al., 1999) methods. Soft clustering allows us to take verb polysemy into account that is often characterised by different patterns of syntactic behaviour for each verb meaning. 2For ambiguous nouns, we chose the sense that led to the highest probability for the current role assignment. 349 A number of parameters were set on the development set, namely the clustering algorithm, the smoothing method within the algorithms and the number of clusters within each run. For our task, the IB algorithm generally yielded better results. We decided which cluster</context>
</contexts>
<marker>Gedeon, Parker, Dimitrov, 2003</marker>
<rawString>Gedeon, T., Parker, A., and Dimitrov, A. (2003). Information distortion and neural coding. Canadian Applied Mathematics Quarterly, 10(1), 33–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
<author>D Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<pages>245--288</pages>
<contexts>
<context position="23706" citStr="Gildea and Jurafsky (2002)" startWordPosition="3859" endWordPosition="3862">e of annotation allows us to induce informative verb classes, whereas the PropBank classes introduce noise at most. 6 Experiment 2: Role Labelling We have shown that our model performs well on its intended task of predicting plausibility judgements, once we have proper smoothing methods in place. But since this task has some similarity to role labelling, we can also compare the model to a standard role labeller on both the prediction and role labelling tasks. The questions are: How well do we do labelling, and does a standard role labeller also predict human judgements? Beginning with work by Gildea and Jurafsky (2002), there has been a large interest in semantic role labelling, as evidenced by its adoption as a shared task in the Senseval-III competition (FrameNet data, Litkowski, 2004) and at the CoNLL-2004 and 2005 conference (PropBank data, Carreras and Márquez, 2005). As our model currently focuses on noun phrase arguments only, we do not adopt these test sets but continue to use ours, defining the correct role label to be the one with the higher probability judgement. We evaluate the model on the McRae test set (recall that the other sets only contain good patients/themes and are therefore susceptible</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Gildea, D. and Jurafsky, D. (2002). Automatic labeling of semantic roles. Computational Linguistics, 28(3), 245– 288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A-M Giuglea</author>
<author>A Moschitti</author>
</authors>
<title>Knowledge discovery using FrameNet, VerbNet and PropBank.</title>
<date>2004</date>
<booktitle>In Proceedings of the Workshop on Ontology and Knowledge Discovering at ECML</booktitle>
<contexts>
<context position="24672" citStr="Giuglea and Moschitti (2004)" startWordPosition="4025" endWordPosition="4029">these test sets but continue to use ours, defining the correct role label to be the one with the higher probability judgement. We evaluate the model on the McRae test set (recall that the other sets only contain good patients/themes and are therefore susceptible to labeller biases). We formulate frequency baselines for our training data. For PropBank, always assigning Arg1 results in F = 45.7 (43.8 on the full test set). For FrameNet, we assign the most frequent role given the verb, so the baseline is F = 34.4 (26.8). We base our standard role labelling system on the SVM labeller described in Giuglea and Moschitti (2004), although without integrating information from PropBank and VerbNet for FrameNet classification as presented in their paper. Thus, we are left with a set of fairly standard features, such as phrase type, voice, governing category or path through parse tree from predicate. These are used to train two classifiers, one which decides which phrases should be considered arguments and one which assigns role labels to these arguments. The SVM labeller’s F score on an unseen test set is F = 80.5 for FrameNet data when using gold argument boundaries. We also trained the labeller on the PropBank data, r</context>
</contexts>
<marker>Giuglea, Moschitti, 2004</marker>
<rawString>Giuglea, A.-M. and Moschitti, A. (2004). Knowledge discovery using FrameNet, VerbNet and PropBank. In Proceedings of the Workshop on Ontology and Knowledge Discovering at ECML 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Jurafsky</author>
</authors>
<title>A probabilistic model of lexical and syntactic access and disambiguation.</title>
<date>1996</date>
<journal>Cognitive Science,</journal>
<volume>20</volume>
<pages>137--194</pages>
<contexts>
<context position="1252" citStr="Jurafsky, 1996" startWordPosition="169" endWordPosition="170">odel benefits from classbased smoothing, which allows it to make correct argument-specific predictions despite a severe sparse data problem. The standard labeller suffers from sparse data and a strong reliance on syntactic cues, especially in the prediction task. 1 Introduction Computational psycholinguistics is concerned with modelling human language processing. Much work has gone into the exploration of sentence comprehension. Syntactic preferences that unfold during the course of the sentence have been successfully modelled using incremental probabilistic context-free parsing models (e.g., Jurafsky, 1996; Crocker and Brants, 2000). These models assume that humans prefer the most likely structural alternative at each point in the sentence. If the preferred structure changes during processing, such models correctly predict processing difficulty for a range of experimentally investigated constructions. They do not, however, incorporate an explicit notion of semantic processing, while there are many phenomena in human sentence processing that demonstrate a non-trivial interaction of syntactic preferences and semantic plausibility. Consider, for example, the well-studied case of reduced relative c</context>
</contexts>
<marker>Jurafsky, 1996</marker>
<rawString>Jurafsky, D. (1996). A probabilistic model of lexical and syntactic access and disambiguation. Cognitive Science, 20, 137–194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Keller</author>
<author>M Lapata</author>
</authors>
<title>Using the web to obtain frequencies for unseen bigrams.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>3</issue>
<pages>459--484</pages>
<contexts>
<context position="11568" citStr="Keller and Lapata (2003)" startWordPosition="1874" endWordPosition="1877"> Human ratings are on a 7-point scale. In order to further test the coverage of our model, we also include 76 items from Trueswell et al. (1994) with one highly plausible object per verb and a rating each for the subject and object reading of the argument. The data were gathered in the same rating study as the McRae et al. data, so we can assume consistency of the ratings. However, in comparison to the McRae data set, the data is impoverished as it lacks ratings for plausible agents (in terms of the example in Table 1, this means there are no ratings for hunter). Lastly, we use 180 items from Keller and Lapata (2003). In contrast with the previous two studies, the verbs and nouns for these data were not handselected for the plausibility of their combination. Rather, they were extracted from the BNC corpus by frequency criteria: Half the verb-noun combinations are seen in the BNC with high, medium and low frequency, half are unseen combinations of the verb set with nouns from the BNC. The data consists of ratings for 30 verbs and 6 arguments each, interpreted as objects. The human ratings were gathered using the Magnitude Estimation technique (Bard et al., 1996). This data set allows us to test on items th</context>
<context position="12943" citStr="Keller and Lapata (2003)" startWordPosition="2106" endWordPosition="2109">rpus. All test pairs were hand-annotated with FrameNet and PropBank roles following the specifications in the FrameNet on-line database and the PropBank frames files.1 The judgement prediction task is very hard to solve if the verb is unseen during training. Backing off to syntactic information or a frequency 1Although a single annotator assigned the roles, the annotation should be reliable as roles were mostly unambiguous and the annotated corpora were used for reference. 347 Total Revised Source FN PB McRae et al. (1998) 100 64 (64%) 92 (92%) Trueswell et al. (1994) 76 52 (68.4%) 72 (94.7%) Keller and Lapata (2003) 180 – 162 (90%) Table 2: Test sets: Total number of ratings and size of revised test sets containing only ratings for seen verbs (% of total ratings). –: Coverage too low (26.7%). baseline only works if the role set is small and syntactically motivated, which is the case for PropBank, but not FrameNet. We present results both for the complete test sets and and for revised sets containing only items with seen verbs. Excluding unseen verbs seems justified for FrameNet and has little effect for the PropBank corpus, since its coverage is generally much better. Table 2 shows the total number of ra</context>
<context position="22676" citStr="Keller and Lapata (2003)" startWordPosition="3694" endWordPosition="3697"> synsets for class-based smoothing of P(a|v,c,gf,r), and rely on GT smoothing if the counts for this term are still sparse. All other model terms are always smoothed using the GT method. Table 4 contains results for three clustering configurations each for the PropBank and FrameNet data that have proven stable on the development set. We characterise them by the clustering algorithm (IB or ID) and number of clusters. Note that the upper bound for our ρ values, human agreement or inter-rater correlation, is below 1 (as indicated by a correlation of Pearson’s r = .640 for the seen pairs from the Keller and Lapata (2003) data). For the FrameNet data, there is a marked increase in performance for both test sets. The human judgements are now reliably predicted with good coverage in five out of six cases. Clearly, equivalent verb-argument counts have furnished accurate item-specific estimates. On the PropBank data set, class-based smoothing is less helpful: ρ values generally drop slightly. Apparently, the FrameNet style of annotation allows us to induce informative verb classes, whereas the PropBank classes introduce noise at most. 6 Experiment 2: Role Labelling We have shown that our model performs well on its</context>
</contexts>
<marker>Keller, Lapata, 2003</marker>
<rawString>Keller, F. and Lapata, M. (2003). Using the web to obtain frequencies for unseen bigrams. Computational Linguistics, 29(3), 459–484.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Litkowski</author>
</authors>
<title>Senseval-3 task: Automatic labeling of semantic roles.</title>
<date>2004</date>
<booktitle>In Proceedings of Senseval-3: The Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text.</booktitle>
<contexts>
<context position="23878" citStr="Litkowski, 2004" startWordPosition="3891" endWordPosition="3892">ms well on its intended task of predicting plausibility judgements, once we have proper smoothing methods in place. But since this task has some similarity to role labelling, we can also compare the model to a standard role labeller on both the prediction and role labelling tasks. The questions are: How well do we do labelling, and does a standard role labeller also predict human judgements? Beginning with work by Gildea and Jurafsky (2002), there has been a large interest in semantic role labelling, as evidenced by its adoption as a shared task in the Senseval-III competition (FrameNet data, Litkowski, 2004) and at the CoNLL-2004 and 2005 conference (PropBank data, Carreras and Márquez, 2005). As our model currently focuses on noun phrase arguments only, we do not adopt these test sets but continue to use ours, defining the correct role label to be the one with the higher probability judgement. We evaluate the model on the McRae test set (recall that the other sets only contain good patients/themes and are therefore susceptible to labeller biases). We formulate frequency baselines for our training data. For PropBank, always assigning Arg1 results in F = 45.7 (43.8 on the full test set). For Frame</context>
</contexts>
<marker>Litkowski, 2004</marker>
<rawString>Litkowski, K. (2004). Senseval-3 task: Automatic labeling of semantic roles. In Proceedings of Senseval-3: The Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Marx</author>
</authors>
<title>Structure-Based computational aspects of similarity and analogy in natural language.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>Hebrew University,</institution>
<location>Jerusalem.</location>
<contexts>
<context position="20473" citStr="Marx, 2004" startWordPosition="3339" endWordPosition="3340">tered according to linguistic context information, namely argument head lemmas, the syntactic configuration of verb and argument, the verb’s semantic class, the gold role information and a combined feature of gold role and syntactic configuration. The evaluation of the clustering task itself is task-based: We choose the clustering configuration that produces optimal results in the the prediction task on the McRae development set. The base corpus for clustering was always used for frequency estimation. We used an implementation of two soft clustering algorithms derived from information theory (Marx, 2004): the Information Distortion (ID) (Gedeon et al., 2003) and Information Bottleneck (IB) (Tishby et al., 1999) methods. Soft clustering allows us to take verb polysemy into account that is often characterised by different patterns of syntactic behaviour for each verb meaning. 2For ambiguous nouns, we chose the sense that led to the highest probability for the current role assignment. 349 A number of parameters were set on the development set, namely the clustering algorithm, the smoothing method within the algorithms and the number of clusters within each run. For our task, the IB algorithm gen</context>
</contexts>
<marker>Marx, 2004</marker>
<rawString>Marx, Z. (2004). Structure-Based computational aspects of similarity and analogy in natural language. Ph.D. thesis, Hebrew University, Jerusalem.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K McRae</author>
<author>M Spivey-Knowlton</author>
<author>M Tanenhaus</author>
</authors>
<title>Modeling the influence of thematic fit (and other constraints) in on-line sentence comprehension.</title>
<date>1998</date>
<journal>Journal of Memory and Language,</journal>
<volume>38</volume>
<pages>283--312</pages>
<contexts>
<context position="2519" citStr="McRae et al., 1998" startWordPosition="365" endWordPosition="368">ng the sentence The deer shot by the hunter was used as a trophy, there is a local ambiguity at shot between continuation as a main clause (as in The deer shot the hunter) or as a reduced relative clause modifying deer (equivalent to The deer which was shot ... ). The main clause continuation is syntactically more likely. However, there is a second, semantic clue provided by the high plausibility of deer being shot and the low plausibility of them shooting. This influences readers to choose the syntactically dispreferred reduced relative reading which interprets the deer as an object of shot (McRae et al., 1998). Plausibility has overridden the syntactic default. On the other hand, for a sentence like The hunter shot by the teenager was only 30 years old, semantic plausibility initially reinforces the syntactic main clause preference and readers show difficulty accommodating the subsequent disambiguation towards the reduced relative. In order to model effects like these, we need to extend existing models of sentence processing by introducing a semantic dimension. Possible ways of integrating different sources of information have been presented e.g. by McRae et al. (1998) and Narayanan and Jurafsky (2</context>
<context position="5933" citStr="McRae et al. (1998)" startWordPosition="927" endWordPosition="930">n 3). We introduce our test and training data in Section 4. It becomes evident immediately that we face a severe sparse data problem, which we tackle on two levels: By smoothing the distribution and by acquiring additional counts for sparse cases. The smoothed model succeeds on the prediction task (Section 5). Finally, in Section 6, we compare our model to a standard role labeller. 2 The Judgement Prediction Task We can measure our intuitions about the plausibility of hunters shooting and deer being shot in terms of plausibility judgements for verb-roleargument triples. Two example items from McRae et al. (1998) are presented in Table 1. The judgements were gathered by asking raters to assign a value on a scale from 1 (not plausible) to 7 (very Verb Noun Role Rating shoot hunter agent 6.9 shoot hunter patient 2.8 shoot deer agent 1.0 shoot deer patient 6.4 Table 1: Test items: Verb-noun pairs with ratings on a 7 point scale from McRae et al. (1998). plausible) to questions like How common is it for a hunter to shoot something? (subject reading: hunter must be agent) or How common is it for a hunter to be shot? (object reading: hunter must be patient). The number of ratings available in each of our th</context>
<context position="10571" citStr="McRae et al. (1998)" startWordPosition="1687" endWordPosition="1690">ps verbs with similar meanings together into frames (i.e. descriptions of situations) with a set of framespecific roles for participants and items involved (e.g. a killer, instrument and victim in the Killing frame). Both the definition of frames as semantic verb classes and the semantic characterisation of frame-specific roles introduces a level of information that is not present in PropBank. Since corpus annotation is frame-driven, only some senses of a verb may be present and word frequencies may not be representative of English. Test Data Our main data set consists of 160 data points from McRae et al. (1998) that were split randomly into a 60 data point development set and a 100 data point test set. The data is made up of two arguments per verb and two ratings for each verb-argument pair, one for the subject and one for the object reading of the argument (see Section 2). Each argument is highly plausible in one of the readings, but implausible in the other (recall Table 1). Human ratings are on a 7-point scale. In order to further test the coverage of our model, we also include 76 items from Trueswell et al. (1994) with one highly plausible object per verb and a rating each for the subject and ob</context>
<context position="12847" citStr="McRae et al. (1998)" startWordPosition="2088" endWordPosition="2091">though the data lacks agenthood ratings and the items are poorly covered by the FrameNet corpus. All test pairs were hand-annotated with FrameNet and PropBank roles following the specifications in the FrameNet on-line database and the PropBank frames files.1 The judgement prediction task is very hard to solve if the verb is unseen during training. Backing off to syntactic information or a frequency 1Although a single annotator assigned the roles, the annotation should be reliable as roles were mostly unambiguous and the annotated corpora were used for reference. 347 Total Revised Source FN PB McRae et al. (1998) 100 64 (64%) 92 (92%) Trueswell et al. (1994) 76 52 (68.4%) 72 (94.7%) Keller and Lapata (2003) 180 – 162 (90%) Table 2: Test sets: Total number of ratings and size of revised test sets containing only ratings for seen verbs (% of total ratings). –: Coverage too low (26.7%). baseline only works if the role set is small and syntactically motivated, which is the case for PropBank, but not FrameNet. We present results both for the complete test sets and and for revised sets containing only items with seen verbs. Excluding unseen verbs seems justified for FrameNet and has little effect for the Pr</context>
</contexts>
<marker>McRae, Spivey-Knowlton, Tanenhaus, 1998</marker>
<rawString>McRae, K., Spivey-Knowlton, M., and Tanenhaus, M. (1998). Modeling the influence of thematic fit (and other constraints) in on-line sentence comprehension. Journal of Memory and Language, 38, 283–312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Narayanan</author>
<author>D Jurafsky</author>
</authors>
<title>A Bayesian model predicts human parse preference and reading time in sentence processing.</title>
<date>2002</date>
<booktitle>Advances in Neural Information Processing Systems 14,</booktitle>
<pages>59--65</pages>
<editor>In S. B. T. G. Dietterich and Z. Ghahramani, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="3123" citStr="Narayanan and Jurafsky (2002)" startWordPosition="461" endWordPosition="464">shot (McRae et al., 1998). Plausibility has overridden the syntactic default. On the other hand, for a sentence like The hunter shot by the teenager was only 30 years old, semantic plausibility initially reinforces the syntactic main clause preference and readers show difficulty accommodating the subsequent disambiguation towards the reduced relative. In order to model effects like these, we need to extend existing models of sentence processing by introducing a semantic dimension. Possible ways of integrating different sources of information have been presented e.g. by McRae et al. (1998) and Narayanan and Jurafsky (2002). Our aim is to formulate a model that reliably predicts human plausibility judgements from corpus resources, in parallel to the standard practice of basing the syntax component of psycholinguistic models on corpus probabilities or even probabilistic treebank grammars. We can then use both the syntactic likelihood and the semantic plausibility score to predict the preferred syntactic alternative, thus accounting for the effects shown e.g. by McRae et al. (1998). Independent of a syntactic model, we want any semantic model we define to satisfy two criteria: First, it needs to be able to make pr</context>
</contexts>
<marker>Narayanan, Jurafsky, 2002</marker>
<rawString>Narayanan, S. and Jurafsky, D. (2002). A Bayesian model predicts human parse preference and reading time in sentence processing. In S. B. T. G. Dietterich and Z. Ghahramani, editors, Advances in Neural Information Processing Systems 14, pages 59–65. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Tishby</author>
<author>F C Pereira</author>
<author>W Bialek</author>
</authors>
<title>The information bottleneck method.</title>
<date>1999</date>
<booktitle>In Proc. of the 37-th Annual Allerton Conference on Communication, Control and Computing,</booktitle>
<pages>368--377</pages>
<contexts>
<context position="20582" citStr="Tishby et al., 1999" startWordPosition="3353" endWordPosition="3356">uration of verb and argument, the verb’s semantic class, the gold role information and a combined feature of gold role and syntactic configuration. The evaluation of the clustering task itself is task-based: We choose the clustering configuration that produces optimal results in the the prediction task on the McRae development set. The base corpus for clustering was always used for frequency estimation. We used an implementation of two soft clustering algorithms derived from information theory (Marx, 2004): the Information Distortion (ID) (Gedeon et al., 2003) and Information Bottleneck (IB) (Tishby et al., 1999) methods. Soft clustering allows us to take verb polysemy into account that is often characterised by different patterns of syntactic behaviour for each verb meaning. 2For ambiguous nouns, we chose the sense that led to the highest probability for the current role assignment. 349 A number of parameters were set on the development set, namely the clustering algorithm, the smoothing method within the algorithms and the number of clusters within each run. For our task, the IB algorithm generally yielded better results. We decided which clustering parametrisations should be tried on the test sets </context>
</contexts>
<marker>Tishby, Pereira, Bialek, 1999</marker>
<rawString>Tishby, N., Pereira, F. C., and Bialek, W. (1999). The information bottleneck method. In Proc. of the 37-th Annual Allerton Conference on Communication, Control and Computing, pages 368–377.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Trueswell</author>
<author>M Tanenhaus</author>
<author>S Garnsey</author>
</authors>
<title>Semantic influences on parsing: Use of thematic role information in syntactic ambiguity resolution.</title>
<date>1994</date>
<journal>Journal of Memory and Language,</journal>
<volume>33</volume>
<pages>285--318</pages>
<contexts>
<context position="11088" citStr="Trueswell et al. (1994)" startWordPosition="1783" endWordPosition="1786">epresentative of English. Test Data Our main data set consists of 160 data points from McRae et al. (1998) that were split randomly into a 60 data point development set and a 100 data point test set. The data is made up of two arguments per verb and two ratings for each verb-argument pair, one for the subject and one for the object reading of the argument (see Section 2). Each argument is highly plausible in one of the readings, but implausible in the other (recall Table 1). Human ratings are on a 7-point scale. In order to further test the coverage of our model, we also include 76 items from Trueswell et al. (1994) with one highly plausible object per verb and a rating each for the subject and object reading of the argument. The data were gathered in the same rating study as the McRae et al. data, so we can assume consistency of the ratings. However, in comparison to the McRae data set, the data is impoverished as it lacks ratings for plausible agents (in terms of the example in Table 1, this means there are no ratings for hunter). Lastly, we use 180 items from Keller and Lapata (2003). In contrast with the previous two studies, the verbs and nouns for these data were not handselected for the plausibili</context>
<context position="12893" citStr="Trueswell et al. (1994)" startWordPosition="2097" endWordPosition="2100">d the items are poorly covered by the FrameNet corpus. All test pairs were hand-annotated with FrameNet and PropBank roles following the specifications in the FrameNet on-line database and the PropBank frames files.1 The judgement prediction task is very hard to solve if the verb is unseen during training. Backing off to syntactic information or a frequency 1Although a single annotator assigned the roles, the annotation should be reliable as roles were mostly unambiguous and the annotated corpora were used for reference. 347 Total Revised Source FN PB McRae et al. (1998) 100 64 (64%) 92 (92%) Trueswell et al. (1994) 76 52 (68.4%) 72 (94.7%) Keller and Lapata (2003) 180 – 162 (90%) Table 2: Test sets: Total number of ratings and size of revised test sets containing only ratings for seen verbs (% of total ratings). –: Coverage too low (26.7%). baseline only works if the role set is small and syntactically motivated, which is the case for PropBank, but not FrameNet. We present results both for the complete test sets and and for revised sets containing only items with seen verbs. Excluding unseen verbs seems justified for FrameNet and has little effect for the PropBank corpus, since its coverage is generally</context>
</contexts>
<marker>Trueswell, Tanenhaus, Garnsey, 1994</marker>
<rawString>Trueswell, J., Tanenhaus, M., and Garnsey, S. (1994). Semantic influences on parsing: Use of thematic role information in syntactic ambiguity resolution. Journal of Memory and Language, 33, 285–318.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>