<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.015636">
<title confidence="0.9984645">
WordNet-based Semantic Relatedness Measures in Automatic Speech
Recognition for Meetings
</title>
<author confidence="0.973495">
Michael Pucher
</author>
<affiliation confidence="0.961623">
Telecommunications Research Center Vienna
</affiliation>
<address confidence="0.597721333333333">
Vienna, Austria
Speech and Signal Processing Lab, TU Graz
Graz, Austria
</address>
<email confidence="0.994257">
pucher@ftw.at
</email>
<sectionHeader confidence="0.998571" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999904285714286">
This paper presents the application of
WordNet-based semantic relatedness mea-
sures to Automatic Speech Recognition
(ASR) in multi-party meetings. Differ-
ent word-utterance context relatedness mea-
sures and utterance-coherence measures are
defined and applied to the rescoring of N-
best lists. No significant improvements
in terms of Word-Error-Rate (WER) are
achieved compared to a large word-based n-
gram baseline model. We discuss our results
and the relation to other work that achieved
an improvement with such models for sim-
pler tasks.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999888551724138">
As (Pucher, 2005) has shown different WordNet-
based measures and contexts are best for word pre-
diction in conversational speech. The JCN (Sec-
tion 2.1) measure performs best for nouns using the
noun-context. The LESK (Section 2.1) measure per-
forms best for verbs and adjectives using a mixed
word-context.
Text-based semantic relatedness measures can
improve word prediction on simulated speech recog-
nition hypotheses as (Demetriou et al., 2000) have
shown. (Demetriou et al., 2000) generated N-best
lists from phoneme confusion data acquired from
a speech recognizer, and a pronunciation lexicon.
Then sentence hypotheses of varying Word-Error-
Rate (WER) were generated based on sentences
from different genres from the British National Cor-
pus (BNC). It was shown by them that the semantic
model can improve recognition, where the amount
of improvement varies with context length and sen-
tence length. Thereby it was shown that these mod-
els can make use of long-term information.
In this paper the best performing measures
from (Pucher, 2005), which outperform baseline
models on word prediction for conversational tele-
phone speech are used for Automatic Speech Recog-
nition (ASR) in multi-party meetings. Thereby we
want to investigate if WordNet-based models can be
used for rescoring of ‘real’ N-best lists in a difficult
task.
</bodyText>
<subsectionHeader confidence="0.99327">
1.1 Word prediction by semantic similarity
</subsectionHeader>
<bodyText confidence="0.999836818181818">
The standard n-gram approach in language mod-
eling for speech recognition cannot cope with
long-term dependencies. Therefore (Bellegarda,
2000) proposed combining n-gram language mod-
els, which are effective for predicting local de-
pendencies, with Latent Semantic Analysis (LSA)
based models for covering long-term dependencies.
WordNet-based semantic relatedness measures can
be used for word prediction using long-term depen-
dencies, as in this example from the CallHome En-
glish telephone speech corpus:
</bodyText>
<equation confidence="0.4632635">
(1) B: I I well, you should see what the
bstudentsc
</equation>
<bodyText confidence="0.769577">
B: after they torture them for six byearsc in
middle bschoolc and high bschoolc they
don’t want to do anything in bcollegec
particular.
In Example 1 college can be predicted from the
noun context using semantic relatedness measures,
</bodyText>
<page confidence="0.981136">
129
</page>
<bodyText confidence="0.908533428571428">
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 129–132,
Prague, June 2007. c�2007 Association for Computational Linguistics
here between students and college. A 3-gram model
gives a ranking of college in the context of anything
in. An 8-gram predicts college from they don’t want
to do anything in, but the strongest predictor is stu-
dents.
</bodyText>
<subsectionHeader confidence="0.999547">
1.2 Test data
</subsectionHeader>
<bodyText confidence="0.9999087">
The JCN and LESK measure that are defined in the
next section are used for N-best list rescoring. For
the WER experiments N-best lists generated from
the decoding of conference room meeting test data
of the NIST Rich Transcription 2005 Spring (RT-
05S) meeting evaluation (Fiscus et al., 2005) are
used. The 4-gram that has to be improved by the
WordNet-based models is trained on various corpora
from conversational telephone speech to web data
that together contain approximately 1 billion words.
</bodyText>
<sectionHeader confidence="0.9867785" genericHeader="method">
2 WordNet-based semantic relatedness
measures
</sectionHeader>
<subsectionHeader confidence="0.999031">
2.1 Basic measures
</subsectionHeader>
<bodyText confidence="0.999978444444444">
Two similarity/distance measures from the Perl
package WordNet-Similarity written by (Pedersen et
al., 2004) are used. The measures are named af-
ter their respective authors. All measures are im-
plemented as similarity measures. JCN (Jiang and
Conrath, 1997) is based on the information content,
and LESK (Banerjee and Pedersen, 2003) allows
for comparison across Part-of-Speech (POS) bound-
aries.
</bodyText>
<subsectionHeader confidence="0.998901">
2.2 Word context relatedness
</subsectionHeader>
<bodyText confidence="0.9999435">
First the relatedness between words is defined based
on the relatedness between senses. S(w) are the
senses of word w. Definition 2 also performs word-
sense disambiguation.
</bodyText>
<subsectionHeader confidence="0.994694">
2.3 Word utterance (context) relatedness
</subsectionHeader>
<bodyText confidence="0.998713571428572">
The performance of the word-context relatedness
(Definition 3) shows how well the measures work
for algorithms that proceed in a left-to-right manner,
since the context is restricted to words that have al-
ready been seen. For the rescoring of N-best lists
it is not necessary to proceed in a left-to-right man-
ner. The word-utterance-context relatedness can be
used for the rescoring of N-best lists. This related-
ness does not only use the context of the preceding
words, but the whole utterance.
Suppose U = (w1, ... , wn) is an utterance. Let
pre(wi, U) be the set Uj&lt;i wj and post(wi, U) be
the set Uj&gt;i wj. Then the word-utterance-context
relatedness is defined as
</bodyText>
<equation confidence="0.9907055">
relU,(wi, U, C) =
relW(wi, pre(wi, U) U post(wi, U) U C) . (4)
</equation>
<bodyText confidence="0.9955155">
In this case there are two types of context. The
first context comes from the respective meeting, and
the second context comes from the actual utterance.
Another definition is obtained if the context C is
eliminated (C = 0) and just the utterance context U
is taken into account.
</bodyText>
<equation confidence="0.9849715">
relU,(wi, U) =
relW(wi, pre(wi, U) U post(wi, U)) (5)
</equation>
<bodyText confidence="0.999958333333333">
Both definitions can be modified for usage with
rescoring in a left-to-right manner by restricting the
contexts only to the preceding words.
</bodyText>
<equation confidence="0.998112">
relU3(wi, U, C) = relW(wi, pre(wi, U) U C) (6)
rel(w, w&apos;) = max rel(ci, cj) (2)
ciES(w) cjES(w&apos;)
</equation>
<bodyText confidence="0.999938333333333">
The relatedness of a word and a context (relW) is
defined as the average of the relatedness of the word
and all words in the context.
</bodyText>
<equation confidence="0.9936675">
1 �
relW(w, C) =  |C |
wiEC
relU,(wi, U) = relW(wi, pre(wi, U)) (7)
</equation>
<subsectionHeader confidence="0.995376">
2.4 Defining utterance coherence
</subsectionHeader>
<bodyText confidence="0.9912562">
Using Definitions 4-7 different concepts of utterance
coherence can be defined. For rescoring the utter-
ance coherence is used, when a score for each el-
ement of an N-best list is needed. U is again an
utterance U = (w1, ... , wn).
</bodyText>
<equation confidence="0.885374">
rel(w, wi) (3)
</equation>
<page confidence="0.889072">
130
</page>
<table confidence="0.90383775">
1 � relU,(w, U, C) (8) list. The final WordNet score is the sum of the two
cohU1(U, C) =  |scores.
U  |wEU The log-linear interpolation method used for the
rescoring is defined as
</table>
<bodyText confidence="0.918078666666667">
The first semantic utterance coherence measure
(Definition 8) is based on all words in the utterance
as well as in the context. It takes the mean of the
relatedness of all words. It is based on the word-
utterance-context relatedness (Definition 4).
The second coherence measure (Definition 9) is
a pure inner-utterance-coherence, which means that
no history apart from the utterance is needed. Such
a measure is very useful for rescoring, since the his-
tory is often not known or because there are speech
recognition errors in the history. It is based on Defi-
nition 5.
</bodyText>
<equation confidence="0.995027">
1 � relU,(w, U, C) (10)
cohU3(U, C) =  |U |
wEU
</equation>
<bodyText confidence="0.992122666666667">
The third (Definition 10) and fourth (Defini-
tion 11) definition are based on Definition 6 and 7,
that do not take future words into account.
</bodyText>
<sectionHeader confidence="0.987771" genericHeader="method">
3 Word-error-rate (WER) experiments
</sectionHeader>
<bodyText confidence="0.999944071428572">
For the rescoring experiments the first-best element
of the previous N-best list is added to the context.
Before applying the WordNet-based measures, the
N-best lists are POS tagged with a decision tree
tagger (Schmid, 1994). The WordNet measures are
then applied to verbs, nouns and adjectives. Then
the similarity values are used as scores, which have
to be combined with the language model scores of
the N-best list elements.
The JCN measure is used for computing a noun
score based on the noun context, and the LESK mea-
sure is used for computing a verb/adjective score
based on the noun/verb/adjective context. In the end
there is a lesk score and a jcn score for each N-best
</bodyText>
<equation confidence="0.871916">
p(S) a pwordnet(S)A p.-gram(S)1−A (12)
</equation>
<bodyText confidence="0.999962764705882">
where a denotes normalization. Based on all Word-
Net scores of an N-best list a probability is esti-
mated, which is then interpolated with the n-gram
model probability. If only the elements in an N-
best list are considered, log-linear interpolation can
be used since it is not necessary to normalize over
all sentences. Then there is only one parameter A to
optimize, which is done with a brute force approach.
For this optimization a small part of the test data is
taken and the WER is computed for different values
of A.
As a baseline the n-gram mixture model trained
on all available training data (Pz� 1 billion words) is
used. It is log-linearly interpolated with the Word-
Net probabilities. Additionally to this sophisticated
interpolation, solely the WordNet scores are used
without the n-gram scores.
</bodyText>
<subsectionHeader confidence="0.6570805">
3.1 WER experiments for inner-utterance
coherence
</subsectionHeader>
<bodyText confidence="0.999952923076923">
In this first group of experiments Definitions 8 and 9
are applied to the rescoring task. Similarity scores
for each element in an N-best list are derived ac-
cording to the definitions. The first-best element of
the last list is always added to the context. The con-
text size is constrained to the last 20 words. Def-
inition 8 includes context apart from the utterance
context, Definition 9 only uses the utterance context.
No improvement over the n-gram baseline is
achieved for these two measures. Neither with the
log-linearly interpolated models nor with the Word-
Net scores alone. The differences between the meth-
ods in terms of WER are not significant.
</bodyText>
<subsectionHeader confidence="0.998603">
3.2 WER experiments for utterance coherence
</subsectionHeader>
<bodyText confidence="0.999862166666667">
In the second group of experiments Definitions 10
and 11 are applied to the rescoring task. There is
again one measure that uses dialog context (10) and
one that only uses utterance context (11).
Also for these experiments no improvement over
the n-gram baseline is achieved. Neither with the
</bodyText>
<equation confidence="0.999569375">
1 �
cohU2(U) =  |U |
wEU
relU2(w, U) (9)
1 �
cohU4(U) =  |U |
wEU
relU,(w, U) (11)
</equation>
<page confidence="0.979459">
131
</page>
<bodyText confidence="0.999730166666667">
log-linearly interpolated models nor with the Word-
Net scores alone. The differences between the meth-
ods in terms of WER are also not significant. There
are also no significant differences in performance
between the second group and the first group of ex-
periments.
</bodyText>
<sectionHeader confidence="0.967433" genericHeader="evaluation">
4 Summary and discussion
</sectionHeader>
<bodyText confidence="0.999971289473685">
We showed how to define more and more complex
relatedness measures on top of the basic relatedness
measures between word senses.
The LESK and JCN measures were used for the
rescoring of N-best lists. It was shown that speech
recognition of multi-party meetings cannot be im-
proved compared to a 4-gram baseline model, when
using WordNet models.
One reason for the poor performance of the mod-
els could be that the task of rescoring simulated N-
best lists, as presented in (Demetriou et al., 2000), is
significantly easier than the rescoring of ‘real’ N-
best lists. (Pucher, 2005) has shown that Word-
Net models can outperform simple random mod-
els on the task of word prediction, in spite of the
noise that is introduced through word-sense disam-
biguation and POS tagging. To improve the word-
sense disambiguation one could use the approach
proposed by (Basili et al., 2004).
In the above WER experiments a 4-gram baseline
model was used, which was trained on nearly 1 bil-
lion words. In (Demetriou et al., 2000) a simpler
baseline has been used. 650 sentences were used
there to generate sentence hypotheses with different
WER using phoneme confusion data and a pronun-
ciation lexicon. Experiments with simpler baseline
models ignore that these simpler models are not used
in today’s recognition systems.
We think that these prediction models can still be
useful for other tasks where only small amounts of
training data are available. Another possibility of
improvement is to use other interpolation techniques
like the maximum entropy framework. WordNet-
based models could also be improved by using a
trigger-based approach. This could be done by not
using the whole WordNet and its similarities, but
defining word-trigger pairs that are used for rescor-
ing.
</bodyText>
<sectionHeader confidence="0.998827" genericHeader="conclusions">
5 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999715">
This work was supported by the European Union 6th
FP IST Integrated Project AMI (Augmented Multi-
party Interaction, and by Kapsch Carrier-Com AG
and Mobilkom Austria AG together with the Aus-
trian competence centre programme Kplus.
</bodyText>
<sectionHeader confidence="0.998675" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999178476190476">
Satanjeev Banerjee and Ted Pedersen. 2003. Extended
gloss overlaps as a measure of semantic relatedness.
In Proceedings of the 18th Int. Joint Conf. on Artificial
Intelligence, pages 805–810, Acapulco.
Roberto Basili, Marco Cammisa, and Fabio Massimo
Zanzotto. 2004. A semantic similarity measure for
unsupervised semantic tagging. In Proc. of the Fourth
International Conference on Language Resources and
Evaluation (LREC2004), Lisbon, Portugal.
Jerome Bellegarda. 2000. Large vocabulary speech
recognition with multispan statistical language mod-
els. IEEE Transactions on Speech and Audio Process-
ing, 8(1), January.
G. Demetriou, E. Atwell, and C. Souter. 2000. Using
lexical semantic knowledge from machine readable
dictionaries for domain independent language mod-
elling. In Proc. ofLREC 2000, 2nd International Con-
ference on Language Resources and Evaluation.
Jonathan G. Fiscus, Nicolas Radde, John S. Garofolo,
Audrey Le, Jerome Ajot, and Christophe Laprun.
2005. The rich transcription 2005 spring meeting
recognition evaluation. In Rich Transcription 2005
Spring Meeting Recognition Evaluation Workshop,
Edinburgh, UK.
Jay J. Jiang and David W. Conrath. 1997. Semantic sim-
ilarity based on corpus statistics and lexical taxonomy.
In Proceedings of the International Conference on Re-
search in Computational Linguistics, Taiwan.
Ted Pedersen, S. Patwardhan, and J. Michelizzi. 2004.
WordNet::Similarity - Measuring the relatedness of
concepts. In Proc. of Fifth Annual Meeting of the
North American Chapter of the ACL (NAACL-04),
Boston, MA.
Michael Pucher. 2005. Performance evaluation of
WordNet-based semantic relatedness measures for
word prediction in conversational speech. In IWCS
6, Sixth International Workshop on Computational Se-
mantics, Tilburg, Netherlands.
H Schmid. 1994. Probabilistic part-of-speech tagging
using decision trees. In Proceedings of International
Conference on New Methods in Language Processing,
Manchester, UK, September.
</reference>
<page confidence="0.997732">
132
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.728707">
<title confidence="0.9998795">WordNet-based Semantic Relatedness Measures in Automatic Speech Recognition for Meetings</title>
<author confidence="0.999779">Michael Pucher</author>
<affiliation confidence="0.994618">Telecommunications Research Center Vienna</affiliation>
<address confidence="0.926173666666667">Vienna, Austria Speech and Signal Processing Lab, TU Graz Graz, Austria</address>
<email confidence="0.992799">pucher@ftw.at</email>
<abstract confidence="0.9953412">This paper presents the application of WordNet-based semantic relatedness meato Speech Recognition (ASR) in multi-party meetings. Different word-utterance context relatedness measures and utterance-coherence measures are and applied to the rescoring of best lists. No significant improvements terms of are compared to a large word-based gram baseline model. We discuss our results and the relation to other work that achieved an improvement with such models for simpler tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>Extended gloss overlaps as a measure of semantic relatedness.</title>
<date>2003</date>
<booktitle>In Proceedings of the 18th Int. Joint Conf. on Artificial Intelligence,</booktitle>
<pages>805--810</pages>
<location>Acapulco.</location>
<contexts>
<context position="4216" citStr="Banerjee and Pedersen, 2003" startWordPosition="638" endWordPosition="641">ion (Fiscus et al., 2005) are used. The 4-gram that has to be improved by the WordNet-based models is trained on various corpora from conversational telephone speech to web data that together contain approximately 1 billion words. 2 WordNet-based semantic relatedness measures 2.1 Basic measures Two similarity/distance measures from the Perl package WordNet-Similarity written by (Pedersen et al., 2004) are used. The measures are named after their respective authors. All measures are implemented as similarity measures. JCN (Jiang and Conrath, 1997) is based on the information content, and LESK (Banerjee and Pedersen, 2003) allows for comparison across Part-of-Speech (POS) boundaries. 2.2 Word context relatedness First the relatedness between words is defined based on the relatedness between senses. S(w) are the senses of word w. Definition 2 also performs wordsense disambiguation. 2.3 Word utterance (context) relatedness The performance of the word-context relatedness (Definition 3) shows how well the measures work for algorithms that proceed in a left-to-right manner, since the context is restricted to words that have already been seen. For the rescoring of N-best lists it is not necessary to proceed in a left</context>
</contexts>
<marker>Banerjee, Pedersen, 2003</marker>
<rawString>Satanjeev Banerjee and Ted Pedersen. 2003. Extended gloss overlaps as a measure of semantic relatedness. In Proceedings of the 18th Int. Joint Conf. on Artificial Intelligence, pages 805–810, Acapulco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Basili</author>
<author>Marco Cammisa</author>
<author>Fabio Massimo Zanzotto</author>
</authors>
<title>A semantic similarity measure for unsupervised semantic tagging.</title>
<date>2004</date>
<booktitle>In Proc. of the Fourth International Conference on Language Resources and Evaluation (LREC2004),</booktitle>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="11081" citStr="Basili et al., 2004" startWordPosition="1818" endWordPosition="1821">party meetings cannot be improved compared to a 4-gram baseline model, when using WordNet models. One reason for the poor performance of the models could be that the task of rescoring simulated Nbest lists, as presented in (Demetriou et al., 2000), is significantly easier than the rescoring of ‘real’ Nbest lists. (Pucher, 2005) has shown that WordNet models can outperform simple random models on the task of word prediction, in spite of the noise that is introduced through word-sense disambiguation and POS tagging. To improve the wordsense disambiguation one could use the approach proposed by (Basili et al., 2004). In the above WER experiments a 4-gram baseline model was used, which was trained on nearly 1 billion words. In (Demetriou et al., 2000) a simpler baseline has been used. 650 sentences were used there to generate sentence hypotheses with different WER using phoneme confusion data and a pronunciation lexicon. Experiments with simpler baseline models ignore that these simpler models are not used in today’s recognition systems. We think that these prediction models can still be useful for other tasks where only small amounts of training data are available. Another possibility of improvement is t</context>
</contexts>
<marker>Basili, Cammisa, Zanzotto, 2004</marker>
<rawString>Roberto Basili, Marco Cammisa, and Fabio Massimo Zanzotto. 2004. A semantic similarity measure for unsupervised semantic tagging. In Proc. of the Fourth International Conference on Language Resources and Evaluation (LREC2004), Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerome Bellegarda</author>
</authors>
<title>Large vocabulary speech recognition with multispan statistical language models.</title>
<date>2000</date>
<journal>IEEE Transactions on Speech and Audio Processing,</journal>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="2305" citStr="Bellegarda, 2000" startWordPosition="340" endWordPosition="341">th. Thereby it was shown that these models can make use of long-term information. In this paper the best performing measures from (Pucher, 2005), which outperform baseline models on word prediction for conversational telephone speech are used for Automatic Speech Recognition (ASR) in multi-party meetings. Thereby we want to investigate if WordNet-based models can be used for rescoring of ‘real’ N-best lists in a difficult task. 1.1 Word prediction by semantic similarity The standard n-gram approach in language modeling for speech recognition cannot cope with long-term dependencies. Therefore (Bellegarda, 2000) proposed combining n-gram language models, which are effective for predicting local dependencies, with Latent Semantic Analysis (LSA) based models for covering long-term dependencies. WordNet-based semantic relatedness measures can be used for word prediction using long-term dependencies, as in this example from the CallHome English telephone speech corpus: (1) B: I I well, you should see what the bstudentsc B: after they torture them for six byearsc in middle bschoolc and high bschoolc they don’t want to do anything in bcollegec particular. In Example 1 college can be predicted from the noun</context>
</contexts>
<marker>Bellegarda, 2000</marker>
<rawString>Jerome Bellegarda. 2000. Large vocabulary speech recognition with multispan statistical language models. IEEE Transactions on Speech and Audio Processing, 8(1), January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Demetriou</author>
<author>E Atwell</author>
<author>C Souter</author>
</authors>
<title>Using lexical semantic knowledge from machine readable dictionaries for domain independent language modelling.</title>
<date>2000</date>
<booktitle>In Proc. ofLREC 2000, 2nd International Conference on Language Resources and Evaluation.</booktitle>
<contexts>
<context position="1236" citStr="Demetriou et al., 2000" startWordPosition="175" endWordPosition="178">d to a large word-based ngram baseline model. We discuss our results and the relation to other work that achieved an improvement with such models for simpler tasks. 1 Introduction As (Pucher, 2005) has shown different WordNetbased measures and contexts are best for word prediction in conversational speech. The JCN (Section 2.1) measure performs best for nouns using the noun-context. The LESK (Section 2.1) measure performs best for verbs and adjectives using a mixed word-context. Text-based semantic relatedness measures can improve word prediction on simulated speech recognition hypotheses as (Demetriou et al., 2000) have shown. (Demetriou et al., 2000) generated N-best lists from phoneme confusion data acquired from a speech recognizer, and a pronunciation lexicon. Then sentence hypotheses of varying Word-ErrorRate (WER) were generated based on sentences from different genres from the British National Corpus (BNC). It was shown by them that the semantic model can improve recognition, where the amount of improvement varies with context length and sentence length. Thereby it was shown that these models can make use of long-term information. In this paper the best performing measures from (Pucher, 2005), wh</context>
<context position="10708" citStr="Demetriou et al., 2000" startWordPosition="1755" endWordPosition="1758">o no significant differences in performance between the second group and the first group of experiments. 4 Summary and discussion We showed how to define more and more complex relatedness measures on top of the basic relatedness measures between word senses. The LESK and JCN measures were used for the rescoring of N-best lists. It was shown that speech recognition of multi-party meetings cannot be improved compared to a 4-gram baseline model, when using WordNet models. One reason for the poor performance of the models could be that the task of rescoring simulated Nbest lists, as presented in (Demetriou et al., 2000), is significantly easier than the rescoring of ‘real’ Nbest lists. (Pucher, 2005) has shown that WordNet models can outperform simple random models on the task of word prediction, in spite of the noise that is introduced through word-sense disambiguation and POS tagging. To improve the wordsense disambiguation one could use the approach proposed by (Basili et al., 2004). In the above WER experiments a 4-gram baseline model was used, which was trained on nearly 1 billion words. In (Demetriou et al., 2000) a simpler baseline has been used. 650 sentences were used there to generate sentence hypo</context>
</contexts>
<marker>Demetriou, Atwell, Souter, 2000</marker>
<rawString>G. Demetriou, E. Atwell, and C. Souter. 2000. Using lexical semantic knowledge from machine readable dictionaries for domain independent language modelling. In Proc. ofLREC 2000, 2nd International Conference on Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan G Fiscus</author>
<author>Nicolas Radde</author>
<author>John S Garofolo</author>
<author>Audrey Le</author>
<author>Jerome Ajot</author>
<author>Christophe Laprun</author>
</authors>
<title>The rich transcription 2005 spring meeting recognition evaluation.</title>
<date>2005</date>
<booktitle>In Rich Transcription</booktitle>
<location>Edinburgh, UK.</location>
<contexts>
<context position="3613" citStr="Fiscus et al., 2005" startWordPosition="548" endWordPosition="551">oster Sessions, pages 129–132, Prague, June 2007. c�2007 Association for Computational Linguistics here between students and college. A 3-gram model gives a ranking of college in the context of anything in. An 8-gram predicts college from they don’t want to do anything in, but the strongest predictor is students. 1.2 Test data The JCN and LESK measure that are defined in the next section are used for N-best list rescoring. For the WER experiments N-best lists generated from the decoding of conference room meeting test data of the NIST Rich Transcription 2005 Spring (RT05S) meeting evaluation (Fiscus et al., 2005) are used. The 4-gram that has to be improved by the WordNet-based models is trained on various corpora from conversational telephone speech to web data that together contain approximately 1 billion words. 2 WordNet-based semantic relatedness measures 2.1 Basic measures Two similarity/distance measures from the Perl package WordNet-Similarity written by (Pedersen et al., 2004) are used. The measures are named after their respective authors. All measures are implemented as similarity measures. JCN (Jiang and Conrath, 1997) is based on the information content, and LESK (Banerjee and Pedersen, 20</context>
</contexts>
<marker>Fiscus, Radde, Garofolo, Le, Ajot, Laprun, 2005</marker>
<rawString>Jonathan G. Fiscus, Nicolas Radde, John S. Garofolo, Audrey Le, Jerome Ajot, and Christophe Laprun. 2005. The rich transcription 2005 spring meeting recognition evaluation. In Rich Transcription 2005 Spring Meeting Recognition Evaluation Workshop, Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay J Jiang</author>
<author>David W Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy.</title>
<date>1997</date>
<booktitle>In Proceedings of the International Conference on Research in Computational Linguistics,</booktitle>
<contexts>
<context position="4140" citStr="Jiang and Conrath, 1997" startWordPosition="626" endWordPosition="629"> data of the NIST Rich Transcription 2005 Spring (RT05S) meeting evaluation (Fiscus et al., 2005) are used. The 4-gram that has to be improved by the WordNet-based models is trained on various corpora from conversational telephone speech to web data that together contain approximately 1 billion words. 2 WordNet-based semantic relatedness measures 2.1 Basic measures Two similarity/distance measures from the Perl package WordNet-Similarity written by (Pedersen et al., 2004) are used. The measures are named after their respective authors. All measures are implemented as similarity measures. JCN (Jiang and Conrath, 1997) is based on the information content, and LESK (Banerjee and Pedersen, 2003) allows for comparison across Part-of-Speech (POS) boundaries. 2.2 Word context relatedness First the relatedness between words is defined based on the relatedness between senses. S(w) are the senses of word w. Definition 2 also performs wordsense disambiguation. 2.3 Word utterance (context) relatedness The performance of the word-context relatedness (Definition 3) shows how well the measures work for algorithms that proceed in a left-to-right manner, since the context is restricted to words that have already been seen</context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>Jay J. Jiang and David W. Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. In Proceedings of the International Conference on Research in Computational Linguistics, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>S Patwardhan</author>
<author>J Michelizzi</author>
</authors>
<title>WordNet::Similarity - Measuring the relatedness of concepts.</title>
<date>2004</date>
<booktitle>In Proc. of Fifth Annual Meeting of the North American Chapter of the ACL (NAACL-04),</booktitle>
<location>Boston, MA.</location>
<contexts>
<context position="3992" citStr="Pedersen et al., 2004" startWordPosition="602" endWordPosition="605"> next section are used for N-best list rescoring. For the WER experiments N-best lists generated from the decoding of conference room meeting test data of the NIST Rich Transcription 2005 Spring (RT05S) meeting evaluation (Fiscus et al., 2005) are used. The 4-gram that has to be improved by the WordNet-based models is trained on various corpora from conversational telephone speech to web data that together contain approximately 1 billion words. 2 WordNet-based semantic relatedness measures 2.1 Basic measures Two similarity/distance measures from the Perl package WordNet-Similarity written by (Pedersen et al., 2004) are used. The measures are named after their respective authors. All measures are implemented as similarity measures. JCN (Jiang and Conrath, 1997) is based on the information content, and LESK (Banerjee and Pedersen, 2003) allows for comparison across Part-of-Speech (POS) boundaries. 2.2 Word context relatedness First the relatedness between words is defined based on the relatedness between senses. S(w) are the senses of word w. Definition 2 also performs wordsense disambiguation. 2.3 Word utterance (context) relatedness The performance of the word-context relatedness (Definition 3) shows ho</context>
</contexts>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>Ted Pedersen, S. Patwardhan, and J. Michelizzi. 2004. WordNet::Similarity - Measuring the relatedness of concepts. In Proc. of Fifth Annual Meeting of the North American Chapter of the ACL (NAACL-04), Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Pucher</author>
</authors>
<title>Performance evaluation of WordNet-based semantic relatedness measures for word prediction in conversational speech.</title>
<date>2005</date>
<booktitle>In IWCS 6, Sixth International Workshop on Computational Semantics,</booktitle>
<location>Tilburg, Netherlands.</location>
<contexts>
<context position="810" citStr="Pucher, 2005" startWordPosition="112" endWordPosition="113">U Graz Graz, Austria pucher@ftw.at Abstract This paper presents the application of WordNet-based semantic relatedness measures to Automatic Speech Recognition (ASR) in multi-party meetings. Different word-utterance context relatedness measures and utterance-coherence measures are defined and applied to the rescoring of Nbest lists. No significant improvements in terms of Word-Error-Rate (WER) are achieved compared to a large word-based ngram baseline model. We discuss our results and the relation to other work that achieved an improvement with such models for simpler tasks. 1 Introduction As (Pucher, 2005) has shown different WordNetbased measures and contexts are best for word prediction in conversational speech. The JCN (Section 2.1) measure performs best for nouns using the noun-context. The LESK (Section 2.1) measure performs best for verbs and adjectives using a mixed word-context. Text-based semantic relatedness measures can improve word prediction on simulated speech recognition hypotheses as (Demetriou et al., 2000) have shown. (Demetriou et al., 2000) generated N-best lists from phoneme confusion data acquired from a speech recognizer, and a pronunciation lexicon. Then sentence hypothe</context>
<context position="10790" citStr="Pucher, 2005" startWordPosition="1770" endWordPosition="1771">experiments. 4 Summary and discussion We showed how to define more and more complex relatedness measures on top of the basic relatedness measures between word senses. The LESK and JCN measures were used for the rescoring of N-best lists. It was shown that speech recognition of multi-party meetings cannot be improved compared to a 4-gram baseline model, when using WordNet models. One reason for the poor performance of the models could be that the task of rescoring simulated Nbest lists, as presented in (Demetriou et al., 2000), is significantly easier than the rescoring of ‘real’ Nbest lists. (Pucher, 2005) has shown that WordNet models can outperform simple random models on the task of word prediction, in spite of the noise that is introduced through word-sense disambiguation and POS tagging. To improve the wordsense disambiguation one could use the approach proposed by (Basili et al., 2004). In the above WER experiments a 4-gram baseline model was used, which was trained on nearly 1 billion words. In (Demetriou et al., 2000) a simpler baseline has been used. 650 sentences were used there to generate sentence hypotheses with different WER using phoneme confusion data and a pronunciation lexicon</context>
</contexts>
<marker>Pucher, 2005</marker>
<rawString>Michael Pucher. 2005. Performance evaluation of WordNet-based semantic relatedness measures for word prediction in conversational speech. In IWCS 6, Sixth International Workshop on Computational Semantics, Tilburg, Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In Proceedings of International Conference on New Methods in Language Processing,</booktitle>
<location>Manchester, UK,</location>
<contexts>
<context position="7499" citStr="Schmid, 1994" startWordPosition="1210" endWordPosition="1211">asure is very useful for rescoring, since the history is often not known or because there are speech recognition errors in the history. It is based on Definition 5. 1 � relU,(w, U, C) (10) cohU3(U, C) = |U | wEU The third (Definition 10) and fourth (Definition 11) definition are based on Definition 6 and 7, that do not take future words into account. 3 Word-error-rate (WER) experiments For the rescoring experiments the first-best element of the previous N-best list is added to the context. Before applying the WordNet-based measures, the N-best lists are POS tagged with a decision tree tagger (Schmid, 1994). The WordNet measures are then applied to verbs, nouns and adjectives. Then the similarity values are used as scores, which have to be combined with the language model scores of the N-best list elements. The JCN measure is used for computing a noun score based on the noun context, and the LESK measure is used for computing a verb/adjective score based on the noun/verb/adjective context. In the end there is a lesk score and a jcn score for each N-best p(S) a pwordnet(S)A p.-gram(S)1−A (12) where a denotes normalization. Based on all WordNet scores of an N-best list a probability is estimated, </context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>H Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of International Conference on New Methods in Language Processing, Manchester, UK, September.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>