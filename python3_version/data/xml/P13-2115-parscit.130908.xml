<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000071">
<title confidence="0.987841">
Diverse Keyword Extraction from Conversations
</title>
<author confidence="0.963668">
Maryam Habibi
</author>
<affiliation confidence="0.940274">
Idiap Research Institute and EPFL
</affiliation>
<address confidence="0.6245585">
Rue Marconi 19, CP 592
1920 Martigny, Switzerland
</address>
<email confidence="0.994615">
maryam.habibi@idiap.ch
</email>
<author confidence="0.888199">
Andrei Popescu-Belis
</author>
<affiliation confidence="0.897411">
Idiap Research Institute
</affiliation>
<address confidence="0.6027555">
Rue Marconi 19, CP 592
1920 Martigny, Switzerland
</address>
<email confidence="0.99453">
andrei.popescu-belis@idiap.ch
</email>
<sectionHeader confidence="0.997342" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999830846153846">
A new method for keyword extraction
from conversations is introduced, which
preserves the diversity of topics that are
mentioned. Inspired from summarization,
the method maximizes the coverage of
topics that are recognized automatically
in transcripts of conversation fragments.
The method is evaluated on excerpts of the
Fisher and AMI corpora, using a crowd-
sourcing platform to elicit comparative
relevance judgments. The results demon-
strate that the method outperforms two
competitive baselines.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999190666666667">
The goal of keyword extraction from texts is to
provide a set of words that are representative of
the semantic content of the texts. In the applica-
tion intended here, keywords are automatically ex-
tracted from transcripts of conversation fragments,
and are used to formulate queries to a just-in-time
document recommender system. It is thus impor-
tant that the keyword set preserves the diversity of
topics from the conversation. While the first key-
word extraction methods ignored topicality as they
were based on word frequencies, more recent me-
thods have considered topic modeling factors for
keyword extraction, but without specifically set-
ting a topic diversity constraint, which is impor-
tant for naturally-occurring conversations.
In this paper, we propose a new method for key-
word extraction that rewards both word similarity,
to extract the most representative words, and word
diversity, to cover several topics if necessary. The
paper is organized as follows. In Section 2 we re-
view existing methods for keyword extraction. In
Section 3 we describe our proposal, which relies
on topic modeling and a novel topic-aware diverse
keyword extraction algorithm. Section 4 presents
the data and tasks for comparing sets of keywords.
In Section 5 we show that our method outperforms
two existing ones.
</bodyText>
<sectionHeader confidence="0.532463" genericHeader="introduction">
2 State of the Art in Keyword Extraction
</sectionHeader>
<bodyText confidence="0.999921472222222">
Numerous studies have been conducted to auto-
matically extract keywords from a text or a tran-
scribed conversation. The earliest techniques have
used word frequencies (Luhn, 1957), TFIDF val-
ues (Salton et al., 1975; Salton and Buckley,
1988), and pairwise word co-occurrence frequen-
cies (Matsuo and Ishizuka, 2004) to rank words
for extraction. These approaches do not con-
sider word meaning, so they may ignore low-
frequency words which together indicate a highly-
salient topic (Nenkova and McKeown, 2012).
To improve over frequency-based methods, se-
veral ways to use lexical semantic information
have been proposed. Semantic relations be-
tween words can be obtained from a manually-
constructed thesaurus such as WordNet, or from
Wikipedia, or from an automatically-built the-
saurus using latent topic modeling techniques.
Ye et al. (2007) used the frequency of all words
belonging to the same WordNet concept set, while
the Wikifier system (Csomai and Mihalcea, 2007)
relied on Wikipedia links to compute a substitute
to word frequency. Harwath and Hazen (2012)
used topic modeling with PLSA to build a the-
saurus, which they used to rank words based on
topical similarity to the topics of a transcribed con-
versation. To consider dependencies among se-
lected words, word co-occurrence has been com-
bined with PageRank by Mihalcea and Tarau
(2004), and additionally with WordNet by Wang
et al. (2007), or with topical information by Z. Liu
et al. (2010). However, as shown empirically by
Mihalcea and Tarau (2004) and by Z. Liu et al.
(2010) with various co-occurrence windows, such
approaches have difficulties modeling long-range
dependencies between words related to the same
</bodyText>
<page confidence="0.982612">
651
</page>
<bodyText confidence="0.917589235294118">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 651–657,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
topic. Z. Liu et al. (2009b) used part-of-speech in-
formation and word clustering techniques, while
F. Liu et al. (2009a) added this information to
the TFIDF method so as to consider both word
dependency and semantic information. However,
although they considered topical similarity, the
above methods did not explicitly reward diversity
and might miss secondary topics.
Supervised methods have been used to learn a
model for extracting keywords with various learn-
ing algorithms (Turney, 1999; Frank et al., 1999;
Hulth, 2003). These approaches, however, rely on
the availability of in-domain training data, and the
objective functions they use for learning do not
consider yet the diversity of keywords.
</bodyText>
<sectionHeader confidence="0.997349" genericHeader="method">
3 Diverse Keyword Extraction
</sectionHeader>
<bodyText confidence="0.998313666666667">
We propose to build a topical representation of
a conversation fragment, and then to select key-
words using topical similarity while also reward-
ing the diversity of topic coverage, inspired by
recent summarization methods (Lin and Bilmes,
2011; Li et al., 2012).
</bodyText>
<subsectionHeader confidence="0.999873">
3.1 Representing Topic Information
</subsectionHeader>
<bodyText confidence="0.999977666666667">
Topic models such as Probabilistic Latent Seman-
tic Analysis (PLSA) or Latent Dirichlet Allocation
(LDA) can be used to determine the distribution
over the topic z of a word w, noted p(z|w), from a
large amount of training documents. LDA imple-
mented in the Mallet toolkit (McCallum, 2002) is
used in this paper because it does not suffer from
the overfitting issue of PLSA (Blei et al., 2003).
The distribution of each topic z in a given con-
versation fragment t, noted p(z|t), can be com-
puted by summing over all probabilities p(z|w) of
the N words w spoken in the fragment:
</bodyText>
<equation confidence="0.9353405">
1 p(z|t) = N p(z|w).
wEt
</equation>
<subsectionHeader confidence="0.999616">
3.2 Selecting Keywords
</subsectionHeader>
<bodyText confidence="0.999970766666667">
The problem of keyword extraction with maximal
topic coverage is formulated as follows. If a con-
versation fragment t mentions a set of topics Z,
and each word w from the fragment t can evoke a
subset of the topics in Z, then the goal is to find
a subset of unique words S C_ t, with |S |&lt; k,
which maximzes the number of covered topics for
each number of keywords k.
This problem is an instance of the maximum
coverage problem, which is NP-hard. Nemhauser
et al. (1978) showed that a greedy algorithm can
find an approximate solution guaranteed to be
within (1 − e) ^_ 0.63 of the optimal solution
if the coverage function is submodular and mono-
tone nondecreasing1.
To find a monotone submodular function for
keyword extraction, we used inspiration from re-
cent work on extractive summarization methods
(Lin and Bilmes, 2011; Li et al., 2012), which pro-
posed a square root function for diverse selection
of sentences to cover the maximum number of key
concepts of a given document. The function re-
wards diversity by increasing the gain of selecting
a sentence including a concept that was not yet
covered by a previously selected sentence. This
must be adapted for keyword extraction by defin-
ing an appropriate reward function.
We first introduce rS,z, the topical similarity
with respect to topic z of the keyword set S se-
lected from the fragment t, defined as follows:
</bodyText>
<equation confidence="0.9978175">
�rS,z = p(z|w) · p(z|t).
wES
</equation>
<bodyText confidence="0.999730666666667">
We then propose the following reward function
for each topic, where p(z|t) is the importance of
the topic and A is a parameter between 0 and 1:
</bodyText>
<equation confidence="0.893469">
f : rS,z —+ p(z|t) · rλS,z .
</equation>
<bodyText confidence="0.9993366">
This is clearly a submodular function with di-
minishing returns as rS,z increases.
Finally, the keywords S C_ t, with |S |&lt; k,
are chosen by maximizing the cumulative reward
function over all the topics, formulated as follows:
</bodyText>
<equation confidence="0.997019">
R(S) = � p(z|t) · rλS,z .
zEZ
</equation>
<bodyText confidence="0.997129444444444">
Since R(S) is submodular, the greedy algo-
rithm for maximizing R(S) is shown as Algo-
rithm 1 on the next page, with rlw},z being similar
to rS,z with S = {w}. If A = 1, the reward func-
tion is linear and only measures the topical simila-
rity of words with the main topics of t. However,
when 0 &lt; A &lt; 1, as soon as a word is selected
from a topic, other words from the same topic start
having diminishing gains.
</bodyText>
<sectionHeader confidence="0.986753" genericHeader="method">
4 Data and Evaluation Method
</sectionHeader>
<bodyText confidence="0.9996135">
The proposed keyword extraction method was
tested on two conversational corpora, the Fisher
</bodyText>
<equation confidence="0.536779">
1A function F is submodular if `dA C B C T \ t, F(A+
t) − F(A) &gt; F(B + t) − F(B) (diminishing returns) and
is monotone nondecreasing if `dA C B, F(A) &lt; F(B).
</equation>
<page confidence="0.970102">
652
</page>
<figure confidence="0.898391">
(a) (b)
Please select one of the following options:
</figure>
<listItem confidence="0.883044">
1. Image (a) represents the conversation fragment better than (b).
2. Image (b) represents the conversation fragment better than (a).
3. Both (a) and (b) offer a good representation of the conversation.
4. None of (a) and (b) offer a good representation of the conversation.
</listItem>
<figureCaption confidence="0.954859">
Figure 1: Example of a HIT based on an AMI discussion about the impact on sales of some features of
</figureCaption>
<bodyText confidence="0.873148">
remote controls (the conversation transcript is given in the Appendix). The word cloud was generated
using WordleTM from the list produced by the diverse keyword extraction method with A = 0.75 (noted
D(.75)) for image (a) and by a topic similarity method (TS) for image (b). TS over-represents the topic
“color” by selecting three words related to it, but misses other topics such as “remote control”, “losing a
device” and “buying a device” which are also representative of the fragment.
Input : a given text t, a set of topics Z, the
number of keywords k
Output: a set of keywords S
</bodyText>
<equation confidence="0.923904142857143">
S ← ∅;
while |S |≤ k do
S ← S U {argmaxw∈t\S(h(w)) where
h(w) = &amp;∈Z p(z|t)[r{w},z + rS,z]λ};
end
return S;
Algorithm 1: Diverse keyword extraction.
</equation>
<bodyText confidence="0.99990362">
Corpus (Cieri et al., 2004), and the AMI Meeting
Corpus (Carletta, 2007). The former corpus con-
tains about 11,000 topic-labeled telephone conver-
sations, on 40 pre-selected topics (one per con-
versation). We created a topic model using Mal-
let over two thirds of the Fisher Corpus, given its
large number of single-topic documents, with 40
topics. The remaining data is used to build 11
artificial “conversations” (1-2 minutes long) for
testing, by concatenating 11 times three fragments
about three different topics.
The AMI Corpus contains 171 half-hour meet-
ings about remote control design, which include
several topics each – so they cannot be directly
used for learning topic models. While selecting
for testing 8 conversation fragments of 2-3 min-
utes each, we trained topic models on a subset of
the English Wikipedia (10% or 124,684 articles).
Following several previous studies, the number of
topics was set to 100 (Boyd-Graber et al., 2009;
Hoffman et al., 2010).
To evaluate the relevance (or representative-
ness) of extracted keywords with respect to a
conversation fragment, we designed comparison
tasks. In each task, a fragment is shown, followed
by three control questions about its content, and
then by two lists of nine keywords each, from two
different extraction methods. To improve readabil-
ity, the keyword lists are presented to the judges
using a word cloud representation generated by
WordleTM (http://www.wordle.net), in which the
words ranked higher are emphasized in the word
cloud (see example in Figure 1). The judges had
to read the conversation transcript, answer the con-
trol questions, and then decide which word cloud
better represents the content of the conversation.
The tasks were crowdsourced via Amazon’s
Mechanical Turk (AMT) as “human intelligence
tasks” (HITs). One of them is exemplified in Fig-
ure 1, without the control questions, and the re-
spective conversation transcript is given in the Ap-
pendix. Ten workers were recruited for each cor-
pus. An example of judgment counts for each of
the 8 AMI HITs comparing two methods is shown
in Table 1. After collecting judgments, the com-
parative relevance values were computed by first
applying a qualification control factor to the hu-
man judgments, and then averaging results over
all judgments (Habibi and Popescu-Belis, 2012).
Moreover, to verify the diversity of the key-
</bodyText>
<page confidence="0.991675">
653
</page>
<figure confidence="0.999865625">
ߙ െNDCG values
0.95
0.85
0.75
0.65
1.05
0.9
0.8
0.7
1
8 9 10 11 12 13 14 15
anking
D(0.50)
D(0.75)
TS
WF
</figure>
<figureCaption confidence="0.9882605">
Figure 2: Average α-NDCG over the 11 conversations from the Fisher Corpus, for 1 to 15 extracted
keywords.
</figureCaption>
<bodyText confidence="0.9993411">
word set, we use the α-NDCG measure (Clarke
et al., 2008) proposed for information retrieval,
which rewards a mixture of relevance and diver-
sity – with equal weights when α = .5 as set here.
We only apply α-NDCG to the three-topic con-
versation fragments from the Fisher Corpus, rel-
evance of a keyword being set to 1 when it be-
longs to the fragment corresponding to the topic.
A higher value indicates that keywords are more
uniformly distributed across the three topics.
</bodyText>
<sectionHeader confidence="0.998095" genericHeader="method">
5 Experimental Results
</sectionHeader>
<bodyText confidence="0.995172444444445">
We have compared several versions of the diverse
keyword extraction method, noted D(A), for A ∈
{.5,.75, 1}, with two other methods. The first
one uses only word frequency (not including stop-
words) and is noted WF. We did not use TFIDF
because it sets low weights on keywords that are
repeated in many fragments but which are never-
theless important to extract. The second method is
based on topical similarity (noted TS) but does not
specifically enforce diversity (Harwath and Hazen,
2012). In fact TS coincides with D(1), so it is
noted TS. As the relevance of keywords for D(.5)
was already quite low, we did not test lower values
of A. Similarly, we did not test additional values
of A above .5 because the resulting word lists were
very similar to tested values.
First of all, we compared the four methods with
respect to the diversity constraint over the con-
</bodyText>
<table confidence="0.9846122">
HIT A B C D E F G H
TS more relevant 4 1 1 1 2 2 1 1
D(.75) more rel. 4 1 8 9 6 6 6 8
Both relevant 2 5 1 0 2 2 3 1
Both irrelevant 0 3 0 0 0 0 0 0
</table>
<tableCaption confidence="0.90367075">
Table 1: Number of answers for each of the four
options of the comparative evaluation task, from
ten human judges. The 8 HITs compare the D(.75)
and TS methods on 8 AMI HITs.
</tableCaption>
<table confidence="0.999263555555556">
Corpus Compared methods Relevance (%)
(m1 vs. m2)
m1 m2
Fisher D(.75) vs. TS 68 32
TS vs. WF 82 18
WF vs. D(.5) 95 5
AMI D(.75) vs. TS 78 22
TS vs. WF 60 40
WF vs. D(.5) 78 22
</table>
<tableCaption confidence="0.9896735">
Table 2: Comparative relevance scores of keyword
extraction methods based on human judgments.
</tableCaption>
<bodyText confidence="0.9992162">
catenated fragments of the Fisher Corpus, by us-
ing α-NDCG to measure how evenly the extracted
keywords were distributed across the three topics.
Figure 2 shows results averaged over 11 conversa-
tions for various sizes of the keyword set (1–15).
The average α-NDCG values for D(.75) and D(.5)
are similar, and clearly higher than WF and TS
for all ranks (except, of course, for a single key-
word). The values for TS are quite low, and only
increase for a large number of keywords, demon-
strating that TS does not cope well with topic di-
versity, but on the contrary first selects keywords
from the dominant topic. The values for WF are
more uniform as it does not consider topics at all.
To measure the overall representativeness of
keywords, we performed binary comparisons be-
tween the outputs of each method, using crowd-
sourcing, over 11 fragments from the Fisher Cor-
pus and 8 fragments from AMI. The goal is to
rank the methods, so we only report here on
the comparisons required for complete ordering.
AMT workers compared two lists of nine key-
words each, with four options: X more represen-
tative or relevant than Y , or vice-versa, or both
relevant, or both irrelevant. Table 1 shows the
judgments collected when comparing the output of
D(.75) with TS on the AMI Corpus. Workers dis-
agreed for the first two HITs, but then found that
the keywords extracted by D(.75) were more rep-
resentative compared to TS. The consolidated rel-
</bodyText>
<page confidence="0.998518">
654
</page>
<bodyText confidence="0.99967485">
evance (Habibi and Popescu-Belis, 2012) is 78%
for D(.75) vs. 22% for TS.
The averaged relevance values for all compar-
isons needed to rank the four methods are shown
in Table 2 separately for the Fisher and AMI Cor-
pora. Although the exact differences vary, the hu-
man judgments over the two corpora both indi-
cate the following ranking: D(.75) &gt; TS &gt; WF &gt;
D(.5). The optimal value of A is thus around .75,
and with this value, our diversity-aware method
extracts more representative keyword sets than TS
and WF. The differences between methods are
larger for the Fisher Corpus, due to the artificial
fragments that concatenate three topics, but they
are still visible on the natural fragments of the
AMI Corpus. The low scores of D(.5) are found
to be due, upon inspection, to the low relevance
of keywords. In particular, the comparative rele-
vance of D(.75) vs. D(.5) on the Fisher Corpus is
very large (96% vs. 4%).
</bodyText>
<sectionHeader confidence="0.996172" genericHeader="method">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9998563125">
The diverse keyword extraction method with A =
.75 provides the keyword sets that are judged most
representative of the conversation fragments (two
conversational datasets) by a large number of hu-
man judges recruited via AMT, and has the high-
est α-NDCG value. Therefore, enforcing both rel-
evance and diversity brings an effective improve-
ment to keyword extraction.
Setting A for a new dataset remains an issue,
and requires a small development data set. How-
ever, preliminary experiments with a third dataset
showed that A = .75 remains a good value.
In the future, we will use keywords to re-
trieve documents from a repository and recom-
mend them to conversation participants by formu-
lating topically-separate queries.
</bodyText>
<sectionHeader confidence="0.856007" genericHeader="method">
Appendix: Conversation transcript of
AMI ES2005a meeting (00:00:5-00:01:52)
</sectionHeader>
<bodyText confidence="0.682505761904762">
The following transcript of a four-party conversa-
tions (speakers noted A through D) was submitted
to our keyword extraction method and a baseline
one, generating respectively the two word clouds
shown in Figure 1.
A: The only the only remote controls
I’ve used usually come with the
television, and they’re fairly basic.
So uh
D: Yeah. Yeah.
C: Mm-hmm.
D: Yeah, I was thinking that as well,
I think the the only ones that I’ve seen
that you buy are the sort of one for
all type things where they’re, yeah. So
presumably that might be an idea to
C: Yeah the universal ones. Yeah.
A: Mm. But but to sell it for twenty
five you need a lot of neat features.
For sure.
D: put into.
</bodyText>
<listItem confidence="0.938950954545454">
C: Yeah.
D: Yeah, yeah. Uh ’cause I mean, what
uh twenty five Euros, that’s about I
dunno, fifteen Pounds or so?
C: Mm-hmm, it’s about that.
D: And that’s quite a lot for a remote
control.
A: Yeah, yeah.
C: Mm. Um well my first thoughts
would be most remote controls are grey
or black. As you said they come with
the TV so it’s normally just your basic
grey black remote control functions, so
maybe we could think about colour? Make
that might make it a bit different from
the rest at least. Um, and as you say,
we need to have some kind of gimmick, so
um I thought maybe something like if you
lose it and you can whistle, you know
those things?
D: Uh-huh. Mm-hmm. Okay. The the
keyrings, yeah yeah. Okay, that’s cool.
</listItem>
<bodyText confidence="0.979120607142857">
C: Because we always lose our remote
control.
B: Uh yeah uh, being as a Marketing
Expert I will like to say like before
deciding the cost of this remote control
or any other things we must see the
market potential for this product like
what is the competition in the market?
What are the available prices of the
other remote controls in the prices?
What speciality other remote controls
are having and how complicated it is to
use these remote controls as compared to
other remote controls available in the
market.
D: Okay.
B: So before deciding or before
finalising this project, we must discuss
all these things, like and apart from
this, it should be having a good look
also, because people really uh like
to play with it when they are watching
movies or playing with or playing with
their CD player, MP three player like
any electronic devices. They really
want to have something good, having a
good design in their hands, so, yes, all
this.
</bodyText>
<sectionHeader confidence="0.998811" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99896775">
The authors are grateful to the Swiss National Sci-
ence Foundation for its financial support through
the IM2 NCCR on Interactive Multimodal Infor-
mation Management (see www.im2.ch).
</bodyText>
<page confidence="0.998767">
655
</page>
<sectionHeader confidence="0.996375" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.983514194444444">
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993–1022.
Jonathan Boyd-Graber, Jordan Chang, Sean Gerrish,
Chong Wang, and David Blei. 2009. Reading tea
leaves: How humans interpret topic models. In Pro-
ceedings of the 23rd Annual Conference on Neural
Information Processing Systems (NIPS).
Jean Carletta. 2007. Unleashing the killer corpus:
Experiences in creating the multi-everything AMI
Meeting Corpus. Language Resources and Evalu-
ation Journal, 41(2):181–190.
Christopher Cieri, David Miller, and Kevin Walker.
2004. The Fisher Corpus: a resource for the next
generations of speech-to-text. In Proceedings of 4th
International Conference on Language Resources
and Evaluation (LREC), pages 69–71.
Charles L. A. Clarke, Maheedhar Kolla, Gordon V.
Cormack, Olga Vechtomova, Azin Ashkan, Stefan
B¨uttcher, and Ian MacKinnon. 2008. Novelty and
diversity in information retrieval evaluation. In Pro-
ceedings of the 31st annual international ACM SI-
GIR conference on Research and development in in-
formation retrieval, pages 659–666.
Andras Csomai and Rada Mihalcea. 2007. Linking
educational materials to encyclopedic knowledge.
Frontiers in Artificial Intelligence and Applications,
158:557.
Eibe Frank, Gordon W. Paynter, Ian H. Witten, Carl
Gutwin, and Craig G. Nevill-Manning. 1999.
Domain-specific keyphrase extraction. In Proceed-
ings of the 16th International Joint Conference on
Artificial Intelligence (IJCAI 1999), pages 668–673,
Stockholm, Sweden.
Maryam Habibi and Andrei Popescu-Belis. 2012. Us-
ing crowdsourcing to compare document recom-
mendation strategies for conversations. In Work-
shop on Recommendation Utility Evaluation: Be-
yond RMSE (RUE 2011), page 15.
David Harwath and Timothy J. Hazen. 2012. Topic
identification based extrinsic evaluation of summa-
rization techniques applied to conversational speech.
In Proceedings of International Conference on
Acoustics, Speech and Signal Processing (ICASSP),
pages 5073–5076. IEEE.
Matthew D. Hoffman, David M. Blei, and Francis
Bach. 2010. Online learning for Latent Dirichlet
Allocation. Proceedings of 24th Annual Conference
on Neural Information Processing Systems, 23:856–
864.
Anette Hulth. 2003. Improved automatic keyword ex-
traction given more linguistic knowledge. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP 2003),
pages 216–223, Sapporo, Japan.
Jingxuan Li, Lei Li, and Tao Li. 2012. Multi-
document summarization via submodularity. Ap-
plied Intelligence, 37(3):420–430.
Hui Lin and Jeff Bilmes. 2011. A class of submodular
functions for document summarization. In Proceed-
ings of the 49th Annual Meeting of the ACL.
Feifan Liu, Deana Pennell, Fei Liu, and Yang Liu.
2009a. Unsupervised approaches for automatic key-
word extraction using meeting transcripts. In Pro-
ceedings of the 2009 Annual Conference of the
North American Chapter of the ACL (HLT-NAACL),
pages 620–628.
Zhiyuan Liu, Wenyi Huang, Yabin Zheng, and
Maosong Sun. 2009b. Clustering to find exemplar
terms for keyphrase extraction. In Proceedings of
the 2009 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP 2009), pages
257–266.
Zhiyuan Liu, Wenyi Huang, Yabin Zheng, and
Maosong Sun. 2010. Automatic keyphrase extrac-
tion via topic decomposition. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2010), pages 366–
376.
Hans Peter Luhn. 1957. A statistical approach to
mechanized encoding and searching of literary in-
formation. IBM Journal of Research and Develop-
ment, 1(4):309–317.
Yutaka Matsuo and Mitsuru Ishizuka. 2004. Key-
word extraction from a single document using word
co-occurrence statistical information. International
Journal on Artificial Intelligence Tools, 13(1):157–
169.
Andrew K. McCallum. 2002. MALLET:
A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Rada Mihalcea and Paul Tarau. 2004. TextRank:
Bringing order into texts. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2004), pages 404–411,
Barcelona.
George L. Nemhauser, Laurence A. Wolsey, and Mar-
shall L. Fisher. 1978. An analysis of approxi-
mations for maximizing submodular set functions.
Mathematical Programming Journal, 14(1):265–
294.
Ani Nenkova and Kathleen McKeown, 2012. A Survey
of Text Summarization Techniques, chapter 3, pages
43–76. Springer.
Gerard Salton and Christopher Buckley. 1988. Term-
weighting approaches in automatic text retrieval.
Information Processing and Management Journal,
24(5):513–523.
</reference>
<page confidence="0.98751">
656
</page>
<reference confidence="0.999613">
Gerard Salton, Chung-Shu Yang, and Clement T. Yu.
1975. A theory of term importance in automatic text
analysis. Journal of the American Society for Infor-
mation Science, 26(1):33–44.
Peter Turney. 1999. Learning to extract keyphrases
from text. Technical Report ERB-1057, National
Research Council Canada (NRC).
Jinghua Wang, Jianyi Liu, and Cong Wang. 2007.
Keyword extraction based on PageRank. In Ad-
vances in Knowledge Discovery and Data Mining
(Proceedings of PAKDD 2007), LNAI 4426, pages
857–864. Springer-Verlag, Berlin.
Shiren Ye, Tat-Seng Chua, Min-Yen Kan, and Long
Qiu. 2007. Document concept lattice for text un-
derstanding and summarization. Information Pro-
cessing and Management, 43(6):1643–1662.
</reference>
<page confidence="0.998028">
657
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.430523">
<title confidence="0.986232">Diverse Keyword Extraction from Conversations</title>
<author confidence="0.666066">Maryam</author>
<affiliation confidence="0.86729">Idiap Research Institute and</affiliation>
<address confidence="0.8295645">Rue Marconi 19, CP 1920 Martigny,</address>
<email confidence="0.9278">maryam.habibi@idiap.ch</email>
<author confidence="0.815739">Andrei</author>
<affiliation confidence="0.979745">Idiap Research</affiliation>
<address confidence="0.9604325">Rue Marconi 19, CP 1920 Martigny,</address>
<email confidence="0.986895">andrei.popescu-belis@idiap.ch</email>
<abstract confidence="0.999331071428571">A new method for keyword extraction from conversations is introduced, which preserves the diversity of topics that are mentioned. Inspired from summarization, the method maximizes the coverage of topics that are recognized automatically in transcripts of conversation fragments. The method is evaluated on excerpts of the Fisher and AMI corpora, using a crowdsourcing platform to elicit comparative relevance judgments. The results demonstrate that the method outperforms two competitive baselines.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet Allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="5438" citStr="Blei et al., 2003" startWordPosition="835" endWordPosition="838">t, and then to select keywords using topical similarity while also rewarding the diversity of topic coverage, inspired by recent summarization methods (Lin and Bilmes, 2011; Li et al., 2012). 3.1 Representing Topic Information Topic models such as Probabilistic Latent Semantic Analysis (PLSA) or Latent Dirichlet Allocation (LDA) can be used to determine the distribution over the topic z of a word w, noted p(z|w), from a large amount of training documents. LDA implemented in the Mallet toolkit (McCallum, 2002) is used in this paper because it does not suffer from the overfitting issue of PLSA (Blei et al., 2003). The distribution of each topic z in a given conversation fragment t, noted p(z|t), can be computed by summing over all probabilities p(z|w) of the N words w spoken in the fragment: 1 p(z|t) = N p(z|w). wEt 3.2 Selecting Keywords The problem of keyword extraction with maximal topic coverage is formulated as follows. If a conversation fragment t mentions a set of topics Z, and each word w from the fragment t can evoke a subset of the topics in Z, then the goal is to find a subset of unique words S C_ t, with |S |&lt; k, which maximzes the number of covered topics for each number of keywords k. Th</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet Allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Boyd-Graber</author>
<author>Jordan Chang</author>
<author>Sean Gerrish</author>
<author>Chong Wang</author>
<author>David Blei</author>
</authors>
<title>Reading tea leaves: How humans interpret topic models.</title>
<date>2009</date>
<booktitle>In Proceedings of the 23rd Annual Conference on Neural Information Processing Systems (NIPS).</booktitle>
<contexts>
<context position="10293" citStr="Boyd-Graber et al., 2009" startWordPosition="1700" endWordPosition="1703">uments, with 40 topics. The remaining data is used to build 11 artificial “conversations” (1-2 minutes long) for testing, by concatenating 11 times three fragments about three different topics. The AMI Corpus contains 171 half-hour meetings about remote control design, which include several topics each – so they cannot be directly used for learning topic models. While selecting for testing 8 conversation fragments of 2-3 minutes each, we trained topic models on a subset of the English Wikipedia (10% or 124,684 articles). Following several previous studies, the number of topics was set to 100 (Boyd-Graber et al., 2009; Hoffman et al., 2010). To evaluate the relevance (or representativeness) of extracted keywords with respect to a conversation fragment, we designed comparison tasks. In each task, a fragment is shown, followed by three control questions about its content, and then by two lists of nine keywords each, from two different extraction methods. To improve readability, the keyword lists are presented to the judges using a word cloud representation generated by WordleTM (http://www.wordle.net), in which the words ranked higher are emphasized in the word cloud (see example in Figure 1). The judges had</context>
</contexts>
<marker>Boyd-Graber, Chang, Gerrish, Wang, Blei, 2009</marker>
<rawString>Jonathan Boyd-Graber, Jordan Chang, Sean Gerrish, Chong Wang, and David Blei. 2009. Reading tea leaves: How humans interpret topic models. In Proceedings of the 23rd Annual Conference on Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Carletta</author>
</authors>
<title>Unleashing the killer corpus: Experiences in creating the multi-everything AMI Meeting Corpus.</title>
<date>2007</date>
<journal>Language Resources and Evaluation Journal,</journal>
<volume>41</volume>
<issue>2</issue>
<contexts>
<context position="9420" citStr="Carletta, 2007" startWordPosition="1562" endWordPosition="1563">75 (noted D(.75)) for image (a) and by a topic similarity method (TS) for image (b). TS over-represents the topic “color” by selecting three words related to it, but misses other topics such as “remote control”, “losing a device” and “buying a device” which are also representative of the fragment. Input : a given text t, a set of topics Z, the number of keywords k Output: a set of keywords S S ← ∅; while |S |≤ k do S ← S U {argmaxw∈t\S(h(w)) where h(w) = &amp;∈Z p(z|t)[r{w},z + rS,z]λ}; end return S; Algorithm 1: Diverse keyword extraction. Corpus (Cieri et al., 2004), and the AMI Meeting Corpus (Carletta, 2007). The former corpus contains about 11,000 topic-labeled telephone conversations, on 40 pre-selected topics (one per conversation). We created a topic model using Mallet over two thirds of the Fisher Corpus, given its large number of single-topic documents, with 40 topics. The remaining data is used to build 11 artificial “conversations” (1-2 minutes long) for testing, by concatenating 11 times three fragments about three different topics. The AMI Corpus contains 171 half-hour meetings about remote control design, which include several topics each – so they cannot be directly used for learning </context>
</contexts>
<marker>Carletta, 2007</marker>
<rawString>Jean Carletta. 2007. Unleashing the killer corpus: Experiences in creating the multi-everything AMI Meeting Corpus. Language Resources and Evaluation Journal, 41(2):181–190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Cieri</author>
<author>David Miller</author>
<author>Kevin Walker</author>
</authors>
<title>The Fisher Corpus: a resource for the next generations of speech-to-text.</title>
<date>2004</date>
<booktitle>In Proceedings of 4th International Conference on Language Resources and Evaluation (LREC),</booktitle>
<pages>69--71</pages>
<contexts>
<context position="9375" citStr="Cieri et al., 2004" startWordPosition="1553" endWordPosition="1556">the diverse keyword extraction method with A = 0.75 (noted D(.75)) for image (a) and by a topic similarity method (TS) for image (b). TS over-represents the topic “color” by selecting three words related to it, but misses other topics such as “remote control”, “losing a device” and “buying a device” which are also representative of the fragment. Input : a given text t, a set of topics Z, the number of keywords k Output: a set of keywords S S ← ∅; while |S |≤ k do S ← S U {argmaxw∈t\S(h(w)) where h(w) = &amp;∈Z p(z|t)[r{w},z + rS,z]λ}; end return S; Algorithm 1: Diverse keyword extraction. Corpus (Cieri et al., 2004), and the AMI Meeting Corpus (Carletta, 2007). The former corpus contains about 11,000 topic-labeled telephone conversations, on 40 pre-selected topics (one per conversation). We created a topic model using Mallet over two thirds of the Fisher Corpus, given its large number of single-topic documents, with 40 topics. The remaining data is used to build 11 artificial “conversations” (1-2 minutes long) for testing, by concatenating 11 times three fragments about three different topics. The AMI Corpus contains 171 half-hour meetings about remote control design, which include several topics each – </context>
</contexts>
<marker>Cieri, Miller, Walker, 2004</marker>
<rawString>Christopher Cieri, David Miller, and Kevin Walker. 2004. The Fisher Corpus: a resource for the next generations of speech-to-text. In Proceedings of 4th International Conference on Language Resources and Evaluation (LREC), pages 69–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles L A Clarke</author>
<author>Maheedhar Kolla</author>
<author>Gordon V Cormack</author>
<author>Olga Vechtomova</author>
<author>Azin Ashkan</author>
<author>Stefan B¨uttcher</author>
<author>Ian MacKinnon</author>
</authors>
<title>Novelty and diversity in information retrieval evaluation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>659--666</pages>
<marker>Clarke, Kolla, Cormack, Vechtomova, Ashkan, B¨uttcher, MacKinnon, 2008</marker>
<rawString>Charles L. A. Clarke, Maheedhar Kolla, Gordon V. Cormack, Olga Vechtomova, Azin Ashkan, Stefan B¨uttcher, and Ian MacKinnon. 2008. Novelty and diversity in information retrieval evaluation. In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval, pages 659–666.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andras Csomai</author>
<author>Rada Mihalcea</author>
</authors>
<title>Linking educational materials to encyclopedic knowledge.</title>
<date>2007</date>
<booktitle>Frontiers in Artificial Intelligence and Applications,</booktitle>
<pages>158--557</pages>
<contexts>
<context position="3118" citStr="Csomai and Mihalcea, 2007" startWordPosition="469" endWordPosition="472">action. These approaches do not consider word meaning, so they may ignore lowfrequency words which together indicate a highlysalient topic (Nenkova and McKeown, 2012). To improve over frequency-based methods, several ways to use lexical semantic information have been proposed. Semantic relations between words can be obtained from a manuallyconstructed thesaurus such as WordNet, or from Wikipedia, or from an automatically-built thesaurus using latent topic modeling techniques. Ye et al. (2007) used the frequency of all words belonging to the same WordNet concept set, while the Wikifier system (Csomai and Mihalcea, 2007) relied on Wikipedia links to compute a substitute to word frequency. Harwath and Hazen (2012) used topic modeling with PLSA to build a thesaurus, which they used to rank words based on topical similarity to the topics of a transcribed conversation. To consider dependencies among selected words, word co-occurrence has been combined with PageRank by Mihalcea and Tarau (2004), and additionally with WordNet by Wang et al. (2007), or with topical information by Z. Liu et al. (2010). However, as shown empirically by Mihalcea and Tarau (2004) and by Z. Liu et al. (2010) with various co-occurrence wi</context>
</contexts>
<marker>Csomai, Mihalcea, 2007</marker>
<rawString>Andras Csomai and Rada Mihalcea. 2007. Linking educational materials to encyclopedic knowledge. Frontiers in Artificial Intelligence and Applications, 158:557.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eibe Frank</author>
<author>Gordon W Paynter</author>
<author>Ian H Witten</author>
<author>Carl Gutwin</author>
<author>Craig G Nevill-Manning</author>
</authors>
<title>Domain-specific keyphrase extraction.</title>
<date>1999</date>
<booktitle>In Proceedings of the 16th International Joint Conference on Artificial Intelligence (IJCAI</booktitle>
<pages>668--673</pages>
<location>Stockholm,</location>
<contexts>
<context position="4528" citStr="Frank et al., 1999" startWordPosition="688" endWordPosition="691">uistics, pages 651–657, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics topic. Z. Liu et al. (2009b) used part-of-speech information and word clustering techniques, while F. Liu et al. (2009a) added this information to the TFIDF method so as to consider both word dependency and semantic information. However, although they considered topical similarity, the above methods did not explicitly reward diversity and might miss secondary topics. Supervised methods have been used to learn a model for extracting keywords with various learning algorithms (Turney, 1999; Frank et al., 1999; Hulth, 2003). These approaches, however, rely on the availability of in-domain training data, and the objective functions they use for learning do not consider yet the diversity of keywords. 3 Diverse Keyword Extraction We propose to build a topical representation of a conversation fragment, and then to select keywords using topical similarity while also rewarding the diversity of topic coverage, inspired by recent summarization methods (Lin and Bilmes, 2011; Li et al., 2012). 3.1 Representing Topic Information Topic models such as Probabilistic Latent Semantic Analysis (PLSA) or Latent Diri</context>
</contexts>
<marker>Frank, Paynter, Witten, Gutwin, Nevill-Manning, 1999</marker>
<rawString>Eibe Frank, Gordon W. Paynter, Ian H. Witten, Carl Gutwin, and Craig G. Nevill-Manning. 1999. Domain-specific keyphrase extraction. In Proceedings of the 16th International Joint Conference on Artificial Intelligence (IJCAI 1999), pages 668–673, Stockholm, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maryam Habibi</author>
<author>Andrei Popescu-Belis</author>
</authors>
<title>Using crowdsourcing to compare document recommendation strategies for conversations.</title>
<date>2012</date>
<booktitle>In Workshop on Recommendation Utility Evaluation: Beyond RMSE (RUE 2011),</booktitle>
<pages>15</pages>
<contexts>
<context position="11656" citStr="Habibi and Popescu-Belis, 2012" startWordPosition="1917" endWordPosition="1920">f the conversation. The tasks were crowdsourced via Amazon’s Mechanical Turk (AMT) as “human intelligence tasks” (HITs). One of them is exemplified in Figure 1, without the control questions, and the respective conversation transcript is given in the Appendix. Ten workers were recruited for each corpus. An example of judgment counts for each of the 8 AMI HITs comparing two methods is shown in Table 1. After collecting judgments, the comparative relevance values were computed by first applying a qualification control factor to the human judgments, and then averaging results over all judgments (Habibi and Popescu-Belis, 2012). Moreover, to verify the diversity of the key653 ߙ െNDCG values 0.95 0.85 0.75 0.65 1.05 0.9 0.8 0.7 1 8 9 10 11 12 13 14 15 anking D(0.50) D(0.75) TS WF Figure 2: Average α-NDCG over the 11 conversations from the Fisher Corpus, for 1 to 15 extracted keywords. word set, we use the α-NDCG measure (Clarke et al., 2008) proposed for information retrieval, which rewards a mixture of relevance and diversity – with equal weights when α = .5 as set here. We only apply α-NDCG to the three-topic conversation fragments from the Fisher Corpus, relevance of a keyword being set to 1 when it belongs to the</context>
<context position="15339" citStr="Habibi and Popescu-Belis, 2012" startWordPosition="2602" endWordPosition="2605">ments from the Fisher Corpus and 8 fragments from AMI. The goal is to rank the methods, so we only report here on the comparisons required for complete ordering. AMT workers compared two lists of nine keywords each, with four options: X more representative or relevant than Y , or vice-versa, or both relevant, or both irrelevant. Table 1 shows the judgments collected when comparing the output of D(.75) with TS on the AMI Corpus. Workers disagreed for the first two HITs, but then found that the keywords extracted by D(.75) were more representative compared to TS. The consolidated rel654 evance (Habibi and Popescu-Belis, 2012) is 78% for D(.75) vs. 22% for TS. The averaged relevance values for all comparisons needed to rank the four methods are shown in Table 2 separately for the Fisher and AMI Corpora. Although the exact differences vary, the human judgments over the two corpora both indicate the following ranking: D(.75) &gt; TS &gt; WF &gt; D(.5). The optimal value of A is thus around .75, and with this value, our diversity-aware method extracts more representative keyword sets than TS and WF. The differences between methods are larger for the Fisher Corpus, due to the artificial fragments that concatenate three topics, </context>
</contexts>
<marker>Habibi, Popescu-Belis, 2012</marker>
<rawString>Maryam Habibi and Andrei Popescu-Belis. 2012. Using crowdsourcing to compare document recommendation strategies for conversations. In Workshop on Recommendation Utility Evaluation: Beyond RMSE (RUE 2011), page 15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Harwath</author>
<author>Timothy J Hazen</author>
</authors>
<title>Topic identification based extrinsic evaluation of summarization techniques applied to conversational speech.</title>
<date>2012</date>
<booktitle>In Proceedings of International Conference on Acoustics, Speech and Signal Processing (ICASSP),</booktitle>
<pages>5073--5076</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="3212" citStr="Harwath and Hazen (2012)" startWordPosition="484" endWordPosition="487">ch together indicate a highlysalient topic (Nenkova and McKeown, 2012). To improve over frequency-based methods, several ways to use lexical semantic information have been proposed. Semantic relations between words can be obtained from a manuallyconstructed thesaurus such as WordNet, or from Wikipedia, or from an automatically-built thesaurus using latent topic modeling techniques. Ye et al. (2007) used the frequency of all words belonging to the same WordNet concept set, while the Wikifier system (Csomai and Mihalcea, 2007) relied on Wikipedia links to compute a substitute to word frequency. Harwath and Hazen (2012) used topic modeling with PLSA to build a thesaurus, which they used to rank words based on topical similarity to the topics of a transcribed conversation. To consider dependencies among selected words, word co-occurrence has been combined with PageRank by Mihalcea and Tarau (2004), and additionally with WordNet by Wang et al. (2007), or with topical information by Z. Liu et al. (2010). However, as shown empirically by Mihalcea and Tarau (2004) and by Z. Liu et al. (2010) with various co-occurrence windows, such approaches have difficulties modeling long-range dependencies between words relate</context>
<context position="12902" citStr="Harwath and Hazen, 2012" startWordPosition="2138" endWordPosition="2141">g to the topic. A higher value indicates that keywords are more uniformly distributed across the three topics. 5 Experimental Results We have compared several versions of the diverse keyword extraction method, noted D(A), for A ∈ {.5,.75, 1}, with two other methods. The first one uses only word frequency (not including stopwords) and is noted WF. We did not use TFIDF because it sets low weights on keywords that are repeated in many fragments but which are nevertheless important to extract. The second method is based on topical similarity (noted TS) but does not specifically enforce diversity (Harwath and Hazen, 2012). In fact TS coincides with D(1), so it is noted TS. As the relevance of keywords for D(.5) was already quite low, we did not test lower values of A. Similarly, we did not test additional values of A above .5 because the resulting word lists were very similar to tested values. First of all, we compared the four methods with respect to the diversity constraint over the conHIT A B C D E F G H TS more relevant 4 1 1 1 2 2 1 1 D(.75) more rel. 4 1 8 9 6 6 6 8 Both relevant 2 5 1 0 2 2 3 1 Both irrelevant 0 3 0 0 0 0 0 0 Table 1: Number of answers for each of the four options of the comparative eva</context>
</contexts>
<marker>Harwath, Hazen, 2012</marker>
<rawString>David Harwath and Timothy J. Hazen. 2012. Topic identification based extrinsic evaluation of summarization techniques applied to conversational speech. In Proceedings of International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5073–5076. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew D Hoffman</author>
<author>David M Blei</author>
<author>Francis Bach</author>
</authors>
<title>Online learning for Latent Dirichlet Allocation.</title>
<date>2010</date>
<booktitle>Proceedings of 24th Annual Conference on Neural Information Processing Systems,</booktitle>
<pages>23--856</pages>
<contexts>
<context position="10316" citStr="Hoffman et al., 2010" startWordPosition="1704" endWordPosition="1707">e remaining data is used to build 11 artificial “conversations” (1-2 minutes long) for testing, by concatenating 11 times three fragments about three different topics. The AMI Corpus contains 171 half-hour meetings about remote control design, which include several topics each – so they cannot be directly used for learning topic models. While selecting for testing 8 conversation fragments of 2-3 minutes each, we trained topic models on a subset of the English Wikipedia (10% or 124,684 articles). Following several previous studies, the number of topics was set to 100 (Boyd-Graber et al., 2009; Hoffman et al., 2010). To evaluate the relevance (or representativeness) of extracted keywords with respect to a conversation fragment, we designed comparison tasks. In each task, a fragment is shown, followed by three control questions about its content, and then by two lists of nine keywords each, from two different extraction methods. To improve readability, the keyword lists are presented to the judges using a word cloud representation generated by WordleTM (http://www.wordle.net), in which the words ranked higher are emphasized in the word cloud (see example in Figure 1). The judges had to read the conversati</context>
</contexts>
<marker>Hoffman, Blei, Bach, 2010</marker>
<rawString>Matthew D. Hoffman, David M. Blei, and Francis Bach. 2010. Online learning for Latent Dirichlet Allocation. Proceedings of 24th Annual Conference on Neural Information Processing Systems, 23:856– 864.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anette Hulth</author>
</authors>
<title>Improved automatic keyword extraction given more linguistic knowledge.</title>
<date>2003</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<pages>216--223</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="4542" citStr="Hulth, 2003" startWordPosition="692" endWordPosition="693">57, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics topic. Z. Liu et al. (2009b) used part-of-speech information and word clustering techniques, while F. Liu et al. (2009a) added this information to the TFIDF method so as to consider both word dependency and semantic information. However, although they considered topical similarity, the above methods did not explicitly reward diversity and might miss secondary topics. Supervised methods have been used to learn a model for extracting keywords with various learning algorithms (Turney, 1999; Frank et al., 1999; Hulth, 2003). These approaches, however, rely on the availability of in-domain training data, and the objective functions they use for learning do not consider yet the diversity of keywords. 3 Diverse Keyword Extraction We propose to build a topical representation of a conversation fragment, and then to select keywords using topical similarity while also rewarding the diversity of topic coverage, inspired by recent summarization methods (Lin and Bilmes, 2011; Li et al., 2012). 3.1 Representing Topic Information Topic models such as Probabilistic Latent Semantic Analysis (PLSA) or Latent Dirichlet Allocati</context>
</contexts>
<marker>Hulth, 2003</marker>
<rawString>Anette Hulth. 2003. Improved automatic keyword extraction given more linguistic knowledge. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2003), pages 216–223, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jingxuan Li</author>
<author>Lei Li</author>
<author>Tao Li</author>
</authors>
<title>Multidocument summarization via submodularity.</title>
<date>2012</date>
<journal>Applied Intelligence,</journal>
<volume>37</volume>
<issue>3</issue>
<contexts>
<context position="5010" citStr="Li et al., 2012" startWordPosition="763" endWordPosition="766">sed methods have been used to learn a model for extracting keywords with various learning algorithms (Turney, 1999; Frank et al., 1999; Hulth, 2003). These approaches, however, rely on the availability of in-domain training data, and the objective functions they use for learning do not consider yet the diversity of keywords. 3 Diverse Keyword Extraction We propose to build a topical representation of a conversation fragment, and then to select keywords using topical similarity while also rewarding the diversity of topic coverage, inspired by recent summarization methods (Lin and Bilmes, 2011; Li et al., 2012). 3.1 Representing Topic Information Topic models such as Probabilistic Latent Semantic Analysis (PLSA) or Latent Dirichlet Allocation (LDA) can be used to determine the distribution over the topic z of a word w, noted p(z|w), from a large amount of training documents. LDA implemented in the Mallet toolkit (McCallum, 2002) is used in this paper because it does not suffer from the overfitting issue of PLSA (Blei et al., 2003). The distribution of each topic z in a given conversation fragment t, noted p(z|t), can be computed by summing over all probabilities p(z|w) of the N words w spoken in the</context>
<context position="6510" citStr="Li et al., 2012" startWordPosition="1029" endWordPosition="1032">the goal is to find a subset of unique words S C_ t, with |S |&lt; k, which maximzes the number of covered topics for each number of keywords k. This problem is an instance of the maximum coverage problem, which is NP-hard. Nemhauser et al. (1978) showed that a greedy algorithm can find an approximate solution guaranteed to be within (1 − e) ^_ 0.63 of the optimal solution if the coverage function is submodular and monotone nondecreasing1. To find a monotone submodular function for keyword extraction, we used inspiration from recent work on extractive summarization methods (Lin and Bilmes, 2011; Li et al., 2012), which proposed a square root function for diverse selection of sentences to cover the maximum number of key concepts of a given document. The function rewards diversity by increasing the gain of selecting a sentence including a concept that was not yet covered by a previously selected sentence. This must be adapted for keyword extraction by defining an appropriate reward function. We first introduce rS,z, the topical similarity with respect to topic z of the keyword set S selected from the fragment t, defined as follows: �rS,z = p(z|w) · p(z|t). wES We then propose the following reward funct</context>
</contexts>
<marker>Li, Li, Li, 2012</marker>
<rawString>Jingxuan Li, Lei Li, and Tao Li. 2012. Multidocument summarization via submodularity. Applied Intelligence, 37(3):420–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Lin</author>
<author>Jeff Bilmes</author>
</authors>
<title>A class of submodular functions for document summarization.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="4992" citStr="Lin and Bilmes, 2011" startWordPosition="759" endWordPosition="762">ondary topics. Supervised methods have been used to learn a model for extracting keywords with various learning algorithms (Turney, 1999; Frank et al., 1999; Hulth, 2003). These approaches, however, rely on the availability of in-domain training data, and the objective functions they use for learning do not consider yet the diversity of keywords. 3 Diverse Keyword Extraction We propose to build a topical representation of a conversation fragment, and then to select keywords using topical similarity while also rewarding the diversity of topic coverage, inspired by recent summarization methods (Lin and Bilmes, 2011; Li et al., 2012). 3.1 Representing Topic Information Topic models such as Probabilistic Latent Semantic Analysis (PLSA) or Latent Dirichlet Allocation (LDA) can be used to determine the distribution over the topic z of a word w, noted p(z|w), from a large amount of training documents. LDA implemented in the Mallet toolkit (McCallum, 2002) is used in this paper because it does not suffer from the overfitting issue of PLSA (Blei et al., 2003). The distribution of each topic z in a given conversation fragment t, noted p(z|t), can be computed by summing over all probabilities p(z|w) of the N wor</context>
<context position="6492" citStr="Lin and Bilmes, 2011" startWordPosition="1025" endWordPosition="1028">the topics in Z, then the goal is to find a subset of unique words S C_ t, with |S |&lt; k, which maximzes the number of covered topics for each number of keywords k. This problem is an instance of the maximum coverage problem, which is NP-hard. Nemhauser et al. (1978) showed that a greedy algorithm can find an approximate solution guaranteed to be within (1 − e) ^_ 0.63 of the optimal solution if the coverage function is submodular and monotone nondecreasing1. To find a monotone submodular function for keyword extraction, we used inspiration from recent work on extractive summarization methods (Lin and Bilmes, 2011; Li et al., 2012), which proposed a square root function for diverse selection of sentences to cover the maximum number of key concepts of a given document. The function rewards diversity by increasing the gain of selecting a sentence including a concept that was not yet covered by a previously selected sentence. This must be adapted for keyword extraction by defining an appropriate reward function. We first introduce rS,z, the topical similarity with respect to topic z of the keyword set S selected from the fragment t, defined as follows: �rS,z = p(z|w) · p(z|t). wES We then propose the foll</context>
</contexts>
<marker>Lin, Bilmes, 2011</marker>
<rawString>Hui Lin and Jeff Bilmes. 2011. A class of submodular functions for document summarization. In Proceedings of the 49th Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Feifan Liu</author>
<author>Deana Pennell</author>
<author>Fei Liu</author>
<author>Yang Liu</author>
</authors>
<title>Unsupervised approaches for automatic keyword extraction using meeting transcripts.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Annual Conference of the North American Chapter of the ACL (HLT-NAACL),</booktitle>
<pages>620--628</pages>
<contexts>
<context position="4043" citStr="Liu et al. (2009" startWordPosition="615" endWordPosition="618">rrence has been combined with PageRank by Mihalcea and Tarau (2004), and additionally with WordNet by Wang et al. (2007), or with topical information by Z. Liu et al. (2010). However, as shown empirically by Mihalcea and Tarau (2004) and by Z. Liu et al. (2010) with various co-occurrence windows, such approaches have difficulties modeling long-range dependencies between words related to the same 651 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 651–657, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics topic. Z. Liu et al. (2009b) used part-of-speech information and word clustering techniques, while F. Liu et al. (2009a) added this information to the TFIDF method so as to consider both word dependency and semantic information. However, although they considered topical similarity, the above methods did not explicitly reward diversity and might miss secondary topics. Supervised methods have been used to learn a model for extracting keywords with various learning algorithms (Turney, 1999; Frank et al., 1999; Hulth, 2003). These approaches, however, rely on the availability of in-domain training data, and the objective f</context>
</contexts>
<marker>Liu, Pennell, Liu, Liu, 2009</marker>
<rawString>Feifan Liu, Deana Pennell, Fei Liu, and Yang Liu. 2009a. Unsupervised approaches for automatic keyword extraction using meeting transcripts. In Proceedings of the 2009 Annual Conference of the North American Chapter of the ACL (HLT-NAACL), pages 620–628.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhiyuan Liu</author>
<author>Wenyi Huang</author>
<author>Yabin Zheng</author>
<author>Maosong Sun</author>
</authors>
<title>Clustering to find exemplar terms for keyphrase extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<pages>257--266</pages>
<contexts>
<context position="4043" citStr="Liu et al. (2009" startWordPosition="615" endWordPosition="618">rrence has been combined with PageRank by Mihalcea and Tarau (2004), and additionally with WordNet by Wang et al. (2007), or with topical information by Z. Liu et al. (2010). However, as shown empirically by Mihalcea and Tarau (2004) and by Z. Liu et al. (2010) with various co-occurrence windows, such approaches have difficulties modeling long-range dependencies between words related to the same 651 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 651–657, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics topic. Z. Liu et al. (2009b) used part-of-speech information and word clustering techniques, while F. Liu et al. (2009a) added this information to the TFIDF method so as to consider both word dependency and semantic information. However, although they considered topical similarity, the above methods did not explicitly reward diversity and might miss secondary topics. Supervised methods have been used to learn a model for extracting keywords with various learning algorithms (Turney, 1999; Frank et al., 1999; Hulth, 2003). These approaches, however, rely on the availability of in-domain training data, and the objective f</context>
</contexts>
<marker>Liu, Huang, Zheng, Sun, 2009</marker>
<rawString>Zhiyuan Liu, Wenyi Huang, Yabin Zheng, and Maosong Sun. 2009b. Clustering to find exemplar terms for keyphrase extraction. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP 2009), pages 257–266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhiyuan Liu</author>
<author>Wenyi Huang</author>
<author>Yabin Zheng</author>
<author>Maosong Sun</author>
</authors>
<title>Automatic keyphrase extraction via topic decomposition.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP 2010),</booktitle>
<pages>366--376</pages>
<contexts>
<context position="3600" citStr="Liu et al. (2010)" startWordPosition="551" endWordPosition="554">(2007) used the frequency of all words belonging to the same WordNet concept set, while the Wikifier system (Csomai and Mihalcea, 2007) relied on Wikipedia links to compute a substitute to word frequency. Harwath and Hazen (2012) used topic modeling with PLSA to build a thesaurus, which they used to rank words based on topical similarity to the topics of a transcribed conversation. To consider dependencies among selected words, word co-occurrence has been combined with PageRank by Mihalcea and Tarau (2004), and additionally with WordNet by Wang et al. (2007), or with topical information by Z. Liu et al. (2010). However, as shown empirically by Mihalcea and Tarau (2004) and by Z. Liu et al. (2010) with various co-occurrence windows, such approaches have difficulties modeling long-range dependencies between words related to the same 651 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 651–657, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics topic. Z. Liu et al. (2009b) used part-of-speech information and word clustering techniques, while F. Liu et al. (2009a) added this information to the TFIDF method so as to consider b</context>
</contexts>
<marker>Liu, Huang, Zheng, Sun, 2010</marker>
<rawString>Zhiyuan Liu, Wenyi Huang, Yabin Zheng, and Maosong Sun. 2010. Automatic keyphrase extraction via topic decomposition. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP 2010), pages 366– 376.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Peter Luhn</author>
</authors>
<title>A statistical approach to mechanized encoding and searching of literary information.</title>
<date>1957</date>
<journal>IBM Journal of Research and Development,</journal>
<volume>1</volume>
<issue>4</issue>
<contexts>
<context position="2334" citStr="Luhn, 1957" startWordPosition="348" endWordPosition="349">al topics if necessary. The paper is organized as follows. In Section 2 we review existing methods for keyword extraction. In Section 3 we describe our proposal, which relies on topic modeling and a novel topic-aware diverse keyword extraction algorithm. Section 4 presents the data and tasks for comparing sets of keywords. In Section 5 we show that our method outperforms two existing ones. 2 State of the Art in Keyword Extraction Numerous studies have been conducted to automatically extract keywords from a text or a transcribed conversation. The earliest techniques have used word frequencies (Luhn, 1957), TFIDF values (Salton et al., 1975; Salton and Buckley, 1988), and pairwise word co-occurrence frequencies (Matsuo and Ishizuka, 2004) to rank words for extraction. These approaches do not consider word meaning, so they may ignore lowfrequency words which together indicate a highlysalient topic (Nenkova and McKeown, 2012). To improve over frequency-based methods, several ways to use lexical semantic information have been proposed. Semantic relations between words can be obtained from a manuallyconstructed thesaurus such as WordNet, or from Wikipedia, or from an automatically-built thesaurus u</context>
</contexts>
<marker>Luhn, 1957</marker>
<rawString>Hans Peter Luhn. 1957. A statistical approach to mechanized encoding and searching of literary information. IBM Journal of Research and Development, 1(4):309–317.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yutaka Matsuo</author>
<author>Mitsuru Ishizuka</author>
</authors>
<title>Keyword extraction from a single document using word co-occurrence statistical information.</title>
<date>2004</date>
<journal>International Journal on Artificial Intelligence Tools,</journal>
<volume>13</volume>
<issue>1</issue>
<pages>169</pages>
<contexts>
<context position="2469" citStr="Matsuo and Ishizuka, 2004" startWordPosition="367" endWordPosition="370">n. In Section 3 we describe our proposal, which relies on topic modeling and a novel topic-aware diverse keyword extraction algorithm. Section 4 presents the data and tasks for comparing sets of keywords. In Section 5 we show that our method outperforms two existing ones. 2 State of the Art in Keyword Extraction Numerous studies have been conducted to automatically extract keywords from a text or a transcribed conversation. The earliest techniques have used word frequencies (Luhn, 1957), TFIDF values (Salton et al., 1975; Salton and Buckley, 1988), and pairwise word co-occurrence frequencies (Matsuo and Ishizuka, 2004) to rank words for extraction. These approaches do not consider word meaning, so they may ignore lowfrequency words which together indicate a highlysalient topic (Nenkova and McKeown, 2012). To improve over frequency-based methods, several ways to use lexical semantic information have been proposed. Semantic relations between words can be obtained from a manuallyconstructed thesaurus such as WordNet, or from Wikipedia, or from an automatically-built thesaurus using latent topic modeling techniques. Ye et al. (2007) used the frequency of all words belonging to the same WordNet concept set, whil</context>
</contexts>
<marker>Matsuo, Ishizuka, 2004</marker>
<rawString>Yutaka Matsuo and Mitsuru Ishizuka. 2004. Keyword extraction from a single document using word co-occurrence statistical information. International Journal on Artificial Intelligence Tools, 13(1):157– 169.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew K McCallum</author>
</authors>
<title>MALLET: A machine learning for language toolkit.</title>
<date>2002</date>
<note>http://mallet.cs.umass.edu.</note>
<contexts>
<context position="5334" citStr="McCallum, 2002" startWordPosition="817" endWordPosition="818">. 3 Diverse Keyword Extraction We propose to build a topical representation of a conversation fragment, and then to select keywords using topical similarity while also rewarding the diversity of topic coverage, inspired by recent summarization methods (Lin and Bilmes, 2011; Li et al., 2012). 3.1 Representing Topic Information Topic models such as Probabilistic Latent Semantic Analysis (PLSA) or Latent Dirichlet Allocation (LDA) can be used to determine the distribution over the topic z of a word w, noted p(z|w), from a large amount of training documents. LDA implemented in the Mallet toolkit (McCallum, 2002) is used in this paper because it does not suffer from the overfitting issue of PLSA (Blei et al., 2003). The distribution of each topic z in a given conversation fragment t, noted p(z|t), can be computed by summing over all probabilities p(z|w) of the N words w spoken in the fragment: 1 p(z|t) = N p(z|w). wEt 3.2 Selecting Keywords The problem of keyword extraction with maximal topic coverage is formulated as follows. If a conversation fragment t mentions a set of topics Z, and each word w from the fragment t can evoke a subset of the topics in Z, then the goal is to find a subset of unique w</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew K. McCallum. 2002. MALLET: A machine learning for language toolkit. http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Paul Tarau</author>
</authors>
<title>TextRank: Bringing order into texts.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2004),</booktitle>
<pages>404--411</pages>
<location>Barcelona.</location>
<contexts>
<context position="3494" citStr="Mihalcea and Tarau (2004)" startWordPosition="532" endWordPosition="535">et, or from Wikipedia, or from an automatically-built thesaurus using latent topic modeling techniques. Ye et al. (2007) used the frequency of all words belonging to the same WordNet concept set, while the Wikifier system (Csomai and Mihalcea, 2007) relied on Wikipedia links to compute a substitute to word frequency. Harwath and Hazen (2012) used topic modeling with PLSA to build a thesaurus, which they used to rank words based on topical similarity to the topics of a transcribed conversation. To consider dependencies among selected words, word co-occurrence has been combined with PageRank by Mihalcea and Tarau (2004), and additionally with WordNet by Wang et al. (2007), or with topical information by Z. Liu et al. (2010). However, as shown empirically by Mihalcea and Tarau (2004) and by Z. Liu et al. (2010) with various co-occurrence windows, such approaches have difficulties modeling long-range dependencies between words related to the same 651 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 651–657, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics topic. Z. Liu et al. (2009b) used part-of-speech information and word cluster</context>
</contexts>
<marker>Mihalcea, Tarau, 2004</marker>
<rawString>Rada Mihalcea and Paul Tarau. 2004. TextRank: Bringing order into texts. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2004), pages 404–411, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George L Nemhauser</author>
<author>Laurence A Wolsey</author>
<author>Marshall L Fisher</author>
</authors>
<title>An analysis of approximations for maximizing submodular set functions.</title>
<date>1978</date>
<journal>Mathematical Programming Journal,</journal>
<volume>14</volume>
<issue>1</issue>
<pages>294</pages>
<contexts>
<context position="6138" citStr="Nemhauser et al. (1978)" startWordPosition="968" endWordPosition="971">p(z|t), can be computed by summing over all probabilities p(z|w) of the N words w spoken in the fragment: 1 p(z|t) = N p(z|w). wEt 3.2 Selecting Keywords The problem of keyword extraction with maximal topic coverage is formulated as follows. If a conversation fragment t mentions a set of topics Z, and each word w from the fragment t can evoke a subset of the topics in Z, then the goal is to find a subset of unique words S C_ t, with |S |&lt; k, which maximzes the number of covered topics for each number of keywords k. This problem is an instance of the maximum coverage problem, which is NP-hard. Nemhauser et al. (1978) showed that a greedy algorithm can find an approximate solution guaranteed to be within (1 − e) ^_ 0.63 of the optimal solution if the coverage function is submodular and monotone nondecreasing1. To find a monotone submodular function for keyword extraction, we used inspiration from recent work on extractive summarization methods (Lin and Bilmes, 2011; Li et al., 2012), which proposed a square root function for diverse selection of sentences to cover the maximum number of key concepts of a given document. The function rewards diversity by increasing the gain of selecting a sentence including </context>
</contexts>
<marker>Nemhauser, Wolsey, Fisher, 1978</marker>
<rawString>George L. Nemhauser, Laurence A. Wolsey, and Marshall L. Fisher. 1978. An analysis of approximations for maximizing submodular set functions. Mathematical Programming Journal, 14(1):265– 294.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Kathleen McKeown</author>
</authors>
<date>2012</date>
<booktitle>A Survey of Text Summarization Techniques, chapter 3,</booktitle>
<pages>43--76</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="2658" citStr="Nenkova and McKeown, 2012" startWordPosition="398" endWordPosition="401">ts of keywords. In Section 5 we show that our method outperforms two existing ones. 2 State of the Art in Keyword Extraction Numerous studies have been conducted to automatically extract keywords from a text or a transcribed conversation. The earliest techniques have used word frequencies (Luhn, 1957), TFIDF values (Salton et al., 1975; Salton and Buckley, 1988), and pairwise word co-occurrence frequencies (Matsuo and Ishizuka, 2004) to rank words for extraction. These approaches do not consider word meaning, so they may ignore lowfrequency words which together indicate a highlysalient topic (Nenkova and McKeown, 2012). To improve over frequency-based methods, several ways to use lexical semantic information have been proposed. Semantic relations between words can be obtained from a manuallyconstructed thesaurus such as WordNet, or from Wikipedia, or from an automatically-built thesaurus using latent topic modeling techniques. Ye et al. (2007) used the frequency of all words belonging to the same WordNet concept set, while the Wikifier system (Csomai and Mihalcea, 2007) relied on Wikipedia links to compute a substitute to word frequency. Harwath and Hazen (2012) used topic modeling with PLSA to build a thes</context>
</contexts>
<marker>Nenkova, McKeown, 2012</marker>
<rawString>Ani Nenkova and Kathleen McKeown, 2012. A Survey of Text Summarization Techniques, chapter 3, pages 43–76. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Christopher Buckley</author>
</authors>
<title>Termweighting approaches in automatic text retrieval.</title>
<date>1988</date>
<journal>Information Processing and Management Journal,</journal>
<volume>24</volume>
<issue>5</issue>
<contexts>
<context position="2396" citStr="Salton and Buckley, 1988" startWordPosition="357" endWordPosition="360">s follows. In Section 2 we review existing methods for keyword extraction. In Section 3 we describe our proposal, which relies on topic modeling and a novel topic-aware diverse keyword extraction algorithm. Section 4 presents the data and tasks for comparing sets of keywords. In Section 5 we show that our method outperforms two existing ones. 2 State of the Art in Keyword Extraction Numerous studies have been conducted to automatically extract keywords from a text or a transcribed conversation. The earliest techniques have used word frequencies (Luhn, 1957), TFIDF values (Salton et al., 1975; Salton and Buckley, 1988), and pairwise word co-occurrence frequencies (Matsuo and Ishizuka, 2004) to rank words for extraction. These approaches do not consider word meaning, so they may ignore lowfrequency words which together indicate a highlysalient topic (Nenkova and McKeown, 2012). To improve over frequency-based methods, several ways to use lexical semantic information have been proposed. Semantic relations between words can be obtained from a manuallyconstructed thesaurus such as WordNet, or from Wikipedia, or from an automatically-built thesaurus using latent topic modeling techniques. Ye et al. (2007) used t</context>
</contexts>
<marker>Salton, Buckley, 1988</marker>
<rawString>Gerard Salton and Christopher Buckley. 1988. Termweighting approaches in automatic text retrieval. Information Processing and Management Journal, 24(5):513–523.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Chung-Shu Yang</author>
<author>Clement T Yu</author>
</authors>
<title>A theory of term importance in automatic text analysis.</title>
<date>1975</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>26</volume>
<issue>1</issue>
<contexts>
<context position="2369" citStr="Salton et al., 1975" startWordPosition="353" endWordPosition="356"> paper is organized as follows. In Section 2 we review existing methods for keyword extraction. In Section 3 we describe our proposal, which relies on topic modeling and a novel topic-aware diverse keyword extraction algorithm. Section 4 presents the data and tasks for comparing sets of keywords. In Section 5 we show that our method outperforms two existing ones. 2 State of the Art in Keyword Extraction Numerous studies have been conducted to automatically extract keywords from a text or a transcribed conversation. The earliest techniques have used word frequencies (Luhn, 1957), TFIDF values (Salton et al., 1975; Salton and Buckley, 1988), and pairwise word co-occurrence frequencies (Matsuo and Ishizuka, 2004) to rank words for extraction. These approaches do not consider word meaning, so they may ignore lowfrequency words which together indicate a highlysalient topic (Nenkova and McKeown, 2012). To improve over frequency-based methods, several ways to use lexical semantic information have been proposed. Semantic relations between words can be obtained from a manuallyconstructed thesaurus such as WordNet, or from Wikipedia, or from an automatically-built thesaurus using latent topic modeling techniqu</context>
</contexts>
<marker>Salton, Yang, Yu, 1975</marker>
<rawString>Gerard Salton, Chung-Shu Yang, and Clement T. Yu. 1975. A theory of term importance in automatic text analysis. Journal of the American Society for Information Science, 26(1):33–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>Learning to extract keyphrases from text.</title>
<date>1999</date>
<tech>Technical Report ERB-1057,</tech>
<institution>National Research Council Canada (NRC).</institution>
<contexts>
<context position="4508" citStr="Turney, 1999" startWordPosition="686" endWordPosition="687">utational Linguistics, pages 651–657, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics topic. Z. Liu et al. (2009b) used part-of-speech information and word clustering techniques, while F. Liu et al. (2009a) added this information to the TFIDF method so as to consider both word dependency and semantic information. However, although they considered topical similarity, the above methods did not explicitly reward diversity and might miss secondary topics. Supervised methods have been used to learn a model for extracting keywords with various learning algorithms (Turney, 1999; Frank et al., 1999; Hulth, 2003). These approaches, however, rely on the availability of in-domain training data, and the objective functions they use for learning do not consider yet the diversity of keywords. 3 Diverse Keyword Extraction We propose to build a topical representation of a conversation fragment, and then to select keywords using topical similarity while also rewarding the diversity of topic coverage, inspired by recent summarization methods (Lin and Bilmes, 2011; Li et al., 2012). 3.1 Representing Topic Information Topic models such as Probabilistic Latent Semantic Analysis (</context>
</contexts>
<marker>Turney, 1999</marker>
<rawString>Peter Turney. 1999. Learning to extract keyphrases from text. Technical Report ERB-1057, National Research Council Canada (NRC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinghua Wang</author>
<author>Jianyi Liu</author>
<author>Cong Wang</author>
</authors>
<title>Keyword extraction based on PageRank.</title>
<date>2007</date>
<booktitle>In Advances in Knowledge Discovery and Data Mining (Proceedings of PAKDD 2007), LNAI 4426,</booktitle>
<pages>857--864</pages>
<publisher>Springer-Verlag,</publisher>
<location>Berlin.</location>
<contexts>
<context position="3547" citStr="Wang et al. (2007)" startWordPosition="541" endWordPosition="544">rus using latent topic modeling techniques. Ye et al. (2007) used the frequency of all words belonging to the same WordNet concept set, while the Wikifier system (Csomai and Mihalcea, 2007) relied on Wikipedia links to compute a substitute to word frequency. Harwath and Hazen (2012) used topic modeling with PLSA to build a thesaurus, which they used to rank words based on topical similarity to the topics of a transcribed conversation. To consider dependencies among selected words, word co-occurrence has been combined with PageRank by Mihalcea and Tarau (2004), and additionally with WordNet by Wang et al. (2007), or with topical information by Z. Liu et al. (2010). However, as shown empirically by Mihalcea and Tarau (2004) and by Z. Liu et al. (2010) with various co-occurrence windows, such approaches have difficulties modeling long-range dependencies between words related to the same 651 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 651–657, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics topic. Z. Liu et al. (2009b) used part-of-speech information and word clustering techniques, while F. Liu et al. (2009a) added thi</context>
</contexts>
<marker>Wang, Liu, Wang, 2007</marker>
<rawString>Jinghua Wang, Jianyi Liu, and Cong Wang. 2007. Keyword extraction based on PageRank. In Advances in Knowledge Discovery and Data Mining (Proceedings of PAKDD 2007), LNAI 4426, pages 857–864. Springer-Verlag, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shiren Ye</author>
<author>Tat-Seng Chua</author>
<author>Min-Yen Kan</author>
<author>Long Qiu</author>
</authors>
<title>Document concept lattice for text understanding and summarization.</title>
<date>2007</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>43--6</pages>
<contexts>
<context position="2989" citStr="Ye et al. (2007)" startWordPosition="448" endWordPosition="451">lton and Buckley, 1988), and pairwise word co-occurrence frequencies (Matsuo and Ishizuka, 2004) to rank words for extraction. These approaches do not consider word meaning, so they may ignore lowfrequency words which together indicate a highlysalient topic (Nenkova and McKeown, 2012). To improve over frequency-based methods, several ways to use lexical semantic information have been proposed. Semantic relations between words can be obtained from a manuallyconstructed thesaurus such as WordNet, or from Wikipedia, or from an automatically-built thesaurus using latent topic modeling techniques. Ye et al. (2007) used the frequency of all words belonging to the same WordNet concept set, while the Wikifier system (Csomai and Mihalcea, 2007) relied on Wikipedia links to compute a substitute to word frequency. Harwath and Hazen (2012) used topic modeling with PLSA to build a thesaurus, which they used to rank words based on topical similarity to the topics of a transcribed conversation. To consider dependencies among selected words, word co-occurrence has been combined with PageRank by Mihalcea and Tarau (2004), and additionally with WordNet by Wang et al. (2007), or with topical information by Z. Liu et</context>
</contexts>
<marker>Ye, Chua, Kan, Qiu, 2007</marker>
<rawString>Shiren Ye, Tat-Seng Chua, Min-Yen Kan, and Long Qiu. 2007. Document concept lattice for text understanding and summarization. Information Processing and Management, 43(6):1643–1662.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>