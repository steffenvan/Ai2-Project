<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000037">
<title confidence="0.989218">
Modeling Commonality among Related Classes in Relation Extraction
</title>
<author confidence="0.987283">
Zhou GuoDong Su Jian Zhang Min
</author>
<affiliation confidence="0.972734">
Institute for Infocomm Research
</affiliation>
<address confidence="0.838758">
21 Heng Mui Keng Terrace, Singapore 119613
</address>
<email confidence="0.968831">
Email: {zhougd, sujian, mzhang}@i2r.a-star.edu.sg
</email>
<sectionHeader confidence="0.993197" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999947115384615">
This paper proposes a novel hierarchical learn-
ing strategy to deal with the data sparseness
problem in relation extraction by modeling the
commonality among related classes. For each
class in the hierarchy either manually prede-
fined or automatically clustered, a linear dis-
criminative function is determined in a top-
down way using a perceptron algorithm with
the lower-level weight vector derived from the
upper-level weight vector. As the upper-level
class normally has much more positive train-
ing examples than the lower-level class, the
corresponding linear discriminative function
can be determined more reliably. The upper-
level discriminative function then can effec-
tively guide the discriminative function learn-
ing in the lower-level, which otherwise might
suffer from limited training data. Evaluation
on the ACE RDC 2003 corpus shows that the
hierarchical strategy much improves the per-
formance by 5.6 and 5.1 in F-measure on
least- and medium- frequent relations respec-
tively. It also shows that our system outper-
forms the previous best-reported system by 2.7
in F-measure on the 24 subtypes using the
same feature set.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99966015625">
With the dramatic increase in the amount of tex-
tual information available in digital archives and
the WWW, there has been growing interest in
techniques for automatically extracting informa-
tion from text. Information Extraction (IE) is
such a technology that IE systems are expected
to identify relevant information (usually of pre-
defined types) from text documents in a certain
domain and put them in a structured format.
According to the scope of the ACE program
(ACE 2000-2005), current research in IE has
three main objectives: Entity Detection and
Tracking (EDT), Relation Detection and
Characterization (RDC), and Event Detection
and Characterization (EDC). This paper will
focus on the ACE RDC task, which detects and
classifies various semantic relations between two
entities. For example, we want to determine
whether a person is at a location, based on the
evidence in the context. Extraction of semantic
relationships between entities can be very useful
for applications such as question answering, e.g.
to answer the query “Who is the president of the
United States?”.
One major challenge in relation extraction is
due to the data sparseness problem (Zhou et al
2005). As the largest annotated corpus in relation
extraction, the ACE RDC 2003 corpus shows
that different subtypes/types of relations are
much unevenly distributed and a few relation
subtypes, such as the subtype “Founder” under
the type “ROLE”, suffers from a small amount of
annotated data. Further experimentation in this
paper (please see Figure 2) shows that most rela-
tion subtypes suffer from the lack of the training
data and fail to achieve steady performance given
the current corpus size. Given the relative large
size of this corpus, it will be time-consuming and
very expensive to further expand the corpus with
a reasonable gain in performance. Even if we can
somehow expend the corpus and achieve steady
performance on major relation subtypes, it will
be still far beyond practice for those minor sub-
types given the much unevenly distribution
among different relation subtypes. While various
machine learning approaches, such as generative
modeling (Miller et al 2000), maximum entropy
(Kambhatla 2004) and support vector machines
(Zhao and Grisman 2005; Zhou et al 2005), have
been applied in the relation extraction task, no
explicit learning strategy is proposed to deal with
the inherent data sparseness problem caused by
the much uneven distribution among different
relations.
This paper proposes a novel hierarchical
learning strategy to deal with the data sparseness
problem by modeling the commonality among
related classes. Through organizing various
classes hierarchically, a linear discriminative
function is determined for each class in a top-
down way using a perceptron algorithm with the
lower-level weight vector derived from the up-
per-level weight vector. Evaluation on the ACE
RDC 2003 corpus shows that the hierarchical
</bodyText>
<page confidence="0.973705">
121
</page>
<note confidence="0.5330175">
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 121–128,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999968454545454">
strategy achieves much better performance than
the flat strategy on least- and medium-frequent
relations. It also shows that our system based on
the hierarchical strategy outperforms the previ-
ous best-reported system.
The rest of this paper is organized as follows.
Section 2 presents related work. Section 3
describes the hierarchical learning strategy using
the perceptron algorithm. Finally, we present
experimentation in Section 4 and conclude this
paper in Section 5.
</bodyText>
<sectionHeader confidence="0.999725" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999748333333333">
The relation extraction task was formulated at
MUC-7(1998). With the increasing popularity of
ACE, this task is starting to attract more and
more researchers within the natural language
processing and machine learning communities.
Typical works include Miller et al (2000), Ze-
lenko et al (2003), Culotta and Sorensen (2004),
Bunescu and Mooney (2005a), Bunescu and
Mooney (2005b), Zhang et al (2005), Roth and
Yih (2002), Kambhatla (2004), Zhao and Grisman
(2005) and Zhou et al (2005).
Miller et al (2000) augmented syntactic full
parse trees with semantic information of entities
and relations, and built generative models to in-
tegrate various tasks such as POS tagging, named
entity recognition, template element extraction
and relation extraction. The problem is that such
integration may impose big challenges, e.g. the
need of a large annotated corpus. To overcome
the data sparseness problem, generative models
typically applied some smoothing techniques to
integrate different scales of contexts in parameter
estimation, e.g. the back-off approach in Miller
et al (2000).
Zelenko et al (2003) proposed extracting re-
lations by computing kernel functions between
parse trees. Culotta and Sorensen (2004) extended
this work to estimate kernel functions between
augmented dependency trees and achieved F-
measure of 45.8 on the 5 relation types in the
ACE RDC 2003 corpus1. Bunescu and Mooney
(2005a) proposed a shortest path dependency
kernel. They argued that the information to
model a relationship between two entities can be
typically captured by the shortest path between
them in the dependency graph. It achieved the F-
measure of 52.5 on the 5 relation types in the
ACE RDC 2003 corpus. Bunescu and Mooney
(2005b) proposed a subsequence kernel and ap-
</bodyText>
<footnote confidence="0.7421405">
1 The ACE RDC 2003 corpus defines 5/24 relation
types/subtypes between 4 entity types.
</footnote>
<bodyText confidence="0.999856530612245">
plied it in protein interaction and ACE relation
extraction tasks. Zhang et al (2005) adopted clus-
tering algorithms in unsupervised relation extrac-
tion using tree kernels. To overcome the data
sparseness problem, various scales of sub-trees
are applied in the tree kernel computation. Al-
though tree kernel-based approaches are able to
explore the huge implicit feature space without
much feature engineering, further research work
is necessary to make them effective and efficient.
Comparably, feature-based approaches
achieved much success recently. Roth and Yih
(2002) used the SNoW classifier to incorporate
various features such as word, part-of-speech and
semantic information from WordNet, and pro-
posed a probabilistic reasoning approach to inte-
grate named entity recognition and relation
extraction. Kambhatla (2004) employed maxi-
mum entropy models with features derived from
word, entity type, mention level, overlap, de-
pendency tree, parse tree and achieved F-
measure of 52.8 on the 24 relation subtypes in
the ACE RDC 2003 corpus. Zhao and Grisman
(2005)2 combined various kinds of knowledge
from tokenization, sentence parsing and deep
dependency analysis through support vector ma-
chines and achieved F-measure of 70.1 on the 7
relation types of the ACE RDC 2004 corpus3.
Zhou et al (2005) further systematically explored
diverse lexical, syntactic and semantic features
through support vector machines and achieved F-
measure of 68.1 and 55.5 on the 5 relation types
and the 24 relation subtypes in the ACE RDC
2003 corpus respectively. To overcome the data
sparseness problem, feature-based approaches
normally incorporate various scales of contexts
into the feature vector extensively. These ap-
proaches then depend on adopted learning algo-
rithms to weight and combine each feature
effectively. For example, an exponential model
and a linear model are applied in the maximum
entropy models and support vector machines re-
spectively to combine each feature via the
learned weight vector.
In summary, although various approaches
have been employed in relation extraction, they
implicitly attack the data sparseness problem by
using features of different contexts in feature-
based approaches or including different sub-
</bodyText>
<footnote confidence="0.925782166666667">
2 Here, we classify this paper into feature-based ap-
proaches since the feature space in the kernels of
Zhao and Grisman (2005) can be easily represented
by an explicit feature vector.
3 The ACE RDC 2004 corpus defines 7/27 relation
types/subtypes between 7 entity types.
</footnote>
<page confidence="0.997121">
122
</page>
<bodyText confidence="0.999957733333333">
structures in kernel-based approaches. Until now,
there are no explicit ways to capture the hierar-
chical topology in relation extraction. Currently,
all the current approaches apply the flat learning
strategy which equally treats training examples
in different relations independently and ignore
the commonality among different relations. This
paper proposes a novel hierarchical learning
strategy to resolve this problem by considering
the relatedness among different relations and
capturing the commonality among related rela-
tions. By doing so, the data sparseness problem
can be well dealt with and much better perform-
ance can be achieved, especially for those rela-
tions with small amounts of annotated examples.
</bodyText>
<sectionHeader confidence="0.995197" genericHeader="method">
3 Hierarchical Learning Strategy
</sectionHeader>
<bodyText confidence="0.999984594594595">
Traditional classifier learning approaches apply
the flat learning strategy. That is, they equally
treat training examples in different classes
independently and ignore the commonality
among related classes. The flat strategy will not
cause any problem when there are a large amount
of training examples for each class, since, in this
case, a classifier learning approach can always
learn a nearly optimal discriminative function for
each class against the remaining classes. How-
ever, such flat strategy may cause big problems
when there is only a small amount of training
examples for some of the classes. In this case, a
classifier learning approach may fail to learn a
reliable (or nearly optimal) discriminative func-
tion for a class with a small amount of training
examples, and, as a result, may significantly af-
fect the performance of the class or even the
overall performance.
To overcome the inherent problems in the
flat strategy, this paper proposes a hierarchical
learning strategy which explores the inherent
commonality among related classes through a
class hierarchy. In this way, the training exam-
ples of related classes can help in learning a reli-
able discriminative function for a class with only
a small amount of training examples. To reduce
computation time and memory requirements, we
will only consider linear classifiers and apply the
simple and widely-used perceptron algorithm for
this purpose with more options open for future
research. In the following, we will first introduce
the perceptron algorithm in linear classifier
learning, followed by the hierarchical learning
strategy using the perceptron algorithm. Finally,
we will consider several ways in building the
class hierarchy.
</bodyText>
<subsectionHeader confidence="0.998802">
3.1 Perceptron Algorithm
</subsectionHeader>
<bodyText confidence="0.89147475">
Input: the initial weight vector w , the training
example sequence
(xt, yt) ∈ X × Y, t =1,2... ,T and the number of
the maximal iterations N (e.g. 10 in this
paper) of the training sequence4
Output: the weight vector w for the linear
discriminative function f = w ⋅ x
BEGIN
</bodyText>
<equation confidence="0.8226125">
w1 =
REPEAT for t=1,2,...,T*N
</equation>
<listItem confidence="0.948830428571428">
1. Receive the instance n
x t ∈ R
2. Compute the output ot = wt ⋅ xt
∧
3. Give the prediction yt = sign(ot )
4. Receive the desired label yt ∈ {−1,+1}
5. Update the hypothesis according to
</listItem>
<equation confidence="0.996479">
wt+1 = wt + δtytxt (1)
</equation>
<bodyText confidence="0.7252985">
where δt = 0 if the margin of wt at the
given example (xt,yt) yt wt ⋅ xt &gt; 0
</bodyText>
<figure confidence="0.394176">
and δt = 1 otherwise
END REPEAT
N
Return w = ∑ wT*i +1 / 5
=N−4
END BEGIN
</figure>
<figureCaption confidence="0.999669">
Figure 1: the perceptron algorithm
</figureCaption>
<bodyText confidence="0.9980615">
This section first deals with binary classification
using linear classifiers. Assume an instance space
</bodyText>
<equation confidence="0.9226235">
X = R and a binary label space Y = {−1,+1} .
n
</equation>
<bodyText confidence="0.971147708333334">
With any weight vector w∈ R and a given
n
instance n
x∈ R , we associate a linear classifier
hw with a linear discriminative function 5
f (x) = w ⋅ x by hw(x) = sign(w⋅ x) , where
sign(w⋅ x) = −1 if w⋅ x &lt; 0 and sign(w⋅ x) = +1
otherwise. Here, the margin of w at (xt, yt) is
defined as ytw⋅ xt . Then if the margin is positive,
we have a correct prediction with hw(x) = yt , and
if the margin is negative, we have an error with
hw(x) ≠ yt . Therefore, given a sequence of
training examples (xt, yt) ∈ X × Y, t =1,2... ,T ,
linear classifier learning attemps to find a weight
vector w that achieves a positive margin on as
many examples as possible.
4 The training example sequence is feed N times for
better performance. Moreover, this number can con-
trol the maximal affect a training example can pose.
This is similar to the regulation parameter C in
SVM, which affects the trade-off between complex-
ity and proportion of non-separable examples. As a
result, it can be used to control over-fitting and
robustness.
</bodyText>
<equation confidence="0.9831472">
5 ( w⋅ x) denotes the dot product of the weight vector
w∈ R and a given instance n
n x ∈ R .
w
i
</equation>
<page confidence="0.987309">
123
</page>
<bodyText confidence="0.999676">
The well-known perceptron algorithm, as
shown in Figure 1, belongs to online learning of
linear classifiers, where the learning algorithm
represents its t -th hyposthesis by a weight vector
</bodyText>
<equation confidence="0.9751112">
wt ∈ R . At trial t , an online algorithm receives
n
an instance xt ∈ R , makes its prediction
n
∧
</equation>
<bodyText confidence="0.975777538461538">
and receives the desired label
yt ∈ {−1,+1}. What distinguishes different online
algorithms is how they update wt into wt+ 1 based
on the example (xt,yt) received at trial t . In
particular, the perceptron algorithm updates the
hypothesis by adding a scalar multiple of the
instance, as shown in Equation 1 of Figure 1,
when there is an error. Normally, the tradictional
perceptron algorithm initializes the hypothesis as
the zero vector w1 = 0. This is usually the most
natural choice, lacking any other preference.
Smoothing
In order to further improve the performance, we
iteratively feed the training examples for a possi-
ble better discriminative function. In this paper,
we have set the maximal iteration number to 10
for both efficiency and stable performance and
the final weight vector in the discriminative func-
tion is averaged over those of the discriminative
functions in the last few iterations (e.g. 5 in this
paper).
Bagging
One more problem with any online classifier
learning algorithm, including the perceptron al-
gorithm, is that the learned discriminative func-
tion somewhat depends on the feeding order of
the training examples. In order to eliminate such
dependence and further improve the perform-
ance, an ensemble technique, called bagging
(Breiman 1996), is applied in this paper. In bag-
ging, the bootstrap technique is first used to build
M (e.g. 10 in this paper) replicate sample sets by
randomly re-sampling with replacement from the
given training set repeatedly. Then, each training
sample set is used to train a certain discrimina-
tive function. Finally, the final weight vector in
the discriminative function is averaged over
those of the M discriminative functions in the
ensemble.
</bodyText>
<subsectionHeader confidence="0.627851">
Multi-Class Classification
</subsectionHeader>
<bodyText confidence="0.999837681818182">
Basically, the perceptron algorithm is only for
binary classification. Therefore, we must extend
the perceptron algorithms to multi-class
classification, such as the ACE RDC task. For
efficiency, we apply the one vs. others strategy,
which builds K classifiers so as to separate one
class from all others. However, the outputs for
the perceptron algorithms of different classes
may be not directly comparable since any
positive scalar multiple of the weight vector will
not affect the actual prediction of a perceptron
algorithm. For comparability, we map the
perceptron algorithm output into the probability
by using an additional sigmoid model:
where f = w ⋅ x is the output of a perceptron
algorithm and the coefficients A &amp; B are to be
trained using the model trust alorithm as
described in Platt (1999). The final decision of an
instance in multi-class classification is
determined by the class which has the maximal
probability from the corresponding perceptron
algorithm.
</bodyText>
<subsectionHeader confidence="0.9891955">
3.2 Hierarchical Learning Strategy using the
Perceptron Algorithm
</subsectionHeader>
<bodyText confidence="0.999798705882353">
Assume we have a class hierarchy for a task, e.g.
the one in the ACE RDC 2003 corpus as shown
in Table 1 of Section 4.1. The hierarchical learn-
ing strategy explores the inherent commonality
among related classes in a top-down way. For
each class in the hierarchy, a linear discrimina-
tive function is determined in a top-down way
with the lower-level weight vector derived from
the upper-level weight vector iteratively. This is
done by initializing the weight vector in training
the linear discriminative function for the lower-
level class as that of the upper-level class. That
is, the lower-level discriminative function has the
preference toward the discriminative function of
its upper-level class. For an example, let’s look
at the training of the “Located” relation subtype
in the class hierarchy as shown in Table 1:
</bodyText>
<listItem confidence="0.985935375">
1) Train the weight vector of the linear
discriminative function for the “YES”
relation vs. the “NON” relation with the
weight vector initialized as the zero vector.
2) Train the weight vector of the linear
discriminative function for the “AT” relation
type vs. all the remaining relation types
(including the “NON” relation) with the
weight vector initialized as the weight vector
of the linear discriminative function for the
“YES” relation vs. the “NON” relation.
3) Train the weight vector of the linear
discriminative function for the “Located”
relation subtype vs. all the remaining relation
subtypes under all the relation types
(including the “NON” relation) with the
</listItem>
<equation confidence="0.957202272727273">
( 1  |)
y f = =
1 exp(
+ Af B
+
p
1
) (2)
yt = sign w ⋅ x
( t t
)
</equation>
<page confidence="0.991356">
124
</page>
<bodyText confidence="0.999759142857143">
weight vector initialized as the weight vector
of the linear discriminative function for the
“AT” relation type vs. all the remaining
relation types.
4) Return the above trained weight vector as the
discriminatie function for the “Located”
relation subtype.
In this way, the training examples in differ-
ent classes are not treated independently any
more, and the commonality among related
classes can be captured via the hierarchical learn-
ing strategy. The intuition behind this strategy is
that the upper-level class normally has more
positive training examples than the lower-level
class so that the corresponding linear discrimina-
tive function can be determined more reliably. In
this way, the training examples of related classes
can help in learning a reliable discriminative
function for a class with only a small amount of
training examples in a top-down way and thus
alleviate its data sparseness problem.
</bodyText>
<subsectionHeader confidence="0.999389">
3.3 Building the Class Hierarchy
</subsectionHeader>
<bodyText confidence="0.998417">
We have just described the hierarchical learning
strategy using a given class hierarchy. Normally,
a rough class hierarchy can be given manually
according to human intuition, such as the one in
the ACE RDC 2003 corpus. In order to explore
more commonality among sibling classes, we
make use of binary hierarchical clustering for
sibling classes at both lowest and all levels. This
can be done by first using the flat learning strat-
egy to learn the discriminative functions for indi-
vidual classes and then iteratively combining the
two most related classes using the cosine similar-
ity function between their weight vectors in a
bottom-up way. The intuition is that related
classes should have similar hyper-planes to sepa-
rate from other classes and thus have similar
weight vectors.
</bodyText>
<listItem confidence="0.927811833333333">
• Lowest-level hybrid: Binary hierarchical
clustering is only done at the lowest level
while keeping the upper-level class hierar-
chy. That is, only sibling classes at the low-
est level are hierarchically clustered.
• All-level hybrid: Binary hierarchical cluster-
ing is done at all levels in a bottom-up way.
That is, sibling classes at the lowest level are
hierarchically clustered first and then sibling
classes at the upper-level. In this way, the bi-
nary class hierarchy can be built iteratively
in a bottom-up way.
</listItem>
<sectionHeader confidence="0.992412" genericHeader="method">
4 Experimentation
</sectionHeader>
<bodyText confidence="0.9999425">
This paper uses the ACE RDC 2003 corpus pro-
vided by LDC to train and evaluate the hierarchi-
cal learning strategy. Same as Zhou et al (2005),
we only model explicit relations and explicitly
model the argument order of the two mentions
involved.
</bodyText>
<subsectionHeader confidence="0.910813">
4.1 Experimental Setting
</subsectionHeader>
<table confidence="0.99986016">
Type Subtype Freq Bin Type
AT Based-In 347 Medium
Located 2126 Large
Residence 308 Medium
NEAR Relative-Location 201 Medium
PART Part-Of 947 Large
Subsidiary 355 Medium
Other 6 Small
ROLE Affiliate-Partner 204 Medium
Citizen-Of 328 Medium
Client 144 Small
Founder 26 Small
General-Staff 1331 Large
Management 1242 Large
Member 1091 Large
Owner 232 Medium
Other 158 Small
SOCIAL Associate 91 Small
Grandparent 12 Small
Other-Personal 85 Small
Other-Professional 339 Medium
Other-Relative 78 Small
Parent 127 Small
Sibling 18 Small
Spouse 77 Small
</table>
<tableCaption confidence="0.998232">
Table 1: Statistics of relation types and subtypes
</tableCaption>
<bodyText confidence="0.996561391304348">
in the training data of the ACE RDC 2003 corpus
(Note: According to frequency, all the subtypes
are divided into three bins: large/ middle/ small,
with 400 as the lower threshold for the large bin
and 200 as the upper threshold for the small bin).
The training data consists of 674 documents
(~300k words) with 9683 relation examples
while the held-out testing data consists of 97
documents (~50k words) with 1386 relation ex-
amples. All the experiments are done five times
on the 24 relation subtypes in the ACE corpus,
except otherwise specified, with the final per-
formance averaged using the same re-sampling
with replacement strategy as the one in the bag-
ging technique. Table 1 lists various types and
subtypes of relations for the ACE RDC 2003
corpus, along with their occurrence frequency in
the training data. It shows that this corpus suffers
from a small amount of annotated data for a few
subtypes such as the subtype “Founder” under
the type “ROLE”.
For comparison, we also adopt the same fea-
ture set as Zhou et al (2005): word, entity type,
</bodyText>
<page confidence="0.997043">
125
</page>
<bodyText confidence="0.998413">
mention level, overlap, base phrase chunking,
dependency tree, parse tree and semantic infor-
mation.
</bodyText>
<subsectionHeader confidence="0.995332">
4.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999947">
Table 2 shows the performance of the hierarchi-
cal learning strategy using the existing class hier-
archy in the given ACE corpus and its
comparison with the flat learning strategy, using
the perceptron algorithm. It shows that the pure
hierarchical strategy outperforms the pure flat
strategy by 1.5 (56.9 vs. 55.4) in F-measure. It
also shows that further smoothing and bagging
improve the performance of the hierarchical and
flat strategies by 0.6 and 0.9 in F-measure re-
spectively. As a result, the final hierarchical
strategy achieves F-measure of 57.8 and outper-
forms the final flat strategy by 1.8 in F-measure.
</bodyText>
<table confidence="0.999841222222222">
Strategies P R F
Flat 58.2 52.8 55.4
Flat+Smoothing 58.9 53.1 55.9
Flat+Bagging 59.0 53.1 55.9
Flat+Both 59.1 53.2 56.0
Hierarchical 61.9 52.6 56.9
Hierarchical+Smoothing 62.7 53.1 57.5
Hierarchical+Bagging 62.9 53.1 57.6
Hierarchical+Both 63.0 53.4 57.8
</table>
<tableCaption confidence="0.843440666666667">
Table 2: Performance of the hierarchical learning
strategy using the existing class hierarchy and its
comparison with the flat learning strategy
</tableCaption>
<table confidence="0.998972">
Class Hierarchies P R F
Existing 63.0 53.4 57.8
Entirely Automatic 63.4 53.1 57.8
Lowest-level Hybrid 63.6 53.5 58.1
All-level Hybrid 63.6 53.6 58.2
</table>
<tableCaption confidence="0.999115">
Table 3: Performance of the hierarchical learning
</tableCaption>
<bodyText confidence="0.98154646031746">
strategy using different class hierarchies
Table 3 compares the performance of the hi-
erarchical learning strategy using different class
hierarchies. It shows that, the lowest-level hybrid
approach, which only automatically updates the
existing class hierarchy at the lowest level, im-
proves the performance by 0.3 in F-measure
while further updating the class hierarchy at up-
per levels in the all-level hybrid approach only
has very slight effect. This is largely due to the
fact that the major data sparseness problem oc-
curs at the lowest level, i.e. the relation subtype
level in the ACE corpus. As a result, the final
hierarchical learning strategy using the class hi-
erarchy built with the all-level hybrid approach
achieves F-measure of 58.2 in F-measure, which
outperforms the final flat strategy by 2.2 in F-
measure. In order to justify the usefulness of our
hierarchical learning strategy when a rough class
hierarchy is not available and difficult to deter-
mine manually, we also experiment using en-
tirely automatically built class hierarchy (using
the traditional binary hierarchical clustering algo-
rithm and the cosine similarity measurement)
without considering the existing class hierarchy.
Table 3 shows that using automatically built
class hierarchy performs comparably with using
only the existing one.
With the major goal of resolving the data
sparseness problem for the classes with a small
amount of training examples, Table 4 compares
the best-performed hierarchical and flat learning
strategies on the relation subtypes of different
training data sizes. Here, we divide various rela-
tion subtypes into three bins: large/middle/small,
according to their available training data sizes.
For the ACE RDC 2003 corpus, we use 400 as
the lower threshold for the large bin6 and 200 as
the upper threshold for the small bin7. As a re-
sult, the large/medium/small bin includes 5/8/11
relation subtypes, respectively. Please see Table
1 for details. Table 4 shows that the hierarchical
strategy outperforms the flat strategy by
1.0/5.1/5.6 in F-measure on the
large/middle/small bin respectively. This indi-
cates that the hierarchical strategy performs
much better than the flat strategy for those
classes with a small or medium amount of anno-
tated examples although the hierarchical strategy
only performs slightly better by 1.0 and 2.2 in F-
measure than the flat strategy on those classes
with a large size of annotated corpus and on all
classes as a whole respectively. This suggests
that the proposed hierarchical strategy can well
deal with the data sparseness problem in the
ACE RDC 2003 corpus.
An interesting question is about the similar-
ity between the linear discriminative functions
learned using the hierarchical and flat learning
strategies. Table 4 compares the cosine similari-
ties between the weight vectors of the linear dis-
criminative functions using the two strategies for
different bins, weighted by the training data sizes
</bodyText>
<footnote confidence="0.985191727272727">
6 The reason to choose this threshold is that no rela-
tion subtype in the ACE RC 2003 corpus has train-
ing examples in between 400 and 900.
7 A few minor relation subtypes only have very few
examples in the testing set. The reason to choose
this threshold is to guarantee a reasonable number of
testing examples in the small bin. For the ACE RC
2003 corpus, using 200 as the upper threshold will
fill the small bin with about 100 testing examples
while using 100 will include too few testing exam-
ples for reasonable performance evaluation.
</footnote>
<page confidence="0.996968">
126
</page>
<bodyText confidence="0.993561442307692">
of different relation subtypes. It shows that the
linear discriminative functions learned using the
two strategies are very similar (with the cosine
similarity 0.98) for the relation subtypes belong-
ing to the large bin while the linear discrimina-
tive functions learned using the two strategies are
not for the relation subtypes belonging to the
medium/small bin with the cosine similarity
0.92/0.81 respectively. This means that the use of
the hierarchical strategy over the flat strategy
only has very slight change on the linear dis-
criminative functions for those classes with a
large amount of annotated examples while its
effect on those with a small amount of annotated
examples is obvious. This contributes to and ex-
plains (the degree of) the performance difference
between the two strategies on the different train-
ing data sizes as shown in Table 4.
Due to the difficulty of building a large an-
notated corpus, another interesting question is
about the learning curve of the hierarchical learn-
ing strategy and its comparison with the flat
learning strategy. Figure 2 shows the effect of
different training data sizes for some major rela-
tion subtypes while keeping all the training ex-
amples of remaining relation subtypes. It shows
that the hierarchical strategy performs much bet-
ter than the flat strategy when only a small
amount of training examples is available. It also
shows that the hierarchical strategy can achieve
stable performance much faster than the flat
strategy. Finally, it shows that the ACE RDC
2003 task suffers from the lack of training exam-
ples. Among the three major relation subtypes,
only the subtype “Located” achieves steady per-
formance.
Finally, we also compare our system with the
previous best-reported systems, such as Kamb-
hatla (2004) and Zhou et al (2005). Table 5
shows that our system outperforms the previous
best-reported system by 2.7 (58.2 vs. 55.5) in F-
measure, largely due to the gain in recall. It indi-
cates that, although support vector machines and
maximum entropy models always perform better
than the simple perceptron algorithm in most (if
not all) applications, the hierarchical learning
strategy using the perceptron algorithm can eas-
ily overcome the difference and outperforms the
flat learning strategy using the overwhelming
support vector machines and maximum entropy
models in relation extraction, at least on the ACE
RDC 2003 corpus.
</bodyText>
<table confidence="0.99448675">
Bin Type(cosine similarity) Large Bin (0.98) Middle Bin (0.92) Small Bin (0.81)
P R F P R F P R F
Flat Strategy 62.3 61.9 62.1 60.8 38.7 47.3 33.0 21.7 26.2
Hierarchical Strategy 66.4 60.2 63.1 67.6 42.7 52.4 40.2 26.3 31.8
</table>
<tableCaption confidence="0.668595">
Table 4: Comparison of the hierarchical and flat learning strategies on the relation subtypes of differ-
ent training data sizes. Notes: the figures in the parentheses indicate the cosine similarities between
the weight vectors of the linear discriminative functions learned using the two strategies.
</tableCaption>
<figureCaption confidence="0.9916255">
Figure 2: Learning curve of the hierarchical strategy and its comparison with the flat strategy for some
major relation subtypes (Note: FS for the flat strategy and HS for the hierarchical strategy)
</figureCaption>
<table confidence="0.9990198">
System Performance
P R F
Our: Perceptron Algorithm + Hierarchical Strategy 63.6 53.6 58.2
Zhou et al (2005): SVM + Flat Strategy 63.1 49.5 55.5
Kambhatla (2004): Maximum Entropy + Flat Strategy 63.5 45.2 52.8
</table>
<tableCaption confidence="0.999925">
Table 5: Comparison of our system with other best-reported systems
</tableCaption>
<figure confidence="0.988663733333333">
60
F-measure
40
20
70
50
30
10
Training Data Size
HS: General-Staff
FS: General-Staff
HS: Part-Of
FS: Part-Of
HS: Located
FS: Located
</figure>
<page confidence="0.970412">
127
</page>
<sectionHeader confidence="0.996504" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999986222222222">
This paper proposes a novel hierarchical learning
strategy to deal with the data sparseness problem
in relation extraction by modeling the common-
ality among related classes. For each class in a
class hierarchy, a linear discriminative function
is determined in a top-down way using the per-
ceptron algorithm with the lower-level weight
vector derived from the upper-level weight vec-
tor. In this way, the upper-level discriminative
function can effectively guide the lower-level
discriminative function learning. Evaluation on
the ACE RDC 2003 corpus shows that the hier-
archical strategy performs much better than the
flat strategy in resolving the critical data sparse-
ness problem in relation extraction.
In the future work, we will explore the hier-
archical learning strategy using other machine
learning approaches besides online classifier
learning approaches such as the simple percep-
tron algorithm applied in this paper. Moreover,
just as indicated in Figure 2, most relation sub-
types in the ACE RDC 2003 corpus (arguably
the largest annotated corpus in relation extrac-
tion) suffer from the lack of training examples.
Therefore, a critical research in relation extrac-
tion is how to rely on semi-supervised learning
approaches (e.g. bootstrap) to alleviate its de-
pendency on a large amount of annotated training
examples and achieve better and steadier per-
formance. Finally, our current work is done when
NER has been perfectly done. Therefore, it
would be interesting to see how imperfect NER
affects the performance in relation extraction.
This will be done by integrating the relation ex-
traction system with our previously developed
NER system as described in Zhou and Su (2002).
</bodyText>
<sectionHeader confidence="0.999275" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999889852459016">
ACE. (2000-2005). Automatic Content Extraction.
http://www.ldc.upenn.edu/Projects/ACE/
Bunescu R. &amp; Mooney R.J. (2005a). A shortest
path dependency kernel for relation extraction.
HLT/EMNLP’2005: 724-731. 6-8 Oct 2005.
Vancover, B.C.
Bunescu R. &amp; Mooney R.J. (2005b). Subsequence
Kernels for Relation Extraction NIPS’2005.
Vancouver, BC, December 2005
Breiman L. (1996) Bagging Predictors. Machine
Learning, 24(2): 123-140.
Collins M. (1999). Head-driven statistical models
for natural language parsing. Ph.D. Dissertation,
University of Pennsylvania.
Culotta A. and Sorensen J. (2004). Dependency
tree kernels for relation extraction. ACL’2004.
423-429. 21-26 July 2004. Barcelona, Spain.
Kambhatla N. (2004). Combining lexical, syntactic
and semantic features with Maximum Entropy
models for extracting relations.
ACL’2004(Poster). 178-181. 21-26 July 2004.
Barcelona, Spain.
Miller G.A. (1990). WordNet: An online lexical
database. International Journal of Lexicography.
3(4):235-312.
Miller S., Fox H., Ramshaw L. and Weischedel R.
(2000). A novel use of statistical parsing to ex-
tract information from text. ANLP’2000. 226-
233. 29 April - 4 May 2000, Seattle, USA
MUC-7. (1998). Proceedings of the 7th Message
Understanding Conference (MUC-7). Morgan
Kaufmann, San Mateo, CA.
Platt J. 1999. Probabilistic Outputs for Support
Vector Machines and Comparisions to regular-
ized Likelihood Methods. In Advances in Large
Margin Classifiers. Edited by Smola .J., Bartlett
P., Scholkopf B. and Schuurmans D. MIT Press.
Roth D. and Yih W.T. (2002). Probabilistic reason-
ing for entities and relation recognition. CoL-
ING’2002. 835-841.26-30 Aug 2002. Taiwan.
Zelenko D., Aone C. and Richardella. (2003). Ker-
nel methods for relation extraction. Journal of
Machine Learning Research. 3(Feb):1083-1106.
Zhang M., Su J., Wang D.M., Zhou G.D. and Tan
C.L. (2005). Discovering Relations from a Large
Raw Corpus Using Tree Similarity-based Clus-
tering, IJCNLP’2005, Lecture Notes in
Computer Science (LNCS 3651). 378-389. 11-16
Oct 2005. Jeju Island, South Korea.
Zhao S.B. and Grisman R. 2005. Extracting rela-
tions with integrated information using kernel
methods. ACL’2005: 419-426. Univ of Michi-
gan-Ann Arbor, USA, 25-30 June 2005.
Zhou G.D. and Su Jian. Named Entity Recogni-
tion Using a HMM-based Chunk Tagger,
ACL’2002. pp473-480. Philadelphia. July
2002.
Zhou G.D., Su J. Zhang J. and Zhang M. (2005).
Exploring various knowledge in relation extrac-
tion. ACL’2005. 427-434. 25-30 June, Ann Ar-
bor, Michgan, USA.
</reference>
<page confidence="0.99675">
128
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.983692">
<title confidence="0.999978">Modeling Commonality among Related Classes in Relation Extraction</title>
<author confidence="0.999753">Zhou GuoDong Su Jian Zhang Min</author>
<affiliation confidence="0.999827">Institute for Infocomm Research</affiliation>
<address confidence="0.99606">21 Heng Mui Keng Terrace, Singapore 119613</address>
<email confidence="0.995208">zhougd,sujian,mzhang}@i2r.a-star.edu.sg</email>
<abstract confidence="0.99971137037037">This paper proposes a novel hierarchical learning strategy to deal with the data sparseness problem in relation extraction by modeling the commonality among related classes. For each class in the hierarchy either manually predefined or automatically clustered, a linear discriminative function is determined in a topdown way using a perceptron algorithm with the lower-level weight vector derived from the upper-level weight vector. As the upper-level class normally has much more positive training examples than the lower-level class, the corresponding linear discriminative function can be determined more reliably. The upperlevel discriminative function then can effectively guide the discriminative function learning in the lower-level, which otherwise might suffer from limited training data. Evaluation on the ACE RDC 2003 corpus shows that the hierarchical strategy much improves the performance by 5.6 and 5.1 in F-measure on leastand mediumfrequent relations respectively. It also shows that our system outperforms the previous best-reported system by 2.7 in F-measure on the 24 subtypes using the same feature set.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>Automatic Content Extraction.</title>
<note>http://www.ldc.upenn.edu/Projects/ACE/</note>
<marker></marker>
<rawString>ACE. (2000-2005). Automatic Content Extraction. http://www.ldc.upenn.edu/Projects/ACE/</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bunescu</author>
<author>R J Mooney</author>
</authors>
<title>A shortest path dependency kernel for relation extraction.</title>
<date>2005</date>
<volume>2005</volume>
<pages>724--731</pages>
<location>Vancover, B.C.</location>
<contexts>
<context position="5340" citStr="Bunescu and Mooney (2005" startWordPosition="811" endWordPosition="814">tem. The rest of this paper is organized as follows. Section 2 presents related work. Section 3 describes the hierarchical learning strategy using the perceptron algorithm. Finally, we present experimentation in Section 4 and conclude this paper in Section 5. 2 Related Work The relation extraction task was formulated at MUC-7(1998). With the increasing popularity of ACE, this task is starting to attract more and more researchers within the natural language processing and machine learning communities. Typical works include Miller et al (2000), Zelenko et al (2003), Culotta and Sorensen (2004), Bunescu and Mooney (2005a), Bunescu and Mooney (2005b), Zhang et al (2005), Roth and Yih (2002), Kambhatla (2004), Zhao and Grisman (2005) and Zhou et al (2005). Miller et al (2000) augmented syntactic full parse trees with semantic information of entities and relations, and built generative models to integrate various tasks such as POS tagging, named entity recognition, template element extraction and relation extraction. The problem is that such integration may impose big challenges, e.g. the need of a large annotated corpus. To overcome the data sparseness problem, generative models typically applied some smoothin</context>
<context position="6710" citStr="Bunescu and Mooney (2005" startWordPosition="1026" endWordPosition="1029">2003) proposed extracting relations by computing kernel functions between parse trees. Culotta and Sorensen (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved Fmeasure of 45.8 on the 5 relation types in the ACE RDC 2003 corpus1. Bunescu and Mooney (2005a) proposed a shortest path dependency kernel. They argued that the information to model a relationship between two entities can be typically captured by the shortest path between them in the dependency graph. It achieved the Fmeasure of 52.5 on the 5 relation types in the ACE RDC 2003 corpus. Bunescu and Mooney (2005b) proposed a subsequence kernel and ap1 The ACE RDC 2003 corpus defines 5/24 relation types/subtypes between 4 entity types. plied it in protein interaction and ACE relation extraction tasks. Zhang et al (2005) adopted clustering algorithms in unsupervised relation extraction using tree kernels. To overcome the data sparseness problem, various scales of sub-trees are applied in the tree kernel computation. Although tree kernel-based approaches are able to explore the huge implicit feature space without much feature engineering, further research work is necessary to make them effective and eff</context>
</contexts>
<marker>Bunescu, Mooney, 2005</marker>
<rawString>Bunescu R. &amp; Mooney R.J. (2005a). A shortest path dependency kernel for relation extraction. HLT/EMNLP’2005: 724-731. 6-8 Oct 2005. Vancover, B.C.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bunescu</author>
<author>R J Mooney</author>
</authors>
<title>Subsequence Kernels for Relation Extraction NIPS’2005.</title>
<date>2005</date>
<location>Vancouver, BC,</location>
<contexts>
<context position="5340" citStr="Bunescu and Mooney (2005" startWordPosition="811" endWordPosition="814">tem. The rest of this paper is organized as follows. Section 2 presents related work. Section 3 describes the hierarchical learning strategy using the perceptron algorithm. Finally, we present experimentation in Section 4 and conclude this paper in Section 5. 2 Related Work The relation extraction task was formulated at MUC-7(1998). With the increasing popularity of ACE, this task is starting to attract more and more researchers within the natural language processing and machine learning communities. Typical works include Miller et al (2000), Zelenko et al (2003), Culotta and Sorensen (2004), Bunescu and Mooney (2005a), Bunescu and Mooney (2005b), Zhang et al (2005), Roth and Yih (2002), Kambhatla (2004), Zhao and Grisman (2005) and Zhou et al (2005). Miller et al (2000) augmented syntactic full parse trees with semantic information of entities and relations, and built generative models to integrate various tasks such as POS tagging, named entity recognition, template element extraction and relation extraction. The problem is that such integration may impose big challenges, e.g. the need of a large annotated corpus. To overcome the data sparseness problem, generative models typically applied some smoothin</context>
<context position="6710" citStr="Bunescu and Mooney (2005" startWordPosition="1026" endWordPosition="1029">2003) proposed extracting relations by computing kernel functions between parse trees. Culotta and Sorensen (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved Fmeasure of 45.8 on the 5 relation types in the ACE RDC 2003 corpus1. Bunescu and Mooney (2005a) proposed a shortest path dependency kernel. They argued that the information to model a relationship between two entities can be typically captured by the shortest path between them in the dependency graph. It achieved the Fmeasure of 52.5 on the 5 relation types in the ACE RDC 2003 corpus. Bunescu and Mooney (2005b) proposed a subsequence kernel and ap1 The ACE RDC 2003 corpus defines 5/24 relation types/subtypes between 4 entity types. plied it in protein interaction and ACE relation extraction tasks. Zhang et al (2005) adopted clustering algorithms in unsupervised relation extraction using tree kernels. To overcome the data sparseness problem, various scales of sub-trees are applied in the tree kernel computation. Although tree kernel-based approaches are able to explore the huge implicit feature space without much feature engineering, further research work is necessary to make them effective and eff</context>
</contexts>
<marker>Bunescu, Mooney, 2005</marker>
<rawString>Bunescu R. &amp; Mooney R.J. (2005b). Subsequence Kernels for Relation Extraction NIPS’2005. Vancouver, BC, December 2005</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Breiman</author>
</authors>
<title>Bagging Predictors.</title>
<date>1996</date>
<booktitle>Machine Learning,</booktitle>
<volume>24</volume>
<issue>2</issue>
<pages>123--140</pages>
<contexts>
<context position="15351" citStr="Breiman 1996" startWordPosition="2462" endWordPosition="2463">In this paper, we have set the maximal iteration number to 10 for both efficiency and stable performance and the final weight vector in the discriminative function is averaged over those of the discriminative functions in the last few iterations (e.g. 5 in this paper). Bagging One more problem with any online classifier learning algorithm, including the perceptron algorithm, is that the learned discriminative function somewhat depends on the feeding order of the training examples. In order to eliminate such dependence and further improve the performance, an ensemble technique, called bagging (Breiman 1996), is applied in this paper. In bagging, the bootstrap technique is first used to build M (e.g. 10 in this paper) replicate sample sets by randomly re-sampling with replacement from the given training set repeatedly. Then, each training sample set is used to train a certain discriminative function. Finally, the final weight vector in the discriminative function is averaged over those of the M discriminative functions in the ensemble. Multi-Class Classification Basically, the perceptron algorithm is only for binary classification. Therefore, we must extend the perceptron algorithms to multi-clas</context>
</contexts>
<marker>Breiman, 1996</marker>
<rawString>Breiman L. (1996) Bagging Predictors. Machine Learning, 24(2): 123-140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>1999</date>
<institution>Ph.D. Dissertation, University of Pennsylvania.</institution>
<marker>Collins, 1999</marker>
<rawString>Collins M. (1999). Head-driven statistical models for natural language parsing. Ph.D. Dissertation, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Culotta</author>
<author>J Sorensen</author>
</authors>
<title>Dependency tree kernels for relation extraction.</title>
<date>2004</date>
<pages>2004--423</pages>
<location>Barcelona,</location>
<contexts>
<context position="5314" citStr="Culotta and Sorensen (2004)" startWordPosition="807" endWordPosition="810">he previous best-reported system. The rest of this paper is organized as follows. Section 2 presents related work. Section 3 describes the hierarchical learning strategy using the perceptron algorithm. Finally, we present experimentation in Section 4 and conclude this paper in Section 5. 2 Related Work The relation extraction task was formulated at MUC-7(1998). With the increasing popularity of ACE, this task is starting to attract more and more researchers within the natural language processing and machine learning communities. Typical works include Miller et al (2000), Zelenko et al (2003), Culotta and Sorensen (2004), Bunescu and Mooney (2005a), Bunescu and Mooney (2005b), Zhang et al (2005), Roth and Yih (2002), Kambhatla (2004), Zhao and Grisman (2005) and Zhou et al (2005). Miller et al (2000) augmented syntactic full parse trees with semantic information of entities and relations, and built generative models to integrate various tasks such as POS tagging, named entity recognition, template element extraction and relation extraction. The problem is that such integration may impose big challenges, e.g. the need of a large annotated corpus. To overcome the data sparseness problem, generative models typic</context>
</contexts>
<marker>Culotta, Sorensen, 2004</marker>
<rawString>Culotta A. and Sorensen J. (2004). Dependency tree kernels for relation extraction. ACL’2004. 423-429. 21-26 July 2004. Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Kambhatla</author>
</authors>
<title>Combining lexical, syntactic and semantic features with Maximum Entropy models for extracting relations.</title>
<date>2004</date>
<booktitle>ACL’2004(Poster).</booktitle>
<pages>178--181</pages>
<location>Barcelona,</location>
<contexts>
<context position="3567" citStr="Kambhatla 2004" startWordPosition="549" endWordPosition="550">e lack of the training data and fail to achieve steady performance given the current corpus size. Given the relative large size of this corpus, it will be time-consuming and very expensive to further expand the corpus with a reasonable gain in performance. Even if we can somehow expend the corpus and achieve steady performance on major relation subtypes, it will be still far beyond practice for those minor subtypes given the much unevenly distribution among different relation subtypes. While various machine learning approaches, such as generative modeling (Miller et al 2000), maximum entropy (Kambhatla 2004) and support vector machines (Zhao and Grisman 2005; Zhou et al 2005), have been applied in the relation extraction task, no explicit learning strategy is proposed to deal with the inherent data sparseness problem caused by the much uneven distribution among different relations. This paper proposes a novel hierarchical learning strategy to deal with the data sparseness problem by modeling the commonality among related classes. Through organizing various classes hierarchically, a linear discriminative function is determined for each class in a topdown way using a perceptron algorithm with the l</context>
<context position="5429" citStr="Kambhatla (2004)" startWordPosition="827" endWordPosition="828">escribes the hierarchical learning strategy using the perceptron algorithm. Finally, we present experimentation in Section 4 and conclude this paper in Section 5. 2 Related Work The relation extraction task was formulated at MUC-7(1998). With the increasing popularity of ACE, this task is starting to attract more and more researchers within the natural language processing and machine learning communities. Typical works include Miller et al (2000), Zelenko et al (2003), Culotta and Sorensen (2004), Bunescu and Mooney (2005a), Bunescu and Mooney (2005b), Zhang et al (2005), Roth and Yih (2002), Kambhatla (2004), Zhao and Grisman (2005) and Zhou et al (2005). Miller et al (2000) augmented syntactic full parse trees with semantic information of entities and relations, and built generative models to integrate various tasks such as POS tagging, named entity recognition, template element extraction and relation extraction. The problem is that such integration may impose big challenges, e.g. the need of a large annotated corpus. To overcome the data sparseness problem, generative models typically applied some smoothing techniques to integrate different scales of contexts in parameter estimation, e.g. the </context>
<context position="7659" citStr="Kambhatla (2004)" startWordPosition="1166" endWordPosition="1167">blem, various scales of sub-trees are applied in the tree kernel computation. Although tree kernel-based approaches are able to explore the huge implicit feature space without much feature engineering, further research work is necessary to make them effective and efficient. Comparably, feature-based approaches achieved much success recently. Roth and Yih (2002) used the SNoW classifier to incorporate various features such as word, part-of-speech and semantic information from WordNet, and proposed a probabilistic reasoning approach to integrate named entity recognition and relation extraction. Kambhatla (2004) employed maximum entropy models with features derived from word, entity type, mention level, overlap, dependency tree, parse tree and achieved Fmeasure of 52.8 on the 24 relation subtypes in the ACE RDC 2003 corpus. Zhao and Grisman (2005)2 combined various kinds of knowledge from tokenization, sentence parsing and deep dependency analysis through support vector machines and achieved F-measure of 70.1 on the 7 relation types of the ACE RDC 2004 corpus3. Zhou et al (2005) further systematically explored diverse lexical, syntactic and semantic features through support vector machines and achiev</context>
<context position="29212" citStr="Kambhatla (2004)" startWordPosition="4695" endWordPosition="4697"> while keeping all the training examples of remaining relation subtypes. It shows that the hierarchical strategy performs much better than the flat strategy when only a small amount of training examples is available. It also shows that the hierarchical strategy can achieve stable performance much faster than the flat strategy. Finally, it shows that the ACE RDC 2003 task suffers from the lack of training examples. Among the three major relation subtypes, only the subtype “Located” achieves steady performance. Finally, we also compare our system with the previous best-reported systems, such as Kambhatla (2004) and Zhou et al (2005). Table 5 shows that our system outperforms the previous best-reported system by 2.7 (58.2 vs. 55.5) in Fmeasure, largely due to the gain in recall. It indicates that, although support vector machines and maximum entropy models always perform better than the simple perceptron algorithm in most (if not all) applications, the hierarchical learning strategy using the perceptron algorithm can easily overcome the difference and outperforms the flat learning strategy using the overwhelming support vector machines and maximum entropy models in relation extraction, at least on th</context>
<context position="30717" citStr="Kambhatla (2004)" startWordPosition="4940" endWordPosition="4941">d flat learning strategies on the relation subtypes of different training data sizes. Notes: the figures in the parentheses indicate the cosine similarities between the weight vectors of the linear discriminative functions learned using the two strategies. Figure 2: Learning curve of the hierarchical strategy and its comparison with the flat strategy for some major relation subtypes (Note: FS for the flat strategy and HS for the hierarchical strategy) System Performance P R F Our: Perceptron Algorithm + Hierarchical Strategy 63.6 53.6 58.2 Zhou et al (2005): SVM + Flat Strategy 63.1 49.5 55.5 Kambhatla (2004): Maximum Entropy + Flat Strategy 63.5 45.2 52.8 Table 5: Comparison of our system with other best-reported systems 60 F-measure 40 20 70 50 30 10 Training Data Size HS: General-Staff FS: General-Staff HS: Part-Of FS: Part-Of HS: Located FS: Located 127 5 Conclusion This paper proposes a novel hierarchical learning strategy to deal with the data sparseness problem in relation extraction by modeling the commonality among related classes. For each class in a class hierarchy, a linear discriminative function is determined in a top-down way using the perceptron algorithm with the lower-level weigh</context>
</contexts>
<marker>Kambhatla, 2004</marker>
<rawString>Kambhatla N. (2004). Combining lexical, syntactic and semantic features with Maximum Entropy models for extracting relations. ACL’2004(Poster). 178-181. 21-26 July 2004. Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
</authors>
<title>WordNet: An online lexical database.</title>
<date>1990</date>
<journal>International Journal of Lexicography.</journal>
<pages>3--4</pages>
<marker>Miller, 1990</marker>
<rawString>Miller G.A. (1990). WordNet: An online lexical database. International Journal of Lexicography. 3(4):235-312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Miller</author>
<author>H Fox</author>
<author>L Ramshaw</author>
<author>R Weischedel</author>
</authors>
<title>A novel use of statistical parsing to extract information from text.</title>
<date>2000</date>
<booktitle>ANLP’2000. 226-233. 29 April - 4</booktitle>
<location>Seattle, USA</location>
<contexts>
<context position="3533" citStr="Miller et al 2000" startWordPosition="543" endWordPosition="546">most relation subtypes suffer from the lack of the training data and fail to achieve steady performance given the current corpus size. Given the relative large size of this corpus, it will be time-consuming and very expensive to further expand the corpus with a reasonable gain in performance. Even if we can somehow expend the corpus and achieve steady performance on major relation subtypes, it will be still far beyond practice for those minor subtypes given the much unevenly distribution among different relation subtypes. While various machine learning approaches, such as generative modeling (Miller et al 2000), maximum entropy (Kambhatla 2004) and support vector machines (Zhao and Grisman 2005; Zhou et al 2005), have been applied in the relation extraction task, no explicit learning strategy is proposed to deal with the inherent data sparseness problem caused by the much uneven distribution among different relations. This paper proposes a novel hierarchical learning strategy to deal with the data sparseness problem by modeling the commonality among related classes. Through organizing various classes hierarchically, a linear discriminative function is determined for each class in a topdown way using</context>
<context position="5263" citStr="Miller et al (2000)" startWordPosition="798" endWordPosition="801"> on the hierarchical strategy outperforms the previous best-reported system. The rest of this paper is organized as follows. Section 2 presents related work. Section 3 describes the hierarchical learning strategy using the perceptron algorithm. Finally, we present experimentation in Section 4 and conclude this paper in Section 5. 2 Related Work The relation extraction task was formulated at MUC-7(1998). With the increasing popularity of ACE, this task is starting to attract more and more researchers within the natural language processing and machine learning communities. Typical works include Miller et al (2000), Zelenko et al (2003), Culotta and Sorensen (2004), Bunescu and Mooney (2005a), Bunescu and Mooney (2005b), Zhang et al (2005), Roth and Yih (2002), Kambhatla (2004), Zhao and Grisman (2005) and Zhou et al (2005). Miller et al (2000) augmented syntactic full parse trees with semantic information of entities and relations, and built generative models to integrate various tasks such as POS tagging, named entity recognition, template element extraction and relation extraction. The problem is that such integration may impose big challenges, e.g. the need of a large annotated corpus. To overcome t</context>
</contexts>
<marker>Miller, Fox, Ramshaw, Weischedel, 2000</marker>
<rawString>Miller S., Fox H., Ramshaw L. and Weischedel R. (2000). A novel use of statistical parsing to extract information from text. ANLP’2000. 226-233. 29 April - 4 May 2000, Seattle, USA</rawString>
</citation>
<citation valid="true">
<authors>
<author>MUC-7</author>
</authors>
<date>1998</date>
<booktitle>Proceedings of the 7th Message Understanding Conference (MUC-7).</booktitle>
<publisher>Morgan Kaufmann,</publisher>
<location>San Mateo, CA.</location>
<marker>MUC-7, 1998</marker>
<rawString>MUC-7. (1998). Proceedings of the 7th Message Understanding Conference (MUC-7). Morgan Kaufmann, San Mateo, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Platt</author>
</authors>
<title>Probabilistic Outputs for Support Vector Machines and Comparisions to regularized Likelihood Methods.</title>
<date>1999</date>
<booktitle>In Advances in Large Margin Classifiers. Edited by</booktitle>
<publisher>MIT Press.</publisher>
<contexts>
<context position="16622" citStr="Platt (1999)" startWordPosition="2666" endWordPosition="2667"> we apply the one vs. others strategy, which builds K classifiers so as to separate one class from all others. However, the outputs for the perceptron algorithms of different classes may be not directly comparable since any positive scalar multiple of the weight vector will not affect the actual prediction of a perceptron algorithm. For comparability, we map the perceptron algorithm output into the probability by using an additional sigmoid model: where f = w ⋅ x is the output of a perceptron algorithm and the coefficients A &amp; B are to be trained using the model trust alorithm as described in Platt (1999). The final decision of an instance in multi-class classification is determined by the class which has the maximal probability from the corresponding perceptron algorithm. 3.2 Hierarchical Learning Strategy using the Perceptron Algorithm Assume we have a class hierarchy for a task, e.g. the one in the ACE RDC 2003 corpus as shown in Table 1 of Section 4.1. The hierarchical learning strategy explores the inherent commonality among related classes in a top-down way. For each class in the hierarchy, a linear discriminative function is determined in a top-down way with the lower-level weight vecto</context>
</contexts>
<marker>Platt, 1999</marker>
<rawString>Platt J. 1999. Probabilistic Outputs for Support Vector Machines and Comparisions to regularized Likelihood Methods. In Advances in Large Margin Classifiers. Edited by Smola .J., Bartlett P., Scholkopf B. and Schuurmans D. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
<author>W T Yih</author>
</authors>
<title>Probabilistic reasoning for entities and relation recognition.</title>
<date>2002</date>
<booktitle>CoLING’2002.</booktitle>
<pages>835--841</pages>
<contexts>
<context position="5411" citStr="Roth and Yih (2002)" startWordPosition="823" endWordPosition="826">ted work. Section 3 describes the hierarchical learning strategy using the perceptron algorithm. Finally, we present experimentation in Section 4 and conclude this paper in Section 5. 2 Related Work The relation extraction task was formulated at MUC-7(1998). With the increasing popularity of ACE, this task is starting to attract more and more researchers within the natural language processing and machine learning communities. Typical works include Miller et al (2000), Zelenko et al (2003), Culotta and Sorensen (2004), Bunescu and Mooney (2005a), Bunescu and Mooney (2005b), Zhang et al (2005), Roth and Yih (2002), Kambhatla (2004), Zhao and Grisman (2005) and Zhou et al (2005). Miller et al (2000) augmented syntactic full parse trees with semantic information of entities and relations, and built generative models to integrate various tasks such as POS tagging, named entity recognition, template element extraction and relation extraction. The problem is that such integration may impose big challenges, e.g. the need of a large annotated corpus. To overcome the data sparseness problem, generative models typically applied some smoothing techniques to integrate different scales of contexts in parameter est</context>
<context position="7406" citStr="Roth and Yih (2002)" startWordPosition="1129" endWordPosition="1132"> relation types/subtypes between 4 entity types. plied it in protein interaction and ACE relation extraction tasks. Zhang et al (2005) adopted clustering algorithms in unsupervised relation extraction using tree kernels. To overcome the data sparseness problem, various scales of sub-trees are applied in the tree kernel computation. Although tree kernel-based approaches are able to explore the huge implicit feature space without much feature engineering, further research work is necessary to make them effective and efficient. Comparably, feature-based approaches achieved much success recently. Roth and Yih (2002) used the SNoW classifier to incorporate various features such as word, part-of-speech and semantic information from WordNet, and proposed a probabilistic reasoning approach to integrate named entity recognition and relation extraction. Kambhatla (2004) employed maximum entropy models with features derived from word, entity type, mention level, overlap, dependency tree, parse tree and achieved Fmeasure of 52.8 on the 24 relation subtypes in the ACE RDC 2003 corpus. Zhao and Grisman (2005)2 combined various kinds of knowledge from tokenization, sentence parsing and deep dependency analysis thro</context>
</contexts>
<marker>Roth, Yih, 2002</marker>
<rawString>Roth D. and Yih W.T. (2002). Probabilistic reasoning for entities and relation recognition. CoLING’2002. 835-841.26-30 Aug 2002. Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Zelenko</author>
<author>C Aone</author>
<author>Richardella</author>
</authors>
<title>Kernel methods for relation extraction.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research.</journal>
<pages>3--1083</pages>
<contexts>
<context position="5285" citStr="Zelenko et al (2003)" startWordPosition="802" endWordPosition="806">strategy outperforms the previous best-reported system. The rest of this paper is organized as follows. Section 2 presents related work. Section 3 describes the hierarchical learning strategy using the perceptron algorithm. Finally, we present experimentation in Section 4 and conclude this paper in Section 5. 2 Related Work The relation extraction task was formulated at MUC-7(1998). With the increasing popularity of ACE, this task is starting to attract more and more researchers within the natural language processing and machine learning communities. Typical works include Miller et al (2000), Zelenko et al (2003), Culotta and Sorensen (2004), Bunescu and Mooney (2005a), Bunescu and Mooney (2005b), Zhang et al (2005), Roth and Yih (2002), Kambhatla (2004), Zhao and Grisman (2005) and Zhou et al (2005). Miller et al (2000) augmented syntactic full parse trees with semantic information of entities and relations, and built generative models to integrate various tasks such as POS tagging, named entity recognition, template element extraction and relation extraction. The problem is that such integration may impose big challenges, e.g. the need of a large annotated corpus. To overcome the data sparseness pro</context>
</contexts>
<marker>Zelenko, Aone, Richardella, 2003</marker>
<rawString>Zelenko D., Aone C. and Richardella. (2003). Kernel methods for relation extraction. Journal of Machine Learning Research. 3(Feb):1083-1106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Zhang</author>
<author>J Su</author>
<author>D M Wang</author>
<author>G D Zhou</author>
<author>C L Tan</author>
</authors>
<title>Discovering Relations from a Large Raw Corpus Using Tree Similarity-based Clustering,</title>
<date>2005</date>
<booktitle>IJCNLP’2005, Lecture Notes in Computer Science (LNCS</booktitle>
<volume>3651</volume>
<pages>378--389</pages>
<location>South</location>
<contexts>
<context position="5390" citStr="Zhang et al (2005)" startWordPosition="819" endWordPosition="822">tion 2 presents related work. Section 3 describes the hierarchical learning strategy using the perceptron algorithm. Finally, we present experimentation in Section 4 and conclude this paper in Section 5. 2 Related Work The relation extraction task was formulated at MUC-7(1998). With the increasing popularity of ACE, this task is starting to attract more and more researchers within the natural language processing and machine learning communities. Typical works include Miller et al (2000), Zelenko et al (2003), Culotta and Sorensen (2004), Bunescu and Mooney (2005a), Bunescu and Mooney (2005b), Zhang et al (2005), Roth and Yih (2002), Kambhatla (2004), Zhao and Grisman (2005) and Zhou et al (2005). Miller et al (2000) augmented syntactic full parse trees with semantic information of entities and relations, and built generative models to integrate various tasks such as POS tagging, named entity recognition, template element extraction and relation extraction. The problem is that such integration may impose big challenges, e.g. the need of a large annotated corpus. To overcome the data sparseness problem, generative models typically applied some smoothing techniques to integrate different scales of cont</context>
<context position="6921" citStr="Zhang et al (2005)" startWordPosition="1060" endWordPosition="1063">re of 45.8 on the 5 relation types in the ACE RDC 2003 corpus1. Bunescu and Mooney (2005a) proposed a shortest path dependency kernel. They argued that the information to model a relationship between two entities can be typically captured by the shortest path between them in the dependency graph. It achieved the Fmeasure of 52.5 on the 5 relation types in the ACE RDC 2003 corpus. Bunescu and Mooney (2005b) proposed a subsequence kernel and ap1 The ACE RDC 2003 corpus defines 5/24 relation types/subtypes between 4 entity types. plied it in protein interaction and ACE relation extraction tasks. Zhang et al (2005) adopted clustering algorithms in unsupervised relation extraction using tree kernels. To overcome the data sparseness problem, various scales of sub-trees are applied in the tree kernel computation. Although tree kernel-based approaches are able to explore the huge implicit feature space without much feature engineering, further research work is necessary to make them effective and efficient. Comparably, feature-based approaches achieved much success recently. Roth and Yih (2002) used the SNoW classifier to incorporate various features such as word, part-of-speech and semantic information fro</context>
</contexts>
<marker>Zhang, Su, Wang, Zhou, Tan, 2005</marker>
<rawString>Zhang M., Su J., Wang D.M., Zhou G.D. and Tan C.L. (2005). Discovering Relations from a Large Raw Corpus Using Tree Similarity-based Clustering, IJCNLP’2005, Lecture Notes in Computer Science (LNCS 3651). 378-389. 11-16 Oct 2005. Jeju Island, South Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S B Zhao</author>
<author>R Grisman</author>
</authors>
<title>Extracting relations with integrated information using kernel methods. ACL’2005: 419-426. Univ of Michigan-Ann Arbor,</title>
<date>2005</date>
<location>USA,</location>
<contexts>
<context position="3618" citStr="Zhao and Grisman 2005" startWordPosition="555" endWordPosition="558">ve steady performance given the current corpus size. Given the relative large size of this corpus, it will be time-consuming and very expensive to further expand the corpus with a reasonable gain in performance. Even if we can somehow expend the corpus and achieve steady performance on major relation subtypes, it will be still far beyond practice for those minor subtypes given the much unevenly distribution among different relation subtypes. While various machine learning approaches, such as generative modeling (Miller et al 2000), maximum entropy (Kambhatla 2004) and support vector machines (Zhao and Grisman 2005; Zhou et al 2005), have been applied in the relation extraction task, no explicit learning strategy is proposed to deal with the inherent data sparseness problem caused by the much uneven distribution among different relations. This paper proposes a novel hierarchical learning strategy to deal with the data sparseness problem by modeling the commonality among related classes. Through organizing various classes hierarchically, a linear discriminative function is determined for each class in a topdown way using a perceptron algorithm with the lower-level weight vector derived from the upper-lev</context>
<context position="5454" citStr="Zhao and Grisman (2005)" startWordPosition="829" endWordPosition="832">rchical learning strategy using the perceptron algorithm. Finally, we present experimentation in Section 4 and conclude this paper in Section 5. 2 Related Work The relation extraction task was formulated at MUC-7(1998). With the increasing popularity of ACE, this task is starting to attract more and more researchers within the natural language processing and machine learning communities. Typical works include Miller et al (2000), Zelenko et al (2003), Culotta and Sorensen (2004), Bunescu and Mooney (2005a), Bunescu and Mooney (2005b), Zhang et al (2005), Roth and Yih (2002), Kambhatla (2004), Zhao and Grisman (2005) and Zhou et al (2005). Miller et al (2000) augmented syntactic full parse trees with semantic information of entities and relations, and built generative models to integrate various tasks such as POS tagging, named entity recognition, template element extraction and relation extraction. The problem is that such integration may impose big challenges, e.g. the need of a large annotated corpus. To overcome the data sparseness problem, generative models typically applied some smoothing techniques to integrate different scales of contexts in parameter estimation, e.g. the back-off approach in Mill</context>
<context position="7899" citStr="Zhao and Grisman (2005)" startWordPosition="1205" endWordPosition="1208">ary to make them effective and efficient. Comparably, feature-based approaches achieved much success recently. Roth and Yih (2002) used the SNoW classifier to incorporate various features such as word, part-of-speech and semantic information from WordNet, and proposed a probabilistic reasoning approach to integrate named entity recognition and relation extraction. Kambhatla (2004) employed maximum entropy models with features derived from word, entity type, mention level, overlap, dependency tree, parse tree and achieved Fmeasure of 52.8 on the 24 relation subtypes in the ACE RDC 2003 corpus. Zhao and Grisman (2005)2 combined various kinds of knowledge from tokenization, sentence parsing and deep dependency analysis through support vector machines and achieved F-measure of 70.1 on the 7 relation types of the ACE RDC 2004 corpus3. Zhou et al (2005) further systematically explored diverse lexical, syntactic and semantic features through support vector machines and achieved Fmeasure of 68.1 and 55.5 on the 5 relation types and the 24 relation subtypes in the ACE RDC 2003 corpus respectively. To overcome the data sparseness problem, feature-based approaches normally incorporate various scales of contexts int</context>
<context position="9185" citStr="Zhao and Grisman (2005)" startWordPosition="1402" endWordPosition="1405">on adopted learning algorithms to weight and combine each feature effectively. For example, an exponential model and a linear model are applied in the maximum entropy models and support vector machines respectively to combine each feature via the learned weight vector. In summary, although various approaches have been employed in relation extraction, they implicitly attack the data sparseness problem by using features of different contexts in featurebased approaches or including different sub2 Here, we classify this paper into feature-based approaches since the feature space in the kernels of Zhao and Grisman (2005) can be easily represented by an explicit feature vector. 3 The ACE RDC 2004 corpus defines 7/27 relation types/subtypes between 7 entity types. 122 structures in kernel-based approaches. Until now, there are no explicit ways to capture the hierarchical topology in relation extraction. Currently, all the current approaches apply the flat learning strategy which equally treats training examples in different relations independently and ignore the commonality among different relations. This paper proposes a novel hierarchical learning strategy to resolve this problem by considering the relatednes</context>
</contexts>
<marker>Zhao, Grisman, 2005</marker>
<rawString>Zhao S.B. and Grisman R. 2005. Extracting relations with integrated information using kernel methods. ACL’2005: 419-426. Univ of Michigan-Ann Arbor, USA, 25-30 June 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G D Zhou</author>
<author>Su Jian</author>
</authors>
<title>Named Entity Recognition Using a HMM-based Chunk Tagger,</title>
<date>2002</date>
<volume>2002</volume>
<pages>473--480</pages>
<location>Philadelphia.</location>
<marker>Zhou, Jian, 2002</marker>
<rawString>Zhou G.D. and Su Jian. Named Entity Recognition Using a HMM-based Chunk Tagger, ACL’2002. pp473-480. Philadelphia. July 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G D Zhou</author>
<author>Su J Zhang J</author>
<author>M Zhang</author>
</authors>
<title>Exploring various knowledge in relation extraction.</title>
<date>2005</date>
<pages>2005--427</pages>
<location>Ann Arbor, Michgan, USA.</location>
<contexts>
<context position="2554" citStr="Zhou et al 2005" startWordPosition="388" endWordPosition="391">EDT), Relation Detection and Characterization (RDC), and Event Detection and Characterization (EDC). This paper will focus on the ACE RDC task, which detects and classifies various semantic relations between two entities. For example, we want to determine whether a person is at a location, based on the evidence in the context. Extraction of semantic relationships between entities can be very useful for applications such as question answering, e.g. to answer the query “Who is the president of the United States?”. One major challenge in relation extraction is due to the data sparseness problem (Zhou et al 2005). As the largest annotated corpus in relation extraction, the ACE RDC 2003 corpus shows that different subtypes/types of relations are much unevenly distributed and a few relation subtypes, such as the subtype “Founder” under the type “ROLE”, suffers from a small amount of annotated data. Further experimentation in this paper (please see Figure 2) shows that most relation subtypes suffer from the lack of the training data and fail to achieve steady performance given the current corpus size. Given the relative large size of this corpus, it will be time-consuming and very expensive to further ex</context>
<context position="5476" citStr="Zhou et al (2005)" startWordPosition="834" endWordPosition="837">ing the perceptron algorithm. Finally, we present experimentation in Section 4 and conclude this paper in Section 5. 2 Related Work The relation extraction task was formulated at MUC-7(1998). With the increasing popularity of ACE, this task is starting to attract more and more researchers within the natural language processing and machine learning communities. Typical works include Miller et al (2000), Zelenko et al (2003), Culotta and Sorensen (2004), Bunescu and Mooney (2005a), Bunescu and Mooney (2005b), Zhang et al (2005), Roth and Yih (2002), Kambhatla (2004), Zhao and Grisman (2005) and Zhou et al (2005). Miller et al (2000) augmented syntactic full parse trees with semantic information of entities and relations, and built generative models to integrate various tasks such as POS tagging, named entity recognition, template element extraction and relation extraction. The problem is that such integration may impose big challenges, e.g. the need of a large annotated corpus. To overcome the data sparseness problem, generative models typically applied some smoothing techniques to integrate different scales of contexts in parameter estimation, e.g. the back-off approach in Miller et al (2000). Zelen</context>
<context position="8135" citStr="Zhou et al (2005)" startWordPosition="1243" endWordPosition="1246">from WordNet, and proposed a probabilistic reasoning approach to integrate named entity recognition and relation extraction. Kambhatla (2004) employed maximum entropy models with features derived from word, entity type, mention level, overlap, dependency tree, parse tree and achieved Fmeasure of 52.8 on the 24 relation subtypes in the ACE RDC 2003 corpus. Zhao and Grisman (2005)2 combined various kinds of knowledge from tokenization, sentence parsing and deep dependency analysis through support vector machines and achieved F-measure of 70.1 on the 7 relation types of the ACE RDC 2004 corpus3. Zhou et al (2005) further systematically explored diverse lexical, syntactic and semantic features through support vector machines and achieved Fmeasure of 68.1 and 55.5 on the 5 relation types and the 24 relation subtypes in the ACE RDC 2003 corpus respectively. To overcome the data sparseness problem, feature-based approaches normally incorporate various scales of contexts into the feature vector extensively. These approaches then depend on adopted learning algorithms to weight and combine each feature effectively. For example, an exponential model and a linear model are applied in the maximum entropy models</context>
<context position="20830" citStr="Zhou et al (2005)" startWordPosition="3352" endWordPosition="3355">only done at the lowest level while keeping the upper-level class hierarchy. That is, only sibling classes at the lowest level are hierarchically clustered. • All-level hybrid: Binary hierarchical clustering is done at all levels in a bottom-up way. That is, sibling classes at the lowest level are hierarchically clustered first and then sibling classes at the upper-level. In this way, the binary class hierarchy can be built iteratively in a bottom-up way. 4 Experimentation This paper uses the ACE RDC 2003 corpus provided by LDC to train and evaluate the hierarchical learning strategy. Same as Zhou et al (2005), we only model explicit relations and explicitly model the argument order of the two mentions involved. 4.1 Experimental Setting Type Subtype Freq Bin Type AT Based-In 347 Medium Located 2126 Large Residence 308 Medium NEAR Relative-Location 201 Medium PART Part-Of 947 Large Subsidiary 355 Medium Other 6 Small ROLE Affiliate-Partner 204 Medium Citizen-Of 328 Medium Client 144 Small Founder 26 Small General-Staff 1331 Large Management 1242 Large Member 1091 Large Owner 232 Medium Other 158 Small SOCIAL Associate 91 Small Grandparent 12 Small Other-Personal 85 Small Other-Professional 339 Mediu</context>
<context position="22589" citStr="Zhou et al (2005)" startWordPosition="3640" endWordPosition="3643">elation examples. All the experiments are done five times on the 24 relation subtypes in the ACE corpus, except otherwise specified, with the final performance averaged using the same re-sampling with replacement strategy as the one in the bagging technique. Table 1 lists various types and subtypes of relations for the ACE RDC 2003 corpus, along with their occurrence frequency in the training data. It shows that this corpus suffers from a small amount of annotated data for a few subtypes such as the subtype “Founder” under the type “ROLE”. For comparison, we also adopt the same feature set as Zhou et al (2005): word, entity type, 125 mention level, overlap, base phrase chunking, dependency tree, parse tree and semantic information. 4.2 Experimental Results Table 2 shows the performance of the hierarchical learning strategy using the existing class hierarchy in the given ACE corpus and its comparison with the flat learning strategy, using the perceptron algorithm. It shows that the pure hierarchical strategy outperforms the pure flat strategy by 1.5 (56.9 vs. 55.4) in F-measure. It also shows that further smoothing and bagging improve the performance of the hierarchical and flat strategies by 0.6 an</context>
<context position="29234" citStr="Zhou et al (2005)" startWordPosition="4699" endWordPosition="4702">e training examples of remaining relation subtypes. It shows that the hierarchical strategy performs much better than the flat strategy when only a small amount of training examples is available. It also shows that the hierarchical strategy can achieve stable performance much faster than the flat strategy. Finally, it shows that the ACE RDC 2003 task suffers from the lack of training examples. Among the three major relation subtypes, only the subtype “Located” achieves steady performance. Finally, we also compare our system with the previous best-reported systems, such as Kambhatla (2004) and Zhou et al (2005). Table 5 shows that our system outperforms the previous best-reported system by 2.7 (58.2 vs. 55.5) in Fmeasure, largely due to the gain in recall. It indicates that, although support vector machines and maximum entropy models always perform better than the simple perceptron algorithm in most (if not all) applications, the hierarchical learning strategy using the perceptron algorithm can easily overcome the difference and outperforms the flat learning strategy using the overwhelming support vector machines and maximum entropy models in relation extraction, at least on the ACE RDC 2003 corpus.</context>
<context position="30664" citStr="Zhou et al (2005)" startWordPosition="4929" endWordPosition="4932">2 26.3 31.8 Table 4: Comparison of the hierarchical and flat learning strategies on the relation subtypes of different training data sizes. Notes: the figures in the parentheses indicate the cosine similarities between the weight vectors of the linear discriminative functions learned using the two strategies. Figure 2: Learning curve of the hierarchical strategy and its comparison with the flat strategy for some major relation subtypes (Note: FS for the flat strategy and HS for the hierarchical strategy) System Performance P R F Our: Perceptron Algorithm + Hierarchical Strategy 63.6 53.6 58.2 Zhou et al (2005): SVM + Flat Strategy 63.1 49.5 55.5 Kambhatla (2004): Maximum Entropy + Flat Strategy 63.5 45.2 52.8 Table 5: Comparison of our system with other best-reported systems 60 F-measure 40 20 70 50 30 10 Training Data Size HS: General-Staff FS: General-Staff HS: Part-Of FS: Part-Of HS: Located FS: Located 127 5 Conclusion This paper proposes a novel hierarchical learning strategy to deal with the data sparseness problem in relation extraction by modeling the commonality among related classes. For each class in a class hierarchy, a linear discriminative function is determined in a top-down way usin</context>
</contexts>
<marker>Zhou, J, Zhang, 2005</marker>
<rawString>Zhou G.D., Su J. Zhang J. and Zhang M. (2005). Exploring various knowledge in relation extraction. ACL’2005. 427-434. 25-30 June, Ann Arbor, Michgan, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>