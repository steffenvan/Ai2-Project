<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.036068">
<title confidence="0.977717">
Coreference-inspired Coherence Modeling
</title>
<author confidence="0.868963">
Micha Elsner and Eugene Charniak
</author>
<affiliation confidence="0.8730565">
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University
</affiliation>
<address confidence="0.941435">
Providence, RI 02912
</address>
<email confidence="0.999765">
{melsner,ec}@cs.brown.edu
</email>
<sectionHeader confidence="0.998581" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9990384">
Research on coreference resolution and sum-
marization has modeled the way entities are
realized as concrete phrases in discourse. In
particular there exist models of the noun
phrase syntax used for discourse-new versus
discourse-old referents, and models describ-
ing the likely distance between a pronoun and
its antecedent. However, models of discourse
coherence, as applied to information ordering
tasks, have ignored these kinds of information.
We apply a discourse-new classifier and pro-
noun coreference algorithm to the information
ordering task, and show significant improve-
ments in performance over the entity grid, a
popular model of local coherence.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999939529411765">
Models of discourse coherence describe the relation-
ships between nearby sentences, in which previous
sentences help make their successors easier to un-
derstand. Models of coherence have been used to
impose an order on sentences for multidocument
summarization (Barzilay et al., 2002), to evaluate
the quality of human-authored essays (Miltsakaki
and Kukich, 2004), and to insert new information
into existing documents (Chen et al., 2007).
These models typically view a sentence either as
a bag of words (Foltz et al., 1998) or as a bag of en-
tities associated with various syntactic roles (Lapata
and Barzilay, 2005). However, a mention of an en-
tity contains more information than just its head and
syntactic role. The referring expression itself con-
tains discourse-motivated information distinguish-
ing familiar entities from unfamiliar and salient from
</bodyText>
<page confidence="0.991062">
41
</page>
<bodyText confidence="0.999858482758621">
non-salient. These patterns have been studied ex-
tensively, by linguists (Prince, 1981; Fraurud, 1990)
and in the field of coreference resolution. We draw
on the coreference work, taking two standard models
from the literature and applying them to coherence
modeling.
Our first model distinguishes discourse-new from
discourse-old noun phrases, using features based
on Uryupina (2003). Discourse-new NPs are those
whose referents have not been previously mentioned
in the discourse. As noted by studies since Hawkins
(1978), there are marked syntactic differences be-
tween the two classes.
Our second model describes pronoun coreference.
To be intelligible, pronouns must be placed close to
appropriate referents with the correct number and
gender. Centering theory (Grosz et al., 1995) de-
scribes additional constraints about which entities in
a discourse can be pronominalized: if there are pro-
nouns in a segment, they must include the backward-
looking center. We use a model which probabilisti-
cally attempts to describe these preferences (Ge et
al., 1998).
These two models can be combined with the en-
tity grid described by Lapata and Barzilay (2005)
for significant improvement. The magnitude of the
improvement is particularly interesting given that
Barzilay and Lapata (2005) do use a coreference sys-
tem but are unable to derive much advantage from it.
</bodyText>
<sectionHeader confidence="0.999324" genericHeader="method">
2 Discourse-new Model
</sectionHeader>
<bodyText confidence="0.911996333333333">
In the task of discourse-new classification, the model
is given a referring expression (as in previous work,
we consider only NPs) from a document and must
</bodyText>
<subsubsectionHeader confidence="0.329094">
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 41–44,
</subsubsectionHeader>
<page confidence="0.487462">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.996101671875">
determine whether it is a first mention (discourse-
new) or a subsequent mention (discourse-old). Fea-
tures such as full names, appositives, and restrictive
relative clauses are associated with the introduction
of unfamiliar entities into discourse (Hawkins, 1978;
Fraurud, 1990; Vieira and Poesio, 2000). Classi-
fiers in the literature include (Poesio et al., 2005;
Uryupina, 2003; Ng and Cardie, 2002). The sys-
tem of Nenkova and McKeown (2003) works in the
opposite direction. It is designed to rewrite the ref-
erences in multi-document summaries, so that they
conform to the common discourse patterns.
We construct a maximum-entropy classifier us-
ing syntactic and lexical features derived from
Uryupina (2003), and a publicly available learning
tool (Daum´e III, 2004). Our system scores 87.4%
(F-score of the disc-new class on the MUC-7 for-
mal test set); this is comparable to the state-of-the-
art system of Uryupina (2003), which scores 86.91.
To model coreference with this system, we assign
each NP in a document a label Lnp E {new, old}.
Since the correct labeling depends on the coref-
erence relationships between the NPs, we need
some way to guess at this; we take all NPs with
the same head to be coreferent, as in the non-
coreference version of (Barzilay and Lapata, 2005)2.
We then take the probability of a document as
Hnp:NPs P(Lnp|np).
We must make several small changes to the model
to adapt it to this setting. For the discourse-new clas-
sification task, the model’s most important feature
is whether the head word of the NP to be classified
has occurred previously (as in Ng and Cardie (2002)
and Vieira and Poesio (2000)). For coherence mod-
eling, we must remove this feature, since it depends
on document order, which is precisely what we are
trying to predict. The coreference heuristic will also
fail to resolve any pronouns, so we discard them.
Another issue is that NPs whose referents are
familiar tend to resemble discourse-old NPs, even
though they have not been previously mentioned
(Fraurud, 1990). These include unique objects like
the FBI or generic ones like danger or percent. To
1Poesio et al. (2005) score 90.2%, but on a different corpus.
2Unfortunately, this represents a substantial sacrifice; as
Poesio and Vieira (1998) show, only about 2/3 of definite de-
scriptions which are anaphoric have the same head as their an-
tecedent.
avoid using these deceptive phrases as examples of
discourse-newness, we attempt to heuristically re-
move them from the training set by discarding any
NP whose head occurs only once in the document3.
The labels we apply to NPs in our test data are
systematically biased by the “same head” heuristic
we use for coreference. This is a disadvantage for
our system, but it has a corresponding advantage–
we can use training data labeled using the same
heuristic, without any loss in performance on the
coherence task. NPs we fail to learn about during
training are likely to be mislabeled at test time any-
way, so performance does not degrade by much. To
counter this slight degradation, we can use a much
larger training corpus, since we no longer require
gold-standard coreference annotations.
</bodyText>
<sectionHeader confidence="0.999401" genericHeader="method">
3 Pronoun Coreference Model
</sectionHeader>
<bodyText confidence="0.9999188">
Pronoun coreference is another important aspect of
coherence– if a pronoun is used too far away from
any natural referent, it becomes hard to interpret,
creating confusion. Too many referents, however,
create ambiguity. To describe this type of restriction,
we must model the probability of the text containing
pronouns (denoted ri), jointly with their referents
ai. (This takes more work than simply resolving the
pronouns conditioned on the text.) The model of Ge
et al. (1998) provides the requisite probabilities:
</bodyText>
<equation confidence="0.908506333333333">
P(ai, ri|ai−1
i ) =P(ai|h(ai), m(ai))
Pgen(ai, ri)Pnum(ai, ri)
</equation>
<bodyText confidence="0.996942333333333">
Here h(a) is the Hobbs distance (Hobbs, 1976),
which measures distance between a pronoun and
prospective antecedent, taking into account various
factors, such as syntactic constraints on pronouns.
m(a) is the number of times the antecedent has
been mentioned previously in the document (again
using “same head” coreference for full NPs, but
also counting the previous antecedents ai−1
i ). Pgen
and Pnum are distributions over gender and num-
ber given words. The model is trained using a small
hand-annotated corpus first used in Ge et al. (1998).
</bodyText>
<footnote confidence="0.992767">
3Bean and Riloff (1999) and Uryupina (2003) construct
quite accurate classifiers to detect unique NPs. However, some
preliminary experiments convinced us that our heuristic method
worked well enough for the purpose.
</footnote>
<page confidence="0.980472">
42
</page>
<table confidence="0.999782285714286">
Disc. Acc Disc. F Ins.
Random 50.00 50.00 12.58
Entity Grid 76.17 77.55 19.57
Disc-New 70.35 73.47 16.27
Pronoun 55.77 62.27 13.95
EGrid+Disc-New 78.88 80.31 21.93
Combined 79.60 81.02 22.98
</table>
<tableCaption confidence="0.999976">
Table 1: Results on 1004 WSJ documents.
</tableCaption>
<bodyText confidence="0.999809">
Finding the probability of a document using this
model requires us to sum out the antecedents a. Un-
fortunately, because each ai is conditioned on the
previous ones, this cannot be done efficiently. In-
stead, we use a greedy search, assigning each pro-
noun left to right. Finally we report the probability
of the resulting sequence of pronoun assignments.
</bodyText>
<sectionHeader confidence="0.999838" genericHeader="method">
4 Baseline Model
</sectionHeader>
<bodyText confidence="0.999990153846154">
As a baseline, we adopt the entity grid (Lapata and
Barzilay, 2005). This model outperforms a variety
of word overlap and semantic similarity models, and
is used as a component in the state-of-the-art system
of Soricut and Marcu (2006). The entity grid rep-
resents each entity by tracking the syntactic roles in
which it appears throughout the document. The in-
ternal syntax of the various referring expressions is
ignored. Since it also uses the “same head” corefer-
ence heuristic, it also disregards pronouns.
Since the three models use very different feature
sets, we combine them by assuming independence
and multiplying the probabilities.
</bodyText>
<sectionHeader confidence="0.999646" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.997160893617021">
We evaluate our models using two tasks, both based
on the assumption that a human-authored document
is coherent, and uses the best possible ordering of
its sentences (see Lapata (2006)). In the discrimina-
tion task (Barzilay and Lapata, 2005), a document
is compared with a random permutation of its sen-
tences, and we score the system correct if it indicates
the original as more coherent4.
4Since the model might refuse to make a decision by scor-
ing a permutation the same as the original, we also report
F-score, where precision is correct/decisions and recall is
correct/total.
Discrimination becomes easier for longer docu-
ments, since a random permutation is likely to be
much less similar to the original. Therefore we also
test our systems on the task of insertion (Chen et al.,
2007), in which we remove a sentence from a doc-
ument, then find the point of insertion which yields
the highest coherence score. The reported score is
the average fraction of sentences per document rein-
serted in their original position (averaged over doc-
uments, not sentences, so that longer documents do
not disproportionally influence the results)5.
We test on sections 14-24 of the Penn Treebank
(1004 documents total). Previous work has fo-
cused on the AIRPLANE corpus (Barzilay and Lee,
2004), which contains short announcements of air-
plane crashes written by and for domain experts.
These texts use a very constrained style, with few
discourse-new markers or pronouns, and so our sys-
tem is ineffective; the WSJ corpus is much more
typical of normal informative writing. Also unlike
previous work, we do not test the task of completely
reconstructing a document’s order, since this is com-
putationally intractable and results on WSJ docu-
ments6 would likely be dominated by search errors.
Our results are shown in table 5. When run alone,
the entity grid outperforms either of our models.
However, all three models are significantly better
than random. Combining all three models raises dis-
crimination performance by 3.5% over the baseline
and insertion by 3.4%. Even the weakest compo-
nent, pronouns, contributes to the joint model; when
it is left out, the resulting EGrid + Disc-New model
is significantly worse than the full combination. We
test significance using Wilcoxon’s signed-rank test;
all results are significant with p &lt; .001.
</bodyText>
<sectionHeader confidence="0.996774" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999948142857143">
The use of these coreference-inspired models leads
to significant improvements in the baseline. Of the
two, the discourse-new detector is by far more ef-
fective. The pronoun model’s main problem is that,
although a pronoun may have been displaced from
its original position, it can often find another seem-
ingly acceptable referent nearby. Despite this issue
</bodyText>
<footnote confidence="0.99907425">
5Although we designed a metric that distinguishes near
misses from random performance, it is very well correlated with
exact precision, so, for simplicity’s sake, we omit it.
6Average 22 sentences, as opposed to 11.5 for AIRPLANE.
</footnote>
<page confidence="0.999794">
43
</page>
<bodyText confidence="0.999986388888889">
it performs significantly better than chance and is
capable of slightly improving the combined model.
Both of these models are very different from the lex-
ical and entity-based models currently used for this
task (Soricut and Marcu, 2006), and are probably
capable of improving the state of the art.
As mentioned, Barzilay and Lapata (2005) uses a
coreference system to attempt to improve the entity
grid, but with mixed results. Their method of com-
bination is quite different from ours; they use the
system’s judgements to define the “entities” whose
repetitions the system measures7. In contrast, we do
not attempt to use any proposed coreference links;
as Barzilay and Lapata (2005) point out, these links
are often erroneous because the disorded input text
is so dissimilar to the training data. Instead we ex-
ploit our models’ ability to measure the probability
of various aspects of the text.
</bodyText>
<sectionHeader confidence="0.998649" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.7947">
Chen and Barzilay, reviewers, DARPA, et al.
</bodyText>
<sectionHeader confidence="0.998327" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.984653531645569">
Regina Barzilay and Mirella Lapata. 2005. Modeling
local coherence: an entity-based approach. In ACL
2005.
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In HLT-NAACL
2004, pages 113–120.
Regina Barzilay, Noemie Elhadad, and Kathleen McKe-
own. 2002. Inferring strategies for sentence ordering
in multidocument news summarization. Journal ofAr-
tificial Intelligence Results (JAIR), 17:35–55.
David L. Bean and Ellen Riloff. 1999. Corpus-based
identification of non-anaphoric noun phrases. In
ACL’99, pages 373–380.
Erdong Chen, Benjamin Snyder, and Regina Barzilay.
2007. Incremental text structuring with online hier-
archical ranking. In Proceedings of EMNLP.
Hal Daum´e III. 2004. Notes on CG and LM-BFGS
optimization of logistic regressio n. Paper available
at http://pub.hal3.name#daume04cg-bfgs, implemen-
tation available at http://hal3.name/megam/, August.
Peter Foltz, Walter Kintsch, and Thomas Landauer.
1998. The measurement of textual coherence with
7We attempted this method for pronouns using our model,
but found it ineffective.
latent semantic analysis. Discourse Processes,
25(2&amp;3):285–307.
Kari Fraurud. 1990. Definiteness and the processing of
noun phrases in natural discourse. Journal of Seman-
tics, 7(4):395–433.
Niyu Ge, John Hale, and Eugene Charniak. 1998. A sta-
tistical approach to anaphora resolution. In Proceed-
ings of the Sixth Workshop on Very Large Corpora,
pages 161–171.
Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.
1995. Centering: A framework for modeling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2):203–225.
John A. Hawkins. 1978. Definiteness and indefinite-
ness: a study in reference and grammaticality predic-
tion. Croom Helm Ltd.
Jerry R. Hobbs. 1976. Pronoun resolution. Technical
Report 76-1, City College New York.
Mirella Lapata and Regina Barzilay. 2005. Automatic
evaluation of text coherence: Models and representa-
tions. In IJCAI, pages 1085–1090.
Mirella Lapata. 2006. Automatic evaluation of informa-
tion ordering: Kendall’s tau. Computational Linguis-
tics, 32(4):1–14.
E. Miltsakaki and K. Kukich. 2004. Evaluation of text
coherence for electronic essay scoring systems. Nat.
Lang. Eng., 10(1):25–55.
Ani Nenkova and Kathleen McKeown. 2003. Refer-
ences to named entities: a corpus study. In NAACL
’03, pages 70–72.
Vincent Ng and Claire Cardie. 2002. Identifying
anaphoric and non-anaphoric noun phrases to improve
coreference resolution. In COLING.
Massimo Poesio and Renata Vieira. 1998. A corpus-
based investigation of definite description use. Com-
putational Linguistics, 24(2):183–216.
Massimo Poesio, Mijail Alexandrov-Kabadjov, Renata
Vieira, Rodrigo Goulart, and Olga Uryupina. 2005.
Does discourse-new detection help definite description
resolution? In Proceedings of the Sixth International
Workshop on Computational Semantics, Tillburg.
Ellen Prince. 1981. Toward a taxonomy of given-new in-
formation. In Peter Cole, editor, Radical Pragmatics,
pages 223–255. Academic Press, New York.
Radu Soricut and Daniel Marcu. 2006. Discourse gener-
ation using utility-trained coherence models. In ACL-
2006.
Olga Uryupina. 2003. High-precision identification of
discourse new and unique noun phrases. In Proceed-
ings of the ACL Student Workshop, Sapporo.
Renata Vieira and Massimo Poesio. 2000. An
empirically-based system for processing definite de-
scriptions. Computational Linguistics, 26(4):539–
593.
</reference>
<page confidence="0.999271">
44
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.976915">
<title confidence="0.997317">Coreference-inspired Coherence Modeling</title>
<author confidence="0.998372">Micha Elsner</author>
<author confidence="0.998372">Eugene Charniak</author>
<affiliation confidence="0.9976045">Brown Laboratory for Linguistic Information Processing (BLLIP) Brown University</affiliation>
<address confidence="0.999335">Providence, RI 02912</address>
<abstract confidence="0.9989698125">Research on coreference resolution and summarization has modeled the way entities are realized as concrete phrases in discourse. In particular there exist models of the noun phrase syntax used for discourse-new versus discourse-old referents, and models describing the likely distance between a pronoun and its antecedent. However, models of discourse coherence, as applied to information ordering tasks, have ignored these kinds of information. We apply a discourse-new classifier and pronoun coreference algorithm to the information ordering task, and show significant improvements in performance over the entity grid, a popular model of local coherence.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Mirella Lapata</author>
</authors>
<title>Modeling local coherence: an entity-based approach.</title>
<date>2005</date>
<booktitle>In ACL</booktitle>
<contexts>
<context position="3014" citStr="Barzilay and Lapata (2005)" startWordPosition="442" endWordPosition="445">ble, pronouns must be placed close to appropriate referents with the correct number and gender. Centering theory (Grosz et al., 1995) describes additional constraints about which entities in a discourse can be pronominalized: if there are pronouns in a segment, they must include the backwardlooking center. We use a model which probabilistically attempts to describe these preferences (Ge et al., 1998). These two models can be combined with the entity grid described by Lapata and Barzilay (2005) for significant improvement. The magnitude of the improvement is particularly interesting given that Barzilay and Lapata (2005) do use a coreference system but are unable to derive much advantage from it. 2 Discourse-new Model In the task of discourse-new classification, the model is given a referring expression (as in previous work, we consider only NPs) from a document and must Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 41–44, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics determine whether it is a first mention (discoursenew) or a subsequent mention (discourse-old). Features such as full names, appositives, and restrictive relative clauses are associated wit</context>
<context position="4701" citStr="Barzilay and Lapata, 2005" startWordPosition="714" endWordPosition="717">ntactic and lexical features derived from Uryupina (2003), and a publicly available learning tool (Daum´e III, 2004). Our system scores 87.4% (F-score of the disc-new class on the MUC-7 formal test set); this is comparable to the state-of-theart system of Uryupina (2003), which scores 86.91. To model coreference with this system, we assign each NP in a document a label Lnp E {new, old}. Since the correct labeling depends on the coreference relationships between the NPs, we need some way to guess at this; we take all NPs with the same head to be coreferent, as in the noncoreference version of (Barzilay and Lapata, 2005)2. We then take the probability of a document as Hnp:NPs P(Lnp|np). We must make several small changes to the model to adapt it to this setting. For the discourse-new classification task, the model’s most important feature is whether the head word of the NP to be classified has occurred previously (as in Ng and Cardie (2002) and Vieira and Poesio (2000)). For coherence modeling, we must remove this feature, since it depends on document order, which is precisely what we are trying to predict. The coreference heuristic will also fail to resolve any pronouns, so we discard them. Another issue is </context>
<context position="9444" citStr="Barzilay and Lapata, 2005" startWordPosition="1481" endWordPosition="1484">ntity by tracking the syntactic roles in which it appears throughout the document. The internal syntax of the various referring expressions is ignored. Since it also uses the “same head” coreference heuristic, it also disregards pronouns. Since the three models use very different feature sets, we combine them by assuming independence and multiplying the probabilities. 5 Experiments We evaluate our models using two tasks, both based on the assumption that a human-authored document is coherent, and uses the best possible ordering of its sentences (see Lapata (2006)). In the discrimination task (Barzilay and Lapata, 2005), a document is compared with a random permutation of its sentences, and we score the system correct if it indicates the original as more coherent4. 4Since the model might refuse to make a decision by scoring a permutation the same as the original, we also report F-score, where precision is correct/decisions and recall is correct/total. Discrimination becomes easier for longer documents, since a random permutation is likely to be much less similar to the original. Therefore we also test our systems on the task of insertion (Chen et al., 2007), in which we remove a sentence from a document, the</context>
<context position="12473" citStr="Barzilay and Lapata (2005)" startWordPosition="1972" endWordPosition="1975">ingly acceptable referent nearby. Despite this issue 5Although we designed a metric that distinguishes near misses from random performance, it is very well correlated with exact precision, so, for simplicity’s sake, we omit it. 6Average 22 sentences, as opposed to 11.5 for AIRPLANE. 43 it performs significantly better than chance and is capable of slightly improving the combined model. Both of these models are very different from the lexical and entity-based models currently used for this task (Soricut and Marcu, 2006), and are probably capable of improving the state of the art. As mentioned, Barzilay and Lapata (2005) uses a coreference system to attempt to improve the entity grid, but with mixed results. Their method of combination is quite different from ours; they use the system’s judgements to define the “entities” whose repetitions the system measures7. In contrast, we do not attempt to use any proposed coreference links; as Barzilay and Lapata (2005) point out, these links are often erroneous because the disorded input text is so dissimilar to the training data. Instead we exploit our models’ ability to measure the probability of various aspects of the text. Acknowledgements Chen and Barzilay, review</context>
</contexts>
<marker>Barzilay, Lapata, 2005</marker>
<rawString>Regina Barzilay and Mirella Lapata. 2005. Modeling local coherence: an entity-based approach. In ACL 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Catching the drift: Probabilistic content models, with applications to generation and summarization. In HLT-NAACL</title>
<date>2004</date>
<pages>113--120</pages>
<contexts>
<context position="10483" citStr="Barzilay and Lee, 2004" startWordPosition="1654" endWordPosition="1657">n is likely to be much less similar to the original. Therefore we also test our systems on the task of insertion (Chen et al., 2007), in which we remove a sentence from a document, then find the point of insertion which yields the highest coherence score. The reported score is the average fraction of sentences per document reinserted in their original position (averaged over documents, not sentences, so that longer documents do not disproportionally influence the results)5. We test on sections 14-24 of the Penn Treebank (1004 documents total). Previous work has focused on the AIRPLANE corpus (Barzilay and Lee, 2004), which contains short announcements of airplane crashes written by and for domain experts. These texts use a very constrained style, with few discourse-new markers or pronouns, and so our system is ineffective; the WSJ corpus is much more typical of normal informative writing. Also unlike previous work, we do not test the task of completely reconstructing a document’s order, since this is computationally intractable and results on WSJ documents6 would likely be dominated by search errors. Our results are shown in table 5. When run alone, the entity grid outperforms either of our models. Howev</context>
</contexts>
<marker>Barzilay, Lee, 2004</marker>
<rawString>Regina Barzilay and Lillian Lee. 2004. Catching the drift: Probabilistic content models, with applications to generation and summarization. In HLT-NAACL 2004, pages 113–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Noemie Elhadad</author>
<author>Kathleen McKeown</author>
</authors>
<title>Inferring strategies for sentence ordering in multidocument news summarization.</title>
<date>2002</date>
<journal>Journal ofArtificial Intelligence Results (JAIR),</journal>
<pages>17--35</pages>
<contexts>
<context position="1163" citStr="Barzilay et al., 2002" startWordPosition="159" endWordPosition="162">models of discourse coherence, as applied to information ordering tasks, have ignored these kinds of information. We apply a discourse-new classifier and pronoun coreference algorithm to the information ordering task, and show significant improvements in performance over the entity grid, a popular model of local coherence. 1 Introduction Models of discourse coherence describe the relationships between nearby sentences, in which previous sentences help make their successors easier to understand. Models of coherence have been used to impose an order on sentences for multidocument summarization (Barzilay et al., 2002), to evaluate the quality of human-authored essays (Miltsakaki and Kukich, 2004), and to insert new information into existing documents (Chen et al., 2007). These models typically view a sentence either as a bag of words (Foltz et al., 1998) or as a bag of entities associated with various syntactic roles (Lapata and Barzilay, 2005). However, a mention of an entity contains more information than just its head and syntactic role. The referring expression itself contains discourse-motivated information distinguishing familiar entities from unfamiliar and salient from 41 non-salient. These pattern</context>
</contexts>
<marker>Barzilay, Elhadad, McKeown, 2002</marker>
<rawString>Regina Barzilay, Noemie Elhadad, and Kathleen McKeown. 2002. Inferring strategies for sentence ordering in multidocument news summarization. Journal ofArtificial Intelligence Results (JAIR), 17:35–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David L Bean</author>
<author>Ellen Riloff</author>
</authors>
<title>Corpus-based identification of non-anaphoric noun phrases.</title>
<date>1999</date>
<booktitle>In ACL’99,</booktitle>
<pages>373--380</pages>
<contexts>
<context position="7751" citStr="Bean and Riloff (1999)" startWordPosition="1211" endWordPosition="1214">−1 i ) =P(ai|h(ai), m(ai)) Pgen(ai, ri)Pnum(ai, ri) Here h(a) is the Hobbs distance (Hobbs, 1976), which measures distance between a pronoun and prospective antecedent, taking into account various factors, such as syntactic constraints on pronouns. m(a) is the number of times the antecedent has been mentioned previously in the document (again using “same head” coreference for full NPs, but also counting the previous antecedents ai−1 i ). Pgen and Pnum are distributions over gender and number given words. The model is trained using a small hand-annotated corpus first used in Ge et al. (1998). 3Bean and Riloff (1999) and Uryupina (2003) construct quite accurate classifiers to detect unique NPs. However, some preliminary experiments convinced us that our heuristic method worked well enough for the purpose. 42 Disc. Acc Disc. F Ins. Random 50.00 50.00 12.58 Entity Grid 76.17 77.55 19.57 Disc-New 70.35 73.47 16.27 Pronoun 55.77 62.27 13.95 EGrid+Disc-New 78.88 80.31 21.93 Combined 79.60 81.02 22.98 Table 1: Results on 1004 WSJ documents. Finding the probability of a document using this model requires us to sum out the antecedents a. Unfortunately, because each ai is conditioned on the previous ones, this can</context>
</contexts>
<marker>Bean, Riloff, 1999</marker>
<rawString>David L. Bean and Ellen Riloff. 1999. Corpus-based identification of non-anaphoric noun phrases. In ACL’99, pages 373–380.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erdong Chen</author>
<author>Benjamin Snyder</author>
<author>Regina Barzilay</author>
</authors>
<title>Incremental text structuring with online hierarchical ranking.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1318" citStr="Chen et al., 2007" startWordPosition="182" endWordPosition="185">un coreference algorithm to the information ordering task, and show significant improvements in performance over the entity grid, a popular model of local coherence. 1 Introduction Models of discourse coherence describe the relationships between nearby sentences, in which previous sentences help make their successors easier to understand. Models of coherence have been used to impose an order on sentences for multidocument summarization (Barzilay et al., 2002), to evaluate the quality of human-authored essays (Miltsakaki and Kukich, 2004), and to insert new information into existing documents (Chen et al., 2007). These models typically view a sentence either as a bag of words (Foltz et al., 1998) or as a bag of entities associated with various syntactic roles (Lapata and Barzilay, 2005). However, a mention of an entity contains more information than just its head and syntactic role. The referring expression itself contains discourse-motivated information distinguishing familiar entities from unfamiliar and salient from 41 non-salient. These patterns have been studied extensively, by linguists (Prince, 1981; Fraurud, 1990) and in the field of coreference resolution. We draw on the coreference work, ta</context>
<context position="9992" citStr="Chen et al., 2007" startWordPosition="1574" endWordPosition="1577">Lapata (2006)). In the discrimination task (Barzilay and Lapata, 2005), a document is compared with a random permutation of its sentences, and we score the system correct if it indicates the original as more coherent4. 4Since the model might refuse to make a decision by scoring a permutation the same as the original, we also report F-score, where precision is correct/decisions and recall is correct/total. Discrimination becomes easier for longer documents, since a random permutation is likely to be much less similar to the original. Therefore we also test our systems on the task of insertion (Chen et al., 2007), in which we remove a sentence from a document, then find the point of insertion which yields the highest coherence score. The reported score is the average fraction of sentences per document reinserted in their original position (averaged over documents, not sentences, so that longer documents do not disproportionally influence the results)5. We test on sections 14-24 of the Penn Treebank (1004 documents total). Previous work has focused on the AIRPLANE corpus (Barzilay and Lee, 2004), which contains short announcements of airplane crashes written by and for domain experts. These texts use a</context>
</contexts>
<marker>Chen, Snyder, Barzilay, 2007</marker>
<rawString>Erdong Chen, Benjamin Snyder, and Regina Barzilay. 2007. Incremental text structuring with online hierarchical ranking. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
</authors>
<title>Notes on CG and LM-BFGS optimization of logistic regressio n. Paper available at http://pub.hal3.name#daume04cg-bfgs, implementation available at http://hal3.name/megam/,</title>
<date>2004</date>
<marker>Daum´e, 2004</marker>
<rawString>Hal Daum´e III. 2004. Notes on CG and LM-BFGS optimization of logistic regressio n. Paper available at http://pub.hal3.name#daume04cg-bfgs, implementation available at http://hal3.name/megam/, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Foltz</author>
<author>Walter Kintsch</author>
<author>Thomas Landauer</author>
</authors>
<title>The measurement of textual coherence with 7We attempted this method for pronouns using our model, but found it ineffective. latent semantic analysis.</title>
<date>1998</date>
<booktitle>Discourse Processes,</booktitle>
<pages>25--2</pages>
<contexts>
<context position="1404" citStr="Foltz et al., 1998" startWordPosition="198" endWordPosition="201">vements in performance over the entity grid, a popular model of local coherence. 1 Introduction Models of discourse coherence describe the relationships between nearby sentences, in which previous sentences help make their successors easier to understand. Models of coherence have been used to impose an order on sentences for multidocument summarization (Barzilay et al., 2002), to evaluate the quality of human-authored essays (Miltsakaki and Kukich, 2004), and to insert new information into existing documents (Chen et al., 2007). These models typically view a sentence either as a bag of words (Foltz et al., 1998) or as a bag of entities associated with various syntactic roles (Lapata and Barzilay, 2005). However, a mention of an entity contains more information than just its head and syntactic role. The referring expression itself contains discourse-motivated information distinguishing familiar entities from unfamiliar and salient from 41 non-salient. These patterns have been studied extensively, by linguists (Prince, 1981; Fraurud, 1990) and in the field of coreference resolution. We draw on the coreference work, taking two standard models from the literature and applying them to coherence modeling. </context>
</contexts>
<marker>Foltz, Kintsch, Landauer, 1998</marker>
<rawString>Peter Foltz, Walter Kintsch, and Thomas Landauer. 1998. The measurement of textual coherence with 7We attempted this method for pronouns using our model, but found it ineffective. latent semantic analysis. Discourse Processes, 25(2&amp;3):285–307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kari Fraurud</author>
</authors>
<title>Definiteness and the processing of noun phrases in natural discourse.</title>
<date>1990</date>
<journal>Journal of Semantics,</journal>
<volume>7</volume>
<issue>4</issue>
<contexts>
<context position="1838" citStr="Fraurud, 1990" startWordPosition="265" endWordPosition="266">i and Kukich, 2004), and to insert new information into existing documents (Chen et al., 2007). These models typically view a sentence either as a bag of words (Foltz et al., 1998) or as a bag of entities associated with various syntactic roles (Lapata and Barzilay, 2005). However, a mention of an entity contains more information than just its head and syntactic role. The referring expression itself contains discourse-motivated information distinguishing familiar entities from unfamiliar and salient from 41 non-salient. These patterns have been studied extensively, by linguists (Prince, 1981; Fraurud, 1990) and in the field of coreference resolution. We draw on the coreference work, taking two standard models from the literature and applying them to coherence modeling. Our first model distinguishes discourse-new from discourse-old noun phrases, using features based on Uryupina (2003). Discourse-new NPs are those whose referents have not been previously mentioned in the discourse. As noted by studies since Hawkins (1978), there are marked syntactic differences between the two classes. Our second model describes pronoun coreference. To be intelligible, pronouns must be placed close to appropriate </context>
<context position="3700" citStr="Fraurud, 1990" startWordPosition="547" endWordPosition="548"> it. 2 Discourse-new Model In the task of discourse-new classification, the model is given a referring expression (as in previous work, we consider only NPs) from a document and must Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 41–44, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics determine whether it is a first mention (discoursenew) or a subsequent mention (discourse-old). Features such as full names, appositives, and restrictive relative clauses are associated with the introduction of unfamiliar entities into discourse (Hawkins, 1978; Fraurud, 1990; Vieira and Poesio, 2000). Classifiers in the literature include (Poesio et al., 2005; Uryupina, 2003; Ng and Cardie, 2002). The system of Nenkova and McKeown (2003) works in the opposite direction. It is designed to rewrite the references in multi-document summaries, so that they conform to the common discourse patterns. We construct a maximum-entropy classifier using syntactic and lexical features derived from Uryupina (2003), and a publicly available learning tool (Daum´e III, 2004). Our system scores 87.4% (F-score of the disc-new class on the MUC-7 formal test set); this is comparable to</context>
<context position="5442" citStr="Fraurud, 1990" startWordPosition="840" endWordPosition="841"> to this setting. For the discourse-new classification task, the model’s most important feature is whether the head word of the NP to be classified has occurred previously (as in Ng and Cardie (2002) and Vieira and Poesio (2000)). For coherence modeling, we must remove this feature, since it depends on document order, which is precisely what we are trying to predict. The coreference heuristic will also fail to resolve any pronouns, so we discard them. Another issue is that NPs whose referents are familiar tend to resemble discourse-old NPs, even though they have not been previously mentioned (Fraurud, 1990). These include unique objects like the FBI or generic ones like danger or percent. To 1Poesio et al. (2005) score 90.2%, but on a different corpus. 2Unfortunately, this represents a substantial sacrifice; as Poesio and Vieira (1998) show, only about 2/3 of definite descriptions which are anaphoric have the same head as their antecedent. avoid using these deceptive phrases as examples of discourse-newness, we attempt to heuristically remove them from the training set by discarding any NP whose head occurs only once in the document3. The labels we apply to NPs in our test data are systematicall</context>
</contexts>
<marker>Fraurud, 1990</marker>
<rawString>Kari Fraurud. 1990. Definiteness and the processing of noun phrases in natural discourse. Journal of Semantics, 7(4):395–433.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Niyu Ge</author>
<author>John Hale</author>
<author>Eugene Charniak</author>
</authors>
<title>A statistical approach to anaphora resolution.</title>
<date>1998</date>
<booktitle>In Proceedings of the Sixth Workshop on Very Large Corpora,</booktitle>
<pages>161--171</pages>
<contexts>
<context position="2791" citStr="Ge et al., 1998" startWordPosition="408" endWordPosition="411">been previously mentioned in the discourse. As noted by studies since Hawkins (1978), there are marked syntactic differences between the two classes. Our second model describes pronoun coreference. To be intelligible, pronouns must be placed close to appropriate referents with the correct number and gender. Centering theory (Grosz et al., 1995) describes additional constraints about which entities in a discourse can be pronominalized: if there are pronouns in a segment, they must include the backwardlooking center. We use a model which probabilistically attempts to describe these preferences (Ge et al., 1998). These two models can be combined with the entity grid described by Lapata and Barzilay (2005) for significant improvement. The magnitude of the improvement is particularly interesting given that Barzilay and Lapata (2005) do use a coreference system but are unable to derive much advantage from it. 2 Discourse-new Model In the task of discourse-new classification, the model is given a referring expression (as in previous work, we consider only NPs) from a document and must Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 41–44, Columbus, Ohio, USA, June 2008. c�2008 Associat</context>
<context position="7079" citStr="Ge et al. (1998)" startWordPosition="1106" endWordPosition="1109"> we can use a much larger training corpus, since we no longer require gold-standard coreference annotations. 3 Pronoun Coreference Model Pronoun coreference is another important aspect of coherence– if a pronoun is used too far away from any natural referent, it becomes hard to interpret, creating confusion. Too many referents, however, create ambiguity. To describe this type of restriction, we must model the probability of the text containing pronouns (denoted ri), jointly with their referents ai. (This takes more work than simply resolving the pronouns conditioned on the text.) The model of Ge et al. (1998) provides the requisite probabilities: P(ai, ri|ai−1 i ) =P(ai|h(ai), m(ai)) Pgen(ai, ri)Pnum(ai, ri) Here h(a) is the Hobbs distance (Hobbs, 1976), which measures distance between a pronoun and prospective antecedent, taking into account various factors, such as syntactic constraints on pronouns. m(a) is the number of times the antecedent has been mentioned previously in the document (again using “same head” coreference for full NPs, but also counting the previous antecedents ai−1 i ). Pgen and Pnum are distributions over gender and number given words. The model is trained using a small hand-</context>
</contexts>
<marker>Ge, Hale, Charniak, 1998</marker>
<rawString>Niyu Ge, John Hale, and Eugene Charniak. 1998. A statistical approach to anaphora resolution. In Proceedings of the Sixth Workshop on Very Large Corpora, pages 161–171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Aravind K Joshi</author>
<author>Scott Weinstein</author>
</authors>
<title>Centering: A framework for modeling the local coherence of discourse.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="2521" citStr="Grosz et al., 1995" startWordPosition="364" endWordPosition="367">ference work, taking two standard models from the literature and applying them to coherence modeling. Our first model distinguishes discourse-new from discourse-old noun phrases, using features based on Uryupina (2003). Discourse-new NPs are those whose referents have not been previously mentioned in the discourse. As noted by studies since Hawkins (1978), there are marked syntactic differences between the two classes. Our second model describes pronoun coreference. To be intelligible, pronouns must be placed close to appropriate referents with the correct number and gender. Centering theory (Grosz et al., 1995) describes additional constraints about which entities in a discourse can be pronominalized: if there are pronouns in a segment, they must include the backwardlooking center. We use a model which probabilistically attempts to describe these preferences (Ge et al., 1998). These two models can be combined with the entity grid described by Lapata and Barzilay (2005) for significant improvement. The magnitude of the improvement is particularly interesting given that Barzilay and Lapata (2005) do use a coreference system but are unable to derive much advantage from it. 2 Discourse-new Model In the </context>
</contexts>
<marker>Grosz, Joshi, Weinstein, 1995</marker>
<rawString>Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein. 1995. Centering: A framework for modeling the local coherence of discourse. Computational Linguistics, 21(2):203–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John A Hawkins</author>
</authors>
<title>Definiteness and indefiniteness: a study in reference and grammaticality prediction. Croom Helm Ltd.</title>
<date>1978</date>
<contexts>
<context position="2259" citStr="Hawkins (1978)" startWordPosition="327" endWordPosition="328">-motivated information distinguishing familiar entities from unfamiliar and salient from 41 non-salient. These patterns have been studied extensively, by linguists (Prince, 1981; Fraurud, 1990) and in the field of coreference resolution. We draw on the coreference work, taking two standard models from the literature and applying them to coherence modeling. Our first model distinguishes discourse-new from discourse-old noun phrases, using features based on Uryupina (2003). Discourse-new NPs are those whose referents have not been previously mentioned in the discourse. As noted by studies since Hawkins (1978), there are marked syntactic differences between the two classes. Our second model describes pronoun coreference. To be intelligible, pronouns must be placed close to appropriate referents with the correct number and gender. Centering theory (Grosz et al., 1995) describes additional constraints about which entities in a discourse can be pronominalized: if there are pronouns in a segment, they must include the backwardlooking center. We use a model which probabilistically attempts to describe these preferences (Ge et al., 1998). These two models can be combined with the entity grid described by</context>
<context position="3685" citStr="Hawkins, 1978" startWordPosition="545" endWordPosition="546"> advantage from it. 2 Discourse-new Model In the task of discourse-new classification, the model is given a referring expression (as in previous work, we consider only NPs) from a document and must Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 41–44, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics determine whether it is a first mention (discoursenew) or a subsequent mention (discourse-old). Features such as full names, appositives, and restrictive relative clauses are associated with the introduction of unfamiliar entities into discourse (Hawkins, 1978; Fraurud, 1990; Vieira and Poesio, 2000). Classifiers in the literature include (Poesio et al., 2005; Uryupina, 2003; Ng and Cardie, 2002). The system of Nenkova and McKeown (2003) works in the opposite direction. It is designed to rewrite the references in multi-document summaries, so that they conform to the common discourse patterns. We construct a maximum-entropy classifier using syntactic and lexical features derived from Uryupina (2003), and a publicly available learning tool (Daum´e III, 2004). Our system scores 87.4% (F-score of the disc-new class on the MUC-7 formal test set); this i</context>
</contexts>
<marker>Hawkins, 1978</marker>
<rawString>John A. Hawkins. 1978. Definiteness and indefiniteness: a study in reference and grammaticality prediction. Croom Helm Ltd.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry R Hobbs</author>
</authors>
<title>Pronoun resolution.</title>
<date>1976</date>
<tech>Technical Report 76-1,</tech>
<institution>City College</institution>
<location>New York.</location>
<contexts>
<context position="7226" citStr="Hobbs, 1976" startWordPosition="1129" endWordPosition="1130">rence is another important aspect of coherence– if a pronoun is used too far away from any natural referent, it becomes hard to interpret, creating confusion. Too many referents, however, create ambiguity. To describe this type of restriction, we must model the probability of the text containing pronouns (denoted ri), jointly with their referents ai. (This takes more work than simply resolving the pronouns conditioned on the text.) The model of Ge et al. (1998) provides the requisite probabilities: P(ai, ri|ai−1 i ) =P(ai|h(ai), m(ai)) Pgen(ai, ri)Pnum(ai, ri) Here h(a) is the Hobbs distance (Hobbs, 1976), which measures distance between a pronoun and prospective antecedent, taking into account various factors, such as syntactic constraints on pronouns. m(a) is the number of times the antecedent has been mentioned previously in the document (again using “same head” coreference for full NPs, but also counting the previous antecedents ai−1 i ). Pgen and Pnum are distributions over gender and number given words. The model is trained using a small hand-annotated corpus first used in Ge et al. (1998). 3Bean and Riloff (1999) and Uryupina (2003) construct quite accurate classifiers to detect unique </context>
</contexts>
<marker>Hobbs, 1976</marker>
<rawString>Jerry R. Hobbs. 1976. Pronoun resolution. Technical Report 76-1, City College New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
<author>Regina Barzilay</author>
</authors>
<title>Automatic evaluation of text coherence: Models and representations.</title>
<date>2005</date>
<booktitle>In IJCAI,</booktitle>
<pages>1085--1090</pages>
<contexts>
<context position="1496" citStr="Lapata and Barzilay, 2005" startWordPosition="214" endWordPosition="217">troduction Models of discourse coherence describe the relationships between nearby sentences, in which previous sentences help make their successors easier to understand. Models of coherence have been used to impose an order on sentences for multidocument summarization (Barzilay et al., 2002), to evaluate the quality of human-authored essays (Miltsakaki and Kukich, 2004), and to insert new information into existing documents (Chen et al., 2007). These models typically view a sentence either as a bag of words (Foltz et al., 1998) or as a bag of entities associated with various syntactic roles (Lapata and Barzilay, 2005). However, a mention of an entity contains more information than just its head and syntactic role. The referring expression itself contains discourse-motivated information distinguishing familiar entities from unfamiliar and salient from 41 non-salient. These patterns have been studied extensively, by linguists (Prince, 1981; Fraurud, 1990) and in the field of coreference resolution. We draw on the coreference work, taking two standard models from the literature and applying them to coherence modeling. Our first model distinguishes discourse-new from discourse-old noun phrases, using features </context>
<context position="2886" citStr="Lapata and Barzilay (2005)" startWordPosition="425" endWordPosition="428"> there are marked syntactic differences between the two classes. Our second model describes pronoun coreference. To be intelligible, pronouns must be placed close to appropriate referents with the correct number and gender. Centering theory (Grosz et al., 1995) describes additional constraints about which entities in a discourse can be pronominalized: if there are pronouns in a segment, they must include the backwardlooking center. We use a model which probabilistically attempts to describe these preferences (Ge et al., 1998). These two models can be combined with the entity grid described by Lapata and Barzilay (2005) for significant improvement. The magnitude of the improvement is particularly interesting given that Barzilay and Lapata (2005) do use a coreference system but are unable to derive much advantage from it. 2 Discourse-new Model In the task of discourse-new classification, the model is given a referring expression (as in previous work, we consider only NPs) from a document and must Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 41–44, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics determine whether it is a first mention (discoursenew) or a s</context>
<context position="8615" citStr="Lapata and Barzilay, 2005" startWordPosition="1350" endWordPosition="1353">12.58 Entity Grid 76.17 77.55 19.57 Disc-New 70.35 73.47 16.27 Pronoun 55.77 62.27 13.95 EGrid+Disc-New 78.88 80.31 21.93 Combined 79.60 81.02 22.98 Table 1: Results on 1004 WSJ documents. Finding the probability of a document using this model requires us to sum out the antecedents a. Unfortunately, because each ai is conditioned on the previous ones, this cannot be done efficiently. Instead, we use a greedy search, assigning each pronoun left to right. Finally we report the probability of the resulting sequence of pronoun assignments. 4 Baseline Model As a baseline, we adopt the entity grid (Lapata and Barzilay, 2005). This model outperforms a variety of word overlap and semantic similarity models, and is used as a component in the state-of-the-art system of Soricut and Marcu (2006). The entity grid represents each entity by tracking the syntactic roles in which it appears throughout the document. The internal syntax of the various referring expressions is ignored. Since it also uses the “same head” coreference heuristic, it also disregards pronouns. Since the three models use very different feature sets, we combine them by assuming independence and multiplying the probabilities. 5 Experiments We evaluate </context>
</contexts>
<marker>Lapata, Barzilay, 2005</marker>
<rawString>Mirella Lapata and Regina Barzilay. 2005. Automatic evaluation of text coherence: Models and representations. In IJCAI, pages 1085–1090.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
</authors>
<title>Automatic evaluation of information ordering: Kendall’s tau.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>4</issue>
<contexts>
<context position="9387" citStr="Lapata (2006)" startWordPosition="1474" endWordPosition="1475">cu (2006). The entity grid represents each entity by tracking the syntactic roles in which it appears throughout the document. The internal syntax of the various referring expressions is ignored. Since it also uses the “same head” coreference heuristic, it also disregards pronouns. Since the three models use very different feature sets, we combine them by assuming independence and multiplying the probabilities. 5 Experiments We evaluate our models using two tasks, both based on the assumption that a human-authored document is coherent, and uses the best possible ordering of its sentences (see Lapata (2006)). In the discrimination task (Barzilay and Lapata, 2005), a document is compared with a random permutation of its sentences, and we score the system correct if it indicates the original as more coherent4. 4Since the model might refuse to make a decision by scoring a permutation the same as the original, we also report F-score, where precision is correct/decisions and recall is correct/total. Discrimination becomes easier for longer documents, since a random permutation is likely to be much less similar to the original. Therefore we also test our systems on the task of insertion (Chen et al., </context>
</contexts>
<marker>Lapata, 2006</marker>
<rawString>Mirella Lapata. 2006. Automatic evaluation of information ordering: Kendall’s tau. Computational Linguistics, 32(4):1–14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Miltsakaki</author>
<author>K Kukich</author>
</authors>
<title>Evaluation of text coherence for electronic essay scoring systems.</title>
<date>2004</date>
<journal>Nat. Lang. Eng.,</journal>
<volume>10</volume>
<issue>1</issue>
<contexts>
<context position="1243" citStr="Miltsakaki and Kukich, 2004" startWordPosition="170" endWordPosition="173">ave ignored these kinds of information. We apply a discourse-new classifier and pronoun coreference algorithm to the information ordering task, and show significant improvements in performance over the entity grid, a popular model of local coherence. 1 Introduction Models of discourse coherence describe the relationships between nearby sentences, in which previous sentences help make their successors easier to understand. Models of coherence have been used to impose an order on sentences for multidocument summarization (Barzilay et al., 2002), to evaluate the quality of human-authored essays (Miltsakaki and Kukich, 2004), and to insert new information into existing documents (Chen et al., 2007). These models typically view a sentence either as a bag of words (Foltz et al., 1998) or as a bag of entities associated with various syntactic roles (Lapata and Barzilay, 2005). However, a mention of an entity contains more information than just its head and syntactic role. The referring expression itself contains discourse-motivated information distinguishing familiar entities from unfamiliar and salient from 41 non-salient. These patterns have been studied extensively, by linguists (Prince, 1981; Fraurud, 1990) and </context>
</contexts>
<marker>Miltsakaki, Kukich, 2004</marker>
<rawString>E. Miltsakaki and K. Kukich. 2004. Evaluation of text coherence for electronic essay scoring systems. Nat. Lang. Eng., 10(1):25–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Kathleen McKeown</author>
</authors>
<title>References to named entities: a corpus study.</title>
<date>2003</date>
<booktitle>In NAACL ’03,</booktitle>
<pages>70--72</pages>
<contexts>
<context position="3866" citStr="Nenkova and McKeown (2003)" startWordPosition="573" endWordPosition="576">NPs) from a document and must Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 41–44, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics determine whether it is a first mention (discoursenew) or a subsequent mention (discourse-old). Features such as full names, appositives, and restrictive relative clauses are associated with the introduction of unfamiliar entities into discourse (Hawkins, 1978; Fraurud, 1990; Vieira and Poesio, 2000). Classifiers in the literature include (Poesio et al., 2005; Uryupina, 2003; Ng and Cardie, 2002). The system of Nenkova and McKeown (2003) works in the opposite direction. It is designed to rewrite the references in multi-document summaries, so that they conform to the common discourse patterns. We construct a maximum-entropy classifier using syntactic and lexical features derived from Uryupina (2003), and a publicly available learning tool (Daum´e III, 2004). Our system scores 87.4% (F-score of the disc-new class on the MUC-7 formal test set); this is comparable to the state-of-theart system of Uryupina (2003), which scores 86.91. To model coreference with this system, we assign each NP in a document a label Lnp E {new, old}. S</context>
</contexts>
<marker>Nenkova, McKeown, 2003</marker>
<rawString>Ani Nenkova and Kathleen McKeown. 2003. References to named entities: a corpus study. In NAACL ’03, pages 70–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
<author>Claire Cardie</author>
</authors>
<title>Identifying anaphoric and non-anaphoric noun phrases to improve coreference resolution.</title>
<date>2002</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="3824" citStr="Ng and Cardie, 2002" startWordPosition="565" endWordPosition="568"> in previous work, we consider only NPs) from a document and must Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 41–44, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics determine whether it is a first mention (discoursenew) or a subsequent mention (discourse-old). Features such as full names, appositives, and restrictive relative clauses are associated with the introduction of unfamiliar entities into discourse (Hawkins, 1978; Fraurud, 1990; Vieira and Poesio, 2000). Classifiers in the literature include (Poesio et al., 2005; Uryupina, 2003; Ng and Cardie, 2002). The system of Nenkova and McKeown (2003) works in the opposite direction. It is designed to rewrite the references in multi-document summaries, so that they conform to the common discourse patterns. We construct a maximum-entropy classifier using syntactic and lexical features derived from Uryupina (2003), and a publicly available learning tool (Daum´e III, 2004). Our system scores 87.4% (F-score of the disc-new class on the MUC-7 formal test set); this is comparable to the state-of-theart system of Uryupina (2003), which scores 86.91. To model coreference with this system, we assign each NP</context>
</contexts>
<marker>Ng, Cardie, 2002</marker>
<rawString>Vincent Ng and Claire Cardie. 2002. Identifying anaphoric and non-anaphoric noun phrases to improve coreference resolution. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimo Poesio</author>
<author>Renata Vieira</author>
</authors>
<title>A corpusbased investigation of definite description use.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>2</issue>
<contexts>
<context position="5675" citStr="Poesio and Vieira (1998)" startWordPosition="875" endWordPosition="878">2000)). For coherence modeling, we must remove this feature, since it depends on document order, which is precisely what we are trying to predict. The coreference heuristic will also fail to resolve any pronouns, so we discard them. Another issue is that NPs whose referents are familiar tend to resemble discourse-old NPs, even though they have not been previously mentioned (Fraurud, 1990). These include unique objects like the FBI or generic ones like danger or percent. To 1Poesio et al. (2005) score 90.2%, but on a different corpus. 2Unfortunately, this represents a substantial sacrifice; as Poesio and Vieira (1998) show, only about 2/3 of definite descriptions which are anaphoric have the same head as their antecedent. avoid using these deceptive phrases as examples of discourse-newness, we attempt to heuristically remove them from the training set by discarding any NP whose head occurs only once in the document3. The labels we apply to NPs in our test data are systematically biased by the “same head” heuristic we use for coreference. This is a disadvantage for our system, but it has a corresponding advantage– we can use training data labeled using the same heuristic, without any loss in performance on </context>
</contexts>
<marker>Poesio, Vieira, 1998</marker>
<rawString>Massimo Poesio and Renata Vieira. 1998. A corpusbased investigation of definite description use. Computational Linguistics, 24(2):183–216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimo Poesio</author>
<author>Mijail Alexandrov-Kabadjov</author>
<author>Renata Vieira</author>
<author>Rodrigo Goulart</author>
<author>Olga Uryupina</author>
</authors>
<title>Does discourse-new detection help definite description resolution?</title>
<date>2005</date>
<booktitle>In Proceedings of the Sixth International Workshop on Computational Semantics, Tillburg.</booktitle>
<contexts>
<context position="3786" citStr="Poesio et al., 2005" startWordPosition="559" endWordPosition="562">l is given a referring expression (as in previous work, we consider only NPs) from a document and must Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 41–44, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics determine whether it is a first mention (discoursenew) or a subsequent mention (discourse-old). Features such as full names, appositives, and restrictive relative clauses are associated with the introduction of unfamiliar entities into discourse (Hawkins, 1978; Fraurud, 1990; Vieira and Poesio, 2000). Classifiers in the literature include (Poesio et al., 2005; Uryupina, 2003; Ng and Cardie, 2002). The system of Nenkova and McKeown (2003) works in the opposite direction. It is designed to rewrite the references in multi-document summaries, so that they conform to the common discourse patterns. We construct a maximum-entropy classifier using syntactic and lexical features derived from Uryupina (2003), and a publicly available learning tool (Daum´e III, 2004). Our system scores 87.4% (F-score of the disc-new class on the MUC-7 formal test set); this is comparable to the state-of-theart system of Uryupina (2003), which scores 86.91. To model coreferen</context>
<context position="5550" citStr="Poesio et al. (2005)" startWordPosition="857" endWordPosition="860">ther the head word of the NP to be classified has occurred previously (as in Ng and Cardie (2002) and Vieira and Poesio (2000)). For coherence modeling, we must remove this feature, since it depends on document order, which is precisely what we are trying to predict. The coreference heuristic will also fail to resolve any pronouns, so we discard them. Another issue is that NPs whose referents are familiar tend to resemble discourse-old NPs, even though they have not been previously mentioned (Fraurud, 1990). These include unique objects like the FBI or generic ones like danger or percent. To 1Poesio et al. (2005) score 90.2%, but on a different corpus. 2Unfortunately, this represents a substantial sacrifice; as Poesio and Vieira (1998) show, only about 2/3 of definite descriptions which are anaphoric have the same head as their antecedent. avoid using these deceptive phrases as examples of discourse-newness, we attempt to heuristically remove them from the training set by discarding any NP whose head occurs only once in the document3. The labels we apply to NPs in our test data are systematically biased by the “same head” heuristic we use for coreference. This is a disadvantage for our system, but it </context>
</contexts>
<marker>Poesio, Alexandrov-Kabadjov, Vieira, Goulart, Uryupina, 2005</marker>
<rawString>Massimo Poesio, Mijail Alexandrov-Kabadjov, Renata Vieira, Rodrigo Goulart, and Olga Uryupina. 2005. Does discourse-new detection help definite description resolution? In Proceedings of the Sixth International Workshop on Computational Semantics, Tillburg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Prince</author>
</authors>
<title>Toward a taxonomy of given-new information.</title>
<date>1981</date>
<booktitle>Radical Pragmatics,</booktitle>
<pages>223--255</pages>
<editor>In Peter Cole, editor,</editor>
<publisher>Academic Press,</publisher>
<location>New York.</location>
<contexts>
<context position="1822" citStr="Prince, 1981" startWordPosition="263" endWordPosition="264">ays (Miltsakaki and Kukich, 2004), and to insert new information into existing documents (Chen et al., 2007). These models typically view a sentence either as a bag of words (Foltz et al., 1998) or as a bag of entities associated with various syntactic roles (Lapata and Barzilay, 2005). However, a mention of an entity contains more information than just its head and syntactic role. The referring expression itself contains discourse-motivated information distinguishing familiar entities from unfamiliar and salient from 41 non-salient. These patterns have been studied extensively, by linguists (Prince, 1981; Fraurud, 1990) and in the field of coreference resolution. We draw on the coreference work, taking two standard models from the literature and applying them to coherence modeling. Our first model distinguishes discourse-new from discourse-old noun phrases, using features based on Uryupina (2003). Discourse-new NPs are those whose referents have not been previously mentioned in the discourse. As noted by studies since Hawkins (1978), there are marked syntactic differences between the two classes. Our second model describes pronoun coreference. To be intelligible, pronouns must be placed close</context>
</contexts>
<marker>Prince, 1981</marker>
<rawString>Ellen Prince. 1981. Toward a taxonomy of given-new information. In Peter Cole, editor, Radical Pragmatics, pages 223–255. Academic Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Soricut</author>
<author>Daniel Marcu</author>
</authors>
<title>Discourse generation using utility-trained coherence models.</title>
<date>2006</date>
<booktitle>In ACL2006.</booktitle>
<contexts>
<context position="8783" citStr="Soricut and Marcu (2006)" startWordPosition="1377" endWordPosition="1380">004 WSJ documents. Finding the probability of a document using this model requires us to sum out the antecedents a. Unfortunately, because each ai is conditioned on the previous ones, this cannot be done efficiently. Instead, we use a greedy search, assigning each pronoun left to right. Finally we report the probability of the resulting sequence of pronoun assignments. 4 Baseline Model As a baseline, we adopt the entity grid (Lapata and Barzilay, 2005). This model outperforms a variety of word overlap and semantic similarity models, and is used as a component in the state-of-the-art system of Soricut and Marcu (2006). The entity grid represents each entity by tracking the syntactic roles in which it appears throughout the document. The internal syntax of the various referring expressions is ignored. Since it also uses the “same head” coreference heuristic, it also disregards pronouns. Since the three models use very different feature sets, we combine them by assuming independence and multiplying the probabilities. 5 Experiments We evaluate our models using two tasks, both based on the assumption that a human-authored document is coherent, and uses the best possible ordering of its sentences (see Lapata (2</context>
<context position="12371" citStr="Soricut and Marcu, 2006" startWordPosition="1955" endWordPosition="1958">lthough a pronoun may have been displaced from its original position, it can often find another seemingly acceptable referent nearby. Despite this issue 5Although we designed a metric that distinguishes near misses from random performance, it is very well correlated with exact precision, so, for simplicity’s sake, we omit it. 6Average 22 sentences, as opposed to 11.5 for AIRPLANE. 43 it performs significantly better than chance and is capable of slightly improving the combined model. Both of these models are very different from the lexical and entity-based models currently used for this task (Soricut and Marcu, 2006), and are probably capable of improving the state of the art. As mentioned, Barzilay and Lapata (2005) uses a coreference system to attempt to improve the entity grid, but with mixed results. Their method of combination is quite different from ours; they use the system’s judgements to define the “entities” whose repetitions the system measures7. In contrast, we do not attempt to use any proposed coreference links; as Barzilay and Lapata (2005) point out, these links are often erroneous because the disorded input text is so dissimilar to the training data. Instead we exploit our models’ ability</context>
</contexts>
<marker>Soricut, Marcu, 2006</marker>
<rawString>Radu Soricut and Daniel Marcu. 2006. Discourse generation using utility-trained coherence models. In ACL2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olga Uryupina</author>
</authors>
<title>High-precision identification of discourse new and unique noun phrases.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL Student Workshop,</booktitle>
<location>Sapporo.</location>
<contexts>
<context position="2120" citStr="Uryupina (2003)" startWordPosition="306" endWordPosition="307">, a mention of an entity contains more information than just its head and syntactic role. The referring expression itself contains discourse-motivated information distinguishing familiar entities from unfamiliar and salient from 41 non-salient. These patterns have been studied extensively, by linguists (Prince, 1981; Fraurud, 1990) and in the field of coreference resolution. We draw on the coreference work, taking two standard models from the literature and applying them to coherence modeling. Our first model distinguishes discourse-new from discourse-old noun phrases, using features based on Uryupina (2003). Discourse-new NPs are those whose referents have not been previously mentioned in the discourse. As noted by studies since Hawkins (1978), there are marked syntactic differences between the two classes. Our second model describes pronoun coreference. To be intelligible, pronouns must be placed close to appropriate referents with the correct number and gender. Centering theory (Grosz et al., 1995) describes additional constraints about which entities in a discourse can be pronominalized: if there are pronouns in a segment, they must include the backwardlooking center. We use a model which pro</context>
<context position="3802" citStr="Uryupina, 2003" startWordPosition="563" endWordPosition="564">g expression (as in previous work, we consider only NPs) from a document and must Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 41–44, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics determine whether it is a first mention (discoursenew) or a subsequent mention (discourse-old). Features such as full names, appositives, and restrictive relative clauses are associated with the introduction of unfamiliar entities into discourse (Hawkins, 1978; Fraurud, 1990; Vieira and Poesio, 2000). Classifiers in the literature include (Poesio et al., 2005; Uryupina, 2003; Ng and Cardie, 2002). The system of Nenkova and McKeown (2003) works in the opposite direction. It is designed to rewrite the references in multi-document summaries, so that they conform to the common discourse patterns. We construct a maximum-entropy classifier using syntactic and lexical features derived from Uryupina (2003), and a publicly available learning tool (Daum´e III, 2004). Our system scores 87.4% (F-score of the disc-new class on the MUC-7 formal test set); this is comparable to the state-of-theart system of Uryupina (2003), which scores 86.91. To model coreference with this sys</context>
<context position="7771" citStr="Uryupina (2003)" startWordPosition="1216" endWordPosition="1217">Pgen(ai, ri)Pnum(ai, ri) Here h(a) is the Hobbs distance (Hobbs, 1976), which measures distance between a pronoun and prospective antecedent, taking into account various factors, such as syntactic constraints on pronouns. m(a) is the number of times the antecedent has been mentioned previously in the document (again using “same head” coreference for full NPs, but also counting the previous antecedents ai−1 i ). Pgen and Pnum are distributions over gender and number given words. The model is trained using a small hand-annotated corpus first used in Ge et al. (1998). 3Bean and Riloff (1999) and Uryupina (2003) construct quite accurate classifiers to detect unique NPs. However, some preliminary experiments convinced us that our heuristic method worked well enough for the purpose. 42 Disc. Acc Disc. F Ins. Random 50.00 50.00 12.58 Entity Grid 76.17 77.55 19.57 Disc-New 70.35 73.47 16.27 Pronoun 55.77 62.27 13.95 EGrid+Disc-New 78.88 80.31 21.93 Combined 79.60 81.02 22.98 Table 1: Results on 1004 WSJ documents. Finding the probability of a document using this model requires us to sum out the antecedents a. Unfortunately, because each ai is conditioned on the previous ones, this cannot be done efficien</context>
</contexts>
<marker>Uryupina, 2003</marker>
<rawString>Olga Uryupina. 2003. High-precision identification of discourse new and unique noun phrases. In Proceedings of the ACL Student Workshop, Sapporo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Renata Vieira</author>
<author>Massimo Poesio</author>
</authors>
<title>An empirically-based system for processing definite descriptions.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>4</issue>
<pages>593</pages>
<contexts>
<context position="3726" citStr="Vieira and Poesio, 2000" startWordPosition="549" endWordPosition="552">e-new Model In the task of discourse-new classification, the model is given a referring expression (as in previous work, we consider only NPs) from a document and must Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 41–44, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics determine whether it is a first mention (discoursenew) or a subsequent mention (discourse-old). Features such as full names, appositives, and restrictive relative clauses are associated with the introduction of unfamiliar entities into discourse (Hawkins, 1978; Fraurud, 1990; Vieira and Poesio, 2000). Classifiers in the literature include (Poesio et al., 2005; Uryupina, 2003; Ng and Cardie, 2002). The system of Nenkova and McKeown (2003) works in the opposite direction. It is designed to rewrite the references in multi-document summaries, so that they conform to the common discourse patterns. We construct a maximum-entropy classifier using syntactic and lexical features derived from Uryupina (2003), and a publicly available learning tool (Daum´e III, 2004). Our system scores 87.4% (F-score of the disc-new class on the MUC-7 formal test set); this is comparable to the state-of-theart syste</context>
<context position="5056" citStr="Vieira and Poesio (2000)" startWordPosition="776" endWordPosition="779">cument a label Lnp E {new, old}. Since the correct labeling depends on the coreference relationships between the NPs, we need some way to guess at this; we take all NPs with the same head to be coreferent, as in the noncoreference version of (Barzilay and Lapata, 2005)2. We then take the probability of a document as Hnp:NPs P(Lnp|np). We must make several small changes to the model to adapt it to this setting. For the discourse-new classification task, the model’s most important feature is whether the head word of the NP to be classified has occurred previously (as in Ng and Cardie (2002) and Vieira and Poesio (2000)). For coherence modeling, we must remove this feature, since it depends on document order, which is precisely what we are trying to predict. The coreference heuristic will also fail to resolve any pronouns, so we discard them. Another issue is that NPs whose referents are familiar tend to resemble discourse-old NPs, even though they have not been previously mentioned (Fraurud, 1990). These include unique objects like the FBI or generic ones like danger or percent. To 1Poesio et al. (2005) score 90.2%, but on a different corpus. 2Unfortunately, this represents a substantial sacrifice; as Poesi</context>
</contexts>
<marker>Vieira, Poesio, 2000</marker>
<rawString>Renata Vieira and Massimo Poesio. 2000. An empirically-based system for processing definite descriptions. Computational Linguistics, 26(4):539– 593.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>