<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.017580">
<title confidence="0.975277">
Reversible Stochastic Attribute-Value Grammars
</title>
<author confidence="0.997503">
Dani¨el de Kok Barbara Plank Gertjan van Noord
</author>
<affiliation confidence="0.999808">
University of Groningen University of Groningen University of Groningen
</affiliation>
<email confidence="0.997087">
d.j.a.de.kok@rug.nl b.plank@rug.nl g.j.m.van.noord@rug.nl
</email>
<sectionHeader confidence="0.995607" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999651875">
An attractive property of attribute-value gram-
mars is their reversibility. Attribute-value
grammars are usually coupled with sepa-
rate statistical components for parse selection
and fluency ranking. We propose reversible
stochastic attribute-value grammars, in which
a single statistical model is employed both for
parse selection and fluency ranking.
</bodyText>
<sectionHeader confidence="0.998786" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999801192982456">
Reversible grammars were introduced as early as
1975 by Martin Kay (1975). In the eighties, the
popularity of attribute-value grammars (AVG) was
in part motivated by their inherent reversible na-
ture. Later, AVG were enriched with a statistical
component (Abney, 1997): stochastic AVG (SAVG).
Training a SAVG is feasible if a stochastic model
is assumed which is conditioned on the input sen-
tences (Johnson et al., 1999). Various parsers based
on this approach now exist for various languages
(Toutanova et al., 2002; Riezler et al., 2002; van
Noord and Malouf, 2005; Miyao and Tsujii, 2005;
Clark and Curran, 2004; Forst, 2007). SAVG can be
applied for generation to select the most fluent real-
ization from the set of possible realizations (Velldal
et al., 2004). In this case, the stochastic model is
conditioned on the input logical forms. Such gener-
ators exist for various languages as well (Velldal and
Oepen, 2006; Nakanishi and Miyao, 2005; Cahill et
al., 2007; de Kok and van Noord, 2010).
If an AVG is applied both to parsing and gen-
eration, two distinct stochastic components are re-
quired, one for parsing, and one for generation. To
some extent this is reasonable, because some fea-
tures are only relevant in a certain direction. For
instance, features that represent aspects of the sur-
face word order are important for generation, but ir-
relevant for parsing. Similarly, features which de-
scribe aspects of the logical form are important for
parsing, but irrelevant for generation. Yet, there are
also many features that are relevant in both direc-
tions. For instance, for Dutch, a very effective fea-
ture signals a direct object NP in fronted position in
main clauses. If a main clause is parsed which starts
with a NP, the disambiguation component will fa-
vor a subject reading of that NP. In generation, the
fluency component will favor subject fronting over
object fronting. Clearly, such shared preferences are
not accidental.
In this paper we propose reversible SAVG in
which a single stochastic component is applied both
in parsing and generation. We provide experimen-
tal evidence that such reversible SAVG achieve sim-
ilar performance as their directional counterparts.
A single, reversible model is to be preferred over
two distinct models because it explains why pref-
erences in a disambiguation component and a flu-
ency component, such as the preference for subject
fronting over object fronting, are shared. A single,
reversible model is furthermore of practical inter-
est for its simplicity, compactness, and maintainabil-
ity. As an important additional advantage, reversible
models are applicable for tasks which combine as-
pects of parsing and generation, such as word-graph
parsing and paraphrasing. In situations where only a
small amount of training data is available for parsing
or generation, cross-pollination improves the perfor-
</bodyText>
<page confidence="0.983559">
194
</page>
<note confidence="0.5842265">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 194–199,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999710166666667">
mance of a model. If preferences are shared between
parsing and generation, it follows that a generator
could benefit from parsing data and vice versa. We
present experimental results indicating that in such a
bootstrap scenario a reversible model achieves better
performance.
</bodyText>
<sectionHeader confidence="0.981205" genericHeader="method">
2 Reversible SAVG
</sectionHeader>
<bodyText confidence="0.999914833333333">
As Abney (1997) shows, we cannot use relatively
simple techniques such as relative frequencies to
obtain a model for estimating derivation probabili-
ties in attribute-value grammars. As an alternative,
he proposes a maximum entropy model, where the
probability of a derivation d is defined as:
</bodyText>
<equation confidence="0.997489">
1 X
p(d) = Z exp
i
</equation>
<bodyText confidence="0.98748">
fi(d) is the frequency of feature fi in derivation
d. A weight λi is associated with each feature fi.
In (1), Z is a normalizer which is defined as fol-
lows, where Q is the set of derivations defined by
the grammar:
</bodyText>
<equation confidence="0.999122">
XZ = Xexp λifi(d0) (2)
d&apos;∈Q i
</equation>
<bodyText confidence="0.999592230769231">
Training this model requires access to all derivations
Q allowed by the grammar, which makes it hard to
implement the model in practice.
Johnson et al. (1999) alleviate this problem by
proposing a model which conditions on the input
sentence s: p(djs). Since the number of derivations
for a given sentence s is usually finite, the calcula-
tion of the normalizer is much more practical. Con-
versely, in generation the model is conditioned on
the input logical form l, p(djl) (Velldal et al., 2004).
In such directional stochastic attribute-value gram-
mars, the probability of a derivation d given an input
x (a sentence or a logical form) is defined as:
</bodyText>
<equation confidence="0.9986498">
1 X
p(djx) = Z(x)exp λifi(x, d) (3)
with Z(x) as (Q(x) are all derivations for input x):
Z(x) = X Xexp λifi(x, d0) (4)
d&apos;∈Q(x) i
</equation>
<bodyText confidence="0.9996808">
Consequently, the constraint put on feature values
during training only refers to derivations with the
same input. If X is the set of inputs (for parsing,
all sentences in the treebank; for generation, all log-
ical forms), then we have:
</bodyText>
<equation confidence="0.9999635">
Ep(fi) − Ep(fi) = 0 ~ (5)
�p(x)p(djx)fi(x,d) −�p(x,d)fi(x,d) = 0
</equation>
<bodyText confidence="0.999932666666667">
Here we assume a uniform distribution for ,5(x).
Let j(d) be a function which returns 0 if the deriva-
tion d is inconsistent with the treebank, and 1 in case
the derivation is correct. 5(x, d) is now defined in
such a way that it is 0 for incorrect derivations, and
uniform for correct derivations for a given input:
</bodyText>
<equation confidence="0.9995005">
p(x,d) = p(x) j(d) 0 (6)
Ed&apos;∈Q(x)j (d )
</equation>
<bodyText confidence="0.9998175">
Directional SAVG make parsing and generation
practically feasible, but require separate models for
parse disambiguation and fluency ranking.
Since parsing and generation both create deriva-
tions that are in agreement with the constraints im-
plied by the input, a single model can accompany
the attribute-value grammar. Such a model estimates
the probability of a derivation d given a set of con-
straints c, p(djc). We use conditional maximum en-
tropy models to estimate p(djc):
</bodyText>
<equation confidence="0.99890725">
1 X
p(djc) = Z(c)exp λifi(c, d) (7)
Z(c) = X Xexp λifi(c, d0) (8)
d&apos;∈Q(c) i
</equation>
<bodyText confidence="0.999996333333333">
We derive a reversible model by training on data
for parse disambiguation and fluency ranking simul-
taneously. In contrast to directional models, we im-
pose the two constraints per feature given in figure 1:
one on the feature value with respect to the sentences
S in the parse disambiguation treebank and the other
on the feature value with respect to logical forms L
in the fluency ranking treebank. As a result of the
constraints on training defined in figure 1, the fea-
ture weights in the reversible model distinguish, at
the same time, good parses from bad parses as well
as good realizations from bad realizations.
</bodyText>
<sectionHeader confidence="0.969644" genericHeader="method">
3 Experimental setup and evaluation
</sectionHeader>
<bodyText confidence="0.9999565">
To evaluate reversible SAVG, we conduct experi-
ments in the context of the Alpino system for Dutch.
</bodyText>
<equation confidence="0.940373636363636">
λifi(d) (1)
i
X
xEX
X
dEn(x)
i
195
s0S � p(s)p(d|c = s)fi(s, d) − p(c = s, d)fi(s, d) = 0
d0l(s)
�p(l)p(d|c = l)fi(l, d) − p(c = l, d)fi(l, d) = 0
</equation>
<figureCaption confidence="0.951314">
Figure 1: Constraints imposed on feature values for training reversible models p(d|c).
</figureCaption>
<equation confidence="0.613476">
� �
l0L d0l(l)
</equation>
<bodyText confidence="0.999614">
Alpino provides a wide-coverage grammar, lexicon
and parser (van Noord, 2006). Recently, a sentence
realizer has been added that uses the same grammar
and lexicon (de Kok and van Noord, 2010).
In the experiments, the cdbl part of the Alpino
Treebank (van der Beek et al., 2002) is used as train-
ing data (7,154 sentences). The WR-P-P-H part
(2,267 sentences) of the LASSY corpus (van Noord
et al., 2010), which consists of text from the Trouw
2001 newspaper, is used for testing.
</bodyText>
<subsectionHeader confidence="0.979759">
3.1 Features
</subsectionHeader>
<bodyText confidence="0.999972894736842">
The features that we use in the experiment are the
same features which are available in the Alpino
parser and generator. In the following section, these
features are described in some detail.
Word adjacency. Two word adjacency features
are used as auxiliary distributions (Johnson and Rie-
zler, 2000). The first feature is the probability of the
sentence according to a word trigram model. The
second feature is the probability of the sentence ac-
cording to a tag trigram model that uses the part-
of-speech tags assigned by the Alpino system. In
both models, linear interpolation smoothing for un-
known trigrams, and Laplacian smoothing for un-
known words and tags is applied. The trigram mod-
els have been trained on the Twente Nieuws Corpus
corpus (approximately 110 million words), exclud-
ing the Trouw 2001 corpus. In conventional pars-
ing tasks, the value of the word trigram model is the
same for all derivations of a given input sentence.
Lexical frames. Lexical analysis is applied dur-
ing parsing to find all possible subcategorization
frames for the tokens in the input sentence. Since
some frames occur more frequently in good parses
than others, we use feature templates that record the
frames that were used in a parse. An example of
such a feature is: ”‘to play’ serves as an intransi-
tive verb”. We also use an auxiliary distribution of
word and frame combinations that was trained on
a large corpus of automatically annotated sentences
(436 million words). The values of lexical frame
features are constant for all derivations in sentence
realization, unless the frame is not specified in the
logical form.
Dependency relations. There are also feature
templates which describe aspects of the dependency
structure. For each dependency, three types of de-
pendency features are extracted. Examples of such
features are ”a pronoun is used as the subject of
a verb”, ”the pronoun ’she’ is used as the sub-
ject of a verb”, ”the noun ’beer’ is used as the
object of the verb ’drink’”. In addition, features
are used which implement auxiliary distributions
for selectional preferences, as described in Van No-
ord (2007). In conventional realization tasks, the
values of these features are constant for all deriva-
tions for a given input representation.
Syntactic features. Syntactic features include fea-
tures which record the application of each grammar
rule, as well as features which record the application
of a rule in the context of another rule. An exam-
ple of the latter is ’rule 167 is used to construct the
second daughter of a derivation constructed by rule
233’. In addition, there are features describing more
complex syntactic patterns such as: fronting of sub-
jects and other noun phrases, orderings in the middle
field, long-distance dependencies, and parallelism of
conjuncts in coordination.
</bodyText>
<subsectionHeader confidence="0.998992">
3.2 Parse disambiguation
</subsectionHeader>
<bodyText confidence="0.999624">
Earlier we assumed that a treebank is a set of cor-
rect derivations. In practice, however, a treebank
only contains an abstraction of such derivations (in
</bodyText>
<page confidence="0.995748">
196
</page>
<bodyText confidence="0.999988583333333">
our case sentences with corresponding dependency
structures), thus abstracting away from syntactic de-
tails needed in a parse disambiguation model. As in
Osborne (2000), the derivations for the parse disam-
biguation model are created by parsing the training
corpus. In the current setting, up to at most 3000
derivations are created for every sentence. These
derivations are then compared to the gold standard
dependency structure to judge the quality of the
parses. For a given sentence, the parses with the
highest concept accuracy (van Noord, 2006) are con-
sidered correct, the rest is treated as incorrect.
</bodyText>
<subsectionHeader confidence="0.997428">
3.3 Fluency ranking
</subsectionHeader>
<bodyText confidence="0.99913652631579">
For fluency ranking we also need access to full
derivations. To ensure that the system is able to
generate from the dependency structures in the tree-
bank, we parse the corresponding sentence, and se-
lect the parse with the dependency structure that
corresponds most closely to the dependency struc-
ture in the treebank. The resulting dependency
structures are fed into the Alpino chart generator
to construct derivations for each dependency struc-
ture. The derivations for which the corresponding
sentences are closest to the original sentence in the
treebank are marked correct. Due to a limit on gen-
eration time, some longer sentences and correspond-
ing dependency structures were excluded from the
data. As a result, the average sentence length was
15.7 tokens, with a maximum of 26 tokens. To com-
pare a realization to the correct sentence, we use the
General Text Matcher (GTM) method (Melamed et
al., 2003; Cahill, 2009).
</bodyText>
<subsectionHeader confidence="0.991049">
3.4 Training the models
</subsectionHeader>
<bodyText confidence="0.998587090909091">
Models are trained by taking an informative sam-
ple of Q(c) for each c in the training data (Osborne,
2000). This sample consists of at most 100 ran-
domly selected derivations. Frequency-based fea-
ture selection is applied (Ratnaparkhi, 1999). A fea-
ture f partitions Q(c), if there are derivations d and
d&apos; in Q(c) such that f(c, d) =� f(c, d&apos;). A feature is
used if it partitions the informative sample of Q(c)
for at least two c. Table 1 lists the resulting charac-
teristics of the training data for each model.
We estimate the parameters of the conditional
</bodyText>
<table confidence="0.99614425">
Features Inputs Derivations
Generation 1727 3688 141808
Parse 25299 7133 376420
Reversible 25578 10811 518228
</table>
<tableCaption confidence="0.99986">
Table 1: Size of the training data for each model
</tableCaption>
<bodyText confidence="0.956604666666667">
maximum entropy models using TinyEst,1 with a
Gaussian (E2) prior distribution (p = 0, u2 = 1000)
to reduce overfitting (Chen and Rosenfeld, 1999).
</bodyText>
<sectionHeader confidence="0.999974" genericHeader="evaluation">
4 Results
</sectionHeader>
<subsectionHeader confidence="0.999408">
4.1 Parse disambiguation
</subsectionHeader>
<bodyText confidence="0.991037666666667">
Table 2 shows the results for parse disambiguation.
The table also provides lower and upper bounds: the
baseline model selects an arbitrary parse per sen-
tence; the oracle chooses the best available parse.
Figure 2 shows the learning curves for the direc-
tional parsing model and the reversible model.
</bodyText>
<table confidence="0.9958282">
Model CA (%) f-score (%)
Baseline 75.88 76.28
Oracle 94.86 95.09
Parse model 90.93 91.28
Reversible 90.87 91.21
</table>
<tableCaption confidence="0.99567">
Table 2: Concept Accuracy scores and f-scores in terms
</tableCaption>
<bodyText confidence="0.95647675">
of named dependency relations for the parsing-specific
model versus the reversible model.
The results show that the general, reversible,
model comes very close to the accuracy obtained
by the dedicated, parsing specific, model. Indeed,
the tiny difference is not statistically significant. We
compute statistical significance using the Approxi-
mate Randomization Test (Noreen, 1989).
</bodyText>
<subsectionHeader confidence="0.998285">
4.2 Fluency ranking
</subsectionHeader>
<bodyText confidence="0.999930444444444">
Table 3 compares the reversible model with a di-
rectional fluency ranking model. Figure 3 shows
the learning curves for the directional generation
model and the reversible model. The reversible
model achieves similar performance as the direc-
tional model (the difference is not significant).
To show that a reversible model can actually profit
from mutually shared features, we report on an ex-
periment where only a small amount of generation
</bodyText>
<footnote confidence="0.989565">
1http://github.com/danieldk/tinyest
</footnote>
<page confidence="0.992343">
197
</page>
<figure confidence="0.992798666666667">
CA (%)
76 78 80 82 84 86 88 90
parse model
reversible model
0.0 0.1 0.2 0.3 0.4 0.5
Proportion parse training data
</figure>
<figureCaption confidence="0.961471333333333">
Figure 2: Learning curve for directional and reversible
models for parsing. The reversible model uses all training
data for generation.
</figureCaption>
<figure confidence="0.9378114">
Model GTM
Random 55.72
Oracle 86.63
Fluency 71.82
Reversible 71.69
</figure>
<tableCaption confidence="0.990277">
Table 3: General Text Matcher scores for fluency ranking
using various models.
</tableCaption>
<bodyText confidence="0.999760272727273">
training data is available. In this experiment, we
manually annotated 234 dependency structures from
the cdbl part of the Alpino Treebank, by adding cor-
rect realizations. In many instances, there is more
than one fluent realization. We then used this data to
train a directional fluency ranking model and a re-
versible model. The results for this experiment are
shown in Table 4. Since the reversible model outper-
forms the directional model we conclude that indeed
fluency ranking benefits from parse disambiguation
data.
</bodyText>
<table confidence="0.904253666666667">
Model GTM
Fluency 70.54
Reversible 71.20
</table>
<tableCaption confidence="0.8318164">
Table 4: Fluency ranking using a small amount of anno-
tated fluency ranking training data (difference is signifi-
cant at p &lt; 0.05).
generation model
reversible model
</tableCaption>
<figure confidence="0.552878">
0.0 0.1 0.2 0.3 0.4 0.5
Proportion generation training data
</figure>
<figureCaption confidence="0.721121666666667">
Figure 3: Learning curves for directional and reversible
models for generation. The reversible models uses all
training data for parsing.
</figureCaption>
<sectionHeader confidence="0.991659" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999924642857143">
We proposed reversible SAVG as an alternative to
directional SAVG, based on the observation that
syntactic preferences are shared between parse dis-
ambiguation and fluency ranking. This framework
is not purely of theoretical interest, since the exper-
iments show that reversible models achieve accura-
cies that are similar to those of directional models.
Moreover, we showed that a fluency ranking model
trained on a small data set can be improved by com-
plementing it with parse disambiguation data.
The integration of knowledge from parse disam-
biguation and fluency ranking could be beneficial for
tasks which combine aspects of parsing and genera-
tion, such as word-graph parsing or paraphrasing.
</bodyText>
<figure confidence="0.911467">
GTM score
60 65 70
</figure>
<page confidence="0.985956">
198
</page>
<sectionHeader confidence="0.988966" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999397342857143">
Steven Abney. 1997. Stochastic attribute-value gram-
mars. Computational Linguistics, 23(4):597–618.
Aoife Cahill, Martin Forst, and Christian Rohrer. 2007.
Stochastic realisation ranking for a free word order
language. In ENLG ’07: Proceedings of the Eleventh
European Workshop on Natural Language Genera-
tion, pages 17–24, Morristown, NJ, USA.
Aoife Cahill. 2009. Correlating human and automatic
evaluation of a german surface realiser. In Proceed-
ings of the ACL-IJCNLP 2009 Conference - Short Pa-
pers, pages 97–100.
Stanley F. Chen and Ronald Rosenfeld. 1999. A gaussian
prior for smoothing maximum entropy models. Tech-
nical report, Carnegie Mellon University, Pittsburg.
Stephen Clark and James R. Curran. 2004. Parsing the
WSJ using CCG and log-linear models. In Proceed-
ings of the 42nd Annual Meeting of the ACL, pages
103–110, Morristown, NJ, USA.
Dani¨el de Kok and Gertjan van Noord. 2010. A sentence
generator for Dutch. In Proceedings of the 20th Com-
putational Linguistics in the Netherlands conference
(CLIN).
Martin Forst. 2007. Filling statistics with linguistics:
property design for the disambiguation of german lfg
parses. In DeepLP ’07: Proceedings of the Workshop
on Deep Linguistic Processing, pages 17–24, Morris-
town, NJ, USA.
Mark Johnson and Stefan Riezler. 2000. Exploiting
auxiliary distributions in stochastic unification-based
grammars. In Proceedings of the 1st Meeting of the
NAACL, pages 154–161, Seattle, Washington.
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi,
and Stefan Riezler. 1999. Estimators for stochastic
“unification-based” grammars. In Proceedings of the
37th Annual Meeting of the ACL.
Martin Kay. 1975. Syntactic processing and functional
sentence perspective. In TINLAP ’75: Proceedings of
the 1975 workshop on Theoretical issues in natural
language processing, pages 12–15, Morristown, NJ,
USA.
I. Dan Melamed, Ryan Green, and Joseph Turian. 2003.
Precision and recall of machine translation. In HLT-
NAACL.
Yusuke Miyao and Jun’ichi Tsujii. 2005. Probabilistic
disambiguation models for wide-coverage hpsg pars-
ing. In Proceedings of the 43rd Annual Meeting of the
ACL, pages 83–90, Morristown, NJ, USA.
Hiroko Nakanishi and Yusuke Miyao. 2005. Probabilis-
tic models for disambiguation of an hpsg-based chart
generator. In Proceedings of the 9th International
Workshop on Parsing Technologies (IWPT), pages 93–
102.
Eric W. Noreen. 1989. Computer-Intensive Methods
for Testing Hypotheses: An Introduction. Wiley-
Interscience.
Miles Osborne. 2000. Estimation of stochastic attribute-
value grammars using an informative sample. In Pro-
ceedings of the 18th conference on Computational lin-
guistics (COLING), pages 586–592.
Adwait Ratnaparkhi. 1999. Learning to parse natural
language with maximum entropy models. Machine
Learning, 34(1):151–175.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan,
Richard Crouch, John T. Maxwell III, and Mark John-
son. 2002. Parsing the wall street journal using a
lexical-functional grammar and discriminative estima-
tion techniques. In Proceedings of the 40th Annual
Meeting of the ACL, pages 271–278, Morristown, NJ,
USA.
Kristina Toutanova, Christopher D. Manning, Stuart M.
Shieber, Dan Flickinger, and Stephan Oepen. 2002.
Parse disambiguation for a rich hpsg grammar. In
First Workshop on Treebanks and Linguistic Theories
(TLT), pages 253–263, Sozopol.
Leonoor van der Beek, Gosse Bouma, Robert Malouf,
and Gertjan van Noord. 2002. The Alpino depen-
dency treebank. In Computational Linguistics in the
Netherlands (CLIN).
Gertjan van Noord and Robert Malouf. 2005. Wide
coverage parsing with stochastic attribute value gram-
mars. Draft available from the authors. A preliminary
version of this paper was published in the Proceedings
of the IJCNLP workshop Beyond Shallow Analyses,
Hainan China, 2004.
Gertjan van Noord, Ineke Schuurman, and Gosse Bouma.
2010. Lassy syntactische annotatie, revision 19053.
Gertjan van Noord. 2006. At Last Parsing Is Now
Operational. In TALN 2006 Verbum Ex Machina,
Actes De La 13e Conference sur Le Traitement Au-
tomatique des Langues naturelles, pages 20–42, Leu-
ven.
Gertjan van Noord. 2007. Using self-trained bilexical
preferences to improve disambiguation accuracy. In
Proceedings of the International Workshop on Parsing
Technology (IWPT), ACL 2007 Workshop, pages 1–
10, Prague.
Erik Velldal and Stephan Oepen. 2006. Statistical rank-
ing in tactical generation. In Proceedings of the 2006
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 517–525, Sydney,
Australia, July. ACL.
Erik Velldal, Stephan Oepen, and Dan Flickinger. 2004.
Paraphrasing treebanks for stochastic realization rank-
ing. In Proceedings of the 3rd Workshop on Treebanks
and Linguistic Theories (TLT), pages 149–160.
</reference>
<page confidence="0.998876">
199
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.933191">
<title confidence="0.999955">Reversible Stochastic Attribute-Value Grammars</title>
<author confidence="0.999087">Dani¨el_de_Kok Barbara Plank Gertjan van_Noord</author>
<affiliation confidence="0.998442">University of Groningen University of Groningen University of Groningen</affiliation>
<email confidence="0.954917">d.j.a.de.kok@rug.nlb.plank@rug.nlg.j.m.van.noord@rug.nl</email>
<abstract confidence="0.997659333333333">An attractive property of attribute-value grammars is their reversibility. Attribute-value grammars are usually coupled with separate statistical components for parse selection and fluency ranking. We propose reversible stochastic attribute-value grammars, in which model is employed both for parse selection and fluency ranking.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Abney</author>
</authors>
<title>Stochastic attribute-value grammars.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>4</issue>
<contexts>
<context position="866" citStr="Abney, 1997" startWordPosition="113" endWordPosition="114">y of attribute-value grammars is their reversibility. Attribute-value grammars are usually coupled with separate statistical components for parse selection and fluency ranking. We propose reversible stochastic attribute-value grammars, in which a single statistical model is employed both for parse selection and fluency ranking. 1 Introduction Reversible grammars were introduced as early as 1975 by Martin Kay (1975). In the eighties, the popularity of attribute-value grammars (AVG) was in part motivated by their inherent reversible nature. Later, AVG were enriched with a statistical component (Abney, 1997): stochastic AVG (SAVG). Training a SAVG is feasible if a stochastic model is assumed which is conditioned on the input sentences (Johnson et al., 1999). Various parsers based on this approach now exist for various languages (Toutanova et al., 2002; Riezler et al., 2002; van Noord and Malouf, 2005; Miyao and Tsujii, 2005; Clark and Curran, 2004; Forst, 2007). SAVG can be applied for generation to select the most fluent realization from the set of possible realizations (Velldal et al., 2004). In this case, the stochastic model is conditioned on the input logical forms. Such generators exist for</context>
<context position="3963" citStr="Abney (1997)" startWordPosition="603" endWordPosition="604">nt of training data is available for parsing or generation, cross-pollination improves the perfor194 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 194–199, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics mance of a model. If preferences are shared between parsing and generation, it follows that a generator could benefit from parsing data and vice versa. We present experimental results indicating that in such a bootstrap scenario a reversible model achieves better performance. 2 Reversible SAVG As Abney (1997) shows, we cannot use relatively simple techniques such as relative frequencies to obtain a model for estimating derivation probabilities in attribute-value grammars. As an alternative, he proposes a maximum entropy model, where the probability of a derivation d is defined as: 1 X p(d) = Z exp i fi(d) is the frequency of feature fi in derivation d. A weight λi is associated with each feature fi. In (1), Z is a normalizer which is defined as follows, where Q is the set of derivations defined by the grammar: XZ = Xexp λifi(d0) (2) d&apos;∈Q i Training this model requires access to all derivations Q a</context>
</contexts>
<marker>Abney, 1997</marker>
<rawString>Steven Abney. 1997. Stochastic attribute-value grammars. Computational Linguistics, 23(4):597–618.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aoife Cahill</author>
<author>Martin Forst</author>
<author>Christian Rohrer</author>
</authors>
<title>Stochastic realisation ranking for a free word order language. In</title>
<date>2007</date>
<booktitle>ENLG ’07: Proceedings of the Eleventh European Workshop on Natural Language Generation,</booktitle>
<pages>17--24</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1565" citStr="Cahill et al., 2007" startWordPosition="228" endWordPosition="231">sumed which is conditioned on the input sentences (Johnson et al., 1999). Various parsers based on this approach now exist for various languages (Toutanova et al., 2002; Riezler et al., 2002; van Noord and Malouf, 2005; Miyao and Tsujii, 2005; Clark and Curran, 2004; Forst, 2007). SAVG can be applied for generation to select the most fluent realization from the set of possible realizations (Velldal et al., 2004). In this case, the stochastic model is conditioned on the input logical forms. Such generators exist for various languages as well (Velldal and Oepen, 2006; Nakanishi and Miyao, 2005; Cahill et al., 2007; de Kok and van Noord, 2010). If an AVG is applied both to parsing and generation, two distinct stochastic components are required, one for parsing, and one for generation. To some extent this is reasonable, because some features are only relevant in a certain direction. For instance, features that represent aspects of the surface word order are important for generation, but irrelevant for parsing. Similarly, features which describe aspects of the logical form are important for parsing, but irrelevant for generation. Yet, there are also many features that are relevant in both directions. For </context>
</contexts>
<marker>Cahill, Forst, Rohrer, 2007</marker>
<rawString>Aoife Cahill, Martin Forst, and Christian Rohrer. 2007. Stochastic realisation ranking for a free word order language. In ENLG ’07: Proceedings of the Eleventh European Workshop on Natural Language Generation, pages 17–24, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aoife Cahill</author>
</authors>
<title>Correlating human and automatic evaluation of a german surface realiser.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP 2009 Conference - Short Papers,</booktitle>
<pages>97--100</pages>
<contexts>
<context position="12518" citStr="Cahill, 2009" startWordPosition="2051" endWordPosition="2052">treebank. The resulting dependency structures are fed into the Alpino chart generator to construct derivations for each dependency structure. The derivations for which the corresponding sentences are closest to the original sentence in the treebank are marked correct. Due to a limit on generation time, some longer sentences and corresponding dependency structures were excluded from the data. As a result, the average sentence length was 15.7 tokens, with a maximum of 26 tokens. To compare a realization to the correct sentence, we use the General Text Matcher (GTM) method (Melamed et al., 2003; Cahill, 2009). 3.4 Training the models Models are trained by taking an informative sample of Q(c) for each c in the training data (Osborne, 2000). This sample consists of at most 100 randomly selected derivations. Frequency-based feature selection is applied (Ratnaparkhi, 1999). A feature f partitions Q(c), if there are derivations d and d&apos; in Q(c) such that f(c, d) =� f(c, d&apos;). A feature is used if it partitions the informative sample of Q(c) for at least two c. Table 1 lists the resulting characteristics of the training data for each model. We estimate the parameters of the conditional Features Inputs De</context>
</contexts>
<marker>Cahill, 2009</marker>
<rawString>Aoife Cahill. 2009. Correlating human and automatic evaluation of a german surface realiser. In Proceedings of the ACL-IJCNLP 2009 Conference - Short Papers, pages 97–100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>A gaussian prior for smoothing maximum entropy models.</title>
<date>1999</date>
<tech>Technical report,</tech>
<institution>Carnegie Mellon University,</institution>
<location>Pittsburg.</location>
<contexts>
<context position="13406" citStr="Chen and Rosenfeld, 1999" startWordPosition="2201" endWordPosition="2204">9). A feature f partitions Q(c), if there are derivations d and d&apos; in Q(c) such that f(c, d) =� f(c, d&apos;). A feature is used if it partitions the informative sample of Q(c) for at least two c. Table 1 lists the resulting characteristics of the training data for each model. We estimate the parameters of the conditional Features Inputs Derivations Generation 1727 3688 141808 Parse 25299 7133 376420 Reversible 25578 10811 518228 Table 1: Size of the training data for each model maximum entropy models using TinyEst,1 with a Gaussian (E2) prior distribution (p = 0, u2 = 1000) to reduce overfitting (Chen and Rosenfeld, 1999). 4 Results 4.1 Parse disambiguation Table 2 shows the results for parse disambiguation. The table also provides lower and upper bounds: the baseline model selects an arbitrary parse per sentence; the oracle chooses the best available parse. Figure 2 shows the learning curves for the directional parsing model and the reversible model. Model CA (%) f-score (%) Baseline 75.88 76.28 Oracle 94.86 95.09 Parse model 90.93 91.28 Reversible 90.87 91.21 Table 2: Concept Accuracy scores and f-scores in terms of named dependency relations for the parsing-specific model versus the reversible model. The re</context>
</contexts>
<marker>Chen, Rosenfeld, 1999</marker>
<rawString>Stanley F. Chen and Ronald Rosenfeld. 1999. A gaussian prior for smoothing maximum entropy models. Technical report, Carnegie Mellon University, Pittsburg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Parsing the WSJ using CCG and log-linear models.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the ACL,</booktitle>
<pages>103--110</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1212" citStr="Clark and Curran, 2004" startWordPosition="169" endWordPosition="172">oduction Reversible grammars were introduced as early as 1975 by Martin Kay (1975). In the eighties, the popularity of attribute-value grammars (AVG) was in part motivated by their inherent reversible nature. Later, AVG were enriched with a statistical component (Abney, 1997): stochastic AVG (SAVG). Training a SAVG is feasible if a stochastic model is assumed which is conditioned on the input sentences (Johnson et al., 1999). Various parsers based on this approach now exist for various languages (Toutanova et al., 2002; Riezler et al., 2002; van Noord and Malouf, 2005; Miyao and Tsujii, 2005; Clark and Curran, 2004; Forst, 2007). SAVG can be applied for generation to select the most fluent realization from the set of possible realizations (Velldal et al., 2004). In this case, the stochastic model is conditioned on the input logical forms. Such generators exist for various languages as well (Velldal and Oepen, 2006; Nakanishi and Miyao, 2005; Cahill et al., 2007; de Kok and van Noord, 2010). If an AVG is applied both to parsing and generation, two distinct stochastic components are required, one for parsing, and one for generation. To some extent this is reasonable, because some features are only relevan</context>
</contexts>
<marker>Clark, Curran, 2004</marker>
<rawString>Stephen Clark and James R. Curran. 2004. Parsing the WSJ using CCG and log-linear models. In Proceedings of the 42nd Annual Meeting of the ACL, pages 103–110, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dani¨el de Kok</author>
<author>Gertjan van Noord</author>
</authors>
<title>A sentence generator for Dutch.</title>
<date>2010</date>
<booktitle>In Proceedings of the 20th Computational Linguistics in the Netherlands conference (CLIN).</booktitle>
<marker>Dani¨el de Kok, van Noord, 2010</marker>
<rawString>Dani¨el de Kok and Gertjan van Noord. 2010. A sentence generator for Dutch. In Proceedings of the 20th Computational Linguistics in the Netherlands conference (CLIN).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Forst</author>
</authors>
<title>Filling statistics with linguistics: property design for the disambiguation of german lfg parses.</title>
<date>2007</date>
<booktitle>In DeepLP ’07: Proceedings of the Workshop on Deep Linguistic Processing,</booktitle>
<pages>17--24</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1226" citStr="Forst, 2007" startWordPosition="173" endWordPosition="174">mars were introduced as early as 1975 by Martin Kay (1975). In the eighties, the popularity of attribute-value grammars (AVG) was in part motivated by their inherent reversible nature. Later, AVG were enriched with a statistical component (Abney, 1997): stochastic AVG (SAVG). Training a SAVG is feasible if a stochastic model is assumed which is conditioned on the input sentences (Johnson et al., 1999). Various parsers based on this approach now exist for various languages (Toutanova et al., 2002; Riezler et al., 2002; van Noord and Malouf, 2005; Miyao and Tsujii, 2005; Clark and Curran, 2004; Forst, 2007). SAVG can be applied for generation to select the most fluent realization from the set of possible realizations (Velldal et al., 2004). In this case, the stochastic model is conditioned on the input logical forms. Such generators exist for various languages as well (Velldal and Oepen, 2006; Nakanishi and Miyao, 2005; Cahill et al., 2007; de Kok and van Noord, 2010). If an AVG is applied both to parsing and generation, two distinct stochastic components are required, one for parsing, and one for generation. To some extent this is reasonable, because some features are only relevant in a certain</context>
</contexts>
<marker>Forst, 2007</marker>
<rawString>Martin Forst. 2007. Filling statistics with linguistics: property design for the disambiguation of german lfg parses. In DeepLP ’07: Proceedings of the Workshop on Deep Linguistic Processing, pages 17–24, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Stefan Riezler</author>
</authors>
<title>Exploiting auxiliary distributions in stochastic unification-based grammars.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st Meeting of the NAACL,</booktitle>
<pages>154--161</pages>
<location>Seattle, Washington.</location>
<contexts>
<context position="8285" citStr="Johnson and Riezler, 2000" startWordPosition="1357" endWordPosition="1361">e Kok and van Noord, 2010). In the experiments, the cdbl part of the Alpino Treebank (van der Beek et al., 2002) is used as training data (7,154 sentences). The WR-P-P-H part (2,267 sentences) of the LASSY corpus (van Noord et al., 2010), which consists of text from the Trouw 2001 newspaper, is used for testing. 3.1 Features The features that we use in the experiment are the same features which are available in the Alpino parser and generator. In the following section, these features are described in some detail. Word adjacency. Two word adjacency features are used as auxiliary distributions (Johnson and Riezler, 2000). The first feature is the probability of the sentence according to a word trigram model. The second feature is the probability of the sentence according to a tag trigram model that uses the partof-speech tags assigned by the Alpino system. In both models, linear interpolation smoothing for unknown trigrams, and Laplacian smoothing for unknown words and tags is applied. The trigram models have been trained on the Twente Nieuws Corpus corpus (approximately 110 million words), excluding the Trouw 2001 corpus. In conventional parsing tasks, the value of the word trigram model is the same for all </context>
</contexts>
<marker>Johnson, Riezler, 2000</marker>
<rawString>Mark Johnson and Stefan Riezler. 2000. Exploiting auxiliary distributions in stochastic unification-based grammars. In Proceedings of the 1st Meeting of the NAACL, pages 154–161, Seattle, Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Stuart Geman</author>
<author>Stephen Canon</author>
<author>Zhiyi Chi</author>
<author>Stefan Riezler</author>
</authors>
<title>Estimators for stochastic “unification-based” grammars.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="1018" citStr="Johnson et al., 1999" startWordPosition="137" endWordPosition="140">se selection and fluency ranking. We propose reversible stochastic attribute-value grammars, in which a single statistical model is employed both for parse selection and fluency ranking. 1 Introduction Reversible grammars were introduced as early as 1975 by Martin Kay (1975). In the eighties, the popularity of attribute-value grammars (AVG) was in part motivated by their inherent reversible nature. Later, AVG were enriched with a statistical component (Abney, 1997): stochastic AVG (SAVG). Training a SAVG is feasible if a stochastic model is assumed which is conditioned on the input sentences (Johnson et al., 1999). Various parsers based on this approach now exist for various languages (Toutanova et al., 2002; Riezler et al., 2002; van Noord and Malouf, 2005; Miyao and Tsujii, 2005; Clark and Curran, 2004; Forst, 2007). SAVG can be applied for generation to select the most fluent realization from the set of possible realizations (Velldal et al., 2004). In this case, the stochastic model is conditioned on the input logical forms. Such generators exist for various languages as well (Velldal and Oepen, 2006; Nakanishi and Miyao, 2005; Cahill et al., 2007; de Kok and van Noord, 2010). If an AVG is applied b</context>
<context position="4663" citStr="Johnson et al. (1999)" startWordPosition="726" endWordPosition="729"> to obtain a model for estimating derivation probabilities in attribute-value grammars. As an alternative, he proposes a maximum entropy model, where the probability of a derivation d is defined as: 1 X p(d) = Z exp i fi(d) is the frequency of feature fi in derivation d. A weight λi is associated with each feature fi. In (1), Z is a normalizer which is defined as follows, where Q is the set of derivations defined by the grammar: XZ = Xexp λifi(d0) (2) d&apos;∈Q i Training this model requires access to all derivations Q allowed by the grammar, which makes it hard to implement the model in practice. Johnson et al. (1999) alleviate this problem by proposing a model which conditions on the input sentence s: p(djs). Since the number of derivations for a given sentence s is usually finite, the calculation of the normalizer is much more practical. Conversely, in generation the model is conditioned on the input logical form l, p(djl) (Velldal et al., 2004). In such directional stochastic attribute-value grammars, the probability of a derivation d given an input x (a sentence or a logical form) is defined as: 1 X p(djx) = Z(x)exp λifi(x, d) (3) with Z(x) as (Q(x) are all derivations for input x): Z(x) = X Xexp λifi(</context>
</contexts>
<marker>Johnson, Geman, Canon, Chi, Riezler, 1999</marker>
<rawString>Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi, and Stefan Riezler. 1999. Estimators for stochastic “unification-based” grammars. In Proceedings of the 37th Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Kay</author>
</authors>
<title>Syntactic processing and functional sentence perspective.</title>
<date>1975</date>
<booktitle>In TINLAP ’75: Proceedings of the 1975 workshop on Theoretical issues in natural language processing,</booktitle>
<pages>12--15</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="672" citStr="Kay (1975)" startWordPosition="84" endWordPosition="85">Barbara Plank Gertjan van Noord University of Groningen University of Groningen University of Groningen d.j.a.de.kok@rug.nl b.plank@rug.nl g.j.m.van.noord@rug.nl Abstract An attractive property of attribute-value grammars is their reversibility. Attribute-value grammars are usually coupled with separate statistical components for parse selection and fluency ranking. We propose reversible stochastic attribute-value grammars, in which a single statistical model is employed both for parse selection and fluency ranking. 1 Introduction Reversible grammars were introduced as early as 1975 by Martin Kay (1975). In the eighties, the popularity of attribute-value grammars (AVG) was in part motivated by their inherent reversible nature. Later, AVG were enriched with a statistical component (Abney, 1997): stochastic AVG (SAVG). Training a SAVG is feasible if a stochastic model is assumed which is conditioned on the input sentences (Johnson et al., 1999). Various parsers based on this approach now exist for various languages (Toutanova et al., 2002; Riezler et al., 2002; van Noord and Malouf, 2005; Miyao and Tsujii, 2005; Clark and Curran, 2004; Forst, 2007). SAVG can be applied for generation to select</context>
</contexts>
<marker>Kay, 1975</marker>
<rawString>Martin Kay. 1975. Syntactic processing and functional sentence perspective. In TINLAP ’75: Proceedings of the 1975 workshop on Theoretical issues in natural language processing, pages 12–15, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
<author>Ryan Green</author>
<author>Joseph Turian</author>
</authors>
<title>Precision and recall of machine translation.</title>
<date>2003</date>
<booktitle>In HLTNAACL.</booktitle>
<contexts>
<context position="12503" citStr="Melamed et al., 2003" startWordPosition="2047" endWordPosition="2050">ency structure in the treebank. The resulting dependency structures are fed into the Alpino chart generator to construct derivations for each dependency structure. The derivations for which the corresponding sentences are closest to the original sentence in the treebank are marked correct. Due to a limit on generation time, some longer sentences and corresponding dependency structures were excluded from the data. As a result, the average sentence length was 15.7 tokens, with a maximum of 26 tokens. To compare a realization to the correct sentence, we use the General Text Matcher (GTM) method (Melamed et al., 2003; Cahill, 2009). 3.4 Training the models Models are trained by taking an informative sample of Q(c) for each c in the training data (Osborne, 2000). This sample consists of at most 100 randomly selected derivations. Frequency-based feature selection is applied (Ratnaparkhi, 1999). A feature f partitions Q(c), if there are derivations d and d&apos; in Q(c) such that f(c, d) =� f(c, d&apos;). A feature is used if it partitions the informative sample of Q(c) for at least two c. Table 1 lists the resulting characteristics of the training data for each model. We estimate the parameters of the conditional Fea</context>
</contexts>
<marker>Melamed, Green, Turian, 2003</marker>
<rawString>I. Dan Melamed, Ryan Green, and Joseph Turian. 2003. Precision and recall of machine translation. In HLTNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Probabilistic disambiguation models for wide-coverage hpsg parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the ACL,</booktitle>
<pages>83--90</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1188" citStr="Miyao and Tsujii, 2005" startWordPosition="165" endWordPosition="168"> fluency ranking. 1 Introduction Reversible grammars were introduced as early as 1975 by Martin Kay (1975). In the eighties, the popularity of attribute-value grammars (AVG) was in part motivated by their inherent reversible nature. Later, AVG were enriched with a statistical component (Abney, 1997): stochastic AVG (SAVG). Training a SAVG is feasible if a stochastic model is assumed which is conditioned on the input sentences (Johnson et al., 1999). Various parsers based on this approach now exist for various languages (Toutanova et al., 2002; Riezler et al., 2002; van Noord and Malouf, 2005; Miyao and Tsujii, 2005; Clark and Curran, 2004; Forst, 2007). SAVG can be applied for generation to select the most fluent realization from the set of possible realizations (Velldal et al., 2004). In this case, the stochastic model is conditioned on the input logical forms. Such generators exist for various languages as well (Velldal and Oepen, 2006; Nakanishi and Miyao, 2005; Cahill et al., 2007; de Kok and van Noord, 2010). If an AVG is applied both to parsing and generation, two distinct stochastic components are required, one for parsing, and one for generation. To some extent this is reasonable, because some f</context>
</contexts>
<marker>Miyao, Tsujii, 2005</marker>
<rawString>Yusuke Miyao and Jun’ichi Tsujii. 2005. Probabilistic disambiguation models for wide-coverage hpsg parsing. In Proceedings of the 43rd Annual Meeting of the ACL, pages 83–90, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroko Nakanishi</author>
<author>Yusuke Miyao</author>
</authors>
<title>Probabilistic models for disambiguation of an hpsg-based chart generator.</title>
<date>2005</date>
<booktitle>In Proceedings of the 9th International Workshop on Parsing Technologies (IWPT),</booktitle>
<pages>93--102</pages>
<contexts>
<context position="1544" citStr="Nakanishi and Miyao, 2005" startWordPosition="224" endWordPosition="227">if a stochastic model is assumed which is conditioned on the input sentences (Johnson et al., 1999). Various parsers based on this approach now exist for various languages (Toutanova et al., 2002; Riezler et al., 2002; van Noord and Malouf, 2005; Miyao and Tsujii, 2005; Clark and Curran, 2004; Forst, 2007). SAVG can be applied for generation to select the most fluent realization from the set of possible realizations (Velldal et al., 2004). In this case, the stochastic model is conditioned on the input logical forms. Such generators exist for various languages as well (Velldal and Oepen, 2006; Nakanishi and Miyao, 2005; Cahill et al., 2007; de Kok and van Noord, 2010). If an AVG is applied both to parsing and generation, two distinct stochastic components are required, one for parsing, and one for generation. To some extent this is reasonable, because some features are only relevant in a certain direction. For instance, features that represent aspects of the surface word order are important for generation, but irrelevant for parsing. Similarly, features which describe aspects of the logical form are important for parsing, but irrelevant for generation. Yet, there are also many features that are relevant in </context>
</contexts>
<marker>Nakanishi, Miyao, 2005</marker>
<rawString>Hiroko Nakanishi and Yusuke Miyao. 2005. Probabilistic models for disambiguation of an hpsg-based chart generator. In Proceedings of the 9th International Workshop on Parsing Technologies (IWPT), pages 93– 102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric W Noreen</author>
</authors>
<title>Computer-Intensive Methods for Testing Hypotheses: An Introduction.</title>
<date>1989</date>
<publisher>WileyInterscience.</publisher>
<contexts>
<context position="14291" citStr="Noreen, 1989" startWordPosition="2337" endWordPosition="2338">rves for the directional parsing model and the reversible model. Model CA (%) f-score (%) Baseline 75.88 76.28 Oracle 94.86 95.09 Parse model 90.93 91.28 Reversible 90.87 91.21 Table 2: Concept Accuracy scores and f-scores in terms of named dependency relations for the parsing-specific model versus the reversible model. The results show that the general, reversible, model comes very close to the accuracy obtained by the dedicated, parsing specific, model. Indeed, the tiny difference is not statistically significant. We compute statistical significance using the Approximate Randomization Test (Noreen, 1989). 4.2 Fluency ranking Table 3 compares the reversible model with a directional fluency ranking model. Figure 3 shows the learning curves for the directional generation model and the reversible model. The reversible model achieves similar performance as the directional model (the difference is not significant). To show that a reversible model can actually profit from mutually shared features, we report on an experiment where only a small amount of generation 1http://github.com/danieldk/tinyest 197 CA (%) 76 78 80 82 84 86 88 90 parse model reversible model 0.0 0.1 0.2 0.3 0.4 0.5 Proportion par</context>
</contexts>
<marker>Noreen, 1989</marker>
<rawString>Eric W. Noreen. 1989. Computer-Intensive Methods for Testing Hypotheses: An Introduction. WileyInterscience.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miles Osborne</author>
</authors>
<title>Estimation of stochastic attributevalue grammars using an informative sample.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th conference on Computational linguistics (COLING),</booktitle>
<pages>586--592</pages>
<contexts>
<context position="11136" citStr="Osborne (2000)" startWordPosition="1825" endWordPosition="1826">ion constructed by rule 233’. In addition, there are features describing more complex syntactic patterns such as: fronting of subjects and other noun phrases, orderings in the middle field, long-distance dependencies, and parallelism of conjuncts in coordination. 3.2 Parse disambiguation Earlier we assumed that a treebank is a set of correct derivations. In practice, however, a treebank only contains an abstraction of such derivations (in 196 our case sentences with corresponding dependency structures), thus abstracting away from syntactic details needed in a parse disambiguation model. As in Osborne (2000), the derivations for the parse disambiguation model are created by parsing the training corpus. In the current setting, up to at most 3000 derivations are created for every sentence. These derivations are then compared to the gold standard dependency structure to judge the quality of the parses. For a given sentence, the parses with the highest concept accuracy (van Noord, 2006) are considered correct, the rest is treated as incorrect. 3.3 Fluency ranking For fluency ranking we also need access to full derivations. To ensure that the system is able to generate from the dependency structures i</context>
<context position="12650" citStr="Osborne, 2000" startWordPosition="2075" endWordPosition="2076">structure. The derivations for which the corresponding sentences are closest to the original sentence in the treebank are marked correct. Due to a limit on generation time, some longer sentences and corresponding dependency structures were excluded from the data. As a result, the average sentence length was 15.7 tokens, with a maximum of 26 tokens. To compare a realization to the correct sentence, we use the General Text Matcher (GTM) method (Melamed et al., 2003; Cahill, 2009). 3.4 Training the models Models are trained by taking an informative sample of Q(c) for each c in the training data (Osborne, 2000). This sample consists of at most 100 randomly selected derivations. Frequency-based feature selection is applied (Ratnaparkhi, 1999). A feature f partitions Q(c), if there are derivations d and d&apos; in Q(c) such that f(c, d) =� f(c, d&apos;). A feature is used if it partitions the informative sample of Q(c) for at least two c. Table 1 lists the resulting characteristics of the training data for each model. We estimate the parameters of the conditional Features Inputs Derivations Generation 1727 3688 141808 Parse 25299 7133 376420 Reversible 25578 10811 518228 Table 1: Size of the training data for e</context>
</contexts>
<marker>Osborne, 2000</marker>
<rawString>Miles Osborne. 2000. Estimation of stochastic attributevalue grammars using an informative sample. In Proceedings of the 18th conference on Computational linguistics (COLING), pages 586–592.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>Learning to parse natural language with maximum entropy models.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="12783" citStr="Ratnaparkhi, 1999" startWordPosition="2094" endWordPosition="2095">correct. Due to a limit on generation time, some longer sentences and corresponding dependency structures were excluded from the data. As a result, the average sentence length was 15.7 tokens, with a maximum of 26 tokens. To compare a realization to the correct sentence, we use the General Text Matcher (GTM) method (Melamed et al., 2003; Cahill, 2009). 3.4 Training the models Models are trained by taking an informative sample of Q(c) for each c in the training data (Osborne, 2000). This sample consists of at most 100 randomly selected derivations. Frequency-based feature selection is applied (Ratnaparkhi, 1999). A feature f partitions Q(c), if there are derivations d and d&apos; in Q(c) such that f(c, d) =� f(c, d&apos;). A feature is used if it partitions the informative sample of Q(c) for at least two c. Table 1 lists the resulting characteristics of the training data for each model. We estimate the parameters of the conditional Features Inputs Derivations Generation 1727 3688 141808 Parse 25299 7133 376420 Reversible 25578 10811 518228 Table 1: Size of the training data for each model maximum entropy models using TinyEst,1 with a Gaussian (E2) prior distribution (p = 0, u2 = 1000) to reduce overfitting (Ch</context>
</contexts>
<marker>Ratnaparkhi, 1999</marker>
<rawString>Adwait Ratnaparkhi. 1999. Learning to parse natural language with maximum entropy models. Machine Learning, 34(1):151–175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>Tracy H King</author>
<author>Ronald M Kaplan</author>
<author>Richard Crouch</author>
<author>John T Maxwell</author>
<author>Mark Johnson</author>
</authors>
<title>Parsing the wall street journal using a lexical-functional grammar and discriminative estimation techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the ACL,</booktitle>
<pages>271--278</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1136" citStr="Riezler et al., 2002" startWordPosition="156" endWordPosition="159">cal model is employed both for parse selection and fluency ranking. 1 Introduction Reversible grammars were introduced as early as 1975 by Martin Kay (1975). In the eighties, the popularity of attribute-value grammars (AVG) was in part motivated by their inherent reversible nature. Later, AVG were enriched with a statistical component (Abney, 1997): stochastic AVG (SAVG). Training a SAVG is feasible if a stochastic model is assumed which is conditioned on the input sentences (Johnson et al., 1999). Various parsers based on this approach now exist for various languages (Toutanova et al., 2002; Riezler et al., 2002; van Noord and Malouf, 2005; Miyao and Tsujii, 2005; Clark and Curran, 2004; Forst, 2007). SAVG can be applied for generation to select the most fluent realization from the set of possible realizations (Velldal et al., 2004). In this case, the stochastic model is conditioned on the input logical forms. Such generators exist for various languages as well (Velldal and Oepen, 2006; Nakanishi and Miyao, 2005; Cahill et al., 2007; de Kok and van Noord, 2010). If an AVG is applied both to parsing and generation, two distinct stochastic components are required, one for parsing, and one for generatio</context>
</contexts>
<marker>Riezler, King, Kaplan, Crouch, Maxwell, Johnson, 2002</marker>
<rawString>Stefan Riezler, Tracy H. King, Ronald M. Kaplan, Richard Crouch, John T. Maxwell III, and Mark Johnson. 2002. Parsing the wall street journal using a lexical-functional grammar and discriminative estimation techniques. In Proceedings of the 40th Annual Meeting of the ACL, pages 271–278, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Christopher D Manning</author>
<author>Stuart M Shieber</author>
<author>Dan Flickinger</author>
<author>Stephan Oepen</author>
</authors>
<title>Parse disambiguation for a rich hpsg grammar.</title>
<date>2002</date>
<booktitle>In First Workshop on Treebanks and Linguistic Theories (TLT),</booktitle>
<pages>253--263</pages>
<contexts>
<context position="1114" citStr="Toutanova et al., 2002" startWordPosition="152" endWordPosition="155"> which a single statistical model is employed both for parse selection and fluency ranking. 1 Introduction Reversible grammars were introduced as early as 1975 by Martin Kay (1975). In the eighties, the popularity of attribute-value grammars (AVG) was in part motivated by their inherent reversible nature. Later, AVG were enriched with a statistical component (Abney, 1997): stochastic AVG (SAVG). Training a SAVG is feasible if a stochastic model is assumed which is conditioned on the input sentences (Johnson et al., 1999). Various parsers based on this approach now exist for various languages (Toutanova et al., 2002; Riezler et al., 2002; van Noord and Malouf, 2005; Miyao and Tsujii, 2005; Clark and Curran, 2004; Forst, 2007). SAVG can be applied for generation to select the most fluent realization from the set of possible realizations (Velldal et al., 2004). In this case, the stochastic model is conditioned on the input logical forms. Such generators exist for various languages as well (Velldal and Oepen, 2006; Nakanishi and Miyao, 2005; Cahill et al., 2007; de Kok and van Noord, 2010). If an AVG is applied both to parsing and generation, two distinct stochastic components are required, one for parsing,</context>
</contexts>
<marker>Toutanova, Manning, Shieber, Flickinger, Oepen, 2002</marker>
<rawString>Kristina Toutanova, Christopher D. Manning, Stuart M. Shieber, Dan Flickinger, and Stephan Oepen. 2002. Parse disambiguation for a rich hpsg grammar. In First Workshop on Treebanks and Linguistic Theories (TLT), pages 253–263, Sozopol.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonoor van der Beek</author>
<author>Gosse Bouma</author>
<author>Robert Malouf</author>
<author>Gertjan van Noord</author>
</authors>
<title>The Alpino dependency treebank.</title>
<date>2002</date>
<booktitle>In Computational Linguistics in the Netherlands (CLIN).</booktitle>
<marker>van der Beek, Bouma, Malouf, van Noord, 2002</marker>
<rawString>Leonoor van der Beek, Gosse Bouma, Robert Malouf, and Gertjan van Noord. 2002. The Alpino dependency treebank. In Computational Linguistics in the Netherlands (CLIN).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gertjan van Noord</author>
<author>Robert Malouf</author>
</authors>
<title>Wide coverage parsing with stochastic attribute value grammars. Draft available from the authors. A preliminary version of this paper was published</title>
<date>2005</date>
<booktitle>in the Proceedings of the IJCNLP workshop Beyond Shallow Analyses,</booktitle>
<location>Hainan</location>
<marker>van Noord, Malouf, 2005</marker>
<rawString>Gertjan van Noord and Robert Malouf. 2005. Wide coverage parsing with stochastic attribute value grammars. Draft available from the authors. A preliminary version of this paper was published in the Proceedings of the IJCNLP workshop Beyond Shallow Analyses, Hainan China, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gertjan van Noord</author>
<author>Ineke Schuurman</author>
<author>Gosse Bouma</author>
</authors>
<title>Lassy syntactische annotatie, revision 19053.</title>
<date>2010</date>
<marker>van Noord, Schuurman, Bouma, 2010</marker>
<rawString>Gertjan van Noord, Ineke Schuurman, and Gosse Bouma. 2010. Lassy syntactische annotatie, revision 19053.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gertjan van Noord</author>
</authors>
<title>At Last Parsing Is Now Operational.</title>
<date>2006</date>
<booktitle>In TALN 2006 Verbum Ex Machina, Actes De La 13e Conference sur Le Traitement Automatique des Langues naturelles,</booktitle>
<pages>20--42</pages>
<location>Leuven.</location>
<marker>van Noord, 2006</marker>
<rawString>Gertjan van Noord. 2006. At Last Parsing Is Now Operational. In TALN 2006 Verbum Ex Machina, Actes De La 13e Conference sur Le Traitement Automatique des Langues naturelles, pages 20–42, Leuven.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gertjan van Noord</author>
</authors>
<title>Using self-trained bilexical preferences to improve disambiguation accuracy.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Workshop on Parsing Technology (IWPT), ACL 2007 Workshop,</booktitle>
<pages>1--10</pages>
<location>Prague.</location>
<marker>van Noord, 2007</marker>
<rawString>Gertjan van Noord. 2007. Using self-trained bilexical preferences to improve disambiguation accuracy. In Proceedings of the International Workshop on Parsing Technology (IWPT), ACL 2007 Workshop, pages 1– 10, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik Velldal</author>
<author>Stephan Oepen</author>
</authors>
<title>Statistical ranking in tactical generation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>517--525</pages>
<publisher>ACL.</publisher>
<location>Sydney, Australia,</location>
<contexts>
<context position="1517" citStr="Velldal and Oepen, 2006" startWordPosition="220" endWordPosition="223">ining a SAVG is feasible if a stochastic model is assumed which is conditioned on the input sentences (Johnson et al., 1999). Various parsers based on this approach now exist for various languages (Toutanova et al., 2002; Riezler et al., 2002; van Noord and Malouf, 2005; Miyao and Tsujii, 2005; Clark and Curran, 2004; Forst, 2007). SAVG can be applied for generation to select the most fluent realization from the set of possible realizations (Velldal et al., 2004). In this case, the stochastic model is conditioned on the input logical forms. Such generators exist for various languages as well (Velldal and Oepen, 2006; Nakanishi and Miyao, 2005; Cahill et al., 2007; de Kok and van Noord, 2010). If an AVG is applied both to parsing and generation, two distinct stochastic components are required, one for parsing, and one for generation. To some extent this is reasonable, because some features are only relevant in a certain direction. For instance, features that represent aspects of the surface word order are important for generation, but irrelevant for parsing. Similarly, features which describe aspects of the logical form are important for parsing, but irrelevant for generation. Yet, there are also many fea</context>
</contexts>
<marker>Velldal, Oepen, 2006</marker>
<rawString>Erik Velldal and Stephan Oepen. 2006. Statistical ranking in tactical generation. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 517–525, Sydney, Australia, July. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik Velldal</author>
<author>Stephan Oepen</author>
<author>Dan Flickinger</author>
</authors>
<title>Paraphrasing treebanks for stochastic realization ranking.</title>
<date>2004</date>
<booktitle>In Proceedings of the 3rd Workshop on Treebanks and Linguistic Theories (TLT),</booktitle>
<pages>149--160</pages>
<contexts>
<context position="1361" citStr="Velldal et al., 2004" startWordPosition="194" endWordPosition="197">was in part motivated by their inherent reversible nature. Later, AVG were enriched with a statistical component (Abney, 1997): stochastic AVG (SAVG). Training a SAVG is feasible if a stochastic model is assumed which is conditioned on the input sentences (Johnson et al., 1999). Various parsers based on this approach now exist for various languages (Toutanova et al., 2002; Riezler et al., 2002; van Noord and Malouf, 2005; Miyao and Tsujii, 2005; Clark and Curran, 2004; Forst, 2007). SAVG can be applied for generation to select the most fluent realization from the set of possible realizations (Velldal et al., 2004). In this case, the stochastic model is conditioned on the input logical forms. Such generators exist for various languages as well (Velldal and Oepen, 2006; Nakanishi and Miyao, 2005; Cahill et al., 2007; de Kok and van Noord, 2010). If an AVG is applied both to parsing and generation, two distinct stochastic components are required, one for parsing, and one for generation. To some extent this is reasonable, because some features are only relevant in a certain direction. For instance, features that represent aspects of the surface word order are important for generation, but irrelevant for pa</context>
<context position="4999" citStr="Velldal et al., 2004" startWordPosition="783" endWordPosition="786">ormalizer which is defined as follows, where Q is the set of derivations defined by the grammar: XZ = Xexp λifi(d0) (2) d&apos;∈Q i Training this model requires access to all derivations Q allowed by the grammar, which makes it hard to implement the model in practice. Johnson et al. (1999) alleviate this problem by proposing a model which conditions on the input sentence s: p(djs). Since the number of derivations for a given sentence s is usually finite, the calculation of the normalizer is much more practical. Conversely, in generation the model is conditioned on the input logical form l, p(djl) (Velldal et al., 2004). In such directional stochastic attribute-value grammars, the probability of a derivation d given an input x (a sentence or a logical form) is defined as: 1 X p(djx) = Z(x)exp λifi(x, d) (3) with Z(x) as (Q(x) are all derivations for input x): Z(x) = X Xexp λifi(x, d0) (4) d&apos;∈Q(x) i Consequently, the constraint put on feature values during training only refers to derivations with the same input. If X is the set of inputs (for parsing, all sentences in the treebank; for generation, all logical forms), then we have: Ep(fi) − Ep(fi) = 0 ~ (5) �p(x)p(djx)fi(x,d) −�p(x,d)fi(x,d) = 0 Here we assume</context>
</contexts>
<marker>Velldal, Oepen, Flickinger, 2004</marker>
<rawString>Erik Velldal, Stephan Oepen, and Dan Flickinger. 2004. Paraphrasing treebanks for stochastic realization ranking. In Proceedings of the 3rd Workshop on Treebanks and Linguistic Theories (TLT), pages 149–160.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>