<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.194037">
<title confidence="0.9979375">
Modeling Machine Transliteration as a Phrase Based Statistical Machine
Translation Problem
</title>
<author confidence="0.996428">
Taraka Rama, Karthik Gali
</author>
<affiliation confidence="0.9321095">
Language Technologies Research Centre,
IIIT, Hyderabad, India.
</affiliation>
<email confidence="0.998359">
{taraka,karthikg}@students.iiit.ac.in
</email>
<sectionHeader confidence="0.997405" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998700555555556">
In this paper we use the popular phrase-
based SMT techniques for the task of
machine transliteration, for English-Hindi
language pair. Minimum error rate train-
ing has been used to learn the model
weights. We have achieved an accuracy of
46.3% on the test set. Our results show
these techniques can be successfully used
for the task of machine transliteration.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99990953125">
Transliteration can be defined as the task of tran-
scribing the words from a source script to a tar-
get script (Surana and Singh, 2008). Translitera-
tion systems find wide applications in Cross Lin-
gual Information Retrieval Systems (CLIR) and
Machine Translation (MT) systems. The systems
also find use in sentence aligners and word align-
ers (Aswani and Gaizauskas, 2005). Transcribing
the words from one language to another language
without the use of a bilingual lexicon is a chal-
lenging task as the output word produced in tar-
get language should be such that it is acceptable
to the readers of the target language. The dif-
ficulty arises due to the huge number of Out Of
Vocabulary (OOV) words which are continuously
added into the language. These OOV words in-
clude named entities, technical words, borrowed
words and loan words.
In this paper we present a technique for translit-
erating named entities from English to Hindi us-
ing a small set of training and development data.
The paper is organised as follows. A survey of the
previous work is presented in the next subsection.
Section 2 describes the problem modeling which
we have adopted from (Rama et al., 2009) which
they use for L2P task. Section 3 describes how
the parameters are tuned for optimal performance.
A brief description of the data sets is provided in
Section 4. Section 5 has the results which we have
obtained for the test data. Finally we conclude
with a summary of the methods and a analysis of
the errors.
</bodyText>
<subsectionHeader confidence="0.979872">
1.1 Previous Work
</subsectionHeader>
<bodyText confidence="0.999938333333333">
Surana and Singh (2008) propose a transliteration
system in which they use two different ways of
transliterating the named entities based on their
origin. A word is classified into two classes either
Indian or foreign using character based n-grams.
They report their results on Telugu and Hindi
data sets. Sherif and Kondrak (2007) propose a
hybrid approach in which they use the Viterbi-
based monotone search algorithm for searching
the possible candidate transliterations. Using the
approach given in (Ristad et al., 1998) the sub-
string translations are learnt. They integrate the
word-based unigram model based on (Knight and
Graehl, 1998; Al-Onaizan and Knight, 2002) with
the above model for improving the quality of
transliterations.
Malik (2006) tries to solve a special case of
transliteration for Punjabi in which they con-
vert from Shahmukhi (Arabic script) to Guru-
mukhi using a set of transliteration rules. Abdul
Jaleel (2003) show that, in the domain of informa-
tion retrieval, the cross language retrieval perfor-
mance was reduced by 50% when the name enti-
ties were not transliterated.
</bodyText>
<sectionHeader confidence="0.978698" genericHeader="method">
2 Problem Modeling
</sectionHeader>
<bodyText confidence="0.999922142857143">
Assume that given a word, represented as a se-
quence of letters of the source language s = sJ1 =
s1...sj...sJ, needs to be transcribed as a sequence
of letters in the target language, represented as t
= tI1 = t1...ti...tI. The problem of finding the best
target language letter sequence among the translit-
erated candidates can be represented as:
</bodyText>
<page confidence="0.958881">
124
</page>
<note confidence="0.9714325">
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 124–127,
Suntec, Singapore, 7 August 2009. c�2009 ACL and AFNLP
</note>
<equation confidence="0.9608375">
tbest = arg max {Pr (t  |s)} (1)
t
</equation>
<bodyText confidence="0.999826666666667">
We model the transliteration problem based on
the noisy channel model. Reformulating the above
equation using Bayes Rule:
</bodyText>
<equation confidence="0.9650555">
tbest = arg max p (s  |t) p (s) (2)
t
</equation>
<bodyText confidence="0.999902181818182">
This formulation allows for a target language
letters’ n-gram model p (t) and a transcription
model p (s  |t). Given a sequence of letters s, the
argmax function is a search function to output the
best target letter sequence.
From the above equation, the best target se-
quence is obtained based on the product of the
probabilities of transcription model and the prob-
abilities of a language model and their respective
weights. The method for obtaining the transcrip-
tion probabilities is described briefly in the next
section. Determining the best weights is necessary
for obtaining the right target language sequence.
The estimation of the models’ weights can be done
in the following manner.
The posterior probability Pr (t  |s) can also be
directly modeled using a log-linear model. In
this model, we have a set of M feature func-
tions hm(t, s), m = 1...M . For each feature
function there exists a weight or model parameter
Am, m = 1...M. Thus the posterior probability
becomes:
</bodyText>
<equation confidence="0.9895835">
Pr (t  |s) = p),M (t  |s) (3)
[ J
exp �� m=1Amhm(t, s)
= Ep1exp [gym 1Amhm(�ti,s)J (4)
</equation>
<bodyText confidence="0.999756928571429">
with the denominator, a normalization factor that
can be ignored in the maximization process.
The above modeling entails finding the suit-
able model parameters or weights which reflect the
properties of our task. We adopt the criterion fol-
lowed in (Och, 2003) for optimising the parame-
ters of the model. The details of the solution and
proof for the convergence are given in Och (2003).
The models’ weights, used for the transliteration
task, are obtained from this training.
All the above tools are available as a part of pub-
licly available MOSES (Koehn et al., 2007) tool
kit. Hence we used the tool kit for our experi-
ments.
</bodyText>
<sectionHeader confidence="0.953958" genericHeader="method">
3 Tuning the parameters
</sectionHeader>
<bodyText confidence="0.999989414634147">
The source language to target language letters
are aligned using GIZA++ (Och and Ney, 2003).
Every letter is treated as a single word for the
GIZA++ input. The alignments are then used to
learn the phrase transliteration probabilities which
are estimated using the scoring function given
in (Koehn et al., 2003).
The parameters which have a major influence
on the performance of a phrase-based SMT model
are the alignment heuristics, the maximum phrase
length (MPR) and the order of the language
model (Koehn et al., 2003). In the context of
transliteration, phrase means a sequence of let-
ters(of source and target language) mapped to each
other with some probability (i.e., the hypothesis)
and stored in a phrase table. The maximum phrase
length corresponds to the maximum number of let-
ters that a hypothesis can contain. Higher phrase
length corresponds a larger phrase table during de-
coding.
We have conducted experiments to see which
combination gives the best output. We initially
trained the model with various parameters on the
training data and tested for various values of the
above parameters. We varied the maximum phrase
length from 2 to 7. The language model was
trained using SRILM toolkit (Stolcke, 2002). We
varied the order of language model from 2 to 8.
We also traversed the alignment heuristics spec-
trum, from the parsimonious intersect at one end
of the spectrum through grow, grow-diag, grow-
diag-final, grow-diag-final-and and srctotrg to the
most lenient union at the other end.
We observed that the best results were obtained
when the language model was trained on 7-gram
and the alignment heuristic was grow-diag-final.
No significant improvement was observed in the
results when the value of MPR was greater than 7.
We have done post-processing and taken care such
that the alignments are always monotonic and no
letter was left unlinked.
</bodyText>
<sectionHeader confidence="0.995447" genericHeader="method">
4 Data Sets
</sectionHeader>
<bodyText confidence="0.999983428571429">
We have used the data sets provided by organis-
ers of the NEWS 2009 Machine Transliteration
Shared Task (Kumaran and Kellner, 2007). Prior
to the release of the test data only the training data
and development data was available. The training
data and development data consisted of a parallel
corpus having entries in both English and Hindi.
</bodyText>
<page confidence="0.997317">
125
</page>
<bodyText confidence="0.999981230769231">
The training data and development data had 9975
entries and 974 entries respectively. We used the
training data given as a part of the shared task
for generating the phrase table and the language
model. For tuning the parameters mentioned in the
previous section, we used the development data.
From the training and development data we
have observed that the words can be roughly di-
vided into following categories, Persian, European
(primarily English), Indian, Arabic words, based
on their origin. The test data consisted of 1000 en-
tries. We proceeded to experiment with the test set
once the set was released.
</bodyText>
<sectionHeader confidence="0.998923" genericHeader="evaluation">
5 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999179333333333">
The parameters described in Section 3 were the
initial settings of the system. The system was
tuned on the development set, as described in
Section 2, for obtaining the appropriate model
weights. The system tuned on the development
data was used to test it against the test data set.
We have obtained the following model weights.
The other features available in the translation sys-
tem such as word penalty, phrase penalty donot
account in the transliteration task and hence were
not included.
language model = 0.099
translation model = 0.122
Prior to the release of the test data, we tested the
system without tuning on development data. The
default model weights were used to test our sys-
tem on the development data. In the next step the
model weights were obtained by tuning the sys-
tem. Although the system allows for a distortion
model, allowing for phrase movements, we did not
use the distortion model as distortion is meaning-
less in the domain of transliteration. The following
measures such as Word Accuracy (ACC), Mean F-
Score, Mean Reciprocal Rank (MRR), MAPTef,
MAP10, MAPsys were used to evaluate our sys-
tem performance. A detailed description of each
measure is available in (Li et al., 2009).
</bodyText>
<table confidence="0.997111285714286">
Measure Result
ACC 0.463
Mean F-Score 0.876
MRR 0.573
MAP,.ef 0.454
MAPIO 0.201
MAP3,3 0.201
</table>
<tableCaption confidence="0.9533595">
Table 1: Evaluation of Various Measures on Test
Data
</tableCaption>
<sectionHeader confidence="0.988245" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.996745866666667">
In this paper we show that we can use the pop-
ular phrase based SMT systems successfully for
the task of transliteration. The publicly available
tool GIZA++ was used to align the letters. Then
the phrases were extracted and counted and stored
in phrase tables. The weights were estimated us-
ing minimum error rate training as described ear-
lier using development data. Then beam-search
based decoder was used to transliterate the English
words into Hindi. After the release of the refer-
ence corpora we examined the error results and
observed that majority of the errors resulted in the
case of the foreign origin words. We provide some
examples of the foreign origin words which were
transliterated erroneously.
</bodyText>
<figureCaption confidence="0.847732">
Figure 1: Error Transliterations of Some Foreign
Origin Words
</figureCaption>
<sectionHeader confidence="0.996771" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99676325">
N. AbdulJaleel and L.S. Larkey. 2003. Statistical
transliteration for english-arabic cross language in-
formation retrieval.
Y. Al-Onaizan and K. Knight. 2002. Machine translit-
eration of names in Arabic text. In Proceedings of
the ACL-02 workshop on Computational approaches
to semitic languages, pages 1–13. Association for
Computational Linguistics Morristown, NJ, USA.
N. Aswani and R. Gaizauskas. 2005. A hybrid ap-
proach to align sentences and words in English-
Hindi parallel corpora. Building and Using Paral-
lel Texts: Data-Driven Machine Translation and Be-
yond, page 57.
K. Knight and J. Graehl. 1998. Machine translitera-
tion. Computational Linguistics, 24(4):599–612.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proceedings of the 2003
Conference of the NAACL:HLT-Volume 1, pages 48–
54. ACL Morristown, NJ, USA.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, et al. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In ACL, volume 45, page 2.
</reference>
<page confidence="0.995562">
126
</page>
<reference confidence="0.945738409090909">
A. Kumaran and T. Kellner. 2007. A generic frame-
work for machine transliteration. In Proceedings
of the 30th annual international ACM SIGIR con-
ference on Research and development in informa-
tion retrieval, pages 721–722. ACM New York, NY,
USA.
H. Li, A. Kumaran, M. Zhang, and V. Pervouch-
ine. 2009. Whitepaper of NEWS 2009 Machine
Transliteration Shared Task. In Proceedings ofACL-
IJCNLP 2009 Named Entities Workshop (NEWS
2009). ACL, Singapore, 2009.
M.G.A. Malik. 2006. Punjabi machine transliteration.
In Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 1137–1144. Association for Compu-
tational Linguistics Morristown, NJ, USA.
F.J. Och and H. Ney. 2003. A Systematic Comparison
of Various Statistical Alignment Models. Computa-
tional Linguistics, 29(1):19–51.
F.J. Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proceedings of the 41st
Annual Meeting on ACL-Volume 1, pages 160–167.
ACL, Morristown, NJ, USA.
T. Rama, A.K. Singh, and S. Kolachina. 2009. Model-
ing letter to phoneme conversion as a phrase based
statistical machine translation problem with mini-
mum error rate training. In The NAACL Student Re-
search Workshop, Boulder, Colorado.
ES Ristad, PN Yianilos, M.T. Inc, and NJ Princeton.
1998. Learning string-edit distance. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
20(5):522–532.
T. Sherif and G. Kondrak. 2007. Substring-
based transliteration. In ANNUAL MEETING-
ASSOCIATION FOR COMPUTATIONAL LIN-
GUISTICS, volume 45, page 944.
A. Stolcke. 2002. Srilm – an extensible language mod-
eling toolkit.
H. Surana and A.K. Singh. 2008. A more discern-
ing and adaptable multilingual transliteration mech-
anism for indian languages. In Proceedings of
the Third International Joint Conference on Natural
Language Processing.
</reference>
<page confidence="0.997187">
127
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.771257">
<title confidence="0.997802">Modeling Machine Transliteration as a Phrase Based Statistical Machine Translation Problem</title>
<author confidence="0.999192">Taraka Rama</author>
<author confidence="0.999192">Karthik</author>
<affiliation confidence="0.979858">Language Technologies Research</affiliation>
<address confidence="0.793955">IIIT, Hyderabad,</address>
<abstract confidence="0.99865">In this paper we use the popular phrasebased SMT techniques for the task of machine transliteration, for English-Hindi language pair. Minimum error rate training has been used to learn the model weights. We have achieved an accuracy of 46.3% on the test set. Our results show these techniques can be successfully used for the task of machine transliteration.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N AbdulJaleel</author>
<author>L S Larkey</author>
</authors>
<title>Statistical transliteration for english-arabic cross language information retrieval.</title>
<date>2003</date>
<marker>AbdulJaleel, Larkey, 2003</marker>
<rawString>N. AbdulJaleel and L.S. Larkey. 2003. Statistical transliteration for english-arabic cross language information retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Al-Onaizan</author>
<author>K Knight</author>
</authors>
<title>Machine transliteration of names in Arabic text.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 workshop on Computational approaches to semitic languages,</booktitle>
<pages>1--13</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="2768" citStr="Al-Onaizan and Knight, 2002" startWordPosition="445" endWordPosition="448">eration system in which they use two different ways of transliterating the named entities based on their origin. A word is classified into two classes either Indian or foreign using character based n-grams. They report their results on Telugu and Hindi data sets. Sherif and Kondrak (2007) propose a hybrid approach in which they use the Viterbibased monotone search algorithm for searching the possible candidate transliterations. Using the approach given in (Ristad et al., 1998) the substring translations are learnt. They integrate the word-based unigram model based on (Knight and Graehl, 1998; Al-Onaizan and Knight, 2002) with the above model for improving the quality of transliterations. Malik (2006) tries to solve a special case of transliteration for Punjabi in which they convert from Shahmukhi (Arabic script) to Gurumukhi using a set of transliteration rules. Abdul Jaleel (2003) show that, in the domain of information retrieval, the cross language retrieval performance was reduced by 50% when the name entities were not transliterated. 2 Problem Modeling Assume that given a word, represented as a sequence of letters of the source language s = sJ1 = s1...sj...sJ, needs to be transcribed as a sequence of lett</context>
</contexts>
<marker>Al-Onaizan, Knight, 2002</marker>
<rawString>Y. Al-Onaizan and K. Knight. 2002. Machine transliteration of names in Arabic text. In Proceedings of the ACL-02 workshop on Computational approaches to semitic languages, pages 1–13. Association for Computational Linguistics Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Aswani</author>
<author>R Gaizauskas</author>
</authors>
<title>A hybrid approach to align sentences and words in EnglishHindi parallel corpora. Building and Using Parallel Texts: Data-Driven Machine Translation and Beyond,</title>
<date>2005</date>
<pages>57</pages>
<contexts>
<context position="969" citStr="Aswani and Gaizauskas, 2005" startWordPosition="142" endWordPosition="145">indi language pair. Minimum error rate training has been used to learn the model weights. We have achieved an accuracy of 46.3% on the test set. Our results show these techniques can be successfully used for the task of machine transliteration. 1 Introduction Transliteration can be defined as the task of transcribing the words from a source script to a target script (Surana and Singh, 2008). Transliteration systems find wide applications in Cross Lingual Information Retrieval Systems (CLIR) and Machine Translation (MT) systems. The systems also find use in sentence aligners and word aligners (Aswani and Gaizauskas, 2005). Transcribing the words from one language to another language without the use of a bilingual lexicon is a challenging task as the output word produced in target language should be such that it is acceptable to the readers of the target language. The difficulty arises due to the huge number of Out Of Vocabulary (OOV) words which are continuously added into the language. These OOV words include named entities, technical words, borrowed words and loan words. In this paper we present a technique for transliterating named entities from English to Hindi using a small set of training and development</context>
</contexts>
<marker>Aswani, Gaizauskas, 2005</marker>
<rawString>N. Aswani and R. Gaizauskas. 2005. A hybrid approach to align sentences and words in EnglishHindi parallel corpora. Building and Using Parallel Texts: Data-Driven Machine Translation and Beyond, page 57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
<author>J Graehl</author>
</authors>
<date>1998</date>
<booktitle>Machine transliteration. Computational Linguistics,</booktitle>
<pages>24--4</pages>
<contexts>
<context position="2738" citStr="Knight and Graehl, 1998" startWordPosition="441" endWordPosition="444">(2008) propose a transliteration system in which they use two different ways of transliterating the named entities based on their origin. A word is classified into two classes either Indian or foreign using character based n-grams. They report their results on Telugu and Hindi data sets. Sherif and Kondrak (2007) propose a hybrid approach in which they use the Viterbibased monotone search algorithm for searching the possible candidate transliterations. Using the approach given in (Ristad et al., 1998) the substring translations are learnt. They integrate the word-based unigram model based on (Knight and Graehl, 1998; Al-Onaizan and Knight, 2002) with the above model for improving the quality of transliterations. Malik (2006) tries to solve a special case of transliteration for Punjabi in which they convert from Shahmukhi (Arabic script) to Gurumukhi using a set of transliteration rules. Abdul Jaleel (2003) show that, in the domain of information retrieval, the cross language retrieval performance was reduced by 50% when the name entities were not transliterated. 2 Problem Modeling Assume that given a word, represented as a sequence of letters of the source language s = sJ1 = s1...sj...sJ, needs to be tra</context>
</contexts>
<marker>Knight, Graehl, 1998</marker>
<rawString>K. Knight and J. Graehl. 1998. Machine transliteration. Computational Linguistics, 24(4):599–612.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the</booktitle>
<volume>1</volume>
<pages>48--54</pages>
<publisher>ACL</publisher>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="5914" citStr="Koehn et al., 2003" startWordPosition="985" endWordPosition="988">or the convergence are given in Och (2003). The models’ weights, used for the transliteration task, are obtained from this training. All the above tools are available as a part of publicly available MOSES (Koehn et al., 2007) tool kit. Hence we used the tool kit for our experiments. 3 Tuning the parameters The source language to target language letters are aligned using GIZA++ (Och and Ney, 2003). Every letter is treated as a single word for the GIZA++ input. The alignments are then used to learn the phrase transliteration probabilities which are estimated using the scoring function given in (Koehn et al., 2003). The parameters which have a major influence on the performance of a phrase-based SMT model are the alignment heuristics, the maximum phrase length (MPR) and the order of the language model (Koehn et al., 2003). In the context of transliteration, phrase means a sequence of letters(of source and target language) mapped to each other with some probability (i.e., the hypothesis) and stored in a phrase table. The maximum phrase length corresponds to the maximum number of letters that a hypothesis can contain. Higher phrase length corresponds a larger phrase table during decoding. We have conducte</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn, F.J. Och, and D. Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Conference of the NAACL:HLT-Volume 1, pages 48– 54. ACL Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
</authors>
<title>Moses: Open Source Toolkit for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In ACL,</booktitle>
<volume>45</volume>
<pages>2</pages>
<contexts>
<context position="5520" citStr="Koehn et al., 2007" startWordPosition="918" endWordPosition="921">xp �� m=1Amhm(t, s) = Ep1exp [gym 1Amhm(�ti,s)J (4) with the denominator, a normalization factor that can be ignored in the maximization process. The above modeling entails finding the suitable model parameters or weights which reflect the properties of our task. We adopt the criterion followed in (Och, 2003) for optimising the parameters of the model. The details of the solution and proof for the convergence are given in Och (2003). The models’ weights, used for the transliteration task, are obtained from this training. All the above tools are available as a part of publicly available MOSES (Koehn et al., 2007) tool kit. Hence we used the tool kit for our experiments. 3 Tuning the parameters The source language to target language letters are aligned using GIZA++ (Och and Ney, 2003). Every letter is treated as a single word for the GIZA++ input. The alignments are then used to learn the phrase transliteration probabilities which are estimated using the scoring function given in (Koehn et al., 2003). The parameters which have a major influence on the performance of a phrase-based SMT model are the alignment heuristics, the maximum phrase length (MPR) and the order of the language model (Koehn et al., </context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, et al. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In ACL, volume 45, page 2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kumaran</author>
<author>T Kellner</author>
</authors>
<title>A generic framework for machine transliteration.</title>
<date>2007</date>
<booktitle>In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>721--722</pages>
<publisher>ACM</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="7610" citStr="Kumaran and Kellner, 2007" startWordPosition="1263" endWordPosition="1266">um through grow, grow-diag, growdiag-final, grow-diag-final-and and srctotrg to the most lenient union at the other end. We observed that the best results were obtained when the language model was trained on 7-gram and the alignment heuristic was grow-diag-final. No significant improvement was observed in the results when the value of MPR was greater than 7. We have done post-processing and taken care such that the alignments are always monotonic and no letter was left unlinked. 4 Data Sets We have used the data sets provided by organisers of the NEWS 2009 Machine Transliteration Shared Task (Kumaran and Kellner, 2007). Prior to the release of the test data only the training data and development data was available. The training data and development data consisted of a parallel corpus having entries in both English and Hindi. 125 The training data and development data had 9975 entries and 974 entries respectively. We used the training data given as a part of the shared task for generating the phrase table and the language model. For tuning the parameters mentioned in the previous section, we used the development data. From the training and development data we have observed that the words can be roughly divid</context>
</contexts>
<marker>Kumaran, Kellner, 2007</marker>
<rawString>A. Kumaran and T. Kellner. 2007. A generic framework for machine transliteration. In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, pages 721–722. ACM New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Li</author>
<author>A Kumaran</author>
<author>M Zhang</author>
<author>V Pervouchine</author>
</authors>
<title>Machine Transliteration Shared Task.</title>
<date>2009</date>
<journal>Whitepaper of NEWS</journal>
<booktitle>In Proceedings ofACLIJCNLP 2009 Named Entities Workshop (NEWS</booktitle>
<publisher>ACL,</publisher>
<contexts>
<context position="9666" citStr="Li et al., 2009" startWordPosition="1608" endWordPosition="1611">ted the system without tuning on development data. The default model weights were used to test our system on the development data. In the next step the model weights were obtained by tuning the system. Although the system allows for a distortion model, allowing for phrase movements, we did not use the distortion model as distortion is meaningless in the domain of transliteration. The following measures such as Word Accuracy (ACC), Mean FScore, Mean Reciprocal Rank (MRR), MAPTef, MAP10, MAPsys were used to evaluate our system performance. A detailed description of each measure is available in (Li et al., 2009). Measure Result ACC 0.463 Mean F-Score 0.876 MRR 0.573 MAP,.ef 0.454 MAPIO 0.201 MAP3,3 0.201 Table 1: Evaluation of Various Measures on Test Data 6 Conclusion In this paper we show that we can use the popular phrase based SMT systems successfully for the task of transliteration. The publicly available tool GIZA++ was used to align the letters. Then the phrases were extracted and counted and stored in phrase tables. The weights were estimated using minimum error rate training as described earlier using development data. Then beam-search based decoder was used to transliterate the English word</context>
</contexts>
<marker>Li, Kumaran, Zhang, Pervouchine, 2009</marker>
<rawString>H. Li, A. Kumaran, M. Zhang, and V. Pervouchine. 2009. Whitepaper of NEWS 2009 Machine Transliteration Shared Task. In Proceedings ofACLIJCNLP 2009 Named Entities Workshop (NEWS 2009). ACL, Singapore, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M G A Malik</author>
</authors>
<title>Punjabi machine transliteration.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>1137--1144</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="2849" citStr="Malik (2006)" startWordPosition="459" endWordPosition="460">n their origin. A word is classified into two classes either Indian or foreign using character based n-grams. They report their results on Telugu and Hindi data sets. Sherif and Kondrak (2007) propose a hybrid approach in which they use the Viterbibased monotone search algorithm for searching the possible candidate transliterations. Using the approach given in (Ristad et al., 1998) the substring translations are learnt. They integrate the word-based unigram model based on (Knight and Graehl, 1998; Al-Onaizan and Knight, 2002) with the above model for improving the quality of transliterations. Malik (2006) tries to solve a special case of transliteration for Punjabi in which they convert from Shahmukhi (Arabic script) to Gurumukhi using a set of transliteration rules. Abdul Jaleel (2003) show that, in the domain of information retrieval, the cross language retrieval performance was reduced by 50% when the name entities were not transliterated. 2 Problem Modeling Assume that given a word, represented as a sequence of letters of the source language s = sJ1 = s1...sj...sJ, needs to be transcribed as a sequence of letters in the target language, represented as t = tI1 = t1...ti...tI. The problem of</context>
</contexts>
<marker>Malik, 2006</marker>
<rawString>M.G.A. Malik. 2006. Punjabi machine transliteration. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 1137–1144. Association for Computational Linguistics Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="5694" citStr="Och and Ney, 2003" startWordPosition="949" endWordPosition="952">nding the suitable model parameters or weights which reflect the properties of our task. We adopt the criterion followed in (Och, 2003) for optimising the parameters of the model. The details of the solution and proof for the convergence are given in Och (2003). The models’ weights, used for the transliteration task, are obtained from this training. All the above tools are available as a part of publicly available MOSES (Koehn et al., 2007) tool kit. Hence we used the tool kit for our experiments. 3 Tuning the parameters The source language to target language letters are aligned using GIZA++ (Och and Ney, 2003). Every letter is treated as a single word for the GIZA++ input. The alignments are then used to learn the phrase transliteration probabilities which are estimated using the scoring function given in (Koehn et al., 2003). The parameters which have a major influence on the performance of a phrase-based SMT model are the alignment heuristics, the maximum phrase length (MPR) and the order of the language model (Koehn et al., 2003). In the context of transliteration, phrase means a sequence of letters(of source and target language) mapped to each other with some probability (i.e., the hypothesis) </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F.J. Och and H. Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on ACL-Volume 1,</booktitle>
<pages>160--167</pages>
<publisher>ACL,</publisher>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="5211" citStr="Och, 2003" startWordPosition="866" endWordPosition="867">ility Pr (t |s) can also be directly modeled using a log-linear model. In this model, we have a set of M feature functions hm(t, s), m = 1...M . For each feature function there exists a weight or model parameter Am, m = 1...M. Thus the posterior probability becomes: Pr (t |s) = p),M (t |s) (3) [ J exp �� m=1Amhm(t, s) = Ep1exp [gym 1Amhm(�ti,s)J (4) with the denominator, a normalization factor that can be ignored in the maximization process. The above modeling entails finding the suitable model parameters or weights which reflect the properties of our task. We adopt the criterion followed in (Och, 2003) for optimising the parameters of the model. The details of the solution and proof for the convergence are given in Och (2003). The models’ weights, used for the transliteration task, are obtained from this training. All the above tools are available as a part of publicly available MOSES (Koehn et al., 2007) tool kit. Hence we used the tool kit for our experiments. 3 Tuning the parameters The source language to target language letters are aligned using GIZA++ (Och and Ney, 2003). Every letter is treated as a single word for the GIZA++ input. The alignments are then used to learn the phrase tra</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F.J. Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting on ACL-Volume 1, pages 160–167. ACL, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Rama</author>
<author>A K Singh</author>
<author>S Kolachina</author>
</authors>
<title>Modeling letter to phoneme conversion as a phrase based statistical machine translation problem with minimum error rate training.</title>
<date>2009</date>
<booktitle>In The NAACL Student Research Workshop,</booktitle>
<location>Boulder, Colorado.</location>
<contexts>
<context position="1765" citStr="Rama et al., 2009" startWordPosition="281" endWordPosition="284">be such that it is acceptable to the readers of the target language. The difficulty arises due to the huge number of Out Of Vocabulary (OOV) words which are continuously added into the language. These OOV words include named entities, technical words, borrowed words and loan words. In this paper we present a technique for transliterating named entities from English to Hindi using a small set of training and development data. The paper is organised as follows. A survey of the previous work is presented in the next subsection. Section 2 describes the problem modeling which we have adopted from (Rama et al., 2009) which they use for L2P task. Section 3 describes how the parameters are tuned for optimal performance. A brief description of the data sets is provided in Section 4. Section 5 has the results which we have obtained for the test data. Finally we conclude with a summary of the methods and a analysis of the errors. 1.1 Previous Work Surana and Singh (2008) propose a transliteration system in which they use two different ways of transliterating the named entities based on their origin. A word is classified into two classes either Indian or foreign using character based n-grams. They report their </context>
</contexts>
<marker>Rama, Singh, Kolachina, 2009</marker>
<rawString>T. Rama, A.K. Singh, and S. Kolachina. 2009. Modeling letter to phoneme conversion as a phrase based statistical machine translation problem with minimum error rate training. In The NAACL Student Research Workshop, Boulder, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>ES Ristad</author>
<author>PN Yianilos</author>
<author>M T Inc</author>
<author>NJ Princeton</author>
</authors>
<title>Learning string-edit distance.</title>
<date>1998</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>20</volume>
<issue>5</issue>
<contexts>
<context position="2621" citStr="Ristad et al., 1998" startWordPosition="423" endWordPosition="426">inally we conclude with a summary of the methods and a analysis of the errors. 1.1 Previous Work Surana and Singh (2008) propose a transliteration system in which they use two different ways of transliterating the named entities based on their origin. A word is classified into two classes either Indian or foreign using character based n-grams. They report their results on Telugu and Hindi data sets. Sherif and Kondrak (2007) propose a hybrid approach in which they use the Viterbibased monotone search algorithm for searching the possible candidate transliterations. Using the approach given in (Ristad et al., 1998) the substring translations are learnt. They integrate the word-based unigram model based on (Knight and Graehl, 1998; Al-Onaizan and Knight, 2002) with the above model for improving the quality of transliterations. Malik (2006) tries to solve a special case of transliteration for Punjabi in which they convert from Shahmukhi (Arabic script) to Gurumukhi using a set of transliteration rules. Abdul Jaleel (2003) show that, in the domain of information retrieval, the cross language retrieval performance was reduced by 50% when the name entities were not transliterated. 2 Problem Modeling Assume t</context>
</contexts>
<marker>Ristad, Yianilos, Inc, Princeton, 1998</marker>
<rawString>ES Ristad, PN Yianilos, M.T. Inc, and NJ Princeton. 1998. Learning string-edit distance. IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(5):522–532.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Sherif</author>
<author>G Kondrak</author>
</authors>
<title>Substringbased transliteration.</title>
<date>2007</date>
<booktitle>In ANNUAL MEETINGASSOCIATION FOR COMPUTATIONAL LINGUISTICS,</booktitle>
<volume>45</volume>
<pages>944</pages>
<contexts>
<context position="2429" citStr="Sherif and Kondrak (2007)" startWordPosition="394" endWordPosition="397"> describes how the parameters are tuned for optimal performance. A brief description of the data sets is provided in Section 4. Section 5 has the results which we have obtained for the test data. Finally we conclude with a summary of the methods and a analysis of the errors. 1.1 Previous Work Surana and Singh (2008) propose a transliteration system in which they use two different ways of transliterating the named entities based on their origin. A word is classified into two classes either Indian or foreign using character based n-grams. They report their results on Telugu and Hindi data sets. Sherif and Kondrak (2007) propose a hybrid approach in which they use the Viterbibased monotone search algorithm for searching the possible candidate transliterations. Using the approach given in (Ristad et al., 1998) the substring translations are learnt. They integrate the word-based unigram model based on (Knight and Graehl, 1998; Al-Onaizan and Knight, 2002) with the above model for improving the quality of transliterations. Malik (2006) tries to solve a special case of transliteration for Punjabi in which they convert from Shahmukhi (Arabic script) to Gurumukhi using a set of transliteration rules. Abdul Jaleel (</context>
</contexts>
<marker>Sherif, Kondrak, 2007</marker>
<rawString>T. Sherif and G. Kondrak. 2007. Substringbased transliteration. In ANNUAL MEETINGASSOCIATION FOR COMPUTATIONAL LINGUISTICS, volume 45, page 944.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>Srilm – an extensible language modeling toolkit.</title>
<date>2002</date>
<contexts>
<context position="6822" citStr="Stolcke, 2002" startWordPosition="1135" endWordPosition="1136">nd target language) mapped to each other with some probability (i.e., the hypothesis) and stored in a phrase table. The maximum phrase length corresponds to the maximum number of letters that a hypothesis can contain. Higher phrase length corresponds a larger phrase table during decoding. We have conducted experiments to see which combination gives the best output. We initially trained the model with various parameters on the training data and tested for various values of the above parameters. We varied the maximum phrase length from 2 to 7. The language model was trained using SRILM toolkit (Stolcke, 2002). We varied the order of language model from 2 to 8. We also traversed the alignment heuristics spectrum, from the parsimonious intersect at one end of the spectrum through grow, grow-diag, growdiag-final, grow-diag-final-and and srctotrg to the most lenient union at the other end. We observed that the best results were obtained when the language model was trained on 7-gram and the alignment heuristic was grow-diag-final. No significant improvement was observed in the results when the value of MPR was greater than 7. We have done post-processing and taken care such that the alignments are alwa</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. Srilm – an extensible language modeling toolkit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Surana</author>
<author>A K Singh</author>
</authors>
<title>A more discerning and adaptable multilingual transliteration mechanism for indian languages.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third International Joint Conference on Natural Language Processing.</booktitle>
<contexts>
<context position="734" citStr="Surana and Singh, 2008" startWordPosition="107" endWordPosition="110"> Gali Language Technologies Research Centre, IIIT, Hyderabad, India. {taraka,karthikg}@students.iiit.ac.in Abstract In this paper we use the popular phrasebased SMT techniques for the task of machine transliteration, for English-Hindi language pair. Minimum error rate training has been used to learn the model weights. We have achieved an accuracy of 46.3% on the test set. Our results show these techniques can be successfully used for the task of machine transliteration. 1 Introduction Transliteration can be defined as the task of transcribing the words from a source script to a target script (Surana and Singh, 2008). Transliteration systems find wide applications in Cross Lingual Information Retrieval Systems (CLIR) and Machine Translation (MT) systems. The systems also find use in sentence aligners and word aligners (Aswani and Gaizauskas, 2005). Transcribing the words from one language to another language without the use of a bilingual lexicon is a challenging task as the output word produced in target language should be such that it is acceptable to the readers of the target language. The difficulty arises due to the huge number of Out Of Vocabulary (OOV) words which are continuously added into the la</context>
<context position="2121" citStr="Surana and Singh (2008)" startWordPosition="345" endWordPosition="348">ies from English to Hindi using a small set of training and development data. The paper is organised as follows. A survey of the previous work is presented in the next subsection. Section 2 describes the problem modeling which we have adopted from (Rama et al., 2009) which they use for L2P task. Section 3 describes how the parameters are tuned for optimal performance. A brief description of the data sets is provided in Section 4. Section 5 has the results which we have obtained for the test data. Finally we conclude with a summary of the methods and a analysis of the errors. 1.1 Previous Work Surana and Singh (2008) propose a transliteration system in which they use two different ways of transliterating the named entities based on their origin. A word is classified into two classes either Indian or foreign using character based n-grams. They report their results on Telugu and Hindi data sets. Sherif and Kondrak (2007) propose a hybrid approach in which they use the Viterbibased monotone search algorithm for searching the possible candidate transliterations. Using the approach given in (Ristad et al., 1998) the substring translations are learnt. They integrate the word-based unigram model based on (Knight</context>
</contexts>
<marker>Surana, Singh, 2008</marker>
<rawString>H. Surana and A.K. Singh. 2008. A more discerning and adaptable multilingual transliteration mechanism for indian languages. In Proceedings of the Third International Joint Conference on Natural Language Processing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>