<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.9990835">
A Note on the Implementation of
Hierarchical Dirichlet Processes
</title>
<author confidence="0.897032">
Phil Blunsom*
</author>
<email confidence="0.785371">
pblunsom@inf.ed.ac.uk
</email>
<author confidence="0.69212">
Sharon Goldwater*
</author>
<affiliation confidence="0.829474">
sgwater@inf.ed.ac.uk
*Department of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.9142045">
Edinburgh, EH8 9AB, UK
Trevor Cohn*
</address>
<email confidence="0.976922">
tcohn@inf.ed.ac.uk
</email>
<author confidence="0.772867">
Mark Johnson†
</author>
<email confidence="0.609424">
mark johnson@brown.edu
</email>
<affiliation confidence="0.980634">
†Department of Cognitive and Linguistic Sciences
Brown University
</affiliation>
<address confidence="0.67233">
Providence, RI, USA
</address>
<sectionHeader confidence="0.954785" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999938125">
The implementation of collapsed Gibbs
samplers for non-parametric Bayesian
models is non-trivial, requiring con-
siderable book-keeping. Goldwater et
al. (2006a) presented an approximation
which significantly reduces the storage
and computation overhead, but we show
here that their formulation was incorrect
and, even after correction, is grossly inac-
curate. We present an alternative formula-
tion which is exact and can be computed
easily. However this approach does not
work for hierarchical models, for which
case we present an efficient data structure
which has a better space complexity than
the naive approach.
</bodyText>
<sectionHeader confidence="0.998782" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999963">
Unsupervised learning of natural language is one
of the most challenging areas in NLP. Recently,
methods from nonparametric Bayesian statistics
have been gaining popularity as a way to approach
unsupervised learning for a variety of tasks,
including language modeling, word and mor-
pheme segmentation, parsing, and machine trans-
lation (Teh et al., 2006; Goldwater et al., 2006a;
Goldwater et al., 2006b; Liang et al., 2007; Finkel
et al., 2007; DeNero et al., 2008). These mod-
els are often based on the Dirichlet process (DP)
(Ferguson, 1973) or hierarchical Dirichlet process
(HDP) (Teh et al., 2006), with Gibbs sampling
as a method of inference. Exact implementation
of such sampling methods requires considerable
bookkeeping of various counts, which motivated
Goldwater et al. (2006a) (henceforth, GGJ06) to
develop an approximation using expected counts.
However, we show here that their approximation
is flawed in two respects: 1) It omits an impor-
tant factor in the expectation, and 2) Even after
correction, the approximation is poor for hierar-
chical models, which are commonly used for NLP
applications. We derive an improved O(1) formula
that gives exact values for the expected counts in
non-hierarchical models. For hierarchical models,
where our formula is not exact, we present an
efficient method for sampling from the HDP (and
related models, such as the hierarchical Pitman-
Yor process) that considerably decreases the mem-
ory footprint of such models as compared to the
naive implementation.
As we have noted, the issues described in this
paper apply to models for various kinds of NLP
tasks; for concreteness, we will focus on n-gram
language modeling for the remainder of the paper,
closely following the presentation in GGJ06.
</bodyText>
<sectionHeader confidence="0.970321" genericHeader="method">
2 The Chinese Restaurant Process
</sectionHeader>
<bodyText confidence="0.999585">
GGJ06 present two nonparametric Bayesian lan-
guage models: a DP unigram model and an HDP
bigram model. Under the DP model, words in a
corpus w = w1 ... wn are generated as follows:
</bodyText>
<equation confidence="0.9990485">
Gjα0, P0 — DP(α0, P0)
wijG — G
</equation>
<bodyText confidence="0.99980225">
where G is a distribution over an infinite set of
possible words, P0 (the base distribution of the
DP) determines the probability that an item will
be in the support of G, and α0 (the concentration
parameter) determines the variance of G.
One way of understanding the predictions that
the DP model makes is through the Chinese restau-
rant process (CRP) (Aldous, 1985). In the CRP,
customers (word tokens wi) enter a restaurant with
an infinite number of tables and choose a seat. The
table chosen by the ith customer, zi, follows the
distribution:
</bodyText>
<equation confidence="0.88433675">
� nz−i
P(zi = kjz−i) = i−1+ao , 0 &lt; k &lt; K(z−i)
i−1+α0, k = K(z−i)
α0
</equation>
<page confidence="0.978915">
337
</page>
<note confidence="0.9798825">
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 337–340,
Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP
</note>
<figureCaption confidence="0.92318625">
Figure 1. A seating assignment describing the state of
a unigram CRP. Letters and numbers uniquely identify
customers and tables. Note that multiple tables may
share a label.
</figureCaption>
<bodyText confidence="0.999791153846154">
where z−i = z1 ... zi−1 are the table assignments
of the previous customers, nz−i
k is the number of
customers at table k in z−i, and K(z−i) is the total
number of occupied tables. If we further assume
that table k is labeled with a word type Ek drawn
from P0, then the assignment of tokens to tables
defines a distribution over words, with wi = zi.
See Figure 1 for an example seating arrangement.
Using this model, the predictive probability of
wi, conditioned on the previous words, can be
found by summing over possible seating assign-
ments for wi, and is given by
</bodyText>
<equation confidence="0.767399">
i − 1 + α0
</equation>
<bodyText confidence="0.999411333333334">
This prediction turns out to be exactly that of the
DP model after integrating out the distribution G.
Note that as long as the base distribution P0 is
fixed, predictions do not depend on the seating
arrangement z−i, only on the count of word w
in the previously observed words (nw−i
w ). How-
ever, in many situations, we may wish to estimate
the base distribution itself, creating a hierarchical
model. Since the base distribution generates table
labels, estimates of this distribution are based on
the counts of those labels, i.e., the number of tables
associated with each word type.
An example of such a hierarchical model is the
HDP bigram model of GGJ06, in which each word
type w is associated with its own restaurant, where
customers in that restaurant correspond to words
that follow w in the corpus. All the bigram restau-
rants share a common base distribution P1 over
unigrams, which must be inferred. Predictions in
this model are as follows:
</bodyText>
<figure confidence="0.517477">
Word frequency (nw)
</figure>
<figureCaption confidence="0.991001">
Figure 2. Comparison of several methods of approx-
</figureCaption>
<bodyText confidence="0.80295992">
imating the number of tables occupied by words of
different frequencies. For each method, results using
α = {100, 1000, 10000, 1000001 are shown (from bottom
to top). Solid lines show the expected number of tables,
computed using (3) and assuming P1 is a fixed uni-
form distribution over a finite vocabulary (values com-
puted using the Digamma formulation (7) are the same).
Dashed lines show the values given by the Antoniak
approximation (4) (the line for α = 100 falls below the
bottom of the graph). Stars show the mean of empirical
table counts as computed over 1000 samples from an
MCMC sampler in which P1 is a fixed uniform distri-
bution, as in the unigram LM. Circles show the mean
of empirical table counts when P1 is inferred, as in the
bigram LM. Standard errors in both cases are no larger
than the marker size. All plots are based on the 30114-
word vocabulary and frequencies found in sections 0-20
of the WSJ corpus.
these kinds of models, the counts are constantly
changing over multiple samples, with tables going
in and out of existence frequently. This can create
significant bookkeeping issues in implementation,
and motivated GGJ06 to present a method of com-
puting approximate table counts based on word
frequencies only.
</bodyText>
<figure confidence="0.9725724">
d e f
c
b
a
h
The
1
cats
2
cats
3
meow
4
g cats
5
1 10 100 1000
Mean number of lexical entries
100
0.1
10
1
Expectation
Antoniak approx.
Empirical, fixed base
Empirical, inferred base
</figure>
<equation confidence="0.999432076923077">
P(wi = w|w−i) =
n
w−i
w + α0P0 (1)
n h−i 3 Approximating Table Counts
P2(wi|h−i) = (wi−1,wi) + α1P1(wi|h−i)
nh−i + α1
(wi−1,∗)
th−i
wi + α0P0(wi)
P1(wi|h−i) = (2)
th−i
∗ + α0
</equation>
<bodyText confidence="0.953851">
where h−i = (w−i, z−i), th−i
wi is the number of
tables labelled with wi, and th−i
∗ is the total num-
ber of occupied tables. Of particular note for our
discussion is that in order to calculate these condi-
tional distributions we must know the table assign-
ments z−i for each of the words in w−i. Moreover,
in the Gibbs samplers often used for inference in
Rather than explicitly tracking the number of
tables tw associated with each word w in their
bigram model, GGJ06 approximate the table
counts using the expectation E[tw]. Expected
counts are used in place of th−i
wi and th−i
∗ in (2).
The exact expectation, due to Antoniak (1974), is
</bodyText>
<equation confidence="0.9858612">
1
α1P1(w) + i − 1 (3)
nw
E[tw] = α1P1(w)
i=1
</equation>
<page confidence="0.971746">
338
</page>
<bodyText confidence="0.9916725">
Antoniak also gives an approximation to this
expectation:
</bodyText>
<equation confidence="0.998823">
nw + α1P1(w)
E[tw] ≈ α1P1(w) log (4)
α1P1(w)
</equation>
<bodyText confidence="0.99992">
but provides no derivation. Due to a misinterpre-
tation of Antoniak (1974), GGJ06 use an approx-
imation that leaves out all the P1(w) terms from
(4).1 Figure 2 compares the approximation to
the exact expectation when the base distribution
is fixed. The approximation is fairly good when
αP1(w) &gt; 1 (the scenario assumed by Antoniak);
however, in most NLP applications, αP1(w) &lt;
1 in order to effect a sparse prior. (We return
to the case of non-fixed based distributions in a
moment.) As an extreme case of the paucity of
this approximation consider α1P1(w) = 1 and
nw = 1 (i.e. only one customer has entered the
restaurant): clearly E[tw] should equal 1, but the
approximation gives log(2).
We now provide a derivation for (4), which will
allow us to obtain an O(1) formula for the expec-
tation in (3). First, we rewrite the summation in (3)
as a difference of fractional harmonic numbers:2
</bodyText>
<equation confidence="0.94874375">
H(α1P1(w)+nw−1) − H(α1P1(w)−1) (5)
Using the recurrence for harmonic numbers:
1
E[tw] ≈ α1P1(w) [H(α1P1(w)+nw)−
</equation>
<bodyText confidence="0.97542275">
We then use the asymptotic expansion,
HF ≈ log F + γ + 1
2F , omiting trailing terms
which are O(F−2) and smaller powers of F:3
</bodyText>
<equation confidence="0.9990865">
E[tw] ≈ α1P1(w) log nw+α1P1(w) + nw
α1P1(w) 2(α1P1(w)+nw)
</equation>
<bodyText confidence="0.9995578">
Omitting the trailing term leads to the
approximation in Antoniak (1974). However, we
can obtain an exact formula for the expecta-
tion by utilising the relationship between the
Digamma function and the harmonic numbers:
</bodyText>
<equation confidence="0.989614333333333">
0(n) = Hn−1 − γ.4 Thus we can rewrite (5) as:5
E[tw] = α1P1(w)·
[0(α1P1(w) + nw) − 0(α1P1(w))] (7)
</equation>
<bodyText confidence="0.636093">
1The authors of GGJ06 realized this error, and current
implementations of their models no longer use these approx-
imations, instead tracking table counts explicitly.
</bodyText>
<footnote confidence="0.687117625">
2Fractional harmonic numbers between 0 and 1 are given
by HF = f01
11 xX dx. All harmonic numbers follow the
recurrence HF = HF−1 + 1F .
3Here, γ is the Euler-Mascheroni constant.
4Accurate O(1) approximations of the Digamma function
are readily available.
5(7) can be derived from (3) using: 7P(x+1)−7P(x) = 1x.
</footnote>
<equation confidence="0.956842833333333">
Explicit table tracking:
customer(wi) → table(zi)
{ }
a : 1, b : 1, c : 2, d : 2, e : 3, f : 4, g : 5, h : 5
table(zi) → label(`)
{ }
1 : The, 2 : cats, 3 : cats, 4 : meow, 5 : cats
Histogram:
{ }
word type → table occupancy → frequency
{ }
The : f2 : 11, cats : f1 : 1, 2 : 21, meow : f1 : 11
</equation>
<figureCaption confidence="0.972368">
Figure 3. The explicit table tracking and histogram rep-
resentations for Figure 1.
</figureCaption>
<bodyText confidence="0.999965730769231">
A significant caveat here is that the expected
table counts given by (3) and (7) are only valid
when the base distribution is a constant. However,
in hierarchical models such as GGJ06’s bigram
model and HDP models, the base distribution is
not constant and instead must be inferred. As can
be seen in Figure 2, table counts can diverge con-
siderably from the expectations based on fixed
P1 when P1 is in fact not fixed. Thus, (7) can
be viewed as an approximation in this case, but
not necessarily an accurate one. Since knowing
the table counts is only necessary for inference
in hierarchical models, but the table counts can-
not be approximated well by any of the formu-
las presented here, we must conclude that the best
inference method is still to keep track of the actual
table counts. The naive method of doing so is to
store which table each customer in the restaurant
is seated at, incrementing and decrementing these
counts as needed during the sampling process. In
the following section, we describe an alternative
method that reduces the amount of memory neces-
sary for implementing HDPs. This method is also
appropriate for hierarchical Pitman-Yor processes,
for which no closed-form approximations to the
table counts have been proposed.
</bodyText>
<sectionHeader confidence="0.985529" genericHeader="method">
4 Efficient Implementation of HDPs
</sectionHeader>
<bodyText confidence="0.999692153846154">
As we do not have an efficient expected table
count approximation for hierarchical models we
could fall back to explicitly tracking which table
each customer that enters the restaurant sits at.
However, here we describe a more compact repre-
sentation for the state of the restaurant that doesn’t
require explicit table tracking.6 Instead we main-
tain a histogram for each dish wi of the frequency
of a table having a particular number of customers.
Figure 3 depicts the histogram and explicit repre-
sentations for the CRP state in Figure 1.
Our alternative method of inference for hierar-
chical Bayesian models takes advantage of their
</bodyText>
<footnote confidence="0.5508295">
6Teh et al. (2006) also note that the exact table assign-
ments for customers are not required for prediction.
</footnote>
<equation confidence="0.999124333333333">
α1P1(w) + nw
()
− H(α1P1(w)+nw) + 1 α1P1(w)i 6
</equation>
<page confidence="0.998637">
339
</page>
<figure confidence="0.908303512195122">
Algorithm 1 A new customer enters the restaurant
1: w: word type
2: P0w: Base probability for w
3: HDw: Seating Histogram for w
4: procedure INCREMENT(w, P0w, HDw)
w−1
nw
5: pshare ←w−1
nw +α0
α0
6: pnew←w−10w
×P
nw +α0
7: r ← random(0, pshare + pnew)
8: if r &lt; pnew or nw−1
w = 0 then
9: HDw[1] = HDw[1] + 1
10: else
� Sample from the histogram of customers at tables
11: r ← random(0, nw−1
w )
12: for c ∈ HDw do &gt; c: customer count
13: r = r − (c × HDw[c])
14: if r ≤ 0 then
15: HDw[c] = HDw[c] + 1
16: Break
17: nw w = nw−1
w + 1 &gt; Update token count
Algorithm 2 A customer leaves the restaurant
1: w: word type
2: HDw: Seating histogram for w
3: procedure DECREMENT(w, P0w, HDw)
4: r ← random(0, nww)
5: for c ∈ HDw do &gt; c: customer count
6: r = r − (c × HDw[c])
7: if r ≤ 0 then
8: HDw[c] = HDw[c] − 1
9: if c &gt; 1 then
10: HDw[c − 1] = HDw[c − 1] + 1
11: Break
12: nww = nww − 1 &gt; Update token count
</figure>
<bodyText confidence="0.999966">
exchangeability, which makes it unnecessary to
know exactly which table each customer is seated
at. The only important information is how many
tables exist with different numbers of customers,
and what their labels are. We simply maintain a
histogram for each word type w, which stores, for
each number of customers m, the number of tables
labeled with w that have m customers. Figure 3
depicts the explicit representation and histogram
for the CRP state in Figure 1.
Algorithms 1 and 2 describe the two operations
required to maintain the state of a CRP.7 When
a customer enters the restaurant (Alogrithm 1)),
we sample whether or not to open a new table.
If not, we sample an old table proportional to the
counts of how many customers are seated there
and update the histogram. When a customer leaves
the restaurant (Algorithm 2), we decrement one
of the tables at random according to the number
of customers seated there. By exchangeability, it
doesn’t actually matter which table the customer
was “really” sitting at.
</bodyText>
<footnote confidence="0.97412">
7A C++ template class that implements
the algorithm presented is made available at:
http://homepages.inf.ed.ac.uk/tcohn/
</footnote>
<sectionHeader confidence="0.991951" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999990428571429">
We’ve shown that the HDP approximation pre-
sented in GGJ06 contained errors and inappropri-
ate assumptions such that it significantly diverges
from the true expectations for the most common
scenarios encountered in NLP. As such we empha-
sise that that formulation should not be used.
Although (7) allows E[t,,,] to be calculated exactly
for constant base distributions, for hierarchical
models this is not valid and no accurate calculation
of the expectations has been proposed. As a rem-
edy we’ve presented an algorithm that efficiently
implements the true HDP without the need for
explicitly tracking customer to table assignments,
while remaining simple to implement.
</bodyText>
<sectionHeader confidence="0.997961" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.930682">
The authors would like to thank Tom Grif-
fiths for providing the code used to produce
Figure 2 and acknowledge the support of the
EPSRC (Blunsom, grant EP/D074959/1; Cohn,
grant GR/T04557/01).
</bodyText>
<sectionHeader confidence="0.998367" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999773764705882">
D. Aldous. 1985. Exchangeability and related topics. In
´Ecole d’ ´Et´e de Probabiliti´es de Saint-Flour XIII 1983, 1–
198. Springer.
C. E. Antoniak. 1974. Mixtures of dirichlet processes with
applications to bayesian nonparametric problems. The
Annals of Statistics, 2(6):1152–1174.
J. DeNero, A. Bouchard-Cˆot´e, D. Klein. 2008. Sampling
alignment structure under a Bayesian translation model.
In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, 314–323, Hon-
olulu, Hawaii. Association for Computational Linguistics.
S. Ferguson. 1973. A Bayesian analysis of some nonpara-
metric problems. Annals of Statistics, 1:209–230.
J. R. Finkel, T. Grenager, C. D. Manning. 2007. The infinite
tree. In Proc. of the 45th Annual Meeting of the ACL
(ACL-2007), Prague, Czech Republic.
S. Goldwater, T. Griffiths, M. Johnson. 2006a. Contex-
tual dependencies in unsupervised word segmentation. In
Proc. of the 44th Annual Meeting of the ACL and 21st
International Conference on Computational Linguistics
(COLING/ACL-2006), Sydney.
S. Goldwater, T. Griffiths, M. Johnson. 2006b. Interpolating
between types and tokens by estimating power-law gener-
ators. In Y. Weiss, B. Sch¨olkopf, J. Platt, eds., Advances
in Neural Information Processing Systems 18, 459–466.
MIT Press, Cambridge, MA.
P. Liang, S. Petrov, M. Jordan, D. Klein. 2007. The infinite
PCFG using hierarchical Dirichlet processes. In Proc. of
the 2007 Conference on Empirical Methods in Natural
Language Processing (EMNLP-2007), 688–697, Prague,
Czech Republic.
Y. W. Teh, M. I. Jordan, M. J. Beal, D. M. Blei. 2006.
Hierarchical Dirichlet processes. Journal of the American
Statistical Association, 101(476):1566–1581.
</reference>
<figure confidence="0.523111">
� share an existing table
� open a new table
</figure>
<page confidence="0.961801">
340
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.636589">
<title confidence="0.996992">A Note on the Implementation of Hierarchical Dirichlet Processes</title>
<email confidence="0.8474355">pblunsom@inf.ed.ac.uksgwater@inf.ed.ac.uk</email>
<affiliation confidence="0.995552">of Informatics University of Edinburgh</affiliation>
<address confidence="0.999573">Edinburgh, EH8 9AB, UK</address>
<email confidence="0.992857">tcohn@inf.ed.ac.uk</email>
<author confidence="0.955419">mark johnsonbrown edu</author>
<affiliation confidence="0.966165">of Cognitive and Linguistic Sciences Brown University</affiliation>
<address confidence="0.999486">Providence, RI, USA</address>
<abstract confidence="0.998442176470588">The implementation of collapsed Gibbs samplers for non-parametric Bayesian models is non-trivial, requiring considerable book-keeping. Goldwater et al. (2006a) presented an approximation which significantly reduces the storage and computation overhead, but we show here that their formulation was incorrect and, even after correction, is grossly inaccurate. We present an alternative formulation which is exact and can be computed easily. However this approach does not work for hierarchical models, for which case we present an efficient data structure which has a better space complexity than the naive approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Aldous</author>
</authors>
<title>Exchangeability and related topics.</title>
<date>1985</date>
<booktitle>In ´Ecole d’ ´Et´e de Probabiliti´es de Saint-Flour XIII</booktitle>
<volume>1</volume>
<pages>198</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="3363" citStr="Aldous, 1985" startWordPosition="518" endWordPosition="519">6. 2 The Chinese Restaurant Process GGJ06 present two nonparametric Bayesian language models: a DP unigram model and an HDP bigram model. Under the DP model, words in a corpus w = w1 ... wn are generated as follows: Gjα0, P0 — DP(α0, P0) wijG — G where G is a distribution over an infinite set of possible words, P0 (the base distribution of the DP) determines the probability that an item will be in the support of G, and α0 (the concentration parameter) determines the variance of G. One way of understanding the predictions that the DP model makes is through the Chinese restaurant process (CRP) (Aldous, 1985). In the CRP, customers (word tokens wi) enter a restaurant with an infinite number of tables and choose a seat. The table chosen by the ith customer, zi, follows the distribution: � nz−i P(zi = kjz−i) = i−1+ao , 0 &lt; k &lt; K(z−i) i−1+α0, k = K(z−i) α0 337 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 337–340, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP Figure 1. A seating assignment describing the state of a unigram CRP. Letters and numbers uniquely identify customers and tables. Note that multiple tables may share a label. where z−i = z1 ... zi−1 are the table ass</context>
</contexts>
<marker>Aldous, 1985</marker>
<rawString>D. Aldous. 1985. Exchangeability and related topics. In ´Ecole d’ ´Et´e de Probabiliti´es de Saint-Flour XIII 1983, 1– 198. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C E Antoniak</author>
</authors>
<title>Mixtures of dirichlet processes with applications to bayesian nonparametric problems.</title>
<date>1974</date>
<journal>The Annals of Statistics,</journal>
<volume>2</volume>
<issue>6</issue>
<contexts>
<context position="7757" citStr="Antoniak (1974)" startWordPosition="1304" endWordPosition="1305">−i), th−i wi is the number of tables labelled with wi, and th−i ∗ is the total number of occupied tables. Of particular note for our discussion is that in order to calculate these conditional distributions we must know the table assignments z−i for each of the words in w−i. Moreover, in the Gibbs samplers often used for inference in Rather than explicitly tracking the number of tables tw associated with each word w in their bigram model, GGJ06 approximate the table counts using the expectation E[tw]. Expected counts are used in place of th−i wi and th−i ∗ in (2). The exact expectation, due to Antoniak (1974), is 1 α1P1(w) + i − 1 (3) nw E[tw] = α1P1(w) i=1 338 Antoniak also gives an approximation to this expectation: nw + α1P1(w) E[tw] ≈ α1P1(w) log (4) α1P1(w) but provides no derivation. Due to a misinterpretation of Antoniak (1974), GGJ06 use an approximation that leaves out all the P1(w) terms from (4).1 Figure 2 compares the approximation to the exact expectation when the base distribution is fixed. The approximation is fairly good when αP1(w) &gt; 1 (the scenario assumed by Antoniak); however, in most NLP applications, αP1(w) &lt; 1 in order to effect a sparse prior. (We return to the case of non-</context>
<context position="9173" citStr="Antoniak (1974)" startWordPosition="1551" endWordPosition="1552"> equal 1, but the approximation gives log(2). We now provide a derivation for (4), which will allow us to obtain an O(1) formula for the expectation in (3). First, we rewrite the summation in (3) as a difference of fractional harmonic numbers:2 H(α1P1(w)+nw−1) − H(α1P1(w)−1) (5) Using the recurrence for harmonic numbers: 1 E[tw] ≈ α1P1(w) [H(α1P1(w)+nw)− We then use the asymptotic expansion, HF ≈ log F + γ + 1 2F , omiting trailing terms which are O(F−2) and smaller powers of F:3 E[tw] ≈ α1P1(w) log nw+α1P1(w) + nw α1P1(w) 2(α1P1(w)+nw) Omitting the trailing term leads to the approximation in Antoniak (1974). However, we can obtain an exact formula for the expectation by utilising the relationship between the Digamma function and the harmonic numbers: 0(n) = Hn−1 − γ.4 Thus we can rewrite (5) as:5 E[tw] = α1P1(w)· [0(α1P1(w) + nw) − 0(α1P1(w))] (7) 1The authors of GGJ06 realized this error, and current implementations of their models no longer use these approximations, instead tracking table counts explicitly. 2Fractional harmonic numbers between 0 and 1 are given by HF = f01 11 xX dx. All harmonic numbers follow the recurrence HF = HF−1 + 1F . 3Here, γ is the Euler-Mascheroni constant. 4Accurate</context>
</contexts>
<marker>Antoniak, 1974</marker>
<rawString>C. E. Antoniak. 1974. Mixtures of dirichlet processes with applications to bayesian nonparametric problems. The Annals of Statistics, 2(6):1152–1174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J DeNero</author>
<author>A Bouchard-Cˆot´e</author>
<author>D Klein</author>
</authors>
<title>Sampling alignment structure under a Bayesian translation model.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>314–323, Honolulu, Hawaii.</location>
<marker>DeNero, Bouchard-Cˆot´e, Klein, 2008</marker>
<rawString>J. DeNero, A. Bouchard-Cˆot´e, D. Klein. 2008. Sampling alignment structure under a Bayesian translation model. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, 314–323, Honolulu, Hawaii. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ferguson</author>
</authors>
<title>A Bayesian analysis of some nonparametric problems.</title>
<date>1973</date>
<journal>Annals of Statistics,</journal>
<pages>1--209</pages>
<contexts>
<context position="1549" citStr="Ferguson, 1973" startWordPosition="220" endWordPosition="221">ich has a better space complexity than the naive approach. 1 Introduction Unsupervised learning of natural language is one of the most challenging areas in NLP. Recently, methods from nonparametric Bayesian statistics have been gaining popularity as a way to approach unsupervised learning for a variety of tasks, including language modeling, word and morpheme segmentation, parsing, and machine translation (Teh et al., 2006; Goldwater et al., 2006a; Goldwater et al., 2006b; Liang et al., 2007; Finkel et al., 2007; DeNero et al., 2008). These models are often based on the Dirichlet process (DP) (Ferguson, 1973) or hierarchical Dirichlet process (HDP) (Teh et al., 2006), with Gibbs sampling as a method of inference. Exact implementation of such sampling methods requires considerable bookkeeping of various counts, which motivated Goldwater et al. (2006a) (henceforth, GGJ06) to develop an approximation using expected counts. However, we show here that their approximation is flawed in two respects: 1) It omits an important factor in the expectation, and 2) Even after correction, the approximation is poor for hierarchical models, which are commonly used for NLP applications. We derive an improved O(1) fo</context>
</contexts>
<marker>Ferguson, 1973</marker>
<rawString>S. Ferguson. 1973. A Bayesian analysis of some nonparametric problems. Annals of Statistics, 1:209–230.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Finkel</author>
<author>T Grenager</author>
<author>C D Manning</author>
</authors>
<title>The infinite tree.</title>
<date>2007</date>
<booktitle>In Proc. of the 45th Annual Meeting of the ACL (ACL-2007),</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="1450" citStr="Finkel et al., 2007" startWordPosition="201" endWordPosition="204">pproach does not work for hierarchical models, for which case we present an efficient data structure which has a better space complexity than the naive approach. 1 Introduction Unsupervised learning of natural language is one of the most challenging areas in NLP. Recently, methods from nonparametric Bayesian statistics have been gaining popularity as a way to approach unsupervised learning for a variety of tasks, including language modeling, word and morpheme segmentation, parsing, and machine translation (Teh et al., 2006; Goldwater et al., 2006a; Goldwater et al., 2006b; Liang et al., 2007; Finkel et al., 2007; DeNero et al., 2008). These models are often based on the Dirichlet process (DP) (Ferguson, 1973) or hierarchical Dirichlet process (HDP) (Teh et al., 2006), with Gibbs sampling as a method of inference. Exact implementation of such sampling methods requires considerable bookkeeping of various counts, which motivated Goldwater et al. (2006a) (henceforth, GGJ06) to develop an approximation using expected counts. However, we show here that their approximation is flawed in two respects: 1) It omits an important factor in the expectation, and 2) Even after correction, the approximation is poor f</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2007</marker>
<rawString>J. R. Finkel, T. Grenager, C. D. Manning. 2007. The infinite tree. In Proc. of the 45th Annual Meeting of the ACL (ACL-2007), Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Goldwater</author>
<author>T Griffiths</author>
<author>M Johnson</author>
</authors>
<title>Contextual dependencies in unsupervised word segmentation.</title>
<date>2006</date>
<booktitle>In Proc. of the 44th Annual Meeting of the ACL and 21st International Conference on Computational Linguistics (COLING/ACL-2006),</booktitle>
<location>Sydney.</location>
<contexts>
<context position="1383" citStr="Goldwater et al., 2006" startWordPosition="189" endWordPosition="192"> formulation which is exact and can be computed easily. However this approach does not work for hierarchical models, for which case we present an efficient data structure which has a better space complexity than the naive approach. 1 Introduction Unsupervised learning of natural language is one of the most challenging areas in NLP. Recently, methods from nonparametric Bayesian statistics have been gaining popularity as a way to approach unsupervised learning for a variety of tasks, including language modeling, word and morpheme segmentation, parsing, and machine translation (Teh et al., 2006; Goldwater et al., 2006a; Goldwater et al., 2006b; Liang et al., 2007; Finkel et al., 2007; DeNero et al., 2008). These models are often based on the Dirichlet process (DP) (Ferguson, 1973) or hierarchical Dirichlet process (HDP) (Teh et al., 2006), with Gibbs sampling as a method of inference. Exact implementation of such sampling methods requires considerable bookkeeping of various counts, which motivated Goldwater et al. (2006a) (henceforth, GGJ06) to develop an approximation using expected counts. However, we show here that their approximation is flawed in two respects: 1) It omits an important factor in the exp</context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2006</marker>
<rawString>S. Goldwater, T. Griffiths, M. Johnson. 2006a. Contextual dependencies in unsupervised word segmentation. In Proc. of the 44th Annual Meeting of the ACL and 21st International Conference on Computational Linguistics (COLING/ACL-2006), Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Goldwater</author>
<author>T Griffiths</author>
<author>M Johnson</author>
</authors>
<title>Interpolating between types and tokens by estimating power-law generators. In</title>
<date>2006</date>
<booktitle>Advances in Neural Information Processing Systems 18,</booktitle>
<pages>459--466</pages>
<editor>Y. Weiss, B. Sch¨olkopf, J. Platt, eds.,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="1383" citStr="Goldwater et al., 2006" startWordPosition="189" endWordPosition="192"> formulation which is exact and can be computed easily. However this approach does not work for hierarchical models, for which case we present an efficient data structure which has a better space complexity than the naive approach. 1 Introduction Unsupervised learning of natural language is one of the most challenging areas in NLP. Recently, methods from nonparametric Bayesian statistics have been gaining popularity as a way to approach unsupervised learning for a variety of tasks, including language modeling, word and morpheme segmentation, parsing, and machine translation (Teh et al., 2006; Goldwater et al., 2006a; Goldwater et al., 2006b; Liang et al., 2007; Finkel et al., 2007; DeNero et al., 2008). These models are often based on the Dirichlet process (DP) (Ferguson, 1973) or hierarchical Dirichlet process (HDP) (Teh et al., 2006), with Gibbs sampling as a method of inference. Exact implementation of such sampling methods requires considerable bookkeeping of various counts, which motivated Goldwater et al. (2006a) (henceforth, GGJ06) to develop an approximation using expected counts. However, we show here that their approximation is flawed in two respects: 1) It omits an important factor in the exp</context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2006</marker>
<rawString>S. Goldwater, T. Griffiths, M. Johnson. 2006b. Interpolating between types and tokens by estimating power-law generators. In Y. Weiss, B. Sch¨olkopf, J. Platt, eds., Advances in Neural Information Processing Systems 18, 459–466. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>S Petrov</author>
<author>M Jordan</author>
<author>D Klein</author>
</authors>
<title>The infinite PCFG using hierarchical Dirichlet processes.</title>
<date>2007</date>
<booktitle>In Proc. of the 2007 Conference on Empirical Methods in Natural Language Processing (EMNLP-2007), 688–697,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="1429" citStr="Liang et al., 2007" startWordPosition="197" endWordPosition="200">sily. However this approach does not work for hierarchical models, for which case we present an efficient data structure which has a better space complexity than the naive approach. 1 Introduction Unsupervised learning of natural language is one of the most challenging areas in NLP. Recently, methods from nonparametric Bayesian statistics have been gaining popularity as a way to approach unsupervised learning for a variety of tasks, including language modeling, word and morpheme segmentation, parsing, and machine translation (Teh et al., 2006; Goldwater et al., 2006a; Goldwater et al., 2006b; Liang et al., 2007; Finkel et al., 2007; DeNero et al., 2008). These models are often based on the Dirichlet process (DP) (Ferguson, 1973) or hierarchical Dirichlet process (HDP) (Teh et al., 2006), with Gibbs sampling as a method of inference. Exact implementation of such sampling methods requires considerable bookkeeping of various counts, which motivated Goldwater et al. (2006a) (henceforth, GGJ06) to develop an approximation using expected counts. However, we show here that their approximation is flawed in two respects: 1) It omits an important factor in the expectation, and 2) Even after correction, the ap</context>
</contexts>
<marker>Liang, Petrov, Jordan, Klein, 2007</marker>
<rawString>P. Liang, S. Petrov, M. Jordan, D. Klein. 2007. The infinite PCFG using hierarchical Dirichlet processes. In Proc. of the 2007 Conference on Empirical Methods in Natural Language Processing (EMNLP-2007), 688–697, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y W Teh</author>
<author>M I Jordan</author>
<author>M J Beal</author>
<author>D M Blei</author>
</authors>
<title>Hierarchical Dirichlet processes.</title>
<date>2006</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>101</volume>
<issue>476</issue>
<contexts>
<context position="1359" citStr="Teh et al., 2006" startWordPosition="185" endWordPosition="188">ent an alternative formulation which is exact and can be computed easily. However this approach does not work for hierarchical models, for which case we present an efficient data structure which has a better space complexity than the naive approach. 1 Introduction Unsupervised learning of natural language is one of the most challenging areas in NLP. Recently, methods from nonparametric Bayesian statistics have been gaining popularity as a way to approach unsupervised learning for a variety of tasks, including language modeling, word and morpheme segmentation, parsing, and machine translation (Teh et al., 2006; Goldwater et al., 2006a; Goldwater et al., 2006b; Liang et al., 2007; Finkel et al., 2007; DeNero et al., 2008). These models are often based on the Dirichlet process (DP) (Ferguson, 1973) or hierarchical Dirichlet process (HDP) (Teh et al., 2006), with Gibbs sampling as a method of inference. Exact implementation of such sampling methods requires considerable bookkeeping of various counts, which motivated Goldwater et al. (2006a) (henceforth, GGJ06) to develop an approximation using expected counts. However, we show here that their approximation is flawed in two respects: 1) It omits an imp</context>
<context position="12205" citStr="Teh et al. (2006)" startWordPosition="2089" endWordPosition="2092">pected table count approximation for hierarchical models we could fall back to explicitly tracking which table each customer that enters the restaurant sits at. However, here we describe a more compact representation for the state of the restaurant that doesn’t require explicit table tracking.6 Instead we maintain a histogram for each dish wi of the frequency of a table having a particular number of customers. Figure 3 depicts the histogram and explicit representations for the CRP state in Figure 1. Our alternative method of inference for hierarchical Bayesian models takes advantage of their 6Teh et al. (2006) also note that the exact table assignments for customers are not required for prediction. α1P1(w) + nw () − H(α1P1(w)+nw) + 1 α1P1(w)i 6 339 Algorithm 1 A new customer enters the restaurant 1: w: word type 2: P0w: Base probability for w 3: HDw: Seating Histogram for w 4: procedure INCREMENT(w, P0w, HDw) w−1 nw 5: pshare ←w−1 nw +α0 α0 6: pnew←w−10w ×P nw +α0 7: r ← random(0, pshare + pnew) 8: if r &lt; pnew or nw−1 w = 0 then 9: HDw[1] = HDw[1] + 1 10: else � Sample from the histogram of customers at tables 11: r ← random(0, nw−1 w ) 12: for c ∈ HDw do &gt; c: customer count 13: r = r − (c × HDw[c]</context>
</contexts>
<marker>Teh, Jordan, Beal, Blei, 2006</marker>
<rawString>Y. W. Teh, M. I. Jordan, M. J. Beal, D. M. Blei. 2006. Hierarchical Dirichlet processes. Journal of the American Statistical Association, 101(476):1566–1581.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>