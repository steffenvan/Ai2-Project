<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.228472">
<note confidence="0.362952">
Invited Presentation
</note>
<title confidence="0.965023">
Repetition and Language Models and Comparable Corpora
</title>
<author confidence="0.986076">
Ken Church
</author>
<affiliation confidence="0.961191">
Human Language Technology Center of Excellence
Johns Hopkins University
</affiliation>
<email confidence="0.970176">
Kenneth.Church@jhu.edu
</email>
<bodyText confidence="0.999951755555556">
I will discuss a couple of non-standard fea-
tures that I believe could be useful for working
with comparable corpora. Dotplots have been
used in biology to find interesting DNA sequences.
Biology is interested in ordered matches, which
show up as (possibly broken) diagonals in dot-
plots. Information Retrieval is more interested in
unordered matches (e.g., cosine similarity), which
show up as squares in dotplots. Parallel corpora
have both squares and diagonals multiplexed to-
gether. The diagonals tell us what is a translation
of what, and the squares tell us what is in the same
language. I would expect dotplots of compara-
ble corpora would contain lots of diagonals and
squares, though the diagonals would be shorter
and more subtle in comparable corpora than in par-
allel corpora.
There is also an opportunity to take advantage
of repetition in comparable corpora. Repetition is
very common. Standard bag-of-word models in
Information Retrieval do not attempt to model dis-
course structure such as given/new. The first men-
tion in a news article (e.g., “Manuel Noriega, for-
mer President of Panama”) is different from sub-
sequent mentions (e.g., “Noriega”). Adaptive lan-
guage models were introduced in Speech Recogni-
tion to capture the fact that probabilities change or
adapt. After we see the first mention, we should
expect a subsequent mention. If the first men-
tion has probability p, then under standard (bag-
of-words) independence assumptions, two men-
tions ought to have probability p2, but we find
the probability is actually closer to p/2. Adapta-
tion matters more for meaningful units of text. In
Japanese, words (meaningful sequences of char-
acters) are more likely to be repeated than frag-
ments (meaningless sequences of characters from
words that happen to be adjacent). In newswire,
we find more adaptation for content words (proper
nouns, technical terminology and good keywords
for information retrieval), and less adaptation for
function words, clichés and ordinary first names.
There is more to meaning than frequency. Content
words are not only low frequency, but likely to be
repeated.
</bodyText>
<page confidence="0.760724">
1
</page>
<note confidence="0.936458">
Proceedings of the 2nd Workshop on Building and Using Comparable Corpora, ACL-IJCNLP 2009, page 1,
Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP
</note>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.190647">
<title confidence="0.9991355">Invited Presentation Repetition and Language Models and Comparable Corpora</title>
<author confidence="0.84787">Ken</author>
<affiliation confidence="0.623037">Human Language Technology Center of Johns Hopkins</affiliation>
<email confidence="0.979138">Kenneth.Church@jhu.edu</email>
<abstract confidence="0.988416659574468">I will discuss a couple of non-standard features that I believe could be useful for working with comparable corpora. Dotplots have been used in biology to find interesting DNA sequences. Biology is interested in ordered matches, which show up as (possibly broken) diagonals in dotplots. Information Retrieval is more interested in matches cosine similarity), which show up as squares in dotplots. Parallel corpora have both squares and diagonals multiplexed together. The diagonals tell us what is a translation of what, and the squares tell us what is in the same language. I would expect dotplots of comparable corpora would contain lots of diagonals and squares, though the diagonals would be shorter and more subtle in comparable corpora than in parallel corpora. There is also an opportunity to take advantage of repetition in comparable corpora. Repetition is very common. Standard bag-of-word models in Information Retrieval do not attempt to model discourse structure such as given/new. The first menin a news article “Manuel Noriega, former President of Panama”) is different from subsequent mentions (e.g., “Noriega”). Adaptive language models were introduced in Speech Recognition to capture the fact that probabilities change or adapt. After we see the first mention, we should expect a subsequent mention. If the first menhas probability then under standard (bagof-words) independence assumptions, two menought to have probability but we find probability is actually closer to Adaptation matters more for meaningful units of text. In Japanese, words (meaningful sequences of characters) are more likely to be repeated than fragments (meaningless sequences of characters from words that happen to be adjacent). In newswire, we find more adaptation for content words (proper nouns, technical terminology and good keywords for information retrieval), and less adaptation for function words, clichés and ordinary first names. There is more to meaning than frequency. Content words are not only low frequency, but likely to be repeated. 1 of the 2nd Workshop on Building and Using Comparable Corpora, ACL-IJCNLP page</abstract>
<intro confidence="0.538617">Singapore, 6 August 2009. ACL and AFNLP</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>