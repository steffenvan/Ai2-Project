<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002054">
<title confidence="0.994079">
A Multimodal Home Entertainment Interface via a Mobile Device
</title>
<author confidence="0.8856585">
Alexander Gruenstein Bo-June (Paul) Hsu James Glass Stephanie Seneff
Lee Hetherington Scott Cyphers Ibrahim Badr Chao Wang Sean Liu
</author>
<affiliation confidence="0.437812">
MIT Computer Science and Artificial Intelligence Laboratory
</affiliation>
<address confidence="0.567515">
32 Vassar St, Cambridge, MA 02139 USA
</address>
<email confidence="0.944556">
http://www.sls.csail.mit.edu/
</email>
<sectionHeader confidence="0.994967" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999908842105263">
We describe a multimodal dialogue system for
interacting with a home entertainment center
via a mobile device. In our working proto-
type, users may utilize both a graphical and
speech user interface to search TV listings,
record and play television programs, and listen
to music. The developed framework is quite
generic, potentially supporting a wide variety
of applications, as we demonstrate by integrat-
ing a weather forecast application. In the pro-
totype, the mobile device serves as the locus
of interaction, providing both a small touch-
screen display, and speech input and output;
while the TV screen features a larger, richer
GUI. The system architecture is agnostic to
the location of the natural language process-
ing components: a consistent user experience
is maintained regardless of whether they run
on a remote server or on the device itself.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999968638297873">
People have access to large libraries of digital con-
tent both in their living rooms and on their mobile
devices. Digital video recorders (DVRs) allow peo-
ple to record TV programs from hundreds of chan-
nels for subsequent viewing at home—or, increas-
ingly, on their mobile devices. Similarly, having
accumulated vast libraries of digital music, people
yearn for an easy way to sift through them from the
comfort of their couches, in their cars, and on the go.
Mobile devices are already central to accessing
digital media libraries while users are away from
home: people listen to music or watch video record-
ings. Mobile devices also play an increasingly im-
portant role in managing digital media libraries. For
instance, a web-enabled mobile phone can be used to
remotely schedule TV recordings through a web site
or via a custom application. Such management tasks
often prove cumbersome, however, as it is challeng-
ing to browse through listings for hundreds of TV
channels on a small display. Indeed, even on a large
screen in the living room, browsing alphabetically,
or by time and channel, for a particular show using
the remote control quickly becomes unwieldy.
Speech and multimodal interfaces provide a nat-
ural means of addressing many of these challenges.
It is effortless for people to say the name of a pro-
gram, for instance, in order to search for existing
recordings. Moreover, such a speech browsing ca-
pability is useful both in the living room and away
from home. Thus, a natural way to provide speech-
based control of a media library is through the user’s
mobile device itself.
In this paper we describe just such a prototype
system. A mobile phone plays a central role in pro-
viding a multimodal, natural language interface to
both a digital video recorder and a music library.
Users can interact with the system—presented as a
dynamic web page on the mobile browser—using
the navigation keys, the stylus, or spoken natural
language. In front of the TV, a much richer GUI is
also available, along with support for playing video
recordings and music.
In the prototype described herein, the mobile de-
vice serves as the locus of natural language in-
teraction, whether a user is in the living room or
walking down the street. Since these environments
may be very different in terms of computational re-
</bodyText>
<page confidence="0.823676">
1
</page>
<note confidence="0.769196">
Proceedings of the ACL-08: HLT Workshop on Mobile Language Processing, pages 1–9,
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.99990619047619">
sources and network bandwidth, it is important that
the architecture allows for multiple configurations in
terms of the location of the natural language pro-
cessing components. For instance, when a device
is connected to a Wi-Fi network at home, recogni-
tion latency may be reduced by performing speech
and natural language processing on the home me-
dia server. Moreover, a powerful server may enable
more sophisticated processing techniques, such as
multipass speech recognition (Hetherington, 2005;
Chung et al., 2004), for improved accuracy. In sit-
uations with reduced network connectivity, latency
may be improved by performing speech recognition
and natural language processing tasks on the mobile
device itself. Given resource constraints, however,
less detailed acoustic and language models may be
required. We have developed just such a flexible ar-
chitecture, with many of the natural language pro-
cessing components able to run on either a server or
the mobile device itself. Regardless of the configu-
ration, a consistent user experience is maintained.
</bodyText>
<sectionHeader confidence="0.999692" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999935764705883">
Various academic researchers and commercial busi-
nesses have demonstrated speech-enabled interfaces
to entertainment centers. A good deal of the work
focuses on adding a microphone to a remote con-
trol, so that speech input may be used in addition
to a traditional remote control. Much commercial
work, for example (Fujita et al., 2003), tends to fo-
cus on constrained grammar systems, where speech
input is limited to a small set of templates corre-
sponding to menu choices. (Berglund and Johans-
son, 2004) present a remote-control based speech
interface for navigating an existing interactive tele-
vision on-screen menu, though experimenters man-
ually transcribed user utterances as they spoke in-
stead of using a speech recognizer. (Oh et al., 2007)
present a dialogue system for TV control that makes
use of concept spotting and statistical dialogue man-
agement to understand queries. A version of their
system can run independently on low-resource de-
vices such as PDAs; however, it has a smaller vo-
cabulary and supports a limited set of user utterance
templates. Finally, (Wittenburg et al., 2006) look
mainly at the problem of searching for television
programs using speech, an on-screen display, and a
remote control. They explore a Speech-In List-Out
interface to searching for episodes of television pro-
grams.
(Portele et al., 2003) depart from the model of
adding a speech interface component to an exist-
ing on-screen menu. Instead, they a create a tablet
PC interface to an electronic program guide, though
they do not use the television display as well. Users
may search an electronic program guide using con-
straints such as date, time, and genre; however, they
can’t search by title. Users can also perform typi-
cal remote-control tasks like turning the television
on and off, and changing the channel. (Johnston et
al., 2007) also use a tablet PC to provide an inter-
face to television content—in this case a database of
movies. The search can be constrained by attributes
such as title, director, or starring actors. The tablet
PC pen can be used to handwrite queries and to point
at items (such as actor names) while the user speaks.
We were also inspired by previous prototypes in
which mobile devices have been used in conjunc-
tion with larger, shared displays. For instance, (Paek
et al., 2004) demonstrate a framework for building
such applications. The prototype we demonstrate
here fits into their “Jukebox” model of interaction.
Interactive workspaces, such as the one described in
(Johanson et al., 2002), also demonstrate the utility
of integrating mobile and large screen displays. Our
prototype is a departure from these systems, how-
ever, in that it provides for spoken interactions.
Finally, there is related work in the use of mobile
devices for various kinds of search. For instance, of-
ferings from Microsoft (Acero et al., 2008), Vlingo,1
and Promptu2 allow users to search for items like
businesses and songs using their mobile phones.
These applications differ from ours in that speech
is used only for search, without any accompanying
command and control capabilities. Also, these ser-
vices do not allow interaction with your own de-
vices at home. Efforts have been made to use mo-
bile devices for control of devices in the home, such
as in (Nichols and Myers, 2006), however these ef-
forts have not involved the use of speech as an input
modality.
</bodyText>
<footnote confidence="0.9999935">
1http://www.vlingo.com
2http://www.promptu.com
</footnote>
<page confidence="0.996668">
2
</page>
<figure confidence="0.9988158">
Hold-
to-talk
button
Opera
web
browser
Status icon indicates conn
microphone icon wh
Navigation pad o
be used to nav
</figure>
<figureCaption confidence="0.988146">
Figure 1: User interface overview.
</figureCaption>
<figure confidence="0.6695652">
(c) Music Library
U: What’s on tonight at seven o’clock?
S: I found seventy entries. [Displays program guide.]
U: Are there any action movies on HBO tomorrow?
S: I found two entries. [Displays results.]
U: When is American Idol on?
S: I found three entries. [Displays results.]
U: Record the first one.
S: I have added American Idol to the list of programs to be
recorded. [Displays recording settings.]
</figure>
<table confidence="0.86248775">
U: Please record Nova on Wednesday night.
S: I have added Nova to the list of programs to be recorded.
[Displays recording settings.]
U: Show all my scheduled recordings.
S: There are seven scheduled recordings. [Shows list.]
U: Delete BBC World News.
S: I have deleted the scheduled recording: BBC World
News. [Updates List.]
U: Show my videos.
S: [Displays list of videos.]
U: Delete Frasier.
S: I have deleted the recording Frasier. [Updates list.]
U: Play Grey’s Anatomy.
S: Playing Grey’s Anatomy. [Plays recording.]
U: Stop Playing.
S: Stopping. [Stops playback.]
</table>
<figure confidence="0.773780625">
U: [Clicks on Music tab.]
S: [Displays artist list.]
U: Show albums by Billie Holiday.
S: I found three albums by Billie Holiday. [Shows albums.]
U: Please play A Hard Rain’s A-Gonna Fall by Bob Dylan.
S: Playing A Hard Rain’s A-Gonna Fall. [Plays song.]
(a) Program Guide
(b) Scheduled Recordings
</figure>
<figureCaption confidence="0.999765">
Figure 2: Screenshots and an example interaction. Utterances are labeled with U for user and S for system.
</figureCaption>
<page confidence="0.996259">
3
</page>
<sectionHeader confidence="0.978215" genericHeader="method">
3 User Experience
</sectionHeader>
<bodyText confidence="0.999987800000001">
Our current prototype system implements the basic
functionalities that one expects from a home enter-
tainment center. Users can navigate through and
record programs from the television’s electronic pro-
gram guide, manage recording settings, and play
recorded videos. They can also browse and listen to
selections from their music libraries. However, un-
like existing prototypes, ours employs a smartphone
with a navigation pad, touch-sensitive screen, and
built-in microphone as the remote control. Figure 1
provides an overview of the graphical user interface
on both the TV and mobile device.
Mirroring the TV’s on-screen display, the proto-
type system presents a reduced view on the mobile
device with synchronized cursors. Users can navi-
gate the hierarchical menu structure using the arrow
keys or directly click on the target item with the sty-
lus. While away from the living room, or when a
recording is playing full screen, users can browse
and manage their media libraries using only the mo-
bile device.
While the navigation pad and stylus are great for
basic navigation and control, searching for media
with specific attributes, such as title, remains cum-
bersome. To facilitate such interactions, the cur-
rent system supports spoken natural language inter-
actions. For example, the user can press the hold-to-
talk button located on the side of the mobile device
and ask “What’s on the National Geographic Chan-
nel this afternoon?” to retrieve a list of shows with
the specified channel and time. The system responds
with a short verbal summary “I found six entries on
January seventh” and presents the resulting list on
both the TV and mobile displays. The user can then
browse the list using the navigation pad or press the
hold-to-talk button to barge in with another com-
mand, e.g. “Please record the second one.” Depress-
ing the hold-to-talk button not only terminates any
current spoken response, but also mutes the TV to
minimize interference with speech recognition. As
the previous example demonstrates, contextual in-
formation is used to resolve list position references
and disambiguate commands.
The speech interface to the user’s music library
works in a similar fashion. Users can search by
artist, album, and song name, and then play the
songs found. To demonstrate the extensibility of
the architecture, we have also integrated an exist-
ing weather information system (Zue et al., 2000),
which has been previously deployed as a telephony
application. Users simply click on the Weather tab
to switch to this domain, allowing them to ask a wide
range of weather queries. The system responds ver-
bally and with a simple graphical forecast.
To create a natural user experience, we designed
the multimodal interface to allow users to seam-
lessly switch among the different input modalities
available on the mobile device. Figure 2 demon-
strates an example interaction with the prototype, as
well as several screenshots of the user interface.
</bodyText>
<sectionHeader confidence="0.984083" genericHeader="method">
4 System Architecture
</sectionHeader>
<bodyText confidence="0.999973677419355">
The system architecture is quite flexible with re-
gards to the placement of the natural language pro-
cessing components. Figure 3 presents two possible
configurations of the system components distributed
across the mobile device, home media server, and
TV display. In 3(a), all speech recognition and nat-
ural language processing components reside on the
server, with the mobile device acting as the micro-
phone, speaker, display, and remote control. In 3(b),
the speech recognizer, language understanding com-
ponent, language generation component, and text-
to-speech (TTS) synthesizer run on the mobile de-
vice. Depending on the capabilities of the mobile
device and network connection, different configu-
rations may be optimal. For instance, on a power-
ful device with slow network connection, recogni-
tion latency may be reduced by performing speech
recognition and natural language processing on the
device. On the other hand, streaming audio via a fast
wireless network to the server for processing may
result in improved accuracy.
In the prototype system, flexible and reusable
speech recognition and natural language processing
capabilities are provided via generic components de-
veloped and deployed in numerous spoken dialogue
systems by our group, with the exception of an off-
the-shelf speech synthesizer. Speech input from the
mobile device is recognized using the landmark-
based SUMMIT system (Glass, 2003). The result-
ing N-best hypotheses are processed by the TINA
language understanding component (Seneff, 1992).
</bodyText>
<page confidence="0.909653">
4
</page>
<figure confidence="0.994199571428571">
Galaxy
Speech Recognizer
Language Understanding
Language Generation
Dialogue Manager
Text-To-Speech
(a) (b)
</figure>
<figureCaption confidence="0.997287">
Figure 3: Two architecture diagrams. In (a) speech recognition and natural language processing occur on the server,
while in (b) processing is primarily performed on the device.
</figureCaption>
<bodyText confidence="0.99996959375">
Based on the resulting meaning representation, the
dialogue manager (Polifroni et al., 2003) incorpo-
rates contextual information (Filisko and Seneff,
2003), and then determines an appropriate response.
The response consists of an update to the graph-
ical display, and a spoken system response which
is realized via the GENESIS (Baptist and Seneff,
2000) language generation module. To support on-
device processing, all the components are linked via
the GALAXY framework (Seneff et al., 1998) with
an additional Mobile Manager component responsi-
ble for coordinating the communication between the
mobile device and the home media server.
In the currently deployed system, we use a mo-
bile phone with a 624 MHz ARM processor run-
ning the Windows Mobile operating system and
Opera Mobile web browser. The TV program and
music databases reside on the home media server
running GNU/Linux. The TV program guide data
and recording capabilities are provided via MythTV,
a full-featured, open-source digital video recorder
software package.3 Daily updates to the program
guide information typically contain hundreds of
unique channel names and thousands of unique pro-
gram names. The music library is comprised of
5,000 songs from over 80 artists and 13 major gen-
res, indexed using the open-source text search en-
gine Lucene.4 Lastly, the TV display can be driven
by a web browser on either the home media server or
a separate computer connected to the server via a fast
Ethernet connection, for high quality video stream-
ing.
</bodyText>
<footnote confidence="0.9997765">
3http://www.mythtv.org/
4http://lucene.apache.org/
</footnote>
<bodyText confidence="0.99965425">
While the focus of this paper is on the natural lan-
guage processing and user interface aspects of the
system, our work is actually situated within a larger
collaborative project at MIT that also includes sim-
plified device configuration (Mazzola Paluska et al.,
2008; Mazzola Paluska et al., 2006), transparent ac-
cess to remote servers (Ford et al., 2006), and im-
proved security.
</bodyText>
<sectionHeader confidence="0.879596" genericHeader="method">
5 Mobile Natural Language Components
</sectionHeader>
<bodyText confidence="0.999987">
Porting the implementation of the various speech
recognizer and natural language processing com-
ponents to mobile devices with limited computa-
tion and memory presents both a research and en-
gineering challenge. Instead of creating a small vo-
cabulary, fixed phrase dialogue system, we aim to
support—on the mobile device—the same flexible
and natural language interactions currently available
on our desktop, tablet, and telephony systems; see
e.g., (Gruenstein et al., 2006; Seneff, 2002; Zue et
al., 2000). In this section, we summarize our ef-
forts thus far in implementing the SUMMIT speech
recognizer and TINA natural language parser. Ports
of the GENESIS language generation system and of
our dialogue manager are well underway, and we ex-
pect to have these components working on the mo-
bile device in the near future.
</bodyText>
<subsectionHeader confidence="0.882766">
5.1 PocketSUMMIT
</subsectionHeader>
<bodyText confidence="0.999976166666667">
To significantly reduce the memory footprint and
overall computation, we chose to reimplement our
segment-based speech recognizer from scratch, uti-
lizing fixed-point arithmetic, parameter quantiza-
tion, and bit-packing in the binary model files.
The resulting PocketSUMMIT recognizer (Hether-
</bodyText>
<page confidence="0.98499">
5
</page>
<bodyText confidence="0.999902296296296">
ington, 2007) utilizes only the landmark features,
initially forgoing segment features such as phonetic
duration, as they introduce algorithmic complexities
for relatively small word error rate (WER) improve-
ments.
In the current system, we quantize the mean and
variance of each Gaussian mixture model dimension
to 5 and 3 bits, respectively. Such quantization not
only results in an 8-fold reduction in model size, but
also yields about a 50% speedup by enabling table
lookups for Gaussian evaluations. Likewise, in the
finite-state transducers (FSTs) used to represent the
language model, lexical, phonological, and class di-
phone constraints, quantizing the FST weights and
bit-packing not only compress the resulting binary
model files, but also reduce the processing time with
improved processor cache locality.
In the aforementioned TV, music, and weather do-
mains with a moderate vocabulary of a few thou-
sand words, the resulting PocketSUMMIT recog-
nizer performs in approximately real-time on 400-
600 MHz ARM processors, using a total of 2-4
MB of memory, including 1-2 MB for memory-
mapped model files. Compared with equivalent non-
quantized models, PocketSUMMIT achieves dra-
matic improvements in speed and memory while
maintaining comparable WER performance.
</bodyText>
<subsectionHeader confidence="0.979405">
5.2 PocketTINA
</subsectionHeader>
<bodyText confidence="0.999980352941177">
Porting the TINA natural language parser to mobile
devices involved significant software engineering to
reduce the memory and computational requirements
of the core data structures and algorithms. TINA
utilizes a best-first search that explores thousands of
partial parses when processing an input utterance.
To efficiently manage memory allocation given the
unpredictability of pruning invalid parses (e.g. due
to subject-verb agreement), we implemented a mark
and sweep garbage collection mechanism. Com-
bined with a more efficient implementation of the
priority queue and the use of aggressive “beam”
pruning, the resulting PocketTINA system provides
identical output as server-side TINA, but can parse
a 10-best recognition hypothesis list into the corre-
sponding meaning representation in under 0.1 sec-
onds, using about 2 MB of memory.
</bodyText>
<sectionHeader confidence="0.979864" genericHeader="method">
6 Rapid Dialogue System Development
</sectionHeader>
<bodyText confidence="0.9999585">
Over the course of developing dialogue systems for
many domains, we have built generic natural lan-
guage understanding components that enable the
rapid development of flexible and natural spoken di-
alogue systems for novel domains. Creating such
prototype systems typically involves customizing
the following to the target domain: recognizer lan-
guage model, language understanding parser gram-
mar, context resolution rules, dialogue management
control script, and language generation rules.
Recognizer Language Model Given a new do-
main, we first identify a set of semantic classes
which correspond to the back-end application’s
database, such as artist, album, and genre. Ideally,
we would have a corpus of tagged utterances col-
lected from real users. However, when building pro-
totypes such as the one described here, little or no
training data is usually available. Thus, we create
a domain-specific context-free grammar to generate
a supplemental corpus of synthetic utterances. The
corpus is used to train probabilities for the natural
language parsing grammar (described immediately
below), which in turn is used to derive a class n-
gram language model (Seneff et al., 2003).
Classes in the language model which corre-
spond to contents of the database are marked as
dynamic, and are populated at runtime from the
database (Chung et al., 2004; Hetherington, 2005).
Database entries are heuristically normalized into
spoken forms. Pronunciations not in our 150,000
word lexicon are automatically generated (Seneff,
2007).
Parser Grammar The TINA parser uses a prob-
abilistic context-free grammar enhanced with sup-
port for wh-movement and grammatical agreement
constraints. We have developed a generic syntac-
tic grammar by examining hundreds of thousands
of utterances collected from real user interactions
with various existing dialogue systems. In addition,
we have developed libraries which parse and inter-
pret common semantic classes like dates, times, and
numbers. The grammar and semantic libraries pro-
vide good coverage for spoken dialogue systems in
database-query domains.
</bodyText>
<page confidence="0.99856">
6
</page>
<bodyText confidence="0.998991909090909">
To build a grammar for a new domain, a devel-
oper extends the generic syntactic grammar by aug-
menting it with domain-specific semantic categories
and their lexical entries. A probability model which
conditions each node category on its left sibling and
parent is then estimated from a training corpus of
utterances (Seneff et al., 2003).
At runtime, the recognizer tags the hypothesized
dynamic class expansions with their class names,
allowing the parser grammar to be independent of
the database contents. Furthermore, each semantic
class is designated either as a semantic entity, or as
an attribute associated with a particular entity. This
enables the generation of a semantic representation
from the parse tree.
Dialogue Management &amp; Language Generation
Once an utterance is recognized and parsed, the
meaning representation is passed to the context res-
olution and dialogue manager component. The con-
text resolution module (Filisko and Seneff, 2003)
applies generic and domain-specific rules to re-
solve anaphora and deixis, and to interpret frag-
ments and ellipsis in context. The dialogue man-
ager then interacts with the application back-end
and database, controlled by a script customized for
the domain (Polifroni et al., 2003). Finally, the
GENESIS module (Baptist and Seneff, 2000) ap-
plies domain-specific rules to generate a natural lan-
guage representation of the dialogue manager’s re-
sponse, which is sent to a speech synthesizer. The
dialogue manager also sends an update to the GUI,
so that, for example, the appropriate database search
results are displayed.
</bodyText>
<sectionHeader confidence="0.932487" genericHeader="method">
7 Mobile Design Challenges
</sectionHeader>
<bodyText confidence="0.999953">
Dialogue systems for mobile devices present a
unique set of design challenges not found in tele-
phony and desktop applications. Here we describe
some of the design choices made while developing
this prototype, and discuss their tradeoffs.
</bodyText>
<subsectionHeader confidence="0.984144">
7.1 Client/Server Tradeoffs
</subsectionHeader>
<bodyText confidence="0.9999635">
Towards supporting network-less scenarios, we have
begun porting various natural language processing
components to mobile platforms, as discussed in
Section 5. Having efficient mobile implementations
further allows the natural language processing tasks
to be performed on either the mobile device or the
server. While building the prototype, we observed
that the Wi-Fi network performance can often be un-
predictable, resulting in erratic recognition latency
that occasionally exceeds on-device recognition la-
tency. However, utilizing the mobile processor for
computationally intensive tasks rapidly drains the
battery. Currently, the component architecture in the
prototype system is pre-configured. A more robust
implementation would dynamically adjust the con-
figuration to optimize the tradeoffs among network
use, CPU utilization, power consumption, and user-
perceived latency/accuracy.
</bodyText>
<subsectionHeader confidence="0.9995">
7.2 Speech User Interface
</subsectionHeader>
<bodyText confidence="0.999989142857143">
As neither open-mic nor push-to-talk with automatic
endpoint detection is practical on mobile devices
with limited battery life, our prototype system em-
ploys a hold-to-talk hardware button for microphone
control. To guide users to speak commands only
while the button is depressed, a short beep is played
as an earcon both when the button is pushed and
released. Since users are less likely to talk over
short audio clips, the use of earcons mitigates the
tendency for users to start speaking before pushing
down the microphone button.
In the current system, media audio is played over
the TV speakers, whereas TTS output is sent to
the mobile device speakers. To reduce background
noise captured from the mobile device’s far-field mi-
crophone, the TV is muted while the microphone
button is depressed. Unlike telephony spoken di-
alogue systems where the recognizer has to con-
stantly monitor for barge-in, the use of a hold-to-
talk button significantly simplifies barge-in support,
while reducing power consumption.
</bodyText>
<subsectionHeader confidence="0.999526">
7.3 Graphical User Interface
</subsectionHeader>
<bodyText confidence="0.9998807">
In addition to supporting interactive natural lan-
guage dialogues via the spoken user interface, the
prototype system implements a graphical user in-
terface (GUI) on the mobile device to supplement
the TV’s on-screen interface. To faciliate rapid pro-
totyping, we chose to implement both the mobile
and TV GUI using web pages with AJAX (Asyn-
chronous Javascript and XML) techniques, an ap-
proach we have leveraged in several existing mul-
timodal dialogue systems, e.g. (Gruenstein et al.,
</bodyText>
<page confidence="0.997706">
7
</page>
<bodyText confidence="0.999520923076923">
2006; McGraw and Seneff, 2007). The resulting in-
terface is largely platform-independent and allows
display updates to be “pushed” to the client browser.
As many users are already familiar with the TV’s
on-screen interface, we chose to mirror the same in-
terface on the mobile device and synchronize the
selection cursor. However, unlike desktop GUIs,
mobile devices are constrained by a small display,
limited computational power, and reduced network
bandwidth. Thus, both the page layout and infor-
mation detail were adjusted for the mobile browser.
Although AJAX is more responsive than traditional
web technology, rendering large formatted pages—
such as the program guide grid—is often still un-
acceptably slow. In the current implementation, we
addressed this problem by displaying only the first
section of the content and providing a “Show More”
button that downloads and renders the full content.
While browser-based GUIs expedite rapid prototyp-
ing, deployed systems may want to take advantage
of native interfaces specific to the device for more
responsive user interactions. Instead of limiting the
mobile interface to reflect the TV GUI, improved us-
ability may be obtained by designing the interface
for the mobile device first and then expanding the
visual content to the TV display.
</bodyText>
<subsectionHeader confidence="0.77872">
7.4 Client/Server Communication
</subsectionHeader>
<bodyText confidence="0.999978071428571">
In the current prototype, communication between
the mobile device and the media server consists of
AJAX HTTP and XML-RPC requests. To enable
server-side “push” updates, the client periodically
pings the server for messages. While such an im-
plementation provides a responsive user interface, it
quickly drains the battery and is not robust to net-
work outages resulting from the device being moved
or switching to power-saving mode. Reestablish-
ing connection with the server further introduces la-
tency. In future implementations, we would like to
examine the use of Bluetooth for lower power con-
sumption, and infrared for immediate response to
common controls and basic navigation.
</bodyText>
<sectionHeader confidence="0.998505" genericHeader="conclusions">
8 Conclusions &amp; Future Work
</sectionHeader>
<bodyText confidence="0.999993608695652">
We have presented a prototype system that demon-
strates the feasibility of deploying a multimodal,
natural language interface on a mobile device for
browsing and managing one’s home media library.
In developing the prototype, we have experimented
with a novel role for a mobile device—that of a
speech-enabled remote control. We have demon-
strated a flexible natural language understanding ar-
chitecture, in which various processing stages may
be performed on either the server or mobile device,
as networking and processing power considerations
require.
While the mobile platform presents many chal-
lenges, it also provides unique opportunities.
Whereas desktop computers and TV remote controls
tend to be shared by multiple users, a mobile device
is typically used by a single individual. By collect-
ing and adapting to the usage data, the system can
personalize the recognition and understanding mod-
els to improve the system accuracy. In future sys-
tems, we hope to not only explore such adaptation
possibilities, but also study how real users interact
with the system to further improve the user interface.
</bodyText>
<sectionHeader confidence="0.997475" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9998565">
This research is sponsored by the TParty Project,
a joint research program between MIT and Quanta
Computer, Inc.; and by Nokia, as part of a joint MIT-
Nokia collaboration. We are also thankful to three
anonymous reviewers for their constructive feed-
back.
</bodyText>
<sectionHeader confidence="0.998481" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997366380952381">
A. Acero, N. Bernstein, R. Chambers, Y. C. Jui, X. Li,
J. Odell, P. Nguyen, O. Scholz, and G. Zweig. 2008.
Live search for mobile: Web services by voice on the
cellphone. In Proc. of ICASSP.
L. Baptist and S. Seneff. 2000. Genesis-II: A versatile
system for language generation in conversational sys-
tem applications. In Proc. of ICSLP.
A. Berglund and P. Johansson. 2004. Using speech and
dialogue for interactive TV navigation. Universal Ac-
cess in the Information Society, 3(3-4):224–238.
G. Chung, S. Seneff, C. Wang, and L. Hetherington.
2004. A dynamic vocabulary spoken dialogue inter-
face. In Proc. of INTERSPEECH, pages 327–330.
E. Filisko and S. Seneff. 2003. A context resolution
server for the GALAXY conversational systems. In
Proc. of EUROSPEECH.
B. Ford, J. Strauss, C. Lesniewski-Laas, S. Rhea,
F. Kaashoek, and R. Morris. 2006. Persistent personal
names for globally connected mobile devices. In Pro-
ceedings of the 7th USENIX Symposium on Operating
Systems Design and Implementation (OSDI ’06).
</reference>
<page confidence="0.976537">
8
</page>
<reference confidence="0.9996763">
K. Fujita, H. Kuwano, T. Tsuzuki, and Y. Ono. 2003.
A new digital TV interface employing speech recog-
nition. IEEE Transactions on Consumer Electronics,
49(3):765–769.
J. Glass. 2003. A probabilistic framework for segment-
based speech recognition. Computer Speech and Lan-
guage, 17:137–152.
A. Gruenstein, S. Seneff, and C. Wang. 2006. Scalable
and portable web-based multimodal dialogue interac-
tion with geographical databases. In Proc. of INTER-
SPEECH.
I. L. Hetherington. 2005. A multi-pass, dynamic-
vocabulary approach to real-time, large-vocabulary
speech recognition. In Proc. of INTERSPEECH.
I. L. Hetherington. 2007. PocketSUMMIT: Small-
footprint continuous speech recognition. In Proc. of
INTERSPEECH, pages 1465–1468.
B. Johanson, A. Fox, and T. Winograd. 2002. The in-
teractive workspaces project: Experiences with ubiq-
uitous computing rooms. IEEE Pervasive Computing,
1(2):67–74.
M. Johnston, L. F. D’Haro, M. Levine, and B. Renger.
2007. A multimodal interface for access to content in
the home. In Proc. of ACL, pages 376–383.
J. Mazzola Paluska, H. Pham, U. Saif, C. Terman, and
S. Ward. 2006. Reducing configuration overhead with
goal-oriented programming. In PerCom Workshops,
pages 596–599. IEEE Computer Society.
J. Mazzola Paluska, H. Pham, U. Saif, G. Chau, C. Ter-
man, and S. Ward. 2008. Structured decomposition of
adapative applications. In Proc. of 6th IEEE Confer-
ence on Pervasive Computing and Communications.
I. McGraw and S. Seneff. 2007. Immersive second lan-
guage acquisition in narrow domains: A prototype IS-
LAND dialogue system. In Proc. of the Speech and
Language Technology in Education Workshop.
J. Nichols and B. A. Myers. 2006. Controlling home and
office appliances with smartphones. IEEE Pervasive
Computing, special issue on SmartPhones, 5(3):60–
67, July-Sept.
H.-J. Oh, C.-H. Lee, M.-G. Jang, and Y. K. Lee. 2007.
An intelligent TV interface based on statistical dia-
logue management. IEEE Transactions on Consumer
Electronics, 53(4).
T. Paek, M. Agrawala, S. Basu, S. Drucker, T. Kristjans-
son, R. Logan, K. Toyama, and A. Wilson. 2004. To-
ward universal mobile interaction for shared displays.
In Proc. of Computer Supported Cooperative Work.
J. Polifroni, G. Chung, and S. Seneff. 2003. Towards
the automatic generation of mixed-initiative dialogue
systems from web content. In Proc. EUROSPEECH,
pages 193–196.
T. Portele, S. Goronzy, M. Emele, A. Kellner, S. Torge,
and J. te Vrugt. 2003. SmartKom-Home - an ad-
vanced multi-modal interface to home entertainment.
In Proc. of INTERSPEECH.
S. Seneff, E. Hurley, R. Lau, C. Pao, P. Schmid, and
V. Zue. 1998. GALAXY-II: A reference architecture
for conversational system development. In Proc. IC-
SLP.
S. Seneff, C. Wang, and T. J. Hazen. 2003. Automatic in-
duction of n-gram language models from a natural lan-
guage grammar. In Proceedings of EUROSPEECH.
S. Seneff. 1992. TINA: A natural language system
for spoken language applications. Computational Lin-
guistics, 18(1):61–86.
S. Seneff. 2002. Response planning and generation in
the MERCURY flight reservation system. Computer
Speech and Language, 16:283–312.
S. Seneff. 2007. Reversible sound-to-letter/letter-to-
sound modeling based on syllable structure. In Proc.
of HLT-NAACL.
K. Wittenburg, T. Lanning, D. Schwenke, H. Shubin, and
A. Vetro. 2006. The prospects for unrestricted speech
input for TV content search. In Proc. of AVI’06.
V. Zue, S. Seneff, J. Glass, J. Polifroni, C. Pao, T. J.
Hazen, and L. Hetherington. 2000. JUPITER: A
telephone-based conversational interface for weather
information. IEEE Transactions on Speech and Audio
Processing, 8(1), January.
</reference>
<page confidence="0.997104">
9
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.965239">
<title confidence="0.999635">A Multimodal Home Entertainment Interface via a Mobile Device</title>
<author confidence="0.9869675">Alexander Gruenstein Bo-June Hsu James Glass Stephanie Lee Hetherington Scott Cyphers Ibrahim Badr Chao Wang Sean</author>
<affiliation confidence="0.998917">MIT Computer Science and Artificial Intelligence</affiliation>
<address confidence="0.999176">32 Vassar St, Cambridge, MA 02139</address>
<web confidence="0.999251">http://www.sls.csail.mit.edu/</web>
<abstract confidence="0.99969085">We describe a multimodal dialogue system for interacting with a home entertainment center via a mobile device. In our working prototype, users may utilize both a graphical and speech user interface to search TV listings, record and play television programs, and listen to music. The developed framework is quite generic, potentially supporting a wide variety of applications, as we demonstrate by integrating a weather forecast application. In the prototype, the mobile device serves as the locus of interaction, providing both a small touchscreen display, and speech input and output; while the TV screen features a larger, richer GUI. The system architecture is agnostic to the location of the natural language processing components: a consistent user experience is maintained regardless of whether they run on a remote server or on the device itself.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Acero</author>
<author>N Bernstein</author>
<author>R Chambers</author>
<author>Y C Jui</author>
<author>X Li</author>
<author>J Odell</author>
<author>P Nguyen</author>
<author>O Scholz</author>
<author>G Zweig</author>
</authors>
<title>Live search for mobile: Web services by voice on the cellphone.</title>
<date>2008</date>
<booktitle>In Proc. of ICASSP.</booktitle>
<contexts>
<context position="7592" citStr="Acero et al., 2008" startWordPosition="1221" endWordPosition="1224">njunction with larger, shared displays. For instance, (Paek et al., 2004) demonstrate a framework for building such applications. The prototype we demonstrate here fits into their “Jukebox” model of interaction. Interactive workspaces, such as the one described in (Johanson et al., 2002), also demonstrate the utility of integrating mobile and large screen displays. Our prototype is a departure from these systems, however, in that it provides for spoken interactions. Finally, there is related work in the use of mobile devices for various kinds of search. For instance, offerings from Microsoft (Acero et al., 2008), Vlingo,1 and Promptu2 allow users to search for items like businesses and songs using their mobile phones. These applications differ from ours in that speech is used only for search, without any accompanying command and control capabilities. Also, these services do not allow interaction with your own devices at home. Efforts have been made to use mobile devices for control of devices in the home, such as in (Nichols and Myers, 2006), however these efforts have not involved the use of speech as an input modality. 1http://www.vlingo.com 2http://www.promptu.com 2 Holdto-talk button Opera web br</context>
</contexts>
<marker>Acero, Bernstein, Chambers, Jui, Li, Odell, Nguyen, Scholz, Zweig, 2008</marker>
<rawString>A. Acero, N. Bernstein, R. Chambers, Y. C. Jui, X. Li, J. Odell, P. Nguyen, O. Scholz, and G. Zweig. 2008. Live search for mobile: Web services by voice on the cellphone. In Proc. of ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Baptist</author>
<author>S Seneff</author>
</authors>
<title>Genesis-II: A versatile system for language generation in conversational system applications.</title>
<date>2000</date>
<booktitle>In Proc. of ICSLP.</booktitle>
<contexts>
<context position="14831" citStr="Baptist and Seneff, 2000" startWordPosition="2373" endWordPosition="2376"> Recognizer Language Understanding Language Generation Dialogue Manager Text-To-Speech (a) (b) Figure 3: Two architecture diagrams. In (a) speech recognition and natural language processing occur on the server, while in (b) processing is primarily performed on the device. Based on the resulting meaning representation, the dialogue manager (Polifroni et al., 2003) incorporates contextual information (Filisko and Seneff, 2003), and then determines an appropriate response. The response consists of an update to the graphical display, and a spoken system response which is realized via the GENESIS (Baptist and Seneff, 2000) language generation module. To support ondevice processing, all the components are linked via the GALAXY framework (Seneff et al., 1998) with an additional Mobile Manager component responsible for coordinating the communication between the mobile device and the home media server. In the currently deployed system, we use a mobile phone with a 624 MHz ARM processor running the Windows Mobile operating system and Opera Mobile web browser. The TV program and music databases reside on the home media server running GNU/Linux. The TV program guide data and recording capabilities are provided via Myt</context>
<context position="23108" citStr="Baptist and Seneff, 2000" startWordPosition="3632" endWordPosition="3635">n of a semantic representation from the parse tree. Dialogue Management &amp; Language Generation Once an utterance is recognized and parsed, the meaning representation is passed to the context resolution and dialogue manager component. The context resolution module (Filisko and Seneff, 2003) applies generic and domain-specific rules to resolve anaphora and deixis, and to interpret fragments and ellipsis in context. The dialogue manager then interacts with the application back-end and database, controlled by a script customized for the domain (Polifroni et al., 2003). Finally, the GENESIS module (Baptist and Seneff, 2000) applies domain-specific rules to generate a natural language representation of the dialogue manager’s response, which is sent to a speech synthesizer. The dialogue manager also sends an update to the GUI, so that, for example, the appropriate database search results are displayed. 7 Mobile Design Challenges Dialogue systems for mobile devices present a unique set of design challenges not found in telephony and desktop applications. Here we describe some of the design choices made while developing this prototype, and discuss their tradeoffs. 7.1 Client/Server Tradeoffs Towards supporting netwo</context>
</contexts>
<marker>Baptist, Seneff, 2000</marker>
<rawString>L. Baptist and S. Seneff. 2000. Genesis-II: A versatile system for language generation in conversational system applications. In Proc. of ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Berglund</author>
<author>P Johansson</author>
</authors>
<title>Using speech and dialogue for interactive TV navigation.</title>
<date>2004</date>
<booktitle>Universal Access in the Information Society,</booktitle>
<pages>3--3</pages>
<contexts>
<context position="5247" citStr="Berglund and Johansson, 2004" startWordPosition="838" endWordPosition="842">erver or the mobile device itself. Regardless of the configuration, a consistent user experience is maintained. 2 Related Work Various academic researchers and commercial businesses have demonstrated speech-enabled interfaces to entertainment centers. A good deal of the work focuses on adding a microphone to a remote control, so that speech input may be used in addition to a traditional remote control. Much commercial work, for example (Fujita et al., 2003), tends to focus on constrained grammar systems, where speech input is limited to a small set of templates corresponding to menu choices. (Berglund and Johansson, 2004) present a remote-control based speech interface for navigating an existing interactive television on-screen menu, though experimenters manually transcribed user utterances as they spoke instead of using a speech recognizer. (Oh et al., 2007) present a dialogue system for TV control that makes use of concept spotting and statistical dialogue management to understand queries. A version of their system can run independently on low-resource devices such as PDAs; however, it has a smaller vocabulary and supports a limited set of user utterance templates. Finally, (Wittenburg et al., 2006) look mai</context>
</contexts>
<marker>Berglund, Johansson, 2004</marker>
<rawString>A. Berglund and P. Johansson. 2004. Using speech and dialogue for interactive TV navigation. Universal Access in the Information Society, 3(3-4):224–238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Chung</author>
<author>S Seneff</author>
<author>C Wang</author>
<author>L Hetherington</author>
</authors>
<title>A dynamic vocabulary spoken dialogue interface.</title>
<date>2004</date>
<booktitle>In Proc. of INTERSPEECH,</booktitle>
<pages>327--330</pages>
<contexts>
<context position="4189" citStr="Chung et al., 2004" startWordPosition="672" endWordPosition="675">ocessing, pages 1–9, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics sources and network bandwidth, it is important that the architecture allows for multiple configurations in terms of the location of the natural language processing components. For instance, when a device is connected to a Wi-Fi network at home, recognition latency may be reduced by performing speech and natural language processing on the home media server. Moreover, a powerful server may enable more sophisticated processing techniques, such as multipass speech recognition (Hetherington, 2005; Chung et al., 2004), for improved accuracy. In situations with reduced network connectivity, latency may be improved by performing speech recognition and natural language processing tasks on the mobile device itself. Given resource constraints, however, less detailed acoustic and language models may be required. We have developed just such a flexible architecture, with many of the natural language processing components able to run on either a server or the mobile device itself. Regardless of the configuration, a consistent user experience is maintained. 2 Related Work Various academic researchers and commercial </context>
<context position="21079" citStr="Chung et al., 2004" startWordPosition="3329" endWordPosition="3332">collected from real users. However, when building prototypes such as the one described here, little or no training data is usually available. Thus, we create a domain-specific context-free grammar to generate a supplemental corpus of synthetic utterances. The corpus is used to train probabilities for the natural language parsing grammar (described immediately below), which in turn is used to derive a class ngram language model (Seneff et al., 2003). Classes in the language model which correspond to contents of the database are marked as dynamic, and are populated at runtime from the database (Chung et al., 2004; Hetherington, 2005). Database entries are heuristically normalized into spoken forms. Pronunciations not in our 150,000 word lexicon are automatically generated (Seneff, 2007). Parser Grammar The TINA parser uses a probabilistic context-free grammar enhanced with support for wh-movement and grammatical agreement constraints. We have developed a generic syntactic grammar by examining hundreds of thousands of utterances collected from real user interactions with various existing dialogue systems. In addition, we have developed libraries which parse and interpret common semantic classes like da</context>
</contexts>
<marker>Chung, Seneff, Wang, Hetherington, 2004</marker>
<rawString>G. Chung, S. Seneff, C. Wang, and L. Hetherington. 2004. A dynamic vocabulary spoken dialogue interface. In Proc. of INTERSPEECH, pages 327–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Filisko</author>
<author>S Seneff</author>
</authors>
<title>A context resolution server for the GALAXY conversational systems.</title>
<date>2003</date>
<booktitle>In Proc. of EUROSPEECH.</booktitle>
<contexts>
<context position="14634" citStr="Filisko and Seneff, 2003" startWordPosition="2341" endWordPosition="2344"> device is recognized using the landmarkbased SUMMIT system (Glass, 2003). The resulting N-best hypotheses are processed by the TINA language understanding component (Seneff, 1992). 4 Galaxy Speech Recognizer Language Understanding Language Generation Dialogue Manager Text-To-Speech (a) (b) Figure 3: Two architecture diagrams. In (a) speech recognition and natural language processing occur on the server, while in (b) processing is primarily performed on the device. Based on the resulting meaning representation, the dialogue manager (Polifroni et al., 2003) incorporates contextual information (Filisko and Seneff, 2003), and then determines an appropriate response. The response consists of an update to the graphical display, and a spoken system response which is realized via the GENESIS (Baptist and Seneff, 2000) language generation module. To support ondevice processing, all the components are linked via the GALAXY framework (Seneff et al., 1998) with an additional Mobile Manager component responsible for coordinating the communication between the mobile device and the home media server. In the currently deployed system, we use a mobile phone with a 624 MHz ARM processor running the Windows Mobile operating</context>
<context position="22772" citStr="Filisko and Seneff, 2003" startWordPosition="3580" endWordPosition="3583"> 2003). At runtime, the recognizer tags the hypothesized dynamic class expansions with their class names, allowing the parser grammar to be independent of the database contents. Furthermore, each semantic class is designated either as a semantic entity, or as an attribute associated with a particular entity. This enables the generation of a semantic representation from the parse tree. Dialogue Management &amp; Language Generation Once an utterance is recognized and parsed, the meaning representation is passed to the context resolution and dialogue manager component. The context resolution module (Filisko and Seneff, 2003) applies generic and domain-specific rules to resolve anaphora and deixis, and to interpret fragments and ellipsis in context. The dialogue manager then interacts with the application back-end and database, controlled by a script customized for the domain (Polifroni et al., 2003). Finally, the GENESIS module (Baptist and Seneff, 2000) applies domain-specific rules to generate a natural language representation of the dialogue manager’s response, which is sent to a speech synthesizer. The dialogue manager also sends an update to the GUI, so that, for example, the appropriate database search resu</context>
</contexts>
<marker>Filisko, Seneff, 2003</marker>
<rawString>E. Filisko and S. Seneff. 2003. A context resolution server for the GALAXY conversational systems. In Proc. of EUROSPEECH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Ford</author>
<author>J Strauss</author>
<author>C Lesniewski-Laas</author>
<author>S Rhea</author>
<author>F Kaashoek</author>
<author>R Morris</author>
</authors>
<title>Persistent personal names for globally connected mobile devices.</title>
<date>2006</date>
<booktitle>In Proceedings of the 7th USENIX Symposium on Operating Systems Design and Implementation (OSDI ’06).</booktitle>
<contexts>
<context position="16394" citStr="Ford et al., 2006" startWordPosition="2622" endWordPosition="2625">gine Lucene.4 Lastly, the TV display can be driven by a web browser on either the home media server or a separate computer connected to the server via a fast Ethernet connection, for high quality video streaming. 3http://www.mythtv.org/ 4http://lucene.apache.org/ While the focus of this paper is on the natural language processing and user interface aspects of the system, our work is actually situated within a larger collaborative project at MIT that also includes simplified device configuration (Mazzola Paluska et al., 2008; Mazzola Paluska et al., 2006), transparent access to remote servers (Ford et al., 2006), and improved security. 5 Mobile Natural Language Components Porting the implementation of the various speech recognizer and natural language processing components to mobile devices with limited computation and memory presents both a research and engineering challenge. Instead of creating a small vocabulary, fixed phrase dialogue system, we aim to support—on the mobile device—the same flexible and natural language interactions currently available on our desktop, tablet, and telephony systems; see e.g., (Gruenstein et al., 2006; Seneff, 2002; Zue et al., 2000). In this section, we summarize ou</context>
</contexts>
<marker>Ford, Strauss, Lesniewski-Laas, Rhea, Kaashoek, Morris, 2006</marker>
<rawString>B. Ford, J. Strauss, C. Lesniewski-Laas, S. Rhea, F. Kaashoek, and R. Morris. 2006. Persistent personal names for globally connected mobile devices. In Proceedings of the 7th USENIX Symposium on Operating Systems Design and Implementation (OSDI ’06).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Fujita</author>
<author>H Kuwano</author>
<author>T Tsuzuki</author>
<author>Y Ono</author>
</authors>
<title>A new digital TV interface employing speech recognition.</title>
<date>2003</date>
<journal>IEEE Transactions on Consumer Electronics,</journal>
<volume>49</volume>
<issue>3</issue>
<contexts>
<context position="5079" citStr="Fujita et al., 2003" startWordPosition="810" endWordPosition="813"> models may be required. We have developed just such a flexible architecture, with many of the natural language processing components able to run on either a server or the mobile device itself. Regardless of the configuration, a consistent user experience is maintained. 2 Related Work Various academic researchers and commercial businesses have demonstrated speech-enabled interfaces to entertainment centers. A good deal of the work focuses on adding a microphone to a remote control, so that speech input may be used in addition to a traditional remote control. Much commercial work, for example (Fujita et al., 2003), tends to focus on constrained grammar systems, where speech input is limited to a small set of templates corresponding to menu choices. (Berglund and Johansson, 2004) present a remote-control based speech interface for navigating an existing interactive television on-screen menu, though experimenters manually transcribed user utterances as they spoke instead of using a speech recognizer. (Oh et al., 2007) present a dialogue system for TV control that makes use of concept spotting and statistical dialogue management to understand queries. A version of their system can run independently on low</context>
</contexts>
<marker>Fujita, Kuwano, Tsuzuki, Ono, 2003</marker>
<rawString>K. Fujita, H. Kuwano, T. Tsuzuki, and Y. Ono. 2003. A new digital TV interface employing speech recognition. IEEE Transactions on Consumer Electronics, 49(3):765–769.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Glass</author>
</authors>
<title>A probabilistic framework for segmentbased speech recognition.</title>
<date>2003</date>
<journal>Computer Speech and Language,</journal>
<pages>17--137</pages>
<contexts>
<context position="14082" citStr="Glass, 2003" startWordPosition="2267" endWordPosition="2268">tion latency may be reduced by performing speech recognition and natural language processing on the device. On the other hand, streaming audio via a fast wireless network to the server for processing may result in improved accuracy. In the prototype system, flexible and reusable speech recognition and natural language processing capabilities are provided via generic components developed and deployed in numerous spoken dialogue systems by our group, with the exception of an offthe-shelf speech synthesizer. Speech input from the mobile device is recognized using the landmarkbased SUMMIT system (Glass, 2003). The resulting N-best hypotheses are processed by the TINA language understanding component (Seneff, 1992). 4 Galaxy Speech Recognizer Language Understanding Language Generation Dialogue Manager Text-To-Speech (a) (b) Figure 3: Two architecture diagrams. In (a) speech recognition and natural language processing occur on the server, while in (b) processing is primarily performed on the device. Based on the resulting meaning representation, the dialogue manager (Polifroni et al., 2003) incorporates contextual information (Filisko and Seneff, 2003), and then determines an appropriate response. T</context>
</contexts>
<marker>Glass, 2003</marker>
<rawString>J. Glass. 2003. A probabilistic framework for segmentbased speech recognition. Computer Speech and Language, 17:137–152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gruenstein</author>
<author>S Seneff</author>
<author>C Wang</author>
</authors>
<title>Scalable and portable web-based multimodal dialogue interaction with geographical databases.</title>
<date>2006</date>
<booktitle>In Proc. of INTERSPEECH.</booktitle>
<contexts>
<context position="16927" citStr="Gruenstein et al., 2006" startWordPosition="2701" endWordPosition="2704">8; Mazzola Paluska et al., 2006), transparent access to remote servers (Ford et al., 2006), and improved security. 5 Mobile Natural Language Components Porting the implementation of the various speech recognizer and natural language processing components to mobile devices with limited computation and memory presents both a research and engineering challenge. Instead of creating a small vocabulary, fixed phrase dialogue system, we aim to support—on the mobile device—the same flexible and natural language interactions currently available on our desktop, tablet, and telephony systems; see e.g., (Gruenstein et al., 2006; Seneff, 2002; Zue et al., 2000). In this section, we summarize our efforts thus far in implementing the SUMMIT speech recognizer and TINA natural language parser. Ports of the GENESIS language generation system and of our dialogue manager are well underway, and we expect to have these components working on the mobile device in the near future. 5.1 PocketSUMMIT To significantly reduce the memory footprint and overall computation, we chose to reimplement our segment-based speech recognizer from scratch, utilizing fixed-point arithmetic, parameter quantization, and bit-packing in the binary mod</context>
</contexts>
<marker>Gruenstein, Seneff, Wang, 2006</marker>
<rawString>A. Gruenstein, S. Seneff, and C. Wang. 2006. Scalable and portable web-based multimodal dialogue interaction with geographical databases. In Proc. of INTERSPEECH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I L Hetherington</author>
</authors>
<title>A multi-pass, dynamicvocabulary approach to real-time, large-vocabulary speech recognition.</title>
<date>2005</date>
<booktitle>In Proc. of INTERSPEECH.</booktitle>
<contexts>
<context position="4168" citStr="Hetherington, 2005" startWordPosition="670" endWordPosition="671">n Mobile Language Processing, pages 1–9, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics sources and network bandwidth, it is important that the architecture allows for multiple configurations in terms of the location of the natural language processing components. For instance, when a device is connected to a Wi-Fi network at home, recognition latency may be reduced by performing speech and natural language processing on the home media server. Moreover, a powerful server may enable more sophisticated processing techniques, such as multipass speech recognition (Hetherington, 2005; Chung et al., 2004), for improved accuracy. In situations with reduced network connectivity, latency may be improved by performing speech recognition and natural language processing tasks on the mobile device itself. Given resource constraints, however, less detailed acoustic and language models may be required. We have developed just such a flexible architecture, with many of the natural language processing components able to run on either a server or the mobile device itself. Regardless of the configuration, a consistent user experience is maintained. 2 Related Work Various academic resear</context>
<context position="21100" citStr="Hetherington, 2005" startWordPosition="3333" endWordPosition="3334">users. However, when building prototypes such as the one described here, little or no training data is usually available. Thus, we create a domain-specific context-free grammar to generate a supplemental corpus of synthetic utterances. The corpus is used to train probabilities for the natural language parsing grammar (described immediately below), which in turn is used to derive a class ngram language model (Seneff et al., 2003). Classes in the language model which correspond to contents of the database are marked as dynamic, and are populated at runtime from the database (Chung et al., 2004; Hetherington, 2005). Database entries are heuristically normalized into spoken forms. Pronunciations not in our 150,000 word lexicon are automatically generated (Seneff, 2007). Parser Grammar The TINA parser uses a probabilistic context-free grammar enhanced with support for wh-movement and grammatical agreement constraints. We have developed a generic syntactic grammar by examining hundreds of thousands of utterances collected from real user interactions with various existing dialogue systems. In addition, we have developed libraries which parse and interpret common semantic classes like dates, times, and numbe</context>
</contexts>
<marker>Hetherington, 2005</marker>
<rawString>I. L. Hetherington. 2005. A multi-pass, dynamicvocabulary approach to real-time, large-vocabulary speech recognition. In Proc. of INTERSPEECH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I L Hetherington</author>
</authors>
<title>PocketSUMMIT: Smallfootprint continuous speech recognition.</title>
<date>2007</date>
<booktitle>In Proc. of INTERSPEECH,</booktitle>
<pages>1465--1468</pages>
<marker>Hetherington, 2007</marker>
<rawString>I. L. Hetherington. 2007. PocketSUMMIT: Smallfootprint continuous speech recognition. In Proc. of INTERSPEECH, pages 1465–1468.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Johanson</author>
<author>A Fox</author>
<author>T Winograd</author>
</authors>
<title>The interactive workspaces project: Experiences with ubiquitous computing rooms.</title>
<date>2002</date>
<journal>IEEE Pervasive Computing,</journal>
<volume>1</volume>
<issue>2</issue>
<contexts>
<context position="7261" citStr="Johanson et al., 2002" startWordPosition="1167" endWordPosition="1170">nt—in this case a database of movies. The search can be constrained by attributes such as title, director, or starring actors. The tablet PC pen can be used to handwrite queries and to point at items (such as actor names) while the user speaks. We were also inspired by previous prototypes in which mobile devices have been used in conjunction with larger, shared displays. For instance, (Paek et al., 2004) demonstrate a framework for building such applications. The prototype we demonstrate here fits into their “Jukebox” model of interaction. Interactive workspaces, such as the one described in (Johanson et al., 2002), also demonstrate the utility of integrating mobile and large screen displays. Our prototype is a departure from these systems, however, in that it provides for spoken interactions. Finally, there is related work in the use of mobile devices for various kinds of search. For instance, offerings from Microsoft (Acero et al., 2008), Vlingo,1 and Promptu2 allow users to search for items like businesses and songs using their mobile phones. These applications differ from ours in that speech is used only for search, without any accompanying command and control capabilities. Also, these services do n</context>
</contexts>
<marker>Johanson, Fox, Winograd, 2002</marker>
<rawString>B. Johanson, A. Fox, and T. Winograd. 2002. The interactive workspaces project: Experiences with ubiquitous computing rooms. IEEE Pervasive Computing, 1(2):67–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnston</author>
<author>L F D’Haro</author>
<author>M Levine</author>
<author>B Renger</author>
</authors>
<title>A multimodal interface for access to content in the home.</title>
<date>2007</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>376--383</pages>
<marker>Johnston, D’Haro, Levine, Renger, 2007</marker>
<rawString>M. Johnston, L. F. D’Haro, M. Levine, and B. Renger. 2007. A multimodal interface for access to content in the home. In Proc. of ACL, pages 376–383.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Mazzola Paluska</author>
<author>H Pham</author>
<author>U Saif</author>
<author>C Terman</author>
<author>S Ward</author>
</authors>
<title>Reducing configuration overhead with goal-oriented programming.</title>
<date>2006</date>
<booktitle>In PerCom Workshops,</booktitle>
<pages>596--599</pages>
<publisher>IEEE Computer Society.</publisher>
<contexts>
<context position="16336" citStr="Paluska et al., 2006" startWordPosition="2612" endWordPosition="2615">13 major genres, indexed using the open-source text search engine Lucene.4 Lastly, the TV display can be driven by a web browser on either the home media server or a separate computer connected to the server via a fast Ethernet connection, for high quality video streaming. 3http://www.mythtv.org/ 4http://lucene.apache.org/ While the focus of this paper is on the natural language processing and user interface aspects of the system, our work is actually situated within a larger collaborative project at MIT that also includes simplified device configuration (Mazzola Paluska et al., 2008; Mazzola Paluska et al., 2006), transparent access to remote servers (Ford et al., 2006), and improved security. 5 Mobile Natural Language Components Porting the implementation of the various speech recognizer and natural language processing components to mobile devices with limited computation and memory presents both a research and engineering challenge. Instead of creating a small vocabulary, fixed phrase dialogue system, we aim to support—on the mobile device—the same flexible and natural language interactions currently available on our desktop, tablet, and telephony systems; see e.g., (Gruenstein et al., 2006; Seneff,</context>
</contexts>
<marker>Paluska, Pham, Saif, Terman, Ward, 2006</marker>
<rawString>J. Mazzola Paluska, H. Pham, U. Saif, C. Terman, and S. Ward. 2006. Reducing configuration overhead with goal-oriented programming. In PerCom Workshops, pages 596–599. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Mazzola Paluska</author>
<author>H Pham</author>
<author>U Saif</author>
<author>G Chau</author>
<author>C Terman</author>
<author>S Ward</author>
</authors>
<title>Structured decomposition of adapative applications.</title>
<date>2008</date>
<booktitle>In Proc. of 6th IEEE Conference on Pervasive Computing and Communications.</booktitle>
<contexts>
<context position="16305" citStr="Paluska et al., 2008" startWordPosition="2607" endWordPosition="2610">ongs from over 80 artists and 13 major genres, indexed using the open-source text search engine Lucene.4 Lastly, the TV display can be driven by a web browser on either the home media server or a separate computer connected to the server via a fast Ethernet connection, for high quality video streaming. 3http://www.mythtv.org/ 4http://lucene.apache.org/ While the focus of this paper is on the natural language processing and user interface aspects of the system, our work is actually situated within a larger collaborative project at MIT that also includes simplified device configuration (Mazzola Paluska et al., 2008; Mazzola Paluska et al., 2006), transparent access to remote servers (Ford et al., 2006), and improved security. 5 Mobile Natural Language Components Porting the implementation of the various speech recognizer and natural language processing components to mobile devices with limited computation and memory presents both a research and engineering challenge. Instead of creating a small vocabulary, fixed phrase dialogue system, we aim to support—on the mobile device—the same flexible and natural language interactions currently available on our desktop, tablet, and telephony systems; see e.g., (G</context>
</contexts>
<marker>Paluska, Pham, Saif, Chau, Terman, Ward, 2008</marker>
<rawString>J. Mazzola Paluska, H. Pham, U. Saif, G. Chau, C. Terman, and S. Ward. 2008. Structured decomposition of adapative applications. In Proc. of 6th IEEE Conference on Pervasive Computing and Communications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I McGraw</author>
<author>S Seneff</author>
</authors>
<title>Immersive second language acquisition in narrow domains: A prototype ISLAND dialogue system.</title>
<date>2007</date>
<booktitle>In Proc. of the Speech and Language Technology in Education Workshop.</booktitle>
<contexts>
<context position="26156" citStr="McGraw and Seneff, 2007" startWordPosition="4093" endWordPosition="4096">on significantly simplifies barge-in support, while reducing power consumption. 7.3 Graphical User Interface In addition to supporting interactive natural language dialogues via the spoken user interface, the prototype system implements a graphical user interface (GUI) on the mobile device to supplement the TV’s on-screen interface. To faciliate rapid prototyping, we chose to implement both the mobile and TV GUI using web pages with AJAX (Asynchronous Javascript and XML) techniques, an approach we have leveraged in several existing multimodal dialogue systems, e.g. (Gruenstein et al., 7 2006; McGraw and Seneff, 2007). The resulting interface is largely platform-independent and allows display updates to be “pushed” to the client browser. As many users are already familiar with the TV’s on-screen interface, we chose to mirror the same interface on the mobile device and synchronize the selection cursor. However, unlike desktop GUIs, mobile devices are constrained by a small display, limited computational power, and reduced network bandwidth. Thus, both the page layout and information detail were adjusted for the mobile browser. Although AJAX is more responsive than traditional web technology, rendering large</context>
</contexts>
<marker>McGraw, Seneff, 2007</marker>
<rawString>I. McGraw and S. Seneff. 2007. Immersive second language acquisition in narrow domains: A prototype ISLAND dialogue system. In Proc. of the Speech and Language Technology in Education Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nichols</author>
<author>B A Myers</author>
</authors>
<title>Controlling home and office appliances with smartphones.</title>
<date>2006</date>
<journal>IEEE Pervasive Computing, special issue on SmartPhones,</journal>
<volume>5</volume>
<issue>3</issue>
<pages>67</pages>
<contexts>
<context position="8030" citStr="Nichols and Myers, 2006" startWordPosition="1296" endWordPosition="1299"> it provides for spoken interactions. Finally, there is related work in the use of mobile devices for various kinds of search. For instance, offerings from Microsoft (Acero et al., 2008), Vlingo,1 and Promptu2 allow users to search for items like businesses and songs using their mobile phones. These applications differ from ours in that speech is used only for search, without any accompanying command and control capabilities. Also, these services do not allow interaction with your own devices at home. Efforts have been made to use mobile devices for control of devices in the home, such as in (Nichols and Myers, 2006), however these efforts have not involved the use of speech as an input modality. 1http://www.vlingo.com 2http://www.promptu.com 2 Holdto-talk button Opera web browser Status icon indicates conn microphone icon wh Navigation pad o be used to nav Figure 1: User interface overview. (c) Music Library U: What’s on tonight at seven o’clock? S: I found seventy entries. [Displays program guide.] U: Are there any action movies on HBO tomorrow? S: I found two entries. [Displays results.] U: When is American Idol on? S: I found three entries. [Displays results.] U: Record the first one. S: I have added </context>
</contexts>
<marker>Nichols, Myers, 2006</marker>
<rawString>J. Nichols and B. A. Myers. 2006. Controlling home and office appliances with smartphones. IEEE Pervasive Computing, special issue on SmartPhones, 5(3):60– 67, July-Sept.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H-J Oh</author>
<author>C-H Lee</author>
<author>M-G Jang</author>
<author>Y K Lee</author>
</authors>
<title>An intelligent TV interface based on statistical dialogue management.</title>
<date>2007</date>
<journal>IEEE Transactions on Consumer Electronics,</journal>
<volume>53</volume>
<issue>4</issue>
<contexts>
<context position="5489" citStr="Oh et al., 2007" startWordPosition="875" endWordPosition="878">ood deal of the work focuses on adding a microphone to a remote control, so that speech input may be used in addition to a traditional remote control. Much commercial work, for example (Fujita et al., 2003), tends to focus on constrained grammar systems, where speech input is limited to a small set of templates corresponding to menu choices. (Berglund and Johansson, 2004) present a remote-control based speech interface for navigating an existing interactive television on-screen menu, though experimenters manually transcribed user utterances as they spoke instead of using a speech recognizer. (Oh et al., 2007) present a dialogue system for TV control that makes use of concept spotting and statistical dialogue management to understand queries. A version of their system can run independently on low-resource devices such as PDAs; however, it has a smaller vocabulary and supports a limited set of user utterance templates. Finally, (Wittenburg et al., 2006) look mainly at the problem of searching for television programs using speech, an on-screen display, and a remote control. They explore a Speech-In List-Out interface to searching for episodes of television programs. (Portele et al., 2003) depart from</context>
</contexts>
<marker>Oh, Lee, Jang, Lee, 2007</marker>
<rawString>H.-J. Oh, C.-H. Lee, M.-G. Jang, and Y. K. Lee. 2007. An intelligent TV interface based on statistical dialogue management. IEEE Transactions on Consumer Electronics, 53(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Paek</author>
<author>M Agrawala</author>
<author>S Basu</author>
<author>S Drucker</author>
<author>T Kristjansson</author>
<author>R Logan</author>
<author>K Toyama</author>
<author>A Wilson</author>
</authors>
<title>Toward universal mobile interaction for shared displays.</title>
<date>2004</date>
<booktitle>In Proc. of Computer Supported Cooperative Work.</booktitle>
<contexts>
<context position="7046" citStr="Paek et al., 2004" startWordPosition="1136" endWordPosition="1139">tle. Users can also perform typical remote-control tasks like turning the television on and off, and changing the channel. (Johnston et al., 2007) also use a tablet PC to provide an interface to television content—in this case a database of movies. The search can be constrained by attributes such as title, director, or starring actors. The tablet PC pen can be used to handwrite queries and to point at items (such as actor names) while the user speaks. We were also inspired by previous prototypes in which mobile devices have been used in conjunction with larger, shared displays. For instance, (Paek et al., 2004) demonstrate a framework for building such applications. The prototype we demonstrate here fits into their “Jukebox” model of interaction. Interactive workspaces, such as the one described in (Johanson et al., 2002), also demonstrate the utility of integrating mobile and large screen displays. Our prototype is a departure from these systems, however, in that it provides for spoken interactions. Finally, there is related work in the use of mobile devices for various kinds of search. For instance, offerings from Microsoft (Acero et al., 2008), Vlingo,1 and Promptu2 allow users to search for item</context>
</contexts>
<marker>Paek, Agrawala, Basu, Drucker, Kristjansson, Logan, Toyama, Wilson, 2004</marker>
<rawString>T. Paek, M. Agrawala, S. Basu, S. Drucker, T. Kristjansson, R. Logan, K. Toyama, and A. Wilson. 2004. Toward universal mobile interaction for shared displays. In Proc. of Computer Supported Cooperative Work.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Polifroni</author>
<author>G Chung</author>
<author>S Seneff</author>
</authors>
<title>Towards the automatic generation of mixed-initiative dialogue systems from web content.</title>
<date>2003</date>
<booktitle>In Proc. EUROSPEECH,</booktitle>
<pages>193--196</pages>
<contexts>
<context position="14571" citStr="Polifroni et al., 2003" startWordPosition="2333" endWordPosition="2336">offthe-shelf speech synthesizer. Speech input from the mobile device is recognized using the landmarkbased SUMMIT system (Glass, 2003). The resulting N-best hypotheses are processed by the TINA language understanding component (Seneff, 1992). 4 Galaxy Speech Recognizer Language Understanding Language Generation Dialogue Manager Text-To-Speech (a) (b) Figure 3: Two architecture diagrams. In (a) speech recognition and natural language processing occur on the server, while in (b) processing is primarily performed on the device. Based on the resulting meaning representation, the dialogue manager (Polifroni et al., 2003) incorporates contextual information (Filisko and Seneff, 2003), and then determines an appropriate response. The response consists of an update to the graphical display, and a spoken system response which is realized via the GENESIS (Baptist and Seneff, 2000) language generation module. To support ondevice processing, all the components are linked via the GALAXY framework (Seneff et al., 1998) with an additional Mobile Manager component responsible for coordinating the communication between the mobile device and the home media server. In the currently deployed system, we use a mobile phone wi</context>
<context position="23052" citStr="Polifroni et al., 2003" startWordPosition="3624" endWordPosition="3627">d with a particular entity. This enables the generation of a semantic representation from the parse tree. Dialogue Management &amp; Language Generation Once an utterance is recognized and parsed, the meaning representation is passed to the context resolution and dialogue manager component. The context resolution module (Filisko and Seneff, 2003) applies generic and domain-specific rules to resolve anaphora and deixis, and to interpret fragments and ellipsis in context. The dialogue manager then interacts with the application back-end and database, controlled by a script customized for the domain (Polifroni et al., 2003). Finally, the GENESIS module (Baptist and Seneff, 2000) applies domain-specific rules to generate a natural language representation of the dialogue manager’s response, which is sent to a speech synthesizer. The dialogue manager also sends an update to the GUI, so that, for example, the appropriate database search results are displayed. 7 Mobile Design Challenges Dialogue systems for mobile devices present a unique set of design challenges not found in telephony and desktop applications. Here we describe some of the design choices made while developing this prototype, and discuss their tradeof</context>
</contexts>
<marker>Polifroni, Chung, Seneff, 2003</marker>
<rawString>J. Polifroni, G. Chung, and S. Seneff. 2003. Towards the automatic generation of mixed-initiative dialogue systems from web content. In Proc. EUROSPEECH, pages 193–196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Portele</author>
<author>S Goronzy</author>
<author>M Emele</author>
<author>A Kellner</author>
<author>S Torge</author>
<author>J te Vrugt</author>
</authors>
<title>SmartKom-Home - an advanced multi-modal interface to home entertainment.</title>
<date>2003</date>
<booktitle>In Proc. of INTERSPEECH.</booktitle>
<contexts>
<context position="6077" citStr="Portele et al., 2003" startWordPosition="969" endWordPosition="972">ech recognizer. (Oh et al., 2007) present a dialogue system for TV control that makes use of concept spotting and statistical dialogue management to understand queries. A version of their system can run independently on low-resource devices such as PDAs; however, it has a smaller vocabulary and supports a limited set of user utterance templates. Finally, (Wittenburg et al., 2006) look mainly at the problem of searching for television programs using speech, an on-screen display, and a remote control. They explore a Speech-In List-Out interface to searching for episodes of television programs. (Portele et al., 2003) depart from the model of adding a speech interface component to an existing on-screen menu. Instead, they a create a tablet PC interface to an electronic program guide, though they do not use the television display as well. Users may search an electronic program guide using constraints such as date, time, and genre; however, they can’t search by title. Users can also perform typical remote-control tasks like turning the television on and off, and changing the channel. (Johnston et al., 2007) also use a tablet PC to provide an interface to television content—in this case a database of movies. </context>
</contexts>
<marker>Portele, Goronzy, Emele, Kellner, Torge, Vrugt, 2003</marker>
<rawString>T. Portele, S. Goronzy, M. Emele, A. Kellner, S. Torge, and J. te Vrugt. 2003. SmartKom-Home - an advanced multi-modal interface to home entertainment. In Proc. of INTERSPEECH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Seneff</author>
<author>E Hurley</author>
<author>R Lau</author>
<author>C Pao</author>
<author>P Schmid</author>
<author>V Zue</author>
</authors>
<title>GALAXY-II: A reference architecture for conversational system development.</title>
<date>1998</date>
<booktitle>In Proc. ICSLP.</booktitle>
<contexts>
<context position="14968" citStr="Seneff et al., 1998" startWordPosition="2394" endWordPosition="2397">ech recognition and natural language processing occur on the server, while in (b) processing is primarily performed on the device. Based on the resulting meaning representation, the dialogue manager (Polifroni et al., 2003) incorporates contextual information (Filisko and Seneff, 2003), and then determines an appropriate response. The response consists of an update to the graphical display, and a spoken system response which is realized via the GENESIS (Baptist and Seneff, 2000) language generation module. To support ondevice processing, all the components are linked via the GALAXY framework (Seneff et al., 1998) with an additional Mobile Manager component responsible for coordinating the communication between the mobile device and the home media server. In the currently deployed system, we use a mobile phone with a 624 MHz ARM processor running the Windows Mobile operating system and Opera Mobile web browser. The TV program and music databases reside on the home media server running GNU/Linux. The TV program guide data and recording capabilities are provided via MythTV, a full-featured, open-source digital video recorder software package.3 Daily updates to the program guide information typically cont</context>
</contexts>
<marker>Seneff, Hurley, Lau, Pao, Schmid, Zue, 1998</marker>
<rawString>S. Seneff, E. Hurley, R. Lau, C. Pao, P. Schmid, and V. Zue. 1998. GALAXY-II: A reference architecture for conversational system development. In Proc. ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Seneff</author>
<author>C Wang</author>
<author>T J Hazen</author>
</authors>
<title>Automatic induction of n-gram language models from a natural language grammar.</title>
<date>2003</date>
<booktitle>In Proceedings of EUROSPEECH.</booktitle>
<contexts>
<context position="20913" citStr="Seneff et al., 2003" startWordPosition="3300" endWordPosition="3303">set of semantic classes which correspond to the back-end application’s database, such as artist, album, and genre. Ideally, we would have a corpus of tagged utterances collected from real users. However, when building prototypes such as the one described here, little or no training data is usually available. Thus, we create a domain-specific context-free grammar to generate a supplemental corpus of synthetic utterances. The corpus is used to train probabilities for the natural language parsing grammar (described immediately below), which in turn is used to derive a class ngram language model (Seneff et al., 2003). Classes in the language model which correspond to contents of the database are marked as dynamic, and are populated at runtime from the database (Chung et al., 2004; Hetherington, 2005). Database entries are heuristically normalized into spoken forms. Pronunciations not in our 150,000 word lexicon are automatically generated (Seneff, 2007). Parser Grammar The TINA parser uses a probabilistic context-free grammar enhanced with support for wh-movement and grammatical agreement constraints. We have developed a generic syntactic grammar by examining hundreds of thousands of utterances collected </context>
<context position="22153" citStr="Seneff et al., 2003" startWordPosition="3488" endWordPosition="3491">actions with various existing dialogue systems. In addition, we have developed libraries which parse and interpret common semantic classes like dates, times, and numbers. The grammar and semantic libraries provide good coverage for spoken dialogue systems in database-query domains. 6 To build a grammar for a new domain, a developer extends the generic syntactic grammar by augmenting it with domain-specific semantic categories and their lexical entries. A probability model which conditions each node category on its left sibling and parent is then estimated from a training corpus of utterances (Seneff et al., 2003). At runtime, the recognizer tags the hypothesized dynamic class expansions with their class names, allowing the parser grammar to be independent of the database contents. Furthermore, each semantic class is designated either as a semantic entity, or as an attribute associated with a particular entity. This enables the generation of a semantic representation from the parse tree. Dialogue Management &amp; Language Generation Once an utterance is recognized and parsed, the meaning representation is passed to the context resolution and dialogue manager component. The context resolution module (Filisk</context>
</contexts>
<marker>Seneff, Wang, Hazen, 2003</marker>
<rawString>S. Seneff, C. Wang, and T. J. Hazen. 2003. Automatic induction of n-gram language models from a natural language grammar. In Proceedings of EUROSPEECH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Seneff</author>
</authors>
<title>TINA: A natural language system for spoken language applications.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>1</issue>
<contexts>
<context position="14189" citStr="Seneff, 1992" startWordPosition="2282" endWordPosition="2283">. On the other hand, streaming audio via a fast wireless network to the server for processing may result in improved accuracy. In the prototype system, flexible and reusable speech recognition and natural language processing capabilities are provided via generic components developed and deployed in numerous spoken dialogue systems by our group, with the exception of an offthe-shelf speech synthesizer. Speech input from the mobile device is recognized using the landmarkbased SUMMIT system (Glass, 2003). The resulting N-best hypotheses are processed by the TINA language understanding component (Seneff, 1992). 4 Galaxy Speech Recognizer Language Understanding Language Generation Dialogue Manager Text-To-Speech (a) (b) Figure 3: Two architecture diagrams. In (a) speech recognition and natural language processing occur on the server, while in (b) processing is primarily performed on the device. Based on the resulting meaning representation, the dialogue manager (Polifroni et al., 2003) incorporates contextual information (Filisko and Seneff, 2003), and then determines an appropriate response. The response consists of an update to the graphical display, and a spoken system response which is realized </context>
</contexts>
<marker>Seneff, 1992</marker>
<rawString>S. Seneff. 1992. TINA: A natural language system for spoken language applications. Computational Linguistics, 18(1):61–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Seneff</author>
</authors>
<title>Response planning and generation in the MERCURY flight reservation system. Computer Speech and Language,</title>
<date>2002</date>
<pages>16--283</pages>
<contexts>
<context position="16941" citStr="Seneff, 2002" startWordPosition="2705" endWordPosition="2706">, 2006), transparent access to remote servers (Ford et al., 2006), and improved security. 5 Mobile Natural Language Components Porting the implementation of the various speech recognizer and natural language processing components to mobile devices with limited computation and memory presents both a research and engineering challenge. Instead of creating a small vocabulary, fixed phrase dialogue system, we aim to support—on the mobile device—the same flexible and natural language interactions currently available on our desktop, tablet, and telephony systems; see e.g., (Gruenstein et al., 2006; Seneff, 2002; Zue et al., 2000). In this section, we summarize our efforts thus far in implementing the SUMMIT speech recognizer and TINA natural language parser. Ports of the GENESIS language generation system and of our dialogue manager are well underway, and we expect to have these components working on the mobile device in the near future. 5.1 PocketSUMMIT To significantly reduce the memory footprint and overall computation, we chose to reimplement our segment-based speech recognizer from scratch, utilizing fixed-point arithmetic, parameter quantization, and bit-packing in the binary model files. The </context>
</contexts>
<marker>Seneff, 2002</marker>
<rawString>S. Seneff. 2002. Response planning and generation in the MERCURY flight reservation system. Computer Speech and Language, 16:283–312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Seneff</author>
</authors>
<title>Reversible sound-to-letter/letter-tosound modeling based on syllable structure.</title>
<date>2007</date>
<booktitle>In Proc. of HLT-NAACL.</booktitle>
<contexts>
<context position="21256" citStr="Seneff, 2007" startWordPosition="3353" endWordPosition="3354">t-free grammar to generate a supplemental corpus of synthetic utterances. The corpus is used to train probabilities for the natural language parsing grammar (described immediately below), which in turn is used to derive a class ngram language model (Seneff et al., 2003). Classes in the language model which correspond to contents of the database are marked as dynamic, and are populated at runtime from the database (Chung et al., 2004; Hetherington, 2005). Database entries are heuristically normalized into spoken forms. Pronunciations not in our 150,000 word lexicon are automatically generated (Seneff, 2007). Parser Grammar The TINA parser uses a probabilistic context-free grammar enhanced with support for wh-movement and grammatical agreement constraints. We have developed a generic syntactic grammar by examining hundreds of thousands of utterances collected from real user interactions with various existing dialogue systems. In addition, we have developed libraries which parse and interpret common semantic classes like dates, times, and numbers. The grammar and semantic libraries provide good coverage for spoken dialogue systems in database-query domains. 6 To build a grammar for a new domain, a</context>
<context position="26156" citStr="Seneff, 2007" startWordPosition="4095" endWordPosition="4096">antly simplifies barge-in support, while reducing power consumption. 7.3 Graphical User Interface In addition to supporting interactive natural language dialogues via the spoken user interface, the prototype system implements a graphical user interface (GUI) on the mobile device to supplement the TV’s on-screen interface. To faciliate rapid prototyping, we chose to implement both the mobile and TV GUI using web pages with AJAX (Asynchronous Javascript and XML) techniques, an approach we have leveraged in several existing multimodal dialogue systems, e.g. (Gruenstein et al., 7 2006; McGraw and Seneff, 2007). The resulting interface is largely platform-independent and allows display updates to be “pushed” to the client browser. As many users are already familiar with the TV’s on-screen interface, we chose to mirror the same interface on the mobile device and synchronize the selection cursor. However, unlike desktop GUIs, mobile devices are constrained by a small display, limited computational power, and reduced network bandwidth. Thus, both the page layout and information detail were adjusted for the mobile browser. Although AJAX is more responsive than traditional web technology, rendering large</context>
</contexts>
<marker>Seneff, 2007</marker>
<rawString>S. Seneff. 2007. Reversible sound-to-letter/letter-tosound modeling based on syllable structure. In Proc. of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Wittenburg</author>
<author>T Lanning</author>
<author>D Schwenke</author>
<author>H Shubin</author>
<author>A Vetro</author>
</authors>
<title>The prospects for unrestricted speech input for TV content search.</title>
<date>2006</date>
<booktitle>In Proc. of AVI’06.</booktitle>
<contexts>
<context position="5838" citStr="Wittenburg et al., 2006" startWordPosition="932" endWordPosition="935">s. (Berglund and Johansson, 2004) present a remote-control based speech interface for navigating an existing interactive television on-screen menu, though experimenters manually transcribed user utterances as they spoke instead of using a speech recognizer. (Oh et al., 2007) present a dialogue system for TV control that makes use of concept spotting and statistical dialogue management to understand queries. A version of their system can run independently on low-resource devices such as PDAs; however, it has a smaller vocabulary and supports a limited set of user utterance templates. Finally, (Wittenburg et al., 2006) look mainly at the problem of searching for television programs using speech, an on-screen display, and a remote control. They explore a Speech-In List-Out interface to searching for episodes of television programs. (Portele et al., 2003) depart from the model of adding a speech interface component to an existing on-screen menu. Instead, they a create a tablet PC interface to an electronic program guide, though they do not use the television display as well. Users may search an electronic program guide using constraints such as date, time, and genre; however, they can’t search by title. Users</context>
</contexts>
<marker>Wittenburg, Lanning, Schwenke, Shubin, Vetro, 2006</marker>
<rawString>K. Wittenburg, T. Lanning, D. Schwenke, H. Shubin, and A. Vetro. 2006. The prospects for unrestricted speech input for TV content search. In Proc. of AVI’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Zue</author>
<author>S Seneff</author>
<author>J Glass</author>
<author>J Polifroni</author>
<author>C Pao</author>
<author>T J Hazen</author>
<author>L Hetherington</author>
</authors>
<title>JUPITER: A telephone-based conversational interface for weather information.</title>
<date>2000</date>
<journal>IEEE Transactions on Speech and Audio Processing,</journal>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="12106" citStr="Zue et al., 2000" startWordPosition="1959" endWordPosition="1962">.g. “Please record the second one.” Depressing the hold-to-talk button not only terminates any current spoken response, but also mutes the TV to minimize interference with speech recognition. As the previous example demonstrates, contextual information is used to resolve list position references and disambiguate commands. The speech interface to the user’s music library works in a similar fashion. Users can search by artist, album, and song name, and then play the songs found. To demonstrate the extensibility of the architecture, we have also integrated an existing weather information system (Zue et al., 2000), which has been previously deployed as a telephony application. Users simply click on the Weather tab to switch to this domain, allowing them to ask a wide range of weather queries. The system responds verbally and with a simple graphical forecast. To create a natural user experience, we designed the multimodal interface to allow users to seamlessly switch among the different input modalities available on the mobile device. Figure 2 demonstrates an example interaction with the prototype, as well as several screenshots of the user interface. 4 System Architecture The system architecture is qui</context>
<context position="16960" citStr="Zue et al., 2000" startWordPosition="2707" endWordPosition="2710">parent access to remote servers (Ford et al., 2006), and improved security. 5 Mobile Natural Language Components Porting the implementation of the various speech recognizer and natural language processing components to mobile devices with limited computation and memory presents both a research and engineering challenge. Instead of creating a small vocabulary, fixed phrase dialogue system, we aim to support—on the mobile device—the same flexible and natural language interactions currently available on our desktop, tablet, and telephony systems; see e.g., (Gruenstein et al., 2006; Seneff, 2002; Zue et al., 2000). In this section, we summarize our efforts thus far in implementing the SUMMIT speech recognizer and TINA natural language parser. Ports of the GENESIS language generation system and of our dialogue manager are well underway, and we expect to have these components working on the mobile device in the near future. 5.1 PocketSUMMIT To significantly reduce the memory footprint and overall computation, we chose to reimplement our segment-based speech recognizer from scratch, utilizing fixed-point arithmetic, parameter quantization, and bit-packing in the binary model files. The resulting PocketSUM</context>
</contexts>
<marker>Zue, Seneff, Glass, Polifroni, Pao, Hazen, Hetherington, 2000</marker>
<rawString>V. Zue, S. Seneff, J. Glass, J. Polifroni, C. Pao, T. J. Hazen, and L. Hetherington. 2000. JUPITER: A telephone-based conversational interface for weather information. IEEE Transactions on Speech and Audio Processing, 8(1), January.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>