<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.986934">
The Necessity of Combining Adaptation Methods
</title>
<author confidence="0.984743">
Ming-Wei Chang, Michael Connor and Dan Roth
</author>
<affiliation confidence="0.988959">
University of Illinois at Urbana Champaign
</affiliation>
<address confidence="0.783052">
Urbana, IL 61801
</address>
<email confidence="0.999404">
{mchang21,connor2,danr}@uiuc.edu
</email>
<sectionHeader confidence="0.992874" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.798478481481482">
Problems stemming from domain adaptation
continue to plague the statistical natural lan-
guage processing community. There has been
continuing work trying to find general purpose
algorithms to alleviate this problem. In this
paper we argue that existing general purpose
approaches usually only focus on one of two
issues related to the difficulties faced by adap-
tation: 1) difference in base feature statistics
or 2) task differences that can be detected with
labeled data.
We argue that it is necessary to combine these
two classes of adaptation algorithms, using
evidence collected through theoretical analy-
sis and simulated and real-world data exper-
iments. We find that the combined approach
often outperforms the individual adaptation
approaches. By combining simple approaches
from each class of adaptation algorithm, we
achieve state-of-the-art results for both Named
Entity Recognition adaptation task and the
Preposition Sense Disambiguation adaptation
task. Second, we also show that applying an
adaptation algorithm that finds shared repre-
sentation between domains often impacts the
choice in adaptation algorithm that makes use
of target labeled data.
</bodyText>
<sectionHeader confidence="0.998874" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999922756097561">
While recent advances in statistical modeling for
natural language processing are exciting, the prob-
lem of domain adaptation remains a big challenge.
It is widely known that a classifier trained on one do-
main (e.g. news domain) usually performs poorly on
a different domain (e.g. medical domain) (Jiang and
Zhai, 2007; Daum´e III, 2007). The inability of cur-
rent statistical models to handle multiple domains is
one of the key obstacles hindering the progress of
NLP.
Several general purpose algorithms have been
proposed to address the domain adaptation prob-
lem: (Blitzer et al., 2006; Jiang and Zhai, 2007;
Daum´e III, 2007; Finkel and Manning, 2009). It
is widely believed that the drop in performance of
statistical models on new domains is due to the
shift of the joint distribution of labels and examples,
P(Y, X), from domain to domain, where X repre-
sents the input space and Y represents the output
space. In general, we can separate existing adap-
tation algorithms into two categories:
Focuses on P(X) This type of adaptation algo-
rithm attempts to resolve the difference between the
feature space statistics of two domains. While many
different techniques have been proposed, the com-
mon goal of these algorithms is to find a better
shared representation that brings the source domain
and the target domain closer. Often these algorithms
do not use labeled examples in the target domain.
The works (Blitzer et al., 2006; Huang and Yates,
2009) all belong to this category.
Focuses on P(Y |X) These adaptation algorithms
assume that there exists a small amount of labeled
data for the target domain. Instead of training two
weight vectors independently (one for source and
the other for the target domain), these algorithms try
to relate the source and target weight vectors. This is
often achieved by using a special designed regular-
ization term. The works (Chelba and Acero, 2004;
Daum´e III, 2007; Finkel and Manning, 2009) belong
to this category.
</bodyText>
<page confidence="0.942687">
767
</page>
<note confidence="0.817301">
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 767–777,
MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.9999650625">
It is important to give the definition of an adapta-
tion framework. An adaptation framework is speci-
fied by the data/resources used and a specific learn-
ing algorithm. For example, a framework that used
only source labeled examples and one that used both
source and target labeled examples should be con-
sidered as two different frameworks, even though
they might use exactly the same training algorithm.
Note that the goal of a good adaptation framework is
to perform well on the target domain and quite often
we only need to change the data/resource used to in-
crease the performance without changing the train-
ing algorithm. We refer to frameworks that do not
use target labeled data and focus on P(X) as Unla-
beled Adaptation Frameworks and refer to frame-
works that use algorithms that focus on P (Y X) as
</bodyText>
<subsectionHeader confidence="0.627122">
Labeled Adaptation Frameworks.
</subsectionHeader>
<bodyText confidence="0.999939090909091">
The major difference between unlabeled adapta-
tion frameworks and labeled adaptation frameworks
is the use of target labeled examples. Unlabeled
adaptation frameworks do not use target labeled ex-
amples1, while the labeled adaptation frameworks
make use of target labeled examples. Under this
definition, we consider that a model trained on both
source and target labeled examples (later referred as
S+T) is a labeled adaptation framework.
It is important to combine the labeled and unla-
beled adaptation frameworks for two reasons:
</bodyText>
<listItem confidence="0.930456230769231">
• Mutual Benefit: We analyze these two types
of frameworks and find that they address dif-
ferent adaptation issues. Therefore, it is often
beneficial to apply them together.
• Complex Interaction: Another, probably
more important issue, is that these two types
of frameworks are not independent. Different
representations will impact how much a labeled
adaptation algorithm can transfer information
between domains. Therefore, in order to have a
clear picture of what is the best labeled adapta-
tion framework, it is necessary to analyze these
two domain adaptation frameworks together.
</listItem>
<bodyText confidence="0.939691">
In this paper, we assume we have both a small
amount of target labeled data and a large amount
1Note that we still use labeled data from source domain in
an unlabeled adaptation framework.
of unlabeled data so that we can perform both unla-
beled and labeled adaptation. The goal of our paper
is to point out the necessity of applying these two
adaptation frameworks together. To the best of our
knowledge, this is the first paper that both theoreti-
cally and empirically analyzes the interdependence
between the impact of labeled and unlabeled adap-
tation frameworks.
The contribution of this paper is as follows:
</bodyText>
<listItem confidence="0.836372333333333">
• Propose a theoretical analysis of the “Frustrat-
ingly Easy” (FE) framework (Daum´e III, 2007)
(Section 3).
</listItem>
<bodyText confidence="0.8463076">
The theoretical analysis shows that for FE to be
effective the domains must already be “close”.
At some threshold of “closeness” it is better to
switch from FE to just pool all training together
as one domain.
</bodyText>
<listItem confidence="0.9785925">
• Demonstrate the complex interaction between
unlabeled and labeled approaches (Section 4)
</listItem>
<bodyText confidence="0.97788425">
We construct artificial experiments that demon-
strate how applying unlabeled adaptation may
impact the behavior of two labeled adaptation
approaches.
</bodyText>
<listItem confidence="0.547317">
• Empirically analyze the interaction on real
datasets (Section 5).
</listItem>
<bodyText confidence="0.999877">
We show that in general combining both ap-
proaches on the tasks of preposition sense
disambiguation and named entity recognition
works better than either individual method.
Our approach not only achieves state-of-the-
art results on these two tasks but it also re-
veals something surprising – finding a bet-
ter shared representation often makes a sim-
ple source+target approach the best adaptation
framework in practice.
</bodyText>
<sectionHeader confidence="0.852147" genericHeader="method">
2 Two Adaptation Aspects: A Review
</sectionHeader>
<bodyText confidence="0.999652">
Why do we need two types of adaptation frame-
works? First, unlabeled adaptation frameworks are
necessary since many features only exist in one do-
main. Therefore, it is important to develop algo-
rithms that find features which work across domains.
On the other hand, labeled adaptation frameworks
</bodyText>
<page confidence="0.995171">
768
</page>
<bodyText confidence="0.999709025641026">
are also required because we would like to take ad-
vantages of target labeled data. Even though differ-
ent domains may have different definitions for la-
bels (say in named entity recognition, specific defi-
nition of PER/LOC/ORG may change), labeled data
should still be useful. We summarize these distinc-
tions in Table 1.
While these two aspects of adaptation both saw
significant progress in the past few years, little anal-
ysis has been done on the interaction between these
two types of algorithms2.
In order to have a deep analysis, it is necessary to
choose specific adaptation algorithms for each as-
pect of adaptation framework. While we mainly
conduct analysis on the algorithms we picked, we
would like to point out that the necessity of com-
bining these two types of adaptation algorithms has
been largely ignored in the community.
As our example adaptation algorithms we se-
lected:
Labeled adaptation: FE framework One of the
most popular adaptation frameworks that requires
the use of labeled target data is the “Frustrat-
ingly Easy” (FE) adaptation framework (Daum´e III,
2007). However, why and when this framework
works remains unclear in the NLP community. The
FE framework can be viewed as an framework that
extends the feature space, and it requires source and
target labeled data to work. We denote n as the
total number of features3 and m is the number of
the “domains”, where one of the domains is the tar-
get domain. The FE framework creates a global
weight vector in Rn(m+1), an extended space for all
domains. The representation x of the t-th domain
is mapped by 4bt(x) E Rn(m+1). In the extended
space, the first n features consist of the “shared”
block, which is always active across all tasks. The
(t+1)-th block (the (nt+1)-th to the (nt+n)-th fea-
tures) is a “specific” block, and is only active when
</bodyText>
<footnote confidence="0.982699">
2Among the previously mentioned work, (Jiang and Zhai,
2007) is a special case given that it discusses both aspects of
adaptation algorithms. However, little analysis on the interac-
tion of the two aspects is discussed in that paper
3We assume that the number of features in each domain is
equal.
</footnote>
<bodyText confidence="0.47887">
extracting examples from the task t. More formally,
</bodyText>
<equation confidence="0.9104256">
2
6
�t(x) = 4x
����
shared
</equation>
<bodyText confidence="0.99702455">
A single weight vector Cv is obtained by training on
the modified labeled data {yti, -bt(xti)}mt�1. Given
that this framework only extends the feature space,
in this paper, we also call it the feature extension
framework (still called FE). We will see in Section 3
that this framework is equivalent to applying a reg-
ularization trick that bridges the source and the tar-
get domains. As it will become clear in Section 3,
in fact, this framework is only effective when there
is target labeled data and hence belongs to labeled
adaptation frameworks.
Although FE framework is quite popular in the
community, there are other even simpler labeled
adaptation frameworks that allow the use of tar-
get labeled data. For example, one of the simplest
frameworks is the S+T framework, which simply
trains a single model on the pooled and unextended
source and target training data.
Unlabeled adaptation: Adding cluster-like fea-
tures Recall that unlabeled adaptation frameworks
find the features that “work” across domain. In this
paper, we find such features in two steps. First,
we use word clusters generated from unlabeled text
and/or third party resources that spans domains.
Then, for every feature template that contains a
word, we append another feature template that uses
the word’s cluster instead of the word itself. This
technique is used in many recent works including
dependency parsing and NER (Koo et al., 2008;
Ratinov and Roth, 2009). Note that the unlabeled
text need not come from the source or target do-
main. In fact, in this paper, we use clusters gen-
erated with the Reuters 1996 dataset, a superset of
the CoNLL03 NER dataset (Koo et al., 2008; Liang,
2005). We adopt the Brown cluster algorithm to find
the word cluster (Brown et al., 1992; Liang, 2005).
We can use other resources to create clusters as well.
For example, in the NER domain, we also include
gazetteers4 as an unlabeled cluster resource, which
can bring the domains together quite effectively.
</bodyText>
<footnote confidence="0.585736">
4Our gazetteers comes from (Ratinov and Roth, 2009).
</footnote>
<equation confidence="0.722458714285714">
(t−1) blocks (�−t) blocks
� �� � � �� �
0 ... 0 ����
x 0 ... 0
specific
3
57. (1)
</equation>
<page confidence="0.990516">
769
</page>
<table confidence="0.9992495">
Framework Labeled Data Unlabeled Data Common Approach
Unlabeled Adaptation Source Encompasses Source and Target. Generate features that span domains us-
(Focus on P(X)) May use other third party resources ing unlabeled data and/or third party re-
(dictionaries, gazetteers, etc.). sources.
Labeled Adaptation Source and Target None Train classifier(s) using both source and
(Focus on P(YIX)) target training data, relating the two.
</table>
<tableCaption confidence="0.988835">
Table 1: Comparison between two general adaptation frameworks discussed in this paper. Each framework is specified by its setting
(data required) and its learning algorithm. Multiple previous adaptation approaches fit in one of either framework.
</tableCaption>
<bodyText confidence="0.948722769230769">
While other more complex algorithms (Ando and
Zhang, 2005; Blitzer et al., 2006) for finding bet-
ter shared representation (without using labeled tar-
get data) have been proposed, we find that using
straightforward clustering features is quite effective
in general.
3 Analysis of the FE Framework
In this section, we propose a simple yet informative
analysis of the FE algorithm from the perspective of
multi-task learning. Note that we ignore the effect
of unlabeled adaptation in this section, and focus on
the analysis of the FE framework as a representative
labeled adaptation framework.
</bodyText>
<subsectionHeader confidence="0.996633">
3.1 Mistake Bound Analysis
</subsectionHeader>
<bodyText confidence="0.999974117647059">
While (Daum´e III, 2007) proposed this framework
for adaptation, a very similar idea had been proposed
in (Evgeniou and Pontil, 2004) as a novel regular-
ization term for multitask learning with support vec-
tor machines. Assume that w1, w2, ... , wm are the
weight vector for the first domain to the m-th do-
main, respectively. The baseline approach is to as-
sume that each weight vector is independent. As-
sume that we adopt a SVM-like optimization prob-
lem that consider all m tasks, the baseline approach
is equivalent to using the following regularization
term in the objective function: Emt=1 Iwt112.
In (Evgeniou and Pontil, 2004; Daum´e III, 2007),
they assume that wt = u + vt, for t = 1, ... m,
where vt is the specific weight vector for t-th do-
main and u is a shared weight vector across all do-
mains. The new regularization term then becomes
</bodyText>
<equation confidence="0.987292">
Ilul12 + �m IlvtII2. (2)
t=1
</equation>
<bodyText confidence="0.8531685">
Note that these two regularization terms are differ-
ent, given that the new regularization term makes
w1, w2, . . . , wm not independent anymore. It fol-
lows that
wTt x = (u + vt)T x = ¯wT bt(x),
where
</bodyText>
<equation confidence="0.9843885">
¯wT = uT vT 1 . . . vT .
m
</equation>
<bodyText confidence="0.98041">
and II¯wI12 equals to Eq. (2). Therefore, we can think
feature extension framework as a learning frame-
work that adopts Eq. (2) as its regularization term.
The FE framework was in fact originally designed
for the problem of multitask learning so in the fol-
lowing, we propose a simple mistake bound analysis
based on the multitask setting, where we calculate
the mistakes on all domains5. We focus on multi-
task setting for two reasons: 1) the analysis is very
easy and intuitive, and 2) in Section 4.1, we empiri-
cally confirm that the analysis holds for the adapta-
tion setting.
In the following, we assume that the training
algorithm used in the FE framework is the on-
line perceptron learning algorithm (Novikoff, 1963).
This allows us to analyze the mistake bound of the
FE framework with the perceptron algorithm. The
bound can give us an insight on when and why one
should adopt the FE framework. By using the stan-
dard mistake bound theorem (Novikoff, 1963), we
show:
Theorem 1. Let Dt be the labeled data of domain t.
Assume that there exist w1, w2, ... , wm such that
ywTt x µ, V(x, y) E Dt,
and assume that max(X,y)EDt JxJJ &lt; R2,Vt =
1... m. Then, the number of mistakes made with
online perceptron training (Novikoff, 1963) and the
</bodyText>
<footnote confidence="0.9796525">
5In the adaptation setting, one generally only cares about the
performance on the target domain.
</footnote>
<page confidence="0.925229">
770
</page>
<equation confidence="0.907535333333333">
FE framework is bounded by
IIwtII2 − II �m t=1 wtII2
m + 1 ). (3)
</equation>
<bodyText confidence="0.994895916666667">
Proof. Define w¯ as a vector in Rn(m+1). We claim
that there exists a set SK, such that for all w¯ E SK,,
¯wT Φt(x) = wTt x for any domain t = 1... m. Note
that Φt(x) is defined in Eq. (1). We can construct SK,
in the following way:
SK, = {[s (w1 − s) ... (wm − s)]  |s E Rn},
where s is an arbitrary vector with n elements.
In order to obtain the best possible bound, we
would like to find the most compressed weight vec-
tor in SK,, w* = minK,ES�„ II ¯wII2.
The optimization problem has an analytical solu-
tion:
</bodyText>
<equation confidence="0.9997745">
IIw*II2 = �m IIwtII2 − II �m wtII2/(m + 1).
t=1 t=1
</equation>
<bodyText confidence="0.998503333333333">
The proof is completed by the standard mis-
take bound theorem and the following fact:
maxi II0t(x)II2 = 2 maxi IIxII2 &lt; 2R2.
</bodyText>
<subsectionHeader confidence="0.997496">
3.2 Mistake Bound Comparison
</subsectionHeader>
<bodyText confidence="0.9999866875">
In the following, we would like to explore under
what circumstances the FE framework can work bet-
ter than individual models and the S+T framework
using Theorem 1. The analysis is done based on the
assumption that all frameworks use the perceptron
algorithm.
Before showing the bound analysis, note that the
framework proposed by (Evgeniou and Pontil, 2004;
Finkel and Manning, 2009) is a generalization over
these three frameworks (FE, S+T, and the base-
line)6. However, our goal in this paper is different:
we try to provide a deep discussion on when and why
one should use a particular framework.
Here, we compare the mistake bounds of the fea-
ture sharing framework to that of the baseline ap-
proach, which learns each task independently7. In
</bodyText>
<footnote confidence="0.64014">
6The framework proposed by (Evgeniou and Pontil, 2004;
Finkel and Manning, 2009) is a generalization of Eq. (1). It
allows the user to weight each block of features. If we put zero
weight on the shared block, it becomes the baseline approach.
On the other hand, if we put zero weight on all task-specific
blocks, the framework becomes the S+T approach.
7Note that mistake bound results can be generalized to gen-
eralization bound results. See (Zhang, 2002).
</footnote>
<bodyText confidence="0.999797375">
order to make the comparison easier, we make some
simplifying assumptions. First, we assume that the
problem contains only two tasks, 1 and 2. We also
assume that IIw1II = IIw2II = a. These assump-
tions greatly reduce the complexity of the analysis
and can give us greater insight into the comparisons.
Following the assumptions and Theorem 1, the
mistake bound for the FE frameworks is
</bodyText>
<equation confidence="0.822609">
4(2 − cos(w1, w2))R2a2/(3µ2) (4)
</equation>
<bodyText confidence="0.992080235294117">
This line of analysis leads to interesting bound com-
parisons for two cases. In the first case, we assume
that task 1 and task 2 are essentially the same. In the
second, more common case, we assume that they are
different.
First, when we know a priori that task 1 and task
2 are essentially the same, we can combine the train-
ing data from the two tasks and train them as a sin-
gle task. Therefore, given that we do not need to
expand the feature space, the number of mistakes is
now bounded by R2a2/µ2. Note that this bound is
in fact better than (4) with cos(w1, w2) = 1. There-
fore, if we know a priori that these two tasks are the
same, training a single model is better than using the
feature shared approach.
In practice, it is often the case that the two tasks
are not the same. In this case, the number of mis-
takes of an independent approach on both task 1 and
2 will be bounded by the summation of the mistake
bounds of task 1 and task 2. Therefore, using the
independent approach, the number of mistakes for
the perceptron algorithm on both tasks is bounded
by 2R2a2/µ2. The following results can be obtained
by directly comparing the two bounds,
Corollary 1. Assume there exists w1 and w2 which
separate D1 and D2 respectively with functional
margin µ, and IIw1II = IIw2II = a. In this case:
(4) will be smaller than the bound of individual ap-
proach, 2R2a2/µ2, if and only if cos(w1, w2) =
(wT1 w2)/(IIw1IIIIw2II) &gt; 12.
If we assume that there is no difference in
P(X) between domains and hence we can treat
cos(w1, w2) as the similarity between two tasks, the
above argument suggests:
</bodyText>
<listItem confidence="0.994578666666667">
• If the two tasks are very different, the baseline
approach (building two models) is better than
FE and S+T.
</listItem>
<equation confidence="0.7561605">
�m
t=1
2R2
µ2 (
</equation>
<page confidence="0.962233">
771
</page>
<listItem confidence="0.9955375">
• If the tasks are similar enough, FE is better than
baseline and S+T.
• If the tasks are almost the same, S+T becomes
better than FE and baseline.
</listItem>
<bodyText confidence="0.8394585">
In Section 4.1, we will evaluate whether these claims
can be justified empirically.
</bodyText>
<sectionHeader confidence="0.997625" genericHeader="method">
4 Artificial Data Experiment Study
</sectionHeader>
<bodyText confidence="0.9808396">
In this section we will present artificial experiments.
We have two primary goals: 1) verifying the analysis
proposed in Section 3, and 2) showing that the repre-
sentation shift will impact the behavior of the FE al-
gorithm. The second point will be verified again in
the real world experiments in Section 5.
Data Generation In the following artificial ex-
periments we experiment with domain adaptation
by generating training and test data for two tasks,
source and target, where we can control the differ-
ence between task definitions. The general proce-
dure can be divided into two steps: 1) generating
weight vectors z1 and z2 (for source and target re-
spectively), and 2) randomly generating labeled in-
stances for training and testing using z1 and z2.
The different experiments start with the same ba-
sic z1 and z2, but then may alter these weights to
introduce task dissimilarities or similarities. The ba-
sic z1 and z2 are both generated by a multivariate
Gaussian distribution with mean z and a diagonal
covariance matrix QI:
z1 - N(z, QI), z2 - N(z, QI),
where N is the normal distribution and z is random
vector with zero mean. Note that z is only used to
generate z1 and z2. There is one parameter, Q, that
controls the variance of the Gaussian distribution.
Hence we use Q to roughly control the “angle” of z1
and z2. When Q is close to zero, z1 and z2 will be
very similar. On the other hand, when Q is large, z1
and z2 can be very different. In these experiments,
we vary Q between 0.01 and 5 so that we are exper-
imenting only with tasks where the weight the task
difference is the “angle” or cosine between z1 and
z2. Once we obtain the z1 and z2, we normalize
them to the unit length.
After selecting z1 and z2, we then generate la-
beled instances (x, y) for the source task in the fol-
lowing way. For each example x, we randomly gen-
erate n binary features, where each feature has 20%
chance to be active. We then label the example by
y = sign(zi x),
The data for the target task is generated similarly
with z2. In these experiments, we fix the number of
features n to be 500 and generate 100 source train-
ing examples and 40 target training examples, along
with 1000 target testing examples. This matches the
reasonable case in NLP where there are more fea-
tures than training examples and each feature vector
is sparse. In all of the experiments, we report the
averaged testing error rate on the target testing data.
</bodyText>
<subsectionHeader confidence="0.965192">
4.1 Experiment 1, FE algorithm
</subsectionHeader>
<bodyText confidence="0.9999658125">
Goal The goal here is to verify our theoretical
analysis in Section 3. Note that we do not introduce
representation shift in this experiment and assume
that both source and target domains use exactly the
same features.
Result Figure 1(a) shows the performance of the
three training algorithms as variance decreases and
thus cosine between weight vectors (or measure of
task similarity) goes to 1. Note that FE labeled adap-
tation framework beats TGT once the task cosine
passes approximately 0.6. Initially FE slightly out-
performs S+T until the tasks are close enough to-
gether that it is better to treat all the data as coming
from one task. Note that while the experiments are
based on the adaptation setting, the results match our
analysis based on the multitask setting in Section 3.
</bodyText>
<subsectionHeader confidence="0.992279">
4.2 Experiment 2, Unseen Features
</subsectionHeader>
<bodyText confidence="0.999941333333333">
Goal So far we have not considered the difference
in P(X) between domains. In the previous exper-
iment, we used only cosine as our task similarity
measurement to decide what is the best framework.
However, task similarity should consider the differ-
ence in both P(X) and P(Y |X), and the cosine
measurement is not sufficient for this. Here we con-
struct a simple example to show that even a simple
representation shift can change the behavior of the
labeled adaptation framework. This case shows that
S+T can be better than FE even when the tasks are
not similar according to the cosine measurement.
</bodyText>
<page confidence="0.986221">
772
</page>
<figure confidence="0.999935333333333">
0 0.2 0.4 0.6 0.8 1
Cosine
(a) Basic Similarity
0 0.2 0.4 0.6 0.8 1
Original Cosine
(b) Shared Features
Tgt
S+T
FE
Tgt
S+T
FE
Target % Error 0.46
0.44
0.42
0.4
0.38
0.36
0.34
0.32
0.3
Target % Error 0.36
0.355
0.35
0.345
0.34
0.335
0.33
0.325
0.32
</figure>
<figureCaption confidence="0.603955333333333">
Figure 1: Artificial Experiment comparing labeled adaptation performance vs. cosine between base weight vectors that defines
two tasks, before and after cross-domain shared features are added. Figure (a) shows results from experiment 1. For FE adaptation
algorithm to work the tasks need to be close (cosine &gt; 0.6), and if the tasks are close enough (cosine Pz� 1, dividing line) then
it is better to just pool source and target training data together (the S+T algorithm). Figure (b) shows results for experiment 3
when shared features are added to the base weight vectors as used in experiment 1. Here the cosine similarity measure is between
the base task weight vectors before the shared features have been added. Both labeled adaptation algorithms effectively use the
shared features to improve over just training on target. With shared features added the dividing line where S+T improves over
FE decreases so even for tasks that are initially further apart, once clusters are added the S+T algorithm does better than FE. Each
point represents the average of 2000 training runs with random initial z1 and z2 generating data.
</figureCaption>
<bodyText confidence="0.9994644375">
Result The second experiment deals with the case
where features may appear in only one domain but
should be treated like known features in the other
domain. An example of this are out of vocabulary
words that may not exist in a small target train-
ing task, but have synonyms in the source train-
ing data. In this case if we had features grouping
words (say by word meanings) then we would re-
cover this cross-domain information. In this experi-
ment we want to explore which adaptation algorithm
performs best before these features are applied.
To simulate this case we start with similar weight
vectors z1 and z2 (sampled with variance = 0.00001,
cos(z1,z2) Pz� 1), but then shift some set of dimen-
sions so that they represent features that appear only
in one domain.
</bodyText>
<equation confidence="0.9999315">
z1 = (a1, b1) zi = (0, b1, a1)
z2 = (a2, b2) z2 = (a2, b2, 0)
</equation>
<bodyText confidence="0.9998824">
By changing the ratio of the size of the dissimilar
subset a to the similar subset b we can make the
two weight vectors zi and z2 more or less similar.
Using these two new weight vectors we can proceed
as above, generating training and testing data.
</bodyText>
<figureCaption confidence="0.9070225">
Figure 2 shows the performance of the three algo-
Figure 2: Artificial Experiment where unknown features are
included in source or target domains, but not the other. The
simple S+T adaptation framework is best able to exploit the
set of shared features so performs best over the whole space of
similarity in this setting.
</figureCaption>
<bodyText confidence="0.998818">
rithms on this data as the number of unrelated fea-
tures are decreased. Over the entire range the com-
bined algorithm S+T does better since it more ef-
ficiently exploits the shared similar b subset of the
feature space. When the FE algorithm tries to cre-
ate the shared features, it considers both the similar
subset b and dissimilar subset a. However, since
a should not be shared, FE algorithm becomes less
</bodyText>
<figure confidence="0.998068066666667">
0 0.2 0.4 0.6 0.8 1
Cosine
Target % Error
0.355
0.345
0.335
0.325
0.315
0.35
0.34
0.33
0.32
Tgt
S+T
FE
</figure>
<page confidence="0.997993">
773
</page>
<bodyText confidence="0.9998768">
effective than the S+T algorithm. See the bound
comparison in Section 3.2 for more intuitions. With
this experiment we have demonstrated that there is
a need to consider label and unlabeled adaptation
frameworks together.
</bodyText>
<subsectionHeader confidence="0.996133">
4.3 Experiment 3, Shared Features
</subsectionHeader>
<bodyText confidence="0.994086775862069">
Goal A good unlabeled adaptation framework
should try to find features that “work” across do-
mains. However, it is not clear how these newly
added features will impact the behavior of the la-
beled adaptation frameworks. In this experiment, we
show that the new shared features will bring the do-
mains together, and hence make S+T a very strong
adaptation framework.
Result For the third experiment we start with the
same setup as in the first experiment, but then aug-
ment the initial weight vector with additional shared
weights. These shared weights correspond to the in-
troduction of features that appear in both domains
and have the same meaning relative to the tasks, the
ideal result of unlabeled adaptation methods.
To generate this case we again start with z1 and
z2 of varying similarity as in section 4.1, then gen-
erate a random weight vector for shared features and
append this to both weight vectors.
zs — N(0, I), z001 = (z1,-yzs), z002 = (z2,-yzs),
where -y is used to put increased importance on the
shared weight vectors by increasing the total weight
of that section relative to the base z1 and z2 subsets.
In our experiments we use 100 shared features to the
500 base features and set -y to 2.
Figure 1(b) shows the performance of the labeled
adaptation algorithms once shared features had been
added. Here the x-axis is the cosine between the
original task weight vectors, demonstrating how the
shared features improve performance on potentially
dissimilar tasks. Whereas in the first experiment
FE does not improve over just training on target data
until the cosine is greater than 0.6, once shared fea-
tures have been added then both FE and S+T use
these features to learn with originally dissimilar
tasks. Furthermore the shared features tend to push
the tasks ‘closer’ so that S+T improves over FE ear-
lier. Comparing to Figure 1(a), there are regions
where before shared features are added it is better
to use FE, and after shared features are added it is
better to use S+T. This shows that labeled adapta-
tion and unlabeled are not independent. Therefore,
it is important to combine these two aspects to see
the real contribution of each adaptation framework.
In these three artificial experiments we have
demonstrated cases where both FE or S+T are
the best algorithm before and after representation
changes like those created with unlabeled adaptation
are imposed. This fact points to the perhaps obvi-
ous conclusion that there is not a single best adapta-
tion algorithm, and the determination of specific best
practices depends on task similarity (in both P(X)
and P(Y |X)), especially after being brought closer
together with other adaptation approaches. If there
is one common trend it is that often once two tasks
have been brought close together using a shared rep-
resentation, then the tasks are now close enough
such that the simple S+T algorithm does well.
</bodyText>
<sectionHeader confidence="0.985962" genericHeader="method">
5 Real World Experiments
</sectionHeader>
<bodyText confidence="0.998181833333333">
In Section 4, we have shown through artificial data
experiments that labeled and unlabeled adaptation
algorithms are not independent. In this section, we
focus on experiments with real datasets.
For the labeled adaptation algorithms, we have the
following options:
</bodyText>
<listItem confidence="0.917484333333333">
• TGT: Only uses target labeled training dataset.
• FE: Uses both labeled datasets.
• FE+: Uses both labeled datasets. A modifica-
tion of the FE algorithm, equivalent to multi-
plying the “shared” part of the FE feature vec-
tor (Eq. (1)) by 10 (Finkel and Manning, 2009).
• S+T: Uses both source and target labeled
datasets to train a single model with all labeled
data directly.
</listItem>
<bodyText confidence="0.9990156">
Throughout all of our experiments, we use SVMs
trained with a modified java implementation8 of
LIBLINEAR as our underlying learning classi-
fier (Hsieh et al., 2008). For the tasks that require
structures, we model each individual decision using
</bodyText>
<footnote confidence="0.9993745">
8Our code is modified from the version available on http:
//www.bwaldvogel.de/liblinear-java/
</footnote>
<page confidence="0.99134">
774
</page>
<table confidence="0.524440714285714">
Algorithm TGT FE FE+ S+T
SRC labeled data? no yes
Target labeled data Token F1
(a) MUC7 Dev 58.6 70.5 74.3 73.1
(a) + cluster 77.5 82.5 83.3 83.3
(b) MUC7 Train 73.0 78.2 80.1 78.7
(b) + cluster 85.4 86.4 86.2 86.5
</table>
<tableCaption confidence="0.970099">
Table 2: NER Experiments. We bold face the best accuracy
</tableCaption>
<bodyText confidence="0.961619906976744">
in a row and underline the runner up. Both unlabeled adapta-
tion algorithms (adding cluster features) and labeled adaptation
algorithm (using source labeled data) help the performance sig-
nificantly. Moreover, adding cluster-like features also changes
the behavior of the labeled adaptation algorithms. Note that
after adding cluster features, S+T becomes quite competitive
with (or slightly better than) the FE+ approach. The size of
MUC7 develop set is roughly 20% of the size of the MUC7
training set.
a local SVM classifier then make our prediction us-
ing a greedy approach from left to right. While we
could use a more complex model such as Condi-
tional Random Field (Lafferty et al., 2001), as we
will see later, our simple model generates state-of-
the-art results for many tasks. Regarding parameter
selection, we selected the SVM regularization pa-
rameter for the baseline model (TGT) and then fix it
for all algorithms9.
Named Entity Recognition Our first task is
Named Entity Recognition (NER). The source do-
main is from the CoNLL03 shared task (Tjong
Kim Sang and De Meulder, 2003) and the target do-
main is from the MUC7 dataset. The goal of this
adaptation system is to maximize the performance
on the test data of MUC7 dataset with CoNLL train-
ing data and (some) MUC7 labeled data. As an unla-
beled adaptation method to address feature sparsity,
we add cluster-like features based on the gazetteers
and word clustering resources used in (Ratinov and
Roth, 2009) to bridge the source and target domain.
We experiment with both MUC development and
training set as our target labeled sets.
The experimental results are in Table 2. First, no-
tice that addressing the feature sparsity issue helps
the performance significantly. Adding cluster-like
9We use L2-hinge loss for all of the experiments, with
C = 2−4 for NER experiments and C = 2−5 for the PSD
experiments.
features improves the Token-F1 by around 10%. On
the other hand, adding target labeled data also helps
the results significantly. Moreover, using both tar-
get labeled data and cluster-like shared representa-
tion are mutually beneficial in all cases.
Importantly, adding cluster-like features changes
the behavior of the labeled adaptation algorithms.
When the cluster-like features are not added, the
FE+ algorithm is in general the best labeled adap-
tation framework. This result agrees with the re-
sults showed in (Finkel and Manning, 2009), where
the authors show that FE+ is the best labeled adap-
tation framework in their settings. However, after
adding the cluster-like features, the simple S+T ap-
proach becomes very competitive to both FE and
FE+. This matches our analysis in Section 4: re-
solving features sparsity will change the behavior of
labeled adaptation frameworks.
We compare the simple S+T algorithm with
cluster-like features to other published results on
adapting from CoNLL dataset to MUC7 dataset in
table 3. Past works on this setting often only fo-
cus on one class of adaption approach. For example,
(Ratinov and Roth, 2009) only use the cluster-like
features to address the feature sparsity problem, and
(Finkel and Manning, 2009) only use target labeled
data without using gazetteers and word-cluster in-
formation. Notice that because of combining two
classes of adaption algorithms, our approach is sig-
nificantly better than these two systems10.
Preposition Sense Disambiguation We also test
the combination of unlabeled and labeled adaption
on the task of Preposition Sense Disambiguation.
Here the data contains multiple prepositions where
each preposition has many different senses. The
goal is to predict the right sense for a given prepo-
sition in the testing data. The source domain is the
SemEval 2007 preposition WSD Task and the target
domain is from the dataset annotated in (Dahlmeier
et al., 2009). Our feature design mainly comes
from (Tratz and Hovy, 2009) (who do not evalu-
ate their system on our target data). As our un-
10The work (Ratinov and Roth, 2009) also combines their
system with several document-level features. While it is possi-
ble to add these features in our system, we do not include any
global features for the sake of simplicity. Note that our sys-
tem is competitive to (Ratinov and Roth, 2009) even though our
system does not use global features.
</bodyText>
<page confidence="0.992436">
775
</page>
<table confidence="0.9961128">
Systems Cluster? TGT? P.F1 T.F1
Our NER y y 84.1 86.5
FM09 n y 79.98 N/A
RR09 y n N/A 83.2
RR09 + global y n N/A 86.2
</table>
<tableCaption confidence="0.837004428571429">
Table 3: Comparisons between different NER systems. P.F1
and T.F1 represent the phrase-level and token-level F1 score,
respectively. We use “Cluster?” to indicate if cluster features
are used and use “TGT?” to indicate if target labeled data is
used. Previous systems often only use one class of adaptation
algorithms. Using both adaptation aspects makes our system
perform significantly better than FM09 and RR09.
</tableCaption>
<table confidence="0.999809428571429">
Algorithm TGT FE FE+ S+T
SRC labeled data? no yes
Target labeled data Accuracy
10% Tgt 43.8 48.2 51.3 49.7
10% Tgt + Cluster 44.9 50.5 51.8 52.0
100% Tgt 59.5 60.5 60.3 61.2
100% Tgt + Cluster 61.3 62.0 61.2 62.1
</table>
<tableCaption confidence="0.994667">
Table 4: Preposition Sense Disambiguation. We mark the best
</tableCaption>
<bodyText confidence="0.98475116">
accuracy in a row using the bold font and underline the runner
up. Note that both adding cluster features and adding source la-
beled data help the performance significantly. Moreover, adding
clusters also changes the behavior of the labeled adaptation al-
gorithms.
labeled adaptation approach we augment all word
based features with cluster information from sepa-
rately generated hierarchical Brown clusters (Brown
et al., 1992).
The experimental results are in Table 4. Note that
we see phenomena similar to what happened in the
NER experiments. First, both labeled and unlabeled
adaptation improves the system. When only 10% of
the target labeled data is used, the inclusion of the
source labeled data helps significantly. When there
is more labeled data, labeled and unlabeled adaption
have similar impact. Again, using unlabeled adap-
tion changes the behavior of the labeled adaption al-
gorithms.
In Table 5, we compare our system to (Dahlmeier
et al., 2009), who do not use the SemEval data but
jointly train their preposition sense disambiguation
system with a semantic role labeling system. With
both labeled and unlabeled adaption, our system is
significantly better.
</bodyText>
<table confidence="0.9871355">
Systems ACC
Our PSD (S+T and cluster) 62.1
DNS09 56.5
DNS09 + SRL 58.8
</table>
<tableCaption confidence="0.6389442">
Table 5: Comparison between different PSD systems. Note
that after adding cluster features and source labeled data with
S+T approach, our system outperforms the state-of-the-art sys-
tem proposed in (Dahlmeier et al., 2009), even though they
jointly learn a PSD and SRL system together.
</tableCaption>
<sectionHeader confidence="0.99865" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999706931034483">
In this paper, we point out the necessities of com-
bining labeled and unlabeled adaptation algorithms.
We analyzed the FE algorithm both theoretically
and empirically, demonstrating that it requires both
a minimal amount of task similarity to work, and
past a certain level of similarity other, simpler ap-
proaches are better. More importantly, through arti-
ficial data experiments we found that applying unla-
beled adaptation algorithms may change the behav-
ior of labeled adaptation algorithms as representa-
tions change, and hence affect the choice of labeled
adaptation algorithm. Experiments with real-world
datasets confirmed that combinations of both adap-
tation methods provide the best results, often allow-
ing the use of simple labeled adaptation approaches.
In the future, we hope to develop a joint algorithm
which addresses both labeled and unlabeled adapta-
tion at the same time.
Acknowledgment We thank Vivek Srikumar for provid-
ing the baseline implementation of preposition sense disam-
biguation. We also thank anonymous reviewers for their use-
ful comments. University of Illinois gratefully acknowledges
the support of Defense Advanced Research Projects Agency
(DARPA) Machine Reading Program under Air Force Research
Laboratory (AFRL) prime contract no. FA8750-09-C-0181.
Any opinions, findings, and conclusion or recommendations ex-
pressed in this material are those of the author(s) and do not
necessarily reflect the view of the DARPA, AFRL, or the US
government.
</bodyText>
<sectionHeader confidence="0.98687" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.886867">
Rie Kubota Ando and Tong Zhang. 2005. A framework
for learning predictive structures from multiple tasks
and unlabeled data. J. Mach. Learn. Res.
</bodyText>
<page confidence="0.996577">
776
</page>
<reference confidence="0.999578884615385">
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In EMNLP.
P. F. Brown, V. J. Della Pietra, P. V. deSouza, J. C. Lai,
and R. L. Mercer. 1992. Class-Based n-gram Models
of Natural Language. Computational Linguistics.
Ciprian Chelba and Alex Acero. 2004. Adaptation of
maximum entropy capitalizer: Little data can help a
lot. In Dekang Lin and Dekai Wu, editors, EMNLP.
D. Dahlmeier, H. T. Ng, and T. Schultz. 2009. Joint
learning of preposition senses and semantic roles of
prepositional phrases. In EMNLP.
Hal Daum´e III. 2007. Frustratingly easy domain adapta-
tion. In ACL.
T. Evgeniou and M. Pontil. 2004. Regularized multi–
task learning. In KDD.
J. R. Finkel and C. D. Manning. 2009. Hierarchical
bayesian domain adaptation. In NAACL.
C.-J. Hsieh, K.-W. Chang, C.-J. Lin, S. S. Keerthi, and
S. Sundararajan. 2008. A dual coordinate descent
method for large-scale linear svm. In ICML.
Fei Huang and Alexander Yates. 2009. Distributional
representations for handling sparsity in supervised
sequence-labeling. In ACL.
Jing Jiang and ChengXiang Zhai. 2007. Instance weight-
ing for domain adaptation in nlp. In ACL.
T. Koo, X. Carreras, and M. Collins. 2008. Simple semi-
supervised dependency parsing. In ACL.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In ICML.
P. Liang. 2005. Semi-supervised learning for natural
language. Master’s thesis, Massachusetts Institute of
Technology.
A. Novikoff. 1963. On convergence proofs for percep-
trons. In Proceeding of the Symposium on the Mathe-
matical Theory of Automata.
L. Ratinov and D. Roth. 2009. Design challenges and
misconceptions in named entity recognition. In Proc.
of the Annual Conference on Computational Natural
Language Learning (CoNLL).
Erik F. Tjong Kim Sang and Fien De Meulder. 2003. In-
troduction to the conll-2003 shared task: Language-
independent named entity recognition. In Walter
Daelemans and Miles Osborne, editors, Proceedings
of CoNLL-2003.
S. Tratz and D. Hovy. 2009. Disambiguation of prepo-
sition sense using linguistically motivated features. In
NAACL.
Tong Zhang. 2002. Covering number bounds of certain
regularized linear function classes. J. Mach. Learn.
Res.
</reference>
<page confidence="0.997304">
777
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.957858">
<title confidence="0.999846">The Necessity of Combining Adaptation Methods</title>
<author confidence="0.998156">Ming-Wei Chang</author>
<author confidence="0.998156">Michael Connor</author>
<author confidence="0.998156">Dan</author>
<affiliation confidence="0.999947">University of Illinois at Urbana</affiliation>
<address confidence="0.970756">Urbana, IL</address>
<abstract confidence="0.999526642857143">Problems stemming from domain adaptation continue to plague the statistical natural language processing community. There has been continuing work trying to find general purpose algorithms to alleviate this problem. In this paper we argue that existing general purpose approaches usually only focus on one of two issues related to the difficulties faced by adaptation: 1) difference in base feature statistics or 2) task differences that can be detected with labeled data. argue that it is combine these two classes of adaptation algorithms, using evidence collected through theoretical analysis and simulated and real-world data experiments. We find that the combined approach often outperforms the individual adaptation approaches. By combining simple approaches from each class of adaptation algorithm, we achieve state-of-the-art results for both Named Entity Recognition adaptation task and the Preposition Sense Disambiguation adaptation task. Second, we also show that applying an adaptation algorithm that finds shared representation between domains often impacts the choice in adaptation algorithm that makes use of target labeled data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Domain adaptation with structural correspondence learning.</title>
<date>2006</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="1953" citStr="Blitzer et al., 2006" startWordPosition="287" endWordPosition="290">arget labeled data. 1 Introduction While recent advances in statistical modeling for natural language processing are exciting, the problem of domain adaptation remains a big challenge. It is widely known that a classifier trained on one domain (e.g. news domain) usually performs poorly on a different domain (e.g. medical domain) (Jiang and Zhai, 2007; Daum´e III, 2007). The inability of current statistical models to handle multiple domains is one of the key obstacles hindering the progress of NLP. Several general purpose algorithms have been proposed to address the domain adaptation problem: (Blitzer et al., 2006; Jiang and Zhai, 2007; Daum´e III, 2007; Finkel and Manning, 2009). It is widely believed that the drop in performance of statistical models on new domains is due to the shift of the joint distribution of labels and examples, P(Y, X), from domain to domain, where X represents the input space and Y represents the output space. In general, we can separate existing adaptation algorithms into two categories: Focuses on P(X) This type of adaptation algorithm attempts to resolve the difference between the feature space statistics of two domains. While many different techniques have been proposed, t</context>
<context position="12519" citStr="Blitzer et al., 2006" startWordPosition="2015" endWordPosition="2018">at span domains us(Focus on P(X)) May use other third party resources ing unlabeled data and/or third party re(dictionaries, gazetteers, etc.). sources. Labeled Adaptation Source and Target None Train classifier(s) using both source and (Focus on P(YIX)) target training data, relating the two. Table 1: Comparison between two general adaptation frameworks discussed in this paper. Each framework is specified by its setting (data required) and its learning algorithm. Multiple previous adaptation approaches fit in one of either framework. While other more complex algorithms (Ando and Zhang, 2005; Blitzer et al., 2006) for finding better shared representation (without using labeled target data) have been proposed, we find that using straightforward clustering features is quite effective in general. 3 Analysis of the FE Framework In this section, we propose a simple yet informative analysis of the FE algorithm from the perspective of multi-task learning. Note that we ignore the effect of unlabeled adaptation in this section, and focus on the analysis of the FE framework as a representative labeled adaptation framework. 3.1 Mistake Bound Analysis While (Daum´e III, 2007) proposed this framework for adaptation</context>
</contexts>
<marker>Blitzer, McDonald, Pereira, 2006</marker>
<rawString>John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspondence learning. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>V J Della Pietra</author>
<author>P V deSouza</author>
<author>J C Lai</author>
<author>R L Mercer</author>
</authors>
<title>Class-Based n-gram Models of Natural Language. Computational Linguistics.</title>
<date>1992</date>
<contexts>
<context position="11405" citStr="Brown et al., 1992" startWordPosition="1840" endWordPosition="1843">party resources that spans domains. Then, for every feature template that contains a word, we append another feature template that uses the word’s cluster instead of the word itself. This technique is used in many recent works including dependency parsing and NER (Koo et al., 2008; Ratinov and Roth, 2009). Note that the unlabeled text need not come from the source or target domain. In fact, in this paper, we use clusters generated with the Reuters 1996 dataset, a superset of the CoNLL03 NER dataset (Koo et al., 2008; Liang, 2005). We adopt the Brown cluster algorithm to find the word cluster (Brown et al., 1992; Liang, 2005). We can use other resources to create clusters as well. For example, in the NER domain, we also include gazetteers4 as an unlabeled cluster resource, which can bring the domains together quite effectively. 4Our gazetteers comes from (Ratinov and Roth, 2009). (t−1) blocks (�−t) blocks � �� � � �� � 0 ... 0 ���� x 0 ... 0 specific 3 57. (1) 769 Framework Labeled Data Unlabeled Data Common Approach Unlabeled Adaptation Source Encompasses Source and Target. Generate features that span domains us(Focus on P(X)) May use other third party resources ing unlabeled data and/or third party</context>
<context position="36960" citStr="Brown et al., 1992" startWordPosition="6265" endWordPosition="6268">10% Tgt 43.8 48.2 51.3 49.7 10% Tgt + Cluster 44.9 50.5 51.8 52.0 100% Tgt 59.5 60.5 60.3 61.2 100% Tgt + Cluster 61.3 62.0 61.2 62.1 Table 4: Preposition Sense Disambiguation. We mark the best accuracy in a row using the bold font and underline the runner up. Note that both adding cluster features and adding source labeled data help the performance significantly. Moreover, adding clusters also changes the behavior of the labeled adaptation algorithms. labeled adaptation approach we augment all word based features with cluster information from separately generated hierarchical Brown clusters (Brown et al., 1992). The experimental results are in Table 4. Note that we see phenomena similar to what happened in the NER experiments. First, both labeled and unlabeled adaptation improves the system. When only 10% of the target labeled data is used, the inclusion of the source labeled data helps significantly. When there is more labeled data, labeled and unlabeled adaption have similar impact. Again, using unlabeled adaption changes the behavior of the labeled adaption algorithms. In Table 5, we compare our system to (Dahlmeier et al., 2009), who do not use the SemEval data but jointly train their prepositio</context>
</contexts>
<marker>Brown, Pietra, deSouza, Lai, Mercer, 1992</marker>
<rawString>P. F. Brown, V. J. Della Pietra, P. V. deSouza, J. C. Lai, and R. L. Mercer. 1992. Class-Based n-gram Models of Natural Language. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>Alex Acero</author>
</authors>
<title>Adaptation of maximum entropy capitalizer: Little data can help a lot.</title>
<date>2004</date>
<editor>In Dekang Lin and Dekai Wu, editors, EMNLP.</editor>
<contexts>
<context position="3255" citStr="Chelba and Acero, 2004" startWordPosition="504" endWordPosition="507">t brings the source domain and the target domain closer. Often these algorithms do not use labeled examples in the target domain. The works (Blitzer et al., 2006; Huang and Yates, 2009) all belong to this category. Focuses on P(Y |X) These adaptation algorithms assume that there exists a small amount of labeled data for the target domain. Instead of training two weight vectors independently (one for source and the other for the target domain), these algorithms try to relate the source and target weight vectors. This is often achieved by using a special designed regularization term. The works (Chelba and Acero, 2004; Daum´e III, 2007; Finkel and Manning, 2009) belong to this category. 767 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 767–777, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics It is important to give the definition of an adaptation framework. An adaptation framework is specified by the data/resources used and a specific learning algorithm. For example, a framework that used only source labeled examples and one that used both source and target labeled examples should be considered as two different fra</context>
</contexts>
<marker>Chelba, Acero, 2004</marker>
<rawString>Ciprian Chelba and Alex Acero. 2004. Adaptation of maximum entropy capitalizer: Little data can help a lot. In Dekang Lin and Dekai Wu, editors, EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Dahlmeier</author>
<author>H T Ng</author>
<author>T Schultz</author>
</authors>
<title>Joint learning of preposition senses and semantic roles of prepositional phrases.</title>
<date>2009</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="35257" citStr="Dahlmeier et al., 2009" startWordPosition="5974" endWordPosition="5977">nd word-cluster information. Notice that because of combining two classes of adaption algorithms, our approach is significantly better than these two systems10. Preposition Sense Disambiguation We also test the combination of unlabeled and labeled adaption on the task of Preposition Sense Disambiguation. Here the data contains multiple prepositions where each preposition has many different senses. The goal is to predict the right sense for a given preposition in the testing data. The source domain is the SemEval 2007 preposition WSD Task and the target domain is from the dataset annotated in (Dahlmeier et al., 2009). Our feature design mainly comes from (Tratz and Hovy, 2009) (who do not evaluate their system on our target data). As our un10The work (Ratinov and Roth, 2009) also combines their system with several document-level features. While it is possible to add these features in our system, we do not include any global features for the sake of simplicity. Note that our system is competitive to (Ratinov and Roth, 2009) even though our system does not use global features. 775 Systems Cluster? TGT? P.F1 T.F1 Our NER y y 84.1 86.5 FM09 n y 79.98 N/A RR09 y n N/A 83.2 RR09 + global y n N/A 86.2 Table 3: C</context>
<context position="37492" citStr="Dahlmeier et al., 2009" startWordPosition="6352" endWordPosition="6355">ster information from separately generated hierarchical Brown clusters (Brown et al., 1992). The experimental results are in Table 4. Note that we see phenomena similar to what happened in the NER experiments. First, both labeled and unlabeled adaptation improves the system. When only 10% of the target labeled data is used, the inclusion of the source labeled data helps significantly. When there is more labeled data, labeled and unlabeled adaption have similar impact. Again, using unlabeled adaption changes the behavior of the labeled adaption algorithms. In Table 5, we compare our system to (Dahlmeier et al., 2009), who do not use the SemEval data but jointly train their preposition sense disambiguation system with a semantic role labeling system. With both labeled and unlabeled adaption, our system is significantly better. Systems ACC Our PSD (S+T and cluster) 62.1 DNS09 56.5 DNS09 + SRL 58.8 Table 5: Comparison between different PSD systems. Note that after adding cluster features and source labeled data with S+T approach, our system outperforms the state-of-the-art system proposed in (Dahlmeier et al., 2009), even though they jointly learn a PSD and SRL system together. 6 Conclusion In this paper, we</context>
</contexts>
<marker>Dahlmeier, Ng, Schultz, 2009</marker>
<rawString>D. Dahlmeier, H. T. Ng, and T. Schultz. 2009. Joint learning of preposition senses and semantic roles of prepositional phrases. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
</authors>
<title>Frustratingly easy domain adaptation.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<marker>Daum´e, 2007</marker>
<rawString>Hal Daum´e III. 2007. Frustratingly easy domain adaptation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Evgeniou</author>
<author>M Pontil</author>
</authors>
<title>Regularized multi– task learning.</title>
<date>2004</date>
<booktitle>In KDD.</booktitle>
<contexts>
<context position="13189" citStr="Evgeniou and Pontil, 2004" startWordPosition="2120" endWordPosition="2123">ithout using labeled target data) have been proposed, we find that using straightforward clustering features is quite effective in general. 3 Analysis of the FE Framework In this section, we propose a simple yet informative analysis of the FE algorithm from the perspective of multi-task learning. Note that we ignore the effect of unlabeled adaptation in this section, and focus on the analysis of the FE framework as a representative labeled adaptation framework. 3.1 Mistake Bound Analysis While (Daum´e III, 2007) proposed this framework for adaptation, a very similar idea had been proposed in (Evgeniou and Pontil, 2004) as a novel regularization term for multitask learning with support vector machines. Assume that w1, w2, ... , wm are the weight vector for the first domain to the m-th domain, respectively. The baseline approach is to assume that each weight vector is independent. Assume that we adopt a SVM-like optimization problem that consider all m tasks, the baseline approach is equivalent to using the following regularization term in the objective function: Emt=1 Iwt112. In (Evgeniou and Pontil, 2004; Daum´e III, 2007), they assume that wt = u + vt, for t = 1, ... m, where vt is the specific weight vect</context>
<context position="16631" citStr="Evgeniou and Pontil, 2004" startWordPosition="2765" endWordPosition="2768">„ II ¯wII2. The optimization problem has an analytical solution: IIw*II2 = �m IIwtII2 − II �m wtII2/(m + 1). t=1 t=1 The proof is completed by the standard mistake bound theorem and the following fact: maxi II0t(x)II2 = 2 maxi IIxII2 &lt; 2R2. 3.2 Mistake Bound Comparison In the following, we would like to explore under what circumstances the FE framework can work better than individual models and the S+T framework using Theorem 1. The analysis is done based on the assumption that all frameworks use the perceptron algorithm. Before showing the bound analysis, note that the framework proposed by (Evgeniou and Pontil, 2004; Finkel and Manning, 2009) is a generalization over these three frameworks (FE, S+T, and the baseline)6. However, our goal in this paper is different: we try to provide a deep discussion on when and why one should use a particular framework. Here, we compare the mistake bounds of the feature sharing framework to that of the baseline approach, which learns each task independently7. In 6The framework proposed by (Evgeniou and Pontil, 2004; Finkel and Manning, 2009) is a generalization of Eq. (1). It allows the user to weight each block of features. If we put zero weight on the shared block, it </context>
</contexts>
<marker>Evgeniou, Pontil, 2004</marker>
<rawString>T. Evgeniou and M. Pontil. 2004. Regularized multi– task learning. In KDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Finkel</author>
<author>C D Manning</author>
</authors>
<title>Hierarchical bayesian domain adaptation.</title>
<date>2009</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="2020" citStr="Finkel and Manning, 2009" startWordPosition="298" endWordPosition="301">atistical modeling for natural language processing are exciting, the problem of domain adaptation remains a big challenge. It is widely known that a classifier trained on one domain (e.g. news domain) usually performs poorly on a different domain (e.g. medical domain) (Jiang and Zhai, 2007; Daum´e III, 2007). The inability of current statistical models to handle multiple domains is one of the key obstacles hindering the progress of NLP. Several general purpose algorithms have been proposed to address the domain adaptation problem: (Blitzer et al., 2006; Jiang and Zhai, 2007; Daum´e III, 2007; Finkel and Manning, 2009). It is widely believed that the drop in performance of statistical models on new domains is due to the shift of the joint distribution of labels and examples, P(Y, X), from domain to domain, where X represents the input space and Y represents the output space. In general, we can separate existing adaptation algorithms into two categories: Focuses on P(X) This type of adaptation algorithm attempts to resolve the difference between the feature space statistics of two domains. While many different techniques have been proposed, the common goal of these algorithms is to find a better shared repre</context>
<context position="3300" citStr="Finkel and Manning, 2009" startWordPosition="511" endWordPosition="514">domain closer. Often these algorithms do not use labeled examples in the target domain. The works (Blitzer et al., 2006; Huang and Yates, 2009) all belong to this category. Focuses on P(Y |X) These adaptation algorithms assume that there exists a small amount of labeled data for the target domain. Instead of training two weight vectors independently (one for source and the other for the target domain), these algorithms try to relate the source and target weight vectors. This is often achieved by using a special designed regularization term. The works (Chelba and Acero, 2004; Daum´e III, 2007; Finkel and Manning, 2009) belong to this category. 767 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 767–777, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics It is important to give the definition of an adaptation framework. An adaptation framework is specified by the data/resources used and a specific learning algorithm. For example, a framework that used only source labeled examples and one that used both source and target labeled examples should be considered as two different frameworks, even though they might use exactly t</context>
<context position="16658" citStr="Finkel and Manning, 2009" startWordPosition="2769" endWordPosition="2772">n problem has an analytical solution: IIw*II2 = �m IIwtII2 − II �m wtII2/(m + 1). t=1 t=1 The proof is completed by the standard mistake bound theorem and the following fact: maxi II0t(x)II2 = 2 maxi IIxII2 &lt; 2R2. 3.2 Mistake Bound Comparison In the following, we would like to explore under what circumstances the FE framework can work better than individual models and the S+T framework using Theorem 1. The analysis is done based on the assumption that all frameworks use the perceptron algorithm. Before showing the bound analysis, note that the framework proposed by (Evgeniou and Pontil, 2004; Finkel and Manning, 2009) is a generalization over these three frameworks (FE, S+T, and the baseline)6. However, our goal in this paper is different: we try to provide a deep discussion on when and why one should use a particular framework. Here, we compare the mistake bounds of the feature sharing framework to that of the baseline approach, which learns each task independently7. In 6The framework proposed by (Evgeniou and Pontil, 2004; Finkel and Manning, 2009) is a generalization of Eq. (1). It allows the user to weight each block of features. If we put zero weight on the shared block, it becomes the baseline approa</context>
<context position="30737" citStr="Finkel and Manning, 2009" startWordPosition="5239" endWordPosition="5242">s are now close enough such that the simple S+T algorithm does well. 5 Real World Experiments In Section 4, we have shown through artificial data experiments that labeled and unlabeled adaptation algorithms are not independent. In this section, we focus on experiments with real datasets. For the labeled adaptation algorithms, we have the following options: • TGT: Only uses target labeled training dataset. • FE: Uses both labeled datasets. • FE+: Uses both labeled datasets. A modification of the FE algorithm, equivalent to multiplying the “shared” part of the FE feature vector (Eq. (1)) by 10 (Finkel and Manning, 2009). • S+T: Uses both source and target labeled datasets to train a single model with all labeled data directly. Throughout all of our experiments, we use SVMs trained with a modified java implementation8 of LIBLINEAR as our underlying learning classifier (Hsieh et al., 2008). For the tasks that require structures, we model each individual decision using 8Our code is modified from the version available on http: //www.bwaldvogel.de/liblinear-java/ 774 Algorithm TGT FE FE+ S+T SRC labeled data? no yes Target labeled data Token F1 (a) MUC7 Dev 58.6 70.5 74.3 73.1 (a) + cluster 77.5 82.5 83.3 83.3 (b</context>
<context position="33867" citStr="Finkel and Manning, 2009" startWordPosition="5753" endWordPosition="5756">the experiments, with C = 2−4 for NER experiments and C = 2−5 for the PSD experiments. features improves the Token-F1 by around 10%. On the other hand, adding target labeled data also helps the results significantly. Moreover, using both target labeled data and cluster-like shared representation are mutually beneficial in all cases. Importantly, adding cluster-like features changes the behavior of the labeled adaptation algorithms. When the cluster-like features are not added, the FE+ algorithm is in general the best labeled adaptation framework. This result agrees with the results showed in (Finkel and Manning, 2009), where the authors show that FE+ is the best labeled adaptation framework in their settings. However, after adding the cluster-like features, the simple S+T approach becomes very competitive to both FE and FE+. This matches our analysis in Section 4: resolving features sparsity will change the behavior of labeled adaptation frameworks. We compare the simple S+T algorithm with cluster-like features to other published results on adapting from CoNLL dataset to MUC7 dataset in table 3. Past works on this setting often only focus on one class of adaption approach. For example, (Ratinov and Roth, 2</context>
</contexts>
<marker>Finkel, Manning, 2009</marker>
<rawString>J. R. Finkel and C. D. Manning. 2009. Hierarchical bayesian domain adaptation. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-J Hsieh</author>
<author>K-W Chang</author>
<author>C-J Lin</author>
<author>S S Keerthi</author>
<author>S Sundararajan</author>
</authors>
<title>A dual coordinate descent method for large-scale linear svm.</title>
<date>2008</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="31010" citStr="Hsieh et al., 2008" startWordPosition="5284" endWordPosition="5287">datasets. For the labeled adaptation algorithms, we have the following options: • TGT: Only uses target labeled training dataset. • FE: Uses both labeled datasets. • FE+: Uses both labeled datasets. A modification of the FE algorithm, equivalent to multiplying the “shared” part of the FE feature vector (Eq. (1)) by 10 (Finkel and Manning, 2009). • S+T: Uses both source and target labeled datasets to train a single model with all labeled data directly. Throughout all of our experiments, we use SVMs trained with a modified java implementation8 of LIBLINEAR as our underlying learning classifier (Hsieh et al., 2008). For the tasks that require structures, we model each individual decision using 8Our code is modified from the version available on http: //www.bwaldvogel.de/liblinear-java/ 774 Algorithm TGT FE FE+ S+T SRC labeled data? no yes Target labeled data Token F1 (a) MUC7 Dev 58.6 70.5 74.3 73.1 (a) + cluster 77.5 82.5 83.3 83.3 (b) MUC7 Train 73.0 78.2 80.1 78.7 (b) + cluster 85.4 86.4 86.2 86.5 Table 2: NER Experiments. We bold face the best accuracy in a row and underline the runner up. Both unlabeled adaptation algorithms (adding cluster features) and labeled adaptation algorithm (using source l</context>
</contexts>
<marker>Hsieh, Chang, Lin, Keerthi, Sundararajan, 2008</marker>
<rawString>C.-J. Hsieh, K.-W. Chang, C.-J. Lin, S. S. Keerthi, and S. Sundararajan. 2008. A dual coordinate descent method for large-scale linear svm. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Huang</author>
<author>Alexander Yates</author>
</authors>
<title>Distributional representations for handling sparsity in supervised sequence-labeling.</title>
<date>2009</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="2818" citStr="Huang and Yates, 2009" startWordPosition="432" endWordPosition="435">main to domain, where X represents the input space and Y represents the output space. In general, we can separate existing adaptation algorithms into two categories: Focuses on P(X) This type of adaptation algorithm attempts to resolve the difference between the feature space statistics of two domains. While many different techniques have been proposed, the common goal of these algorithms is to find a better shared representation that brings the source domain and the target domain closer. Often these algorithms do not use labeled examples in the target domain. The works (Blitzer et al., 2006; Huang and Yates, 2009) all belong to this category. Focuses on P(Y |X) These adaptation algorithms assume that there exists a small amount of labeled data for the target domain. Instead of training two weight vectors independently (one for source and the other for the target domain), these algorithms try to relate the source and target weight vectors. This is often achieved by using a special designed regularization term. The works (Chelba and Acero, 2004; Daum´e III, 2007; Finkel and Manning, 2009) belong to this category. 767 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, </context>
</contexts>
<marker>Huang, Yates, 2009</marker>
<rawString>Fei Huang and Alexander Yates. 2009. Distributional representations for handling sparsity in supervised sequence-labeling. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Jiang</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Instance weighting for domain adaptation in nlp.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1685" citStr="Jiang and Zhai, 2007" startWordPosition="244" endWordPosition="247">ntity Recognition adaptation task and the Preposition Sense Disambiguation adaptation task. Second, we also show that applying an adaptation algorithm that finds shared representation between domains often impacts the choice in adaptation algorithm that makes use of target labeled data. 1 Introduction While recent advances in statistical modeling for natural language processing are exciting, the problem of domain adaptation remains a big challenge. It is widely known that a classifier trained on one domain (e.g. news domain) usually performs poorly on a different domain (e.g. medical domain) (Jiang and Zhai, 2007; Daum´e III, 2007). The inability of current statistical models to handle multiple domains is one of the key obstacles hindering the progress of NLP. Several general purpose algorithms have been proposed to address the domain adaptation problem: (Blitzer et al., 2006; Jiang and Zhai, 2007; Daum´e III, 2007; Finkel and Manning, 2009). It is widely believed that the drop in performance of statistical models on new domains is due to the shift of the joint distribution of labels and examples, P(Y, X), from domain to domain, where X represents the input space and Y represents the output space. In </context>
<context position="9340" citStr="Jiang and Zhai, 2007" startWordPosition="1491" endWordPosition="1494"> source and target labeled data to work. We denote n as the total number of features3 and m is the number of the “domains”, where one of the domains is the target domain. The FE framework creates a global weight vector in Rn(m+1), an extended space for all domains. The representation x of the t-th domain is mapped by 4bt(x) E Rn(m+1). In the extended space, the first n features consist of the “shared” block, which is always active across all tasks. The (t+1)-th block (the (nt+1)-th to the (nt+n)-th features) is a “specific” block, and is only active when 2Among the previously mentioned work, (Jiang and Zhai, 2007) is a special case given that it discusses both aspects of adaptation algorithms. However, little analysis on the interaction of the two aspects is discussed in that paper 3We assume that the number of features in each domain is equal. extracting examples from the task t. More formally, 2 6 �t(x) = 4x ���� shared A single weight vector Cv is obtained by training on the modified labeled data {yti, -bt(xti)}mt�1. Given that this framework only extends the feature space, in this paper, we also call it the feature extension framework (still called FE). We will see in Section 3 that this framework </context>
</contexts>
<marker>Jiang, Zhai, 2007</marker>
<rawString>Jing Jiang and ChengXiang Zhai. 2007. Instance weighting for domain adaptation in nlp. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Koo</author>
<author>X Carreras</author>
<author>M Collins</author>
</authors>
<title>Simple semisupervised dependency parsing.</title>
<date>2008</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="11068" citStr="Koo et al., 2008" startWordPosition="1778" endWordPosition="1781">ngle model on the pooled and unextended source and target training data. Unlabeled adaptation: Adding cluster-like features Recall that unlabeled adaptation frameworks find the features that “work” across domain. In this paper, we find such features in two steps. First, we use word clusters generated from unlabeled text and/or third party resources that spans domains. Then, for every feature template that contains a word, we append another feature template that uses the word’s cluster instead of the word itself. This technique is used in many recent works including dependency parsing and NER (Koo et al., 2008; Ratinov and Roth, 2009). Note that the unlabeled text need not come from the source or target domain. In fact, in this paper, we use clusters generated with the Reuters 1996 dataset, a superset of the CoNLL03 NER dataset (Koo et al., 2008; Liang, 2005). We adopt the Brown cluster algorithm to find the word cluster (Brown et al., 1992; Liang, 2005). We can use other resources to create clusters as well. For example, in the NER domain, we also include gazetteers4 as an unlabeled cluster resource, which can bring the domains together quite effectively. 4Our gazetteers comes from (Ratinov and Ro</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>T. Koo, X. Carreras, and M. Collins. 2008. Simple semisupervised dependency parsing. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="32152" citStr="Lafferty et al., 2001" startWordPosition="5473" endWordPosition="5476">ithms (adding cluster features) and labeled adaptation algorithm (using source labeled data) help the performance significantly. Moreover, adding cluster-like features also changes the behavior of the labeled adaptation algorithms. Note that after adding cluster features, S+T becomes quite competitive with (or slightly better than) the FE+ approach. The size of MUC7 develop set is roughly 20% of the size of the MUC7 training set. a local SVM classifier then make our prediction using a greedy approach from left to right. While we could use a more complex model such as Conditional Random Field (Lafferty et al., 2001), as we will see later, our simple model generates state-ofthe-art results for many tasks. Regarding parameter selection, we selected the SVM regularization parameter for the baseline model (TGT) and then fix it for all algorithms9. Named Entity Recognition Our first task is Named Entity Recognition (NER). The source domain is from the CoNLL03 shared task (Tjong Kim Sang and De Meulder, 2003) and the target domain is from the MUC7 dataset. The goal of this adaptation system is to maximize the performance on the test data of MUC7 dataset with CoNLL training data and (some) MUC7 labeled data. As</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
</authors>
<title>Semi-supervised learning for natural language. Master’s thesis,</title>
<date>2005</date>
<institution>Massachusetts Institute of Technology.</institution>
<contexts>
<context position="11322" citStr="Liang, 2005" startWordPosition="1827" endWordPosition="1828">teps. First, we use word clusters generated from unlabeled text and/or third party resources that spans domains. Then, for every feature template that contains a word, we append another feature template that uses the word’s cluster instead of the word itself. This technique is used in many recent works including dependency parsing and NER (Koo et al., 2008; Ratinov and Roth, 2009). Note that the unlabeled text need not come from the source or target domain. In fact, in this paper, we use clusters generated with the Reuters 1996 dataset, a superset of the CoNLL03 NER dataset (Koo et al., 2008; Liang, 2005). We adopt the Brown cluster algorithm to find the word cluster (Brown et al., 1992; Liang, 2005). We can use other resources to create clusters as well. For example, in the NER domain, we also include gazetteers4 as an unlabeled cluster resource, which can bring the domains together quite effectively. 4Our gazetteers comes from (Ratinov and Roth, 2009). (t−1) blocks (�−t) blocks � �� � � �� � 0 ... 0 ���� x 0 ... 0 specific 3 57. (1) 769 Framework Labeled Data Unlabeled Data Common Approach Unlabeled Adaptation Source Encompasses Source and Target. Generate features that span domains us(Focus</context>
</contexts>
<marker>Liang, 2005</marker>
<rawString>P. Liang. 2005. Semi-supervised learning for natural language. Master’s thesis, Massachusetts Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Novikoff</author>
</authors>
<title>On convergence proofs for perceptrons.</title>
<date>1963</date>
<booktitle>In Proceeding of the Symposium on the Mathematical Theory of Automata.</booktitle>
<contexts>
<context position="14874" citStr="Novikoff, 1963" startWordPosition="2433" endWordPosition="2434">work that adopts Eq. (2) as its regularization term. The FE framework was in fact originally designed for the problem of multitask learning so in the following, we propose a simple mistake bound analysis based on the multitask setting, where we calculate the mistakes on all domains5. We focus on multitask setting for two reasons: 1) the analysis is very easy and intuitive, and 2) in Section 4.1, we empirically confirm that the analysis holds for the adaptation setting. In the following, we assume that the training algorithm used in the FE framework is the online perceptron learning algorithm (Novikoff, 1963). This allows us to analyze the mistake bound of the FE framework with the perceptron algorithm. The bound can give us an insight on when and why one should adopt the FE framework. By using the standard mistake bound theorem (Novikoff, 1963), we show: Theorem 1. Let Dt be the labeled data of domain t. Assume that there exist w1, w2, ... , wm such that ywTt x µ, V(x, y) E Dt, and assume that max(X,y)EDt JxJJ &lt; R2,Vt = 1... m. Then, the number of mistakes made with online perceptron training (Novikoff, 1963) and the 5In the adaptation setting, one generally only cares about the performance on th</context>
</contexts>
<marker>Novikoff, 1963</marker>
<rawString>A. Novikoff. 1963. On convergence proofs for perceptrons. In Proceeding of the Symposium on the Mathematical Theory of Automata.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Ratinov</author>
<author>D Roth</author>
</authors>
<title>Design challenges and misconceptions in named entity recognition.</title>
<date>2009</date>
<booktitle>In Proc. of the Annual Conference on Computational Natural Language Learning (CoNLL).</booktitle>
<contexts>
<context position="11093" citStr="Ratinov and Roth, 2009" startWordPosition="1782" endWordPosition="1785">pooled and unextended source and target training data. Unlabeled adaptation: Adding cluster-like features Recall that unlabeled adaptation frameworks find the features that “work” across domain. In this paper, we find such features in two steps. First, we use word clusters generated from unlabeled text and/or third party resources that spans domains. Then, for every feature template that contains a word, we append another feature template that uses the word’s cluster instead of the word itself. This technique is used in many recent works including dependency parsing and NER (Koo et al., 2008; Ratinov and Roth, 2009). Note that the unlabeled text need not come from the source or target domain. In fact, in this paper, we use clusters generated with the Reuters 1996 dataset, a superset of the CoNLL03 NER dataset (Koo et al., 2008; Liang, 2005). We adopt the Brown cluster algorithm to find the word cluster (Brown et al., 1992; Liang, 2005). We can use other resources to create clusters as well. For example, in the NER domain, we also include gazetteers4 as an unlabeled cluster resource, which can bring the domains together quite effectively. 4Our gazetteers comes from (Ratinov and Roth, 2009). (t−1) blocks (</context>
<context position="32928" citStr="Ratinov and Roth, 2009" startWordPosition="5604" endWordPosition="5607">ion parameter for the baseline model (TGT) and then fix it for all algorithms9. Named Entity Recognition Our first task is Named Entity Recognition (NER). The source domain is from the CoNLL03 shared task (Tjong Kim Sang and De Meulder, 2003) and the target domain is from the MUC7 dataset. The goal of this adaptation system is to maximize the performance on the test data of MUC7 dataset with CoNLL training data and (some) MUC7 labeled data. As an unlabeled adaptation method to address feature sparsity, we add cluster-like features based on the gazetteers and word clustering resources used in (Ratinov and Roth, 2009) to bridge the source and target domain. We experiment with both MUC development and training set as our target labeled sets. The experimental results are in Table 2. First, notice that addressing the feature sparsity issue helps the performance significantly. Adding cluster-like 9We use L2-hinge loss for all of the experiments, with C = 2−4 for NER experiments and C = 2−5 for the PSD experiments. features improves the Token-F1 by around 10%. On the other hand, adding target labeled data also helps the results significantly. Moreover, using both target labeled data and cluster-like shared repr</context>
<context position="34471" citStr="Ratinov and Roth, 2009" startWordPosition="5852" endWordPosition="5855"> and Manning, 2009), where the authors show that FE+ is the best labeled adaptation framework in their settings. However, after adding the cluster-like features, the simple S+T approach becomes very competitive to both FE and FE+. This matches our analysis in Section 4: resolving features sparsity will change the behavior of labeled adaptation frameworks. We compare the simple S+T algorithm with cluster-like features to other published results on adapting from CoNLL dataset to MUC7 dataset in table 3. Past works on this setting often only focus on one class of adaption approach. For example, (Ratinov and Roth, 2009) only use the cluster-like features to address the feature sparsity problem, and (Finkel and Manning, 2009) only use target labeled data without using gazetteers and word-cluster information. Notice that because of combining two classes of adaption algorithms, our approach is significantly better than these two systems10. Preposition Sense Disambiguation We also test the combination of unlabeled and labeled adaption on the task of Preposition Sense Disambiguation. Here the data contains multiple prepositions where each preposition has many different senses. The goal is to predict the right sen</context>
</contexts>
<marker>Ratinov, Roth, 2009</marker>
<rawString>L. Ratinov and D. Roth. 2009. Design challenges and misconceptions in named entity recognition. In Proc. of the Annual Conference on Computational Natural Language Learning (CoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Erik</author>
</authors>
<title>Tjong Kim Sang and Fien De Meulder.</title>
<date>2003</date>
<booktitle>In Walter Daelemans and</booktitle>
<editor>Miles Osborne, editors,</editor>
<marker>Erik, 2003</marker>
<rawString>Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the conll-2003 shared task: Languageindependent named entity recognition. In Walter Daelemans and Miles Osborne, editors, Proceedings of CoNLL-2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Tratz</author>
<author>D Hovy</author>
</authors>
<title>Disambiguation of preposition sense using linguistically motivated features.</title>
<date>2009</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="35318" citStr="Tratz and Hovy, 2009" startWordPosition="5984" endWordPosition="5987">wo classes of adaption algorithms, our approach is significantly better than these two systems10. Preposition Sense Disambiguation We also test the combination of unlabeled and labeled adaption on the task of Preposition Sense Disambiguation. Here the data contains multiple prepositions where each preposition has many different senses. The goal is to predict the right sense for a given preposition in the testing data. The source domain is the SemEval 2007 preposition WSD Task and the target domain is from the dataset annotated in (Dahlmeier et al., 2009). Our feature design mainly comes from (Tratz and Hovy, 2009) (who do not evaluate their system on our target data). As our un10The work (Ratinov and Roth, 2009) also combines their system with several document-level features. While it is possible to add these features in our system, we do not include any global features for the sake of simplicity. Note that our system is competitive to (Ratinov and Roth, 2009) even though our system does not use global features. 775 Systems Cluster? TGT? P.F1 T.F1 Our NER y y 84.1 86.5 FM09 n y 79.98 N/A RR09 y n N/A 83.2 RR09 + global y n N/A 86.2 Table 3: Comparisons between different NER systems. P.F1 and T.F1 repre</context>
</contexts>
<marker>Tratz, Hovy, 2009</marker>
<rawString>S. Tratz and D. Hovy. 2009. Disambiguation of preposition sense using linguistically motivated features. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tong Zhang</author>
</authors>
<title>Covering number bounds of certain regularized linear function classes.</title>
<date>2002</date>
<journal>J. Mach. Learn. Res.</journal>
<contexts>
<context position="17474" citStr="Zhang, 2002" startWordPosition="2911" endWordPosition="2912"> framework. Here, we compare the mistake bounds of the feature sharing framework to that of the baseline approach, which learns each task independently7. In 6The framework proposed by (Evgeniou and Pontil, 2004; Finkel and Manning, 2009) is a generalization of Eq. (1). It allows the user to weight each block of features. If we put zero weight on the shared block, it becomes the baseline approach. On the other hand, if we put zero weight on all task-specific blocks, the framework becomes the S+T approach. 7Note that mistake bound results can be generalized to generalization bound results. See (Zhang, 2002). order to make the comparison easier, we make some simplifying assumptions. First, we assume that the problem contains only two tasks, 1 and 2. We also assume that IIw1II = IIw2II = a. These assumptions greatly reduce the complexity of the analysis and can give us greater insight into the comparisons. Following the assumptions and Theorem 1, the mistake bound for the FE frameworks is 4(2 − cos(w1, w2))R2a2/(3µ2) (4) This line of analysis leads to interesting bound comparisons for two cases. In the first case, we assume that task 1 and task 2 are essentially the same. In the second, more commo</context>
</contexts>
<marker>Zhang, 2002</marker>
<rawString>Tong Zhang. 2002. Covering number bounds of certain regularized linear function classes. J. Mach. Learn. Res.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>