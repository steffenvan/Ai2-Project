<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.040843">
<title confidence="0.987557">
An analysis of textual inference in German customer emails
</title>
<author confidence="0.971668">
Kathrin Eichler*, Aleksandra Gabryszak*, G¨unter Neumann†
</author>
<affiliation confidence="0.898535">
*German Research Center for Artificial Intelligence (DFKI), Berlin
</affiliation>
<email confidence="0.953616">
(kathrin.eichler|aleksandra.gabryszak@dfki.de)
</email>
<affiliation confidence="0.283195">
†German Research Center for Artificial Intelligence (DFKI), Saarbr¨ucken
</affiliation>
<email confidence="0.993788">
(neumann@dfki.de)
</email>
<sectionHeader confidence="0.99374" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99997705882353">
Human language allows us to express the
same meaning in various ways. Recogniz-
ing that the meaning of one text can be in-
ferred from the meaning of another can be
of help in many natural language process-
ing applications. One such application is
the categorization of emails. In this paper,
we describe the analysis of a real-world
dataset of manually categorized customer
emails written in the German language.
We investigate the nature of textual infer-
ence in this data, laying the ground for de-
veloping an inference-based email catego-
rization system. This is the first analysis
of this kind on German data. We compare
our results to previous analyses on English
data and present major differences.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999796461538462">
A typical situation in customer support is that
many customers send requests describing the same
issue. Recognizing that two different customer
emails refer to the same problem can help save
resources, but can turn out to be a difficult task.
Customer requests are usually written in the form
of unstructured natural language text, i.e., when
automatically processing them, we are faced with
the issue of variability: Different speakers of a lan-
guage express the same meanings using different
linguistic forms. There are, in fact, cases where
two sentences expressing the same meaning do not
share a single word:
</bodyText>
<listItem confidence="0.9758746">
1. “Bild und Ton sind asynchron.” [Picture and
sound are asynchronous.]
2. “Die Tonspur stimmt nicht mit dem Film
¨uberein.” [The audio track does not match the
video.]
</listItem>
<bodyText confidence="0.97615775">
Detecting the semantic equivalence of sentences
1 and 2 requires several textual inference steps: At
the lexical level, it requires mapping the word pic-
ture to video and sound to audio track. At the
level of compositional semantics, it requires de-
tecting the equivalence of the expressions A and B
are asynchronous and A does not match B.
In this paper, we describe our analysis of a large
set of manually categorized customer emails, lay-
ing the ground for developing an email catego-
rization system based on textual inference. In
our analysis, we compared each email text to the
description of its associated category in order to
investigate the nature of the inference steps in-
volved. In particular, our analysis aims to give an-
swers to the following questions: What text repre-
sentation is appropriate for the email categoriza-
tion task? What kind of inference steps are in-
volved and how are they distributed in real-world
data? Answering these questions will not only
help us decide, which existing tools and resources
to integrate in an inference-based email catego-
rization system, but also, which non-existing tools
may be needed in addition.
</bodyText>
<sectionHeader confidence="0.999766" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999586">
The task of email categorization has been ad-
dressed by numerous people in the last decade.
In the customer support domain, work to be men-
tioned includes Eichler (2005), Wicke (2010), and
Eichler et al. (2012).
Approaching the task using textual inference re-
lates to two tasks, for which active research is go-
ing on: Semantic Textual Similarity, which mea-
sures the degree of semantic equivalence (Agirre
et al., 2012) of two texts, and Recognizing Textual
Entailment (RTE), which is defined as recogniz-
ing, given a hypothesis H and a text T, whether the
meaning of H can be inferred from (is entailed in)
T (Dagan et al., 2005). The task of email catego-
rization can be viewed as an RTE task, where T
</bodyText>
<page confidence="0.993219">
69
</page>
<note confidence="0.7613435">
Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 69–74,
Dublin, Ireland, August 23-24 2014.
</note>
<bodyText confidence="0.99995756097561">
refers to the email text and H refers to the cate-
gory description. The goal then is to find out if the
email text entails the category description, and if
so, assign it to the respective category.
In connection with RTE, several groups have
analyzed existing datasets in order to investigate
the nature of textual inference. Bar-Haim (2010)
introduces two levels of entailment, lexical and
lexical-syntactic, and analyzes the contribution of
each level and of individual inference mechanisms
within each level over a sample from the first RTE
Challenge test set (Dagan et al., 2005). He con-
cludes that the main contributors are paraphrases
and syntactic transformations.
Volokh and Neumann (2011) analyzed a subset
of the RTE-7 (Bentivogli et al., 2011) development
data to measure the complexity of the task. They
divide the T/H pairs into three different classes,
depending on the type of knowledge required to
solve the problem: In class A, the relevant infor-
mation is expressed with the same words in both
T and H. In class B, the words used in T are
synonyms to those used in H. In class C, recog-
nizing entailment between H and T requires the
use of logical inference and/or world knowledge.
They conclude that for two thirds of the data a
good word-level analysis is enough, whereas the
remainder of the data contains diverse phenomena
calling for a more sophisticated approach.
A detailed analysis of the linguistic phenomena
involved in semantic inferences in the T-H pairs of
the RTE-5 dataset was presented by (Cabrio and
Magnini, 2013).
As the approaches described above, our anal-
ysis aims at measuring the contribution of infer-
ence mechanisms at different representation lev-
els. However, we focus on a different type of text
(customer request as compared to news) and a dif-
ferent language (German as compared to English).
We thus expect our results to differ from the ones
obtained in previous work.
</bodyText>
<sectionHeader confidence="0.995088" genericHeader="method">
3 Setup
</sectionHeader>
<subsectionHeader confidence="0.975472">
3.1 Dataset
</subsectionHeader>
<bodyText confidence="0.9999878">
We analyzed a dataset consisting of a set of emails
and a set of categories associated to these emails.
The emails contain customer requests sent to the
support center of a multimedia software company,
and mainly concern the products offered by this
company. Each email was manually assigned to
one or more matching categories by a customer
support agent (a domain expert). These categories,
predefined by the data provider, represent previ-
ously identified problems reported by customers.
All emails and category descriptions are written in
German. As is common for this type of data, many
emails contain spelling mistakes, grammatical er-
rors or abbreviations, which make automatic text
processing difficult. An anonymized1 version of
the dataset is available online2. Our data analysis
was done on the original dataset. The data exam-
ples we use in the following, however, are taken
from the anonymized dataset.
In our analysis, we manually compared the
email texts to the descriptions of their associated
categories in order to investigate the nature of the
inference steps involved. In order to reduce the
complexity of the task, we based our analysis on
the subset of categories, for which the category
text described a single problem (a single H, speak-
ing in RTE terms). We also removed emails for
which we were not able to relate the category de-
scription to the email text. However, we kept
emails associated to several categories and ana-
lyzed all of the assignments. The reduced dataset
we used for our analysis consists of 369 emails as-
sociated to 25 categories. The email lengths vary
between 2 and 1246 tokens. Category descriptions
usually consist of a single sentence or a phrase.
</bodyText>
<subsectionHeader confidence="0.999301">
3.2 Task definition
</subsectionHeader>
<bodyText confidence="0.99917580952381">
The task of automatically assigning emails to
matching categories can be viewed as an RTE task,
where T refers to the email text and H refers to the
category description. The goal then is to find out if
the email text entails the category description, and
if so, assign it to the respective category.
For the analysis of inference steps involved, we
distinguish between two levels of inference: lexi-
cal semantics and compositional semantics. At the
lexical level, we distinguish two different types of
text representation: First, the bag-of-tokens repre-
sentation, where both the email text and the cate-
gory description are represented as the set of con-
tent word tokens contained in the respective text.
1The anonymization step was performed to eliminate ref-
erences to the data provider and anonymize personal data
about the customers. During this step, the data was trans-
ferred into a different product domain (online auction sales).
However, the anonymized version is very similar to the orig-
inal one in terms of language style (including spelling errors,
anglicisms, abbreviations, and special characters).
</bodyText>
<footnote confidence="0.9944095">
2http://www.excitement-project.eu/attachments/
article/97/omq_public_email_data.zip
</footnote>
<page confidence="0.997826">
70
</page>
<bodyText confidence="0.999961444444445">
Second, the bag-of-terms representation, where a
“term” can consist of one or more content tokens
occurring consecutively. At this level, following
Bar Haim (2010), we assume that entailment holds
between T (the email) and H (the category descrip-
tion) if every token (term) in H can be matched by
a corresponding entailing token (term) in T.
At the level of compositional semantics, we rep-
resent each text as the set of complex expressions
(combinations of terms linked syntactically and
semantically) contained in it. At this level, we
assume that entailment holds between T and H if
every term in H is part of at least one complex ex-
pression that can be matched by a corresponding
entailing expression in T.
The data analysis was carried out by two people
separately (one of them an author of this paper),
who analyzed each assignment of an email E to
a category C based on predefined analysis guide-
lines. For each of the text representation types de-
scribed above, the task of the annotators was to
find, for each expression in the description of C, a
semantically equivalent or entailing expression in
E.3 If such an expression was found, all involved
inference steps were to be noted down in an anno-
tation table. The predefined list of possible infer-
ence steps is explained in detail in the following.
</bodyText>
<sectionHeader confidence="0.999759" genericHeader="method">
4 Inference steps
</sectionHeader>
<subsectionHeader confidence="0.99893">
4.1 Lexical semantics level
</subsectionHeader>
<bodyText confidence="0.999932866666667">
For each of the three different types of represen-
tation (token, term, complex expression), we dis-
tinguish various inference steps. At the lexical
level, we distinguish among spelling, inflection,
derivation, composition, lexical semantics at the
token level and lexical semantics at the term level.
This distinction was made based on the assump-
tion that for each of these steps a different NLP
tool or resource is required (e.g., a lemmatizer for
inflection, a compound splitter for composition,
a lexical-semantic net for lexical semantics). We
also distinguish between token and term level lexi-
cal semantics, as, for term-level lexical semantics,
we assume that a tool for detecting multi-token
terms would be required.
</bodyText>
<footnote confidence="0.783657142857143">
3A preanalysis of the data revealed that in some cases,
the entailment direction seemed to be flipped: Expressions
in the category description entailed expressions in the email
text, e.g. “Video” (video) → “Film” (film). In our analysis,
we counted these as positive cases if the context suggested
that both expressions were used to express the same idea. We
consider this an interesting issue to be further investigated.
</footnote>
<subsectionHeader confidence="0.971992">
4.2 Compositional semantics level
</subsectionHeader>
<bodyText confidence="0.999977897435898">
At the level of compositional semantics, we con-
sider inference steps involving complex expres-
sions.4 These steps go beyond the lexical level
and would require the usage of at least a syntac-
tic parser for detecting word dependencies and a
tool for recognizing entailment between two com-
plex expressions. At this level, we also record the
frequency of three particular phenomena: parti-
cle verbs, negation, and light verb constructions,
which we considered worth addressing separately.
Particle verbs are important when processing
German because, unlike in English, they can oc-
cur both as one token or two, dependending on the
syntactic construction, in which they are embed-
ded (e.g., “aufnehmen” and “nehme [...] auf” [(to)
record]. Recognizing the scope of negation can be
required in cases where negation is expressed im-
plicitly in one of the sentences, e.g., “A und B sind
nicht synchron” [A and B are not synchronous]
vs. “Es kommt zu Versetzung zwischen A und
B” [There is a misaligment between A and B]. By
light verbs we refer to verbs with little semantic
content of their own, forming a linguistic unit with
a noun or prepositional phrase, for which a single
verb with a similar meaning exists, e.g., “Meldung
kommt” [message appears] vs. “melden” [notify].
For example, for the text pair “Das Brennen
bricht ab mit der Meldung X” [Burning breaks
with message X] and “Beim Brennen kommt die
Fehlermeldung X” [When burning, error message
X appears], the word “Meldung” [message] was
recorded as inference at the token level because
it can be derived from “Fehlermeldung” [error
message] using decomposition. The verb “bricht
ab” [break] was considered inference at the level
of compositional semantics because there is no
lexical-semantic relation to the verb “kommt” [ap-
pears]. The verb can thus only be matched by con-
sidering the complete expression.
</bodyText>
<subsectionHeader confidence="0.998655">
4.3 Possible effects on precision
</subsectionHeader>
<bodyText confidence="0.999946714285714">
The focus of the analysis described so far was
on ways to improve recall in an email catego-
rization system: We count the inference steps re-
quired to increase the amount of mappable infor-
mation (similar to query expansion in informa-
tion retrieval). However, the figures do not show
the impact of these mappings on precision, i.e.,
</bodyText>
<footnote confidence="0.984391">
4Additional lexical inference steps required at this level
are not recorded.
</footnote>
<page confidence="0.999189">
71
</page>
<bodyText confidence="0.999987333333333">
whether an inference step we take would nega-
tively affect the precision of the system. Taking
a more precision-oriented view at the problem, we
also counted the number of cases for which a more
complex representation could be “helpful” (albeit
not necessary). For example, inferring the negated
expression “Programm kann die DVD nicht ab-
spielen” [Program cannot play the DVD] from
“Programm kann die DVD nicht laden”’ [Program
does not load the DVD] is possible at the lexical
level, assuming that “abspielen” [(to) play] entails
“laden” [(to) load]. However, knowing that both
verbal expressions are negated is expected to be
beneficial to precision, in order to avoid wrongly
inferring a negated from a non-negated expression.
</bodyText>
<sectionHeader confidence="0.999983" genericHeader="evaluation">
5 Results
</sectionHeader>
<subsectionHeader confidence="0.988927">
5.1 Interannotator agreement
</subsectionHeader>
<bodyText confidence="0.999975045454545">
Our analysis was done by two people separately,
which allowed us to measure the reliability of the
annotation for the different inference steps. The
kappa coefficient (Cohen, 1960) for spelling, in-
flection, derivation and composition ranged be-
tween 0.46 and 0.67, i.e., moderate to substan-
tial agreement according to the scale proposed by
Landis and Koch (1977). For lexical semantics,
the value is only fair (0.38). An analysis showed
that the identification of a lexical semantic rela-
tion is often not straightforward, and may require
a good knowledge of the domain. For example,
the verbs “aufrufen” [call] and “importieren” [im-
port], which would usually not be considered to
be semantically related, may in fact be used to de-
scribe the same action in the computer domain, re-
ferring to files. Also for the more complex infer-
ence steps, we measured only fair agreement, due
to the number of positive and negative cases being
very skewed. For the “helpful” cases, the values
ranged between 0.73 and 0.79 (substantial agree-
ment).
</bodyText>
<subsectionHeader confidence="0.999977">
5.2 Distribution of inference steps
</subsectionHeader>
<bodyText confidence="0.999881375">
Table 1 summarizes the distribution of inference
steps identified in our data for each text represen-
tation type, ordered by their frequency of occur-
rence.5 For multi-token terms, particle verbs, and
negation, the number of “helpful” cases is given in
brackets.
Our results show that the most important infer-
ence step at the lexical level is lexical semantics.
</bodyText>
<footnote confidence="0.828688">
5Based on the steps agreed on after a consolidation phase.
</footnote>
<bodyText confidence="0.999981048780488">
At the lexical level, we found 157 different word
mappings. Only 26 of them correspond to a re-
lation in GermaNet (Hamp and Feldweg, 1997),
version 7.0. 48 of the involved words had no Ger-
maNet entry at all, due to the word being an an-
glicism (e.g., “Error” instead of “Fehler”), a non-
lexicalized compound (e.g., “Bildschirmbereich”
[screen area]) or a highly domain- or application-
specific word (for only 37.5% of the words miss-
ing in GermaNet, we found an entry in Wikipedia).
In 72 cases, both words had a GermaNet entry,
but no relation existed, usually because the rela-
tion was too domain-specific.
For more than 30% of the words (as compared
to 10.1% in Bar-Haim’s (2010) analysis on En-
glish), a morphological transformation is required,
which can be explained by the high complexity of
German morphology as compared to the morphol-
ogy of English. Spelling mistakes or differences,
which are not considered in other analyses, are
also found in a considerable number of words, the
reason being that customer emails are less well-
formed than, for example, news texts.
The significance of multi-token terms was sur-
prisingly high for German, where word combina-
tions are usually expressed in the form of com-
pounds (i.e., a single token). In our data, multi-
token terms were usually compounds consisting
of at least one anglicism (e.g., “USB Anschluss”
[USB port]). This suggests that texts written in
a domain language with a high proportion of En-
glish loan words may be more difficult to process
than general language texts, as multi-token terms
have to be recognized.
At the level of compositional semantics, it
should be noted that, in many cases, recogniz-
ing the entailment relation between two expres-
sions requires world or domain knowledge. Sev-
eral of the mappings involved particle verbs or
light verbs. Detecting negation scope is expected
to be important in a precision-oriented system.
</bodyText>
<subsectionHeader confidence="0.99968">
5.3 Comparing text representations
</subsectionHeader>
<bodyText confidence="0.999941875">
We also had a look at the amount of information
left unmapped at each level. For the lexical level,
we determined for how many of the content tokens
(terms) occurring in the category descriptions, no
matching expression was found in the associated
emails. For the level of compositional semantics,
we looked at each term left unmapped at the lexi-
cal level and tried to map a complex expression in
</bodyText>
<page confidence="0.996373">
72
</page>
<table confidence="0.9998111875">
Type of inference Data example Total (Share)
Lexical semantics “Anfang” [start] → “Beginn” [beginning] 310 (20.2%)
(Token)
Inflection “startet” [starts] → “starten” [start] 206 (13.4%)
Derivation “Import” [import] → “importieren” [(to) import] 164 (10.7%)
Composition “Fehlermeldung” [error message] → “Meldung” [message] 158 (10.3%)
Spelling “Dateine” → “Dateien” [files] 47 (3.1%)
Lexic(Term) al semantics [+1240(8 .1%)]
“MPEG Datei“ [MPEG file] → “Video” [video]
Particle verbs “spielt [...] ab” [play] → “abspielen” [play] 26 (1.8%)
[+34 (2.4%)]
Light verbs “Meldung kommt” [message appears] → “melden” [notify] 17 (1.2%)
Negation “Brennerger¨at kann nicht gefunden werden” [Burning device cannot be found] 8 (0.6%)
→ “Es wird kein Brenner gefunden” [No burner is found ] [+121 (8.4%)]
Other complex “Das Brennen bricht ab mit der Meldung X” [Burning breaks with message X] 83 (5.7%)
expressions → “Beim Brennen kommt die Fehlermeldung X” [Burning yields error message X ]
</table>
<tableCaption confidence="0.999531">
Table 1: Distribution of inference steps in the dataset.
</tableCaption>
<bodyText confidence="0.99679325">
which the term occurred. If for none of these ex-
pressions a matching expression was found in the
email, the term was counted as non-mappable at
this level.
</bodyText>
<table confidence="0.991055">
Representation Non-mappable Share
Tokens 428 / 1538 27.8%
Terms 365 /1446 25.2%
Complex expressions 229 / 1446 15.8%
</table>
<bodyText confidence="0.999893222222222">
The above table shows that the majority of
the required inference relates to the lexical level.
Choosing a representation that allows us to map
more complex expressions, increases the amount
of mappable terms by almost 10%. However, even
with this more complex representation, a consider-
able amount of terms (15.8%) cannot be mapped
at all because the email text does not contain all
information specified in the category description.
</bodyText>
<sectionHeader confidence="0.999499" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999976612903226">
In our analysis, we examined the inference steps
required to determine that the text of a category de-
scription can be inferred from the text of a particu-
lar email associated to this category. We identified
major inference phenomena and determined their
distribution in a German real-world dataset. Our
analysis supports previous results for English data
in that a large portion of the required inference re-
lates to the lexical level. Choosing a representa-
tion that allows us to map more complex expres-
sions significantly increases the amount of map-
pable expressions, but some expressions simply
cannot be mapped because the categorization was
done relying on partial information in the email.
Our results extend previous results by investi-
gating inference steps specific to the German lan-
guage (such as morphology, composition, and par-
ticle verbs). Some outcomes are unexpected for
the German language, such as the high share of
multi-token terms. Our analysis also stresses the
importance of inference steps relying on domain-
specific resources, i.e., for this type of data, the
development of tools and resources to support in-
ference in highly specialized domains is crucial.
We are currently using the results of our anal-
ysis to build an email categorization system that
integrates linguistic resources and tools to expand
the linguistic expressions in an incoming email
with entailed expressions. This will allow us to
measure the performance of such a system, in par-
ticular with respect to the effect on precision.
</bodyText>
<sectionHeader confidence="0.99495" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999757625">
This work was partially supported by the EX-
CITEMENT project (EU grant FP7 ICT-287923)
and the German Federal Ministry of Education and
Research (Software Campus grant 01—S12050 ).
We would like to thank OMQ GmbH for provid-
ing the dataset, Britta Zeller and Jonas Placzek for
the data anonymization, and Stefania Racioppa for
her help in the annotation phase.
</bodyText>
<footnote confidence="0.9686745">
This work is licensed under a Creative Commons Attribution
4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http:
//creativecommons.org/licenses/by/4.0/
</footnote>
<page confidence="0.999279">
73
</page>
<sectionHeader confidence="0.995856" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99914296">
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 Task 6:
A Pilot on Semantic Textual Similarity. In *SEM
2012: The First Joint Conference on Lexical and
Computational Semantics (SemEval 2012), pages
385–393, Montr´eal, Canada, 7-8 June. Association
for Computational Linguistics.
Roy Bar-Haim. 2010. Semantic Inference at the
Lexical-Syntactic Level. Ph.D. thesis, Department
of Computer Science, Bar Ilan University, Ramat
Gan, Israel.
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa T. Dang,
and Danilo Giampiccolo. 2011. The Seventh PAS-
CAL Recognizing Textual Entailment Challenge. In
Proceedings of TAC.
Elena Cabrio and Bernardo Magnini. 2013. Decom-
posing Semantic Inferences. Linguistics Issues in
Language Technology - LiLT. Special Issues on the
Semantics of Entailment, 9(1), August.
Jacob Cohen. 1960. A coefficient of agreement
for nominal scales. Educational and Psychological
Measurement, 20(1):37.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The PASCAL Recognising Textual Entail-
ment Challenge. In Proceedings of the PASCAL
Challenges Workshop on Recognising Textual En-
tailment.
Kathrin Eichler, Matthias Meisdrock, and Sven
Schmeier. 2012. Search and Topic Detection in
Customer Requests - Optimizing a Customer Sup-
port System. KI, 26(4):419–422.
Kathrin Eichler. 2005. Automatic classification of
Swedish email messages. Bachelor thesis, Eberhard-
Karls-Universit¨at, T¨ubingen, Germany.
Birgit Hamp and Helmut Feldweg. 1997. GermaNet -
a Lexical-Semantic Net for German. In In Proceed-
ings of ACL workshop Automatic Information Ex-
traction and Building of Lexical Semantic Resources
for NLP Applications, pages 9–15.
J. R. Landis and G. G. Koch. 1977. The Measurement
of Observer Agreement for Categorical Data. Bio-
metrics, 33(1):159–174, March.
Alexander Volokh and G¨unter Neumann. 2011. Using
MT-Based Metrics for RTE. In Proceedings of the
4th Text Analysis Conference (TAC 2011), Gaithers-
burg, Maryland, USA, November. National Institute
of Standards and Technology.
Janine Wicke. 2010. Automated Email Classification
using Semantic Relationships. Master thesis, KTH
Royal Institute of Technology, Stockholm, Sweden.
</reference>
<page confidence="0.999148">
74
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.921485">
<title confidence="0.995001">An analysis of textual inference in German customer emails</title>
<author confidence="0.999293">Aleksandra G¨unter</author>
<affiliation confidence="0.98577">Research Center for Artificial Intelligence (DFKI), Berlin</affiliation>
<address confidence="0.959979">Research Center for Artificial Intelligence (DFKI), Saarbr¨ucken</address>
<abstract confidence="0.998710277777778">Human language allows us to express the same meaning in various ways. Recognizing that the meaning of one text can be inferred from the meaning of another can be of help in many natural language processing applications. One such application is the categorization of emails. In this paper, we describe the analysis of a real-world dataset of manually categorized customer emails written in the German language. We investigate the nature of textual inference in this data, laying the ground for developing an inference-based email categorization system. This is the first analysis of this kind on German data. We compare our results to previous analyses on English data and present major differences.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor Gonzalez-Agirre</author>
</authors>
<date>2012</date>
<booktitle>SemEval-2012 Task 6: A Pilot on Semantic Textual Similarity. In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics (SemEval</booktitle>
<pages>385--393</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal,</location>
<contexts>
<context position="3404" citStr="Agirre et al., 2012" startWordPosition="538" endWordPosition="541">ll not only help us decide, which existing tools and resources to integrate in an inference-based email categorization system, but also, which non-existing tools may be needed in addition. 2 Related Work The task of email categorization has been addressed by numerous people in the last decade. In the customer support domain, work to be mentioned includes Eichler (2005), Wicke (2010), and Eichler et al. (2012). Approaching the task using textual inference relates to two tasks, for which active research is going on: Semantic Textual Similarity, which measures the degree of semantic equivalence (Agirre et al., 2012) of two texts, and Recognizing Textual Entailment (RTE), which is defined as recognizing, given a hypothesis H and a text T, whether the meaning of H can be inferred from (is entailed in) T (Dagan et al., 2005). The task of email categorization can be viewed as an RTE task, where T 69 Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 69–74, Dublin, Ireland, August 23-24 2014. refers to the email text and H refers to the category description. The goal then is to find out if the email text entails the category description, and if so, assign it to</context>
</contexts>
<marker>Agirre, Cer, Diab, Gonzalez-Agirre, 2012</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. 2012. SemEval-2012 Task 6: A Pilot on Semantic Textual Similarity. In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics (SemEval 2012), pages 385–393, Montr´eal, Canada, 7-8 June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Bar-Haim</author>
</authors>
<title>Semantic Inference at the Lexical-Syntactic Level.</title>
<date>2010</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer Science, Bar Ilan University, Ramat Gan, Israel.</institution>
<contexts>
<context position="4173" citStr="Bar-Haim (2010)" startWordPosition="673" endWordPosition="674">be inferred from (is entailed in) T (Dagan et al., 2005). The task of email categorization can be viewed as an RTE task, where T 69 Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 69–74, Dublin, Ireland, August 23-24 2014. refers to the email text and H refers to the category description. The goal then is to find out if the email text entails the category description, and if so, assign it to the respective category. In connection with RTE, several groups have analyzed existing datasets in order to investigate the nature of textual inference. Bar-Haim (2010) introduces two levels of entailment, lexical and lexical-syntactic, and analyzes the contribution of each level and of individual inference mechanisms within each level over a sample from the first RTE Challenge test set (Dagan et al., 2005). He concludes that the main contributors are paraphrases and syntactic transformations. Volokh and Neumann (2011) analyzed a subset of the RTE-7 (Bentivogli et al., 2011) development data to measure the complexity of the task. They divide the T/H pairs into three different classes, depending on the type of knowledge required to solve the problem: In class</context>
</contexts>
<marker>Bar-Haim, 2010</marker>
<rawString>Roy Bar-Haim. 2010. Semantic Inference at the Lexical-Syntactic Level. Ph.D. thesis, Department of Computer Science, Bar Ilan University, Ramat Gan, Israel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luisa Bentivogli</author>
<author>Peter Clark</author>
<author>Ido Dagan</author>
<author>Hoa T Dang</author>
<author>Danilo Giampiccolo</author>
</authors>
<title>The Seventh PASCAL Recognizing Textual Entailment Challenge.</title>
<date>2011</date>
<booktitle>In Proceedings of TAC.</booktitle>
<contexts>
<context position="4586" citStr="Bentivogli et al., 2011" startWordPosition="734" endWordPosition="737">egory description, and if so, assign it to the respective category. In connection with RTE, several groups have analyzed existing datasets in order to investigate the nature of textual inference. Bar-Haim (2010) introduces two levels of entailment, lexical and lexical-syntactic, and analyzes the contribution of each level and of individual inference mechanisms within each level over a sample from the first RTE Challenge test set (Dagan et al., 2005). He concludes that the main contributors are paraphrases and syntactic transformations. Volokh and Neumann (2011) analyzed a subset of the RTE-7 (Bentivogli et al., 2011) development data to measure the complexity of the task. They divide the T/H pairs into three different classes, depending on the type of knowledge required to solve the problem: In class A, the relevant information is expressed with the same words in both T and H. In class B, the words used in T are synonyms to those used in H. In class C, recognizing entailment between H and T requires the use of logical inference and/or world knowledge. They conclude that for two thirds of the data a good word-level analysis is enough, whereas the remainder of the data contains diverse phenomena calling for</context>
</contexts>
<marker>Bentivogli, Clark, Dagan, Dang, Giampiccolo, 2011</marker>
<rawString>Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa T. Dang, and Danilo Giampiccolo. 2011. The Seventh PASCAL Recognizing Textual Entailment Challenge. In Proceedings of TAC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elena Cabrio</author>
<author>Bernardo Magnini</author>
</authors>
<title>Decomposing Semantic Inferences. Linguistics Issues</title>
<date>2013</date>
<booktitle>in Language Technology - LiLT. Special Issues on the Semantics of Entailment,</booktitle>
<volume>9</volume>
<issue>1</issue>
<contexts>
<context position="5379" citStr="Cabrio and Magnini, 2013" startWordPosition="871" endWordPosition="874">e problem: In class A, the relevant information is expressed with the same words in both T and H. In class B, the words used in T are synonyms to those used in H. In class C, recognizing entailment between H and T requires the use of logical inference and/or world knowledge. They conclude that for two thirds of the data a good word-level analysis is enough, whereas the remainder of the data contains diverse phenomena calling for a more sophisticated approach. A detailed analysis of the linguistic phenomena involved in semantic inferences in the T-H pairs of the RTE-5 dataset was presented by (Cabrio and Magnini, 2013). As the approaches described above, our analysis aims at measuring the contribution of inference mechanisms at different representation levels. However, we focus on a different type of text (customer request as compared to news) and a different language (German as compared to English). We thus expect our results to differ from the ones obtained in previous work. 3 Setup 3.1 Dataset We analyzed a dataset consisting of a set of emails and a set of categories associated to these emails. The emails contain customer requests sent to the support center of a multimedia software company, and mainly c</context>
</contexts>
<marker>Cabrio, Magnini, 2013</marker>
<rawString>Elena Cabrio and Bernardo Magnini. 2013. Decomposing Semantic Inferences. Linguistics Issues in Language Technology - LiLT. Special Issues on the Semantics of Entailment, 9(1), August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Cohen</author>
</authors>
<title>A coefficient of agreement for nominal scales.</title>
<date>1960</date>
<booktitle>Educational and Psychological Measurement,</booktitle>
<pages>20--1</pages>
<contexts>
<context position="14454" citStr="Cohen, 1960" startWordPosition="2337" endWordPosition="2338">bspielen” [Program cannot play the DVD] from “Programm kann die DVD nicht laden”’ [Program does not load the DVD] is possible at the lexical level, assuming that “abspielen” [(to) play] entails “laden” [(to) load]. However, knowing that both verbal expressions are negated is expected to be beneficial to precision, in order to avoid wrongly inferring a negated from a non-negated expression. 5 Results 5.1 Interannotator agreement Our analysis was done by two people separately, which allowed us to measure the reliability of the annotation for the different inference steps. The kappa coefficient (Cohen, 1960) for spelling, inflection, derivation and composition ranged between 0.46 and 0.67, i.e., moderate to substantial agreement according to the scale proposed by Landis and Koch (1977). For lexical semantics, the value is only fair (0.38). An analysis showed that the identification of a lexical semantic relation is often not straightforward, and may require a good knowledge of the domain. For example, the verbs “aufrufen” [call] and “importieren” [import], which would usually not be considered to be semantically related, may in fact be used to describe the same action in the computer domain, refe</context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>Jacob Cohen. 1960. A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20(1):37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The PASCAL Recognising Textual Entailment Challenge.</title>
<date>2005</date>
<booktitle>In Proceedings of the PASCAL Challenges Workshop on Recognising Textual Entailment.</booktitle>
<contexts>
<context position="3614" citStr="Dagan et al., 2005" startWordPosition="577" endWordPosition="580">k of email categorization has been addressed by numerous people in the last decade. In the customer support domain, work to be mentioned includes Eichler (2005), Wicke (2010), and Eichler et al. (2012). Approaching the task using textual inference relates to two tasks, for which active research is going on: Semantic Textual Similarity, which measures the degree of semantic equivalence (Agirre et al., 2012) of two texts, and Recognizing Textual Entailment (RTE), which is defined as recognizing, given a hypothesis H and a text T, whether the meaning of H can be inferred from (is entailed in) T (Dagan et al., 2005). The task of email categorization can be viewed as an RTE task, where T 69 Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 69–74, Dublin, Ireland, August 23-24 2014. refers to the email text and H refers to the category description. The goal then is to find out if the email text entails the category description, and if so, assign it to the respective category. In connection with RTE, several groups have analyzed existing datasets in order to investigate the nature of textual inference. Bar-Haim (2010) introduces two levels of entailment, lex</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2005</marker>
<rawString>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The PASCAL Recognising Textual Entailment Challenge. In Proceedings of the PASCAL Challenges Workshop on Recognising Textual Entailment.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathrin Eichler</author>
<author>Matthias Meisdrock</author>
<author>Sven Schmeier</author>
</authors>
<title>Search and Topic Detection in Customer Requests - Optimizing a Customer Support System.</title>
<date>2012</date>
<pages>26--4</pages>
<publisher>KI,</publisher>
<contexts>
<context position="3196" citStr="Eichler et al. (2012)" startWordPosition="504" endWordPosition="507">ing questions: What text representation is appropriate for the email categorization task? What kind of inference steps are involved and how are they distributed in real-world data? Answering these questions will not only help us decide, which existing tools and resources to integrate in an inference-based email categorization system, but also, which non-existing tools may be needed in addition. 2 Related Work The task of email categorization has been addressed by numerous people in the last decade. In the customer support domain, work to be mentioned includes Eichler (2005), Wicke (2010), and Eichler et al. (2012). Approaching the task using textual inference relates to two tasks, for which active research is going on: Semantic Textual Similarity, which measures the degree of semantic equivalence (Agirre et al., 2012) of two texts, and Recognizing Textual Entailment (RTE), which is defined as recognizing, given a hypothesis H and a text T, whether the meaning of H can be inferred from (is entailed in) T (Dagan et al., 2005). The task of email categorization can be viewed as an RTE task, where T 69 Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 69–74,</context>
</contexts>
<marker>Eichler, Meisdrock, Schmeier, 2012</marker>
<rawString>Kathrin Eichler, Matthias Meisdrock, and Sven Schmeier. 2012. Search and Topic Detection in Customer Requests - Optimizing a Customer Support System. KI, 26(4):419–422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathrin Eichler</author>
</authors>
<title>Automatic classification of Swedish email messages. Bachelor thesis,</title>
<date>2005</date>
<location>EberhardKarls-Universit¨at, T¨ubingen, Germany.</location>
<contexts>
<context position="3155" citStr="Eichler (2005)" startWordPosition="499" endWordPosition="500">aims to give answers to the following questions: What text representation is appropriate for the email categorization task? What kind of inference steps are involved and how are they distributed in real-world data? Answering these questions will not only help us decide, which existing tools and resources to integrate in an inference-based email categorization system, but also, which non-existing tools may be needed in addition. 2 Related Work The task of email categorization has been addressed by numerous people in the last decade. In the customer support domain, work to be mentioned includes Eichler (2005), Wicke (2010), and Eichler et al. (2012). Approaching the task using textual inference relates to two tasks, for which active research is going on: Semantic Textual Similarity, which measures the degree of semantic equivalence (Agirre et al., 2012) of two texts, and Recognizing Textual Entailment (RTE), which is defined as recognizing, given a hypothesis H and a text T, whether the meaning of H can be inferred from (is entailed in) T (Dagan et al., 2005). The task of email categorization can be viewed as an RTE task, where T 69 Proceedings of the Third Joint Conference on Lexical and Computat</context>
</contexts>
<marker>Eichler, 2005</marker>
<rawString>Kathrin Eichler. 2005. Automatic classification of Swedish email messages. Bachelor thesis, EberhardKarls-Universit¨at, T¨ubingen, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Birgit Hamp</author>
<author>Helmut Feldweg</author>
</authors>
<title>GermaNet -a Lexical-Semantic Net for German. In</title>
<date>1997</date>
<booktitle>In Proceedings of ACL workshop Automatic Information Extraction and Building of Lexical Semantic Resources for NLP Applications,</booktitle>
<pages>9--15</pages>
<contexts>
<context position="15896" citStr="Hamp and Feldweg, 1997" startWordPosition="2574" endWordPosition="2577"> 0.79 (substantial agreement). 5.2 Distribution of inference steps Table 1 summarizes the distribution of inference steps identified in our data for each text representation type, ordered by their frequency of occurrence.5 For multi-token terms, particle verbs, and negation, the number of “helpful” cases is given in brackets. Our results show that the most important inference step at the lexical level is lexical semantics. 5Based on the steps agreed on after a consolidation phase. At the lexical level, we found 157 different word mappings. Only 26 of them correspond to a relation in GermaNet (Hamp and Feldweg, 1997), version 7.0. 48 of the involved words had no GermaNet entry at all, due to the word being an anglicism (e.g., “Error” instead of “Fehler”), a nonlexicalized compound (e.g., “Bildschirmbereich” [screen area]) or a highly domain- or applicationspecific word (for only 37.5% of the words missing in GermaNet, we found an entry in Wikipedia). In 72 cases, both words had a GermaNet entry, but no relation existed, usually because the relation was too domain-specific. For more than 30% of the words (as compared to 10.1% in Bar-Haim’s (2010) analysis on English), a morphological transformation is requ</context>
</contexts>
<marker>Hamp, Feldweg, 1997</marker>
<rawString>Birgit Hamp and Helmut Feldweg. 1997. GermaNet -a Lexical-Semantic Net for German. In In Proceedings of ACL workshop Automatic Information Extraction and Building of Lexical Semantic Resources for NLP Applications, pages 9–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Landis</author>
<author>G G Koch</author>
</authors>
<title>The Measurement of Observer Agreement for Categorical Data.</title>
<date>1977</date>
<journal>Biometrics,</journal>
<volume>33</volume>
<issue>1</issue>
<contexts>
<context position="14635" citStr="Landis and Koch (1977)" startWordPosition="2364" endWordPosition="2367">n” [(to) play] entails “laden” [(to) load]. However, knowing that both verbal expressions are negated is expected to be beneficial to precision, in order to avoid wrongly inferring a negated from a non-negated expression. 5 Results 5.1 Interannotator agreement Our analysis was done by two people separately, which allowed us to measure the reliability of the annotation for the different inference steps. The kappa coefficient (Cohen, 1960) for spelling, inflection, derivation and composition ranged between 0.46 and 0.67, i.e., moderate to substantial agreement according to the scale proposed by Landis and Koch (1977). For lexical semantics, the value is only fair (0.38). An analysis showed that the identification of a lexical semantic relation is often not straightforward, and may require a good knowledge of the domain. For example, the verbs “aufrufen” [call] and “importieren” [import], which would usually not be considered to be semantically related, may in fact be used to describe the same action in the computer domain, referring to files. Also for the more complex inference steps, we measured only fair agreement, due to the number of positive and negative cases being very skewed. For the “helpful” cas</context>
</contexts>
<marker>Landis, Koch, 1977</marker>
<rawString>J. R. Landis and G. G. Koch. 1977. The Measurement of Observer Agreement for Categorical Data. Biometrics, 33(1):159–174, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Volokh</author>
<author>G¨unter Neumann</author>
</authors>
<title>Using MT-Based Metrics for RTE.</title>
<date>2011</date>
<booktitle>In Proceedings of the 4th Text Analysis Conference (TAC 2011),</booktitle>
<institution>National Institute of Standards and Technology.</institution>
<location>Gaithersburg, Maryland, USA,</location>
<contexts>
<context position="4529" citStr="Volokh and Neumann (2011)" startWordPosition="724" endWordPosition="727">goal then is to find out if the email text entails the category description, and if so, assign it to the respective category. In connection with RTE, several groups have analyzed existing datasets in order to investigate the nature of textual inference. Bar-Haim (2010) introduces two levels of entailment, lexical and lexical-syntactic, and analyzes the contribution of each level and of individual inference mechanisms within each level over a sample from the first RTE Challenge test set (Dagan et al., 2005). He concludes that the main contributors are paraphrases and syntactic transformations. Volokh and Neumann (2011) analyzed a subset of the RTE-7 (Bentivogli et al., 2011) development data to measure the complexity of the task. They divide the T/H pairs into three different classes, depending on the type of knowledge required to solve the problem: In class A, the relevant information is expressed with the same words in both T and H. In class B, the words used in T are synonyms to those used in H. In class C, recognizing entailment between H and T requires the use of logical inference and/or world knowledge. They conclude that for two thirds of the data a good word-level analysis is enough, whereas the rem</context>
</contexts>
<marker>Volokh, Neumann, 2011</marker>
<rawString>Alexander Volokh and G¨unter Neumann. 2011. Using MT-Based Metrics for RTE. In Proceedings of the 4th Text Analysis Conference (TAC 2011), Gaithersburg, Maryland, USA, November. National Institute of Standards and Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janine Wicke</author>
</authors>
<title>Automated Email Classification using Semantic Relationships. Master thesis,</title>
<date>2010</date>
<institution>KTH Royal Institute of Technology,</institution>
<location>Stockholm,</location>
<contexts>
<context position="3169" citStr="Wicke (2010)" startWordPosition="501" endWordPosition="502">wers to the following questions: What text representation is appropriate for the email categorization task? What kind of inference steps are involved and how are they distributed in real-world data? Answering these questions will not only help us decide, which existing tools and resources to integrate in an inference-based email categorization system, but also, which non-existing tools may be needed in addition. 2 Related Work The task of email categorization has been addressed by numerous people in the last decade. In the customer support domain, work to be mentioned includes Eichler (2005), Wicke (2010), and Eichler et al. (2012). Approaching the task using textual inference relates to two tasks, for which active research is going on: Semantic Textual Similarity, which measures the degree of semantic equivalence (Agirre et al., 2012) of two texts, and Recognizing Textual Entailment (RTE), which is defined as recognizing, given a hypothesis H and a text T, whether the meaning of H can be inferred from (is entailed in) T (Dagan et al., 2005). The task of email categorization can be viewed as an RTE task, where T 69 Proceedings of the Third Joint Conference on Lexical and Computational Semantic</context>
</contexts>
<marker>Wicke, 2010</marker>
<rawString>Janine Wicke. 2010. Automated Email Classification using Semantic Relationships. Master thesis, KTH Royal Institute of Technology, Stockholm, Sweden.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>