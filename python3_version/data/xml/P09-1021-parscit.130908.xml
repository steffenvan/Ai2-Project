<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.994988">
Active Learning for Multilingual Statistical Machine Translation*
</title>
<author confidence="0.961499">
Gholamreza Haffari and Anoop Sarkar
</author>
<affiliation confidence="0.776026">
School of Computing Science, Simon Fraser University
British Columbia, Canada
</affiliation>
<email confidence="0.961042">
{ghaffar1,anoop}@cs.sfu.ca
</email>
<sectionHeader confidence="0.982965" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999710111111111">
Statistical machine translation (SMT)
models require bilingual corpora for train-
ing, and these corpora are often multi-
lingual with parallel text in multiple lan-
guages simultaneously. We introduce an
active learning task of adding a new lan-
guage to an existing multilingual set of
parallel text and constructing high quality
MT systems, from each language in the
collection into this new target language.
We show that adding a new language using
active learning to the EuroParl corpus pro-
vides a significant improvement compared
to a random sentence selection baseline.
We also provide new highly effective sen-
tence selection methods that improve AL
for phrase-based SMT in the multilingual
and single language pair setting.
</bodyText>
<sectionHeader confidence="0.998429" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.989535484848485">
The main source of training data for statistical
machine translation (SMT) models is a parallel
corpus. In many cases, the same information is
available in multiple languages simultaneously as
a multilingual parallel corpus, e.g., European Par-
liament (EuroParl) and U.N. proceedings. In this
paper, we consider how to use active learning (AL)
in order to add a new language to such a multilin-
gual parallel corpus and at the same time we con-
struct an MT system from each language in the
original corpus into this new target language. We
introduce a novel combined measure of translation
quality for multiple target language outputs (the
same content from multiple source languages).
The multilingual setting provides new opportu-
nities for AL over and above a single language
pair. This setting is similar to the multi-task AL
scenario (Reichart et al., 2008). In our case, the
multiple tasks are individual machine translation
tasks for several language pairs. The nature of the
translation processes vary from any of the source
*Thanks to James Peltier for systems support for our ex-
periments. This research was partially supported by NSERC,
Canada (RGPIN: 264905) and an IBM Faculty Award.
languages to the new language depending on the
characteristics of each source-target language pair,
hence these tasks are competing for annotating the
same resource. However it may be that in a single
language pair, AL would pick a particular sentence
for annotation, but in a multilingual setting, a dif-
ferent source language might be able to provide a
good translation, thus saving annotation effort. In
this paper, we explore how multiple MT systems
can be used to effectively pick instances that are
more likely to improve training quality.
Active learning is framed as an iterative learn-
ing process. In each iteration new human labeled
instances (manual translations) are added to the
training data based on their expected training qual-
ity. However, if we start with only a small amount
of initial parallel data for the new target language,
then translation quality is very poor and requires
a very large injection of human labeled data to
be effective. To deal with this, we use a novel
framework for active learning: we assume we are
given a small amount of parallel text and a large
amount of monolingual source language text; us-
ing these resources, we create a large noisy par-
allel text which we then iteratively improve using
small injections of human translations. When we
build multiple MT systems from multiple source
languages to the new target language, each MT
system can be seen as a different ‘view’ on the de-
sired output translation. Thus, we can train our
multiple MT systems using either self-training or
co-training (Blum and Mitchell, 1998). In self-
training each MT system is re-trained using human
labeled data plus its own noisy translation output
on the unlabeled data. In co-training each MT sys-
tem is re-trained using human labeled data plus
noisy translation output from the other MT sys-
tems in the ensemble. We use consensus transla-
tions (He et al., 2008; Rosti et al., 2007; Matusov
et al., 2006) as an effective method for co-training
between multiple MT systems.
This paper makes the following contributions:
</bodyText>
<listItem confidence="0.94716575">
• We provide a new framework for multilingual
MT, in which we build multiple MT systems
and add a new language to an existing multi-
lingual parallel corpus. The multilingual set-
</listItem>
<page confidence="0.976454">
181
</page>
<note confidence="0.999609">
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 181–189,
Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999841666666667">
ting allows new features for active learning
which we exploit to improve translation qual-
ity while reducing annotation effort.
</bodyText>
<listItem confidence="0.986401214285714">
• We introduce new highly effective sentence
selection methods that improve phrase-based
SMT in the multilingual and single language
pair setting.
• We describe a novel co-training based active
learning framework that exploits consensus
translations to effectively select only those
sentences that are difficult to translate for all
MT systems, thus sharing annotation cost.
• We show that using active learning to add
a new language to the EuroParl corpus pro-
vides a significant improvement compared to
the strong random sentence selection base-
line.
</listItem>
<sectionHeader confidence="0.7021" genericHeader="introduction">
2 AL-SMT: Multilingual Setting
</sectionHeader>
<bodyText confidence="0.998618375">
Consider a multilingual parallel corpus, such as
EuroParl, which contains parallel sentences for
several languages. Our goal is to add a new lan-
guage to this corpus, and at the same time to con-
struct high quality MT systems from the existing
languages (in the multilingual corpus) to the new
language. This goal is formalized by the following
objective function:
</bodyText>
<equation confidence="0.993905666666667">
D
O = αd x TQ(MFd,E) (1)
d=1
</equation>
<bodyText confidence="0.999959434782609">
where Fd’s are the source languages in the mul-
tilingual corpus (D is the total number of lan-
guages), and E is the new language. The transla-
tion quality is measured by TQ for individual sys-
tems MFd_,E; it can be BLEU score or WER/PER
(Word error rate and position independent WER)
which induces a maximization or minimization
problem, respectively. The non-negative weights
αd reflect the importance of the different transla-
tion tasks and Ed αd = 1. AL-SMT formulation
for single language pair is a special case of this
formulation where only one of the αd’s in the ob-
jective function (1) is one and the rest are zero.
Moreover the algorithmic framework that we in-
troduce in Sec. 2.1 for AL in the multilingual set-
ting includes the single language pair setting as a
special case (Haffari et al., 2009).
We denote the large unlabeled multilingual cor-
pus by U:= {(f1j , .., fDj )}, and the small labeled
multilingual corpus by L := {(f1i , .., fDi , ei)}. We
overload the term entry to denote a tuple in L or
in U (it should be clear from the context). For a
single language pair we use U and L.
</bodyText>
<subsectionHeader confidence="0.992209">
2.1 The Algorithmic Framework
</subsectionHeader>
<bodyText confidence="0.999947191489362">
Algorithm 1 represents our AL approach for the
multilingual setting. We train our initial MT sys-
tems {MFd-,E}Dd=1 on the multilingual corpus L,
and use them to translate all monolingual sen-
tences in U. We denote sentences in U together
with their multiple translations by U+ (line 4 of
Algorithm 1). Then we retrain the SMT sys-
tems on L U U+ and use the resulting model to
decode the test set. Afterwards, we select and
remove a subset of highly informative sentences
from U, and add those sentences together with
their human-provided translations to L. This pro-
cess is continued iteratively until a certain level of
translation quality is met (we use the BLEU score,
WER and PER) (Papineni et al., 2002). In the
baseline, against which we compare our sentence
selection methods, the sentences are chosen ran-
domly.
When (re-)training the models, two phrase ta-
bles are learned for each SMT model: one from
the labeled data L and the other one from pseudo-
labeled data U+ (which we call the main and aux-
iliary phrase tables respectively). (Ueffing et al.,
2007; Haffari et al., 2009) show that treating U+
as a source for a new feature function in a log-
linear model for SMT (Och and Ney, 2004) allows
us to maximally take advantage of unlabeled data
by finding a weight for this feature using minimum
error-rate training (MERT) (Och, 2003).
Since each entry in U+ has multiple transla-
tions, there are two options when building the aux-
iliary table for a particular language pair (Fd, E):
(i) to use the corresponding translation ed of the
source language in a self-training setting, or (ii) to
use the consensus translation among all the trans-
lation candidates (e1, .., eD) in a co-training set-
ting (sharing information between multiple SMT
models).
A whole range of methods exist in the literature
for combining the output translations of multiple
MT systems for a single language pair, operating
either at the sentence, phrase, or word level (He et
al., 2008; Rosti et al., 2007; Matusov et al., 2006).
The method that we use in this work operates at
the sentence level, and picks a single high qual-
ity translation from the union of the n-best lists
generated by multiple SMT models. Sec. 5 gives
</bodyText>
<page confidence="0.996319">
182
</page>
<construct confidence="0.399261">
Algorithm 1 AL-SMT-Multiple
</construct>
<listItem confidence="0.977238583333333">
1: Given multilingual corpora L and U
2: {MFd,E}Dd=1 = multrain(L, 0)
3: for t = 1, 2,... do
4: U+ = multranslate(U, {MFd,E}Dd=1)
5: Select k sentences from U+, and ask a hu-
man for their true translations.
6: Remove the k sentences from U, and add
the k sentence pairs (translated by human)
to L
7: {MFd,E}Dd=1 = multrain(L, U+)
8: Monitor the performance on the test set
9: end for
</listItem>
<bodyText confidence="0.9981662">
more details about features which are used in our
consensus finding method, and how it is trained.
Now let us address the important question of se-
lecting highly informative sentences (step 5 in the
Algorithm 1) in the following section.
</bodyText>
<sectionHeader confidence="0.867424" genericHeader="method">
3 Sentence Selection: Multiple Language
Pairs
</sectionHeader>
<bodyText confidence="0.998335571428571">
The goal is to optimize the objective function
(1) with minimum human effort in providing the
translations. This motivates selecting sentences
which are maximally beneficial for all the MT sys-
tems. In this section, we present several protocols
for sentence selection based on the combined in-
formation from multiple language pairs.
</bodyText>
<subsectionHeader confidence="0.999829">
3.1 Alternating Selection
</subsectionHeader>
<bodyText confidence="0.999976666666667">
The simplest selection protocol is to choose k sen-
tences (entries) in the first iteration of AL which
improve maximally the first model MFi_,E, while
ignoring other models. In the second iteration, the
sentences are selected with respect to the second
model, and so on (Reichart et al., 2008).
</bodyText>
<subsectionHeader confidence="0.999726">
3.2 Combined Ranking
</subsectionHeader>
<bodyText confidence="0.999925375">
Pick any AL-SMT scoring method for a single lan-
guage pair (see Sec. 4). Using this method, we
rank the entries in unlabeled data U for each trans-
lation task defined by language pair (Fd, E). This
results in several ranking lists, each of which rep-
resents the importance of entries with respect to
a particular translation task. We combine these
rankings using a combined score:
</bodyText>
<equation confidence="0.955422666666667">
D
Score((f1, .., fD)) = αdRankd(fd)
d=1
</equation>
<bodyText confidence="0.985006">
Rankd(.) is the ranking of a sentence in the list for
the dth translation task (Reichart et al., 2008).
</bodyText>
<subsectionHeader confidence="0.995218">
3.3 Disagreement Among the Translations
</subsectionHeader>
<bodyText confidence="0.99985225">
Disagreement among the candidate translations of
a particular entry is evidence for the difficulty of
that entry for different translation models. The
reason is that disagreement increases the possibil-
ity that most of the translations are not correct.
Therefore it would be beneficial to ask human for
the translation of these hard entries.
Now the question is how to quantify the no-
tion of disagreement among the candidate trans-
lations (e1,.., eD). We propose two measures of
disagreement which are related to the portion of
shared n-grams (n &lt; 4) among the translations:
</bodyText>
<listItem confidence="0.996055333333333">
• Let ec be the consensus among all the can-
didate translations, then define the disagree-
ment as Ed αd (1 − BLEU(ec, ed)).
• Based on the disagreement of every pair
of candidate translations: Ed αd Ed, (1 −
BLEU(edI, ed)).
</listItem>
<bodyText confidence="0.999869666666667">
For the single language pair setting, (Haffari et
al., 2009) presents and compares several sentence
selection methods for statistical phrase-based ma-
chine translation. We introduce novel techniques
which outperform those methods in the next sec-
tion.
</bodyText>
<sectionHeader confidence="0.8747735" genericHeader="method">
4 Sentence Selection: Single Language
Pair
</sectionHeader>
<bodyText confidence="0.999955863636364">
Phrases are basic units of translation in phrase-
based SMT models. The phrases which may po-
tentially be extracted from a sentence indicate its
informativeness. The more new phrases a sen-
tence can offer, the more informative it is; since it
boosts the generalization of the model. Addition-
ally phrase translation probabilities need to be es-
timated accurately, which means sentences that of-
fer phrases whose occurrences in the corpus were
rare are informative. When selecting new sen-
tences for human translation, we need to pay atten-
tion to this tradeoff between exploration and ex-
ploitation, i.e. selecting sentences to discover new
phrases v.s. estimating accurately the phrase trans-
lation probabilities. Smoothing techniques partly
handle accurate estimation of translation probabil-
ities when the events occur rarely (indeed it is the
main reason for smoothing). So we mainly focus
on how to expand effectively the lexicon or set of
phrases of the model.
The more frequent a phrase (not a phrase pair)
is in the unlabeled data, the more important it is to
</bodyText>
<page confidence="0.99554">
183
</page>
<bodyText confidence="0.9767294">
know its translation; since it is more likely to see
it in test data (specially when the test data is in-
domain with respect to unlabeled data). The more
frequent a phrase is in the labeled data, the more
unimportant it is; since probably we have observed
most of its translations.
In the labeled data L, phrases are the ones which
are extracted by the SMT models; but what are
the candidate phrases in the unlabeled data U?
We use the currently trained SMT models to an-
swer this question. Each translation in the n-best
list of translations (generated by the SMT mod-
els) corresponds to a particular segmentation of
a sentence, which breaks that sentence into sev-
eral fragments (see Fig. 1). Some of these frag-
ments are the source language part of a phrase pair
available in the phrase table, which we call regular
phrases and denote their set by Xreg
s for a sentence
s. However, there are some fragments in the sen-
tence which are not covered by the phrase table –
possibly because of the OOVs (out-of-vocabulary
words) or the constraints imposed by the phrase
extraction algorithm – called Xoov sfor a sentence
s. Each member of Xoov soffers a set of potential
phrases (also referred to as OOV phrases) which
are not observed due to the latent segmentation of
this fragment. We present two generative models
for the phrases and show how to estimate and use
them for sentence selection.
</bodyText>
<subsectionHeader confidence="0.997838">
4.1 Model 1
</subsectionHeader>
<bodyText confidence="0.999984222222222">
In the first model, the generative story is to gen-
erate phrases for each sentence based on indepen-
dent draws from a multinomial. The sample space
of the multinomial consists of both regular and
OOV phrases.
We build two models, i.e. two multinomials,
one for labeled data and the other one for unla-
beled data. Each model is trained by maximizing
the log-likelihood of its corresponding data:
</bodyText>
<equation confidence="0.9299095">
�LD �= P�(s) � log P(x|θD) (2)
sED xEXs
</equation>
<bodyText confidence="0.999893333333333">
where D is either L or U, P(s) is the empiri-
cal distribution of the sentences1, and θD is the
parameter vector of the corresponding probability
</bodyText>
<equation confidence="0.886438083333333">
1 P�(s) is the number of times that the sentence s is seen
in D divided by the number of all sentences in D.
distribution. When x E Xoov
s , we will have
P(x|θU) = � P(x, h|θU)
hEHx
�= P(h)P(x|h, θU)
hEHx
1 � �
 |θU(y) (3)
|
Hx hEHx yEYx�
</equation>
<bodyText confidence="0.999003294117647">
where Hx is the space of all possible segmenta-
tions for the OOV fragment x, Yxh is the result-
ing phrases from x based on the segmentation h,
and θU(y) is the probability of the OOV phrase
y in the multinomial associated with U. We let
Hx to be all possible segmentations of the frag-
ment x for which the resulting phrase lengths are
not greater than the maximum length constraint for
phrase extraction in the underlying SMT model.
Since we do not know anything about the segmen-
tations a priori, we have put a uniform distribution
over such segmentations.
Maximizing (2) to find the maximum likelihood
parameters for this model is an extremely diffi-
cult problem2. Therefore, we maximize the fol-
lowing lower-bound on the log-likelihood which
is derived using Jensen’s inequality:
</bodyText>
<table confidence="0.9969485">
� � � log θD(x)
LD &gt; P� (s)
sED xEX�e�
s
� � 1 � �log θD(y) (4)
+ hEHx |Hx |yEYx�
xEXoov
s
</table>
<bodyText confidence="0.976359333333333">
Maximizing (4) amounts to set the probability of
each regular / potential phrase proportional to its
count / expected count in the data D.
Let Pk(xi:j) be the number of possible segmen-
tations from position i to position j of an OOV
fragment x, and k is the maximum phrase length;
</bodyText>
<equation confidence="0.9988015">
E Pk(x1:|x|) = { 0, if |x |= 0
kx ) otherwise
1, if |x |= 1
i=1 Pk( i+1:�x|
</equation>
<bodyText confidence="0.9808322">
which gives us a dynamic programming algorithm
to compute the number of segmentation |Hx |=
Pk(x1:|x|) of the OOV fragment x. The expected
count of a potential phrase y based on an OOV
segment x is (see Fig. 1.c):
</bodyText>
<equation confidence="0.990457">
E[ |x] = Ei&lt;j s[y=xz:j]Pk(x1:i−1)Pk(xj+1:|x|)
y
Pk(x)
</equation>
<footnote confidence="0.594072333333333">
2Setting partial derivatives of the Lagrangian to zero
amounts to finding the roots of a system of multivariate poly-
nomials (a major topic in Algebraic Geometry).
</footnote>
<page confidence="0.9894">
184
</page>
<figure confidence="0.999097764705882">
Regular Phrases
source target prob
on
go to school
i will
OOV segment
potential phr.
count
go 2/3
to 1/3
friday school 2/3
go to 1/3
to school 1/3
i will XXX .01
in friday XXX .004
... ... ...
(a) (b) (c)
</figure>
<figureCaption confidence="0.980299">
Figure 1: The given sentence in (b) is segmented, based on the source side phrases extracted from the phrase table in (a), to
yield regular phrases and OOV segment. The table in (c) shows the potential phrases extracted from the OOV segment “go to
school” and their expected counts (denoted by count) where the maximum length for the potential phrases is set to 2. In the
example, “go to school” has 3 segmentations with maximum phrase length 2: (go)(to school), (go to)(school), (go)(to)(school).
</figureCaption>
<bodyText confidence="0.975018571428571">
+
where S[C� is 1 if the condition C is true, and zero
otherwise. We have used the fact that the num-
ber of occurrences of a phrase spanning the indices
[i, j] is the product of the number of segmentations
of the left and the right sub-fragments, which are
Pk(x1:i_1) and Pk(xj+1:|x|) respectively.
</bodyText>
<subsectionHeader confidence="0.847924">
4.2 Model 2
</subsectionHeader>
<bodyText confidence="0.999947285714286">
In the second model, we consider a mixture model
of two multinomials responsible for generating
phrases in each of the labeled and unlabeled data
sets. To generate a phrase, we first toss a coin and
depending on the outcome we either generate the
phrase from the multinomial associated with regu-
lar phrases θreg
</bodyText>
<equation confidence="0.935134">
U or potential phrases θoov
U :
P(x|θU) := QUθreg
U (x) + (1 − �U)θoov
U (x)
</equation>
<bodyText confidence="0.999900166666667">
where θU includes the mixing weight Q and the
parameter vectors of the two multinomials. The
mixture model associated with L is written simi-
larly. The parameter estimation is based on maxi-
mizing a lower-bound on the log-likelihood which
is similar to what was done for the Model 1.
</bodyText>
<subsectionHeader confidence="0.998979">
4.3 Sentence Scoring
</subsectionHeader>
<bodyText confidence="0.999755666666667">
The sentence score is a linear combination of two
terms: one coming from regular phrases and the
other from OOV phrases:
</bodyText>
<equation confidence="0.9773288">
� X01(s) := |Xreg
s
|
xEXreg
s
</equation>
<bodyText confidence="0.999211375">
where we use either Model 1 or Model 2 for
P(.|θD). The first term is the log probability ra-
tio of regular phrases under phrase models corre-
sponding to unlabeled and labeled data, and the
second term is the expected log probability ratio
(ELPR) under the two models. Another option for
the contribution of OOV phrases is to take log of
expected probability ratio (LEPR):
</bodyText>
<equation confidence="0.994493">
� X
02(s) := |Xreg
s |
xEXreg
s
1 − A X
|Xoov
s |xEXsoov
</equation>
<bodyText confidence="0.999670666666667">
It is not difficult to prove that there is no difference
between Model 1 and Model 2 when ELPR scor-
ing is used for sentence selection. However, the
situation is different for LEPR scoring: the two
models produce different sentence rankings in this
case.
</bodyText>
<sectionHeader confidence="0.999269" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.969456791666667">
Corpora. We pre-processed the EuroParl corpus
(http://www.statmt.org/europarl) (Koehn, 2005)
and built a multilingual parallel corpus with
653,513 sentences, excluding the Q4/2000 por-
tion of the data (2000-10 to 2000-12) which is
reserved as the test set. We subsampled 5,000
sentences as the labeled data L and 20,000 sen-
tences as N for the pool of untranslated sentences
(while hiding the English part). The test set con-
sists of 2,000 multi-language sentences and comes
from the multilingual parallel corpus built from
Q4/2000 portion of the data.
Consensus Finding. Let T be the union of the n-
best lists of translations for a particular sentence.
The consensus translation tc is
arg max w1 LM(t) +w2 Qd(t) +w3Rd(t)+w4,d
tET |t ||t|
where LM(t) is the score from a 3-gram language
model, Qd(t) is the translation score generated by
the decoder for MFd,E if t is produced by the
dth SMT model, Rd(t) is the rank of the transla-
tion in the n-best list produced by the dth model,
w4,d is a bias term for each translation model to
make their scores comparable, and |t |is the length
</bodyText>
<equation confidence="0.528383416666667">
P(x|θU)
log
P(x|θL)
+ 1 − A X X 1 Y P(y|θU)
|Xoov hEHx |Hx |log yEYxh P(y|θL)
s |xEXsoov
P(x|θU)
log
P(x|θL)
X 1 Y P(y|θU)
log |Hx |yEYxh P(y|θL)
hEHx
</equation>
<page confidence="0.79198">
185
</page>
<note confidence="0.421768">
French to English Spanish to English German to English
</note>
<figure confidence="0.999274826923077">
BLEU Score
23.6
23.5
23.4
23.3
23.2
23.1
22.9
22.8
22.7
22.6
1000 2000
23
3000 4000 5000
Model 2 − LEPR
Model 1 − ELPR
Geom Phrase
Random
BLEU Score
24.8
24.6
24.4
24.2
23.8
23.6
23.4
23.2
1000 2000
25
24
3000 4000 5000
Model 2 − LEPR
Model 1 − ELPR
Geom Phrase
Random
BLEU Score
17.8
17.6
17.4
17.2
16.8
16.6
16.4
16.2
1000 2000 3000
17
Model 2 − LEPR
Model 1 − ELPR
Geom Phrase
Random
4000 5000
Added Sentences Added Sentences Added Sentences
</figure>
<figureCaption confidence="0.999772333333333">
Figure 2: The performance of different sentence selection strategies as the iteration of AL loop goes on for three translation
tasks. Plots show the performance of sentence selection methods for single language pair in Sec. 4 compared to the GeomPhrase
(Haffari et al., 2009) and random sentence selection baseline.
</figureCaption>
<bodyText confidence="0.981676583333333">
of the translation sentence. The number of weights
wi is 3 plus the number of source languages, and
they are trained using minimum error-rate training
(MERT) to maximize the BLEU score (Och, 2003)
on a development set.
Parameters. We use add-c smoothing where c =
.5 to smooth the probabilities in Sec. 4; moreover
A = .4 for ELPR and LEPR sentence scoring and
maximum phrase length k is set to 4. For the mul-
tilingual experiments (which involve four source
languages) we set αd = .25 to make the impor-
tance of individual translation tasks equal.
</bodyText>
<figure confidence="0.693055">
Mulilingual da−de−nl−sv to en
Added Sentences
</figure>
<figureCaption confidence="0.9941145">
Figure 3: Random sentence selection baseline using self-
training and co-training (Germanic languages to English).
</figureCaption>
<subsectionHeader confidence="0.629187">
5.1 Results
</subsectionHeader>
<bodyText confidence="0.9941861">
First we evaluate the proposed sentence selection
methods in Sec. 4 for the single language pair.
Then the best method from the single language
pair setting is used to evaluate sentence selection
methods for AL in multilingual setting. After
building the initial MT system for each experi-
ment, we select and remove 500 sentences from
U and add them together with translations to L for
10 total iterations. The random sentence selection
baselines are averaged over 3 independent runs.
</bodyText>
<table confidence="0.983143">
mode self-train co-train
Method wer per wer per
Combined Rank 40.2 30.0 40.0 29.6
Alternate 41.0 30.2 40.1 30.1
Disagree-Pairwise 41.9 32.0 40.5 30.9
Disagree-Center 41.8 31.8 40.6 30.7
Random Baseline 41.6 31.0 40.5 30.7
Germanic languages to English
mode self-train co-train
Method wer per wer per
Combined Rank 37.7 27.3 37.3 27.0
Alternate 37.7 27.3 37.3 27.0
Random Baseline 38.6 28.1 38.1 27.6
</table>
<tableCaption confidence="0.789218">
Romance languages to English
Table 1: Comparison of multilingual selection methods with
</tableCaption>
<bodyText confidence="0.969648045454546">
WER (word error rate), PER (position independent WER).
95% confidence interval for WER numbers is 0.7 and for PER
numbers is 0.5. Bold: best result, italic: significantly better.
We use three language pairs in our single lan-
guage pair experiments: French-English, German-
English, and Spanish- English. In addition to ran-
dom sentence selection baseline, we also compare
the methods proposed in this paper to the best
method reported in (Haffari et al., 2009) denoted
by GeomPhrase, which differs from our models
since it considers each individual OOV segment as
a single OOV phrase and does not consider subse-
quences. The results are presented in Fig. 2. Se-
lecting sentences based on our proposed methods
outperform the random sentence selection baseline
and GeomPhrase. We suspect for the situations
where L is out-of-domain and the average phrase
length is relatively small, our method will outper-
form GeomPhrase even more.
For the multilingual experiments, we use Ger-
manic (German, Dutch, Danish, Swedish) and Ro-
mance (French, Spanish, Italian, Portuguese3) lan-
</bodyText>
<figure confidence="0.975994902439024">
3A reviewer pointed out that EuroParl English-Portuguese
20.5
SelFTraining
Co−Training
20
1000 2000 3000 4000 5000
18.5
180
19.5
19
Avg BLEU Score
186
Avg BLEU Score
Self−Train Mulilingual da−de−nl−sv to en Co−Train Mulilingual da−de−nl−sv to en
20.3
20.2
20.1
Avg BLEU Score
20
19.9
19.8
19.7
19.6
19.5
19.4
18.2
Alternate
CombineRank
Disagree−Pairwise
Disagree−Center
Random
Alternate
CombineRank
Disagree−Pairwise
Disagree−Center
Random
2000 3000 4000 5000
19.3
1000 1500 2000 2500 3000 3500 4000 4500 5000
0 1000
20
19.8
19.6
19.4
19.2
19
18.8
18.6
18.4
Added Sentences
Added Sentences
Avg BLEU Score
Added Sentences Added Sentences
23.6
23.4
23.2
23.6
23.8
Avg BLEU Score
23.4
23.2
23
Alternate
CombineRank
Random
Alternate
CombineRank
Random
22.8
21.60
1000 2000 3000 4000 5000
22.6
1000 1500 2000 2500 3000 3500 4000 4500 5000
23
22.8
22.6
22.4
22.2
22
21.8
Self−Train Mulilingual fr−es−it−pt to en
Co−Train Mulilingual fr−es−it−pt to en
</figure>
<figureCaption confidence="0.99846575">
Figure 4: The left/right plot show the performance of our AL methods for multilingual setting combined with self-training/co-
training. The sentence selection methods from Sec. 3 are compared with random sentence selection baseline. The top plots cor-
respond to Danish-German-Dutch-Swedish to English, and the bottom plots correspond to French-Spanish-Italian-Portuguese
to English.
</figureCaption>
<bodyText confidence="0.999942181818182">
guages as the source and English as the target lan-
guage as two sets of experiments.4 Fig. 3 shows
the performance of random sentence selection for
AL combined with self-training/co-training for the
multi-source translation from the four Germanic
languages to English. It shows that the co-training
mode outperforms the self-training mode by al-
most 1 BLEU point. The results of selection
strategies in the multilingual setting are presented
in Fig. 4 and Tbl. 1. Having noticed that Model
1 with ELPR performs well in the single language
pair setting, we use it to rank entries for individual
translation tasks. Then these rankings are used by
‘Alternate’ and ‘Combined Rank’ selection strate-
gies in the multilingual case. The ‘Combined
Rank’ method outperforms all the other methods
including the strong random selection baseline in
both self-training and co-training modes. The
disagreement-based selection methods underper-
form the baseline for translation of Germanic lan-
guages to English, so we omitted them for the Ro-
mance language experiments.
</bodyText>
<subsectionHeader confidence="0.995503">
5.2 Analysis
</subsectionHeader>
<bodyText confidence="0.9995735">
The basis for our proposed methods has been the
popularity of regular/OOV phrases in U and their
</bodyText>
<footnote confidence="0.774842">
data is very noisy and future work should omit this pair.
4Choice of Germanic and Romance for our experimental
setting is inspired by results in (Cohn and Lapata, 2007)
</footnote>
<bodyText confidence="0.94027195">
unpopularity in L, which is measured by p�X|�U
|e� .
We need P(x|9U), the estimated distribution of
phrases in U, to be as similar as possible to P*(x),
the true distribution of phrases in U. We investi-
gate this issue for regular/OOV phrases as follows:
• Using the output of the initially trained MT sys-
tem on L, we extract the regular/OOV phrases as
described in §4. The smoothed relative frequen-
cies give us the regular/OOV phrasal distributions.
• Using the true English translation of the sen-
tences in U, we extract the true phrases. Separat-
ing the phrases into two sets of regular and OOV
phrases defined by the previous step, we use the
smoothed relative frequencies and form the true
OOV/regular phrasal distributions.
We use the KL-divergence to see how dissim-
ilar are a pair of given probability distributions.
As Tbl. 2 shows, the KL-divergence between the
true and estimated distributions are less than that
</bodyText>
<table confidence="0.9982056">
De2En Fr2En Es2En
KL(P∗reg 11 Preg) 4.37 4.17 4.38
KL(P∗reg 11 unif) 5.37 5.21 5.80
KL(P∗oov 11 Poov) 3.04 4.58 4.73
KL(P∗oov 11 unif) 3.41 4.75 4.99
</table>
<tableCaption confidence="0.939498666666667">
Table 2: For regular/OOV phrases, the KL-divergence be-
tween the true distribution (P∗) and the estimated (P) or uni-
form (unif ) distributions are shown, where: W
</tableCaption>
<equation confidence="0.2522695">
KL(P∗ 11 P) :_ �x P∗(x) log P
.
</equation>
<page confidence="0.901939">
187
</page>
<figure confidence="0.988309666666667">
Rank
OOV Phrases in U
Rank
</figure>
<figureCaption confidence="0.999558">
Figure 5: The log-log Zipf plots representing the true and
</figureCaption>
<bodyText confidence="0.973034764705883">
estimated probabilities of a (source) phrase vs the rank of
that phrase in the German to English translation task. The
plots for the Spanish to English and French to English tasks
are also similar to the above plots, and confirm a power law
behavior in the true phrasal distributions.
between the true and uniform distributions, in all
three language pairs. Since uniform distribution
conveys no information, this is evidence that there
is some information encoded in the estimated dis-
tribution about the true distribution. However
we noticed that the true distributions of regu-
lar/OOV phrases exhibit Zipfian (power law) be-
havior5 which is not well captured by the esti-
mated distributions (see Fig. 5). Enhancing the es-
timated distributions to capture this power law be-
havior would improve the quality of the proposed
sentence selection methods.
</bodyText>
<sectionHeader confidence="0.99997" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.97775328">
(Haffari et al., 2009) provides results for active
learning for MT using a single language pair. Our
work generalizes to the use of multilingual corpora
using new methods that are not possible with a sin-
gle language pair. In this paper, we also introduce
new selection methods that outperform the meth-
ods in (Haffari et al., 2009) even for MT with a
single language pair. In addition in this paper by
considering multilingual parallel corpora we were
able to introduce co-training for AL, while (Haf-
fari et al., 2009) only use self-training since they
are using a single language pair.
5This observation is at the phrase level and not at the word
(Zipf, 1932) or even n-gram level (Ha et al., 2002).
(Reichart et al., 2008) introduces multi-task ac-
tive learning where unlabeled data require annota-
tions for multiple tasks, e.g. they consider named-
entities and parse trees, and showed that multi-
ple tasks helps selection compared to individual
tasks. Our setting is different in that the target lan-
guage is the same across multiple MT tasks, which
we exploit to use consensus translations and co-
training to improve active learning performance.
(Callison-Burch and Osborne, 2003b; Callison-
Burch and Osborne, 2003a) provide a co-training
approach to MT, where one language pair creates
data for another language pair. In contrast, our
co-training approach uses consensus translations
and our setting for active learning is very differ-
ent from their semi-supervised setting. A Ph.D.
proposal by Chris Callison-Burch (Callison-burch,
2003) lays out the promise of AL for SMT and
proposes some algorithms. However, the lack of
experimental results means that performance and
feasibility of those methods cannot be compared
to ours.
While we use consensus translations (He et al.,
2008; Rosti et al., 2007; Matusov et al., 2006)
as an effective method for co-training in this pa-
per, unlike consensus for system combination, the
source languages for each of our MT systems are
different, which rules out a set of popular methods
for obtaining consensus translations which assume
translation for a single language pair. Finally, we
briefly note that triangulation (see (Cohn and Lap-
ata, 2007)) is orthogonal to the use of co-training
in our work, since it only enhances each MT sys-
tem in our ensemble by exploiting the multilingual
data. In future work, we plan to incorporate trian-
gulation into our active learning approach.
</bodyText>
<sectionHeader confidence="0.998918" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.998406785714286">
This paper introduced the novel active learning
task of adding a new language to an existing multi-
lingual set of parallel text. We construct SMT sys-
tems from each language in the collection into the
new target language. We show that we can take ad-
vantage of multilingual corpora to decrease anno-
tation effort thanks to the highly effective sentence
selection methods we devised for active learning
in the single language-pair setting which we then
applied to the multilingual sentence selection pro-
tocols. In the multilingual setting, a novel co-
training method for active learning in SMT is pro-
posed using consensus translations which outper-
forms AL-SMT with self-training.
</bodyText>
<figure confidence="0.999101869565217">
Regular Phrases in U
Estimated Distribution
True Distribution
100 101 102 103 104 105
100
10−1
10−2
10−3
10−4
10−5
10−6
Probability
100
10−1
10−2
10−3
10−4
10−5
10−6
100 101 102 103 104 105
Estimated Distribution
True Distribution
Probability
</figure>
<page confidence="0.982789">
188
</page>
<sectionHeader confidence="0.991929" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999889344262295">
Avrim Blum and Tom Mitchell. 1998. Combin-
ing Labeled and Unlabeled Data with Co-Training.
In Proceedings of the Eleventh Annual Conference
on Computational Learning Theory (COLT 1998),
Madison, Wisconsin, USA, July 24-26. ACM.
Chris Callison-Burch and Miles Osborne. 2003a.
Bootstrapping parallel corpora. In NAACL work-
shop: Building and Using Parallel Texts: Data
Driven Machine Translation and Beyond.
Chris Callison-Burch and Miles Osborne. 2003b. Co-
training for statistical machine translation. In Pro-
ceedings of the 6th Annual CLUK Research Collo-
quium.
Chris Callison-burch. 2003. Active learning for statis-
tical machine translation. In PhD Proposal, Edin-
burgh University.
Trevor Cohn and Mirella Lapata. 2007. Machine
translation by triangulation: Making effective use of
multi-parallel corpora. In ACL.
Le Quan Ha, E. I. Sicilia-Garcia, Ji Ming, and F.J.
Smith. 2002. Extension of zipf’s law to words and
phrases. In Proceedings of the 19th international
conference on Computational linguistics.
Gholamreza Haffari, Maxim Roy, and Anoop Sarkar.
2009. Active learning for statistical phrase-based
machine translation. In NAACL.
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick
Nguyen, and Robert Moore. 2008. Indirect-hmm-
based hypothesis alignment for combining outputs
from machine translation systems. In EMNLP.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing consensus translation from multi-
ple machine translation systems using enhanced hy-
potheses alignment. In EACL.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4):417–449.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In ACL ’03: Pro-
ceedings of the 41st Annual Meeting on Association
for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei
jing Zhu. 2002. Bleu: A method for automatic eval-
uation of machine translation. In ACL ’02: Proceed-
ings of the 41st Annual Meeting on Association for
Computational Linguistics.
Roi Reichart, Katrin Tomanek, Udo Hahn, and Ari
Rappoport. 2008. Multi-task active learning for lin-
guistic annotations. In ACL.
Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang,
Spyros Matsoukas, Richard M. Schwartz, and Bon-
nie Jean Dorr. 2007. Combining outputs from mul-
tiple machine translation systems. In NAACL.
Nicola Ueffing, Gholamreza Haffari, and Anoop
Sarkar. 2007. Transductive learning for statistical
machine translation. In ACL.
George Zipf. 1932. Selective Studies and the Principle
of Relative Frequency in Language. Harvard Uni-
versity Press.
</reference>
<page confidence="0.998923">
189
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.992297">
<title confidence="0.99992">Learning for Multilingual Statistical Machine</title>
<author confidence="0.999646">Haffari Sarkar</author>
<affiliation confidence="0.999996">School of Computing Science, Simon Fraser University</affiliation>
<address confidence="0.996814">British Columbia, Canada</address>
<abstract confidence="0.999772368421053">Statistical machine translation (SMT) models require bilingual corpora for training, and these corpora are often multilingual with parallel text in multiple languages simultaneously. We introduce an learning of adding a new language to an existing multilingual set of parallel text and constructing high quality MT systems, from each language in the collection into this new target language. We show that adding a new language using active learning to the EuroParl corpus provides a significant improvement compared to a random sentence selection baseline. We also provide new highly effective sentence selection methods that improve AL for phrase-based SMT in the multilingual and single language pair setting.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Tom Mitchell</author>
</authors>
<title>Combining Labeled and Unlabeled Data with Co-Training.</title>
<date>1998</date>
<booktitle>In Proceedings of the Eleventh Annual Conference on Computational Learning Theory (COLT 1998),</booktitle>
<publisher>ACM.</publisher>
<location>Madison, Wisconsin, USA,</location>
<contexts>
<context position="3709" citStr="Blum and Mitchell, 1998" startWordPosition="587" endWordPosition="590">ta to be effective. To deal with this, we use a novel framework for active learning: we assume we are given a small amount of parallel text and a large amount of monolingual source language text; using these resources, we create a large noisy parallel text which we then iteratively improve using small injections of human translations. When we build multiple MT systems from multiple source languages to the new target language, each MT system can be seen as a different ‘view’ on the desired output translation. Thus, we can train our multiple MT systems using either self-training or co-training (Blum and Mitchell, 1998). In selftraining each MT system is re-trained using human labeled data plus its own noisy translation output on the unlabeled data. In co-training each MT system is re-trained using human labeled data plus noisy translation output from the other MT systems in the ensemble. We use consensus translations (He et al., 2008; Rosti et al., 2007; Matusov et al., 2006) as an effective method for co-training between multiple MT systems. This paper makes the following contributions: • We provide a new framework for multilingual MT, in which we build multiple MT systems and add a new language to an exis</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Avrim Blum and Tom Mitchell. 1998. Combining Labeled and Unlabeled Data with Co-Training. In Proceedings of the Eleventh Annual Conference on Computational Learning Theory (COLT 1998), Madison, Wisconsin, USA, July 24-26. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
</authors>
<title>Bootstrapping parallel corpora.</title>
<date>2003</date>
<booktitle>In NAACL workshop: Building and Using Parallel Texts: Data Driven Machine Translation and Beyond.</booktitle>
<contexts>
<context position="30295" citStr="Callison-Burch and Osborne, 2003" startWordPosition="5123" endWordPosition="5126">ce they are using a single language pair. 5This observation is at the phrase level and not at the word (Zipf, 1932) or even n-gram level (Ha et al., 2002). (Reichart et al., 2008) introduces multi-task active learning where unlabeled data require annotations for multiple tasks, e.g. they consider namedentities and parse trees, and showed that multiple tasks helps selection compared to individual tasks. Our setting is different in that the target language is the same across multiple MT tasks, which we exploit to use consensus translations and cotraining to improve active learning performance. (Callison-Burch and Osborne, 2003b; CallisonBurch and Osborne, 2003a) provide a co-training approach to MT, where one language pair creates data for another language pair. In contrast, our co-training approach uses consensus translations and our setting for active learning is very different from their semi-supervised setting. A Ph.D. proposal by Chris Callison-Burch (Callison-burch, 2003) lays out the promise of AL for SMT and proposes some algorithms. However, the lack of experimental results means that performance and feasibility of those methods cannot be compared to ours. While we use consensus translations (He et al., 20</context>
</contexts>
<marker>Callison-Burch, Osborne, 2003</marker>
<rawString>Chris Callison-Burch and Miles Osborne. 2003a. Bootstrapping parallel corpora. In NAACL workshop: Building and Using Parallel Texts: Data Driven Machine Translation and Beyond.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
</authors>
<title>Cotraining for statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 6th Annual CLUK Research Colloquium.</booktitle>
<contexts>
<context position="30295" citStr="Callison-Burch and Osborne, 2003" startWordPosition="5123" endWordPosition="5126">ce they are using a single language pair. 5This observation is at the phrase level and not at the word (Zipf, 1932) or even n-gram level (Ha et al., 2002). (Reichart et al., 2008) introduces multi-task active learning where unlabeled data require annotations for multiple tasks, e.g. they consider namedentities and parse trees, and showed that multiple tasks helps selection compared to individual tasks. Our setting is different in that the target language is the same across multiple MT tasks, which we exploit to use consensus translations and cotraining to improve active learning performance. (Callison-Burch and Osborne, 2003b; CallisonBurch and Osborne, 2003a) provide a co-training approach to MT, where one language pair creates data for another language pair. In contrast, our co-training approach uses consensus translations and our setting for active learning is very different from their semi-supervised setting. A Ph.D. proposal by Chris Callison-Burch (Callison-burch, 2003) lays out the promise of AL for SMT and proposes some algorithms. However, the lack of experimental results means that performance and feasibility of those methods cannot be compared to ours. While we use consensus translations (He et al., 20</context>
</contexts>
<marker>Callison-Burch, Osborne, 2003</marker>
<rawString>Chris Callison-Burch and Miles Osborne. 2003b. Cotraining for statistical machine translation. In Proceedings of the 6th Annual CLUK Research Colloquium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-burch</author>
</authors>
<title>Active learning for statistical machine translation. In</title>
<date>2003</date>
<tech>PhD Proposal,</tech>
<institution>Edinburgh University.</institution>
<contexts>
<context position="30653" citStr="Callison-burch, 2003" startWordPosition="5176" endWordPosition="5177">ection compared to individual tasks. Our setting is different in that the target language is the same across multiple MT tasks, which we exploit to use consensus translations and cotraining to improve active learning performance. (Callison-Burch and Osborne, 2003b; CallisonBurch and Osborne, 2003a) provide a co-training approach to MT, where one language pair creates data for another language pair. In contrast, our co-training approach uses consensus translations and our setting for active learning is very different from their semi-supervised setting. A Ph.D. proposal by Chris Callison-Burch (Callison-burch, 2003) lays out the promise of AL for SMT and proposes some algorithms. However, the lack of experimental results means that performance and feasibility of those methods cannot be compared to ours. While we use consensus translations (He et al., 2008; Rosti et al., 2007; Matusov et al., 2006) as an effective method for co-training in this paper, unlike consensus for system combination, the source languages for each of our MT systems are different, which rules out a set of popular methods for obtaining consensus translations which assume translation for a single language pair. Finally, we briefly not</context>
</contexts>
<marker>Callison-burch, 2003</marker>
<rawString>Chris Callison-burch. 2003. Active learning for statistical machine translation. In PhD Proposal, Edinburgh University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Mirella Lapata</author>
</authors>
<title>Machine translation by triangulation: Making effective use of multi-parallel corpora.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="26902" citStr="Cohn and Lapata, 2007" startWordPosition="4551" endWordPosition="4554">ltilingual case. The ‘Combined Rank’ method outperforms all the other methods including the strong random selection baseline in both self-training and co-training modes. The disagreement-based selection methods underperform the baseline for translation of Germanic languages to English, so we omitted them for the Romance language experiments. 5.2 Analysis The basis for our proposed methods has been the popularity of regular/OOV phrases in U and their data is very noisy and future work should omit this pair. 4Choice of Germanic and Romance for our experimental setting is inspired by results in (Cohn and Lapata, 2007) unpopularity in L, which is measured by p�X|�U |e� . We need P(x|9U), the estimated distribution of phrases in U, to be as similar as possible to P*(x), the true distribution of phrases in U. We investigate this issue for regular/OOV phrases as follows: • Using the output of the initially trained MT system on L, we extract the regular/OOV phrases as described in §4. The smoothed relative frequencies give us the regular/OOV phrasal distributions. • Using the true English translation of the sentences in U, we extract the true phrases. Separating the phrases into two sets of regular and OOV phra</context>
<context position="31302" citStr="Cohn and Lapata, 2007" startWordPosition="5279" endWordPosition="5283"> for SMT and proposes some algorithms. However, the lack of experimental results means that performance and feasibility of those methods cannot be compared to ours. While we use consensus translations (He et al., 2008; Rosti et al., 2007; Matusov et al., 2006) as an effective method for co-training in this paper, unlike consensus for system combination, the source languages for each of our MT systems are different, which rules out a set of popular methods for obtaining consensus translations which assume translation for a single language pair. Finally, we briefly note that triangulation (see (Cohn and Lapata, 2007)) is orthogonal to the use of co-training in our work, since it only enhances each MT system in our ensemble by exploiting the multilingual data. In future work, we plan to incorporate triangulation into our active learning approach. 7 Conclusion This paper introduced the novel active learning task of adding a new language to an existing multilingual set of parallel text. We construct SMT systems from each language in the collection into the new target language. We show that we can take advantage of multilingual corpora to decrease annotation effort thanks to the highly effective sentence sele</context>
</contexts>
<marker>Cohn, Lapata, 2007</marker>
<rawString>Trevor Cohn and Mirella Lapata. 2007. Machine translation by triangulation: Making effective use of multi-parallel corpora. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Le Quan Ha</author>
<author>E I Sicilia-Garcia</author>
<author>Ji Ming</author>
<author>F J Smith</author>
</authors>
<title>Extension of zipf’s law to words and phrases.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th international conference on Computational linguistics.</booktitle>
<contexts>
<context position="29817" citStr="Ha et al., 2002" startWordPosition="5049" endWordPosition="5052">language pair. Our work generalizes to the use of multilingual corpora using new methods that are not possible with a single language pair. In this paper, we also introduce new selection methods that outperform the methods in (Haffari et al., 2009) even for MT with a single language pair. In addition in this paper by considering multilingual parallel corpora we were able to introduce co-training for AL, while (Haffari et al., 2009) only use self-training since they are using a single language pair. 5This observation is at the phrase level and not at the word (Zipf, 1932) or even n-gram level (Ha et al., 2002). (Reichart et al., 2008) introduces multi-task active learning where unlabeled data require annotations for multiple tasks, e.g. they consider namedentities and parse trees, and showed that multiple tasks helps selection compared to individual tasks. Our setting is different in that the target language is the same across multiple MT tasks, which we exploit to use consensus translations and cotraining to improve active learning performance. (Callison-Burch and Osborne, 2003b; CallisonBurch and Osborne, 2003a) provide a co-training approach to MT, where one language pair creates data for anothe</context>
</contexts>
<marker>Ha, Sicilia-Garcia, Ming, Smith, 2002</marker>
<rawString>Le Quan Ha, E. I. Sicilia-Garcia, Ji Ming, and F.J. Smith. 2002. Extension of zipf’s law to words and phrases. In Proceedings of the 19th international conference on Computational linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gholamreza Haffari</author>
<author>Maxim Roy</author>
<author>Anoop Sarkar</author>
</authors>
<title>Active learning for statistical phrase-based machine translation.</title>
<date>2009</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="6424" citStr="Haffari et al., 2009" startWordPosition="1043" endWordPosition="1046">idual systems MFd_,E; it can be BLEU score or WER/PER (Word error rate and position independent WER) which induces a maximization or minimization problem, respectively. The non-negative weights αd reflect the importance of the different translation tasks and Ed αd = 1. AL-SMT formulation for single language pair is a special case of this formulation where only one of the αd’s in the objective function (1) is one and the rest are zero. Moreover the algorithmic framework that we introduce in Sec. 2.1 for AL in the multilingual setting includes the single language pair setting as a special case (Haffari et al., 2009). We denote the large unlabeled multilingual corpus by U:= {(f1j , .., fDj )}, and the small labeled multilingual corpus by L := {(f1i , .., fDi , ei)}. We overload the term entry to denote a tuple in L or in U (it should be clear from the context). For a single language pair we use U and L. 2.1 The Algorithmic Framework Algorithm 1 represents our AL approach for the multilingual setting. We train our initial MT systems {MFd-,E}Dd=1 on the multilingual corpus L, and use them to translate all monolingual sentences in U. We denote sentences in U together with their multiple translations by U+ (l</context>
<context position="7827" citStr="Haffari et al., 2009" startWordPosition="1296" endWordPosition="1299">entences from U, and add those sentences together with their human-provided translations to L. This process is continued iteratively until a certain level of translation quality is met (we use the BLEU score, WER and PER) (Papineni et al., 2002). In the baseline, against which we compare our sentence selection methods, the sentences are chosen randomly. When (re-)training the models, two phrase tables are learned for each SMT model: one from the labeled data L and the other one from pseudolabeled data U+ (which we call the main and auxiliary phrase tables respectively). (Ueffing et al., 2007; Haffari et al., 2009) show that treating U+ as a source for a new feature function in a loglinear model for SMT (Och and Ney, 2004) allows us to maximally take advantage of unlabeled data by finding a weight for this feature using minimum error-rate training (MERT) (Och, 2003). Since each entry in U+ has multiple translations, there are two options when building the auxiliary table for a particular language pair (Fd, E): (i) to use the corresponding translation ed of the source language in a self-training setting, or (ii) to use the consensus translation among all the translation candidates (e1, .., eD) in a co-tr</context>
<context position="11729" citStr="Haffari et al., 2009" startWordPosition="1961" endWordPosition="1964"> are not correct. Therefore it would be beneficial to ask human for the translation of these hard entries. Now the question is how to quantify the notion of disagreement among the candidate translations (e1,.., eD). We propose two measures of disagreement which are related to the portion of shared n-grams (n &lt; 4) among the translations: • Let ec be the consensus among all the candidate translations, then define the disagreement as Ed αd (1 − BLEU(ec, ed)). • Based on the disagreement of every pair of candidate translations: Ed αd Ed, (1 − BLEU(edI, ed)). For the single language pair setting, (Haffari et al., 2009) presents and compares several sentence selection methods for statistical phrase-based machine translation. We introduce novel techniques which outperform those methods in the next section. 4 Sentence Selection: Single Language Pair Phrases are basic units of translation in phrasebased SMT models. The phrases which may potentially be extracted from a sentence indicate its informativeness. The more new phrases a sentence can offer, the more informative it is; since it boosts the generalization of the model. Additionally phrase translation probabilities need to be estimated accurately, which mea</context>
<context position="21457" citStr="Haffari et al., 2009" startWordPosition="3694" endWordPosition="3697">LEPR Model 1 − ELPR Geom Phrase Random BLEU Score 24.8 24.6 24.4 24.2 23.8 23.6 23.4 23.2 1000 2000 25 24 3000 4000 5000 Model 2 − LEPR Model 1 − ELPR Geom Phrase Random BLEU Score 17.8 17.6 17.4 17.2 16.8 16.6 16.4 16.2 1000 2000 3000 17 Model 2 − LEPR Model 1 − ELPR Geom Phrase Random 4000 5000 Added Sentences Added Sentences Added Sentences Figure 2: The performance of different sentence selection strategies as the iteration of AL loop goes on for three translation tasks. Plots show the performance of sentence selection methods for single language pair in Sec. 4 compared to the GeomPhrase (Haffari et al., 2009) and random sentence selection baseline. of the translation sentence. The number of weights wi is 3 plus the number of source languages, and they are trained using minimum error-rate training (MERT) to maximize the BLEU score (Och, 2003) on a development set. Parameters. We use add-c smoothing where c = .5 to smooth the probabilities in Sec. 4; moreover A = .4 for ELPR and LEPR sentence scoring and maximum phrase length k is set to 4. For the multilingual experiments (which involve four source languages) we set αd = .25 to make the importance of individual translation tasks equal. Mulilingual </context>
<context position="23644" citStr="Haffari et al., 2009" startWordPosition="4047" endWordPosition="4050"> 37.3 27.0 Alternate 37.7 27.3 37.3 27.0 Random Baseline 38.6 28.1 38.1 27.6 Romance languages to English Table 1: Comparison of multilingual selection methods with WER (word error rate), PER (position independent WER). 95% confidence interval for WER numbers is 0.7 and for PER numbers is 0.5. Bold: best result, italic: significantly better. We use three language pairs in our single language pair experiments: French-English, GermanEnglish, and Spanish- English. In addition to random sentence selection baseline, we also compare the methods proposed in this paper to the best method reported in (Haffari et al., 2009) denoted by GeomPhrase, which differs from our models since it considers each individual OOV segment as a single OOV phrase and does not consider subsequences. The results are presented in Fig. 2. Selecting sentences based on our proposed methods outperform the random sentence selection baseline and GeomPhrase. We suspect for the situations where L is out-of-domain and the average phrase length is relatively small, our method will outperform GeomPhrase even more. For the multilingual experiments, we use Germanic (German, Dutch, Danish, Swedish) and Romance (French, Spanish, Italian, Portuguese</context>
<context position="29141" citStr="Haffari et al., 2009" startWordPosition="4931" endWordPosition="4934">hrasal distributions. between the true and uniform distributions, in all three language pairs. Since uniform distribution conveys no information, this is evidence that there is some information encoded in the estimated distribution about the true distribution. However we noticed that the true distributions of regular/OOV phrases exhibit Zipfian (power law) behavior5 which is not well captured by the estimated distributions (see Fig. 5). Enhancing the estimated distributions to capture this power law behavior would improve the quality of the proposed sentence selection methods. 6 Related Work (Haffari et al., 2009) provides results for active learning for MT using a single language pair. Our work generalizes to the use of multilingual corpora using new methods that are not possible with a single language pair. In this paper, we also introduce new selection methods that outperform the methods in (Haffari et al., 2009) even for MT with a single language pair. In addition in this paper by considering multilingual parallel corpora we were able to introduce co-training for AL, while (Haffari et al., 2009) only use self-training since they are using a single language pair. 5This observation is at the phrase l</context>
</contexts>
<marker>Haffari, Roy, Sarkar, 2009</marker>
<rawString>Gholamreza Haffari, Maxim Roy, and Anoop Sarkar. 2009. Active learning for statistical phrase-based machine translation. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodong He</author>
<author>Mei Yang</author>
<author>Jianfeng Gao</author>
<author>Patrick Nguyen</author>
<author>Robert Moore</author>
</authors>
<title>Indirect-hmmbased hypothesis alignment for combining outputs from machine translation systems.</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="4030" citStr="He et al., 2008" startWordPosition="643" endWordPosition="646">tions. When we build multiple MT systems from multiple source languages to the new target language, each MT system can be seen as a different ‘view’ on the desired output translation. Thus, we can train our multiple MT systems using either self-training or co-training (Blum and Mitchell, 1998). In selftraining each MT system is re-trained using human labeled data plus its own noisy translation output on the unlabeled data. In co-training each MT system is re-trained using human labeled data plus noisy translation output from the other MT systems in the ensemble. We use consensus translations (He et al., 2008; Rosti et al., 2007; Matusov et al., 2006) as an effective method for co-training between multiple MT systems. This paper makes the following contributions: • We provide a new framework for multilingual MT, in which we build multiple MT systems and add a new language to an existing multilingual parallel corpus. The multilingual set181 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 181–189, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP ting allows new features for active learning which we exploit to improve translation quality while reducing</context>
<context position="8703" citStr="He et al., 2008" startWordPosition="1446" endWordPosition="1449">ce each entry in U+ has multiple translations, there are two options when building the auxiliary table for a particular language pair (Fd, E): (i) to use the corresponding translation ed of the source language in a self-training setting, or (ii) to use the consensus translation among all the translation candidates (e1, .., eD) in a co-training setting (sharing information between multiple SMT models). A whole range of methods exist in the literature for combining the output translations of multiple MT systems for a single language pair, operating either at the sentence, phrase, or word level (He et al., 2008; Rosti et al., 2007; Matusov et al., 2006). The method that we use in this work operates at the sentence level, and picks a single high quality translation from the union of the n-best lists generated by multiple SMT models. Sec. 5 gives 182 Algorithm 1 AL-SMT-Multiple 1: Given multilingual corpora L and U 2: {MFd,E}Dd=1 = multrain(L, 0) 3: for t = 1, 2,... do 4: U+ = multranslate(U, {MFd,E}Dd=1) 5: Select k sentences from U+, and ask a human for their true translations. 6: Remove the k sentences from U, and add the k sentence pairs (translated by human) to L 7: {MFd,E}Dd=1 = multrain(L, U+) </context>
<context position="30897" citStr="He et al., 2008" startWordPosition="5214" endWordPosition="5217">Osborne, 2003b; CallisonBurch and Osborne, 2003a) provide a co-training approach to MT, where one language pair creates data for another language pair. In contrast, our co-training approach uses consensus translations and our setting for active learning is very different from their semi-supervised setting. A Ph.D. proposal by Chris Callison-Burch (Callison-burch, 2003) lays out the promise of AL for SMT and proposes some algorithms. However, the lack of experimental results means that performance and feasibility of those methods cannot be compared to ours. While we use consensus translations (He et al., 2008; Rosti et al., 2007; Matusov et al., 2006) as an effective method for co-training in this paper, unlike consensus for system combination, the source languages for each of our MT systems are different, which rules out a set of popular methods for obtaining consensus translations which assume translation for a single language pair. Finally, we briefly note that triangulation (see (Cohn and Lapata, 2007)) is orthogonal to the use of co-training in our work, since it only enhances each MT system in our ensemble by exploiting the multilingual data. In future work, we plan to incorporate triangulat</context>
</contexts>
<marker>He, Yang, Gao, Nguyen, Moore, 2008</marker>
<rawString>Xiaodong He, Mei Yang, Jianfeng Gao, Patrick Nguyen, and Robert Moore. 2008. Indirect-hmmbased hypothesis alignment for combining outputs from machine translation systems. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In MT Summit.</booktitle>
<contexts>
<context position="19539" citStr="Koehn, 2005" startWordPosition="3345" endWordPosition="3346">ta, and the second term is the expected log probability ratio (ELPR) under the two models. Another option for the contribution of OOV phrases is to take log of expected probability ratio (LEPR): � X 02(s) := |Xreg s | xEXreg s 1 − A X |Xoov s |xEXsoov It is not difficult to prove that there is no difference between Model 1 and Model 2 when ELPR scoring is used for sentence selection. However, the situation is different for LEPR scoring: the two models produce different sentence rankings in this case. 5 Experiments Corpora. We pre-processed the EuroParl corpus (http://www.statmt.org/europarl) (Koehn, 2005) and built a multilingual parallel corpus with 653,513 sentences, excluding the Q4/2000 portion of the data (2000-10 to 2000-12) which is reserved as the test set. We subsampled 5,000 sentences as the labeled data L and 20,000 sentences as N for the pool of untranslated sentences (while hiding the English part). The test set consists of 2,000 multi-language sentences and comes from the multilingual parallel corpus built from Q4/2000 portion of the data. Consensus Finding. Let T be the union of the nbest lists of translations for a particular sentence. The consensus translation tc is arg max w1</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In MT Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeny Matusov</author>
<author>Nicola Ueffing</author>
<author>Hermann Ney</author>
</authors>
<title>Computing consensus translation from multiple machine translation systems using enhanced hypotheses alignment.</title>
<date>2006</date>
<booktitle>In EACL.</booktitle>
<contexts>
<context position="4073" citStr="Matusov et al., 2006" startWordPosition="651" endWordPosition="654">ems from multiple source languages to the new target language, each MT system can be seen as a different ‘view’ on the desired output translation. Thus, we can train our multiple MT systems using either self-training or co-training (Blum and Mitchell, 1998). In selftraining each MT system is re-trained using human labeled data plus its own noisy translation output on the unlabeled data. In co-training each MT system is re-trained using human labeled data plus noisy translation output from the other MT systems in the ensemble. We use consensus translations (He et al., 2008; Rosti et al., 2007; Matusov et al., 2006) as an effective method for co-training between multiple MT systems. This paper makes the following contributions: • We provide a new framework for multilingual MT, in which we build multiple MT systems and add a new language to an existing multilingual parallel corpus. The multilingual set181 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 181–189, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP ting allows new features for active learning which we exploit to improve translation quality while reducing annotation effort. • We introduce new high</context>
<context position="8746" citStr="Matusov et al., 2006" startWordPosition="1454" endWordPosition="1457">slations, there are two options when building the auxiliary table for a particular language pair (Fd, E): (i) to use the corresponding translation ed of the source language in a self-training setting, or (ii) to use the consensus translation among all the translation candidates (e1, .., eD) in a co-training setting (sharing information between multiple SMT models). A whole range of methods exist in the literature for combining the output translations of multiple MT systems for a single language pair, operating either at the sentence, phrase, or word level (He et al., 2008; Rosti et al., 2007; Matusov et al., 2006). The method that we use in this work operates at the sentence level, and picks a single high quality translation from the union of the n-best lists generated by multiple SMT models. Sec. 5 gives 182 Algorithm 1 AL-SMT-Multiple 1: Given multilingual corpora L and U 2: {MFd,E}Dd=1 = multrain(L, 0) 3: for t = 1, 2,... do 4: U+ = multranslate(U, {MFd,E}Dd=1) 5: Select k sentences from U+, and ask a human for their true translations. 6: Remove the k sentences from U, and add the k sentence pairs (translated by human) to L 7: {MFd,E}Dd=1 = multrain(L, U+) 8: Monitor the performance on the test set </context>
<context position="30940" citStr="Matusov et al., 2006" startWordPosition="5222" endWordPosition="5225">orne, 2003a) provide a co-training approach to MT, where one language pair creates data for another language pair. In contrast, our co-training approach uses consensus translations and our setting for active learning is very different from their semi-supervised setting. A Ph.D. proposal by Chris Callison-Burch (Callison-burch, 2003) lays out the promise of AL for SMT and proposes some algorithms. However, the lack of experimental results means that performance and feasibility of those methods cannot be compared to ours. While we use consensus translations (He et al., 2008; Rosti et al., 2007; Matusov et al., 2006) as an effective method for co-training in this paper, unlike consensus for system combination, the source languages for each of our MT systems are different, which rules out a set of popular methods for obtaining consensus translations which assume translation for a single language pair. Finally, we briefly note that triangulation (see (Cohn and Lapata, 2007)) is orthogonal to the use of co-training in our work, since it only enhances each MT system in our ensemble by exploiting the multilingual data. In future work, we plan to incorporate triangulation into our active learning approach. 7 Co</context>
</contexts>
<marker>Matusov, Ueffing, Ney, 2006</marker>
<rawString>Evgeny Matusov, Nicola Ueffing, and Hermann Ney. 2006. Computing consensus translation from multiple machine translation systems using enhanced hypotheses alignment. In EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="7937" citStr="Och and Ney, 2004" startWordPosition="1319" endWordPosition="1322">ntinued iteratively until a certain level of translation quality is met (we use the BLEU score, WER and PER) (Papineni et al., 2002). In the baseline, against which we compare our sentence selection methods, the sentences are chosen randomly. When (re-)training the models, two phrase tables are learned for each SMT model: one from the labeled data L and the other one from pseudolabeled data U+ (which we call the main and auxiliary phrase tables respectively). (Ueffing et al., 2007; Haffari et al., 2009) show that treating U+ as a source for a new feature function in a loglinear model for SMT (Och and Ney, 2004) allows us to maximally take advantage of unlabeled data by finding a weight for this feature using minimum error-rate training (MERT) (Och, 2003). Since each entry in U+ has multiple translations, there are two options when building the auxiliary table for a particular language pair (Fd, E): (i) to use the corresponding translation ed of the source language in a self-training setting, or (ii) to use the consensus translation among all the translation candidates (e1, .., eD) in a co-training setting (sharing information between multiple SMT models). A whole range of methods exist in the litera</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Josef Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4):417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In ACL ’03: Proceedings of the 41st Annual Meeting on Association for Computational Linguistics.</booktitle>
<contexts>
<context position="8083" citStr="Och, 2003" startWordPosition="1344" endWordPosition="1345">inst which we compare our sentence selection methods, the sentences are chosen randomly. When (re-)training the models, two phrase tables are learned for each SMT model: one from the labeled data L and the other one from pseudolabeled data U+ (which we call the main and auxiliary phrase tables respectively). (Ueffing et al., 2007; Haffari et al., 2009) show that treating U+ as a source for a new feature function in a loglinear model for SMT (Och and Ney, 2004) allows us to maximally take advantage of unlabeled data by finding a weight for this feature using minimum error-rate training (MERT) (Och, 2003). Since each entry in U+ has multiple translations, there are two options when building the auxiliary table for a particular language pair (Fd, E): (i) to use the corresponding translation ed of the source language in a self-training setting, or (ii) to use the consensus translation among all the translation candidates (e1, .., eD) in a co-training setting (sharing information between multiple SMT models). A whole range of methods exist in the literature for combining the output translations of multiple MT systems for a single language pair, operating either at the sentence, phrase, or word le</context>
<context position="21694" citStr="Och, 2003" startWordPosition="3734" endWordPosition="3735"> LEPR Model 1 − ELPR Geom Phrase Random 4000 5000 Added Sentences Added Sentences Added Sentences Figure 2: The performance of different sentence selection strategies as the iteration of AL loop goes on for three translation tasks. Plots show the performance of sentence selection methods for single language pair in Sec. 4 compared to the GeomPhrase (Haffari et al., 2009) and random sentence selection baseline. of the translation sentence. The number of weights wi is 3 plus the number of source languages, and they are trained using minimum error-rate training (MERT) to maximize the BLEU score (Och, 2003) on a development set. Parameters. We use add-c smoothing where c = .5 to smooth the probabilities in Sec. 4; moreover A = .4 for ELPR and LEPR sentence scoring and maximum phrase length k is set to 4. For the multilingual experiments (which involve four source languages) we set αd = .25 to make the importance of individual translation tasks equal. Mulilingual da−de−nl−sv to en Added Sentences Figure 3: Random sentence selection baseline using selftraining and co-training (Germanic languages to English). 5.1 Results First we evaluate the proposed sentence selection methods in Sec. 4 for the si</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In ACL ’03: Proceedings of the 41st Annual Meeting on Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei jing Zhu</author>
</authors>
<title>Bleu: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In ACL ’02: Proceedings of the 41st Annual Meeting on Association for Computational Linguistics.</booktitle>
<contexts>
<context position="7451" citStr="Papineni et al., 2002" startWordPosition="1231" endWordPosition="1234">initial MT systems {MFd-,E}Dd=1 on the multilingual corpus L, and use them to translate all monolingual sentences in U. We denote sentences in U together with their multiple translations by U+ (line 4 of Algorithm 1). Then we retrain the SMT systems on L U U+ and use the resulting model to decode the test set. Afterwards, we select and remove a subset of highly informative sentences from U, and add those sentences together with their human-provided translations to L. This process is continued iteratively until a certain level of translation quality is met (we use the BLEU score, WER and PER) (Papineni et al., 2002). In the baseline, against which we compare our sentence selection methods, the sentences are chosen randomly. When (re-)training the models, two phrase tables are learned for each SMT model: one from the labeled data L and the other one from pseudolabeled data U+ (which we call the main and auxiliary phrase tables respectively). (Ueffing et al., 2007; Haffari et al., 2009) show that treating U+ as a source for a new feature function in a loglinear model for SMT (Och and Ney, 2004) allows us to maximally take advantage of unlabeled data by finding a weight for this feature using minimum error-</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wei jing Zhu. 2002. Bleu: A method for automatic evaluation of machine translation. In ACL ’02: Proceedings of the 41st Annual Meeting on Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roi Reichart</author>
<author>Katrin Tomanek</author>
<author>Udo Hahn</author>
<author>Ari Rappoport</author>
</authors>
<title>Multi-task active learning for linguistic annotations.</title>
<date>2008</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1812" citStr="Reichart et al., 2008" startWordPosition="274" endWordPosition="277">pean Parliament (EuroParl) and U.N. proceedings. In this paper, we consider how to use active learning (AL) in order to add a new language to such a multilingual parallel corpus and at the same time we construct an MT system from each language in the original corpus into this new target language. We introduce a novel combined measure of translation quality for multiple target language outputs (the same content from multiple source languages). The multilingual setting provides new opportunities for AL over and above a single language pair. This setting is similar to the multi-task AL scenario (Reichart et al., 2008). In our case, the multiple tasks are individual machine translation tasks for several language pairs. The nature of the translation processes vary from any of the source *Thanks to James Peltier for systems support for our experiments. This research was partially supported by NSERC, Canada (RGPIN: 264905) and an IBM Faculty Award. languages to the new language depending on the characteristics of each source-target language pair, hence these tasks are competing for annotating the same resource. However it may be that in a single language pair, AL would pick a particular sentence for annotation</context>
<context position="10289" citStr="Reichart et al., 2008" startWordPosition="1716" endWordPosition="1719"> objective function (1) with minimum human effort in providing the translations. This motivates selecting sentences which are maximally beneficial for all the MT systems. In this section, we present several protocols for sentence selection based on the combined information from multiple language pairs. 3.1 Alternating Selection The simplest selection protocol is to choose k sentences (entries) in the first iteration of AL which improve maximally the first model MFi_,E, while ignoring other models. In the second iteration, the sentences are selected with respect to the second model, and so on (Reichart et al., 2008). 3.2 Combined Ranking Pick any AL-SMT scoring method for a single language pair (see Sec. 4). Using this method, we rank the entries in unlabeled data U for each translation task defined by language pair (Fd, E). This results in several ranking lists, each of which represents the importance of entries with respect to a particular translation task. We combine these rankings using a combined score: D Score((f1, .., fD)) = αdRankd(fd) d=1 Rankd(.) is the ranking of a sentence in the list for the dth translation task (Reichart et al., 2008). 3.3 Disagreement Among the Translations Disagreement am</context>
<context position="29842" citStr="Reichart et al., 2008" startWordPosition="5053" endWordPosition="5056">work generalizes to the use of multilingual corpora using new methods that are not possible with a single language pair. In this paper, we also introduce new selection methods that outperform the methods in (Haffari et al., 2009) even for MT with a single language pair. In addition in this paper by considering multilingual parallel corpora we were able to introduce co-training for AL, while (Haffari et al., 2009) only use self-training since they are using a single language pair. 5This observation is at the phrase level and not at the word (Zipf, 1932) or even n-gram level (Ha et al., 2002). (Reichart et al., 2008) introduces multi-task active learning where unlabeled data require annotations for multiple tasks, e.g. they consider namedentities and parse trees, and showed that multiple tasks helps selection compared to individual tasks. Our setting is different in that the target language is the same across multiple MT tasks, which we exploit to use consensus translations and cotraining to improve active learning performance. (Callison-Burch and Osborne, 2003b; CallisonBurch and Osborne, 2003a) provide a co-training approach to MT, where one language pair creates data for another language pair. In contr</context>
</contexts>
<marker>Reichart, Tomanek, Hahn, Rappoport, 2008</marker>
<rawString>Roi Reichart, Katrin Tomanek, Udo Hahn, and Ari Rappoport. 2008. Multi-task active learning for linguistic annotations. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antti-Veikko Rosti</author>
<author>Necip Fazil Ayan</author>
<author>Bing Xiang</author>
<author>Spyros Matsoukas</author>
<author>Richard M Schwartz</author>
<author>Bonnie Jean Dorr</author>
</authors>
<title>Combining outputs from multiple machine translation systems.</title>
<date>2007</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="4050" citStr="Rosti et al., 2007" startWordPosition="647" endWordPosition="650">ild multiple MT systems from multiple source languages to the new target language, each MT system can be seen as a different ‘view’ on the desired output translation. Thus, we can train our multiple MT systems using either self-training or co-training (Blum and Mitchell, 1998). In selftraining each MT system is re-trained using human labeled data plus its own noisy translation output on the unlabeled data. In co-training each MT system is re-trained using human labeled data plus noisy translation output from the other MT systems in the ensemble. We use consensus translations (He et al., 2008; Rosti et al., 2007; Matusov et al., 2006) as an effective method for co-training between multiple MT systems. This paper makes the following contributions: • We provide a new framework for multilingual MT, in which we build multiple MT systems and add a new language to an existing multilingual parallel corpus. The multilingual set181 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 181–189, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP ting allows new features for active learning which we exploit to improve translation quality while reducing annotation effort. </context>
<context position="8723" citStr="Rosti et al., 2007" startWordPosition="1450" endWordPosition="1453">U+ has multiple translations, there are two options when building the auxiliary table for a particular language pair (Fd, E): (i) to use the corresponding translation ed of the source language in a self-training setting, or (ii) to use the consensus translation among all the translation candidates (e1, .., eD) in a co-training setting (sharing information between multiple SMT models). A whole range of methods exist in the literature for combining the output translations of multiple MT systems for a single language pair, operating either at the sentence, phrase, or word level (He et al., 2008; Rosti et al., 2007; Matusov et al., 2006). The method that we use in this work operates at the sentence level, and picks a single high quality translation from the union of the n-best lists generated by multiple SMT models. Sec. 5 gives 182 Algorithm 1 AL-SMT-Multiple 1: Given multilingual corpora L and U 2: {MFd,E}Dd=1 = multrain(L, 0) 3: for t = 1, 2,... do 4: U+ = multranslate(U, {MFd,E}Dd=1) 5: Select k sentences from U+, and ask a human for their true translations. 6: Remove the k sentences from U, and add the k sentence pairs (translated by human) to L 7: {MFd,E}Dd=1 = multrain(L, U+) 8: Monitor the perfo</context>
<context position="30917" citStr="Rosti et al., 2007" startWordPosition="5218" endWordPosition="5221">allisonBurch and Osborne, 2003a) provide a co-training approach to MT, where one language pair creates data for another language pair. In contrast, our co-training approach uses consensus translations and our setting for active learning is very different from their semi-supervised setting. A Ph.D. proposal by Chris Callison-Burch (Callison-burch, 2003) lays out the promise of AL for SMT and proposes some algorithms. However, the lack of experimental results means that performance and feasibility of those methods cannot be compared to ours. While we use consensus translations (He et al., 2008; Rosti et al., 2007; Matusov et al., 2006) as an effective method for co-training in this paper, unlike consensus for system combination, the source languages for each of our MT systems are different, which rules out a set of popular methods for obtaining consensus translations which assume translation for a single language pair. Finally, we briefly note that triangulation (see (Cohn and Lapata, 2007)) is orthogonal to the use of co-training in our work, since it only enhances each MT system in our ensemble by exploiting the multilingual data. In future work, we plan to incorporate triangulation into our active </context>
</contexts>
<marker>Rosti, Ayan, Xiang, Matsoukas, Schwartz, Dorr, 2007</marker>
<rawString>Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang, Spyros Matsoukas, Richard M. Schwartz, and Bonnie Jean Dorr. 2007. Combining outputs from multiple machine translation systems. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Ueffing</author>
<author>Gholamreza Haffari</author>
<author>Anoop Sarkar</author>
</authors>
<title>Transductive learning for statistical machine translation.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="7804" citStr="Ueffing et al., 2007" startWordPosition="1292" endWordPosition="1295">f highly informative sentences from U, and add those sentences together with their human-provided translations to L. This process is continued iteratively until a certain level of translation quality is met (we use the BLEU score, WER and PER) (Papineni et al., 2002). In the baseline, against which we compare our sentence selection methods, the sentences are chosen randomly. When (re-)training the models, two phrase tables are learned for each SMT model: one from the labeled data L and the other one from pseudolabeled data U+ (which we call the main and auxiliary phrase tables respectively). (Ueffing et al., 2007; Haffari et al., 2009) show that treating U+ as a source for a new feature function in a loglinear model for SMT (Och and Ney, 2004) allows us to maximally take advantage of unlabeled data by finding a weight for this feature using minimum error-rate training (MERT) (Och, 2003). Since each entry in U+ has multiple translations, there are two options when building the auxiliary table for a particular language pair (Fd, E): (i) to use the corresponding translation ed of the source language in a self-training setting, or (ii) to use the consensus translation among all the translation candidates </context>
</contexts>
<marker>Ueffing, Haffari, Sarkar, 2007</marker>
<rawString>Nicola Ueffing, Gholamreza Haffari, and Anoop Sarkar. 2007. Transductive learning for statistical machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Zipf</author>
</authors>
<title>Selective Studies and the Principle of Relative Frequency in Language.</title>
<date>1932</date>
<publisher>Harvard University Press.</publisher>
<contexts>
<context position="29778" citStr="Zipf, 1932" startWordPosition="5043" endWordPosition="5044">ve learning for MT using a single language pair. Our work generalizes to the use of multilingual corpora using new methods that are not possible with a single language pair. In this paper, we also introduce new selection methods that outperform the methods in (Haffari et al., 2009) even for MT with a single language pair. In addition in this paper by considering multilingual parallel corpora we were able to introduce co-training for AL, while (Haffari et al., 2009) only use self-training since they are using a single language pair. 5This observation is at the phrase level and not at the word (Zipf, 1932) or even n-gram level (Ha et al., 2002). (Reichart et al., 2008) introduces multi-task active learning where unlabeled data require annotations for multiple tasks, e.g. they consider namedentities and parse trees, and showed that multiple tasks helps selection compared to individual tasks. Our setting is different in that the target language is the same across multiple MT tasks, which we exploit to use consensus translations and cotraining to improve active learning performance. (Callison-Burch and Osborne, 2003b; CallisonBurch and Osborne, 2003a) provide a co-training approach to MT, where on</context>
</contexts>
<marker>Zipf, 1932</marker>
<rawString>George Zipf. 1932. Selective Studies and the Principle of Relative Frequency in Language. Harvard University Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>