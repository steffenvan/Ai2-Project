<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.034972">
<title confidence="0.777583">
Transfer Learning, Feature Selection and Word Sense Disambguation
</title>
<author confidence="0.631778">
Paramveer S. Dhillon and Lyle H. Ungar
</author>
<affiliation confidence="0.946444">
Computer and Information Science
University of Pennsylvania, Philadelphia, PA, U.S.A
</affiliation>
<email confidence="0.994893">
{pasingh,ungar}@seas.upenn.edu
</email>
<sectionHeader confidence="0.993778" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999857">
We propose a novel approach for improv-
ing Feature Selection for Word Sense Dis-
ambiguation by incorporating a feature
relevance prior for each word indicating
which features are more likely to be se-
lected. We use transfer of knowledge from
similar words to learn this prior over the
features, which permits us to learn higher
accuracy models, particularly for the rarer
word senses. Results on the ONTONOTES
verb data show significant improvement
over the baseline feature selection algo-
rithm and results that are comparable to or
better than other state-of-the-art methods.
</bodyText>
<sectionHeader confidence="0.998793" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999676720588236">
The task of WSD has been mostly studied in
a supervised learning setting e.g. (Florian and
Yarowsky, 2002) and feature selection has always
been an important component of high accuracy
word sense disambiguation, as one often has thou-
sands of features but only hundreds of observa-
tions of the words (Florian and Yarowsky, 2002).
The main problem that arises with supervised
WSD techniques, including ones that do feature
selection, is the paucity of labeled data. For ex-
ample, the training set of SENSEVAL-2 English
lexical sample task has only 10 labeled examples
per sense (Florian and Yarowsky, 2002), which
makes it difficult to build high accuracy models
using only supervised learning techniques. It is
thus an attractive alternative to use transfer learn-
ing (Ando and Zhang, 2005), which improves per-
formance by generalizing from solutions to “sim-
ilar” learning problems. (Ando, 2006) (abbrevi-
ated as Ando[CoNLL’06]) have successfully ap-
plied the ASO (Alternating Structure Optimiza-
tion) technique proposed by (Ando and Zhang,
2005), in its transfer learning configuration, to the
problem of WSD by doing joint empirical risk
minimization of a set of related problems (words
in this case). In this paper, we show how a novel
form of transfer learning that learns a feature rel-
evance prior from similar word senses, aids in the
process of feature selection and hence benefits the
task of WSD.
Feature selection algorithms usually put a uni-
form prior over the features. I.e., they consider
each feature to have the same probability of being
selected. In this paper we relax this overly sim-
plistic assumption by transferring a prior for fea-
ture relevance of a given word sense from “simi-
lar” word senses. Learning this prior for feature
relevance of a test word sense makes those fea-
tures that have been selected in the models of other
“similar” word senses become more likely to be
selected.
We learn the feature relevance prior only from
distributionally similar word senses, rather than
“all” senses of each word, as it is difficult to find
words which are similar in “all” the senses. We
can, however, often find words which have one or
a few similar senses. For example, one sense of
“fire” (as in “fire someone”) should share features
with one sense of “dismiss” (as in “dismiss some-
one”), but other senses of “fire” (as in “fire the
gun”) do not. Similarly, other meanings of “dis-
miss” (as in “dismiss an idea”) should not share
features with “fire”.
As just mentioned, knowledge can only be
fruitfully transfered between the shared senses of
different words, even though the models being
learned are for disambiguating different senses of
a single word. To address this problem, we cluster
similar word senses of different words, and then
use the models learned for all but one of the word
senses in the cluster (called the “training word
senses”) to put a feature relevance prior on which
features will be more predictive for the held out
test word sense. We hold out each word sense in
the cluster once and learn a prior from the remain-
ing word senses in that cluster. For example, we
can use the models for discriminating the senses
of the words “kill” and the senses of “capture”, to
</bodyText>
<page confidence="0.958637">
257
</page>
<note confidence="0.9247235">
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 257–260,
Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999778181818182">
put a prior on what features should be included in
a model to disambiguate corresponding senses of
the distributionally similar word “arrest”.
The remainder of the paper is organized as fol-
lows. In Section 2 we describe our “baseline” in-
formation theoretic feature selection method, and
extend it to our “TRANSFEAT” method. Section 3
contains experimental results comparing TRANS-
FEAT with the baseline and Ando[CoNLL’06] on
ONTONOTES data. We conclude with a brief sum-
mary in Section 4.
</bodyText>
<sectionHeader confidence="0.900391" genericHeader="method">
2 Feature Selection for WSD
</sectionHeader>
<bodyText confidence="0.999963625">
We use an information theoretic approach to fea-
ture selection based on the Minimum Description
Length (MDL) (Rissanen, 1999) principle, which
makes it easy to incorporate information about
feature relevance priors. These information theo-
retic models have a ‘dual’ Bayesian interpretation,
which provides a clean setting for feature selec-
tion.
</bodyText>
<subsectionHeader confidence="0.941804">
2.1 Information Theoretic Feature Selection
</subsectionHeader>
<bodyText confidence="0.978954357142857">
The state-of-the-art feature selection methods in
WSD use either an E0 or an E1 penalty on the coef-
ficients. E1 penalty methods such as Lasso, being
convex, can be solved by optimization and give
guaranteed optimal solutions. On the other hand,
E0 penalty methods, like stepwise feature selec-
tion, give approximate solutions but produce mod-
els that are much sparser than the models given by
E1 methods, which is quite crucial in WSD (Flo-
rian and Yarowsky, 2002). E0 models are also more
amenable to theoretical analysis for setting thresh-
olds, and hence for incorporating priors.
Penalized likelihood methods which are widely
used for feature selection minimize a score:
</bodyText>
<equation confidence="0.985212">
Score = −2log(likelihood) + Fq (1)
</equation>
<bodyText confidence="0.999703034482759">
where F is a function designed to penalize model
complexity, and q represents the number of fea-
tures currently included in the model at a given
point. The first term in the above equation repre-
sents a measure of the in-sample error given the
model, while the second term is a model complex-
ity penalty.
As is obvious from Eq. 1, the description length
of the MDL (Minimum Description Length) mes-
sage is composed of two parts: SE, the num-
ber of bits for encoding the residual errors given
the models and SM, the number of bits for en-
coding the model. Hence the description length
can be written as: S = SE + SM. Now, when
we evaluate a feature for possible addition to our
model, we want to maximize the reduction of “de-
scription length” incurred by adding this feature
to the model. This change in description length
is: AS = ASE − ASM; where ASE &gt; 0 is the
number of bits saved in describing residual error
due to increase in the likelihood of the data given
the new feature and ASM &gt; 0 is the extra bits
used for coding this new feature.
In our baseline feature selection model, we use
the following coding schemes:
Coding Scheme for SE :
The term SE represents the cost of coding the
residual errors given the models and can be written
as:
</bodyText>
<equation confidence="0.945742">
SE = − log(P(y|w, x))
</equation>
<bodyText confidence="0.999740666666667">
ASE represents the increase in likelihood (in
bits) of the data by adding this new feature to the
model. We assume a Gaussian model, giving:
</bodyText>
<equation confidence="0.940169">
P(y |w, x) — exp (− (
</equation>
<bodyText confidence="0.99729348">
where y is the response \(word senses in our case),
x’s are the features, w’s are the regression weights
and Q2 is the variance of the Gaussian noise.
Coding Scheme for ASM : For describing SM,
the number of bits for encoding the model, we
need the bits to code the index of the feature (i.e.,
which feature from amongst the total m candidate
features) and the bits to code the coefficient of this
feature.
The total cost can be represented as:
SM=lf+le
where lf is the cost to code the index of the feature
and le is the number of bits required to code the
coefficient of the selected feature.
In our baseline feature selection algorithm, we
code lf by using log(m) bits (where m is the
total number of candidate features), which is
equivalent to the standard RIC (or the Bonferroni
penalty) (Foster and George, 1994) commonly
used in information theory. The above coding
scheme1 corresponds to putting a uniform prior
over all the features; I.e., each feature is equally
likely to get selected.
For coding the coefficients of the selected fea-
ture we use 2 bits, which is quite similar to the AIC
</bodyText>
<footnote confidence="0.954162">
1There is a duality between Information Theory and
Bayesian terminology: If there is 1 probability of a fact being
true, then we need −log( ) = log(k) bits to code it.
</footnote>
<equation confidence="0.586881285714286">
2��
E
n=
(
i
w
)
2Q2
i
1
y
−
·
i
</equation>
<page confidence="0.980626">
258
</page>
<bodyText confidence="0.973557">
(Akaike Information Criterion) (Rissanen, 1999).
Our final equation for SM is therefore:
</bodyText>
<equation confidence="0.947891">
SM = log(m) + 2 (2)
</equation>
<subsectionHeader confidence="0.977157">
2.2 Extension to TRANSFEAT
</subsectionHeader>
<bodyText confidence="0.953004307692308">
We now extend the baseline feature selection al-
gorithm to include the feature relevance prior. We
define a binary random variable fi E {0,1} that
denotes the event of the ith feature being in or not
being in the model for the test word sense. We can
parameterize the distribution as p(fi = 1|θi) = θi.
I.e., we have a Bernoulli Distribution over the fea-
tures.
Given the data for the ith feature for all the
training word senses, we can write: Di =
{fi1, ..., fiv, ..., fit}. We then construct the like-
lihood functions from the data (under the i.i.d as-
sumption) as:
</bodyText>
<equation confidence="0.999584333333333">
t t
p(Dfi|θi) = H p(fiv|θi) = H θfiv(1 − θi)1−fiv
v=1 v=1
</equation>
<bodyText confidence="0.995485333333333">
The posteriors can be calculated by putting a prior
over the parameters θi and using Bayes rule as fol-
lows:
</bodyText>
<equation confidence="0.998478">
p(θi|Dfi) = p(Dfi|θi) x p(θi|a, b)
</equation>
<bodyText confidence="0.999853333333333">
where a and b are the hyperparameters of the Beta
Prior (conjugate of Bernoulli). The predictive dis-
tribution of θi is:
</bodyText>
<equation confidence="0.965136">
p(fi = 1|Dfi) = 11 θip(θi  |Dfi )dθi = E [θi  |Dfi ]
k + a (3)
k+l+a+b
</equation>
<bodyText confidence="0.999978875">
where k is the number of times that the ith feature
is selected and l is the complement of k, i.e. the
number of times the ith feature is not selected in
the training data.
In light of above, the coding scheme, which in-
corporates the prior information about the predic-
tive quality of the various features obtained from
similar word senses, can be formulated as follows:
</bodyText>
<equation confidence="0.938537">
SM = − log (p(fi = 1|Dfi)) + 2
</equation>
<bodyText confidence="0.999885">
In the above equation, the first term repre-
sents the cost of coding the features, and the sec-
ond term codes the coefficients. The negative
signs appear due to the duality between Bayesian
and Information-Theoretic representation, as ex-
plained earlier.
</bodyText>
<sectionHeader confidence="0.996246" genericHeader="method">
3 Experimental Results
</sectionHeader>
<bodyText confidence="0.999031">
In this section we present the experimental results
of TRANSFEAT on ONTONOTES data.
</bodyText>
<subsectionHeader confidence="0.99954">
3.1 Similarity Determination
</subsectionHeader>
<bodyText confidence="0.999511153846154">
To determine which verbs to transfer from, we
cluster verb senses into groups based on the
TF/IDF similarity of the vector of features se-
lected for that verb sense in the baseline (non-
transfer learning) model. We use only those
features that are positively correlated with the
given sense; they are the features most closely
associated with the given sense. We cluster
senses using a “foreground-background” cluster-
ing algorithm (Kandylas et al., 2007) rather than
the more common k-means clustering because
many word senses are not sufficiently similar to
any other word sense to warrant putting into a
cluster. Foreground-background clustering gives
highly cohesive clusters of word senses (the “fore-
ground”) and puts all the remaining word senses
in the “background”. The parameters that it takes
as input are the % of data points to put in “back-
ground” (i.e., what would be the singleton clus-
ters) and a similarity threshold which impacts
the number of “foreground” clusters. We exper-
imented with putting 20% and 33% data points in
background and adjusted the similarity threshold
to give us 50 − 100 “foreground” clusters. The
results reported below have 20% background and
50 − 100 “foreground” clusters.
</bodyText>
<subsectionHeader confidence="0.999977">
3.2 Description of Data and Results
</subsectionHeader>
<bodyText confidence="0.999261222222222">
We performed our experiments on ONTONOTES
data of 172 verbs (Hovy et al., 2006). The data
consists of a rich set of linguistic features which
have proven to be beneficial for WSD.
A sample feature vector for the word “add”,
given below, shows typical features.
word_added pos_vbd morph_normal
subj_use subjsyn_16993 dobj_money
dobjsyn_16993 pos+1+2+3_rp+to+cd
tp_account tp_accumulate tp_actual
The 172 verbs each had between 1,000 and 10,000
nonzero features. The number of senses varied
from 2 (For example, “add”) to 15 (For example,
“turn”).
We tested our transfer learning algorithm in
three slightly varied settings to tease apart the con-
tributions of different features to the overall per-
formance. In our main setting, we cluster the word
</bodyText>
<page confidence="0.996209">
259
</page>
<bodyText confidence="0.7077558">
senses based on the “semantic + syntactic” fea-
tures. In Setting 2, we do clustering based only on
“semantic” features (topic features) and in Setting
3 we cluster based on only “syntactic” (pos, dobj
etc.) features.
</bodyText>
<tableCaption confidence="0.81364525">
Table 1: 10-fold CV (microaveraged) accuracies
of various methods for various Transfer Learning
settings. Note: These are true cross-validation ac-
curacies; No parameters have been tuned on them.
</tableCaption>
<table confidence="0.999375">
Method Setting 1 Setting 2 Setting 3
TRANSFEAT 85.75 85.11 85.37
Baseline Feat. Sel. 83.50 83.09 83.34
SVM (Poly. Kernel) 83.77 83.44 83.57
Ando[CoNLL’06] 85.94 85.00 85.51
Most Freq. Sense 76.59 77.14 77.24
</table>
<bodyText confidence="0.998877121212121">
We compare TRANSFEAT against Baseline Fea-
ture Selection, Ando[CoNLL’06], SVM (libSVM
package) with a cross-validated polynomial kernel
and a simple most frequent sense baseline. We
tuned the “d” parameter of the polynomial kernel
using a separate cross validation.
The results for the different settings are shown
in Table 1 and are significantly better at the 5%
significance level (Paired t-test) than the base-
line feature selection algorithm and the SVM. It
is comparable in accuracy to Ando[CoNLL’06].
Settings 2 and 3, in which we cluster based on
only “semantic” or “syntactic” features, respec-
tively, also gave significant (5% level in a Paired
t-Test) improvement in accuracy over the baseline
and SVM model. But these settings performed
slightly worse than Setting 1, which suggests that
it is a good idea to have clusters in which the word
senses have “semantic” as well as “syntactic” dis-
tributional similarity.
Some examples will help to emphasize the point
that we made earlier that transfer helps the most in
cases in which the target word sense has much less
data than the word senses from which knowledge
is being transferred. “kill” had roughly 6 times
more data than all other word senses in its cluster
(i.e., “arrest”, “capture”, “strengthen”, etc.) In this
case, TRANSFEAT gave 3.19 − 8.67% higher ac-
curacies than competing methods2 on these three
words. Also, for the case of word “do,” which
had roughly 10 times more data than the other
word senses in its cluster (E.g., “die” and “save”),
TRANSFEAT gave 4.09−6.21% higher accuracies
</bodyText>
<footnote confidence="0.952159333333333">
2TRANSFEAT does better than Ando[CoNLL’06] on these
words even though on average over all 172 verbs, the differ-
ence is slender.
</footnote>
<bodyText confidence="0.9971445">
than other methods. Transfer makes the biggest
difference when the target words have much less
data than the word senses they are generalizing
from, but even in cases where the words have sim-
ilar amounts of data we still get a 1.5 − 2.5% in-
crease in accuracy.
</bodyText>
<sectionHeader confidence="0.998727" genericHeader="method">
4 Summary
</sectionHeader>
<bodyText confidence="0.9999887">
This paper presented a Transfer Learning formula-
tion which learns a prior suggesting which features
are most useful for disambiguating ambiguous
words. Successful transfer requires finding similar
word senses. We used “foreground/background”
clustering to find cohesive clusters for various
word senses in the ONTONOTES data, consider-
ing both “semantic” and “syntactic” similarity be-
tween the word senses. Learning priors on features
was found to give significant accuracy boosts,
with both syntactic and semantic features con-
tributing to successful transfer. Both feature sets
gave substantial benefits over the baseline meth-
ods that did not use any transfer and gave compa-
rable accuracy to recent Transfer Learning meth-
ods like Ando[CoNLL’06]. The performance im-
provement of our Transfer Learning becomes even
more pronounced when the word senses that we
are generalizing from have more observations than
the ones that are being learned.
</bodyText>
<sectionHeader confidence="0.999253" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999566476190476">
R. Ando and T. Zhang. 2005. A framework for learn-
ing predictive structures from multiple tasks and un-
labeled data. JMLR, 6:1817–1853.
R. Ando. 2006. Applying alternating structure
optimization to word sense disambiguation. In
(CoNLL).
R. Florian and D. Yarowsky. 2002. Modeling consen-
sus: classifier combination for word sense disam-
biguation. In EMNLP ’02, pages 25–32.
D. P. Foster and E. I. George. 1994. The risk infla-
tion criterion for multiple regression. The Annals of
Statistics, 22(4):1947–1975.
E. H. Hovy, M. P. Marcus, M. Palmer, L. A. Ramshaw,
and R. M. Weischedel. 2006. Ontonotes: The 90%
solution. In HLT-NAACL.
V. Kandylas, S. P. Upham, and L. H. Ungar. 2007.
Finding cohesive clusters for analyzing knowledge
communities. In ICDM, pages 203–212.
J. Rissanen. 1999. Hypothesis selection and testing by
the mdl principle. The Computer Journal, 42:260–
269.
</reference>
<page confidence="0.996858">
260
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.701188">
<title confidence="0.999747">Transfer Learning, Feature Selection and Word Sense Disambguation</title>
<author confidence="0.999706">Paramveer S Dhillon</author>
<author confidence="0.999706">Lyle H Ungar</author>
<affiliation confidence="0.8526855">Computer and Information Science University of Pennsylvania, Philadelphia, PA, U.S.A</affiliation>
<abstract confidence="0.9995982">We propose a novel approach for improving Feature Selection for Word Sense Disambiguation by incorporating a feature relevance prior for each word indicating which features are more likely to be selected. We use transfer of knowledge from similar words to learn this prior over the features, which permits us to learn higher accuracy models, particularly for the rarer senses. Results on the verb data show significant improvement over the baseline feature selection algorithm and results that are comparable to or better than other state-of-the-art methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Ando</author>
<author>T Zhang</author>
</authors>
<title>A framework for learning predictive structures from multiple tasks and unlabeled data.</title>
<date>2005</date>
<journal>JMLR,</journal>
<pages>6--1817</pages>
<contexts>
<context position="1605" citStr="Ando and Zhang, 2005" startWordPosition="243" endWordPosition="246">nt of high accuracy word sense disambiguation, as one often has thousands of features but only hundreds of observations of the words (Florian and Yarowsky, 2002). The main problem that arises with supervised WSD techniques, including ones that do feature selection, is the paucity of labeled data. For example, the training set of SENSEVAL-2 English lexical sample task has only 10 labeled examples per sense (Florian and Yarowsky, 2002), which makes it difficult to build high accuracy models using only supervised learning techniques. It is thus an attractive alternative to use transfer learning (Ando and Zhang, 2005), which improves performance by generalizing from solutions to “similar” learning problems. (Ando, 2006) (abbreviated as Ando[CoNLL’06]) have successfully applied the ASO (Alternating Structure Optimization) technique proposed by (Ando and Zhang, 2005), in its transfer learning configuration, to the problem of WSD by doing joint empirical risk minimization of a set of related problems (words in this case). In this paper, we show how a novel form of transfer learning that learns a feature relevance prior from similar word senses, aids in the process of feature selection and hence benefits the t</context>
</contexts>
<marker>Ando, Zhang, 2005</marker>
<rawString>R. Ando and T. Zhang. 2005. A framework for learning predictive structures from multiple tasks and unlabeled data. JMLR, 6:1817–1853.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Ando</author>
</authors>
<title>Applying alternating structure optimization to word sense disambiguation.</title>
<date>2006</date>
<booktitle>In (CoNLL).</booktitle>
<contexts>
<context position="1709" citStr="Ando, 2006" startWordPosition="260" endWordPosition="261">tions of the words (Florian and Yarowsky, 2002). The main problem that arises with supervised WSD techniques, including ones that do feature selection, is the paucity of labeled data. For example, the training set of SENSEVAL-2 English lexical sample task has only 10 labeled examples per sense (Florian and Yarowsky, 2002), which makes it difficult to build high accuracy models using only supervised learning techniques. It is thus an attractive alternative to use transfer learning (Ando and Zhang, 2005), which improves performance by generalizing from solutions to “similar” learning problems. (Ando, 2006) (abbreviated as Ando[CoNLL’06]) have successfully applied the ASO (Alternating Structure Optimization) technique proposed by (Ando and Zhang, 2005), in its transfer learning configuration, to the problem of WSD by doing joint empirical risk minimization of a set of related problems (words in this case). In this paper, we show how a novel form of transfer learning that learns a feature relevance prior from similar word senses, aids in the process of feature selection and hence benefits the task of WSD. Feature selection algorithms usually put a uniform prior over the features. I.e., they consi</context>
</contexts>
<marker>Ando, 2006</marker>
<rawString>R. Ando. 2006. Applying alternating structure optimization to word sense disambiguation. In (CoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Florian</author>
<author>D Yarowsky</author>
</authors>
<title>Modeling consensus: classifier combination for word sense disambiguation.</title>
<date>2002</date>
<booktitle>In EMNLP ’02,</booktitle>
<pages>25--32</pages>
<contexts>
<context position="925" citStr="Florian and Yarowsky, 2002" startWordPosition="134" endWordPosition="137"> Sense Disambiguation by incorporating a feature relevance prior for each word indicating which features are more likely to be selected. We use transfer of knowledge from similar words to learn this prior over the features, which permits us to learn higher accuracy models, particularly for the rarer word senses. Results on the ONTONOTES verb data show significant improvement over the baseline feature selection algorithm and results that are comparable to or better than other state-of-the-art methods. 1 Introduction The task of WSD has been mostly studied in a supervised learning setting e.g. (Florian and Yarowsky, 2002) and feature selection has always been an important component of high accuracy word sense disambiguation, as one often has thousands of features but only hundreds of observations of the words (Florian and Yarowsky, 2002). The main problem that arises with supervised WSD techniques, including ones that do feature selection, is the paucity of labeled data. For example, the training set of SENSEVAL-2 English lexical sample task has only 10 labeled examples per sense (Florian and Yarowsky, 2002), which makes it difficult to build high accuracy models using only supervised learning techniques. It i</context>
<context position="5515" citStr="Florian and Yarowsky, 2002" startWordPosition="891" endWordPosition="895">information theoretic models have a ‘dual’ Bayesian interpretation, which provides a clean setting for feature selection. 2.1 Information Theoretic Feature Selection The state-of-the-art feature selection methods in WSD use either an E0 or an E1 penalty on the coefficients. E1 penalty methods such as Lasso, being convex, can be solved by optimization and give guaranteed optimal solutions. On the other hand, E0 penalty methods, like stepwise feature selection, give approximate solutions but produce models that are much sparser than the models given by E1 methods, which is quite crucial in WSD (Florian and Yarowsky, 2002). E0 models are also more amenable to theoretical analysis for setting thresholds, and hence for incorporating priors. Penalized likelihood methods which are widely used for feature selection minimize a score: Score = −2log(likelihood) + Fq (1) where F is a function designed to penalize model complexity, and q represents the number of features currently included in the model at a given point. The first term in the above equation represents a measure of the in-sample error given the model, while the second term is a model complexity penalty. As is obvious from Eq. 1, the description length of t</context>
</contexts>
<marker>Florian, Yarowsky, 2002</marker>
<rawString>R. Florian and D. Yarowsky. 2002. Modeling consensus: classifier combination for word sense disambiguation. In EMNLP ’02, pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D P Foster</author>
<author>E I George</author>
</authors>
<title>The risk inflation criterion for multiple regression. The Annals of Statistics,</title>
<date>1994</date>
<contexts>
<context position="8005" citStr="Foster and George, 1994" startWordPosition="1347" endWordPosition="1350">the number of bits for encoding the model, we need the bits to code the index of the feature (i.e., which feature from amongst the total m candidate features) and the bits to code the coefficient of this feature. The total cost can be represented as: SM=lf+le where lf is the cost to code the index of the feature and le is the number of bits required to code the coefficient of the selected feature. In our baseline feature selection algorithm, we code lf by using log(m) bits (where m is the total number of candidate features), which is equivalent to the standard RIC (or the Bonferroni penalty) (Foster and George, 1994) commonly used in information theory. The above coding scheme1 corresponds to putting a uniform prior over all the features; I.e., each feature is equally likely to get selected. For coding the coefficients of the selected feature we use 2 bits, which is quite similar to the AIC 1There is a duality between Information Theory and Bayesian terminology: If there is 1 probability of a fact being true, then we need −log( ) = log(k) bits to code it. 2�� E n= ( i w ) 2Q2 i 1 y − · i 258 (Akaike Information Criterion) (Rissanen, 1999). Our final equation for SM is therefore: SM = log(m) + 2 (2) 2.2 Ex</context>
</contexts>
<marker>Foster, George, 1994</marker>
<rawString>D. P. Foster and E. I. George. 1994. The risk inflation criterion for multiple regression. The Annals of Statistics, 22(4):1947–1975.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E H Hovy</author>
<author>M P Marcus</author>
<author>M Palmer</author>
<author>L A Ramshaw</author>
<author>R M Weischedel</author>
</authors>
<title>Ontonotes: The 90% solution.</title>
<date>2006</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="11696" citStr="Hovy et al., 2006" startWordPosition="1997" endWordPosition="2000">”) and puts all the remaining word senses in the “background”. The parameters that it takes as input are the % of data points to put in “background” (i.e., what would be the singleton clusters) and a similarity threshold which impacts the number of “foreground” clusters. We experimented with putting 20% and 33% data points in background and adjusted the similarity threshold to give us 50 − 100 “foreground” clusters. The results reported below have 20% background and 50 − 100 “foreground” clusters. 3.2 Description of Data and Results We performed our experiments on ONTONOTES data of 172 verbs (Hovy et al., 2006). The data consists of a rich set of linguistic features which have proven to be beneficial for WSD. A sample feature vector for the word “add”, given below, shows typical features. word_added pos_vbd morph_normal subj_use subjsyn_16993 dobj_money dobjsyn_16993 pos+1+2+3_rp+to+cd tp_account tp_accumulate tp_actual The 172 verbs each had between 1,000 and 10,000 nonzero features. The number of senses varied from 2 (For example, “add”) to 15 (For example, “turn”). We tested our transfer learning algorithm in three slightly varied settings to tease apart the contributions of different features to</context>
</contexts>
<marker>Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2006</marker>
<rawString>E. H. Hovy, M. P. Marcus, M. Palmer, L. A. Ramshaw, and R. M. Weischedel. 2006. Ontonotes: The 90% solution. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Kandylas</author>
<author>S P Upham</author>
<author>L H Ungar</author>
</authors>
<title>Finding cohesive clusters for analyzing knowledge communities.</title>
<date>2007</date>
<booktitle>In ICDM,</booktitle>
<pages>203--212</pages>
<contexts>
<context position="10822" citStr="Kandylas et al., 2007" startWordPosition="1854" endWordPosition="1857">esentation, as explained earlier. 3 Experimental Results In this section we present the experimental results of TRANSFEAT on ONTONOTES data. 3.1 Similarity Determination To determine which verbs to transfer from, we cluster verb senses into groups based on the TF/IDF similarity of the vector of features selected for that verb sense in the baseline (nontransfer learning) model. We use only those features that are positively correlated with the given sense; they are the features most closely associated with the given sense. We cluster senses using a “foreground-background” clustering algorithm (Kandylas et al., 2007) rather than the more common k-means clustering because many word senses are not sufficiently similar to any other word sense to warrant putting into a cluster. Foreground-background clustering gives highly cohesive clusters of word senses (the “foreground”) and puts all the remaining word senses in the “background”. The parameters that it takes as input are the % of data points to put in “background” (i.e., what would be the singleton clusters) and a similarity threshold which impacts the number of “foreground” clusters. We experimented with putting 20% and 33% data points in background and a</context>
</contexts>
<marker>Kandylas, Upham, Ungar, 2007</marker>
<rawString>V. Kandylas, S. P. Upham, and L. H. Ungar. 2007. Finding cohesive clusters for analyzing knowledge communities. In ICDM, pages 203–212.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Rissanen</author>
</authors>
<title>Hypothesis selection and testing by the mdl principle.</title>
<date>1999</date>
<journal>The Computer Journal,</journal>
<volume>42</volume>
<pages>269</pages>
<contexts>
<context position="4791" citStr="Rissanen, 1999" startWordPosition="780" endWordPosition="781">ures should be included in a model to disambiguate corresponding senses of the distributionally similar word “arrest”. The remainder of the paper is organized as follows. In Section 2 we describe our “baseline” information theoretic feature selection method, and extend it to our “TRANSFEAT” method. Section 3 contains experimental results comparing TRANSFEAT with the baseline and Ando[CoNLL’06] on ONTONOTES data. We conclude with a brief summary in Section 4. 2 Feature Selection for WSD We use an information theoretic approach to feature selection based on the Minimum Description Length (MDL) (Rissanen, 1999) principle, which makes it easy to incorporate information about feature relevance priors. These information theoretic models have a ‘dual’ Bayesian interpretation, which provides a clean setting for feature selection. 2.1 Information Theoretic Feature Selection The state-of-the-art feature selection methods in WSD use either an E0 or an E1 penalty on the coefficients. E1 penalty methods such as Lasso, being convex, can be solved by optimization and give guaranteed optimal solutions. On the other hand, E0 penalty methods, like stepwise feature selection, give approximate solutions but produce </context>
<context position="8537" citStr="Rissanen, 1999" startWordPosition="1448" endWordPosition="1449">quivalent to the standard RIC (or the Bonferroni penalty) (Foster and George, 1994) commonly used in information theory. The above coding scheme1 corresponds to putting a uniform prior over all the features; I.e., each feature is equally likely to get selected. For coding the coefficients of the selected feature we use 2 bits, which is quite similar to the AIC 1There is a duality between Information Theory and Bayesian terminology: If there is 1 probability of a fact being true, then we need −log( ) = log(k) bits to code it. 2�� E n= ( i w ) 2Q2 i 1 y − · i 258 (Akaike Information Criterion) (Rissanen, 1999). Our final equation for SM is therefore: SM = log(m) + 2 (2) 2.2 Extension to TRANSFEAT We now extend the baseline feature selection algorithm to include the feature relevance prior. We define a binary random variable fi E {0,1} that denotes the event of the ith feature being in or not being in the model for the test word sense. We can parameterize the distribution as p(fi = 1|θi) = θi. I.e., we have a Bernoulli Distribution over the features. Given the data for the ith feature for all the training word senses, we can write: Di = {fi1, ..., fiv, ..., fit}. We then construct the likelihood fun</context>
</contexts>
<marker>Rissanen, 1999</marker>
<rawString>J. Rissanen. 1999. Hypothesis selection and testing by the mdl principle. The Computer Journal, 42:260– 269.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>