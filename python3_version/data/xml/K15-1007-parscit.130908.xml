<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000041">
<title confidence="0.99843">
Analyzing Optimization for Statistical Machine Translation:
MERT Learns Verbosity, PRO Learns Length
</title>
<author confidence="0.941462">
Francisco Guzm´an Preslav Nakov and Stephan Vogel
</author>
<affiliation confidence="0.9321555">
ALT Research Group
Qatar Computing Research Institute, HBKU
</affiliation>
<email confidence="0.987597">
ffguzman,pnakov,svogell@qf.org.qa
</email>
<sectionHeader confidence="0.993626" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999944071428571">
We study the impact of source length and
verbosity of the tuning dataset on the per-
formance of parameter optimizers such as
MERT and PRO for statistical machine
translation. In particular, we test whether
the verbosity of the resulting translations
can be modified by varying the length
or the verbosity of the tuning sentences.
We find that MERT learns the tuning set
verbosity very well, while PRO is sensi-
tive to both the verbosity and the length
of the source sentences in the tuning set;
yet, overall PRO learns best from high-
verbosity tuning datasets.
Given these dependencies, and potentially
some other such as amount of reorder-
ing, number of unknown words, syntac-
tic complexity, and evaluation measure, to
mention just a few, we argue for the need
of controlled evaluation scenarios, so that
the selection of tuning set and optimiza-
tion strategy does not overshadow scien-
tific advances in modeling or decoding.
In the mean time, until we develop such
controlled scenarios, we recommend us-
ing PRO with a large verbosity tuning set,
which, in our experiments, yields highest
BLEU across datasets and language pairs.
</bodyText>
<sectionHeader confidence="0.999336" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999932326086957">
Statistical machine translation (SMT) systems
nowadays are complex and consist of many com-
ponents such as a translation model, a reorder-
ing model, a language model, etc., each of which
could have several sub-components. All compo-
nents and their elements work together to score
full and partial hypotheses proposed by the SMT
system’s search algorithms.
Thus, putting them together requires assigning
them relative weights, e.g., how much weight we
should give to the translation model vs. the lan-
guage model vs. the reordering table. These rela-
tive weights are typically learned discriminatively
in a log-linear framework, and their values are op-
timized to maximize some automatic metric, typi-
cally BLEU, on a tuning dataset.
Given this setup, it is clear that the choice of a
tuning set and its characteristics, can have signif-
icant impact on the SMT system’s performance:
if the experimental framework (training data, tun-
ing set, and test set) is highly consistent, i.e.,
there is close similarity in terms of genre, domain
and verbosity,1 then translation quality can be im-
proved by careful selection of tuning sentences
that exhibit high degree of similarity to the test set
(Zheng et al., 2010; Li et al., 2010).
In our recent work (Nakov et al., 2012), we have
studied the relationship between optimizers such
as MERT, PRO and MIRA, and we have pointed
out that PRO tends to generate relatively shorter
translations, which could lead to lower BLEU
scores on testing. Our solution there was to fix
the objective function being optimized: PRO uses
sentence-level smoothed BLEU+1, as opposed to
the standard dataset-level BLEU.
Here we are interested in a related but dif-
ferent question: the relationship between prop-
erties of the tuning dataset and the optimizer’s
performance. More specifically, we study how
the verbosity, i.e., the average target/source sen-
tence length ratio, learned by optimizers such as
MERT and PRO depends on the nature of the tun-
ing dataset. This could potentially allow us to ma-
nipulate the verbosity of the translation hypotheses
generated at test time by changing some character-
istics of the tuning dataset.
</bodyText>
<footnote confidence="0.979714666666667">
1Verbosity also depends on the translator; it is often a
stylistic choice. and not necessarily related to fluency or ade-
quacy. This aspect is beyond the scope of the present work.
</footnote>
<page confidence="0.977009">
62
</page>
<note confidence="0.9833985">
Proceedings of the 19th Conference on Computational Language Learning, pages 62–72,
Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.997423" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999872591836734">
Tuning the parameters of a log-linear model for
statistical machine translation is an active area of
research. The standard approach is to use mini-
mum error rate training, or MERT, (Och, 2003),
which optimizes BLEU directly.
Recently, there has been a surge in new opti-
mization techniques for SMT. Two parameter op-
timizers that have recently become popular in-
clude the margin-infused relaxed algorithm or
MIRA (Watanabe et al., 2007; Chiang et al.,
2008; Chiang et al., 2009), which is an on-line
sentence-level perceptron-like passive-aggressive
optimizer, and pairwise ranking optimization or
PRO (Hopkins and May, 2011), which operates in
batch mode and sees tuning as ranking.
A number of improved versions thereof have
been proposed in the literature including a batch
version of MIRA (Cherry and Foster, 2012), with
local updates (Liu et al., 2012), a linear regression
version of PRO (Bazrafshan et al., 2012), and a
non-sampling version of PRO (Dreyer and Dong,
2015); another example is Rampeon (Gimpel and
Smith, 2012). We refer the interested reader to
three recent overviews on parameter optimization
for SMT: (McAllester and Keshet, 2011; Cherry
and Foster, 2012; Gimpel and Smith, 2012).
Still, MERT remains the de-facto standard in
the statistical machine translation community. Its
stability has been of concern, and is widely stud-
ied. Suggestions to improve it include using
regularization (Cer et al., 2008), random restarts
(Moore and Quirk, 2008), multiple replications
(Clark et al., 2011), and parameter aggregation
(Cettolo et al., 2011).
With the emergence of new optimization tech-
niques there have been also studies that compare
stability between MIRA–MERT (Chiang et al.,
2008; Chiang et al., 2009; Cherry and Foster,
2012), PRO–MERT (Hopkins and May, 2011),
MIRA–PRO–MERT (Cherry and Foster, 2012;
Gimpel and Smith, 2012; Nakov et al., 2012).
Pathological verbosity was reported when using
MERT on recall-oriented metrics such as ME-
TEOR (Lavie and Denkowski, 2009; Denkowski
and Lavie, 2011), as well as large variance with
MIRA (Simianer et al., 2012). However, we are
not aware of any previous studies of the impact of
sentence length and dataset verbosity across opti-
mizers.
</bodyText>
<sectionHeader confidence="0.992169" genericHeader="method">
3 Method
</sectionHeader>
<bodyText confidence="0.999749">
For the following analysis, we need to define the
following four quantities:
</bodyText>
<listItem confidence="0.996169">
• source-side length: the number of words in
the source sentence;
• length ratio: the ratio of the number of words
in the output hypothesis to those in the refer-
ence;2
• verbosity: the ratio of the number of words in
the reference to those in the source;3
• hypothesis verbosity: the ratio of the num-
ber of words in the hypothesis to those in the
source.
</listItem>
<bodyText confidence="0.999798428571429">
Naturally, the verbosity varies across differ-
ent tuning/testing datasets, e.g., because of style,
translator choice, etc. Interestingly, verbosity can
also differ across sentences with different source
lengths drawn from the same dataset. This is illus-
trated in Figure 1, which plots the average sam-
ple source length vs. the average verbosity for
100 samples, each containing 500 randomly se-
lected sentence pairs, drawn from the concatena-
tion of the MT04, MT05, MT06, MT09 datasets
for Arabic-English and of newstest2008-2011 for
Spanish-English.4
We can see that for Arabic-English, the English
translations are longer than the Arabic source sen-
tences, i.e., the verbosity is greater than one. This
relationship is accentuated by length: verbosity in-
creases with sentence length: see the slightly pos-
itive slope of the regression line. Note that the
increasing verbosity can be observed in single-
reference sets (we used the first reference), and to
a lesser extent in multiple-reference sets (five ref-
erences for MT04 and MT05, and four for MT06
and MT09). For Spanish-English, the story is
different: here the English sentences tend to be
shorter than the Spanish ones, and the verbosity
decreases as the sentence length increases. Over-
all, in all three cases, the verbosity appears to be
length-dependent.
</bodyText>
<footnote confidence="0.999270888888889">
2For multi-reference sets, we use the length of the refer-
ence that is closest to the length of the hypothesis. This is
the best match length from the original paper on BLEU (Pap-
ineni et al., 2002); it is default in the NIST scoring tool v13a,
which we use in our experiments.
3When dealing with multi-reference sets, we use the aver-
age reference length.
4The datasets we experiment with are described in more
detail in Section 4 below.
</footnote>
<page confidence="0.998922">
63
</page>
<figure confidence="0.998338979166667">
Source length vs. avg. verbosity
Arabic−English Spanish−English
set ● Ar−En−multi Ar−En−single
● ●● A ●
● ●
● ● ● ● ●
● ● ● ● ● ●
● ● ● ● ● ● ●
● ● ●● ● ●
●●
● ● ● ● ● ● ●
● ● ● ●● ● ●
● ● ● ●
● ● ●●
●
●
● ● ● ●● ●
●
●
● ●
● ● ● ● ●
● ● ● ● ●
● ●
● ●●
● ● ● ● ●
● ● ● ● ●
● ● ● ●
● ● ● ● ● ● ● ●
● ● ● ● ● ● ● ●
● ●
● ● ● ●
● ● ●
● ● ● ● ●
● ● ● ● ●
1.175
Average verbosity
1.150
1.125
1.100
1.075
1.025
Average verbosity
1.000
0.975
0.950
0.925
28 30 32 24 26 28
Average source side length (words) Average source side length (words)
</figure>
<figureCaption confidence="0.983827">
Figure 1: Average source sentence length (x axis) vs. average verbosity (y axis) for 100 random samples,
each with 500 sentence pairs extracted from NIST (Left: Arabic-English, multi- and single-reference)
and from WMT (Right: Spanish-English, single-reference) data.
</figureCaption>
<bodyText confidence="0.9999363">
The main research question we are interested
in, and which we will explore in this paper, is
whether the SMT parameter optimizers are able
to learn the verbosity from the tuning set. We
are also interested in the question of how the hy-
pothesis verbosity learned by optimizers such as
MERT and PRO depends on the nature of the tun-
ing dataset, i.e., its verbosity. Understanding this
could potentially allow us to manipulate the hy-
pothesis verbosity of the translations generated at
test time simply by changing the characteristics of
the tuning dataset in a systematic and controlled
way. While controlling the verbosity of a tuning
set might be an appealing idea, this is unrealistic
in practice, given that the verbosity of a test set is
always unknown. However, the results in Figure 1
suggest that it is possible to manipulate verbosity
by controlling the average source sentence length
of the dataset (and the source-side length is always
known for any test set). Thus, in our study, we use
the source-side sentence length as a data selection
criterion; still, we also report results for selection
based on verbosity.
In order to shed some light on our initial ques-
tion (whether the SMT parameter optimizers are
able to learn the verbosity from the tuning dataset),
we contrast the verbosity that two different opti-
mizers, MERT and PRO, learn as a function of
the average length of the sentences in the tuning
dataset.5
</bodyText>
<footnote confidence="0.9985995">
5In this work, we consider both optimizers, MERT and
PRO, as black-boxes. For a detailed analysis of how their
inner workings can affect optimization, see our earlier work
(Nakov et al., 2012).
</footnote>
<sectionHeader confidence="0.993032" genericHeader="method">
4 Experiments and Evaluation
</sectionHeader>
<bodyText confidence="0.999913827586207">
We experimented with single-reference and multi-
reference tuning and testing datasets for two
language pairs: Spanish-English and Arabic-
English. For Spanish-English, we used the
single-reference datasets newstest2008, new-
stest2009, newstest2010, and newstest2011 from
the WMT 2012, Workshop on Machine Transla-
tion Evaluation.6 For Arabic-English, we used
the multi-reference datasets MT04, MT05, MT06,
and MT09 from the NIST 2012 OpenMT Eval-
uation;7 we further experimented with single-
reference versions of the MT0x datasets, using the
first reference only.
In addition to the above datasets, we con-
structed tuning sets of different source-side sen-
tence lengths: short, middle and long. Given an
original tuning dataset, we selected 50% of its sen-
tence pairs: shortest 50%, middle 50%, or longest
50%. This yielded tuning datasets with the same
number of sentence pairs but with different num-
ber of words, e.g., for our Arabic-English datasets,
longest has about twice as many English words
as middle, and about four times as many words
as shortest. Constructing tuning datasets with the
same number of sentences instead of the same
number of tokens is intentional as we wanted to
ensure that in each of the conditions, the SMT pa-
rameter optimizers learn on the same number of
training examples.
</bodyText>
<footnote confidence="0.999314">
6www.statmt.org/wmt12/
7www.nist.gov/itl/iad/mig/openmt12.cfm
</footnote>
<page confidence="0.998624">
64
</page>
<subsectionHeader confidence="0.97689">
4.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999974282051282">
We experimented with the phrase-based SMT
model (Koehn et al., 2003) as implemented in
Moses (Koehn et al., 2007). For Arabic-English,
we trained on all data that was allowed for use
in the NIST 2012 except for the UN corpus. For
Spanish-English, we used all WMT12 data, again
except for the UN data.
We tokenized and truecased the English and the
Spanish side of all bi-texts and also the monolin-
gual data for language modeling using the stan-
dard tokenizer of Moses. We segmented the words
on the Arabic side using the MADA ATB segmen-
tation scheme (Roth et al., 2008). We built our
phrase tables using the Moses pipeline with max-
phrase-length 7 and Kneser-Ney smoothing. We
also built a lexicalized reordering model (Koehn
et al., 2005): msd-bidirectional-fe. We used
a 5-gram language model trained on GigaWord
v.5 with Kneser-Ney smoothing using KenLM
(Heafield, 2011).
On tuning and testing, we dropped the unknown
words for Arabic-English, and we used monotone-
at-punctuation decoding for Spanish-English. We
tuned using MERT and PRO. We used the standard
implementation of MERT from the Moses toolkit,
and a fixed version of PRO, as we recommended
in (Nakov et al., 2013), which solves instability
issues when tuning on the long sentences; we will
discuss our PRO fix and the reasons it is needed in
Section 5 below. In order to ensure convergence,
we allowed both MERT and PRO to run for up to
25 iterations (default: 16); we further used 1000-
best lists (default: 100).
In our experiments below, we perform three re-
runs of parameter optimization, tuning on each of
the twelve tuning datasets; in the figures, we plot
the results of the three reruns, while in the tables,
we report BLEU averaged over the three reruns, as
suggested by Clark et al. (2011).
</bodyText>
<subsectionHeader confidence="0.999347">
4.2 Learning Verbosity
</subsectionHeader>
<bodyText confidence="0.9999824375">
We performed parameter optimization using
MERT and PRO on each dataset, and we used the
resulting parameters to translate the same dataset.
The purpose of this experiment was to study the
ability of the optimizers to learn the verbosity of
the tuning sets. Getting the hypothesis verbosity
right means that it is highly correlated with the
tuning set verbosity, which in turn is determined
by the dataset source length.
The results are shown in Figure 2. In each
graph, there are 36 points (many of them very
close and overlapping) since we performed three
reruns with our twelve tuning datasets (three
length-based subsets for each of the four original
tuning datasets). There are several observations
that we can make:
</bodyText>
<listItem confidence="0.949484391304348">
(1) MERT is fairly stable with respect to the
length of the input tuning sentences. Note how
the MERT regression lines imitate those in Fig-
ure 1. In fact, the correlation between the verbosity
and the hypothesis verbosity for MERT is r=0.980.
PRO, on the other hand, has harder time learning
the tuning set verbosity, and the correlation with
the hypothesis verbosity is only r=0.44. Interest-
ingly, its length ratio is more sensitive to the in-
put length (r=0.67): on short sentences, it learns
to output translations that are slightly shorter than
the reference, while on long sentences, it yields
increasingly longer translations. The dependence
of PRO on source length can be explained by the
sentence-level smoothing in BLEU+1 and the bro-
ken balance between BLEU’s precision compo-
nent and BP (Nakov et al., 2012). The problem is
bigger for short sentences since there +1 is added
to smaller counts; this results in preference for
shorter translations.
(2) Looking at the results for Arabic-English,
we observe that having multiple references makes
both MERT and PRO appear more stable, allowing
</listItem>
<bodyText confidence="0.823172727272727">
them to generate hypotheses that are less spread,
and closer to 1. This can be attributed to the best
match reference length, which naturally dampens
the effect of verbosity during optimization by se-
lecting the reference that is closest to the respec-
tive hypothesis.
Overall, we can conclude that MERT learns the
tuning set’s verbosity more accurately than PRO.
PRO learns verbosity that is more dependent on
the source side length of the sentences in the tun-
ing dataset.
</bodyText>
<subsectionHeader confidence="0.998324">
4.3 Performance on the Test Dataset
</subsectionHeader>
<bodyText confidence="0.999990875">
Next, we study the performance of MERT and
PRO when testing on datasets that are different
from the one used for tuning. First, we test the
robustness of the parameters obtained for spe-
cific tuning datasets when testing on various test
datasets. Second, we test whether selecting a
tuning dataset based on the length of the testing
dataset (i.e., closest) is a good strategy.
</bodyText>
<page confidence="0.996077">
65
</page>
<figure confidence="0.996879144927536">
Tuning set source side length vs. hypothesis verbosity
MERT PRO−fix
● ●
● ● ● ● ● ● ● ● ●
● ● ● ● ● ●
● ● ● ● ● ●
●
● ● ● ●
●
● ● ● ● ● ● ● ● ● ● ● ●
● ●
● ● ●
1.2
1.1
1.0
0.9
●
●
●
short
mid
long
10 20 30 40 10 20 30 40
NIST Ar−En (multi−ref) tuning set source side length (words)
MERT PRO−fix
● ● ● ● ●● ● ● ● ●
● ● ● ● ● ●
● ● ● ●
● ●
● ● ● ● ●●●● ●● ● ●
● ● ● ● ● ● ● ●
● ●
● ●●
10 20 30 40 10 20 30 40
NIST Ar−En (single−ref) tuning set source side length (words)
hypothesis verbosity
1.2
1.1
1.0
0.9
●
●
●
short
mid
long
MERT PRO−fix
● ● ● ● ● ●● ● ●
● ●
● ● ●
● ●
● ● ●● ●
● ● ● ●
● ● ●● ● ● ● ●
● ● ● ● ●
●
●
1.2
1.1
1.0
0.9
●
●
●
short
mid
long
10 20 30 40 10 20 30 40
WMT Es−En tuning set source side length (words)
</figure>
<figureCaption confidence="0.9910405">
Figure 2: Source-side length vs. hypothesis verbosity for the tuning dataset. There are 36 points per
language pair: four tuning sets, each split into three datasets (short, middle, and long) times three reruns.
</figureCaption>
<bodyText confidence="0.999402130434783">
For this purpose, we perform a grid comparison
of tuning and testing on all our datasets: we tune
on each short/middle/long dataset, and we test on
the remaining short/middle/long datasets.
The results are shown in Table 1, where each
cell is an average over 36 BLEU scores: four
tuning sets times three test sets times three re-
runs. For example, 49.63 in row 1 (tune: short),
column 2 (test: middle), corresponds to the av-
erage over three reruns of (i) tune on MT04-
short and test on MT05-middle, MT06-middle,
and MT09-middle, (ii) tune on MT05-short and
test on MT04-middle, MT06-middle, and MT09-
middle, (iii) tune on MT06-short and test on
MT04-middle, MT05-middle, and MT09-middle,
and (iv) tune on MT09-short and test on MT04-
middle, MT05-middle, and MT06-middle. We
further include two statistics: (1) the range of
values (max-min), measuring test BLEU variance
depending on the tuning set, and (2) the loss in
BLEU when tuning on closest instead of on the
best-performing dataset.
There are several interesting observations:
</bodyText>
<listItem confidence="0.879175333333333">
(1) PRO and MERT behave quite differently
with respect to the input tuning set. For MERT,
tuning on a specific length condition yields the
best results when testing on a similar condition,
i.e., zero-loss. This is a satisfactory result since it
confirms the common wisdom that tuning datasets
should be as similar as possible to test-time input
in terms of source side length. In contrast, PRO
behaves better when tuning on mid-length tuning
sets. However, the average loss incurred by apply-
ing the closest strategy with PRO is rather small,
and in practice, choosing a tuning set based on test
set’s average length is a good strategy.
(2) MERT has higher variance than PRO and
fluctuates more depending on the input tuning set.
PRO on the contrary, tends to perform more con-
sistently, regardless of the length of the tuning set.
(3) MERT yields the best BLEU across datasets
and language pairs. Thus, when several tuning sets
are available, we recommend choosing the one
closest in length to the test set and using MERT.
</listItem>
<page confidence="0.936018">
66
</page>
<table confidence="0.9997832">
test
Arabic-English (multi-ref) Arabic-English (1-ref) WMT Spanish-English
tuning short mid long short mid long short mid long avg
MERT 47.26* 50.71 50.82 26.69* 28.14 27.49 25.17* 25.94 27.64
short
mid 46.53 51.11* 51.31 26.22 28.39* 27.96 24.96 26.27* 27.97
long 46.23 50.84 51.74* 25.80 28.20 28.27* 24.57 26.08 28.29*
max-min 1.04 0.40 0.91 0.89 0.25 0.78 0.59 0.34 0.65 0.65
loss if using closest 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
PRO-fix 46.74 50.57 50.97 25.95 27.66 27.28 24.66 25.83 27.89
short
mid 46.59 50.83 51.41 25.98 28.23 28.19 24.67 25.81 27.64
long 46.08 50.56 51.18 25.87 28.11 28.05 24.58 25.77 27.81
max-min 0.66 0.27 0.44 0.11 0.58 0.92 0.09 0.06 0.25 0.38
loss if using closest 0.00 0.00 0.23 0.02 0.00 0.15 0.01 0.00 0.08 0.06
</table>
<tableCaption confidence="0.829653333333333">
Table 1: Average test BLEU when tuning on each short/mid/long dataset, and testing on the remaining
short/mid/long datasets. Each cell represents the average over 36 scores (see the text). The best score for
either MERT or PRO is bold; the best overall score is marked with a ∗.
</tableCaption>
<subsubsectionHeader confidence="0.664867">
4.3.1 Performance vs. Length and Verbosity
</subsubsectionHeader>
<bodyText confidence="0.9999703125">
The above results give rise to some interesting
questions: What if we do not know the source-
side length of the test set? What if we can choose
a tuning set based on its verbosity? Would it then
be better to choose based on length or based on
verbosity?
To answer these questions, we analyzed the av-
erage results according to two orthogonal views:
one based on the tuning set length (using the above
50% length-based subsets of tuning: short, mid,
long), and another one based on the tuning set
verbosity (using new 50% subsets verbosity-based
subsets of tuning: low-verb, mid-verb, high-verb).
This time, we translated the full test datasets (e.g.,
MT06, MT09); the results are shown in Table 2.
We can make the following observations:
</bodyText>
<listItem confidence="0.909952636363636">
(1) The best results for PRO are better than the
best results for MERT, in all conditions.
(2) Length-based tuning subsets: With a sin-
gle reference, PRO performs best when tuning on
short sentences, but with multiple references, it
works best with mid-length sentences. MERT, on
the other hand, prefers tuning on long sentences
for all testing datasets.
(3) Verbosity-based tuning subsets: PRO yields
best results when the tuning sets have high ver-
bosity; in fact, the best verbosity-based results in
</listItem>
<bodyText confidence="0.940519">
the table are obtained with this setting. With mul-
tiple references, MERT performs best when tuning
on high-verbosity datasets; however, with a single
reference, it prefers mid-verbosity.
Based on the above results, we recommend that,
whenever we have no access to the input side of
the testing dataset beforehand, we should tune on
datasets with high verbosity.
</bodyText>
<subsectionHeader confidence="0.9961635">
4.4 Test vs. Tuning Verbosity and Source
Length
</subsectionHeader>
<bodyText confidence="0.999538583333333">
In the previous subsection, we have seen that
MERT and PRO perform differently in terms of
BLEU, depending on the characteristics of the tun-
ing dataset. Here, we study a different aspect:
i.e. how they behave with respect to verbosity and
source side length.
We have seen that MERT and PRO perform dif-
ferently in terms of BLEU depending on the char-
acteristics of the tuning dataset. Below we study
how other characteristics of the output of PRO and
MERT are affected by tuning set verbosity and
source side length.
</bodyText>
<subsectionHeader confidence="0.77315">
4.4.1 MERT – Sensitive to Verbosity
</subsectionHeader>
<bodyText confidence="0.99989">
Figure 3 shows a scatter plot of tuning verbosity
vs. test hypothesis verbosity when using MERT
to tune under different conditions, and testing on
each of the unseen full datasets. We test on full
datasets to avoid the verbosity bias that might oc-
cur for specific conditions (see Section 3).
We can see strong positive correlation between
the tuning set verbosity and the hypothesis ver-
bosity on the test datasets. The average correla-
tion for Arabic-English is r=0.95 with multiple
references and r=0.98 with a single reference; for
Spanish-English, it is r=0.97.
</bodyText>
<page confidence="0.999277">
67
</page>
<table confidence="0.999623636363636">
test
Arabic-English (multi-ref) Arabic-English (1-ref) WMT Spanish-English
tuning MERT PRO-fix MERT PRO-fix MERT PRO-fix
length
short 48.71 49.12 26.74 27.35 26.79 27.07
mid 49.27 49.59 26.97 27.23 26.99 26.88
long 49.35 49.20 27.23 27.28 27.02 26.84
verbosity
low-verb 47.90 47.60 25.89 25.88 26.70 26.61
mid-verb 49.16 49.52 27.69 27.95 27.09 26.81
high-verb 50.28 50.79∗ 27.36 28.03∗ 27.01 27.38∗
</table>
<tableCaption confidence="0.981585666666667">
Table 2: Average test BLEU scores when tuning on different length- and verbosity-based datasets, and
testing on the remaining full datasets. Each cell represents the average over 36 scores. The best score for
either MERT or PRO is bold; the best overall score is marked with a ∗.
</tableCaption>
<figure confidence="0.625485818181818">
MERT−tuning verbosity vs. test set length ratio
Ar−En−multi Ar−En−single Es−En
●
● ● ●
● ●
●
● ●
● ●
● ● ● ● ● ● ● ●
● ● ● ● ● ● ● ● ●
● ● ● ●
● ●
●
● ● ● ●● ● ● ● ●
● ● ●
● ● ●
● ● ● ● ●
● ● ● ● ● ● ●
● ● ● ● ●●
●
0.95 1.00 1.05 1.10 1.15 0.95 1.00 1.05 1.10 1.15 0.95 1.00 1.05 1.10 1.15
Tuning set verbosity
</figure>
<figureCaption confidence="0.844870666666667">
Figure 3: Tuning set verbosity vs. test hypothesis verbosity when using MERT. Each point represents the
result for an unseen testing dataset, given a specific tuning condition. The linear regressions show the
tendencies for each of the test datasets (note that they all overlap for Es-En and look like a single line).
</figureCaption>
<figure confidence="0.97935125">
Test−set hypothesis verbosity
1.2
1.1
1.0
</figure>
<bodyText confidence="0.99977575">
These are very strong positive correlations and
they show that MERT tends to learn SMT pa-
rameters that yield translations preserving the ver-
bosity, e.g., lower verbosity on the tuning dataset
will yield test-time translations that are less ver-
bose, while higher verbosity on the tuning dataset
will yield test-time translations that are more ver-
bose. In other words, MERT learns to generate
a fixed number of words per input word. This
can be explained by the fact that MERT optimizes
BLEU score directly, and thus learns to output the
“right” verbosity on the tuning dataset (in contrast,
PRO optimizes sentence-level BLEU+1, which is
an approximation to BLEU, but it is not the actual
BLEU). This explains why MERT performs best
when the tuning conditions and the testing condi-
tions are in sync. Yet, this makes it dependent on
a parameter that we do not necessarily control or
have access to beforehand: the length of the test
references.
</bodyText>
<subsectionHeader confidence="0.705731">
4.4.2 PRO – Sensitive to Source Length
</subsectionHeader>
<bodyText confidence="0.999667052631579">
Figure 4 shows the tuning set average source-side
length vs. the testing hypothesis/reference length
ratio when using PRO to tune on short, middle,
and long and testing on each of the unseen full
datasets, as in the previous subsection. We can see
that there is positive correlation between the tun-
ing set average source side length and the testing
hypothesis/reference length ratio. For Spanish-
English, it is quite strong (r=0.64), and for Arabic-
English, it is more clearly expressed with one
(r=0.42) than with multiple references (r=0.34).
The correlation is significant (p &lt; 0.001) when
we take into account the contribution of the tuning
set verbosity in the model. This suggests that for
PRO, both source length and verbosity influence
the hypotheses lengths, i.e., PRO learns the tuning
set’s verbosity, much like MERT; yet, the contri-
bution of the length of the source sentences from
the tuning dataset is not negligible.
</bodyText>
<page confidence="0.999065">
68
</page>
<table confidence="0.974425394736842">
PRO−tuning source side length vs. test set length ratio
Ar−En−multi Ar−En−single Es−En
● ● ● ● ● ● ● ●
● ● ●
● ● ● ●
● ● ● ● ●
● ● ●
● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●
● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ●
● ● ● ● ● ● ● ● ●
● ● ● ● ● ●
● ● ● ● ● ● ● ●●
● ● ● ● ● ●
● ● ● ● ●●
●
●
● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ●●
● ● ● ● ● ● ● ● ● ● ● ● ● ●● ●
● ● ● ● ● ●● ● ● ● ● ●
● ● ●● ● ● ● ● ● ● ●
● ● ● ● ● ● ●
● ● ● ●
● ● ●
●
●
● ●
● ●
● ●● ● ● ● ● ● ● ●
● ● ● ● ●
● ●
● ● ●
● ●
● ● ● ● ● ● ●
● ● ● ●
● ●
●●
10 20 30 40 10 20 30 40 10 20 30 40
Tuning set source sentence length
</table>
<figureCaption confidence="0.952646333333333">
Figure 4: Tuning set average source length vs. test hypothesis/reference length ratio for PRO. Each
point represents the result for an unseen testing dataset, given a specific tuning condition. The linear
regressions shows the tendencies across each of the testing datasets.
</figureCaption>
<figure confidence="0.97598275">
1.1
Test−set length ratio
1.0
0.9
</figure>
<bodyText confidence="0.9969948">
Finally, note the “stratification” effect for the
Arabic-English single-reference data. We attribute
it to the differences across test datasets. These
differences are attenuated with multiple references
due to the closest-match reference length.
</bodyText>
<sectionHeader confidence="0.998059" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999951666666667">
We have observed that high-verbosity tuning sets
yield better results with PRO. We have further seen
that we can manipulate verbosity by adjusting the
average length of the tuning dataset. This leads to
the natural question: can this yield better BLEU?
It turns out that the answer is “yes”. Below, we
present an example that makes this evident.
First, recall that for Arabic-English longer tun-
ing datasets have higher verbosity. Moreover, our
previous findings suggest that for PRO, higher-
verbosity tuning datasets will perform better in
this situation. Therefore, we should expect that
longer tuning datasets could yield better BLEU.
Table 3 presents the results for PRO with Arabic-
English when tuning on MT06, or subsets thereof,
and testing on MT09. The table shows the re-
sults for both multi- and single-reference experi-
ments; naturally, manipulating the tuning set has
stronger effect with a single reference. Lines 1-
3 show that as the average length of the tuning
dataset increases, so does the length ratio, which
means better brevity penalty for BLEU and thus
higher BLEU score. Line 4 shows that selecting
a random-50% subset (included here to show the
effect of using mixed-length sentences) yields re-
sults that are very close to those for middle.
Comparing line 3 to lines 4 and 5, we can see
that tuning on long yields longer translations and
also higher BLEU, compared to tuning on the full
dataset or on random.
Next, lines 6 and 7 show the results when apply-
ing our smoothing fix for sentence-level BLEU+1
(Nakov et al., 2012), which prevents translations
from becoming too short; we can see that long
yields very comparable results. Yet, manipulat-
ing the tuning dataset might be preferable since it
allows (i) faster tuning, by using part of the tun-
ing dataset, (ii) flexibility in the selection of the
desired verbosity, and (iii) applicability to other
MT evaluation measures. Point (ii) is illustrated
on Figure 5, which shows that there is direct pos-
itive correlation between verbosity, length ratio,
and BLEU; note that the tuning set size does not
matter much: in fact, better results are obtained
when using less tuning data.
</bodyText>
<figure confidence="0.9525905">
30.0
29.5
29.0
28.5
</figure>
<page confidence="0.716661">
69
</page>
<table confidence="0.992193777777778">
multi-ref 1-ref
BLEU len. ratio BLEU len. ratio
46.38 0.961 27.44 0.894
47.44 0.977 29.11 0.950
47.47 0.980 29.51 0.969
47.43 0.978 28.96 0.941
47.18 0.972 28.88 0.934
47.52 0.984 29.43 0.962
47.61 0.991 29.68 0.979
</table>
<figure confidence="0.897853125">
Tuning
1 tune-short
2 tune-mid
3 tune-long
4 tune-random
5 tune-full
6 tune-full, BP-smooth=1
7 tune-full, BP-smooth=1, grounded
</figure>
<tableCaption confidence="0.9854835">
Table 3: PRO, Arabic-English: tuning on MT06, or subsets thereof, and testing on MT09. Statistically
significant improvements over tune-full are in bold: using the sign test (Collins et al., 2005), p &lt; 0.05.
</tableCaption>
<sectionHeader confidence="0.962609" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.99992715625">
Machine translation has, and continues to, benefit
immensely from automatic evaluation measures.
However, we frequently observe delicate depen-
dencies between the evaluation metric, the system
optimization strategy, and the pairing of tuning
and test datasets. This leaves us with the situation
that getting lucky in the selection of tuning datasets
and optimization strategy overshadows scientific
advances in modeling or decoding. Understand-
ing these dependencies in detail puts us in a bet-
ter position to construct tuning sets that match the
test datasets in such a way that improvements in
models, training, and decoding algorithms can be
measured more reliably.
To this end, we have studied the impact that
source-side length and verbosity of tuning sets
have on the performance of the translation system
when tuning the system with different optimizers
such as MERT and PRO. We observed that MERT
learns the verbosity of the tuning dataset very well,
but this can be a disadvantage because we do not
know the verbosity of unseen test sentences. In
contrast, PRO is affected by both the verbosity and
the source-side length of the tuning dataset.
There may be other characteristics of test
datasets, e.g., amount of reordering, number of
unknown words, complexity of the sentences in
terms of syntactic structure, etc. that could have
similar effects of creating good or bad luck when
deciding how to tune an SMT system. Until
we have such controlled evaluation scenarios, our
short-term recommendations are as follows:
</bodyText>
<listItem confidence="0.9754874">
• Know your tuning datasets: Different lan-
guage pairs and translation directions may
have different source-side length – verbosity
dependencies.
• When optimizing with PRO: select or con-
</listItem>
<bodyText confidence="0.906185125">
struct a high-verbosity dataset as this could
potentially compensate for PROs tendency
to yield too short translations. Note that
for Arabic-English, higher verbosity means
longer tuning sentences, while for Spanish-
English, it means shorter ones; translation di-
rection might matter too.
• When optimizing with MERT: If you know
beforehand the test set, select the closest tun-
ing set. Otherwise, tune on longer sentences.
We plan to extend this study in a number of di-
rections. First, we would like to include other pa-
rameter optimizers such as Rampeon (Gimpel and
Smith, 2012) and MIRA. Second, we want to ex-
periment with other metrics, such as TER (Snover
et al., 2006), which typically yields short trans-
lations, and METEOR (Lavie and Denkowski,
2009), which yields too long translations. Third,
we would like to explore other SMT models such
as hierarchical (Chiang, 2005) and syntax-based
(Galley et al., 2004; Quirk et al., 2005), and other
decoders such as cdec (Dyer et al., 2010), Joshua
(Li et al., 2009), and Jane (Vilar et al., 2010).
A long-term objective would be to design a met-
ric that measures the closeness between tuning and
test datasets, which includes the different char-
acteristics, such as length distribution, verbosity
distribution, syntactic complexity, etc., to guaran-
tee a more stable evaluation situations, but which
would also allow to systematically test the robust-
ness of translation systems, when deviating from
the matching conditions.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99985">
We would like to thank the anonymous review-
ers for their constructive comments, which have
helped us to improve the paper.
</bodyText>
<page confidence="0.997023">
70
</page>
<sectionHeader confidence="0.979261" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998499">
Marzieh Bazrafshan, Tagyoung Chung, and Daniel
Gildea. 2012. Tuning as linear regression. In
Proceedings of the 2012 Annual Conference of
the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, NAACL-HLT ’12, pages 543–547,
Montr´eal, Canada.
Daniel Cer, Daniel Jurafsky, and Christopher D Man-
ning. 2008. Regularization and search for min-
imum error rate training. In Proceedings of the
Third Workshop on Statistical Machine Translation,
WMT ’08, pages 26–34, Columbus, Ohio, USA.
Mauro Cettolo, Nicola Bertoldi, and Marcello Fed-
erico. 2011. Methods for smoothing the optimizer
instability in SMT. In Proceedings of the Thir-
teenth Machine Translation Summit, MT Summit
XIII, pages 32–39, Xiamen, China.
Colin Cherry and George Foster. 2012. Batch
tuning strategies for statistical machine translation.
In Proceedings of the 2012 Annual Conference of
the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, NAACL-HLT ’12, pages 427–436,
Montr´eal, Canada.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of the
2008 Conference on Empirical Methods in Natural
Language Processing, EMNLP ’08, pages 224–233,
Honolulu, Hawaii, USA.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine transla-
tion. In Proceedings of the 2009 Annual Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, NAACL-HLT ’09, pages 218–
226, Boulder, Colorado, USA.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics, ACL ’05, pages
263–270, Ann Arbor, Michigan, USA.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing for
statistical machine translation: Controlling for op-
timizer instability. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, ACL-
HLT ’11, pages 176–181, Portland, Oregon, USA.
Michael Collins, Philipp Koehn, and Ivona Kuˇcerov´a.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual
Meeting on Association for Computational Linguis-
tics, ACL ’05, pages 531–540, Ann Arbor, Michi-
gan.
Michael Denkowski and Alon Lavie. 2011. Meteor-
tuned phrase-based SMT: CMU French-English and
Haitian-English systems for WMT 2011. Techni-
cal report, Technical Report CMU-LTI-11-011, Lan-
guage Technologies Institute, Carnegie Mellon Uni-
versity.
Markus Dreyer and Yuanzhe Dong. 2015. APRO:
All-pairs ranking optimization for MT tuning. In
Proceedings of the 2015 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
NAACL-HLT ’15, pages 1018–1023, Denver, Col-
orado, USA.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proceedings of the ACL 2010 System Demonstra-
tions, ACL ’10, pages 7–12, Uppsala, Sweden.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What’s in a translation
rule? In Proceedings of the 2004 Annual Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, NAACL-HLT ’04, pages 273–
280, Boston, Massachusetts, USA.
Kevin Gimpel and Noah A. Smith. 2012. Struc-
tured ramp loss minimization for machine transla-
tion. In Proceedings of the 2012 Annual Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, NAACL-HLT ’12, pages 221–
231, Montr´eal, Canada.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of the
Sixth Workshop on Statistical Machine Translation,
WMT ’11, Edinburgh, United Kingdom.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the 2011 Conference
on Empirical Methods in Natural Language Pro-
cessing, EMNLP ’11, pages 1352–1362, Edinburgh,
Scotland, United Kingdom.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology (Vol-
ume 1), HLT-NAACL ’03, pages 48–54, Edmonton,
Canada.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system descrip-
tion for the 2005 IWSLT speech translation evalu-
ation. In Proceedings of the International Workshop
on Spoken Language Translation, IWSLT ’05, Pitts-
burgh, Pennsylvania, USA.
</reference>
<page confidence="0.979833">
71
</page>
<reference confidence="0.999863930434782">
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation for Computational Linguistics (Demonstra-
tion session)., ACL ’07, pages 177–180, Prague,
Czech Republic.
Alon Lavie and Michael J. Denkowski. 2009. The ME-
TEOR metric for automatic evaluation of machine
translation. Machine Translation, 23:105–115.
Zhifei Li, Chris Callison-Burch, Chris Dyer, San-
jeev Khudanpur, Lane Schwartz, Wren Thornton,
Jonathan Weese, and Omar Zaidan. 2009. Joshua:
An open source toolkit for parsing-based machine
translation. In Proceedings of the Fourth Workshop
on Statistical Machine Translation, WMT ’09, pages
135–139, Athens, Greece.
Mu Li, Yinggong Zhao, Dongdong Zhang, and Ming
Zhou. 2010. Adaptive development data selec-
tion for log-linear model in statistical machine trans-
lation. In Proceedings of the 23rd International
Conference on Computational Linguistics, COL-
ING ’10, pages 662–670, Beijing, China.
Lemao Liu, Hailong Cao, Taro Watanabe, Tiejun Zhao,
Mo Yu, and Conghui Zhu. 2012. Locally train-
ing the log-linear model for SMT. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, EMNLP-CoNLL ’12,
pages 402–411, Jeju Island, Korea.
David McAllester and Joseph Keshet. 2011. Gen-
eralization bounds and consistency for latent struc-
tural probit and ramp loss. In J. Shawe-Taylor,
R.S. Zemel, P. Bartlett, F.C.N. Pereira, and K.Q.
Weinberger, editors, Advances in Neural Informa-
tion Processing Systems 24, NIPS ’11, pages 2205–
2212, Granada, Spain.
Robert C Moore and Chris Quirk. 2008. Random
restarts in minimum error rate training for statis-
tical machine translation. In Proceedings of the
22nd International Conference on Computational
Linguistics-Volume 1, COLING ’08, pages 585–592,
Manchester, United Kingdom.
Preslav Nakov, Francisco Guzm´an, and Stephan Vo-
gel. 2012. Optimizing for sentence-level BLEU+1
yields short translations. In Proceedings of the
24th International Conference on Computational
Linguistics, COLING ’12, pages 1979–1994, Mum-
bai, India.
Preslav Nakov, Francisco Guzm´an, and Stephan Vogel.
2013. A tale about PRO and monsters. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), ACL ’13, pages 12–17, Sofia, Bulgaria.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, ACL ’03, pages 160–167, Sap-
poro, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics, ACL ’02, pages 311–
318, Philadelphia, Pennsylvania, USA.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: Syntactically in-
formed phrasal smt. In Proceedings of the 43rd
Annual Meeting on Association for Computational
Linguistics, ACL ’05, pages 271–279, Ann Arbor,
Michigan, USA.
Ryan Roth, Owen Rambow, Nizar Habash, Mona Diab,
and Cynthia Rudin. 2008. Arabic morphological
tagging, diacritization, and lemmatization using lex-
eme models and feature ranking. In Proceedings of
the 46th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, ACL ’08, pages 117–120, Columbus, Ohio,
USA.
Patrick Simianer, Stefan Riezler, and Chris Dyer. 2012.
Joint feature selection in distributed stochastic learn-
ing for large-scale discriminative training in SMT.
In Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics: Long Pa-
pers - Volume 1, ACL ’12, pages 11–21, Jeju Island,
Korea.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the 7th Biennial Conference of the
Association for Machine Translation in the Ameri-
cas, AMTA ’06, pages 223–231, Cambridge, Mas-
sachusetts, USA.
David Vilar, Daniel Stein, Matthias Huck, and Her-
mann Ney. 2010. Jane: Open source hierarchi-
cal translation, extended with reordering and lexi-
con models. In Proceedings of the 5th Workshop on
Statistical Machine Translation and MetricsMATR,
WMT ’10, pages 262–270, Uppsala, Sweden.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and
Hideki Isozaki. 2007. Online large-margin training
for statistical machine translation. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, EMNLP-CoNLL ’07,
pages 764–773, Prague, Czech Republic.
Zhongguang Zheng, Zhongjun He, Yao Meng, and Hao
Yu. 2010. Domain adaptation for statistical ma-
chine translation in development corpus selection.
In Proceedings of the 4th International Universal
Communication Symposium, IUCS ’10, pages 2–7,
Beijing, China.
</reference>
<page confidence="0.998724">
72
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.874978">
<title confidence="0.986652">Analyzing Optimization for Statistical Machine MERT Learns Verbosity, PRO Learns Length</title>
<author confidence="0.96122">Guzm´an Preslav Nakov</author>
<affiliation confidence="0.9896635">ALT Research Qatar Computing Research Institute,</affiliation>
<email confidence="0.973075">ffguzman,pnakov,svogell@qf.org.qa</email>
<abstract confidence="0.99885675862069">We study the impact of source length and verbosity of the tuning dataset on the performance of parameter optimizers such as MERT and PRO for statistical machine translation. In particular, we test whether the verbosity of the resulting translations can be modified by varying the length or the verbosity of the tuning sentences. We find that MERT learns the tuning set verbosity very well, while PRO is sensitive to both the verbosity and the length of the source sentences in the tuning set; yet, overall PRO learns best from highverbosity tuning datasets. Given these dependencies, and potentially some other such as amount of reordering, number of unknown words, syntactic complexity, and evaluation measure, to mention just a few, we argue for the need of controlled evaluation scenarios, so that the selection of tuning set and optimization strategy does not overshadow scientific advances in modeling or decoding. In the mean time, until we develop such controlled scenarios, we recommend using PRO with a large verbosity tuning set, which, in our experiments, yields highest BLEU across datasets and language pairs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marzieh Bazrafshan</author>
<author>Tagyoung Chung</author>
<author>Daniel Gildea</author>
</authors>
<title>Tuning as linear regression.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT ’12,</booktitle>
<pages>543--547</pages>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="4814" citStr="Bazrafshan et al., 2012" startWordPosition="761" endWordPosition="764"> SMT. Two parameter optimizers that have recently become popular include the margin-infused relaxed algorithm or MIRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009), which is an on-line sentence-level perceptron-like passive-aggressive optimizer, and pairwise ranking optimization or PRO (Hopkins and May, 2011), which operates in batch mode and sees tuning as ranking. A number of improved versions thereof have been proposed in the literature including a batch version of MIRA (Cherry and Foster, 2012), with local updates (Liu et al., 2012), a linear regression version of PRO (Bazrafshan et al., 2012), and a non-sampling version of PRO (Dreyer and Dong, 2015); another example is Rampeon (Gimpel and Smith, 2012). We refer the interested reader to three recent overviews on parameter optimization for SMT: (McAllester and Keshet, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012). Still, MERT remains the de-facto standard in the statistical machine translation community. Its stability has been of concern, and is widely studied. Suggestions to improve it include using regularization (Cer et al., 2008), random restarts (Moore and Quirk, 2008), multiple replications (Clark et al., 2011), and </context>
</contexts>
<marker>Bazrafshan, Chung, Gildea, 2012</marker>
<rawString>Marzieh Bazrafshan, Tagyoung Chung, and Daniel Gildea. 2012. Tuning as linear regression. In Proceedings of the 2012 Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT ’12, pages 543–547, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Cer</author>
<author>Daniel Jurafsky</author>
<author>Christopher D Manning</author>
</authors>
<title>Regularization and search for minimum error rate training.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation, WMT ’08,</booktitle>
<pages>26--34</pages>
<location>Columbus, Ohio, USA.</location>
<contexts>
<context position="5323" citStr="Cer et al., 2008" startWordPosition="839" endWordPosition="842">, 2012), with local updates (Liu et al., 2012), a linear regression version of PRO (Bazrafshan et al., 2012), and a non-sampling version of PRO (Dreyer and Dong, 2015); another example is Rampeon (Gimpel and Smith, 2012). We refer the interested reader to three recent overviews on parameter optimization for SMT: (McAllester and Keshet, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012). Still, MERT remains the de-facto standard in the statistical machine translation community. Its stability has been of concern, and is widely studied. Suggestions to improve it include using regularization (Cer et al., 2008), random restarts (Moore and Quirk, 2008), multiple replications (Clark et al., 2011), and parameter aggregation (Cettolo et al., 2011). With the emergence of new optimization techniques there have been also studies that compare stability between MIRA–MERT (Chiang et al., 2008; Chiang et al., 2009; Cherry and Foster, 2012), PRO–MERT (Hopkins and May, 2011), MIRA–PRO–MERT (Cherry and Foster, 2012; Gimpel and Smith, 2012; Nakov et al., 2012). Pathological verbosity was reported when using MERT on recall-oriented metrics such as METEOR (Lavie and Denkowski, 2009; Denkowski and Lavie, 2011), as we</context>
</contexts>
<marker>Cer, Jurafsky, Manning, 2008</marker>
<rawString>Daniel Cer, Daniel Jurafsky, and Christopher D Manning. 2008. Regularization and search for minimum error rate training. In Proceedings of the Third Workshop on Statistical Machine Translation, WMT ’08, pages 26–34, Columbus, Ohio, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mauro Cettolo</author>
<author>Nicola Bertoldi</author>
<author>Marcello Federico</author>
</authors>
<title>Methods for smoothing the optimizer instability in SMT.</title>
<date>2011</date>
<booktitle>In Proceedings of the Thirteenth Machine Translation Summit, MT Summit XIII,</booktitle>
<pages>32--39</pages>
<location>Xiamen, China.</location>
<contexts>
<context position="5458" citStr="Cettolo et al., 2011" startWordPosition="858" endWordPosition="861">rsion of PRO (Dreyer and Dong, 2015); another example is Rampeon (Gimpel and Smith, 2012). We refer the interested reader to three recent overviews on parameter optimization for SMT: (McAllester and Keshet, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012). Still, MERT remains the de-facto standard in the statistical machine translation community. Its stability has been of concern, and is widely studied. Suggestions to improve it include using regularization (Cer et al., 2008), random restarts (Moore and Quirk, 2008), multiple replications (Clark et al., 2011), and parameter aggregation (Cettolo et al., 2011). With the emergence of new optimization techniques there have been also studies that compare stability between MIRA–MERT (Chiang et al., 2008; Chiang et al., 2009; Cherry and Foster, 2012), PRO–MERT (Hopkins and May, 2011), MIRA–PRO–MERT (Cherry and Foster, 2012; Gimpel and Smith, 2012; Nakov et al., 2012). Pathological verbosity was reported when using MERT on recall-oriented metrics such as METEOR (Lavie and Denkowski, 2009; Denkowski and Lavie, 2011), as well as large variance with MIRA (Simianer et al., 2012). However, we are not aware of any previous studies of the impact of sentence len</context>
</contexts>
<marker>Cettolo, Bertoldi, Federico, 2011</marker>
<rawString>Mauro Cettolo, Nicola Bertoldi, and Marcello Federico. 2011. Methods for smoothing the optimizer instability in SMT. In Proceedings of the Thirteenth Machine Translation Summit, MT Summit XIII, pages 32–39, Xiamen, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>George Foster</author>
</authors>
<title>Batch tuning strategies for statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT ’12,</booktitle>
<pages>427--436</pages>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="4713" citStr="Cherry and Foster, 2012" startWordPosition="744" endWordPosition="747">), which optimizes BLEU directly. Recently, there has been a surge in new optimization techniques for SMT. Two parameter optimizers that have recently become popular include the margin-infused relaxed algorithm or MIRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009), which is an on-line sentence-level perceptron-like passive-aggressive optimizer, and pairwise ranking optimization or PRO (Hopkins and May, 2011), which operates in batch mode and sees tuning as ranking. A number of improved versions thereof have been proposed in the literature including a batch version of MIRA (Cherry and Foster, 2012), with local updates (Liu et al., 2012), a linear regression version of PRO (Bazrafshan et al., 2012), and a non-sampling version of PRO (Dreyer and Dong, 2015); another example is Rampeon (Gimpel and Smith, 2012). We refer the interested reader to three recent overviews on parameter optimization for SMT: (McAllester and Keshet, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012). Still, MERT remains the de-facto standard in the statistical machine translation community. Its stability has been of concern, and is widely studied. Suggestions to improve it include using regularization (Cer et </context>
</contexts>
<marker>Cherry, Foster, 2012</marker>
<rawString>Colin Cherry and George Foster. 2012. Batch tuning strategies for statistical machine translation. In Proceedings of the 2012 Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT ’12, pages 427–436, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Online large-margin training of syntactic and structural translation features.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, EMNLP ’08,</booktitle>
<pages>224--233</pages>
<location>Honolulu, Hawaii, USA.</location>
<contexts>
<context position="4351" citStr="Chiang et al., 2008" startWordPosition="690" endWordPosition="693">he 19th Conference on Computational Language Learning, pages 62–72, Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics 2 Related Work Tuning the parameters of a log-linear model for statistical machine translation is an active area of research. The standard approach is to use minimum error rate training, or MERT, (Och, 2003), which optimizes BLEU directly. Recently, there has been a surge in new optimization techniques for SMT. Two parameter optimizers that have recently become popular include the margin-infused relaxed algorithm or MIRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009), which is an on-line sentence-level perceptron-like passive-aggressive optimizer, and pairwise ranking optimization or PRO (Hopkins and May, 2011), which operates in batch mode and sees tuning as ranking. A number of improved versions thereof have been proposed in the literature including a batch version of MIRA (Cherry and Foster, 2012), with local updates (Liu et al., 2012), a linear regression version of PRO (Bazrafshan et al., 2012), and a non-sampling version of PRO (Dreyer and Dong, 2015); another example is Rampeon (Gimpel and Smith, 2012). We refer the interested</context>
<context position="5600" citStr="Chiang et al., 2008" startWordPosition="880" endWordPosition="883">ews on parameter optimization for SMT: (McAllester and Keshet, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012). Still, MERT remains the de-facto standard in the statistical machine translation community. Its stability has been of concern, and is widely studied. Suggestions to improve it include using regularization (Cer et al., 2008), random restarts (Moore and Quirk, 2008), multiple replications (Clark et al., 2011), and parameter aggregation (Cettolo et al., 2011). With the emergence of new optimization techniques there have been also studies that compare stability between MIRA–MERT (Chiang et al., 2008; Chiang et al., 2009; Cherry and Foster, 2012), PRO–MERT (Hopkins and May, 2011), MIRA–PRO–MERT (Cherry and Foster, 2012; Gimpel and Smith, 2012; Nakov et al., 2012). Pathological verbosity was reported when using MERT on recall-oriented metrics such as METEOR (Lavie and Denkowski, 2009; Denkowski and Lavie, 2011), as well as large variance with MIRA (Simianer et al., 2012). However, we are not aware of any previous studies of the impact of sentence length and dataset verbosity across optimizers. 3 Method For the following analysis, we need to define the following four quantities: • source-si</context>
</contexts>
<marker>Chiang, Marton, Resnik, 2008</marker>
<rawString>David Chiang, Yuval Marton, and Philip Resnik. 2008. Online large-margin training of syntactic and structural translation features. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, EMNLP ’08, pages 224–233, Honolulu, Hawaii, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Kevin Knight</author>
<author>Wei Wang</author>
</authors>
<title>11,001 new features for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT ’09,</booktitle>
<pages>218--226</pages>
<location>Boulder, Colorado, USA.</location>
<contexts>
<context position="4373" citStr="Chiang et al., 2009" startWordPosition="694" endWordPosition="697"> Computational Language Learning, pages 62–72, Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics 2 Related Work Tuning the parameters of a log-linear model for statistical machine translation is an active area of research. The standard approach is to use minimum error rate training, or MERT, (Och, 2003), which optimizes BLEU directly. Recently, there has been a surge in new optimization techniques for SMT. Two parameter optimizers that have recently become popular include the margin-infused relaxed algorithm or MIRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009), which is an on-line sentence-level perceptron-like passive-aggressive optimizer, and pairwise ranking optimization or PRO (Hopkins and May, 2011), which operates in batch mode and sees tuning as ranking. A number of improved versions thereof have been proposed in the literature including a batch version of MIRA (Cherry and Foster, 2012), with local updates (Liu et al., 2012), a linear regression version of PRO (Bazrafshan et al., 2012), and a non-sampling version of PRO (Dreyer and Dong, 2015); another example is Rampeon (Gimpel and Smith, 2012). We refer the interested reader to three recen</context>
<context position="5621" citStr="Chiang et al., 2009" startWordPosition="884" endWordPosition="887">mization for SMT: (McAllester and Keshet, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012). Still, MERT remains the de-facto standard in the statistical machine translation community. Its stability has been of concern, and is widely studied. Suggestions to improve it include using regularization (Cer et al., 2008), random restarts (Moore and Quirk, 2008), multiple replications (Clark et al., 2011), and parameter aggregation (Cettolo et al., 2011). With the emergence of new optimization techniques there have been also studies that compare stability between MIRA–MERT (Chiang et al., 2008; Chiang et al., 2009; Cherry and Foster, 2012), PRO–MERT (Hopkins and May, 2011), MIRA–PRO–MERT (Cherry and Foster, 2012; Gimpel and Smith, 2012; Nakov et al., 2012). Pathological verbosity was reported when using MERT on recall-oriented metrics such as METEOR (Lavie and Denkowski, 2009; Denkowski and Lavie, 2011), as well as large variance with MIRA (Simianer et al., 2012). However, we are not aware of any previous studies of the impact of sentence length and dataset verbosity across optimizers. 3 Method For the following analysis, we need to define the following four quantities: • source-side length: the number</context>
</contexts>
<marker>Chiang, Knight, Wang, 2009</marker>
<rawString>David Chiang, Kevin Knight, and Wei Wang. 2009. 11,001 new features for statistical machine translation. In Proceedings of the 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT ’09, pages 218– 226, Boulder, Colorado, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, ACL ’05,</booktitle>
<pages>263--270</pages>
<location>Ann Arbor, Michigan, USA.</location>
<contexts>
<context position="33223" citStr="Chiang, 2005" startWordPosition="5762" endWordPosition="5763">ection might matter too. • When optimizing with MERT: If you know beforehand the test set, select the closest tuning set. Otherwise, tune on longer sentences. We plan to extend this study in a number of directions. First, we would like to include other parameter optimizers such as Rampeon (Gimpel and Smith, 2012) and MIRA. Second, we want to experiment with other metrics, such as TER (Snover et al., 2006), which typically yields short translations, and METEOR (Lavie and Denkowski, 2009), which yields too long translations. Third, we would like to explore other SMT models such as hierarchical (Chiang, 2005) and syntax-based (Galley et al., 2004; Quirk et al., 2005), and other decoders such as cdec (Dyer et al., 2010), Joshua (Li et al., 2009), and Jane (Vilar et al., 2010). A long-term objective would be to design a metric that measures the closeness between tuning and test datasets, which includes the different characteristics, such as length distribution, verbosity distribution, syntactic complexity, etc., to guarantee a more stable evaluation situations, but which would also allow to systematically test the robustness of translation systems, when deviating from the matching conditions. Acknow</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, ACL ’05, pages 263–270, Ann Arbor, Michigan, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan H Clark</author>
<author>Chris Dyer</author>
<author>Alon Lavie</author>
<author>Noah A Smith</author>
</authors>
<title>Better hypothesis testing for statistical machine translation: Controlling for optimizer instability.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, ACLHLT ’11,</booktitle>
<pages>176--181</pages>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="5408" citStr="Clark et al., 2011" startWordPosition="851" endWordPosition="854">(Bazrafshan et al., 2012), and a non-sampling version of PRO (Dreyer and Dong, 2015); another example is Rampeon (Gimpel and Smith, 2012). We refer the interested reader to three recent overviews on parameter optimization for SMT: (McAllester and Keshet, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012). Still, MERT remains the de-facto standard in the statistical machine translation community. Its stability has been of concern, and is widely studied. Suggestions to improve it include using regularization (Cer et al., 2008), random restarts (Moore and Quirk, 2008), multiple replications (Clark et al., 2011), and parameter aggregation (Cettolo et al., 2011). With the emergence of new optimization techniques there have been also studies that compare stability between MIRA–MERT (Chiang et al., 2008; Chiang et al., 2009; Cherry and Foster, 2012), PRO–MERT (Hopkins and May, 2011), MIRA–PRO–MERT (Cherry and Foster, 2012; Gimpel and Smith, 2012; Nakov et al., 2012). Pathological verbosity was reported when using MERT on recall-oriented metrics such as METEOR (Lavie and Denkowski, 2009; Denkowski and Lavie, 2011), as well as large variance with MIRA (Simianer et al., 2012). However, we are not aware of </context>
<context position="13885" citStr="Clark et al. (2011)" startWordPosition="2311" endWordPosition="2314">ecommended in (Nakov et al., 2013), which solves instability issues when tuning on the long sentences; we will discuss our PRO fix and the reasons it is needed in Section 5 below. In order to ensure convergence, we allowed both MERT and PRO to run for up to 25 iterations (default: 16); we further used 1000- best lists (default: 100). In our experiments below, we perform three reruns of parameter optimization, tuning on each of the twelve tuning datasets; in the figures, we plot the results of the three reruns, while in the tables, we report BLEU averaged over the three reruns, as suggested by Clark et al. (2011). 4.2 Learning Verbosity We performed parameter optimization using MERT and PRO on each dataset, and we used the resulting parameters to translate the same dataset. The purpose of this experiment was to study the ability of the optimizers to learn the verbosity of the tuning sets. Getting the hypothesis verbosity right means that it is highly correlated with the tuning set verbosity, which in turn is determined by the dataset source length. The results are shown in Figure 2. In each graph, there are 36 points (many of them very close and overlapping) since we performed three reruns with our tw</context>
</contexts>
<marker>Clark, Dyer, Lavie, Smith, 2011</marker>
<rawString>Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A. Smith. 2011. Better hypothesis testing for statistical machine translation: Controlling for optimizer instability. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, ACLHLT ’11, pages 176–181, Portland, Oregon, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
<author>Ivona Kuˇcerov´a</author>
</authors>
<title>Clause restructuring for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05,</booktitle>
<pages>531--540</pages>
<location>Ann Arbor, Michigan.</location>
<marker>Collins, Koehn, Kuˇcerov´a, 2005</marker>
<rawString>Michael Collins, Philipp Koehn, and Ivona Kuˇcerov´a. 2005. Clause restructuring for statistical machine translation. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05, pages 531–540, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Denkowski</author>
<author>Alon Lavie</author>
</authors>
<title>Meteortuned phrase-based SMT: CMU French-English and Haitian-English systems for WMT</title>
<date>2011</date>
<tech>Technical report, Technical Report CMU-LTI-11-011,</tech>
<institution>Language Technologies Institute, Carnegie Mellon University.</institution>
<contexts>
<context position="5916" citStr="Denkowski and Lavie, 2011" startWordPosition="928" endWordPosition="931">regularization (Cer et al., 2008), random restarts (Moore and Quirk, 2008), multiple replications (Clark et al., 2011), and parameter aggregation (Cettolo et al., 2011). With the emergence of new optimization techniques there have been also studies that compare stability between MIRA–MERT (Chiang et al., 2008; Chiang et al., 2009; Cherry and Foster, 2012), PRO–MERT (Hopkins and May, 2011), MIRA–PRO–MERT (Cherry and Foster, 2012; Gimpel and Smith, 2012; Nakov et al., 2012). Pathological verbosity was reported when using MERT on recall-oriented metrics such as METEOR (Lavie and Denkowski, 2009; Denkowski and Lavie, 2011), as well as large variance with MIRA (Simianer et al., 2012). However, we are not aware of any previous studies of the impact of sentence length and dataset verbosity across optimizers. 3 Method For the following analysis, we need to define the following four quantities: • source-side length: the number of words in the source sentence; • length ratio: the ratio of the number of words in the output hypothesis to those in the reference;2 • verbosity: the ratio of the number of words in the reference to those in the source;3 • hypothesis verbosity: the ratio of the number of words in the hypothe</context>
</contexts>
<marker>Denkowski, Lavie, 2011</marker>
<rawString>Michael Denkowski and Alon Lavie. 2011. Meteortuned phrase-based SMT: CMU French-English and Haitian-English systems for WMT 2011. Technical report, Technical Report CMU-LTI-11-011, Language Technologies Institute, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Dreyer</author>
<author>Yuanzhe Dong</author>
</authors>
<title>APRO: All-pairs ranking optimization for MT tuning.</title>
<date>2015</date>
<booktitle>In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT ’15,</booktitle>
<pages>1018--1023</pages>
<location>Denver, Colorado, USA.</location>
<contexts>
<context position="4873" citStr="Dreyer and Dong, 2015" startWordPosition="771" endWordPosition="774">lar include the margin-infused relaxed algorithm or MIRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009), which is an on-line sentence-level perceptron-like passive-aggressive optimizer, and pairwise ranking optimization or PRO (Hopkins and May, 2011), which operates in batch mode and sees tuning as ranking. A number of improved versions thereof have been proposed in the literature including a batch version of MIRA (Cherry and Foster, 2012), with local updates (Liu et al., 2012), a linear regression version of PRO (Bazrafshan et al., 2012), and a non-sampling version of PRO (Dreyer and Dong, 2015); another example is Rampeon (Gimpel and Smith, 2012). We refer the interested reader to three recent overviews on parameter optimization for SMT: (McAllester and Keshet, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012). Still, MERT remains the de-facto standard in the statistical machine translation community. Its stability has been of concern, and is widely studied. Suggestions to improve it include using regularization (Cer et al., 2008), random restarts (Moore and Quirk, 2008), multiple replications (Clark et al., 2011), and parameter aggregation (Cettolo et al., 2011). With the emer</context>
</contexts>
<marker>Dreyer, Dong, 2015</marker>
<rawString>Markus Dreyer and Yuanzhe Dong. 2015. APRO: All-pairs ranking optimization for MT tuning. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT ’15, pages 1018–1023, Denver, Colorado, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Adam Lopez</author>
<author>Juri Ganitkevitch</author>
<author>Jonathan Weese</author>
<author>Ferhan Ture</author>
<author>Phil Blunsom</author>
<author>Hendra Setiawan</author>
<author>Vladimir Eidelman</author>
<author>Philip Resnik</author>
</authors>
<title>cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 System Demonstrations, ACL ’10,</booktitle>
<pages>7--12</pages>
<location>Uppsala,</location>
<contexts>
<context position="33335" citStr="Dyer et al., 2010" startWordPosition="5780" endWordPosition="5783">st tuning set. Otherwise, tune on longer sentences. We plan to extend this study in a number of directions. First, we would like to include other parameter optimizers such as Rampeon (Gimpel and Smith, 2012) and MIRA. Second, we want to experiment with other metrics, such as TER (Snover et al., 2006), which typically yields short translations, and METEOR (Lavie and Denkowski, 2009), which yields too long translations. Third, we would like to explore other SMT models such as hierarchical (Chiang, 2005) and syntax-based (Galley et al., 2004; Quirk et al., 2005), and other decoders such as cdec (Dyer et al., 2010), Joshua (Li et al., 2009), and Jane (Vilar et al., 2010). A long-term objective would be to design a metric that measures the closeness between tuning and test datasets, which includes the different characteristics, such as length distribution, verbosity distribution, syntactic complexity, etc., to guarantee a more stable evaluation situations, but which would also allow to systematically test the robustness of translation systems, when deviating from the matching conditions. Acknowledgments We would like to thank the anonymous reviewers for their constructive comments, which have helped us t</context>
</contexts>
<marker>Dyer, Lopez, Ganitkevitch, Weese, Ture, Blunsom, Setiawan, Eidelman, Resnik, 2010</marker>
<rawString>Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan, Vladimir Eidelman, and Philip Resnik. 2010. cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models. In Proceedings of the ACL 2010 System Demonstrations, ACL ’10, pages 7–12, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT ’04,</booktitle>
<pages>273--280</pages>
<location>Boston, Massachusetts, USA.</location>
<contexts>
<context position="33261" citStr="Galley et al., 2004" startWordPosition="5766" endWordPosition="5769">optimizing with MERT: If you know beforehand the test set, select the closest tuning set. Otherwise, tune on longer sentences. We plan to extend this study in a number of directions. First, we would like to include other parameter optimizers such as Rampeon (Gimpel and Smith, 2012) and MIRA. Second, we want to experiment with other metrics, such as TER (Snover et al., 2006), which typically yields short translations, and METEOR (Lavie and Denkowski, 2009), which yields too long translations. Third, we would like to explore other SMT models such as hierarchical (Chiang, 2005) and syntax-based (Galley et al., 2004; Quirk et al., 2005), and other decoders such as cdec (Dyer et al., 2010), Joshua (Li et al., 2009), and Jane (Vilar et al., 2010). A long-term objective would be to design a metric that measures the closeness between tuning and test datasets, which includes the different characteristics, such as length distribution, verbosity distribution, syntactic complexity, etc., to guarantee a more stable evaluation situations, but which would also allow to systematically test the robustness of translation systems, when deviating from the matching conditions. Acknowledgments We would like to thank the a</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proceedings of the 2004 Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT ’04, pages 273– 280, Boston, Massachusetts, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Gimpel</author>
<author>Noah A Smith</author>
</authors>
<title>Structured ramp loss minimization for machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT ’12,</booktitle>
<pages>221--231</pages>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="4926" citStr="Gimpel and Smith, 2012" startWordPosition="779" endWordPosition="782">MIRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009), which is an on-line sentence-level perceptron-like passive-aggressive optimizer, and pairwise ranking optimization or PRO (Hopkins and May, 2011), which operates in batch mode and sees tuning as ranking. A number of improved versions thereof have been proposed in the literature including a batch version of MIRA (Cherry and Foster, 2012), with local updates (Liu et al., 2012), a linear regression version of PRO (Bazrafshan et al., 2012), and a non-sampling version of PRO (Dreyer and Dong, 2015); another example is Rampeon (Gimpel and Smith, 2012). We refer the interested reader to three recent overviews on parameter optimization for SMT: (McAllester and Keshet, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012). Still, MERT remains the de-facto standard in the statistical machine translation community. Its stability has been of concern, and is widely studied. Suggestions to improve it include using regularization (Cer et al., 2008), random restarts (Moore and Quirk, 2008), multiple replications (Clark et al., 2011), and parameter aggregation (Cettolo et al., 2011). With the emergence of new optimization techniques there have been </context>
<context position="32924" citStr="Gimpel and Smith, 2012" startWordPosition="5711" endWordPosition="5714">cies. • When optimizing with PRO: select or construct a high-verbosity dataset as this could potentially compensate for PROs tendency to yield too short translations. Note that for Arabic-English, higher verbosity means longer tuning sentences, while for SpanishEnglish, it means shorter ones; translation direction might matter too. • When optimizing with MERT: If you know beforehand the test set, select the closest tuning set. Otherwise, tune on longer sentences. We plan to extend this study in a number of directions. First, we would like to include other parameter optimizers such as Rampeon (Gimpel and Smith, 2012) and MIRA. Second, we want to experiment with other metrics, such as TER (Snover et al., 2006), which typically yields short translations, and METEOR (Lavie and Denkowski, 2009), which yields too long translations. Third, we would like to explore other SMT models such as hierarchical (Chiang, 2005) and syntax-based (Galley et al., 2004; Quirk et al., 2005), and other decoders such as cdec (Dyer et al., 2010), Joshua (Li et al., 2009), and Jane (Vilar et al., 2010). A long-term objective would be to design a metric that measures the closeness between tuning and test datasets, which includes the</context>
</contexts>
<marker>Gimpel, Smith, 2012</marker>
<rawString>Kevin Gimpel and Noah A. Smith. 2012. Structured ramp loss minimization for machine translation. In Proceedings of the 2012 Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT ’12, pages 221– 231, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
</authors>
<title>KenLM: Faster and smaller language model queries.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation, WMT ’11,</booktitle>
<location>Edinburgh, United Kingdom.</location>
<contexts>
<context position="12995" citStr="Heafield, 2011" startWordPosition="2159" endWordPosition="2160">ll WMT12 data, again except for the UN data. We tokenized and truecased the English and the Spanish side of all bi-texts and also the monolingual data for language modeling using the standard tokenizer of Moses. We segmented the words on the Arabic side using the MADA ATB segmentation scheme (Roth et al., 2008). We built our phrase tables using the Moses pipeline with maxphrase-length 7 and Kneser-Ney smoothing. We also built a lexicalized reordering model (Koehn et al., 2005): msd-bidirectional-fe. We used a 5-gram language model trained on GigaWord v.5 with Kneser-Ney smoothing using KenLM (Heafield, 2011). On tuning and testing, we dropped the unknown words for Arabic-English, and we used monotoneat-punctuation decoding for Spanish-English. We tuned using MERT and PRO. We used the standard implementation of MERT from the Moses toolkit, and a fixed version of PRO, as we recommended in (Nakov et al., 2013), which solves instability issues when tuning on the long sentences; we will discuss our PRO fix and the reasons it is needed in Section 5 below. In order to ensure convergence, we allowed both MERT and PRO to run for up to 25 iterations (default: 16); we further used 1000- best lists (default:</context>
</contexts>
<marker>Heafield, 2011</marker>
<rawString>Kenneth Heafield. 2011. KenLM: Faster and smaller language model queries. In Proceedings of the Sixth Workshop on Statistical Machine Translation, WMT ’11, Edinburgh, United Kingdom.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hopkins</author>
<author>Jonathan May</author>
</authors>
<title>Tuning as ranking.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<pages>1352--1362</pages>
<location>Edinburgh, Scotland, United Kingdom.</location>
<contexts>
<context position="4520" citStr="Hopkins and May, 2011" startWordPosition="712" endWordPosition="715">Tuning the parameters of a log-linear model for statistical machine translation is an active area of research. The standard approach is to use minimum error rate training, or MERT, (Och, 2003), which optimizes BLEU directly. Recently, there has been a surge in new optimization techniques for SMT. Two parameter optimizers that have recently become popular include the margin-infused relaxed algorithm or MIRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009), which is an on-line sentence-level perceptron-like passive-aggressive optimizer, and pairwise ranking optimization or PRO (Hopkins and May, 2011), which operates in batch mode and sees tuning as ranking. A number of improved versions thereof have been proposed in the literature including a batch version of MIRA (Cherry and Foster, 2012), with local updates (Liu et al., 2012), a linear regression version of PRO (Bazrafshan et al., 2012), and a non-sampling version of PRO (Dreyer and Dong, 2015); another example is Rampeon (Gimpel and Smith, 2012). We refer the interested reader to three recent overviews on parameter optimization for SMT: (McAllester and Keshet, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012). Still, MERT remains </context>
</contexts>
<marker>Hopkins, May, 2011</marker>
<rawString>Mark Hopkins and Jonathan May. 2011. Tuning as ranking. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 1352–1362, Edinburgh, Scotland, United Kingdom.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology (Volume 1), HLT-NAACL ’03,</booktitle>
<pages>48--54</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="12192" citStr="Koehn et al., 2003" startWordPosition="2022" endWordPosition="2025">th the same number of sentence pairs but with different number of words, e.g., for our Arabic-English datasets, longest has about twice as many English words as middle, and about four times as many words as shortest. Constructing tuning datasets with the same number of sentences instead of the same number of tokens is intentional as we wanted to ensure that in each of the conditions, the SMT parameter optimizers learn on the same number of training examples. 6www.statmt.org/wmt12/ 7www.nist.gov/itl/iad/mig/openmt12.cfm 64 4.1 Experimental Setup We experimented with the phrase-based SMT model (Koehn et al., 2003) as implemented in Moses (Koehn et al., 2007). For Arabic-English, we trained on all data that was allowed for use in the NIST 2012 except for the UN corpus. For Spanish-English, we used all WMT12 data, again except for the UN data. We tokenized and truecased the English and the Spanish side of all bi-texts and also the monolingual data for language modeling using the standard tokenizer of Moses. We segmented the words on the Arabic side using the MADA ATB segmentation scheme (Roth et al., 2008). We built our phrase tables using the Moses pipeline with maxphrase-length 7 and Kneser-Ney smoothi</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology (Volume 1), HLT-NAACL ’03, pages 48–54, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Amittai Axelrod</author>
<author>Alexandra Birch Mayne</author>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>David Talbot</author>
</authors>
<title>Edinburgh system description for the 2005 IWSLT speech translation evaluation.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation, IWSLT ’05,</booktitle>
<location>Pittsburgh, Pennsylvania, USA.</location>
<contexts>
<context position="12861" citStr="Koehn et al., 2005" startWordPosition="2139" endWordPosition="2142"> Arabic-English, we trained on all data that was allowed for use in the NIST 2012 except for the UN corpus. For Spanish-English, we used all WMT12 data, again except for the UN data. We tokenized and truecased the English and the Spanish side of all bi-texts and also the monolingual data for language modeling using the standard tokenizer of Moses. We segmented the words on the Arabic side using the MADA ATB segmentation scheme (Roth et al., 2008). We built our phrase tables using the Moses pipeline with maxphrase-length 7 and Kneser-Ney smoothing. We also built a lexicalized reordering model (Koehn et al., 2005): msd-bidirectional-fe. We used a 5-gram language model trained on GigaWord v.5 with Kneser-Ney smoothing using KenLM (Heafield, 2011). On tuning and testing, we dropped the unknown words for Arabic-English, and we used monotoneat-punctuation decoding for Spanish-English. We tuned using MERT and PRO. We used the standard implementation of MERT from the Moses toolkit, and a fixed version of PRO, as we recommended in (Nakov et al., 2013), which solves instability issues when tuning on the long sentences; we will discuss our PRO fix and the reasons it is needed in Section 5 below. In order to ens</context>
</contexts>
<marker>Koehn, Axelrod, Mayne, Callison-Burch, Osborne, Talbot, 2005</marker>
<rawString>Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne, Chris Callison-Burch, Miles Osborne, and David Talbot. 2005. Edinburgh system description for the 2005 IWSLT speech translation evaluation. In Proceedings of the International Workshop on Spoken Language Translation, IWSLT ’05, Pittsburgh, Pennsylvania, USA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (Demonstration session)., ACL ’07,</booktitle>
<pages>177--180</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="12237" citStr="Koehn et al., 2007" startWordPosition="2030" endWordPosition="2033"> different number of words, e.g., for our Arabic-English datasets, longest has about twice as many English words as middle, and about four times as many words as shortest. Constructing tuning datasets with the same number of sentences instead of the same number of tokens is intentional as we wanted to ensure that in each of the conditions, the SMT parameter optimizers learn on the same number of training examples. 6www.statmt.org/wmt12/ 7www.nist.gov/itl/iad/mig/openmt12.cfm 64 4.1 Experimental Setup We experimented with the phrase-based SMT model (Koehn et al., 2003) as implemented in Moses (Koehn et al., 2007). For Arabic-English, we trained on all data that was allowed for use in the NIST 2012 except for the UN corpus. For Spanish-English, we used all WMT12 data, again except for the UN data. We tokenized and truecased the English and the Spanish side of all bi-texts and also the monolingual data for language modeling using the standard tokenizer of Moses. We segmented the words on the Arabic side using the MADA ATB segmentation scheme (Roth et al., 2008). We built our phrase tables using the Moses pipeline with maxphrase-length 7 and Kneser-Ney smoothing. We also built a lexicalized reordering mo</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (Demonstration session)., ACL ’07, pages 177–180, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alon Lavie</author>
<author>Michael J Denkowski</author>
</authors>
<title>The METEOR metric for automatic evaluation of machine translation.</title>
<date>2009</date>
<journal>Machine Translation,</journal>
<pages>23--105</pages>
<contexts>
<context position="5888" citStr="Lavie and Denkowski, 2009" startWordPosition="924" endWordPosition="927">o improve it include using regularization (Cer et al., 2008), random restarts (Moore and Quirk, 2008), multiple replications (Clark et al., 2011), and parameter aggregation (Cettolo et al., 2011). With the emergence of new optimization techniques there have been also studies that compare stability between MIRA–MERT (Chiang et al., 2008; Chiang et al., 2009; Cherry and Foster, 2012), PRO–MERT (Hopkins and May, 2011), MIRA–PRO–MERT (Cherry and Foster, 2012; Gimpel and Smith, 2012; Nakov et al., 2012). Pathological verbosity was reported when using MERT on recall-oriented metrics such as METEOR (Lavie and Denkowski, 2009; Denkowski and Lavie, 2011), as well as large variance with MIRA (Simianer et al., 2012). However, we are not aware of any previous studies of the impact of sentence length and dataset verbosity across optimizers. 3 Method For the following analysis, we need to define the following four quantities: • source-side length: the number of words in the source sentence; • length ratio: the ratio of the number of words in the output hypothesis to those in the reference;2 • verbosity: the ratio of the number of words in the reference to those in the source;3 • hypothesis verbosity: the ratio of the nu</context>
<context position="33101" citStr="Lavie and Denkowski, 2009" startWordPosition="5741" endWordPosition="5744">at for Arabic-English, higher verbosity means longer tuning sentences, while for SpanishEnglish, it means shorter ones; translation direction might matter too. • When optimizing with MERT: If you know beforehand the test set, select the closest tuning set. Otherwise, tune on longer sentences. We plan to extend this study in a number of directions. First, we would like to include other parameter optimizers such as Rampeon (Gimpel and Smith, 2012) and MIRA. Second, we want to experiment with other metrics, such as TER (Snover et al., 2006), which typically yields short translations, and METEOR (Lavie and Denkowski, 2009), which yields too long translations. Third, we would like to explore other SMT models such as hierarchical (Chiang, 2005) and syntax-based (Galley et al., 2004; Quirk et al., 2005), and other decoders such as cdec (Dyer et al., 2010), Joshua (Li et al., 2009), and Jane (Vilar et al., 2010). A long-term objective would be to design a metric that measures the closeness between tuning and test datasets, which includes the different characteristics, such as length distribution, verbosity distribution, syntactic complexity, etc., to guarantee a more stable evaluation situations, but which would al</context>
</contexts>
<marker>Lavie, Denkowski, 2009</marker>
<rawString>Alon Lavie and Michael J. Denkowski. 2009. The METEOR metric for automatic evaluation of machine translation. Machine Translation, 23:105–115.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Chris Callison-Burch</author>
<author>Chris Dyer</author>
<author>Sanjeev Khudanpur</author>
<author>Lane Schwartz</author>
<author>Wren Thornton</author>
<author>Jonathan Weese</author>
<author>Omar Zaidan</author>
</authors>
<title>Joshua: An open source toolkit for parsing-based machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Machine Translation, WMT ’09,</booktitle>
<pages>135--139</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="33361" citStr="Li et al., 2009" startWordPosition="5785" endWordPosition="5788">ne on longer sentences. We plan to extend this study in a number of directions. First, we would like to include other parameter optimizers such as Rampeon (Gimpel and Smith, 2012) and MIRA. Second, we want to experiment with other metrics, such as TER (Snover et al., 2006), which typically yields short translations, and METEOR (Lavie and Denkowski, 2009), which yields too long translations. Third, we would like to explore other SMT models such as hierarchical (Chiang, 2005) and syntax-based (Galley et al., 2004; Quirk et al., 2005), and other decoders such as cdec (Dyer et al., 2010), Joshua (Li et al., 2009), and Jane (Vilar et al., 2010). A long-term objective would be to design a metric that measures the closeness between tuning and test datasets, which includes the different characteristics, such as length distribution, verbosity distribution, syntactic complexity, etc., to guarantee a more stable evaluation situations, but which would also allow to systematically test the robustness of translation systems, when deviating from the matching conditions. Acknowledgments We would like to thank the anonymous reviewers for their constructive comments, which have helped us to improve the paper. 70 Re</context>
</contexts>
<marker>Li, Callison-Burch, Dyer, Khudanpur, Schwartz, Thornton, Weese, Zaidan, 2009</marker>
<rawString>Zhifei Li, Chris Callison-Burch, Chris Dyer, Sanjeev Khudanpur, Lane Schwartz, Wren Thornton, Jonathan Weese, and Omar Zaidan. 2009. Joshua: An open source toolkit for parsing-based machine translation. In Proceedings of the Fourth Workshop on Statistical Machine Translation, WMT ’09, pages 135–139, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mu Li</author>
<author>Yinggong Zhao</author>
<author>Dongdong Zhang</author>
<author>Ming Zhou</author>
</authors>
<title>Adaptive development data selection for log-linear model in statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics, COLING ’10,</booktitle>
<pages>662--670</pages>
<location>Beijing, China.</location>
<contexts>
<context position="2608" citStr="Li et al., 2010" startWordPosition="411" endWordPosition="414"> log-linear framework, and their values are optimized to maximize some automatic metric, typically BLEU, on a tuning dataset. Given this setup, it is clear that the choice of a tuning set and its characteristics, can have significant impact on the SMT system’s performance: if the experimental framework (training data, tuning set, and test set) is highly consistent, i.e., there is close similarity in terms of genre, domain and verbosity,1 then translation quality can be improved by careful selection of tuning sentences that exhibit high degree of similarity to the test set (Zheng et al., 2010; Li et al., 2010). In our recent work (Nakov et al., 2012), we have studied the relationship between optimizers such as MERT, PRO and MIRA, and we have pointed out that PRO tends to generate relatively shorter translations, which could lead to lower BLEU scores on testing. Our solution there was to fix the objective function being optimized: PRO uses sentence-level smoothed BLEU+1, as opposed to the standard dataset-level BLEU. Here we are interested in a related but different question: the relationship between properties of the tuning dataset and the optimizer’s performance. More specifically, we study how th</context>
</contexts>
<marker>Li, Zhao, Zhang, Zhou, 2010</marker>
<rawString>Mu Li, Yinggong Zhao, Dongdong Zhang, and Ming Zhou. 2010. Adaptive development data selection for log-linear model in statistical machine translation. In Proceedings of the 23rd International Conference on Computational Linguistics, COLING ’10, pages 662–670, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lemao Liu</author>
<author>Hailong Cao</author>
<author>Taro Watanabe</author>
<author>Tiejun Zhao</author>
<author>Mo Yu</author>
<author>Conghui Zhu</author>
</authors>
<title>Locally training the log-linear model for SMT.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12,</booktitle>
<pages>402--411</pages>
<location>Jeju Island,</location>
<contexts>
<context position="4752" citStr="Liu et al., 2012" startWordPosition="751" endWordPosition="754">ere has been a surge in new optimization techniques for SMT. Two parameter optimizers that have recently become popular include the margin-infused relaxed algorithm or MIRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009), which is an on-line sentence-level perceptron-like passive-aggressive optimizer, and pairwise ranking optimization or PRO (Hopkins and May, 2011), which operates in batch mode and sees tuning as ranking. A number of improved versions thereof have been proposed in the literature including a batch version of MIRA (Cherry and Foster, 2012), with local updates (Liu et al., 2012), a linear regression version of PRO (Bazrafshan et al., 2012), and a non-sampling version of PRO (Dreyer and Dong, 2015); another example is Rampeon (Gimpel and Smith, 2012). We refer the interested reader to three recent overviews on parameter optimization for SMT: (McAllester and Keshet, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012). Still, MERT remains the de-facto standard in the statistical machine translation community. Its stability has been of concern, and is widely studied. Suggestions to improve it include using regularization (Cer et al., 2008), random restarts (Moore and </context>
</contexts>
<marker>Liu, Cao, Watanabe, Zhao, Yu, Zhu, 2012</marker>
<rawString>Lemao Liu, Hailong Cao, Taro Watanabe, Tiejun Zhao, Mo Yu, and Conghui Zhu. 2012. Locally training the log-linear model for SMT. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12, pages 402–411, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McAllester</author>
<author>Joseph Keshet</author>
</authors>
<title>Generalization bounds and consistency for latent structural probit and ramp loss.</title>
<date>2011</date>
<booktitle>Advances in Neural Information Processing Systems 24, NIPS ’11,</booktitle>
<pages>2205--2212</pages>
<editor>In J. Shawe-Taylor, R.S. Zemel, P. Bartlett, F.C.N. Pereira, and K.Q. Weinberger, editors,</editor>
<location>Granada, Spain.</location>
<contexts>
<context position="5048" citStr="McAllester and Keshet, 2011" startWordPosition="797" endWordPosition="800">like passive-aggressive optimizer, and pairwise ranking optimization or PRO (Hopkins and May, 2011), which operates in batch mode and sees tuning as ranking. A number of improved versions thereof have been proposed in the literature including a batch version of MIRA (Cherry and Foster, 2012), with local updates (Liu et al., 2012), a linear regression version of PRO (Bazrafshan et al., 2012), and a non-sampling version of PRO (Dreyer and Dong, 2015); another example is Rampeon (Gimpel and Smith, 2012). We refer the interested reader to three recent overviews on parameter optimization for SMT: (McAllester and Keshet, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012). Still, MERT remains the de-facto standard in the statistical machine translation community. Its stability has been of concern, and is widely studied. Suggestions to improve it include using regularization (Cer et al., 2008), random restarts (Moore and Quirk, 2008), multiple replications (Clark et al., 2011), and parameter aggregation (Cettolo et al., 2011). With the emergence of new optimization techniques there have been also studies that compare stability between MIRA–MERT (Chiang et al., 2008; Chiang et al., 2009; Cherry and Foster, 2012),</context>
</contexts>
<marker>McAllester, Keshet, 2011</marker>
<rawString>David McAllester and Joseph Keshet. 2011. Generalization bounds and consistency for latent structural probit and ramp loss. In J. Shawe-Taylor, R.S. Zemel, P. Bartlett, F.C.N. Pereira, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 24, NIPS ’11, pages 2205– 2212, Granada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
<author>Chris Quirk</author>
</authors>
<title>Random restarts in minimum error rate training for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1, COLING ’08,</booktitle>
<pages>585--592</pages>
<location>Manchester, United Kingdom.</location>
<contexts>
<context position="5364" citStr="Moore and Quirk, 2008" startWordPosition="845" endWordPosition="848">al., 2012), a linear regression version of PRO (Bazrafshan et al., 2012), and a non-sampling version of PRO (Dreyer and Dong, 2015); another example is Rampeon (Gimpel and Smith, 2012). We refer the interested reader to three recent overviews on parameter optimization for SMT: (McAllester and Keshet, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012). Still, MERT remains the de-facto standard in the statistical machine translation community. Its stability has been of concern, and is widely studied. Suggestions to improve it include using regularization (Cer et al., 2008), random restarts (Moore and Quirk, 2008), multiple replications (Clark et al., 2011), and parameter aggregation (Cettolo et al., 2011). With the emergence of new optimization techniques there have been also studies that compare stability between MIRA–MERT (Chiang et al., 2008; Chiang et al., 2009; Cherry and Foster, 2012), PRO–MERT (Hopkins and May, 2011), MIRA–PRO–MERT (Cherry and Foster, 2012; Gimpel and Smith, 2012; Nakov et al., 2012). Pathological verbosity was reported when using MERT on recall-oriented metrics such as METEOR (Lavie and Denkowski, 2009; Denkowski and Lavie, 2011), as well as large variance with MIRA (Simianer </context>
</contexts>
<marker>Moore, Quirk, 2008</marker>
<rawString>Robert C Moore and Chris Quirk. 2008. Random restarts in minimum error rate training for statistical machine translation. In Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1, COLING ’08, pages 585–592, Manchester, United Kingdom.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Francisco Guzm´an</author>
<author>Stephan Vogel</author>
</authors>
<title>Optimizing for sentence-level BLEU+1 yields short translations.</title>
<date>2012</date>
<booktitle>In Proceedings of the 24th International Conference on Computational Linguistics, COLING ’12,</booktitle>
<pages>1979--1994</pages>
<location>Mumbai, India.</location>
<marker>Nakov, Guzm´an, Vogel, 2012</marker>
<rawString>Preslav Nakov, Francisco Guzm´an, and Stephan Vogel. 2012. Optimizing for sentence-level BLEU+1 yields short translations. In Proceedings of the 24th International Conference on Computational Linguistics, COLING ’12, pages 1979–1994, Mumbai, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Francisco Guzm´an</author>
<author>Stephan Vogel</author>
</authors>
<title>A tale about PRO and monsters.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), ACL ’13,</booktitle>
<pages>12--17</pages>
<location>Sofia, Bulgaria.</location>
<marker>Nakov, Guzm´an, Vogel, 2013</marker>
<rawString>Preslav Nakov, Francisco Guzm´an, and Stephan Vogel. 2013. A tale about PRO and monsters. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), ACL ’13, pages 12–17, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, ACL ’03,</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="4090" citStr="Och, 2003" startWordPosition="650" endWordPosition="651">changing some characteristics of the tuning dataset. 1Verbosity also depends on the translator; it is often a stylistic choice. and not necessarily related to fluency or adequacy. This aspect is beyond the scope of the present work. 62 Proceedings of the 19th Conference on Computational Language Learning, pages 62–72, Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics 2 Related Work Tuning the parameters of a log-linear model for statistical machine translation is an active area of research. The standard approach is to use minimum error rate training, or MERT, (Och, 2003), which optimizes BLEU directly. Recently, there has been a surge in new optimization techniques for SMT. Two parameter optimizers that have recently become popular include the margin-infused relaxed algorithm or MIRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009), which is an on-line sentence-level perceptron-like passive-aggressive optimizer, and pairwise ranking optimization or PRO (Hopkins and May, 2011), which operates in batch mode and sees tuning as ranking. A number of improved versions thereof have been proposed in the literature including a batch version of MIRA (C</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, ACL ’03, pages 160–167, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, ACL ’02,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, Pennsylvania, USA.</location>
<contexts>
<context position="8049" citStr="Papineni et al., 2002" startWordPosition="1281" endWordPosition="1285">rved in singlereference sets (we used the first reference), and to a lesser extent in multiple-reference sets (five references for MT04 and MT05, and four for MT06 and MT09). For Spanish-English, the story is different: here the English sentences tend to be shorter than the Spanish ones, and the verbosity decreases as the sentence length increases. Overall, in all three cases, the verbosity appears to be length-dependent. 2For multi-reference sets, we use the length of the reference that is closest to the length of the hypothesis. This is the best match length from the original paper on BLEU (Papineni et al., 2002); it is default in the NIST scoring tool v13a, which we use in our experiments. 3When dealing with multi-reference sets, we use the average reference length. 4The datasets we experiment with are described in more detail in Section 4 below. 63 Source length vs. avg. verbosity Arabic−English Spanish−English set ● Ar−En−multi Ar−En−single ● ●● A ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ●● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● 1.175 A</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, ACL ’02, pages 311– 318, Philadelphia, Pennsylvania, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
<author>Colin Cherry</author>
</authors>
<title>Dependency treelet translation: Syntactically informed phrasal smt.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05,</booktitle>
<pages>271--279</pages>
<location>Ann Arbor, Michigan, USA.</location>
<contexts>
<context position="33282" citStr="Quirk et al., 2005" startWordPosition="5770" endWordPosition="5773"> If you know beforehand the test set, select the closest tuning set. Otherwise, tune on longer sentences. We plan to extend this study in a number of directions. First, we would like to include other parameter optimizers such as Rampeon (Gimpel and Smith, 2012) and MIRA. Second, we want to experiment with other metrics, such as TER (Snover et al., 2006), which typically yields short translations, and METEOR (Lavie and Denkowski, 2009), which yields too long translations. Third, we would like to explore other SMT models such as hierarchical (Chiang, 2005) and syntax-based (Galley et al., 2004; Quirk et al., 2005), and other decoders such as cdec (Dyer et al., 2010), Joshua (Li et al., 2009), and Jane (Vilar et al., 2010). A long-term objective would be to design a metric that measures the closeness between tuning and test datasets, which includes the different characteristics, such as length distribution, verbosity distribution, syntactic complexity, etc., to guarantee a more stable evaluation situations, but which would also allow to systematically test the robustness of translation systems, when deviating from the matching conditions. Acknowledgments We would like to thank the anonymous reviewers fo</context>
</contexts>
<marker>Quirk, Menezes, Cherry, 2005</marker>
<rawString>Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Dependency treelet translation: Syntactically informed phrasal smt. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05, pages 271–279, Ann Arbor, Michigan, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan Roth</author>
<author>Owen Rambow</author>
<author>Nizar Habash</author>
<author>Mona Diab</author>
<author>Cynthia Rudin</author>
</authors>
<title>Arabic morphological tagging, diacritization, and lemmatization using lexeme models and feature ranking.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, ACL ’08,</booktitle>
<pages>117--120</pages>
<location>Columbus, Ohio, USA.</location>
<contexts>
<context position="12692" citStr="Roth et al., 2008" startWordPosition="2112" endWordPosition="2115">tl/iad/mig/openmt12.cfm 64 4.1 Experimental Setup We experimented with the phrase-based SMT model (Koehn et al., 2003) as implemented in Moses (Koehn et al., 2007). For Arabic-English, we trained on all data that was allowed for use in the NIST 2012 except for the UN corpus. For Spanish-English, we used all WMT12 data, again except for the UN data. We tokenized and truecased the English and the Spanish side of all bi-texts and also the monolingual data for language modeling using the standard tokenizer of Moses. We segmented the words on the Arabic side using the MADA ATB segmentation scheme (Roth et al., 2008). We built our phrase tables using the Moses pipeline with maxphrase-length 7 and Kneser-Ney smoothing. We also built a lexicalized reordering model (Koehn et al., 2005): msd-bidirectional-fe. We used a 5-gram language model trained on GigaWord v.5 with Kneser-Ney smoothing using KenLM (Heafield, 2011). On tuning and testing, we dropped the unknown words for Arabic-English, and we used monotoneat-punctuation decoding for Spanish-English. We tuned using MERT and PRO. We used the standard implementation of MERT from the Moses toolkit, and a fixed version of PRO, as we recommended in (Nakov et al</context>
</contexts>
<marker>Roth, Rambow, Habash, Diab, Rudin, 2008</marker>
<rawString>Ryan Roth, Owen Rambow, Nizar Habash, Mona Diab, and Cynthia Rudin. 2008. Arabic morphological tagging, diacritization, and lemmatization using lexeme models and feature ranking. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, ACL ’08, pages 117–120, Columbus, Ohio, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Simianer</author>
<author>Stefan Riezler</author>
<author>Chris Dyer</author>
</authors>
<title>Joint feature selection in distributed stochastic learning for large-scale discriminative training in SMT.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1, ACL ’12,</booktitle>
<pages>11--21</pages>
<location>Jeju Island,</location>
<contexts>
<context position="5977" citStr="Simianer et al., 2012" startWordPosition="939" endWordPosition="942">rk, 2008), multiple replications (Clark et al., 2011), and parameter aggregation (Cettolo et al., 2011). With the emergence of new optimization techniques there have been also studies that compare stability between MIRA–MERT (Chiang et al., 2008; Chiang et al., 2009; Cherry and Foster, 2012), PRO–MERT (Hopkins and May, 2011), MIRA–PRO–MERT (Cherry and Foster, 2012; Gimpel and Smith, 2012; Nakov et al., 2012). Pathological verbosity was reported when using MERT on recall-oriented metrics such as METEOR (Lavie and Denkowski, 2009; Denkowski and Lavie, 2011), as well as large variance with MIRA (Simianer et al., 2012). However, we are not aware of any previous studies of the impact of sentence length and dataset verbosity across optimizers. 3 Method For the following analysis, we need to define the following four quantities: • source-side length: the number of words in the source sentence; • length ratio: the ratio of the number of words in the output hypothesis to those in the reference;2 • verbosity: the ratio of the number of words in the reference to those in the source;3 • hypothesis verbosity: the ratio of the number of words in the hypothesis to those in the source. Naturally, the verbosity varies a</context>
</contexts>
<marker>Simianer, Riezler, Dyer, 2012</marker>
<rawString>Patrick Simianer, Stefan Riezler, and Chris Dyer. 2012. Joint feature selection in distributed stochastic learning for large-scale discriminative training in SMT. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1, ACL ’12, pages 11–21, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 7th Biennial Conference of the Association for Machine Translation in the Americas, AMTA ’06,</booktitle>
<pages>223--231</pages>
<location>Cambridge, Massachusetts, USA.</location>
<contexts>
<context position="33018" citStr="Snover et al., 2006" startWordPosition="5729" endWordPosition="5732">ntially compensate for PROs tendency to yield too short translations. Note that for Arabic-English, higher verbosity means longer tuning sentences, while for SpanishEnglish, it means shorter ones; translation direction might matter too. • When optimizing with MERT: If you know beforehand the test set, select the closest tuning set. Otherwise, tune on longer sentences. We plan to extend this study in a number of directions. First, we would like to include other parameter optimizers such as Rampeon (Gimpel and Smith, 2012) and MIRA. Second, we want to experiment with other metrics, such as TER (Snover et al., 2006), which typically yields short translations, and METEOR (Lavie and Denkowski, 2009), which yields too long translations. Third, we would like to explore other SMT models such as hierarchical (Chiang, 2005) and syntax-based (Galley et al., 2004; Quirk et al., 2005), and other decoders such as cdec (Dyer et al., 2010), Joshua (Li et al., 2009), and Jane (Vilar et al., 2010). A long-term objective would be to design a metric that measures the closeness between tuning and test datasets, which includes the different characteristics, such as length distribution, verbosity distribution, syntactic com</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings of the 7th Biennial Conference of the Association for Machine Translation in the Americas, AMTA ’06, pages 223–231, Cambridge, Massachusetts, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vilar</author>
<author>Daniel Stein</author>
<author>Matthias Huck</author>
<author>Hermann Ney</author>
</authors>
<title>Jane: Open source hierarchical translation, extended with reordering and lexicon models.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th Workshop on Statistical Machine Translation and MetricsMATR, WMT ’10,</booktitle>
<pages>262--270</pages>
<location>Uppsala,</location>
<marker>Vilar, Stein, Huck, Ney, 2010</marker>
<rawString>David Vilar, Daniel Stein, Matthias Huck, and Hermann Ney. 2010. Jane: Open source hierarchical translation, extended with reordering and lexicon models. In Proceedings of the 5th Workshop on Statistical Machine Translation and MetricsMATR, WMT ’10, pages 262–270, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taro Watanabe</author>
<author>Jun Suzuki</author>
<author>Hajime Tsukada</author>
<author>Hideki Isozaki</author>
</authors>
<title>Online large-margin training for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’07,</booktitle>
<pages>764--773</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="4330" citStr="Watanabe et al., 2007" startWordPosition="686" endWordPosition="689">rk. 62 Proceedings of the 19th Conference on Computational Language Learning, pages 62–72, Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics 2 Related Work Tuning the parameters of a log-linear model for statistical machine translation is an active area of research. The standard approach is to use minimum error rate training, or MERT, (Och, 2003), which optimizes BLEU directly. Recently, there has been a surge in new optimization techniques for SMT. Two parameter optimizers that have recently become popular include the margin-infused relaxed algorithm or MIRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009), which is an on-line sentence-level perceptron-like passive-aggressive optimizer, and pairwise ranking optimization or PRO (Hopkins and May, 2011), which operates in batch mode and sees tuning as ranking. A number of improved versions thereof have been proposed in the literature including a batch version of MIRA (Cherry and Foster, 2012), with local updates (Liu et al., 2012), a linear regression version of PRO (Bazrafshan et al., 2012), and a non-sampling version of PRO (Dreyer and Dong, 2015); another example is Rampeon (Gimpel and Smith, 2012). We</context>
</contexts>
<marker>Watanabe, Suzuki, Tsukada, Isozaki, 2007</marker>
<rawString>Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki Isozaki. 2007. Online large-margin training for statistical machine translation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’07, pages 764–773, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongguang Zheng</author>
<author>Zhongjun He</author>
<author>Yao Meng</author>
<author>Hao Yu</author>
</authors>
<title>Domain adaptation for statistical machine translation in development corpus selection.</title>
<date>2010</date>
<booktitle>In Proceedings of the 4th International Universal Communication Symposium, IUCS ’10,</booktitle>
<pages>2--7</pages>
<location>Beijing, China.</location>
<contexts>
<context position="2590" citStr="Zheng et al., 2010" startWordPosition="407" endWordPosition="410">iscriminatively in a log-linear framework, and their values are optimized to maximize some automatic metric, typically BLEU, on a tuning dataset. Given this setup, it is clear that the choice of a tuning set and its characteristics, can have significant impact on the SMT system’s performance: if the experimental framework (training data, tuning set, and test set) is highly consistent, i.e., there is close similarity in terms of genre, domain and verbosity,1 then translation quality can be improved by careful selection of tuning sentences that exhibit high degree of similarity to the test set (Zheng et al., 2010; Li et al., 2010). In our recent work (Nakov et al., 2012), we have studied the relationship between optimizers such as MERT, PRO and MIRA, and we have pointed out that PRO tends to generate relatively shorter translations, which could lead to lower BLEU scores on testing. Our solution there was to fix the objective function being optimized: PRO uses sentence-level smoothed BLEU+1, as opposed to the standard dataset-level BLEU. Here we are interested in a related but different question: the relationship between properties of the tuning dataset and the optimizer’s performance. More specificall</context>
</contexts>
<marker>Zheng, He, Meng, Yu, 2010</marker>
<rawString>Zhongguang Zheng, Zhongjun He, Yao Meng, and Hao Yu. 2010. Domain adaptation for statistical machine translation in development corpus selection. In Proceedings of the 4th International Universal Communication Symposium, IUCS ’10, pages 2–7, Beijing, China.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>