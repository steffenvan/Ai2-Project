<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<note confidence="0.939199166666667">
Automatic Evaluation of Topic Coherence
David Newman,&apos;� Jey Han Lau,° Karl Grieser♦, and Timothy Baldwin,&apos;°
4 NICTA Victoria Research Laboratory, Australia
46 Dept of Computer Science, University of California, Irvine
C7 Dept of Computer Science and Software Engineering, University of Melbourne, Australia
Q Dept of Information Systems, University of Melbourne, Australia
</note>
<email confidence="0.979482">
newman@uci.edu,depthchargex@gmail.com,
kgrieser@csse.unimelb.edu.au,tb@ldwin.net
</email>
<sectionHeader confidence="0.994429" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999937263157895">
This paper introduces the novel task of topic
coherence evaluation, whereby a set of words,
as generated by a topic model, is rated for
coherence or interpretability. We apply a
range of topic scoring models to the evaluation
task, drawing on WordNet, Wikipedia and the
Google search engine, and existing research
on lexical similarity/relatedness. In compar-
ison with human scores for a set of learned
topics over two distinct datasets, we show a
simple co-occurrence measure based on point-
wise mutual information over Wikipedia data
is able to achieve results for the task at or
nearing the level of inter-annotator correla-
tion, and that other Wikipedia-based lexical
relatedness methods also achieve strong re-
sults. Google produces strong, if less consis-
tent, results, while our results over WordNet
are patchy at best.
</bodyText>
<sectionHeader confidence="0.998784" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999955961538461">
There has traditionally been strong interest within
computational linguistics in techniques for learning
sets of words (aka topics) which capture the latent
semantics of a document or document collection, in
the form of methods such as latent semantic analysis
(Deerwester et al., 1990), probabilistic latent seman-
tic analysis (Hofmann, 2001), random projection
(Widdows and Ferraro, 2008), and more recently, la-
tent Dirichlet allocation (Blei et al., 2003; Griffiths
and Steyvers, 2004). Such methods have been suc-
cessfully applied to a myriad of tasks including word
sense discrimination (Brody and Lapata, 2009), doc-
ument summarisation (Haghighi and Vanderwende,
2009), areal linguistic analysis (Daume III, 2009)
and text segmentation (Sun et al., 2008). In each
case, extrinsic evaluation has been used to demon-
strate the effectiveness of the learned topics in the
application domain, but standardly, no attempt has
been made to perform intrinsic evaluation of the top-
ics themselves, either qualitatively or quantitatively.
In machine learning, on the other hand, researchers
have modified and extended topic models in a vari-
ety of ways, and evaluated intrinsically in terms of
model perplexity (Wallach et al., 2009), but there has
been less effort on qualitative understanding of the
semantic nature of the learned topics.
This research seeks to fill the gap between topic
evaluation in computational linguistics and machine
learning, in developing techniques to perform intrin-
sic qualitative evaluation of learned topics. That
is, we develop methods for evaluating the qual-
ity of a given topic, in terms of its coherence to
a human. After learning topics from a collection
of news articles and a collection of books, we ask
humans to decide whether individual learned top-
ics are coherent, in terms of their interpretability
and association with a single over-arching seman-
tic concept. We then propose models to predict
topic coherence, based on resources such as Word-
Net, Wikipedia and the Google search engine, and
methods ranging from ontological similarity to link
overlap and term co-occurrence. Over topics learned
from two distinct datasets, we demonstrate that there
is remarkable inter-annotator agreement on what is
a coherent topic, and additionally that our methods
based on Wikipedia are able to achieve nearly perfect
agreement with humans over the evaluation of topic
coherence.
This research forms part of a larger research
agenda on the utility of topic modelling in gist-
ing and visualising document collections, and ulti-
mately enhancing search/discovery interfaces over
</bodyText>
<page confidence="0.907845">
100
</page>
<note confidence="0.7549605">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 100–108,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.990872857142857">
document collections (Newman et al., to appeara).
Evaluating topic coherence is a component of the
larger question of what are good topics, what char-
acteristics of a document collection make it more
amenable to topic modelling, and how can the po-
tential of topic modelling be harnessed for human
consumption (Newman et al., to appearb).
</bodyText>
<sectionHeader confidence="0.999815" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999954909090909">
Most earlier work on intrinsically evaluating learned
topics has been on the basis of perplexity results,
where a model is learned on a collection of train-
ing documents, then the log probability of the un-
seen test documents is computed using that learned
model. Usually perplexity is reported, which is the
inverse of the geometric mean per-word likelihood.
Perplexity is useful for model selection and adjust-
ing parameters (e.g. number of topics T), and is
the standard way of demonstrating the advantage of
one model over another. Wallach et al. (2009) pre-
sented efficient and unbiased methods for computing
perplexity and evaluating almost any type of topic
model.
While statistical evaluation of topic models is
reasonably well understood, there has been much
less work on evaluating the intrinsic semantic qual-
ity of topics learned by topic models, which could
have a far greater impact on the overall value of
topic modeling for end-user applications. Some re-
searchers have started to address this problem, in-
cluding Mei et al. (2007) who presented approaches
for automatic labeling of topics (which is core to the
question of coherence and semantic interpretabil-
ity), and Griffiths and Steyvers (2006) who applied
topic models to word sense discrimination tasks.
Misra et al. (2008) used topic modelling to identify
semantically incoherent documents within a docu-
ment collection (vs. coherent topics, as targeted in
this research). Chang et al. (2009) presented the
first human-evaluation of topic models by creating
a task where humans were asked to identify which
word in a list of five topic words had been ran-
domly switched with a word from another topic.
This work showed some possibly counter-intuitive
results, where in some cases humans preferred mod-
els with higher perplexity. This type of result shows
the need for further exploring measures other than
perplexity for evaluating topic models. In earlier
work, we carried out preliminary experimentation
using pointwise mutual information and Google re-
sults to evaluate topic coherence over the same set
of topics as used in this research (Newman et al.,
2009).
Part of this research takes inspiration from the
work on automatic evaluation in machine translation
(Papineni et al., 2002) and automatic summarisation
(Lin, 2004). Here, the development of automated
methods with high correlation with human subjects
has opened the door to large-scale automated evalua-
tion of system outputs, revolutionising the respective
fields. While our aspirations are more modest, the
basic aim is the same: to develop a fully-automated
method for evaluating a well-grounded task, which
achieves near-human correlation.
</bodyText>
<sectionHeader confidence="0.995402" genericHeader="method">
3 Topic Modelling
</sectionHeader>
<bodyText confidence="0.999759833333333">
In order to evaluate topic modelling, we require a
topic model and set of topics for a given document
collection. While the evaluation methodology we
describe generalises to any method which gener-
ates sets of words, all of our experiments are based
on Latent Dirichlet Allocation (LDA, aka Discrete
Principal Component Analysis), on the grounds that
it is a state-of-the-art method for generating topics.
LDA is a Bayesian graphical model for text docu-
ment collections represented by bags-of-words (see
Blei et al. (2003), Griffiths and Steyvers (2004),
Buntine and Jakulin (2004)). In a topic model, each
document in the collection of D documents is mod-
elled as a multinomial distribution over T topics,
where each topic is a multinomial distribution over
W words. Typically, only a small number of words
are important (have high likelihood) in each topic,
and only a small number of topics are present in each
document.
The collapsed Gibbs sampled topic model simul-
taneously learns the topics and the mixture of topics
in documents by iteratively sampling the topic as-
signment z to every word in every document, using
the Gibbs sampling update:
</bodyText>
<equation confidence="0.991558555555556">
p(zid = t xid = w, Z—id) OC
N�id
wt +0
�wN�id
wt + WO
N�id
td + α
EtN�id
td + Tα
</equation>
<page confidence="0.987791">
101
</page>
<bodyText confidence="0.940290666666667">
where zid = t is the assignment of the ith word in
document d to topic t, xid = w indicates that the
current observed word is w, and z—id is the vector of
all topic assignments not including the current word.
Nwt represents integer count arrays (with the sub-
scripts denoting what is counted), and α and β are
Dirichlet priors.
The maximum a posterior (MAP) estimates of the
topics p(w|t), t = 1... T are given by:
</bodyText>
<equation confidence="0.999529">
Nwt + β
p(w|t) =
E
w Nwt + Wβ
</equation>
<bodyText confidence="0.999498142857143">
We will follow the convention of representing a
topic via its top-n words, ordered by p(w|t). Here,
we use the top-ten words, as they usually provide
sufficient detail to convey the subject of a topic,
and distinguish one topic from another. For the
remainder of this paper, we will refer to individ-
ual topics by its list of top-ten words, denoted by
</bodyText>
<equation confidence="0.921949">
w = (w1, ... , w10).
</equation>
<sectionHeader confidence="0.99111" genericHeader="method">
4 Topic Evaluation Methods
</sectionHeader>
<bodyText confidence="0.998932727272727">
We experiment with scoring methods based on
WordNet (Section 4.1), Wikipedia (Section 4.2) and
the Google search engine (Section 4.3). In the case
of Google, we query for the entire topic, but with
WordNet and Wikipedia, this takes the form of scor-
ing each word-pair in a given topic w based on the
component words (w1, ... , w10). Given some (sym-
metric) word-similarity measure D(wi, wj), two
straightforward ways of producing a combined score
2)) word-pair scores are: (1) the
from the 45 (i.e. (
</bodyText>
<equation confidence="0.92375125">
arithmetic mean, and (2) the median, as follows:
Mean-D-Score(w) =
mean{D(wi, wj), ij E 1... 10,i &lt; j}
Median-D-Score(w) =
</equation>
<bodyText confidence="0.977424333333333">
median{D(wi, wj), ij E 1... 10,i &lt; j}
Intuitively, the median seems the more natural rep-
resentation, as it is less affected by outlier scores,
but we experiment with both, and fall back to empir-
ical verification of which is the better combination
method.
</bodyText>
<subsectionHeader confidence="0.998516">
4.1 WordNet similarity
</subsectionHeader>
<bodyText confidence="0.990180326086956">
WordNet (Fellbaum, 1998) is a lexical ontology
that represents word sense via “synsets”, which
are structured in a hypernym/hyponym hierarchy
(nouns) or hypernym/troponym hierarchy (verbs).
WordNet additionally links both synsets and words
via lexical relations including antonymy, morpho-
logical derivation and holonymy/meronym.
In parallel with the development of WordNet, a
number of computational methods for calculating
the semantic relatedness/similarity between synset
pairs (i.e. sense-specified word pairs) have been de-
veloped, as we outline below. These methods ap-
ply to synset rather than word pairs, so to generate a
single score for a given word pair, we look up each
word in WordNet and exhaustively generate scores
for each sense pairing defined by them, and calcu-
late their arithmetic mean.l
The majority of the methods (all methods other
than HSO, VECTOR and LESK) are restricted to op-
erating strictly over hierarchical links within a sin-
gle hierarchy. As the verb and noun hierarchies are
not connected (other than via derivational links), this
means that it is generally not possible to calculate
the similarity between noun and verb senses, for ex-
ample. In such cases, we simply drop the synset
pairing in question from our calculation of the mean.
The least common subsumer (LCS) is a common
feature to a number of the measures, and is defined
as the deepest node in the hierarchy that subsumes
both of the synsets under question.
For all our experiments over WordNet, we use the
WordNet::Similarity package.
Path distance (PATH)
The simplest of the WordNet-based measures is
to count the number of nodes visited while going
from one word to another via the hypernym hierar-
chy. The path distance between two nodes is de-
fined as the number of nodes that lie on the short-
est path between two words in the hierarchy. This
&apos;We also experimented with the median, and trialled filter-
ing the set of senses in a variety of ways, e.g. using only the
first sense (the sense with the highest prior) for a given word,
or using only the word senses associated with the POS with the
highest prior. In all cases, the overall trend was for the correla-
tion with the human scores to drop relative to the mean, so we
only present the numbers for the mean in this paper.
</bodyText>
<page confidence="0.99459">
102
</page>
<bodyText confidence="0.99292">
count of nodes includes the beginning and ending
word nodes.
</bodyText>
<subsectionHeader confidence="0.406468">
Leacock-Chodorow (LCH)
</subsectionHeader>
<bodyText confidence="0.999702166666667">
The measure of semantic similarity devised by
Leacock et al. (1998) finds the shortest path between
two WordNet synsets (sp(c1, c2)) using hypernym
and synonym relationships. This path length is then
scaled by the maximum depth of WordNet (D), and
the log likelihood taken:
</bodyText>
<subsectionHeader confidence="0.581264">
Wu-Palmer (WUP)
</subsectionHeader>
<bodyText confidence="0.999715333333333">
Wu and Palmer (1994) proposed to scale the depth
of the two synset nodes (depthc1 and depthc2) by
the depth of their LCS (depth(lcsc1,c2)):
</bodyText>
<equation confidence="0.9994">
simwup(c1, c2) =
2 · depth(lcsc1,c2)
depthc1 + depthc2 + 2 · depth(lcsc1,c2)
</equation>
<bodyText confidence="0.999064142857143">
The scaling means that specific terms (deeper in the
hierarchy) that are close together are more semanti-
cally similar than more general terms, which have a
short path distance between them. Only hypernym
relationships are used in this measure, as the LCS
is defined by the common member in the concepts’
hypernym path.
</bodyText>
<subsectionHeader confidence="0.695149">
Hirst-St Onge (HSO)
</subsectionHeader>
<bodyText confidence="0.9998757">
Hirst and St-Onge (1998) define a measure of se-
mantic similarity based on length and tortuosity of
the path between nodes. Hirst and St-Onge attribute
directions (up, down and horizontal) to the larger set
of WordNet relationships, and identify the path from
one word to another utilising all of these relation-
ships. The relatedness score is then computed by
the weighted sum of the path length between the two
words (len(c1, c2)) and the number of turns the path
makes (turns(c1, c2)) to take this route:
</bodyText>
<equation confidence="0.997314">
relhso(c1,c2) =
C − len(c1, c2) − k x turns(c1, c2)
</equation>
<bodyText confidence="0.9998846">
where C and k are constants. Additionally, a set of
restrictions is placed on the path so that it may not
be more than a certain length, may not contain more
than a set number of turns, and may only take turns
in certain directions.
</bodyText>
<sectionHeader confidence="0.849108" genericHeader="method">
Resnik Information Content (RES)
</sectionHeader>
<bodyText confidence="0.991563363636364">
Resnik (1995) presents a method for weighting
edges in WordNet (avoiding the assumption that all
edges between nodes have equal importance), by
weighting edges between nodes by their frequency
of use in textual corpora.
Resnik found that the most effective measure of
comparison using this methodology was to measure
the Information Content (IC(c) = − log p(c)) of
the subsumer with the greatest Information Content
from the set of all concepts that subsumed the two
initial concepts (S(c1, c2)) being compared:
</bodyText>
<equation confidence="0.945278666666667">
simres(c1, c2) = max [− log p(c)]
cES(c1,c2)
Lin (LIN)
</equation>
<bodyText confidence="0.99396275">
Lin (1998) expanded on the Information Theo-
retic approach presented by Resnik by scaling the
Information Content of each node by the informa-
tion content of their LCS:
</bodyText>
<equation confidence="0.9998955">
simlin(c1,c2) =
log p(c1) + log p(c2)
</equation>
<bodyText confidence="0.997647">
This measure contrasts the joint content of the two
concepts with the difference between them.
</bodyText>
<sectionHeader confidence="0.515392" genericHeader="method">
Jiang-Conrath (JCN)
</sectionHeader>
<bodyText confidence="0.998986666666667">
Jiang and Conrath (1997) define a measure that
utilises the components of the information content
of the LCS in a different manner:
</bodyText>
<equation confidence="0.998978333333333">
simjcn(c1, c2) =
1
IC(a) + IC(b) − 2 x IC(lcsa,b)
</equation>
<bodyText confidence="0.999580333333333">
Instead of defining commonality and difference as
with Lin’s measure, the key determinant is the speci-
ficity of the two nodes compared with their LCS.
</bodyText>
<subsectionHeader confidence="0.534154">
Lesk (LESK)
</subsectionHeader>
<bodyText confidence="0.999645625">
Lesk (1986) proposed a significantly different ap-
proach to lexical similarity to that proposed in the
methods presented above, using the lexical over-
lap in dictionary definitions (or glosses) to disam-
biguate word sense. The sense definitions that con-
tain the most words in common indicate the most
likely sense of the word given its co-occurrence with
similar word senses. Banerjee and Pedersen (2002)
</bodyText>
<equation confidence="0.995873">
simlch(c1, c2) = − log 2 · D
sp(c1, c2)
2 x log p(lcsc1,c2)
</equation>
<page confidence="0.991169">
103
</page>
<bodyText confidence="0.956286538461539">
adapted this method to utilise WordNet sense glosses
rather than dictionary definitions, and expand the
dictionary definitions via ontological links, and it is
this method we experiment with in this paper.
Vector (VECTOR)
Sch¨utze (1998) uses the words surrounding a term
in a piece of text to form a context vector that de-
scribes the context in which the word sense appears.
For a set of words associated with a target sense, a
context vector is computed as the centroid vector of
these words. The centroid context vectors each rep-
resent a word sense. To compare word senses, the
cosine similarity of the context vectors is used.
</bodyText>
<subsectionHeader confidence="0.811807">
4.2 Wikipedia
</subsectionHeader>
<bodyText confidence="0.999970947368421">
In the last few years, there has been a surge of in-
terest in using Wikipedia to calculate semantic sim-
ilarity, using the Wikipedia article content, in-article
links and document categories (Str¨ube and Ponzetto,
2006; Gabrilovich and Markovitch, 2007; Milne and
Witten, 2008). We present a selection of such meth-
ods below. There are a number of Wikipedia-based
scoring methods which we do not present results
for here (notably Str¨ube and Ponzetto (2006) and
Gabrilovich and Markovitch (2007)), due to their
computational complexity and uncertainty about the
full implementation details of the methods.
As with WordNet, a given word will often have
multiple entries in Wikipedia, grouped in a disam-
biguation page. For MIW, RACO and DOCSIM,
we apply the same strategy as we did with Word-
Net, in exhaustively calculating the pairwise scores
between the sets of documents associated with each
term, and averaging across them.
</bodyText>
<sectionHeader confidence="0.668625" genericHeader="method">
Milne-Witten (MIW)
</sectionHeader>
<bodyText confidence="0.999030777777778">
Milne and Witten (2008) adapted the Resnik
(1995) methodology to utilise the count of links
pointing to an article. As Wikipedia is self-
referential (articles link to related articles), no ex-
ternal data is needed to find the “referred-to-edness”
of a concept. Milne and Witten use an adapted In-
formation Content measure that weights the number
of links from one article to another (c1 → c2) by the
total number of links to the second article:
</bodyText>
<equation confidence="0.9816555">
�w(c1 → c2) = |c1 → c2 |× log
xEW
</equation>
<bodyText confidence="0.997893">
where x is an article in W, Wikipedia. This mea-
sure provides the similarity of one article to another,
however this is asymmetrical. The above metric is
used to find the weights of all outlinks from the two
articles being compared:
</bodyText>
<equation confidence="0.99988">
~c1 = (w(c1 → l1), w(c1 → l2), ··· ,w(c1 → ln))
~c2 = (w(c2 → l1),w(c2 → l2), · · · , w(c2 → ln))
</equation>
<bodyText confidence="0.9997745">
for the set of links l that is the union of the sets of
outlinks from both articles. The overall similarity
of the two articles is then calculated by taking the
cosine similarity of the two vectors.
</bodyText>
<sectionHeader confidence="0.479778" genericHeader="method">
Related Article Concept Overlap (RACO)
</sectionHeader>
<bodyText confidence="0.998563">
We also determine the category overlap of two
articles by examining the outlinks of both articles,
in the form of the Related Article Concept Overlap
(RACO) measure. The concept overlap of the sets
of respective outlinks is given by the union of the
two sets of categories from the outlinks from each
article:
</bodyText>
<equation confidence="0.999738">
overlap(c1,c1) =
��( U cat(l)) n( U cat(l))
lEol(c1) lEol(c2)
</equation>
<bodyText confidence="0.999126">
where ol(c1) is the set of outlinks from article c1,
and cat(l) is the set of categories of which the arti-
cle at outlink l is a member. To account for article
size (and differing number of outlinks), the Jaccard
coefficient is used:
</bodyText>
<equation confidence="0.999924">
relraco(c1,c2) =
��(UlEol(c1) cat(l)) n(UlEol(c2) cat(l))��
IU��
lEol(c1) cat(l)I + IUlEol(c2) cat(l)
</equation>
<subsectionHeader confidence="0.746771">
Document Similarity (DOCSIM)
</subsectionHeader>
<bodyText confidence="0.99990725">
In addition to these two measures of semantic re-
latedness, we experiment with simple cosine simi-
larity of the text of Wikipedia articles as a measure
of semantic relatedness.
</bodyText>
<subsectionHeader confidence="0.82526">
Term Co-occurrence (PMI)
</subsectionHeader>
<bodyText confidence="0.9998585">
Another variant is to treat Wikipedia as a single
meta-document and score word pairs using term co-
occurrence. Here, we calculate the pointwise mu-
tual information (PMI) of each word pair, estimated
</bodyText>
<equation confidence="0.452521">
|W|
|c1, x)|
</equation>
<page confidence="0.985385">
104
</page>
<figure confidence="0.7056977">
Selected high-scoring topics (unanimous score=3):
[NEWS] space earth moon science scientist light nasa mission planet mars ...
[NEWS] health disease aids virus vaccine infection hiv cases infected asthma ...
[BOOKS] steam engine valve cylinder pressure piston boiler air pump pipe ...
[BOOKS] furniture chair table cabinet wood leg mahogany piece oak louis ...
Selected low-scoring topics (unanimous score=1):
[NEWS] king bond berry bill ray rate james treas byrd key ...
[NEWS] dog moment hand face love self eye turn young character ...
[BOOKS] soon short longer carried rest turned raised filled turn allowed ...
[BOOKS] act sense adv person ppr plant sax genus applied dis ...
</figure>
<tableCaption confidence="0.987985">
Table 1: A selection of high-scoring and low-scoring topics
</tableCaption>
<bodyText confidence="0.999741444444445">
from the entire corpus of over two million English
Wikipedia articles (—1 billion words). PMI has been
studied variously in the context of collocation ex-
traction (Pecina, 2008), and is one measure of the
statistical independence of observing two words in
close proximity. Using a sliding window of 10-
words to identify co-occurrence, we computed the
PMI of all a given word pair (wi, wj) as, following
Newman et al. (2009):
</bodyText>
<equation confidence="0.9645905">
PMI(wi, wj) = log p(wi, wj)
p(wi)p(wj)
</equation>
<subsectionHeader confidence="0.988226">
4.3 Search engine-based similarity
</subsectionHeader>
<bodyText confidence="0.987282176470588">
Finally, we present two search engine-based scor-
ing methods, based on Newman et al. (2009). In
this case the external data source is the entire World
Wide Web, via the Google search engine. Unlike
the methods presented above, here we query for the
topic in its entirety,2 meaning that we return a topic-
level score rather than scores for individual word or
word sense pairs. In each case, we mark each search
term with the advanced search option + to search
for the terms exactly as is and prevent Google from
using synonyms or lexical variants of the term. An
example query is: +space +earth +moon +science
+scientist +light +nasa +mission +planet +mars.
Google title matches (TITLES)
Firstly, we score topics by the relative occurrence
of their component words in the titles of documents
returned by Google:
</bodyText>
<equation confidence="0.541648">
Google-titles-match(w) = 1 [wi = vj]
</equation>
<bodyText confidence="0.929153545454545">
2All queries were run on 15/09/2009.
where i = 1, ... ,10 and j = 1, ... , �V J, vj are
all the unique terms mentioned in the titles from the
top-100 search results, and 1 is the indicator function
to count matches. For example, in the top-100 re-
sults for our query above, there are 194 matches with
the ten topic words, so Google-titles-match(w) =
194.
Google log hits matches (LOGHITS)
Second, we issue queries as above, but return the
log number of hits for our query:
</bodyText>
<equation confidence="0.9652445">
Google-log-hits(w) =
log10(# results from search for w)
</equation>
<bodyText confidence="0.999623833333333">
where w is the search string +w1 +w2 +w3 ...
+w10. For example, our query above returns
171,000 results, so Google-log-hits(w) = 5.2. and
the URL titles from the top-100 results include a to-
tal of 194 matches with the ten topic words, so for
this topic Google-titles-match(w)=194.
</bodyText>
<sectionHeader confidence="0.997468" genericHeader="method">
5 Experimental Setup
</sectionHeader>
<bodyText confidence="0.99998025">
We learned topics for two document collections: a
collection of news articles, and a collection of books.
These collections were chosen to produce sets of
topics that have more variable quality than one typi-
cally observes when topic modeling highly uniform
content. The collection of D = 55, 000 news arti-
cles was selected from English Gigaword, and the
collection of D = 12, 000 books was downloaded
from the Internet Archive. We refer to these collec-
tions as NEWS and BOOKS, respectively.
Standard procedures were used to tokenize each
collection and create the bags-of-words. We learned
</bodyText>
<page confidence="0.99508">
105
</page>
<table confidence="0.999806555555555">
Resource Method Median Mean
HSO −0.29 0.34
JCN 0.08 0.22
LCH −0.18 −0.07
LESK 0.38 0.37
WordNet LIN 0.18 0.25
PATH 0.19 0.11
RES −0.10 0.13
VECTOR 0.07 0.20
WUP 0.03 0.10
RACO 0.61 0.63
Wikipedia MIW 0.69 0.60
DOCSIM 0.45 0.50
PMI 0.78 0.77
TITLES 0.80
Google LOGHITS
0.46
Gold-standard IAA 0.79 0.73
</table>
<tableCaption confidence="0.9995075">
Table 2: Spearman rank correlation p values for the
different scoring methods over the NEWS dataset (best-
performing method for each resource underlined; best-
performing method overall in boldface)
</tableCaption>
<table confidence="0.999899611111111">
Resource Method Median Mean
HSO 0.15 0.59
JCN −0.20 0.19
LCH −0.31 −0.15
LESK 0.53 0.53
WordNet LIN 0.09 0.28
PATH 0.29 0.12
RES 0.57 0.66
VECTOR −0.08 0.27
WUP 0.41 0.26
RACO 0.62 0.69
Wikipedia MIW 0.68 0.70
DOCSIM 0.59 0.60
PMI 0.74 0.77
TITLES 0.51
Google LOGHITS
−0.19
Gold-standard IAA 0.82 0.78
</table>
<tableCaption confidence="0.9959715">
Table 3: Spearman rank correlation p values for the dif-
ferent scoring methods over the BOOKS dataset (best-
performing method for each resource underlined; best-
performing method overall in boldface)
</tableCaption>
<bodyText confidence="0.999018903225807">
topic models of NEWS and BOOKS using T = 200
and T = 400 topics respectively. We randomly
selected a total of 237 topics from the two collec-
tions for user scoring. We asked N = 9 users to
score each of the 237 topics on a 3-point scale where
3=“useful” (coherent) and 1=“useless” (less coher-
ent).
We provided annotators with a rubric and guide-
lines on how to judge whether a topic was useful
or useless. In addition to showing several examples
of useful and useless topics, we instructed users to
decide whether the topic was to some extent coher-
ent, meaningful, interpretable, subject-heading-like,
and something-you-could-easily-label. For our pur-
poses, the usefulness of a topic can be thought of
as whether one could imagine using the topic in a
search interface to retrieve documents about a par-
ticular subject. One indicator of usefulness is the
ease by which one could think of a short label to de-
scribe a topic.
Table 1 shows a selection of high- and low-
scoring topics, as scored by the N = 9 users. The
first topic illustrates the notion of labelling coher-
ence, as space exploration, e.g., would be an obvi-
ous label for the topic. The low-scoring topics dis-
play little coherence, and one would not expect them
to be useful as categories or facets in a search inter-
face. Note that the useless topics from both collec-
tions are not chance artifacts produced by the mod-
els, but are in fact stable and robust statistical fea-
tures in the data sets.
</bodyText>
<sectionHeader confidence="0.999831" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.999952894736842">
The results for the different topic scoring methods
over the NEWS and BOOKS collections are pre-
sented in Tables 2 and 3, respectively. In each ta-
ble, we separate out the scoring methods into those
based on WordNet (from Section 4.1), those based
on Wikipedia (from Section 4.2), and those based on
Google (from Section 4.3).
As stated in Section 4, we experiment with two
methods for combining the word-pair scores (for all
methods other than the two Google methods, which
operate natively over a word set), namely the arith-
metic mean and median. We present the numbers
for these two methods in each table. In each case,
we evaluate via Spearman rank correlation, revers-
ing the sign of the calculated p value for PATH (as it
is the only instance of a distance metric, where the
gold-standard is made up of similarity values).
We include the inter-annotator agreement (IAA)
in the final row of each table, which we consider
</bodyText>
<page confidence="0.996457">
106
</page>
<bodyText confidence="0.996556707692308">
to be the upper bound for the task. This is calcu-
lated as the average Spearman rank correlation be-
tween each annotator and the mean/median of the
remaining annotators for that topic. Encouragingly,
there is relatively little difference in the IAA be-
tween the two datasets; the median-based calcula-
tion produces slightly higher p values and is empiri-
cally the method of choice.3
Of all the topic scoring methods tested, PMI
(term co-occurrence via simple pointwise mutual in-
formation) is the most consistent performer, achiev-
ing the best or near-best results over both datasets,
and approaching or surpassing the inter-annotator
agreement. This indicates both that the task of
topic evaluation as defined in this paper is com-
putationally tractable, and that word-pair based co-
occurrence is highly successful at modelling topic
coherence.
Comparing the different resources, Wikipedia is
far and away the most consistent performing, with
PMI producing the best results, followed by MIW
and RACO, and finally DOCSIM. There is rela-
tively little difference in results between NEWS and
BOOKS for the Wikipedia methods. Google achieves
the best results over NEWS, for TITLES (actually
slightly above the IAA), but the results fall away
sharply over BOOKS. The reason for this can be
seen in the sample topics in Table 1: the topics for
BOOKS tend to be more varied in word class than
for NEWS, and contain less proper names; also, the
genre of BOOKS is less well represented on the web.
We hypothesise that Wikipedia’s encyclopedic na-
ture means that it has good coverage over both do-
mains, and thus more robust.
Turning to WordNet, the overall results are
markedly better over BOOKS, again largely because
of the relative sparsity of proper names in the re-
source. The results for individual methods are some-
what surprising. Whereas JCN and LCH have been
shown to be two of the best-performing methods
over lexical similarity tasks (Budanitsky and Hirst,
2005; Agirre et al., 2009), they perform abysmally
at the topic scoring task. Indeed, the spread of re-
sults across the WordNet similarity methods (no-
3Note that the choice of mean or median for IAA is in-
dependent of that for the scoring methods, as they are com-
bining different things: annotator scores in the one hand, and
word/concept pair scores on the other.
tably HSO, JCN, LCH, LIN, RES and WUP) is
much greater than we had expected. The single most
consistent method is LESK, which is based on lexi-
cal overlap in definition sentences and makes rela-
tively modest use of the WordNet hierarchy. Supple-
mentary evaluation where we filtered out all proper
nouns from the topics (based on simple POS priors
for each word learned from an automatically-tagged
version of the British National Corpus) led to a slight
increase in results for the WordNet methods; the full
results are omitted for reasons of space. In future
work, we intend to carry out error analysis to deter-
mine why some of the methods performed so badly,
or inconsistently across the two datasets.
There is no clear answer to the question of
whether the mean or median is the best method for
combining the pair-wise scores.
</bodyText>
<sectionHeader confidence="0.999428" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.9999753125">
We have proposed the novel task of topic coher-
ence evaluation as a form of intrinsic topic evalu-
ation with relevance in document search/discovery
and visualisation applications. We constructed
a gold-standard dataset of topic coherence scores
over the output of a topic model for two distinct
datasets, and evaluated a wide range of topic scor-
ing methods over this dataset, drawing on WordNet,
Wikipedia and the Google search engine. The sin-
gle best-performing method was term co-occurrence
within Wikipedia based on pointwise mutual infor-
mation, which achieve results very close to the inter-
annotator agreement for the task. Google was also
found to perform well over one of the two datasets,
while the results for the WordNet-based methods
were overall surprisingly low.
</bodyText>
<sectionHeader confidence="0.994961" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.986192285714286">
NICTA is funded by the Australian government as rep-
resented by Department of Broadband, Communication
and Digital Economy, and the Australian Research Coun-
cil through the ICT centre of Excellence programme. DN
has also been supported by a grant from the Institute of
Museum and Library Services, and a Google Research
Award.
</bodyText>
<sectionHeader confidence="0.981073" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.8034535">
E Agirre, E Alfonseca, K Hall, J Kravalova, M Pas¸ca,
and A Soroa. 2009. A study on similarity and re-
</reference>
<page confidence="0.993815">
107
</page>
<reference confidence="0.999369752136752">
latedness using distributional and WordNet-based ap-
proaches. In Proc. of HLT: NAACL 2009, pages 19–
27, Boulder, Colorado.
S Banerjee and T Pedersen. 2002. An adapted Lesk algo-
rithm for word sense disambiguation using WordNet.
Proc. of CICLing’02, pages 136–145.
DM Blei, AY Ng, and MI Jordan. 2003. Latent Dirich-
let allocation. Journal of Machine Learning Research,
3:993–1022.
S Brody and M Lapata. 2009. Bayesian word sense
induction. In Proc. of EACL 2009, pages 103–111,
Athens, Greece.
A Budanitsky and G Hirst. 2005. Evaluating WordNet-
based Measures of Lexical Sematic Relatedness.
Computational Linguistics, 32(1):13–47.
WL Buntine and A Jakulin. 2004. Applying discrete
PCA in data analysis. In Proc. of UAI 2004, pages
59–66.
J Chang, J Boyd-Graber, S Gerris, C Wang, and D Blei.
2009. Reading tea leaves: How humans interpret topic
models. In Proc. of NIPS 2009.
H Daume III. 2009. Non-parametric bayesian areal lin-
guistics. In Proc. of HLT: NAACL 2009, pages 593–
601, Boulder, USA.
Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas K. Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal of the
American Society of Information Science, 41(6).
C Fellbaum, editor. 1998. WordNet: An Electronic Lexi-
cal Database. MIT Press, Cambridge, USA.
E Gabrilovich and S Markovitch. 2007. Computing se-
mantic relatedness using Wikipedia-based explicit se-
mantic analysis. In Proc. of IJCAI’07, pages 1606–
1611, Hyderabad, India.
T Griffiths and M Steyvers. 2004. Finding scientific top-
ics. In Proc. of the National Academy of Sciences, vol-
ume 101, pages 5228–5235.
T Griffiths and M Steyvers. 2006. Probabilistic topic
models. In Latent Semantic Analysis: A Road to
Meaning.
A Haghighi and L Vanderwende. 2009. Exploring con-
tent models for multi-document summarization. In
Proc. of HLT: NAACL 2009, pages 362–370, Boulder,
USA.
G Hirst and D St-Onge. 1998. Lexical chains as repre-
sentations of context for the detection and correction
of malapropism. In Fellbaum (Fellbaum, 1998), pages
305–332.
T Hofmann. 2001. Unsupervised learning by proba-
bilistic latent semantic analysis. Machine Learning,
42(1):177–196.
JJ Jiang and DW Conrath. 1997. Semantic similarity
based on corpus statistics and lexical taxonomy. In
Proc. of COLING’97, pages 19–33, Taipei, Taiwan.
C Leacock, G A Miller, and M Chodorow. 1998. Using
corpus statistics and WordNet relations for sense iden-
tification. Computational Linguistics, 24(1):147–65.
M Lesk. 1986. Automatic sense disambiguation us-
ing machine readable dictionaries: how to tell a pine
cone from an ice cream cone. In Proc. of SIGDOC’86,
pages 24–26, Toronto, Canada.
D Lin. 1998. Automatic retrieval and clustering of sim-
ilar words. In Proc. of COLING/ACL’98, pages 768–
774, Montreal, Canada.
C-Y Lin. 2004. ROUGE: a package for automatic
evaluation of summaries. In Proc. of the ACL 2004
Workshop on Text Summarization Branches Out (WAS
2004), pages 74–81, Barcelona, Spain.
Q Mei, X Shen, and CX Zhai. 2007. Automatic labeling
of multinomial topic models. In Proc. of KDD 2007,
pages 490–499.
D Milne and IH Witten. 2008. An effective, low-
cost measure of semantic relatedness obtained from
Wikipedia links. In Proc. of AAAI Workshop on
Wikipedia and Artificial Intelligence, pages 25–30,
Chicago, USA.
H Misra, O Cappe, and F Yvon. 2008. Using LDA to
detect semantically incoherent documents. In Proc. of
CoNLL 2008, pages 41–48, Manchester, England.
D Newman, S Karimi, and L Cavedon. 2009. External
evaluation of topic models. In Proc. of ADCS 2009,
pages 11–18, Sydney, Australia.
D Newman, T Baldwin, L Cavedon, S Karimi, D Mar-
tinez, and J Zobel. to appeara. Visualizing docu-
ment collections and search results using topic map-
ping. Journal of Web Semantics.
D Newman, Y Noh, E Talley, S Karimi, and T Bald-
win. to appearb. Evaluating topic models for digital
libraries. In Proc. of JCDL/ICADL 2010, Gold Coast,
Australia.
K Papineni, S Roukos, T Ward, and W-J Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL 2002, pages 311–318,
Philadelphia, USA.
P Pecina. 2008. Lexical Association Measures: Colloca-
tion Extraction. Ph.D. thesis, Charles University.
P Resnik. 1995. Using information content to evalu-
ate semantic similarity in a taxonomy. In Proc. of IJ-
CAI’95, pages 448–453, Montreal, Canada.
H Sch¨utze. 1998. Automatic word sense discrimination.
Computational Linguistics, 24(1):97–123.
M Str¨ube and SP Ponzetto. 2006. WikiRelate! comput-
ing semantic relateness using Wikipedia. In Proc. of
AAAI’06, pages 1419–1424, Boston, USA.
Q Sun, R Li, D Luo, and X Wu. 2008. Text segmentation
with LDA-based Fisher kernel. In Proc. of ACL-08:
HLT, pages 269–272.
HM Wallach, I Murray, R Salakhutdinov, and
DM Mimno. 2009. Evaluation methods for
topic models. In Proc. of ICML 2009, page 139.
D Widdows and K Ferraro. 2008. Semantic Vectors:
A scalable open source package and online technol-
ogy management application. In Proc. of LREC 2008,
Marrakech, Morocco.
Z Wu and M Palmer. 1994. Verb selection and lexical
selection. In Proc. of ACL’94, pages 133–138, Las
Cruces, USA.
</reference>
<page confidence="0.99837">
108
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.982294">
<title confidence="0.999783">Automatic Evaluation of Topic Coherence</title>
<author confidence="0.999508">Han Karl</author>
<affiliation confidence="0.99756575">Victoria Research Laboratory, of Computer Science, University of California, of Computer Science and Software Engineering, University of Melbourne, of Information Systems, University of Melbourne,</affiliation>
<abstract confidence="0.99960165">This paper introduces the novel task of topic coherence evaluation, whereby a set of words, as generated by a topic model, is rated for coherence or interpretability. We apply a range of topic scoring models to the evaluation drawing on Wikipedia the engine, and existing research on lexical similarity/relatedness. In comparison with human scores for a set of learned topics over two distinct datasets, we show a simple co-occurrence measure based on pointmutual information over is able to achieve results for the task at or nearing the level of inter-annotator correlaand that other lexical relatedness methods also achieve strong restrong, if less consisresults, while our results over are patchy at best.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>E Alfonseca</author>
<author>K Hall</author>
<author>J Kravalova</author>
<author>M Pas¸ca</author>
<author>A Soroa</author>
</authors>
<title>A study on similarity and relatedness using distributional and WordNet-based approaches.</title>
<date>2009</date>
<booktitle>In Proc. of HLT: NAACL</booktitle>
<pages>19--27</pages>
<location>Boulder, Colorado.</location>
<marker>Agirre, Alfonseca, Hall, Kravalova, Pas¸ca, Soroa, 2009</marker>
<rawString>E Agirre, E Alfonseca, K Hall, J Kravalova, M Pas¸ca, and A Soroa. 2009. A study on similarity and relatedness using distributional and WordNet-based approaches. In Proc. of HLT: NAACL 2009, pages 19– 27, Boulder, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Banerjee</author>
<author>T Pedersen</author>
</authors>
<title>An adapted Lesk algorithm for word sense disambiguation using WordNet.</title>
<date>2002</date>
<booktitle>Proc. of CICLing’02,</booktitle>
<pages>136--145</pages>
<contexts>
<context position="15775" citStr="Banerjee and Pedersen (2002)" startWordPosition="2557" endWordPosition="2560">erent manner: simjcn(c1, c2) = 1 IC(a) + IC(b) − 2 x IC(lcsa,b) Instead of defining commonality and difference as with Lin’s measure, the key determinant is the specificity of the two nodes compared with their LCS. Lesk (LESK) Lesk (1986) proposed a significantly different approach to lexical similarity to that proposed in the methods presented above, using the lexical overlap in dictionary definitions (or glosses) to disambiguate word sense. The sense definitions that contain the most words in common indicate the most likely sense of the word given its co-occurrence with similar word senses. Banerjee and Pedersen (2002) simlch(c1, c2) = − log 2 · D sp(c1, c2) 2 x log p(lcsc1,c2) 103 adapted this method to utilise WordNet sense glosses rather than dictionary definitions, and expand the dictionary definitions via ontological links, and it is this method we experiment with in this paper. Vector (VECTOR) Sch¨utze (1998) uses the words surrounding a term in a piece of text to form a context vector that describes the context in which the word sense appears. For a set of words associated with a target sense, a context vector is computed as the centroid vector of these words. The centroid context vectors each repres</context>
</contexts>
<marker>Banerjee, Pedersen, 2002</marker>
<rawString>S Banerjee and T Pedersen. 2002. An adapted Lesk algorithm for word sense disambiguation using WordNet. Proc. of CICLing’02, pages 136–145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>DM Blei</author>
<author>AY Ng</author>
<author>MI Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="1756" citStr="Blei et al., 2003" startWordPosition="250" endWordPosition="253">ness methods also achieve strong results. Google produces strong, if less consistent, results, while our results over WordNet are patchy at best. 1 Introduction There has traditionally been strong interest within computational linguistics in techniques for learning sets of words (aka topics) which capture the latent semantics of a document or document collection, in the form of methods such as latent semantic analysis (Deerwester et al., 1990), probabilistic latent semantic analysis (Hofmann, 2001), random projection (Widdows and Ferraro, 2008), and more recently, latent Dirichlet allocation (Blei et al., 2003; Griffiths and Steyvers, 2004). Such methods have been successfully applied to a myriad of tasks including word sense discrimination (Brody and Lapata, 2009), document summarisation (Haghighi and Vanderwende, 2009), areal linguistic analysis (Daume III, 2009) and text segmentation (Sun et al., 2008). In each case, extrinsic evaluation has been used to demonstrate the effectiveness of the learned topics in the application domain, but standardly, no attempt has been made to perform intrinsic evaluation of the topics themselves, either qualitatively or quantitatively. In machine learning, on the</context>
<context position="7659" citStr="Blei et al. (2003)" startWordPosition="1170" endWordPosition="1173">od for evaluating a well-grounded task, which achieves near-human correlation. 3 Topic Modelling In order to evaluate topic modelling, we require a topic model and set of topics for a given document collection. While the evaluation methodology we describe generalises to any method which generates sets of words, all of our experiments are based on Latent Dirichlet Allocation (LDA, aka Discrete Principal Component Analysis), on the grounds that it is a state-of-the-art method for generating topics. LDA is a Bayesian graphical model for text document collections represented by bags-of-words (see Blei et al. (2003), Griffiths and Steyvers (2004), Buntine and Jakulin (2004)). In a topic model, each document in the collection of D documents is modelled as a multinomial distribution over T topics, where each topic is a multinomial distribution over W words. Typically, only a small number of words are important (have high likelihood) in each topic, and only a small number of topics are present in each document. The collapsed Gibbs sampled topic model simultaneously learns the topics and the mixture of topics in documents by iteratively sampling the topic assignment z to every word in every document, using t</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>DM Blei, AY Ng, and MI Jordan. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Brody</author>
<author>M Lapata</author>
</authors>
<title>Bayesian word sense induction.</title>
<date>2009</date>
<booktitle>In Proc. of EACL 2009,</booktitle>
<pages>103--111</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="1914" citStr="Brody and Lapata, 2009" startWordPosition="274" endWordPosition="277">oduction There has traditionally been strong interest within computational linguistics in techniques for learning sets of words (aka topics) which capture the latent semantics of a document or document collection, in the form of methods such as latent semantic analysis (Deerwester et al., 1990), probabilistic latent semantic analysis (Hofmann, 2001), random projection (Widdows and Ferraro, 2008), and more recently, latent Dirichlet allocation (Blei et al., 2003; Griffiths and Steyvers, 2004). Such methods have been successfully applied to a myriad of tasks including word sense discrimination (Brody and Lapata, 2009), document summarisation (Haghighi and Vanderwende, 2009), areal linguistic analysis (Daume III, 2009) and text segmentation (Sun et al., 2008). In each case, extrinsic evaluation has been used to demonstrate the effectiveness of the learned topics in the application domain, but standardly, no attempt has been made to perform intrinsic evaluation of the topics themselves, either qualitatively or quantitatively. In machine learning, on the other hand, researchers have modified and extended topic models in a variety of ways, and evaluated intrinsically in terms of model perplexity (Wallach et al</context>
</contexts>
<marker>Brody, Lapata, 2009</marker>
<rawString>S Brody and M Lapata. 2009. Bayesian word sense induction. In Proc. of EACL 2009, pages 103–111, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Budanitsky</author>
<author>G Hirst</author>
</authors>
<title>Evaluating WordNetbased Measures of Lexical Sematic Relatedness.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>1</issue>
<contexts>
<context position="28466" citStr="Budanitsky and Hirst, 2005" startWordPosition="4710" endWordPosition="4713">for BOOKS tend to be more varied in word class than for NEWS, and contain less proper names; also, the genre of BOOKS is less well represented on the web. We hypothesise that Wikipedia’s encyclopedic nature means that it has good coverage over both domains, and thus more robust. Turning to WordNet, the overall results are markedly better over BOOKS, again largely because of the relative sparsity of proper names in the resource. The results for individual methods are somewhat surprising. Whereas JCN and LCH have been shown to be two of the best-performing methods over lexical similarity tasks (Budanitsky and Hirst, 2005; Agirre et al., 2009), they perform abysmally at the topic scoring task. Indeed, the spread of results across the WordNet similarity methods (no3Note that the choice of mean or median for IAA is independent of that for the scoring methods, as they are combining different things: annotator scores in the one hand, and word/concept pair scores on the other. tably HSO, JCN, LCH, LIN, RES and WUP) is much greater than we had expected. The single most consistent method is LESK, which is based on lexical overlap in definition sentences and makes relatively modest use of the WordNet hierarchy. Supple</context>
</contexts>
<marker>Budanitsky, Hirst, 2005</marker>
<rawString>A Budanitsky and G Hirst. 2005. Evaluating WordNetbased Measures of Lexical Sematic Relatedness. Computational Linguistics, 32(1):13–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>WL Buntine</author>
<author>A Jakulin</author>
</authors>
<title>Applying discrete PCA in data analysis.</title>
<date>2004</date>
<booktitle>In Proc. of UAI</booktitle>
<pages>59--66</pages>
<contexts>
<context position="7718" citStr="Buntine and Jakulin (2004)" startWordPosition="1178" endWordPosition="1181">ves near-human correlation. 3 Topic Modelling In order to evaluate topic modelling, we require a topic model and set of topics for a given document collection. While the evaluation methodology we describe generalises to any method which generates sets of words, all of our experiments are based on Latent Dirichlet Allocation (LDA, aka Discrete Principal Component Analysis), on the grounds that it is a state-of-the-art method for generating topics. LDA is a Bayesian graphical model for text document collections represented by bags-of-words (see Blei et al. (2003), Griffiths and Steyvers (2004), Buntine and Jakulin (2004)). In a topic model, each document in the collection of D documents is modelled as a multinomial distribution over T topics, where each topic is a multinomial distribution over W words. Typically, only a small number of words are important (have high likelihood) in each topic, and only a small number of topics are present in each document. The collapsed Gibbs sampled topic model simultaneously learns the topics and the mixture of topics in documents by iteratively sampling the topic assignment z to every word in every document, using the Gibbs sampling update: p(zid = t xid = w, Z—id) OC N�id </context>
</contexts>
<marker>Buntine, Jakulin, 2004</marker>
<rawString>WL Buntine and A Jakulin. 2004. Applying discrete PCA in data analysis. In Proc. of UAI 2004, pages 59–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Chang</author>
<author>J Boyd-Graber</author>
<author>S Gerris</author>
<author>C Wang</author>
<author>D Blei</author>
</authors>
<title>Reading tea leaves: How humans interpret topic models.</title>
<date>2009</date>
<booktitle>In Proc. of NIPS</booktitle>
<contexts>
<context position="5907" citStr="Chang et al. (2009)" startWordPosition="898" endWordPosition="901"> topic models, which could have a far greater impact on the overall value of topic modeling for end-user applications. Some researchers have started to address this problem, including Mei et al. (2007) who presented approaches for automatic labeling of topics (which is core to the question of coherence and semantic interpretability), and Griffiths and Steyvers (2006) who applied topic models to word sense discrimination tasks. Misra et al. (2008) used topic modelling to identify semantically incoherent documents within a document collection (vs. coherent topics, as targeted in this research). Chang et al. (2009) presented the first human-evaluation of topic models by creating a task where humans were asked to identify which word in a list of five topic words had been randomly switched with a word from another topic. This work showed some possibly counter-intuitive results, where in some cases humans preferred models with higher perplexity. This type of result shows the need for further exploring measures other than perplexity for evaluating topic models. In earlier work, we carried out preliminary experimentation using pointwise mutual information and Google results to evaluate topic coherence over t</context>
</contexts>
<marker>Chang, Boyd-Graber, Gerris, Wang, Blei, 2009</marker>
<rawString>J Chang, J Boyd-Graber, S Gerris, C Wang, and D Blei. 2009. Reading tea leaves: How humans interpret topic models. In Proc. of NIPS 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Daume</author>
</authors>
<title>Non-parametric bayesian areal linguistics.</title>
<date>2009</date>
<booktitle>In Proc. of HLT: NAACL</booktitle>
<pages>593--601</pages>
<location>Boulder, USA.</location>
<marker>Daume, 2009</marker>
<rawString>H Daume III. 2009. Non-parametric bayesian areal linguistics. In Proc. of HLT: NAACL 2009, pages 593– 601, Boulder, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Deerwester</author>
<author>Susan T Dumais</author>
<author>George W Furnas</author>
<author>Thomas K Landauer</author>
<author>Richard Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society of Information Science,</journal>
<volume>41</volume>
<issue>6</issue>
<contexts>
<context position="1586" citStr="Deerwester et al., 1990" startWordPosition="226" endWordPosition="229">al information over Wikipedia data is able to achieve results for the task at or nearing the level of inter-annotator correlation, and that other Wikipedia-based lexical relatedness methods also achieve strong results. Google produces strong, if less consistent, results, while our results over WordNet are patchy at best. 1 Introduction There has traditionally been strong interest within computational linguistics in techniques for learning sets of words (aka topics) which capture the latent semantics of a document or document collection, in the form of methods such as latent semantic analysis (Deerwester et al., 1990), probabilistic latent semantic analysis (Hofmann, 2001), random projection (Widdows and Ferraro, 2008), and more recently, latent Dirichlet allocation (Blei et al., 2003; Griffiths and Steyvers, 2004). Such methods have been successfully applied to a myriad of tasks including word sense discrimination (Brody and Lapata, 2009), document summarisation (Haghighi and Vanderwende, 2009), areal linguistic analysis (Daume III, 2009) and text segmentation (Sun et al., 2008). In each case, extrinsic evaluation has been used to demonstrate the effectiveness of the learned topics in the application doma</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society of Information Science, 41(6).</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>C Fellbaum, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, USA.</location>
<contexts>
<context position="12539" citStr="(1998)" startWordPosition="2019" endWordPosition="2019">he hierarchy. This &apos;We also experimented with the median, and trialled filtering the set of senses in a variety of ways, e.g. using only the first sense (the sense with the highest prior) for a given word, or using only the word senses associated with the POS with the highest prior. In all cases, the overall trend was for the correlation with the human scores to drop relative to the mean, so we only present the numbers for the mean in this paper. 102 count of nodes includes the beginning and ending word nodes. Leacock-Chodorow (LCH) The measure of semantic similarity devised by Leacock et al. (1998) finds the shortest path between two WordNet synsets (sp(c1, c2)) using hypernym and synonym relationships. This path length is then scaled by the maximum depth of WordNet (D), and the log likelihood taken: Wu-Palmer (WUP) Wu and Palmer (1994) proposed to scale the depth of the two synset nodes (depthc1 and depthc2) by the depth of their LCS (depth(lcsc1,c2)): simwup(c1, c2) = 2 · depth(lcsc1,c2) depthc1 + depthc2 + 2 · depth(lcsc1,c2) The scaling means that specific terms (deeper in the hierarchy) that are close together are more semantically similar than more general terms, which have a shor</context>
<context position="14719" citStr="(1998)" startWordPosition="2386" endWordPosition="2386">ik Information Content (RES) Resnik (1995) presents a method for weighting edges in WordNet (avoiding the assumption that all edges between nodes have equal importance), by weighting edges between nodes by their frequency of use in textual corpora. Resnik found that the most effective measure of comparison using this methodology was to measure the Information Content (IC(c) = − log p(c)) of the subsumer with the greatest Information Content from the set of all concepts that subsumed the two initial concepts (S(c1, c2)) being compared: simres(c1, c2) = max [− log p(c)] cES(c1,c2) Lin (LIN) Lin (1998) expanded on the Information Theoretic approach presented by Resnik by scaling the Information Content of each node by the information content of their LCS: simlin(c1,c2) = log p(c1) + log p(c2) This measure contrasts the joint content of the two concepts with the difference between them. Jiang-Conrath (JCN) Jiang and Conrath (1997) define a measure that utilises the components of the information content of the LCS in a different manner: simjcn(c1, c2) = 1 IC(a) + IC(b) − 2 x IC(lcsa,b) Instead of defining commonality and difference as with Lin’s measure, the key determinant is the specificity</context>
<context position="16077" citStr="(1998)" startWordPosition="2610" endWordPosition="2610"> the methods presented above, using the lexical overlap in dictionary definitions (or glosses) to disambiguate word sense. The sense definitions that contain the most words in common indicate the most likely sense of the word given its co-occurrence with similar word senses. Banerjee and Pedersen (2002) simlch(c1, c2) = − log 2 · D sp(c1, c2) 2 x log p(lcsc1,c2) 103 adapted this method to utilise WordNet sense glosses rather than dictionary definitions, and expand the dictionary definitions via ontological links, and it is this method we experiment with in this paper. Vector (VECTOR) Sch¨utze (1998) uses the words surrounding a term in a piece of text to form a context vector that describes the context in which the word sense appears. For a set of words associated with a target sense, a context vector is computed as the centroid vector of these words. The centroid context vectors each represent a word sense. To compare word senses, the cosine similarity of the context vectors is used. 4.2 Wikipedia In the last few years, there has been a surge of interest in using Wikipedia to calculate semantic similarity, using the Wikipedia article content, in-article links and document categories (St</context>
</contexts>
<marker>1998</marker>
<rawString>C Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Gabrilovich</author>
<author>S Markovitch</author>
</authors>
<title>Computing semantic relatedness using Wikipedia-based explicit semantic analysis.</title>
<date>2007</date>
<booktitle>In Proc. of IJCAI’07,</booktitle>
<pages>1606--1611</pages>
<location>Hyderabad, India.</location>
<contexts>
<context position="16735" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="2720" endWordPosition="2723">g a term in a piece of text to form a context vector that describes the context in which the word sense appears. For a set of words associated with a target sense, a context vector is computed as the centroid vector of these words. The centroid context vectors each represent a word sense. To compare word senses, the cosine similarity of the context vectors is used. 4.2 Wikipedia In the last few years, there has been a surge of interest in using Wikipedia to calculate semantic similarity, using the Wikipedia article content, in-article links and document categories (Str¨ube and Ponzetto, 2006; Gabrilovich and Markovitch, 2007; Milne and Witten, 2008). We present a selection of such methods below. There are a number of Wikipedia-based scoring methods which we do not present results for here (notably Str¨ube and Ponzetto (2006) and Gabrilovich and Markovitch (2007)), due to their computational complexity and uncertainty about the full implementation details of the methods. As with WordNet, a given word will often have multiple entries in Wikipedia, grouped in a disambiguation page. For MIW, RACO and DOCSIM, we apply the same strategy as we did with WordNet, in exhaustively calculating the pairwise scores between the</context>
</contexts>
<marker>Gabrilovich, Markovitch, 2007</marker>
<rawString>E Gabrilovich and S Markovitch. 2007. Computing semantic relatedness using Wikipedia-based explicit semantic analysis. In Proc. of IJCAI’07, pages 1606– 1611, Hyderabad, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Griffiths</author>
<author>M Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>In Proc. of the National Academy of Sciences,</booktitle>
<volume>101</volume>
<pages>5228--5235</pages>
<contexts>
<context position="1787" citStr="Griffiths and Steyvers, 2004" startWordPosition="254" endWordPosition="257">chieve strong results. Google produces strong, if less consistent, results, while our results over WordNet are patchy at best. 1 Introduction There has traditionally been strong interest within computational linguistics in techniques for learning sets of words (aka topics) which capture the latent semantics of a document or document collection, in the form of methods such as latent semantic analysis (Deerwester et al., 1990), probabilistic latent semantic analysis (Hofmann, 2001), random projection (Widdows and Ferraro, 2008), and more recently, latent Dirichlet allocation (Blei et al., 2003; Griffiths and Steyvers, 2004). Such methods have been successfully applied to a myriad of tasks including word sense discrimination (Brody and Lapata, 2009), document summarisation (Haghighi and Vanderwende, 2009), areal linguistic analysis (Daume III, 2009) and text segmentation (Sun et al., 2008). In each case, extrinsic evaluation has been used to demonstrate the effectiveness of the learned topics in the application domain, but standardly, no attempt has been made to perform intrinsic evaluation of the topics themselves, either qualitatively or quantitatively. In machine learning, on the other hand, researchers have m</context>
<context position="7690" citStr="Griffiths and Steyvers (2004)" startWordPosition="1174" endWordPosition="1177">well-grounded task, which achieves near-human correlation. 3 Topic Modelling In order to evaluate topic modelling, we require a topic model and set of topics for a given document collection. While the evaluation methodology we describe generalises to any method which generates sets of words, all of our experiments are based on Latent Dirichlet Allocation (LDA, aka Discrete Principal Component Analysis), on the grounds that it is a state-of-the-art method for generating topics. LDA is a Bayesian graphical model for text document collections represented by bags-of-words (see Blei et al. (2003), Griffiths and Steyvers (2004), Buntine and Jakulin (2004)). In a topic model, each document in the collection of D documents is modelled as a multinomial distribution over T topics, where each topic is a multinomial distribution over W words. Typically, only a small number of words are important (have high likelihood) in each topic, and only a small number of topics are present in each document. The collapsed Gibbs sampled topic model simultaneously learns the topics and the mixture of topics in documents by iteratively sampling the topic assignment z to every word in every document, using the Gibbs sampling update: p(zid</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>T Griffiths and M Steyvers. 2004. Finding scientific topics. In Proc. of the National Academy of Sciences, volume 101, pages 5228–5235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Griffiths</author>
<author>M Steyvers</author>
</authors>
<title>Probabilistic topic models. In Latent Semantic Analysis: A Road to Meaning.</title>
<date>2006</date>
<contexts>
<context position="5657" citStr="Griffiths and Steyvers (2006)" startWordPosition="860" endWordPosition="863">unbiased methods for computing perplexity and evaluating almost any type of topic model. While statistical evaluation of topic models is reasonably well understood, there has been much less work on evaluating the intrinsic semantic quality of topics learned by topic models, which could have a far greater impact on the overall value of topic modeling for end-user applications. Some researchers have started to address this problem, including Mei et al. (2007) who presented approaches for automatic labeling of topics (which is core to the question of coherence and semantic interpretability), and Griffiths and Steyvers (2006) who applied topic models to word sense discrimination tasks. Misra et al. (2008) used topic modelling to identify semantically incoherent documents within a document collection (vs. coherent topics, as targeted in this research). Chang et al. (2009) presented the first human-evaluation of topic models by creating a task where humans were asked to identify which word in a list of five topic words had been randomly switched with a word from another topic. This work showed some possibly counter-intuitive results, where in some cases humans preferred models with higher perplexity. This type of re</context>
</contexts>
<marker>Griffiths, Steyvers, 2006</marker>
<rawString>T Griffiths and M Steyvers. 2006. Probabilistic topic models. In Latent Semantic Analysis: A Road to Meaning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Haghighi</author>
<author>L Vanderwende</author>
</authors>
<title>Exploring content models for multi-document summarization.</title>
<date>2009</date>
<booktitle>In Proc. of HLT: NAACL 2009,</booktitle>
<pages>362--370</pages>
<location>Boulder, USA.</location>
<contexts>
<context position="1971" citStr="Haghighi and Vanderwende, 2009" startWordPosition="281" endWordPosition="284">rest within computational linguistics in techniques for learning sets of words (aka topics) which capture the latent semantics of a document or document collection, in the form of methods such as latent semantic analysis (Deerwester et al., 1990), probabilistic latent semantic analysis (Hofmann, 2001), random projection (Widdows and Ferraro, 2008), and more recently, latent Dirichlet allocation (Blei et al., 2003; Griffiths and Steyvers, 2004). Such methods have been successfully applied to a myriad of tasks including word sense discrimination (Brody and Lapata, 2009), document summarisation (Haghighi and Vanderwende, 2009), areal linguistic analysis (Daume III, 2009) and text segmentation (Sun et al., 2008). In each case, extrinsic evaluation has been used to demonstrate the effectiveness of the learned topics in the application domain, but standardly, no attempt has been made to perform intrinsic evaluation of the topics themselves, either qualitatively or quantitatively. In machine learning, on the other hand, researchers have modified and extended topic models in a variety of ways, and evaluated intrinsically in terms of model perplexity (Wallach et al., 2009), but there has been less effort on qualitative u</context>
</contexts>
<marker>Haghighi, Vanderwende, 2009</marker>
<rawString>A Haghighi and L Vanderwende. 2009. Exploring content models for multi-document summarization. In Proc. of HLT: NAACL 2009, pages 362–370, Boulder, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Hirst</author>
<author>D St-Onge</author>
</authors>
<title>Lexical chains as representations of context for the detection and correction of malapropism. In Fellbaum (Fellbaum,</title>
<date>1998</date>
<pages>305--332</pages>
<contexts>
<context position="13342" citStr="Hirst and St-Onge (1998)" startWordPosition="2148" endWordPosition="2151">d the log likelihood taken: Wu-Palmer (WUP) Wu and Palmer (1994) proposed to scale the depth of the two synset nodes (depthc1 and depthc2) by the depth of their LCS (depth(lcsc1,c2)): simwup(c1, c2) = 2 · depth(lcsc1,c2) depthc1 + depthc2 + 2 · depth(lcsc1,c2) The scaling means that specific terms (deeper in the hierarchy) that are close together are more semantically similar than more general terms, which have a short path distance between them. Only hypernym relationships are used in this measure, as the LCS is defined by the common member in the concepts’ hypernym path. Hirst-St Onge (HSO) Hirst and St-Onge (1998) define a measure of semantic similarity based on length and tortuosity of the path between nodes. Hirst and St-Onge attribute directions (up, down and horizontal) to the larger set of WordNet relationships, and identify the path from one word to another utilising all of these relationships. The relatedness score is then computed by the weighted sum of the path length between the two words (len(c1, c2)) and the number of turns the path makes (turns(c1, c2)) to take this route: relhso(c1,c2) = C − len(c1, c2) − k x turns(c1, c2) where C and k are constants. Additionally, a set of restrictions i</context>
</contexts>
<marker>Hirst, St-Onge, 1998</marker>
<rawString>G Hirst and D St-Onge. 1998. Lexical chains as representations of context for the detection and correction of malapropism. In Fellbaum (Fellbaum, 1998), pages 305–332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hofmann</author>
</authors>
<title>Unsupervised learning by probabilistic latent semantic analysis.</title>
<date>2001</date>
<booktitle>Machine Learning,</booktitle>
<volume>42</volume>
<issue>1</issue>
<contexts>
<context position="1642" citStr="Hofmann, 2001" startWordPosition="235" endWordPosition="236">the task at or nearing the level of inter-annotator correlation, and that other Wikipedia-based lexical relatedness methods also achieve strong results. Google produces strong, if less consistent, results, while our results over WordNet are patchy at best. 1 Introduction There has traditionally been strong interest within computational linguistics in techniques for learning sets of words (aka topics) which capture the latent semantics of a document or document collection, in the form of methods such as latent semantic analysis (Deerwester et al., 1990), probabilistic latent semantic analysis (Hofmann, 2001), random projection (Widdows and Ferraro, 2008), and more recently, latent Dirichlet allocation (Blei et al., 2003; Griffiths and Steyvers, 2004). Such methods have been successfully applied to a myriad of tasks including word sense discrimination (Brody and Lapata, 2009), document summarisation (Haghighi and Vanderwende, 2009), areal linguistic analysis (Daume III, 2009) and text segmentation (Sun et al., 2008). In each case, extrinsic evaluation has been used to demonstrate the effectiveness of the learned topics in the application domain, but standardly, no attempt has been made to perform </context>
</contexts>
<marker>Hofmann, 2001</marker>
<rawString>T Hofmann. 2001. Unsupervised learning by probabilistic latent semantic analysis. Machine Learning, 42(1):177–196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>JJ Jiang</author>
<author>DW Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy.</title>
<date>1997</date>
<booktitle>In Proc. of COLING’97,</booktitle>
<pages>19--33</pages>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="15053" citStr="Jiang and Conrath (1997)" startWordPosition="2437" endWordPosition="2440">is methodology was to measure the Information Content (IC(c) = − log p(c)) of the subsumer with the greatest Information Content from the set of all concepts that subsumed the two initial concepts (S(c1, c2)) being compared: simres(c1, c2) = max [− log p(c)] cES(c1,c2) Lin (LIN) Lin (1998) expanded on the Information Theoretic approach presented by Resnik by scaling the Information Content of each node by the information content of their LCS: simlin(c1,c2) = log p(c1) + log p(c2) This measure contrasts the joint content of the two concepts with the difference between them. Jiang-Conrath (JCN) Jiang and Conrath (1997) define a measure that utilises the components of the information content of the LCS in a different manner: simjcn(c1, c2) = 1 IC(a) + IC(b) − 2 x IC(lcsa,b) Instead of defining commonality and difference as with Lin’s measure, the key determinant is the specificity of the two nodes compared with their LCS. Lesk (LESK) Lesk (1986) proposed a significantly different approach to lexical similarity to that proposed in the methods presented above, using the lexical overlap in dictionary definitions (or glosses) to disambiguate word sense. The sense definitions that contain the most words in common</context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>JJ Jiang and DW Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. In Proc. of COLING’97, pages 19–33, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Leacock</author>
<author>G A Miller</author>
<author>M Chodorow</author>
</authors>
<title>Using corpus statistics and WordNet relations for sense identification.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<contexts>
<context position="12539" citStr="Leacock et al. (1998)" startWordPosition="2016" endWordPosition="2019"> two words in the hierarchy. This &apos;We also experimented with the median, and trialled filtering the set of senses in a variety of ways, e.g. using only the first sense (the sense with the highest prior) for a given word, or using only the word senses associated with the POS with the highest prior. In all cases, the overall trend was for the correlation with the human scores to drop relative to the mean, so we only present the numbers for the mean in this paper. 102 count of nodes includes the beginning and ending word nodes. Leacock-Chodorow (LCH) The measure of semantic similarity devised by Leacock et al. (1998) finds the shortest path between two WordNet synsets (sp(c1, c2)) using hypernym and synonym relationships. This path length is then scaled by the maximum depth of WordNet (D), and the log likelihood taken: Wu-Palmer (WUP) Wu and Palmer (1994) proposed to scale the depth of the two synset nodes (depthc1 and depthc2) by the depth of their LCS (depth(lcsc1,c2)): simwup(c1, c2) = 2 · depth(lcsc1,c2) depthc1 + depthc2 + 2 · depth(lcsc1,c2) The scaling means that specific terms (deeper in the hierarchy) that are close together are more semantically similar than more general terms, which have a shor</context>
</contexts>
<marker>Leacock, Miller, Chodorow, 1998</marker>
<rawString>C Leacock, G A Miller, and M Chodorow. 1998. Using corpus statistics and WordNet relations for sense identification. Computational Linguistics, 24(1):147–65.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lesk</author>
</authors>
<title>Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone.</title>
<date>1986</date>
<booktitle>In Proc. of SIGDOC’86,</booktitle>
<pages>24--26</pages>
<location>Toronto, Canada.</location>
<contexts>
<context position="15385" citStr="Lesk (1986)" startWordPosition="2497" endWordPosition="2498">ed by Resnik by scaling the Information Content of each node by the information content of their LCS: simlin(c1,c2) = log p(c1) + log p(c2) This measure contrasts the joint content of the two concepts with the difference between them. Jiang-Conrath (JCN) Jiang and Conrath (1997) define a measure that utilises the components of the information content of the LCS in a different manner: simjcn(c1, c2) = 1 IC(a) + IC(b) − 2 x IC(lcsa,b) Instead of defining commonality and difference as with Lin’s measure, the key determinant is the specificity of the two nodes compared with their LCS. Lesk (LESK) Lesk (1986) proposed a significantly different approach to lexical similarity to that proposed in the methods presented above, using the lexical overlap in dictionary definitions (or glosses) to disambiguate word sense. The sense definitions that contain the most words in common indicate the most likely sense of the word given its co-occurrence with similar word senses. Banerjee and Pedersen (2002) simlch(c1, c2) = − log 2 · D sp(c1, c2) 2 x log p(lcsc1,c2) 103 adapted this method to utilise WordNet sense glosses rather than dictionary definitions, and expand the dictionary definitions via ontological li</context>
</contexts>
<marker>Lesk, 1986</marker>
<rawString>M Lesk. 1986. Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone. In Proc. of SIGDOC’86, pages 24–26, Toronto, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proc. of COLING/ACL’98,</booktitle>
<pages>768--774</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="14719" citStr="Lin (1998)" startWordPosition="2385" endWordPosition="2386">Resnik Information Content (RES) Resnik (1995) presents a method for weighting edges in WordNet (avoiding the assumption that all edges between nodes have equal importance), by weighting edges between nodes by their frequency of use in textual corpora. Resnik found that the most effective measure of comparison using this methodology was to measure the Information Content (IC(c) = − log p(c)) of the subsumer with the greatest Information Content from the set of all concepts that subsumed the two initial concepts (S(c1, c2)) being compared: simres(c1, c2) = max [− log p(c)] cES(c1,c2) Lin (LIN) Lin (1998) expanded on the Information Theoretic approach presented by Resnik by scaling the Information Content of each node by the information content of their LCS: simlin(c1,c2) = log p(c1) + log p(c2) This measure contrasts the joint content of the two concepts with the difference between them. Jiang-Conrath (JCN) Jiang and Conrath (1997) define a measure that utilises the components of the information content of the LCS in a different manner: simjcn(c1, c2) = 1 IC(a) + IC(b) − 2 x IC(lcsa,b) Instead of defining commonality and difference as with Lin’s measure, the key determinant is the specificity</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>D Lin. 1998. Automatic retrieval and clustering of similar words. In Proc. of COLING/ACL’98, pages 768– 774, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-Y Lin</author>
</authors>
<title>ROUGE: a package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In Proc. of the ACL 2004 Workshop on Text Summarization Branches Out (WAS 2004),</booktitle>
<pages>74--81</pages>
<location>Barcelona,</location>
<contexts>
<context position="6741" citStr="Lin, 2004" startWordPosition="1032" endWordPosition="1033">d some possibly counter-intuitive results, where in some cases humans preferred models with higher perplexity. This type of result shows the need for further exploring measures other than perplexity for evaluating topic models. In earlier work, we carried out preliminary experimentation using pointwise mutual information and Google results to evaluate topic coherence over the same set of topics as used in this research (Newman et al., 2009). Part of this research takes inspiration from the work on automatic evaluation in machine translation (Papineni et al., 2002) and automatic summarisation (Lin, 2004). Here, the development of automated methods with high correlation with human subjects has opened the door to large-scale automated evaluation of system outputs, revolutionising the respective fields. While our aspirations are more modest, the basic aim is the same: to develop a fully-automated method for evaluating a well-grounded task, which achieves near-human correlation. 3 Topic Modelling In order to evaluate topic modelling, we require a topic model and set of topics for a given document collection. While the evaluation methodology we describe generalises to any method which generates se</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>C-Y Lin. 2004. ROUGE: a package for automatic evaluation of summaries. In Proc. of the ACL 2004 Workshop on Text Summarization Branches Out (WAS 2004), pages 74–81, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Mei</author>
<author>X Shen</author>
<author>CX Zhai</author>
</authors>
<title>Automatic labeling of multinomial topic models.</title>
<date>2007</date>
<booktitle>In Proc. of KDD</booktitle>
<pages>490--499</pages>
<contexts>
<context position="5489" citStr="Mei et al. (2007)" startWordPosition="835" endWordPosition="838"> (e.g. number of topics T), and is the standard way of demonstrating the advantage of one model over another. Wallach et al. (2009) presented efficient and unbiased methods for computing perplexity and evaluating almost any type of topic model. While statistical evaluation of topic models is reasonably well understood, there has been much less work on evaluating the intrinsic semantic quality of topics learned by topic models, which could have a far greater impact on the overall value of topic modeling for end-user applications. Some researchers have started to address this problem, including Mei et al. (2007) who presented approaches for automatic labeling of topics (which is core to the question of coherence and semantic interpretability), and Griffiths and Steyvers (2006) who applied topic models to word sense discrimination tasks. Misra et al. (2008) used topic modelling to identify semantically incoherent documents within a document collection (vs. coherent topics, as targeted in this research). Chang et al. (2009) presented the first human-evaluation of topic models by creating a task where humans were asked to identify which word in a list of five topic words had been randomly switched with </context>
</contexts>
<marker>Mei, Shen, Zhai, 2007</marker>
<rawString>Q Mei, X Shen, and CX Zhai. 2007. Automatic labeling of multinomial topic models. In Proc. of KDD 2007, pages 490–499.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Milne</author>
<author>IH Witten</author>
</authors>
<title>An effective, lowcost measure of semantic relatedness obtained from Wikipedia links.</title>
<date>2008</date>
<booktitle>In Proc. of AAAI Workshop on Wikipedia and Artificial Intelligence,</booktitle>
<pages>25--30</pages>
<location>Chicago, USA.</location>
<contexts>
<context position="16760" citStr="Milne and Witten, 2008" startWordPosition="2724" endWordPosition="2727">m a context vector that describes the context in which the word sense appears. For a set of words associated with a target sense, a context vector is computed as the centroid vector of these words. The centroid context vectors each represent a word sense. To compare word senses, the cosine similarity of the context vectors is used. 4.2 Wikipedia In the last few years, there has been a surge of interest in using Wikipedia to calculate semantic similarity, using the Wikipedia article content, in-article links and document categories (Str¨ube and Ponzetto, 2006; Gabrilovich and Markovitch, 2007; Milne and Witten, 2008). We present a selection of such methods below. There are a number of Wikipedia-based scoring methods which we do not present results for here (notably Str¨ube and Ponzetto (2006) and Gabrilovich and Markovitch (2007)), due to their computational complexity and uncertainty about the full implementation details of the methods. As with WordNet, a given word will often have multiple entries in Wikipedia, grouped in a disambiguation page. For MIW, RACO and DOCSIM, we apply the same strategy as we did with WordNet, in exhaustively calculating the pairwise scores between the sets of documents associ</context>
</contexts>
<marker>Milne, Witten, 2008</marker>
<rawString>D Milne and IH Witten. 2008. An effective, lowcost measure of semantic relatedness obtained from Wikipedia links. In Proc. of AAAI Workshop on Wikipedia and Artificial Intelligence, pages 25–30, Chicago, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Misra</author>
<author>O Cappe</author>
<author>F Yvon</author>
</authors>
<title>Using LDA to detect semantically incoherent documents.</title>
<date>2008</date>
<booktitle>In Proc. of CoNLL</booktitle>
<pages>41--48</pages>
<location>Manchester, England.</location>
<contexts>
<context position="5738" citStr="Misra et al. (2008)" startWordPosition="873" endWordPosition="876">ile statistical evaluation of topic models is reasonably well understood, there has been much less work on evaluating the intrinsic semantic quality of topics learned by topic models, which could have a far greater impact on the overall value of topic modeling for end-user applications. Some researchers have started to address this problem, including Mei et al. (2007) who presented approaches for automatic labeling of topics (which is core to the question of coherence and semantic interpretability), and Griffiths and Steyvers (2006) who applied topic models to word sense discrimination tasks. Misra et al. (2008) used topic modelling to identify semantically incoherent documents within a document collection (vs. coherent topics, as targeted in this research). Chang et al. (2009) presented the first human-evaluation of topic models by creating a task where humans were asked to identify which word in a list of five topic words had been randomly switched with a word from another topic. This work showed some possibly counter-intuitive results, where in some cases humans preferred models with higher perplexity. This type of result shows the need for further exploring measures other than perplexity for eval</context>
</contexts>
<marker>Misra, Cappe, Yvon, 2008</marker>
<rawString>H Misra, O Cappe, and F Yvon. 2008. Using LDA to detect semantically incoherent documents. In Proc. of CoNLL 2008, pages 41–48, Manchester, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Newman</author>
<author>S Karimi</author>
<author>L Cavedon</author>
</authors>
<title>External evaluation of topic models.</title>
<date>2009</date>
<booktitle>In Proc. of ADCS 2009,</booktitle>
<pages>11--18</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="6575" citStr="Newman et al., 2009" startWordPosition="1006" endWordPosition="1009">odels by creating a task where humans were asked to identify which word in a list of five topic words had been randomly switched with a word from another topic. This work showed some possibly counter-intuitive results, where in some cases humans preferred models with higher perplexity. This type of result shows the need for further exploring measures other than perplexity for evaluating topic models. In earlier work, we carried out preliminary experimentation using pointwise mutual information and Google results to evaluate topic coherence over the same set of topics as used in this research (Newman et al., 2009). Part of this research takes inspiration from the work on automatic evaluation in machine translation (Papineni et al., 2002) and automatic summarisation (Lin, 2004). Here, the development of automated methods with high correlation with human subjects has opened the door to large-scale automated evaluation of system outputs, revolutionising the respective fields. While our aspirations are more modest, the basic aim is the same: to develop a fully-automated method for evaluating a well-grounded task, which achieves near-human correlation. 3 Topic Modelling In order to evaluate topic modelling,</context>
<context position="20785" citStr="Newman et al. (2009)" startWordPosition="3400" endWordPosition="3403">S] soon short longer carried rest turned raised filled turn allowed ... [BOOKS] act sense adv person ppr plant sax genus applied dis ... Table 1: A selection of high-scoring and low-scoring topics from the entire corpus of over two million English Wikipedia articles (—1 billion words). PMI has been studied variously in the context of collocation extraction (Pecina, 2008), and is one measure of the statistical independence of observing two words in close proximity. Using a sliding window of 10- words to identify co-occurrence, we computed the PMI of all a given word pair (wi, wj) as, following Newman et al. (2009): PMI(wi, wj) = log p(wi, wj) p(wi)p(wj) 4.3 Search engine-based similarity Finally, we present two search engine-based scoring methods, based on Newman et al. (2009). In this case the external data source is the entire World Wide Web, via the Google search engine. Unlike the methods presented above, here we query for the topic in its entirety,2 meaning that we return a topiclevel score rather than scores for individual word or word sense pairs. In each case, we mark each search term with the advanced search option + to search for the terms exactly as is and prevent Google from using synonyms </context>
</contexts>
<marker>Newman, Karimi, Cavedon, 2009</marker>
<rawString>D Newman, S Karimi, and L Cavedon. 2009. External evaluation of topic models. In Proc. of ADCS 2009, pages 11–18, Sydney, Australia.</rawString>
</citation>
<citation valid="false">
<authors>
<author>D Newman</author>
<author>T Baldwin</author>
<author>L Cavedon</author>
<author>S Karimi</author>
<author>D Martinez</author>
<author>J Zobel</author>
</authors>
<title>to appeara. Visualizing document collections and search results using topic mapping.</title>
<journal>Journal of Web Semantics.</journal>
<marker>Newman, Baldwin, Cavedon, Karimi, Martinez, Zobel, </marker>
<rawString>D Newman, T Baldwin, L Cavedon, S Karimi, D Martinez, and J Zobel. to appeara. Visualizing document collections and search results using topic mapping. Journal of Web Semantics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Newman</author>
<author>Y Noh</author>
<author>E Talley</author>
<author>S Karimi</author>
<author>T Baldwin</author>
</authors>
<title>to appearb. Evaluating topic models for digital libraries.</title>
<date>2010</date>
<booktitle>In Proc. of JCDL/ICADL</booktitle>
<location>Gold Coast, Australia.</location>
<marker>Newman, Noh, Talley, Karimi, Baldwin, 2010</marker>
<rawString>D Newman, Y Noh, E Talley, S Karimi, and T Baldwin. to appearb. Evaluating topic models for digital libraries. In Proc. of JCDL/ICADL 2010, Gold Coast, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>311--318</pages>
<location>Philadelphia, USA.</location>
<contexts>
<context position="6701" citStr="Papineni et al., 2002" startWordPosition="1025" endWordPosition="1028">ched with a word from another topic. This work showed some possibly counter-intuitive results, where in some cases humans preferred models with higher perplexity. This type of result shows the need for further exploring measures other than perplexity for evaluating topic models. In earlier work, we carried out preliminary experimentation using pointwise mutual information and Google results to evaluate topic coherence over the same set of topics as used in this research (Newman et al., 2009). Part of this research takes inspiration from the work on automatic evaluation in machine translation (Papineni et al., 2002) and automatic summarisation (Lin, 2004). Here, the development of automated methods with high correlation with human subjects has opened the door to large-scale automated evaluation of system outputs, revolutionising the respective fields. While our aspirations are more modest, the basic aim is the same: to develop a fully-automated method for evaluating a well-grounded task, which achieves near-human correlation. 3 Topic Modelling In order to evaluate topic modelling, we require a topic model and set of topics for a given document collection. While the evaluation methodology we describe gene</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K Papineni, S Roukos, T Ward, and W-J Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proc. of ACL 2002, pages 311–318, Philadelphia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pecina</author>
</authors>
<title>Lexical Association Measures: Collocation Extraction.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>Charles University.</institution>
<contexts>
<context position="20538" citStr="Pecina, 2008" startWordPosition="3359" endWordPosition="3360">hair table cabinet wood leg mahogany piece oak louis ... Selected low-scoring topics (unanimous score=1): [NEWS] king bond berry bill ray rate james treas byrd key ... [NEWS] dog moment hand face love self eye turn young character ... [BOOKS] soon short longer carried rest turned raised filled turn allowed ... [BOOKS] act sense adv person ppr plant sax genus applied dis ... Table 1: A selection of high-scoring and low-scoring topics from the entire corpus of over two million English Wikipedia articles (—1 billion words). PMI has been studied variously in the context of collocation extraction (Pecina, 2008), and is one measure of the statistical independence of observing two words in close proximity. Using a sliding window of 10- words to identify co-occurrence, we computed the PMI of all a given word pair (wi, wj) as, following Newman et al. (2009): PMI(wi, wj) = log p(wi, wj) p(wi)p(wj) 4.3 Search engine-based similarity Finally, we present two search engine-based scoring methods, based on Newman et al. (2009). In this case the external data source is the entire World Wide Web, via the Google search engine. Unlike the methods presented above, here we query for the topic in its entirety,2 meani</context>
</contexts>
<marker>Pecina, 2008</marker>
<rawString>P Pecina. 2008. Lexical Association Measures: Collocation Extraction. Ph.D. thesis, Charles University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Using information content to evaluate semantic similarity in a taxonomy.</title>
<date>1995</date>
<booktitle>In Proc. of IJCAI’95,</booktitle>
<pages>448--453</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="14155" citStr="Resnik (1995)" startWordPosition="2294" endWordPosition="2295">nships, and identify the path from one word to another utilising all of these relationships. The relatedness score is then computed by the weighted sum of the path length between the two words (len(c1, c2)) and the number of turns the path makes (turns(c1, c2)) to take this route: relhso(c1,c2) = C − len(c1, c2) − k x turns(c1, c2) where C and k are constants. Additionally, a set of restrictions is placed on the path so that it may not be more than a certain length, may not contain more than a set number of turns, and may only take turns in certain directions. Resnik Information Content (RES) Resnik (1995) presents a method for weighting edges in WordNet (avoiding the assumption that all edges between nodes have equal importance), by weighting edges between nodes by their frequency of use in textual corpora. Resnik found that the most effective measure of comparison using this methodology was to measure the Information Content (IC(c) = − log p(c)) of the subsumer with the greatest Information Content from the set of all concepts that subsumed the two initial concepts (S(c1, c2)) being compared: simres(c1, c2) = max [− log p(c)] cES(c1,c2) Lin (LIN) Lin (1998) expanded on the Information Theoret</context>
<context position="17476" citStr="Resnik (1995)" startWordPosition="2840" endWordPosition="2841">hich we do not present results for here (notably Str¨ube and Ponzetto (2006) and Gabrilovich and Markovitch (2007)), due to their computational complexity and uncertainty about the full implementation details of the methods. As with WordNet, a given word will often have multiple entries in Wikipedia, grouped in a disambiguation page. For MIW, RACO and DOCSIM, we apply the same strategy as we did with WordNet, in exhaustively calculating the pairwise scores between the sets of documents associated with each term, and averaging across them. Milne-Witten (MIW) Milne and Witten (2008) adapted the Resnik (1995) methodology to utilise the count of links pointing to an article. As Wikipedia is selfreferential (articles link to related articles), no external data is needed to find the “referred-to-edness” of a concept. Milne and Witten use an adapted Information Content measure that weights the number of links from one article to another (c1 → c2) by the total number of links to the second article: �w(c1 → c2) = |c1 → c2 |× log xEW where x is an article in W, Wikipedia. This measure provides the similarity of one article to another, however this is asymmetrical. The above metric is used to find the wei</context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>P Resnik. 1995. Using information content to evaluate semantic similarity in a taxonomy. In Proc. of IJCAI’95, pages 448–453, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Sch¨utze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<marker>Sch¨utze, 1998</marker>
<rawString>H Sch¨utze. 1998. Automatic word sense discrimination. Computational Linguistics, 24(1):97–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Str¨ube</author>
<author>SP Ponzetto</author>
</authors>
<title>WikiRelate! computing semantic relateness using Wikipedia.</title>
<date>2006</date>
<booktitle>In Proc. of AAAI’06,</booktitle>
<pages>1419--1424</pages>
<location>Boston, USA.</location>
<marker>Str¨ube, Ponzetto, 2006</marker>
<rawString>M Str¨ube and SP Ponzetto. 2006. WikiRelate! computing semantic relateness using Wikipedia. In Proc. of AAAI’06, pages 1419–1424, Boston, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Sun</author>
<author>R Li</author>
<author>D Luo</author>
<author>X Wu</author>
</authors>
<title>Text segmentation with LDA-based Fisher kernel.</title>
<date>2008</date>
<booktitle>In Proc. of ACL-08: HLT,</booktitle>
<pages>269--272</pages>
<contexts>
<context position="2057" citStr="Sun et al., 2008" startWordPosition="294" endWordPosition="297">pture the latent semantics of a document or document collection, in the form of methods such as latent semantic analysis (Deerwester et al., 1990), probabilistic latent semantic analysis (Hofmann, 2001), random projection (Widdows and Ferraro, 2008), and more recently, latent Dirichlet allocation (Blei et al., 2003; Griffiths and Steyvers, 2004). Such methods have been successfully applied to a myriad of tasks including word sense discrimination (Brody and Lapata, 2009), document summarisation (Haghighi and Vanderwende, 2009), areal linguistic analysis (Daume III, 2009) and text segmentation (Sun et al., 2008). In each case, extrinsic evaluation has been used to demonstrate the effectiveness of the learned topics in the application domain, but standardly, no attempt has been made to perform intrinsic evaluation of the topics themselves, either qualitatively or quantitatively. In machine learning, on the other hand, researchers have modified and extended topic models in a variety of ways, and evaluated intrinsically in terms of model perplexity (Wallach et al., 2009), but there has been less effort on qualitative understanding of the semantic nature of the learned topics. This research seeks to fill</context>
</contexts>
<marker>Sun, Li, Luo, Wu, 2008</marker>
<rawString>Q Sun, R Li, D Luo, and X Wu. 2008. Text segmentation with LDA-based Fisher kernel. In Proc. of ACL-08: HLT, pages 269–272.</rawString>
</citation>
<citation valid="true">
<authors>
<author>HM Wallach</author>
<author>I Murray</author>
<author>R Salakhutdinov</author>
<author>DM Mimno</author>
</authors>
<title>Evaluation methods for topic models.</title>
<date>2009</date>
<booktitle>In Proc. of ICML 2009,</booktitle>
<pages>139</pages>
<contexts>
<context position="2522" citStr="Wallach et al., 2009" startWordPosition="367" endWordPosition="370">Lapata, 2009), document summarisation (Haghighi and Vanderwende, 2009), areal linguistic analysis (Daume III, 2009) and text segmentation (Sun et al., 2008). In each case, extrinsic evaluation has been used to demonstrate the effectiveness of the learned topics in the application domain, but standardly, no attempt has been made to perform intrinsic evaluation of the topics themselves, either qualitatively or quantitatively. In machine learning, on the other hand, researchers have modified and extended topic models in a variety of ways, and evaluated intrinsically in terms of model perplexity (Wallach et al., 2009), but there has been less effort on qualitative understanding of the semantic nature of the learned topics. This research seeks to fill the gap between topic evaluation in computational linguistics and machine learning, in developing techniques to perform intrinsic qualitative evaluation of learned topics. That is, we develop methods for evaluating the quality of a given topic, in terms of its coherence to a human. After learning topics from a collection of news articles and a collection of books, we ask humans to decide whether individual learned topics are coherent, in terms of their interpr</context>
<context position="5003" citStr="Wallach et al. (2009)" startWordPosition="757" endWordPosition="760"> for human consumption (Newman et al., to appearb). 2 Related Work Most earlier work on intrinsically evaluating learned topics has been on the basis of perplexity results, where a model is learned on a collection of training documents, then the log probability of the unseen test documents is computed using that learned model. Usually perplexity is reported, which is the inverse of the geometric mean per-word likelihood. Perplexity is useful for model selection and adjusting parameters (e.g. number of topics T), and is the standard way of demonstrating the advantage of one model over another. Wallach et al. (2009) presented efficient and unbiased methods for computing perplexity and evaluating almost any type of topic model. While statistical evaluation of topic models is reasonably well understood, there has been much less work on evaluating the intrinsic semantic quality of topics learned by topic models, which could have a far greater impact on the overall value of topic modeling for end-user applications. Some researchers have started to address this problem, including Mei et al. (2007) who presented approaches for automatic labeling of topics (which is core to the question of coherence and semanti</context>
</contexts>
<marker>Wallach, Murray, Salakhutdinov, Mimno, 2009</marker>
<rawString>HM Wallach, I Murray, R Salakhutdinov, and DM Mimno. 2009. Evaluation methods for topic models. In Proc. of ICML 2009, page 139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Widdows</author>
<author>K Ferraro</author>
</authors>
<title>Semantic Vectors: A scalable open source package and online technology management application.</title>
<date>2008</date>
<booktitle>In Proc. of LREC</booktitle>
<location>Marrakech, Morocco.</location>
<contexts>
<context position="1689" citStr="Widdows and Ferraro, 2008" startWordPosition="239" endWordPosition="242"> inter-annotator correlation, and that other Wikipedia-based lexical relatedness methods also achieve strong results. Google produces strong, if less consistent, results, while our results over WordNet are patchy at best. 1 Introduction There has traditionally been strong interest within computational linguistics in techniques for learning sets of words (aka topics) which capture the latent semantics of a document or document collection, in the form of methods such as latent semantic analysis (Deerwester et al., 1990), probabilistic latent semantic analysis (Hofmann, 2001), random projection (Widdows and Ferraro, 2008), and more recently, latent Dirichlet allocation (Blei et al., 2003; Griffiths and Steyvers, 2004). Such methods have been successfully applied to a myriad of tasks including word sense discrimination (Brody and Lapata, 2009), document summarisation (Haghighi and Vanderwende, 2009), areal linguistic analysis (Daume III, 2009) and text segmentation (Sun et al., 2008). In each case, extrinsic evaluation has been used to demonstrate the effectiveness of the learned topics in the application domain, but standardly, no attempt has been made to perform intrinsic evaluation of the topics themselves, </context>
</contexts>
<marker>Widdows, Ferraro, 2008</marker>
<rawString>D Widdows and K Ferraro. 2008. Semantic Vectors: A scalable open source package and online technology management application. In Proc. of LREC 2008, Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Wu</author>
<author>M Palmer</author>
</authors>
<title>Verb selection and lexical selection.</title>
<date>1994</date>
<booktitle>In Proc. of ACL’94,</booktitle>
<pages>133--138</pages>
<location>Las Cruces, USA.</location>
<contexts>
<context position="12782" citStr="Wu and Palmer (1994)" startWordPosition="2055" endWordPosition="2058">senses associated with the POS with the highest prior. In all cases, the overall trend was for the correlation with the human scores to drop relative to the mean, so we only present the numbers for the mean in this paper. 102 count of nodes includes the beginning and ending word nodes. Leacock-Chodorow (LCH) The measure of semantic similarity devised by Leacock et al. (1998) finds the shortest path between two WordNet synsets (sp(c1, c2)) using hypernym and synonym relationships. This path length is then scaled by the maximum depth of WordNet (D), and the log likelihood taken: Wu-Palmer (WUP) Wu and Palmer (1994) proposed to scale the depth of the two synset nodes (depthc1 and depthc2) by the depth of their LCS (depth(lcsc1,c2)): simwup(c1, c2) = 2 · depth(lcsc1,c2) depthc1 + depthc2 + 2 · depth(lcsc1,c2) The scaling means that specific terms (deeper in the hierarchy) that are close together are more semantically similar than more general terms, which have a short path distance between them. Only hypernym relationships are used in this measure, as the LCS is defined by the common member in the concepts’ hypernym path. Hirst-St Onge (HSO) Hirst and St-Onge (1998) define a measure of semantic similarity</context>
</contexts>
<marker>Wu, Palmer, 1994</marker>
<rawString>Z Wu and M Palmer. 1994. Verb selection and lexical selection. In Proc. of ACL’94, pages 133–138, Las Cruces, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>