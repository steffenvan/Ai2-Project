<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.993487">
Stop-probability estimates computed on a large corpus
improve Unsupervised Dependency Parsing
</title>
<author confidence="0.984057">
David Mareˇcek and Milan Straka
</author>
<affiliation confidence="0.9501555">
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
</affiliation>
<address confidence="0.806436">
Malostransk´e n´amˇest´ı 25, 11800 Prague, Czech Republic
</address>
<email confidence="0.994764">
{marecek,straka}@ufal.mff.cuni.cz
</email>
<sectionHeader confidence="0.994658" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999869357142857">
Even though the quality of unsupervised
dependency parsers grows, they often fail
in recognition of very basic dependencies.
In this paper, we exploit a prior knowledge
of STOP-probabilities (whether a given
word has any children in a given direc-
tion), which is obtained from a large raw
corpus using the reducibility principle. By
incorporating this knowledge into Depen-
dency Model with Valence, we managed to
considerably outperform the state-of-the-
art results in terms of average attachment
score over 20 treebanks from CoNLL 2006
and 2007 shared tasks.
</bodyText>
<sectionHeader confidence="0.998425" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.967367">
The task of unsupervised dependency parsing
(which strongly relates to the grammar induction
task) has become popular in the last decade, and
its quality has been greatly increasing during this
period.
The first implementation of Dependency Model
with Valence (DMV) (Klein and Manning, 2004)
with a simple inside-outside inference algo-
rithm (Baker, 1979) achieved 36% attachment
score on English and was the first system outper-
forming the adjacent-word baseline.1
Current attachment scores of state-of-the-art un-
supervised parsers are higher than 50% for many
languages (Spitkovsky et al., 2012; Blunsom and
Cohn, 2010). This is still far below the super-
vised approaches, but their indisputable advan-
tage is the fact that no annotated treebanks are
needed and the induced structures are not bur-
dened by any linguistic conventions. Moreover,
1The adjacent-word baseline is a dependency tree in
which each word is attached to the previous (or the follow-
ing) word. The attachment score of 35.9% on all the WSJ
test sentences was taken from (Blunsom and Cohn, 2010).
supervised parsers always only simulate the tree-
banks they were trained on, whereas unsupervised
parsers have an ability to be fitted to different par-
ticular applications.
Some of the current approaches are based on
the DMV, a generative model where the gram-
mar is expressed by two probability distributions:
Pchoose(cd|ch, dir), which generates a new child
cd attached to the head ch in the direction dir (left
or right), and Pstop(STOP|ch, dir, · · · ), which
makes a decision whether to generate another
child of ch in the direction dir or not.2 Such a
grammar is then inferred using sampling or varia-
tional methods.
Unfortunately, there are still cases where the in-
ferred grammar is very different from the gram-
mar we would expect, e.g. verbs become leaves
instead of governing the sentences. Rasooli and
Faili (2012) and Bisk and Hockenmaier (2012)
made some efforts to boost the verbocentricity of
the inferred structures; however, both of the ap-
proaches require manual identification of the POS
tags marking the verbs, which renders them use-
less when unsupervised POS tags are employed.
The main contribution of this paper is a consid-
erable improvement of unsupervised parsing qual-
ity by estimating the Pstop probabilities externally
using a very large corpus, and employing this prior
knowledge in the standard inference of DMV. The
estimation is done using the reducibility principle
introduced in (Mareˇcek and ˇZabokrtsk´y, 2012).
The reducibility principle postulates that if a word
(or a sequence of words) can be removed from
a sentence without violating its grammatical cor-
rectness, it is a leaf (or a subtree) in its dependency
structure. For the purposes of this paper, we as-
sume the following hypothesis:
If a sequence of words can be removed from
</bodyText>
<footnote confidence="0.995900666666667">
2The Pstop probability may be conditioned by additional
parameters, such as adjacency adj or fringe word cf, which
will be described in Section 4.
</footnote>
<page confidence="0.916179">
281
</page>
<note confidence="0.9599035">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 281–290,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<figureCaption confidence="0.996633">
Figure 1: Example of a dependency tree. Se-
quences of words that can be reduced are under-
lined.
</figureCaption>
<bodyText confidence="0.963020184210526">
a sentence without violating its grammatical cor-
rectness, no word outside the sequence depends on
any word in the sequence.
Our hypothesis is a generalization of the origi-
nal hypothesis since it allows a reducible sequence
to form several adjacent subtrees.
Let’s outline the connection between the Pstop
probabilities and the property of reducibility. Fig-
ure 1 shows an example of a dependency tree. Se-
quences of reducible words are marked by thick
lines below the sentence. Consider for example
the word “further”. It can be removed and thus,
according to our hypothesis, no other word de-
pends on it. Therefore, we can deduce that the
Pstop probability for such word is high both for
the left and for the right direction. The phrase
“for further discussions” is reducible as well and
we can deduce that the Pstop of its first word
(“for”) in the left direction is high since it cannot
have any left children. We do not know anything
about its right children, because they can be lo-
cated within the sequence (and there is really one
in Figure 1). Similarly, the word “discussions”,
which is the last word in this sequence, cannot
have any right children and we can estimate that its
right Pstop probability is high. On the other hand,
non-reducible words such, as the verb “asked” in
our example, can have children, and therefore their
Pstop can be estimated as low for both directions.
The most difficult task in this approach is to au-
tomatically recognize reducible sequences. This
problem, together with the estimation of the stop-
probabilities, is described in Section 3. Our
model, not much different from the classic DMV,
is introduced in Section 4. Section 5 describes the
inference algorithm based on Gibbs sampling. Ex-
periments and results are discussed in Section 6.
Section 7 concludes the paper.
</bodyText>
<sectionHeader confidence="0.999865" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999808076923077">
Reducibility: The notion of reducibility belongs
to the traditional linguistic criteria for recogniz-
ing dependency relations. As mentioned e.g. by
K¨ubler et al. (2009), the head h of a construction c
determines the syntactic category of c and can of-
ten replace c. In other words, the descendants of h
can be often removed without making the sentence
incorrect. Similarly, in the Dependency Analysis
by Reduction (Lopatkov´a et al., 2005), the authors
assume that stepwise deletions of dependent ele-
ments within a sentence preserve its syntactic cor-
rectness. A similar idea of dependency analysis
by splitting a sentence into all possible acceptable
fragments is used by Gerdes and Kahane (2011).
We have directly utilized the aforementioned
criteria for dependency relations in unsuper-
vised dependency parsing in our previous pa-
per (Mareˇcek and ˇZabokrtsk´y, 2012). Our depen-
dency model contained a submodel which directly
prioritized subtrees that form reducible sequences
of POS tags. Reducibility scores of given POS tag
sequences were estimated using a large corpus of
Wikipedia articles. The weakness of this approach
was the fact that longer sequences of POS tags
are very sparse and no reducibility scores could
be estimated for them. In this paper, we avoid this
shortcoming by estimating the STOP probabilities
for individual POS tags only.
Another task related to reducibility is sentence
compression (Knight and Marcu, 2002; Cohn and
Lapata, 2008), which was used for text summa-
rization. The task is to shorten the sentences while
retaining the most important pieces of informa-
tion, using the knowledge of the grammar. Con-
versely, our task is to induce the grammar using
the sentences and their shortened versions.
Dependency Model with Valence (DMV) has
been the most popular approach to unsupervised
dependency parsing in the recent years. It was in-
troduced by Klein and Manning (2004) and fur-
ther improved by Smith (2007) and Cohen et al.
(2008). Headden III et al. (2009) introduce the
Extended Valence Grammar and add lexicaliza-
tion and smoothing. Blunsom and Cohn (2010)
use tree substitution grammars, which allow learn-
ing of larger dependency fragments by employ-
ing the Pitman-Yor process. Spitkovsky et al.
(2010) improve the inference using iterated learn-
ing of increasingly longer sentences. Further im-
provements were achieved by better dealing with
punctuation (Spitkovsky et al., 2011b) and new
“boundary” models (Spitkovsky et al., 2012).
</bodyText>
<page confidence="0.994282">
282
</page>
<bodyText confidence="0.9997096">
Other approaches to unsupervised dependency
parsing were described e.g. in (Søgaard, 2011),
(Cohen et al., 2011), and (Bisk and Hockenmaier,
2012). There also exist “less unsupervised” ap-
proaches that utilize an external knowledge of the
POS tagset. For example, Rasooli and Faili (2012)
identify the last verb in the sentence, minimize
its probability of reduction and thus push it to
the root position. Naseem et al. (2010) make use
of manually-specified universal dependency rules
such as Verb→Noun, Noun→Adjective. McDon-
ald et al. (2011) identify the POS tags by a cross-
lingual transfer. Such approaches achieve better
results; however, they are useless for grammar in-
duction from plain text.
</bodyText>
<sectionHeader confidence="0.998773" genericHeader="method">
3 STOP-probability estimation
</sectionHeader>
<subsectionHeader confidence="0.999987">
3.1 Recognition of reducible sequences
</subsectionHeader>
<bodyText confidence="0.996742681818182">
We introduced a simple procedure for recog-
nition of reducible sequences in (Mareˇcek and
ˇZabokrtsk´y, 2012): The particular sequence of
words is removed from the sentence and if the
remainder of the sentence exists elsewhere in the
corpus, the sequence is considered reducible. We
provide an example in Figure 2. The bigram “this
weekend” in the sentence “The next competition
is this weekend at Lillehammer in Norway.” is re-
ducible since the same sentence without this bi-
gram, i.e., “The next competition is at Lilleham-
mer in Norway.”, is in the corpus as well. Simi-
larly, the prepositional phrase “of Switzerland” is
also reducible.
It is apparent that only very few reducible se-
quences can be found by this procedure. If we
use a corpus containing about 10,000 sentences, it
is possible that we found no reducible sequences
at all. However, we managed to find a sufficient
amount of reducible sequences in corpora contain-
ing millions of sentences, see Section 6.1 and Ta-
ble 1.
</bodyText>
<subsectionHeader confidence="0.9859705">
3.2 Computing the STOP-probability
estimations
</subsectionHeader>
<bodyText confidence="0.999527125">
Recall our hypothesis from Section 1: If a se-
quence of words is reducible, no word outside the
sequence can depend on any word in the sequence.
Or, in terms of dependency structure: A reducible
sequence consists of one or more adjacent sub-
trees. This means that the first word of a reducible
sequence does not have any left children and, sim-
ilarly, the last word in a reducible sequence does
</bodyText>
<figureCaption confidence="0.749099">
Figure 2: Example of reducible sequences of
words found in a large corpus.
</figureCaption>
<bodyText confidence="0.995486333333333">
not have any right children. We make use of this
property directly for estimating Pstop probabili-
ties.
</bodyText>
<subsectionHeader confidence="0.653002">
Hereinafter, Pest
</subsectionHeader>
<bodyText confidence="0.993154611111111">
stop(ch, dir) denotes the STOP-
probability we want to estimate from a large cor-
pus; ch is the head’s POS tag and dir is the direc-
tion in which the STOP probability is estimated.
If ch is very often in the first position of reducible
sequences, Pest
stop(ch, left) will be high. Similarly,
if ch is often in the last position of reducible se-
quences, P est
stop(ch, right) will be high.
For each POS tag ch in the given corpus,
we first compute its left and right “raw” score
Sstop(ch, left) and Sstop(ch, right) as the relative
number of times a word with POS tag ch was in
the first (or last) position in a reducible sequence
found in the corpus. We do not deal with se-
quences longer than a trigram since they are highly
biased.
</bodyText>
<equation confidence="0.642822">
Sstop(ch, left) _ # ch in the corpus
# red.seq. [... , ch] + λ
Sstop(ch, right) _ # ch in the corpus
</equation>
<bodyText confidence="0.988788666666667">
Note that the Sstop scores are not probabilities.
Their main purpose is to sort the POS tags accord-
ing to their “reducibility”.
It may happen that for many POS tags there
are no reducible sequences found. To avoid zero
scores, we use a simple smoothing by adding λ to
</bodyText>
<listItem confidence="0.5854195">
each count:
# all reducible sequences
</listItem>
<equation confidence="0.9373055">
λ _
W
</equation>
<bodyText confidence="0.954584466666667">
Third place went to Simon Ammann of Switzerland . Ammann
is currently just fifth , overall with 120 points . The next
competition is at Lillehammer in Norway .
Martin Fourcade was sixth , maintaining his lead at the top of
the overall World Cup standings , although Svendsen is now
only 59 points away from the Frenchman in second . The next
competition is this weekend at Lillehammer in Norway .
Larinto saw off allcomers at Kuopio with jumps of 129.5 and
124m for a total 240.9 points , just 0.1 points ahead of
compatriot Matti Hautamaeki , who landed efforts of 127 and
129.5m . Third place went to Simon Ammann . Andreas
Kofler , who won at the weekend at Kuusamo , was fourth but
stays top of the season standings with 150 points .
# red.seq. [ch,... ] + λ
,
</bodyText>
<page confidence="0.947123">
283
</page>
<bodyText confidence="0.999889888888889">
where W denotes the number of words in the
given corpus. Such smoothing ensures that more
frequent irreducible POS tags get a lower Sstop
score than the less frequent ones.
Since reducible sequences found are very
sparse, the values of Sstop(ch, dir) scores are very
small. To convert them to estimated probabilities
Pest stop(ch, dir), we need a smoothing that fulfills
the following properties:
</bodyText>
<equation confidence="0.607809">
(1) P est
</equation>
<bodyText confidence="0.95097592">
stop is a probability and therefore its value
must be between 0 and 1.
(2) The number of no-stop decisions (no matter
in which direction) equals to W (number of
words) since such decision is made before
each word is generated. The number of stop
decisions is 2W since they come after gener-
ating the last children in both the directions.
Therefore, the average P est
stop(h, dir) over all
words in the treebank should be 2/3.
After some experimenting, we chose the follow-
ing normalization formula
with a normalization constant v. The condition
(1) is fulfilled for any positive value of v. Its exact
value is set in accordance with the requirement (2)
so that the average value of P est
stop is 2/3.
2
count(c)Pest
stop(c, dir) = 3 · 2W,
where count(c) is the number of words with POS
tag c in the corpus. We find the unique value of v
that fulfills the previous equation numerically us-
ing a binary search algorithm.
</bodyText>
<sectionHeader confidence="0.990046" genericHeader="method">
4 Model
</sectionHeader>
<bodyText confidence="0.99786">
We use the standard generative Dependency
Model with Valence (Klein and Manning, 2004).
The generative story is the following: First, the
head of the sentence is generated. Then, for each
head, all its left children are generated, then the
left STOP, then all its right children, and then the
right STOP. When a child is generated, the al-
gorithm immediately recurses to generate its sub-
tree. When deciding whether to generate another
child in the direction dir or the STOP symbol,
we use the Pdmv
stop (STOP|ch, dir, adj, cf) model.
The new child cd in the direction dir is generated
according to the Pchoose(cd|ch, dir) model. The
probability of the whole dependency tree T is the
following:
</bodyText>
<equation confidence="0.900680571428571">
Ptree(T) = Pchoose(head(T)|ROOT, right)
· Ptree(D(head(T )))
Ptree(D(ch)) =
Pdmv
stop (-,STOP|ch, dir, adj, cf)
Pchoose(cd|ch, dir)Ptree(D(cd))
Pdmv
</equation>
<bodyText confidence="0.988557176470588">
stop (STOP|ch, dir, adj, cf),
where Ptree(D(ch)) is probability of the subtree
governed by h in the tree T.
The set of features on which the Pdmv
stop and
Pchoose probabilities are conditioned varies among
the previous works. Our Pdmv
stop depends on the
head POS tag ch, direction dir, adjacency adj,
and fringe POS tag cf (described below). The
use of adjacency is standard in DMV and enables
us to have different Pdmv
stop for situations when no
child was generated so far (adj = 1). That is,
Pdmv
stop (ch, dir, adj = 1, cf) decides whether the
word ch has any children in the direction dir at
all, whereas Pdmv
stop (h, dir, adj = 0, cf) decides
whether another child will be generated next to
the already generated one. This distinction is of
crucial importance for us: although we know how
to estimate the STOP probabilities for adj = 1
from large data, we do not know anything about
the STOP probabilities for adj = 0.
The last factor cf, called fringe, is the POS tag
of the previously generated sibling in the current
direction dir. If there is no such sibling (in case
adj = 1), the head ch is used as the fringe cf.
This is a relatively novel idea in DMV, introduced
by Spitkovsky et al. (2012). We decided to use
the fringe word in our model since it gives slightly
better results.
We assume that the distributions of Pchoose and
</bodyText>
<subsectionHeader confidence="0.455659">
Pdmv
</subsectionHeader>
<bodyText confidence="0.999800142857143">
stop are good if the majority of the probabil-
ity mass is concentrated on few factors; therefore,
we apply a Chinese Restaurant process (CRP) on
them.
The probability of generating a new child node
cd attached to ch in the direction dir given the his-
tory (all the nodes we have generated so far) is
</bodyText>
<equation confidence="0.664938090909091">
Sstop(ch, dir)
Pest
stop(ch, dir) =
Sstop(ch, dir) + v
� �
dirE{l,r} cEC
ri
dirE{l,r}
H
cdE
deps(dir,h)
</equation>
<page confidence="0.909432">
284
</page>
<bodyText confidence="0.944611">
computed using the following formula: Finally, we obtain the probability of the whole
</bodyText>
<equation confidence="0.9279872">
generated treebank as a product over the trees:
Pchoose(cd|ch, dir) =
|C |αc 1 + count −(cd,ch, dir)
=
αc + count−(ch, dir) ,
</equation>
<bodyText confidence="0.991696444444444">
where count−(cd,ch, dir) denotes the number of
times a child node cd has been attached to ch
in the direction dir in the history. Similarly,
count−(ch, dir) is the number of times something
has been attached to ch in the direction dir. The
αc is a hyperparameter and |C |is the number of
distinct POS tags in the corpus.3
The STOP probability is computed in a similar
way:
</bodyText>
<equation confidence="0.9154315">
Pdmv
stop (STOP|ch, dir, adj, cf) =
= αs 2 3 + count−(STOP, ch, dir, adj, cf)
αs + count−(ch, dir, adj, cf)
</equation>
<bodyText confidence="0.998449545454546">
where count−(STOP, ch, dir, adj, cf) is the
number of times a head ch had the last child cf
in the direction dir in the history.
The contribution of this paper is the inclusion
of the stop-probability estimates into the DMV.
Therefore, we introduce a new model P dmv+est
stop ,
in which the probability based on the previously
generated data is linearly combined with the prob-
ability estimates based on large corpora (Sec-
tion 3).
</bodyText>
<equation confidence="0.80088175">
Pdmv+est
stop (STOP|ch, dir, 0, cf) =
= Pdmv
stop (STOP|ch, dir, 0, cf)
</equation>
<bodyText confidence="0.985867444444444">
The hyperparameter β defines the ratio between
the CRP-based and estimation-based probability.
The definition of the Pdmv+est for adj = 0 equals
stop
the basic Pdmv
stop since we are able to estimate only
the probability whether a particular head POS tag
ch can or cannot have children in a particular di-
rection, i.e if adj = 1.
</bodyText>
<footnote confidence="0.914241">
3The number of classes ICI is often used in the denomi-
nator. We decided to put its reverse value into the numerator
since we observed such model to perform better for a constant
value of α. over different languages and tagsets.
</footnote>
<equation confidence="0.9562115">
�Ptreebank = Ptree(T).
TEtreebank
</equation>
<bodyText confidence="0.99991475">
An important property of the CRP is the fact that
the factors are exchangeable – i.e. no matter how
the trees are ordered in the treebank, the Ptreebank
is always the same.
</bodyText>
<sectionHeader confidence="0.999723" genericHeader="method">
5 Inference
</sectionHeader>
<bodyText confidence="0.999776">
We employ the Gibbs sampling algorithm (Gilks
et al., 1996). Unlike in (Mareˇcek and ˇZabokrtsk´y,
2012), where edges were sampled individually,
we sample whole trees from all possibilities on a
given sentence using dynamic programming. The
algorithm works as follows:
</bodyText>
<listItem confidence="0.997705470588235">
1. A random projective dependency tree is as-
signed to each sentence in the corpus.
2. Sampling: We go through the sentences in a
random order. For each sentence, we sam-
ple a new dependency tree based on all other
trees that are currently in the corpus.
3. Step 2 is repeated in many iterations. In
this work, the number of iterations was set
to 1000.
4. After the burn-in period (which was set to the
first 500 iterations), we start collecting counts
of edges between particular words that ap-
peared during the sampling.
5. Parsing: Based on the collected counts, we
compute the final dependency trees using
the Chu-Liu/Edmonds’ algorithm (1965) for
finding maximum directed spanning trees.
</listItem>
<subsectionHeader confidence="0.990832">
5.1 Sampling
</subsectionHeader>
<bodyText confidence="0.9982465">
Our goal is to sample a new projective dependency
tree T with probability proportional to Ptree(T).
Since the factors are exchangeable, we can deal
with any tree as if it was the last one in the corpus.
We use dynamic programming to sample a
tree with N nodes in O(N4) time. Neverthe-
less, we sample trees using a modified probabil-
ity P0tree(T). In Ptree(T), the probability of an
edge depends on counts of all other edges, includ-
ing the edges in the same tree. We instead use
P0tree(T ), where the counts are computed using
only the other trees in the corpus, i.e., probabilities
</bodyText>
<table confidence="0.445969333333333">
Pdmv+est
stop (STOP|ch, dir, 1, cf) =
(1 − β) αs 3 + count−(STOP, ch, dir, 1, c f) ·
αs + count−(ch, dir, 1, cf)
+ β · Pest
stop(ch, dir)
</table>
<page confidence="0.989072">
285
</page>
<bodyText confidence="0.998098230769231">
of edges of T are independent. There is a stan-
dard way to sample using the real Ptree(T) – we
can use P0tree(T) as a proposal distribution in the
Metropolis-Hastings algorithm (Hastings, 1970),
which then produces trees with probabilities pro-
portional to Ptree(T) using acceptance-rejection
scheme. We do not take this approach and we
sample proportionally to P0tree(T) only, because
we believe that for large enough corpora, the two
distributions are nearly identical.
To sample a tree containing words w1, ... , wN
with probability proportional to P0tree(T), we first
compute three tables:
</bodyText>
<listItem confidence="0.993962727272727">
• ti(g, i, j) for g &lt; i or g &gt; j is the sum of
probabilities of any tree on words wi,... , wj
whose root is a child of wg, but not an outer-
most child in its direction;
• to(g, i, j) is the same, but the tree is the out-
ermost child of wg;
• fo(g, i, j) for g &lt; i or g &gt; j is the
sum of probabilities of any forest on words
wi, ... , wj, such that all the trees are children
of wg and are the outermost children of wg in
their direction.
</listItem>
<bodyText confidence="0.998088375">
All the probabilities are computed using the P0tree.
If we compute the tables inductively from the
smallest trees to the largest trees, we can precom-
pute all the O(N3) values in O(N4) time.
Using these tables, we sample the tree recur-
sively, starting from the root. At first, we sam-
ple the root r proportionally to the probability of
a tree with the root r, which is a product of the
probability of left children of r and right chil-
dren of r. The probability of left children of r
is either P0stop(STOP|r, left) if r has no children,
or P0stop(¬STOP|r, left)fo(r,1, r −1) otherwise;
the probability of right children is analogous.
After sampling the root, we sample the ranges
of its left children, if any. We sample the first left
child range l1 proportionally either to to(r,1, r−1)
</bodyText>
<construct confidence="0.9526664">
if l1 = 1, or to ti(r, l1, r − 1)fo(r,1, l1 − 1)
if l1 &gt; 1. Then we sample the second left child
range l2 proportionally either to to(r,1, l1 − 1)
if l2 = 1, or to ti(r, l2, l1 − 1)fo(r,1, l2 − 1)
if l2 &gt; 1, and so on, while there are any left
</construct>
<bodyText confidence="0.996016166666667">
children. The right children ranges are sampled
similarly. Finally, we recursively sample the chil-
dren, i.e., their roots, their children and so on. It
is simple to verify using the definition of Ptree that
the described method indeed samples trees propor-
tionally to P0tree.
</bodyText>
<subsectionHeader confidence="0.998926">
5.2 Parsing
</subsectionHeader>
<bodyText confidence="0.999829769230769">
Beginning the 500th iteration, we start collecting
counts of individual dependency edges during the
remaining iterations. After each iteration is fin-
ished (all the trees in the corpus are re-sampled),
we increment the counter of all directed pairs of
nodes which are connected by a dependency edge
in the current trees.
After the last iteration, we use these collected
counts as weights and compute maximum directed
spanning trees using the Chu-Liu/Edmonds’ algo-
rithm (Chu and Liu, 1965). Therefore, the result-
ing trees consist of edges maximizing the sum of
individual counts:
</bodyText>
<equation confidence="0.999517">
�TMST = arg max
T eET
</equation>
<bodyText confidence="0.999974428571428">
It is important to note that the MST algorithm
may produce non-projective trees. Even if we
average the strictly projective dependency trees,
some non-projective edges may appear in the re-
sult. This might be an advantage since correct
non-projective edges can be predicted; however,
this relaxation may introduce mistakes as well.
</bodyText>
<sectionHeader confidence="0.998954" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<subsectionHeader confidence="0.985829">
6.1 Data
</subsectionHeader>
<bodyText confidence="0.999948083333333">
We use two types of resources in our experiments.
The first type are CoNLL treebanks from the year
2006 (Buchholz and Marsi, 2006) and 2007 (Nivre
et al., 2007), which we use for inference and for
evaluation. As is the standard practice in unsuper-
vised parsing evaluation, we removed all punctu-
ation marks from the trees. In case a punctuation
node was not a leaf, its children are attached to the
parent of the removed node.
For estimating the STOP probabilities (Sec-
tion 3), we use the Wikipedia articles from W2C
corpus (Majliˇs and ˇZabokrtsk´y, 2012), which pro-
vide sufficient amount of data for our purposes.
Statistics across languages are shown in Table 1.
The Wikipedia texts were automatically tok-
enized and segmented to sentences so that their
tokenization was similar to the one in the CoNLL
evaluation treebanks. Unfortunately, we were not
able to find any segmenter for Chinese that would
produce a desired segmentation; therefore, we re-
moved Chinese from evaluation.
The next step was to provide the Wikipedia
texts with POS tags. We employed the TnT tag-
ger (Brants, 2000) which was trained on the re-
</bodyText>
<equation confidence="0.821588">
count(e)
</equation>
<page confidence="0.991703">
286
</page>
<table confidence="0.999382909090909">
language tokens red. language tokens red.
(mil.) seq. (mil.) seq.
Arabic 19.7 546 Greek 20.9 1037
Basque 14.1 645 Hungarian 26.3 2237
Bulgarian 18.8 1808 Italian 39.7 723
Catalan 27.0 712 Japanese 2.6 31
Czech 20.3 930 Portuguese 31.7 4765
Danish 15.9 576 Slovenian 13.7 513
Dutch 27.1 880 Spanish 53.4 1156
English 85.0 7603 Swedish 19.2 481
German 56.9 1488 Turkish 16.5 5706
</table>
<tableCaption confidence="0.784082">
Table 1: Wikipedia texts statistics: total number of
tokens and number of reducible sequences found
in them.
</tableCaption>
<bodyText confidence="0.9413278">
spective CoNLL training data. The quality of such
tagging is not very high since we do not use any
lexicons or pretrained models. However, it is suf-
ficient for obtaining usable stop probability esti-
mates.
</bodyText>
<subsectionHeader confidence="0.983891">
6.2 Estimated STOP probabilities
</subsectionHeader>
<bodyText confidence="0.994777848484848">
We applied the algorithm described in Section 3 on
the prepared Wikipedia corpora and obtained the
stop-probabilities Pest
stop in both directions for all
the languages and their POS tags. To evaluate the
quality of our estimations, we compare them with
Ptbp, the stop probabilities computed directly on
sto
the evaluation treebanks. The comparisons on five
selected languages are shown in Figure 3. The in-
dividual points represent the individual POS tags,
their size (area) shows their frequency in the par-
ticular treebank. The y-axis shows the stop prob-
abilities estimated on Wikipedia by our algorithm,
while the x-axis shows the stop probabilities com-
puted on the evaluation CoNLL data. Ideally, the
computed and estimated stop probabilities should
be the same, i.e. all the points should be on the
diagonal.
Let’s focus on the graphs for English. Our
method correctly recognizes that adverbs RB and
adjectives JJ are often leaves (their stop proba-
bilities in both directions are very high). More-
over, the estimates for RB are even higher than
JJ, which will contribute to attaching adverbs to
adjectives and not reversely. Nouns (NN, NNS)
are somewhere in the middle, the stop probabili-
ties for proper nouns (NNP) are estimated higher,
which is correct since they have much less modi-
fiers then the common nouns NN. The determin-
ers are more problematic. Their estimated stop
probability is not very high (about 0.65), while in
the real treebank they are almost always leaves.
</bodyText>
<figure confidence="0.543467">
0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1
estimation computed on the treebank
</figure>
<figureCaption confidence="0.999646">
Figure 3: Comparison of P est
</figureCaption>
<bodyText confidence="0.9903145">
stop probabilities esti-
mated from raw Wikipedia corpora (y-axis) and
of Ptb
stop probabilities computed from CoNLL tree-
banks (x-axis). The area of each point shows the
relative frequency of an individual tag.
</bodyText>
<figure confidence="0.999783804878049">
English left-stop English right-stop
VBD
NNS
NN
NNP
RB
IN
DT
JJ
VBD
IN
NNS
NN
NNP
DT
RB
JJ
0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1
estimation computed on the treebank
estimation from Wiki estimation from Wiki estimation from Wiki estimation from Wiki estimation from Wiki
Czech left-stop Czech right-stop
Db
AA
Z:
VB
C=
RR
Vp
J^
NN
Vf
VB
Vp
J^
NN
Vf
AA
Z:
C=
RR
Db
1
0.8
0.6
0.4
0.2
0
0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1
estimation computed on the treebank
German left-stop German right-stop
APPR
VVFIN
VAFIN
NN
NE
ADV
ADJA
ART
VVFIN
VAFIN
NN
ADJA
APPR
ART
ADV
0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1
1
0.8
0.6
0.4
0.2
0
estimation computed on the treebank
Spanish left-stop Spanish right-stop
Fc
np
sp
nc
vm
aq
cc
da
sp
vm
nc
aq
np
Fc
da
cc
1
0.8
0.6
0.4
0.2
0
0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1
estimation computed on the treebank
Hungarian left-stop Hungarian right-stop
Wpunc
Af Tf
Nc
Vm
Np
Cc
Vm
Np
Cc Wpunc
Nc
Af
Tf
1
0.8
0.6
0.4
0.2
0
1
0.8
0.6
0.4
0.2
0
</figure>
<page confidence="0.990717">
287
</page>
<bodyText confidence="0.999958235294118">
This is caused by the fact that determiners are of-
ten obligatory in English and cannot be simply
removed as, e.g., adjectives. The stop probabil-
ities of prepositions (IN) are also very well rec-
ognized. While their left-stop is very probable
(prepositions always start prepositional phrases),
their right-stop probability is very low. The verbs
(the most frequent verbal tag is VBD) have very
low both right and left-stop probabilities. Our es-
timation assigns them the stop probability about
0.3 in both directions. This is quite high, but still,
it is one of the lowest among other more frequent
tags, and thus verbs tend to be the roots of the de-
pendency trees. We could make similar analyses
for other languages, but due to space reasons we
only provide graphs for Czech, German, Spanish,
and Hungarian in Figure 3.
</bodyText>
<subsectionHeader confidence="0.998458">
6.3 Settings
</subsectionHeader>
<bodyText confidence="0.9999035">
After a manual tuning, we have set our hyperpa-
rameters to the following values:
</bodyText>
<equation confidence="0.578258">
αc = 50, αs = 1, β = 1/3
</equation>
<bodyText confidence="0.999987571428571">
We have also found that the Gibbs sampler does
not always converge to a similar grammar. For a
couple of languages, the individual runs end up
with very different trees. To prevent such differ-
ences, we run each inference 50 times and take the
run with the highest final Ptreebank (see Section 4)
for the evaluation.
</bodyText>
<subsectionHeader confidence="0.813606">
6.4 Results
</subsectionHeader>
<bodyText confidence="0.997002557377049">
Table 2 shows the results of our unsupervised
parser and compares them with results previously
reported in other works. In order to see the im-
pact of using the estimated stop probabilities (us-
ing model Pdmv+est
stop ), we provide results for clas-
sical DMV (using model Pdmv
stop ) as well. We do
not provide results for Chinese since we do not
have any appropriate tokenizer at our disposal (see
Section 3), and also for Turkish from CoNLL 2006
since the data is not available to us.
We now focus on the third and fourth column of
Table 2. The addition of estimated stop probabil-
ities based on large corpora improves the parsing
accuracy on 15 out of 20 treebanks. In many cases,
the improvement is substantial, which means that
the estimated stop probabilities forced the model
to completely rebuild the structures. For exam-
ple, in Bulgarian, if the Pdmv
stop model is used,
all the prepositions are leaves and the verbs sel-
dom govern sentences. If the Pdmv+est model
stop
is used, prepositions correctly govern nouns and
verbs move to roots. We observe similar changes
on Swedish as well. Unfortunately, there are also
negative examples, such as Hungarian, where the
addition of the estimated stop probabilities de-
creases the attachment score from 60.1% to 34%.
This is probably caused by not very good estimates
of the right-stop probability (see the last graph in
Figure 3). Nevertheless, the estimated stop proba-
bilities increase the average score over all the tree-
banks by more than 12% and therefore prove its
usefulness.
In the last two columns of Table 2, we provide
results of two other works reported in the last year.
The first one (spi12) is the DMV-based grammar
inducer by Spitkovsky et al. (2012),4 the second
one (mar12) is our previous work (Mareˇcek and
ˇZabokrtsk´y, 2012). Comparing with (Spitkovsky
et al., 2012), our parser reached better accuracy on
12 out of 20 treebanks. Although this might not
seem as a big improvement, if we compare the av-
erage scores over the treebanks, our system signif-
icantly wins by more than 6%. The second system
(mar12) outperforms our parser only on one tree-
bank (on Italian by less than 3%) and its average
score over all the treebanks is only 40%, i.e., more
than 8% lower than the average score of our parser.
To see the theoretical upper bound of our model
performance, we replaced the P est
stop estimates by
the Ptbp estimates computed from the evaluation
sto
treebanks and run the same inference algorithm
with the same setting. The average attachment
score of such reference DMV is almost 65%. This
shows a huge space in which the estimation of
STOP probabilities could be further improved.
</bodyText>
<sectionHeader confidence="0.996438" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.9997963">
In this work, we studied the possibility of esti-
mating the DMV stop-probabilities from a large
raw corpus. We proved that such prior knowledge
about stop-probabilities incorporated into the stan-
dard DMV model significantly improves the unsu-
pervised dependency parsing and, since we are not
aware of any other fully unsupervised dependency
parser with higher average attachment score over
CoNLL data, we state that we reached a new state-
of-the-art result.5
</bodyText>
<footnote confidence="0.964759333333333">
4Possibly the current state-of-the-art results. They were
compared with many previous works.
5A possible competitive work may be the work by Blun-
som and Cohn (2010), who reached 55% accuracy on English
as well. However, they do not provide scores measured on
other CoNLL treebanks.
</footnote>
<page confidence="0.980726">
288
</page>
<table confidence="0.999800541666667">
CoNLL this work other systems
language year P dmv P dmv+est reference P dmv+tb spi12 mar12
stop stop stop
Arabic 06 10.6 (±8.7) 38.2 (±0.5) 61.2 10.9 26.5
Arabic 07 22.0 (±0.1) 35.3 (±0.2) 65.3 44.9 27.9
Basque 07 41.1 (±0.2) 35.5 (±0.2) 52.3 33.3 26.8
Bulgarian 06 25.9 (±1.4) 54.9 (±0.2) 73.2 65.2 46.0
Catalan 07 34.9 (±3.4) 67.0 (±1.7) 72.0 62.1 47.0
Czech 06 32.3 (±3.8) 52.4 (±5.2) 64.0 55.1 49.5
Czech 07 32.9 (±0.8) 51.9 (±5.2) 62.1 54.2 48.0
Danish 06 30.8 (±4.3) 41.6 (±1.1) 60.0 22.2 38.6
Dutch 06 25.7 (±5.7) 47.5 (±0.4) 58.9 46.6 44.2
English 07 36.5 (±5.9) 55.4 (±0.2) 63.7 29.6 49.2
German 06 29.9 (±4.6) 52.4 (±0.7) 65.5 39.1 44.8
Greek 07 42.5 (±6.0) 26.3 (±0.1) 64.7 26.9 20.2
Hungarian 07 60.8 (±0.2) 34.0 (±0.3) 68.3 58.2 51.8
Italian 07 34.5 (±0.3) 39.4 (±0.5) 64.5 40.7 43.3
Japanese 06 64.8 (±3.4) 61.2 (±1.7) 76.4 22.7 50.8
Portuguese 06 35.7 (±4.3) 69.6 (±0.1) 77.3 72.4 50.6
Slovenian 06 50.1 (±0.2) 35.7 (±0.2) 50.2 35.2 18.1
Spanish 06 38.1 (±5.9) 61.1 (±0.1) 65.6 28.2 51.9
Swedish 06 28.0 (±2.3) 54.5 (±0.4) 61.6 50.7 48.2
Turkish 07 51.6 (±5.5) 56.9 (±0.2) 67.0 44.8 15.7
Average: 36.4 48.7 64.7 42.2 40.0
</table>
<tableCaption confidence="0.999032">
Table 2: Attachment scores on CoNLL 2006 and 2007 data. Standard deviations are provided in brack-
</tableCaption>
<bodyText confidence="0.97958875">
ets. DMV model using standard Pdmv
stop probability is compared with DMV with Pdmv+est
stop , which in-
corporates STOP estimations based on reducibility principle. The reference DMV uses Ptbp, which are
sto
computed directly on the treebanks. The results reported in previous works by Spitkovsky et al. (2012),
and Mareˇcek and ˇZabokrtsk´y (2012) follows.
In future work, we would like to focus
on unsupervised parsing without gold POS
tags (see e.g. Spitkovsky et al. (2011a) and
Christodoulopoulos et al. (2012)). We suppose
that many of the current works on unsupervised
dependency parsers use gold POS tags only as a
simplification of this task, and that the ultimate
purpose of this effort is to develop a fully unsu-
pervised induction of linguistic structure from raw
texts that would be useful across many languages,
domains, and applications.
The software which implements the algorithms
described in this paper, together with Pest
stop estima-
tions computed on Wikipedia texts, can be down-
loaded at
http://ufal.mff.cuni.cz/˜marecek/udp/.
</bodyText>
<sectionHeader confidence="0.997696" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999699181818182">
This work has been supported by the AMALACH grant
(DF12P01OVV02) of the Ministry of Culture of the Czech
Republic.
Data and some tools used as a prerequisite for
the research described herein have been provided by
the LINDAT/CLARIN Large Infrastructural project, No.
LM2010013 of the Ministry of Education, Youth and Sports
of the Czech Republic.
We would like to thank Martin Popel, Zdenˇek ˇZabokrtsk´y,
Rudolf Rosa, and three anonymous reviewers for many useful
comments on the manuscript of this paper.
</bodyText>
<sectionHeader confidence="0.997699" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.987206033333333">
James K. Baker. 1979. Trainable grammars for speech
recognition. In Speech communication papers presented
at the 97th Meeting of the Acoustical Society, pages 547–
550.
Yonatan Bisk and Julia Hockenmaier. 2012. Induction of lin-
guistic structure with combinatory categorial grammars.
The NAACL-HLT Workshop on the Induction of Linguistic
Structure, page 90.
Phil Blunsom and Trevor Cohn. 2010. Unsupervised induc-
tion of tree substitution grammars for dependency pars-
ing. In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP ’10,
pages 1204–1213, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Thorsten Brants. 2000. TnT - A Statistical Part-of-Speech
Tagger. Proceedings of the sixth conference on Applied
natural language processing, page 8.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proceedings
of the Tenth Conference on Computational Natural Lan-
guage Learning, CoNLL-X ’06, pages 149–164, Strouds-
burg, PA, USA. Association for Computational Linguis-
tics.
Christos Christodoulopoulos, Sharon Goldwater, and Mark
Steedman. 2012. Turning the pipeline into a loop: Iter-
ated unsupervised dependency parsing and PoS induction.
In Proceedings of the NAACL-HLT Workshop on the In-
duction of Linguistic Structure, pages 96–99, June.
Y. J. Chu and T. H. Liu. 1965. On the Shortest Arborescence
of a Directed Graph. Science Sinica, 14:1396–1400.
</reference>
<page confidence="0.977731">
289
</page>
<reference confidence="0.999905788135594">
Shay B. Cohen, Kevin Gimpel, and Noah A. Smith. 2008.
Logistic normal priors for unsupervised probabilistic
grammar induction. In Neural Information Processing
Systems, pages 321–328.
Shay B. Cohen, Dipanjan Das, and Noah A. Smith. 2011.
Unsupervised structure prediction with non-parallel mul-
tilingual guidance. In Proceedings of the Conference
on Empirical Methods in Natural Language Processing,
EMNLP ’11, pages 50–61, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Trevor Cohn and Mirella Lapata. 2008. Sentence compres-
sion beyond word deletion. In Proceedings of the 22nd
International Conference on Computational Linguistics -
Volume 1, COLING ’08, pages 137–144, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Kim Gerdes and Sylvain Kahane. 2011. Defining depen-
dencies (and constituents). In Proceedings of Dependency
Linguistics 2011, Barcelona.
Walter R. Gilks, S. Richardson, and David J. Spiegelhalter.
1996. Markov chain Monte Carlo in practice. Interdisci-
plinary statistics. Chapman &amp; Hall.
W. Keith Hastings. 1970. Monte carlo sampling methods
using markov chains and their applications. Biometrika,
57(1):pp. 97–109.
William P. Headden III, Mark Johnson, and David McClosky.
2009. Improving unsupervised dependency parsing with
richer contexts and smoothing. In Proceedings of Hu-
man Language Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the Association
for Computational Linguistics, NAACL ’09, pages 101–
109, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Dan Klein and Christopher D. Manning. 2004. Corpus-
based induction of syntactic structure: models of depen-
dency and constituency. In Proceedings of the 42nd An-
nual Meeting on Association for Computational Linguis-
tics, ACL ’04, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Kevin Knight and Daniel Marcu. 2002. Summarization be-
yond sentence extraction: a probabilistic approach to sen-
tence compression. Artif. Intell., 139(1):91–107, July.
Sandra K¨ubler, Ryan T. McDonald, and Joakim Nivre. 2009.
Dependency Parsing. Synthesis Lectures on Human Lan-
guage Technologies. Morgan &amp; Claypool Publishers.
Mark´eta Lopatkov´a, Martin Pl´atek, and Vladislav Kuboˇn.
2005. Modeling syntax of free word-order languages:
Dependency analysis by reduction. In V´aclav Matouˇsek,
Pavel Mautner, and Tom´aˇs Pavelka, editors, Lecture Notes
in Artificial Intelligence, Proceedings of the 8th Interna-
tional Conference, TSD 2005, volume 3658 of Lecture
Notes in Computer Science, pages 140–147, Berlin / Hei-
delberg. Springer.
Martin Majliˇs and Zdenˇek ˇZabokrtsk´y. 2012. Language
richness of the web. In Proceedings of the Eight Interna-
tional Conference on Language Resources and Evaluation
(LREC 2012), Istanbul, Turkey, May. European Language
Resources Association (ELRA).
David Mareˇcek and Zdenˇek ˇZabokrtsk´y. 2012. Exploiting
reducibility in unsupervised dependency parsing. In Pro-
ceedings of the 2012 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Computational
Natural Language Learning, EMNLP-CoNLL ’12, pages
297–307, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011. Multi-
source transfer of delexicalized dependency parsers. In
Proceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 62–72, Edin-
burgh, Scotland, UK., July. Association for Computational
Linguistics.
Tahira Naseem, Harr Chen, Regina Barzilay, and Mark John-
son. 2010. Using universal linguistic knowledge to guide
grammar induction. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language Pro-
cessing, EMNLP ’10, pages 1234–1244, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Joakim Nivre, Johan Hall, Sandra K¨ubler, Ryan McDonald,
Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007.
The CoNLL 2007 Shared Task on Dependency Parsing.
In Proceedings of the CoNLL Shared Task Session of
EMNLP-CoNLL 2007, pages 915–932, Prague, Czech Re-
public, June. Association for Computational Linguistics.
Mohammad Sadegh Rasooli and Heshaam Faili. 2012. Fast
unsupervised dependency parsing with arc-standard tran-
sitions. In Proceedings of ROBUS-UNSUP, pages 1–9.
Noah Ashton Smith. 2007. Novel estimation methods
for unsupervised discovery of latent structure in natu-
ral language text. Ph.D. thesis, Baltimore, MD, USA.
AAI3240799.
Anders Søgaard. 2011. From ranked words to dependency
trees: two-stage unsupervised non-projective dependency
parsing. In Proceedings of TextGraphs-6: Graph-based
Methods for Natural Language Processing, TextGraphs-
6, pages 60–68, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Jurafsky.
2010. From baby steps to leapfrog: how ”less is more” in
unsupervised dependency parsing. In Human Language
Technologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computational
Linguistics, HLT ’10, pages 751–759, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Valentin I. Spitkovsky, Hiyan Alshawi, Angel X. Chang, and
Daniel Jurafsky. 2011a. Unsupervised dependency pars-
ing without gold part-of-speech tags. In Proceedings of
the 2011 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2011).
Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Juraf-
sky. 2011b. Punctuation: Making a point in unsuper-
vised dependency parsing. In Proceedings of the Fifteenth
Conference on Computational Natural Language Learn-
ing (CoNLL-2011).
Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Juraf-
sky. 2012. Three Dependency-and-Boundary Models for
Grammar Induction. In Proceedings of the 2012 Con-
ference on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learning
(EMNLP-CoNLL 2012).
</reference>
<page confidence="0.997109">
290
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.450104">
<title confidence="0.9866975">Stop-probability estimates computed on a large improve Unsupervised Dependency Parsing</title>
<author confidence="0.627755">Mareˇcek</author>
<affiliation confidence="0.850675">Charles University in Prague, Faculty of Mathematics and Institute of Formal and Applied</affiliation>
<address confidence="0.729108">Malostransk´e n´amˇest´ı 25, 11800 Prague, Czech</address>
<abstract confidence="0.9974094">Even though the quality of unsupervised dependency parsers grows, they often fail in recognition of very basic dependencies. In this paper, we exploit a prior knowledge of STOP-probabilities (whether a given word has any children in a given direction), which is obtained from a large raw corpus using the reducibility principle. By incorporating this knowledge into Dependency Model with Valence, we managed to considerably outperform the state-of-theart results in terms of average attachment score over 20 treebanks from CoNLL 2006 and 2007 shared tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James K Baker</author>
</authors>
<title>Trainable grammars for speech recognition.</title>
<date>1979</date>
<booktitle>In Speech communication papers presented at the 97th Meeting of the Acoustical Society,</booktitle>
<pages>547--550</pages>
<contexts>
<context position="1262" citStr="Baker, 1979" startWordPosition="180" endWordPosition="181">ity principle. By incorporating this knowledge into Dependency Model with Valence, we managed to considerably outperform the state-of-theart results in terms of average attachment score over 20 treebanks from CoNLL 2006 and 2007 shared tasks. 1 Introduction The task of unsupervised dependency parsing (which strongly relates to the grammar induction task) has become popular in the last decade, and its quality has been greatly increasing during this period. The first implementation of Dependency Model with Valence (DMV) (Klein and Manning, 2004) with a simple inside-outside inference algorithm (Baker, 1979) achieved 36% attachment score on English and was the first system outperforming the adjacent-word baseline.1 Current attachment scores of state-of-the-art unsupervised parsers are higher than 50% for many languages (Spitkovsky et al., 2012; Blunsom and Cohn, 2010). This is still far below the supervised approaches, but their indisputable advantage is the fact that no annotated treebanks are needed and the induced structures are not burdened by any linguistic conventions. Moreover, 1The adjacent-word baseline is a dependency tree in which each word is attached to the previous (or the following</context>
</contexts>
<marker>Baker, 1979</marker>
<rawString>James K. Baker. 1979. Trainable grammars for speech recognition. In Speech communication papers presented at the 97th Meeting of the Acoustical Society, pages 547– 550.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yonatan Bisk</author>
<author>Julia Hockenmaier</author>
</authors>
<title>Induction of linguistic structure with combinatory categorial grammars.</title>
<date>2012</date>
<booktitle>The NAACL-HLT Workshop on the Induction of Linguistic Structure,</booktitle>
<pages>90</pages>
<contexts>
<context position="2824" citStr="Bisk and Hockenmaier (2012)" startWordPosition="435" endWordPosition="438"> generative model where the grammar is expressed by two probability distributions: Pchoose(cd|ch, dir), which generates a new child cd attached to the head ch in the direction dir (left or right), and Pstop(STOP|ch, dir, · · · ), which makes a decision whether to generate another child of ch in the direction dir or not.2 Such a grammar is then inferred using sampling or variational methods. Unfortunately, there are still cases where the inferred grammar is very different from the grammar we would expect, e.g. verbs become leaves instead of governing the sentences. Rasooli and Faili (2012) and Bisk and Hockenmaier (2012) made some efforts to boost the verbocentricity of the inferred structures; however, both of the approaches require manual identification of the POS tags marking the verbs, which renders them useless when unsupervised POS tags are employed. The main contribution of this paper is a considerable improvement of unsupervised parsing quality by estimating the Pstop probabilities externally using a very large corpus, and employing this prior knowledge in the standard inference of DMV. The estimation is done using the reducibility principle introduced in (Mareˇcek and ˇZabokrtsk´y, 2012). The reducib</context>
<context position="8609" citStr="Bisk and Hockenmaier, 2012" startWordPosition="1367" endWordPosition="1370">introduce the Extended Valence Grammar and add lexicalization and smoothing. Blunsom and Cohn (2010) use tree substitution grammars, which allow learning of larger dependency fragments by employing the Pitman-Yor process. Spitkovsky et al. (2010) improve the inference using iterated learning of increasingly longer sentences. Further improvements were achieved by better dealing with punctuation (Spitkovsky et al., 2011b) and new “boundary” models (Spitkovsky et al., 2012). 282 Other approaches to unsupervised dependency parsing were described e.g. in (Søgaard, 2011), (Cohen et al., 2011), and (Bisk and Hockenmaier, 2012). There also exist “less unsupervised” approaches that utilize an external knowledge of the POS tagset. For example, Rasooli and Faili (2012) identify the last verb in the sentence, minimize its probability of reduction and thus push it to the root position. Naseem et al. (2010) make use of manually-specified universal dependency rules such as Verb→Noun, Noun→Adjective. McDonald et al. (2011) identify the POS tags by a crosslingual transfer. Such approaches achieve better results; however, they are useless for grammar induction from plain text. 3 STOP-probability estimation 3.1 Recognition of </context>
</contexts>
<marker>Bisk, Hockenmaier, 2012</marker>
<rawString>Yonatan Bisk and Julia Hockenmaier. 2012. Induction of linguistic structure with combinatory categorial grammars. The NAACL-HLT Workshop on the Induction of Linguistic Structure, page 90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
</authors>
<title>Unsupervised induction of tree substitution grammars for dependency parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10,</booktitle>
<pages>1204--1213</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1527" citStr="Blunsom and Cohn, 2010" startWordPosition="217" endWordPosition="220">ion The task of unsupervised dependency parsing (which strongly relates to the grammar induction task) has become popular in the last decade, and its quality has been greatly increasing during this period. The first implementation of Dependency Model with Valence (DMV) (Klein and Manning, 2004) with a simple inside-outside inference algorithm (Baker, 1979) achieved 36% attachment score on English and was the first system outperforming the adjacent-word baseline.1 Current attachment scores of state-of-the-art unsupervised parsers are higher than 50% for many languages (Spitkovsky et al., 2012; Blunsom and Cohn, 2010). This is still far below the supervised approaches, but their indisputable advantage is the fact that no annotated treebanks are needed and the induced structures are not burdened by any linguistic conventions. Moreover, 1The adjacent-word baseline is a dependency tree in which each word is attached to the previous (or the following) word. The attachment score of 35.9% on all the WSJ test sentences was taken from (Blunsom and Cohn, 2010). supervised parsers always only simulate the treebanks they were trained on, whereas unsupervised parsers have an ability to be fitted to different particula</context>
<context position="8082" citStr="Blunsom and Cohn (2010)" startWordPosition="1290" endWordPosition="1293">h was used for text summarization. The task is to shorten the sentences while retaining the most important pieces of information, using the knowledge of the grammar. Conversely, our task is to induce the grammar using the sentences and their shortened versions. Dependency Model with Valence (DMV) has been the most popular approach to unsupervised dependency parsing in the recent years. It was introduced by Klein and Manning (2004) and further improved by Smith (2007) and Cohen et al. (2008). Headden III et al. (2009) introduce the Extended Valence Grammar and add lexicalization and smoothing. Blunsom and Cohn (2010) use tree substitution grammars, which allow learning of larger dependency fragments by employing the Pitman-Yor process. Spitkovsky et al. (2010) improve the inference using iterated learning of increasingly longer sentences. Further improvements were achieved by better dealing with punctuation (Spitkovsky et al., 2011b) and new “boundary” models (Spitkovsky et al., 2012). 282 Other approaches to unsupervised dependency parsing were described e.g. in (Søgaard, 2011), (Cohen et al., 2011), and (Bisk and Hockenmaier, 2012). There also exist “less unsupervised” approaches that utilize an externa</context>
<context position="32837" citStr="Blunsom and Cohn (2010)" startWordPosition="5643" endWordPosition="5647">rk In this work, we studied the possibility of estimating the DMV stop-probabilities from a large raw corpus. We proved that such prior knowledge about stop-probabilities incorporated into the standard DMV model significantly improves the unsupervised dependency parsing and, since we are not aware of any other fully unsupervised dependency parser with higher average attachment score over CoNLL data, we state that we reached a new stateof-the-art result.5 4Possibly the current state-of-the-art results. They were compared with many previous works. 5A possible competitive work may be the work by Blunsom and Cohn (2010), who reached 55% accuracy on English as well. However, they do not provide scores measured on other CoNLL treebanks. 288 CoNLL this work other systems language year P dmv P dmv+est reference P dmv+tb spi12 mar12 stop stop stop Arabic 06 10.6 (±8.7) 38.2 (±0.5) 61.2 10.9 26.5 Arabic 07 22.0 (±0.1) 35.3 (±0.2) 65.3 44.9 27.9 Basque 07 41.1 (±0.2) 35.5 (±0.2) 52.3 33.3 26.8 Bulgarian 06 25.9 (±1.4) 54.9 (±0.2) 73.2 65.2 46.0 Catalan 07 34.9 (±3.4) 67.0 (±1.7) 72.0 62.1 47.0 Czech 06 32.3 (±3.8) 52.4 (±5.2) 64.0 55.1 49.5 Czech 07 32.9 (±0.8) 51.9 (±5.2) 62.1 54.2 48.0 Danish 06 30.8 (±4.3) 41.6 </context>
</contexts>
<marker>Blunsom, Cohn, 2010</marker>
<rawString>Phil Blunsom and Trevor Cohn. 2010. Unsupervised induction of tree substitution grammars for dependency parsing. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10, pages 1204–1213, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
</authors>
<title>TnT - A Statistical Part-of-Speech Tagger.</title>
<date>2000</date>
<booktitle>Proceedings of the sixth conference on Applied natural language processing,</booktitle>
<pages>8</pages>
<contexts>
<context position="24732" citStr="Brants, 2000" startWordPosition="4225" endWordPosition="4226">), we use the Wikipedia articles from W2C corpus (Majliˇs and ˇZabokrtsk´y, 2012), which provide sufficient amount of data for our purposes. Statistics across languages are shown in Table 1. The Wikipedia texts were automatically tokenized and segmented to sentences so that their tokenization was similar to the one in the CoNLL evaluation treebanks. Unfortunately, we were not able to find any segmenter for Chinese that would produce a desired segmentation; therefore, we removed Chinese from evaluation. The next step was to provide the Wikipedia texts with POS tags. We employed the TnT tagger (Brants, 2000) which was trained on the recount(e) 286 language tokens red. language tokens red. (mil.) seq. (mil.) seq. Arabic 19.7 546 Greek 20.9 1037 Basque 14.1 645 Hungarian 26.3 2237 Bulgarian 18.8 1808 Italian 39.7 723 Catalan 27.0 712 Japanese 2.6 31 Czech 20.3 930 Portuguese 31.7 4765 Danish 15.9 576 Slovenian 13.7 513 Dutch 27.1 880 Spanish 53.4 1156 English 85.0 7603 Swedish 19.2 481 German 56.9 1488 Turkish 16.5 5706 Table 1: Wikipedia texts statistics: total number of tokens and number of reducible sequences found in them. spective CoNLL training data. The quality of such tagging is not very hi</context>
</contexts>
<marker>Brants, 2000</marker>
<rawString>Thorsten Brants. 2000. TnT - A Statistical Part-of-Speech Tagger. Proceedings of the sixth conference on Applied natural language processing, page 8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<title>CoNLL-X shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Tenth Conference on Computational Natural Language Learning, CoNLL-X ’06,</booktitle>
<pages>149--164</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="23775" citStr="Buchholz and Marsi, 2006" startWordPosition="4063" endWordPosition="4066">lgorithm (Chu and Liu, 1965). Therefore, the resulting trees consist of edges maximizing the sum of individual counts: �TMST = arg max T eET It is important to note that the MST algorithm may produce non-projective trees. Even if we average the strictly projective dependency trees, some non-projective edges may appear in the result. This might be an advantage since correct non-projective edges can be predicted; however, this relaxation may introduce mistakes as well. 6 Experiments 6.1 Data We use two types of resources in our experiments. The first type are CoNLL treebanks from the year 2006 (Buchholz and Marsi, 2006) and 2007 (Nivre et al., 2007), which we use for inference and for evaluation. As is the standard practice in unsupervised parsing evaluation, we removed all punctuation marks from the trees. In case a punctuation node was not a leaf, its children are attached to the parent of the removed node. For estimating the STOP probabilities (Section 3), we use the Wikipedia articles from W2C corpus (Majliˇs and ˇZabokrtsk´y, 2012), which provide sufficient amount of data for our purposes. Statistics across languages are shown in Table 1. The Wikipedia texts were automatically tokenized and segmented to</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In Proceedings of the Tenth Conference on Computational Natural Language Learning, CoNLL-X ’06, pages 149–164, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christos Christodoulopoulos</author>
<author>Sharon Goldwater</author>
<author>Mark Steedman</author>
</authors>
<title>Turning the pipeline into a loop: Iterated unsupervised dependency parsing and PoS induction.</title>
<date>2012</date>
<booktitle>In Proceedings of the NAACL-HLT Workshop on the Induction of Linguistic Structure,</booktitle>
<pages>96--99</pages>
<contexts>
<context position="34706" citStr="Christodoulopoulos et al. (2012)" startWordPosition="5966" endWordPosition="5969">4 48.7 64.7 42.2 40.0 Table 2: Attachment scores on CoNLL 2006 and 2007 data. Standard deviations are provided in brackets. DMV model using standard Pdmv stop probability is compared with DMV with Pdmv+est stop , which incorporates STOP estimations based on reducibility principle. The reference DMV uses Ptbp, which are sto computed directly on the treebanks. The results reported in previous works by Spitkovsky et al. (2012), and Mareˇcek and ˇZabokrtsk´y (2012) follows. In future work, we would like to focus on unsupervised parsing without gold POS tags (see e.g. Spitkovsky et al. (2011a) and Christodoulopoulos et al. (2012)). We suppose that many of the current works on unsupervised dependency parsers use gold POS tags only as a simplification of this task, and that the ultimate purpose of this effort is to develop a fully unsupervised induction of linguistic structure from raw texts that would be useful across many languages, domains, and applications. The software which implements the algorithms described in this paper, together with Pest stop estimations computed on Wikipedia texts, can be downloaded at http://ufal.mff.cuni.cz/˜marecek/udp/. Acknowledgments This work has been supported by the AMALACH grant (D</context>
</contexts>
<marker>Christodoulopoulos, Goldwater, Steedman, 2012</marker>
<rawString>Christos Christodoulopoulos, Sharon Goldwater, and Mark Steedman. 2012. Turning the pipeline into a loop: Iterated unsupervised dependency parsing and PoS induction. In Proceedings of the NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 96–99, June. Y. J. Chu and T. H. Liu. 1965. On the Shortest Arborescence of a Directed Graph. Science Sinica, 14:1396–1400.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shay B Cohen</author>
<author>Kevin Gimpel</author>
<author>Noah A Smith</author>
</authors>
<title>Logistic normal priors for unsupervised probabilistic grammar induction.</title>
<date>2008</date>
<booktitle>In Neural Information Processing Systems,</booktitle>
<pages>321--328</pages>
<contexts>
<context position="7954" citStr="Cohen et al. (2008)" startWordPosition="1270" endWordPosition="1273">ags only. Another task related to reducibility is sentence compression (Knight and Marcu, 2002; Cohn and Lapata, 2008), which was used for text summarization. The task is to shorten the sentences while retaining the most important pieces of information, using the knowledge of the grammar. Conversely, our task is to induce the grammar using the sentences and their shortened versions. Dependency Model with Valence (DMV) has been the most popular approach to unsupervised dependency parsing in the recent years. It was introduced by Klein and Manning (2004) and further improved by Smith (2007) and Cohen et al. (2008). Headden III et al. (2009) introduce the Extended Valence Grammar and add lexicalization and smoothing. Blunsom and Cohn (2010) use tree substitution grammars, which allow learning of larger dependency fragments by employing the Pitman-Yor process. Spitkovsky et al. (2010) improve the inference using iterated learning of increasingly longer sentences. Further improvements were achieved by better dealing with punctuation (Spitkovsky et al., 2011b) and new “boundary” models (Spitkovsky et al., 2012). 282 Other approaches to unsupervised dependency parsing were described e.g. in (Søgaard, 2011),</context>
</contexts>
<marker>Cohen, Gimpel, Smith, 2008</marker>
<rawString>Shay B. Cohen, Kevin Gimpel, and Noah A. Smith. 2008. Logistic normal priors for unsupervised probabilistic grammar induction. In Neural Information Processing Systems, pages 321–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shay B Cohen</author>
<author>Dipanjan Das</author>
<author>Noah A Smith</author>
</authors>
<title>Unsupervised structure prediction with non-parallel multilingual guidance.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<pages>50--61</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="8575" citStr="Cohen et al., 2011" startWordPosition="1362" endWordPosition="1365">Headden III et al. (2009) introduce the Extended Valence Grammar and add lexicalization and smoothing. Blunsom and Cohn (2010) use tree substitution grammars, which allow learning of larger dependency fragments by employing the Pitman-Yor process. Spitkovsky et al. (2010) improve the inference using iterated learning of increasingly longer sentences. Further improvements were achieved by better dealing with punctuation (Spitkovsky et al., 2011b) and new “boundary” models (Spitkovsky et al., 2012). 282 Other approaches to unsupervised dependency parsing were described e.g. in (Søgaard, 2011), (Cohen et al., 2011), and (Bisk and Hockenmaier, 2012). There also exist “less unsupervised” approaches that utilize an external knowledge of the POS tagset. For example, Rasooli and Faili (2012) identify the last verb in the sentence, minimize its probability of reduction and thus push it to the root position. Naseem et al. (2010) make use of manually-specified universal dependency rules such as Verb→Noun, Noun→Adjective. McDonald et al. (2011) identify the POS tags by a crosslingual transfer. Such approaches achieve better results; however, they are useless for grammar induction from plain text. 3 STOP-probabil</context>
</contexts>
<marker>Cohen, Das, Smith, 2011</marker>
<rawString>Shay B. Cohen, Dipanjan Das, and Noah A. Smith. 2011. Unsupervised structure prediction with non-parallel multilingual guidance. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 50–61, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Mirella Lapata</author>
</authors>
<title>Sentence compression beyond word deletion.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics -Volume 1, COLING ’08,</booktitle>
<pages>137--144</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="7453" citStr="Cohn and Lapata, 2008" startWordPosition="1185" endWordPosition="1188">ˇcek and ˇZabokrtsk´y, 2012). Our dependency model contained a submodel which directly prioritized subtrees that form reducible sequences of POS tags. Reducibility scores of given POS tag sequences were estimated using a large corpus of Wikipedia articles. The weakness of this approach was the fact that longer sequences of POS tags are very sparse and no reducibility scores could be estimated for them. In this paper, we avoid this shortcoming by estimating the STOP probabilities for individual POS tags only. Another task related to reducibility is sentence compression (Knight and Marcu, 2002; Cohn and Lapata, 2008), which was used for text summarization. The task is to shorten the sentences while retaining the most important pieces of information, using the knowledge of the grammar. Conversely, our task is to induce the grammar using the sentences and their shortened versions. Dependency Model with Valence (DMV) has been the most popular approach to unsupervised dependency parsing in the recent years. It was introduced by Klein and Manning (2004) and further improved by Smith (2007) and Cohen et al. (2008). Headden III et al. (2009) introduce the Extended Valence Grammar and add lexicalization and smoot</context>
</contexts>
<marker>Cohn, Lapata, 2008</marker>
<rawString>Trevor Cohn and Mirella Lapata. 2008. Sentence compression beyond word deletion. In Proceedings of the 22nd International Conference on Computational Linguistics -Volume 1, COLING ’08, pages 137–144, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kim Gerdes</author>
<author>Sylvain Kahane</author>
</authors>
<title>Defining dependencies (and constituents).</title>
<date>2011</date>
<booktitle>In Proceedings of Dependency Linguistics 2011,</booktitle>
<location>Barcelona.</location>
<contexts>
<context position="6688" citStr="Gerdes and Kahane (2011)" startWordPosition="1068" endWordPosition="1071">guistic criteria for recognizing dependency relations. As mentioned e.g. by K¨ubler et al. (2009), the head h of a construction c determines the syntactic category of c and can often replace c. In other words, the descendants of h can be often removed without making the sentence incorrect. Similarly, in the Dependency Analysis by Reduction (Lopatkov´a et al., 2005), the authors assume that stepwise deletions of dependent elements within a sentence preserve its syntactic correctness. A similar idea of dependency analysis by splitting a sentence into all possible acceptable fragments is used by Gerdes and Kahane (2011). We have directly utilized the aforementioned criteria for dependency relations in unsupervised dependency parsing in our previous paper (Mareˇcek and ˇZabokrtsk´y, 2012). Our dependency model contained a submodel which directly prioritized subtrees that form reducible sequences of POS tags. Reducibility scores of given POS tag sequences were estimated using a large corpus of Wikipedia articles. The weakness of this approach was the fact that longer sequences of POS tags are very sparse and no reducibility scores could be estimated for them. In this paper, we avoid this shortcoming by estimat</context>
</contexts>
<marker>Gerdes, Kahane, 2011</marker>
<rawString>Kim Gerdes and Sylvain Kahane. 2011. Defining dependencies (and constituents). In Proceedings of Dependency Linguistics 2011, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter R Gilks</author>
<author>S Richardson</author>
<author>David J Spiegelhalter</author>
</authors>
<title>Markov chain Monte Carlo in practice. Interdisciplinary statistics.</title>
<date>1996</date>
<publisher>Chapman &amp; Hall.</publisher>
<contexts>
<context position="18712" citStr="Gilks et al., 1996" startWordPosition="3160" endWordPosition="3163">lity whether a particular head POS tag ch can or cannot have children in a particular direction, i.e if adj = 1. 3The number of classes ICI is often used in the denominator. We decided to put its reverse value into the numerator since we observed such model to perform better for a constant value of α. over different languages and tagsets. �Ptreebank = Ptree(T). TEtreebank An important property of the CRP is the fact that the factors are exchangeable – i.e. no matter how the trees are ordered in the treebank, the Ptreebank is always the same. 5 Inference We employ the Gibbs sampling algorithm (Gilks et al., 1996). Unlike in (Mareˇcek and ˇZabokrtsk´y, 2012), where edges were sampled individually, we sample whole trees from all possibilities on a given sentence using dynamic programming. The algorithm works as follows: 1. A random projective dependency tree is assigned to each sentence in the corpus. 2. Sampling: We go through the sentences in a random order. For each sentence, we sample a new dependency tree based on all other trees that are currently in the corpus. 3. Step 2 is repeated in many iterations. In this work, the number of iterations was set to 1000. 4. After the burn-in period (which was </context>
</contexts>
<marker>Gilks, Richardson, Spiegelhalter, 1996</marker>
<rawString>Walter R. Gilks, S. Richardson, and David J. Spiegelhalter. 1996. Markov chain Monte Carlo in practice. Interdisciplinary statistics. Chapman &amp; Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Keith Hastings</author>
</authors>
<title>Monte carlo sampling methods using markov chains and their applications.</title>
<date>1970</date>
<journal>Biometrika,</journal>
<volume>57</volume>
<issue>1</issue>
<pages>97--109</pages>
<contexts>
<context position="20539" citStr="Hastings, 1970" startWordPosition="3482" endWordPosition="3483">trees using a modified probability P0tree(T). In Ptree(T), the probability of an edge depends on counts of all other edges, including the edges in the same tree. We instead use P0tree(T ), where the counts are computed using only the other trees in the corpus, i.e., probabilities Pdmv+est stop (STOP|ch, dir, 1, cf) = (1 − β) αs 3 + count−(STOP, ch, dir, 1, c f) · αs + count−(ch, dir, 1, cf) + β · Pest stop(ch, dir) 285 of edges of T are independent. There is a standard way to sample using the real Ptree(T) – we can use P0tree(T) as a proposal distribution in the Metropolis-Hastings algorithm (Hastings, 1970), which then produces trees with probabilities proportional to Ptree(T) using acceptance-rejection scheme. We do not take this approach and we sample proportionally to P0tree(T) only, because we believe that for large enough corpora, the two distributions are nearly identical. To sample a tree containing words w1, ... , wN with probability proportional to P0tree(T), we first compute three tables: • ti(g, i, j) for g &lt; i or g &gt; j is the sum of probabilities of any tree on words wi,... , wj whose root is a child of wg, but not an outermost child in its direction; • to(g, i, j) is the same, but t</context>
</contexts>
<marker>Hastings, 1970</marker>
<rawString>W. Keith Hastings. 1970. Monte carlo sampling methods using markov chains and their applications. Biometrika, 57(1):pp. 97–109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William P Headden Mark Johnson</author>
<author>David McClosky</author>
</authors>
<title>Improving unsupervised dependency parsing with richer contexts and smoothing.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL ’09,</booktitle>
<pages>101--109</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Johnson, McClosky, 2009</marker>
<rawString>William P. Headden III, Mark Johnson, and David McClosky. 2009. Improving unsupervised dependency parsing with richer contexts and smoothing. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL ’09, pages 101– 109, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Corpusbased induction of syntactic structure: models of dependency and constituency.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, ACL ’04,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1199" citStr="Klein and Manning, 2004" startWordPosition="169" endWordPosition="172">n direction), which is obtained from a large raw corpus using the reducibility principle. By incorporating this knowledge into Dependency Model with Valence, we managed to considerably outperform the state-of-theart results in terms of average attachment score over 20 treebanks from CoNLL 2006 and 2007 shared tasks. 1 Introduction The task of unsupervised dependency parsing (which strongly relates to the grammar induction task) has become popular in the last decade, and its quality has been greatly increasing during this period. The first implementation of Dependency Model with Valence (DMV) (Klein and Manning, 2004) with a simple inside-outside inference algorithm (Baker, 1979) achieved 36% attachment score on English and was the first system outperforming the adjacent-word baseline.1 Current attachment scores of state-of-the-art unsupervised parsers are higher than 50% for many languages (Spitkovsky et al., 2012; Blunsom and Cohn, 2010). This is still far below the supervised approaches, but their indisputable advantage is the fact that no annotated treebanks are needed and the induced structures are not burdened by any linguistic conventions. Moreover, 1The adjacent-word baseline is a dependency tree i</context>
<context position="7893" citStr="Klein and Manning (2004)" startWordPosition="1258" endWordPosition="1261">rtcoming by estimating the STOP probabilities for individual POS tags only. Another task related to reducibility is sentence compression (Knight and Marcu, 2002; Cohn and Lapata, 2008), which was used for text summarization. The task is to shorten the sentences while retaining the most important pieces of information, using the knowledge of the grammar. Conversely, our task is to induce the grammar using the sentences and their shortened versions. Dependency Model with Valence (DMV) has been the most popular approach to unsupervised dependency parsing in the recent years. It was introduced by Klein and Manning (2004) and further improved by Smith (2007) and Cohen et al. (2008). Headden III et al. (2009) introduce the Extended Valence Grammar and add lexicalization and smoothing. Blunsom and Cohn (2010) use tree substitution grammars, which allow learning of larger dependency fragments by employing the Pitman-Yor process. Spitkovsky et al. (2010) improve the inference using iterated learning of increasingly longer sentences. Further improvements were achieved by better dealing with punctuation (Spitkovsky et al., 2011b) and new “boundary” models (Spitkovsky et al., 2012). 282 Other approaches to unsupervis</context>
<context position="14172" citStr="Klein and Manning, 2004" startWordPosition="2348" endWordPosition="2351">l words in the treebank should be 2/3. After some experimenting, we chose the following normalization formula with a normalization constant v. The condition (1) is fulfilled for any positive value of v. Its exact value is set in accordance with the requirement (2) so that the average value of P est stop is 2/3. 2 count(c)Pest stop(c, dir) = 3 · 2W, where count(c) is the number of words with POS tag c in the corpus. We find the unique value of v that fulfills the previous equation numerically using a binary search algorithm. 4 Model We use the standard generative Dependency Model with Valence (Klein and Manning, 2004). The generative story is the following: First, the head of the sentence is generated. Then, for each head, all its left children are generated, then the left STOP, then all its right children, and then the right STOP. When a child is generated, the algorithm immediately recurses to generate its subtree. When deciding whether to generate another child in the direction dir or the STOP symbol, we use the Pdmv stop (STOP|ch, dir, adj, cf) model. The new child cd in the direction dir is generated according to the Pchoose(cd|ch, dir) model. The probability of the whole dependency tree T is the foll</context>
</contexts>
<marker>Klein, Manning, 2004</marker>
<rawString>Dan Klein and Christopher D. Manning. 2004. Corpusbased induction of syntactic structure: models of dependency and constituency. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, ACL ’04, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Summarization beyond sentence extraction: a probabilistic approach to sentence compression.</title>
<date>2002</date>
<journal>Artif. Intell.,</journal>
<volume>139</volume>
<issue>1</issue>
<contexts>
<context position="7429" citStr="Knight and Marcu, 2002" startWordPosition="1181" endWordPosition="1184">our previous paper (Mareˇcek and ˇZabokrtsk´y, 2012). Our dependency model contained a submodel which directly prioritized subtrees that form reducible sequences of POS tags. Reducibility scores of given POS tag sequences were estimated using a large corpus of Wikipedia articles. The weakness of this approach was the fact that longer sequences of POS tags are very sparse and no reducibility scores could be estimated for them. In this paper, we avoid this shortcoming by estimating the STOP probabilities for individual POS tags only. Another task related to reducibility is sentence compression (Knight and Marcu, 2002; Cohn and Lapata, 2008), which was used for text summarization. The task is to shorten the sentences while retaining the most important pieces of information, using the knowledge of the grammar. Conversely, our task is to induce the grammar using the sentences and their shortened versions. Dependency Model with Valence (DMV) has been the most popular approach to unsupervised dependency parsing in the recent years. It was introduced by Klein and Manning (2004) and further improved by Smith (2007) and Cohen et al. (2008). Headden III et al. (2009) introduce the Extended Valence Grammar and add </context>
</contexts>
<marker>Knight, Marcu, 2002</marker>
<rawString>Kevin Knight and Daniel Marcu. 2002. Summarization beyond sentence extraction: a probabilistic approach to sentence compression. Artif. Intell., 139(1):91–107, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandra K¨ubler</author>
<author>Ryan T McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Dependency Parsing. Synthesis Lectures on Human Language Technologies.</title>
<date>2009</date>
<publisher>Morgan &amp; Claypool Publishers.</publisher>
<marker>K¨ubler, McDonald, Nivre, 2009</marker>
<rawString>Sandra K¨ubler, Ryan T. McDonald, and Joakim Nivre. 2009. Dependency Parsing. Synthesis Lectures on Human Language Technologies. Morgan &amp; Claypool Publishers.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Mark´eta Lopatkov´a</author>
<author>Martin Pl´atek</author>
<author>Vladislav Kuboˇn</author>
</authors>
<title>Modeling syntax of free word-order languages: Dependency analysis by reduction.</title>
<date>2005</date>
<booktitle>Lecture Notes in Artificial Intelligence, Proceedings of the 8th International Conference, TSD 2005,</booktitle>
<volume>3658</volume>
<pages>140--147</pages>
<editor>In V´aclav Matouˇsek, Pavel Mautner, and Tom´aˇs Pavelka, editors,</editor>
<publisher>Springer.</publisher>
<location>Berlin / Heidelberg.</location>
<marker>Lopatkov´a, Pl´atek, Kuboˇn, 2005</marker>
<rawString>Mark´eta Lopatkov´a, Martin Pl´atek, and Vladislav Kuboˇn. 2005. Modeling syntax of free word-order languages: Dependency analysis by reduction. In V´aclav Matouˇsek, Pavel Mautner, and Tom´aˇs Pavelka, editors, Lecture Notes in Artificial Intelligence, Proceedings of the 8th International Conference, TSD 2005, volume 3658 of Lecture Notes in Computer Science, pages 140–147, Berlin / Heidelberg. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Majliˇs</author>
<author>Zdenˇek ˇZabokrtsk´y</author>
</authors>
<title>Language richness of the web.</title>
<date>2012</date>
<journal>European Language Resources Association (ELRA).</journal>
<booktitle>In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC 2012),</booktitle>
<location>Istanbul, Turkey,</location>
<marker>Majliˇs, ˇZabokrtsk´y, 2012</marker>
<rawString>Martin Majliˇs and Zdenˇek ˇZabokrtsk´y. 2012. Language richness of the web. In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC 2012), Istanbul, Turkey, May. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Mareˇcek</author>
<author>Zdenˇek ˇZabokrtsk´y</author>
</authors>
<title>Exploiting reducibility in unsupervised dependency parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12,</booktitle>
<pages>297--307</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Mareˇcek, ˇZabokrtsk´y, 2012</marker>
<rawString>David Mareˇcek and Zdenˇek ˇZabokrtsk´y. 2012. Exploiting reducibility in unsupervised dependency parsing. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12, pages 297–307, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Slav Petrov</author>
<author>Keith Hall</author>
</authors>
<title>Multisource transfer of delexicalized dependency parsers.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>62--72</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="9004" citStr="McDonald et al. (2011)" startWordPosition="1428" endWordPosition="1432">itkovsky et al., 2011b) and new “boundary” models (Spitkovsky et al., 2012). 282 Other approaches to unsupervised dependency parsing were described e.g. in (Søgaard, 2011), (Cohen et al., 2011), and (Bisk and Hockenmaier, 2012). There also exist “less unsupervised” approaches that utilize an external knowledge of the POS tagset. For example, Rasooli and Faili (2012) identify the last verb in the sentence, minimize its probability of reduction and thus push it to the root position. Naseem et al. (2010) make use of manually-specified universal dependency rules such as Verb→Noun, Noun→Adjective. McDonald et al. (2011) identify the POS tags by a crosslingual transfer. Such approaches achieve better results; however, they are useless for grammar induction from plain text. 3 STOP-probability estimation 3.1 Recognition of reducible sequences We introduced a simple procedure for recognition of reducible sequences in (Mareˇcek and ˇZabokrtsk´y, 2012): The particular sequence of words is removed from the sentence and if the remainder of the sentence exists elsewhere in the corpus, the sequence is considered reducible. We provide an example in Figure 2. The bigram “this weekend” in the sentence “The next competiti</context>
</contexts>
<marker>McDonald, Petrov, Hall, 2011</marker>
<rawString>Ryan McDonald, Slav Petrov, and Keith Hall. 2011. Multisource transfer of delexicalized dependency parsers. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 62–72, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tahira Naseem</author>
<author>Harr Chen</author>
<author>Regina Barzilay</author>
<author>Mark Johnson</author>
</authors>
<title>Using universal linguistic knowledge to guide grammar induction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10,</booktitle>
<pages>1234--1244</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="8888" citStr="Naseem et al. (2010)" startWordPosition="1413" endWordPosition="1416">arning of increasingly longer sentences. Further improvements were achieved by better dealing with punctuation (Spitkovsky et al., 2011b) and new “boundary” models (Spitkovsky et al., 2012). 282 Other approaches to unsupervised dependency parsing were described e.g. in (Søgaard, 2011), (Cohen et al., 2011), and (Bisk and Hockenmaier, 2012). There also exist “less unsupervised” approaches that utilize an external knowledge of the POS tagset. For example, Rasooli and Faili (2012) identify the last verb in the sentence, minimize its probability of reduction and thus push it to the root position. Naseem et al. (2010) make use of manually-specified universal dependency rules such as Verb→Noun, Noun→Adjective. McDonald et al. (2011) identify the POS tags by a crosslingual transfer. Such approaches achieve better results; however, they are useless for grammar induction from plain text. 3 STOP-probability estimation 3.1 Recognition of reducible sequences We introduced a simple procedure for recognition of reducible sequences in (Mareˇcek and ˇZabokrtsk´y, 2012): The particular sequence of words is removed from the sentence and if the remainder of the sentence exists elsewhere in the corpus, the sequence is co</context>
</contexts>
<marker>Naseem, Chen, Barzilay, Johnson, 2010</marker>
<rawString>Tahira Naseem, Harr Chen, Regina Barzilay, and Mark Johnson. 2010. Using universal linguistic knowledge to guide grammar induction. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10, pages 1234–1244, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Sandra K¨ubler</author>
<author>Ryan McDonald</author>
<author>Jens Nilsson</author>
<author>Sebastian Riedel</author>
<author>Deniz Yuret</author>
</authors>
<title>The CoNLL</title>
<date>2007</date>
<booktitle>In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007,</booktitle>
<pages>915--932</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<marker>Nivre, Hall, K¨ubler, McDonald, Nilsson, Riedel, Yuret, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Sandra K¨ubler, Ryan McDonald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007. The CoNLL 2007 Shared Task on Dependency Parsing. In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pages 915–932, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohammad Sadegh Rasooli</author>
<author>Heshaam Faili</author>
</authors>
<title>Fast unsupervised dependency parsing with arc-standard transitions.</title>
<date>2012</date>
<booktitle>In Proceedings of ROBUS-UNSUP,</booktitle>
<pages>1--9</pages>
<contexts>
<context position="2792" citStr="Rasooli and Faili (2012)" startWordPosition="430" endWordPosition="433">aches are based on the DMV, a generative model where the grammar is expressed by two probability distributions: Pchoose(cd|ch, dir), which generates a new child cd attached to the head ch in the direction dir (left or right), and Pstop(STOP|ch, dir, · · · ), which makes a decision whether to generate another child of ch in the direction dir or not.2 Such a grammar is then inferred using sampling or variational methods. Unfortunately, there are still cases where the inferred grammar is very different from the grammar we would expect, e.g. verbs become leaves instead of governing the sentences. Rasooli and Faili (2012) and Bisk and Hockenmaier (2012) made some efforts to boost the verbocentricity of the inferred structures; however, both of the approaches require manual identification of the POS tags marking the verbs, which renders them useless when unsupervised POS tags are employed. The main contribution of this paper is a considerable improvement of unsupervised parsing quality by estimating the Pstop probabilities externally using a very large corpus, and employing this prior knowledge in the standard inference of DMV. The estimation is done using the reducibility principle introduced in (Mareˇcek and </context>
<context position="8750" citStr="Rasooli and Faili (2012)" startWordPosition="1389" endWordPosition="1392"> learning of larger dependency fragments by employing the Pitman-Yor process. Spitkovsky et al. (2010) improve the inference using iterated learning of increasingly longer sentences. Further improvements were achieved by better dealing with punctuation (Spitkovsky et al., 2011b) and new “boundary” models (Spitkovsky et al., 2012). 282 Other approaches to unsupervised dependency parsing were described e.g. in (Søgaard, 2011), (Cohen et al., 2011), and (Bisk and Hockenmaier, 2012). There also exist “less unsupervised” approaches that utilize an external knowledge of the POS tagset. For example, Rasooli and Faili (2012) identify the last verb in the sentence, minimize its probability of reduction and thus push it to the root position. Naseem et al. (2010) make use of manually-specified universal dependency rules such as Verb→Noun, Noun→Adjective. McDonald et al. (2011) identify the POS tags by a crosslingual transfer. Such approaches achieve better results; however, they are useless for grammar induction from plain text. 3 STOP-probability estimation 3.1 Recognition of reducible sequences We introduced a simple procedure for recognition of reducible sequences in (Mareˇcek and ˇZabokrtsk´y, 2012): The particu</context>
</contexts>
<marker>Rasooli, Faili, 2012</marker>
<rawString>Mohammad Sadegh Rasooli and Heshaam Faili. 2012. Fast unsupervised dependency parsing with arc-standard transitions. In Proceedings of ROBUS-UNSUP, pages 1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah Ashton Smith</author>
</authors>
<title>Novel estimation methods for unsupervised discovery of latent structure in natural language text.</title>
<date>2007</date>
<booktitle>Ph.D. thesis,</booktitle>
<location>Baltimore, MD, USA. AAI3240799.</location>
<contexts>
<context position="7930" citStr="Smith (2007)" startWordPosition="1267" endWordPosition="1268"> individual POS tags only. Another task related to reducibility is sentence compression (Knight and Marcu, 2002; Cohn and Lapata, 2008), which was used for text summarization. The task is to shorten the sentences while retaining the most important pieces of information, using the knowledge of the grammar. Conversely, our task is to induce the grammar using the sentences and their shortened versions. Dependency Model with Valence (DMV) has been the most popular approach to unsupervised dependency parsing in the recent years. It was introduced by Klein and Manning (2004) and further improved by Smith (2007) and Cohen et al. (2008). Headden III et al. (2009) introduce the Extended Valence Grammar and add lexicalization and smoothing. Blunsom and Cohn (2010) use tree substitution grammars, which allow learning of larger dependency fragments by employing the Pitman-Yor process. Spitkovsky et al. (2010) improve the inference using iterated learning of increasingly longer sentences. Further improvements were achieved by better dealing with punctuation (Spitkovsky et al., 2011b) and new “boundary” models (Spitkovsky et al., 2012). 282 Other approaches to unsupervised dependency parsing were described </context>
</contexts>
<marker>Smith, 2007</marker>
<rawString>Noah Ashton Smith. 2007. Novel estimation methods for unsupervised discovery of latent structure in natural language text. Ph.D. thesis, Baltimore, MD, USA. AAI3240799.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Søgaard</author>
</authors>
<title>From ranked words to dependency trees: two-stage unsupervised non-projective dependency parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of TextGraphs-6: Graph-based Methods for Natural Language Processing, TextGraphs6,</booktitle>
<pages>60--68</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="8553" citStr="Søgaard, 2011" startWordPosition="1360" endWordPosition="1361">n et al. (2008). Headden III et al. (2009) introduce the Extended Valence Grammar and add lexicalization and smoothing. Blunsom and Cohn (2010) use tree substitution grammars, which allow learning of larger dependency fragments by employing the Pitman-Yor process. Spitkovsky et al. (2010) improve the inference using iterated learning of increasingly longer sentences. Further improvements were achieved by better dealing with punctuation (Spitkovsky et al., 2011b) and new “boundary” models (Spitkovsky et al., 2012). 282 Other approaches to unsupervised dependency parsing were described e.g. in (Søgaard, 2011), (Cohen et al., 2011), and (Bisk and Hockenmaier, 2012). There also exist “less unsupervised” approaches that utilize an external knowledge of the POS tagset. For example, Rasooli and Faili (2012) identify the last verb in the sentence, minimize its probability of reduction and thus push it to the root position. Naseem et al. (2010) make use of manually-specified universal dependency rules such as Verb→Noun, Noun→Adjective. McDonald et al. (2011) identify the POS tags by a crosslingual transfer. Such approaches achieve better results; however, they are useless for grammar induction from plain</context>
</contexts>
<marker>Søgaard, 2011</marker>
<rawString>Anders Søgaard. 2011. From ranked words to dependency trees: two-stage unsupervised non-projective dependency parsing. In Proceedings of TextGraphs-6: Graph-based Methods for Natural Language Processing, TextGraphs6, pages 60–68, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valentin I Spitkovsky</author>
<author>Hiyan Alshawi</author>
<author>Daniel Jurafsky</author>
</authors>
<title>From baby steps to leapfrog: how ”less is more” in unsupervised dependency parsing.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10,</booktitle>
<pages>751--759</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="8228" citStr="Spitkovsky et al. (2010)" startWordPosition="1312" endWordPosition="1315">edge of the grammar. Conversely, our task is to induce the grammar using the sentences and their shortened versions. Dependency Model with Valence (DMV) has been the most popular approach to unsupervised dependency parsing in the recent years. It was introduced by Klein and Manning (2004) and further improved by Smith (2007) and Cohen et al. (2008). Headden III et al. (2009) introduce the Extended Valence Grammar and add lexicalization and smoothing. Blunsom and Cohn (2010) use tree substitution grammars, which allow learning of larger dependency fragments by employing the Pitman-Yor process. Spitkovsky et al. (2010) improve the inference using iterated learning of increasingly longer sentences. Further improvements were achieved by better dealing with punctuation (Spitkovsky et al., 2011b) and new “boundary” models (Spitkovsky et al., 2012). 282 Other approaches to unsupervised dependency parsing were described e.g. in (Søgaard, 2011), (Cohen et al., 2011), and (Bisk and Hockenmaier, 2012). There also exist “less unsupervised” approaches that utilize an external knowledge of the POS tagset. For example, Rasooli and Faili (2012) identify the last verb in the sentence, minimize its probability of reduction</context>
</contexts>
<marker>Spitkovsky, Alshawi, Jurafsky, 2010</marker>
<rawString>Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Jurafsky. 2010. From baby steps to leapfrog: how ”less is more” in unsupervised dependency parsing. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10, pages 751–759, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valentin I Spitkovsky</author>
<author>Hiyan Alshawi</author>
<author>Angel X Chang</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Unsupervised dependency parsing without gold part-of-speech tags.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<contexts>
<context position="8403" citStr="Spitkovsky et al., 2011" startWordPosition="1337" endWordPosition="1340">ar approach to unsupervised dependency parsing in the recent years. It was introduced by Klein and Manning (2004) and further improved by Smith (2007) and Cohen et al. (2008). Headden III et al. (2009) introduce the Extended Valence Grammar and add lexicalization and smoothing. Blunsom and Cohn (2010) use tree substitution grammars, which allow learning of larger dependency fragments by employing the Pitman-Yor process. Spitkovsky et al. (2010) improve the inference using iterated learning of increasingly longer sentences. Further improvements were achieved by better dealing with punctuation (Spitkovsky et al., 2011b) and new “boundary” models (Spitkovsky et al., 2012). 282 Other approaches to unsupervised dependency parsing were described e.g. in (Søgaard, 2011), (Cohen et al., 2011), and (Bisk and Hockenmaier, 2012). There also exist “less unsupervised” approaches that utilize an external knowledge of the POS tagset. For example, Rasooli and Faili (2012) identify the last verb in the sentence, minimize its probability of reduction and thus push it to the root position. Naseem et al. (2010) make use of manually-specified universal dependency rules such as Verb→Noun, Noun→Adjective. McDonald et al. (2011</context>
<context position="34667" citStr="Spitkovsky et al. (2011" startWordPosition="5961" endWordPosition="5964">2) 67.0 44.8 15.7 Average: 36.4 48.7 64.7 42.2 40.0 Table 2: Attachment scores on CoNLL 2006 and 2007 data. Standard deviations are provided in brackets. DMV model using standard Pdmv stop probability is compared with DMV with Pdmv+est stop , which incorporates STOP estimations based on reducibility principle. The reference DMV uses Ptbp, which are sto computed directly on the treebanks. The results reported in previous works by Spitkovsky et al. (2012), and Mareˇcek and ˇZabokrtsk´y (2012) follows. In future work, we would like to focus on unsupervised parsing without gold POS tags (see e.g. Spitkovsky et al. (2011a) and Christodoulopoulos et al. (2012)). We suppose that many of the current works on unsupervised dependency parsers use gold POS tags only as a simplification of this task, and that the ultimate purpose of this effort is to develop a fully unsupervised induction of linguistic structure from raw texts that would be useful across many languages, domains, and applications. The software which implements the algorithms described in this paper, together with Pest stop estimations computed on Wikipedia texts, can be downloaded at http://ufal.mff.cuni.cz/˜marecek/udp/. Acknowledgments This work has</context>
</contexts>
<marker>Spitkovsky, Alshawi, Chang, Jurafsky, 2011</marker>
<rawString>Valentin I. Spitkovsky, Hiyan Alshawi, Angel X. Chang, and Daniel Jurafsky. 2011a. Unsupervised dependency parsing without gold part-of-speech tags. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP 2011).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valentin I Spitkovsky</author>
<author>Hiyan Alshawi</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Punctuation: Making a point in unsupervised dependency parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifteenth Conference on Computational Natural Language Learning (CoNLL-2011).</booktitle>
<contexts>
<context position="8403" citStr="Spitkovsky et al., 2011" startWordPosition="1337" endWordPosition="1340">ar approach to unsupervised dependency parsing in the recent years. It was introduced by Klein and Manning (2004) and further improved by Smith (2007) and Cohen et al. (2008). Headden III et al. (2009) introduce the Extended Valence Grammar and add lexicalization and smoothing. Blunsom and Cohn (2010) use tree substitution grammars, which allow learning of larger dependency fragments by employing the Pitman-Yor process. Spitkovsky et al. (2010) improve the inference using iterated learning of increasingly longer sentences. Further improvements were achieved by better dealing with punctuation (Spitkovsky et al., 2011b) and new “boundary” models (Spitkovsky et al., 2012). 282 Other approaches to unsupervised dependency parsing were described e.g. in (Søgaard, 2011), (Cohen et al., 2011), and (Bisk and Hockenmaier, 2012). There also exist “less unsupervised” approaches that utilize an external knowledge of the POS tagset. For example, Rasooli and Faili (2012) identify the last verb in the sentence, minimize its probability of reduction and thus push it to the root position. Naseem et al. (2010) make use of manually-specified universal dependency rules such as Verb→Noun, Noun→Adjective. McDonald et al. (2011</context>
<context position="34667" citStr="Spitkovsky et al. (2011" startWordPosition="5961" endWordPosition="5964">2) 67.0 44.8 15.7 Average: 36.4 48.7 64.7 42.2 40.0 Table 2: Attachment scores on CoNLL 2006 and 2007 data. Standard deviations are provided in brackets. DMV model using standard Pdmv stop probability is compared with DMV with Pdmv+est stop , which incorporates STOP estimations based on reducibility principle. The reference DMV uses Ptbp, which are sto computed directly on the treebanks. The results reported in previous works by Spitkovsky et al. (2012), and Mareˇcek and ˇZabokrtsk´y (2012) follows. In future work, we would like to focus on unsupervised parsing without gold POS tags (see e.g. Spitkovsky et al. (2011a) and Christodoulopoulos et al. (2012)). We suppose that many of the current works on unsupervised dependency parsers use gold POS tags only as a simplification of this task, and that the ultimate purpose of this effort is to develop a fully unsupervised induction of linguistic structure from raw texts that would be useful across many languages, domains, and applications. The software which implements the algorithms described in this paper, together with Pest stop estimations computed on Wikipedia texts, can be downloaded at http://ufal.mff.cuni.cz/˜marecek/udp/. Acknowledgments This work has</context>
</contexts>
<marker>Spitkovsky, Alshawi, Jurafsky, 2011</marker>
<rawString>Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Jurafsky. 2011b. Punctuation: Making a point in unsupervised dependency parsing. In Proceedings of the Fifteenth Conference on Computational Natural Language Learning (CoNLL-2011).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valentin I Spitkovsky</author>
<author>Hiyan Alshawi</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Three Dependency-and-Boundary Models for Grammar Induction.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</booktitle>
<contexts>
<context position="1502" citStr="Spitkovsky et al., 2012" startWordPosition="213" endWordPosition="216">shared tasks. 1 Introduction The task of unsupervised dependency parsing (which strongly relates to the grammar induction task) has become popular in the last decade, and its quality has been greatly increasing during this period. The first implementation of Dependency Model with Valence (DMV) (Klein and Manning, 2004) with a simple inside-outside inference algorithm (Baker, 1979) achieved 36% attachment score on English and was the first system outperforming the adjacent-word baseline.1 Current attachment scores of state-of-the-art unsupervised parsers are higher than 50% for many languages (Spitkovsky et al., 2012; Blunsom and Cohn, 2010). This is still far below the supervised approaches, but their indisputable advantage is the fact that no annotated treebanks are needed and the induced structures are not burdened by any linguistic conventions. Moreover, 1The adjacent-word baseline is a dependency tree in which each word is attached to the previous (or the following) word. The attachment score of 35.9% on all the WSJ test sentences was taken from (Blunsom and Cohn, 2010). supervised parsers always only simulate the treebanks they were trained on, whereas unsupervised parsers have an ability to be fitt</context>
<context position="8457" citStr="Spitkovsky et al., 2012" startWordPosition="1345" endWordPosition="1348"> recent years. It was introduced by Klein and Manning (2004) and further improved by Smith (2007) and Cohen et al. (2008). Headden III et al. (2009) introduce the Extended Valence Grammar and add lexicalization and smoothing. Blunsom and Cohn (2010) use tree substitution grammars, which allow learning of larger dependency fragments by employing the Pitman-Yor process. Spitkovsky et al. (2010) improve the inference using iterated learning of increasingly longer sentences. Further improvements were achieved by better dealing with punctuation (Spitkovsky et al., 2011b) and new “boundary” models (Spitkovsky et al., 2012). 282 Other approaches to unsupervised dependency parsing were described e.g. in (Søgaard, 2011), (Cohen et al., 2011), and (Bisk and Hockenmaier, 2012). There also exist “less unsupervised” approaches that utilize an external knowledge of the POS tagset. For example, Rasooli and Faili (2012) identify the last verb in the sentence, minimize its probability of reduction and thus push it to the root position. Naseem et al. (2010) make use of manually-specified universal dependency rules such as Verb→Noun, Noun→Adjective. McDonald et al. (2011) identify the POS tags by a crosslingual transfer. Su</context>
<context position="16134" citStr="Spitkovsky et al. (2012)" startWordPosition="2695" endWordPosition="2698">irection dir at all, whereas Pdmv stop (h, dir, adj = 0, cf) decides whether another child will be generated next to the already generated one. This distinction is of crucial importance for us: although we know how to estimate the STOP probabilities for adj = 1 from large data, we do not know anything about the STOP probabilities for adj = 0. The last factor cf, called fringe, is the POS tag of the previously generated sibling in the current direction dir. If there is no such sibling (in case adj = 1), the head ch is used as the fringe cf. This is a relatively novel idea in DMV, introduced by Spitkovsky et al. (2012). We decided to use the fringe word in our model since it gives slightly better results. We assume that the distributions of Pchoose and Pdmv stop are good if the majority of the probability mass is concentrated on few factors; therefore, we apply a Chinese Restaurant process (CRP) on them. The probability of generating a new child node cd attached to ch in the direction dir given the history (all the nodes we have generated so far) is Sstop(ch, dir) Pest stop(ch, dir) = Sstop(ch, dir) + v � � dirE{l,r} cEC ri dirE{l,r} H cdE deps(dir,h) 284 computed using the following formula: Finally, we ob</context>
<context position="31244" citStr="Spitkovsky et al. (2012)" startWordPosition="5380" endWordPosition="5383">l. Unfortunately, there are also negative examples, such as Hungarian, where the addition of the estimated stop probabilities decreases the attachment score from 60.1% to 34%. This is probably caused by not very good estimates of the right-stop probability (see the last graph in Figure 3). Nevertheless, the estimated stop probabilities increase the average score over all the treebanks by more than 12% and therefore prove its usefulness. In the last two columns of Table 2, we provide results of two other works reported in the last year. The first one (spi12) is the DMV-based grammar inducer by Spitkovsky et al. (2012),4 the second one (mar12) is our previous work (Mareˇcek and ˇZabokrtsk´y, 2012). Comparing with (Spitkovsky et al., 2012), our parser reached better accuracy on 12 out of 20 treebanks. Although this might not seem as a big improvement, if we compare the average scores over the treebanks, our system significantly wins by more than 6%. The second system (mar12) outperforms our parser only on one treebank (on Italian by less than 3%) and its average score over all the treebanks is only 40%, i.e., more than 8% lower than the average score of our parser. To see the theoretical upper bound of our m</context>
<context position="34501" citStr="Spitkovsky et al. (2012)" startWordPosition="5934" endWordPosition="5937"> (±0.2) 35.7 (±0.2) 50.2 35.2 18.1 Spanish 06 38.1 (±5.9) 61.1 (±0.1) 65.6 28.2 51.9 Swedish 06 28.0 (±2.3) 54.5 (±0.4) 61.6 50.7 48.2 Turkish 07 51.6 (±5.5) 56.9 (±0.2) 67.0 44.8 15.7 Average: 36.4 48.7 64.7 42.2 40.0 Table 2: Attachment scores on CoNLL 2006 and 2007 data. Standard deviations are provided in brackets. DMV model using standard Pdmv stop probability is compared with DMV with Pdmv+est stop , which incorporates STOP estimations based on reducibility principle. The reference DMV uses Ptbp, which are sto computed directly on the treebanks. The results reported in previous works by Spitkovsky et al. (2012), and Mareˇcek and ˇZabokrtsk´y (2012) follows. In future work, we would like to focus on unsupervised parsing without gold POS tags (see e.g. Spitkovsky et al. (2011a) and Christodoulopoulos et al. (2012)). We suppose that many of the current works on unsupervised dependency parsers use gold POS tags only as a simplification of this task, and that the ultimate purpose of this effort is to develop a fully unsupervised induction of linguistic structure from raw texts that would be useful across many languages, domains, and applications. The software which implements the algorithms described in </context>
</contexts>
<marker>Spitkovsky, Alshawi, Jurafsky, 2012</marker>
<rawString>Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Jurafsky. 2012. Three Dependency-and-Boundary Models for Grammar Induction. In Proceedings of the 2012 Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL 2012).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>