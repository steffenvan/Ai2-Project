<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000408">
<title confidence="0.9904045">
Improving Distributional Semantic Vectors through Context Selection and
Normalisation
</title>
<author confidence="0.993442">
Tamara Polajnar
</author>
<affiliation confidence="0.9830325">
University of Cambridge
Computer Laboratory
</affiliation>
<email confidence="0.989664">
tp366@cam.ac.uk
</email>
<sectionHeader confidence="0.997289" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999810266666667">
Distributional semantic models (DSMs)
have been effective at representing seman-
tics at the word level, and research has re-
cently moved on to building distributional
representations for larger segments of text.
In this paper, we introduce novel ways of
applying context selection and normalisa-
tion to vary model sparsity and the range
of values of the DSM vectors. We show
how these methods enhance the quality of
the vectors and thus result in improved
low dimensional and composed represen-
tations. We demonstrate these effects on
standard word and phrase datasets, and on
a new definition retrieval task and dataset.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999846636363636">
Distributional semantic models (DSMs) (Turney
and Pantel, 2010; Clarke, 2012) encode word
meaning by counting co-occurrences with other
words within a context window and recording
these counts in a vector. Various IR and NLP
tasks, such as word sense disambiguation, query
expansion, and paraphrasing, take advantage of
DSMs at a word level. More recently, researchers
have been exploring methods that combine word
vectors to represent phrases (Mitchell and Lapata,
2010; Baroni and Zamparelli, 2010) and sentences
(Coecke et al., 2010; Socher et al., 2012). In this
paper, we introduce two techniques that improve
the quality of word vectors and can be easily tuned
to adapt the vectors to particular lexical and com-
positional tasks.
The quality of the word vectors is generally as-
sessed on standard datasets that consist of a list of
word pairs and a corresponding list of gold stan-
dard scores. These scores are gathered through an
annotation task and reflect the similarity between
the words as perceived by human judges (Bruni et
</bodyText>
<author confidence="0.532025">
Stephen Clark
</author>
<affiliation confidence="0.916851">
University of Cambridge
Computer Laboratory
</affiliation>
<email confidence="0.948751">
sc609@cam.ac.uk
</email>
<bodyText confidence="0.997193146341464">
al., 2012). Evaluation is conducted by comparing
the word similarity predicted by the model with
the gold standard using a correlation test such as
Spearman’s ρ.
While words, and perhaps some frequent
shorter phrases, can be represented by distri-
butional vectors learned through co-occurrence
statistics, infrequent phrases and novel construc-
tions are impossible to represent in that way. The
goal of compositional DSMs is to find methods of
combining word vectors, or perhaps higher-order
tensors, into a single vector that represents the
meaning of the whole segment of text. Elemen-
tary approaches to composition employ simple op-
erations, such as addition and elementwise prod-
uct, directly on the word vectors. These have been
shown to be effective for phrase similarity evalua-
tion (Mitchell and Lapata, 2010) and detection of
anomalous phrases (Kochmar and Briscoe, 2013).
The methods that will be introduced in this pa-
per can be applied to co-occurrence vectors to pro-
duce improvements on word similarity and com-
positional tasks with simple operators. We chose
to examine the use of sum, elementwise prod-
uct, and circular convolution (Jones and Mewhort,
2007), because they are often used due to their
simplicity, or as components of more complex
models (Zanzotto and Dell’Arciprete, 2011).
The first method is context selection (CS), in
which the top N highest weighted context words
per vector are selected, and the rest of the values
are discarded (by setting to zero). This technique
is similar to the way that Explicit Semantic Analy-
sis (ESA) (Gabrilovich and Markovitch, 2007) se-
lects the number of topics that represent a word,
and the word filtering approach in Gamallo and
Bordag (2011). It has the advantage of improv-
ing word representations and vector sum represen-
tations (for compositional tasks) while using vec-
tors with fewer non-zero elements. Programming
languages often have efficient strategies for stor-
</bodyText>
<page confidence="0.949249">
230
</page>
<note confidence="0.9927255">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 230–238,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999981723404256">
ing these sparse vectors, leading to lower memory
usage. As an example of the resulting accuracy
improvements, when vectors with up to 10,000
non-zero elements are reduced to a maximum of
N = 240 non-zero elements, the Spearman p im-
proves from 0.61 to 0.76 on a standard word sim-
ilarity task. We also see an improvement when
used in conjunction with further, standard dimen-
sionality reduction techniques: the CS sparse vec-
tors lead to reduced-dimensional representations
that produce higher correlations with human simi-
larity judgements than the original full vectors.
The second method is a weighted l2-
normalisation of the vectors prior to application of
singular value decomposition (SVD) (Deerwester
et al., 1990) or compositional vector operators. It
has the effect of drastically improving SVD with
100 or fewer dimensions. For example, we find
that applying normalisation before SVD improves
correlation from p = 0.48 to p = 0.70 for 20
dimensions, on the word similarity task. This
is an essential finding as many more complex
models of compositional semantics (Coecke et al.,
2010; Baroni and Zamparelli, 2010; Andreas and
Ghahramani, 2013) work with tensor objects and
require good quality low-dimensional represen-
tations of words in order to lower computational
costs. This technique also improves the perfor-
mance of vector addition on texts of any length
and vector elementwise product on shorter texts,
on both the similarity and definitions tasks.
The definition task and dataset are an additional
contribution. We produced a new dataset of words
and their definitions, which is separated into nine
parts, each consisting of definitions of a particular
length. This allows us to examine how composi-
tional operators interact with CS and normalisa-
tion as the number of vector operations increases.
This paper is divided into three main sections.
Section 2 describes the construction of the word
vectors that underlie all of our experiments and the
two methods for adaptation of the vectors to spe-
cific tasks. In Section 3 we assess the effects of
CS and normalisation on standard word similar-
ity datasets. In Section 4 we present the compo-
sitional experiments on phrase data and our new
definitions dataset.
</bodyText>
<sectionHeader confidence="0.947293" genericHeader="method">
2 Word Vector Construction
</sectionHeader>
<bodyText confidence="0.999947697674418">
The distributional hypothesis assumes that words
that occur within similar contexts share similar
meanings; hence semantic vector construction first
requires a defintition of context. Here we use
a window method, where the context is defined
as a particular sequence of words either side of
the target word. The vectors are then populated
through traversal of a large corpus, by recording
the number of times each of the target words co-
occurs with a context word within the window,
which gives the raw target-context co-occurrence
frequency vectors (Freq).
The rest of this section contains a description
of the particular settings used to construct the raw
word vectors and the weighting schemes (tTest,
PPMI) that we considered in our experiments.
This is followed by a detailed description of the
context selection (CS) and normalisation tech-
niques. Finally, dimensionality reduction (SVD) is
proposed as a way of combating sparsity and ran-
dom indexing (RI) as an essential step of encoding
vectors for use with the convolution operator.
Raw Vectors We used a cleaned-up corpus
of 1.7 billion lemmatised tokens (Minnen et
al., 2001) from the October, 2013 snapshot of
Wikipedia, and constructed context vectors by us-
ing sentence boundaries to provide the window.
The set of context words C consisted of the 10,000
most frequent words occurring in this dataset, with
the exception of stopwords from a standard stop-
word list. Therefore, a frequency vector for a tar-
get word wi E W is represented as wi = {fwicj}j,
where cj E C (ICI = 10, 000), W is a set of target
words in a particular evaluation dataset, and fwicj
is the co-occurrence frequency between the target
word, wi and context word, cj.
Vector Weighting We used the tTest and PPMI
weighting schemes, since they both performed
well on the development data. The vectors result-
ing from the application of the weighting schemes
are as follows, where the tTest and PPMI functions
give weighted values for the basis vector corre-
sponding to context word cj for target word wi:
</bodyText>
<equation confidence="0.932138833333333">
tTest♣ ~wi, cjq ✏ p♣wi, cjq ✁p♣wiqp♣cjq (1)
❛p♣wiqp♣cjq
PPMI♣ ~wi, cjq ✏ p♣wi, cjq log
✂p♣wiqp♣cjq
♣wq ✡ (2)
➦ ➦
j fwicj i fwicj
where p♣wiq ✏ ➦ ➦ ➦ ➦
l fwkcl , p♣cjq ✏ l fwkcl , and
k k
p♣wi, cjq ✏ fwicj ➦ ➦ l fwkcl .
k
</equation>
<page confidence="0.995484">
231
</page>
<figure confidence="0.957183454545455">
3.5
3
2.5
2
1.5
1
0.5
0
−0.5
−1
Original Normalised Normalised*10
</figure>
<figureCaption confidence="0.995594">
Figure 1: The range of context weights on tTest
weighted vectors before and after normalisation.
</figureCaption>
<bodyText confidence="0.992392896103896">
to repeated multiplications of small numbers. This
normalisation has no effect on the ordering of con-
text weights or cosine similarity calculations be-
tween single-word vectors. We apply normalisa-
tion prior to dimensionality reduction and RI.
SVD SVD transforms vectors from their target-
context representation into a target-topic space.
The resulting space is dense, in that the vectors
no longer contain any zero elements. If M is a
IwI x ICI matrix whose rows are made of word
vectors wz, then the lower dimensional representa-
tion of those vectors is encoded in the IW I x K
matrix ˆMK = UKSK where SVD(M, K) =
Context Ranking and Selection The weight-
ing schemes change the importance of individ-
ual target-context raw co-occurrence counts by
considering the frequency with which each con-
text word occurs with other target words. This
is similar to term-weighting in IR and many re-
trieval functions are also used as weighting func-
tions in DSMs. In the retrieval-based model ESA
(Gabrilovich and Markovitch, 2007), only the N
highest-weighted contexts are kept as a represen-
tative set of “topics” for a particular target word,
and the rest are set to zero. Here we use a sim-
ilar technique and, for each target word, retain
only the N-highest weighted context words, using
a word-similarity development set to choose the
N that maximises correlation across all words in
that dataset. Throughout the paper, we will refer
to this technique as context selection (CS) and use
N to indicate the maximum number of contexts
per word. Hence all word vectors have at most N
non-zero elements, effectively adjusting the spar-
sity of the vectors, which may have an effect on
the sum and elementwise product operations when
composing vectors.
Normalisation PPMI has only positive values
that span the range [0, oc], while tTest spans
[-1, 1], but generally produces values tightly con-
centrated around zero. We found that these ranges
can produce poor performance due to numerical
problems, so we corrected this through weighted
row normalisation: w� := A w�
⑤⑤ �w⑤⑤2 . With A = 10 this
has the effect of restricting the values to [-10,10]
for tTest and [0,10] for PPMI. Figure 1 shows the
range of values for tTest. In general we use A = 1,
but for some experiments we use A = 10 to push
the highest weights above 1, as a way of combat-
ing the numerical errors that are likely to arise due
UKSKVK (Deerwester et al., 1990). We also
tried non-negative matrix factorisation (NNMF)
(Seung and Lee, 2001), but found that it did not
perform as well as SVD. We used the standard
Matlab implementation of SVD.
Random Indexing There are two ways of creat-
ing RI-based DSMs, the most popular being to ini-
tialise all target word vectors to zero and to gener-
ate a random vector for each context word. Then,
while traversing through the corpus, each time a
target word and a context word co-occur, the con-
text word vector is added to the vector represent-
ing the target word. This method allows the RI
vectors to be created in one step through a single
traversal of the corpus. The other method, follow-
ing Jones and Mewhort (2007), is to create the RI
vectors through matrix multiplication rather than
sequentially. We employ this method and assign
each context word a random vector e�, = {rk}k
where rk are drawn from the normal distribution
JV (0, 1D) and I e�,I = D = 4096. The RI repre-
sentation of a target word RI( wz) = wzR is con-
structed by multiplying the word vector wz, ob-
tained as before, by the ICI x D matrix R where
each column represents the vectors e�,. Weighting
is performed prior to random indexing.
</bodyText>
<sectionHeader confidence="0.965891" genericHeader="method">
3 Word Similarity Experiments
</sectionHeader>
<bodyText confidence="0.999978666666667">
In this section we investigate the effects of context
selection and normalisation on the quality of word
vectors using standard word similarity datasets.
The datasets consist of word pairs and a gold stan-
dard score that indicates the human judgement of
the similarity between the words within each pair.
We calculated the similarity between word vectors
for each pair and compared our results with the
gold standard using Spearman correlation.
</bodyText>
<page confidence="0.98338">
232
</page>
<table confidence="0.9995916">
tTest PPMI Freq
Data Max p Full p Max p Full p Max p Full p
MENdev✿ 0.75 0.73 0.76 0.61 0.66 0.57
MENtest 0.76 0.73 0.76 0.61 0.66 0.56
WS353 0.70 0.63 0.70 0.41 0.57 0.41
</table>
<tableCaption confidence="0.987209">
Table 1: Values N learned on dev (�) also improve
</tableCaption>
<bodyText confidence="0.994742421052632">
performance on the test data. Max p indicates cor-
relation at the values of N that lead to the high-
est Spearman correlation on the development data.
For each weighting scheme these are: 140 (tTest),
240 (PPMI), and 20 (Freq). Full p indicates the
correlation when using full vectors without CS.
The cosine, Jaccard, and Lin similarity mea-
sures (Curran, 2004) were all used to ensure the
results reflect genuine effects of context selection,
and not an artefact of any particular similarity
measure. The similarity measure and value of N
were chosen, given a particular weighting scheme,
to maximise correlation on the development part
of the MEN data (Bruni et al., 2012) (MENdev).
Testing was performed on the remaining section
of MEN and the entire WS353 dataset (Finkelstein
et al., 2002). The MEN dataset consists of 3,000
word pairs rated for similarity, which is divided
into a 2,000-pair development set and a 1,000-pair
test set. WS353 consists only of 353 pairs, but has
been consistently used as a benchmark word simi-
larity dataset throughout the past decade.
Results Figure 2 shows how correlation varies
with N for the MEN development data. The
peak performance for tTest is achieved when using
around 140 top-ranked contexts per word, while
for PPMI it is at N = 240, and for Freq N = 20.
The dramatic drop in performance is demonstrated
when using all three similarity measures, although
Jaccard seems particularly sensitive to the nega-
tive tTest weights that are introduced when lower-
ranked contexts are added to the vectors. The re-
maining experiments only consider cosine similar-
ity. We also find that context selection improves
correlation for tTest, PPMI, and the unweighted
Freq vectors on the test data (Table 1). Moreover,
the lower the correlation from the full vectors, the
larger the improvement when using CS.
</bodyText>
<subsectionHeader confidence="0.999261">
3.1 Dimensionality Reduction
</subsectionHeader>
<bodyText confidence="0.9901965">
Figure 3 shows the effects of dimensionality re-
duction described in the following experiments.
</bodyText>
<figure confidence="0.9494355">
0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000
Maximum nonzero elements per vector
</figure>
<figureCaption confidence="0.79529425">
Figure 2: Correlation decreases as more lower-
ranked context words are introduced (MENdev),
with cosine (top), Lin (bottom left), and Jaccard
(bottom right) simialrity measures.
</figureCaption>
<subsectionHeader confidence="0.593737">
3.1.1 SVD and CS
</subsectionHeader>
<bodyText confidence="0.997923217391305">
To check whether CS improves the correlation
through increased sparsity or whether it improves
the contextual representation of the words, we in-
vestigated the behaviour of SVD on three differ-
ent levels of vector sparsity. To construct the most
sparse vectors, we chose the best performing N
for each weighting scheme (from Table 1). Thus
sparse tTest vectors had 140
10000 = 0.0140, or 1.4%,
non-zero elements. We also chose a mid-range
of N = 3300 for up to 33% of non-zero ele-
ments per vector, and finally the full vectors with
N = 10000.
Results In general the CS-tuned vectors lead
to better lower-dimensional representations. The
mid-range contexts in the tTest weighting scheme
seem to hold information that hinders SVD, while
the lowest-ranked negative weights appear to help
(when the mid-range contexts are present as well).
For the PPMI weighting, fewer contexts consis-
tently lead to better representations, while the un-
weighted vectors seem to mainly hold information
in the top 20 most frequent contexts for each word.
</bodyText>
<subsubsectionHeader confidence="0.409163">
3.1.2 SVD, CS, and Normalisation
</subsubsectionHeader>
<bodyText confidence="0.9967015">
We also consider the combination of normalisation
and context selection followed by SVD.
</bodyText>
<figure confidence="0.995399192307692">
0.76
0.74
0.72
0.7
0.68
0.66
0.64
0.62
0.6
0.58
ttest
ppmi
freq
max
max
max
0.8
0.8
0.7
0.6
0.7
0.6
ttest
ppmi
freq
max
max
max
Spearman
0.5
0.4
0.3
0.3
0.2
ttest
ppmi
freq
max
max
max
0.2
0.1
0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000
Maximum nonzero elements per vector
0
01000 2000 3000 4000 5000 6000 7000 8000 9000 10000
Maximum nonzero elements per vector
0.5
0.4
Spearman
Spearman
233
tTest PPMI Freq
Spearman
Spearman
0.8
0.7
0.6
0.5
0.4
0.30 100 200 300 400 500 600 700 800
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.10 100 200 300 400 500 600 700 800
140
N=3300
N=10000
norm 140
norm N=3300
norm N=10000
all 140
all N=3300
all N=10000
240
N=3300
N=10000
norm 240
norm N=3300
norm N=10000
all 240
all N=3300
all N=10000
0.7
0.6
0.20 100 200 300 400 500 600 700 800
20
N=3300
N=10000
norm 20
norm N=3300
norm N=10000
all 20
all N=3300
all N=10000
0.5
Spearman
0.4
0.3
Number of dimensions (Iq Number of dimensions (Kq Number of dimensions (I)
</figure>
<figureCaption confidence="0.827767">
Figure 3: Vectors tuned for sparseness (blue) consistently produce equal or better dimensionality reduc-
tions (results on MENdev). The solid lines show improvement in lower dimensional representations of
SVD when dimensionality reduction is applied after normalisation.
</figureCaption>
<bodyText confidence="0.999224388888889">
Results Normalisation leads to more stable SVD
representations, with a large improvement for
small numbers of dimensions (K) as demonstrated
by the solid lines in Figure 3. At K = 20 the
Spearman correlation increases from 0.61 to 0.71.
In addition, for tTest there is an improvement in
the mid-range vectors, and a knock-on effect for
the full vectors. As the tTest values effectively
range from —0.1 to 0.1, the mid-range values are
very small numbers closely grouped around zero.
Normalisation spreads and increases these num-
bers, perhaps making them more relevant to the
SVD algorithm. The effect is also visible for
PPMI weighting where at K = 20 the correlation
increases from 0.48 to 0.70. For PPMI and Freq
we also see that, for the full and mid-range vec-
tors, the SVD representations have slightly higher
correlations than the unreduced vectors.
</bodyText>
<subsectionHeader confidence="0.998264">
3.2 Random Indexing
</subsectionHeader>
<bodyText confidence="0.999983">
We use random indexing primarily to produce a
vector representation for convolution (Section 4).
While this produces a lower-dimensional repre-
sentation, it may not use less memory since the re-
sulting vectors, although smaller, are fully dense.
In summary, the RI encoded vectors with di-
mensions of D = 4096 lead to only slightly re-
duced correlation values compared to their unen-
coded counterparts. We find that for tTest we
get similar performance with or without CS at
any level, while for PPMI CS helps especially for
D &gt;, 512. On Freq we find that CS with N = 60
leads to higher correlation, but mid-range and full
vectors have equivalent performance. For Freq,
the correlation is equivalent to full vectors from
D = 128, while for the weighted vectors 512 di-
mensions appear to be sufficient. Unlike for SVD,
normalisation slightly reduces the performance for
mid-range dimensions.
</bodyText>
<sectionHeader confidence="0.995449" genericHeader="method">
4 Compositional Experiments
</sectionHeader>
<bodyText confidence="0.983508486486486">
We examine the performance of vectors aug-
mented by CS and normalisation in two compo-
sitional tasks. The first is an extension of the word
similarity task to phrase pairs, using the dataset
of Mitchell and Lapata (2010). Each entry in the
dataset consists of two phrases, each consisting of
two words (in various syntactic relations, such as
verb-object and adjective noun), and a gold stan-
dard score. We combine the two word vectors into
a single phrase vector using various operators de-
scribed below. We then calculate the similarity
between the phrase vectors using cosine and com-
pare the resulting scores against the gold standard
using Spearman correlation. The second task is
our new definitions task where, again, word vec-
tors from each definition are composed to form a
single vector, which can then be compared for sim-
ilarity with the target term.
We use PPMI- and tTest-weighted vectors at
three CS cutoff points: the best chosen N from
Section 3, the top third of the ranked contexts at
N = 3300, and the full vectors without CS at
N = 10000. This gives us a range of values to
examine, without directly tuning on this dataset.
For dimensionality reduction we consider vectors
reduced with SVD to 100 and 700 dimensions. In
some cases we exclude the results for SVD700 be-
cause they are very close to the scores for unre-
duced vectors. We experiment with 3 values of D
from 1512, 1024, 40961 for the RI vectors.
Operators To combine distributional vectors
into a single-vector sentence representation, we
use a representative set of methods from Mitchell
and Lapata (2010). In particular, we use vector
addition, elementwise (Hadamard) product, Kro-
necker product, and circular convolution (Plate,
1991; Jones and Mewhort, 2007), which are de-
</bodyText>
<page confidence="0.994883">
234
</page>
<bodyText confidence="0.990105">
fined as follows for two word vectors ~x, ~y:
</bodyText>
<equation confidence="0.9518134">
Sum x~+ y~✏ t~xi + ~yi✉i
Prod x~❞ y~ ✏ t~xi ☎ ~yi✉i
Kron x~❜ y~✏ t~xi ☎ ~yj✉ij
Conv x~ ❣ y~ ✏ ✦➦nj✏0♣~xqj%n ☎ ♣~yq♣i✁j)%n✮
i
</equation>
<bodyText confidence="0.9999812">
Repeated application of the Sum operation adds
contexts for each of the words that occur in a
phrase, which maintains (and mixes) any noisy
parts of the component word vectors. Our inten-
tion was that use of the CS vectors would lead
to less noisy word vectors and hence less noisy
phrase and sentence vectors. The Prod operator,
on the other hand, provides a phrase or sentence
representation consisting only of the contexts that
are common to all of the words in the sentence
(since zeros in any of the word vectors lead to
zeros in the same position in the sentence vec-
tor). This effect is particularly problematic for rare
words which may have sparse vectors, leading to
a sparse vector for the sentence.1 We address the
sparsity problem through the use of dimensional-
ity reduction, which produces more dense vectors.
Kron, the Kronecker (or tensor) product of two
vectors, produces a matrix (second order tensor)
whose diagonal matches the result of the Prod
operation, but whose off-diagonal entries are all
the other products of elements of the two vectors.
We only apply Kron to SVD-reduced vectors, and
to compare two matrices we turn them into vec-
tors by concatenating matrix rows, and use co-
sine similarity on the resulting vectors. While in
the more complex, type-driven methods (Baroni
and Zamparelli, 2010; Coecke et al., 2010) ten-
sors represent functions, and off-diagonal entries
have a particular transformational interpretation as
part of a linear map, the significance of the off-
diagonal elements is difficult to interpret in our
setting, apart from their role as encoders of the or-
der of operands. We only examine Kron as the un-
encoded version of the Conv operator to see how
the performance is affected by the random index-
ing and the modular summation by which Conv
differs from Kron.2 We cannot use Kron for com-
bining more than two words as the size of the re-
sulting tensor grows exponentially with the num-
</bodyText>
<footnote confidence="0.998177">
1Sparsity is a problem that may be addressable through
smoothing (Zhai and Lafferty, 2001), although we do not in-
vestigate that avenue in this paper.
2Conv also differs from Kron in that it is commutative,
unless one of the operands is permuted. In this paper we do
not permute the operands.
</footnote>
<table confidence="0.9998759">
Oper N=140 N=3300 N=10000
sum ttest 0.40 (0.41) 0.40 (0.40) 0.40 (0.40)
SVD100 0.37 (0.42) 0.35 (0.41) 0.37 (0.40)
Prod ttest 0.32 (0.32) 0.40 (0.40) 0.32 (0.32)
SVD100 0.25 (0.23) 0.23 (0.23) 0.21 (0.23)
kron SVD100 0.31 (0.34) 0.34 (0.38) 0.29 (0.32)
SVD700 0.39 (0.39) 0.37 (0.37) 0.30 (0.30)
Conv RI512 0.10 (0.12) 0.26 (0.21) 0.25 (0.25)
RI1024 0.22 (0.15) 0.29 (0.27) 0.25 (0.26)
RI4096 0.16 (0.19) 0.33 (0.34) 0.28 (0.30)
</table>
<tableCaption confidence="0.987666">
Table 2: Behaviour of vector operators with tTest
vectors on ML2010 (Spearman correlation). Val-
ues for normalised vectors in parentheses.
</tableCaption>
<table confidence="0.9999573">
Oper N=240 N=3300 N=10000
sum ppmi 0.40 (0.39) 0.40 (0.39) 0.29 (0.29)
SVD100 0.40 (0.40) 0.38 (0.40) 0.29 (0.30)
Prod ppmi 0.28 (0.28) 0.40 (0.40) 0.30 (0.30)
SVD100 0.23 (0.17) 0.18 (0.22) 0.14 (0.12)
kron SVD100 0.37 (0.30) 0.36 (0.38) 0.27 (0.27)
SVD700 0.38 (0.37) 0.37 (0.37) 0.26 (0.26)
RI512 0.09 (0.09) 0.27 (0.30) 0.25 (0.24)
Conv RI1024 0.08 (0.14) 0.33 (0.37) 0.25 (0.27)
RI4096 0.18 (0.19) 0.37 (0.38) 0.27 (0.27)
</table>
<tableCaption confidence="0.912839333333333">
Table 3: Behaviour of vector operators with PPMI
vectors on ML2010 (Spearman correlation). Val-
ues for normalised vectors in parentheses.
</tableCaption>
<bodyText confidence="0.998642333333333">
ber of vector operations, but we can use Conv as
an encoded alternative as it results in a vector of
the same dimension as the two operands.
</bodyText>
<subsectionHeader confidence="0.99991">
4.1 Phrase Similarity
</subsectionHeader>
<bodyText confidence="0.996524125">
To test how CS, normalisation, and dimensional-
ity reduction affect simple compositional vector
operations we use the test portion of the phrasal
similarity dataset from Mitchell and Lapata (2010)
(ML2010). This dataset consists of pairs of two-
word phrases and a human similarity judgement
on the scale of 1-7. There are three types of
phrases: noun-noun, adjective-noun, and verb-
object. In the original paper, and some subse-
quent works, these were treated as three different
datasets; however, here we combine the datasets
into one single phrase pair dataset. This allows us
to summarise the effects of different types of vec-
tors on phrasal composition in general.
Results Our results (Tables 2 and 3) are compa-
rable to those in Mitchell and Lapata (2010) av-
eraged across the phrase-types (ρ ✏ 0.44), but
are achieved with much smaller vectors. We find
that with normalisation, and the optimal choice
of N, there is little difference between Prod and
Sum. Sum and Kron benefit from normalisa-
tion, especially in combination with SVD, but for
Prod it either makes no difference or reduces per-
formance. Product-based methods (Prod, Kron,
</bodyText>
<page confidence="0.995262">
235
</page>
<bodyText confidence="0.999893333333333">
Conv) have a preference for context selection that
includes the mid-rank contexts (N ✏ 3300), but
not the full vector (N ✏ 10000). On tTest vec-
tors Sum is relatively stable across different CS
and SVD settings, but with PPMI weighting, there
is a preference for lower N. SVD reduces perfor-
mance for Prod, but not for Kron. Finally, Conv
gets higher correlation with higher-dimensional RI
vectors and with PPMI weights.
</bodyText>
<subsectionHeader confidence="0.998479">
4.2 Definition Retrieval
</subsectionHeader>
<bodyText confidence="0.999935575">
In this task, which is formulated as a retrieval task,
we investigate the behaviour of different vector
operators as multiple operations are chained to-
gether. We first encode each definition into a sin-
gle vector through repeated application of one of
the operators on the distributional vectors of the
content words in the definition. Then, for each
head (defined) word, we rank all the different defi-
nition vectors in decreasing order according to in-
ner product (unnormalised cosine) similarity with
the head word’s distributional vector.
Performance is measured using precision and
Mean Reciprocal Rank (MRR). If the correct defi-
nition is ranked first, the precision (P@1) is 1, oth-
erwise 0. Since there is only one definition per
head word, the reciprocal rank (RR) is the inverse
of the rank of the correct definition. So if the cor-
rect definition is ranked fourth, for example, then
RR is 4. MRR is the average of the RR across all
head words.
The difficulty of the task depends on how many
words there are in the dataset and how similar their
definitions are. In addition, if a head word oc-
curs in the definition of another word in the same
dataset, it may cause the incorrect definition to be
ranked higher than the correct one. These prob-
lems are more likely to occur with higher fre-
quency words and in a larger dataset. In order
to counter these effects, we average our results
over ten repeated random samplings of 100 word-
definition pairs. The sampling also gives us a ran-
dom baseline for P@1 of 0.0130 ✟ 0.0106 and for
MRR 0.0576 ✟ 0.0170, which can be interpreted
as there is a chance of slightly more than 1 in 100
of ranking the correct definition first, and on aver-
age the correct definition is ranked around the 20
mark.
For this task all experiments were performed
using the tTest-weighted vectors. When applying
normalisation we use A ✏ 1 (Norm) and A ✏ 10
</bodyText>
<tableCaption confidence="0.935174">
Table 4: Number of definitions per dataset.
</tableCaption>
<bodyText confidence="0.996744523809524">
(Norm10). In addition, we examine the effect of
continually applying Norm after every operation
(CNorm).
Dataset We developed a new dataset (DD) con-
sisting of 3,386 definitions from the Wiktionary
BNC spoken-word frequency list.3 Most of the
words have several definitions, but we considered
only the first definition with at least two non-
stopwords. The word-definition pairs were di-
vided into nine separate datasets according to the
number of non-stopwords in the definition. For ex-
ample, all of the definitions that have five content
words are in DD5. The exception is DD10, which
contains all the definitions of ten or more words.
Table 4 shows the number of definitions in each
dataset.
Results Figure 4 shows how the MRR varies
with different DD datasets for Sum, Prod, and
Conv. The CS, SVD, and RI settings for each op-
erator correspond to the best average settings from
Table 5. In some cases other settings had simi-
lar performance, but we chose these for illustrative
purposes. We can see that all operators have rel-
atively higher MRR on smaller datasets (DD6-9).
Compensating for that effect, we can hypothesise
that Sum has a steady performance across differ-
ent definition sizes, while the performance of both
Prod and Conv declines as the number of oper-
ations increases. Normalisation helps with Sum
throughout, with little difference in performance
between Norm and Norm10, but with a slight de-
crease when CNorm is used. On the other hand,
only CNorm improves the ranking of Prod-based
vectors. Normalisation makes no difference for RI
vectors combined with convolution and the results
in Table 5 show that, on average, Conv performs
worse than the random baseline.
In Figure 5 we can see that, although dimen-
sionality reduction leads to lower MRR, for Sum,
normalisation prior to SVD counteracts this effect,
while, for Prod, dimensionality reduction, in gen-
eral, reduces the performance.
</bodyText>
<footnote confidence="0.797547">
3http://simple.wiktionary.org/wiki/Wiktionary:BNC spoken freq
</footnote>
<figure confidence="0.818323">
DD2 DD3 DD4 DD5 DD6 DD7 DD8 DD9
DD10
346 547 594 537 409 300 216 150
287
</figure>
<page confidence="0.883104">
236
</page>
<figureCaption confidence="0.9923535">
Figure 4: Per-dataset breakdown of best nor-
malised and unnormalised vectors for each vector
operator. Stars indicate the dataset size from Ta-
ble 4 divided by 1000.
</figureCaption>
<table confidence="0.999362">
Sum Prod Conv
Norm No Yes No CN No Yes
CS (N) 140 140 3300 10000 140 3300
SVD(K)/RI(D) 700 700 None None 2048 512
mean P@1 0.18 0.23 0.01 0.11 0.00 0.00
mean MRR 0.28 0.35 0.06 0.17 0.02 0.02
</table>
<tableCaption confidence="0.970671">
Table 5: Best settings for operators calculated
</tableCaption>
<bodyText confidence="0.973011">
from the highest average MRR across all the
datasets, with and without normalisation. The
results for vectors with no normalisation or CS
are: Sum - P@1=0.1567, MRR=0.2624; Prod -
P@1=0.0147, MRR=0.0542; Conv P@1=0.0027,
MRR=0.0192.
</bodyText>
<sectionHeader confidence="0.999826" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999577588235294">
In this paper we introduced context selection and
normalisation as techniques for improving the se-
mantic vector space representations of words. We
found that, although our untuned vectors perform
better on WS353 data (p = 0.63) than vectors used
by Mitchell and Lapata (2010) (p = 0.42), our
best phrase composition model (Sum, p = 0.40)
produces a lower performance than an estimate of
their best model (Prod, p = 0.44).4 This indicates
that better performance on word-similarity data
does not directly translate into better performance
on compositional tasks; however, CS and normal-
isation are both effective in increasing the qual-
ity of the composed representation (p = 0.42).
Since CS and normalisation are computationally
inexpensive, they are an excellent way to improve
model quality compared to the alternative, which
</bodyText>
<footnote confidence="0.9993285">
4The estimate is computed as an average across the three
phrase-type results.
</footnote>
<figureCaption confidence="0.98350625">
Figure 5: Per-dataset breakdown of best nor-
malised and unnormalised SVD vectors for Sum
and Prod. For both operators the best CS and SVD
settings for normalised vectors were N = 140,
</figureCaption>
<equation confidence="0.8135875">
K = 700, and for unnormalised were N = 10000,
K = 700.
</equation>
<bodyText confidence="0.999971181818182">
is building several models with various context
types, in order to find which one suits the data best.
Furthermore, we show that, as the number of
vector operations increases, Sum is the most sta-
ble operator and that it benefits from sparser rep-
resentations (low N) and normalisation. Employ-
ing both of these methods, we are able to build an
SVD-based representation that performs as well
as full-dimensional vectors which, together with
Sum, give the best results on both phrase and def-
inition tasks. In fact, normalisation and CS both
improve the SVD representations of the vectors
across different weighting schemes. This is a key
result, as many of the more complex composi-
tional methods require low dimensional represen-
tations for computational reasons.
Future work will include application of CS
and normalised lower-dimensional vectors to more
complex compositional methods, and investiga-
tions into whether these strategies apply to other
context types and other dimensionality reduction
methods such as LDA (Blei et al., 2003).
</bodyText>
<sectionHeader confidence="0.997753" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999462571428572">
Tamara Polajnar is supported by ERC Starting
Grant DisCoTex (306920). Stephen Clark is sup-
ported by ERC Starting Grant DisCoTex (306920)
and EPSRC grant EP/I037512/1. We would like
to thank Laura Rimell for helpful discussion, and
Laura and the anonymous reviewers for helpful
comments on the paper.
</bodyText>
<figure confidence="0.999366838709677">
DD2 DD3 DD4 DD5 DD6 DD7 DDB DD9 DD10
MRR 0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
Sum
Sum+Norm
Prod
Prod+Norm
Conv
Conv+Norm
DDsize/1000
DD2 DD3 DD4 DD5 DD6 DD7 DD8 DD9 DD10
MRR 0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
Sum Best
Sum+SVD
Sum+SVD+Norm10
Prod Best
Prod+SVD
Prod+SVD+CNorm
</figure>
<page confidence="0.980967">
237
</page>
<sectionHeader confidence="0.997304" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999679444444444">
Jacob Andreas and Zoubin Ghahramani. 2013. A gen-
erative model of vector space semantics. In Pro-
ceedings of the ACL 2013 Workshop on Continu-
ous Vector Space Models and their Compositional-
ity, Sofia, Bulgaria.
M. Baroni and R. Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-10), Cambridge, MA.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. J. Mach. Learn.
Res., 3:993–1022.
Elia Bruni, Gemma Boleda, Marco Baroni, and
Nam Khanh Tran. 2012. Distributional semantics
in technicolor. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 136–145,
Jeju Island, Korea, July. Association for Computa-
tional Linguistics.
Daoud Clarke. 2012. A context-theoretic frame-
work for compositionality in distributional seman-
tics. Comput. Linguist., 38(1):41–71, March.
B. Coecke, M. Sadrzadeh, and S. Clark. 2010. Math-
ematical foundations for a compositional distribu-
tional model of meaning. In J. van Bentham,
M. Moortgat, and W. Buszkowski, editors, Linguis-
tic Analysis (Lambek Festschrift), volume 36, pages
345–384.
James R. Curran. 2004. From Distributional to Seman-
tic Similarity. Ph.D. thesis, University of Edinburgh.
Scott Deerwester, Susan T. Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard Harshman.
1990. Indexing by latent semantic analysis. Journal
of the Society for Information Science, 41(6):391–
407.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2002. Placing search in context: The
concept revisited. ACM Transactions on Informa-
tion Systems, 20:116–131.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using Wikipedia-
based explicit semantic analysis. In Proceedings of
the 20th international joint conference on Artifical
intelligence, IJCAI’07, pages 1606–1611, San Fran-
cisco, CA, USA. Morgan Kaufmann Publishers Inc.
Pablo Gamallo and Stefan Bordag. 2011. Is singu-
lar value decomposition useful for word similarity
extraction? Language Resources and Evaluation,
45(2):95–119.
Michael N. Jones and Douglas J. K. Mewhort. 2007.
Representing word meaning and order information
in a composite holographic lexicon. Psychological
Review, 114:1–37.
Ekaterina Kochmar and Ted Briscoe. 2013. Capturing
anomalies in the choice of content words in compo-
sitional distributional semantic space. In Proceed-
ings of the Recent Advances in Natural Language
Processing (RANLP-2013), Hissar, Bulgaria.
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of English. Nat-
ural Language Engineering, 7(3):207–223.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388–1429.
T. A. Plate. 1991. Holographic reduced Repre-
sentations: Convolution algebra for compositional
distributed representations. In J. Mylopoulos and
R. Reiter, editors, Proceedings of the 12th Inter-
national Joint Conference on Artificial Intelligence,
Sydney, Australia, August 1991, pages 30–35, San
Mateo, CA. Morgan Kauffman.
D Seung and L Lee. 2001. Algorithms for non-
negative matrix factorization. Advances in neural
information processing systems, 13:556–562.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic Composi-
tionality Through Recursive Matrix-Vector Spaces.
In Proceedings of the 2012 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), Jeju Island, Korea.
Peter Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141–188.
Fabio Massimo Zanzotto and Lorenzo Dell’Arciprete.
2011. Distributed structures and distributional
meaning. In Proceedings of the Workshop on Dis-
tributional Semantics and Compositionality, DiSCo-
11, pages 10–15, Portland, Oregon. Association for
Computational Linguistics.
Chengxiang Zhai and John Lafferty. 2001. A study
of smoothing methods for language models applied
to ad hoc information retrieval. In Proceedings of
the 24th annual international ACM SIGIR confer-
ence on Research and development in information
retrieval, SIGIR ’01, pages 334–342, New York,
NY, USA. ACM.
</reference>
<page confidence="0.997059">
238
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.871616">
<title confidence="0.998731">Improving Distributional Semantic Vectors through Context Selection and Normalisation</title>
<author confidence="0.992289">Tamara</author>
<affiliation confidence="0.9633125">University of Computer</affiliation>
<email confidence="0.951579">tp366@cam.ac.uk</email>
<abstract confidence="0.9985789375">Distributional semantic models (DSMs) have been effective at representing semantics at the word level, and research has recently moved on to building distributional representations for larger segments of text. In this paper, we introduce novel ways of applying context selection and normalisation to vary model sparsity and the range of values of the DSM vectors. We show how these methods enhance the quality of the vectors and thus result in improved low dimensional and composed representations. We demonstrate these effects on standard word and phrase datasets, and on a new definition retrieval task and dataset.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jacob Andreas</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>A generative model of vector space semantics.</title>
<date>2013</date>
<booktitle>In Proceedings of the ACL 2013 Workshop on Continuous Vector Space Models and their Compositionality,</booktitle>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="5202" citStr="Andreas and Ghahramani, 2013" startWordPosition="805" endWordPosition="808">rity judgements than the original full vectors. The second method is a weighted l2- normalisation of the vectors prior to application of singular value decomposition (SVD) (Deerwester et al., 1990) or compositional vector operators. It has the effect of drastically improving SVD with 100 or fewer dimensions. For example, we find that applying normalisation before SVD improves correlation from p = 0.48 to p = 0.70 for 20 dimensions, on the word similarity task. This is an essential finding as many more complex models of compositional semantics (Coecke et al., 2010; Baroni and Zamparelli, 2010; Andreas and Ghahramani, 2013) work with tensor objects and require good quality low-dimensional representations of words in order to lower computational costs. This technique also improves the performance of vector addition on texts of any length and vector elementwise product on shorter texts, on both the similarity and definitions tasks. The definition task and dataset are an additional contribution. We produced a new dataset of words and their definitions, which is separated into nine parts, each consisting of definitions of a particular length. This allows us to examine how compositional operators interact with CS and</context>
</contexts>
<marker>Andreas, Ghahramani, 2013</marker>
<rawString>Jacob Andreas and Zoubin Ghahramani. 2013. A generative model of vector space semantics. In Proceedings of the ACL 2013 Workshop on Continuous Vector Space Models and their Compositionality, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Baroni</author>
<author>R Zamparelli</author>
</authors>
<title>Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space.</title>
<date>2010</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing (EMNLP-10),</booktitle>
<location>Cambridge, MA.</location>
<contexts>
<context position="1304" citStr="Baroni and Zamparelli, 2010" startWordPosition="189" endWordPosition="192">onstrate these effects on standard word and phrase datasets, and on a new definition retrieval task and dataset. 1 Introduction Distributional semantic models (DSMs) (Turney and Pantel, 2010; Clarke, 2012) encode word meaning by counting co-occurrences with other words within a context window and recording these counts in a vector. Various IR and NLP tasks, such as word sense disambiguation, query expansion, and paraphrasing, take advantage of DSMs at a word level. More recently, researchers have been exploring methods that combine word vectors to represent phrases (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010) and sentences (Coecke et al., 2010; Socher et al., 2012). In this paper, we introduce two techniques that improve the quality of word vectors and can be easily tuned to adapt the vectors to particular lexical and compositional tasks. The quality of the word vectors is generally assessed on standard datasets that consist of a list of word pairs and a corresponding list of gold standard scores. These scores are gathered through an annotation task and reflect the similarity between the words as perceived by human judges (Bruni et Stephen Clark University of Cambridge Computer Laboratory sc609@ca</context>
<context position="5171" citStr="Baroni and Zamparelli, 2010" startWordPosition="801" endWordPosition="804">orrelations with human similarity judgements than the original full vectors. The second method is a weighted l2- normalisation of the vectors prior to application of singular value decomposition (SVD) (Deerwester et al., 1990) or compositional vector operators. It has the effect of drastically improving SVD with 100 or fewer dimensions. For example, we find that applying normalisation before SVD improves correlation from p = 0.48 to p = 0.70 for 20 dimensions, on the word similarity task. This is an essential finding as many more complex models of compositional semantics (Coecke et al., 2010; Baroni and Zamparelli, 2010; Andreas and Ghahramani, 2013) work with tensor objects and require good quality low-dimensional representations of words in order to lower computational costs. This technique also improves the performance of vector addition on texts of any length and vector elementwise product on shorter texts, on both the similarity and definitions tasks. The definition task and dataset are an additional contribution. We produced a new dataset of words and their definitions, which is separated into nine parts, each consisting of definitions of a particular length. This allows us to examine how compositional</context>
<context position="22650" citStr="Baroni and Zamparelli, 2010" startWordPosition="3781" endWordPosition="3784">or the sentence.1 We address the sparsity problem through the use of dimensionality reduction, which produces more dense vectors. Kron, the Kronecker (or tensor) product of two vectors, produces a matrix (second order tensor) whose diagonal matches the result of the Prod operation, but whose off-diagonal entries are all the other products of elements of the two vectors. We only apply Kron to SVD-reduced vectors, and to compare two matrices we turn them into vectors by concatenating matrix rows, and use cosine similarity on the resulting vectors. While in the more complex, type-driven methods (Baroni and Zamparelli, 2010; Coecke et al., 2010) tensors represent functions, and off-diagonal entries have a particular transformational interpretation as part of a linear map, the significance of the offdiagonal elements is difficult to interpret in our setting, apart from their role as encoders of the order of operands. We only examine Kron as the unencoded version of the Conv operator to see how the performance is affected by the random indexing and the modular summation by which Conv differs from Kron.2 We cannot use Kron for combining more than two words as the size of the resulting tensor grows exponentially wit</context>
</contexts>
<marker>Baroni, Zamparelli, 2010</marker>
<rawString>M. Baroni and R. Zamparelli. 2010. Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space. In Conference on Empirical Methods in Natural Language Processing (EMNLP-10), Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>3--993</pages>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. J. Mach. Learn. Res., 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elia Bruni</author>
<author>Gemma Boleda</author>
<author>Marco Baroni</author>
<author>Nam Khanh Tran</author>
</authors>
<title>Distributional semantics in technicolor.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>136--145</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="13679" citStr="Bruni et al., 2012" startWordPosition="2266" endWordPosition="2269">lation at the values of N that lead to the highest Spearman correlation on the development data. For each weighting scheme these are: 140 (tTest), 240 (PPMI), and 20 (Freq). Full p indicates the correlation when using full vectors without CS. The cosine, Jaccard, and Lin similarity measures (Curran, 2004) were all used to ensure the results reflect genuine effects of context selection, and not an artefact of any particular similarity measure. The similarity measure and value of N were chosen, given a particular weighting scheme, to maximise correlation on the development part of the MEN data (Bruni et al., 2012) (MENdev). Testing was performed on the remaining section of MEN and the entire WS353 dataset (Finkelstein et al., 2002). The MEN dataset consists of 3,000 word pairs rated for similarity, which is divided into a 2,000-pair development set and a 1,000-pair test set. WS353 consists only of 353 pairs, but has been consistently used as a benchmark word similarity dataset throughout the past decade. Results Figure 2 shows how correlation varies with N for the MEN development data. The peak performance for tTest is achieved when using around 140 top-ranked contexts per word, while for PPMI it is at</context>
</contexts>
<marker>Bruni, Boleda, Baroni, Tran, 2012</marker>
<rawString>Elia Bruni, Gemma Boleda, Marco Baroni, and Nam Khanh Tran. 2012. Distributional semantics in technicolor. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 136–145, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daoud Clarke</author>
</authors>
<title>A context-theoretic framework for compositionality in distributional semantics.</title>
<date>2012</date>
<journal>Comput. Linguist.,</journal>
<volume>38</volume>
<issue>1</issue>
<contexts>
<context position="881" citStr="Clarke, 2012" startWordPosition="127" endWordPosition="128">evel, and research has recently moved on to building distributional representations for larger segments of text. In this paper, we introduce novel ways of applying context selection and normalisation to vary model sparsity and the range of values of the DSM vectors. We show how these methods enhance the quality of the vectors and thus result in improved low dimensional and composed representations. We demonstrate these effects on standard word and phrase datasets, and on a new definition retrieval task and dataset. 1 Introduction Distributional semantic models (DSMs) (Turney and Pantel, 2010; Clarke, 2012) encode word meaning by counting co-occurrences with other words within a context window and recording these counts in a vector. Various IR and NLP tasks, such as word sense disambiguation, query expansion, and paraphrasing, take advantage of DSMs at a word level. More recently, researchers have been exploring methods that combine word vectors to represent phrases (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010) and sentences (Coecke et al., 2010; Socher et al., 2012). In this paper, we introduce two techniques that improve the quality of word vectors and can be easily tuned to adapt t</context>
</contexts>
<marker>Clarke, 2012</marker>
<rawString>Daoud Clarke. 2012. A context-theoretic framework for compositionality in distributional semantics. Comput. Linguist., 38(1):41–71, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Coecke</author>
<author>M Sadrzadeh</author>
<author>S Clark</author>
</authors>
<title>Mathematical foundations for a compositional distributional model of meaning.</title>
<date>2010</date>
<booktitle>Linguistic Analysis (Lambek Festschrift),</booktitle>
<volume>36</volume>
<pages>345--384</pages>
<editor>In J. van Bentham, M. Moortgat, and W. Buszkowski, editors,</editor>
<contexts>
<context position="1339" citStr="Coecke et al., 2010" startWordPosition="195" endWordPosition="198">phrase datasets, and on a new definition retrieval task and dataset. 1 Introduction Distributional semantic models (DSMs) (Turney and Pantel, 2010; Clarke, 2012) encode word meaning by counting co-occurrences with other words within a context window and recording these counts in a vector. Various IR and NLP tasks, such as word sense disambiguation, query expansion, and paraphrasing, take advantage of DSMs at a word level. More recently, researchers have been exploring methods that combine word vectors to represent phrases (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010) and sentences (Coecke et al., 2010; Socher et al., 2012). In this paper, we introduce two techniques that improve the quality of word vectors and can be easily tuned to adapt the vectors to particular lexical and compositional tasks. The quality of the word vectors is generally assessed on standard datasets that consist of a list of word pairs and a corresponding list of gold standard scores. These scores are gathered through an annotation task and reflect the similarity between the words as perceived by human judges (Bruni et Stephen Clark University of Cambridge Computer Laboratory sc609@cam.ac.uk al., 2012). Evaluation is c</context>
<context position="5142" citStr="Coecke et al., 2010" startWordPosition="797" endWordPosition="800">that produce higher correlations with human similarity judgements than the original full vectors. The second method is a weighted l2- normalisation of the vectors prior to application of singular value decomposition (SVD) (Deerwester et al., 1990) or compositional vector operators. It has the effect of drastically improving SVD with 100 or fewer dimensions. For example, we find that applying normalisation before SVD improves correlation from p = 0.48 to p = 0.70 for 20 dimensions, on the word similarity task. This is an essential finding as many more complex models of compositional semantics (Coecke et al., 2010; Baroni and Zamparelli, 2010; Andreas and Ghahramani, 2013) work with tensor objects and require good quality low-dimensional representations of words in order to lower computational costs. This technique also improves the performance of vector addition on texts of any length and vector elementwise product on shorter texts, on both the similarity and definitions tasks. The definition task and dataset are an additional contribution. We produced a new dataset of words and their definitions, which is separated into nine parts, each consisting of definitions of a particular length. This allows us</context>
<context position="22672" citStr="Coecke et al., 2010" startWordPosition="3785" endWordPosition="3788">the sparsity problem through the use of dimensionality reduction, which produces more dense vectors. Kron, the Kronecker (or tensor) product of two vectors, produces a matrix (second order tensor) whose diagonal matches the result of the Prod operation, but whose off-diagonal entries are all the other products of elements of the two vectors. We only apply Kron to SVD-reduced vectors, and to compare two matrices we turn them into vectors by concatenating matrix rows, and use cosine similarity on the resulting vectors. While in the more complex, type-driven methods (Baroni and Zamparelli, 2010; Coecke et al., 2010) tensors represent functions, and off-diagonal entries have a particular transformational interpretation as part of a linear map, the significance of the offdiagonal elements is difficult to interpret in our setting, apart from their role as encoders of the order of operands. We only examine Kron as the unencoded version of the Conv operator to see how the performance is affected by the random indexing and the modular summation by which Conv differs from Kron.2 We cannot use Kron for combining more than two words as the size of the resulting tensor grows exponentially with the num1Sparsity is </context>
</contexts>
<marker>Coecke, Sadrzadeh, Clark, 2010</marker>
<rawString>B. Coecke, M. Sadrzadeh, and S. Clark. 2010. Mathematical foundations for a compositional distributional model of meaning. In J. van Bentham, M. Moortgat, and W. Buszkowski, editors, Linguistic Analysis (Lambek Festschrift), volume 36, pages 345–384.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
</authors>
<title>From Distributional to Semantic Similarity.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="13366" citStr="Curran, 2004" startWordPosition="2217" endWordPosition="2218">d using Spearman correlation. 232 tTest PPMI Freq Data Max p Full p Max p Full p Max p Full p MENdev✿ 0.75 0.73 0.76 0.61 0.66 0.57 MENtest 0.76 0.73 0.76 0.61 0.66 0.56 WS353 0.70 0.63 0.70 0.41 0.57 0.41 Table 1: Values N learned on dev (�) also improve performance on the test data. Max p indicates correlation at the values of N that lead to the highest Spearman correlation on the development data. For each weighting scheme these are: 140 (tTest), 240 (PPMI), and 20 (Freq). Full p indicates the correlation when using full vectors without CS. The cosine, Jaccard, and Lin similarity measures (Curran, 2004) were all used to ensure the results reflect genuine effects of context selection, and not an artefact of any particular similarity measure. The similarity measure and value of N were chosen, given a particular weighting scheme, to maximise correlation on the development part of the MEN data (Bruni et al., 2012) (MENdev). Testing was performed on the remaining section of MEN and the entire WS353 dataset (Finkelstein et al., 2002). The MEN dataset consists of 3,000 word pairs rated for similarity, which is divided into a 2,000-pair development set and a 1,000-pair test set. WS353 consists only </context>
</contexts>
<marker>Curran, 2004</marker>
<rawString>James R. Curran. 2004. From Distributional to Semantic Similarity. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Deerwester</author>
<author>Susan T Dumais</author>
<author>Thomas K Landauer</author>
<author>George W Furnas</author>
<author>Richard Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the Society for Information Science,</journal>
<volume>41</volume>
<issue>6</issue>
<pages>407</pages>
<contexts>
<context position="4770" citStr="Deerwester et al., 1990" startWordPosition="736" endWordPosition="739">cy improvements, when vectors with up to 10,000 non-zero elements are reduced to a maximum of N = 240 non-zero elements, the Spearman p improves from 0.61 to 0.76 on a standard word similarity task. We also see an improvement when used in conjunction with further, standard dimensionality reduction techniques: the CS sparse vectors lead to reduced-dimensional representations that produce higher correlations with human similarity judgements than the original full vectors. The second method is a weighted l2- normalisation of the vectors prior to application of singular value decomposition (SVD) (Deerwester et al., 1990) or compositional vector operators. It has the effect of drastically improving SVD with 100 or fewer dimensions. For example, we find that applying normalisation before SVD improves correlation from p = 0.48 to p = 0.70 for 20 dimensions, on the word similarity task. This is an essential finding as many more complex models of compositional semantics (Coecke et al., 2010; Baroni and Zamparelli, 2010; Andreas and Ghahramani, 2013) work with tensor objects and require good quality low-dimensional representations of words in order to lower computational costs. This technique also improves the perf</context>
<context position="11126" citStr="Deerwester et al., 1990" startWordPosition="1817" endWordPosition="1820">the range [0, oc], while tTest spans [-1, 1], but generally produces values tightly concentrated around zero. We found that these ranges can produce poor performance due to numerical problems, so we corrected this through weighted row normalisation: w� := A w�  �w2 . With A = 10 this has the effect of restricting the values to [-10,10] for tTest and [0,10] for PPMI. Figure 1 shows the range of values for tTest. In general we use A = 1, but for some experiments we use A = 10 to push the highest weights above 1, as a way of combating the numerical errors that are likely to arise due UKSKVK (Deerwester et al., 1990). We also tried non-negative matrix factorisation (NNMF) (Seung and Lee, 2001), but found that it did not perform as well as SVD. We used the standard Matlab implementation of SVD. Random Indexing There are two ways of creating RI-based DSMs, the most popular being to initialise all target word vectors to zero and to generate a random vector for each context word. Then, while traversing through the corpus, each time a target word and a context word co-occur, the context word vector is added to the vector representing the target word. This method allows the RI vectors to be created in one step </context>
</contexts>
<marker>Deerwester, Dumais, Landauer, Furnas, Harshman, 1990</marker>
<rawString>Scott Deerwester, Susan T. Dumais, Thomas K. Landauer, George W. Furnas, and Richard Harshman. 1990. Indexing by latent semantic analysis. Journal of the Society for Information Science, 41(6):391– 407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Finkelstein</author>
<author>Evgeniy Gabrilovich</author>
<author>Yossi Matias</author>
<author>Ehud Rivlin</author>
<author>Zach Solan</author>
<author>Gadi Wolfman</author>
<author>Eytan Ruppin</author>
</authors>
<title>Placing search in context: The concept revisited.</title>
<date>2002</date>
<journal>ACM Transactions on Information Systems,</journal>
<pages>20--116</pages>
<contexts>
<context position="13799" citStr="Finkelstein et al., 2002" startWordPosition="2285" endWordPosition="2288">g scheme these are: 140 (tTest), 240 (PPMI), and 20 (Freq). Full p indicates the correlation when using full vectors without CS. The cosine, Jaccard, and Lin similarity measures (Curran, 2004) were all used to ensure the results reflect genuine effects of context selection, and not an artefact of any particular similarity measure. The similarity measure and value of N were chosen, given a particular weighting scheme, to maximise correlation on the development part of the MEN data (Bruni et al., 2012) (MENdev). Testing was performed on the remaining section of MEN and the entire WS353 dataset (Finkelstein et al., 2002). The MEN dataset consists of 3,000 word pairs rated for similarity, which is divided into a 2,000-pair development set and a 1,000-pair test set. WS353 consists only of 353 pairs, but has been consistently used as a benchmark word similarity dataset throughout the past decade. Results Figure 2 shows how correlation varies with N for the MEN development data. The peak performance for tTest is achieved when using around 140 top-ranked contexts per word, while for PPMI it is at N = 240, and for Freq N = 20. The dramatic drop in performance is demonstrated when using all three similarity measures</context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, Ruppin, 2002</marker>
<rawString>Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. 2002. Placing search in context: The concept revisited. ACM Transactions on Information Systems, 20:116–131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeniy Gabrilovich</author>
<author>Shaul Markovitch</author>
</authors>
<title>Computing semantic relatedness using Wikipediabased explicit semantic analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of the 20th international joint conference on Artifical intelligence, IJCAI’07,</booktitle>
<pages>1606--1611</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="3498" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="541" endWordPosition="544">ence vectors to produce improvements on word similarity and compositional tasks with simple operators. We chose to examine the use of sum, elementwise product, and circular convolution (Jones and Mewhort, 2007), because they are often used due to their simplicity, or as components of more complex models (Zanzotto and Dell’Arciprete, 2011). The first method is context selection (CS), in which the top N highest weighted context words per vector are selected, and the rest of the values are discarded (by setting to zero). This technique is similar to the way that Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007) selects the number of topics that represent a word, and the word filtering approach in Gamallo and Bordag (2011). It has the advantage of improving word representations and vector sum representations (for compositional tasks) while using vectors with fewer non-zero elements. Programming languages often have efficient strategies for stor230 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 230–238, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics ing these sparse vectors, leading to lower mem</context>
<context position="9732" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="1570" endWordPosition="1573">ectors no longer contain any zero elements. If M is a IwI x ICI matrix whose rows are made of word vectors wz, then the lower dimensional representation of those vectors is encoded in the IW I x K matrix ˆMK = UKSK where SVD(M, K) = Context Ranking and Selection The weighting schemes change the importance of individual target-context raw co-occurrence counts by considering the frequency with which each context word occurs with other target words. This is similar to term-weighting in IR and many retrieval functions are also used as weighting functions in DSMs. In the retrieval-based model ESA (Gabrilovich and Markovitch, 2007), only the N highest-weighted contexts are kept as a representative set of “topics” for a particular target word, and the rest are set to zero. Here we use a similar technique and, for each target word, retain only the N-highest weighted context words, using a word-similarity development set to choose the N that maximises correlation across all words in that dataset. Throughout the paper, we will refer to this technique as context selection (CS) and use N to indicate the maximum number of contexts per word. Hence all word vectors have at most N non-zero elements, effectively adjusting the spar</context>
</contexts>
<marker>Gabrilovich, Markovitch, 2007</marker>
<rawString>Evgeniy Gabrilovich and Shaul Markovitch. 2007. Computing semantic relatedness using Wikipediabased explicit semantic analysis. In Proceedings of the 20th international joint conference on Artifical intelligence, IJCAI’07, pages 1606–1611, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pablo Gamallo</author>
<author>Stefan Bordag</author>
</authors>
<title>Is singular value decomposition useful for word similarity extraction? Language Resources and Evaluation,</title>
<date>2011</date>
<volume>45</volume>
<issue>2</issue>
<contexts>
<context position="3611" citStr="Gamallo and Bordag (2011)" startWordPosition="561" endWordPosition="564">e the use of sum, elementwise product, and circular convolution (Jones and Mewhort, 2007), because they are often used due to their simplicity, or as components of more complex models (Zanzotto and Dell’Arciprete, 2011). The first method is context selection (CS), in which the top N highest weighted context words per vector are selected, and the rest of the values are discarded (by setting to zero). This technique is similar to the way that Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007) selects the number of topics that represent a word, and the word filtering approach in Gamallo and Bordag (2011). It has the advantage of improving word representations and vector sum representations (for compositional tasks) while using vectors with fewer non-zero elements. Programming languages often have efficient strategies for stor230 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 230–238, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics ing these sparse vectors, leading to lower memory usage. As an example of the resulting accuracy improvements, when vectors with up to 10,000 non-zero elements</context>
</contexts>
<marker>Gamallo, Bordag, 2011</marker>
<rawString>Pablo Gamallo and Stefan Bordag. 2011. Is singular value decomposition useful for word similarity extraction? Language Resources and Evaluation, 45(2):95–119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael N Jones</author>
<author>Douglas J K Mewhort</author>
</authors>
<title>Representing word meaning and order information in a composite holographic lexicon. Psychological Review,</title>
<date>2007</date>
<pages>114--1</pages>
<contexts>
<context position="3075" citStr="Jones and Mewhort, 2007" startWordPosition="472" endWordPosition="475"> meaning of the whole segment of text. Elementary approaches to composition employ simple operations, such as addition and elementwise product, directly on the word vectors. These have been shown to be effective for phrase similarity evaluation (Mitchell and Lapata, 2010) and detection of anomalous phrases (Kochmar and Briscoe, 2013). The methods that will be introduced in this paper can be applied to co-occurrence vectors to produce improvements on word similarity and compositional tasks with simple operators. We chose to examine the use of sum, elementwise product, and circular convolution (Jones and Mewhort, 2007), because they are often used due to their simplicity, or as components of more complex models (Zanzotto and Dell’Arciprete, 2011). The first method is context selection (CS), in which the top N highest weighted context words per vector are selected, and the rest of the values are discarded (by setting to zero). This technique is similar to the way that Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007) selects the number of topics that represent a word, and the word filtering approach in Gamallo and Bordag (2011). It has the advantage of improving word representations and vec</context>
<context position="11820" citStr="Jones and Mewhort (2007)" startWordPosition="1941" endWordPosition="1944">e, 2001), but found that it did not perform as well as SVD. We used the standard Matlab implementation of SVD. Random Indexing There are two ways of creating RI-based DSMs, the most popular being to initialise all target word vectors to zero and to generate a random vector for each context word. Then, while traversing through the corpus, each time a target word and a context word co-occur, the context word vector is added to the vector representing the target word. This method allows the RI vectors to be created in one step through a single traversal of the corpus. The other method, following Jones and Mewhort (2007), is to create the RI vectors through matrix multiplication rather than sequentially. We employ this method and assign each context word a random vector e�, = {rk}k where rk are drawn from the normal distribution JV (0, 1D) and I e�,I = D = 4096. The RI representation of a target word RI( wz) = wzR is constructed by multiplying the word vector wz, obtained as before, by the ICI x D matrix R where each column represents the vectors e�,. Weighting is performed prior to random indexing. 3 Word Similarity Experiments In this section we investigate the effects of context selection and normalisation</context>
<context position="21143" citStr="Jones and Mewhort, 2007" startWordPosition="3517" endWordPosition="3520">ly tuning on this dataset. For dimensionality reduction we consider vectors reduced with SVD to 100 and 700 dimensions. In some cases we exclude the results for SVD700 because they are very close to the scores for unreduced vectors. We experiment with 3 values of D from 1512, 1024, 40961 for the RI vectors. Operators To combine distributional vectors into a single-vector sentence representation, we use a representative set of methods from Mitchell and Lapata (2010). In particular, we use vector addition, elementwise (Hadamard) product, Kronecker product, and circular convolution (Plate, 1991; Jones and Mewhort, 2007), which are de234 fined as follows for two word vectors ~x, ~y: Sum x~+ y~✏ t~xi + ~yi✉i Prod x~❞ y~ ✏ t~xi ☎ ~yi✉i Kron x~❜ y~✏ t~xi ☎ ~yj✉ij Conv x~ ❣ y~ ✏ ✦➦nj✏0♣~xqj%n ☎ ♣~yq♣i✁j)%n✮ i Repeated application of the Sum operation adds contexts for each of the words that occur in a phrase, which maintains (and mixes) any noisy parts of the component word vectors. Our intention was that use of the CS vectors would lead to less noisy word vectors and hence less noisy phrase and sentence vectors. The Prod operator, on the other hand, provides a phrase or sentence representation consisting only of</context>
</contexts>
<marker>Jones, Mewhort, 2007</marker>
<rawString>Michael N. Jones and Douglas J. K. Mewhort. 2007. Representing word meaning and order information in a composite holographic lexicon. Psychological Review, 114:1–37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ekaterina Kochmar</author>
<author>Ted Briscoe</author>
</authors>
<title>Capturing anomalies in the choice of content words in compositional distributional semantic space.</title>
<date>2013</date>
<booktitle>In Proceedings of the Recent Advances in Natural Language Processing (RANLP-2013),</booktitle>
<location>Hissar, Bulgaria.</location>
<contexts>
<context position="2786" citStr="Kochmar and Briscoe, 2013" startWordPosition="424" endWordPosition="427">tional vectors learned through co-occurrence statistics, infrequent phrases and novel constructions are impossible to represent in that way. The goal of compositional DSMs is to find methods of combining word vectors, or perhaps higher-order tensors, into a single vector that represents the meaning of the whole segment of text. Elementary approaches to composition employ simple operations, such as addition and elementwise product, directly on the word vectors. These have been shown to be effective for phrase similarity evaluation (Mitchell and Lapata, 2010) and detection of anomalous phrases (Kochmar and Briscoe, 2013). The methods that will be introduced in this paper can be applied to co-occurrence vectors to produce improvements on word similarity and compositional tasks with simple operators. We chose to examine the use of sum, elementwise product, and circular convolution (Jones and Mewhort, 2007), because they are often used due to their simplicity, or as components of more complex models (Zanzotto and Dell’Arciprete, 2011). The first method is context selection (CS), in which the top N highest weighted context words per vector are selected, and the rest of the values are discarded (by setting to zero</context>
</contexts>
<marker>Kochmar, Briscoe, 2013</marker>
<rawString>Ekaterina Kochmar and Ted Briscoe. 2013. Capturing anomalies in the choice of content words in compositional distributional semantic space. In Proceedings of the Recent Advances in Natural Language Processing (RANLP-2013), Hissar, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guido Minnen</author>
<author>John Carroll</author>
<author>Darren Pearce</author>
</authors>
<title>Applied morphological processing of English.</title>
<date>2001</date>
<journal>Natural Language Engineering,</journal>
<volume>7</volume>
<issue>3</issue>
<contexts>
<context position="7435" citStr="Minnen et al., 2001" startWordPosition="1162" endWordPosition="1165">t co-occurrence frequency vectors (Freq). The rest of this section contains a description of the particular settings used to construct the raw word vectors and the weighting schemes (tTest, PPMI) that we considered in our experiments. This is followed by a detailed description of the context selection (CS) and normalisation techniques. Finally, dimensionality reduction (SVD) is proposed as a way of combating sparsity and random indexing (RI) as an essential step of encoding vectors for use with the convolution operator. Raw Vectors We used a cleaned-up corpus of 1.7 billion lemmatised tokens (Minnen et al., 2001) from the October, 2013 snapshot of Wikipedia, and constructed context vectors by using sentence boundaries to provide the window. The set of context words C consisted of the 10,000 most frequent words occurring in this dataset, with the exception of stopwords from a standard stopword list. Therefore, a frequency vector for a target word wi E W is represented as wi = {fwicj}j, where cj E C (ICI = 10, 000), W is a set of target words in a particular evaluation dataset, and fwicj is the co-occurrence frequency between the target word, wi and context word, cj. Vector Weighting We used the tTest a</context>
</contexts>
<marker>Minnen, Carroll, Pearce, 2001</marker>
<rawString>Guido Minnen, John Carroll, and Darren Pearce. 2001. Applied morphological processing of English. Natural Language Engineering, 7(3):207–223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Composition in distributional models of semantics.</title>
<date>2010</date>
<journal>Cognitive Science,</journal>
<volume>34</volume>
<issue>8</issue>
<contexts>
<context position="1274" citStr="Mitchell and Lapata, 2010" startWordPosition="185" endWordPosition="188">sed representations. We demonstrate these effects on standard word and phrase datasets, and on a new definition retrieval task and dataset. 1 Introduction Distributional semantic models (DSMs) (Turney and Pantel, 2010; Clarke, 2012) encode word meaning by counting co-occurrences with other words within a context window and recording these counts in a vector. Various IR and NLP tasks, such as word sense disambiguation, query expansion, and paraphrasing, take advantage of DSMs at a word level. More recently, researchers have been exploring methods that combine word vectors to represent phrases (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010) and sentences (Coecke et al., 2010; Socher et al., 2012). In this paper, we introduce two techniques that improve the quality of word vectors and can be easily tuned to adapt the vectors to particular lexical and compositional tasks. The quality of the word vectors is generally assessed on standard datasets that consist of a list of word pairs and a corresponding list of gold standard scores. These scores are gathered through an annotation task and reflect the similarity between the words as perceived by human judges (Bruni et Stephen Clark University of Cambridg</context>
<context position="2723" citStr="Mitchell and Lapata, 2010" startWordPosition="415" endWordPosition="418">s some frequent shorter phrases, can be represented by distributional vectors learned through co-occurrence statistics, infrequent phrases and novel constructions are impossible to represent in that way. The goal of compositional DSMs is to find methods of combining word vectors, or perhaps higher-order tensors, into a single vector that represents the meaning of the whole segment of text. Elementary approaches to composition employ simple operations, such as addition and elementwise product, directly on the word vectors. These have been shown to be effective for phrase similarity evaluation (Mitchell and Lapata, 2010) and detection of anomalous phrases (Kochmar and Briscoe, 2013). The methods that will be introduced in this paper can be applied to co-occurrence vectors to produce improvements on word similarity and compositional tasks with simple operators. We chose to examine the use of sum, elementwise product, and circular convolution (Jones and Mewhort, 2007), because they are often used due to their simplicity, or as components of more complex models (Zanzotto and Dell’Arciprete, 2011). The first method is context selection (CS), in which the top N highest weighted context words per vector are selecte</context>
<context position="19626" citStr="Mitchell and Lapata (2010)" startWordPosition="3263" endWordPosition="3266">especially for D &gt;, 512. On Freq we find that CS with N = 60 leads to higher correlation, but mid-range and full vectors have equivalent performance. For Freq, the correlation is equivalent to full vectors from D = 128, while for the weighted vectors 512 dimensions appear to be sufficient. Unlike for SVD, normalisation slightly reduces the performance for mid-range dimensions. 4 Compositional Experiments We examine the performance of vectors augmented by CS and normalisation in two compositional tasks. The first is an extension of the word similarity task to phrase pairs, using the dataset of Mitchell and Lapata (2010). Each entry in the dataset consists of two phrases, each consisting of two words (in various syntactic relations, such as verb-object and adjective noun), and a gold standard score. We combine the two word vectors into a single phrase vector using various operators described below. We then calculate the similarity between the phrase vectors using cosine and compare the resulting scores against the gold standard using Spearman correlation. The second task is our new definitions task where, again, word vectors from each definition are composed to form a single vector, which can then be compared</context>
<context position="20988" citStr="Mitchell and Lapata (2010)" startWordPosition="3496" endWordPosition="3499">3, the top third of the ranked contexts at N = 3300, and the full vectors without CS at N = 10000. This gives us a range of values to examine, without directly tuning on this dataset. For dimensionality reduction we consider vectors reduced with SVD to 100 and 700 dimensions. In some cases we exclude the results for SVD700 because they are very close to the scores for unreduced vectors. We experiment with 3 values of D from 1512, 1024, 40961 for the RI vectors. Operators To combine distributional vectors into a single-vector sentence representation, we use a representative set of methods from Mitchell and Lapata (2010). In particular, we use vector addition, elementwise (Hadamard) product, Kronecker product, and circular convolution (Plate, 1991; Jones and Mewhort, 2007), which are de234 fined as follows for two word vectors ~x, ~y: Sum x~+ y~✏ t~xi + ~yi✉i Prod x~❞ y~ ✏ t~xi ☎ ~yi✉i Kron x~❜ y~✏ t~xi ☎ ~yj✉ij Conv x~ ❣ y~ ✏ ✦➦nj✏0♣~xqj%n ☎ ♣~yq♣i✁j)%n✮ i Repeated application of the Sum operation adds contexts for each of the words that occur in a phrase, which maintains (and mixes) any noisy parts of the component word vectors. Our intention was that use of the CS vectors would lead to less noisy word vect</context>
<context position="25040" citStr="Mitchell and Lapata (2010)" startWordPosition="4180" endWordPosition="4183">512 0.09 (0.09) 0.27 (0.30) 0.25 (0.24) Conv RI1024 0.08 (0.14) 0.33 (0.37) 0.25 (0.27) RI4096 0.18 (0.19) 0.37 (0.38) 0.27 (0.27) Table 3: Behaviour of vector operators with PPMI vectors on ML2010 (Spearman correlation). Values for normalised vectors in parentheses. ber of vector operations, but we can use Conv as an encoded alternative as it results in a vector of the same dimension as the two operands. 4.1 Phrase Similarity To test how CS, normalisation, and dimensionality reduction affect simple compositional vector operations we use the test portion of the phrasal similarity dataset from Mitchell and Lapata (2010) (ML2010). This dataset consists of pairs of twoword phrases and a human similarity judgement on the scale of 1-7. There are three types of phrases: noun-noun, adjective-noun, and verbobject. In the original paper, and some subsequent works, these were treated as three different datasets; however, here we combine the datasets into one single phrase pair dataset. This allows us to summarise the effects of different types of vectors on phrasal composition in general. Results Our results (Tables 2 and 3) are comparable to those in Mitchell and Lapata (2010) averaged across the phrase-types (ρ ✏ 0</context>
<context position="31314" citStr="Mitchell and Lapata (2010)" startWordPosition="5237" endWordPosition="5240">01 0.11 0.00 0.00 mean MRR 0.28 0.35 0.06 0.17 0.02 0.02 Table 5: Best settings for operators calculated from the highest average MRR across all the datasets, with and without normalisation. The results for vectors with no normalisation or CS are: Sum - P@1=0.1567, MRR=0.2624; Prod - P@1=0.0147, MRR=0.0542; Conv P@1=0.0027, MRR=0.0192. 5 Discussion In this paper we introduced context selection and normalisation as techniques for improving the semantic vector space representations of words. We found that, although our untuned vectors perform better on WS353 data (p = 0.63) than vectors used by Mitchell and Lapata (2010) (p = 0.42), our best phrase composition model (Sum, p = 0.40) produces a lower performance than an estimate of their best model (Prod, p = 0.44).4 This indicates that better performance on word-similarity data does not directly translate into better performance on compositional tasks; however, CS and normalisation are both effective in increasing the quality of the composed representation (p = 0.42). Since CS and normalisation are computationally inexpensive, they are an excellent way to improve model quality compared to the alternative, which 4The estimate is computed as an average across th</context>
</contexts>
<marker>Mitchell, Lapata, 2010</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2010. Composition in distributional models of semantics. Cognitive Science, 34(8):1388–1429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T A Plate</author>
</authors>
<title>Holographic reduced Representations: Convolution algebra for compositional distributed representations.</title>
<date>1991</date>
<booktitle>Proceedings of the 12th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>30--35</pages>
<editor>In J. Mylopoulos and R. Reiter, editors,</editor>
<publisher>Morgan Kauffman.</publisher>
<location>Sydney, Australia,</location>
<contexts>
<context position="21117" citStr="Plate, 1991" startWordPosition="3515" endWordPosition="3516">ithout directly tuning on this dataset. For dimensionality reduction we consider vectors reduced with SVD to 100 and 700 dimensions. In some cases we exclude the results for SVD700 because they are very close to the scores for unreduced vectors. We experiment with 3 values of D from 1512, 1024, 40961 for the RI vectors. Operators To combine distributional vectors into a single-vector sentence representation, we use a representative set of methods from Mitchell and Lapata (2010). In particular, we use vector addition, elementwise (Hadamard) product, Kronecker product, and circular convolution (Plate, 1991; Jones and Mewhort, 2007), which are de234 fined as follows for two word vectors ~x, ~y: Sum x~+ y~✏ t~xi + ~yi✉i Prod x~❞ y~ ✏ t~xi ☎ ~yi✉i Kron x~❜ y~✏ t~xi ☎ ~yj✉ij Conv x~ ❣ y~ ✏ ✦➦nj✏0♣~xqj%n ☎ ♣~yq♣i✁j)%n✮ i Repeated application of the Sum operation adds contexts for each of the words that occur in a phrase, which maintains (and mixes) any noisy parts of the component word vectors. Our intention was that use of the CS vectors would lead to less noisy word vectors and hence less noisy phrase and sentence vectors. The Prod operator, on the other hand, provides a phrase or sentence represe</context>
</contexts>
<marker>Plate, 1991</marker>
<rawString>T. A. Plate. 1991. Holographic reduced Representations: Convolution algebra for compositional distributed representations. In J. Mylopoulos and R. Reiter, editors, Proceedings of the 12th International Joint Conference on Artificial Intelligence, Sydney, Australia, August 1991, pages 30–35, San Mateo, CA. Morgan Kauffman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Seung</author>
<author>L Lee</author>
</authors>
<title>Algorithms for nonnegative matrix factorization. Advances in neural information processing systems,</title>
<date>2001</date>
<pages>13--556</pages>
<contexts>
<context position="11204" citStr="Seung and Lee, 2001" startWordPosition="1828" endWordPosition="1831">y concentrated around zero. We found that these ranges can produce poor performance due to numerical problems, so we corrected this through weighted row normalisation: w� := A w�  �w2 . With A = 10 this has the effect of restricting the values to [-10,10] for tTest and [0,10] for PPMI. Figure 1 shows the range of values for tTest. In general we use A = 1, but for some experiments we use A = 10 to push the highest weights above 1, as a way of combating the numerical errors that are likely to arise due UKSKVK (Deerwester et al., 1990). We also tried non-negative matrix factorisation (NNMF) (Seung and Lee, 2001), but found that it did not perform as well as SVD. We used the standard Matlab implementation of SVD. Random Indexing There are two ways of creating RI-based DSMs, the most popular being to initialise all target word vectors to zero and to generate a random vector for each context word. Then, while traversing through the corpus, each time a target word and a context word co-occur, the context word vector is added to the vector representing the target word. This method allows the RI vectors to be created in one step through a single traversal of the corpus. The other method, following Jones an</context>
</contexts>
<marker>Seung, Lee, 2001</marker>
<rawString>D Seung and L Lee. 2001. Algorithms for nonnegative matrix factorization. Advances in neural information processing systems, 13:556–562.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic Compositionality Through Recursive Matrix-Vector Spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference on Empirical Methods in Natural Language Processing (EMNLP), Jeju Island,</booktitle>
<contexts>
<context position="1361" citStr="Socher et al., 2012" startWordPosition="199" endWordPosition="202">on a new definition retrieval task and dataset. 1 Introduction Distributional semantic models (DSMs) (Turney and Pantel, 2010; Clarke, 2012) encode word meaning by counting co-occurrences with other words within a context window and recording these counts in a vector. Various IR and NLP tasks, such as word sense disambiguation, query expansion, and paraphrasing, take advantage of DSMs at a word level. More recently, researchers have been exploring methods that combine word vectors to represent phrases (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010) and sentences (Coecke et al., 2010; Socher et al., 2012). In this paper, we introduce two techniques that improve the quality of word vectors and can be easily tuned to adapt the vectors to particular lexical and compositional tasks. The quality of the word vectors is generally assessed on standard datasets that consist of a list of word pairs and a corresponding list of gold standard scores. These scores are gathered through an annotation task and reflect the similarity between the words as perceived by human judges (Bruni et Stephen Clark University of Cambridge Computer Laboratory sc609@cam.ac.uk al., 2012). Evaluation is conducted by comparing </context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. 2012. Semantic Compositionality Through Recursive Matrix-Vector Spaces. In Proceedings of the 2012 Conference on Empirical Methods in Natural Language Processing (EMNLP), Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>37--141</pages>
<contexts>
<context position="866" citStr="Turney and Pantel, 2010" startWordPosition="123" endWordPosition="126">g semantics at the word level, and research has recently moved on to building distributional representations for larger segments of text. In this paper, we introduce novel ways of applying context selection and normalisation to vary model sparsity and the range of values of the DSM vectors. We show how these methods enhance the quality of the vectors and thus result in improved low dimensional and composed representations. We demonstrate these effects on standard word and phrase datasets, and on a new definition retrieval task and dataset. 1 Introduction Distributional semantic models (DSMs) (Turney and Pantel, 2010; Clarke, 2012) encode word meaning by counting co-occurrences with other words within a context window and recording these counts in a vector. Various IR and NLP tasks, such as word sense disambiguation, query expansion, and paraphrasing, take advantage of DSMs at a word level. More recently, researchers have been exploring methods that combine word vectors to represent phrases (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010) and sentences (Coecke et al., 2010; Socher et al., 2012). In this paper, we introduce two techniques that improve the quality of word vectors and can be easily t</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research, 37:141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Massimo Zanzotto</author>
<author>Lorenzo Dell’Arciprete</author>
</authors>
<title>Distributed structures and distributional meaning.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Distributional Semantics and Compositionality, DiSCo11,</booktitle>
<pages>10--15</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon.</location>
<marker>Zanzotto, Dell’Arciprete, 2011</marker>
<rawString>Fabio Massimo Zanzotto and Lorenzo Dell’Arciprete. 2011. Distributed structures and distributional meaning. In Proceedings of the Workshop on Distributional Semantics and Compositionality, DiSCo11, pages 10–15, Portland, Oregon. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chengxiang Zhai</author>
<author>John Lafferty</author>
</authors>
<title>A study of smoothing methods for language models applied to ad hoc information retrieval.</title>
<date>2001</date>
<booktitle>In Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’01,</booktitle>
<pages>334--342</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="23349" citStr="Zhai and Lafferty, 2001" startWordPosition="3902" endWordPosition="3905">s have a particular transformational interpretation as part of a linear map, the significance of the offdiagonal elements is difficult to interpret in our setting, apart from their role as encoders of the order of operands. We only examine Kron as the unencoded version of the Conv operator to see how the performance is affected by the random indexing and the modular summation by which Conv differs from Kron.2 We cannot use Kron for combining more than two words as the size of the resulting tensor grows exponentially with the num1Sparsity is a problem that may be addressable through smoothing (Zhai and Lafferty, 2001), although we do not investigate that avenue in this paper. 2Conv also differs from Kron in that it is commutative, unless one of the operands is permuted. In this paper we do not permute the operands. Oper N=140 N=3300 N=10000 sum ttest 0.40 (0.41) 0.40 (0.40) 0.40 (0.40) SVD100 0.37 (0.42) 0.35 (0.41) 0.37 (0.40) Prod ttest 0.32 (0.32) 0.40 (0.40) 0.32 (0.32) SVD100 0.25 (0.23) 0.23 (0.23) 0.21 (0.23) kron SVD100 0.31 (0.34) 0.34 (0.38) 0.29 (0.32) SVD700 0.39 (0.39) 0.37 (0.37) 0.30 (0.30) Conv RI512 0.10 (0.12) 0.26 (0.21) 0.25 (0.25) RI1024 0.22 (0.15) 0.29 (0.27) 0.25 (0.26) RI4096 0.16 </context>
</contexts>
<marker>Zhai, Lafferty, 2001</marker>
<rawString>Chengxiang Zhai and John Lafferty. 2001. A study of smoothing methods for language models applied to ad hoc information retrieval. In Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’01, pages 334–342, New York, NY, USA. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>