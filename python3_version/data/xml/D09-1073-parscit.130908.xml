<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000132">
<title confidence="0.926494">
Tree Kernel-based SVM with Structured Syntactic Know-
ledge for BTG-based Phrase Reordering
</title>
<author confidence="0.950194">
Min Zhang Haizhou Li
</author>
<affiliation confidence="0.94246">
Institute for Infocomm Research
</affiliation>
<address confidence="0.9618255">
1 Fusionopolis Way,#21-01 Connexis (South Tower)
Singapore 138632
</address>
<email confidence="0.998799">
{mzhang,hli}@i2r.a-star.edu.sg
</email>
<sectionHeader confidence="0.996655" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999875588235294">
Structured syntactic knowledge is important
for phrase reordering. This paper proposes us-
ing convolution tree kernel over source parse
tree to model structured syntactic knowledge
for BTG-based phrase reordering in the con-
text of statistical machine translation. Our
study reveals that the structured syntactic fea-
tures over the source phrases are very effective
for BTG constraint-based phrase reordering
and those features can be well captured by the
tree kernel. We further combine the structured
features and other commonly-used linear fea-
tures into a composite kernel. Experimental re-
sults on the NIST MT-2005 Chinese-English
translation tasks show that our proposed
phrase reordering model statistically signifi-
cantly outperforms the baseline methods.
</bodyText>
<sectionHeader confidence="0.998775" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999944115384616">
Phrase-based method (Koehn et al., 2003; Och
and Ney, 2004; Koehn et al., 2007) and syntax-
based method (Wu, 1997; Yamada and Knight,
2001; Eisner, 2003; Chiang, 2005; Cowan et al.,
2006; Marcu et al., 2006; Liu et al., 2007; Zhang
et al., 2007c, 2008a, 2008b; Shen et al., 2008; Mi
and Huang, 2008) represent the state-of-the-art
technologies in statistical machine translation
(SMT). As the two technologies are complemen-
tary in many ways, an interesting research topic
is how to combine the strengths of the two me-
thods. Many research efforts have been made to
address this issue, which can be summarized into
two ideas. One is to add syntax into phrase-based
model while another one is to enhance syntax-
based model to handle non-syntactic phrases. In
this paper, we bring forward the first idea by
studying the issue of how to utilize structured
syntactic features for phrase reordering in a
phrase-based SMT system with BTG (Bracketing
Transduction Grammar) constraints (Wu, 1997).
Word and phrase reordering is a crucial com-
ponent in a SMT system. In syntax-based method,
word reordering is implicitly addressed by trans-
lation rules, thus the performance is subject to
parsing errors to a large extent (zhang et al.,
2007a) and the impact of syntax on reordering is
difficult to single out (Li et al., 2007). In phrase-
based method, local word reordering1 can be ef-
fectively captured by phrase pairs directly while
local phrase reordering is explicitly modeled by
phrase reordering model and distortion model.
Recently, many phrase reordering methods have
been proposed, ranging from simple distance-
based distortion model (Koehn et al., 2003; Och
and Ney, 2004), flat reordering model (Wu, 1997;
Zens et al., 2004), lexicalized reordering model
(Tillmann, 2004; Kumar and Byrne, 2005), to
hierarchical phrase-based model (Chiang, 2005;
Setiawan et al., 2007) and classifier-based reor-
dering model with linear features (Zens and Ney,
2006; Xiong et al., 2006; Zhang et al., 2007a;
Xiong et al., 2008). However, one of the major
limitations of these advances is the structured
syntactic knowledge, which is important to glob-
al reordering (Li et al., 2007; Elming, 2008), has
not been well exploited. This makes the phrase-
based method particularly weak in handling
global phrase reordering. From machine learning
viewpoint (Vapnik, 1995), it is computationally
infeasible to explicitly generate features involv-
ing structured information in many NLP applica-
</bodyText>
<footnote confidence="0.99672725">
1 This paper follows the term convention of global reorder-
ing and local reordering of Li et al. (2007), between which
the distinction is solely defined by reordering distance
(whether beyond four source words) (Li et al., 2007).
</footnote>
<page confidence="0.850032">
698
</page>
<note confidence="0.996628">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 698–707,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999891163636364">
tions. For example, one cannot enumerate effi-
ciently all the sub-tree features for a full parse
tree. This would be the reason why structured
features are not fully utilized in previous statis-
tical feature-based phrase reordering model.
Thanks to the nice property of kernel-based
machine learning method that can implicitly ex-
plore (structured) features in a high dimensional
feature space (Vapnik, 1995), in this paper we
propose using convolution tree kernel (Haussler,
1999; Collins and Duffy, 2001) to explore the
structured syntactic knowledge for phrase reor-
dering and further combine the tree kernel with
other diverse linear features into a composite
kernel to strengthen the model’s predictive abili-
ty. Indeed, using tree kernel methods to mine
structured knowledge has shown success in some
NLP applications like parsing (Collins and Duffy,
2001), semantic role labeling (Moschitti, 2004;
Zhang et al., 2007b), relation extraction (Zhang
et al., 2006), pronoun resolution (Yang et al.,
2006) and question classification (Zhang and
Lee, 2003). However, to our knowledge, such
technique still remains unexplored for phrase
reordering.
In this paper, we look into the phrase reorder-
ing problem in two aspects: 1) how to model and
optimize structured features, and 2) how to com-
bine the structured features with other linear fea-
tures and further integrate them into the log-
linear model-based translation framework. Our
study shows that: 1) the structured syntactic fea-
tures are very useful and 2) our kernel-based
model can well explore diverse knowledge, in-
cluding previously-used linear features and the
structured syntactic features, for phrase reorder-
ing. Our model displays one advantage over the
previous work that it is able to utilize the struc-
tured syntactic features without the need for ex-
tensive feature engineering in decoding a parse
tree into a set of linear syntactic features.
To have a more insightful evaluation, we de-
sign three experiments with three different eval-
uation metrics. Experimental results on the NIST
MT-2005 Chinese-English translation tasks show
that our method statistically significantly outper-
forms the baseline methods in term of the three
different evaluation metrics.
The rest of the paper is organized as follows.
Section 2 introduces the baseline method of
BTG-based phrase translation method while sec-
tion 3 discusses the proposed method in detail.
The experimental results are reported and dis-
cussed in section 4. Finally, we conclude the pa-
per in section 5.
</bodyText>
<sectionHeader confidence="0.524467" genericHeader="method">
2 Baseline System and Method
</sectionHeader>
<bodyText confidence="0.99910525">
We use the MaxEnt-based BTG translation sys-
tem (Xiong et al., 2006) as our baseline. It is a
phrase-based SMT system with BTG reordering
constraint. The system uses the BTG lexical
translation rules ( ܣ ՜ ݔ/ݕ ) to translate the
source phrase ݔ into target phrase ݕ , and the
BTG merging rules (ܣ ՜ ሾܣ, ܣሿ |൏ ܣ, ܣ ൐ ) to
combine two neighboring phrases with a straight
or inverted order. In the translation model, the
BTG lexical rules are weighted with several fea-
tures, such as phrase translation, word penalty
and language models, in a log-linear form. With
the BTG constraint, the reordering model Ω is
defined on the two neighboring phrases ܣଵ and
ܣଶ and their order ݋ א ሼݏݐݎ݄ܽ݅݃ݐ, ݅݊ݒ݁ݎݐ݁݀ሽ as
follows:
</bodyText>
<equation confidence="0.941102">
Ω ൌ f(݋, ܣଵ, ܣଶ) (1)
</equation>
<bodyText confidence="0.9997602">
In the baseline system, a MaxEnt-based clas-
sifier with boundary words of the two neighbor-
ing phrases as features is used to model the
merging/reordering order. The baseline MaxEnt-
based reordering model is formulized as follows:
</bodyText>
<equation confidence="0.995591">
Ω ൌ ݌ఏ(݋|ܣଵ, ܣଶ) ൌ ௘௫௣(∑೔ఏ೔௛೔(௢,஺భ,஺మ)) ∑೚ ௘௫௣(∑೔ ఏ೔௛೔(௢,஺భ,஺మ)) (2)
</equation>
<bodyText confidence="0.9999594">
where the functions ݄௜(݋, ܣଵ, ܣଶ) א ሼ0,1ሽ are
model feature functions using the boundary
words of the two neighboring phrases as features,
and ߠ௜ are feature weights that are trained based
on the MaxEnt-based criteria.
</bodyText>
<sectionHeader confidence="0.9918955" genericHeader="method">
3 Tree Kernel-based Phrase Reordering
Model
</sectionHeader>
<subsectionHeader confidence="0.9997965">
3.1 Kernel-based Classifier Solution to
Phrase Reordering
</subsectionHeader>
<bodyText confidence="0.999738722222222">
In this paper, phrase reordering is recast as a
classification issue as done in previous work
(Xiong et al., 2006 &amp; 2008; Zhang et al., 2007a).
In training, we use a machine learning algorithm
training on the annotated phrase reordering in-
stances that are automatically extracted from
word-aligned, source sentence parsed training
corpus, to learn a classifier. In testing (decoding),
the learned classifier is applied to two adjacent
source phrases to decide whether they should be
merged (straight) or reordered (inverted) and
what their probabilities are, and then these prob-
abilities are used as one feature in the log-linear
model in a phrase-based decoder.
In addition to the previously-used linear fea-
tures, we are more interested in the value of
structured syntax in phrase reordering and how
to capture it using kernel methods. However, not
</bodyText>
<page confidence="0.998533">
699
</page>
<bodyText confidence="0.996863692307692">
all classifiers are able to work with kernel me-
thods. Only those dot-product-based classifiers
can work with kernels by replacing the dot prod-
uct with a kernel function, where the kernel func-
tion is able to directly calculate the similarity
between two (structured) objects without enume-
rating them into linear feature vectors. In this
paper, we select SVM as our classifier. In this
section, we first define the structured syntactic
features and introduce the commonly used linear
features, and then discuss how to utilize these
features by kernel methods together SVM for
phrase reordering
</bodyText>
<subsectionHeader confidence="0.999717">
3.2 Structured Syntactic Features
</subsectionHeader>
<bodyText confidence="0.99991625">
A reordering instance x = [Al,AZ1 (see Eq.1) in
this paper refers to two adjacent source phrases
Al and AZ to be translated. The structured syn-
tactic feature spaces of a reordering instance are
defined as the portion of a parse tree of the
source sentence that at least covers the span of
the reordering instance (i.e. the two neighboring
phrases). The syntactic features are defined as all
</bodyText>
<figure confidence="0.937412">
T1) Minimum Sub-Tree (MST)
T2) Minimum Sub-Structure (MSS) T4) Chunking Tree (CT)
T3) Context-sensitive Minimum Sub-Structure (CMSS)
</figure>
<figureCaption confidence="0.94156">
Figure 1. Different representations of structured syntactic features of a reordering instance in the example
</figureCaption>
<bodyText confidence="0.98825575">
sentence excerpted from our training corpus “...建立/build 规模/scale 宏大/mighty 的/of 各类/various
types 人才/qualified personnel 队伍/contingent 首先/above all 迫切/urgently 需要/necessary 中央
/central authorities 统筹/overall 规划/planning...(To build a mighty contingent of qualified personnel of
various types, it is necessary, above all, for the central authorities to make overall planning.) ”, where “各
类/various types 人才/qualified personnel 队伍/contingent (contingent of qualified personnel of various
types)” is the 1st/left phrase and “首先/above all 迫切/urgent 需要/necessary (it is necessary, above all,
...)” is the 2nd/right phrase. Note that different function tags are attached to the grammar tag of each inter-
nal node.
</bodyText>
<page confidence="0.989359">
700
</page>
<bodyText confidence="0.9999135">
the possible subtrees in the structured feature
spaces. We can see that the structured feature
spaces and their features are encapsulated by a
full parse tree of source sentences. Thus, it is
critical to understand which portion of a parse
tree (i.e. structured feature space) is the most ef-
fective to represent a reordering instance. Moti-
vated by the work of (Zhang et al., 2006), we
here examine four cases that contain different
sub-structures as shown in Fig. 1.
</bodyText>
<listItem confidence="0.929625869565217">
(1) Minimum Sub-Tree (MST): the sub-tree
rooted by the nearest common ancestor of the
two phrases. This feature records the minimum
sub-structure covering the two phrases and its
left and right contexts as shown in Fig 1.T1.
(2) Minimum Sub-Structure (MSS): the smal-
lest common sub-structure covering the two
phrases. It is enclosed by the shortest path link-
ing the two phrases. Thus, its leaf nodes exactly
consist of all the phrasal words.
(3) Context-sensitive Minimum Sub-Structure
(CMSS): the MSS extending with the 1st left
sibling node of the left phrase and the 1st right
sibling node of the right phrase and their descen-
dants. If sibling is unavailable, then we move to
the parent of current node and repeat the same
process until the sibling is available or the root of
the MST is reached.
(4) Chunking Tree (CT): the base phrase list
extracted from the MSS. We prune out all the
internal structures of the MSS and only keep the
root node and the base phrase list for generating
the chunking tree.
</listItem>
<bodyText confidence="0.999810147058823">
Fig. 1 illustrates the different representations
of an example reordering instance. T1 is the MST
for the example instance, where the sub-structure
circled by a dotted line is the MSS, which is also
shown in T2 for clarity. We can see that the MSS
is a subset of the MST. By T2 we would like to
evaluate whether the structured information is
effective for phrase reordering while by compar-
ing the system performance when using T1 and
T2, we would like to evaluate whether the struc-
tured context information embedded in the MST
is useful to phrase reordering. T3 is the CMSS,
where the two sub-structures circled by dotted
lines are included as the context to T2 and make
T3 limited context-sensitive. This is to evaluate
whether the limited context information in the
CMSS is helpful. By comparing the performance
of T1 and T3, we would like to see whether the
larger context in T1 is a noisy feature. T4 is the
CT, where only the basic structured information
is kept. By comparing the performance of T2 and
T4, we would like to study whether the high-level
structured syntactic features in T2 are useful to
phrase reordering.
After defining the four structured feature
spaces, we further partition each feature space
into five parts according to their functionalities.
Because it only makes sense to evaluate two par-
titions of the same functionality between two
reordering instances, the feature space partition
leads to a more precise similarity calculation. As
shown in Fig 1, all the internal nodes in each par-
tition are labeled with a unique function tag in
the following way:
</bodyText>
<listItem confidence="0.999684923076923">
• Left Context (-lc): nodes in this partition
do not cover any phrase word and they are
all in the left of the left phrase.
• Right Context (-rc): nodes in this partition
do not cover any phrase word and they are
all in the right of the right phrase.
• Left Phrase (-lp): nodes in this partition
only cover the first phrase and/or its left
context.
• Right Phrase (-rp): nodes in this partition
only cover the second phrase and/or its right
context.
• Shared Part (-sp): nodes in this partition at
</listItem>
<bodyText confidence="0.7969585">
least cover both of the two phrases partially.
No lexical word is tagged since it is not a part
of the structured features, and therefore not par-
ticipating in the tree kernel computing.
</bodyText>
<subsectionHeader confidence="0.998809">
3.3 Linear Features
</subsectionHeader>
<bodyText confidence="0.999776">
In our study, we define the following lexicalized
linear features which are easily to be extracted
and integrated to our composite kernel:
</bodyText>
<listItem confidence="0.999551">
• Leftmost and rightmost boundary words of
the left and right source phrases
• Leftmost and rightmost boundary words of
the left and right target phrases
• Internal words of the four phrases (exclud-
ing boundary words)
• Target language model (LM) score differ-
ence (monotone-inverted)
</listItem>
<bodyText confidence="0.999986636363636">
In total, we arrive at 13 features, including 8
boundary word features, 4 (kinds of) internal
word features and 1 LM feature. The first 12 fea-
tures have been proven useful (Xiong et al.,
2006; Zhang et al., 2007a) to phrase reordering.
LM score is certainly a strong evidence for mod-
eling word orders and lexical selection. Although
it is already used as a standalone feature in the
log-linear model, we also would like to explicitly
re-optimize it together with other reordering fea-
tures in our reordering model.
</bodyText>
<page confidence="0.987891">
701
</page>
<subsectionHeader confidence="0.485974">
3.4 Tree Kernel, Composite Kernel and In-
</subsectionHeader>
<bodyText confidence="0.981319285714286">
tegrating into our Reordering Model
As discussed before, we use convolution tree
kernel to capture the structured syntactic feature
implicitly by directly computing similarity be-
tween the parse-tree representations of two reor-
dering instances with explicitly enumerating all
the features one by one. In convolution tree ker-
nel (Collins and Duffy, 2001), a parse tree T is
implicitly represented by a vector of integer
counts of each sub-tree type (regardless of its
ancestors):
φ(T) = (# subtree1(T), ..., # subtreen(T))
where # subtreei(T) is the occurrence number of
the ith sub-tree type (subtreei) in T. Since the
number of different sub-trees is exponential with
the parse tree size, it is computationally infeasi-
ble to directly use the feature vector φ(T) . To
solve this computational issue, Collins and Duffy
(2001) proposed the following parse tree kernel
to calculate the dot product between the above
high dimensional vectors implicitly.
</bodyText>
<equation confidence="0.995011125">
K T T
( , ) ( ), ( )
=&lt; φ φ
T T
1 2 1 2
∑ ∑
(( subtreei ) (
I ( )
n ⋅ ∑ I i ))
( )
n
i n N 1 n N subtree 2
1 1
∈ 2 2
∈
= ∑n1∈N1 ∑n2∈N2 Δ
</equation>
<bodyText confidence="0.980124">
where N1 and N2 are the sets of nodes in trees T1
and T2, respectively, and ( )
I subtreei n is a function
that is 1 iff the subtreei occurs with root at node n
and zero otherwise, and Δ(n1, n2) is the number of
the common subtrees rooted at n1 and n2, i.e.,
</bodyText>
<equation confidence="0.95511885">
Δ n n = ∑ I
( , ) ( 1 )
n ⋅I ( 2)
n
1 2 i subtree i subtreei
Δ(n1, n2) can be further computed efficiently by
the following recursive rules:
Rule 1: if the productions (CFG rules) at n1 and
n2 are different, Δ(n1, n2) = 0;
Rule 2: else if both n1 and n2 are pre-terminals
(POS tags), Δ(n1,n2)=1×λ;
Rule 3: else,
nc n
( 1)
Δ ( , )
n n λ =
= ∏ + Δ
(1 ( ( , ), ( , )))
ch n j ch n j ,
1 2 j 1 1 2
</equation>
<bodyText confidence="0.997407294117647">
where nc(n1) is the child number of n1, ch(n,j) is
the jth child of node n andλ (0&lt;λ &lt;1) is the de-
cay factor in order to make the kernel value less
variable with respect to the subtree sizes. In ad-
dition, the recursive Rule 3 holds because given
two nodes with the same children, one can con-
struct common sub-trees using these children and
common sub-trees of further offspring. The time
complexity for computing this kernel is
O( |N1  |⋅  |N2 |) and in practice in near to linear
computational time without the need of enume-
rating all subtree features.
In our study, the linear feature-based similarity
is simply calculated using dot-product. We then
define the following composite kernel to com-
bine the structured features-based and the linear
features-based similarities:
</bodyText>
<equation confidence="0.998005">
KC(xl,x2) = a · Kt(xl,x2) + (1 — a) · K1(x1,x2) (3)
</equation>
<bodyText confidence="0.999402142857143">
where Kt is the tree kernel over the structured
features and Kl is the linear kernel (dot-product)
over the linear features. The composite kernel Kc
is a linear combination of the two individual ker-
nels, where the coefficient α is set to its default
value 0.3 as that in Moschitti (2004)’s implemen-
tation. The kernels return the similarities be-
tween two reordering instances based on their
features used. Our basic assumption is, the more
similar the two reordering instances of x1 and x2
are, the more chance they share the same order.
Now let us see how to integrate the kernel
functions into SVM. The linear classifier learned
by SVM is formulized as:
</bodyText>
<equation confidence="0.829351">
fx = ∑ y a x • x + b
</equation>
<bodyText confidence="0.988228">
( ) sgn( i i i i ) (4)
where ai is the weight of a support vector xi (i.e.,
a support reordering instance xi = {A1,A2)in our
study), yi is its class label (1: straight or -
1: inverted in our study) and b is the intercept
of the hyperplane. An input reordering instance x
is classified as positive (negative) if f (x) &gt;0 (
f (x) &lt;0).
Based on the linear classifier, a kernelized
SVM can be easily implemented by simply re-
placing the dot product x ∗xi in Eq (4) with a
kernel function K(x, xi) . Thus, the kernelized
SVM classifier is formulated as:
</bodyText>
<equation confidence="0.843892">
f x = ∑ y a K x x + b
( ) sgn( i i i ( , i) ) (5)
</equation>
<bodyText confidence="0.9521385">
where K(x, xi) is either Kc (x, xi ) , Kt (x, xi ) or
Kl (x, xi) in our study. Following Eq (1), our
reordering model (implemented by the kerne-
lized SVM) can be formulized as follows:
</bodyText>
<equation confidence="0.9999455">
n = f(o,A1,A2) = p&amp;quot;m(o|x = {A1,A21)
= sgn(Zi(yiaiK(x,xi) + b)) (6)
</equation>
<bodyText confidence="0.95994575">
A reordering instance x is classified as straight
(or inverted) if psvm(o|x) &gt; 0 (or psvm(o|x) &lt;
0). Eq (6) and Eq (2) show the difference be-
tween our kernalized SVM-based reordering
</bodyText>
<equation confidence="0.851009">
&gt;
(n1, n2)
</equation>
<page confidence="0.97966">
702
</page>
<bodyText confidence="0.999945733333333">
model and the MaxEnt-based reordering model.
The main difference between them lies in that
our model is able to utilize structured syntactic
features by kernalized SVM while the previous
work can only use lexicalized word features by
MaxEnt-based classifier.
Finally, because the return value of
Psvm(o|x) is a distance function rather than a
probability, we use a sigmoid function to convert
Psvm(o|x) to a posterior probability as shown
using the following to functions and apply it as
one feature to the log-linear model in the decod-
ing.
where straight represents a positive instance and
inverted represents a negative instance.
</bodyText>
<sectionHeader confidence="0.997987" genericHeader="evaluation">
4 Experiments and Discussion
</sectionHeader>
<subsectionHeader confidence="0.966257">
4.1 Experimental Settings
</subsectionHeader>
<bodyText confidence="0.998723050632912">
Basic Settings: we evaluate our method on Chi-
nese-English translation task. We use the FBIS
corpus as training set, the NIST MT-2002 test set
as development (dev) set and the NIST MT-2005
test set as test set. The Stanford parser (Klein and
Manning, 2003) is used to parse Chinese sen-
tences on the training, dev and test sets. GIZA++
(Och and Ney, 2004) and the heuristics “grow-
diag-final-and” are used to generate m-to-n word
alignments. The translation model is trained on
the FBIS corpus and a 4-gram language model is
trained on the Xinhua portion of the English Gi-
gaword corpus using the SRILM Toolkits
(Stolcke, 2002) with modified Kneser-Ney
smoothing (Kenser and Ney, 1995). For the
MER training (Och, 2003), we modify Koehn’s
MER trainer (Koehn, 2004) to train our system.
For significance test, we use Zhang et al’s im-
plementation (Zhang et al, 2004).
Baseline Systems: we set three baseline sys-
tems: B1) Moses (Koehn et al., 2007) that uses
lexicalized unigram reordering model to predict
three orientations: monotone, swap and discon-
tinuous; B2) MaxEnt-based reordering model
with lexical boundary word features only (Xiong
et al., 2006); B3) Linguistically annotated reor-
dering model for BTG-based (LABTG) SMT
(Xiong et al., 2008). For Moses, we used the de-
fault settings. We build a CKY-style decoder and
integrate the corresponding reordering modelling
methods into the decoder to implement the 2nd
and the 3rd baseline systems and our system. Ex-
cept reordering models, all the four systems use
the same features in translation model, language
model and distortion model as Moses in the log-
linear framework. We tune the four systems us-
ing the strategies as discussed previously in this
section.
Reordering Model Training: we extract all
reordering instances from the m-to-n word-
aligned training corpus. The reordering instances
include the two source phrases, two target phras-
es, order label and its corresponding parse tree.
We generate the boundary word features from
the extracted reordering instances in the same
way as discussed in Xiong et al. (2006) and use
Zhang’s MaxEnt Tools 2 to train a reordering
model for the 2nd baseline system. Similarly, we
use the algorithm 1 in Xiong et al. (2008) to ex-
tract features and use the same MaxEnt Tools to
train a reordering model for the 3rd baseline sys-
tem. Based on the extracted reordering instances,
we generate the four structured features and the
linear features, and then use the Tree Kernel
Tools (Moschitti, 2004) to train our kernel-based
reordering model (linear, tree and composite).
Experimental Design and Evaluation Met-
rics: we design three experiments and evaluate
them using three metrics.
Classification-based: in the first experiment,
we extract all reordering instances and their fea-
tures from the dev and test sets, and then use the
reordering models trained on the training set to
classify (label) those instances extracted from the
dev and test sets. In this way, we can isolate the
reordering problem from the influence of others,
such as translation model, pruning and decoding
strategies, to better examine the reordering mod-
els’ ability and to give analytical insights into the
features. Classification Accuracy (CAcc), the
percentage of the correctly labeled instances over
all trials, is used as the evaluation metric.
Forced decoding3-based and normal decoding-
based: the two experiments evaluate the reorder-
ing models through a real SMT system. The
reordering model and the language model are the
same in the two experiments. However, in forced
decoding, we train two translation models, one
using training data only while another using both
</bodyText>
<footnote confidence="0.987336">
2 http://homepages.inf.ed.ac.uk/s0450736/maxent.html
3 A normal SMT decoder filters a translation model accord-
ing to the source sentences, whereas in forced decoding, a
translation model is filtered based on both source sentence
and target references. In other words, in forced decoding,
the decoder is forced to use those phrases whose translations
are already in the references.
</footnote>
<equation confidence="0.9898896">
1
(straight  |x) =
(inverted  |x) = 1
P
P
1+e− psvm
(o|x) and
(  |)
o x
1+epsvm
</equation>
<page confidence="0.992401">
703
</page>
<bodyText confidence="0.999786705882353">
training, dev and test data. By forced decoding,
we aim to isolate the reordering problem from
those of OOV and lexical selections resulting
from imperfect translation model in the context
of a real SMT task. Besides the the case-sensitive
BLEU-4 (Papineni et al., 2002) used in the two
experiments, we design another evaluation me-
trics Reordering Accuracy (RAcc) for forced de-
coding evaluation. RAcc is the percentage of the
adjacent word pairs with correct word order4
over all words in one-best translation results.
Similar to BLEU score, we also use the similar
Brevity Penalty BP (Papineni et al., 2002) to pe-
nalize the short translations in computing RAcc.
Finally, please note for the three evaluation me-
trics, the higher values represent better perfor-
mance.
</bodyText>
<table confidence="0.999840545454545">
Feature Spaces CAcc (%)
Dev Test
Minimum Sub-Tree (MST) 89.87 89.92
Minimum Sub-Structure (MSS) 87.95 87.88
Context-Sensitive MSS (CMSS) 89.11 89.01
Chunking Tree (CT) 86.17 86.21
Linear Features (Kl) 90.79 90.46
Kl w/o using LM feature (Kl-LM) 84.24 84.06
Composite Kernel (Kc: MST+Kl) 92.98 92.67
MST w/o the 5 function tags 86.94 87.03
All are straight (monotonic) 78.92 78.67
</table>
<tableCaption confidence="0.962982">
Table 1: Performance of our methods on the
dev and test sets with different feature combi-
nations
</tableCaption>
<subsectionHeader confidence="0.992778">
4.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999955454545455">
Classification of Instances: Table 1 reports the
performance of our defined four structured fea-
tures, linear feature and the composite kernel.
The results are summarized as follows.
The last row reports the performance without
using any reordering features. We just suppose
that all the translations are monotonic, no reor-
dering happens. The CAccs of 78.92% and 78.67%
serve as the bottom line in our study. Compared
with the bottom line, the tree kernels over the 4
structured features are very effective for phrase
</bodyText>
<footnote confidence="0.94823375">
4 An adjacent word pair wiwi+1 in a translation have correct
word order if and only if wi appears before wi+1 in transla-
tion references. Note than the two words may not be adja-
cent in the references even if they have correct word order.
</footnote>
<bodyText confidence="0.997978708333333">
reordering since only structured information is
used in the tree kernel5.
The CTs performs the worst among the 4
structured features. This suggests that the middle
and high-level structures beyond base phrases are
very useful for phrase reordering. The MSSs
show lower performance than the CMSSs and
the MSTs achieve the best performance. This
clearly indicates that the structured context in-
formation is useful for phrase reordering. For this
reason, the subsequent discussions are focused
on the MSTs, unless otherwise specified. The
MSSs without using the 5 function tags perform
much worse than the original ones. This suggests
that the partitions of the structured feature spaces
are very helpful, which can effectively avoid the
undesired matching between partitions of differ-
ent functionalities. Comparison of Kl and Kl-LM
shows the LM plays an important role in phrase
reordering. The composite kernel (Kc) performs
much better than the two individual kernels. This
suggests that the structured and linear features
are complementary and the composite kernel can
well integrate them for phrase reordering.
</bodyText>
<table confidence="0.997648875">
Methods CAcc (%)
Dev Test
Minimum Sub-Tree (MST) 89.87 89.92
Linear Features (Kl) 90.79 90.46
Composite Kernel (Kc: MST+Kl) 92.98 92.67
MaxEnt+boundary word (B2) 88.33 86.97
MaxEnt+linguistic features (B3_1) 84.83 83.92
MaxEnt+LABTG (B3: B2+ B3_1) 88.82 88.18
</table>
<tableCaption confidence="0.972601">
Table 2: Performance comparison of different me-
thods
</tableCaption>
<bodyText confidence="0.989558533333333">
Table 2 compares the performance of the base-
line methods with ours. Comparison between
B3_1 and MST clearly demonstrates that the
structured syntactic features are much more ef-
fective than the linear syntactic features that are
manually extracted via heuristics. It also suggests
that the tree kernel can well capture the struc-
tured features implicitly. Kl outperforms B2. This
is mainly due to the contribution of LM features.
B2 (MaxEnt-based) significantly outperforms Kl-
LM in Table 1 (SVM-based). This suggests that
phrase reordering may not be a good linearly bi-
nary-separable task if only boundary word fea-
tures are used. Our composite kernel (Kc) signifi-
cantly outperforms LABTG (B3). This mainly
</bodyText>
<footnote confidence="0.597517">
5 The tree kernel algorithm only compares internal struc-
tures. It does not concern any lexical leaf nodes.
</footnote>
<page confidence="0.996468">
704
</page>
<bodyText confidence="0.999617068965518">
attributes to the contributions of structured syn-
tactic features, LM and the tree kernel.
Forced Decoding: Table 3 compares the per-
formance of our composite kernel with that of
the LABTG (Baseline 3) in forced decoding. As
discussed before, here we try two translation
models.
The composite kernel outperforms the
LABTG in all test cases. This further validates
the effectiveness of the kernel methods in phrase
reordering. There are still around 30% words
reordered incorrectly even if we use the transla-
tion model trained on both training, dev and test
sets. This reveals the limitations of current SMT
modeling methods and suggests interesting fu-
ture work in this area. The source language
OOV6 rate in forced decoding (13.6%) is much
higher that in normal decoding (6.22%, see table
4). This is mainly due to the fact that the phrase
table in forced decoding is filtered out based on
both source and target languages while in normal
decoding it is based on source language only. As
a result, more phrases are filtered out in the
forced decoding. There is 1.4% OOV even if the
translation model is trained on the test set. This is
due to the incorrect word alignment, large-span
word alignment and different English tokeniza-
tion strategies used in BLEU-scoring tool and
ours.
</bodyText>
<table confidence="0.995045125">
Methods Test Set (%)
RAcc OOV BLEU
Composite Kernel (K,) 51.03 13.6 38.56
+translation model on 72.67 1.41 62.87
Training, dev and test
MaxEnt+LABTG (B3) 48.96 13.6 37.32
+translation model on 71.45 1.41 62.14
training, dev and test
</table>
<tableCaption confidence="0.941952">
Table 3: Performance comparison of forced de-
coding
</tableCaption>
<table confidence="0.9989715">
Methods Test Set
BLEU(%) OOV(%)
Composite Kernel (K,) 27.65 6.26
Moses (B1) 25.71 6.17
MaxEnt+boundary word(B2) 25.99 6.22
MaxEnt+LABTG (B3) 26.63 6.22
</table>
<tableCaption confidence="0.999218">
Table 4: Performance comparison
</tableCaption>
<footnote confidence="0.381919">
6 OOV means a source words has no any English translation
according to the translation model. OOV rate is the percent-
age of the number of OOV words over all the source words.
</footnote>
<bodyText confidence="0.997700939393939">
Normal Decoding/Translation: Table 4 reports
the translation performance of our system and
the three baseline systems.
Moses (B1) and the MaxEnt-based boundary
word model (B2) achieve comparable perfor-
mance. This means the lexicalized orientation-
based reordering model in Moses performs simi-
larly to the boundary word-based reordering
model since the two models are both lexical
word-based. However, theoretically, the Max-
Ent-based model may suffer less from data
sparseness issue since it does not depends on
internal phrasal words and uses MaxEnt to op-
timize feature weights while the orientation-
based model uses relative frequency of the entire
phrases to compute the posterior probabilities. s.
The MaxEnt-based LABTG model significantly
outperforms (p&lt;0.05) the MaxEnt-based boun-
dary word model and the lexicalized orientation-
based reordering model. This indicates that the
linearly linguistically syntactic information is a
useful feature to phrase reordering.
Our composite kernel-based model signifi-
cantly outperforms (p&lt;0.01) the three baseline
methods. This again proves that the structured
syntactic features are much more effective than
the linear syntactic features for phrase reordering
and the tree kernel method can well capture the
informative structured features. The four me-
thods show very slight difference in OOV rates.
This is mainly due to the difference in implemen-
tation detail, such as different OOV penalties and
other pruning thresholds.
</bodyText>
<sectionHeader confidence="0.992708" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999938105263158">
Structured syntactic knowledge is very useful to
phrase reordering. This paper provides insights
into how the structured feature can be used for
phrase reordering. In previous work, the struc-
tured features are selected manually by heuristics
and represented by a linear feature vector. This
may largely compromise the contribution of the
structured features to phrase reordering. Thanks
to the nice properties of kernel-based learning
method and SVM classifier, we propose leverag-
ing on the kernelized SVM learning algorithm to
address the problem. Specifically, we propose
using convolution tree kernel to capture the
structured features and design a composite kernel
to combine the structured features and other li-
near features for phrase reordering. The tree ker-
nel is able to directly take the structured reorder-
ing instances as inputs and compute their similar-
ities without enumerating them into a set of liner
</bodyText>
<page confidence="0.994448">
705
</page>
<bodyText confidence="0.999785962962963">
features. In addition, we also study how to find
the optimal structured feature space and how to
partition the structured feature spaces according
to their functionalities. Finally, we evaluate our
method on the NIST MT-2005 Chinese-English
translation tasks. To provide insights into the
model, we design three kinds of experiments to-
gether with three different evaluation metrics.
Experimental results show that the structured
features are very effective and our composite
kernel can well capture both the structured and
the linear features without the need for extensive
feature engineering. It also shows that our me-
thod significantly outperforms the baseline me-
thods.
The tree kernel-based phrase reordering me-
thod is not only applicable to adjacent phrases. It
is able to work with any long phrase pairs with
gap of any length in-between. We will study this
case in the near future. We would also like to use
one individual tree kernel for one partition in a
structured feature space. In doing so, we are able
to give different weights to different partitions
according to their functionalities and contribu-
tions. Note that these weights can be automati-
cally tuned and optimized easily against a dev
set.
</bodyText>
<sectionHeader confidence="0.997189" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999762962962963">
David Chiang. 2005. A hierarchical phrase-based
model for SMT. ACL-05. 263-270.
Michael Collins and N. Duffy. 2001. Convolution
Kernels for Natural Language. NIPS-2001.
M. R. Costa-jussà and J.A.R. Fonollosa. 2006. Statis-
tical Machine Reordering. EMNLP-06. 70-76.
Brooke Cowan, Ivona Kucerova and Michael Collins.
2006. A discriminative model for tree-to-tree trans-
lation. EMNLP-06. 232-241.
Jason Eisner. 2003. Learning non-isomorphic tree
mappings for MT. ACL-03 (companion volume).
Jakob Elming. 2008. Syntactic Reordering Integrated
with Phrase-Based SMT. COLING-08. 209-216.
Michel Galley and Christopher D. Manning. 2008. A
Simple and Effective Hierarchical Phrase Reorder-
ing Model. EMNLP-08. 848-856.
David Haussler. 1999. Convolution Kernels on Dis-
crete Structures. TR UCS-CRL-99-10.
T. Joachims. 1998. Text Categorization with SVM:
learning with many relevant features. ECML-98.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. ACL-03. 423-430.
Reinhard Kenser and Hermann Ney. 1995. Improved
backing-off for M-gram language modeling.
ICASSP-95, 181-184
Philipp Koehn, F. Och and D. Marcu. 2003. Statistical
phrase-based translation. HLT-NAACL-03.
Philipp Koehn, H. Hoang, A. Birch, C. C.-Burch, M.
Federico, N. Bertoldi, B. Cowan, W. Shen, C. Mo-
ran, R. Zens, C. Dyer, O. Bojar, A. Constantin and
E. Herbst. 2007. Moses: Open Source Toolkit for
SMT. ACL-07 (poster). 77-180.
Shankar Kumar and William Byrne. 2005. Local
Phrase Reordering Models for Statistical Machine
Translation. HLT-EMNLP-2005. 161-168.
Chi-Ho Li, Dongdong Zhang, Mu Li, Ming Zhou,
Minghui Li and Yi Guan. 2007. A Probabilistic
Approach to Syntax-based Reordering for Statis-
tical Machine Translation. ACL-07. 720-727.
Yang Liu, Yun Huang, Qun Liu and Shouxun Lin.
2007. Forest-to-String Statistical Translation
Rules. ACL-07. 704-711.
Daniel Marcu, W. Wang, A. Echihabi and K. Knight.
2006. SPMT: SMT with Syntactified Target Lan-
guage Phrases. EMNLP-06. 44-52.
Haitao Mi and Liang Huang. 2008. Forest-based
Translation Rule Extraction. EMNLP-08. 206-214.
Alessandro Moschitti. 2004. A Study on Convolution
Kernels for Shallow Semantic Parsing. ACL-04.
Masaaki Nagata, Kuniko Saito, Kazuhide Yamamoto
and Kazuteru Ohashi. 2006. A Clustered Global
Phrase Reordering Model for Statistical Machine
Translation. COLING-ACL-06. 713-720.
Franz J. Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statis-
tical machine translation. ACL-02. 295-302.
Franz J. Och. 2003. Minimum error rate training in
statistical machine translation. ACL-03. 160-167.
Franz J. Och and H. Ney. 2003. A Systematic Com-
parison of Various Statistical Alignment Methods.
Computational Linguistics, 29(1):20-51.
Franz J. Och and H. Ney. 2004. The alignment tem-
plate approach to statistical machine translation.
Computational Linguistics, 30(4):417-449.
Kishore Papineni, S. Roukos, T. and W. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. ACL-02. 311-318.
Hendra Setiawan, Min-Yen Kan and Haizhou Li.
2007. Ordering Phrases with Function Words.
ACL-07. 712-719.
Libin Shen, Jinxi Xu and Ralph Weischedel. 2008. A
New String-to-Dependency Machine Translation
Algorithm with a Target Dependency Language
Model. ACL-HLT-08. 577-585.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. ICSLP-02. 901-904.
Christoph Tillmann. 2004. A Unigram Orientation
Model for Statistical Machine Translation. HLT-
NAACL-04 (short paper).
Vladimir N. Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer.
</reference>
<page confidence="0.990781">
706
</page>
<reference confidence="0.980538578947369">
provement do we need to have a better system?
LREC-04. 2051-2054.
Chao Wang, M. Collins and P. Koehn. 2007. Chinese
Syntactic Reordering for Statistical Machine
Translation. EMNLP-CONLL-07. 734-745.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpo-
ra. Computational Linguistics, 23(3):377-403.
Fei Xia and Michael McCord. 2004. Improving a Sta-
tistical MT System with Automatically Learned
Rewrite Patterns. COLING-04.
Deyi Xiong, Qun Liu and Shouxun Lin. 2006. Maxi-
mum Entropy Based Phrase Reordering Model for
SMT. COLING-ACL-06. 521–528.
Deyi Xiong, Min Zhang, Aiti Aw and Haizhou Li.
2008. A Linguistically Annotated Reordering Mod-
el for BTG-based Statistical Machine Translation.
ACL-HLT-08 (short paper). 149-152.
Kenji Yamada and K. Knight. 2001. A syntax-based
statistical translation model. ACL-01. 523-530.
Xiaofeng Yang, Jian Su and Chew Lim Tan. 2006.
Kernel-Based Pronoun Resolution with Structured
Syntactic Knowledge. COLING-ACL-06. 41-48.
Richard Zens, H. Ney, T. Watanabe and E. Sumita.
2004. Reordering Constraints for Phrase-Based
Statistical Machine Translation. COLING-04.
Richard Zens and Hermann Ney. 2006. Discrimina-
tive Reordering Models for Statistical Machine
Translation. WSMT-2006.
Dell Zhang and W. Lee. 2003. Question classification
using support vector machines. SIGIR-03.
Min Zhang, Jie Zhang, Jian Su and GuoDong Zhou.
2006. A Composite Kernel to Extract Relations be-
tween Entities with Both Flat and Structured Fea-
tures. COLING-ACL-06. 825-832.
Dongdong Zhang, M. Li, C.H. Li and M. Zhou.
2007a. Phrase Reordering Model Integrating Syn-
tactic Knowledge for SMT. EMNLP-CONLL-07.
533-540.
Min Zhang, W. Che, A. Aw, C. Tan, G. Zhou, T. Liu
and S. Li. 2007b. A Grammar-driven Convolution
Tree Kernel for Semantic Role Classification.
ACL-07. 200-207.
Min Zhang, Hongfei Jiang, Ai Ti Aw, Jun Sun, Sheng
Li and Chew Lim Tan. 2007c. A Tree-to-Tree
Alignment-based Model for Statistical Machine
Translation.MT-Summit-07. 535-542
Min Zhang, Hongfei Jiang, Ai Ti Aw, Haizhou Li,
Chew Lim Tan and Chew Lim Tan and Sheng Li.
2008a. A Tree Sequence Alignment-based Tree-to-
Tree Translation Model. ACL-HLT-08. 559-567.
Min Zhang, Hongfei Jiang, Haizhou Li, Aiti Aw,
Sheng Li. 2008b. Grammar Comparison Study for
Translational Equivalence Modeling and Statistic-
al Machine Translation. COLING-08. 1097-1104
Ying Zhang, Stephan Vogel and Alex Waibel. 2004.
Interpreting BLEU/NIST scores: How much im-
</reference>
<page confidence="0.997399">
707
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.454519">
<title confidence="0.9992905">Tree Kernel-based SVM with Structured Syntactic Knowledge for BTG-based Phrase Reordering</title>
<author confidence="0.99958">Min Zhang Haizhou Li</author>
<affiliation confidence="0.772026">Institute for Infocomm Research 1 Fusionopolis Way,#21-01 Connexis (South</affiliation>
<address confidence="0.660663">Singapore 138632</address>
<email confidence="0.753446">mzhang@i2r.a-star.edu.sg</email>
<email confidence="0.753446">hli@i2r.a-star.edu.sg</email>
<abstract confidence="0.998214333333334">Structured syntactic knowledge is important for phrase reordering. This paper proposes using convolution tree kernel over source parse tree to model structured syntactic knowledge for BTG-based phrase reordering in the context of statistical machine translation. Our study reveals that the structured syntactic features over the source phrases are very effective for BTG constraint-based phrase reordering and those features can be well captured by the tree kernel. We further combine the structured features and other commonly-used linear features into a composite kernel. Experimental results on the NIST MT-2005 Chinese-English translation tasks show that our proposed phrase reordering model statistically significantly outperforms the baseline methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for SMT.</title>
<date>2005</date>
<booktitle>ACL-05.</booktitle>
<pages>263--270</pages>
<contexts>
<context position="1187" citStr="Chiang, 2005" startWordPosition="167" endWordPosition="168">he source phrases are very effective for BTG constraint-based phrase reordering and those features can be well captured by the tree kernel. We further combine the structured features and other commonly-used linear features into a composite kernel. Experimental results on the NIST MT-2005 Chinese-English translation tasks show that our proposed phrase reordering model statistically significantly outperforms the baseline methods. 1 Introduction Phrase-based method (Koehn et al., 2003; Och and Ney, 2004; Koehn et al., 2007) and syntaxbased method (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Chiang, 2005; Cowan et al., 2006; Marcu et al., 2006; Liu et al., 2007; Zhang et al., 2007c, 2008a, 2008b; Shen et al., 2008; Mi and Huang, 2008) represent the state-of-the-art technologies in statistical machine translation (SMT). As the two technologies are complementary in many ways, an interesting research topic is how to combine the strengths of the two methods. Many research efforts have been made to address this issue, which can be summarized into two ideas. One is to add syntax into phrase-based model while another one is to enhance syntaxbased model to handle non-syntactic phrases. In this paper,</context>
<context position="2860" citStr="Chiang, 2005" startWordPosition="435" endWordPosition="436">a) and the impact of syntax on reordering is difficult to single out (Li et al., 2007). In phrasebased method, local word reordering1 can be effectively captured by phrase pairs directly while local phrase reordering is explicitly modeled by phrase reordering model and distortion model. Recently, many phrase reordering methods have been proposed, ranging from simple distancebased distortion model (Koehn et al., 2003; Och and Ney, 2004), flat reordering model (Wu, 1997; Zens et al., 2004), lexicalized reordering model (Tillmann, 2004; Kumar and Byrne, 2005), to hierarchical phrase-based model (Chiang, 2005; Setiawan et al., 2007) and classifier-based reordering model with linear features (Zens and Ney, 2006; Xiong et al., 2006; Zhang et al., 2007a; Xiong et al., 2008). However, one of the major limitations of these advances is the structured syntactic knowledge, which is important to global reordering (Li et al., 2007; Elming, 2008), has not been well exploited. This makes the phrasebased method particularly weak in handling global phrase reordering. From machine learning viewpoint (Vapnik, 1995), it is computationally infeasible to explicitly generate features involving structured information </context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for SMT. ACL-05. 263-270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>N Duffy</author>
</authors>
<title>Convolution Kernels for Natural Language.</title>
<date>2001</date>
<contexts>
<context position="4367" citStr="Collins and Duffy, 2001" startWordPosition="666" endWordPosition="669">nce on Empirical Methods in Natural Language Processing, pages 698–707, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP tions. For example, one cannot enumerate efficiently all the sub-tree features for a full parse tree. This would be the reason why structured features are not fully utilized in previous statistical feature-based phrase reordering model. Thanks to the nice property of kernel-based machine learning method that can implicitly explore (structured) features in a high dimensional feature space (Vapnik, 1995), in this paper we propose using convolution tree kernel (Haussler, 1999; Collins and Duffy, 2001) to explore the structured syntactic knowledge for phrase reordering and further combine the tree kernel with other diverse linear features into a composite kernel to strengthen the model’s predictive ability. Indeed, using tree kernel methods to mine structured knowledge has shown success in some NLP applications like parsing (Collins and Duffy, 2001), semantic role labeling (Moschitti, 2004; Zhang et al., 2007b), relation extraction (Zhang et al., 2006), pronoun resolution (Yang et al., 2006) and question classification (Zhang and Lee, 2003). However, to our knowledge, such technique still r</context>
<context position="15670" citStr="Collins and Duffy, 2001" startWordPosition="2513" endWordPosition="2516">ng word orders and lexical selection. Although it is already used as a standalone feature in the log-linear model, we also would like to explicitly re-optimize it together with other reordering features in our reordering model. 701 3.4 Tree Kernel, Composite Kernel and Integrating into our Reordering Model As discussed before, we use convolution tree kernel to capture the structured syntactic feature implicitly by directly computing similarity between the parse-tree representations of two reordering instances with explicitly enumerating all the features one by one. In convolution tree kernel (Collins and Duffy, 2001), a parse tree T is implicitly represented by a vector of integer counts of each sub-tree type (regardless of its ancestors): φ(T) = (# subtree1(T), ..., # subtreen(T)) where # subtreei(T) is the occurrence number of the ith sub-tree type (subtreei) in T. Since the number of different sub-trees is exponential with the parse tree size, it is computationally infeasible to directly use the feature vector φ(T) . To solve this computational issue, Collins and Duffy (2001) proposed the following parse tree kernel to calculate the dot product between the above high dimensional vectors implicitly. K T</context>
</contexts>
<marker>Collins, Duffy, 2001</marker>
<rawString>Michael Collins and N. Duffy. 2001. Convolution Kernels for Natural Language. NIPS-2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M R Costa-jussà</author>
<author>J A R Fonollosa</author>
</authors>
<date>2006</date>
<booktitle>Statistical Machine Reordering. EMNLP-06.</booktitle>
<pages>70--76</pages>
<marker>Costa-jussà, Fonollosa, 2006</marker>
<rawString>M. R. Costa-jussà and J.A.R. Fonollosa. 2006. Statistical Machine Reordering. EMNLP-06. 70-76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brooke Cowan</author>
</authors>
<title>Ivona Kucerova and Michael Collins.</title>
<date>2006</date>
<pages>06--232</pages>
<marker>Cowan, 2006</marker>
<rawString>Brooke Cowan, Ivona Kucerova and Michael Collins. 2006. A discriminative model for tree-to-tree translation. EMNLP-06. 232-241.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Learning non-isomorphic tree mappings for MT.</title>
<date>2003</date>
<note>ACL-03 (companion volume).</note>
<contexts>
<context position="1173" citStr="Eisner, 2003" startWordPosition="165" endWordPosition="166">eatures over the source phrases are very effective for BTG constraint-based phrase reordering and those features can be well captured by the tree kernel. We further combine the structured features and other commonly-used linear features into a composite kernel. Experimental results on the NIST MT-2005 Chinese-English translation tasks show that our proposed phrase reordering model statistically significantly outperforms the baseline methods. 1 Introduction Phrase-based method (Koehn et al., 2003; Och and Ney, 2004; Koehn et al., 2007) and syntaxbased method (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Chiang, 2005; Cowan et al., 2006; Marcu et al., 2006; Liu et al., 2007; Zhang et al., 2007c, 2008a, 2008b; Shen et al., 2008; Mi and Huang, 2008) represent the state-of-the-art technologies in statistical machine translation (SMT). As the two technologies are complementary in many ways, an interesting research topic is how to combine the strengths of the two methods. Many research efforts have been made to address this issue, which can be summarized into two ideas. One is to add syntax into phrase-based model while another one is to enhance syntaxbased model to handle non-syntactic phrases. </context>
</contexts>
<marker>Eisner, 2003</marker>
<rawString>Jason Eisner. 2003. Learning non-isomorphic tree mappings for MT. ACL-03 (companion volume).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jakob Elming</author>
</authors>
<date>2008</date>
<booktitle>Syntactic Reordering Integrated with Phrase-Based SMT. COLING-08.</booktitle>
<pages>209--216</pages>
<contexts>
<context position="3193" citStr="Elming, 2008" startWordPosition="490" endWordPosition="491">e been proposed, ranging from simple distancebased distortion model (Koehn et al., 2003; Och and Ney, 2004), flat reordering model (Wu, 1997; Zens et al., 2004), lexicalized reordering model (Tillmann, 2004; Kumar and Byrne, 2005), to hierarchical phrase-based model (Chiang, 2005; Setiawan et al., 2007) and classifier-based reordering model with linear features (Zens and Ney, 2006; Xiong et al., 2006; Zhang et al., 2007a; Xiong et al., 2008). However, one of the major limitations of these advances is the structured syntactic knowledge, which is important to global reordering (Li et al., 2007; Elming, 2008), has not been well exploited. This makes the phrasebased method particularly weak in handling global phrase reordering. From machine learning viewpoint (Vapnik, 1995), it is computationally infeasible to explicitly generate features involving structured information in many NLP applica1 This paper follows the term convention of global reordering and local reordering of Li et al. (2007), between which the distinction is solely defined by reordering distance (whether beyond four source words) (Li et al., 2007). 698 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Proce</context>
</contexts>
<marker>Elming, 2008</marker>
<rawString>Jakob Elming. 2008. Syntactic Reordering Integrated with Phrase-Based SMT. COLING-08. 209-216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>A Simple and Effective Hierarchical Phrase Reordering Model.</title>
<date>2008</date>
<volume>08</volume>
<pages>848--856</pages>
<marker>Galley, Manning, 2008</marker>
<rawString>Michel Galley and Christopher D. Manning. 2008. A Simple and Effective Hierarchical Phrase Reordering Model. EMNLP-08. 848-856.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Haussler</author>
</authors>
<title>Convolution Kernels on Discrete Structures.</title>
<date>1999</date>
<tech>TR</tech>
<pages>99--10</pages>
<contexts>
<context position="4341" citStr="Haussler, 1999" startWordPosition="664" endWordPosition="665">the 2009 Conference on Empirical Methods in Natural Language Processing, pages 698–707, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP tions. For example, one cannot enumerate efficiently all the sub-tree features for a full parse tree. This would be the reason why structured features are not fully utilized in previous statistical feature-based phrase reordering model. Thanks to the nice property of kernel-based machine learning method that can implicitly explore (structured) features in a high dimensional feature space (Vapnik, 1995), in this paper we propose using convolution tree kernel (Haussler, 1999; Collins and Duffy, 2001) to explore the structured syntactic knowledge for phrase reordering and further combine the tree kernel with other diverse linear features into a composite kernel to strengthen the model’s predictive ability. Indeed, using tree kernel methods to mine structured knowledge has shown success in some NLP applications like parsing (Collins and Duffy, 2001), semantic role labeling (Moschitti, 2004; Zhang et al., 2007b), relation extraction (Zhang et al., 2006), pronoun resolution (Yang et al., 2006) and question classification (Zhang and Lee, 2003). However, to our knowled</context>
</contexts>
<marker>Haussler, 1999</marker>
<rawString>David Haussler. 1999. Convolution Kernels on Discrete Structures. TR UCS-CRL-99-10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Text Categorization with SVM: learning with many relevant features.</title>
<date>1998</date>
<pages>98</pages>
<marker>Joachims, 1998</marker>
<rawString>T. Joachims. 1998. Text Categorization with SVM: learning with many relevant features. ECML-98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<date>2003</date>
<booktitle>Accurate Unlexicalized Parsing. ACL-03.</booktitle>
<pages>423--430</pages>
<contexts>
<context position="20548" citStr="Klein and Manning, 2003" startWordPosition="3456" endWordPosition="3459">e function rather than a probability, we use a sigmoid function to convert Psvm(o|x) to a posterior probability as shown using the following to functions and apply it as one feature to the log-linear model in the decoding. where straight represents a positive instance and inverted represents a negative instance. 4 Experiments and Discussion 4.1 Experimental Settings Basic Settings: we evaluate our method on Chinese-English translation task. We use the FBIS corpus as training set, the NIST MT-2002 test set as development (dev) set and the NIST MT-2005 test set as test set. The Stanford parser (Klein and Manning, 2003) is used to parse Chinese sentences on the training, dev and test sets. GIZA++ (Och and Ney, 2004) and the heuristics “growdiag-final-and” are used to generate m-to-n word alignments. The translation model is trained on the FBIS corpus and a 4-gram language model is trained on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing (Kenser and Ney, 1995). For the MER training (Och, 2003), we modify Koehn’s MER trainer (Koehn, 2004) to train our system. For significance test, we use Zhang et al’s implementation (Zhang et al, </context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate Unlexicalized Parsing. ACL-03. 423-430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kenser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved backing-off for M-gram language modeling.</title>
<date>1995</date>
<tech>ICASSP-95,</tech>
<pages>181--184</pages>
<contexts>
<context position="20974" citStr="Kenser and Ney, 1995" startWordPosition="3527" endWordPosition="3530">anslation task. We use the FBIS corpus as training set, the NIST MT-2002 test set as development (dev) set and the NIST MT-2005 test set as test set. The Stanford parser (Klein and Manning, 2003) is used to parse Chinese sentences on the training, dev and test sets. GIZA++ (Och and Ney, 2004) and the heuristics “growdiag-final-and” are used to generate m-to-n word alignments. The translation model is trained on the FBIS corpus and a 4-gram language model is trained on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing (Kenser and Ney, 1995). For the MER training (Och, 2003), we modify Koehn’s MER trainer (Koehn, 2004) to train our system. For significance test, we use Zhang et al’s implementation (Zhang et al, 2004). Baseline Systems: we set three baseline systems: B1) Moses (Koehn et al., 2007) that uses lexicalized unigram reordering model to predict three orientations: monotone, swap and discontinuous; B2) MaxEnt-based reordering model with lexical boundary word features only (Xiong et al., 2006); B3) Linguistically annotated reordering model for BTG-based (LABTG) SMT (Xiong et al., 2008). For Moses, we used the default setti</context>
</contexts>
<marker>Kenser, Ney, 1995</marker>
<rawString>Reinhard Kenser and Hermann Ney. 1995. Improved backing-off for M-gram language modeling. ICASSP-95, 181-184</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>F Och</author>
<author>D Marcu</author>
</authors>
<date>2003</date>
<note>Statistical phrase-based translation. HLT-NAACL-03.</note>
<contexts>
<context position="1061" citStr="Koehn et al., 2003" startWordPosition="143" endWordPosition="146">phrase reordering in the context of statistical machine translation. Our study reveals that the structured syntactic features over the source phrases are very effective for BTG constraint-based phrase reordering and those features can be well captured by the tree kernel. We further combine the structured features and other commonly-used linear features into a composite kernel. Experimental results on the NIST MT-2005 Chinese-English translation tasks show that our proposed phrase reordering model statistically significantly outperforms the baseline methods. 1 Introduction Phrase-based method (Koehn et al., 2003; Och and Ney, 2004; Koehn et al., 2007) and syntaxbased method (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Chiang, 2005; Cowan et al., 2006; Marcu et al., 2006; Liu et al., 2007; Zhang et al., 2007c, 2008a, 2008b; Shen et al., 2008; Mi and Huang, 2008) represent the state-of-the-art technologies in statistical machine translation (SMT). As the two technologies are complementary in many ways, an interesting research topic is how to combine the strengths of the two methods. Many research efforts have been made to address this issue, which can be summarized into two ideas. One is to add sy</context>
<context position="2667" citStr="Koehn et al., 2003" startWordPosition="405" endWordPosition="408">omponent in a SMT system. In syntax-based method, word reordering is implicitly addressed by translation rules, thus the performance is subject to parsing errors to a large extent (zhang et al., 2007a) and the impact of syntax on reordering is difficult to single out (Li et al., 2007). In phrasebased method, local word reordering1 can be effectively captured by phrase pairs directly while local phrase reordering is explicitly modeled by phrase reordering model and distortion model. Recently, many phrase reordering methods have been proposed, ranging from simple distancebased distortion model (Koehn et al., 2003; Och and Ney, 2004), flat reordering model (Wu, 1997; Zens et al., 2004), lexicalized reordering model (Tillmann, 2004; Kumar and Byrne, 2005), to hierarchical phrase-based model (Chiang, 2005; Setiawan et al., 2007) and classifier-based reordering model with linear features (Zens and Ney, 2006; Xiong et al., 2006; Zhang et al., 2007a; Xiong et al., 2008). However, one of the major limitations of these advances is the structured syntactic knowledge, which is important to global reordering (Li et al., 2007; Elming, 2008), has not been well exploited. This makes the phrasebased method particula</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, F. Och and D. Marcu. 2003. Statistical phrase-based translation. HLT-NAACL-03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C C-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
<author>C Dyer</author>
<author>O Bojar</author>
<author>A Constantin</author>
<author>E Herbst</author>
</authors>
<title>Moses: Open Source Toolkit for SMT.</title>
<date>2007</date>
<pages>07--77</pages>
<contexts>
<context position="1101" citStr="Koehn et al., 2007" startWordPosition="151" endWordPosition="154">tistical machine translation. Our study reveals that the structured syntactic features over the source phrases are very effective for BTG constraint-based phrase reordering and those features can be well captured by the tree kernel. We further combine the structured features and other commonly-used linear features into a composite kernel. Experimental results on the NIST MT-2005 Chinese-English translation tasks show that our proposed phrase reordering model statistically significantly outperforms the baseline methods. 1 Introduction Phrase-based method (Koehn et al., 2003; Och and Ney, 2004; Koehn et al., 2007) and syntaxbased method (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Chiang, 2005; Cowan et al., 2006; Marcu et al., 2006; Liu et al., 2007; Zhang et al., 2007c, 2008a, 2008b; Shen et al., 2008; Mi and Huang, 2008) represent the state-of-the-art technologies in statistical machine translation (SMT). As the two technologies are complementary in many ways, an interesting research topic is how to combine the strengths of the two methods. Many research efforts have been made to address this issue, which can be summarized into two ideas. One is to add syntax into phrase-based model while anoth</context>
<context position="21234" citStr="Koehn et al., 2007" startWordPosition="3572" endWordPosition="3575">ets. GIZA++ (Och and Ney, 2004) and the heuristics “growdiag-final-and” are used to generate m-to-n word alignments. The translation model is trained on the FBIS corpus and a 4-gram language model is trained on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing (Kenser and Ney, 1995). For the MER training (Och, 2003), we modify Koehn’s MER trainer (Koehn, 2004) to train our system. For significance test, we use Zhang et al’s implementation (Zhang et al, 2004). Baseline Systems: we set three baseline systems: B1) Moses (Koehn et al., 2007) that uses lexicalized unigram reordering model to predict three orientations: monotone, swap and discontinuous; B2) MaxEnt-based reordering model with lexical boundary word features only (Xiong et al., 2006); B3) Linguistically annotated reordering model for BTG-based (LABTG) SMT (Xiong et al., 2008). For Moses, we used the default settings. We build a CKY-style decoder and integrate the corresponding reordering modelling methods into the decoder to implement the 2nd and the 3rd baseline systems and our system. Except reordering models, all the four systems use the same features in translatio</context>
</contexts>
<marker>Koehn, Hoang, Birch, C-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Philipp Koehn, H. Hoang, A. Birch, C. C.-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin and E. Herbst. 2007. Moses: Open Source Toolkit for SMT. ACL-07 (poster). 77-180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>William Byrne</author>
</authors>
<title>Local Phrase Reordering Models for Statistical Machine Translation.</title>
<date>2005</date>
<volume>2005</volume>
<pages>161--168</pages>
<contexts>
<context position="2810" citStr="Kumar and Byrne, 2005" startWordPosition="427" endWordPosition="430">ject to parsing errors to a large extent (zhang et al., 2007a) and the impact of syntax on reordering is difficult to single out (Li et al., 2007). In phrasebased method, local word reordering1 can be effectively captured by phrase pairs directly while local phrase reordering is explicitly modeled by phrase reordering model and distortion model. Recently, many phrase reordering methods have been proposed, ranging from simple distancebased distortion model (Koehn et al., 2003; Och and Ney, 2004), flat reordering model (Wu, 1997; Zens et al., 2004), lexicalized reordering model (Tillmann, 2004; Kumar and Byrne, 2005), to hierarchical phrase-based model (Chiang, 2005; Setiawan et al., 2007) and classifier-based reordering model with linear features (Zens and Ney, 2006; Xiong et al., 2006; Zhang et al., 2007a; Xiong et al., 2008). However, one of the major limitations of these advances is the structured syntactic knowledge, which is important to global reordering (Li et al., 2007; Elming, 2008), has not been well exploited. This makes the phrasebased method particularly weak in handling global phrase reordering. From machine learning viewpoint (Vapnik, 1995), it is computationally infeasible to explicitly g</context>
</contexts>
<marker>Kumar, Byrne, 2005</marker>
<rawString>Shankar Kumar and William Byrne. 2005. Local Phrase Reordering Models for Statistical Machine Translation. HLT-EMNLP-2005. 161-168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chi-Ho Li</author>
<author>Dongdong Zhang</author>
<author>Mu Li</author>
<author>Ming Zhou</author>
<author>Minghui Li</author>
<author>Yi Guan</author>
</authors>
<title>A Probabilistic Approach to Syntax-based Reordering for Statistical Machine Translation.</title>
<date>2007</date>
<volume>07</volume>
<pages>720--727</pages>
<contexts>
<context position="2334" citStr="Li et al., 2007" startWordPosition="356" endWordPosition="359">nce syntaxbased model to handle non-syntactic phrases. In this paper, we bring forward the first idea by studying the issue of how to utilize structured syntactic features for phrase reordering in a phrase-based SMT system with BTG (Bracketing Transduction Grammar) constraints (Wu, 1997). Word and phrase reordering is a crucial component in a SMT system. In syntax-based method, word reordering is implicitly addressed by translation rules, thus the performance is subject to parsing errors to a large extent (zhang et al., 2007a) and the impact of syntax on reordering is difficult to single out (Li et al., 2007). In phrasebased method, local word reordering1 can be effectively captured by phrase pairs directly while local phrase reordering is explicitly modeled by phrase reordering model and distortion model. Recently, many phrase reordering methods have been proposed, ranging from simple distancebased distortion model (Koehn et al., 2003; Och and Ney, 2004), flat reordering model (Wu, 1997; Zens et al., 2004), lexicalized reordering model (Tillmann, 2004; Kumar and Byrne, 2005), to hierarchical phrase-based model (Chiang, 2005; Setiawan et al., 2007) and classifier-based reordering model with linear</context>
<context position="3581" citStr="Li et al. (2007)" startWordPosition="547" endWordPosition="550">Xiong et al., 2006; Zhang et al., 2007a; Xiong et al., 2008). However, one of the major limitations of these advances is the structured syntactic knowledge, which is important to global reordering (Li et al., 2007; Elming, 2008), has not been well exploited. This makes the phrasebased method particularly weak in handling global phrase reordering. From machine learning viewpoint (Vapnik, 1995), it is computationally infeasible to explicitly generate features involving structured information in many NLP applica1 This paper follows the term convention of global reordering and local reordering of Li et al. (2007), between which the distinction is solely defined by reordering distance (whether beyond four source words) (Li et al., 2007). 698 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 698–707, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP tions. For example, one cannot enumerate efficiently all the sub-tree features for a full parse tree. This would be the reason why structured features are not fully utilized in previous statistical feature-based phrase reordering model. Thanks to the nice property of kernel-based machine learning method that can imp</context>
</contexts>
<marker>Li, Zhang, Li, Zhou, Li, Guan, 2007</marker>
<rawString>Chi-Ho Li, Dongdong Zhang, Mu Li, Ming Zhou, Minghui Li and Yi Guan. 2007. A Probabilistic Approach to Syntax-based Reordering for Statistical Machine Translation. ACL-07. 720-727.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Yun Huang</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Forest-to-String Statistical Translation Rules.</title>
<date>2007</date>
<volume>07</volume>
<pages>704--711</pages>
<contexts>
<context position="1245" citStr="Liu et al., 2007" startWordPosition="177" endWordPosition="180">t-based phrase reordering and those features can be well captured by the tree kernel. We further combine the structured features and other commonly-used linear features into a composite kernel. Experimental results on the NIST MT-2005 Chinese-English translation tasks show that our proposed phrase reordering model statistically significantly outperforms the baseline methods. 1 Introduction Phrase-based method (Koehn et al., 2003; Och and Ney, 2004; Koehn et al., 2007) and syntaxbased method (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Chiang, 2005; Cowan et al., 2006; Marcu et al., 2006; Liu et al., 2007; Zhang et al., 2007c, 2008a, 2008b; Shen et al., 2008; Mi and Huang, 2008) represent the state-of-the-art technologies in statistical machine translation (SMT). As the two technologies are complementary in many ways, an interesting research topic is how to combine the strengths of the two methods. Many research efforts have been made to address this issue, which can be summarized into two ideas. One is to add syntax into phrase-based model while another one is to enhance syntaxbased model to handle non-syntactic phrases. In this paper, we bring forward the first idea by studying the issue of </context>
</contexts>
<marker>Liu, Huang, Liu, Lin, 2007</marker>
<rawString>Yang Liu, Yun Huang, Qun Liu and Shouxun Lin. 2007. Forest-to-String Statistical Translation Rules. ACL-07. 704-711.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>W Wang</author>
<author>A Echihabi</author>
<author>K Knight</author>
</authors>
<title>SPMT: SMT with Syntactified Target Language Phrases.</title>
<date>2006</date>
<volume>06</volume>
<pages>44--52</pages>
<contexts>
<context position="1227" citStr="Marcu et al., 2006" startWordPosition="173" endWordPosition="176">ve for BTG constraint-based phrase reordering and those features can be well captured by the tree kernel. We further combine the structured features and other commonly-used linear features into a composite kernel. Experimental results on the NIST MT-2005 Chinese-English translation tasks show that our proposed phrase reordering model statistically significantly outperforms the baseline methods. 1 Introduction Phrase-based method (Koehn et al., 2003; Och and Ney, 2004; Koehn et al., 2007) and syntaxbased method (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Chiang, 2005; Cowan et al., 2006; Marcu et al., 2006; Liu et al., 2007; Zhang et al., 2007c, 2008a, 2008b; Shen et al., 2008; Mi and Huang, 2008) represent the state-of-the-art technologies in statistical machine translation (SMT). As the two technologies are complementary in many ways, an interesting research topic is how to combine the strengths of the two methods. Many research efforts have been made to address this issue, which can be summarized into two ideas. One is to add syntax into phrase-based model while another one is to enhance syntaxbased model to handle non-syntactic phrases. In this paper, we bring forward the first idea by stud</context>
</contexts>
<marker>Marcu, Wang, Echihabi, Knight, 2006</marker>
<rawString>Daniel Marcu, W. Wang, A. Echihabi and K. Knight. 2006. SPMT: SMT with Syntactified Target Language Phrases. EMNLP-06. 44-52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Liang Huang</author>
</authors>
<title>Forest-based Translation Rule Extraction.</title>
<date>2008</date>
<volume>08</volume>
<pages>206--214</pages>
<contexts>
<context position="1320" citStr="Mi and Huang, 2008" startWordPosition="191" endWordPosition="194"> tree kernel. We further combine the structured features and other commonly-used linear features into a composite kernel. Experimental results on the NIST MT-2005 Chinese-English translation tasks show that our proposed phrase reordering model statistically significantly outperforms the baseline methods. 1 Introduction Phrase-based method (Koehn et al., 2003; Och and Ney, 2004; Koehn et al., 2007) and syntaxbased method (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Chiang, 2005; Cowan et al., 2006; Marcu et al., 2006; Liu et al., 2007; Zhang et al., 2007c, 2008a, 2008b; Shen et al., 2008; Mi and Huang, 2008) represent the state-of-the-art technologies in statistical machine translation (SMT). As the two technologies are complementary in many ways, an interesting research topic is how to combine the strengths of the two methods. Many research efforts have been made to address this issue, which can be summarized into two ideas. One is to add syntax into phrase-based model while another one is to enhance syntaxbased model to handle non-syntactic phrases. In this paper, we bring forward the first idea by studying the issue of how to utilize structured syntactic features for phrase reordering in a phr</context>
</contexts>
<marker>Mi, Huang, 2008</marker>
<rawString>Haitao Mi and Liang Huang. 2008. Forest-based Translation Rule Extraction. EMNLP-08. 206-214.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>A Study on Convolution Kernels for Shallow Semantic Parsing.</title>
<date>2004</date>
<contexts>
<context position="4762" citStr="Moschitti, 2004" startWordPosition="727" endWordPosition="728">chine learning method that can implicitly explore (structured) features in a high dimensional feature space (Vapnik, 1995), in this paper we propose using convolution tree kernel (Haussler, 1999; Collins and Duffy, 2001) to explore the structured syntactic knowledge for phrase reordering and further combine the tree kernel with other diverse linear features into a composite kernel to strengthen the model’s predictive ability. Indeed, using tree kernel methods to mine structured knowledge has shown success in some NLP applications like parsing (Collins and Duffy, 2001), semantic role labeling (Moschitti, 2004; Zhang et al., 2007b), relation extraction (Zhang et al., 2006), pronoun resolution (Yang et al., 2006) and question classification (Zhang and Lee, 2003). However, to our knowledge, such technique still remains unexplored for phrase reordering. In this paper, we look into the phrase reordering problem in two aspects: 1) how to model and optimize structured features, and 2) how to combine the structured features with other linear features and further integrate them into the loglinear model-based translation framework. Our study shows that: 1) the structured syntactic features are very useful a</context>
<context position="18169" citStr="Moschitti (2004)" startWordPosition="3020" endWordPosition="3021">e without the need of enumerating all subtree features. In our study, the linear feature-based similarity is simply calculated using dot-product. We then define the following composite kernel to combine the structured features-based and the linear features-based similarities: KC(xl,x2) = a · Kt(xl,x2) + (1 — a) · K1(x1,x2) (3) where Kt is the tree kernel over the structured features and Kl is the linear kernel (dot-product) over the linear features. The composite kernel Kc is a linear combination of the two individual kernels, where the coefficient α is set to its default value 0.3 as that in Moschitti (2004)’s implementation. The kernels return the similarities between two reordering instances based on their features used. Our basic assumption is, the more similar the two reordering instances of x1 and x2 are, the more chance they share the same order. Now let us see how to integrate the kernel functions into SVM. The linear classifier learned by SVM is formulized as: fx = ∑ y a x • x + b ( ) sgn( i i i i ) (4) where ai is the weight of a support vector xi (i.e., a support reordering instance xi = {A1,A2)in our study), yi is its class label (1: straight or - 1: inverted in our study) and b is the</context>
<context position="22780" citStr="Moschitti, 2004" startWordPosition="3823" endWordPosition="3824">es, two target phrases, order label and its corresponding parse tree. We generate the boundary word features from the extracted reordering instances in the same way as discussed in Xiong et al. (2006) and use Zhang’s MaxEnt Tools 2 to train a reordering model for the 2nd baseline system. Similarly, we use the algorithm 1 in Xiong et al. (2008) to extract features and use the same MaxEnt Tools to train a reordering model for the 3rd baseline system. Based on the extracted reordering instances, we generate the four structured features and the linear features, and then use the Tree Kernel Tools (Moschitti, 2004) to train our kernel-based reordering model (linear, tree and composite). Experimental Design and Evaluation Metrics: we design three experiments and evaluate them using three metrics. Classification-based: in the first experiment, we extract all reordering instances and their features from the dev and test sets, and then use the reordering models trained on the training set to classify (label) those instances extracted from the dev and test sets. In this way, we can isolate the reordering problem from the influence of others, such as translation model, pruning and decoding strategies, to bett</context>
</contexts>
<marker>Moschitti, 2004</marker>
<rawString>Alessandro Moschitti. 2004. A Study on Convolution Kernels for Shallow Semantic Parsing. ACL-04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaaki Nagata</author>
<author>Kuniko Saito</author>
<author>Kazuhide Yamamoto</author>
<author>Kazuteru Ohashi</author>
</authors>
<title>A Clustered Global Phrase Reordering Model for Statistical Machine Translation.</title>
<date>2006</date>
<volume>06</volume>
<pages>713--720</pages>
<marker>Nagata, Saito, Yamamoto, Ohashi, 2006</marker>
<rawString>Masaaki Nagata, Kuniko Saito, Kazuhide Yamamoto and Kazuteru Ohashi. 2006. A Clustered Global Phrase Reordering Model for Statistical Machine Translation. COLING-ACL-06. 713-720.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<pages>02--295</pages>
<marker>Och, Ney, 2002</marker>
<rawString>Franz J. Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. ACL-02. 295-302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<pages>03--160</pages>
<contexts>
<context position="21008" citStr="Och, 2003" startWordPosition="3535" endWordPosition="3536">ining set, the NIST MT-2002 test set as development (dev) set and the NIST MT-2005 test set as test set. The Stanford parser (Klein and Manning, 2003) is used to parse Chinese sentences on the training, dev and test sets. GIZA++ (Och and Ney, 2004) and the heuristics “growdiag-final-and” are used to generate m-to-n word alignments. The translation model is trained on the FBIS corpus and a 4-gram language model is trained on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing (Kenser and Ney, 1995). For the MER training (Och, 2003), we modify Koehn’s MER trainer (Koehn, 2004) to train our system. For significance test, we use Zhang et al’s implementation (Zhang et al, 2004). Baseline Systems: we set three baseline systems: B1) Moses (Koehn et al., 2007) that uses lexicalized unigram reordering model to predict three orientations: monotone, swap and discontinuous; B2) MaxEnt-based reordering model with lexical boundary word features only (Xiong et al., 2006); B3) Linguistically annotated reordering model for BTG-based (LABTG) SMT (Xiong et al., 2008). For Moses, we used the default settings. We build a CKY-style decoder </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz J. Och. 2003. Minimum error rate training in statistical machine translation. ACL-03. 160-167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>H Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Methods.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<pages>29--1</pages>
<marker>Och, Ney, 2003</marker>
<rawString>Franz J. Och and H. Ney. 2003. A Systematic Comparison of Various Statistical Alignment Methods. Computational Linguistics, 29(1):20-51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>H Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<pages>30--4</pages>
<contexts>
<context position="1080" citStr="Och and Ney, 2004" startWordPosition="147" endWordPosition="150"> the context of statistical machine translation. Our study reveals that the structured syntactic features over the source phrases are very effective for BTG constraint-based phrase reordering and those features can be well captured by the tree kernel. We further combine the structured features and other commonly-used linear features into a composite kernel. Experimental results on the NIST MT-2005 Chinese-English translation tasks show that our proposed phrase reordering model statistically significantly outperforms the baseline methods. 1 Introduction Phrase-based method (Koehn et al., 2003; Och and Ney, 2004; Koehn et al., 2007) and syntaxbased method (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Chiang, 2005; Cowan et al., 2006; Marcu et al., 2006; Liu et al., 2007; Zhang et al., 2007c, 2008a, 2008b; Shen et al., 2008; Mi and Huang, 2008) represent the state-of-the-art technologies in statistical machine translation (SMT). As the two technologies are complementary in many ways, an interesting research topic is how to combine the strengths of the two methods. Many research efforts have been made to address this issue, which can be summarized into two ideas. One is to add syntax into phrase-ba</context>
<context position="2687" citStr="Och and Ney, 2004" startWordPosition="409" endWordPosition="412">stem. In syntax-based method, word reordering is implicitly addressed by translation rules, thus the performance is subject to parsing errors to a large extent (zhang et al., 2007a) and the impact of syntax on reordering is difficult to single out (Li et al., 2007). In phrasebased method, local word reordering1 can be effectively captured by phrase pairs directly while local phrase reordering is explicitly modeled by phrase reordering model and distortion model. Recently, many phrase reordering methods have been proposed, ranging from simple distancebased distortion model (Koehn et al., 2003; Och and Ney, 2004), flat reordering model (Wu, 1997; Zens et al., 2004), lexicalized reordering model (Tillmann, 2004; Kumar and Byrne, 2005), to hierarchical phrase-based model (Chiang, 2005; Setiawan et al., 2007) and classifier-based reordering model with linear features (Zens and Ney, 2006; Xiong et al., 2006; Zhang et al., 2007a; Xiong et al., 2008). However, one of the major limitations of these advances is the structured syntactic knowledge, which is important to global reordering (Li et al., 2007; Elming, 2008), has not been well exploited. This makes the phrasebased method particularly weak in handling</context>
<context position="20646" citStr="Och and Ney, 2004" startWordPosition="3475" endWordPosition="3478">ability as shown using the following to functions and apply it as one feature to the log-linear model in the decoding. where straight represents a positive instance and inverted represents a negative instance. 4 Experiments and Discussion 4.1 Experimental Settings Basic Settings: we evaluate our method on Chinese-English translation task. We use the FBIS corpus as training set, the NIST MT-2002 test set as development (dev) set and the NIST MT-2005 test set as test set. The Stanford parser (Klein and Manning, 2003) is used to parse Chinese sentences on the training, dev and test sets. GIZA++ (Och and Ney, 2004) and the heuristics “growdiag-final-and” are used to generate m-to-n word alignments. The translation model is trained on the FBIS corpus and a 4-gram language model is trained on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing (Kenser and Ney, 1995). For the MER training (Och, 2003), we modify Koehn’s MER trainer (Koehn, 2004) to train our system. For significance test, we use Zhang et al’s implementation (Zhang et al, 2004). Baseline Systems: we set three baseline systems: B1) Moses (Koehn et al., 2007) that uses l</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz J. Och and H. Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4):417-449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>S Roukos</author>
<author>T</author>
<author>W Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<pages>02--311</pages>
<contexts>
<context position="24662" citStr="Papineni et al., 2002" startWordPosition="4117" endWordPosition="4120">ing to the source sentences, whereas in forced decoding, a translation model is filtered based on both source sentence and target references. In other words, in forced decoding, the decoder is forced to use those phrases whose translations are already in the references. 1 (straight |x) = (inverted |x) = 1 P P 1+e− psvm (o|x) and ( |) o x 1+epsvm 703 training, dev and test data. By forced decoding, we aim to isolate the reordering problem from those of OOV and lexical selections resulting from imperfect translation model in the context of a real SMT task. Besides the the case-sensitive BLEU-4 (Papineni et al., 2002) used in the two experiments, we design another evaluation metrics Reordering Accuracy (RAcc) for forced decoding evaluation. RAcc is the percentage of the adjacent word pairs with correct word order4 over all words in one-best translation results. Similar to BLEU score, we also use the similar Brevity Penalty BP (Papineni et al., 2002) to penalize the short translations in computing RAcc. Finally, please note for the three evaluation metrics, the higher values represent better performance. Feature Spaces CAcc (%) Dev Test Minimum Sub-Tree (MST) 89.87 89.92 Minimum Sub-Structure (MSS) 87.95 87</context>
</contexts>
<marker>Papineni, Roukos, T, Zhu, 2002</marker>
<rawString>Kishore Papineni, S. Roukos, T. and W. Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. ACL-02. 311-318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hendra Setiawan</author>
<author>Min-Yen Kan</author>
<author>Haizhou Li</author>
</authors>
<title>Ordering Phrases with Function Words.</title>
<date>2007</date>
<pages>712--719</pages>
<contexts>
<context position="2884" citStr="Setiawan et al., 2007" startWordPosition="437" endWordPosition="440">act of syntax on reordering is difficult to single out (Li et al., 2007). In phrasebased method, local word reordering1 can be effectively captured by phrase pairs directly while local phrase reordering is explicitly modeled by phrase reordering model and distortion model. Recently, many phrase reordering methods have been proposed, ranging from simple distancebased distortion model (Koehn et al., 2003; Och and Ney, 2004), flat reordering model (Wu, 1997; Zens et al., 2004), lexicalized reordering model (Tillmann, 2004; Kumar and Byrne, 2005), to hierarchical phrase-based model (Chiang, 2005; Setiawan et al., 2007) and classifier-based reordering model with linear features (Zens and Ney, 2006; Xiong et al., 2006; Zhang et al., 2007a; Xiong et al., 2008). However, one of the major limitations of these advances is the structured syntactic knowledge, which is important to global reordering (Li et al., 2007; Elming, 2008), has not been well exploited. This makes the phrasebased method particularly weak in handling global phrase reordering. From machine learning viewpoint (Vapnik, 1995), it is computationally infeasible to explicitly generate features involving structured information in many NLP applica1 Thi</context>
</contexts>
<marker>Setiawan, Kan, Li, 2007</marker>
<rawString>Hendra Setiawan, Min-Yen Kan and Haizhou Li. 2007. Ordering Phrases with Function Words. ACL-07. 712-719.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Jinxi Xu</author>
<author>Ralph Weischedel</author>
</authors>
<title>A New String-to-Dependency Machine Translation Algorithm with a Target Dependency Language Model.</title>
<date>2008</date>
<volume>08</volume>
<pages>577--585</pages>
<contexts>
<context position="1299" citStr="Shen et al., 2008" startWordPosition="187" endWordPosition="190">ell captured by the tree kernel. We further combine the structured features and other commonly-used linear features into a composite kernel. Experimental results on the NIST MT-2005 Chinese-English translation tasks show that our proposed phrase reordering model statistically significantly outperforms the baseline methods. 1 Introduction Phrase-based method (Koehn et al., 2003; Och and Ney, 2004; Koehn et al., 2007) and syntaxbased method (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Chiang, 2005; Cowan et al., 2006; Marcu et al., 2006; Liu et al., 2007; Zhang et al., 2007c, 2008a, 2008b; Shen et al., 2008; Mi and Huang, 2008) represent the state-of-the-art technologies in statistical machine translation (SMT). As the two technologies are complementary in many ways, an interesting research topic is how to combine the strengths of the two methods. Many research efforts have been made to address this issue, which can be summarized into two ideas. One is to add syntax into phrase-based model while another one is to enhance syntaxbased model to handle non-syntactic phrases. In this paper, we bring forward the first idea by studying the issue of how to utilize structured syntactic features for phras</context>
</contexts>
<marker>Shen, Xu, Weischedel, 2008</marker>
<rawString>Libin Shen, Jinxi Xu and Ralph Weischedel. 2008. A New String-to-Dependency Machine Translation Algorithm with a Target Dependency Language Model. ACL-HLT-08. 577-585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - an extensible language modeling toolkit.</title>
<date>2002</date>
<pages>02--901</pages>
<contexts>
<context position="20916" citStr="Stolcke, 2002" startWordPosition="3521" endWordPosition="3522">tings: we evaluate our method on Chinese-English translation task. We use the FBIS corpus as training set, the NIST MT-2002 test set as development (dev) set and the NIST MT-2005 test set as test set. The Stanford parser (Klein and Manning, 2003) is used to parse Chinese sentences on the training, dev and test sets. GIZA++ (Och and Ney, 2004) and the heuristics “growdiag-final-and” are used to generate m-to-n word alignments. The translation model is trained on the FBIS corpus and a 4-gram language model is trained on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing (Kenser and Ney, 1995). For the MER training (Och, 2003), we modify Koehn’s MER trainer (Koehn, 2004) to train our system. For significance test, we use Zhang et al’s implementation (Zhang et al, 2004). Baseline Systems: we set three baseline systems: B1) Moses (Koehn et al., 2007) that uses lexicalized unigram reordering model to predict three orientations: monotone, swap and discontinuous; B2) MaxEnt-based reordering model with lexical boundary word features only (Xiong et al., 2006); B3) Linguistically annotated reordering model for BTG-based (LABTG) SMT </context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - an extensible language modeling toolkit. ICSLP-02. 901-904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
</authors>
<title>A Unigram Orientation Model for Statistical Machine Translation.</title>
<date>2004</date>
<note>HLTNAACL-04 (short paper).</note>
<contexts>
<context position="2786" citStr="Tillmann, 2004" startWordPosition="425" endWordPosition="426">rformance is subject to parsing errors to a large extent (zhang et al., 2007a) and the impact of syntax on reordering is difficult to single out (Li et al., 2007). In phrasebased method, local word reordering1 can be effectively captured by phrase pairs directly while local phrase reordering is explicitly modeled by phrase reordering model and distortion model. Recently, many phrase reordering methods have been proposed, ranging from simple distancebased distortion model (Koehn et al., 2003; Och and Ney, 2004), flat reordering model (Wu, 1997; Zens et al., 2004), lexicalized reordering model (Tillmann, 2004; Kumar and Byrne, 2005), to hierarchical phrase-based model (Chiang, 2005; Setiawan et al., 2007) and classifier-based reordering model with linear features (Zens and Ney, 2006; Xiong et al., 2006; Zhang et al., 2007a; Xiong et al., 2008). However, one of the major limitations of these advances is the structured syntactic knowledge, which is important to global reordering (Li et al., 2007; Elming, 2008), has not been well exploited. This makes the phrasebased method particularly weak in handling global phrase reordering. From machine learning viewpoint (Vapnik, 1995), it is computationally in</context>
</contexts>
<marker>Tillmann, 2004</marker>
<rawString>Christoph Tillmann. 2004. A Unigram Orientation Model for Statistical Machine Translation. HLTNAACL-04 (short paper).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir N Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory.</title>
<date>1995</date>
<publisher>Springer.</publisher>
<contexts>
<context position="3360" citStr="Vapnik, 1995" startWordPosition="514" endWordPosition="515">alized reordering model (Tillmann, 2004; Kumar and Byrne, 2005), to hierarchical phrase-based model (Chiang, 2005; Setiawan et al., 2007) and classifier-based reordering model with linear features (Zens and Ney, 2006; Xiong et al., 2006; Zhang et al., 2007a; Xiong et al., 2008). However, one of the major limitations of these advances is the structured syntactic knowledge, which is important to global reordering (Li et al., 2007; Elming, 2008), has not been well exploited. This makes the phrasebased method particularly weak in handling global phrase reordering. From machine learning viewpoint (Vapnik, 1995), it is computationally infeasible to explicitly generate features involving structured information in many NLP applica1 This paper follows the term convention of global reordering and local reordering of Li et al. (2007), between which the distinction is solely defined by reordering distance (whether beyond four source words) (Li et al., 2007). 698 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 698–707, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP tions. For example, one cannot enumerate efficiently all the sub-tree features for a full parse </context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>Vladimir N. Vapnik. 1995. The Nature of Statistical Learning Theory. Springer.</rawString>
</citation>
<citation valid="false">
<title>provement do we need to have a better system?</title>
<pages>04--2051</pages>
<marker></marker>
<rawString>provement do we need to have a better system? LREC-04. 2051-2054.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chao Wang</author>
<author>M Collins</author>
<author>P Koehn</author>
</authors>
<title>Chinese Syntactic Reordering for Statistical Machine Translation.</title>
<date>2007</date>
<volume>07</volume>
<pages>734--745</pages>
<marker>Wang, Collins, Koehn, 2007</marker>
<rawString>Chao Wang, M. Collins and P. Koehn. 2007. Chinese Syntactic Reordering for Statistical Machine Translation. EMNLP-CONLL-07. 734-745.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<pages>23--3</pages>
<contexts>
<context position="1134" citStr="Wu, 1997" startWordPosition="159" endWordPosition="160">als that the structured syntactic features over the source phrases are very effective for BTG constraint-based phrase reordering and those features can be well captured by the tree kernel. We further combine the structured features and other commonly-used linear features into a composite kernel. Experimental results on the NIST MT-2005 Chinese-English translation tasks show that our proposed phrase reordering model statistically significantly outperforms the baseline methods. 1 Introduction Phrase-based method (Koehn et al., 2003; Och and Ney, 2004; Koehn et al., 2007) and syntaxbased method (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Chiang, 2005; Cowan et al., 2006; Marcu et al., 2006; Liu et al., 2007; Zhang et al., 2007c, 2008a, 2008b; Shen et al., 2008; Mi and Huang, 2008) represent the state-of-the-art technologies in statistical machine translation (SMT). As the two technologies are complementary in many ways, an interesting research topic is how to combine the strengths of the two methods. Many research efforts have been made to address this issue, which can be summarized into two ideas. One is to add syntax into phrase-based model while another one is to enhance syntaxbased </context>
<context position="2720" citStr="Wu, 1997" startWordPosition="416" endWordPosition="417">ng is implicitly addressed by translation rules, thus the performance is subject to parsing errors to a large extent (zhang et al., 2007a) and the impact of syntax on reordering is difficult to single out (Li et al., 2007). In phrasebased method, local word reordering1 can be effectively captured by phrase pairs directly while local phrase reordering is explicitly modeled by phrase reordering model and distortion model. Recently, many phrase reordering methods have been proposed, ranging from simple distancebased distortion model (Koehn et al., 2003; Och and Ney, 2004), flat reordering model (Wu, 1997; Zens et al., 2004), lexicalized reordering model (Tillmann, 2004; Kumar and Byrne, 2005), to hierarchical phrase-based model (Chiang, 2005; Setiawan et al., 2007) and classifier-based reordering model with linear features (Zens and Ney, 2006; Xiong et al., 2006; Zhang et al., 2007a; Xiong et al., 2008). However, one of the major limitations of these advances is the structured syntactic knowledge, which is important to global reordering (Li et al., 2007; Elming, 2008), has not been well exploited. This makes the phrasebased method particularly weak in handling global phrase reordering. From m</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377-403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
<author>Michael McCord</author>
</authors>
<date>2004</date>
<booktitle>Improving a Statistical MT System with Automatically Learned Rewrite Patterns. COLING-04.</booktitle>
<marker>Xia, McCord, 2004</marker>
<rawString>Fei Xia and Michael McCord. 2004. Improving a Statistical MT System with Automatically Learned Rewrite Patterns. COLING-04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Maximum Entropy Based Phrase Reordering Model for SMT.</title>
<date>2006</date>
<pages>06--521</pages>
<contexts>
<context position="2983" citStr="Xiong et al., 2006" startWordPosition="453" endWordPosition="456">word reordering1 can be effectively captured by phrase pairs directly while local phrase reordering is explicitly modeled by phrase reordering model and distortion model. Recently, many phrase reordering methods have been proposed, ranging from simple distancebased distortion model (Koehn et al., 2003; Och and Ney, 2004), flat reordering model (Wu, 1997; Zens et al., 2004), lexicalized reordering model (Tillmann, 2004; Kumar and Byrne, 2005), to hierarchical phrase-based model (Chiang, 2005; Setiawan et al., 2007) and classifier-based reordering model with linear features (Zens and Ney, 2006; Xiong et al., 2006; Zhang et al., 2007a; Xiong et al., 2008). However, one of the major limitations of these advances is the structured syntactic knowledge, which is important to global reordering (Li et al., 2007; Elming, 2008), has not been well exploited. This makes the phrasebased method particularly weak in handling global phrase reordering. From machine learning viewpoint (Vapnik, 1995), it is computationally infeasible to explicitly generate features involving structured information in many NLP applica1 This paper follows the term convention of global reordering and local reordering of Li et al. (2007), </context>
<context position="6471" citStr="Xiong et al., 2006" startWordPosition="996" endWordPosition="999">evaluation metrics. Experimental results on the NIST MT-2005 Chinese-English translation tasks show that our method statistically significantly outperforms the baseline methods in term of the three different evaluation metrics. The rest of the paper is organized as follows. Section 2 introduces the baseline method of BTG-based phrase translation method while section 3 discusses the proposed method in detail. The experimental results are reported and discussed in section 4. Finally, we conclude the paper in section 5. 2 Baseline System and Method We use the MaxEnt-based BTG translation system (Xiong et al., 2006) as our baseline. It is a phrase-based SMT system with BTG reordering constraint. The system uses the BTG lexical translation rules ( ܣ ՜ ݔ/ݕ ) to translate the source phrase ݔ into target phrase ݕ , and the BTG merging rules (ܣ ՜ ሾܣ, ܣሿ | ܣ, ܣ  ) to combine two neighboring phrases with a straight or inverted order. In the translation model, the BTG lexical rules are weighted with several features, such as phrase translation, word penalty and language models, in a log-linear form. With the BTG constraint, the reordering model Ω is defined on the two neighboring phrases ܣଵ and ܣଶ and their or</context>
<context position="7864" citStr="Xiong et al., 2006" startWordPosition="1233" endWordPosition="1236">is used to model the merging/reordering order. The baseline MaxEntbased reordering model is formulized as follows: Ω ൌ ఏ(|ܣଵ, ܣଶ) ൌ ௫(∑ఏ(,భ,మ)) ∑ ௫(∑ ఏ(,భ,మ)) (2) where the functions ݄(, ܣଵ, ܣଶ) א ሼ0,1ሽ are model feature functions using the boundary words of the two neighboring phrases as features, and ߠ are feature weights that are trained based on the MaxEnt-based criteria. 3 Tree Kernel-based Phrase Reordering Model 3.1 Kernel-based Classifier Solution to Phrase Reordering In this paper, phrase reordering is recast as a classification issue as done in previous work (Xiong et al., 2006 &amp; 2008; Zhang et al., 2007a). In training, we use a machine learning algorithm training on the annotated phrase reordering instances that are automatically extracted from word-aligned, source sentence parsed training corpus, to learn a classifier. In testing (decoding), the learned classifier is applied to two adjacent source phrases to decide whether they should be merged (straight) or reordered (inverted) and what their probabilities are, and then these probabilities are used as one feature in the log-linear model in a phrase-based decoder. In addition to the previously-used linear features</context>
<context position="14951" citStr="Xiong et al., 2006" startWordPosition="2400" endWordPosition="2403">res In our study, we define the following lexicalized linear features which are easily to be extracted and integrated to our composite kernel: • Leftmost and rightmost boundary words of the left and right source phrases • Leftmost and rightmost boundary words of the left and right target phrases • Internal words of the four phrases (excluding boundary words) • Target language model (LM) score difference (monotone-inverted) In total, we arrive at 13 features, including 8 boundary word features, 4 (kinds of) internal word features and 1 LM feature. The first 12 features have been proven useful (Xiong et al., 2006; Zhang et al., 2007a) to phrase reordering. LM score is certainly a strong evidence for modeling word orders and lexical selection. Although it is already used as a standalone feature in the log-linear model, we also would like to explicitly re-optimize it together with other reordering features in our reordering model. 701 3.4 Tree Kernel, Composite Kernel and Integrating into our Reordering Model As discussed before, we use convolution tree kernel to capture the structured syntactic feature implicitly by directly computing similarity between the parse-tree representations of two reordering </context>
<context position="21442" citStr="Xiong et al., 2006" startWordPosition="3601" endWordPosition="3604">on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing (Kenser and Ney, 1995). For the MER training (Och, 2003), we modify Koehn’s MER trainer (Koehn, 2004) to train our system. For significance test, we use Zhang et al’s implementation (Zhang et al, 2004). Baseline Systems: we set three baseline systems: B1) Moses (Koehn et al., 2007) that uses lexicalized unigram reordering model to predict three orientations: monotone, swap and discontinuous; B2) MaxEnt-based reordering model with lexical boundary word features only (Xiong et al., 2006); B3) Linguistically annotated reordering model for BTG-based (LABTG) SMT (Xiong et al., 2008). For Moses, we used the default settings. We build a CKY-style decoder and integrate the corresponding reordering modelling methods into the decoder to implement the 2nd and the 3rd baseline systems and our system. Except reordering models, all the four systems use the same features in translation model, language model and distortion model as Moses in the loglinear framework. We tune the four systems using the strategies as discussed previously in this section. Reordering Model Training: we extract a</context>
</contexts>
<marker>Xiong, Liu, Lin, 2006</marker>
<rawString>Deyi Xiong, Qun Liu and Shouxun Lin. 2006. Maximum Entropy Based Phrase Reordering Model for SMT. COLING-ACL-06. 521–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Min Zhang</author>
<author>Aiti Aw</author>
<author>Haizhou Li</author>
</authors>
<title>A Linguistically Annotated Reordering Model for BTG-based Statistical Machine Translation. ACL-HLT-08 (short paper).</title>
<date>2008</date>
<pages>149--152</pages>
<contexts>
<context position="3025" citStr="Xiong et al., 2008" startWordPosition="461" endWordPosition="464">red by phrase pairs directly while local phrase reordering is explicitly modeled by phrase reordering model and distortion model. Recently, many phrase reordering methods have been proposed, ranging from simple distancebased distortion model (Koehn et al., 2003; Och and Ney, 2004), flat reordering model (Wu, 1997; Zens et al., 2004), lexicalized reordering model (Tillmann, 2004; Kumar and Byrne, 2005), to hierarchical phrase-based model (Chiang, 2005; Setiawan et al., 2007) and classifier-based reordering model with linear features (Zens and Ney, 2006; Xiong et al., 2006; Zhang et al., 2007a; Xiong et al., 2008). However, one of the major limitations of these advances is the structured syntactic knowledge, which is important to global reordering (Li et al., 2007; Elming, 2008), has not been well exploited. This makes the phrasebased method particularly weak in handling global phrase reordering. From machine learning viewpoint (Vapnik, 1995), it is computationally infeasible to explicitly generate features involving structured information in many NLP applica1 This paper follows the term convention of global reordering and local reordering of Li et al. (2007), between which the distinction is solely de</context>
<context position="21536" citStr="Xiong et al., 2008" startWordPosition="3615" endWordPosition="3618">with modified Kneser-Ney smoothing (Kenser and Ney, 1995). For the MER training (Och, 2003), we modify Koehn’s MER trainer (Koehn, 2004) to train our system. For significance test, we use Zhang et al’s implementation (Zhang et al, 2004). Baseline Systems: we set three baseline systems: B1) Moses (Koehn et al., 2007) that uses lexicalized unigram reordering model to predict three orientations: monotone, swap and discontinuous; B2) MaxEnt-based reordering model with lexical boundary word features only (Xiong et al., 2006); B3) Linguistically annotated reordering model for BTG-based (LABTG) SMT (Xiong et al., 2008). For Moses, we used the default settings. We build a CKY-style decoder and integrate the corresponding reordering modelling methods into the decoder to implement the 2nd and the 3rd baseline systems and our system. Except reordering models, all the four systems use the same features in translation model, language model and distortion model as Moses in the loglinear framework. We tune the four systems using the strategies as discussed previously in this section. Reordering Model Training: we extract all reordering instances from the m-to-n wordaligned training corpus. The reordering instances </context>
</contexts>
<marker>Xiong, Zhang, Aw, Li, 2008</marker>
<rawString>Deyi Xiong, Min Zhang, Aiti Aw and Haizhou Li. 2008. A Linguistically Annotated Reordering Model for BTG-based Statistical Machine Translation. ACL-HLT-08 (short paper). 149-152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>K Knight</author>
</authors>
<title>A syntax-based statistical translation model.</title>
<date>2001</date>
<pages>01--523</pages>
<contexts>
<context position="1159" citStr="Yamada and Knight, 2001" startWordPosition="161" endWordPosition="164">he structured syntactic features over the source phrases are very effective for BTG constraint-based phrase reordering and those features can be well captured by the tree kernel. We further combine the structured features and other commonly-used linear features into a composite kernel. Experimental results on the NIST MT-2005 Chinese-English translation tasks show that our proposed phrase reordering model statistically significantly outperforms the baseline methods. 1 Introduction Phrase-based method (Koehn et al., 2003; Och and Ney, 2004; Koehn et al., 2007) and syntaxbased method (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Chiang, 2005; Cowan et al., 2006; Marcu et al., 2006; Liu et al., 2007; Zhang et al., 2007c, 2008a, 2008b; Shen et al., 2008; Mi and Huang, 2008) represent the state-of-the-art technologies in statistical machine translation (SMT). As the two technologies are complementary in many ways, an interesting research topic is how to combine the strengths of the two methods. Many research efforts have been made to address this issue, which can be summarized into two ideas. One is to add syntax into phrase-based model while another one is to enhance syntaxbased model to handle non-synta</context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>Kenji Yamada and K. Knight. 2001. A syntax-based statistical translation model. ACL-01. 523-530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaofeng Yang</author>
<author>Jian Su</author>
<author>Chew Lim Tan</author>
</authors>
<title>Kernel-Based Pronoun Resolution with Structured Syntactic Knowledge.</title>
<date>2006</date>
<volume>06</volume>
<pages>41--48</pages>
<contexts>
<context position="4866" citStr="Yang et al., 2006" startWordPosition="741" endWordPosition="744"> space (Vapnik, 1995), in this paper we propose using convolution tree kernel (Haussler, 1999; Collins and Duffy, 2001) to explore the structured syntactic knowledge for phrase reordering and further combine the tree kernel with other diverse linear features into a composite kernel to strengthen the model’s predictive ability. Indeed, using tree kernel methods to mine structured knowledge has shown success in some NLP applications like parsing (Collins and Duffy, 2001), semantic role labeling (Moschitti, 2004; Zhang et al., 2007b), relation extraction (Zhang et al., 2006), pronoun resolution (Yang et al., 2006) and question classification (Zhang and Lee, 2003). However, to our knowledge, such technique still remains unexplored for phrase reordering. In this paper, we look into the phrase reordering problem in two aspects: 1) how to model and optimize structured features, and 2) how to combine the structured features with other linear features and further integrate them into the loglinear model-based translation framework. Our study shows that: 1) the structured syntactic features are very useful and 2) our kernel-based model can well explore diverse knowledge, including previously-used linear featur</context>
</contexts>
<marker>Yang, Su, Tan, 2006</marker>
<rawString>Xiaofeng Yang, Jian Su and Chew Lim Tan. 2006. Kernel-Based Pronoun Resolution with Structured Syntactic Knowledge. COLING-ACL-06. 41-48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>H Ney</author>
<author>T Watanabe</author>
<author>E Sumita</author>
</authors>
<title>Reordering Constraints for Phrase-Based Statistical Machine Translation.</title>
<date>2004</date>
<tech>COLING-04.</tech>
<contexts>
<context position="2740" citStr="Zens et al., 2004" startWordPosition="418" endWordPosition="421">icitly addressed by translation rules, thus the performance is subject to parsing errors to a large extent (zhang et al., 2007a) and the impact of syntax on reordering is difficult to single out (Li et al., 2007). In phrasebased method, local word reordering1 can be effectively captured by phrase pairs directly while local phrase reordering is explicitly modeled by phrase reordering model and distortion model. Recently, many phrase reordering methods have been proposed, ranging from simple distancebased distortion model (Koehn et al., 2003; Och and Ney, 2004), flat reordering model (Wu, 1997; Zens et al., 2004), lexicalized reordering model (Tillmann, 2004; Kumar and Byrne, 2005), to hierarchical phrase-based model (Chiang, 2005; Setiawan et al., 2007) and classifier-based reordering model with linear features (Zens and Ney, 2006; Xiong et al., 2006; Zhang et al., 2007a; Xiong et al., 2008). However, one of the major limitations of these advances is the structured syntactic knowledge, which is important to global reordering (Li et al., 2007; Elming, 2008), has not been well exploited. This makes the phrasebased method particularly weak in handling global phrase reordering. From machine learning view</context>
</contexts>
<marker>Zens, Ney, Watanabe, Sumita, 2004</marker>
<rawString>Richard Zens, H. Ney, T. Watanabe and E. Sumita. 2004. Reordering Constraints for Phrase-Based Statistical Machine Translation. COLING-04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative Reordering Models for Statistical Machine Translation.</title>
<date>2006</date>
<contexts>
<context position="2963" citStr="Zens and Ney, 2006" startWordPosition="449" endWordPosition="452">based method, local word reordering1 can be effectively captured by phrase pairs directly while local phrase reordering is explicitly modeled by phrase reordering model and distortion model. Recently, many phrase reordering methods have been proposed, ranging from simple distancebased distortion model (Koehn et al., 2003; Och and Ney, 2004), flat reordering model (Wu, 1997; Zens et al., 2004), lexicalized reordering model (Tillmann, 2004; Kumar and Byrne, 2005), to hierarchical phrase-based model (Chiang, 2005; Setiawan et al., 2007) and classifier-based reordering model with linear features (Zens and Ney, 2006; Xiong et al., 2006; Zhang et al., 2007a; Xiong et al., 2008). However, one of the major limitations of these advances is the structured syntactic knowledge, which is important to global reordering (Li et al., 2007; Elming, 2008), has not been well exploited. This makes the phrasebased method particularly weak in handling global phrase reordering. From machine learning viewpoint (Vapnik, 1995), it is computationally infeasible to explicitly generate features involving structured information in many NLP applica1 This paper follows the term convention of global reordering and local reordering o</context>
</contexts>
<marker>Zens, Ney, 2006</marker>
<rawString>Richard Zens and Hermann Ney. 2006. Discriminative Reordering Models for Statistical Machine Translation. WSMT-2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dell Zhang</author>
<author>W Lee</author>
</authors>
<title>Question classification using support vector machines.</title>
<date>2003</date>
<pages>03</pages>
<contexts>
<context position="4916" citStr="Zhang and Lee, 2003" startWordPosition="748" endWordPosition="751">using convolution tree kernel (Haussler, 1999; Collins and Duffy, 2001) to explore the structured syntactic knowledge for phrase reordering and further combine the tree kernel with other diverse linear features into a composite kernel to strengthen the model’s predictive ability. Indeed, using tree kernel methods to mine structured knowledge has shown success in some NLP applications like parsing (Collins and Duffy, 2001), semantic role labeling (Moschitti, 2004; Zhang et al., 2007b), relation extraction (Zhang et al., 2006), pronoun resolution (Yang et al., 2006) and question classification (Zhang and Lee, 2003). However, to our knowledge, such technique still remains unexplored for phrase reordering. In this paper, we look into the phrase reordering problem in two aspects: 1) how to model and optimize structured features, and 2) how to combine the structured features with other linear features and further integrate them into the loglinear model-based translation framework. Our study shows that: 1) the structured syntactic features are very useful and 2) our kernel-based model can well explore diverse knowledge, including previously-used linear features and the structured syntactic features, for phra</context>
</contexts>
<marker>Zhang, Lee, 2003</marker>
<rawString>Dell Zhang and W. Lee. 2003. Question classification using support vector machines. SIGIR-03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Jie Zhang</author>
<author>Jian Su</author>
<author>GuoDong Zhou</author>
</authors>
<title>A Composite Kernel to Extract Relations between Entities with Both Flat and Structured Features.</title>
<date>2006</date>
<volume>06</volume>
<pages>825--832</pages>
<contexts>
<context position="4826" citStr="Zhang et al., 2006" startWordPosition="735" endWordPosition="738">d) features in a high dimensional feature space (Vapnik, 1995), in this paper we propose using convolution tree kernel (Haussler, 1999; Collins and Duffy, 2001) to explore the structured syntactic knowledge for phrase reordering and further combine the tree kernel with other diverse linear features into a composite kernel to strengthen the model’s predictive ability. Indeed, using tree kernel methods to mine structured knowledge has shown success in some NLP applications like parsing (Collins and Duffy, 2001), semantic role labeling (Moschitti, 2004; Zhang et al., 2007b), relation extraction (Zhang et al., 2006), pronoun resolution (Yang et al., 2006) and question classification (Zhang and Lee, 2003). However, to our knowledge, such technique still remains unexplored for phrase reordering. In this paper, we look into the phrase reordering problem in two aspects: 1) how to model and optimize structured features, and 2) how to combine the structured features with other linear features and further integrate them into the loglinear model-based translation framework. Our study shows that: 1) the structured syntactic features are very useful and 2) our kernel-based model can well explore diverse knowledge,</context>
<context position="10956" citStr="Zhang et al., 2006" startWordPosition="1710" endWordPosition="1713">el of various types)” is the 1st/left phrase and “首先/above all 迫切/urgent 需要/necessary (it is necessary, above all, ...)” is the 2nd/right phrase. Note that different function tags are attached to the grammar tag of each internal node. 700 the possible subtrees in the structured feature spaces. We can see that the structured feature spaces and their features are encapsulated by a full parse tree of source sentences. Thus, it is critical to understand which portion of a parse tree (i.e. structured feature space) is the most effective to represent a reordering instance. Motivated by the work of (Zhang et al., 2006), we here examine four cases that contain different sub-structures as shown in Fig. 1. (1) Minimum Sub-Tree (MST): the sub-tree rooted by the nearest common ancestor of the two phrases. This feature records the minimum sub-structure covering the two phrases and its left and right contexts as shown in Fig 1.T1. (2) Minimum Sub-Structure (MSS): the smallest common sub-structure covering the two phrases. It is enclosed by the shortest path linking the two phrases. Thus, its leaf nodes exactly consist of all the phrasal words. (3) Context-sensitive Minimum Sub-Structure (CMSS): the MSS extending w</context>
</contexts>
<marker>Zhang, Zhang, Su, Zhou, 2006</marker>
<rawString>Min Zhang, Jie Zhang, Jian Su and GuoDong Zhou. 2006. A Composite Kernel to Extract Relations between Entities with Both Flat and Structured Features. COLING-ACL-06. 825-832.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dongdong Zhang</author>
<author>M Li</author>
<author>C H Li</author>
<author>M Zhou</author>
</authors>
<title>Phrase Reordering Model Integrating Syntactic Knowledge for SMT.</title>
<date>2007</date>
<pages>07--533</pages>
<contexts>
<context position="1265" citStr="Zhang et al., 2007" startWordPosition="181" endWordPosition="184">rdering and those features can be well captured by the tree kernel. We further combine the structured features and other commonly-used linear features into a composite kernel. Experimental results on the NIST MT-2005 Chinese-English translation tasks show that our proposed phrase reordering model statistically significantly outperforms the baseline methods. 1 Introduction Phrase-based method (Koehn et al., 2003; Och and Ney, 2004; Koehn et al., 2007) and syntaxbased method (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Chiang, 2005; Cowan et al., 2006; Marcu et al., 2006; Liu et al., 2007; Zhang et al., 2007c, 2008a, 2008b; Shen et al., 2008; Mi and Huang, 2008) represent the state-of-the-art technologies in statistical machine translation (SMT). As the two technologies are complementary in many ways, an interesting research topic is how to combine the strengths of the two methods. Many research efforts have been made to address this issue, which can be summarized into two ideas. One is to add syntax into phrase-based model while another one is to enhance syntaxbased model to handle non-syntactic phrases. In this paper, we bring forward the first idea by studying the issue of how to utilize struc</context>
<context position="3003" citStr="Zhang et al., 2007" startWordPosition="457" endWordPosition="460"> be effectively captured by phrase pairs directly while local phrase reordering is explicitly modeled by phrase reordering model and distortion model. Recently, many phrase reordering methods have been proposed, ranging from simple distancebased distortion model (Koehn et al., 2003; Och and Ney, 2004), flat reordering model (Wu, 1997; Zens et al., 2004), lexicalized reordering model (Tillmann, 2004; Kumar and Byrne, 2005), to hierarchical phrase-based model (Chiang, 2005; Setiawan et al., 2007) and classifier-based reordering model with linear features (Zens and Ney, 2006; Xiong et al., 2006; Zhang et al., 2007a; Xiong et al., 2008). However, one of the major limitations of these advances is the structured syntactic knowledge, which is important to global reordering (Li et al., 2007; Elming, 2008), has not been well exploited. This makes the phrasebased method particularly weak in handling global phrase reordering. From machine learning viewpoint (Vapnik, 1995), it is computationally infeasible to explicitly generate features involving structured information in many NLP applica1 This paper follows the term convention of global reordering and local reordering of Li et al. (2007), between which the di</context>
<context position="4782" citStr="Zhang et al., 2007" startWordPosition="729" endWordPosition="732">thod that can implicitly explore (structured) features in a high dimensional feature space (Vapnik, 1995), in this paper we propose using convolution tree kernel (Haussler, 1999; Collins and Duffy, 2001) to explore the structured syntactic knowledge for phrase reordering and further combine the tree kernel with other diverse linear features into a composite kernel to strengthen the model’s predictive ability. Indeed, using tree kernel methods to mine structured knowledge has shown success in some NLP applications like parsing (Collins and Duffy, 2001), semantic role labeling (Moschitti, 2004; Zhang et al., 2007b), relation extraction (Zhang et al., 2006), pronoun resolution (Yang et al., 2006) and question classification (Zhang and Lee, 2003). However, to our knowledge, such technique still remains unexplored for phrase reordering. In this paper, we look into the phrase reordering problem in two aspects: 1) how to model and optimize structured features, and 2) how to combine the structured features with other linear features and further integrate them into the loglinear model-based translation framework. Our study shows that: 1) the structured syntactic features are very useful and 2) our kernel-bas</context>
<context position="7891" citStr="Zhang et al., 2007" startWordPosition="1239" endWordPosition="1242">g/reordering order. The baseline MaxEntbased reordering model is formulized as follows: Ω ൌ ఏ(|ܣଵ, ܣଶ) ൌ ௫(∑ఏ(,భ,మ)) ∑ ௫(∑ ఏ(,భ,మ)) (2) where the functions ݄(, ܣଵ, ܣଶ) א ሼ0,1ሽ are model feature functions using the boundary words of the two neighboring phrases as features, and ߠ are feature weights that are trained based on the MaxEnt-based criteria. 3 Tree Kernel-based Phrase Reordering Model 3.1 Kernel-based Classifier Solution to Phrase Reordering In this paper, phrase reordering is recast as a classification issue as done in previous work (Xiong et al., 2006 &amp; 2008; Zhang et al., 2007a). In training, we use a machine learning algorithm training on the annotated phrase reordering instances that are automatically extracted from word-aligned, source sentence parsed training corpus, to learn a classifier. In testing (decoding), the learned classifier is applied to two adjacent source phrases to decide whether they should be merged (straight) or reordered (inverted) and what their probabilities are, and then these probabilities are used as one feature in the log-linear model in a phrase-based decoder. In addition to the previously-used linear features, we are more interested in</context>
<context position="14971" citStr="Zhang et al., 2007" startWordPosition="2404" endWordPosition="2407"> define the following lexicalized linear features which are easily to be extracted and integrated to our composite kernel: • Leftmost and rightmost boundary words of the left and right source phrases • Leftmost and rightmost boundary words of the left and right target phrases • Internal words of the four phrases (excluding boundary words) • Target language model (LM) score difference (monotone-inverted) In total, we arrive at 13 features, including 8 boundary word features, 4 (kinds of) internal word features and 1 LM feature. The first 12 features have been proven useful (Xiong et al., 2006; Zhang et al., 2007a) to phrase reordering. LM score is certainly a strong evidence for modeling word orders and lexical selection. Although it is already used as a standalone feature in the log-linear model, we also would like to explicitly re-optimize it together with other reordering features in our reordering model. 701 3.4 Tree Kernel, Composite Kernel and Integrating into our Reordering Model As discussed before, we use convolution tree kernel to capture the structured syntactic feature implicitly by directly computing similarity between the parse-tree representations of two reordering instances with expli</context>
</contexts>
<marker>Zhang, Li, Li, Zhou, 2007</marker>
<rawString>Dongdong Zhang, M. Li, C.H. Li and M. Zhou. 2007a. Phrase Reordering Model Integrating Syntactic Knowledge for SMT. EMNLP-CONLL-07. 533-540.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>W Che</author>
<author>A Aw</author>
<author>C Tan</author>
<author>G Zhou</author>
<author>T Liu</author>
<author>S Li</author>
</authors>
<title>A Grammar-driven Convolution Tree Kernel for Semantic Role Classification.</title>
<date>2007</date>
<volume>07</volume>
<pages>200--207</pages>
<contexts>
<context position="1265" citStr="Zhang et al., 2007" startWordPosition="181" endWordPosition="184">rdering and those features can be well captured by the tree kernel. We further combine the structured features and other commonly-used linear features into a composite kernel. Experimental results on the NIST MT-2005 Chinese-English translation tasks show that our proposed phrase reordering model statistically significantly outperforms the baseline methods. 1 Introduction Phrase-based method (Koehn et al., 2003; Och and Ney, 2004; Koehn et al., 2007) and syntaxbased method (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Chiang, 2005; Cowan et al., 2006; Marcu et al., 2006; Liu et al., 2007; Zhang et al., 2007c, 2008a, 2008b; Shen et al., 2008; Mi and Huang, 2008) represent the state-of-the-art technologies in statistical machine translation (SMT). As the two technologies are complementary in many ways, an interesting research topic is how to combine the strengths of the two methods. Many research efforts have been made to address this issue, which can be summarized into two ideas. One is to add syntax into phrase-based model while another one is to enhance syntaxbased model to handle non-syntactic phrases. In this paper, we bring forward the first idea by studying the issue of how to utilize struc</context>
<context position="3003" citStr="Zhang et al., 2007" startWordPosition="457" endWordPosition="460"> be effectively captured by phrase pairs directly while local phrase reordering is explicitly modeled by phrase reordering model and distortion model. Recently, many phrase reordering methods have been proposed, ranging from simple distancebased distortion model (Koehn et al., 2003; Och and Ney, 2004), flat reordering model (Wu, 1997; Zens et al., 2004), lexicalized reordering model (Tillmann, 2004; Kumar and Byrne, 2005), to hierarchical phrase-based model (Chiang, 2005; Setiawan et al., 2007) and classifier-based reordering model with linear features (Zens and Ney, 2006; Xiong et al., 2006; Zhang et al., 2007a; Xiong et al., 2008). However, one of the major limitations of these advances is the structured syntactic knowledge, which is important to global reordering (Li et al., 2007; Elming, 2008), has not been well exploited. This makes the phrasebased method particularly weak in handling global phrase reordering. From machine learning viewpoint (Vapnik, 1995), it is computationally infeasible to explicitly generate features involving structured information in many NLP applica1 This paper follows the term convention of global reordering and local reordering of Li et al. (2007), between which the di</context>
<context position="4782" citStr="Zhang et al., 2007" startWordPosition="729" endWordPosition="732">thod that can implicitly explore (structured) features in a high dimensional feature space (Vapnik, 1995), in this paper we propose using convolution tree kernel (Haussler, 1999; Collins and Duffy, 2001) to explore the structured syntactic knowledge for phrase reordering and further combine the tree kernel with other diverse linear features into a composite kernel to strengthen the model’s predictive ability. Indeed, using tree kernel methods to mine structured knowledge has shown success in some NLP applications like parsing (Collins and Duffy, 2001), semantic role labeling (Moschitti, 2004; Zhang et al., 2007b), relation extraction (Zhang et al., 2006), pronoun resolution (Yang et al., 2006) and question classification (Zhang and Lee, 2003). However, to our knowledge, such technique still remains unexplored for phrase reordering. In this paper, we look into the phrase reordering problem in two aspects: 1) how to model and optimize structured features, and 2) how to combine the structured features with other linear features and further integrate them into the loglinear model-based translation framework. Our study shows that: 1) the structured syntactic features are very useful and 2) our kernel-bas</context>
<context position="7891" citStr="Zhang et al., 2007" startWordPosition="1239" endWordPosition="1242">g/reordering order. The baseline MaxEntbased reordering model is formulized as follows: Ω ൌ ఏ(|ܣଵ, ܣଶ) ൌ ௫(∑ఏ(,భ,మ)) ∑ ௫(∑ ఏ(,భ,మ)) (2) where the functions ݄(, ܣଵ, ܣଶ) א ሼ0,1ሽ are model feature functions using the boundary words of the two neighboring phrases as features, and ߠ are feature weights that are trained based on the MaxEnt-based criteria. 3 Tree Kernel-based Phrase Reordering Model 3.1 Kernel-based Classifier Solution to Phrase Reordering In this paper, phrase reordering is recast as a classification issue as done in previous work (Xiong et al., 2006 &amp; 2008; Zhang et al., 2007a). In training, we use a machine learning algorithm training on the annotated phrase reordering instances that are automatically extracted from word-aligned, source sentence parsed training corpus, to learn a classifier. In testing (decoding), the learned classifier is applied to two adjacent source phrases to decide whether they should be merged (straight) or reordered (inverted) and what their probabilities are, and then these probabilities are used as one feature in the log-linear model in a phrase-based decoder. In addition to the previously-used linear features, we are more interested in</context>
<context position="14971" citStr="Zhang et al., 2007" startWordPosition="2404" endWordPosition="2407"> define the following lexicalized linear features which are easily to be extracted and integrated to our composite kernel: • Leftmost and rightmost boundary words of the left and right source phrases • Leftmost and rightmost boundary words of the left and right target phrases • Internal words of the four phrases (excluding boundary words) • Target language model (LM) score difference (monotone-inverted) In total, we arrive at 13 features, including 8 boundary word features, 4 (kinds of) internal word features and 1 LM feature. The first 12 features have been proven useful (Xiong et al., 2006; Zhang et al., 2007a) to phrase reordering. LM score is certainly a strong evidence for modeling word orders and lexical selection. Although it is already used as a standalone feature in the log-linear model, we also would like to explicitly re-optimize it together with other reordering features in our reordering model. 701 3.4 Tree Kernel, Composite Kernel and Integrating into our Reordering Model As discussed before, we use convolution tree kernel to capture the structured syntactic feature implicitly by directly computing similarity between the parse-tree representations of two reordering instances with expli</context>
</contexts>
<marker>Zhang, Che, Aw, Tan, Zhou, Liu, Li, 2007</marker>
<rawString>Min Zhang, W. Che, A. Aw, C. Tan, G. Zhou, T. Liu and S. Li. 2007b. A Grammar-driven Convolution Tree Kernel for Semantic Role Classification. ACL-07. 200-207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Hongfei Jiang</author>
</authors>
<title>Ai Ti Aw, Jun Sun, Sheng Li and Chew Lim Tan.</title>
<date>2007</date>
<pages>535--542</pages>
<marker>Zhang, Jiang, 2007</marker>
<rawString>Min Zhang, Hongfei Jiang, Ai Ti Aw, Jun Sun, Sheng Li and Chew Lim Tan. 2007c. A Tree-to-Tree Alignment-based Model for Statistical Machine Translation.MT-Summit-07. 535-542</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Hongfei Jiang</author>
</authors>
<title>Ai Ti Aw, Haizhou Li, Chew Lim Tan and Chew Lim Tan and Sheng Li.</title>
<date>2008</date>
<volume>08</volume>
<pages>559--567</pages>
<marker>Zhang, Jiang, 2008</marker>
<rawString>Min Zhang, Hongfei Jiang, Ai Ti Aw, Haizhou Li, Chew Lim Tan and Chew Lim Tan and Sheng Li. 2008a. A Tree Sequence Alignment-based Tree-toTree Translation Model. ACL-HLT-08. 559-567.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Min Zhang</author>
<author>Hongfei Jiang</author>
<author>Haizhou Li</author>
</authors>
<title>Aiti Aw, Sheng Li.</title>
<booktitle>2008b. Grammar Comparison Study for Translational Equivalence Modeling and Statistical Machine Translation. COLING-08.</booktitle>
<pages>1097--1104</pages>
<marker>Zhang, Jiang, Li, </marker>
<rawString>Min Zhang, Hongfei Jiang, Haizhou Li, Aiti Aw, Sheng Li. 2008b. Grammar Comparison Study for Translational Equivalence Modeling and Statistical Machine Translation. COLING-08. 1097-1104</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Zhang</author>
<author>Stephan Vogel</author>
<author>Alex Waibel</author>
</authors>
<title>Interpreting BLEU/NIST scores: How much im-</title>
<date>2004</date>
<contexts>
<context position="21153" citStr="Zhang et al, 2004" startWordPosition="3558" endWordPosition="3561">anning, 2003) is used to parse Chinese sentences on the training, dev and test sets. GIZA++ (Och and Ney, 2004) and the heuristics “growdiag-final-and” are used to generate m-to-n word alignments. The translation model is trained on the FBIS corpus and a 4-gram language model is trained on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing (Kenser and Ney, 1995). For the MER training (Och, 2003), we modify Koehn’s MER trainer (Koehn, 2004) to train our system. For significance test, we use Zhang et al’s implementation (Zhang et al, 2004). Baseline Systems: we set three baseline systems: B1) Moses (Koehn et al., 2007) that uses lexicalized unigram reordering model to predict three orientations: monotone, swap and discontinuous; B2) MaxEnt-based reordering model with lexical boundary word features only (Xiong et al., 2006); B3) Linguistically annotated reordering model for BTG-based (LABTG) SMT (Xiong et al., 2008). For Moses, we used the default settings. We build a CKY-style decoder and integrate the corresponding reordering modelling methods into the decoder to implement the 2nd and the 3rd baseline systems and our system. E</context>
</contexts>
<marker>Zhang, Vogel, Waibel, 2004</marker>
<rawString>Ying Zhang, Stephan Vogel and Alex Waibel. 2004. Interpreting BLEU/NIST scores: How much im-</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>