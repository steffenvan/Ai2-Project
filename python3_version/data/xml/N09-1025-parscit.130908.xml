<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000294">
<title confidence="0.976072">
11,001 New Features for Statistical Machine Translation*
</title>
<author confidence="0.952791">
David Chiang and Kevin Knight Wei Wang
</author>
<affiliation confidence="0.800993666666667">
USC Information Sciences Institute Language Weaver, Inc.
4676 Admiralty Way, Suite 1001 4640 Admiralty Way, Suite 1210
Marina del Rey, CA 90292 USA Marina del Rey, CA 90292 USA
</affiliation>
<sectionHeader confidence="0.970943" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999893636363636">
We use the Margin Infused Relaxed Algo-
rithm of Crammer et al. to add a large num-
ber of new features to two machine transla-
tion systems: the Hiero hierarchical phrase-
based translation system and our syntax-based
translation system. On a large-scale Chinese-
English translation task, we obtain statistically
significant improvements of +1.5 BLEU and
+1.1 BLEU, respectively. We analyze the im-
pact of the new features and the performance
of the learning algorithm.
</bodyText>
<sectionHeader confidence="0.998419" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99867675">
What linguistic features can improve statistical ma-
chine translation (MT)? This is a fundamental ques-
tion for the discipline, particularly as it pertains to
improving the best systems we have. Further:
</bodyText>
<listItem confidence="0.982904666666667">
• Do syntax-based translation systems have
unique and effective levers to pull when design-
ing new features?
• Can large numbers of feature weights be
learned efficiently and stably on modest
amounts of data?
</listItem>
<bodyText confidence="0.999888">
In this paper, we address these questions by exper-
imenting with a large number of new features. We
add more than 250 features to improve a syntax-
based MT system—already the highest-scoring sin-
gle system in the NIST 2008 Chinese-English
common-data track—by +1.1 BLEU. We also add
more than 10,000 features to Hiero (Chiang, 2005)
and obtain a +1.5 BLEU improvement.
</bodyText>
<footnote confidence="0.9652615">
*This research was supported in part by DARPA contract
HR0011-06-C-0022 under subcontract to BBN Technologies.
</footnote>
<bodyText confidence="0.999660538461538">
Many of the new features use syntactic informa-
tion, and in particular depend on information that
is available only inside a syntax-based translation
model. Thus they widen the advantage that syntax-
based models have over other types of models.
The models are trained using the Margin Infused
Relaxed Algorithm or MIRA (Crammer et al., 2006)
instead of the standard minimum-error-rate training
or MERT algorithm (Och, 2003). Our results add
to a growing body of evidence (Watanabe et al.,
2007; Chiang et al., 2008) that MIRA is preferable to
MERT across languages and systems, even for very
large-scale tasks.
</bodyText>
<sectionHeader confidence="0.999795" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99975045">
The work of Och et al (2004) is perhaps the best-
known study of new features and their impact on
translation quality. However, it had a few shortcom-
ings. First, it used the features for reranking n-best
lists of translations, rather than for decoding or for-
est reranking (Huang, 2008). Second, it attempted to
incorporate syntax by applying off-the-shelf part-of-
speech taggers and parsers to MT output, a task these
tools were never designed for. By contrast, we incor-
porate features directly into hierarchical and syntax-
based decoders.
A third difficulty with Och et al.’s study was that
it used MERT, which is not an ideal vehicle for fea-
ture exploration because it is observed not to per-
form well with large feature sets. Others have in-
troduced alternative discriminative training meth-
ods (Tillmann and Zhang, 2006; Liang et al., 2006;
Turian et al., 2007; Blunsom et al., 2008; Macherey
et al., 2008), in which a recurring challenge is scal-
ability: to train many features, we need many train-
</bodyText>
<page confidence="0.967634">
218
</page>
<note confidence="0.8911615">
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218–226,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999316166666667">
ing examples, and to train discriminatively, we need
to search through all possible translations of each
training example. Another line of research (Watan-
abe et al., 2007; Chiang et al., 2008) tries to squeeze
as many features as possible from a relatively small
dataset. We follow this approach here.
</bodyText>
<sectionHeader confidence="0.984338" genericHeader="method">
3 Systems Used
</sectionHeader>
<subsectionHeader confidence="0.953851">
3.1 Hiero
</subsectionHeader>
<bodyText confidence="0.999911">
Hiero (Chiang, 2005) is a hierarchical, string-to-
string translation system. Its rules, which are ex-
tracted from unparsed, word-aligned parallel text,
are synchronous CFG productions, for example:
</bodyText>
<equation confidence="0.871203">
X → X1 de X2, X2 of X1
</equation>
<bodyText confidence="0.9998389">
As the number of nonterminals is limited to two, the
grammar is equivalent to an inversion transduction
grammar (Wu, 1997).
The baseline model includes 12 features whose
weights are optimized using MERT. Two of the fea-
tures are n-gram language models, which require
intersecting the synchronous CFG with finite-state
automata representing the language models. This
grammar can be parsed efficiently using cube prun-
ing (Chiang, 2007).
</bodyText>
<subsectionHeader confidence="0.998986">
3.2 Syntax-based system
</subsectionHeader>
<bodyText confidence="0.999904083333333">
Our syntax-based system transforms source Chinese
strings into target English syntax trees. Following
previous work in statistical MT (Brown et al., 1993),
we envision a noisy-channel model in which a lan-
guage model generates English, and then a transla-
tion model transforms English trees into Chinese.
We represent the translation model as a tree trans-
ducer (Knight and Graehl, 2005). It is obtained from
bilingual text that has been word-aligned and whose
English side has been syntactically parsed. From this
data, we use the the GHKM minimal-rule extraction
algorithm of (Galley et al., 2004) to yield rules like:
</bodyText>
<equation confidence="0.918957">
NP-C(x0:NPB PP(IN(of x1:NPB)) H x1 de x0
</equation>
<bodyText confidence="0.9999416">
Though this rule can be used in either direction,
here we use it right-to-left (Chinese to English). We
follow Galley et al. (2006) in allowing unaligned
Chinese words to participate in multiple translation
rules, and in collecting larger rules composed of
minimal rules. These larger rules have been shown
to substantially improve translation accuracy (Gal-
ley et al., 2006; DeNeefe et al., 2007).
We apply Good-Turing discounting to the trans-
ducer rule counts and obtain probability estimates:
</bodyText>
<equation confidence="0.997891333333333">
count(rule)
P(rule) =
count(LHS-root(rule))
</equation>
<bodyText confidence="0.999985">
When we apply these probabilities to derive an En-
glish sentence e and a corresponding Chinese sen-
tence c, we wind up with the joint probability P(e, c).
The baseline model includes log P(e, c), the two
n-gram language models log P(e), and other features
for a total of 25. For example, there is a pair of
features to punish rules that drop Chinese content
words or introduce spurious English content words.
All features are linearly combined and their weights
are optimized using MERT.
For efficient decoding with integrated n-gram lan-
guage models, all transducer rules must be binarized
into rules that contain at most two variables and
can be incrementally scored by the language model
(Zhang et al., 2006). Then we use a CKY-style parser
(Yamada and Knight, 2002; Galley et al., 2006) with
cube pruning to decode new sentences.
We include two other techniques in our baseline.
To get more general translation rules, we restruc-
ture our English training trees using expectation-
maximization (Wang et al., 2007), and to get more
specific translation rules, we relabel the trees with up
to 4 specialized versions of each nonterminal sym-
bol, again using expectation-maximization and the
split/merge technique of Petrov et al. (2006).
</bodyText>
<subsectionHeader confidence="0.999413">
3.3 MIRA training
</subsectionHeader>
<bodyText confidence="0.999791857142857">
We incorporate all our new features into a linear
model (Och and Ney, 2002) and train them using
MIRA (Crammer et al., 2006), following previous
work (Watanabe et al., 2007; Chiang et al., 2008).
Let e stand for output strings or their derivations,
and let h(e) stand for the feature vector for e. Initial-
ize the feature weights w. Then, repeatedly:
</bodyText>
<listItem confidence="0.99676875">
• Select a batch of input sentences f1, ... , fm and
decode each fi to obtain a forest of translations.
• For each i, select from the forest a set of hy-
pothesis translations ei1, ... , ein, which are the
</listItem>
<page confidence="0.997591">
219
</page>
<bodyText confidence="0.902895">
10-best translations according to each of:
</bodyText>
<equation confidence="0.944098428571429">
h(e) · w
Bᴇᴜ(e) + h(e) · w (1)
−Bᴇᴜ(e) + h(e) · w
• For each i, select an oracle translation:
e∗ = arg max (Bᴇᴜ(e) + h(e) · w) (2)
e
Let Δhi j = h(e∗i ) − h(ei j).
</equation>
<listItem confidence="0.900912">
• For each eij, compute the loss
l&apos;ij = Bᴇᴜ(e∗i ) − Bᴇᴜ(eij) (3)
• Update w to the value of w0 that minimizes:
</listItem>
<equation confidence="0.6341335">
max (l&apos;ij − Δhi j · w0) (4)
1≤j≤n
</equation>
<bodyText confidence="0.9997103125">
where C = 0.01. This minimization is per-
formed by a variant of sequential minimal opti-
mization (Platt, 1998).
Following Chiang et al. (2008), we calculate the sen-
tence Bᴇᴜ scores in (1), (2), and (3) in the context
of some previous 1-best translations. We run 20 of
these learners in parallel, and when training is fin-
ished, the weight vectors from all iterations of all
learners are averaged together.
Since the interface between the trainer and the de-
coder is fairly simple—for each sentence, the de-
coder sends the trainer a forest, and the trainer re-
turns a weight update—it is easy to use this algo-
rithm with a variety of CKY-based decoders: here,
we are using it in conjunction with both the Hiero
decoder and our syntax-based decoder.
</bodyText>
<sectionHeader confidence="0.999417" genericHeader="method">
4 Features
</sectionHeader>
<bodyText confidence="0.999691666666667">
In this section, we describe the new features intro-
duced on top of our baseline systems.
Discount features Both of our systems calculate
several features based on observed counts of rules in
the training data. Though the syntax-based system
uses Good-Turing discounting when computing the
P(e, c) feature, we find, as noted above, that it uses
quite a few one-count rules, suggesting that their
probabilities have been overestimated. We can di-
rectly attack this problem by adding features counti
that reward or punish rules seen i times, or features
count[i,j] for rules seen between i and j times.
</bodyText>
<subsectionHeader confidence="0.989006">
4.1 Target-side features
</subsectionHeader>
<bodyText confidence="0.999973125">
String-to-tree MT offers some unique levers to pull,
in terms of target-side features. Because the system
outputs English trees, we can analyze output trees on
the tuning set and design new features to encourage
the decoder to produce more grammatical trees.
Rule overlap features While individual rules ob-
served in decoder output are often quite reasonable,
two adjacent rules can create problems. For exam-
ple, a rule that has a variable of type IN (preposi-
tion) needs another rule rooted with IN to fill the po-
sition. If the second rule supplies the wrong prepo-
sition, a bad translation results. The IN node here
is an overlap point between rules. Considering that
certain nonterminal symbols may be more reliable
overlap points than others, we create a binary fea-
ture for each nonterminal. A rule like:
</bodyText>
<equation confidence="0.77275">
IN(at) ↔ zai
</equation>
<bodyText confidence="0.9997857">
will have feature rule-root-IN set to 1 and all
other rule-root features set to 0. Our rule root fea-
tures range over the original (non-split) nontermi-
nal set; we have 105 in total. Even though the
rule root features are locally attached to individual
rules—and therefore cause no additional problems
for the decoder search—they are aimed at problem-
atic rule/rule interactions.
Bad single-level rewrites Sometimes the decoder
uses questionable rules, for example:
</bodyText>
<equation confidence="0.987273">
PP(x0:VBN x1:NP-C) ↔ x0 x1
</equation>
<bodyText confidence="0.999893375">
This rule is learned from 62 cases in our training
data, where the VBN is almost always the word
given. However, the decoder misuses this rule with
other VBNs. So we can add a feature that penalizes
any rule in which a PP dominates a VBN and NP-C.
The feature class bad-rewrite comprises penalties
for the following configurations based on our analy-
sis of the tuning set:
</bodyText>
<equation confidence="0.998276375">
PP → VBN NP-C
PP-BAR → NP-C IN
VP → NP-C PP
CONJP → RB IN
2kw0 − wk2 + C
1
�m
i=1
</equation>
<page confidence="0.969695">
220
</page>
<bodyText confidence="0.999691571428571">
Node count features It is possible that the de-
coder creates English trees with too many or too few
nodes of a particular syntactic category. For exam-
ple, there may be an tendency to generate too many
determiners or past-tense verbs. We therefore add a
count feature for each of the 109 (non-split) English
nonterminal symbols. For a rule like
</bodyText>
<equation confidence="0.931259">
NPB(NNP(us) NNP(president) x0:NNP)
H meiguo zongtong x0
</equation>
<bodyText confidence="0.999864714285714">
the feature node-count-NPB gets value 1, node-
count-NNP gets value 2, and all others get 0.
Insertion features Among the rules we extract
from bilingual corpora are target-language insertion
rules, which have a word on the English side, but no
words on the source Chinese side. Sample syntax-
based insertion rules are:
</bodyText>
<equation confidence="0.9988425">
NPB(DT(the) x0:NN) H x0
S(x0:NP-C VP(VBZ(is) x1:VP-C)) H x0 x1
</equation>
<bodyText confidence="0.999997266666667">
We notice that our decoder, however, frequently fails
to insert words like is and are, which often have no
equivalent in the Chinese source. We also notice that
the-insertion rules sometimes have a good effect, as
in the translation “in the bloom of youth,” but other
times have a bad effect, as in “people seek areas of
the conspiracy.”
Each time the decoder uses (or fails to use) an in-
sertion rule, it incurs some risk. There is no guaran-
tee that the interaction of the rule probabilities and
the language model provides the best way to manage
this risk. We therefore provide MIRA with a feature
for each of the most common English words appear-
ing in insertion rules, e.g., insert-the and insert-is.
There are 35 such features.
</bodyText>
<subsectionHeader confidence="0.982597">
4.2 Source-side features
</subsectionHeader>
<bodyText confidence="0.999955934782609">
We now turn to features that make use of source-side
context. Although these features capture dependen-
cies that cross boundaries between rules, they are
still local in the sense that no new states need to
be added to the decoder. This is because the entire
source sentence, being fixed, is always available to
every feature.
Soft syntactic constraints Neither of our systems
uses source-side syntactic information; hence, both
could potentially benefit from soft syntactic con-
straints as described by Marton and Resnik (2008).
In brief, these features use the output of an in-
dependent syntactic parser on the source sentence,
rewarding decoder constituents that match syntac-
tic constituents and punishing decoder constituents
that cross syntactic constituents. We use separately-
tunable features for each syntactic category.
Structural distortion features Both of our sys-
tems have rules with variables that generalize over
possible fillers, but neither system’s basic model
conditions a rule application on the size of a filler,
making it difficult to distinguish long-distance re-
orderings from short-distance reorderings. To rem-
edy this problem, Chiang et al. (2008) introduce a
structural distortion model, which we include in our
experiment. Our syntax-based baseline includes the
generative version of this model already.
Word context During rule extraction, we retain
word alignments from the training data in the ex-
tracted rules. (If a rule is observed with more than
one set of word alignments, we keep only the
most frequent one.) We then define, for each triple
(f, e, f+1), a feature that counts the number of times
that f is aligned to e and f+1 occurs to the right of
f; and similarly for triples (f, e, f−1) with f−1 occur-
ring to the left of f. In order to limit the size of the
model, we restrict words to be among the 100 most
frequently occurring words from the training data;
all other words are replaced with a token &lt;unk&gt;.
These features are somewhat similar to features
used by Watanabe et al. (2007), but more in the spirit
of features used in the word sense disambiguation
model introduced by Lee and Ng (2002) and incor-
porated as a submodel of a translation system by
Chan et al. (2007); here, we are incorporating some
of its features directly into the translation model.
</bodyText>
<sectionHeader confidence="0.999668" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9999702">
For our experiments, we used a 260 million word
Chinese/English bitext. We ran GIZA++ on the en-
tire bitext to produce IBM Model 4 word align-
ments, and then the link deletion algorithm (Fossum
et al., 2008) to yield better-quality alignments. For
</bodyText>
<page confidence="0.998453">
221
</page>
<tableCaption confidence="0.985818">
Table 1: Adding new features with MIRA significantly improves translation accuracy. Scores are case-insensitive IBM
BLEU scores. * or ** = significantly better than MERT baseline (p &lt; 0.05 or 0.01, respectively).
</tableCaption>
<table confidence="0.789136727272727">
System Training
Hiero MERT
MIRA
Syntax MERT
MIRA
Features #
baseline 11
syntax, distortion 56
syntax, distortion, discount 61
all source-side, discount 10990
baseline 25
baseline 25
Tune Test
35.4 36.1
35.9 36.9*
36.6 37.3**
38.4 37.6**
38.6 39.5
38.5 39.8*
overlap 132 38.7 39.9*
node count 136 38.7 40.0**
all target-side, discount 283 39.6 40.6**
</table>
<bodyText confidence="0.998991333333333">
the syntax-based system, we ran a reimplementation
of the Collins parser (Collins, 1997) on the English
half of the bitext to produce parse trees, then restruc-
tured and relabeled them as described in Section 3.2.
Syntax-based rule extraction was performed on a 65
million word subset of the training data. For Hiero,
rules with up to two nonterminals were extracted
from a 38 million word subset and phrasal rules were
extracted from the remainder of the training data.
We trained three 5-gram language models: one on
the English half of the bitext, used by both systems,
one on one billion words of English, used by the
syntax-based system, and one on two billion words
of English, used by Hiero. Modified Kneser-Ney
smoothing (Chen and Goodman, 1998) was applied
to all language models. The language models are
represented using randomized data structures simi-
lar to those of Talbot et al. (2007).
Our tuning set (2010 sentences) and test set (1994
sentences) were drawn from newswire data from the
NIST 2004 and 2005 evaluations and the GALE pro-
gram (with no overlap at either the segment or doc-
ument level). For the source-side syntax features,
we used the Berkeley parser (Petrov et al., 2006) to
parse the Chinese side of both sets.
We implemented the source-side context features
for Hiero and the target-side syntax features for the
syntax-based system, and the discount features for
both. We then ran MIRA on the tuning set with 20
parallel learners for Hiero and 73 parallel learners
for the syntax-based system. We chose a stopping it-
eration based on the BLEU score on the tuning set,
and used the averaged feature weights from all iter-
</bodyText>
<table confidence="0.996588857142857">
Syntax-based Hiero
count weight count weight
1 +1.28 1 +2.23
2 +0.35 2 +0.77
3–5 −0.73 3 +0.54
6–10 −0.64 4 +0.29
5+ −0.02
</table>
<tableCaption confidence="0.947080666666667">
Table 2: Weights learned for discount features. Nega-
tive weights indicate bonuses; positive weights indicate
penalties.
</tableCaption>
<bodyText confidence="0.990818923076923">
ations of all learners to decode the test set.
The results (Table 1) show significant improve-
ments in both systems (p &lt; 0.01) over already very
strong MERT baselines. Adding the source-side and
discount features to Hiero yields a +1.5 BLEU im-
provement, and adding the target-side syntax and
discount features to the syntax-based system yields a
+1.1 BLEU improvement. The results also show that
for Hiero, the various classes of features contributed
roughly equally; for the syntax-based system, we see
that two of the feature classes make small contribu-
tions but time constraints unfortunately did not per-
mit isolated testing of all feature classes.
</bodyText>
<sectionHeader confidence="0.992743" genericHeader="method">
6 Analysis
</sectionHeader>
<bodyText confidence="0.999960714285714">
How did the various new features improve the trans-
lation quality of our two systems? We begin by ex-
amining the discount features. For these features,
we used slightly different schemes for the two sys-
tems, shown in Table 2 with their learned feature
weights. We see in both cases that one-count rules
are strongly penalized, as expected.
</bodyText>
<page confidence="0.994967">
222
</page>
<table confidence="0.977992">
Reward Penalty
−0.42 a +0.67 of
−0.13 are +0.56 the
−0.09 at +0.47 comma
−0.09 on +0.13 period
−0.05 was +0.11 in
−0.05 from +0.08 for
−0.04 ’s +0.06 to
−0.04 by +0.05 will
−0.04 is +0.04 and
−0.03 it +0.02 as
−0.03 its +0.02 have
... ...
</table>
<tableCaption confidence="0.997251">
Table 3: Weights learned for inserting target English
words with rules that lack Chinese words.
</tableCaption>
<subsectionHeader confidence="0.996088">
6.1 Syntax features
</subsectionHeader>
<bodyText confidence="0.999951266666667">
Table 3 shows word-insertion feature weights. The
system rewards insertion of forms of be; examples
1–3 in Figure 1 show typical improved translations
that result. Among determiners, inserting a is re-
warded, while inserting the is punished. This seems
to be because the is often part of a fixed phrase, such
as the White House, and therefore comes naturally
as part of larger phrasal rules. Inserting the outside
these fixed phrases is a risk that the generative model
is too inclined to take. We also note that the system
learns to punish unmotivated insertions of commas
and periods, which get into our grammar via quirks
in the MT training data.
Table 4 shows weights for rule-overlap features.
MIRA punishes the case where rules overlap with
an IN (preposition) node. This makes sense: if a
rule has a variable that can be filled by any English
preposition, there is a risk that an incorrect preposi-
tion will fill it. On the other hand, splitting at a pe-
riod is a safe bet, and frees the model to use rules that
dig deeper into NP and VP trees when constructing
a top-level S. Table 5 shows weights for generated
English nonterminals: SBAR-C nodes are rewarded
and commas are punished.
The combined effect of all weights is subtle.
To interpret them further, it helps to look at gross
changes in the system’s behavior. For example, a
major error in the baseline system is to move “X
said” or “X asked” from the beginning of the Chi-
nese input to the middle or end of the English trans-
</bodyText>
<table confidence="0.985764473684211">
Bonus Penalty
−0.50 period +0.93 IN
−0.39 VP-C +0.57 NNP
−0.36 VB +0.44 NN
−0.31 SG-C +0.41 DT
−0.30 MD +0.34 JJ
−0.26 VBG +0.24 right double quote
−0.25 ADJP +0.20 VBZ
−0.22 -LRB- +0.19 NP
−0.21 VP-BAR +0.16 TO
−0.20 NPB-BAR +0.15 ADJP-BAR
−0.16 FRAG +0.14 PRN-BAR
−0.16 PRN +0.14 NML
−0.15 NPB +0.13 comma
−0.13 RB +0.12 VBD
−0.12 SBAR-C +0.12 NNPS
−0.12 VP-C-BAR +0.12 PRP
−0.11 -RRB- +0.11 SG
... ...
</table>
<tableCaption confidence="0.9915755">
Table 4: Weights learned for employing rules whose En-
glish sides are rooted at particular syntactic categories.
</tableCaption>
<table confidence="0.983577842105263">
Bonus Penalty
−0.73 SBAR-C +1.30 comma
−0.54 VBZ +0.80 DT
−0.54 IN +0.58 PP
−0.52 NN +0.44 TO
−0.51 PP-C +0.33 NNP
−0.47 right double quote +0.30 NNS
−0.39 ADJP +0.30 NML
−0.34 POS +0.22 CD
−0.31 ADVP +0.18 PRN
−0.30 RP +0.16 SYM
−0.29 PRT +0.15 ADJP-BAR
−0.27 SG-C +0.15 NP
−0.22 S-C +0.15 MD
−0.21 NNPS +0.15 HYPH
−0.21 VP-BAR +0.14 PRN-BAR
−0.20 PRP +0.14 NP-C
−0.20 NPB-BAR +0.11 ADJP-C
... ...
</table>
<tableCaption confidence="0.990822">
Table 5: Weights learned for generating syntactic nodes
of various types anywhere in the English translation.
</tableCaption>
<page confidence="0.998712">
223
</page>
<bodyText confidence="0.999390666666667">
lation. The error occurs with many speaking verbs,
and each time, we trace it to a different rule. The
problematic rules can even be non-lexical, e.g.:
</bodyText>
<equation confidence="0.976406">
S(x0:NP-C x1:VP x2:, x3:NP-C x4:VP x5:.)
↔ x3 x4 x2 x0 x1 x5
</equation>
<bodyText confidence="0.999966882352941">
It is therefore difficult to come up with a straightfor-
ward feature to address the problem. However, when
we apply MIRA with the features already listed,
these translation errors all disappear, as demon-
strated by examples 4–5 in Figure 1. Why does this
happen? It turns out that in translation hypotheses
that move “X said” or “X asked” away from the be-
ginning of the sentence, more commas appear, and
fewer S-C and SBAR-C nodes appear. Therefore, the
new features work to discourage these hypotheses.
Example 6 shows additionally that commas next to
speaking verbs are now correctly deleted.
Examples 7–8 in Figure 1 show other kinds of
unanticipated improvements. We do not have space
for a fuller analysis, but we note that the specific ef-
fects we describe above account for only part of the
overall B1.Eu improvement.
</bodyText>
<subsectionHeader confidence="0.999874">
6.2 Word context features
</subsectionHeader>
<bodyText confidence="0.999958590909091">
In Table 6 are shown feature weights learned for the
word-context features. A surprising number of the
highest-weighted features have to do with transla-
tions of dates and bylines. Many of the penalties
seem to discourage spurious insertion or deletion
of frequent words (for, ’s, said, parentheses, and
quotes). Finally, we note that several of the features
(the third- and eighth-ranked reward and twelfth-
ranked penalty) shape the translation of shuo ‘said’,
preferring translations with an overt complementizer
that and without a comma. Thus these features work
together to attack a frequent problem that our target-
syntax features also addressed.
Figure 2 shows the performance of Hiero with all
of its features on the tuning and test sets over time.
The scores on the tuning set rise rapidly, and the
scores on the test set also rise, but much more slowly,
and there appears to be slight degradation after the
18th pass through the tuning data. This seems in line
with the finding of Watanabe et al. (2007) that with
on the order of 10,000 features, overfitting is possi-
ble, but we can still improve accuracy on new data.
</bodyText>
<figure confidence="0.9845615">
0 5 10 15 20 25
Epoch
</figure>
<figureCaption confidence="0.975586">
Figure 2: Using over 10,000 word-context features leads
to overfitting, but its detrimental effects are modest.
Scores on the tuning set were obtained from the 1-best
output of the online learning algorithm, whereas scores
on the test set were obtained using averaged weights.
</figureCaption>
<bodyText confidence="0.953728">
Early stopping would have given +0.2 B1.Eu over the
results reported in Table 1.1
</bodyText>
<sectionHeader confidence="0.988426" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9999852">
We have described a variety of features for statisti-
cal machine translation and applied them to syntax-
based and hierarchical systems. We saw that these
features, discriminatively trained using MIRA, led
to significant improvements, and took a closer look
at the results to see how the new features qualita-
tively improved translation quality. We draw three
conclusions from this study.
First, we have shown that these new features can
improve the performance even of top-scoring MT
systems. Second, these results add to a growing body
of evidence that MIRA is preferable to MERT for
discriminative training. When training over 10,000
features on a modest amount of data, we, like Watan-
abe et al. (2007), did observe overfitting, yet saw
improvements on new data. Third, we have shown
that syntax-based machine translation offers possi-
bilities for features not available in other models,
making syntax-based MT and MIRA an especially
strong combination for future work.
</bodyText>
<footnote confidence="0.9659725">
1It was this iteration, in fact, which was used to derive the
combined feature count used in the title of this paper.
</footnote>
<figure confidence="0.956316272727273">
38.5
38
37.5
37
36.5
36
35.5
35
Tune
Test
BLEU
</figure>
<page confidence="0.990772">
224
</page>
<note confidence="0.9138145">
1 MERT: the united states pending israeli clarification on golan settlement plan
MIRA: the united states is waiting for israeli clarification on golan settlement plan
</note>
<bodyText confidence="0.841133">
2 MERT: ... the average life expectancy of only 18 months , canada ’s minority goverment will ...
MIRA: ... the average life expectancy of canada’s previous minority government is only 18 months ...
</bodyText>
<listItem confidence="0.89892">
3 MERT: ... since un inspectors expelled by north korea ...
MIRA: ... since un inspectors were expelled by north korea ...
4 MERT: another thing is ... , &amp;quot; he said, &amp;quot; obviously, the first thing we need to do ... .
MIRA: he said: &amp;quot; obviously, the first thing we need to do ... , and another thing is .... &amp;quot;
5 MERT: the actual timing ... reopened in january , yoon said.
MIRA: yoon said the issue of the timing ...
6 MERT: ... us - led coalition forces , said today that the crash ...
MIRA: ... us - led coalition forces said today that a us military ...
7 MERT: ... and others will feel the danger.
MIRA: ... and others will not feel the danger.
8 MERT: in residential or public activities within 200 meters of the region, .. .
MIRA: within 200 m of residential or public activities area,...
</listItem>
<figureCaption confidence="0.999198">
Figure 1: Improved syntax-based translations due to MIRA-trained weights.
</figureCaption>
<figure confidence="0.989502">
f Bonus
e
−1.19 &lt;unk&gt; &lt;unk&gt;
−1.01 &lt;unk&gt; &lt;unk&gt;
−0.84 , that
−0.82 yue ‘month’ &lt;unk&gt;
−0.78 &amp;quot; &amp;quot;
−0.76 &amp;quot; &amp;quot;
−0.66 &lt;unk&gt; &lt;unk&gt;
−0.65 , that
...
context
</figure>
<equation confidence="0.96844825">
f−1 = ri ‘day’
f−1 = (
f−1 = shuo ‘say’
f+1 = &lt;unk&gt;
f−1 = &lt;unk&gt;
f+1 = &lt;unk&gt;
f+1 = nian ‘year’
f+1 = &lt;unk&gt;
</equation>
<table confidence="0.980293">
Penalty e
f
+1.12 &lt;unk&gt; )
+0.83 jiang ‘shall’ be
+0.83 zhengfu ‘government’ the
+0.73 &lt;unk&gt; )
+0.73 &lt;unk&gt; (
+0.72 &lt;unk&gt; )
+0.70 &lt;unk&gt; (
+0.69 &lt;unk&gt; (
+0.66 &lt;unk&gt; for
+0.66 &lt;unk&gt; ’s
+0.65 &lt;unk&gt; said
+0.60 , ,
context
</table>
<equation confidence="0.994156916666667">
f+1 = &lt;unk&gt;
f+1 = &lt;unk&gt;
f−1 = &lt;unk&gt;
f−1 = &lt;unk&gt;
f+1 = &lt;unk&gt;
f−1 = ri ‘day’
f−1 = ri ‘day’
f−1 = &lt;unk&gt;
f−1 = &lt;unk&gt;
f−1 = ,
f−1 = &lt;unk&gt;
f−1 = shuo ‘say’
</equation>
<bodyText confidence="0.444664">
...
</bodyText>
<tableCaption confidence="0.8788585">
Table 6: Weights learned for word-context features, which fire when English word e is generated aligned to Chinese
word f, with Chinese word f−1 to the left or f+1 to the right. Glosses for Chinese words are not part of features.
</tableCaption>
<page confidence="0.998312">
225
</page>
<sectionHeader confidence="0.98343" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999851660377358">
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008. A
discriminative latent variable model for statistical ma-
chine translation. In Proc. ACL-08: HLT.
Peter F. Brown, Stephen A. Della Pietra, Vincent Della J.
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational Linguistics, 19(2):263–312.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word sense disambiguation improves statistical ma-
chine translation. In Proc. ACL 2007.
Stanley F. Chen and Joshua T. Goodman. 1998. An
empirical study of smoothing techniques for language
modeling. Technical Report TR-10-98, Computer Sci-
ence Group, Harvard University.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proc. EMNLP 2008.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proc. ACL 2005.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2).
Michael Collins. 1997. Three generative, lexicalized
models for statistical parsing. In Proc. ACL 1997.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551–585.
Steve DeNeefe, Kevin Knight, Wei Wang, and Daniel
Marcu. 2007. What can syntax-based MT learn from
phrase-based MT? In Proc. EMNLP-CoNLL-2007.
Victoria Fossum, Kevin Knight, and Steven Abney. 2008.
Using syntax to improve word alignment for syntax-
based statistical machine translation. In Proc. Third
Workshop on Statistical Machine Translation.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What’s in a translation rule? In
Proc. HLT-NAACL 2004, Boston, Massachusetts.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic models. In Proc. ACL 2006.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proc. ACL 2008.
Kevin Knight and Jonathan Graehl. 2005. An overview
of probabilistic tree transducers for natural language
processing. In Proceedings of the Sixth International
Conference on Intelligent Text Processing and Compu-
tational Linguistics (CICLing).
Yoong Keok Lee and Hwee Tou Ng. 2002. An em-
pirical evaluation of knowledge sources and learn-
ing algorithms for word sense disambiguation. In
Proc. EMNLP 2002, pages 41–48.
Percy Liang, Alexandre Bouchard-Cˆot´e, Dan Klein, and
Ben Taskar. 2006. An end-to-end discriminative ap-
proach to machine translation. In Proc. COLING-ACL
2006.
Wolfgang Macherey, Franz Josef Och, Ignacio Thayer,
and Jakob Uskoreit. 2008. Lattice-based minimum
error rate training for statistical machine translation.
In Proc. EMNLP 2008.
Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrased-based translation.
In Proc. ACL-08: HLT.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In Proc. ACL 2002.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A
smorgasbord of features for statistical machine trans-
lation. In Proc. HLT-NAACL 2004, pages 161–168.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. ACL 2003.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proc. ACL 2006.
John C. Platt. 1998. Fast training of support vector
machines using sequential minimal optimization. In
B. Sch¨olkopf, C. J. C. Burges, and A. J. Smola, editors,
Advances in Kernel Methods: Support Vector Learn-
ing, pages 195–208. MIT Press.
David Talbot and Miles Osborne. 2007. Randomised
language modelling for statistical machine translation.
In Proc. ACL 2007, pages 512–519.
Christoph Tillmann and Tong Zhang. 2006. A discrimi-
native global training algorithm for statistical MT. In
Proc. COLING-ACL 2006.
Joseph Turian, Benjamin Wellington, and I. Dan
Melamed. 2007. Scalable discriminative learn-
ing for natural language parsing and translation. In
Proc. NIPS 2006.
Wei Wang, Kevin Knight, and Daniel Marcu. 2007. Bi-
narizing syntax trees to improve syntax-based machine
translation accuracy. In Proc. EMNLP-CoNLL 2007.
Taro Watanabe, Jun Suzuki, Hajime Tsukuda, and Hideki
Isozaki. 2007. Online large-margin training for statis-
tical machine translation. In Proc. EMNLP 2007.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23:377–404.
Kenji Yamada and Kevin Knight. 2002. A decoder for
syntax-based statistical MT. In Proc. ACL 2002.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for machine
translation. In Proc. HLT-NAACL 2006.
</reference>
<page confidence="0.998846">
226
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.979408">
<title confidence="0.995982">New Features for Statistical Machine</title>
<author confidence="0.999336">Chiang Knight Wei Wang</author>
<affiliation confidence="0.999398">USC Information Sciences Institute Language Weaver, Inc.</affiliation>
<address confidence="0.993647">4676 Admiralty Way, Suite 1001 4640 Admiralty Way, Suite 1210 Marina del Rey, CA 90292 USA Marina del Rey, CA 90292 USA</address>
<abstract confidence="0.999744666666667">We use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrasebased translation system and our syntax-based translation system. On a large-scale Chinese- English translation task, we obtain statistically improvements of respectively. We analyze the impact of the new features and the performance of the learning algorithm.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
<author>Miles Osborne</author>
</authors>
<title>A discriminative latent variable model for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proc. ACL-08: HLT.</booktitle>
<contexts>
<context position="3148" citStr="Blunsom et al., 2008" startWordPosition="507" endWordPosition="510"> reranking (Huang, 2008). Second, it attempted to incorporate syntax by applying off-the-shelf part-ofspeech taggers and parsers to MT output, a task these tools were never designed for. By contrast, we incorporate features directly into hierarchical and syntaxbased decoders. A third difficulty with Och et al.’s study was that it used MERT, which is not an ideal vehicle for feature exploration because it is observed not to perform well with large feature sets. Others have introduced alternative discriminative training methods (Tillmann and Zhang, 2006; Liang et al., 2006; Turian et al., 2007; Blunsom et al., 2008; Macherey et al., 2008), in which a recurring challenge is scalability: to train many features, we need many train218 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218–226, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics ing examples, and to train discriminatively, we need to search through all possible translations of each training example. Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset. We follow this ap</context>
</contexts>
<marker>Blunsom, Cohn, Osborne, 2008</marker>
<rawString>Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008. A discriminative latent variable model for statistical machine translation. In Proc. ACL-08: HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent Della J Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="4617" citStr="Brown et al., 1993" startWordPosition="734" endWordPosition="737"> of X1 As the number of nonterminals is limited to two, the grammar is equivalent to an inversion transduction grammar (Wu, 1997). The baseline model includes 12 features whose weights are optimized using MERT. Two of the features are n-gram language models, which require intersecting the synchronous CFG with finite-state automata representing the language models. This grammar can be parsed efficiently using cube pruning (Chiang, 2007). 3.2 Syntax-based system Our syntax-based system transforms source Chinese strings into target English syntax trees. Following previous work in statistical MT (Brown et al., 1993), we envision a noisy-channel model in which a language model generates English, and then a translation model transforms English trees into Chinese. We represent the translation model as a tree transducer (Knight and Graehl, 2005). It is obtained from bilingual text that has been word-aligned and whose English side has been syntactically parsed. From this data, we use the the GHKM minimal-rule extraction algorithm of (Galley et al., 2004) to yield rules like: NP-C(x0:NPB PP(IN(of x1:NPB)) H x1 de x0 Though this rule can be used in either direction, here we use it right-to-left (Chinese to Engl</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent Della J. Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Hwee Tou Ng</author>
<author>David Chiang</author>
</authors>
<title>Word sense disambiguation improves statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. ACL</booktitle>
<contexts>
<context position="14712" citStr="Chan et al. (2007)" startWordPosition="2456" endWordPosition="2459">re that counts the number of times that f is aligned to e and f+1 occurs to the right of f; and similarly for triples (f, e, f−1) with f−1 occurring to the left of f. In order to limit the size of the model, we restrict words to be among the 100 most frequently occurring words from the training data; all other words are replaced with a token &lt;unk&gt;. These features are somewhat similar to features used by Watanabe et al. (2007), but more in the spirit of features used in the word sense disambiguation model introduced by Lee and Ng (2002) and incorporated as a submodel of a translation system by Chan et al. (2007); here, we are incorporating some of its features directly into the translation model. 5 Experiments For our experiments, we used a 260 million word Chinese/English bitext. We ran GIZA++ on the entire bitext to produce IBM Model 4 word alignments, and then the link deletion algorithm (Fossum et al., 2008) to yield better-quality alignments. For 221 Table 1: Adding new features with MIRA significantly improves translation accuracy. Scores are case-insensitive IBM BLEU scores. * or ** = significantly better than MERT baseline (p &lt; 0.05 or 0.01, respectively). System Training Hiero MERT MIRA Synt</context>
</contexts>
<marker>Chan, Ng, Chiang, 2007</marker>
<rawString>Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007. Word sense disambiguation improves statistical machine translation. In Proc. ACL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua T Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1998</date>
<tech>Technical Report TR-10-98,</tech>
<institution>Computer Science Group, Harvard University.</institution>
<contexts>
<context position="16378" citStr="Chen and Goodman, 1998" startWordPosition="2728" endWordPosition="2731">duce parse trees, then restructured and relabeled them as described in Section 3.2. Syntax-based rule extraction was performed on a 65 million word subset of the training data. For Hiero, rules with up to two nonterminals were extracted from a 38 million word subset and phrasal rules were extracted from the remainder of the training data. We trained three 5-gram language models: one on the English half of the bitext, used by both systems, one on one billion words of English, used by the syntax-based system, and one on two billion words of English, used by Hiero. Modified Kneser-Ney smoothing (Chen and Goodman, 1998) was applied to all language models. The language models are represented using randomized data structures similar to those of Talbot et al. (2007). Our tuning set (2010 sentences) and test set (1994 sentences) were drawn from newswire data from the NIST 2004 and 2005 evaluations and the GALE program (with no overlap at either the segment or document level). For the source-side syntax features, we used the Berkeley parser (Petrov et al., 2006) to parse the Chinese side of both sets. We implemented the source-side context features for Hiero and the target-side syntax features for the syntax-base</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>Stanley F. Chen and Joshua T. Goodman. 1998. An empirical study of smoothing techniques for language modeling. Technical Report TR-10-98, Computer Science Group, Harvard University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Online large-margin training of syntactic and structural translation features.</title>
<date>2008</date>
<booktitle>In Proc. EMNLP</booktitle>
<contexts>
<context position="2158" citStr="Chiang et al., 2008" startWordPosition="340" endWordPosition="343">s research was supported in part by DARPA contract HR0011-06-C-0022 under subcontract to BBN Technologies. Many of the new features use syntactic information, and in particular depend on information that is available only inside a syntax-based translation model. Thus they widen the advantage that syntaxbased models have over other types of models. The models are trained using the Margin Infused Relaxed Algorithm or MIRA (Crammer et al., 2006) instead of the standard minimum-error-rate training or MERT algorithm (Och, 2003). Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks. 2 Related Work The work of Och et al (2004) is perhaps the bestknown study of new features and their impact on translation quality. However, it had a few shortcomings. First, it used the features for reranking n-best lists of translations, rather than for decoding or forest reranking (Huang, 2008). Second, it attempted to incorporate syntax by applying off-the-shelf part-ofspeech taggers and parsers to MT output, a task these tools were never designed for. By contrast, we incorporate features direct</context>
<context position="3651" citStr="Chiang et al., 2008" startWordPosition="585" endWordPosition="588">criminative training methods (Tillmann and Zhang, 2006; Liang et al., 2006; Turian et al., 2007; Blunsom et al., 2008; Macherey et al., 2008), in which a recurring challenge is scalability: to train many features, we need many train218 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218–226, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics ing examples, and to train discriminatively, we need to search through all possible translations of each training example. Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset. We follow this approach here. 3 Systems Used 3.1 Hiero Hiero (Chiang, 2005) is a hierarchical, string-tostring translation system. Its rules, which are extracted from unparsed, word-aligned parallel text, are synchronous CFG productions, for example: X → X1 de X2, X2 of X1 As the number of nonterminals is limited to two, the grammar is equivalent to an inversion transduction grammar (Wu, 1997). The baseline model includes 12 features whose weights are optimized using MERT. Two of the features are n-gram language mo</context>
<context position="7104" citStr="Chiang et al., 2008" startWordPosition="1138" endWordPosition="1141">tences. We include two other techniques in our baseline. To get more general translation rules, we restructure our English training trees using expectationmaximization (Wang et al., 2007), and to get more specific translation rules, we relabel the trees with up to 4 specialized versions of each nonterminal symbol, again using expectation-maximization and the split/merge technique of Petrov et al. (2006). 3.3 MIRA training We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008). Let e stand for output strings or their derivations, and let h(e) stand for the feature vector for e. Initialize the feature weights w. Then, repeatedly: • Select a batch of input sentences f1, ... , fm and decode each fi to obtain a forest of translations. • For each i, select from the forest a set of hypothesis translations ei1, ... , ein, which are the 219 10-best translations according to each of: h(e) · w Bᴇᴜ(e) + h(e) · w (1) −Bᴇᴜ(e) + h(e) · w • For each i, select an oracle translation: e∗ = arg max (Bᴇᴜ(e) + h(e) · w) (2) e Let Δhi j = h(e∗i ) − h(ei j). • For each eij, compute th</context>
<context position="13670" citStr="Chiang et al. (2008)" startWordPosition="2267" endWordPosition="2270">tures use the output of an independent syntactic parser on the source sentence, rewarding decoder constituents that match syntactic constituents and punishing decoder constituents that cross syntactic constituents. We use separatelytunable features for each syntactic category. Structural distortion features Both of our systems have rules with variables that generalize over possible fillers, but neither system’s basic model conditions a rule application on the size of a filler, making it difficult to distinguish long-distance reorderings from short-distance reorderings. To remedy this problem, Chiang et al. (2008) introduce a structural distortion model, which we include in our experiment. Our syntax-based baseline includes the generative version of this model already. Word context During rule extraction, we retain word alignments from the training data in the extracted rules. (If a rule is observed with more than one set of word alignments, we keep only the most frequent one.) We then define, for each triple (f, e, f+1), a feature that counts the number of times that f is aligned to e and f+1 occurs to the right of f; and similarly for triples (f, e, f−1) with f−1 occurring to the left of f. In order </context>
</contexts>
<marker>Chiang, Marton, Resnik, 2008</marker>
<rawString>David Chiang, Yuval Marton, and Philip Resnik. 2008. Online large-margin training of syntactic and structural translation features. In Proc. EMNLP 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. ACL</booktitle>
<contexts>
<context position="1497" citStr="Chiang, 2005" startWordPosition="238" endWordPosition="239">particularly as it pertains to improving the best systems we have. Further: • Do syntax-based translation systems have unique and effective levers to pull when designing new features? • Can large numbers of feature weights be learned efficiently and stably on modest amounts of data? In this paper, we address these questions by experimenting with a large number of new features. We add more than 250 features to improve a syntaxbased MT system—already the highest-scoring single system in the NIST 2008 Chinese-English common-data track—by +1.1 BLEU. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 BLEU improvement. *This research was supported in part by DARPA contract HR0011-06-C-0022 under subcontract to BBN Technologies. Many of the new features use syntactic information, and in particular depend on information that is available only inside a syntax-based translation model. Thus they widen the advantage that syntaxbased models have over other types of models. The models are trained using the Margin Infused Relaxed Algorithm or MIRA (Crammer et al., 2006) instead of the standard minimum-error-rate training or MERT algorithm (Och, 2003). Our results add to a growing </context>
<context position="3806" citStr="Chiang, 2005" startWordPosition="613" endWordPosition="614">g challenge is scalability: to train many features, we need many train218 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218–226, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics ing examples, and to train discriminatively, we need to search through all possible translations of each training example. Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset. We follow this approach here. 3 Systems Used 3.1 Hiero Hiero (Chiang, 2005) is a hierarchical, string-tostring translation system. Its rules, which are extracted from unparsed, word-aligned parallel text, are synchronous CFG productions, for example: X → X1 de X2, X2 of X1 As the number of nonterminals is limited to two, the grammar is equivalent to an inversion transduction grammar (Wu, 1997). The baseline model includes 12 features whose weights are optimized using MERT. Two of the features are n-gram language models, which require intersecting the synchronous CFG with finite-state automata representing the language models. This grammar can be parsed efficiently us</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proc. ACL 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="4437" citStr="Chiang, 2007" startWordPosition="711" endWordPosition="712">, string-tostring translation system. Its rules, which are extracted from unparsed, word-aligned parallel text, are synchronous CFG productions, for example: X → X1 de X2, X2 of X1 As the number of nonterminals is limited to two, the grammar is equivalent to an inversion transduction grammar (Wu, 1997). The baseline model includes 12 features whose weights are optimized using MERT. Two of the features are n-gram language models, which require intersecting the synchronous CFG with finite-state automata representing the language models. This grammar can be parsed efficiently using cube pruning (Chiang, 2007). 3.2 Syntax-based system Our syntax-based system transforms source Chinese strings into target English syntax trees. Following previous work in statistical MT (Brown et al., 1993), we envision a noisy-channel model in which a language model generates English, and then a translation model transforms English trees into Chinese. We represent the translation model as a tree transducer (Knight and Graehl, 2005). It is obtained from bilingual text that has been word-aligned and whose English side has been syntactically parsed. From this data, we use the the GHKM minimal-rule extraction algorithm of</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalized models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proc. ACL</booktitle>
<contexts>
<context position="15714" citStr="Collins, 1997" startWordPosition="2617" endWordPosition="2618">A significantly improves translation accuracy. Scores are case-insensitive IBM BLEU scores. * or ** = significantly better than MERT baseline (p &lt; 0.05 or 0.01, respectively). System Training Hiero MERT MIRA Syntax MERT MIRA Features # baseline 11 syntax, distortion 56 syntax, distortion, discount 61 all source-side, discount 10990 baseline 25 baseline 25 Tune Test 35.4 36.1 35.9 36.9* 36.6 37.3** 38.4 37.6** 38.6 39.5 38.5 39.8* overlap 132 38.7 39.9* node count 136 38.7 40.0** all target-side, discount 283 39.6 40.6** the syntax-based system, we ran a reimplementation of the Collins parser (Collins, 1997) on the English half of the bitext to produce parse trees, then restructured and relabeled them as described in Section 3.2. Syntax-based rule extraction was performed on a 65 million word subset of the training data. For Hiero, rules with up to two nonterminals were extracted from a 38 million word subset and phrasal rules were extracted from the remainder of the training data. We trained three 5-gram language models: one on the English half of the bitext, used by both systems, one on one billion words of English, used by the syntax-based system, and one on two billion words of English, used </context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three generative, lexicalized models for statistical parsing. In Proc. ACL 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai ShalevShwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online passiveaggressive algorithms.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>7--551</pages>
<contexts>
<context position="1984" citStr="Crammer et al., 2006" startWordPosition="312" endWordPosition="315"> system in the NIST 2008 Chinese-English common-data track—by +1.1 BLEU. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 BLEU improvement. *This research was supported in part by DARPA contract HR0011-06-C-0022 under subcontract to BBN Technologies. Many of the new features use syntactic information, and in particular depend on information that is available only inside a syntax-based translation model. Thus they widen the advantage that syntaxbased models have over other types of models. The models are trained using the Margin Infused Relaxed Algorithm or MIRA (Crammer et al., 2006) instead of the standard minimum-error-rate training or MERT algorithm (Och, 2003). Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks. 2 Related Work The work of Och et al (2004) is perhaps the bestknown study of new features and their impact on translation quality. However, it had a few shortcomings. First, it used the features for reranking n-best lists of translations, rather than for decoding or forest reranking (Huang, 2008). Second, it attempted to incorp</context>
<context position="7034" citStr="Crammer et al., 2006" startWordPosition="1127" endWordPosition="1130"> Knight, 2002; Galley et al., 2006) with cube pruning to decode new sentences. We include two other techniques in our baseline. To get more general translation rules, we restructure our English training trees using expectationmaximization (Wang et al., 2007), and to get more specific translation rules, we relabel the trees with up to 4 specialized versions of each nonterminal symbol, again using expectation-maximization and the split/merge technique of Petrov et al. (2006). 3.3 MIRA training We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008). Let e stand for output strings or their derivations, and let h(e) stand for the feature vector for e. Initialize the feature weights w. Then, repeatedly: • Select a batch of input sentences f1, ... , fm and decode each fi to obtain a forest of translations. • For each i, select from the forest a set of hypothesis translations ei1, ... , ein, which are the 219 10-best translations according to each of: h(e) · w Bᴇᴜ(e) + h(e) · w (1) −Bᴇᴜ(e) + h(e) · w • For each i, select an oracle translation: e∗ = arg max (Bᴇᴜ(e) + h(e</context>
</contexts>
<marker>Crammer, Dekel, Keshet, ShalevShwartz, Singer, 2006</marker>
<rawString>Koby Crammer, Ofer Dekel, Joseph Keshet, Shai ShalevShwartz, and Yoram Singer. 2006. Online passiveaggressive algorithms. Journal of Machine Learning Research, 7:551–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve DeNeefe</author>
<author>Kevin Knight</author>
<author>Wei Wang</author>
<author>Daniel Marcu</author>
</authors>
<title>What can syntax-based MT learn from phrase-based MT? In</title>
<date>2007</date>
<booktitle>Proc. EMNLP-CoNLL-2007.</booktitle>
<contexts>
<context position="5518" citStr="DeNeefe et al., 2007" startWordPosition="880" endWordPosition="883"> word-aligned and whose English side has been syntactically parsed. From this data, we use the the GHKM minimal-rule extraction algorithm of (Galley et al., 2004) to yield rules like: NP-C(x0:NPB PP(IN(of x1:NPB)) H x1 de x0 Though this rule can be used in either direction, here we use it right-to-left (Chinese to English). We follow Galley et al. (2006) in allowing unaligned Chinese words to participate in multiple translation rules, and in collecting larger rules composed of minimal rules. These larger rules have been shown to substantially improve translation accuracy (Galley et al., 2006; DeNeefe et al., 2007). We apply Good-Turing discounting to the transducer rule counts and obtain probability estimates: count(rule) P(rule) = count(LHS-root(rule)) When we apply these probabilities to derive an English sentence e and a corresponding Chinese sentence c, we wind up with the joint probability P(e, c). The baseline model includes log P(e, c), the two n-gram language models log P(e), and other features for a total of 25. For example, there is a pair of features to punish rules that drop Chinese content words or introduce spurious English content words. All features are linearly combined and their weigh</context>
</contexts>
<marker>DeNeefe, Knight, Wang, Marcu, 2007</marker>
<rawString>Steve DeNeefe, Kevin Knight, Wei Wang, and Daniel Marcu. 2007. What can syntax-based MT learn from phrase-based MT? In Proc. EMNLP-CoNLL-2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victoria Fossum</author>
<author>Kevin Knight</author>
<author>Steven Abney</author>
</authors>
<title>Using syntax to improve word alignment for syntaxbased statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proc. Third Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="15018" citStr="Fossum et al., 2008" startWordPosition="2508" endWordPosition="2511">l other words are replaced with a token &lt;unk&gt;. These features are somewhat similar to features used by Watanabe et al. (2007), but more in the spirit of features used in the word sense disambiguation model introduced by Lee and Ng (2002) and incorporated as a submodel of a translation system by Chan et al. (2007); here, we are incorporating some of its features directly into the translation model. 5 Experiments For our experiments, we used a 260 million word Chinese/English bitext. We ran GIZA++ on the entire bitext to produce IBM Model 4 word alignments, and then the link deletion algorithm (Fossum et al., 2008) to yield better-quality alignments. For 221 Table 1: Adding new features with MIRA significantly improves translation accuracy. Scores are case-insensitive IBM BLEU scores. * or ** = significantly better than MERT baseline (p &lt; 0.05 or 0.01, respectively). System Training Hiero MERT MIRA Syntax MERT MIRA Features # baseline 11 syntax, distortion 56 syntax, distortion, discount 61 all source-side, discount 10990 baseline 25 baseline 25 Tune Test 35.4 36.1 35.9 36.9* 36.6 37.3** 38.4 37.6** 38.6 39.5 38.5 39.8* overlap 132 38.7 39.9* node count 136 38.7 40.0** all target-side, discount 283 39.6</context>
</contexts>
<marker>Fossum, Knight, Abney, 2008</marker>
<rawString>Victoria Fossum, Kevin Knight, and Steven Abney. 2008. Using syntax to improve word alignment for syntaxbased statistical machine translation. In Proc. Third Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Proc. HLT-NAACL 2004,</booktitle>
<location>Boston, Massachusetts.</location>
<contexts>
<context position="5059" citStr="Galley et al., 2004" startWordPosition="806" endWordPosition="809">3.2 Syntax-based system Our syntax-based system transforms source Chinese strings into target English syntax trees. Following previous work in statistical MT (Brown et al., 1993), we envision a noisy-channel model in which a language model generates English, and then a translation model transforms English trees into Chinese. We represent the translation model as a tree transducer (Knight and Graehl, 2005). It is obtained from bilingual text that has been word-aligned and whose English side has been syntactically parsed. From this data, we use the the GHKM minimal-rule extraction algorithm of (Galley et al., 2004) to yield rules like: NP-C(x0:NPB PP(IN(of x1:NPB)) H x1 de x0 Though this rule can be used in either direction, here we use it right-to-left (Chinese to English). We follow Galley et al. (2006) in allowing unaligned Chinese words to participate in multiple translation rules, and in collecting larger rules composed of minimal rules. These larger rules have been shown to substantially improve translation accuracy (Galley et al., 2006; DeNeefe et al., 2007). We apply Good-Turing discounting to the transducer rule counts and obtain probability estimates: count(rule) P(rule) = count(LHS-root(rule)</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proc. HLT-NAACL 2004, Boston, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic models.</title>
<date>2006</date>
<booktitle>In Proc. ACL</booktitle>
<contexts>
<context position="5253" citStr="Galley et al. (2006)" startWordPosition="840" endWordPosition="843">noisy-channel model in which a language model generates English, and then a translation model transforms English trees into Chinese. We represent the translation model as a tree transducer (Knight and Graehl, 2005). It is obtained from bilingual text that has been word-aligned and whose English side has been syntactically parsed. From this data, we use the the GHKM minimal-rule extraction algorithm of (Galley et al., 2004) to yield rules like: NP-C(x0:NPB PP(IN(of x1:NPB)) H x1 de x0 Though this rule can be used in either direction, here we use it right-to-left (Chinese to English). We follow Galley et al. (2006) in allowing unaligned Chinese words to participate in multiple translation rules, and in collecting larger rules composed of minimal rules. These larger rules have been shown to substantially improve translation accuracy (Galley et al., 2006; DeNeefe et al., 2007). We apply Good-Turing discounting to the transducer rule counts and obtain probability estimates: count(rule) P(rule) = count(LHS-root(rule)) When we apply these probabilities to derive an English sentence e and a corresponding Chinese sentence c, we wind up with the joint probability P(e, c). The baseline model includes log P(e, c)</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic models. In Proc. ACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
</authors>
<title>Forest reranking: Discriminative parsing with non-local features.</title>
<date>2008</date>
<booktitle>In Proc. ACL</booktitle>
<contexts>
<context position="2552" citStr="Huang, 2008" startWordPosition="410" endWordPosition="411">xed Algorithm or MIRA (Crammer et al., 2006) instead of the standard minimum-error-rate training or MERT algorithm (Och, 2003). Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks. 2 Related Work The work of Och et al (2004) is perhaps the bestknown study of new features and their impact on translation quality. However, it had a few shortcomings. First, it used the features for reranking n-best lists of translations, rather than for decoding or forest reranking (Huang, 2008). Second, it attempted to incorporate syntax by applying off-the-shelf part-ofspeech taggers and parsers to MT output, a task these tools were never designed for. By contrast, we incorporate features directly into hierarchical and syntaxbased decoders. A third difficulty with Och et al.’s study was that it used MERT, which is not an ideal vehicle for feature exploration because it is observed not to perform well with large feature sets. Others have introduced alternative discriminative training methods (Tillmann and Zhang, 2006; Liang et al., 2006; Turian et al., 2007; Blunsom et al., 2008; Ma</context>
</contexts>
<marker>Huang, 2008</marker>
<rawString>Liang Huang. 2008. Forest reranking: Discriminative parsing with non-local features. In Proc. ACL 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Jonathan Graehl</author>
</authors>
<title>An overview of probabilistic tree transducers for natural language processing.</title>
<date>2005</date>
<booktitle>In Proceedings of the Sixth International Conference on Intelligent Text Processing and Computational Linguistics (CICLing).</booktitle>
<contexts>
<context position="4847" citStr="Knight and Graehl, 2005" startWordPosition="772" endWordPosition="775">tures are n-gram language models, which require intersecting the synchronous CFG with finite-state automata representing the language models. This grammar can be parsed efficiently using cube pruning (Chiang, 2007). 3.2 Syntax-based system Our syntax-based system transforms source Chinese strings into target English syntax trees. Following previous work in statistical MT (Brown et al., 1993), we envision a noisy-channel model in which a language model generates English, and then a translation model transforms English trees into Chinese. We represent the translation model as a tree transducer (Knight and Graehl, 2005). It is obtained from bilingual text that has been word-aligned and whose English side has been syntactically parsed. From this data, we use the the GHKM minimal-rule extraction algorithm of (Galley et al., 2004) to yield rules like: NP-C(x0:NPB PP(IN(of x1:NPB)) H x1 de x0 Though this rule can be used in either direction, here we use it right-to-left (Chinese to English). We follow Galley et al. (2006) in allowing unaligned Chinese words to participate in multiple translation rules, and in collecting larger rules composed of minimal rules. These larger rules have been shown to substantially i</context>
</contexts>
<marker>Knight, Graehl, 2005</marker>
<rawString>Kevin Knight and Jonathan Graehl. 2005. An overview of probabilistic tree transducers for natural language processing. In Proceedings of the Sixth International Conference on Intelligent Text Processing and Computational Linguistics (CICLing).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoong Keok Lee</author>
<author>Hwee Tou Ng</author>
</authors>
<title>An empirical evaluation of knowledge sources and learning algorithms for word sense disambiguation.</title>
<date>2002</date>
<booktitle>In Proc. EMNLP</booktitle>
<pages>41--48</pages>
<contexts>
<context position="14635" citStr="Lee and Ng (2002)" startWordPosition="2441" endWordPosition="2444">the most frequent one.) We then define, for each triple (f, e, f+1), a feature that counts the number of times that f is aligned to e and f+1 occurs to the right of f; and similarly for triples (f, e, f−1) with f−1 occurring to the left of f. In order to limit the size of the model, we restrict words to be among the 100 most frequently occurring words from the training data; all other words are replaced with a token &lt;unk&gt;. These features are somewhat similar to features used by Watanabe et al. (2007), but more in the spirit of features used in the word sense disambiguation model introduced by Lee and Ng (2002) and incorporated as a submodel of a translation system by Chan et al. (2007); here, we are incorporating some of its features directly into the translation model. 5 Experiments For our experiments, we used a 260 million word Chinese/English bitext. We ran GIZA++ on the entire bitext to produce IBM Model 4 word alignments, and then the link deletion algorithm (Fossum et al., 2008) to yield better-quality alignments. For 221 Table 1: Adding new features with MIRA significantly improves translation accuracy. Scores are case-insensitive IBM BLEU scores. * or ** = significantly better than MERT ba</context>
</contexts>
<marker>Lee, Ng, 2002</marker>
<rawString>Yoong Keok Lee and Hwee Tou Ng. 2002. An empirical evaluation of knowledge sources and learning algorithms for word sense disambiguation. In Proc. EMNLP 2002, pages 41–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Alexandre Bouchard-Cˆot´e</author>
<author>Dan Klein</author>
<author>Ben Taskar</author>
</authors>
<title>An end-to-end discriminative approach to machine translation.</title>
<date>2006</date>
<booktitle>In Proc. COLING-ACL</booktitle>
<marker>Liang, Bouchard-Cˆot´e, Klein, Taskar, 2006</marker>
<rawString>Percy Liang, Alexandre Bouchard-Cˆot´e, Dan Klein, and Ben Taskar. 2006. An end-to-end discriminative approach to machine translation. In Proc. COLING-ACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Macherey</author>
<author>Franz Josef Och</author>
<author>Ignacio Thayer</author>
<author>Jakob Uskoreit</author>
</authors>
<title>Lattice-based minimum error rate training for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proc. EMNLP</booktitle>
<contexts>
<context position="3172" citStr="Macherey et al., 2008" startWordPosition="511" endWordPosition="514">8). Second, it attempted to incorporate syntax by applying off-the-shelf part-ofspeech taggers and parsers to MT output, a task these tools were never designed for. By contrast, we incorporate features directly into hierarchical and syntaxbased decoders. A third difficulty with Och et al.’s study was that it used MERT, which is not an ideal vehicle for feature exploration because it is observed not to perform well with large feature sets. Others have introduced alternative discriminative training methods (Tillmann and Zhang, 2006; Liang et al., 2006; Turian et al., 2007; Blunsom et al., 2008; Macherey et al., 2008), in which a recurring challenge is scalability: to train many features, we need many train218 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218–226, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics ing examples, and to train discriminatively, we need to search through all possible translations of each training example. Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset. We follow this approach here. 3 Systems U</context>
</contexts>
<marker>Macherey, Och, Thayer, Uskoreit, 2008</marker>
<rawString>Wolfgang Macherey, Franz Josef Och, Ignacio Thayer, and Jakob Uskoreit. 2008. Lattice-based minimum error rate training for statistical machine translation. In Proc. EMNLP 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Soft syntactic constraints for hierarchical phrased-based translation.</title>
<date>2008</date>
<booktitle>In Proc. ACL-08: HLT.</booktitle>
<contexts>
<context position="13029" citStr="Marton and Resnik (2008)" startWordPosition="2173" endWordPosition="2176">ertion rules, e.g., insert-the and insert-is. There are 35 such features. 4.2 Source-side features We now turn to features that make use of source-side context. Although these features capture dependencies that cross boundaries between rules, they are still local in the sense that no new states need to be added to the decoder. This is because the entire source sentence, being fixed, is always available to every feature. Soft syntactic constraints Neither of our systems uses source-side syntactic information; hence, both could potentially benefit from soft syntactic constraints as described by Marton and Resnik (2008). In brief, these features use the output of an independent syntactic parser on the source sentence, rewarding decoder constituents that match syntactic constituents and punishing decoder constituents that cross syntactic constituents. We use separatelytunable features for each syntactic category. Structural distortion features Both of our systems have rules with variables that generalize over possible fillers, but neither system’s basic model conditions a rule application on the size of a filler, making it difficult to distinguish long-distance reorderings from short-distance reorderings. To </context>
</contexts>
<marker>Marton, Resnik, 2008</marker>
<rawString>Yuval Marton and Philip Resnik. 2008. Soft syntactic constraints for hierarchical phrased-based translation. In Proc. ACL-08: HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proc. ACL</booktitle>
<contexts>
<context position="6985" citStr="Och and Ney, 2002" startWordPosition="1118" endWordPosition="1121">6). Then we use a CKY-style parser (Yamada and Knight, 2002; Galley et al., 2006) with cube pruning to decode new sentences. We include two other techniques in our baseline. To get more general translation rules, we restructure our English training trees using expectationmaximization (Wang et al., 2007), and to get more specific translation rules, we relabel the trees with up to 4 specialized versions of each nonterminal symbol, again using expectation-maximization and the split/merge technique of Petrov et al. (2006). 3.3 MIRA training We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008). Let e stand for output strings or their derivations, and let h(e) stand for the feature vector for e. Initialize the feature weights w. Then, repeatedly: • Select a batch of input sentences f1, ... , fm and decode each fi to obtain a forest of translations. • For each i, select from the forest a set of hypothesis translations ei1, ... , ein, which are the 219 10-best translations according to each of: h(e) · w Bᴇᴜ(e) + h(e) · w (1) −Bᴇᴜ(e) + h(e) · w • For each i, select a</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proc. ACL 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Daniel Gildea</author>
</authors>
<title>Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar,</title>
<date>2004</date>
<booktitle>In Proc. HLT-NAACL</booktitle>
<pages>161--168</pages>
<location>Libin Shen, David Smith, Katherine Eng, Viren Jain, Zhen</location>
<marker>Och, Gildea, 2004</marker>
<rawString>Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar, Libin Shen, David Smith, Katherine Eng, Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A smorgasbord of features for statistical machine translation. In Proc. HLT-NAACL 2004, pages 161–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. ACL</booktitle>
<contexts>
<context position="2066" citStr="Och, 2003" startWordPosition="325" endWordPosition="326">n 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 BLEU improvement. *This research was supported in part by DARPA contract HR0011-06-C-0022 under subcontract to BBN Technologies. Many of the new features use syntactic information, and in particular depend on information that is available only inside a syntax-based translation model. Thus they widen the advantage that syntaxbased models have over other types of models. The models are trained using the Margin Infused Relaxed Algorithm or MIRA (Crammer et al., 2006) instead of the standard minimum-error-rate training or MERT algorithm (Och, 2003). Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks. 2 Related Work The work of Och et al (2004) is perhaps the bestknown study of new features and their impact on translation quality. However, it had a few shortcomings. First, it used the features for reranking n-best lists of translations, rather than for decoding or forest reranking (Huang, 2008). Second, it attempted to incorporate syntax by applying off-the-shelf part-ofspeech taggers and parsers to MT out</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proc. ACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proc. ACL</booktitle>
<contexts>
<context position="6890" citStr="Petrov et al. (2006)" startWordPosition="1101" endWordPosition="1104">in at most two variables and can be incrementally scored by the language model (Zhang et al., 2006). Then we use a CKY-style parser (Yamada and Knight, 2002; Galley et al., 2006) with cube pruning to decode new sentences. We include two other techniques in our baseline. To get more general translation rules, we restructure our English training trees using expectationmaximization (Wang et al., 2007), and to get more specific translation rules, we relabel the trees with up to 4 specialized versions of each nonterminal symbol, again using expectation-maximization and the split/merge technique of Petrov et al. (2006). 3.3 MIRA training We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008). Let e stand for output strings or their derivations, and let h(e) stand for the feature vector for e. Initialize the feature weights w. Then, repeatedly: • Select a batch of input sentences f1, ... , fm and decode each fi to obtain a forest of translations. • For each i, select from the forest a set of hypothesis translations ei1, ... , ein, which are the 219 10-best translations a</context>
<context position="16824" citStr="Petrov et al., 2006" startWordPosition="2804" endWordPosition="2807"> one billion words of English, used by the syntax-based system, and one on two billion words of English, used by Hiero. Modified Kneser-Ney smoothing (Chen and Goodman, 1998) was applied to all language models. The language models are represented using randomized data structures similar to those of Talbot et al. (2007). Our tuning set (2010 sentences) and test set (1994 sentences) were drawn from newswire data from the NIST 2004 and 2005 evaluations and the GALE program (with no overlap at either the segment or document level). For the source-side syntax features, we used the Berkeley parser (Petrov et al., 2006) to parse the Chinese side of both sets. We implemented the source-side context features for Hiero and the target-side syntax features for the syntax-based system, and the discount features for both. We then ran MIRA on the tuning set with 20 parallel learners for Hiero and 73 parallel learners for the syntax-based system. We chose a stopping iteration based on the BLEU score on the tuning set, and used the averaged feature weights from all iterSyntax-based Hiero count weight count weight 1 +1.28 1 +2.23 2 +0.35 2 +0.77 3–5 −0.73 3 +0.54 6–10 −0.64 4 +0.29 5+ −0.02 Table 2: Weights learned for</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proc. ACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John C Platt</author>
</authors>
<title>Fast training of support vector machines using sequential minimal optimization.</title>
<date>1998</date>
<booktitle>Advances in Kernel Methods: Support Vector Learning,</booktitle>
<pages>195--208</pages>
<editor>In B. Sch¨olkopf, C. J. C. Burges, and A. J. Smola, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="7933" citStr="Platt, 1998" startWordPosition="1310" endWordPosition="1311">ach fi to obtain a forest of translations. • For each i, select from the forest a set of hypothesis translations ei1, ... , ein, which are the 219 10-best translations according to each of: h(e) · w Bᴇᴜ(e) + h(e) · w (1) −Bᴇᴜ(e) + h(e) · w • For each i, select an oracle translation: e∗ = arg max (Bᴇᴜ(e) + h(e) · w) (2) e Let Δhi j = h(e∗i ) − h(ei j). • For each eij, compute the loss l&apos;ij = Bᴇᴜ(e∗i ) − Bᴇᴜ(eij) (3) • Update w to the value of w0 that minimizes: max (l&apos;ij − Δhi j · w0) (4) 1≤j≤n where C = 0.01. This minimization is performed by a variant of sequential minimal optimization (Platt, 1998). Following Chiang et al. (2008), we calculate the sentence Bᴇᴜ scores in (1), (2), and (3) in the context of some previous 1-best translations. We run 20 of these learners in parallel, and when training is finished, the weight vectors from all iterations of all learners are averaged together. Since the interface between the trainer and the decoder is fairly simple—for each sentence, the decoder sends the trainer a forest, and the trainer returns a weight update—it is easy to use this algorithm with a variety of CKY-based decoders: here, we are using it in conjunction with both the Hiero deco</context>
</contexts>
<marker>Platt, 1998</marker>
<rawString>John C. Platt. 1998. Fast training of support vector machines using sequential minimal optimization. In B. Sch¨olkopf, C. J. C. Burges, and A. J. Smola, editors, Advances in Kernel Methods: Support Vector Learning, pages 195–208. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Talbot</author>
<author>Miles Osborne</author>
</authors>
<title>Randomised language modelling for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. ACL</booktitle>
<pages>512--519</pages>
<marker>Talbot, Osborne, 2007</marker>
<rawString>David Talbot and Miles Osborne. 2007. Randomised language modelling for statistical machine translation. In Proc. ACL 2007, pages 512–519.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
<author>Tong Zhang</author>
</authors>
<title>A discriminative global training algorithm for statistical MT.</title>
<date>2006</date>
<booktitle>In Proc. COLING-ACL</booktitle>
<contexts>
<context position="3085" citStr="Tillmann and Zhang, 2006" startWordPosition="495" endWordPosition="498">ng n-best lists of translations, rather than for decoding or forest reranking (Huang, 2008). Second, it attempted to incorporate syntax by applying off-the-shelf part-ofspeech taggers and parsers to MT output, a task these tools were never designed for. By contrast, we incorporate features directly into hierarchical and syntaxbased decoders. A third difficulty with Och et al.’s study was that it used MERT, which is not an ideal vehicle for feature exploration because it is observed not to perform well with large feature sets. Others have introduced alternative discriminative training methods (Tillmann and Zhang, 2006; Liang et al., 2006; Turian et al., 2007; Blunsom et al., 2008; Macherey et al., 2008), in which a recurring challenge is scalability: to train many features, we need many train218 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218–226, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics ing examples, and to train discriminatively, we need to search through all possible translations of each training example. Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features</context>
</contexts>
<marker>Tillmann, Zhang, 2006</marker>
<rawString>Christoph Tillmann and Tong Zhang. 2006. A discriminative global training algorithm for statistical MT. In Proc. COLING-ACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Benjamin Wellington</author>
<author>I Dan Melamed</author>
</authors>
<title>Scalable discriminative learning for natural language parsing and translation.</title>
<date>2007</date>
<booktitle>In Proc. NIPS</booktitle>
<contexts>
<context position="3126" citStr="Turian et al., 2007" startWordPosition="503" endWordPosition="506">or decoding or forest reranking (Huang, 2008). Second, it attempted to incorporate syntax by applying off-the-shelf part-ofspeech taggers and parsers to MT output, a task these tools were never designed for. By contrast, we incorporate features directly into hierarchical and syntaxbased decoders. A third difficulty with Och et al.’s study was that it used MERT, which is not an ideal vehicle for feature exploration because it is observed not to perform well with large feature sets. Others have introduced alternative discriminative training methods (Tillmann and Zhang, 2006; Liang et al., 2006; Turian et al., 2007; Blunsom et al., 2008; Macherey et al., 2008), in which a recurring challenge is scalability: to train many features, we need many train218 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218–226, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics ing examples, and to train discriminatively, we need to search through all possible translations of each training example. Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small data</context>
</contexts>
<marker>Turian, Wellington, Melamed, 2007</marker>
<rawString>Joseph Turian, Benjamin Wellington, and I. Dan Melamed. 2007. Scalable discriminative learning for natural language parsing and translation. In Proc. NIPS 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Wang</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Binarizing syntax trees to improve syntax-based machine translation accuracy.</title>
<date>2007</date>
<booktitle>In Proc. EMNLP-CoNLL</booktitle>
<contexts>
<context position="6671" citStr="Wang et al., 2007" startWordPosition="1067" endWordPosition="1070">content words. All features are linearly combined and their weights are optimized using MERT. For efficient decoding with integrated n-gram language models, all transducer rules must be binarized into rules that contain at most two variables and can be incrementally scored by the language model (Zhang et al., 2006). Then we use a CKY-style parser (Yamada and Knight, 2002; Galley et al., 2006) with cube pruning to decode new sentences. We include two other techniques in our baseline. To get more general translation rules, we restructure our English training trees using expectationmaximization (Wang et al., 2007), and to get more specific translation rules, we relabel the trees with up to 4 specialized versions of each nonterminal symbol, again using expectation-maximization and the split/merge technique of Petrov et al. (2006). 3.3 MIRA training We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008). Let e stand for output strings or their derivations, and let h(e) stand for the feature vector for e. Initialize the feature weights w. Then, repeatedly: • Select a </context>
</contexts>
<marker>Wang, Knight, Marcu, 2007</marker>
<rawString>Wei Wang, Kevin Knight, and Daniel Marcu. 2007. Binarizing syntax trees to improve syntax-based machine translation accuracy. In Proc. EMNLP-CoNLL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taro Watanabe</author>
<author>Jun Suzuki</author>
<author>Hajime Tsukuda</author>
<author>Hideki Isozaki</author>
</authors>
<title>Online large-margin training for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. EMNLP</booktitle>
<contexts>
<context position="2136" citStr="Watanabe et al., 2007" startWordPosition="336" endWordPosition="339"> BLEU improvement. *This research was supported in part by DARPA contract HR0011-06-C-0022 under subcontract to BBN Technologies. Many of the new features use syntactic information, and in particular depend on information that is available only inside a syntax-based translation model. Thus they widen the advantage that syntaxbased models have over other types of models. The models are trained using the Margin Infused Relaxed Algorithm or MIRA (Crammer et al., 2006) instead of the standard minimum-error-rate training or MERT algorithm (Och, 2003). Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks. 2 Related Work The work of Och et al (2004) is perhaps the bestknown study of new features and their impact on translation quality. However, it had a few shortcomings. First, it used the features for reranking n-best lists of translations, rather than for decoding or forest reranking (Huang, 2008). Second, it attempted to incorporate syntax by applying off-the-shelf part-ofspeech taggers and parsers to MT output, a task these tools were never designed for. By contrast, we incor</context>
<context position="3629" citStr="Watanabe et al., 2007" startWordPosition="580" endWordPosition="584">roduced alternative discriminative training methods (Tillmann and Zhang, 2006; Liang et al., 2006; Turian et al., 2007; Blunsom et al., 2008; Macherey et al., 2008), in which a recurring challenge is scalability: to train many features, we need many train218 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218–226, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics ing examples, and to train discriminatively, we need to search through all possible translations of each training example. Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset. We follow this approach here. 3 Systems Used 3.1 Hiero Hiero (Chiang, 2005) is a hierarchical, string-tostring translation system. Its rules, which are extracted from unparsed, word-aligned parallel text, are synchronous CFG productions, for example: X → X1 de X2, X2 of X1 As the number of nonterminals is limited to two, the grammar is equivalent to an inversion transduction grammar (Wu, 1997). The baseline model includes 12 features whose weights are optimized using MERT. Two of the features </context>
<context position="7082" citStr="Watanabe et al., 2007" startWordPosition="1134" endWordPosition="1137">uning to decode new sentences. We include two other techniques in our baseline. To get more general translation rules, we restructure our English training trees using expectationmaximization (Wang et al., 2007), and to get more specific translation rules, we relabel the trees with up to 4 specialized versions of each nonterminal symbol, again using expectation-maximization and the split/merge technique of Petrov et al. (2006). 3.3 MIRA training We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008). Let e stand for output strings or their derivations, and let h(e) stand for the feature vector for e. Initialize the feature weights w. Then, repeatedly: • Select a batch of input sentences f1, ... , fm and decode each fi to obtain a forest of translations. • For each i, select from the forest a set of hypothesis translations ei1, ... , ein, which are the 219 10-best translations according to each of: h(e) · w Bᴇᴜ(e) + h(e) · w (1) −Bᴇᴜ(e) + h(e) · w • For each i, select an oracle translation: e∗ = arg max (Bᴇᴜ(e) + h(e) · w) (2) e Let Δhi j = h(e∗i ) − h(ei j). • Fo</context>
<context position="14523" citStr="Watanabe et al. (2007)" startWordPosition="2421" endWordPosition="2424">training data in the extracted rules. (If a rule is observed with more than one set of word alignments, we keep only the most frequent one.) We then define, for each triple (f, e, f+1), a feature that counts the number of times that f is aligned to e and f+1 occurs to the right of f; and similarly for triples (f, e, f−1) with f−1 occurring to the left of f. In order to limit the size of the model, we restrict words to be among the 100 most frequently occurring words from the training data; all other words are replaced with a token &lt;unk&gt;. These features are somewhat similar to features used by Watanabe et al. (2007), but more in the spirit of features used in the word sense disambiguation model introduced by Lee and Ng (2002) and incorporated as a submodel of a translation system by Chan et al. (2007); here, we are incorporating some of its features directly into the translation model. 5 Experiments For our experiments, we used a 260 million word Chinese/English bitext. We ran GIZA++ on the entire bitext to produce IBM Model 4 word alignments, and then the link deletion algorithm (Fossum et al., 2008) to yield better-quality alignments. For 221 Table 1: Adding new features with MIRA significantly improve</context>
<context position="23462" citStr="Watanabe et al. (2007)" startWordPosition="3949" endWordPosition="3952"> reward and twelfthranked penalty) shape the translation of shuo ‘said’, preferring translations with an overt complementizer that and without a comma. Thus these features work together to attack a frequent problem that our targetsyntax features also addressed. Figure 2 shows the performance of Hiero with all of its features on the tuning and test sets over time. The scores on the tuning set rise rapidly, and the scores on the test set also rise, but much more slowly, and there appears to be slight degradation after the 18th pass through the tuning data. This seems in line with the finding of Watanabe et al. (2007) that with on the order of 10,000 features, overfitting is possible, but we can still improve accuracy on new data. 0 5 10 15 20 25 Epoch Figure 2: Using over 10,000 word-context features leads to overfitting, but its detrimental effects are modest. Scores on the tuning set were obtained from the 1-best output of the online learning algorithm, whereas scores on the test set were obtained using averaged weights. Early stopping would have given +0.2 B1.Eu over the results reported in Table 1.1 7 Conclusion We have described a variety of features for statistical machine translation and applied th</context>
</contexts>
<marker>Watanabe, Suzuki, Tsukuda, Isozaki, 2007</marker>
<rawString>Taro Watanabe, Jun Suzuki, Hajime Tsukuda, and Hideki Isozaki. 2007. Online large-margin training for statistical machine translation. In Proc. EMNLP 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<pages>23--377</pages>
<contexts>
<context position="4127" citStr="Wu, 1997" startWordPosition="665" endWordPosition="666">rch through all possible translations of each training example. Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset. We follow this approach here. 3 Systems Used 3.1 Hiero Hiero (Chiang, 2005) is a hierarchical, string-tostring translation system. Its rules, which are extracted from unparsed, word-aligned parallel text, are synchronous CFG productions, for example: X → X1 de X2, X2 of X1 As the number of nonterminals is limited to two, the grammar is equivalent to an inversion transduction grammar (Wu, 1997). The baseline model includes 12 features whose weights are optimized using MERT. Two of the features are n-gram language models, which require intersecting the synchronous CFG with finite-state automata representing the language models. This grammar can be parsed efficiently using cube pruning (Chiang, 2007). 3.2 Syntax-based system Our syntax-based system transforms source Chinese strings into target English syntax trees. Following previous work in statistical MT (Brown et al., 1993), we envision a noisy-channel model in which a language model generates English, and then a translation model </context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23:377–404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A decoder for syntax-based statistical MT.</title>
<date>2002</date>
<booktitle>In Proc. ACL</booktitle>
<contexts>
<context position="6426" citStr="Yamada and Knight, 2002" startWordPosition="1028" endWordPosition="1031">y P(e, c). The baseline model includes log P(e, c), the two n-gram language models log P(e), and other features for a total of 25. For example, there is a pair of features to punish rules that drop Chinese content words or introduce spurious English content words. All features are linearly combined and their weights are optimized using MERT. For efficient decoding with integrated n-gram language models, all transducer rules must be binarized into rules that contain at most two variables and can be incrementally scored by the language model (Zhang et al., 2006). Then we use a CKY-style parser (Yamada and Knight, 2002; Galley et al., 2006) with cube pruning to decode new sentences. We include two other techniques in our baseline. To get more general translation rules, we restructure our English training trees using expectationmaximization (Wang et al., 2007), and to get more specific translation rules, we relabel the trees with up to 4 specialized versions of each nonterminal symbol, again using expectation-maximization and the split/merge technique of Petrov et al. (2006). 3.3 MIRA training We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al</context>
</contexts>
<marker>Yamada, Knight, 2002</marker>
<rawString>Kenji Yamada and Kevin Knight. 2002. A decoder for syntax-based statistical MT. In Proc. ACL 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Liang Huang</author>
<author>Daniel Gildea</author>
<author>Kevin Knight</author>
</authors>
<title>Synchronous binarization for machine translation.</title>
<date>2006</date>
<booktitle>In Proc. HLT-NAACL</booktitle>
<contexts>
<context position="6369" citStr="Zhang et al., 2006" startWordPosition="1018" endWordPosition="1021">nese sentence c, we wind up with the joint probability P(e, c). The baseline model includes log P(e, c), the two n-gram language models log P(e), and other features for a total of 25. For example, there is a pair of features to punish rules that drop Chinese content words or introduce spurious English content words. All features are linearly combined and their weights are optimized using MERT. For efficient decoding with integrated n-gram language models, all transducer rules must be binarized into rules that contain at most two variables and can be incrementally scored by the language model (Zhang et al., 2006). Then we use a CKY-style parser (Yamada and Knight, 2002; Galley et al., 2006) with cube pruning to decode new sentences. We include two other techniques in our baseline. To get more general translation rules, we restructure our English training trees using expectationmaximization (Wang et al., 2007), and to get more specific translation rules, we relabel the trees with up to 4 specialized versions of each nonterminal symbol, again using expectation-maximization and the split/merge technique of Petrov et al. (2006). 3.3 MIRA training We incorporate all our new features into a linear model (Oc</context>
</contexts>
<marker>Zhang, Huang, Gildea, Knight, 2006</marker>
<rawString>Hao Zhang, Liang Huang, Daniel Gildea, and Kevin Knight. 2006. Synchronous binarization for machine translation. In Proc. HLT-NAACL 2006.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>