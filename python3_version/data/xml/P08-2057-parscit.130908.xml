<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001712">
<title confidence="0.978214">
Enriching spoken language translation with dialog acts
</title>
<author confidence="0.7221955">
Vivek Kumar Rangarajan Sridhar
Shrikanth Narayanan
</author>
<affiliation confidence="0.8886505">
Speech Analysis and Interpretation Laboratory
University of Southern California
</affiliation>
<email confidence="0.970615">
vrangara@usc.edu,shri@sipi.usc.edu
</email>
<author confidence="0.382231">
Srinivas Bangalore
</author>
<affiliation confidence="0.324828">
AT&amp;T Labs - Research
</affiliation>
<address confidence="0.7748015">
180 Park Avenue
Florham Park, NJ 07932, U.S.A.
</address>
<email confidence="0.976562">
srini@research.att.com
</email>
<sectionHeader confidence="0.993365" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999399647058824">
Current statistical speech translation ap-
proaches predominantly rely on just text tran-
scripts and do not adequately utilize the
rich contextual information such as conveyed
through prosody and discourse function. In
this paper, we explore the role of context char-
acterized through dialog acts (DAs) in statis-
tical translation. We demonstrate the integra-
tion of the dialog acts in a phrase-based statis-
tical translation framework, employing 3 lim-
ited domain parallel corpora (Farsi-English,
Japanese-English and Chinese-English). For
all three language pairs, in addition to produc-
ing interpretable DA enriched target language
translations, we also obtain improvements in
terms of objective evaluation metrics such as
lexical selection accuracy and BLEU score.
</bodyText>
<sectionHeader confidence="0.998788" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999956543478261">
Recent approaches to statistical speech translation
have relied on improving translation quality with
the use of phrase translation (Och and Ney, 2003;
Koehn, 2004). The quality of phrase translation
is typically measured using n-gram precision based
metrics such as BLEU (Papineni et al., 2002) and
NIST scores. However, in many dialog based speech
translation scenarios, vital information beyond what
is robustly captured by words and phrases is car-
ried by the communicative act (e.g., question, ac-
knowledgement, etc.) representing the function of
the utterance. Our approach for incorporating di-
alog act tags in speech translation is motivated by
the fact that it is important to capture and convey
not only what is being communicated (the words)
but how something is being communicated (the con-
text). Augmenting current statistical translation
frameworks with dialog acts can potentially improve
translation quality and facilitate successful cross-
lingual interactions in terms of improved informa-
tion transfer.
Dialog act tags have been previously used in the
VERBMOBIL statistical speech-to-speech transla-
tion system (Reithinger et al., 1996). In that work,
the predicted DA tags were mainly used to improve
speech recognition, semantic evaluation, and infor-
mation extraction modules. Discourse information
in the form of speech acts has also been used in in-
terlingua translation systems (Mayfield et al., 1995)
to map input text to semantic concepts, which are
then translated to target text.
In contrast with previous work, in this paper we
demonstrate how dialog act tags can be directly ex-
ploited in phrase based statistical speech translation
systems (Koehn, 2004). The framework presented
in this paper is particularly suited for human-human
and human-computer interactions in a dialog set-
ting, where information loss due to erroneous con-
tent may be compensated to some extent through the
correct transfer of the appropriate dialog act. The
dialog acts can also be potentially used for impart-
ing correct utterance level intonation during speech
synthesis in the target language. Figure 1 shows an
example where the detection and transfer of dialog
act information is beneficial in resolving ambiguous
intention associated with the translation output.
</bodyText>
<figureCaption confidence="0.982942">
Figure 1: Example of speech translation output enriched with
dialog act
</figureCaption>
<bodyText confidence="0.999906714285714">
The remainder of this paper is organized as fol-
lows: Section 2 describes the dialog act tagger used
in this work, Section 3 formulates the problem, Sec-
tion 4 describes the parallel corpora used in our ex-
periments, Section 5 summarizes our experimental
results and Section 6 concludes the paper with a
brief discussion and outline for future work.
</bodyText>
<footnote confidence="0.682769">
2 Dialog act tagger
In this work, we use a dialog act tagger trained on
the Switchboard DAMSL corpus (Jurafsky et al.,
</footnote>
<page confidence="0.974013">
225
</page>
<reference confidence="0.220552">
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 225–228,
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</reference>
<bodyText confidence="0.99981724137931">
1998) using a maximum entropy (maxent) model.
The Switchboard-DAMSL (SWBD-DAMSL) cor-
pus consists of 1155 dialogs and 218,898 utterances
from the Switchboard corpus of telephone conver-
sations, tagged with discourse labels from a shal-
low discourse tagset. The original tagset of 375
unique tags was clustered to obtain 42 dialog tags
as in (Jurafsky et al., 1998). In addition, we also
grouped the 42 tags into 7 disjoint classes, based
on the frequency of the classes and grouped the re-
maining classes into an “Other” category constitut-
ing less than 3% of the entire data. The simplified
tagset consisted of the following classes: statement,
acknowledgment, abandoned, agreement, question,
appreciation, other.
We use a maximum entropy sequence tagging
model for the automatic DA tagging. Given a se-
quence of utterances U = u1, u2, · · · , un and a
dialog act vocabulary (di 2 D, |D |= K), we
need to assign the best dialog act sequence D∗ =
d1, d2, · · · , dn. The classifier is used to assign to
each utterance a dialog act label conditioned on a
vector of local contextual feature vectors comprising
the lexical, syntactic and acoustic information. We
used the machine learning toolkit LLAMA (Haffner,
2006) to estimate the conditional distribution using
maxent. The performance of the maxent dialog act
tagger on a test set comprising 29K utterances of
SWBD-DAMSL is shown in Table 1.
</bodyText>
<table confidence="0.9968782">
Cues used (current utterance) Accuracy (%)
42 tags 7 tags
Lexical 69.7 81.9
Lexical+Syntactic 70.0 82.4
Lexical+Syntactic+Prosodic 70.4 82.9
</table>
<tableCaption confidence="0.9879675">
Table 1: Dialog act tagging accuracies for various cues on the
SWBD-DAMSL corpus.
</tableCaption>
<sectionHeader confidence="0.95093" genericHeader="method">
3 Enriched translation using DAs
</sectionHeader>
<bodyText confidence="0.99074">
If Ss, Ts and St, Tt are the speech signals and equiv-
alent textual transcription in the source and target
language, and Ls the enriched representation for the
source speech, we formalize our proposed enriched
S2S translation in the following manner:
</bodyText>
<equation confidence="0.915011">
1: � P(St|Tt, Ls).P(Tt, Ts, Ls|Ss) (3)
Tt,Ts,Ls
</equation>
<bodyText confidence="0.999541142857143">
where Eq.(3) is obtained through conditional inde-
pendence assumptions. Even though the recogni-
tion and translation can be performed jointly (Ma-
tusov et al., 2005), typical S2S translation frame-
works compartmentalize the ASR, MT and TTS,
with each component maximized for performance
individually.
</bodyText>
<equation confidence="0.985261">
P(St|Ss) Pz� max
St P (St|Tt∗, L∗s)
P(Tt|Ts∗, L∗s) (4)
P(Ls|Ts∗,Ss) x max P(Ts|Ss)
Ts
</equation>
<bodyText confidence="0.999956454545455">
where Ts∗, Tt∗ and S∗t are the arguments maximiz-
ing each of the individual components in the transla-
tion engine. L∗ s is the rich annotation detected from
the source speech signal and text, Ss and Ts∗ respec-
tively. In this work, we do not address the speech
synthesis part and assume that we have access to the
reference transcripts or 1-best recognition hypothe-
sis of the source utterances. The rich annotations
(Ls) can be syntactic or semantic concepts (Gu et
al., 2006), prosody (Ag¨uero et al., 2006), or, as in
this work, dialog act tags.
</bodyText>
<subsectionHeader confidence="0.973219">
3.1 Phrase-based translation with dialog acts
</subsectionHeader>
<bodyText confidence="0.9991334">
One of the currently popular and predominant
schemes for statistical translation is the phrase-
based approach (Koehn, 2004). Typical phrase-
based SMT approaches obtain word-level align-
ments from a bilingual corpus using tools such as
GIZA++ (Och and Ney, 2003) and extract phrase
translation pairs from the bilingual word alignment
using heuristics. Suppose, the SMT had access to
source language dialog acts (Ls), the translation
problem may be reformulated as,
</bodyText>
<equation confidence="0.96882825">
Tt∗ = arg max P (Tt|Ts, Ls)
Tt
= arg max P(Ts|Tt,Ls).P(Tt|Ls) (5)
Tt
</equation>
<bodyText confidence="0.999889">
The first term in Eq.(5) corresponds to a dialog act
specific MT model and the second term to a dia-
log act specific language model. Given sufficient
amount of training data such a system can possibly
generate hypotheses that are more accurate than the
scheme without the use of dialog acts. However, for
small scale and limited domain applications, Eq.(5)
leads to an implicit partitioning of the data corpus
</bodyText>
<figure confidence="0.8457159">
S∗t = arg max P(St|Ss) (1)
St
P(St|Ss) = 1: P(St, Tt, Ts, Ls|Ss) (2)
Tt,Ts,Ls
max
St
x max
Tt
x max
Ls
</figure>
<page confidence="0.995511">
226
</page>
<table confidence="0.999625166666667">
Training Test
Farsi Eng Jap Eng Chinese Eng Farsi Eng Jap Eng Chinese Eng
Sentences 8066 12239 46311 925 604 506
Running words 76321 86756 64096 77959 351060 376615 5442 6073 4619 6028 3826 3897
Vocabulary 6140 3908 4271 2079 11178 11232 1487 1103 926 567 931 898
Singletons 2819 1508 2749 1156 4348 4866 903 573 638 316 600 931
</table>
<tableCaption confidence="0.999509">
Table 2: Statistics of the training and test data used in the experiments.
</tableCaption>
<bodyText confidence="0.999455789473684">
and might generate inferioir translations in terms of
lexical selection accuracy or BLEU score.
A natural step to overcome the sparsity issue is
to employ an appropriate back-off mechanism that
would exploit the phrase translation pairs derived
from the complete data. A typical phrase transla-
tion table consists of 5 phrase translation scores for
each pair of phrases, source-to-target phrase transla-
tion probability (A1), target-to-source phrase transla-
tion probability (AA source-to-target lexical weight
(A3), target-to-word lexical weight (A4) and phrase
penalty (A5= 2.718). The lexical weights are the
product of word translation probabilities obtained
from the word alignments. To each phrase trans-
lation table belonging to a particular DA-specific
translation model, we append those entries from the
baseline model that are not present in phrase table
of the DA-specific translation model. The appended
entries are weighted by a factor α.
</bodyText>
<equation confidence="0.994306">
(Ts —* Tt)L3 = (Ts —* Tt)Ls U fα.(Ts —* Tt)
s.t. (Ts —* Tt) V (Ts —* Tt)Ls} (6)
</equation>
<bodyText confidence="0.999956153846154">
where (Ts —* Tt) is a short-hand1 notation for a
phrase translation table. (Ts —* Tt)Ls is the DA-
specific phrase translation table, (Ts —* Tt) is the
phrase translation table constructed from entire data
and (Ts —* Tt)L3 is the newly interpolated phrase
translation table. The interpolation factor α is used
to weight each of the four translation scores (phrase
translation and lexical probabilities for the bilan-
guage) with the phrase penalty remaining a con-
stant. Such a scheme ensures that phrase translation
pairs belonging to a specific DA model are weighted
higher and also ensures better coverage than a parti-
tioned data set.
</bodyText>
<sectionHeader confidence="0.998528" genericHeader="method">
4 Data
</sectionHeader>
<bodyText confidence="0.9934595">
We report experiments on three different paral-
lel corpora: Farsi-English, Japanese-English and
</bodyText>
<footnote confidence="0.6568885">
1(Ts --+ Tt) represents the mapping between source alpha-
bet sequences to target alphabet sequences, where every pair
</footnote>
<bodyText confidence="0.967335555555556">
(ts1, , tsn, tt1, ,ttm) has a weight sequence λ1, , λ5
(five weights).
Chinese-English. The Farsi-English data used in
this paper was collected for human-mediated doctor-
patient mediated interactions in which an English
speaking doctor interacts with a Persian speaking
patient (Narayanan et al., 2006). We used a subset
of this corpus consisting of 9315 parallel sentences.
The Japanese-English parallel corpus is a part
of the “How May I Help You” (HMIHY) (Gorin
et al., 1997) corpus of operator-customer conversa-
tions related to telephone services. The corpus con-
sists of 12239 parallel sentences. The conversations
are spontaneous even though the domain is lim-
ited. The Chinese-English corpus corresponds to the
IWSLT06 training and 2005 development set com-
prising 46K and 506 sentences respectively (Paul,
2006).
</bodyText>
<sectionHeader confidence="0.990862" genericHeader="evaluation">
5 Experiments and Results
</sectionHeader>
<bodyText confidence="0.99606945">
In all our experiments we assume that the same di-
alog act is shared by a parallel sentence pair. Thus,
even though the dialog act prediction is performed
for English, we use the predicted dialog act as the di-
alog act for the source language sentence. We used
the Moses2 toolkit for statistical phrase-based trans-
lation. The language models were trigram models
created only from the training portion of each cor-
pus. Due to the relatively small size of the corpora
used in the experiments, we could not devote a sep-
arate development set for tuning the parameters of
the phrase-based translation scheme. Hence, the ex-
periments are strictly performed on the training and
test sets reported in Table 23.
The lexical selection accuracy and BLEU scores
for the three parallel corpora is presented in Table 3.
Lexical selection accuracy is measured in terms of
the F-measure derived from recall ( |R es∩R|ef  |* 100)
 |Refand precision ( |R es∩Ref  |* 100), where Ref is the
|Reset of words in the reference translation and Res is
</bodyText>
<footnote confidence="0.985698666666667">
2http://www.statmt.org/moses
3A very small subset of the data was reserved for optimizing
the interpolation factor (α) described in Section 3.1
</footnote>
<page confidence="0.984503">
227
</page>
<table confidence="0.999486333333333">
Language pair F-score (%) BLEU (%)
w/o DA tags w/ DA tags w/o DA tags w/ DA tags
7tags 42tags 7tags 42tags
Farsi-English 56.46 57.32 57.74 22.90 23.50 23.75
Japanese-English 79.05 79.40 79.51 54.15 54.21 54.32
Chinese-English 65.85 67.24 67.49 48.59 52.12 53.04
</table>
<tableCaption confidence="0.999675">
Table 3: F-measure and BLEU scores with and without use of dialog act tags.
</tableCaption>
<bodyText confidence="0.999915652173913">
the set of words in the translation output. Adding di-
alog act tags (either 7 or 42 tag vocabulary) consis-
tently improves both the lexical selection accuracy
and BLEU score for all the language pairs. The im-
provements for Farsi-English and Chinese-English
corpora are more pronounced than the improve-
ments in Japanese-English corpus. This is due to the
skewed distribution of dialog acts in the Japanese-
English corpus; 80% of the test data are statements
while other and questions category make up 16%
and 3.5% of the data respectively. The important
observation here is that, appending DA tags in the
form described in this work, can improve translation
performance even in terms of conventional objective
evaluation metrics. However, the performance gain
measured in terms of objective metrics that are de-
signed to reflect only the orthographic accuracy dur-
ing translation is not a complete evaluation of the
translation quality of the proposed framework. We
are currently planning of adding human evaluation
to bring to fore the usefulness of such rich anno-
tations in interpreting and supplementing typically
noisy translations.
</bodyText>
<sectionHeader confidence="0.998752" genericHeader="conclusions">
6 Discussion and Future Work
</sectionHeader>
<bodyText confidence="0.999979875">
It is important to note that the dialog act tags used
in our translation system are predictions from the
maxent based DA tagger described in Section 2. We
do not have access to the reference tags; thus, some
amount of error is to be expected in the DA tagging.
Despite the lack of reference DA tags, we are still
able to achieve modest improvements in the trans-
lation quality. Improving the current DA tagger and
developing suitable adaptation techniques are part of
future work.
While we have demonstrated here that using dia-
log act tags can improve translation quality in terms
of word based automatic evaluation metrics, the real
benefits of such a scheme would be attested through
further human evaluations. We are currently work-
ing on conducting subjective evaluations.
</bodyText>
<sectionHeader confidence="0.998472" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999366977777778">
P. D. Ag¨uero, J. Adell, and A. Bonafonte. 2006. Prosody
generation for speech-to-speech translation. In Proc.
ofICASSP, Toulouse, France, May.
A. Gorin, G. Riccardi, and J. Wright. 1997. How May I
Help You? Speech Communication, 23:113–127.
L. Gu, Y. Gao, F. H. Liu, and M. Picheny. 2006.
Concept-based speech-to-speech translation using
maximum entropy models for statistical natural con-
cept generation. IEEE Transactions on Audio, Speech
and Language Processing, 14(2):377–392, March.
P. Haffner. 2006. Scaling large margin classifiers for spo-
ken language understanding. Speech Communication,
48(iv):239–261.
D. Jurafsky, R. Bates, N. Coccaro, R. Martin, M. Meteer,
K. Ries, E. Shriberg, S. Stolcke, P. Taylor, and C. Van
Ess-Dykema. 1998. Switchboard discourse language
modeling project report. Technical report research
note 30, Johns Hopkins University, Baltimore, MD.
P. Koehn. 2004. Pharaoh: A beam search decoder for
phrasebased statistical machine translation models. In
Proc. ofAMTA-04, pages 115–124.
E. Matusov, S. Kanthak, and H. Ney. 2005. On the in-
tegration of speech recognition and statistical machine
translation. In Proc. of Eurospeech.
L. Mayfield, M. Gavalda, W. Ward, and A. Waibel.
1995. Concept-based speech translation. In Proc. of
ICASSP, volume 1, pages 97–100, May.
S. Narayanan et al. 2006. Speech recognition engineer-
ing issues in speech to speech translation system de-
sign for low resource languages and domains. In Proc.
ofICASSP, Toulose, France, May.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19–51.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. Technical report, IBM T.J. Watson Re-
search Center.
M. Paul. 2006. Overview of the IWSLT 2006 Evaluation
Campaign. In Proc. of the IWSLT, pages 1–15, Kyoto,
Japan.
N. Reithinger, R. Engel, M. Kipp, and M. Klesen. 1996.
Predicting dialogue acts for a speech-to-speech trans-
lation system. In Proc. of ICSLP, volume 2, pages
654–657, Oct.
</reference>
<page confidence="0.997641">
228
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.734980">
<title confidence="0.998671">Enriching spoken language translation with dialog acts</title>
<author confidence="0.932822">Vivek Kumar Rangarajan Sridhar Shrikanth Narayanan</author>
<affiliation confidence="0.9994835">Speech Analysis and Interpretation Laboratory University of Southern California</affiliation>
<email confidence="0.999156">vrangara@usc.edu,shri@sipi.usc.edu</email>
<author confidence="0.870148">Srinivas Bangalore</author>
<affiliation confidence="0.996486">AT&amp;T Labs - Research</affiliation>
<address confidence="0.9986495">180 Park Avenue Florham Park, NJ 07932, U.S.A.</address>
<email confidence="0.999899">srini@research.att.com</email>
<abstract confidence="0.9990335">Current statistical speech translation approaches predominantly rely on just text transcripts and do not adequately utilize the rich contextual information such as conveyed through prosody and discourse function. In this paper, we explore the role of context charthrough acts in statistical translation. We demonstrate the integration of the dialog acts in a phrase-based statistical translation framework, employing 3 limited domain parallel corpora (Farsi-English, Japanese-English and Chinese-English). For all three language pairs, in addition to producing interpretable DA enriched target language translations, we also obtain improvements in terms of objective evaluation metrics such as lexical selection accuracy and BLEU score.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<booktitle>Proceedings of ACL-08: HLT, Short Papers (Companion Volume),</booktitle>
<pages>225--228</pages>
<marker></marker>
<rawString>Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 225–228,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Columbus</author>
</authors>
<title>c�2008 Association for Computational Linguistics</title>
<date>2008</date>
<booktitle>In Proc. ofICASSP,</booktitle>
<location>Ohio, USA,</location>
<marker>Columbus, 2008</marker>
<rawString>Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics P. D. Ag¨uero, J. Adell, and A. Bonafonte. 2006. Prosody generation for speech-to-speech translation. In Proc. ofICASSP, Toulouse, France, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gorin</author>
<author>G Riccardi</author>
<author>J Wright</author>
</authors>
<title>How May I Help You? Speech Communication,</title>
<date>1997</date>
<pages>23--113</pages>
<marker>Gorin, Riccardi, Wright, 1997</marker>
<rawString>A. Gorin, G. Riccardi, and J. Wright. 1997. How May I Help You? Speech Communication, 23:113–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Gu</author>
<author>Y Gao</author>
<author>F H Liu</author>
<author>M Picheny</author>
</authors>
<title>Concept-based speech-to-speech translation using maximum entropy models for statistical natural concept generation.</title>
<date>2006</date>
<journal>IEEE Transactions on Audio, Speech and Language Processing,</journal>
<volume>14</volume>
<issue>2</issue>
<marker>Gu, Gao, Liu, Picheny, 2006</marker>
<rawString>L. Gu, Y. Gao, F. H. Liu, and M. Picheny. 2006. Concept-based speech-to-speech translation using maximum entropy models for statistical natural concept generation. IEEE Transactions on Audio, Speech and Language Processing, 14(2):377–392, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Haffner</author>
</authors>
<title>Scaling large margin classifiers for spoken language understanding. Speech Communication,</title>
<date>2006</date>
<marker>Haffner, 2006</marker>
<rawString>P. Haffner. 2006. Scaling large margin classifiers for spoken language understanding. Speech Communication, 48(iv):239–261.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Jurafsky</author>
<author>R Bates</author>
<author>N Coccaro</author>
<author>R Martin</author>
<author>M Meteer</author>
<author>K Ries</author>
<author>E Shriberg</author>
<author>S Stolcke</author>
<author>P Taylor</author>
<author>C Van Ess-Dykema</author>
</authors>
<title>Switchboard discourse language modeling project report. Technical report research note 30,</title>
<date>1998</date>
<institution>Johns Hopkins University,</institution>
<location>Baltimore, MD.</location>
<marker>Jurafsky, Bates, Coccaro, Martin, Meteer, Ries, Shriberg, Stolcke, Taylor, Van Ess-Dykema, 1998</marker>
<rawString>D. Jurafsky, R. Bates, N. Coccaro, R. Martin, M. Meteer, K. Ries, E. Shriberg, S. Stolcke, P. Taylor, and C. Van Ess-Dykema. 1998. Switchboard discourse language modeling project report. Technical report research note 30, Johns Hopkins University, Baltimore, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Pharaoh: A beam search decoder for phrasebased statistical machine translation models.</title>
<date>2004</date>
<booktitle>In Proc. ofAMTA-04,</booktitle>
<pages>115--124</pages>
<contexts>
<context position="1279" citStr="Koehn, 2004" startWordPosition="173" endWordPosition="174">We demonstrate the integration of the dialog acts in a phrase-based statistical translation framework, employing 3 limited domain parallel corpora (Farsi-English, Japanese-English and Chinese-English). For all three language pairs, in addition to producing interpretable DA enriched target language translations, we also obtain improvements in terms of objective evaluation metrics such as lexical selection accuracy and BLEU score. 1 Introduction Recent approaches to statistical speech translation have relied on improving translation quality with the use of phrase translation (Och and Ney, 2003; Koehn, 2004). The quality of phrase translation is typically measured using n-gram precision based metrics such as BLEU (Papineni et al., 2002) and NIST scores. However, in many dialog based speech translation scenarios, vital information beyond what is robustly captured by words and phrases is carried by the communicative act (e.g., question, acknowledgement, etc.) representing the function of the utterance. Our approach for incorporating dialog act tags in speech translation is motivated by the fact that it is important to capture and convey not only what is being communicated (the words) but how someth</context>
<context position="2789" citStr="Koehn, 2004" startWordPosition="403" endWordPosition="404">e VERBMOBIL statistical speech-to-speech translation system (Reithinger et al., 1996). In that work, the predicted DA tags were mainly used to improve speech recognition, semantic evaluation, and information extraction modules. Discourse information in the form of speech acts has also been used in interlingua translation systems (Mayfield et al., 1995) to map input text to semantic concepts, which are then translated to target text. In contrast with previous work, in this paper we demonstrate how dialog act tags can be directly exploited in phrase based statistical speech translation systems (Koehn, 2004). The framework presented in this paper is particularly suited for human-human and human-computer interactions in a dialog setting, where information loss due to erroneous content may be compensated to some extent through the correct transfer of the appropriate dialog act. The dialog acts can also be potentially used for imparting correct utterance level intonation during speech synthesis in the target language. Figure 1 shows an example where the detection and transfer of dialog act information is beneficial in resolving ambiguous intention associated with the translation output. Figure 1: Ex</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>P. Koehn. 2004. Pharaoh: A beam search decoder for phrasebased statistical machine translation models. In Proc. ofAMTA-04, pages 115–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Matusov</author>
<author>S Kanthak</author>
<author>H Ney</author>
</authors>
<title>On the integration of speech recognition and statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. of</booktitle>
<volume>1</volume>
<pages>97--100</pages>
<marker>Matusov, Kanthak, Ney, 2005</marker>
<rawString>E. Matusov, S. Kanthak, and H. Ney. 2005. On the integration of speech recognition and statistical machine translation. In Proc. of Eurospeech. L. Mayfield, M. Gavalda, W. Ward, and A. Waibel. 1995. Concept-based speech translation. In Proc. of ICASSP, volume 1, pages 97–100, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Narayanan</author>
</authors>
<title>Speech recognition engineering issues in speech to speech translation system design for low resource languages and domains. In</title>
<date>2006</date>
<booktitle>Proc. ofICASSP,</booktitle>
<location>Toulose, France,</location>
<marker>Narayanan, 2006</marker>
<rawString>S. Narayanan et al. 2006. Speech recognition engineering issues in speech to speech translation system design for low resource languages and domains. In Proc. ofICASSP, Toulose, France, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="1265" citStr="Och and Ney, 2003" startWordPosition="169" endWordPosition="172">tical translation. We demonstrate the integration of the dialog acts in a phrase-based statistical translation framework, employing 3 limited domain parallel corpora (Farsi-English, Japanese-English and Chinese-English). For all three language pairs, in addition to producing interpretable DA enriched target language translations, we also obtain improvements in terms of objective evaluation metrics such as lexical selection accuracy and BLEU score. 1 Introduction Recent approaches to statistical speech translation have relied on improving translation quality with the use of phrase translation (Och and Ney, 2003; Koehn, 2004). The quality of phrase translation is typically measured using n-gram precision based metrics such as BLEU (Papineni et al., 2002) and NIST scores. However, in many dialog based speech translation scenarios, vital information beyond what is robustly captured by words and phrases is carried by the communicative act (e.g., question, acknowledgement, etc.) representing the function of the utterance. Our approach for incorporating dialog act tags in speech translation is motivated by the fact that it is important to capture and convey not only what is being communicated (the words) </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F. J. Och and H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W J Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<tech>Technical report, IBM</tech>
<institution>T.J. Watson Research Center.</institution>
<contexts>
<context position="1410" citStr="Papineni et al., 2002" startWordPosition="191" endWordPosition="194">d domain parallel corpora (Farsi-English, Japanese-English and Chinese-English). For all three language pairs, in addition to producing interpretable DA enriched target language translations, we also obtain improvements in terms of objective evaluation metrics such as lexical selection accuracy and BLEU score. 1 Introduction Recent approaches to statistical speech translation have relied on improving translation quality with the use of phrase translation (Och and Ney, 2003; Koehn, 2004). The quality of phrase translation is typically measured using n-gram precision based metrics such as BLEU (Papineni et al., 2002) and NIST scores. However, in many dialog based speech translation scenarios, vital information beyond what is robustly captured by words and phrases is carried by the communicative act (e.g., question, acknowledgement, etc.) representing the function of the utterance. Our approach for incorporating dialog act tags in speech translation is motivated by the fact that it is important to capture and convey not only what is being communicated (the words) but how something is being communicated (the context). Augmenting current statistical translation frameworks with dialog acts can potentially imp</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. Technical report, IBM T.J. Watson Research Center.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Paul</author>
</authors>
<title>Overview of the IWSLT</title>
<date>2006</date>
<booktitle>In Proc. of the IWSLT,</booktitle>
<pages>1--15</pages>
<location>Kyoto, Japan.</location>
<marker>Paul, 2006</marker>
<rawString>M. Paul. 2006. Overview of the IWSLT 2006 Evaluation Campaign. In Proc. of the IWSLT, pages 1–15, Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Reithinger</author>
<author>R Engel</author>
<author>M Kipp</author>
<author>M Klesen</author>
</authors>
<title>Predicting dialogue acts for a speech-to-speech translation system.</title>
<date>1996</date>
<booktitle>In Proc. of ICSLP,</booktitle>
<volume>2</volume>
<pages>654--657</pages>
<contexts>
<context position="2262" citStr="Reithinger et al., 1996" startWordPosition="317" endWordPosition="320">presenting the function of the utterance. Our approach for incorporating dialog act tags in speech translation is motivated by the fact that it is important to capture and convey not only what is being communicated (the words) but how something is being communicated (the context). Augmenting current statistical translation frameworks with dialog acts can potentially improve translation quality and facilitate successful crosslingual interactions in terms of improved information transfer. Dialog act tags have been previously used in the VERBMOBIL statistical speech-to-speech translation system (Reithinger et al., 1996). In that work, the predicted DA tags were mainly used to improve speech recognition, semantic evaluation, and information extraction modules. Discourse information in the form of speech acts has also been used in interlingua translation systems (Mayfield et al., 1995) to map input text to semantic concepts, which are then translated to target text. In contrast with previous work, in this paper we demonstrate how dialog act tags can be directly exploited in phrase based statistical speech translation systems (Koehn, 2004). The framework presented in this paper is particularly suited for human-</context>
</contexts>
<marker>Reithinger, Engel, Kipp, Klesen, 1996</marker>
<rawString>N. Reithinger, R. Engel, M. Kipp, and M. Klesen. 1996. Predicting dialogue acts for a speech-to-speech translation system. In Proc. of ICSLP, volume 2, pages 654–657, Oct.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>