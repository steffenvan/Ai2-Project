<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000023">
<title confidence="0.9986125">
Quasi-Synchronous Phrase Dependency Grammars
for Machine Translation
</title>
<author confidence="0.996339">
Kevin Gimpel Noah A. Smith
</author>
<affiliation confidence="0.847374666666667">
Language Technologies Institute
Carnegie Mellon Univeristy
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.999507">
{kgimpel,nasmith}@cs.cmu.edu
</email>
<sectionHeader confidence="0.994816" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999462833333333">
We present a quasi-synchronous dependency
grammar (Smith and Eisner, 2006) for ma-
chine translation in which the leaves of the
tree are phrases rather than words as in pre-
vious work (Gimpel and Smith, 2009). This
formulation allows us to combine structural
components of phrase-based and syntax-based
MT in a single model. We describe a method
of extracting phrase dependencies from paral-
lel text using a target-side dependency parser.
For decoding, we describe a coarse-to-fine ap-
proach based on lattice dependency parsing of
phrase lattices. We demonstrate performance
improvements for Chinese-English and Urdu-
English translation over a phrase-based base-
line. We also investigate the use of unsuper-
vised dependency parsers, reporting encourag-
ing preliminary results.
</bodyText>
<sectionHeader confidence="0.998787" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99995225">
Two approaches currently dominate statistical ma-
chine translation (MT) research. Phrase-based mod-
els (Koehn et al., 2003) excel at capturing local
reordering phenomena and memorizing multi-word
translations. Models that employ syntax or syntax-
like representations (Chiang, 2005; Galley et al.,
2006; Zollmann and Venugopal, 2006; Huang et al.,
2006) handle long-distance reordering better than
phrase-based systems (Auli et al., 2009) but often re-
quire constraints on the formalism or rule extraction
method in order to achieve computational tractabil-
ity. As a result, certain instances of syntactic diver-
gence are more naturally handled by phrase-based
systems (DeNeefe et al., 2007).
In this paper we present a new way of combin-
ing the advantages of phrase-based and syntax-based
MT. We propose a model in which phrases are orga-
nized into a tree structure inspired by dependency
syntax. Instead of standard dependency trees in
which words are vertices, our trees have phrases as
vertices. We describe a simple heuristic to extract
phrase dependencies from an aligned parallel cor-
pus parsed on the target side, and use them to com-
pute target-side tree features. We define additional
string-to-tree features and, if a source-side depen-
dency parser is available, tree-to-tree features to cap-
ture properties of how phrase dependencies interact
with reordering.
To leverage standard phrase-based features along-
side our novel features, we require a formalism
that supports flexible feature combination and effi-
cient decoding. Quasi-synchronous grammar (QG)
provides this backbone (Smith and Eisner, 2006);
we describe a coarse-to-fine approach for decod-
ing within this framework, advancing substantially
over earlier QG machine translation systems (Gim-
pel and Smith, 2009). The decoder involves generat-
ing a phrase lattice (Ueffing et al., 2002) in a coarse
pass using a phrase-based model, followed by lat-
tice dependency parsing of the phrase lattice. This
approach allows us to feasibly explore the combined
search space of segmentations, phrase alignments,
and target phrase dependency trees.
Our experiments demonstrate an average im-
provement of +0.65 BLEU in Chinese-English
translation across three test sets and an improvement
of +0.75 BLEU in Urdu-English translation over
a phrase-based baseline. We also describe experi-
ments in which we replace supervised dependency
parsers with unsupervised parsers, reporting promis-
ing results: using a supervised Chinese parser and
a state-of-the-art unsupervised English parser pro-
vides our best results, giving an averaged gain of
+0.79 BLEU over the baseline. We also discuss how
our model improves translation quality and discuss
future possibilities for combining approaches to ma-
</bodyText>
<page confidence="0.984847">
474
</page>
<note confidence="0.961454">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 474–485,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.830532">
chine translation using our framework.
</bodyText>
<sectionHeader confidence="0.997696" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999981720930233">
We previously applied quasi-synchronous grammar
to machine translation (Gimpel and Smith, 2009),
but that system performed translation fundamentally
at the word level. Here we generalize that model to
function on phrases, enabling a tighter coupling be-
tween the phrase segmentation and syntactic struc-
tures. We also present a decoder efficient enough to
scale to large data sets and present performance im-
provements in large-scale experiments over a state-
of-the-art phrase-based baseline.
Aside from QG, there have been many efforts
to use dependency syntax in machine translation.
Quirk et al. (2005) used a source-side dependency
parser and projected automatic parses across word
alignments in order to model dependency syntax on
phrase pairs. Shen et al. (2008) presented an exten-
sion to Hiero (Chiang, 2005) in which rules have
target-side dependency syntax and therefore enable
the use of a dependency language model.
More recently, researchers have sought the bene-
fits of dependency syntax while preserving the ad-
vantages of phrase-based models, such as efficiency
and coverage. Galley and Manning (2009) loos-
ened standard assumptions about dependency pars-
ing so that the efficient left-to-right decoding pro-
cedure of phrase-based translation could be retained
while a dependency language model is incorporated.
Carreras and Collins (2009) presented a string-to-
dependency system that permits non-projective de-
pendency trees (thereby allowing a larger space of
translations) and use a rule extraction procedure that
includes rules for every phrase in the phrase table.
We take an additional step in this direction by
working with dependency grammars on the phrases
themselves, thereby bringing together the structural
components of phrase-based and dependency-based
MT in a single model. While others have worked
on combining rules from multiple syntax-based sys-
tems (Liu et al., 2009) or using posteriors from mul-
tiple models to score translations (DeNero et al.,
2010), we are not aware of any other work that seeks
to directly integrate phrase-based and syntax-based
machine translation at the modeling level.1
</bodyText>
<note confidence="0.420452">
1Dymetman and Cancedda (2010) present a formal analy-
</note>
<sectionHeader confidence="0.968762" genericHeader="method">
3 Model
</sectionHeader>
<bodyText confidence="0.999949833333333">
Given a sentence s and its dependency tree τs,
we formulate the translation problem as finding the
target sentence t*, the segmentation γ* of s into
phrases, the segmentation φ* of t* into phrases, the
dependency tree τ*φ on the target phrases φ*, and the
one-to-one phrase alignment a* such that
</bodyText>
<equation confidence="0.9226016">
(t*, γ*, φ*, τ*φ, a*)= argmax p(t, γ, φ, τφ, a|s, τs)
(t,γ,φ,τφ,a)
We use a linear model (Och and Ney, 2002):
p(t, γ,φ, τφ, a  |s, τs) a
exp{θTg(s, τs, t, γ, φ, τφ, a)}
</equation>
<bodyText confidence="0.941960375">
where g is a vector of arbitrary feature functions on
the full set of structures and θ holds corresponding
feature weights. Table 1 summarizes our notation.
In modeling p(t, γ, φ, τφ, a  |s, τs), we make
use of quasi-synchronous grammar (QG; Smith
and Eisner, 2006). Given a source sentence and
its parse, a QG induces a probabilistic monolingual
grammar over sentences “inspired” by the source
sentence and tree. We denote this grammar by Gs,τs;
its (weighted) language is the set of translations of s.
Quasi-synchronous grammar makes no restric-
tions on the form of the target monolingual gram-
mar, though dependency grammars have been used
in most previous applications of QG (Wang et al.,
2007; Das and Smith, 2009; Smith and Eisner,
2009), including previous work in MT (Smith and
Eisner, 2006; Gimpel and Smith, 2009). We pre-
viously presented a word-based machine translation
model based on a quasi-synchronous dependency
grammar. However, it is well-known in the MT com-
munity that translation quality is improved when
larger units are modeled. Therefore, we use a de-
pendency grammar in which the leaves are phrases
rather than words.
We define a phrase dependency grammar as a
model p(φ,τφ|t) over the joint space of segmen-
tations of a sentence into phrases and dependency
trees on the phrases.2 Phrase dependency grammars
sis of the problem of intersecting phrase-based and hierarchical
translation models, but do not provide experimental results.
2We restrict our attention to projective trees in this paper,
but the generalization to non-projective trees is easily made.
</bodyText>
<page confidence="0.976361">
475
</page>
<equation confidence="0.992901">
s = hs1, ... , sni source language sentence
t = ht1, ... , tmi target language sentence, translation of s
γ = hγ1, ... , γn&apos;i · γn&apos; = s segmentation of s into phrases
∀i, γi = hsj, ... , ski s.t. γ1 · ...
φ = hφ1, ... , φm&apos;i · φm&apos; = t segmentation of t into phrases
∀i, φi = htj, ... , tki s.t. φ1 · ...
τ8 : {1, ... , n} → {0, ... , n} dependency tree on source words s, where τ8(i) is the index of
</equation>
<tableCaption confidence="0.466651166666667">
the parent of word si (0 is the root, $)
τo : {1, ... , m&apos;} → {0, ... , m&apos;} dependency tree on target phrases φ, where τo(i) is the index of
the parent of phrase φi
a : {1,.. . , m&apos;} → {1,. .. , n&apos;} one-to-one alignment from phrases in φ to phrases in γ
θ = hλ, ψi parameters of the full model (λ = phrase-based, ψ = QPDG)
able 1: Key notation.
</tableCaption>
<bodyText confidence="0.999322346153846">
have recently been used by Wu et al. (2009) for fea-
ture extraction for opinion mining. When used for
translation modeling, they allow us to capture phe-
nomena like local reordering and idiomatic transla-
tions within each phrase as well as long-distance re-
lationships among the phrases in a sentence.
We then define a quasi-synchronous phrase
dependency grammar (QPDG) as a conditional
model p(t, γ, φ, τφ, a  |s, τ8) that induces a prob-
abilistic monolingual phrase dependency grammar
over sentences inspired by the source sentence and
(lexical) dependency tree. The source and tar-
get sentences are segmented into phrases and the
phrases are aligned in a one-to-one alignment.
We note that we actually depart here slightly from
the original definition of QG. The alignment variable
in QG links target tree nodes to source tree nodes.
However, we never commit to a source phrase de-
pendency tree, instead using a source lexical depen-
dency tree output by a dependency parser, so our
alignment variable a is a function from target tree
nodes (phrases in φ) to source phrases in γ, which
might not be source tree nodes. The features in our
model may consider a large number of source phrase
dependency trees as long as they are consistent with
τ8.
</bodyText>
<sectionHeader confidence="0.999594" genericHeader="method">
4 Features
</sectionHeader>
<bodyText confidence="0.999888428571429">
Our model contains all of the standard phrase-based
features found in systems like Moses (Koehn et al.,
2007), including four phrase table probability fea-
tures, a phrase penalty feature, an n-gram language
model, a distortion cost, six lexicalized reordering
features, and a word penalty feature.
We now describe in detail the additional features
</bodyText>
<equation confidence="0.9522885">
$ ← said: $ ← we should
$ ← said that $ ← has been
$ ← is a - us → relations
$ ← will be $ ← he said
$ ← it is cross - strait → relations
$ ← this is $ ← pointed out that
$ ← we must ,and → is
the → united states the chinese → government
the → development of $ ← is the
the two → countries $ ← said,
he → said: one - china → principle
$ ← he said : sino - us → relations
</equation>
<bodyText confidence="0.902654076923077">
Table 2: Most frequent phrase dependencies with at least
2 words in one of the phrases (dependencies in which one
phrase is entirely punctuation are not shown). $ indicates
the root of the tree.
in our model that are used to score phrase depen-
dency trees. We shall refer to these as QPDG
features and will find it useful later to notation-
ally distinguish their feature weights from those of
the phrase-based model. We use λ for weights of
the standard phrase-based model features and ψ for
weights of the QPDG features. We include three cat-
egories of features, differentiated by what pieces of
structure they consider.
</bodyText>
<subsectionHeader confidence="0.998308">
4.1 Target Tree Features
</subsectionHeader>
<bodyText confidence="0.999598222222222">
We first include features that only consider t, φ,
and τφ. These features can be categorized as “syn-
tactic language model” features (Shen et al., 2008;
Galley and Manning, 2009), though unlike previous
work our features model both the phrase segmenta-
tion and dependency structure. Typically, these sorts
of features are probabilities estimated from a corpus
parsed using a supervised parser. However, there do
not currently exist treebanks with annotated phrase
</bodyText>
<page confidence="0.998627">
476
</page>
<table confidence="0.9736059">
, made up 0.057
he made up 0.021
supreme court made up 0.014
court made up 0.014
in september 2000 made up 0.014
in september 2000 , made up 0.014
made up of 0.065
made up . 0.029
made up , 0.016
made up mind to 0.01
</table>
<tableCaption confidence="0.971982">
Table 3: Most probable child phrases for the parent
</tableCaption>
<bodyText confidence="0.988889029411765">
phrase “made up” for each direction, sorted by the con-
ditional probability of the child phrase given the parent
phrase and direction.
dependency trees.
Our solution is to use a standard supervised de-
pendency parser and extract phrase dependencies us-
ing bilingual information.3 We begin by obtaining
symmetrized word alignments and extracting phrase
pairs using the standard heuristic from phrase-based
MT (Koehn et al., 2003). Given the set of extracted
phrase pairs for a sentence, denote by W the set of
unique target-side phrases among them. We parse
the target sentence with a dependency parser and, for
each pair of phrases u, v E W, we extract a phrase
dependency (along with its direction) if u and v do
not overlap and there is at least one lexical depen-
dency between a word in u and a word in v. If there
are lexical dependencies in both directions, we ex-
tract a phrase dependency only for the single longest
one. Since we use a projective dependency parser,
the longest lexical dependency between two phrases
is guaranteed to be unique. Table 2 shows a listing
of the most frequent phrase dependencies extracted
(lexical dependencies are omitted).
We note that during training we never explicitly
commit to any single phrase dependency tree for a
target sentence. Rather, we extract phrase depen-
dencies from all phrase dependency trees consis-
tent with the word alignments and the lexical de-
pendency tree. Thus we treat phrase dependency
trees analogously to phrase segmentations in stan-
dard phrase extraction.
We perform this procedure on all sentence pairs
in the parallel corpus. Given a set of extracted
</bodyText>
<footnote confidence="0.540748">
3For a monolingual task, Wu et al. (2009) used a shal-
low parser to convert lexical dependencies from a dependency
parser into phrase dependencies.
</footnote>
<bodyText confidence="0.999984727272727">
phrase dependencies of the form (u, v, d), where
u is the head phrase, v is the child phrase, and
d E {left, right} is the direction, we then estimate
conditional probabilities p(v|u, d) using relative fre-
quency estimation. Table 3 shows the most probable
child phrases for an example parent phrase. To com-
bat data sparseness, we perform the same procedure
with each word replaced by its word cluster ID ob-
tained from Brown clustering (Brown et al., 1992).
We include a feature in the model for the sum of
the scaled log-probabilities of each attachment:
</bodyText>
<equation confidence="0.987079333333333">
m/
( )
max 0, C + log p(φi|φτφ(i), d(i) (1)
</equation>
<bodyText confidence="0.996670941176471">
where d(i) = I[τφ(i) − i &gt; 0] is the direction of the
dependency arc.
Although we use log-probabilities in this feature
function, we first add a constant C to each to ensure
they are all positive.4 The max expression protects
unseen parent-child phrase dependencies from caus-
ing the score to be negative infinity. Our motivation
is a desire for the features to be used to prefer one
derivation over another but not to rule out a deriva-
tion completely if it merely happens to contain a de-
pendency unobserved in the training data.
We also include lexical weighting features simi-
lar to those used in phrase-based MT (Koehn et al.,
2003). Whenever we extract a phrase dependency,
we extract the longest lexical dependency contained
within it. For all (parent, child, direction) lexi-
cal dependency tuples (x, y, d), we estimate condi-
tional probabilities plex (y|x, d) from the parsed cor-
pus using relative frequency estimation. Then, for a
phrase dependency with longest lexical dependency
(x, y, d), we add a feature for plex(y|x, d) to the
model, using a formula similar to Eq. 1. Different
instances of a phrase dependency may have different
lexical dependencies extracted with them. We add
the lexical weight for the most frequent, breaking
ties by choosing the lexical dependency that maxi-
mizes p(y|x, d), as was also done by Koehn et al.
(2003).
In all, we include 4 target tree features: one for
phrase dependencies, one for lexical dependencies,
4The reasoning here is that whenever we use a phrase de-
pendency that we have observed in the training data, we want to
boost the score of the translation. If we used log-probabilities,
each observed dependency would incur a penalty.
</bodyText>
<equation confidence="0.9059085">
�
i=1
</equation>
<page confidence="0.981453">
477
</page>
<figureCaption confidence="0.985351">
Figure 1: String-to-tree configurations; each is associated
with a feature that counts its occurrences in a derivation.
</figureCaption>
<construct confidence="0.43648325">
Input: sentence s, dependency parse 7-s, coarse
parameters AM, fine parameters (A, ψ)
Output: translation t
LMERT +- GenerateLattices (s, AM);
LFB +- FBPrune (LMERT, AM);
(t, -y, φ, 7-φ, a) +- QGDEPPARSE(LFB, (A,ψ));
return t;
Algorithm 1: CoarseToFineDecode
</construct>
<figure confidence="0.9935549375">
c d e
a b
c d e
a b
x y
z
a b
c d e
x y
z
a b
c d e
x y
z
x y
z
</figure>
<bodyText confidence="0.996680666666667">
and the same features computed from a transformed
version of the corpus in which each word is replaced
by its Brown cluster.
</bodyText>
<subsectionHeader confidence="0.939999">
4.2 String-to-Tree Configurations
</subsectionHeader>
<bodyText confidence="0.999968363636364">
We consider features that count instances of reorder-
ing configurations involving phrase dependencies.
In addition to the target-side structures, these fea-
tures consider -y and a, though not s or 7-8. For ex-
ample, when building a parent-child phrase depen-
dency with the child to the left, one feature value is
incremented if their aligned source-side phrases are
in the same order. This configuration is the leftmost
in Fig. 1; we include features for the other three con-
figurations there as well, for a total of 4 features in
this category.
</bodyText>
<subsectionHeader confidence="0.989552">
4.3 Tree-to-Tree Configurations
</subsectionHeader>
<bodyText confidence="0.9999420625">
We include features that consider s, -y, and 7-8 in ad-
dition to t, φ, and 7-0. We begin with features for
each of the quasi-synchronous configurations from
Smith and Eisner (2006), adapted to phrase depen-
dency grammars. That is, for a parent-child pair
(7-0(i), i) in 7-0, we consider the relationship be-
tween a(7-0(i)) and a(i), the source-side phrases
to which 7-0(i) and i align. We use the follow-
ing named configurations from Smith and Eisner:
root-root, parent-child, child-parent, grandparent-
grandchild, sibling, and c-command.5 We define a
feature to count instances of each of these configu-
rations, including an additional feature for “other”
configurations that do not fit into these categories.6
When using a QPDG, there are multiple ways
to compute tree-to-tree configuration features, since
</bodyText>
<footnote confidence="0.7211166">
5See Fig. 3 in Smith and Eisner (2006) for illustrations.
6We actually include two versions of each configuration fea-
ture other than “root-root”: one for the source phrases being in
the same order as the target phrases and one for them being
swapped.
</footnote>
<bodyText confidence="0.999630444444445">
we use a phrase dependency tree for the target side,
a lexical dependency tree for the source side, and
a phrase alignment. We use the following heuristic
approach. Given a pair of source words, one with
index j in source phrase a(7-0(i)) and the other with
index k in source phrase a(i), we have a parent-
child configuration if 7-8(k) = j; if 7-8(j) = k, a
child-parent configuration is present. In order for the
grandparent-grandchild configuration to be present,
the intervening parent word must be outside both
phrases. For sibling and other c-command config-
urations, the shared parent or ancestor must also be
outside both phrases.
After obtaining a list of all configurations present
for each pair of words (j, k), we fire the feature for
the single configuration corresponding to the max-
imum distance jj − kj. If no configurations are
present between any pair of words, the “other” fea-
ture fires. Therefore, only one configuration feature
fires for each phrase dependency attachment.
Finally, we include features that consider the
dependency path distance between phrases in the
source-side dependency tree that are aligned to
parent-child pairs in 7-0. We include a feature that
sums, for each target phrase i, the inverse of the
minimum undirected path length between each word
in a(i) and each word in 7-0(a(i)). The minimum
undirected path length is defined as the number of
dependency arcs that must be crossed to travel from
one word to the other in 7-8. We use one feature
for undirected path length and one other for directed
path length. If there is no (un)directed path from a
word in a(i) to a word in 7-0(a(i)), we use oc as the
minimum length.
There are 15 features in this category, for a total
of 23 QPDG features.
</bodyText>
<page confidence="0.997922">
478
</page>
<sectionHeader confidence="0.997177" genericHeader="method">
5 Decoding
</sectionHeader>
<bodyText confidence="0.999309697674419">
For a QPDG model, decoding consists of finding
the highest-scoring tuple (t, -y, 0, τφ, a) for an in-
put sentence s and its parse τs, i.e., finding the most
probable derivation under the s/τs-specific grammar
Gs,τ8. We follow Gimpel and Smith (2009) in con-
structing a lattice to represent Gs,τ8 and using lattice
parsing to search for the best derivation, but we con-
struct the lattice differently and employ a coarse-to-
fine strategy (Petrov, 2009) to speed up decoding.
It has become common in recent years for MT re-
searchers to exploit efficient data structures for en-
coding concise representations of the pruned search
space of the model, such as phrase lattices for
phrase-based MT (Ueffing et al., 2002; Macherey
et al., 2008; Tromble et al., 2008). Each edge in
a phrase lattice corresponds to a phrase pair and
each path through the lattice corresponds to a tuple
(t, -y, 0, a) for the input s. Decoding for a phrase
lattice consists of finding the highest-scoring path,
which is done using dynamic programming. To also
maximize over τφ, we perform lattice dependency
parsing, which allows us to search over the space of
tuples (t, -y, 0, a, τφ). Given the lattice and Gs,τ8,
lattice parsing is a straightforward generalization of
the standard arc-factored dynamic programming al-
gorithm from Eisner (1996).
The lattice parsing algorithm requires O(E2V )
time and O(E2 + V E) space, where E is the num-
ber of edges in the lattice and V is the number of
nodes.7 Typical phrase lattices might easily contain
tens of thousands of nodes and edges, making exact
search prohibitively expensive for all but the small-
est lattices. So, we use approximate search based on
coarse-to-fine decoding. We now discuss each step
of this procedure; an outline is shown as Alg. 1.
Pass 1: Lattice Pruning After generating phrase
lattices using a phrase-based MT system, we prune
lattice edges using forward-backward pruning (Six-
tus and Ortmanns, 1999), which has also been used
in previous work using phrase lattices (Tromble et
al., 2008). This pruning method computes the max-
marginal for each lattice edge, which is the score of
the best full path that uses that edge. Max-marginals
</bodyText>
<footnote confidence="0.906411">
7To prevent confusion, we use the term edge to refer to a
phrase lattice edge and arc to refer to a parent-child dependency
in the phrase dependency tree.
</footnote>
<bodyText confidence="0.999690723404255">
offer the advantage that the best path in the lattice is
preserved during pruning. For each lattice, we use
a grid search to find the most liberal threshold that
leaves fewer than 1000 edges in the resulting lattice.
As complexity is quadratic in E, forcing E to be
less than 1000 improves runtime substantially. Af-
ter pruning, the lattices still contain more than 1016
paths on average and oracle BLEU scores are typi-
cally 12-15 points higher than the model-best paths.
Pass 2: Parent Ranking Given a pruned lattice,
we then remove some candidate dependency arcs
from consideration. It is common in dependency
parsing to use a coarse model to rank the top k par-
ents for each word, and to only consider these during
parsing (Martins et al., 2009; Bergsma and Cherry,
2010). Unlike string parsing, our phrase lattices im-
pose several types of constraints on allowable arcs.
For example, each node in the phrase lattice is an-
notated with a coverage vector—a bit vector indicat-
ing which words in the source sentence have been
translated—which implies a topological ordering of
the nodes. To handle constraints like these, we first
use the Floyd-Warshall algorithm (Floyd, 1962) to
find the best score between every pair of nodes in
the lattice. This algorithm also tells us whether each
edge is reachable from each other edge, allowing
us to immediately prune dependency arcs between
edges that are unreachable from each other.
After eliminating impossible arcs, we turn to
pruning away unlikely ones. In standard (string) de-
pendency parsing, every word is assigned a parent.
In lattice parsing, however, most lattice edges will
not be assigned any parent. Certain lattice edges are
much more likely to be contained within paths, so
we allow some edges to have more candidate parent
edges than others. We introduce hyperparameters
α, 0, and µ to denote, respectively, the minimum,
maximum, and average number of parent edges to
be considered for each lattice edge (α &lt; µ &lt; 0).
We rank the full set of E2 arcs according to their
scores (using the QPDG features and their weights
ψ) and choose the top µE of these arcs while en-
suring that each edge has at least α and at most 0
potential parent edges.
This step reduces the time complexity from
O(E2V ) to O(µEV ), where µ &lt; E. In our ex-
periments, we set µ = 300, α = 100, and 0 = 400.
</bodyText>
<page confidence="0.998452">
479
</page>
<bodyText confidence="0.9712054">
Input: tuning set D = hS, Ti, initial weights λ0 for
coarse model, initial weights ψ0 for
additional features in fine model
Output: coarse model learned weights: λM, fine
model learned weights: hλ∗, ψ∗i
</bodyText>
<equation confidence="0.869449142857143">
λM ← MERT (S, T, λ0, 100, MOSES);
LMERT ← GenerateLattices (S, λM);
LFB ← FBPrune (LMERT, λM);
hλ∗, ψ∗i ←
MERT (LFB, T, hλM, ψ0i, 200, QGDEPPARSE);
return λM, hλ∗, ψ∗i;
Algorithm 2: CoarseToFineTrain
</equation>
<bodyText confidence="0.981197333333333">
Pass 3: Lattice Dependency Parsing After com-
pleting the coarse passes, we parse using bottom-up
dynamic programming based on the agenda algo-
rithm (Nederhof, 2003; Eisner et al., 2005). We only
consider arcs that survived the filtering in Pass 2.
We weight agenda items by the sum of their scores
and the Floyd-Warshall best path scores both from
the start node of the lattice to the beginning of the
item and the end of the item to any final node. This
heuristic helps us to favor exploration of items that
are highly likely under the phrase-based model.
If the score of the partial structure can only get
worse when combining it with other structures (e.g.,
in a PCFG), then the first time that we pop an item
of type GOAL from the agenda, we are guaranteed
to have the best parse. However, in our model, some
features are positive and others negative, making this
property no longer hold; as a result, GOAL items
may be popped out of order from the agenda. There-
fore, we use an approximation, simply popping G
GOAL items from the agenda and then stopping. The
items are sorted by their scores and the best is re-
turned by the decoder (or the k best in the case of
MERT). In our experiments, we set G = 4000.
The combined strategy yields average decoding
times in the range of 30 seconds per sentence, which
is comparable to other syntax-based MT systems.
</bodyText>
<sectionHeader confidence="0.996737" genericHeader="method">
6 Training
</sectionHeader>
<bodyText confidence="0.9999785">
For tuning the coarse and fine parameters, we use
minimum error rate training (MERT; Och, 2003) in
a procedure shown as Alg. 2. We first use MERT to
train parameters for the coarse phrase-based model
used to generate phrase lattices. Then, after gener-
ating the lattices, we prune them and run MERT a
second time to tune parameters of the fine model,
which includes all phrase-based and QPDG param-
eters. The arguments to MERT are a vector of source
sentences (or lattices), a vector of target sentences,
the initial parameter values, the size of the k-best
list, and finally the decoder. We initialize λ to the
default Moses feature weights and for ψ we ini-
tialize the two target phrase dependency weights to
0.004, the two lexical dependency weights to 0.001,
and the weights for all configuration features to 0.0.
Our training procedure requires two executions of
MERT, and the second typically takes more itera-
tions to converge (10 to 20 is typical) than the first
due to the use of a larger feature set and increased
possibility for search error due to the enlarged search
space.
</bodyText>
<sectionHeader confidence="0.999354" genericHeader="method">
7 Experiments
</sectionHeader>
<bodyText confidence="0.999721482758621">
For experimental evaluation, we consider Chinese-
to-English (ZH-EN) and Urdu-to-English (UR-
EN) translation and compare our system to
Moses (Koehn et al., 2007). For ZH-EN, we
used 303k sentence pairs from the FBIS corpus
(LDC2003E14). We segmented the Chinese data
using the Stanford Chinese segmenter in “CTB”
mode (Chang et al., 2008), giving us 7.9M Chinese
words and 9.4M English words. For UR-EN, we
used parallel data from the NIST MT08 evaluation
consisting of 1.2M Urdu words and 1.1M English
words.
We trained a baseline Moses system using de-
fault settings and features. Word alignment was
performed using GIZA++ (Och and Ney, 2003) in
both directions and the grow-diag-final-and
heuristic was used to symmetrize the alignments.
We used a max phrase length of 7 when extracting
phrases. Trigram language models were estimated
using the SRI language modeling toolkit (Stolcke,
2002) with modified Kneser-Ney smoothing (Chen
and Goodman, 1998). To estimate language models
for each language pair, we used the English side of
the parallel corpus concatenated with 200M words
of randomly-selected sentences from the Gigaword
v4 corpus (excluding the NY Times and LA Times).
We used this baseline Moses system to gener-
ate phrase lattices for our system, so our model in-
cludes all of the Moses features in addition to the
</bodyText>
<page confidence="0.995435">
480
</page>
<table confidence="0.99963425">
MT03 (tune) MT02 MT05 MT06 Average
Moses 33.84 33.35 31.81 28.82 31.33
QPDG (TT) 34.63 (+0.79) 34.10 (+0.75) 32.15 (+0.34) 29.33 (+0.51) 31.86 (+0.53)
QPDG (TT+S2T+T2T) 34.98 (+1.14) 34.26 (+0.91) 32.34 (+0.53) 29.35 (+0.53) 31.98 (+0.65)
</table>
<tableCaption confidence="0.999199">
Table 4: Chinese-English Results (% BLEU).
</tableCaption>
<bodyText confidence="0.99981845">
QPDG features described in §4. In our experiments,
we compare our QPDG system (lattice parsing on
each lattice) to the Moses baseline (finding the best
path through each lattice). The conventional wis-
dom holds that hierarchical phrase-based transla-
tion (Chiang, 2005) performs better than phrase-
based translation for language pairs that require
large amounts of reordering, such as ZH-EN and
UR-EN. However, researchers have shown that this
performance gap diminishes when using a larger dis-
tortion limit (Zollmann et al., 2008) and may dis-
appear entirely when using a lexicalized reordering
model (Lopez, 2008; Galley and Manning, 2010).
So, we increase the Moses distortion limit from 6
(the default) to 10 and use Moses’ default lexical-
ized reordering model (Koehn et al., 2005).
We parsed the Chinese text using the Stanford
parser (Levy and Manning, 2003) and the English
text using TurboParser (Martins et al., 2009). We
note that computing our features requires parsing the
target (English) side of the parallel text, but not the
source side. We only need to parse the source side
of the tuning and test sets, and the only features that
look at the source-side parse are those from §4.3.
To obtain Brown clusters for the target tree fea-
tures in §4.1, we used code from Liang (2005).8
We induced 100 clusters from the English side of
the parallel corpus concatenated with 10M words of
randomly-selected Gigaword sentences. Only words
that appeared at least twice in this data were con-
sidered during clustering. An additional cluster was
created for all other words; this allowed us to use
phrase dependency cluster features even for out-of-
vocabulary words. We used a max phrase length of
7 when extracting phrase dependencies to match the
max phrase length used in phrase extraction. Ap-
proximately 87M unique phrase dependencies were
extracted from the ZH-EN data and 7M from the
UR-EN data.
We tuned the weights of our model using the pro-
</bodyText>
<footnote confidence="0.9882535">
8http://www.cs.berkeley.edu/˜pliang/
software
</footnote>
<table confidence="0.996407">
Dev (tune) MT09
Moses 24.21 23.56
QPDG (TT+S2T) 24.94 (+0.73) 24.31 (+0.75)
</table>
<tableCaption confidence="0.998943">
Table 5: Urdu-English Results (% BLEU).
</tableCaption>
<bodyText confidence="0.999435142857143">
cedure described in §6. For ZH-EN we used MT03
for tuning and MT02, MT05, and MT06 for test-
ing. For UR-EN we used half of the documents (882
sentence pairs) from the MT08 test set for tuning
(“Dev”) and MT09 for testing. We evaluated trans-
lation output using case-insensitive IBM BLEU (Pa-
pineni et al., 2001).
</bodyText>
<subsectionHeader confidence="0.754323">
7.1 Results
</subsectionHeader>
<bodyText confidence="0.998907964285714">
Results for ZH-EN and UR-EN translation are
shown in Tables 4 and 5. We show results when us-
ing only the target tree features from §4.1 (TT), as
well as when adding the string-to-tree features from
§4.2 (S2T) and the tree-to-tree features from §4.3
(T2T). We note that T2T features are unavailable for
UR-EN because we do not have an Urdu parser. We
find that we can achieve moderate but consistent im-
provements over the baseline Moses system, for an
average increase of 0.65 BLEU points for ZH-EN
and 0.75 for UR-EN.
Fig. 2 shows an example sentence from the MT05
test set along with its translation output and deriva-
tions produced by Moses and our QPDG system
with the full feature set. This example shows the
kind of improvements that our system makes. In
Chinese, modifiers such as prepositional phrases and
clauses are generally placed in front of the words
they modify, frequently the opposite of English. In
addition, Chinese occasionally uses postpositions
where English uses prepositions. The Chinese sen-
tence in Fig. 2 exhibits both of these, as the preposi-
tional phrase “after the Palestinian election” appears
before the verb “strengthen” in the Chinese sen-
tence and “after” appears as a postposition. Moses
(Fig. 2(a)) does not properly reorder the preposi-
tional phrase, while our system (Fig. 2(b)) properly
handles both reorderings.9 We shall discuss these
</bodyText>
<footnote confidence="0.889561">
9Our system’s derivation is not perfect, in that “in” is incor-
</footnote>
<page confidence="0.994118">
481
</page>
<listItem confidence="0.98507875">
(c) References bush : us set to boost peace efforts after palestinian election
bush : us to step up peace efforts after palestinian elections
bush : u.s. will enhance peace efforts after palestinian election
us to boost peace efforts after palestinian elections : bush
</listItem>
<figureCaption confidence="0.996843">
Figure 2: (a) Moses translation output along with γ, φ, and a. An English gloss is shown above the Chinese sentence
and above the gloss is shown the dependency parse from the Stanford parser. (b) QPDG system output with additional
structure τφ. (c) reference translations.
</figureCaption>
<figure confidence="0.986493105263158">
$
united
bush : states will in
palestine elections after strengthen peace efforts
布希
美 将
加强 和平
努力
大
后
:
在 巴勒斯坦
bush
palestinian
in
:
presidential election
the united states will
strengthen the peace efforts
$
bush
the
peace
: the united states will
strengthen
efforts
after
the palestinian
election
布希
: 美 将
在 巴勒斯坦
加强
和平
努力
后
大
types of improvements further in §8.
</figure>
<subsectionHeader confidence="0.989921">
7.2 Unsupervised Parsing
</subsectionHeader>
<bodyText confidence="0.999503023255814">
Our results thus far use supervised parsers for both
Chinese and English, but parsers are only available
for a small fraction of the languages we would like
to translate. Fortunately, unsupervised dependency
grammar induction has improved substantially in re-
cent years due to a flurry of recent research. While
attachment accuracies on standard treebank test sets
are still relatively low, it may be the case that even
though unsupervised parsers do not match treebank
annotations very well, they may perform well when
used for extrinsic applications. We believe that
syntax-based MT offers a compelling platform for
development and extrinsic evaluation of unsuper-
vised parsers.
In this paper, we use the standard dependency
model with valence (DMV; Klein and Manning,
2004). When training is initialized using the out-
put of a simpler, concave dependency model, the
rectly translated and reordered, but the system was nonetheless
able to use it to improve the fluency of the output.
DMV can approach state-of-the-art unsupervised ac-
curacy (Gimpel and Smith, 2011). For English, the
resulting parser achieves 53.1% attachment accu-
racy on Section 23 of the Penn Treebank (Marcus et
al., 1993), which approaches the 55.7% accuracy of
a recent state-of-the-art unsupervised model (Blun-
som and Cohn, 2010). The Chinese parser, ini-
tialized and trained the same way, achieves 44.4%,
which is the highest reported accuracy on the Chi-
nese Treebank (Xue et al., 2004) test set.
Most unsupervised grammar induction models
assume gold standard POS tags and sentences
stripped of punctuation. We use the Stanford tag-
ger (Toutanova et al., 2003) to obtain tags for both
English and Chinese, parse the sentences without
punctuation using the DMV, and then attach punc-
tuation tokens to the root word of the tree in a post-
processing step. For English, the predicted parents
agreed with those of TurboParser for 48.7% of the
tokens in the corpus.
We considered all four scenarios: supervised and
unsupervised English parsing paired with supervised
and unsupervised Chinese parsing. Table 6 shows
</bodyText>
<page confidence="0.994583">
482
</page>
<table confidence="0.902032">
EN
unsupervised supervised
31.18 (33.76) 31.86 (34.78)
32.12 (34.74) 31.98 (34.98)
31.33 (33.84)
</table>
<tableCaption confidence="0.993879">
Table 6: Results when using unsupervised dependency
parsers. Cells contain averaged % BLEU on the three test
sets and % BLEU on tuning data (MT03) in parentheses.
</tableCaption>
<table confidence="0.999944">
Feature Initial Learned
Left child, same order 9.0 8.9
Left child, swap phrases 1.1 0.0
Right child, same order 7.3 7.3
Right child, swap phrases 1.6 2.3
Root-root 0.4 0.8
Parent-child 4.2 6.1
Child-parent 1.2 0.4
Grandparent-grandchild 1.0 0.2
Sibling 2.4 1.9
C-command 6.1 6.7
Other 1.5 0.9
</table>
<tableCaption confidence="0.8997898">
Table 7: Average feature values across best translations
of sentences in the MT03 tuning set, both before MERT
(column 2) and after (column 3). “Same” versions of tree-
to-tree configuration features are shown; the rarer “swap”
features showed a similar trend.
</tableCaption>
<bodyText confidence="0.999480105263158">
BLEU scores averaged over the three test sets with
tuning data BLEU in parentheses. Surprisingly, we
achieve our best results when using the unsupervised
English parser in place of the supervised one (+0.79
over Moses), while keeping the Chinese parser su-
pervised. Competitive performance is also found
by using the unsupervised Chinese parser and super-
vised English parser (+0.53 over Moses).
However, when using unsupervised parsers for
both languages, performance was below that of
Moses. During tuning for this configuration, we
found that MERT struggled to find good parameter
estimates, typically converging to suboptimal solu-
tions after a small number of iterations. We believe
this is due to the large number of features (37), the
noise in the parse trees, and known instabilities of
MERT. In future work we plan to experiment with
training algorithms that are more stable and that can
handle larger numbers of features.
</bodyText>
<sectionHeader confidence="0.992217" genericHeader="evaluation">
8 Analysis
</sectionHeader>
<bodyText confidence="0.999991380952381">
To understand what our model learns during MER
training, we computed the feature vectors of the best
derivation for each sentence in the tuning data at
both the start and end of tuning. Table 7 shows
these feature values averaged across all tuning sen-
tences. The first four features are the configurations
from Fig. 1, in order from left to right. From these
rows, we can observe that the model learns to en-
courage swapping when generating right children
and penalize swapping for left children. In addi-
tion to objects, right children in English are often
prepositional phrases, relative clauses, or other mod-
ifiers; as we noted above, Chinese generally places
these modifiers before their heads, requiring reorder-
ing during translation. Here the model appears to be
learning this reordering behavior.
From the second set of features, we see that the
model learns to favor producing dependency trees
that are mostly isomorphic to the source tree, by fa-
voring root-root and parent-child configurations at
the expense of most others.
</bodyText>
<sectionHeader confidence="0.999022" genericHeader="conclusions">
9 Discussion
</sectionHeader>
<bodyText confidence="0.999985888888889">
In looking at BLEU score differences between the
two systems, the unigram precisions were typically
equal or only slightly different, while precisions for
higher-order n-grams contained the bulk of the im-
provement. This suggests that our system is not
finding substantially better translations for individ-
ual words in the input, but rather is focused on re-
ordering the existing translations. This is not sur-
prising given our choice of features, which focus on
syntactic language modeling and syntax-based re-
ordering. The obvious next step for our framework
is to include bilingual rules that include source syn-
tax (Quirk et al., 2005), target syntax (Shen et al.,
2008), and syntax on both sides. Our framework al-
lows integrating together all of these and other types
of structures, with the ultimate goal of combining
the strengths of multiple approaches to translation
in a single model.
</bodyText>
<sectionHeader confidence="0.998082" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.921795">
We thank Chris Dyer and the anonymous reviewers for
helpful comments that improved this paper. This research
was supported in part by the NSF through grant IIS-
0844507, the U. S. Army Research Laboratory and the
U. S. Army Research Office under contract/grant number
W911NF-10-1-0533, and Sandia National Laboratories
(fellowship to K. Gimpel).
</bodyText>
<figure confidence="0.940070666666667">
ZH unsupervised
supervised
Moses
</figure>
<page confidence="0.998685">
483
</page>
<sectionHeader confidence="0.983407" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999782504761905">
M. Auli, A. Lopez, H. Hoang, and P. Koehn. 2009. A
systematic analysis of translation model search spaces.
In Proceedings of the Fourth Workshop on Statistical
Machine Translation.
S. Bergsma and C. Cherry. 2010. Fast and accurate arc
filtering for dependency parsing. In Proc. of COLING.
P. Blunsom and T. Cohn. 2010. Unsupervised induction
of tree substitution grammars for dependency parsing.
In Proc. of EMNLP.
P. F. Brown, P. V. deSouza, R. L. Mercer, V. J. Della
Pietra, and J. C. Lai. 1992. Class-based n-gram mod-
els of natural language. Computational Linguistics,
18.
X. Carreras and M. Collins. 2009. Non-projective pars-
ing for statistical machine translation. In Proc. of
EMNLP.
P. Chang, M. Galley, and C. Manning. 2008. Optimiz-
ing Chinese word segmentation for machine transla-
tion performance. In Proc. of the Third Workshop on
Statistical Machine Translation.
S. Chen and J. Goodman. 1998. An empirical study of
smoothing techniques for language modeling. Techni-
cal report 10-98, Harvard University.
D. Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proc. ofACL.
D. Das and N. A. Smith. 2009. Paraphrase identifica-
tion as probabilistic quasi-synchronous recognition. In
Proc. of ACL-IJCNLP.
S. DeNeefe, K. Knight, W. Wang, and D. Marcu. 2007.
What can syntax-based MT learn from phrase-based
MT? In Proc. of EMNLP-CoNLL.
J. DeNero, S. Kumar, C. Chelba, and F. J. Och. 2010.
Model combination for machine translation. In Proc.
of NAACL.
M. Dymetman and N. Cancedda. 2010. Intersecting hi-
erarchical and phrase-based models of translation. for-
mal aspects and algorithms. In Proc. of SSST-4.
J. Eisner, E. Goldlust, and N. A. Smith. 2005. Com-
piling Comp Ling: Practical weighted dynamic pro-
gramming and the Dyna language. In Proc. of HLT-
EMNLP.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In Proc. of COL-
ING.
R. W. Floyd. 1962. Algorithm 97: Shortest path. Com-
munications of the ACM, 5(6).
M. Galley and C. D. Manning. 2009. Quadratic-time
dependency parsing for machine translation. In Proc.
of ACL-IJCNLP.
M. Galley and C. D. Manning. 2010. Accurate non-
hierarchical phrase-based translation. In Proc. of
NAACL.
M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe,
W. Wang, and I. Thayer. 2006. Scalable inference and
training of context-rich syntactic translation models.
In Proc. of COLING-ACL.
K. Gimpel and N. A. Smith. 2009. Feature-rich transla-
tion by quasi-synchronous lattice parsing. In Proc. of
EMNLP.
K. Gimpel and N. A. Smith. 2011. Concavity and initial-
ization for unsupervised dependency grammar induc-
tion. Technical report, Carnegie Mellon University.
L. Huang, K. Knight, and A. Joshi. 2006. Statistical
syntax-directed translation with extended domain of
locality. In Proc. of AMTA.
D. Klein and C. D. Manning. 2004. Corpus-based induc-
tion of syntactic structure: Models of dependency and
constituency. In Proc. of ACL.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. of HLT-NAACL.
P. Koehn, A. Axelrod, A. Birch Mayne, C. Callison-
Burch, M. Osborne, and D. Talbot. 2005. Edinburgh
system description for the 2005 iwslt speech transla-
tion evaluation. In Proc. of IWSLT.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In Proc. of ACL (demo
session).
R. Levy and C. D. Manning. 2003. Is it harder to parse
chinese, or the chinese treebank? In Proc. of ACL.
P. Liang. 2005. Semi-supervised learning for natural
language. Master’s thesis, Massachusetts Institute of
Technology.
Y. Liu, H. Mi, Y. Feng, and Q. Liu. 2009. Joint de-
coding with multiple translation models. In Proc. of
ACL-IJCNLP.
A. Lopez. 2008. Tera-scale translation models via pat-
tern matching. In Proc. of COLING.
W. Macherey, F. Och, I. Thayer, and J. Uszkoreit. 2008.
Lattice-based minimum error rate training for statisti-
cal machine translation. In EMNLP.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19:313–330.
A. F. T. Martins, N. A. Smith, and E. P. Xing. 2009.
Concise integer linear programming formulations for
dependency parsing. In Proc. of ACL.
M.-J. Nederhof. 2003. Weighted deductive parsing and
knuth’s algorithm. Computational Linguistics, 29(1).
F. J. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical machine
translation. In Proc. of ACL.
</reference>
<page confidence="0.988686">
484
</page>
<reference confidence="0.999222456140351">
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1).
F. J. Och. 2003. Minimum error rate training for statisti-
cal machine translation. In Proc. of ACL.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2001.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL.
S. Petrov. 2009. Coarse-to-Fine Natural Language
Processing. Ph.D. thesis, University of California at
Berkeley.
C. Quirk, A. Menezes, and C. Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In Proc. of ACL.
L. Shen, J. Xu, and R. Weischedel. 2008. A new string-
to-dependency machine translation algorithm with a
target dependency language model. In Proc. of ACL.
A. Sixtus and S. Ortmanns. 1999. High quality word
graphs using forward-backward pruning. In Proc. of
the IEEE Int. Conf. on Acoustics, Speech and Signal
Processing.
D. A. Smith and J. Eisner. 2006. Quasi-synchronous
grammars: Alignment by soft projection of syntactic
dependencies. In Proc. of HLT-NAACL Workshop on
Statistical Machine Translation.
D. A. Smith and J. Eisner. 2009. Parser adaptation and
projection with quasi-synchronous features. In Proc.
of EMNLP.
A. Stolcke. 2002. SRILM—an extensible language mod-
eling toolkit. In Proc. of ICSLP.
K. Toutanova, D. Klein, C. D. Manning, and Y. Singer.
2003. Feature-rich part-of-speech tagging with a
cyclic dependency network. In Proc. of HLT-NAACL.
R. Tromble, S. Kumar, F. Och, and W. Macherey. 2008.
Lattice Minimum Bayes-Risk decoding for statistical
machine translation. In EMNLP.
N. Ueffing, F. J. Och, and H. Ney. 2002. Generation of
word graphs in statistical machine translation. In Proc.
of EMNLP.
M. Wang, N. A. Smith, and T. Mitamura. 2007. What
is the Jeopardy model? a quasi-synchronous grammar
for QA. In Proc. of EMNLP-CoNLL.
Y. Wu, Q. Zhang, X. Huang, and L. Wu. 2009. Phrase
dependency parsing for opinion mining. In Proc. of
EMNLP.
N. Xue, F. Xia, F.-D. Chiou, and M. Palmer. 2004. The
Penn Chinese Treebank: Phrase structure annotation
of a large corpus. Natural Language Engineering,
10(4):1–30.
A. Zollmann and A. Venugopal. 2006. Syntax aug-
mented machine translation via chart parsing. In
Proc. of NAACL 2006 Workshop on Statistical Ma-
chine Translation.
A. Zollmann, A. Venugopal, F. J. Och, and J. Ponte.
2008. A systematic comparison of phrase-based, hi-
erarchical and syntax-augmented statistical MT. In
Proc. of COLING.
</reference>
<page confidence="0.999076">
485
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.956531">
<title confidence="0.999761">Quasi-Synchronous Phrase Dependency for Machine Translation</title>
<author confidence="0.993426">Kevin Gimpel Noah A</author>
<affiliation confidence="0.9970795">Language Technologies Institute Carnegie Mellon Univeristy</affiliation>
<address confidence="0.995889">Pittsburgh, PA 15213, USA</address>
<abstract confidence="0.998522105263158">We present a quasi-synchronous dependency grammar (Smith and Eisner, 2006) for machine translation in which the leaves of the are than words as in previous work (Gimpel and Smith, 2009). This formulation allows us to combine structural components of phrase-based and syntax-based MT in a single model. We describe a method of extracting phrase dependencies from parallel text using a target-side dependency parser. For decoding, we describe a coarse-to-fine approach based on lattice dependency parsing of phrase lattices. We demonstrate performance improvements for Chinese-English and Urdu- English translation over a phrase-based base- We also investigate the use of unsuperparsers, reporting encouraging preliminary results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Auli</author>
<author>A Lopez</author>
<author>H Hoang</author>
<author>P Koehn</author>
</authors>
<title>A systematic analysis of translation model search spaces.</title>
<date>2009</date>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="1436" citStr="Auli et al., 2009" startWordPosition="197" endWordPosition="200">ish and UrduEnglish translation over a phrase-based baseline. We also investigate the use of unsupervised dependency parsers, reporting encouraging preliminary results. 1 Introduction Two approaches currently dominate statistical machine translation (MT) research. Phrase-based models (Koehn et al., 2003) excel at capturing local reordering phenomena and memorizing multi-word translations. Models that employ syntax or syntaxlike representations (Chiang, 2005; Galley et al., 2006; Zollmann and Venugopal, 2006; Huang et al., 2006) handle long-distance reordering better than phrase-based systems (Auli et al., 2009) but often require constraints on the formalism or rule extraction method in order to achieve computational tractability. As a result, certain instances of syntactic divergence are more naturally handled by phrase-based systems (DeNeefe et al., 2007). In this paper we present a new way of combining the advantages of phrase-based and syntax-based MT. We propose a model in which phrases are organized into a tree structure inspired by dependency syntax. Instead of standard dependency trees in which words are vertices, our trees have phrases as vertices. We describe a simple heuristic to extract p</context>
</contexts>
<marker>Auli, Lopez, Hoang, Koehn, 2009</marker>
<rawString>M. Auli, A. Lopez, H. Hoang, and P. Koehn. 2009. A systematic analysis of translation model search spaces. In Proceedings of the Fourth Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bergsma</author>
<author>C Cherry</author>
</authors>
<title>Fast and accurate arc filtering for dependency parsing.</title>
<date>2010</date>
<booktitle>In Proc. of COLING.</booktitle>
<contexts>
<context position="23432" citStr="Bergsma and Cherry, 2010" startWordPosition="3919" endWordPosition="3922">threshold that leaves fewer than 1000 edges in the resulting lattice. As complexity is quadratic in E, forcing E to be less than 1000 improves runtime substantially. After pruning, the lattices still contain more than 1016 paths on average and oracle BLEU scores are typically 12-15 points higher than the model-best paths. Pass 2: Parent Ranking Given a pruned lattice, we then remove some candidate dependency arcs from consideration. It is common in dependency parsing to use a coarse model to rank the top k parents for each word, and to only consider these during parsing (Martins et al., 2009; Bergsma and Cherry, 2010). Unlike string parsing, our phrase lattices impose several types of constraints on allowable arcs. For example, each node in the phrase lattice is annotated with a coverage vector—a bit vector indicating which words in the source sentence have been translated—which implies a topological ordering of the nodes. To handle constraints like these, we first use the Floyd-Warshall algorithm (Floyd, 1962) to find the best score between every pair of nodes in the lattice. This algorithm also tells us whether each edge is reachable from each other edge, allowing us to immediately prune dependency arcs </context>
</contexts>
<marker>Bergsma, Cherry, 2010</marker>
<rawString>S. Bergsma and C. Cherry. 2010. Fast and accurate arc filtering for dependency parsing. In Proc. of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Blunsom</author>
<author>T Cohn</author>
</authors>
<title>Unsupervised induction of tree substitution grammars for dependency parsing.</title>
<date>2010</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="35570" citStr="Blunsom and Cohn, 2010" startWordPosition="5957" endWordPosition="5961">. In this paper, we use the standard dependency model with valence (DMV; Klein and Manning, 2004). When training is initialized using the output of a simpler, concave dependency model, the rectly translated and reordered, but the system was nonetheless able to use it to improve the fluency of the output. DMV can approach state-of-the-art unsupervised accuracy (Gimpel and Smith, 2011). For English, the resulting parser achieves 53.1% attachment accuracy on Section 23 of the Penn Treebank (Marcus et al., 1993), which approaches the 55.7% accuracy of a recent state-of-the-art unsupervised model (Blunsom and Cohn, 2010). The Chinese parser, initialized and trained the same way, achieves 44.4%, which is the highest reported accuracy on the Chinese Treebank (Xue et al., 2004) test set. Most unsupervised grammar induction models assume gold standard POS tags and sentences stripped of punctuation. We use the Stanford tagger (Toutanova et al., 2003) to obtain tags for both English and Chinese, parse the sentences without punctuation using the DMV, and then attach punctuation tokens to the root word of the tree in a postprocessing step. For English, the predicted parents agreed with those of TurboParser for 48.7% </context>
</contexts>
<marker>Blunsom, Cohn, 2010</marker>
<rawString>P. Blunsom and T. Cohn. 2010. Unsupervised induction of tree substitution grammars for dependency parsing. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>P V deSouza</author>
<author>R L Mercer</author>
<author>V J Della Pietra</author>
<author>J C Lai</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<contexts>
<context position="14520" citStr="Brown et al., 1992" startWordPosition="2397" endWordPosition="2400">set of extracted 3For a monolingual task, Wu et al. (2009) used a shallow parser to convert lexical dependencies from a dependency parser into phrase dependencies. phrase dependencies of the form (u, v, d), where u is the head phrase, v is the child phrase, and d E {left, right} is the direction, we then estimate conditional probabilities p(v|u, d) using relative frequency estimation. Table 3 shows the most probable child phrases for an example parent phrase. To combat data sparseness, we perform the same procedure with each word replaced by its word cluster ID obtained from Brown clustering (Brown et al., 1992). We include a feature in the model for the sum of the scaled log-probabilities of each attachment: m/ ( ) max 0, C + log p(φi|φτφ(i), d(i) (1) where d(i) = I[τφ(i) − i &gt; 0] is the direction of the dependency arc. Although we use log-probabilities in this feature function, we first add a constant C to each to ensure they are all positive.4 The max expression protects unseen parent-child phrase dependencies from causing the score to be negative infinity. Our motivation is a desire for the features to be used to prefer one derivation over another but not to rule out a derivation completely if it</context>
</contexts>
<marker>Brown, deSouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>P. F. Brown, P. V. deSouza, R. L. Mercer, V. J. Della Pietra, and J. C. Lai. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Carreras</author>
<author>M Collins</author>
</authors>
<title>Non-projective parsing for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="5328" citStr="Carreras and Collins (2009)" startWordPosition="779" endWordPosition="782">ncy syntax on phrase pairs. Shen et al. (2008) presented an extension to Hiero (Chiang, 2005) in which rules have target-side dependency syntax and therefore enable the use of a dependency language model. More recently, researchers have sought the benefits of dependency syntax while preserving the advantages of phrase-based models, such as efficiency and coverage. Galley and Manning (2009) loosened standard assumptions about dependency parsing so that the efficient left-to-right decoding procedure of phrase-based translation could be retained while a dependency language model is incorporated. Carreras and Collins (2009) presented a string-todependency system that permits non-projective dependency trees (thereby allowing a larger space of translations) and use a rule extraction procedure that includes rules for every phrase in the phrase table. We take an additional step in this direction by working with dependency grammars on the phrases themselves, thereby bringing together the structural components of phrase-based and dependency-based MT in a single model. While others have worked on combining rules from multiple syntax-based systems (Liu et al., 2009) or using posteriors from multiple models to score tran</context>
</contexts>
<marker>Carreras, Collins, 2009</marker>
<rawString>X. Carreras and M. Collins. 2009. Non-projective parsing for statistical machine translation. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Chang</author>
<author>M Galley</author>
<author>C Manning</author>
</authors>
<title>Optimizing Chinese word segmentation for machine translation performance.</title>
<date>2008</date>
<booktitle>In Proc. of the Third Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="28196" citStr="Chang et al., 2008" startWordPosition="4751" endWordPosition="4754">. Our training procedure requires two executions of MERT, and the second typically takes more iterations to converge (10 to 20 is typical) than the first due to the use of a larger feature set and increased possibility for search error due to the enlarged search space. 7 Experiments For experimental evaluation, we consider Chineseto-English (ZH-EN) and Urdu-to-English (UREN) translation and compare our system to Moses (Koehn et al., 2007). For ZH-EN, we used 303k sentence pairs from the FBIS corpus (LDC2003E14). We segmented the Chinese data using the Stanford Chinese segmenter in “CTB” mode (Chang et al., 2008), giving us 7.9M Chinese words and 9.4M English words. For UR-EN, we used parallel data from the NIST MT08 evaluation consisting of 1.2M Urdu words and 1.1M English words. We trained a baseline Moses system using default settings and features. Word alignment was performed using GIZA++ (Och and Ney, 2003) in both directions and the grow-diag-final-and heuristic was used to symmetrize the alignments. We used a max phrase length of 7 when extracting phrases. Trigram language models were estimated using the SRI language modeling toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and </context>
</contexts>
<marker>Chang, Galley, Manning, 2008</marker>
<rawString>P. Chang, M. Galley, and C. Manning. 2008. Optimizing Chinese word segmentation for machine translation performance. In Proc. of the Third Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Chen</author>
<author>J Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1998</date>
<tech>Technical report 10-98,</tech>
<institution>Harvard University.</institution>
<contexts>
<context position="28810" citStr="Chen and Goodman, 1998" startWordPosition="4847" endWordPosition="4850">l., 2008), giving us 7.9M Chinese words and 9.4M English words. For UR-EN, we used parallel data from the NIST MT08 evaluation consisting of 1.2M Urdu words and 1.1M English words. We trained a baseline Moses system using default settings and features. Word alignment was performed using GIZA++ (Och and Ney, 2003) in both directions and the grow-diag-final-and heuristic was used to symmetrize the alignments. We used a max phrase length of 7 when extracting phrases. Trigram language models were estimated using the SRI language modeling toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998). To estimate language models for each language pair, we used the English side of the parallel corpus concatenated with 200M words of randomly-selected sentences from the Gigaword v4 corpus (excluding the NY Times and LA Times). We used this baseline Moses system to generate phrase lattices for our system, so our model includes all of the Moses features in addition to the 480 MT03 (tune) MT02 MT05 MT06 Average Moses 33.84 33.35 31.81 28.82 31.33 QPDG (TT) 34.63 (+0.79) 34.10 (+0.75) 32.15 (+0.34) 29.33 (+0.51) 31.86 (+0.53) QPDG (TT+S2T+T2T) 34.98 (+1.14) 34.26 (+0.91) 32.34 (+0.53) 29.35 (+0.</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>S. Chen and J. Goodman. 1998. An empirical study of smoothing techniques for language modeling. Technical report 10-98, Harvard University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. ofACL.</booktitle>
<contexts>
<context position="1279" citStr="Chiang, 2005" startWordPosition="176" endWordPosition="177"> we describe a coarse-to-fine approach based on lattice dependency parsing of phrase lattices. We demonstrate performance improvements for Chinese-English and UrduEnglish translation over a phrase-based baseline. We also investigate the use of unsupervised dependency parsers, reporting encouraging preliminary results. 1 Introduction Two approaches currently dominate statistical machine translation (MT) research. Phrase-based models (Koehn et al., 2003) excel at capturing local reordering phenomena and memorizing multi-word translations. Models that employ syntax or syntaxlike representations (Chiang, 2005; Galley et al., 2006; Zollmann and Venugopal, 2006; Huang et al., 2006) handle long-distance reordering better than phrase-based systems (Auli et al., 2009) but often require constraints on the formalism or rule extraction method in order to achieve computational tractability. As a result, certain instances of syntactic divergence are more naturally handled by phrase-based systems (DeNeefe et al., 2007). In this paper we present a new way of combining the advantages of phrase-based and syntax-based MT. We propose a model in which phrases are organized into a tree structure inspired by depende</context>
<context position="4794" citStr="Chiang, 2005" startWordPosition="703" endWordPosition="704">to function on phrases, enabling a tighter coupling between the phrase segmentation and syntactic structures. We also present a decoder efficient enough to scale to large data sets and present performance improvements in large-scale experiments over a stateof-the-art phrase-based baseline. Aside from QG, there have been many efforts to use dependency syntax in machine translation. Quirk et al. (2005) used a source-side dependency parser and projected automatic parses across word alignments in order to model dependency syntax on phrase pairs. Shen et al. (2008) presented an extension to Hiero (Chiang, 2005) in which rules have target-side dependency syntax and therefore enable the use of a dependency language model. More recently, researchers have sought the benefits of dependency syntax while preserving the advantages of phrase-based models, such as efficiency and coverage. Galley and Manning (2009) loosened standard assumptions about dependency parsing so that the efficient left-to-right decoding procedure of phrase-based translation could be retained while a dependency language model is incorporated. Carreras and Collins (2009) presented a string-todependency system that permits non-projectiv</context>
<context position="29738" citStr="Chiang, 2005" startWordPosition="4997" endWordPosition="4998">model includes all of the Moses features in addition to the 480 MT03 (tune) MT02 MT05 MT06 Average Moses 33.84 33.35 31.81 28.82 31.33 QPDG (TT) 34.63 (+0.79) 34.10 (+0.75) 32.15 (+0.34) 29.33 (+0.51) 31.86 (+0.53) QPDG (TT+S2T+T2T) 34.98 (+1.14) 34.26 (+0.91) 32.34 (+0.53) 29.35 (+0.53) 31.98 (+0.65) Table 4: Chinese-English Results (% BLEU). QPDG features described in §4. In our experiments, we compare our QPDG system (lattice parsing on each lattice) to the Moses baseline (finding the best path through each lattice). The conventional wisdom holds that hierarchical phrase-based translation (Chiang, 2005) performs better than phrasebased translation for language pairs that require large amounts of reordering, such as ZH-EN and UR-EN. However, researchers have shown that this performance gap diminishes when using a larger distortion limit (Zollmann et al., 2008) and may disappear entirely when using a lexicalized reordering model (Lopez, 2008; Galley and Manning, 2010). So, we increase the Moses distortion limit from 6 (the default) to 10 and use Moses’ default lexicalized reordering model (Koehn et al., 2005). We parsed the Chinese text using the Stanford parser (Levy and Manning, 2003) and th</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>D. Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Das</author>
<author>N A Smith</author>
</authors>
<title>Paraphrase identification as probabilistic quasi-synchronous recognition.</title>
<date>2009</date>
<booktitle>In Proc. of ACL-IJCNLP.</booktitle>
<contexts>
<context position="7342" citStr="Das and Smith, 2009" startWordPosition="1114" endWordPosition="1117">responding feature weights. Table 1 summarizes our notation. In modeling p(t, γ, φ, τφ, a |s, τs), we make use of quasi-synchronous grammar (QG; Smith and Eisner, 2006). Given a source sentence and its parse, a QG induces a probabilistic monolingual grammar over sentences “inspired” by the source sentence and tree. We denote this grammar by Gs,τs; its (weighted) language is the set of translations of s. Quasi-synchronous grammar makes no restrictions on the form of the target monolingual grammar, though dependency grammars have been used in most previous applications of QG (Wang et al., 2007; Das and Smith, 2009; Smith and Eisner, 2009), including previous work in MT (Smith and Eisner, 2006; Gimpel and Smith, 2009). We previously presented a word-based machine translation model based on a quasi-synchronous dependency grammar. However, it is well-known in the MT community that translation quality is improved when larger units are modeled. Therefore, we use a dependency grammar in which the leaves are phrases rather than words. We define a phrase dependency grammar as a model p(φ,τφ|t) over the joint space of segmentations of a sentence into phrases and dependency trees on the phrases.2 Phrase dependen</context>
</contexts>
<marker>Das, Smith, 2009</marker>
<rawString>D. Das and N. A. Smith. 2009. Paraphrase identification as probabilistic quasi-synchronous recognition. In Proc. of ACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S DeNeefe</author>
<author>K Knight</author>
<author>W Wang</author>
<author>D Marcu</author>
</authors>
<title>What can syntax-based MT learn from phrase-based MT?</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="1686" citStr="DeNeefe et al., 2007" startWordPosition="236" endWordPosition="239">on (MT) research. Phrase-based models (Koehn et al., 2003) excel at capturing local reordering phenomena and memorizing multi-word translations. Models that employ syntax or syntaxlike representations (Chiang, 2005; Galley et al., 2006; Zollmann and Venugopal, 2006; Huang et al., 2006) handle long-distance reordering better than phrase-based systems (Auli et al., 2009) but often require constraints on the formalism or rule extraction method in order to achieve computational tractability. As a result, certain instances of syntactic divergence are more naturally handled by phrase-based systems (DeNeefe et al., 2007). In this paper we present a new way of combining the advantages of phrase-based and syntax-based MT. We propose a model in which phrases are organized into a tree structure inspired by dependency syntax. Instead of standard dependency trees in which words are vertices, our trees have phrases as vertices. We describe a simple heuristic to extract phrase dependencies from an aligned parallel corpus parsed on the target side, and use them to compute target-side tree features. We define additional string-to-tree features and, if a source-side dependency parser is available, tree-to-tree features </context>
</contexts>
<marker>DeNeefe, Knight, Wang, Marcu, 2007</marker>
<rawString>S. DeNeefe, K. Knight, W. Wang, and D. Marcu. 2007. What can syntax-based MT learn from phrase-based MT? In Proc. of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J DeNero</author>
<author>S Kumar</author>
<author>C Chelba</author>
<author>F J Och</author>
</authors>
<title>Model combination for machine translation.</title>
<date>2010</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="5958" citStr="DeNero et al., 2010" startWordPosition="875" endWordPosition="878"> a string-todependency system that permits non-projective dependency trees (thereby allowing a larger space of translations) and use a rule extraction procedure that includes rules for every phrase in the phrase table. We take an additional step in this direction by working with dependency grammars on the phrases themselves, thereby bringing together the structural components of phrase-based and dependency-based MT in a single model. While others have worked on combining rules from multiple syntax-based systems (Liu et al., 2009) or using posteriors from multiple models to score translations (DeNero et al., 2010), we are not aware of any other work that seeks to directly integrate phrase-based and syntax-based machine translation at the modeling level.1 1Dymetman and Cancedda (2010) present a formal analy3 Model Given a sentence s and its dependency tree τs, we formulate the translation problem as finding the target sentence t*, the segmentation γ* of s into phrases, the segmentation φ* of t* into phrases, the dependency tree τ*φ on the target phrases φ*, and the one-to-one phrase alignment a* such that (t*, γ*, φ*, τ*φ, a*)= argmax p(t, γ, φ, τφ, a|s, τs) (t,γ,φ,τφ,a) We use a linear model (Och and N</context>
</contexts>
<marker>DeNero, Kumar, Chelba, Och, 2010</marker>
<rawString>J. DeNero, S. Kumar, C. Chelba, and F. J. Och. 2010. Model combination for machine translation. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Dymetman</author>
<author>N Cancedda</author>
</authors>
<title>Intersecting hierarchical and phrase-based models of translation. formal aspects and algorithms.</title>
<date>2010</date>
<booktitle>In Proc. of SSST-4.</booktitle>
<contexts>
<context position="6131" citStr="Dymetman and Cancedda (2010)" startWordPosition="901" endWordPosition="904"> includes rules for every phrase in the phrase table. We take an additional step in this direction by working with dependency grammars on the phrases themselves, thereby bringing together the structural components of phrase-based and dependency-based MT in a single model. While others have worked on combining rules from multiple syntax-based systems (Liu et al., 2009) or using posteriors from multiple models to score translations (DeNero et al., 2010), we are not aware of any other work that seeks to directly integrate phrase-based and syntax-based machine translation at the modeling level.1 1Dymetman and Cancedda (2010) present a formal analy3 Model Given a sentence s and its dependency tree τs, we formulate the translation problem as finding the target sentence t*, the segmentation γ* of s into phrases, the segmentation φ* of t* into phrases, the dependency tree τ*φ on the target phrases φ*, and the one-to-one phrase alignment a* such that (t*, γ*, φ*, τ*φ, a*)= argmax p(t, γ, φ, τφ, a|s, τs) (t,γ,φ,τφ,a) We use a linear model (Och and Ney, 2002): p(t, γ,φ, τφ, a |s, τs) a exp{θTg(s, τs, t, γ, φ, τφ, a)} where g is a vector of arbitrary feature functions on the full set of structures and θ holds correspondi</context>
</contexts>
<marker>Dymetman, Cancedda, 2010</marker>
<rawString>M. Dymetman and N. Cancedda. 2010. Intersecting hierarchical and phrase-based models of translation. formal aspects and algorithms. In Proc. of SSST-4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
<author>E Goldlust</author>
<author>N A Smith</author>
</authors>
<title>Compiling Comp Ling: Practical weighted dynamic programming and the Dyna language.</title>
<date>2005</date>
<booktitle>In Proc. of HLTEMNLP.</booktitle>
<contexts>
<context position="25580" citStr="Eisner et al., 2005" startWordPosition="4292" endWordPosition="4295"> = 300, α = 100, and 0 = 400. 479 Input: tuning set D = hS, Ti, initial weights λ0 for coarse model, initial weights ψ0 for additional features in fine model Output: coarse model learned weights: λM, fine model learned weights: hλ∗, ψ∗i λM ← MERT (S, T, λ0, 100, MOSES); LMERT ← GenerateLattices (S, λM); LFB ← FBPrune (LMERT, λM); hλ∗, ψ∗i ← MERT (LFB, T, hλM, ψ0i, 200, QGDEPPARSE); return λM, hλ∗, ψ∗i; Algorithm 2: CoarseToFineTrain Pass 3: Lattice Dependency Parsing After completing the coarse passes, we parse using bottom-up dynamic programming based on the agenda algorithm (Nederhof, 2003; Eisner et al., 2005). We only consider arcs that survived the filtering in Pass 2. We weight agenda items by the sum of their scores and the Floyd-Warshall best path scores both from the start node of the lattice to the beginning of the item and the end of the item to any final node. This heuristic helps us to favor exploration of items that are highly likely under the phrase-based model. If the score of the partial structure can only get worse when combining it with other structures (e.g., in a PCFG), then the first time that we pop an item of type GOAL from the agenda, we are guaranteed to have the best parse. </context>
</contexts>
<marker>Eisner, Goldlust, Smith, 2005</marker>
<rawString>J. Eisner, E. Goldlust, and N. A. Smith. 2005. Compiling Comp Ling: Practical weighted dynamic programming and the Dyna language. In Proc. of HLTEMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing: An exploration.</title>
<date>1996</date>
<booktitle>In Proc. of COLING.</booktitle>
<contexts>
<context position="21646" citStr="Eisner (1996)" startWordPosition="3613" endWordPosition="3614">l., 2002; Macherey et al., 2008; Tromble et al., 2008). Each edge in a phrase lattice corresponds to a phrase pair and each path through the lattice corresponds to a tuple (t, -y, 0, a) for the input s. Decoding for a phrase lattice consists of finding the highest-scoring path, which is done using dynamic programming. To also maximize over τφ, we perform lattice dependency parsing, which allows us to search over the space of tuples (t, -y, 0, a, τφ). Given the lattice and Gs,τ8, lattice parsing is a straightforward generalization of the standard arc-factored dynamic programming algorithm from Eisner (1996). The lattice parsing algorithm requires O(E2V ) time and O(E2 + V E) space, where E is the number of edges in the lattice and V is the number of nodes.7 Typical phrase lattices might easily contain tens of thousands of nodes and edges, making exact search prohibitively expensive for all but the smallest lattices. So, we use approximate search based on coarse-to-fine decoding. We now discuss each step of this procedure; an outline is shown as Alg. 1. Pass 1: Lattice Pruning After generating phrase lattices using a phrase-based MT system, we prune lattice edges using forward-backward pruning (S</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>J. Eisner. 1996. Three new probabilistic models for dependency parsing: An exploration. In Proc. of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R W Floyd</author>
</authors>
<title>Algorithm 97: Shortest path.</title>
<date>1962</date>
<journal>Communications of the ACM,</journal>
<volume>5</volume>
<issue>6</issue>
<contexts>
<context position="23833" citStr="Floyd, 1962" startWordPosition="3984" endWordPosition="3985">cs from consideration. It is common in dependency parsing to use a coarse model to rank the top k parents for each word, and to only consider these during parsing (Martins et al., 2009; Bergsma and Cherry, 2010). Unlike string parsing, our phrase lattices impose several types of constraints on allowable arcs. For example, each node in the phrase lattice is annotated with a coverage vector—a bit vector indicating which words in the source sentence have been translated—which implies a topological ordering of the nodes. To handle constraints like these, we first use the Floyd-Warshall algorithm (Floyd, 1962) to find the best score between every pair of nodes in the lattice. This algorithm also tells us whether each edge is reachable from each other edge, allowing us to immediately prune dependency arcs between edges that are unreachable from each other. After eliminating impossible arcs, we turn to pruning away unlikely ones. In standard (string) dependency parsing, every word is assigned a parent. In lattice parsing, however, most lattice edges will not be assigned any parent. Certain lattice edges are much more likely to be contained within paths, so we allow some edges to have more candidate p</context>
</contexts>
<marker>Floyd, 1962</marker>
<rawString>R. W. Floyd. 1962. Algorithm 97: Shortest path. Communications of the ACM, 5(6).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>C D Manning</author>
</authors>
<title>Quadratic-time dependency parsing for machine translation.</title>
<date>2009</date>
<booktitle>In Proc. of ACL-IJCNLP.</booktitle>
<contexts>
<context position="5093" citStr="Galley and Manning (2009)" startWordPosition="746" endWordPosition="749">ne. Aside from QG, there have been many efforts to use dependency syntax in machine translation. Quirk et al. (2005) used a source-side dependency parser and projected automatic parses across word alignments in order to model dependency syntax on phrase pairs. Shen et al. (2008) presented an extension to Hiero (Chiang, 2005) in which rules have target-side dependency syntax and therefore enable the use of a dependency language model. More recently, researchers have sought the benefits of dependency syntax while preserving the advantages of phrase-based models, such as efficiency and coverage. Galley and Manning (2009) loosened standard assumptions about dependency parsing so that the efficient left-to-right decoding procedure of phrase-based translation could be retained while a dependency language model is incorporated. Carreras and Collins (2009) presented a string-todependency system that permits non-projective dependency trees (thereby allowing a larger space of translations) and use a rule extraction procedure that includes rules for every phrase in the phrase table. We take an additional step in this direction by working with dependency grammars on the phrases themselves, thereby bringing together th</context>
<context position="11743" citStr="Galley and Manning, 2009" startWordPosition="1926" endWordPosition="1929">n our model that are used to score phrase dependency trees. We shall refer to these as QPDG features and will find it useful later to notationally distinguish their feature weights from those of the phrase-based model. We use λ for weights of the standard phrase-based model features and ψ for weights of the QPDG features. We include three categories of features, differentiated by what pieces of structure they consider. 4.1 Target Tree Features We first include features that only consider t, φ, and τφ. These features can be categorized as “syntactic language model” features (Shen et al., 2008; Galley and Manning, 2009), though unlike previous work our features model both the phrase segmentation and dependency structure. Typically, these sorts of features are probabilities estimated from a corpus parsed using a supervised parser. However, there do not currently exist treebanks with annotated phrase 476 , made up 0.057 he made up 0.021 supreme court made up 0.014 court made up 0.014 in september 2000 made up 0.014 in september 2000 , made up 0.014 made up of 0.065 made up . 0.029 made up , 0.016 made up mind to 0.01 Table 3: Most probable child phrases for the parent phrase “made up” for each direction, sorte</context>
</contexts>
<marker>Galley, Manning, 2009</marker>
<rawString>M. Galley and C. D. Manning. 2009. Quadratic-time dependency parsing for machine translation. In Proc. of ACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>C D Manning</author>
</authors>
<title>Accurate nonhierarchical phrase-based translation.</title>
<date>2010</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="30108" citStr="Galley and Manning, 2010" startWordPosition="5052" endWordPosition="5055">s described in §4. In our experiments, we compare our QPDG system (lattice parsing on each lattice) to the Moses baseline (finding the best path through each lattice). The conventional wisdom holds that hierarchical phrase-based translation (Chiang, 2005) performs better than phrasebased translation for language pairs that require large amounts of reordering, such as ZH-EN and UR-EN. However, researchers have shown that this performance gap diminishes when using a larger distortion limit (Zollmann et al., 2008) and may disappear entirely when using a lexicalized reordering model (Lopez, 2008; Galley and Manning, 2010). So, we increase the Moses distortion limit from 6 (the default) to 10 and use Moses’ default lexicalized reordering model (Koehn et al., 2005). We parsed the Chinese text using the Stanford parser (Levy and Manning, 2003) and the English text using TurboParser (Martins et al., 2009). We note that computing our features requires parsing the target (English) side of the parallel text, but not the source side. We only need to parse the source side of the tuning and test sets, and the only features that look at the source-side parse are those from §4.3. To obtain Brown clusters for the target tr</context>
</contexts>
<marker>Galley, Manning, 2010</marker>
<rawString>M. Galley and C. D. Manning. 2010. Accurate nonhierarchical phrase-based translation. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>J Graehl</author>
<author>K Knight</author>
<author>D Marcu</author>
<author>S DeNeefe</author>
<author>W Wang</author>
<author>I Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In Proc. of COLING-ACL.</booktitle>
<contexts>
<context position="1300" citStr="Galley et al., 2006" startWordPosition="178" endWordPosition="181"> coarse-to-fine approach based on lattice dependency parsing of phrase lattices. We demonstrate performance improvements for Chinese-English and UrduEnglish translation over a phrase-based baseline. We also investigate the use of unsupervised dependency parsers, reporting encouraging preliminary results. 1 Introduction Two approaches currently dominate statistical machine translation (MT) research. Phrase-based models (Koehn et al., 2003) excel at capturing local reordering phenomena and memorizing multi-word translations. Models that employ syntax or syntaxlike representations (Chiang, 2005; Galley et al., 2006; Zollmann and Venugopal, 2006; Huang et al., 2006) handle long-distance reordering better than phrase-based systems (Auli et al., 2009) but often require constraints on the formalism or rule extraction method in order to achieve computational tractability. As a result, certain instances of syntactic divergence are more naturally handled by phrase-based systems (DeNeefe et al., 2007). In this paper we present a new way of combining the advantages of phrase-based and syntax-based MT. We propose a model in which phrases are organized into a tree structure inspired by dependency syntax. Instead o</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe, W. Wang, and I. Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proc. of COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Gimpel</author>
<author>N A Smith</author>
</authors>
<title>Feature-rich translation by quasi-synchronous lattice parsing.</title>
<date>2009</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="2770" citStr="Gimpel and Smith, 2009" startWordPosition="400" endWordPosition="404">de tree features. We define additional string-to-tree features and, if a source-side dependency parser is available, tree-to-tree features to capture properties of how phrase dependencies interact with reordering. To leverage standard phrase-based features alongside our novel features, we require a formalism that supports flexible feature combination and efficient decoding. Quasi-synchronous grammar (QG) provides this backbone (Smith and Eisner, 2006); we describe a coarse-to-fine approach for decoding within this framework, advancing substantially over earlier QG machine translation systems (Gimpel and Smith, 2009). The decoder involves generating a phrase lattice (Ueffing et al., 2002) in a coarse pass using a phrase-based model, followed by lattice dependency parsing of the phrase lattice. This approach allows us to feasibly explore the combined search space of segmentations, phrase alignments, and target phrase dependency trees. Our experiments demonstrate an average improvement of +0.65 BLEU in Chinese-English translation across three test sets and an improvement of +0.75 BLEU in Urdu-English translation over a phrase-based baseline. We also describe experiments in which we replace supervised depend</context>
<context position="4078" citStr="Gimpel and Smith, 2009" startWordPosition="590" endWordPosition="593">ed Chinese parser and a state-of-the-art unsupervised English parser provides our best results, giving an averaged gain of +0.79 BLEU over the baseline. We also discuss how our model improves translation quality and discuss future possibilities for combining approaches to ma474 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 474–485, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics chine translation using our framework. 2 Related Work We previously applied quasi-synchronous grammar to machine translation (Gimpel and Smith, 2009), but that system performed translation fundamentally at the word level. Here we generalize that model to function on phrases, enabling a tighter coupling between the phrase segmentation and syntactic structures. We also present a decoder efficient enough to scale to large data sets and present performance improvements in large-scale experiments over a stateof-the-art phrase-based baseline. Aside from QG, there have been many efforts to use dependency syntax in machine translation. Quirk et al. (2005) used a source-side dependency parser and projected automatic parses across word alignments in</context>
<context position="7447" citStr="Gimpel and Smith, 2009" startWordPosition="1131" endWordPosition="1134"> make use of quasi-synchronous grammar (QG; Smith and Eisner, 2006). Given a source sentence and its parse, a QG induces a probabilistic monolingual grammar over sentences “inspired” by the source sentence and tree. We denote this grammar by Gs,τs; its (weighted) language is the set of translations of s. Quasi-synchronous grammar makes no restrictions on the form of the target monolingual grammar, though dependency grammars have been used in most previous applications of QG (Wang et al., 2007; Das and Smith, 2009; Smith and Eisner, 2009), including previous work in MT (Smith and Eisner, 2006; Gimpel and Smith, 2009). We previously presented a word-based machine translation model based on a quasi-synchronous dependency grammar. However, it is well-known in the MT community that translation quality is improved when larger units are modeled. Therefore, we use a dependency grammar in which the leaves are phrases rather than words. We define a phrase dependency grammar as a model p(φ,τφ|t) over the joint space of segmentations of a sentence into phrases and dependency trees on the phrases.2 Phrase dependency grammars sis of the problem of intersecting phrase-based and hierarchical translation models, but do n</context>
<context position="20584" citStr="Gimpel and Smith (2009)" startWordPosition="3433" endWordPosition="3436">umber of dependency arcs that must be crossed to travel from one word to the other in 7-8. We use one feature for undirected path length and one other for directed path length. If there is no (un)directed path from a word in a(i) to a word in 7-0(a(i)), we use oc as the minimum length. There are 15 features in this category, for a total of 23 QPDG features. 478 5 Decoding For a QPDG model, decoding consists of finding the highest-scoring tuple (t, -y, 0, τφ, a) for an input sentence s and its parse τs, i.e., finding the most probable derivation under the s/τs-specific grammar Gs,τ8. We follow Gimpel and Smith (2009) in constructing a lattice to represent Gs,τ8 and using lattice parsing to search for the best derivation, but we construct the lattice differently and employ a coarse-tofine strategy (Petrov, 2009) to speed up decoding. It has become common in recent years for MT researchers to exploit efficient data structures for encoding concise representations of the pruned search space of the model, such as phrase lattices for phrase-based MT (Ueffing et al., 2002; Macherey et al., 2008; Tromble et al., 2008). Each edge in a phrase lattice corresponds to a phrase pair and each path through the lattice co</context>
</contexts>
<marker>Gimpel, Smith, 2009</marker>
<rawString>K. Gimpel and N. A. Smith. 2009. Feature-rich translation by quasi-synchronous lattice parsing. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Gimpel</author>
<author>N A Smith</author>
</authors>
<title>Concavity and initialization for unsupervised dependency grammar induction.</title>
<date>2011</date>
<tech>Technical report,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="35333" citStr="Gimpel and Smith, 2011" startWordPosition="5921" endWordPosition="5924">arsers do not match treebank annotations very well, they may perform well when used for extrinsic applications. We believe that syntax-based MT offers a compelling platform for development and extrinsic evaluation of unsupervised parsers. In this paper, we use the standard dependency model with valence (DMV; Klein and Manning, 2004). When training is initialized using the output of a simpler, concave dependency model, the rectly translated and reordered, but the system was nonetheless able to use it to improve the fluency of the output. DMV can approach state-of-the-art unsupervised accuracy (Gimpel and Smith, 2011). For English, the resulting parser achieves 53.1% attachment accuracy on Section 23 of the Penn Treebank (Marcus et al., 1993), which approaches the 55.7% accuracy of a recent state-of-the-art unsupervised model (Blunsom and Cohn, 2010). The Chinese parser, initialized and trained the same way, achieves 44.4%, which is the highest reported accuracy on the Chinese Treebank (Xue et al., 2004) test set. Most unsupervised grammar induction models assume gold standard POS tags and sentences stripped of punctuation. We use the Stanford tagger (Toutanova et al., 2003) to obtain tags for both English</context>
</contexts>
<marker>Gimpel, Smith, 2011</marker>
<rawString>K. Gimpel and N. A. Smith. 2011. Concavity and initialization for unsupervised dependency grammar induction. Technical report, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Huang</author>
<author>K Knight</author>
<author>A Joshi</author>
</authors>
<title>Statistical syntax-directed translation with extended domain of locality.</title>
<date>2006</date>
<booktitle>In Proc. of AMTA.</booktitle>
<contexts>
<context position="1351" citStr="Huang et al., 2006" startWordPosition="186" endWordPosition="189">y parsing of phrase lattices. We demonstrate performance improvements for Chinese-English and UrduEnglish translation over a phrase-based baseline. We also investigate the use of unsupervised dependency parsers, reporting encouraging preliminary results. 1 Introduction Two approaches currently dominate statistical machine translation (MT) research. Phrase-based models (Koehn et al., 2003) excel at capturing local reordering phenomena and memorizing multi-word translations. Models that employ syntax or syntaxlike representations (Chiang, 2005; Galley et al., 2006; Zollmann and Venugopal, 2006; Huang et al., 2006) handle long-distance reordering better than phrase-based systems (Auli et al., 2009) but often require constraints on the formalism or rule extraction method in order to achieve computational tractability. As a result, certain instances of syntactic divergence are more naturally handled by phrase-based systems (DeNeefe et al., 2007). In this paper we present a new way of combining the advantages of phrase-based and syntax-based MT. We propose a model in which phrases are organized into a tree structure inspired by dependency syntax. Instead of standard dependency trees in which words are vert</context>
</contexts>
<marker>Huang, Knight, Joshi, 2006</marker>
<rawString>L. Huang, K. Knight, and A. Joshi. 2006. Statistical syntax-directed translation with extended domain of locality. In Proc. of AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Corpus-based induction of syntactic structure: Models of dependency and constituency.</title>
<date>2004</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="35044" citStr="Klein and Manning, 2004" startWordPosition="5875" endWordPosition="5878">d like to translate. Fortunately, unsupervised dependency grammar induction has improved substantially in recent years due to a flurry of recent research. While attachment accuracies on standard treebank test sets are still relatively low, it may be the case that even though unsupervised parsers do not match treebank annotations very well, they may perform well when used for extrinsic applications. We believe that syntax-based MT offers a compelling platform for development and extrinsic evaluation of unsupervised parsers. In this paper, we use the standard dependency model with valence (DMV; Klein and Manning, 2004). When training is initialized using the output of a simpler, concave dependency model, the rectly translated and reordered, but the system was nonetheless able to use it to improve the fluency of the output. DMV can approach state-of-the-art unsupervised accuracy (Gimpel and Smith, 2011). For English, the resulting parser achieves 53.1% attachment accuracy on Section 23 of the Penn Treebank (Marcus et al., 1993), which approaches the 55.7% accuracy of a recent state-of-the-art unsupervised model (Blunsom and Cohn, 2010). The Chinese parser, initialized and trained the same way, achieves 44.4%</context>
</contexts>
<marker>Klein, Manning, 2004</marker>
<rawString>D. Klein and C. D. Manning. 2004. Corpus-based induction of syntactic structure: Models of dependency and constituency. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proc. of HLT-NAACL.</booktitle>
<contexts>
<context position="1123" citStr="Koehn et al., 2003" startWordPosition="154" endWordPosition="157">d syntax-based MT in a single model. We describe a method of extracting phrase dependencies from parallel text using a target-side dependency parser. For decoding, we describe a coarse-to-fine approach based on lattice dependency parsing of phrase lattices. We demonstrate performance improvements for Chinese-English and UrduEnglish translation over a phrase-based baseline. We also investigate the use of unsupervised dependency parsers, reporting encouraging preliminary results. 1 Introduction Two approaches currently dominate statistical machine translation (MT) research. Phrase-based models (Koehn et al., 2003) excel at capturing local reordering phenomena and memorizing multi-word translations. Models that employ syntax or syntaxlike representations (Chiang, 2005; Galley et al., 2006; Zollmann and Venugopal, 2006; Huang et al., 2006) handle long-distance reordering better than phrase-based systems (Auli et al., 2009) but often require constraints on the formalism or rule extraction method in order to achieve computational tractability. As a result, certain instances of syntactic divergence are more naturally handled by phrase-based systems (DeNeefe et al., 2007). In this paper we present a new way </context>
<context position="12726" citStr="Koehn et al., 2003" startWordPosition="2089" endWordPosition="2092">0.014 in september 2000 made up 0.014 in september 2000 , made up 0.014 made up of 0.065 made up . 0.029 made up , 0.016 made up mind to 0.01 Table 3: Most probable child phrases for the parent phrase “made up” for each direction, sorted by the conditional probability of the child phrase given the parent phrase and direction. dependency trees. Our solution is to use a standard supervised dependency parser and extract phrase dependencies using bilingual information.3 We begin by obtaining symmetrized word alignments and extracting phrase pairs using the standard heuristic from phrase-based MT (Koehn et al., 2003). Given the set of extracted phrase pairs for a sentence, denote by W the set of unique target-side phrases among them. We parse the target sentence with a dependency parser and, for each pair of phrases u, v E W, we extract a phrase dependency (along with its direction) if u and v do not overlap and there is at least one lexical dependency between a word in u and a word in v. If there are lexical dependencies in both directions, we extract a phrase dependency only for the single longest one. Since we use a projective dependency parser, the longest lexical dependency between two phrases is gua</context>
<context position="15297" citStr="Koehn et al., 2003" startWordPosition="2537" endWordPosition="2540">[τφ(i) − i &gt; 0] is the direction of the dependency arc. Although we use log-probabilities in this feature function, we first add a constant C to each to ensure they are all positive.4 The max expression protects unseen parent-child phrase dependencies from causing the score to be negative infinity. Our motivation is a desire for the features to be used to prefer one derivation over another but not to rule out a derivation completely if it merely happens to contain a dependency unobserved in the training data. We also include lexical weighting features similar to those used in phrase-based MT (Koehn et al., 2003). Whenever we extract a phrase dependency, we extract the longest lexical dependency contained within it. For all (parent, child, direction) lexical dependency tuples (x, y, d), we estimate conditional probabilities plex (y|x, d) from the parsed corpus using relative frequency estimation. Then, for a phrase dependency with longest lexical dependency (x, y, d), we add a feature for plex(y|x, d) to the model, using a formula similar to Eq. 1. Different instances of a phrase dependency may have different lexical dependencies extracted with them. We add the lexical weight for the most frequent, br</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrase-based translation. In Proc. of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>A Axelrod</author>
<author>A Birch Mayne</author>
<author>C CallisonBurch</author>
<author>M Osborne</author>
<author>D Talbot</author>
</authors>
<title>Edinburgh system description for the 2005 iwslt speech translation evaluation.</title>
<date>2005</date>
<booktitle>In Proc. of IWSLT.</booktitle>
<contexts>
<context position="30252" citStr="Koehn et al., 2005" startWordPosition="5077" endWordPosition="5080">gh each lattice). The conventional wisdom holds that hierarchical phrase-based translation (Chiang, 2005) performs better than phrasebased translation for language pairs that require large amounts of reordering, such as ZH-EN and UR-EN. However, researchers have shown that this performance gap diminishes when using a larger distortion limit (Zollmann et al., 2008) and may disappear entirely when using a lexicalized reordering model (Lopez, 2008; Galley and Manning, 2010). So, we increase the Moses distortion limit from 6 (the default) to 10 and use Moses’ default lexicalized reordering model (Koehn et al., 2005). We parsed the Chinese text using the Stanford parser (Levy and Manning, 2003) and the English text using TurboParser (Martins et al., 2009). We note that computing our features requires parsing the target (English) side of the parallel text, but not the source side. We only need to parse the source side of the tuning and test sets, and the only features that look at the source-side parse are those from §4.3. To obtain Brown clusters for the target tree features in §4.1, we used code from Liang (2005).8 We induced 100 clusters from the English side of the parallel corpus concatenated with 10M</context>
</contexts>
<marker>Koehn, Axelrod, Mayne, CallisonBurch, Osborne, Talbot, 2005</marker>
<rawString>P. Koehn, A. Axelrod, A. Birch Mayne, C. CallisonBurch, M. Osborne, and D. Talbot. 2005. Edinburgh system description for the 2005 iwslt speech translation evaluation. In Proc. of IWSLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
<author>C Dyer</author>
<author>O Bojar</author>
<author>A Constantin</author>
<author>E Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of ACL</booktitle>
<note>(demo session).</note>
<contexts>
<context position="10313" citStr="Koehn et al., 2007" startWordPosition="1653" endWordPosition="1656">inition of QG. The alignment variable in QG links target tree nodes to source tree nodes. However, we never commit to a source phrase dependency tree, instead using a source lexical dependency tree output by a dependency parser, so our alignment variable a is a function from target tree nodes (phrases in φ) to source phrases in γ, which might not be source tree nodes. The features in our model may consider a large number of source phrase dependency trees as long as they are consistent with τ8. 4 Features Our model contains all of the standard phrase-based features found in systems like Moses (Koehn et al., 2007), including four phrase table probability features, a phrase penalty feature, an n-gram language model, a distortion cost, six lexicalized reordering features, and a word penalty feature. We now describe in detail the additional features $ ← said: $ ← we should $ ← said that $ ← has been $ ← is a - us → relations $ ← will be $ ← he said $ ← it is cross - strait → relations $ ← this is $ ← pointed out that $ ← we must ,and → is the → united states the chinese → government the → development of $ ← is the the two → countries $ ← said, he → said: one - china → principle $ ← he said : sino - us → r</context>
<context position="28019" citStr="Koehn et al., 2007" startWordPosition="4722" endWordPosition="4725">hts and for ψ we initialize the two target phrase dependency weights to 0.004, the two lexical dependency weights to 0.001, and the weights for all configuration features to 0.0. Our training procedure requires two executions of MERT, and the second typically takes more iterations to converge (10 to 20 is typical) than the first due to the use of a larger feature set and increased possibility for search error due to the enlarged search space. 7 Experiments For experimental evaluation, we consider Chineseto-English (ZH-EN) and Urdu-to-English (UREN) translation and compare our system to Moses (Koehn et al., 2007). For ZH-EN, we used 303k sentence pairs from the FBIS corpus (LDC2003E14). We segmented the Chinese data using the Stanford Chinese segmenter in “CTB” mode (Chang et al., 2008), giving us 7.9M Chinese words and 9.4M English words. For UR-EN, we used parallel data from the NIST MT08 evaluation consisting of 1.2M Urdu words and 1.1M English words. We trained a baseline Moses system using default settings and features. Word alignment was performed using GIZA++ (Och and Ney, 2003) in both directions and the grow-diag-final-and heuristic was used to symmetrize the alignments. We used a max phrase </context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proc. of ACL (demo session).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Levy</author>
<author>C D Manning</author>
</authors>
<title>Is it harder to parse chinese, or the chinese treebank?</title>
<date>2003</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="30331" citStr="Levy and Manning, 2003" startWordPosition="5090" endWordPosition="5093">sed translation (Chiang, 2005) performs better than phrasebased translation for language pairs that require large amounts of reordering, such as ZH-EN and UR-EN. However, researchers have shown that this performance gap diminishes when using a larger distortion limit (Zollmann et al., 2008) and may disappear entirely when using a lexicalized reordering model (Lopez, 2008; Galley and Manning, 2010). So, we increase the Moses distortion limit from 6 (the default) to 10 and use Moses’ default lexicalized reordering model (Koehn et al., 2005). We parsed the Chinese text using the Stanford parser (Levy and Manning, 2003) and the English text using TurboParser (Martins et al., 2009). We note that computing our features requires parsing the target (English) side of the parallel text, but not the source side. We only need to parse the source side of the tuning and test sets, and the only features that look at the source-side parse are those from §4.3. To obtain Brown clusters for the target tree features in §4.1, we used code from Liang (2005).8 We induced 100 clusters from the English side of the parallel corpus concatenated with 10M words of randomly-selected Gigaword sentences. Only words that appeared at lea</context>
</contexts>
<marker>Levy, Manning, 2003</marker>
<rawString>R. Levy and C. D. Manning. 2003. Is it harder to parse chinese, or the chinese treebank? In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
</authors>
<title>Semi-supervised learning for natural language. Master’s thesis,</title>
<date>2005</date>
<institution>Massachusetts Institute of Technology.</institution>
<contexts>
<context position="30759" citStr="Liang (2005)" startWordPosition="5169" endWordPosition="5170">limit from 6 (the default) to 10 and use Moses’ default lexicalized reordering model (Koehn et al., 2005). We parsed the Chinese text using the Stanford parser (Levy and Manning, 2003) and the English text using TurboParser (Martins et al., 2009). We note that computing our features requires parsing the target (English) side of the parallel text, but not the source side. We only need to parse the source side of the tuning and test sets, and the only features that look at the source-side parse are those from §4.3. To obtain Brown clusters for the target tree features in §4.1, we used code from Liang (2005).8 We induced 100 clusters from the English side of the parallel corpus concatenated with 10M words of randomly-selected Gigaword sentences. Only words that appeared at least twice in this data were considered during clustering. An additional cluster was created for all other words; this allowed us to use phrase dependency cluster features even for out-ofvocabulary words. We used a max phrase length of 7 when extracting phrase dependencies to match the max phrase length used in phrase extraction. Approximately 87M unique phrase dependencies were extracted from the ZH-EN data and 7M from the UR</context>
</contexts>
<marker>Liang, 2005</marker>
<rawString>P. Liang. 2005. Semi-supervised learning for natural language. Master’s thesis, Massachusetts Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Liu</author>
<author>H Mi</author>
<author>Y Feng</author>
<author>Q Liu</author>
</authors>
<title>Joint decoding with multiple translation models.</title>
<date>2009</date>
<booktitle>In Proc. of ACL-IJCNLP.</booktitle>
<contexts>
<context position="5873" citStr="Liu et al., 2009" startWordPosition="861" endWordPosition="864">a dependency language model is incorporated. Carreras and Collins (2009) presented a string-todependency system that permits non-projective dependency trees (thereby allowing a larger space of translations) and use a rule extraction procedure that includes rules for every phrase in the phrase table. We take an additional step in this direction by working with dependency grammars on the phrases themselves, thereby bringing together the structural components of phrase-based and dependency-based MT in a single model. While others have worked on combining rules from multiple syntax-based systems (Liu et al., 2009) or using posteriors from multiple models to score translations (DeNero et al., 2010), we are not aware of any other work that seeks to directly integrate phrase-based and syntax-based machine translation at the modeling level.1 1Dymetman and Cancedda (2010) present a formal analy3 Model Given a sentence s and its dependency tree τs, we formulate the translation problem as finding the target sentence t*, the segmentation γ* of s into phrases, the segmentation φ* of t* into phrases, the dependency tree τ*φ on the target phrases φ*, and the one-to-one phrase alignment a* such that (t*, γ*, φ*, τ</context>
</contexts>
<marker>Liu, Mi, Feng, Liu, 2009</marker>
<rawString>Y. Liu, H. Mi, Y. Feng, and Q. Liu. 2009. Joint decoding with multiple translation models. In Proc. of ACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lopez</author>
</authors>
<title>Tera-scale translation models via pattern matching.</title>
<date>2008</date>
<booktitle>In Proc. of COLING.</booktitle>
<contexts>
<context position="30081" citStr="Lopez, 2008" startWordPosition="5050" endWordPosition="5051"> QPDG features described in §4. In our experiments, we compare our QPDG system (lattice parsing on each lattice) to the Moses baseline (finding the best path through each lattice). The conventional wisdom holds that hierarchical phrase-based translation (Chiang, 2005) performs better than phrasebased translation for language pairs that require large amounts of reordering, such as ZH-EN and UR-EN. However, researchers have shown that this performance gap diminishes when using a larger distortion limit (Zollmann et al., 2008) and may disappear entirely when using a lexicalized reordering model (Lopez, 2008; Galley and Manning, 2010). So, we increase the Moses distortion limit from 6 (the default) to 10 and use Moses’ default lexicalized reordering model (Koehn et al., 2005). We parsed the Chinese text using the Stanford parser (Levy and Manning, 2003) and the English text using TurboParser (Martins et al., 2009). We note that computing our features requires parsing the target (English) side of the parallel text, but not the source side. We only need to parse the source side of the tuning and test sets, and the only features that look at the source-side parse are those from §4.3. To obtain Brown</context>
</contexts>
<marker>Lopez, 2008</marker>
<rawString>A. Lopez. 2008. Tera-scale translation models via pattern matching. In Proc. of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Macherey</author>
<author>F Och</author>
<author>I Thayer</author>
<author>J Uszkoreit</author>
</authors>
<title>Lattice-based minimum error rate training for statistical machine translation.</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="21064" citStr="Macherey et al., 2008" startWordPosition="3514" endWordPosition="3517">tence s and its parse τs, i.e., finding the most probable derivation under the s/τs-specific grammar Gs,τ8. We follow Gimpel and Smith (2009) in constructing a lattice to represent Gs,τ8 and using lattice parsing to search for the best derivation, but we construct the lattice differently and employ a coarse-tofine strategy (Petrov, 2009) to speed up decoding. It has become common in recent years for MT researchers to exploit efficient data structures for encoding concise representations of the pruned search space of the model, such as phrase lattices for phrase-based MT (Ueffing et al., 2002; Macherey et al., 2008; Tromble et al., 2008). Each edge in a phrase lattice corresponds to a phrase pair and each path through the lattice corresponds to a tuple (t, -y, 0, a) for the input s. Decoding for a phrase lattice consists of finding the highest-scoring path, which is done using dynamic programming. To also maximize over τφ, we perform lattice dependency parsing, which allows us to search over the space of tuples (t, -y, 0, a, τφ). Given the lattice and Gs,τ8, lattice parsing is a straightforward generalization of the standard arc-factored dynamic programming algorithm from Eisner (1996). The lattice pars</context>
</contexts>
<marker>Macherey, Och, Thayer, Uszkoreit, 2008</marker>
<rawString>W. Macherey, F. Och, I. Thayer, and J. Uszkoreit. 2008. Lattice-based minimum error rate training for statistical machine translation. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="35460" citStr="Marcus et al., 1993" startWordPosition="5942" endWordPosition="5945">ntax-based MT offers a compelling platform for development and extrinsic evaluation of unsupervised parsers. In this paper, we use the standard dependency model with valence (DMV; Klein and Manning, 2004). When training is initialized using the output of a simpler, concave dependency model, the rectly translated and reordered, but the system was nonetheless able to use it to improve the fluency of the output. DMV can approach state-of-the-art unsupervised accuracy (Gimpel and Smith, 2011). For English, the resulting parser achieves 53.1% attachment accuracy on Section 23 of the Penn Treebank (Marcus et al., 1993), which approaches the 55.7% accuracy of a recent state-of-the-art unsupervised model (Blunsom and Cohn, 2010). The Chinese parser, initialized and trained the same way, achieves 44.4%, which is the highest reported accuracy on the Chinese Treebank (Xue et al., 2004) test set. Most unsupervised grammar induction models assume gold standard POS tags and sentences stripped of punctuation. We use the Stanford tagger (Toutanova et al., 2003) to obtain tags for both English and Chinese, parse the sentences without punctuation using the DMV, and then attach punctuation tokens to the root word of the</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19:313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A F T Martins</author>
<author>N A Smith</author>
<author>E P Xing</author>
</authors>
<title>Concise integer linear programming formulations for dependency parsing.</title>
<date>2009</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="23405" citStr="Martins et al., 2009" startWordPosition="3915" endWordPosition="3918">find the most liberal threshold that leaves fewer than 1000 edges in the resulting lattice. As complexity is quadratic in E, forcing E to be less than 1000 improves runtime substantially. After pruning, the lattices still contain more than 1016 paths on average and oracle BLEU scores are typically 12-15 points higher than the model-best paths. Pass 2: Parent Ranking Given a pruned lattice, we then remove some candidate dependency arcs from consideration. It is common in dependency parsing to use a coarse model to rank the top k parents for each word, and to only consider these during parsing (Martins et al., 2009; Bergsma and Cherry, 2010). Unlike string parsing, our phrase lattices impose several types of constraints on allowable arcs. For example, each node in the phrase lattice is annotated with a coverage vector—a bit vector indicating which words in the source sentence have been translated—which implies a topological ordering of the nodes. To handle constraints like these, we first use the Floyd-Warshall algorithm (Floyd, 1962) to find the best score between every pair of nodes in the lattice. This algorithm also tells us whether each edge is reachable from each other edge, allowing us to immedia</context>
<context position="30393" citStr="Martins et al., 2009" startWordPosition="5100" endWordPosition="5103">translation for language pairs that require large amounts of reordering, such as ZH-EN and UR-EN. However, researchers have shown that this performance gap diminishes when using a larger distortion limit (Zollmann et al., 2008) and may disappear entirely when using a lexicalized reordering model (Lopez, 2008; Galley and Manning, 2010). So, we increase the Moses distortion limit from 6 (the default) to 10 and use Moses’ default lexicalized reordering model (Koehn et al., 2005). We parsed the Chinese text using the Stanford parser (Levy and Manning, 2003) and the English text using TurboParser (Martins et al., 2009). We note that computing our features requires parsing the target (English) side of the parallel text, but not the source side. We only need to parse the source side of the tuning and test sets, and the only features that look at the source-side parse are those from §4.3. To obtain Brown clusters for the target tree features in §4.1, we used code from Liang (2005).8 We induced 100 clusters from the English side of the parallel corpus concatenated with 10M words of randomly-selected Gigaword sentences. Only words that appeared at least twice in this data were considered during clustering. An ad</context>
</contexts>
<marker>Martins, Smith, Xing, 2009</marker>
<rawString>A. F. T. Martins, N. A. Smith, and E. P. Xing. 2009. Concise integer linear programming formulations for dependency parsing. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M-J Nederhof</author>
</authors>
<title>Weighted deductive parsing and knuth’s algorithm.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="25558" citStr="Nederhof, 2003" startWordPosition="4290" endWordPosition="4291">iments, we set µ = 300, α = 100, and 0 = 400. 479 Input: tuning set D = hS, Ti, initial weights λ0 for coarse model, initial weights ψ0 for additional features in fine model Output: coarse model learned weights: λM, fine model learned weights: hλ∗, ψ∗i λM ← MERT (S, T, λ0, 100, MOSES); LMERT ← GenerateLattices (S, λM); LFB ← FBPrune (LMERT, λM); hλ∗, ψ∗i ← MERT (LFB, T, hλM, ψ0i, 200, QGDEPPARSE); return λM, hλ∗, ψ∗i; Algorithm 2: CoarseToFineTrain Pass 3: Lattice Dependency Parsing After completing the coarse passes, we parse using bottom-up dynamic programming based on the agenda algorithm (Nederhof, 2003; Eisner et al., 2005). We only consider arcs that survived the filtering in Pass 2. We weight agenda items by the sum of their scores and the Floyd-Warshall best path scores both from the start node of the lattice to the beginning of the item and the end of the item to any final node. This heuristic helps us to favor exploration of items that are highly likely under the phrase-based model. If the score of the partial structure can only get worse when combining it with other structures (e.g., in a PCFG), then the first time that we pop an item of type GOAL from the agenda, we are guaranteed to</context>
</contexts>
<marker>Nederhof, 2003</marker>
<rawString>M.-J. Nederhof. 2003. Weighted deductive parsing and knuth’s algorithm. Computational Linguistics, 29(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="6567" citStr="Och and Ney, 2002" startWordPosition="980" endWordPosition="983">l., 2010), we are not aware of any other work that seeks to directly integrate phrase-based and syntax-based machine translation at the modeling level.1 1Dymetman and Cancedda (2010) present a formal analy3 Model Given a sentence s and its dependency tree τs, we formulate the translation problem as finding the target sentence t*, the segmentation γ* of s into phrases, the segmentation φ* of t* into phrases, the dependency tree τ*φ on the target phrases φ*, and the one-to-one phrase alignment a* such that (t*, γ*, φ*, τ*φ, a*)= argmax p(t, γ, φ, τφ, a|s, τs) (t,γ,φ,τφ,a) We use a linear model (Och and Ney, 2002): p(t, γ,φ, τφ, a |s, τs) a exp{θTg(s, τs, t, γ, φ, τφ, a)} where g is a vector of arbitrary feature functions on the full set of structures and θ holds corresponding feature weights. Table 1 summarizes our notation. In modeling p(t, γ, φ, τφ, a |s, τs), we make use of quasi-synchronous grammar (QG; Smith and Eisner, 2006). Given a source sentence and its parse, a QG induces a probabilistic monolingual grammar over sentences “inspired” by the source sentence and tree. We denote this grammar by Gs,τs; its (weighted) language is the set of translations of s. Quasi-synchronous grammar makes no re</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>F. J. Och and H. Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="28501" citStr="Och and Ney, 2003" startWordPosition="4802" endWordPosition="4805">ation, we consider Chineseto-English (ZH-EN) and Urdu-to-English (UREN) translation and compare our system to Moses (Koehn et al., 2007). For ZH-EN, we used 303k sentence pairs from the FBIS corpus (LDC2003E14). We segmented the Chinese data using the Stanford Chinese segmenter in “CTB” mode (Chang et al., 2008), giving us 7.9M Chinese words and 9.4M English words. For UR-EN, we used parallel data from the NIST MT08 evaluation consisting of 1.2M Urdu words and 1.1M English words. We trained a baseline Moses system using default settings and features. Word alignment was performed using GIZA++ (Och and Ney, 2003) in both directions and the grow-diag-final-and heuristic was used to symmetrize the alignments. We used a max phrase length of 7 when extracting phrases. Trigram language models were estimated using the SRI language modeling toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998). To estimate language models for each language pair, we used the English side of the parallel corpus concatenated with 200M words of randomly-selected sentences from the Gigaword v4 corpus (excluding the NY Times and LA Times). We used this baseline Moses system to generate phrase lattices</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F. J. Och and H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>Minimum error rate training for statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="26859" citStr="Och, 2003" startWordPosition="4528" endWordPosition="4529">, making this property no longer hold; as a result, GOAL items may be popped out of order from the agenda. Therefore, we use an approximation, simply popping G GOAL items from the agenda and then stopping. The items are sorted by their scores and the best is returned by the decoder (or the k best in the case of MERT). In our experiments, we set G = 4000. The combined strategy yields average decoding times in the range of 30 seconds per sentence, which is comparable to other syntax-based MT systems. 6 Training For tuning the coarse and fine parameters, we use minimum error rate training (MERT; Och, 2003) in a procedure shown as Alg. 2. We first use MERT to train parameters for the coarse phrase-based model used to generate phrase lattices. Then, after generating the lattices, we prune them and run MERT a second time to tune parameters of the fine model, which includes all phrase-based and QPDG parameters. The arguments to MERT are a vector of source sentences (or lattices), a vector of target sentences, the initial parameter values, the size of the k-best list, and finally the decoder. We initialize λ to the default Moses feature weights and for ψ we initialize the two target phrase dependenc</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F. J. Och. 2003. Minimum error rate training for statistical machine translation. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W J Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2001</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="31886" citStr="Papineni et al., 2001" startWordPosition="5350" endWordPosition="5354">roximately 87M unique phrase dependencies were extracted from the ZH-EN data and 7M from the UR-EN data. We tuned the weights of our model using the pro8http://www.cs.berkeley.edu/˜pliang/ software Dev (tune) MT09 Moses 24.21 23.56 QPDG (TT+S2T) 24.94 (+0.73) 24.31 (+0.75) Table 5: Urdu-English Results (% BLEU). cedure described in §6. For ZH-EN we used MT03 for tuning and MT02, MT05, and MT06 for testing. For UR-EN we used half of the documents (882 sentence pairs) from the MT08 test set for tuning (“Dev”) and MT09 for testing. We evaluated translation output using case-insensitive IBM BLEU (Papineni et al., 2001). 7.1 Results Results for ZH-EN and UR-EN translation are shown in Tables 4 and 5. We show results when using only the target tree features from §4.1 (TT), as well as when adding the string-to-tree features from §4.2 (S2T) and the tree-to-tree features from §4.3 (T2T). We note that T2T features are unavailable for UR-EN because we do not have an Urdu parser. We find that we can achieve moderate but consistent improvements over the baseline Moses system, for an average increase of 0.65 BLEU points for ZH-EN and 0.75 for UR-EN. Fig. 2 shows an example sentence from the MT05 test set along with i</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2001. BLEU: a method for automatic evaluation of machine translation. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
</authors>
<title>Coarse-to-Fine Natural Language Processing.</title>
<date>2009</date>
<tech>Ph.D. thesis,</tech>
<institution>University of California at Berkeley.</institution>
<contexts>
<context position="20782" citStr="Petrov, 2009" startWordPosition="3468" endWordPosition="3469">h from a word in a(i) to a word in 7-0(a(i)), we use oc as the minimum length. There are 15 features in this category, for a total of 23 QPDG features. 478 5 Decoding For a QPDG model, decoding consists of finding the highest-scoring tuple (t, -y, 0, τφ, a) for an input sentence s and its parse τs, i.e., finding the most probable derivation under the s/τs-specific grammar Gs,τ8. We follow Gimpel and Smith (2009) in constructing a lattice to represent Gs,τ8 and using lattice parsing to search for the best derivation, but we construct the lattice differently and employ a coarse-tofine strategy (Petrov, 2009) to speed up decoding. It has become common in recent years for MT researchers to exploit efficient data structures for encoding concise representations of the pruned search space of the model, such as phrase lattices for phrase-based MT (Ueffing et al., 2002; Macherey et al., 2008; Tromble et al., 2008). Each edge in a phrase lattice corresponds to a phrase pair and each path through the lattice corresponds to a tuple (t, -y, 0, a) for the input s. Decoding for a phrase lattice consists of finding the highest-scoring path, which is done using dynamic programming. To also maximize over τφ, we </context>
</contexts>
<marker>Petrov, 2009</marker>
<rawString>S. Petrov. 2009. Coarse-to-Fine Natural Language Processing. Ph.D. thesis, University of California at Berkeley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Quirk</author>
<author>A Menezes</author>
<author>C Cherry</author>
</authors>
<title>Dependency treelet translation: Syntactically informed phrasal SMT.</title>
<date>2005</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="4584" citStr="Quirk et al. (2005)" startWordPosition="668" endWordPosition="671">k. 2 Related Work We previously applied quasi-synchronous grammar to machine translation (Gimpel and Smith, 2009), but that system performed translation fundamentally at the word level. Here we generalize that model to function on phrases, enabling a tighter coupling between the phrase segmentation and syntactic structures. We also present a decoder efficient enough to scale to large data sets and present performance improvements in large-scale experiments over a stateof-the-art phrase-based baseline. Aside from QG, there have been many efforts to use dependency syntax in machine translation. Quirk et al. (2005) used a source-side dependency parser and projected automatic parses across word alignments in order to model dependency syntax on phrase pairs. Shen et al. (2008) presented an extension to Hiero (Chiang, 2005) in which rules have target-side dependency syntax and therefore enable the use of a dependency language model. More recently, researchers have sought the benefits of dependency syntax while preserving the advantages of phrase-based models, such as efficiency and coverage. Galley and Manning (2009) loosened standard assumptions about dependency parsing so that the efficient left-to-right</context>
<context position="39783" citStr="Quirk et al., 2005" startWordPosition="6635" endWordPosition="6638">LEU score differences between the two systems, the unigram precisions were typically equal or only slightly different, while precisions for higher-order n-grams contained the bulk of the improvement. This suggests that our system is not finding substantially better translations for individual words in the input, but rather is focused on reordering the existing translations. This is not surprising given our choice of features, which focus on syntactic language modeling and syntax-based reordering. The obvious next step for our framework is to include bilingual rules that include source syntax (Quirk et al., 2005), target syntax (Shen et al., 2008), and syntax on both sides. Our framework allows integrating together all of these and other types of structures, with the ultimate goal of combining the strengths of multiple approaches to translation in a single model. Acknowledgments We thank Chris Dyer and the anonymous reviewers for helpful comments that improved this paper. This research was supported in part by the NSF through grant IIS0844507, the U. S. Army Research Laboratory and the U. S. Army Research Office under contract/grant number W911NF-10-1-0533, and Sandia National Laboratories (fellowship</context>
</contexts>
<marker>Quirk, Menezes, Cherry, 2005</marker>
<rawString>C. Quirk, A. Menezes, and C. Cherry. 2005. Dependency treelet translation: Syntactically informed phrasal SMT. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Shen</author>
<author>J Xu</author>
<author>R Weischedel</author>
</authors>
<title>A new stringto-dependency machine translation algorithm with a target dependency language model.</title>
<date>2008</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="4747" citStr="Shen et al. (2008)" startWordPosition="693" endWordPosition="696">ly at the word level. Here we generalize that model to function on phrases, enabling a tighter coupling between the phrase segmentation and syntactic structures. We also present a decoder efficient enough to scale to large data sets and present performance improvements in large-scale experiments over a stateof-the-art phrase-based baseline. Aside from QG, there have been many efforts to use dependency syntax in machine translation. Quirk et al. (2005) used a source-side dependency parser and projected automatic parses across word alignments in order to model dependency syntax on phrase pairs. Shen et al. (2008) presented an extension to Hiero (Chiang, 2005) in which rules have target-side dependency syntax and therefore enable the use of a dependency language model. More recently, researchers have sought the benefits of dependency syntax while preserving the advantages of phrase-based models, such as efficiency and coverage. Galley and Manning (2009) loosened standard assumptions about dependency parsing so that the efficient left-to-right decoding procedure of phrase-based translation could be retained while a dependency language model is incorporated. Carreras and Collins (2009) presented a string</context>
<context position="11716" citStr="Shen et al., 2008" startWordPosition="1922" endWordPosition="1925">root of the tree. in our model that are used to score phrase dependency trees. We shall refer to these as QPDG features and will find it useful later to notationally distinguish their feature weights from those of the phrase-based model. We use λ for weights of the standard phrase-based model features and ψ for weights of the QPDG features. We include three categories of features, differentiated by what pieces of structure they consider. 4.1 Target Tree Features We first include features that only consider t, φ, and τφ. These features can be categorized as “syntactic language model” features (Shen et al., 2008; Galley and Manning, 2009), though unlike previous work our features model both the phrase segmentation and dependency structure. Typically, these sorts of features are probabilities estimated from a corpus parsed using a supervised parser. However, there do not currently exist treebanks with annotated phrase 476 , made up 0.057 he made up 0.021 supreme court made up 0.014 court made up 0.014 in september 2000 made up 0.014 in september 2000 , made up 0.014 made up of 0.065 made up . 0.029 made up , 0.016 made up mind to 0.01 Table 3: Most probable child phrases for the parent phrase “made up</context>
<context position="39818" citStr="Shen et al., 2008" startWordPosition="6641" endWordPosition="6644">o systems, the unigram precisions were typically equal or only slightly different, while precisions for higher-order n-grams contained the bulk of the improvement. This suggests that our system is not finding substantially better translations for individual words in the input, but rather is focused on reordering the existing translations. This is not surprising given our choice of features, which focus on syntactic language modeling and syntax-based reordering. The obvious next step for our framework is to include bilingual rules that include source syntax (Quirk et al., 2005), target syntax (Shen et al., 2008), and syntax on both sides. Our framework allows integrating together all of these and other types of structures, with the ultimate goal of combining the strengths of multiple approaches to translation in a single model. Acknowledgments We thank Chris Dyer and the anonymous reviewers for helpful comments that improved this paper. This research was supported in part by the NSF through grant IIS0844507, the U. S. Army Research Laboratory and the U. S. Army Research Office under contract/grant number W911NF-10-1-0533, and Sandia National Laboratories (fellowship to K. Gimpel). ZH unsupervised sup</context>
</contexts>
<marker>Shen, Xu, Weischedel, 2008</marker>
<rawString>L. Shen, J. Xu, and R. Weischedel. 2008. A new stringto-dependency machine translation algorithm with a target dependency language model. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Sixtus</author>
<author>S Ortmanns</author>
</authors>
<title>High quality word graphs using forward-backward pruning.</title>
<date>1999</date>
<booktitle>In Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal Processing.</booktitle>
<contexts>
<context position="22271" citStr="Sixtus and Ortmanns, 1999" startWordPosition="3716" endWordPosition="3720">). The lattice parsing algorithm requires O(E2V ) time and O(E2 + V E) space, where E is the number of edges in the lattice and V is the number of nodes.7 Typical phrase lattices might easily contain tens of thousands of nodes and edges, making exact search prohibitively expensive for all but the smallest lattices. So, we use approximate search based on coarse-to-fine decoding. We now discuss each step of this procedure; an outline is shown as Alg. 1. Pass 1: Lattice Pruning After generating phrase lattices using a phrase-based MT system, we prune lattice edges using forward-backward pruning (Sixtus and Ortmanns, 1999), which has also been used in previous work using phrase lattices (Tromble et al., 2008). This pruning method computes the maxmarginal for each lattice edge, which is the score of the best full path that uses that edge. Max-marginals 7To prevent confusion, we use the term edge to refer to a phrase lattice edge and arc to refer to a parent-child dependency in the phrase dependency tree. offer the advantage that the best path in the lattice is preserved during pruning. For each lattice, we use a grid search to find the most liberal threshold that leaves fewer than 1000 edges in the resulting lat</context>
</contexts>
<marker>Sixtus, Ortmanns, 1999</marker>
<rawString>A. Sixtus and S. Ortmanns. 1999. High quality word graphs using forward-backward pruning. In Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Smith</author>
<author>J Eisner</author>
</authors>
<title>Quasi-synchronous grammars: Alignment by soft projection of syntactic dependencies.</title>
<date>2006</date>
<booktitle>In Proc. of HLT-NAACL Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="2602" citStr="Smith and Eisner, 2006" startWordPosition="377" endWordPosition="380"> as vertices. We describe a simple heuristic to extract phrase dependencies from an aligned parallel corpus parsed on the target side, and use them to compute target-side tree features. We define additional string-to-tree features and, if a source-side dependency parser is available, tree-to-tree features to capture properties of how phrase dependencies interact with reordering. To leverage standard phrase-based features alongside our novel features, we require a formalism that supports flexible feature combination and efficient decoding. Quasi-synchronous grammar (QG) provides this backbone (Smith and Eisner, 2006); we describe a coarse-to-fine approach for decoding within this framework, advancing substantially over earlier QG machine translation systems (Gimpel and Smith, 2009). The decoder involves generating a phrase lattice (Ueffing et al., 2002) in a coarse pass using a phrase-based model, followed by lattice dependency parsing of the phrase lattice. This approach allows us to feasibly explore the combined search space of segmentations, phrase alignments, and target phrase dependency trees. Our experiments demonstrate an average improvement of +0.65 BLEU in Chinese-English translation across three</context>
<context position="6891" citStr="Smith and Eisner, 2006" startWordPosition="1040" endWordPosition="1043">sentence t*, the segmentation γ* of s into phrases, the segmentation φ* of t* into phrases, the dependency tree τ*φ on the target phrases φ*, and the one-to-one phrase alignment a* such that (t*, γ*, φ*, τ*φ, a*)= argmax p(t, γ, φ, τφ, a|s, τs) (t,γ,φ,τφ,a) We use a linear model (Och and Ney, 2002): p(t, γ,φ, τφ, a |s, τs) a exp{θTg(s, τs, t, γ, φ, τφ, a)} where g is a vector of arbitrary feature functions on the full set of structures and θ holds corresponding feature weights. Table 1 summarizes our notation. In modeling p(t, γ, φ, τφ, a |s, τs), we make use of quasi-synchronous grammar (QG; Smith and Eisner, 2006). Given a source sentence and its parse, a QG induces a probabilistic monolingual grammar over sentences “inspired” by the source sentence and tree. We denote this grammar by Gs,τs; its (weighted) language is the set of translations of s. Quasi-synchronous grammar makes no restrictions on the form of the target monolingual grammar, though dependency grammars have been used in most previous applications of QG (Wang et al., 2007; Das and Smith, 2009; Smith and Eisner, 2009), including previous work in MT (Smith and Eisner, 2006; Gimpel and Smith, 2009). We previously presented a word-based machi</context>
<context position="17710" citStr="Smith and Eisner (2006)" startWordPosition="2952" endWordPosition="2955">ide structures, these features consider -y and a, though not s or 7-8. For example, when building a parent-child phrase dependency with the child to the left, one feature value is incremented if their aligned source-side phrases are in the same order. This configuration is the leftmost in Fig. 1; we include features for the other three configurations there as well, for a total of 4 features in this category. 4.3 Tree-to-Tree Configurations We include features that consider s, -y, and 7-8 in addition to t, φ, and 7-0. We begin with features for each of the quasi-synchronous configurations from Smith and Eisner (2006), adapted to phrase dependency grammars. That is, for a parent-child pair (7-0(i), i) in 7-0, we consider the relationship between a(7-0(i)) and a(i), the source-side phrases to which 7-0(i) and i align. We use the following named configurations from Smith and Eisner: root-root, parent-child, child-parent, grandparentgrandchild, sibling, and c-command.5 We define a feature to count instances of each of these configurations, including an additional feature for “other” configurations that do not fit into these categories.6 When using a QPDG, there are multiple ways to compute tree-to-tree config</context>
</contexts>
<marker>Smith, Eisner, 2006</marker>
<rawString>D. A. Smith and J. Eisner. 2006. Quasi-synchronous grammars: Alignment by soft projection of syntactic dependencies. In Proc. of HLT-NAACL Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Smith</author>
<author>J Eisner</author>
</authors>
<title>Parser adaptation and projection with quasi-synchronous features.</title>
<date>2009</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="7367" citStr="Smith and Eisner, 2009" startWordPosition="1118" endWordPosition="1121">ights. Table 1 summarizes our notation. In modeling p(t, γ, φ, τφ, a |s, τs), we make use of quasi-synchronous grammar (QG; Smith and Eisner, 2006). Given a source sentence and its parse, a QG induces a probabilistic monolingual grammar over sentences “inspired” by the source sentence and tree. We denote this grammar by Gs,τs; its (weighted) language is the set of translations of s. Quasi-synchronous grammar makes no restrictions on the form of the target monolingual grammar, though dependency grammars have been used in most previous applications of QG (Wang et al., 2007; Das and Smith, 2009; Smith and Eisner, 2009), including previous work in MT (Smith and Eisner, 2006; Gimpel and Smith, 2009). We previously presented a word-based machine translation model based on a quasi-synchronous dependency grammar. However, it is well-known in the MT community that translation quality is improved when larger units are modeled. Therefore, we use a dependency grammar in which the leaves are phrases rather than words. We define a phrase dependency grammar as a model p(φ,τφ|t) over the joint space of segmentations of a sentence into phrases and dependency trees on the phrases.2 Phrase dependency grammars sis of the pr</context>
</contexts>
<marker>Smith, Eisner, 2009</marker>
<rawString>D. A. Smith and J. Eisner. 2009. Parser adaptation and projection with quasi-synchronous features. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM—an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proc. of ICSLP.</booktitle>
<contexts>
<context position="28750" citStr="Stolcke, 2002" startWordPosition="4841" endWordPosition="4842">tanford Chinese segmenter in “CTB” mode (Chang et al., 2008), giving us 7.9M Chinese words and 9.4M English words. For UR-EN, we used parallel data from the NIST MT08 evaluation consisting of 1.2M Urdu words and 1.1M English words. We trained a baseline Moses system using default settings and features. Word alignment was performed using GIZA++ (Och and Ney, 2003) in both directions and the grow-diag-final-and heuristic was used to symmetrize the alignments. We used a max phrase length of 7 when extracting phrases. Trigram language models were estimated using the SRI language modeling toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998). To estimate language models for each language pair, we used the English side of the parallel corpus concatenated with 200M words of randomly-selected sentences from the Gigaword v4 corpus (excluding the NY Times and LA Times). We used this baseline Moses system to generate phrase lattices for our system, so our model includes all of the Moses features in addition to the 480 MT03 (tune) MT02 MT05 MT06 Average Moses 33.84 33.35 31.81 28.82 31.33 QPDG (TT) 34.63 (+0.79) 34.10 (+0.75) 32.15 (+0.34) 29.33 (+0.51) 31.86 (+0.53) QPDG (TT+S</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. SRILM—an extensible language modeling toolkit. In Proc. of ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>D Klein</author>
<author>C D Manning</author>
<author>Y Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network. In</title>
<date>2003</date>
<booktitle>Proc. of HLT-NAACL.</booktitle>
<contexts>
<context position="35901" citStr="Toutanova et al., 2003" startWordPosition="6012" endWordPosition="6015">-the-art unsupervised accuracy (Gimpel and Smith, 2011). For English, the resulting parser achieves 53.1% attachment accuracy on Section 23 of the Penn Treebank (Marcus et al., 1993), which approaches the 55.7% accuracy of a recent state-of-the-art unsupervised model (Blunsom and Cohn, 2010). The Chinese parser, initialized and trained the same way, achieves 44.4%, which is the highest reported accuracy on the Chinese Treebank (Xue et al., 2004) test set. Most unsupervised grammar induction models assume gold standard POS tags and sentences stripped of punctuation. We use the Stanford tagger (Toutanova et al., 2003) to obtain tags for both English and Chinese, parse the sentences without punctuation using the DMV, and then attach punctuation tokens to the root word of the tree in a postprocessing step. For English, the predicted parents agreed with those of TurboParser for 48.7% of the tokens in the corpus. We considered all four scenarios: supervised and unsupervised English parsing paired with supervised and unsupervised Chinese parsing. Table 6 shows 482 EN unsupervised supervised 31.18 (33.76) 31.86 (34.78) 32.12 (34.74) 31.98 (34.98) 31.33 (33.84) Table 6: Results when using unsupervised dependency </context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>K. Toutanova, D. Klein, C. D. Manning, and Y. Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In Proc. of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Tromble</author>
<author>S Kumar</author>
<author>F Och</author>
<author>W Macherey</author>
</authors>
<title>Lattice Minimum Bayes-Risk decoding for statistical machine translation.</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="21087" citStr="Tromble et al., 2008" startWordPosition="3518" endWordPosition="3521">s, i.e., finding the most probable derivation under the s/τs-specific grammar Gs,τ8. We follow Gimpel and Smith (2009) in constructing a lattice to represent Gs,τ8 and using lattice parsing to search for the best derivation, but we construct the lattice differently and employ a coarse-tofine strategy (Petrov, 2009) to speed up decoding. It has become common in recent years for MT researchers to exploit efficient data structures for encoding concise representations of the pruned search space of the model, such as phrase lattices for phrase-based MT (Ueffing et al., 2002; Macherey et al., 2008; Tromble et al., 2008). Each edge in a phrase lattice corresponds to a phrase pair and each path through the lattice corresponds to a tuple (t, -y, 0, a) for the input s. Decoding for a phrase lattice consists of finding the highest-scoring path, which is done using dynamic programming. To also maximize over τφ, we perform lattice dependency parsing, which allows us to search over the space of tuples (t, -y, 0, a, τφ). Given the lattice and Gs,τ8, lattice parsing is a straightforward generalization of the standard arc-factored dynamic programming algorithm from Eisner (1996). The lattice parsing algorithm requires </context>
<context position="22359" citStr="Tromble et al., 2008" startWordPosition="3732" endWordPosition="3735">number of edges in the lattice and V is the number of nodes.7 Typical phrase lattices might easily contain tens of thousands of nodes and edges, making exact search prohibitively expensive for all but the smallest lattices. So, we use approximate search based on coarse-to-fine decoding. We now discuss each step of this procedure; an outline is shown as Alg. 1. Pass 1: Lattice Pruning After generating phrase lattices using a phrase-based MT system, we prune lattice edges using forward-backward pruning (Sixtus and Ortmanns, 1999), which has also been used in previous work using phrase lattices (Tromble et al., 2008). This pruning method computes the maxmarginal for each lattice edge, which is the score of the best full path that uses that edge. Max-marginals 7To prevent confusion, we use the term edge to refer to a phrase lattice edge and arc to refer to a parent-child dependency in the phrase dependency tree. offer the advantage that the best path in the lattice is preserved during pruning. For each lattice, we use a grid search to find the most liberal threshold that leaves fewer than 1000 edges in the resulting lattice. As complexity is quadratic in E, forcing E to be less than 1000 improves runtime s</context>
</contexts>
<marker>Tromble, Kumar, Och, Macherey, 2008</marker>
<rawString>R. Tromble, S. Kumar, F. Och, and W. Macherey. 2008. Lattice Minimum Bayes-Risk decoding for statistical machine translation. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ueffing</author>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Generation of word graphs in statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="2843" citStr="Ueffing et al., 2002" startWordPosition="413" endWordPosition="416">urce-side dependency parser is available, tree-to-tree features to capture properties of how phrase dependencies interact with reordering. To leverage standard phrase-based features alongside our novel features, we require a formalism that supports flexible feature combination and efficient decoding. Quasi-synchronous grammar (QG) provides this backbone (Smith and Eisner, 2006); we describe a coarse-to-fine approach for decoding within this framework, advancing substantially over earlier QG machine translation systems (Gimpel and Smith, 2009). The decoder involves generating a phrase lattice (Ueffing et al., 2002) in a coarse pass using a phrase-based model, followed by lattice dependency parsing of the phrase lattice. This approach allows us to feasibly explore the combined search space of segmentations, phrase alignments, and target phrase dependency trees. Our experiments demonstrate an average improvement of +0.65 BLEU in Chinese-English translation across three test sets and an improvement of +0.75 BLEU in Urdu-English translation over a phrase-based baseline. We also describe experiments in which we replace supervised dependency parsers with unsupervised parsers, reporting promising results: usin</context>
<context position="21041" citStr="Ueffing et al., 2002" startWordPosition="3510" endWordPosition="3513">φ, a) for an input sentence s and its parse τs, i.e., finding the most probable derivation under the s/τs-specific grammar Gs,τ8. We follow Gimpel and Smith (2009) in constructing a lattice to represent Gs,τ8 and using lattice parsing to search for the best derivation, but we construct the lattice differently and employ a coarse-tofine strategy (Petrov, 2009) to speed up decoding. It has become common in recent years for MT researchers to exploit efficient data structures for encoding concise representations of the pruned search space of the model, such as phrase lattices for phrase-based MT (Ueffing et al., 2002; Macherey et al., 2008; Tromble et al., 2008). Each edge in a phrase lattice corresponds to a phrase pair and each path through the lattice corresponds to a tuple (t, -y, 0, a) for the input s. Decoding for a phrase lattice consists of finding the highest-scoring path, which is done using dynamic programming. To also maximize over τφ, we perform lattice dependency parsing, which allows us to search over the space of tuples (t, -y, 0, a, τφ). Given the lattice and Gs,τ8, lattice parsing is a straightforward generalization of the standard arc-factored dynamic programming algorithm from Eisner (</context>
</contexts>
<marker>Ueffing, Och, Ney, 2002</marker>
<rawString>N. Ueffing, F. J. Och, and H. Ney. 2002. Generation of word graphs in statistical machine translation. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Wang</author>
<author>N A Smith</author>
<author>T Mitamura</author>
</authors>
<title>What is the Jeopardy model? a quasi-synchronous grammar for QA.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="7321" citStr="Wang et al., 2007" startWordPosition="1110" endWordPosition="1113">res and θ holds corresponding feature weights. Table 1 summarizes our notation. In modeling p(t, γ, φ, τφ, a |s, τs), we make use of quasi-synchronous grammar (QG; Smith and Eisner, 2006). Given a source sentence and its parse, a QG induces a probabilistic monolingual grammar over sentences “inspired” by the source sentence and tree. We denote this grammar by Gs,τs; its (weighted) language is the set of translations of s. Quasi-synchronous grammar makes no restrictions on the form of the target monolingual grammar, though dependency grammars have been used in most previous applications of QG (Wang et al., 2007; Das and Smith, 2009; Smith and Eisner, 2009), including previous work in MT (Smith and Eisner, 2006; Gimpel and Smith, 2009). We previously presented a word-based machine translation model based on a quasi-synchronous dependency grammar. However, it is well-known in the MT community that translation quality is improved when larger units are modeled. Therefore, we use a dependency grammar in which the leaves are phrases rather than words. We define a phrase dependency grammar as a model p(φ,τφ|t) over the joint space of segmentations of a sentence into phrases and dependency trees on the phra</context>
</contexts>
<marker>Wang, Smith, Mitamura, 2007</marker>
<rawString>M. Wang, N. A. Smith, and T. Mitamura. 2007. What is the Jeopardy model? a quasi-synchronous grammar for QA. In Proc. of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wu</author>
<author>Q Zhang</author>
<author>X Huang</author>
<author>L Wu</author>
</authors>
<title>Phrase dependency parsing for opinion mining.</title>
<date>2009</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="8997" citStr="Wu et al. (2009)" startWordPosition="1432" endWordPosition="1435">i = hsj, ... , ski s.t. γ1 · ... φ = hφ1, ... , φm&apos;i · φm&apos; = t segmentation of t into phrases ∀i, φi = htj, ... , tki s.t. φ1 · ... τ8 : {1, ... , n} → {0, ... , n} dependency tree on source words s, where τ8(i) is the index of the parent of word si (0 is the root, $) τo : {1, ... , m&apos;} → {0, ... , m&apos;} dependency tree on target phrases φ, where τo(i) is the index of the parent of phrase φi a : {1,.. . , m&apos;} → {1,. .. , n&apos;} one-to-one alignment from phrases in φ to phrases in γ θ = hλ, ψi parameters of the full model (λ = phrase-based, ψ = QPDG) able 1: Key notation. have recently been used by Wu et al. (2009) for feature extraction for opinion mining. When used for translation modeling, they allow us to capture phenomena like local reordering and idiomatic translations within each phrase as well as long-distance relationships among the phrases in a sentence. We then define a quasi-synchronous phrase dependency grammar (QPDG) as a conditional model p(t, γ, φ, τφ, a |s, τ8) that induces a probabilistic monolingual phrase dependency grammar over sentences inspired by the source sentence and (lexical) dependency tree. The source and target sentences are segmented into phrases and the phrases are align</context>
<context position="13959" citStr="Wu et al. (2009)" startWordPosition="2301" endWordPosition="2304">que. Table 2 shows a listing of the most frequent phrase dependencies extracted (lexical dependencies are omitted). We note that during training we never explicitly commit to any single phrase dependency tree for a target sentence. Rather, we extract phrase dependencies from all phrase dependency trees consistent with the word alignments and the lexical dependency tree. Thus we treat phrase dependency trees analogously to phrase segmentations in standard phrase extraction. We perform this procedure on all sentence pairs in the parallel corpus. Given a set of extracted 3For a monolingual task, Wu et al. (2009) used a shallow parser to convert lexical dependencies from a dependency parser into phrase dependencies. phrase dependencies of the form (u, v, d), where u is the head phrase, v is the child phrase, and d E {left, right} is the direction, we then estimate conditional probabilities p(v|u, d) using relative frequency estimation. Table 3 shows the most probable child phrases for an example parent phrase. To combat data sparseness, we perform the same procedure with each word replaced by its word cluster ID obtained from Brown clustering (Brown et al., 1992). We include a feature in the model for</context>
</contexts>
<marker>Wu, Zhang, Huang, Wu, 2009</marker>
<rawString>Y. Wu, Q. Zhang, X. Huang, and L. Wu. 2009. Phrase dependency parsing for opinion mining. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Xue</author>
<author>F Xia</author>
<author>F-D Chiou</author>
<author>M Palmer</author>
</authors>
<title>The Penn Chinese Treebank: Phrase structure annotation of a large corpus.</title>
<date>2004</date>
<journal>Natural Language Engineering,</journal>
<volume>10</volume>
<issue>4</issue>
<contexts>
<context position="35727" citStr="Xue et al., 2004" startWordPosition="5985" endWordPosition="5988">ave dependency model, the rectly translated and reordered, but the system was nonetheless able to use it to improve the fluency of the output. DMV can approach state-of-the-art unsupervised accuracy (Gimpel and Smith, 2011). For English, the resulting parser achieves 53.1% attachment accuracy on Section 23 of the Penn Treebank (Marcus et al., 1993), which approaches the 55.7% accuracy of a recent state-of-the-art unsupervised model (Blunsom and Cohn, 2010). The Chinese parser, initialized and trained the same way, achieves 44.4%, which is the highest reported accuracy on the Chinese Treebank (Xue et al., 2004) test set. Most unsupervised grammar induction models assume gold standard POS tags and sentences stripped of punctuation. We use the Stanford tagger (Toutanova et al., 2003) to obtain tags for both English and Chinese, parse the sentences without punctuation using the DMV, and then attach punctuation tokens to the root word of the tree in a postprocessing step. For English, the predicted parents agreed with those of TurboParser for 48.7% of the tokens in the corpus. We considered all four scenarios: supervised and unsupervised English parsing paired with supervised and unsupervised Chinese pa</context>
</contexts>
<marker>Xue, Xia, Chiou, Palmer, 2004</marker>
<rawString>N. Xue, F. Xia, F.-D. Chiou, and M. Palmer. 2004. The Penn Chinese Treebank: Phrase structure annotation of a large corpus. Natural Language Engineering, 10(4):1–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Zollmann</author>
<author>A Venugopal</author>
</authors>
<title>Syntax augmented machine translation via chart parsing.</title>
<date>2006</date>
<booktitle>In Proc. of NAACL 2006 Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="1330" citStr="Zollmann and Venugopal, 2006" startWordPosition="182" endWordPosition="185">ach based on lattice dependency parsing of phrase lattices. We demonstrate performance improvements for Chinese-English and UrduEnglish translation over a phrase-based baseline. We also investigate the use of unsupervised dependency parsers, reporting encouraging preliminary results. 1 Introduction Two approaches currently dominate statistical machine translation (MT) research. Phrase-based models (Koehn et al., 2003) excel at capturing local reordering phenomena and memorizing multi-word translations. Models that employ syntax or syntaxlike representations (Chiang, 2005; Galley et al., 2006; Zollmann and Venugopal, 2006; Huang et al., 2006) handle long-distance reordering better than phrase-based systems (Auli et al., 2009) but often require constraints on the formalism or rule extraction method in order to achieve computational tractability. As a result, certain instances of syntactic divergence are more naturally handled by phrase-based systems (DeNeefe et al., 2007). In this paper we present a new way of combining the advantages of phrase-based and syntax-based MT. We propose a model in which phrases are organized into a tree structure inspired by dependency syntax. Instead of standard dependency trees in</context>
</contexts>
<marker>Zollmann, Venugopal, 2006</marker>
<rawString>A. Zollmann and A. Venugopal. 2006. Syntax augmented machine translation via chart parsing. In Proc. of NAACL 2006 Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Zollmann</author>
<author>A Venugopal</author>
<author>F J Och</author>
<author>J Ponte</author>
</authors>
<title>A systematic comparison of phrase-based, hierarchical and syntax-augmented statistical MT.</title>
<date>2008</date>
<booktitle>In Proc. of COLING.</booktitle>
<contexts>
<context position="29999" citStr="Zollmann et al., 2008" startWordPosition="5035" endWordPosition="5038"> (+0.91) 32.34 (+0.53) 29.35 (+0.53) 31.98 (+0.65) Table 4: Chinese-English Results (% BLEU). QPDG features described in §4. In our experiments, we compare our QPDG system (lattice parsing on each lattice) to the Moses baseline (finding the best path through each lattice). The conventional wisdom holds that hierarchical phrase-based translation (Chiang, 2005) performs better than phrasebased translation for language pairs that require large amounts of reordering, such as ZH-EN and UR-EN. However, researchers have shown that this performance gap diminishes when using a larger distortion limit (Zollmann et al., 2008) and may disappear entirely when using a lexicalized reordering model (Lopez, 2008; Galley and Manning, 2010). So, we increase the Moses distortion limit from 6 (the default) to 10 and use Moses’ default lexicalized reordering model (Koehn et al., 2005). We parsed the Chinese text using the Stanford parser (Levy and Manning, 2003) and the English text using TurboParser (Martins et al., 2009). We note that computing our features requires parsing the target (English) side of the parallel text, but not the source side. We only need to parse the source side of the tuning and test sets, and the onl</context>
</contexts>
<marker>Zollmann, Venugopal, Och, Ponte, 2008</marker>
<rawString>A. Zollmann, A. Venugopal, F. J. Och, and J. Ponte. 2008. A systematic comparison of phrase-based, hierarchical and syntax-augmented statistical MT. In Proc. of COLING.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>