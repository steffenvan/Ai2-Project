<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.087339">
<title confidence="0.994077">
A machine-learning approach to the identification of WH gaps
</title>
<author confidence="0.958502">
Derrick Higgins
</author>
<affiliation confidence="0.795156">
Educational Testing Service
</affiliation>
<email confidence="0.987433">
dchiggin@alumni.uchicago.edu
</email>
<sectionHeader confidence="0.997273" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999750857142857">
In this paper, we pursue a multi-
modular, statistical approach to WH de-
pendencies, using a feedforward net-
work as our modeling tool. The empiri-
cal basis of this model and the availabil-
ity of performance measures for our sys-
tem address deficiencies in earlier com-
putational work on WH gaps, which re-
quire richer sources of semantic and lex-
ical information in order to run. The
statistical nature of our models allows
them to be simply combined with other
modules of grammar, such as a syntactic
parser.
</bodyText>
<sectionHeader confidence="0.997544" genericHeader="keywords">
1 Overview
</sectionHeader>
<bodyText confidence="0.999958170212766">
This paper concerns the phenomenon of WH de-
pendencies, a subclass of unbounded dependen-
cies (also known as A dependencies or filler-gap
structures). WH dependencies are structures in
which a constituent headed by a WH word (such
as &amp;quot;who&amp;quot; or &amp;quot;where&amp;quot;) is found somewhere other
than where it belongs for semantic interpretation
and subcategorization.
A dependencies have played an important role
in syntactic theory, but discovering the location of
a gap corresponding to a WH phrase found in the
syntactic representation of a sentence is also of in-
terest for computational applications. Identifica-
tion of the syntactic gap may be necessary for in-
terpretation of the sentence, and could contribute
to a natural language understanding or machine
translation application. Since WH dependencies
also tend to distort the surface subcategorization
properties of verbs, identifying gaps could also aid
in automatic lexical acquisition techniques. Many
other applications are imaginable as well, using
the gap location to inform intonation, semantics,
collocation frequency, etc.
The contribution of this paper consists in the de-
velopment of a machine-learning approach to the
identification of WH gaps. This approach reduces
the lexical prerequisites for this task, while main-
taining a high degree of accuracy. In addition, the
modular treatment of WH dependencies allows the
model to be easily incorporated with many differ-
ent models of surface syntax. In ongoing work, we
are investigating ways in which our model may be
combined with a syntactic parser.
The idea that the task of associating a WH ele-
ment with its gap can be done in an independent
module of grammar is not only interesting for rea-
sons of computational efficacy. The fact of un-
bounded dependencies has played a central role in
the development of linguistic theory as well. It
has been used as an argument for the necessity of
transformations (Chomsky, 1957), and prompted
the introduction of powerful mechanisms such as
the SLASH feature of GPSG (Gazdar et al., 1985).
To the extent that these phenomena can be de-
scribed in an independent module of grammar, our
theory of the syntax of natural language can be ac-
cordingly simplified.
</bodyText>
<sectionHeader confidence="0.997166" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.999819714285714">
In theoretical linguistics, WH dependencies have
typically been dealt with as part of the syntax (but
cf. Kuno, Takami &amp; Wu (1999) for an alterna-
tive approach). Early generative treatments used a
WH-movement transformation (McCawley, 1998)
to describe the relationship between a WH phrase
and its gap, while later work in the Government &amp;
</bodyText>
<page confidence="0.997679">
99
</page>
<bodyText confidence="0.99997406">
Binding framework subsumes this transformation
under the general heading of overt A Movement
(Huang, 1995; Aoun and Li, 1993). Feature-based
syntactic formalisms such as GPSG use feature-
percolation mechanisms to transfer information
from the location in which a WH phrase is sub-
categorized for (the gap), to the location where it
is realized (the filler position) (Gazdar et al., 1985;
Pollard and Sag, 1994).
Most work in computational linguistics has fol-
lowed these theoretical approaches to WH depen-
dencies very closely. Berwick &amp; Fong (1995) im-
plement a transformational account of WH gaps in
their Government &amp; Binding parser, although the
grammatical coverage of their system is extremely
limited. The SRI Core Language Engine (Al-
shawi, 1992) incorporates a feature-based account
of WH gaps known as &amp;quot;gap-threading&amp;quot;, which is
essentially identical to the feature-passing mecha-
nisms used in GPSG. Both systems require an ex-
tensive lexical database of valency information in
order to identify potential gap locations.
While there are no published results regarding
the accuracy of these methods in correctly asso-
ciating WH phrases and their gaps, we feel that
these methods can be improved upon by adopting
a corpus-based approach. First, deriving general-
izations about the distribution of WH phrases di-
rectly from corpus data addresses the problem that
the data may not conform to our theoretical pre-
conceptions. Second, we hope to show that much
of the work of identifying WH dependencies can
be done without access to the subcategorization
frame of every verb and preposition in the corpus,
which is a prerequisite for the methods mentioned
above.
The only previous work we are aware of which
addresses the task of identification of WH gaps
from a statistical perspective is Collins (1999),
which employs a lexicalized PCFG augmented
with &amp;quot;gap features&amp;quot;. Unfortunately, our results
are not directly comparable to those reported by
Collins, because his model of WH dependencies is
integrated with a syntactic parser, so that his sys-
tem is responsible for producing syntactic phrase
structure trees as well as gap locations. Since our
model takes these trees as given, it identifies the
correct gap location more consistently. Integration
of our model of WH dependencies with a parser is
a goal of future development.
</bodyText>
<sectionHeader confidence="0.924613" genericHeader="method">
3 Modeling WH dependencies
</sectionHeader>
<bodyText confidence="0.9999754">
The task with which we are concerned in this sec-
tion is determining the location of a WH gap,
given evidence regarding the WH phrase and the
syntactic environment. Following much recent
work which applies the tools of machine learn-
ing to linguistic problems, we will treat this as an
example of a classification task. In Section 3.2
below, we describe the neural network classifier
which serves as our grammatical module respon-
sible for WH gap identification.
</bodyText>
<subsectionHeader confidence="0.976369">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.999979875">
The data on which the classifiers are trained and
tested is an extract of 7915 sentences from the
Penn Treebank (Marcus et al., 1993), which are
tagged to indicate the location of WH gaps. This
selection comprises essentially all of the sentences
in the treebank which contain WH gaps. Figure 1
shows a simplified example of WH-fronting from
the Penn Treebank in which the WH word why
is associated with the matrix VP node, despite its
fronted position in the syntax. Note that it cannot
be associated with the lower VP node. The Penn
Treebank already indicates the location of &amp;quot;WH-
traces&amp;quot; (and other empty categories), so it was not
necessary to edit the data for this project by hand,
although they were automatically pre-processed to
prune out any nodes which dominate only phono-
logically empty material.
In treating the identification of WH gaps as a
classification task, however, we are immediately
faced with the issues of identifying a finite num-
ber of classes into which our model will divide the
data, and determining the features which will be
available to the model.
Using the movement metaphor of syntactic the-
ory as our guide, we would ideally like to identify
a complete path downward from the surface syn-
tactic location of the WH phrase to the location of
its associated gap. However, it is hard to see how
such a path could be represented as one of a finite
number of classes. Therefore, we treat the search
downward through the phrase-structure tree for the
location of a gap as a Markov process. That is, we
</bodyText>
<page confidence="0.96884">
100
</page>
<figureCaption confidence="0.99184">
Figure 1: Simplified tree from the Penn Treebank.
</figureCaption>
<bodyText confidence="0.974071666666667">
The fronted WH-word &apos;why&apos; is associated with a
gap in the matrix VP, indicated by the empty con-
stituent (-NONE- *T*-2).
</bodyText>
<equation confidence="0.999483727272727">
(SBARQ
(WHADVP-2 (WRB Why) )
(SQ (VBP do)
(NP-SBJ (PRP you) )
(VP (VB maintain)
(SEAR (-NONE- 0)
(S
(NP (DT the) (NW plan)
(VP (VBZ is)
(NP
(DT a)
(JJ temporary)
(NN reduction) ))))
(S BAR
(WHADVP-1 (WRB when) )
(S
(NP (PRP it) )
(VP (VBZ is) (RB not)
(NP (-NONE- *?*) )
(ADVO (-NONE- *T*-1) ))))
(ADVP (-NONE- *T*-2) )))
(. ?) )
</equation>
<bodyText confidence="0.984559692307692">
begin at the first branching node dominating the
WH operator, and train a classifier to trace down-
ward from there, eventually predicting the location
of the gap. At each node we encounter, the classi-
fier chooses either to recurse into one of the child
nodes, or to predict the existence of a gap between
two of the overt child nodes. (Since the number
of daughters in a subtree is limited, the number of
classes is also bounded.) This decision is condi-
tioned only on the category labels of the current
subtree, the nature of the WH word extracted, and
the depth to which we have already proceeded in
searching for the gap. This greedy search process
is illustrated in Figure 2.
Each sentence was thus represented as a series
of records indicating, for each subtree in the path
from the WH phrase to the gap, the relevant syn-
tactic attributes of the subtree and WH phrase, and
the action to be taken at that stage (e.g., GAP-
0, RECURSE-2).1 Sample records are shown in
Figure 3. The &amp;quot;join category&amp;quot; is defined as the
lowest node dominating both the WH phrase and
We indicate the target classes for this task as
RECURSE-n, indicating that the Markov process should re-
curse into the nth daughter node, or GAP-n, indicating that
a gap will be posited at offset n in the subtree.
</bodyText>
<figureCaption confidence="0.6257">
Figure 2: Illustration of the path a classifier must
</figureCaption>
<bodyText confidence="0.9815615">
trace in order to identify the location of the gap
from Figure 1. At the top level, it must choose to
recurse into the SQ node, and at the second level,
into the VP node. Finally, within the VP subtree
it should predict the location of the gap as the last
child of the parent VP.
</bodyText>
<equation confidence="0.930319166666667">
SBARQ
WHADVP — SQ
_
/
why VBP NP VP— 9
do you VB SBAR SBAR
</equation>
<bodyText confidence="0.9992775">
its associated gap; the meanings of the other fea-
tures in Figure 3 should be clear.
</bodyText>
<subsectionHeader confidence="0.997121">
3.2 Classifier
</subsectionHeader>
<bodyText confidence="0.999980304347826">
For our classifier model of WH dependencies, we
used a simple feed-forward multi-layer percep-
tron, with a single hidden layer of 10 nodes. The
data to be classified is presented as a vector of fea-
tures at the input layer, and the output layer has
nodes representing the possible classes for the data
(RECURSE-1, RECURSE-2, GAP-0, GAP-1,
etc.). At the input layer, the information from
records such as those in Figure 3 is presented as
binary-valued inmputs; i.e., for each combination
of feature type and feature value in a record (say,
mother cat = S), there is a single node at the input
layer indicating whether that combination is real-
ized.
We trained the connection weights of the net-
work using the quickprop algorithm (Fahlman,
1988) on 4690 example sentences from the train-
ing corpus (12000 classification stages), reserving
1562 sentences (4001 classification stages) for val-
idation to avoid over-training the classifier. In Ta-
ble 1 we present the results of the neural network
in classifying our 1663 test sentences after train-
ing.
</bodyText>
<sectionHeader confidence="0.997631" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.935358">
These performance levels seem quite good, al-
though at this point there are no published results
</bodyText>
<page confidence="0.997426">
101
</page>
<figureCaption confidence="0.978822">
Figure 3: Example records corresponding to the sentence shown in Figures 1 &amp; 2
</figureCaption>
<table confidence="0.996947692307692">
target class: RECURSE-2 target class: RECURSE-3 target class: GAP-3
depth: 0 depth: I depth: 2
WI-teat: WHADVP WH eat: WHADVP WH cat: WHADVP
WIT lex: why WIT lex: why WTI lex: why
join cat: SBARQ join cat: SBARQ join cat: SBARQ
mother cat: SBARQ mother cat: SQ mother cat: VP
daughter catl : WHADVP daughter call: VBP daughter catl: VB
daughter cat2: SQ daughter cat2: NP daughter cat2: SBAR
daughter cat3: daughter cat: VP daughter cat3: SBAR
daughter cat4: UNDEFINED daughter cat4: UNDEFINED daughter cat4: UNDEFINED
daughter cat5: UNDEFINED daughter cat5: UNDEFINED daughter cat5: UNDEFINED
daughter cath: UNDEFINED daughter cat6: UNDEFINED daughter cath: UNDEFINED
daughter cat7: UNDEFINED daughter cat7: UNDEFINED daughter cat7: UNDEFINED
</table>
<tableCaption confidence="0.999675">
Table 1: Test-set performance of network
</tableCaption>
<subsectionHeader confidence="0.452293">
Percentage Correct
</subsectionHeader>
<bodyText confidence="0.995885">
Complete path 1530/1663 = 92.0%
String location 1563/1663 = 94.0%
Each stage 4093/4242 = 96.5%
for other systems to which we can compare them.
We take this level of success as an indication of the
feasibility of our data-driven, modular approach.
Additionally, our approach has the advantage of
wide coverage. Since it does not require an exten-
sive lexicon, and is trained on corpus data, it is eas-
ily adaptable to many different NLP applications.
Also, since the treatment of WH dependencies is
factored out from the syntax, it should be possi-
ble to employ a simple model of phrase structure,
such as a PCFG.
In future work, we hope to explore this possibil-
ity, by combining the classifier model of WH de-
pendencies developed here with a syntactic parser,
so that our results can be directly compared with
those of Collins (1999). The general mechanism
for combining these two models is the same one
used by Higgins &amp; Sadock (2003) for combin-
ing a quantifier scope component with a parser,
taking the syntactic component to define a prior
probability P(S) over syntactic structures, and
the additional component to define the probability
P(K1S), where K ranges over the values which
the other grammatical component may take on.
</bodyText>
<sectionHeader confidence="0.999013" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999633076923077">
Hiyan Alshawi, editor. 1992. The Core Language En-
gine. MIT Press.
Joseph Aoun and Yen-hui Audrey Li. 1993. The Syn-
tax of Scope. MIT Press, Cambridge, MA.
Robert C. Berwick and Sandiway Fong. 1995. A quar-
ter century of parsing with transformational gram-
mar. In J. Cole, G. Green, and J. Morgan, editors,
Linguistics and Computation, pages 103-143.
Noam Chomsky. 1957. Syntactic Structures. Janua
Linguarum. Mouton, The Hague.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania, Philadelphia, PA.
S. E. Fahlman. 1988. An empirical study of learning
speed in back-propagation networks. Technical Re-
port CMU-CS-88-162, Carnegie Mellon University.
Gerald Gazdar, Ewan Klein, Geoffrey Pullum, and Ivan
Sag. 1985. Generalized Phrase Structure Gram-
mar. Harvard University Press, Cambridge, MA.
Derrick Higgins and Jerrold M. Sadock. 2003. A ma-
chine learning approach to modeling scope prefer-
ences. Computational Linguistics, 29(1):73-96.
C.-T. James Huang. 1995. Logical form. In G. Webel-
huth, editor, Government and Binding Theory and
the Minimalist Program, pages 125-175. Blackwell,
Oxford.
Susumu Kuno, Ken-Ichi Takami, and Yuru Wu. 1999.
Quantifi er scope in English, Chinese, and Japanese.
Language, 75(1):63-111.
M. Marcus, S. Santorini, and M. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: the Penn Treebank. Computational Linguis-
tics, 19(2):313-330.
James D. McCawley. 1998. The Syntactic Phenomena
of English. University of Chicago Press, Chicago,
second edition.
Carl Pollard and Ivan Sag. 1994. Head-Driven Phrase
Structure Grammar. University of Chicago Press,
Chicago.
</reference>
<page confidence="0.998624">
102
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.317690">
<title confidence="0.999561">A machine-learning approach to the identification of WH gaps</title>
<author confidence="0.8177555">Derrick Higgins Educational Testing Service</author>
<email confidence="0.999666">dchiggin@alumni.uchicago.edu</email>
<abstract confidence="0.990858719298246">In this paper, we pursue a multimodular, statistical approach to WH dependencies, using a feedforward network as our modeling tool. The empirical basis of this model and the availability of performance measures for our system address deficiencies in earlier computational work on WH gaps, which require richer sources of semantic and lexical information in order to run. The statistical nature of our models allows them to be simply combined with other modules of grammar, such as a syntactic parser. 1 Overview paper concerns the phenomenon of desubclass of dependenknown as dependencies are structures in which a constituent headed by a WH word (such as &amp;quot;who&amp;quot; or &amp;quot;where&amp;quot;) is found somewhere other than where it belongs for semantic interpretation and subcategorization. A dependencies have played an important role in syntactic theory, but discovering the location of a gap corresponding to a WH phrase found in the syntactic representation of a sentence is also of interest for computational applications. Identification of the syntactic gap may be necessary for interpretation of the sentence, and could contribute to a natural language understanding or machine translation application. Since WH dependencies also tend to distort the surface subcategorization properties of verbs, identifying gaps could also aid in automatic lexical acquisition techniques. Many other applications are imaginable as well, using the gap location to inform intonation, semantics, collocation frequency, etc. The contribution of this paper consists in the development of a machine-learning approach to the identification of WH gaps. This approach reduces the lexical prerequisites for this task, while maintaining a high degree of accuracy. In addition, the modular treatment of WH dependencies allows the model to be easily incorporated with many different models of surface syntax. In ongoing work, we are investigating ways in which our model may be combined with a syntactic parser. The idea that the task of associating a WH element with its gap can be done in an independent module of grammar is not only interesting for reasons of computational efficacy. The fact of unbounded dependencies has played a central role in the development of linguistic theory as well. It has been used as an argument for the necessity of transformations (Chomsky, 1957), and prompted</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>The Core Language Engine.</title>
<date>1992</date>
<editor>Hiyan Alshawi, editor.</editor>
<publisher>MIT Press.</publisher>
<marker>1992</marker>
<rawString>Hiyan Alshawi, editor. 1992. The Core Language Engine. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Aoun</author>
<author>Yen-hui Audrey Li</author>
</authors>
<title>The Syntax of Scope.</title>
<date>1993</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="3332" citStr="Aoun and Li, 1993" startWordPosition="530" endWordPosition="533">mena can be described in an independent module of grammar, our theory of the syntax of natural language can be accordingly simplified. 2 Previous Work In theoretical linguistics, WH dependencies have typically been dealt with as part of the syntax (but cf. Kuno, Takami &amp; Wu (1999) for an alternative approach). Early generative treatments used a WH-movement transformation (McCawley, 1998) to describe the relationship between a WH phrase and its gap, while later work in the Government &amp; 99 Binding framework subsumes this transformation under the general heading of overt A Movement (Huang, 1995; Aoun and Li, 1993). Feature-based syntactic formalisms such as GPSG use featurepercolation mechanisms to transfer information from the location in which a WH phrase is subcategorized for (the gap), to the location where it is realized (the filler position) (Gazdar et al., 1985; Pollard and Sag, 1994). Most work in computational linguistics has followed these theoretical approaches to WH dependencies very closely. Berwick &amp; Fong (1995) implement a transformational account of WH gaps in their Government &amp; Binding parser, although the grammatical coverage of their system is extremely limited. The SRI Core Language</context>
</contexts>
<marker>Aoun, Li, 1993</marker>
<rawString>Joseph Aoun and Yen-hui Audrey Li. 1993. The Syntax of Scope. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Berwick</author>
<author>Sandiway Fong</author>
</authors>
<title>A quarter century of parsing with transformational grammar.</title>
<date>1995</date>
<booktitle>Linguistics and Computation,</booktitle>
<pages>103--143</pages>
<editor>In J. Cole, G. Green, and J. Morgan, editors,</editor>
<contexts>
<context position="3752" citStr="Berwick &amp; Fong (1995)" startWordPosition="596" endWordPosition="599">ip between a WH phrase and its gap, while later work in the Government &amp; 99 Binding framework subsumes this transformation under the general heading of overt A Movement (Huang, 1995; Aoun and Li, 1993). Feature-based syntactic formalisms such as GPSG use featurepercolation mechanisms to transfer information from the location in which a WH phrase is subcategorized for (the gap), to the location where it is realized (the filler position) (Gazdar et al., 1985; Pollard and Sag, 1994). Most work in computational linguistics has followed these theoretical approaches to WH dependencies very closely. Berwick &amp; Fong (1995) implement a transformational account of WH gaps in their Government &amp; Binding parser, although the grammatical coverage of their system is extremely limited. The SRI Core Language Engine (Alshawi, 1992) incorporates a feature-based account of WH gaps known as &amp;quot;gap-threading&amp;quot;, which is essentially identical to the feature-passing mechanisms used in GPSG. Both systems require an extensive lexical database of valency information in order to identify potential gap locations. While there are no published results regarding the accuracy of these methods in correctly associating WH phrases and their </context>
</contexts>
<marker>Berwick, Fong, 1995</marker>
<rawString>Robert C. Berwick and Sandiway Fong. 1995. A quarter century of parsing with transformational grammar. In J. Cole, G. Green, and J. Morgan, editors, Linguistics and Computation, pages 103-143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>Syntactic Structures. Janua Linguarum.</title>
<date>1957</date>
<publisher>Mouton, The Hague.</publisher>
<contexts>
<context position="2572" citStr="Chomsky, 1957" startWordPosition="408" endWordPosition="409">n addition, the modular treatment of WH dependencies allows the model to be easily incorporated with many different models of surface syntax. In ongoing work, we are investigating ways in which our model may be combined with a syntactic parser. The idea that the task of associating a WH element with its gap can be done in an independent module of grammar is not only interesting for reasons of computational efficacy. The fact of unbounded dependencies has played a central role in the development of linguistic theory as well. It has been used as an argument for the necessity of transformations (Chomsky, 1957), and prompted the introduction of powerful mechanisms such as the SLASH feature of GPSG (Gazdar et al., 1985). To the extent that these phenomena can be described in an independent module of grammar, our theory of the syntax of natural language can be accordingly simplified. 2 Previous Work In theoretical linguistics, WH dependencies have typically been dealt with as part of the syntax (but cf. Kuno, Takami &amp; Wu (1999) for an alternative approach). Early generative treatments used a WH-movement transformation (McCawley, 1998) to describe the relationship between a WH phrase and its gap, while</context>
</contexts>
<marker>Chomsky, 1957</marker>
<rawString>Noam Chomsky. 1957. Syntactic Structures. Janua Linguarum. Mouton, The Hague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia, PA.</location>
<contexts>
<context position="5003" citStr="Collins (1999)" startWordPosition="798" endWordPosition="799">be improved upon by adopting a corpus-based approach. First, deriving generalizations about the distribution of WH phrases directly from corpus data addresses the problem that the data may not conform to our theoretical preconceptions. Second, we hope to show that much of the work of identifying WH dependencies can be done without access to the subcategorization frame of every verb and preposition in the corpus, which is a prerequisite for the methods mentioned above. The only previous work we are aware of which addresses the task of identification of WH gaps from a statistical perspective is Collins (1999), which employs a lexicalized PCFG augmented with &amp;quot;gap features&amp;quot;. Unfortunately, our results are not directly comparable to those reported by Collins, because his model of WH dependencies is integrated with a syntactic parser, so that his system is responsible for producing syntactic phrase structure trees as well as gap locations. Since our model takes these trees as given, it identifies the correct gap location more consistently. Integration of our model of WH dependencies with a parser is a goal of future development. 3 Modeling WH dependencies The task with which we are concerned in this s</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Fahlman</author>
</authors>
<title>An empirical study of learning speed in back-propagation networks.</title>
<date>1988</date>
<tech>Technical Report CMU-CS-88-162,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="10559" citStr="Fahlman, 1988" startWordPosition="1779" endWordPosition="1780"> of 10 nodes. The data to be classified is presented as a vector of features at the input layer, and the output layer has nodes representing the possible classes for the data (RECURSE-1, RECURSE-2, GAP-0, GAP-1, etc.). At the input layer, the information from records such as those in Figure 3 is presented as binary-valued inmputs; i.e., for each combination of feature type and feature value in a record (say, mother cat = S), there is a single node at the input layer indicating whether that combination is realized. We trained the connection weights of the network using the quickprop algorithm (Fahlman, 1988) on 4690 example sentences from the training corpus (12000 classification stages), reserving 1562 sentences (4001 classification stages) for validation to avoid over-training the classifier. In Table 1 we present the results of the neural network in classifying our 1663 test sentences after training. 4 Conclusion These performance levels seem quite good, although at this point there are no published results 101 Figure 3: Example records corresponding to the sentence shown in Figures 1 &amp; 2 target class: RECURSE-2 target class: RECURSE-3 target class: GAP-3 depth: 0 depth: I depth: 2 WI-teat: WH</context>
</contexts>
<marker>Fahlman, 1988</marker>
<rawString>S. E. Fahlman. 1988. An empirical study of learning speed in back-propagation networks. Technical Report CMU-CS-88-162, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald Gazdar</author>
<author>Ewan Klein</author>
<author>Geoffrey Pullum</author>
<author>Ivan Sag</author>
</authors>
<title>Generalized Phrase Structure Grammar.</title>
<date>1985</date>
<publisher>Harvard University Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="2682" citStr="Gazdar et al., 1985" startWordPosition="424" endWordPosition="427">y different models of surface syntax. In ongoing work, we are investigating ways in which our model may be combined with a syntactic parser. The idea that the task of associating a WH element with its gap can be done in an independent module of grammar is not only interesting for reasons of computational efficacy. The fact of unbounded dependencies has played a central role in the development of linguistic theory as well. It has been used as an argument for the necessity of transformations (Chomsky, 1957), and prompted the introduction of powerful mechanisms such as the SLASH feature of GPSG (Gazdar et al., 1985). To the extent that these phenomena can be described in an independent module of grammar, our theory of the syntax of natural language can be accordingly simplified. 2 Previous Work In theoretical linguistics, WH dependencies have typically been dealt with as part of the syntax (but cf. Kuno, Takami &amp; Wu (1999) for an alternative approach). Early generative treatments used a WH-movement transformation (McCawley, 1998) to describe the relationship between a WH phrase and its gap, while later work in the Government &amp; 99 Binding framework subsumes this transformation under the general heading of</context>
</contexts>
<marker>Gazdar, Klein, Pullum, Sag, 1985</marker>
<rawString>Gerald Gazdar, Ewan Klein, Geoffrey Pullum, and Ivan Sag. 1985. Generalized Phrase Structure Grammar. Harvard University Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Derrick Higgins</author>
<author>Jerrold M Sadock</author>
</authors>
<title>A machine learning approach to modeling scope preferences.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<pages>29--1</pages>
<marker>Higgins, Sadock, 2003</marker>
<rawString>Derrick Higgins and Jerrold M. Sadock. 2003. A machine learning approach to modeling scope preferences. Computational Linguistics, 29(1):73-96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-T James Huang</author>
</authors>
<title>Logical form.</title>
<date>1995</date>
<booktitle>Government and Binding Theory and the Minimalist Program,</booktitle>
<pages>125--175</pages>
<editor>In G. Webelhuth, editor,</editor>
<publisher>Blackwell,</publisher>
<location>Oxford.</location>
<contexts>
<context position="3312" citStr="Huang, 1995" startWordPosition="528" endWordPosition="529">t these phenomena can be described in an independent module of grammar, our theory of the syntax of natural language can be accordingly simplified. 2 Previous Work In theoretical linguistics, WH dependencies have typically been dealt with as part of the syntax (but cf. Kuno, Takami &amp; Wu (1999) for an alternative approach). Early generative treatments used a WH-movement transformation (McCawley, 1998) to describe the relationship between a WH phrase and its gap, while later work in the Government &amp; 99 Binding framework subsumes this transformation under the general heading of overt A Movement (Huang, 1995; Aoun and Li, 1993). Feature-based syntactic formalisms such as GPSG use featurepercolation mechanisms to transfer information from the location in which a WH phrase is subcategorized for (the gap), to the location where it is realized (the filler position) (Gazdar et al., 1985; Pollard and Sag, 1994). Most work in computational linguistics has followed these theoretical approaches to WH dependencies very closely. Berwick &amp; Fong (1995) implement a transformational account of WH gaps in their Government &amp; Binding parser, although the grammatical coverage of their system is extremely limited. T</context>
</contexts>
<marker>Huang, 1995</marker>
<rawString>C.-T. James Huang. 1995. Logical form. In G. Webelhuth, editor, Government and Binding Theory and the Minimalist Program, pages 125-175. Blackwell, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susumu Kuno</author>
<author>Ken-Ichi Takami</author>
<author>Yuru Wu</author>
</authors>
<date>1999</date>
<pages>75--1</pages>
<note>Quantifi er scope in</note>
<marker>Kuno, Takami, Wu, 1999</marker>
<rawString>Susumu Kuno, Ken-Ichi Takami, and Yuru Wu. 1999. Quantifi er scope in English, Chinese, and Japanese. Language, 75(1):63-111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>S Santorini</author>
<author>M Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--2</pages>
<contexts>
<context position="6159" citStr="Marcus et al., 1993" startWordPosition="986" endWordPosition="989">g WH dependencies The task with which we are concerned in this section is determining the location of a WH gap, given evidence regarding the WH phrase and the syntactic environment. Following much recent work which applies the tools of machine learning to linguistic problems, we will treat this as an example of a classification task. In Section 3.2 below, we describe the neural network classifier which serves as our grammatical module responsible for WH gap identification. 3.1 Data The data on which the classifiers are trained and tested is an extract of 7915 sentences from the Penn Treebank (Marcus et al., 1993), which are tagged to indicate the location of WH gaps. This selection comprises essentially all of the sentences in the treebank which contain WH gaps. Figure 1 shows a simplified example of WH-fronting from the Penn Treebank in which the WH word why is associated with the matrix VP node, despite its fronted position in the syntax. Note that it cannot be associated with the lower VP node. The Penn Treebank already indicates the location of &amp;quot;WHtraces&amp;quot; (and other empty categories), so it was not necessary to edit the data for this project by hand, although they were automatically pre-processed </context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. Marcus, S. Santorini, and M. Marcinkiewicz. 1993. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19(2):313-330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James D McCawley</author>
</authors>
<title>The Syntactic Phenomena of English.</title>
<date>1998</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago,</location>
<note>second edition.</note>
<contexts>
<context position="3104" citStr="McCawley, 1998" startWordPosition="494" endWordPosition="495">t has been used as an argument for the necessity of transformations (Chomsky, 1957), and prompted the introduction of powerful mechanisms such as the SLASH feature of GPSG (Gazdar et al., 1985). To the extent that these phenomena can be described in an independent module of grammar, our theory of the syntax of natural language can be accordingly simplified. 2 Previous Work In theoretical linguistics, WH dependencies have typically been dealt with as part of the syntax (but cf. Kuno, Takami &amp; Wu (1999) for an alternative approach). Early generative treatments used a WH-movement transformation (McCawley, 1998) to describe the relationship between a WH phrase and its gap, while later work in the Government &amp; 99 Binding framework subsumes this transformation under the general heading of overt A Movement (Huang, 1995; Aoun and Li, 1993). Feature-based syntactic formalisms such as GPSG use featurepercolation mechanisms to transfer information from the location in which a WH phrase is subcategorized for (the gap), to the location where it is realized (the filler position) (Gazdar et al., 1985; Pollard and Sag, 1994). Most work in computational linguistics has followed these theoretical approaches to WH </context>
</contexts>
<marker>McCawley, 1998</marker>
<rawString>James D. McCawley. 1998. The Syntactic Phenomena of English. University of Chicago Press, Chicago, second edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan Sag</author>
</authors>
<title>Head-Driven Phrase Structure Grammar.</title>
<date>1994</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago.</location>
<contexts>
<context position="3615" citStr="Pollard and Sag, 1994" startWordPosition="575" endWordPosition="578">9) for an alternative approach). Early generative treatments used a WH-movement transformation (McCawley, 1998) to describe the relationship between a WH phrase and its gap, while later work in the Government &amp; 99 Binding framework subsumes this transformation under the general heading of overt A Movement (Huang, 1995; Aoun and Li, 1993). Feature-based syntactic formalisms such as GPSG use featurepercolation mechanisms to transfer information from the location in which a WH phrase is subcategorized for (the gap), to the location where it is realized (the filler position) (Gazdar et al., 1985; Pollard and Sag, 1994). Most work in computational linguistics has followed these theoretical approaches to WH dependencies very closely. Berwick &amp; Fong (1995) implement a transformational account of WH gaps in their Government &amp; Binding parser, although the grammatical coverage of their system is extremely limited. The SRI Core Language Engine (Alshawi, 1992) incorporates a feature-based account of WH gaps known as &amp;quot;gap-threading&amp;quot;, which is essentially identical to the feature-passing mechanisms used in GPSG. Both systems require an extensive lexical database of valency information in order to identify potential g</context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>Carl Pollard and Ivan Sag. 1994. Head-Driven Phrase Structure Grammar. University of Chicago Press, Chicago.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>