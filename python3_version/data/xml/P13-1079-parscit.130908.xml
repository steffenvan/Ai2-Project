<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000514">
<title confidence="0.985017">
Hierarchical Phrase Table Combination for Machine Translation
</title>
<author confidence="0.999353">
Conghui Zhu1 Taro Watanabe2 Eiichiro Sumita2 Tiejun Zhao1
</author>
<affiliation confidence="0.998418">
1School of Computer Science and Technology
Harbin Institute of Technology (HIT), Harbin, China
2National Institute of Information and Communication Technology
</affiliation>
<address confidence="0.968074">
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan
</address>
<email confidence="0.993028">
{chzhu,tjzhao}@mtlab.hit.edu.cn
{taro.watanabe,Sumita}@nict.go.jp
</email>
<sectionHeader confidence="0.993694" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999949181818182">
Typical statistical machine translation sys-
tems are batch trained with a given train-
ing data and their performances are large-
ly influenced by the amount of data. With
the growth of the available data across
different domains, it is computationally
demanding to perform batch training ev-
ery time when new data comes. In face
of the problem, we propose an efficient
phrase table combination method. In par-
ticular, we train a Bayesian phrasal inver-
sion transduction grammars for each do-
main separately. The learned phrase ta-
bles are hierarchically combined as if they
are drawn from a hierarchical Pitman-Yor
process. The performance measured by
BLEU is at least as comparable to the tra-
ditional batch training method. Further-
more, each phrase table is trained sepa-
rately in each domain, and while compu-
tational overhead is significantly reduced
by training them in parallel.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997669561403509">
Statistical machine translation (SMT) system-
s usually achieve ’crowd-sourced’ improvements
with batch training. Phrase pair extraction, the
key step to discover translation knowledge, heav-
ily relies on the scale of training data. Typi-
cally, the more parallel corpora used, the more
phrase pairs and more accurate parameters will
be learned, which can obviously be beneficial to
improving translation performances. Today, more
parallel sentences are drawn from divergent do-
mains, and the size keeps growing. Consequent-
ly, how to effectively use those data and improve
translation performance becomes a challenging is-
sue.
This joint work was done while the first author visited
NICT.
Batch retraining is not acceptable for this case,
since it demands serious computational overhead
when training on a large data set, and it requires
us to re-train every time new training data is avail-
able. Even if we can handle the large computation
cost, improvement is not guaranteed every time we
perform batch tuning on the newly updated train-
ing data obtained from divergent domains. Tradi-
tional domain adaption methods for SMT are also
not adequate in this scenario. Most of them have
been proposed in order to make translation sys-
tems perform better for resource-scarce domain-
s when most training data comes from resource-
rich domains, and ignore performance on a more
generic domain without domain bias (Wang et al.,
2012). As an alternative, incremental learning
may resolve the gap by incrementally adding da-
ta sentence-by-sentence into the training data. S-
ince SMT systems trend to employ very large scale
training data for translation knowledge extraction,
updating several sentence pairs each time will be
annihilated in the existing corpus.
This paper proposes a new phrase table combi-
nation method. First, phrase pairs are extracted
from each domain without interfering with oth-
er domains. In particular, we employ the non-
parametric Bayesian phrasal inversion transduc-
tion grammar (ITG) of Neubig et al. (2011) to per-
form phrase table extraction. Second, extracted
phrase tables are combined as if they are drawn
from a hierarchical Pitman-Yor process, in which
the phrase tables represented as tables in the Chi-
nese restaurant process (CRP) are hierarchically
chained by treating each of the previously learned
phrase tables as prior to the current one. Thus, we
can easily update the chain of phrase tables by ap-
pending the newly extracted phrase table and by
treating the chain of the previous ones as its prior.
Experiment results indicate that our method can
achieve better translation performance when there
exists a large divergence in domains, and can
</bodyText>
<page confidence="0.962916">
802
</page>
<note confidence="0.914269">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 802–810,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.9995074">
achieve at least comparable results to batch train-
ing methods, with a significantly less computa-
tional overhead.
The rest of the paper is organized as follows.
In Section 2, we introduce related work. In sec-
tion 3, we briefly describe the translation mod-
el with phrasal ITGs and Pitman-Yor process. In
section 4, we explain our hierarchical combination
approach and give experiment results in section 5.
We conclude the paper in the last section.
</bodyText>
<sectionHeader confidence="0.999781" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999957298507463">
Bilingual phrases are cornerstones for phrase-
based SMT systems (Och and Ney, 2004; Koehn
et al., 2003; Chiang, 2005) and existing translation
systems often get ‘crowd-sourced’ improvements
(Levenberg et al., 2010). A number of approaches
have been proposed to make use of the full poten-
tial of the available parallel sentences from vari-
ous domains, such as domain adaptation and in-
cremental learning for SMT.
The translation model and language model
are primary components in SMT. Previous work
proved successful in the use of large-scale data for
language models from diverse domains (Brants et
al., 2007; Schwenk and Koehn, 2008). Alterna-
tively, the language model is incrementally up-
dated by using a succinct data structure with a
interpolation technique (Levenberg and Osborne,
2009; Levenberg et al., 2011).
In the case of the previous work on translation
modeling, mixed methods have been investigat-
ed for domain adaptation in SMT by adding do-
main information as additional labels to the orig-
inal phrase table (Foster and Kuhn, 2007). Un-
der this framework, the training data is first di-
vided into several parts, and phase pairs are ex-
tracted with some sub-domain features. Then al-
l the phrase pairs and features are tuned together
with different weights during decoding. As a way
to choose the right domain for the domain adap-
tion, a classifier-based method and a feature-based
method have been proposed. Classification-based
methods must at least add an explicit label to indi-
cate which domain the current phrase pair comes
from. This is traditionally done with an automat-
ic domain classifier, and each input sentence is
classified into its corresponding domain (Xu et al.,
2007). As an alternative to the classification-based
approach, Wang et al. (2012) employed a feature-
based approach, in which phrase pairs are enriched
by a feature set to potentially reflect the domain in-
formation. The similarity calculated by a informa-
tion retrieval system between the training subset
and the test set is used as a feature for each paral-
lel sentence (Lu et al., 2007). Monolingual topic
information is taken as a new feature for a domain
adaptive translation model and tuned on the devel-
opment set (Su et al., 2012). Regardless of under-
lying methods, either classifier-based or feature-
based method, the performance of current domain
adaptive phrase extraction methods is more sensi-
tive to the development set selection. Usually the
domain similar to a given development data is usu-
ally assigned higher weights.
Incremental learning in which new parallel sen-
tences are incrementally updated to the training
data is employed for SMT. Compared to tradi-
tional frequent batch oriented methods, an online
EM algorithm and active learning are applied to
phrase pair extraction and achieves almost compa-
rable translation performance with less computa-
tional overhead (Levenberg et al., 2010; Gonz´alez-
Rubio et al., 2011). However, their methods usu-
ally require numbers of hyperparameters, such as
mini-batch size, step size, or human judgment to
determine the quality of phrases, and still rely on a
heuristic phrase extraction method in each phrase
table update.
</bodyText>
<sectionHeader confidence="0.9448545" genericHeader="method">
3 Phrase Pair Extraction with
Unsupervised Phrasal ITGs
</sectionHeader>
<bodyText confidence="0.9995463">
Recently, phrase alignment with ITGs (Cherry
and Lin, 2007; Zhang et al., 2008; Blunsom et
al., 2008) and parameter estimation with Gibb-
s sampling (DeNero and Klein, 2008; Blunsom
and Cohn, 2010) are popular. Here, we em-
ploy a method proposed by Neubig et al. (2011),
which uses parametric Bayesian inference with the
phrasal ITGs (Wu, 1997). It can achieve com-
parable translation accuracy with a much small-
er phrase table than the traditional GIZA++ and
heuristic phrase extraction methods. It has al-
so been proved successful in adjusting the phrase
length granularity by applying character-based
SMT with more sophisticated inference (Neubig
et al., 2012).
ITG is a synchronous grammar formalism
which analyzes bilingual text by introducing in-
verted rules, and each ITG derivation corresponds
to the alignment of a sentence pair (Wu, 1997).
Translation probabilities of ITG phrasal align-
</bodyText>
<page confidence="0.997795">
803
</page>
<bodyText confidence="0.997623">
ments can be estimated in polynomial time by s-
lightly limiting word reordering (DeNero and K-
lein, 2008).
More formally, P((e, f); θx, θt) are the proba-
bility of phrase pairs (e, f), which is parameter-
ized by a phrase pair distribution θt and a symbol
distribution θx. θx is a Dirichlet prior, and θt is es-
timated with the Pitman-Yor process (Pitman and
Yor, 1997; Teh, 2006), which is expressed as
</bodyText>
<equation confidence="0.996939">
θt — PY (d, s, Pdac) (1)
</equation>
<bodyText confidence="0.999503125">
where d is the discount parameter, s is the strength
parameter, and , and Pdac is a prior probability
which acts as a fallback probability when a phrase
pair is not in the model.
Under this model, the probability for a phrase
pair found in a bilingual corpus (E, F) can be rep-
resented by the following equation using the Chi-
nese restaurant process (Teh, 2006):
</bodyText>
<equation confidence="0.9915255">
1
P �(ei, fi); (E, F)) = C + s(ci — d x ti)+
</equation>
<bodyText confidence="0.995041">
where
</bodyText>
<listItem confidence="0.991238">
1. ci and ti are the customer and table count of
the ith phrase pair (ei, fi) found in a bilingual
corpus (E, F);
2. C and T are the total customer and table count
in corpus (E, F);
3. d and s are the discount and strengthen hyper-
parameters.
</listItem>
<bodyText confidence="0.98899275">
The prior probability Pdac is recursively defined
by breaking a longer phrase pair into two through
the recursive ITG’s generative story as follows
(Neubig et al., 2011):
</bodyText>
<listItem confidence="0.9825836">
1. Generate symbol x from Px(x; θx) with three
possible values: Base, REG, or INV.
2. Depending on the value of x take the following
actions.
a. If x = Base, generate a new phrase pair
directly from Pbase.
b. If x = REG, generate (e1, f1) and
(e2, f2) from P((e, f); θx, θt), and con-
catenate them into a single phrase pair
(e1e2, f1f2).
</listItem>
<figureCaption confidence="0.73376">
Figure 1: A word alignment (a), and its hierarchi-
cal derivation (b).
</figureCaption>
<bodyText confidence="0.961888636363636">
c. If x = INV, follow a similar process as b,
but concatenate f1 and f2 in reverse order
(e1e2, f2f1).
Note that the Pdac is recursively defined through
the binary branched P, which in turns employs
Pdac as a prior probability. Pbase is a base measure
defined as a combination of the IBM Models in t-
wo directions and the unigram language models in
both sides. Inference is carried out by a heuristic
beam search based block sampling with an effi-
cient look ahead for a faster convergence (Neubig
et al., 2012).
Compared to GIZA++ with heuristic phrase ex-
traction, the Bayesian phrasal ITG can achieve
competitive accuracy under a smaller phrase ta-
ble size. Further, the fallback model can incor-
porate phrases of all granularity by following the
ITG’s recursive definition. Figure 1 (b) illustrates
an example of the phrasal ITG derivation for word
alignment in Figure 1 (a) in which a bilingual sen-
tence pair is recursively divided into two through
the recursively defined generative story.
</bodyText>
<sectionHeader confidence="0.987929" genericHeader="method">
4 Hierarchical Phrase Table
Combination
</sectionHeader>
<bodyText confidence="0.998781571428571">
We propose a new phrase table combination
method, in which individually learned phrase ta-
ble are hierarchically chained through a hierarchi-
cal Pitman-Yor process.
Firstly, we assume that the whole train-
ing data (E, F) can be split into J domains,
{(E1, F1), ... , (EJ, FJ)}. Then phrase pairs are
</bodyText>
<equation confidence="0.971372">
1 (s + d x T) x Pdac((ei, fi)) (2)
C + s
</equation>
<page confidence="0.988833">
804
</page>
<figureCaption confidence="0.966894">
Figure 2: A hierarchical phrase table combination (a), and a basic unit of a Chinese restaurant process
with K tables and N customers.
</figureCaption>
<bodyText confidence="0.989637666666667">
extracted from each domain j (1 ≤ j ≤ J) sepa-
rately with the method introduced in Section 3. In
traditional domain adaptation approaches, phrase
pairs are extracted together with their probabili-
ties and/or frequencies so that the extracted phrase
pairs are merged uniformly or after scaling.
In this work, we extract the table counts for each
phrase pair under the Chinese restaurant process
given in Section 3. In Figure 2 (b), a CRP is illus-
trated which has K tables and N customers with
each chair representing a customer. Meanwhile
there are two parameters, discount and strength for
each domain similar to the ones in Equation (1).
Our proposed hierarchical phrase table combi-
nation can be formally expressed as following:
</bodyText>
<equation confidence="0.942361833333333">
θ1 ∼ PY (d1, s1, P2)
· · · · · ·
θj ∼ PY (dj, sj, Pj+1)
· · · · · ·
θJ ∼ PY (dJ, sJ, PJ ) (3)
base
</equation>
<bodyText confidence="0.996359476190476">
Here the (j + 1)th layer hierarchical Pitman-Yor
process is employed as a base measure for the
jth layer hierarchical Pitman-Yor process. The
hierarchical chain is terminated by the base mea-
sure from the Jth domain PJbase. The hierarchi-
cal structure is illustrated in Figure 2 (a) in which
the solid lines implies a fall back using the ta-
ble counts from the subsequent domains, and the
dotted lines means the final fallback to the base
measure PJbase. When we query a probability of
a phrase pair he, fi, we first query the probabil-
ity of the first layer P1(he, fi). If he, fi is not
in the model, we will fallback to the next level of
P2(he, fi). This process continues until we reach
the Jth base measure of PJ(he, fi). Each fallback
can be viewed as a translation knowledge integra-
tion process between subsequent domains.
For example in Figure 2 (a), the ith phrase pair
hei, fii appears only in the domain 1 and domain
2, so its translation probability can be calculated
by substituting Equation (3) with Equation (2):
</bodyText>
<equation confidence="0.9970282">
1
P (hei, fii; hE, Fi) = C1 + s1 (c1i − d1 × t1i )
s1 + d1 × T1 + (C1 + s1) × (C2 + s2)(c2 i − d2 × t2i )
(sj + dj × Tj) × Pb
Cj + sj ase (hei, fii) (4)
</equation>
<bodyText confidence="0.999976444444445">
where the superscript indicates the domain for the
corresponding counts, i.e. cji for the customer
count in the jth domain. The first term in Equa-
tion (4) is the phrase probability from the first do-
main, and the second one comes from the second
domain, but weighted by the fallback weight of the
1st domain. Since hei, fii does not appear in the
rest of the layers, the last term is taken from al-
l the fallback weight from the second layer to the
Jth layer with the final PJbase. All the parameter-
s θj and hyperparameters dj and sj, are obtained
by learning on the jth domain. Returning the hy-
perparameters again when cascading another do-
main may improve the performance of the combi-
nation weight, but we will leave it for future work.
The hierarchical process can be viewed as an in-
stance of adapted integration of translation knowl-
edge from each sub-domain.
</bodyText>
<figure confidence="0.757625">
J
+H
j=1
</figure>
<page confidence="0.975839">
805
</page>
<bodyText confidence="0.9627068">
Algorithm 1 Translation Probabilities Estima-
tion
Input: cji, tji, Pjbase, Cj, Tj, dj and sj
Output: The translation probabilities for each
pair
</bodyText>
<listItem confidence="0.968780384615385">
1: for all phrase pair (ei, fi) do
2: Initialize the P((ei, fi)) = 0 and wi = 1
3: for all domain (Ej, Fj) such that 1 j
J − 1 do
4: if (ei, fi) E (Ej, Fj) then
5: P((ei, fi)) += wi x (Cji − dj x
tj i )/(Cj + sj)
6: end if
7: wi = wi x (sj + dj x Tj)/(Cj + sj)
8: end for
9: P((ei, fi)) += wi x (CJi − dJ x tJi + (sJ +
dJ x TJ) x PJbase((ei, fi)))/(CJ + sJ)
10: end for
</listItem>
<bodyText confidence="0.999904692307692">
Our approach has several advantages. First,
each phrase pair extraction can concentrate on a s-
mall portion of domain-specific data without inter-
fering with other domains. Since no tuning stage
is involved in the hierarchical combination, we can
easily include a new phrase table from a new do-
main by simply chaining them together. Second,
phrase pair phrase extraction in each domain is
completely independent, so it is easy to parallelize
in a situation where the training data is too large
to fit into a small amount of memory. Finally, new
domains can be integrated incrementally. When
we encounter a new domain, and if a phrase pair is
completely new in terms of the model, the phrase
pair is simply appended to the current model, and
computed without the fallback probabilities, since
otherwise, the phrase pair would be boosted by the
fallback probabilities. Pitman-Yor process is also
employed in n-gram language models which are
hierarchically represented through the hierarchi-
cal Pitman-Yor process with switch priors to in-
tegrate different domains in all the levels (Wood
and Teh, 2009). Our work incrementally combines
the models from different domains by directly em-
ploying the hierarchical process through the base
measures.
</bodyText>
<sectionHeader confidence="0.999246" genericHeader="evaluation">
5 Experiment
</sectionHeader>
<bodyText confidence="0.997338666666667">
We evaluate the proposed approach on the
Chinese-to-English translation task with three data
sets with different scales.
</bodyText>
<table confidence="0.999867076923077">
Data set Corpus #sent. pairs
IWSLT HIT 52,603
BTEC 19,975
Domain 1 47,993
Domain 2 30,272
FBIS Domain 3 49,509
Domain 4 38,228
Domain 5 55,913
News 221,915
News 95,593
LDC Magazine 98,335
Magazine 254,488
Finance 86,112
</table>
<tableCaption confidence="0.999957">
Table 1: The sentence pairs used in each data set.
</tableCaption>
<subsectionHeader confidence="0.934774">
5.1 Experiment Setup
</subsectionHeader>
<bodyText confidence="0.999490055555556">
The first data set comes from the IWSLT2012
OLYMPICS task consisting of two training sets:
the HIT corpus, which is closely related to the Bei-
jing 2008 Olympic Games, and the BTEC corpus,
which is a multilingual speech corpus containing
tourism-related sentences. The second data set,
the FBIS corpus, is a collection of news articles
and does not have domain information itself, so a
Latent Dirichlet Allocation (LDA) tool, PLDA1,
is used to divide the whole corpus into 5 different
sub-domains according to the concatenation of the
source side and target side as a single sentence (Li-
u et al., 2011). The third data set is composed of 5
corpora2 from LDC with various domains, includ-
ing news, magazine, and finance. The details are
shown in Table 1.
In order to evaluate our approach, four phrase
pair extraction methods are performed:
</bodyText>
<listItem confidence="0.9692305">
1. GIZA-linear: Phase pairs are extracted in each
domain by GIZA++ (Och and Ney, 2003) and
the ”grow-diag-final-and” method with a max-
imum length 7. The phrase tables from vari-
ous domains are linearly combined by averag-
ing the feature values.
2. Pialign-linear: Similar to GIZA-linear, but we
employed the phrasal ITG method described in
</listItem>
<bodyText confidence="0.763981">
Section 3 using the pialign toolkit 3 (Neubig et
</bodyText>
<footnote confidence="0.998972">
1http://code.google.com/p/plda/
2In particular, they come from LDC catalog number:
LDC2002E18, LDC2002E58, LDC2003E14, LDC2005E47,
LDC2006E26, in this order.
3http://www.phontron.com/pialign/
</footnote>
<page confidence="0.994034">
806
</page>
<table confidence="0.999296625">
Methods IWSLT FBIS LDC
BLEU Size BLEU Size BLEU Size
GIZA-linear 19.222 1,200,877 29.342 15,369,028 30.67 77,927,347
Pialign-linear 19.534 876,059 29.858 7,235,342 31.12 28,877,149
GIZA-batch 19.616 1,185,255 31.38 13,737,258 32.06 63,606,056
Pialign-batch 19.506 841,931 31.104 6,459,200
Pialign-adaptive 19.624 841,931 30.926 6,459,200
Hier-combin 20.32 876,059 31.29 7,235,342 32.03 28,877,149
</table>
<tableCaption confidence="0.7518012">
Table 2: BLEU scores and phrase table size by alignment method and probabilities estimation method.
Pialign was run with five samples. Because of computational overhead, the baseline Pialign-batch and
Pialign-adaptive were not run on the largest data set.
al., 2011). Extracted phrase pairs are linearly
combined by averaging the feature values.
</tableCaption>
<listItem confidence="0.9991947">
3. GIZA-batch: Instead of splitting into each do-
main, the data set is merged as a single corpus
and then a heuristic GZA-based phrase extrac-
tion is performed, similar as GIZA-linear.
4. Pialign-batch: Similar to the GIZA-batch, a s-
ingle model is estimated from a single, merged
corpus. Since pialign cannot handle large data,
we did not experiment on the largest LDC data
set.
5. Pialign-adaptive: Alignment and phrase pairs
</listItem>
<bodyText confidence="0.92748875">
extraction are same to Pialign-batch, while
translation probabilities are estimated by the
adaptive method with monolingual topic in-
formation (Su et al., 2012). The method es-
tablished the relationship between the out-of-
domain bilingual corpus and in-domain mono-
lingual corpora via topic distribution to esti-
mate the translation probability.
</bodyText>
<equation confidence="0.999652666666667">
O(˜e |˜f) = �
tf
˜f) · P(tf|˜f) (5)
</equation>
<bodyText confidence="0.99634605882353">
where O(˜e|tf, ˜f) is the probability of translating
into e˜ given the source-side topic f˜ , P(tf|˜f) is
f˜
the phrase-topic distribution of f.
The method we proposed is named Hier-
combin. It extracts phrase pairs in the same way as
the Pialign-linear. In the phrase table combination
process, the translation probability of each phrase
pair is estimated by the Hier-combin and the other
features are also linearly combined by averaging
the feature values. Pialign is used with default pa-
rameters. The parameter ’samps’ is set to 5, which
indicates 5 samples are generated for a sentence
pair.
The IWSLT data consists of roughly 2, 000 sen-
tences and 3, 000 sentences each from the HIT and
BTEC for development purposes, and the test da-
ta consists of 1, 000 sentences. For the FBIS and
LDC task, we used NIST MT 2002 and 2004 for
development and testing purposes, consisting of
878 and 1, 788 sentences respectively. We em-
ploy Moses, an open-source toolkit for our exper-
iment (Koehn et al., 2007). SRILM Toolkit (Stol-
cke, 2002) is employed to train 4-gram language
models on the Xinhua portion of Gigaword cor-
pus, while for the IWLST2012 data set, only its
training set is used. We use batch-MIRA (Cher-
ry and Foster, 2012) to tune the weight for each
feature and translation quality is evaluated by the
case-insensitive BLEU-4 metric (Papineni et al.,
2002). The BLEU scores reported in this paper
are the average of 5 independent runs of indepen-
dent batch-MIRA weight training, as suggested by
(Clark et al., 2011).
</bodyText>
<subsectionHeader confidence="0.788519">
5.2 Result and Analysis
5.2.1 Performances of various extraction
methods
</subsectionHeader>
<bodyText confidence="0.9998279">
We carry out a series of experiments to evaluate
translation performance. The results are listed in
Table 2. Our method significantly outperforms the
baseline Pialign-linear. Except for the translation
probabilities, the phrase pairs of two methods are
exactly same, so the number of phrase pairs are
equal in the two methods. Further more, the per-
formance of the baseline Pialign-adaptive is also
higher than the baseline Pialign-linear’s and lower
than ours. This proves that the adaptive method
</bodyText>
<figure confidence="0.58628875">
˜f)
O(˜e, tf|
�= O(˜e|tf,
tf
</figure>
<page confidence="0.904731">
807
</page>
<table confidence="0.9875038">
Methods Task Time(minute)
Batch Retraining 536.9
Hierarchical Parallel Extraction 122.55
Combination Integrating 1.5
Total 124.05
</table>
<tableCaption confidence="0.997276">
Table 3: Minutes used for alignment and phase
</tableCaption>
<bodyText confidence="0.992727957446809">
pair extraction in the FBIS data set.
with monolingual topic information is useful in
the tasks, but our approach with the hierarchical
Pitman-Yor process can estimate more accurate
translation probabilities based on all the data from
various domains.
Compared with the GIZA-batch, our approach
achieves competitive performance with a much s-
maller phrase table. The number of phase pairs
generated by our method is only 73.9%, 52.7%,
and 45.4% of the GIZA-batch’s respectively. In
the IWLST2012 data set, there is a huge difference
gap between the HIT corpus and the BTEC corpus,
and our method gains 0.814 BLEU improvement.
While the FBIS data set is artificially divided and
no clear human assigned differences among sub-
domains, our method loses 0.09 BLEU.
In the framework we proposed, phrase pairs are
extracted from each domain completely indepen-
dent of each other, so those tasks can be executed
on different machines, at different times, and of
course in parallel when we assume that the do-
mains are not incrementally added in the train-
ing data. The runtime of our approach and the
batch-based ITGs sampling method in the FBIS
data set is listed in Table 3 measured on a 2.7 GHz
E5-2680 CPU and 128 Gigabyte memory. When
comparing the hier-combin with the pialign-batch,
the BLEU scores are a little higher while the time
spent for training is much lower, almost one quar-
ter of the pialign-batch.
Even the performance of the pialign-linear is
better than the Baseline GIZA-linear’s, which
means that phrase pair extraction with hierarchi-
cal phrasal ITGs and sampling is more suitable
for domain adaptation tasks than the combination
GIZA++ and a heuristic method.
Generally, the hierarchical combination method
exploits the nature of a hierarchical Pitman-Yor
process and gains the advantage of its smoothing
effect, and our approach can incrementally gener-
ate a succinct phrase table based on all the data
from various domains with more accurate prob-
abilities. Traditional SMT phrase pair extraction
is batch-based, while our method has no obvious
shortcomings in translation accuracy, not to men-
tion efficiency.
</bodyText>
<subsectionHeader confidence="0.952036">
5.2.2 Effect of Integration Order
</subsectionHeader>
<bodyText confidence="0.999793633333333">
Here, we evaluate whether our hierarchical com-
bination is sensitive to the order of the domains
when forming a hierarchical structure. Through
Equation (3), in our experiments, we chained the
domains in the order listed in Table 1, which is
in almost chronological order. Table 4 shows the
BLEU scores for the three data sets, in which the
order of combining phrase tables from each do-
main is alternated in the ascending and descending
of the similarity to the test data. The similarity be-
tween the data from each domain and the test data
is calculated using the perplexity measure with 5-
gram language model. The model learned from
the domain more similar to the test data is placed
in the front so that it can largely influence the
parameter computation with less backoff effects.
There is a big difference between the two opposite
order in IWSLT 2012 data set, in which more than
one point of decline in BLEU score when taking
the BTEC corpus as the first layer. Note that the
perplexity of BTEC was 344.589 while that of HIT
was 107.788. The result may indicate that our hi-
erarchical phrase combination method is sensitive
to the integration order when the training data is
small and there exists large gap in the similarity.
However, if most domains are similar (FBIS data
set) or if there are enough parallel sentence pairs
(NIST data set) in each domain, then the transla-
tion performances are almost similar even with the
opposite integrating orders.
</bodyText>
<table confidence="0.96098725">
IWSLT FBIS LDC
Descending 20.154 30.491 31.268
Ascending 19.066 30.388 31.254
Difference 1.088 0.103 0.014
</table>
<tableCaption confidence="0.944722">
Table 4: BLEU scores for the hierarchical model
with different integrating orders. Here Pialign was
run without multi-samples.
</tableCaption>
<sectionHeader confidence="0.976434" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.99132975">
In this paper, we present a novel hierarchical
phrase table combination method for SMT, which
can exploit more of the potential from all of da-
ta coming from various fields and generate a suc-
</bodyText>
<page confidence="0.995276">
808
</page>
<bodyText confidence="0.999984666666667">
cinct phrase table with more accurate translation
probabilities. The method assumes that a com-
bined model is derived from a hierarchical Pitman-
Yor process with each prior learned separately in
each domain, and achieves BLEU scores competi-
tive with traditional batch-based ones. Meanwhile,
the framework has natural characteristics for par-
allel and incremental phrase pair extraction. The
experiment results on three different data sets in-
dicate the effectiveness of our approach.
In future work, we will also introduce incre-
mental learning for phase pair extraction inside a
domain, which means using the current translation
probabilities already obtained as the base measure
of sampling parameters for the upcoming domain.
Furthermore, we will investigate any tradeoffs be-
tween the accuracy of the probability estimation
and the coverage of phrase pairs.
</bodyText>
<sectionHeader confidence="0.99856" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999697888888889">
We would like to thank our colleagues in both
HIT and NICT for insightful discussions, and
three anonymous reviewers for many invaluable
comments and suggestions to improve our paper.
This work is supported by National Natural Sci-
ence Foundation of China (61100093, 61173073,
61073130, 61272384), and the Key Project of the
National High Technology Research and Develop-
ment Program of China (2011AA01A207).
</bodyText>
<sectionHeader confidence="0.998006" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.936271962025316">
Phil Blunsom and Trevor Cohn. 2010. Inducing syn-
chronous grammars with slice sampling. In Human
Language Technologies: The 2010 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 238–241,
Los Angeles, California, June. Association for Com-
putational Linguistics.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statisti-
cal machine translation. In Proceedings of ACL,
pages 200–208, Columbus, Ohio, June. Association
for Computational Linguistics.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proceedings of
the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computation-
al Natural Language Learning (EMNLP-CoNLL),
pages 858–867, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
427–436, Montr´eal, Canada, June. Association for
Computational Linguistics.
Colin Cherry and Dekang Lin. 2007. Inversion
transduction grammar for joint phrasal translation
modeling. In Proceedings of SSST, NAACL-HLT
2007/AMTA Workshop on Syntax and Structure in
Statistical Translation, pages 17–24.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, ACL ’05, pages 263–
270, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing for
statistical machine translation: controlling for opti-
mizer instability. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies: short pa-
pers - Volume 2, HLT ’11, pages 176–181, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
John DeNero and Dan Klein. 2008. The complexi-
ty of phrase alignment problems. In Proceedings of
ACL-08: HLT, Short Papers, pages 25–28, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
George Foster and Roland Kuhn. 2007. Mixture-
model adaptation for smt. In Proceedings of the Sec-
ond Workshop on Statistical Machine Translation,
pages 128–135.
Jes´us Gonz´alez-Rubio, Daniel Ortiz-Martinez, and
Francisco Casacuberta. 2011. Fast incremental ac-
tive learning for statistical machine translation. A-
VANCES EN INTELIGENCIA ARTIFICIAL.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of HLT-NAACL, pages 45–54.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertol-
di, Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ’07, pages 177–180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Abby Levenberg and Miles Osborne. 2009. Stream-
based randomised language models for smt. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing: Volume 2-
Volume 2, pages 756–764. Association for Compu-
tational Linguistics.
</reference>
<page confidence="0.993364">
809
</page>
<reference confidence="0.999908254901961">
Abby Levenberg, Chris Callison-Burch, and Miles Os-
borne. 2010. Stream-based translation models
for statistical machine translation. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, HLT ’10, pages 394–
402, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Abby Levenberg, Miles Osborne, and David Matthews.
2011. Multiple-stream language models for statisti-
cal machine translation. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
177–186, Edinburgh, Scotland, July. Association for
Computational Linguistics.
Zhiyuan Liu, Yuzhou Zhang, Edward Y Chang, and
Maosong Sun. 2011. Plda+: Parallel latent dirichlet
allocation with data placement and pipeline process-
ing. ACM Transactions on Intelligent Systems and
Technology (TIST), 2(3):1–18.
Yajuan Lu, Jin Huang, and Qun Liu. 2007. Improving
statistical machine translation performance by train-
ing data selection and optimization. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computation-
al Natural Language Learning (EMNLP-CoNLL),
pages 343–350, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.
Graham Neubig, Taro Watanabe, Eiichiro Sumita,
Shinsuke Mori, and Tatsuya Kawahara. 2011. An
unsupervised model for joint phrase alignment and
extraction. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistic-
s: Human Language Technologies, pages 632–641,
Portland, Oregon, USA, June. Association for Com-
putational Linguistics.
Graham Neubig, Taro Watanabe, Shinsuke Mori, and
Tatsuya Kawahara. 2012. Machine translation with-
out words through substring alignment. In Proceed-
ings of the 50th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 165–174, Jeju Island, Korea, July. As-
sociation for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A systemat-
ic comparison of various statistical alignment mod-
els. Computational linguistics, 29(1):19–51.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Comput. Linguist., 30(4):417–449, Decem-
ber.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic e-
valuation of machine translation. In Proceedings of
40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311–318, Philadelphia,
Pennsylvania, USA, July. Association for Computa-
tional Linguistics.
Jim Pitman and Marc Yor. 1997. The two-parameter
poisson-dirichlet distribution derived from a stable
subordinator. The Annals of Probability, 25(2):855–
900.
Holger Schwenk and Philipp Koehn. 2008. Large
and diverse language models for statistical machine
translation. In International Joint Conference on
Natural Language Processing, pages 661–668.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In Proc. of ICSLP.
Jinsong Su, Hua Wu, Haifeng Wang, Yidong Chen, X-
iaodong Shi, Huailin Dong, and Qun Liu. 2012.
Translation model adaptation for statistical machine
translation with monolingual topic information. In
Proceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 459–468.
Yee Whye Teh. 2006. A hierarchical bayesian lan-
guage model based on pitman-yor processes. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 985–992. Association for Computa-
tional Linguistics.
Wei Wang, Klaus Macherey, Wolfgang Macherey,
Franz Och, and Peng Xu. 2012. Improved do-
main adaptation for statistical machine translation.
In Proceedings of the Conference of the Association
for Machine translation, Americas.
F. Wood and Y. W. Teh. 2009. A hierarchical non-
parametric Bayesian approach to statistical language
model domain adaptation. In Proceedings of the In-
ternational Conference on Artificial Intelligence and
Statistics, volume 12.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational linguistics, 23(3):377–403.
Jia Xu, Yonggang Deng, Yuqing Gao, and Hermann
Ney. 2007. Domain dependent statistical machine
translation. In Proceedings of the MT Summit XI.
Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous parsing.
In Proceedings of ACL-08: HLT, pages 97–105,
Columbus, Ohio, June. Association for Computa-
tional Linguistics.
</reference>
<page confidence="0.99761">
810
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.796926">
<title confidence="0.999974">Hierarchical Phrase Table Combination for Machine Translation</title>
<author confidence="0.998294">Taro Eiichiro Tiejun</author>
<affiliation confidence="0.998502333333333">of Computer Science and Harbin Institute of Technology (HIT), Harbin, Institute of Information and Communication</affiliation>
<address confidence="0.809171">3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto,</address>
<abstract confidence="0.999475043478261">Typical statistical machine translation systems are batch trained with a given training data and their performances are largely influenced by the amount of data. With the growth of the available data across different domains, it is computationally demanding to perform batch training every time when new data comes. In face of the problem, we propose an efficient phrase table combination method. In particular, we train a Bayesian phrasal inversion transduction grammars for each domain separately. The learned phrase tables are hierarchically combined as if they are drawn from a hierarchical Pitman-Yor process. The performance measured by BLEU is at least as comparable to the traditional batch training method. Furthermore, each phrase table is trained separately in each domain, and while computational overhead is significantly reduced by training them in parallel.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
</authors>
<title>Inducing synchronous grammars with slice sampling.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>238--241</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California,</location>
<contexts>
<context position="8009" citStr="Blunsom and Cohn, 2010" startWordPosition="1253" endWordPosition="1256">almost comparable translation performance with less computational overhead (Levenberg et al., 2010; Gonz´alezRubio et al., 2011). However, their methods usually require numbers of hyperparameters, such as mini-batch size, step size, or human judgment to determine the quality of phrases, and still rely on a heuristic phrase extraction method in each phrase table update. 3 Phrase Pair Extraction with Unsupervised Phrasal ITGs Recently, phrase alignment with ITGs (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2008) and parameter estimation with Gibbs sampling (DeNero and Klein, 2008; Blunsom and Cohn, 2010) are popular. Here, we employ a method proposed by Neubig et al. (2011), which uses parametric Bayesian inference with the phrasal ITGs (Wu, 1997). It can achieve comparable translation accuracy with a much smaller phrase table than the traditional GIZA++ and heuristic phrase extraction methods. It has also been proved successful in adjusting the phrase length granularity by applying character-based SMT with more sophisticated inference (Neubig et al., 2012). ITG is a synchronous grammar formalism which analyzes bilingual text by introducing inverted rules, and each ITG derivation corresponds </context>
</contexts>
<marker>Blunsom, Cohn, 2010</marker>
<rawString>Phil Blunsom and Trevor Cohn. 2010. Inducing synchronous grammars with slice sampling. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 238–241, Los Angeles, California, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
<author>Miles Osborne</author>
</authors>
<title>A discriminative latent variable model for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>200--208</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="7915" citStr="Blunsom et al., 2008" startWordPosition="1238" endWordPosition="1241"> online EM algorithm and active learning are applied to phrase pair extraction and achieves almost comparable translation performance with less computational overhead (Levenberg et al., 2010; Gonz´alezRubio et al., 2011). However, their methods usually require numbers of hyperparameters, such as mini-batch size, step size, or human judgment to determine the quality of phrases, and still rely on a heuristic phrase extraction method in each phrase table update. 3 Phrase Pair Extraction with Unsupervised Phrasal ITGs Recently, phrase alignment with ITGs (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2008) and parameter estimation with Gibbs sampling (DeNero and Klein, 2008; Blunsom and Cohn, 2010) are popular. Here, we employ a method proposed by Neubig et al. (2011), which uses parametric Bayesian inference with the phrasal ITGs (Wu, 1997). It can achieve comparable translation accuracy with a much smaller phrase table than the traditional GIZA++ and heuristic phrase extraction methods. It has also been proved successful in adjusting the phrase length granularity by applying character-based SMT with more sophisticated inference (Neubig et al., 2012). ITG is a synchronous grammar formalism whi</context>
</contexts>
<marker>Blunsom, Cohn, Osborne, 2008</marker>
<rawString>Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008. A discriminative latent variable model for statistical machine translation. In Proceedings of ACL, pages 200–208, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Ashok C Popat</author>
<author>Peng Xu</author>
<author>Franz J Och</author>
<author>Jeffrey Dean</author>
</authors>
<title>Large language models in machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>858--867</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="5209" citStr="Brants et al., 2007" startWordPosition="803" endWordPosition="806"> Related Work Bilingual phrases are cornerstones for phrasebased SMT systems (Och and Ney, 2004; Koehn et al., 2003; Chiang, 2005) and existing translation systems often get ‘crowd-sourced’ improvements (Levenberg et al., 2010). A number of approaches have been proposed to make use of the full potential of the available parallel sentences from various domains, such as domain adaptation and incremental learning for SMT. The translation model and language model are primary components in SMT. Previous work proved successful in the use of large-scale data for language models from diverse domains (Brants et al., 2007; Schwenk and Koehn, 2008). Alternatively, the language model is incrementally updated by using a succinct data structure with a interpolation technique (Levenberg and Osborne, 2009; Levenberg et al., 2011). In the case of the previous work on translation modeling, mixed methods have been investigated for domain adaptation in SMT by adding domain information as additional labels to the original phrase table (Foster and Kuhn, 2007). Under this framework, the training data is first divided into several parts, and phase pairs are extracted with some sub-domain features. Then all the phrase pairs </context>
</contexts>
<marker>Brants, Popat, Xu, Och, Dean, 2007</marker>
<rawString>Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och, and Jeffrey Dean. 2007. Large language models in machine translation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 858–867, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>George Foster</author>
</authors>
<title>Batch tuning strategies for statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>427--436</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="21109" citStr="Cherry and Foster, 2012" startWordPosition="3533" endWordPosition="3537"> IWSLT data consists of roughly 2, 000 sentences and 3, 000 sentences each from the HIT and BTEC for development purposes, and the test data consists of 1, 000 sentences. For the FBIS and LDC task, we used NIST MT 2002 and 2004 for development and testing purposes, consisting of 878 and 1, 788 sentences respectively. We employ Moses, an open-source toolkit for our experiment (Koehn et al., 2007). SRILM Toolkit (Stolcke, 2002) is employed to train 4-gram language models on the Xinhua portion of Gigaword corpus, while for the IWLST2012 data set, only its training set is used. We use batch-MIRA (Cherry and Foster, 2012) to tune the weight for each feature and translation quality is evaluated by the case-insensitive BLEU-4 metric (Papineni et al., 2002). The BLEU scores reported in this paper are the average of 5 independent runs of independent batch-MIRA weight training, as suggested by (Clark et al., 2011). 5.2 Result and Analysis 5.2.1 Performances of various extraction methods We carry out a series of experiments to evaluate translation performance. The results are listed in Table 2. Our method significantly outperforms the baseline Pialign-linear. Except for the translation probabilities, the phrase pair</context>
</contexts>
<marker>Cherry, Foster, 2012</marker>
<rawString>Colin Cherry and George Foster. 2012. Batch tuning strategies for statistical machine translation. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 427–436, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>Dekang Lin</author>
</authors>
<title>Inversion transduction grammar for joint phrasal translation modeling.</title>
<date>2007</date>
<booktitle>In Proceedings of SSST, NAACL-HLT 2007/AMTA Workshop on Syntax and Structure in Statistical Translation,</booktitle>
<pages>17--24</pages>
<contexts>
<context position="7872" citStr="Cherry and Lin, 2007" startWordPosition="1230" endWordPosition="1233">tional frequent batch oriented methods, an online EM algorithm and active learning are applied to phrase pair extraction and achieves almost comparable translation performance with less computational overhead (Levenberg et al., 2010; Gonz´alezRubio et al., 2011). However, their methods usually require numbers of hyperparameters, such as mini-batch size, step size, or human judgment to determine the quality of phrases, and still rely on a heuristic phrase extraction method in each phrase table update. 3 Phrase Pair Extraction with Unsupervised Phrasal ITGs Recently, phrase alignment with ITGs (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2008) and parameter estimation with Gibbs sampling (DeNero and Klein, 2008; Blunsom and Cohn, 2010) are popular. Here, we employ a method proposed by Neubig et al. (2011), which uses parametric Bayesian inference with the phrasal ITGs (Wu, 1997). It can achieve comparable translation accuracy with a much smaller phrase table than the traditional GIZA++ and heuristic phrase extraction methods. It has also been proved successful in adjusting the phrase length granularity by applying character-based SMT with more sophisticated inference (Neubig et al., 2012).</context>
</contexts>
<marker>Cherry, Lin, 2007</marker>
<rawString>Colin Cherry and Dekang Lin. 2007. Inversion transduction grammar for joint phrasal translation modeling. In Proceedings of SSST, NAACL-HLT 2007/AMTA Workshop on Syntax and Structure in Statistical Translation, pages 17–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05,</booktitle>
<pages>263--270</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4720" citStr="Chiang, 2005" startWordPosition="728" endWordPosition="729">tion for Computational Linguistics achieve at least comparable results to batch training methods, with a significantly less computational overhead. The rest of the paper is organized as follows. In Section 2, we introduce related work. In section 3, we briefly describe the translation model with phrasal ITGs and Pitman-Yor process. In section 4, we explain our hierarchical combination approach and give experiment results in section 5. We conclude the paper in the last section. 2 Related Work Bilingual phrases are cornerstones for phrasebased SMT systems (Och and Ney, 2004; Koehn et al., 2003; Chiang, 2005) and existing translation systems often get ‘crowd-sourced’ improvements (Levenberg et al., 2010). A number of approaches have been proposed to make use of the full potential of the available parallel sentences from various domains, such as domain adaptation and incremental learning for SMT. The translation model and language model are primary components in SMT. Previous work proved successful in the use of large-scale data for language models from diverse domains (Brants et al., 2007; Schwenk and Koehn, 2008). Alternatively, the language model is incrementally updated by using a succinct data</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05, pages 263– 270, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jonathan H Clark</author>
<author>Chris Dyer</author>
<author>Alon Lavie</author>
<author>Noah A Smith</author>
</authors>
<title>Better hypothesis testing for statistical machine translation: controlling for optimizer instability.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers - Volume 2, HLT ’11,</booktitle>
<pages>176--181</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="21402" citStr="Clark et al., 2011" startWordPosition="3582" endWordPosition="3585">tences respectively. We employ Moses, an open-source toolkit for our experiment (Koehn et al., 2007). SRILM Toolkit (Stolcke, 2002) is employed to train 4-gram language models on the Xinhua portion of Gigaword corpus, while for the IWLST2012 data set, only its training set is used. We use batch-MIRA (Cherry and Foster, 2012) to tune the weight for each feature and translation quality is evaluated by the case-insensitive BLEU-4 metric (Papineni et al., 2002). The BLEU scores reported in this paper are the average of 5 independent runs of independent batch-MIRA weight training, as suggested by (Clark et al., 2011). 5.2 Result and Analysis 5.2.1 Performances of various extraction methods We carry out a series of experiments to evaluate translation performance. The results are listed in Table 2. Our method significantly outperforms the baseline Pialign-linear. Except for the translation probabilities, the phrase pairs of two methods are exactly same, so the number of phrase pairs are equal in the two methods. Further more, the performance of the baseline Pialign-adaptive is also higher than the baseline Pialign-linear’s and lower than ours. This proves that the adaptive method ˜f) O(˜e, tf| �= O(˜e|tf, t</context>
</contexts>
<marker>Clark, Dyer, Lavie, Smith, 2011</marker>
<rawString>Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A. Smith. 2011. Better hypothesis testing for statistical machine translation: controlling for optimizer instability. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers - Volume 2, HLT ’11, pages 176–181, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>The complexity of phrase alignment problems.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT, Short Papers,</booktitle>
<pages>25--28</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="7984" citStr="DeNero and Klein, 2008" startWordPosition="1249" endWordPosition="1252">extraction and achieves almost comparable translation performance with less computational overhead (Levenberg et al., 2010; Gonz´alezRubio et al., 2011). However, their methods usually require numbers of hyperparameters, such as mini-batch size, step size, or human judgment to determine the quality of phrases, and still rely on a heuristic phrase extraction method in each phrase table update. 3 Phrase Pair Extraction with Unsupervised Phrasal ITGs Recently, phrase alignment with ITGs (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2008) and parameter estimation with Gibbs sampling (DeNero and Klein, 2008; Blunsom and Cohn, 2010) are popular. Here, we employ a method proposed by Neubig et al. (2011), which uses parametric Bayesian inference with the phrasal ITGs (Wu, 1997). It can achieve comparable translation accuracy with a much smaller phrase table than the traditional GIZA++ and heuristic phrase extraction methods. It has also been proved successful in adjusting the phrase length granularity by applying character-based SMT with more sophisticated inference (Neubig et al., 2012). ITG is a synchronous grammar formalism which analyzes bilingual text by introducing inverted rules, and each IT</context>
</contexts>
<marker>DeNero, Klein, 2008</marker>
<rawString>John DeNero and Dan Klein. 2008. The complexity of phrase alignment problems. In Proceedings of ACL-08: HLT, Short Papers, pages 25–28, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Roland Kuhn</author>
</authors>
<title>Mixturemodel adaptation for smt.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>128--135</pages>
<contexts>
<context position="5643" citStr="Foster and Kuhn, 2007" startWordPosition="873" endWordPosition="876">slation model and language model are primary components in SMT. Previous work proved successful in the use of large-scale data for language models from diverse domains (Brants et al., 2007; Schwenk and Koehn, 2008). Alternatively, the language model is incrementally updated by using a succinct data structure with a interpolation technique (Levenberg and Osborne, 2009; Levenberg et al., 2011). In the case of the previous work on translation modeling, mixed methods have been investigated for domain adaptation in SMT by adding domain information as additional labels to the original phrase table (Foster and Kuhn, 2007). Under this framework, the training data is first divided into several parts, and phase pairs are extracted with some sub-domain features. Then all the phrase pairs and features are tuned together with different weights during decoding. As a way to choose the right domain for the domain adaption, a classifier-based method and a feature-based method have been proposed. Classification-based methods must at least add an explicit label to indicate which domain the current phrase pair comes from. This is traditionally done with an automatic domain classifier, and each input sentence is classified </context>
</contexts>
<marker>Foster, Kuhn, 2007</marker>
<rawString>George Foster and Roland Kuhn. 2007. Mixturemodel adaptation for smt. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 128–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gonz´alez-Rubio</author>
<author>Daniel Ortiz-Martinez</author>
<author>Francisco Casacuberta</author>
</authors>
<title>Fast incremental active learning for statistical machine translation.</title>
<date>2011</date>
<journal>AVANCES EN INTELIGENCIA ARTIFICIAL.</journal>
<marker>Gonz´alez-Rubio, Ortiz-Martinez, Casacuberta, 2011</marker>
<rawString>Jes´us Gonz´alez-Rubio, Daniel Ortiz-Martinez, and Francisco Casacuberta. 2011. Fast incremental active learning for statistical machine translation. AVANCES EN INTELIGENCIA ARTIFICIAL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proc. of HLT-NAACL,</booktitle>
<pages>45--54</pages>
<contexts>
<context position="4705" citStr="Koehn et al., 2003" startWordPosition="724" endWordPosition="727">2013. c�2013 Association for Computational Linguistics achieve at least comparable results to batch training methods, with a significantly less computational overhead. The rest of the paper is organized as follows. In Section 2, we introduce related work. In section 3, we briefly describe the translation model with phrasal ITGs and Pitman-Yor process. In section 4, we explain our hierarchical combination approach and give experiment results in section 5. We conclude the paper in the last section. 2 Related Work Bilingual phrases are cornerstones for phrasebased SMT systems (Och and Ney, 2004; Koehn et al., 2003; Chiang, 2005) and existing translation systems often get ‘crowd-sourced’ improvements (Levenberg et al., 2010). A number of approaches have been proposed to make use of the full potential of the available parallel sentences from various domains, such as domain adaptation and incremental learning for SMT. The translation model and language model are primary components in SMT. Previous work proved successful in the use of large-scale data for language models from diverse domains (Brants et al., 2007; Schwenk and Koehn, 2008). Alternatively, the language model is incrementally updated by using </context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proc. of HLT-NAACL, pages 45–54.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondˇrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL ’07,</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="20883" citStr="Koehn et al., 2007" startWordPosition="3494" endWordPosition="3497">d the other features are also linearly combined by averaging the feature values. Pialign is used with default parameters. The parameter ’samps’ is set to 5, which indicates 5 samples are generated for a sentence pair. The IWSLT data consists of roughly 2, 000 sentences and 3, 000 sentences each from the HIT and BTEC for development purposes, and the test data consists of 1, 000 sentences. For the FBIS and LDC task, we used NIST MT 2002 and 2004 for development and testing purposes, consisting of 878 and 1, 788 sentences respectively. We employ Moses, an open-source toolkit for our experiment (Koehn et al., 2007). SRILM Toolkit (Stolcke, 2002) is employed to train 4-gram language models on the Xinhua portion of Gigaword corpus, while for the IWLST2012 data set, only its training set is used. We use batch-MIRA (Cherry and Foster, 2012) to tune the weight for each feature and translation quality is evaluated by the case-insensitive BLEU-4 metric (Papineni et al., 2002). The BLEU scores reported in this paper are the average of 5 independent runs of independent batch-MIRA weight training, as suggested by (Clark et al., 2011). 5.2 Result and Analysis 5.2.1 Performances of various extraction methods We car</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL ’07, pages 177–180, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abby Levenberg</author>
<author>Miles Osborne</author>
</authors>
<title>Streambased randomised language models for smt.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume</booktitle>
<volume>2</volume>
<pages>756--764</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5390" citStr="Levenberg and Osborne, 2009" startWordPosition="830" endWordPosition="833"> ‘crowd-sourced’ improvements (Levenberg et al., 2010). A number of approaches have been proposed to make use of the full potential of the available parallel sentences from various domains, such as domain adaptation and incremental learning for SMT. The translation model and language model are primary components in SMT. Previous work proved successful in the use of large-scale data for language models from diverse domains (Brants et al., 2007; Schwenk and Koehn, 2008). Alternatively, the language model is incrementally updated by using a succinct data structure with a interpolation technique (Levenberg and Osborne, 2009; Levenberg et al., 2011). In the case of the previous work on translation modeling, mixed methods have been investigated for domain adaptation in SMT by adding domain information as additional labels to the original phrase table (Foster and Kuhn, 2007). Under this framework, the training data is first divided into several parts, and phase pairs are extracted with some sub-domain features. Then all the phrase pairs and features are tuned together with different weights during decoding. As a way to choose the right domain for the domain adaption, a classifier-based method and a feature-based me</context>
</contexts>
<marker>Levenberg, Osborne, 2009</marker>
<rawString>Abby Levenberg and Miles Osborne. 2009. Streambased randomised language models for smt. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2-Volume 2, pages 756–764. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abby Levenberg</author>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
</authors>
<title>Stream-based translation models for statistical machine translation.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10,</booktitle>
<pages>394--402</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4817" citStr="Levenberg et al., 2010" startWordPosition="738" endWordPosition="741">g methods, with a significantly less computational overhead. The rest of the paper is organized as follows. In Section 2, we introduce related work. In section 3, we briefly describe the translation model with phrasal ITGs and Pitman-Yor process. In section 4, we explain our hierarchical combination approach and give experiment results in section 5. We conclude the paper in the last section. 2 Related Work Bilingual phrases are cornerstones for phrasebased SMT systems (Och and Ney, 2004; Koehn et al., 2003; Chiang, 2005) and existing translation systems often get ‘crowd-sourced’ improvements (Levenberg et al., 2010). A number of approaches have been proposed to make use of the full potential of the available parallel sentences from various domains, such as domain adaptation and incremental learning for SMT. The translation model and language model are primary components in SMT. Previous work proved successful in the use of large-scale data for language models from diverse domains (Brants et al., 2007; Schwenk and Koehn, 2008). Alternatively, the language model is incrementally updated by using a succinct data structure with a interpolation technique (Levenberg and Osborne, 2009; Levenberg et al., 2011). </context>
<context position="7484" citStr="Levenberg et al., 2010" startWordPosition="1170" endWordPosition="1173">lassifier-based or featurebased method, the performance of current domain adaptive phrase extraction methods is more sensitive to the development set selection. Usually the domain similar to a given development data is usually assigned higher weights. Incremental learning in which new parallel sentences are incrementally updated to the training data is employed for SMT. Compared to traditional frequent batch oriented methods, an online EM algorithm and active learning are applied to phrase pair extraction and achieves almost comparable translation performance with less computational overhead (Levenberg et al., 2010; Gonz´alezRubio et al., 2011). However, their methods usually require numbers of hyperparameters, such as mini-batch size, step size, or human judgment to determine the quality of phrases, and still rely on a heuristic phrase extraction method in each phrase table update. 3 Phrase Pair Extraction with Unsupervised Phrasal ITGs Recently, phrase alignment with ITGs (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2008) and parameter estimation with Gibbs sampling (DeNero and Klein, 2008; Blunsom and Cohn, 2010) are popular. Here, we employ a method proposed by Neubig et al. (2011), wh</context>
</contexts>
<marker>Levenberg, Callison-Burch, Osborne, 2010</marker>
<rawString>Abby Levenberg, Chris Callison-Burch, and Miles Osborne. 2010. Stream-based translation models for statistical machine translation. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10, pages 394– 402, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abby Levenberg</author>
<author>Miles Osborne</author>
<author>David Matthews</author>
</authors>
<title>Multiple-stream language models for statistical machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>177--186</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland,</location>
<contexts>
<context position="5415" citStr="Levenberg et al., 2011" startWordPosition="834" endWordPosition="837"> (Levenberg et al., 2010). A number of approaches have been proposed to make use of the full potential of the available parallel sentences from various domains, such as domain adaptation and incremental learning for SMT. The translation model and language model are primary components in SMT. Previous work proved successful in the use of large-scale data for language models from diverse domains (Brants et al., 2007; Schwenk and Koehn, 2008). Alternatively, the language model is incrementally updated by using a succinct data structure with a interpolation technique (Levenberg and Osborne, 2009; Levenberg et al., 2011). In the case of the previous work on translation modeling, mixed methods have been investigated for domain adaptation in SMT by adding domain information as additional labels to the original phrase table (Foster and Kuhn, 2007). Under this framework, the training data is first divided into several parts, and phase pairs are extracted with some sub-domain features. Then all the phrase pairs and features are tuned together with different weights during decoding. As a way to choose the right domain for the domain adaption, a classifier-based method and a feature-based method have been proposed. </context>
</contexts>
<marker>Levenberg, Osborne, Matthews, 2011</marker>
<rawString>Abby Levenberg, Miles Osborne, and David Matthews. 2011. Multiple-stream language models for statistical machine translation. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 177–186, Edinburgh, Scotland, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhiyuan Liu</author>
<author>Yuzhou Zhang</author>
<author>Edward Y Chang</author>
<author>Maosong Sun</author>
</authors>
<title>Plda+: Parallel latent dirichlet allocation with data placement and pipeline processing.</title>
<date>2011</date>
<booktitle>ACM Transactions on Intelligent Systems and Technology (TIST),</booktitle>
<pages>2--3</pages>
<contexts>
<context position="17523" citStr="Liu et al., 2011" startWordPosition="2971" endWordPosition="2975">xperiment Setup The first data set comes from the IWSLT2012 OLYMPICS task consisting of two training sets: the HIT corpus, which is closely related to the Beijing 2008 Olympic Games, and the BTEC corpus, which is a multilingual speech corpus containing tourism-related sentences. The second data set, the FBIS corpus, is a collection of news articles and does not have domain information itself, so a Latent Dirichlet Allocation (LDA) tool, PLDA1, is used to divide the whole corpus into 5 different sub-domains according to the concatenation of the source side and target side as a single sentence (Liu et al., 2011). The third data set is composed of 5 corpora2 from LDC with various domains, including news, magazine, and finance. The details are shown in Table 1. In order to evaluate our approach, four phrase pair extraction methods are performed: 1. GIZA-linear: Phase pairs are extracted in each domain by GIZA++ (Och and Ney, 2003) and the ”grow-diag-final-and” method with a maximum length 7. The phrase tables from various domains are linearly combined by averaging the feature values. 2. Pialign-linear: Similar to GIZA-linear, but we employed the phrasal ITG method described in Section 3 using the piali</context>
</contexts>
<marker>Liu, Zhang, Chang, Sun, 2011</marker>
<rawString>Zhiyuan Liu, Yuzhou Zhang, Edward Y Chang, and Maosong Sun. 2011. Plda+: Parallel latent dirichlet allocation with data placement and pipeline processing. ACM Transactions on Intelligent Systems and Technology (TIST), 2(3):1–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yajuan Lu</author>
<author>Jin Huang</author>
<author>Qun Liu</author>
</authors>
<title>Improving statistical machine translation performance by training data selection and optimization.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>343--350</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="6670" citStr="Lu et al., 2007" startWordPosition="1044" endWordPosition="1047">ast add an explicit label to indicate which domain the current phrase pair comes from. This is traditionally done with an automatic domain classifier, and each input sentence is classified into its corresponding domain (Xu et al., 2007). As an alternative to the classification-based approach, Wang et al. (2012) employed a featurebased approach, in which phrase pairs are enriched by a feature set to potentially reflect the domain information. The similarity calculated by a information retrieval system between the training subset and the test set is used as a feature for each parallel sentence (Lu et al., 2007). Monolingual topic information is taken as a new feature for a domain adaptive translation model and tuned on the development set (Su et al., 2012). Regardless of underlying methods, either classifier-based or featurebased method, the performance of current domain adaptive phrase extraction methods is more sensitive to the development set selection. Usually the domain similar to a given development data is usually assigned higher weights. Incremental learning in which new parallel sentences are incrementally updated to the training data is employed for SMT. Compared to traditional frequent ba</context>
</contexts>
<marker>Lu, Huang, Liu, 2007</marker>
<rawString>Yajuan Lu, Jin Huang, and Qun Liu. 2007. Improving statistical machine translation performance by training data selection and optimization. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 343–350, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Neubig</author>
<author>Taro Watanabe</author>
<author>Eiichiro Sumita</author>
<author>Shinsuke Mori</author>
<author>Tatsuya Kawahara</author>
</authors>
<title>An unsupervised model for joint phrase alignment and extraction.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>632--641</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="3301" citStr="Neubig et al. (2011)" startWordPosition="499" endWordPosition="502"> bias (Wang et al., 2012). As an alternative, incremental learning may resolve the gap by incrementally adding data sentence-by-sentence into the training data. Since SMT systems trend to employ very large scale training data for translation knowledge extraction, updating several sentence pairs each time will be annihilated in the existing corpus. This paper proposes a new phrase table combination method. First, phrase pairs are extracted from each domain without interfering with other domains. In particular, we employ the nonparametric Bayesian phrasal inversion transduction grammar (ITG) of Neubig et al. (2011) to perform phrase table extraction. Second, extracted phrase tables are combined as if they are drawn from a hierarchical Pitman-Yor process, in which the phrase tables represented as tables in the Chinese restaurant process (CRP) are hierarchically chained by treating each of the previously learned phrase tables as prior to the current one. Thus, we can easily update the chain of phrase tables by appending the newly extracted phrase table and by treating the chain of the previous ones as its prior. Experiment results indicate that our method can achieve better translation performance when th</context>
<context position="8080" citStr="Neubig et al. (2011)" startWordPosition="1267" endWordPosition="1270"> (Levenberg et al., 2010; Gonz´alezRubio et al., 2011). However, their methods usually require numbers of hyperparameters, such as mini-batch size, step size, or human judgment to determine the quality of phrases, and still rely on a heuristic phrase extraction method in each phrase table update. 3 Phrase Pair Extraction with Unsupervised Phrasal ITGs Recently, phrase alignment with ITGs (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2008) and parameter estimation with Gibbs sampling (DeNero and Klein, 2008; Blunsom and Cohn, 2010) are popular. Here, we employ a method proposed by Neubig et al. (2011), which uses parametric Bayesian inference with the phrasal ITGs (Wu, 1997). It can achieve comparable translation accuracy with a much smaller phrase table than the traditional GIZA++ and heuristic phrase extraction methods. It has also been proved successful in adjusting the phrase length granularity by applying character-based SMT with more sophisticated inference (Neubig et al., 2012). ITG is a synchronous grammar formalism which analyzes bilingual text by introducing inverted rules, and each ITG derivation corresponds to the alignment of a sentence pair (Wu, 1997). Translation probabiliti</context>
<context position="9953" citStr="Neubig et al., 2011" startWordPosition="1599" endWordPosition="1602">obability for a phrase pair found in a bilingual corpus (E, F) can be represented by the following equation using the Chinese restaurant process (Teh, 2006): 1 P �(ei, fi); (E, F)) = C + s(ci — d x ti)+ where 1. ci and ti are the customer and table count of the ith phrase pair (ei, fi) found in a bilingual corpus (E, F); 2. C and T are the total customer and table count in corpus (E, F); 3. d and s are the discount and strengthen hyperparameters. The prior probability Pdac is recursively defined by breaking a longer phrase pair into two through the recursive ITG’s generative story as follows (Neubig et al., 2011): 1. Generate symbol x from Px(x; θx) with three possible values: Base, REG, or INV. 2. Depending on the value of x take the following actions. a. If x = Base, generate a new phrase pair directly from Pbase. b. If x = REG, generate (e1, f1) and (e2, f2) from P((e, f); θx, θt), and concatenate them into a single phrase pair (e1e2, f1f2). Figure 1: A word alignment (a), and its hierarchical derivation (b). c. If x = INV, follow a similar process as b, but concatenate f1 and f2 in reverse order (e1e2, f2f1). Note that the Pdac is recursively defined through the binary branched P, which in turns e</context>
</contexts>
<marker>Neubig, Watanabe, Sumita, Mori, Kawahara, 2011</marker>
<rawString>Graham Neubig, Taro Watanabe, Eiichiro Sumita, Shinsuke Mori, and Tatsuya Kawahara. 2011. An unsupervised model for joint phrase alignment and extraction. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 632–641, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Neubig</author>
<author>Taro Watanabe</author>
<author>Shinsuke Mori</author>
<author>Tatsuya Kawahara</author>
</authors>
<title>Machine translation without words through substring alignment.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>165--174</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="8471" citStr="Neubig et al., 2012" startWordPosition="1326" endWordPosition="1329">(Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2008) and parameter estimation with Gibbs sampling (DeNero and Klein, 2008; Blunsom and Cohn, 2010) are popular. Here, we employ a method proposed by Neubig et al. (2011), which uses parametric Bayesian inference with the phrasal ITGs (Wu, 1997). It can achieve comparable translation accuracy with a much smaller phrase table than the traditional GIZA++ and heuristic phrase extraction methods. It has also been proved successful in adjusting the phrase length granularity by applying character-based SMT with more sophisticated inference (Neubig et al., 2012). ITG is a synchronous grammar formalism which analyzes bilingual text by introducing inverted rules, and each ITG derivation corresponds to the alignment of a sentence pair (Wu, 1997). Translation probabilities of ITG phrasal align803 ments can be estimated in polynomial time by slightly limiting word reordering (DeNero and Klein, 2008). More formally, P((e, f); θx, θt) are the probability of phrase pairs (e, f), which is parameterized by a phrase pair distribution θt and a symbol distribution θx. θx is a Dirichlet prior, and θt is estimated with the Pitman-Yor process (Pitman and Yor, 1997; </context>
<context position="10869" citStr="Neubig et al., 2012" startWordPosition="1770" endWordPosition="1773">em into a single phrase pair (e1e2, f1f2). Figure 1: A word alignment (a), and its hierarchical derivation (b). c. If x = INV, follow a similar process as b, but concatenate f1 and f2 in reverse order (e1e2, f2f1). Note that the Pdac is recursively defined through the binary branched P, which in turns employs Pdac as a prior probability. Pbase is a base measure defined as a combination of the IBM Models in two directions and the unigram language models in both sides. Inference is carried out by a heuristic beam search based block sampling with an efficient look ahead for a faster convergence (Neubig et al., 2012). Compared to GIZA++ with heuristic phrase extraction, the Bayesian phrasal ITG can achieve competitive accuracy under a smaller phrase table size. Further, the fallback model can incorporate phrases of all granularity by following the ITG’s recursive definition. Figure 1 (b) illustrates an example of the phrasal ITG derivation for word alignment in Figure 1 (a) in which a bilingual sentence pair is recursively divided into two through the recursively defined generative story. 4 Hierarchical Phrase Table Combination We propose a new phrase table combination method, in which individually learne</context>
</contexts>
<marker>Neubig, Watanabe, Mori, Kawahara, 2012</marker>
<rawString>Graham Neubig, Taro Watanabe, Shinsuke Mori, and Tatsuya Kawahara. 2012. Machine translation without words through substring alignment. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 165–174, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational linguistics,</journal>
<pages>29--1</pages>
<contexts>
<context position="17846" citStr="Och and Ney, 2003" startWordPosition="3027" endWordPosition="3030"> collection of news articles and does not have domain information itself, so a Latent Dirichlet Allocation (LDA) tool, PLDA1, is used to divide the whole corpus into 5 different sub-domains according to the concatenation of the source side and target side as a single sentence (Liu et al., 2011). The third data set is composed of 5 corpora2 from LDC with various domains, including news, magazine, and finance. The details are shown in Table 1. In order to evaluate our approach, four phrase pair extraction methods are performed: 1. GIZA-linear: Phase pairs are extracted in each domain by GIZA++ (Och and Ney, 2003) and the ”grow-diag-final-and” method with a maximum length 7. The phrase tables from various domains are linearly combined by averaging the feature values. 2. Pialign-linear: Similar to GIZA-linear, but we employed the phrasal ITG method described in Section 3 using the pialign toolkit 3 (Neubig et 1http://code.google.com/p/plda/ 2In particular, they come from LDC catalog number: LDC2002E18, LDC2002E58, LDC2003E14, LDC2005E47, LDC2006E26, in this order. 3http://www.phontron.com/pialign/ 806 Methods IWSLT FBIS LDC BLEU Size BLEU Size BLEU Size GIZA-linear 19.222 1,200,877 29.342 15,369,028 30.</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Comput. Linguist.,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="4685" citStr="Och and Ney, 2004" startWordPosition="720" endWordPosition="723">lgaria, August 4-9 2013. c�2013 Association for Computational Linguistics achieve at least comparable results to batch training methods, with a significantly less computational overhead. The rest of the paper is organized as follows. In Section 2, we introduce related work. In section 3, we briefly describe the translation model with phrasal ITGs and Pitman-Yor process. In section 4, we explain our hierarchical combination approach and give experiment results in section 5. We conclude the paper in the last section. 2 Related Work Bilingual phrases are cornerstones for phrasebased SMT systems (Och and Ney, 2004; Koehn et al., 2003; Chiang, 2005) and existing translation systems often get ‘crowd-sourced’ improvements (Levenberg et al., 2010). A number of approaches have been proposed to make use of the full potential of the available parallel sentences from various domains, such as domain adaptation and incremental learning for SMT. The translation model and language model are primary components in SMT. Previous work proved successful in the use of large-scale data for language models from diverse domains (Brants et al., 2007; Schwenk and Koehn, 2008). Alternatively, the language model is incremental</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Josef Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Comput. Linguist., 30(4):417–449, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Philadelphia, Pennsylvania, USA,</location>
<contexts>
<context position="21244" citStr="Papineni et al., 2002" startWordPosition="3555" endWordPosition="3558">ata consists of 1, 000 sentences. For the FBIS and LDC task, we used NIST MT 2002 and 2004 for development and testing purposes, consisting of 878 and 1, 788 sentences respectively. We employ Moses, an open-source toolkit for our experiment (Koehn et al., 2007). SRILM Toolkit (Stolcke, 2002) is employed to train 4-gram language models on the Xinhua portion of Gigaword corpus, while for the IWLST2012 data set, only its training set is used. We use batch-MIRA (Cherry and Foster, 2012) to tune the weight for each feature and translation quality is evaluated by the case-insensitive BLEU-4 metric (Papineni et al., 2002). The BLEU scores reported in this paper are the average of 5 independent runs of independent batch-MIRA weight training, as suggested by (Clark et al., 2011). 5.2 Result and Analysis 5.2.1 Performances of various extraction methods We carry out a series of experiments to evaluate translation performance. The results are listed in Table 2. Our method significantly outperforms the baseline Pialign-linear. Except for the translation probabilities, the phrase pairs of two methods are exactly same, so the number of phrase pairs are equal in the two methods. Further more, the performance of the bas</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, Pennsylvania, USA, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jim Pitman</author>
<author>Marc Yor</author>
</authors>
<title>The two-parameter poisson-dirichlet distribution derived from a stable subordinator. The Annals of Probability,</title>
<date>1997</date>
<volume>25</volume>
<issue>2</issue>
<pages>900</pages>
<contexts>
<context position="9069" citStr="Pitman and Yor, 1997" startWordPosition="1428" endWordPosition="1431"> (Neubig et al., 2012). ITG is a synchronous grammar formalism which analyzes bilingual text by introducing inverted rules, and each ITG derivation corresponds to the alignment of a sentence pair (Wu, 1997). Translation probabilities of ITG phrasal align803 ments can be estimated in polynomial time by slightly limiting word reordering (DeNero and Klein, 2008). More formally, P((e, f); θx, θt) are the probability of phrase pairs (e, f), which is parameterized by a phrase pair distribution θt and a symbol distribution θx. θx is a Dirichlet prior, and θt is estimated with the Pitman-Yor process (Pitman and Yor, 1997; Teh, 2006), which is expressed as θt — PY (d, s, Pdac) (1) where d is the discount parameter, s is the strength parameter, and , and Pdac is a prior probability which acts as a fallback probability when a phrase pair is not in the model. Under this model, the probability for a phrase pair found in a bilingual corpus (E, F) can be represented by the following equation using the Chinese restaurant process (Teh, 2006): 1 P �(ei, fi); (E, F)) = C + s(ci — d x ti)+ where 1. ci and ti are the customer and table count of the ith phrase pair (ei, fi) found in a bilingual corpus (E, F); 2. C and T ar</context>
</contexts>
<marker>Pitman, Yor, 1997</marker>
<rawString>Jim Pitman and Marc Yor. 1997. The two-parameter poisson-dirichlet distribution derived from a stable subordinator. The Annals of Probability, 25(2):855– 900.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
<author>Philipp Koehn</author>
</authors>
<title>Large and diverse language models for statistical machine translation.</title>
<date>2008</date>
<booktitle>In International Joint Conference on Natural Language Processing,</booktitle>
<pages>661--668</pages>
<contexts>
<context position="5235" citStr="Schwenk and Koehn, 2008" startWordPosition="807" endWordPosition="810">al phrases are cornerstones for phrasebased SMT systems (Och and Ney, 2004; Koehn et al., 2003; Chiang, 2005) and existing translation systems often get ‘crowd-sourced’ improvements (Levenberg et al., 2010). A number of approaches have been proposed to make use of the full potential of the available parallel sentences from various domains, such as domain adaptation and incremental learning for SMT. The translation model and language model are primary components in SMT. Previous work proved successful in the use of large-scale data for language models from diverse domains (Brants et al., 2007; Schwenk and Koehn, 2008). Alternatively, the language model is incrementally updated by using a succinct data structure with a interpolation technique (Levenberg and Osborne, 2009; Levenberg et al., 2011). In the case of the previous work on translation modeling, mixed methods have been investigated for domain adaptation in SMT by adding domain information as additional labels to the original phrase table (Foster and Kuhn, 2007). Under this framework, the training data is first divided into several parts, and phase pairs are extracted with some sub-domain features. Then all the phrase pairs and features are tuned tog</context>
</contexts>
<marker>Schwenk, Koehn, 2008</marker>
<rawString>Holger Schwenk and Philipp Koehn. 2008. Large and diverse language models for statistical machine translation. In International Joint Conference on Natural Language Processing, pages 661–668.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proc. of ICSLP.</booktitle>
<contexts>
<context position="20914" citStr="Stolcke, 2002" startWordPosition="3500" endWordPosition="3502">ly combined by averaging the feature values. Pialign is used with default parameters. The parameter ’samps’ is set to 5, which indicates 5 samples are generated for a sentence pair. The IWSLT data consists of roughly 2, 000 sentences and 3, 000 sentences each from the HIT and BTEC for development purposes, and the test data consists of 1, 000 sentences. For the FBIS and LDC task, we used NIST MT 2002 and 2004 for development and testing purposes, consisting of 878 and 1, 788 sentences respectively. We employ Moses, an open-source toolkit for our experiment (Koehn et al., 2007). SRILM Toolkit (Stolcke, 2002) is employed to train 4-gram language models on the Xinhua portion of Gigaword corpus, while for the IWLST2012 data set, only its training set is used. We use batch-MIRA (Cherry and Foster, 2012) to tune the weight for each feature and translation quality is evaluated by the case-insensitive BLEU-4 metric (Papineni et al., 2002). The BLEU scores reported in this paper are the average of 5 independent runs of independent batch-MIRA weight training, as suggested by (Clark et al., 2011). 5.2 Result and Analysis 5.2.1 Performances of various extraction methods We carry out a series of experiments </context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. Srilm - an extensible language modeling toolkit. In Proc. of ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinsong Su</author>
<author>Hua Wu</author>
<author>Haifeng Wang</author>
<author>Yidong Chen</author>
<author>Xiaodong Shi</author>
<author>Huailin Dong</author>
<author>Qun Liu</author>
</authors>
<title>Translation model adaptation for statistical machine translation with monolingual topic information.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>459--468</pages>
<contexts>
<context position="6818" citStr="Su et al., 2012" startWordPosition="1070" endWordPosition="1073">r, and each input sentence is classified into its corresponding domain (Xu et al., 2007). As an alternative to the classification-based approach, Wang et al. (2012) employed a featurebased approach, in which phrase pairs are enriched by a feature set to potentially reflect the domain information. The similarity calculated by a information retrieval system between the training subset and the test set is used as a feature for each parallel sentence (Lu et al., 2007). Monolingual topic information is taken as a new feature for a domain adaptive translation model and tuned on the development set (Su et al., 2012). Regardless of underlying methods, either classifier-based or featurebased method, the performance of current domain adaptive phrase extraction methods is more sensitive to the development set selection. Usually the domain similar to a given development data is usually assigned higher weights. Incremental learning in which new parallel sentences are incrementally updated to the training data is employed for SMT. Compared to traditional frequent batch oriented methods, an online EM algorithm and active learning are applied to phrase pair extraction and achieves almost comparable translation pe</context>
<context position="19670" citStr="Su et al., 2012" startWordPosition="3287" endWordPosition="3290">y averaging the feature values. 3. GIZA-batch: Instead of splitting into each domain, the data set is merged as a single corpus and then a heuristic GZA-based phrase extraction is performed, similar as GIZA-linear. 4. Pialign-batch: Similar to the GIZA-batch, a single model is estimated from a single, merged corpus. Since pialign cannot handle large data, we did not experiment on the largest LDC data set. 5. Pialign-adaptive: Alignment and phrase pairs extraction are same to Pialign-batch, while translation probabilities are estimated by the adaptive method with monolingual topic information (Su et al., 2012). The method established the relationship between the out-ofdomain bilingual corpus and in-domain monolingual corpora via topic distribution to estimate the translation probability. O(˜e |˜f) = � tf ˜f) · P(tf|˜f) (5) where O(˜e|tf, ˜f) is the probability of translating into e˜ given the source-side topic f˜ , P(tf|˜f) is f˜ the phrase-topic distribution of f. The method we proposed is named Hiercombin. It extracts phrase pairs in the same way as the Pialign-linear. In the phrase table combination process, the translation probability of each phrase pair is estimated by the Hier-combin and the </context>
</contexts>
<marker>Su, Wu, Wang, Chen, Shi, Dong, Liu, 2012</marker>
<rawString>Jinsong Su, Hua Wu, Haifeng Wang, Yidong Chen, Xiaodong Shi, Huailin Dong, and Qun Liu. 2012. Translation model adaptation for statistical machine translation with monolingual topic information. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 459–468.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
</authors>
<title>A hierarchical bayesian language model based on pitman-yor processes.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>985--992</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9081" citStr="Teh, 2006" startWordPosition="1432" endWordPosition="1433">. ITG is a synchronous grammar formalism which analyzes bilingual text by introducing inverted rules, and each ITG derivation corresponds to the alignment of a sentence pair (Wu, 1997). Translation probabilities of ITG phrasal align803 ments can be estimated in polynomial time by slightly limiting word reordering (DeNero and Klein, 2008). More formally, P((e, f); θx, θt) are the probability of phrase pairs (e, f), which is parameterized by a phrase pair distribution θt and a symbol distribution θx. θx is a Dirichlet prior, and θt is estimated with the Pitman-Yor process (Pitman and Yor, 1997; Teh, 2006), which is expressed as θt — PY (d, s, Pdac) (1) where d is the discount parameter, s is the strength parameter, and , and Pdac is a prior probability which acts as a fallback probability when a phrase pair is not in the model. Under this model, the probability for a phrase pair found in a bilingual corpus (E, F) can be represented by the following equation using the Chinese restaurant process (Teh, 2006): 1 P �(ei, fi); (E, F)) = C + s(ci — d x ti)+ where 1. ci and ti are the customer and table count of the ith phrase pair (ei, fi) found in a bilingual corpus (E, F); 2. C and T are the total </context>
</contexts>
<marker>Teh, 2006</marker>
<rawString>Yee Whye Teh. 2006. A hierarchical bayesian language model based on pitman-yor processes. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 985–992. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Wang</author>
<author>Klaus Macherey</author>
<author>Wolfgang Macherey</author>
<author>Franz Och</author>
<author>Peng Xu</author>
</authors>
<title>Improved domain adaptation for statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the Conference of the Association for Machine translation,</booktitle>
<location>Americas.</location>
<contexts>
<context position="2706" citStr="Wang et al., 2012" startWordPosition="408" endWordPosition="411">on a large data set, and it requires us to re-train every time new training data is available. Even if we can handle the large computation cost, improvement is not guaranteed every time we perform batch tuning on the newly updated training data obtained from divergent domains. Traditional domain adaption methods for SMT are also not adequate in this scenario. Most of them have been proposed in order to make translation systems perform better for resource-scarce domains when most training data comes from resourcerich domains, and ignore performance on a more generic domain without domain bias (Wang et al., 2012). As an alternative, incremental learning may resolve the gap by incrementally adding data sentence-by-sentence into the training data. Since SMT systems trend to employ very large scale training data for translation knowledge extraction, updating several sentence pairs each time will be annihilated in the existing corpus. This paper proposes a new phrase table combination method. First, phrase pairs are extracted from each domain without interfering with other domains. In particular, we employ the nonparametric Bayesian phrasal inversion transduction grammar (ITG) of Neubig et al. (2011) to p</context>
<context position="6366" citStr="Wang et al. (2012)" startWordPosition="991" endWordPosition="994">ted with some sub-domain features. Then all the phrase pairs and features are tuned together with different weights during decoding. As a way to choose the right domain for the domain adaption, a classifier-based method and a feature-based method have been proposed. Classification-based methods must at least add an explicit label to indicate which domain the current phrase pair comes from. This is traditionally done with an automatic domain classifier, and each input sentence is classified into its corresponding domain (Xu et al., 2007). As an alternative to the classification-based approach, Wang et al. (2012) employed a featurebased approach, in which phrase pairs are enriched by a feature set to potentially reflect the domain information. The similarity calculated by a information retrieval system between the training subset and the test set is used as a feature for each parallel sentence (Lu et al., 2007). Monolingual topic information is taken as a new feature for a domain adaptive translation model and tuned on the development set (Su et al., 2012). Regardless of underlying methods, either classifier-based or featurebased method, the performance of current domain adaptive phrase extraction met</context>
</contexts>
<marker>Wang, Macherey, Macherey, Och, Xu, 2012</marker>
<rawString>Wei Wang, Klaus Macherey, Wolfgang Macherey, Franz Och, and Peng Xu. 2012. Improved domain adaptation for statistical machine translation. In Proceedings of the Conference of the Association for Machine translation, Americas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Wood</author>
<author>Y W Teh</author>
</authors>
<title>A hierarchical nonparametric Bayesian approach to statistical language model domain adaptation.</title>
<date>2009</date>
<booktitle>In Proceedings of the International Conference on Artificial Intelligence and Statistics,</booktitle>
<volume>12</volume>
<contexts>
<context position="16354" citStr="Wood and Teh, 2009" startWordPosition="2782" endWordPosition="2785">a is too large to fit into a small amount of memory. Finally, new domains can be integrated incrementally. When we encounter a new domain, and if a phrase pair is completely new in terms of the model, the phrase pair is simply appended to the current model, and computed without the fallback probabilities, since otherwise, the phrase pair would be boosted by the fallback probabilities. Pitman-Yor process is also employed in n-gram language models which are hierarchically represented through the hierarchical Pitman-Yor process with switch priors to integrate different domains in all the levels (Wood and Teh, 2009). Our work incrementally combines the models from different domains by directly employing the hierarchical process through the base measures. 5 Experiment We evaluate the proposed approach on the Chinese-to-English translation task with three data sets with different scales. Data set Corpus #sent. pairs IWSLT HIT 52,603 BTEC 19,975 Domain 1 47,993 Domain 2 30,272 FBIS Domain 3 49,509 Domain 4 38,228 Domain 5 55,913 News 221,915 News 95,593 LDC Magazine 98,335 Magazine 254,488 Finance 86,112 Table 1: The sentence pairs used in each data set. 5.1 Experiment Setup The first data set comes from th</context>
</contexts>
<marker>Wood, Teh, 2009</marker>
<rawString>F. Wood and Y. W. Teh. 2009. A hierarchical nonparametric Bayesian approach to statistical language model domain adaptation. In Proceedings of the International Conference on Artificial Intelligence and Statistics, volume 12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational linguistics,</journal>
<pages>23--3</pages>
<contexts>
<context position="8155" citStr="Wu, 1997" startWordPosition="1280" endWordPosition="1281"> require numbers of hyperparameters, such as mini-batch size, step size, or human judgment to determine the quality of phrases, and still rely on a heuristic phrase extraction method in each phrase table update. 3 Phrase Pair Extraction with Unsupervised Phrasal ITGs Recently, phrase alignment with ITGs (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2008) and parameter estimation with Gibbs sampling (DeNero and Klein, 2008; Blunsom and Cohn, 2010) are popular. Here, we employ a method proposed by Neubig et al. (2011), which uses parametric Bayesian inference with the phrasal ITGs (Wu, 1997). It can achieve comparable translation accuracy with a much smaller phrase table than the traditional GIZA++ and heuristic phrase extraction methods. It has also been proved successful in adjusting the phrase length granularity by applying character-based SMT with more sophisticated inference (Neubig et al., 2012). ITG is a synchronous grammar formalism which analyzes bilingual text by introducing inverted rules, and each ITG derivation corresponds to the alignment of a sentence pair (Wu, 1997). Translation probabilities of ITG phrasal align803 ments can be estimated in polynomial time by sli</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational linguistics, 23(3):377–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jia Xu</author>
<author>Yonggang Deng</author>
<author>Yuqing Gao</author>
<author>Hermann Ney</author>
</authors>
<title>Domain dependent statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the MT</booktitle>
<location>Summit XI.</location>
<contexts>
<context position="6290" citStr="Xu et al., 2007" startWordPosition="980" endWordPosition="983">ining data is first divided into several parts, and phase pairs are extracted with some sub-domain features. Then all the phrase pairs and features are tuned together with different weights during decoding. As a way to choose the right domain for the domain adaption, a classifier-based method and a feature-based method have been proposed. Classification-based methods must at least add an explicit label to indicate which domain the current phrase pair comes from. This is traditionally done with an automatic domain classifier, and each input sentence is classified into its corresponding domain (Xu et al., 2007). As an alternative to the classification-based approach, Wang et al. (2012) employed a featurebased approach, in which phrase pairs are enriched by a feature set to potentially reflect the domain information. The similarity calculated by a information retrieval system between the training subset and the test set is used as a feature for each parallel sentence (Lu et al., 2007). Monolingual topic information is taken as a new feature for a domain adaptive translation model and tuned on the development set (Su et al., 2012). Regardless of underlying methods, either classifier-based or featureba</context>
</contexts>
<marker>Xu, Deng, Gao, Ney, 2007</marker>
<rawString>Jia Xu, Yonggang Deng, Yuqing Gao, and Hermann Ney. 2007. Domain dependent statistical machine translation. In Proceedings of the MT Summit XI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Chris Quirk</author>
<author>Robert C Moore</author>
<author>Daniel Gildea</author>
</authors>
<title>Bayesian learning of noncompositional phrases with synchronous parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>97--105</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="7892" citStr="Zhang et al., 2008" startWordPosition="1234" endWordPosition="1237">oriented methods, an online EM algorithm and active learning are applied to phrase pair extraction and achieves almost comparable translation performance with less computational overhead (Levenberg et al., 2010; Gonz´alezRubio et al., 2011). However, their methods usually require numbers of hyperparameters, such as mini-batch size, step size, or human judgment to determine the quality of phrases, and still rely on a heuristic phrase extraction method in each phrase table update. 3 Phrase Pair Extraction with Unsupervised Phrasal ITGs Recently, phrase alignment with ITGs (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2008) and parameter estimation with Gibbs sampling (DeNero and Klein, 2008; Blunsom and Cohn, 2010) are popular. Here, we employ a method proposed by Neubig et al. (2011), which uses parametric Bayesian inference with the phrasal ITGs (Wu, 1997). It can achieve comparable translation accuracy with a much smaller phrase table than the traditional GIZA++ and heuristic phrase extraction methods. It has also been proved successful in adjusting the phrase length granularity by applying character-based SMT with more sophisticated inference (Neubig et al., 2012). ITG is a synchronou</context>
</contexts>
<marker>Zhang, Quirk, Moore, Gildea, 2008</marker>
<rawString>Hao Zhang, Chris Quirk, Robert C. Moore, and Daniel Gildea. 2008. Bayesian learning of noncompositional phrases with synchronous parsing. In Proceedings of ACL-08: HLT, pages 97–105, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>