<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000597">
<title confidence="0.968386">
Learning Graph Walk Based Similarity Measures for Parsed Text
</title>
<author confidence="0.960044">
Einat Minkov∗
</author>
<affiliation confidence="0.907281">
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.997931">
einat@cs.cmu.edu
</email>
<author confidence="0.940279">
William W. Cohen
</author>
<affiliation confidence="0.914539333333333">
Machine Learning Department
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.998622">
wcohen@cs.cmu.edu
</email>
<sectionHeader confidence="0.995642" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999237111111111">
We consider a parsed text corpus as an in-
stance of a labelled directed graph, where
nodes represent words and weighted directed
edges represent the syntactic relations be-
tween them. We show that graph walks, com-
bined with existing techniques of supervised
learning, can be used to derive a task-specific
word similarity measure in this graph. We also
propose a new path-constrained graph walk
method, in which the graph walk process is
guided by high-level knowledge about mean-
ingful edge sequences (paths). Empirical eval-
uation on the task of named entity coordinate
term extraction shows that this framework is
preferable to vector-based models for small-
sized corpora. It is also shown that the path-
constrained graph walk algorithm yields both
performance and scalability gains.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.993531021276596">
Graph-based similarity measures have been used
for a variety of language processing applications.
In this paper we assume directed graphs, where
typed nodes denote entities and labelled directed
and weighted edges denote the relations between
them. In this framework, graph walks can be ap-
plied to draw a measure of similarity between the
graph nodes. Previous works have applied graph
walks to draw a notion of semantic similarity over
such graphs that were carefully designed and man-
ually tuned, based on WordNet reations (Toutanova
∗Current address: Nokia Research Center Cambridge, Cam-
bridge, MA 02142, USA.
et al., 2004; Collins-Thompson and Callan, 2005;
Hughes and Ramage, 2007).
While these and other researchers have used
WordNet to evaluate similarity between words, there
has been much interest in extracting such a measure
from text corpora (e.g., (Snow et al., 2005; Pad´o
and Lapata, 2007)). In this paper, we suggest pro-
cessing dependency parse trees within the general
framework of directed labelled graphs. We construct
a graph that directly represents a corpus of struc-
tured (parsed) text. In the suggested graph scheme,
nodes denote words and weighted edges represent
the dependency relations between them. We apply
graph walks to derive an inter-word similarity mea-
sure. We further apply learning techniques, adapted
to this framework, to improve the derived corpus-
based similarity measure.
The learning methods applied include existing
learning techniques, namely edge weight tuning,
where weights are associated with the edge types,
and discriminative reranking of graph nodes, using
features that describe the possible paths between a
graph node and the initial “query nodes” (Minkov
and Cohen, 2007).
In addition, we outline in this paper a novel
method for learning path-constrained graph walks.
While reranking allows use of high-level features
that describe properties of the traversed paths, the
suggested algorithm incorporates this information in
the graph walk process. More specifically, we allow
the probability flow in the graph to be conditioned
on the history of the walk. We show that this method
results in improved performance as it directs proba-
bility flow to meaningful paths. In addition, it leads
</bodyText>
<page confidence="0.944943">
907
</page>
<note confidence="0.962749">
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 907–916,
Honolulu, October 2008.c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.999160694444444">
to substantial gains in terms of runtime performance.
The graph representation and the set of learning
techniques suggested are empirically evaluated on
the task of coordinate term extraction1 from small
to moderately sized corpora, where we compare
them against vector-based models, including a state-
of-the-art syntactic distributional similarity method
(Pad´o and Lapata, 2007). It is shown that the graph
walk based approach gives preferable results for the
smaller datasets (and comparable otherwise), where
learning yields significant gains in accuracy.
There are several contributions of this paper.
First, we represent dependency-parsed corpora
within a general graph walk framework, and derive
inter-word similarity measures using graph walks
and learning techniques available in this framework.
To our knowledge, the application of graph walks to
parsed text in general, and to the extraction of coor-
dinate terms in particular, is novel. Another main
contribution of this paper is the path-constrained
graph walk variant, which is a general learning tech-
nique for calculating the similarity between graph
nodes in directed and labelled graphs.
Below we first outline our proposed scheme for
representing a dependency-parsed text corpus as a
graph, and provide some intuitions about the asso-
ciated similarity metric (Section 2). We then give
an overview of the graph-walk based similarity met-
ric (Section 3), as well as the known edge weight
tuning and reranking learning techniques (Section
4). We next present the proposed algorithm of path-
constrained graph walks (Section 5). The paper pro-
ceeds with a review of related work (Section 6), a
discussion of the coordinate term extraction task,
empirical evaluation and our conclusions (Sections
7-9).
</bodyText>
<sectionHeader confidence="0.574853" genericHeader="introduction">
2 Representing a Corpus as a Graph
</sectionHeader>
<bodyText confidence="0.999980857142857">
A typed dependency parse tree consists of directed
links between words, where dependencies are la-
belled with the relevant grammatical relation (e.g.,
nominal subject, indirect object etc.). We suggest
representing a text corpus as a connected graph
of dependency structures, according to the scheme
shown in Figure 1. The graph shown in the figure
</bodyText>
<footnote confidence="0.993003">
1In particular, we focus on the extraction of named entity
classes.
</footnote>
<figureCaption confidence="0.9956505">
Figure 1: The suggested graph schema, demonstrated for
a two-sentence corpus.
</figureCaption>
<bodyText confidence="0.999183727272727">
includes the dependency analysis of two sentences:
“boys like playing with all kinds of cars”, and “girls
like playing with dolls”. In the graph, each word
mention is represented as a node, which includes the
index of the sentence in which it appears, as well
as its position within the sentence. Word mentions
are marked as circles in the figure. The “type” of
each word – henceforth a term node – is denoted by
a square in the figure. Each word mention is linked
to the corresponding term; for example, the nodes
“like1” and “like2” represent distinct word mentions
and both nodes are linked to the term “like”. For
every edge in the graph, we add another edge in the
opposite direction (not shown in the figure); for ex-
ample, an inverse edge exists from “like1” to “girls1”
with an edge labelled as “nsubj-inv”. The resulting
graph is highly interconnected and cyclic.
We will apply graph walks to derive an extended
measure of similarity, or relatedness, between word
terms (as defined above). For example, starting from
the term “girls”, we will reach the semantically re-
lated term “boys” via the following two paths:
</bodyText>
<equation confidence="0.985791888888889">
(1) girls mention −→girls1 nsubj
−→ like1 as−term
−→ like mention
−→
like2 nsubj−inverse
−→ boys2 as−term
−→ boys
(2) girls mention
−→ girls1 nsubj
−→ like1 partmod
−→ playing1
as−term
−→ playing mention
−→ playing2 partmod−inverse
−→ like2
nsubj−inverse
−→ boys2 as−term
−→ boys
</equation>
<bodyText confidence="0.9999136">
Intuitively, in a graph representing a large cor-
pus, terms that are more semantically related will
be linked by a larger number of connecting paths. In
addition, shorter connecting paths may be in general
more meaningful. In the next section we show that
</bodyText>
<page confidence="0.996941">
908
</page>
<bodyText confidence="0.999731285714286">
the graph walk paradigm addresses both of these re-
quirements. Further, different edge types, as well as
the paths traversed, are expected to have varying im-
portance in different types of word similarity (for ex-
ample, verbs and nouns are associated with different
connectivity patterns). These issues are addressed
using learning.
</bodyText>
<sectionHeader confidence="0.848756" genericHeader="method">
3 Graph Walks and Similarity Queries
</sectionHeader>
<bodyText confidence="0.999853296296296">
This section provides a quick overview of the graph
walk induced similarity measure. For details, the
reader is referred to previous publications (e.g.,
(Toutanova et al., 2004; Minkov and Cohen, 2007)).
In summary, similarity between two nodes in the
graph is defined by a weighted graph walk process,
where an edge of type E is assigned an edge weight,
O`, determined by its type.2 The transition proba-
bility of reaching node y from node x over a single
time step, Pr(x −→ y), is defined as the weight
of their connecting edge, Ol, normalized by the to-
tal outgoing weight from x. Given these transition
probabilities, and starting from an initial distribu-
tion Vq of interest (a query), we perform a graph
walk for a finite number of steps K. Further, at each
step of the walk, a proportion -y of the probability
mass at every node is emitted. Thus, this model ap-
plies exponential decay on path length. The final
probability distribution of this walk over the graph
nodes, which we denote as R, is computed as fol-
lows: R = �Ki=1 -yiVqMi, where M is the transi-
tion matrix.3 The answer to a query, Vq, is a list of
nodes, ranked by the scores in the final distribution
R. In this multi-step walk, nodes that are reached
from the query nodes by many shorter paths will be
assigned a higher score than nodes connected over
fewer longer paths.
</bodyText>
<sectionHeader confidence="0.987824" genericHeader="method">
4 Learning
</sectionHeader>
<bodyText confidence="0.999867">
We consider a supervised setting, where we are
given a dataset of example queries and labels over
the graph nodes, indicating which nodes are relevant
to which query. For completeness, we describe here
two methods previously described by Minkov and
</bodyText>
<footnote confidence="0.9954995">
2In this paper, we consider either uniform edge weights; or,
learn the set of weights O from examples.
3We tune K empirically and set γ = 0.5, as in (Minkov and
Cohen, 2007).
</footnote>
<bodyText confidence="0.99596025">
Cohen (Minkov and Cohen, 2007): a hill-climbing
method that tunes the graph weights; and a reranking
method. We also specify the feature set to be used by
the reranking method in the domain of parsed text.
</bodyText>
<subsectionHeader confidence="0.995677">
4.1 Weight Tuning
</subsectionHeader>
<bodyText confidence="0.997193966666666">
There are several motivations for learning the graph
weights O in this domain. First, some dependency
relations – foremost, subject and object – are in gen-
eral more salient than others (Lin, 1998; Pad´o and
Lapata, 2007). In addition, dependency relations
may have varying importance per different notions
of word similarity (e.g., noun vs. verb similarity
(Resnik and Diab, 2000)). Weight tuning allows the
adaption of edge weights to each task (i.e., distribu-
tion of queries).
The weight tuning method implemented in this
work is based on an error backpropagation hill
climbing algorithm (Diligenti et al., 2005). The al-
gorithm minimizes the following cost function:
(pz − pOpt
z )2
where ez is the error for a target node z defined as the
squared difference between the final score assigned
to z by the graph walk, pz, and some ideal score ac-
cording to the example’s labels, pOpt
z .4 Specifically,
pOpt
z is set to 1 in case that the node z is relevant
or 0 otherwise. The error is averaged over a set of
example instantiations of size N. The cost function
is minimized by gradient descent where the deriva-
tive of the error with respect to an edge weight O`
is derived by decomposing the walk into single time
steps, and considering the contribution of each node
traversed to the final node score.
</bodyText>
<subsectionHeader confidence="0.99652">
4.2 Node Reranking
</subsectionHeader>
<bodyText confidence="0.999994142857143">
Reranking of the top candidates in a ranked list
has been successfully applied to multiple NLP tasks
(Collins, 2002; Collins and Koo, 2005). In essence,
discriminative reranking allows the re-ordering of
results obtained by methods that perform some form
of local search, using features that encode higher
level information.
</bodyText>
<footnote confidence="0.8860795">
4For every example query, a handful of the retrieved nodes
are considered, including both relevant and irrelevant nodes.
</footnote>
<equation confidence="0.960136375">
1
N z∈N
1
N z∈N
E=
ez =
1
2
</equation>
<page confidence="0.988552">
909
</page>
<bodyText confidence="0.999994222222222">
A number of features describing the set of paths
from VQ can be conveniently computed in the pro-
cess of executing the graph walk, and it has been
shown that reranking using these features can im-
prove results significantly. It has also been shown
that reranking is complementary to weight tuning
(Minkov and Cohen, 2007), in the sense that the
two techniques can be usefully combined by tuning
weights, and then reranking the results.
In the reranking approach, for every training ex-
ample i (1 &lt; i &lt; N), the reranking algorithm is
provided with the corresponding output ranked list
of li nodes. Let zij be the output node ranked at rank
j in li, and let pzij be the probability assigned to zij
by the graph walk. Each output node zij is repre-
sented through m features, which are computed by
pre-defined feature functions f1, ... , fm. The rank-
ing function for node zij is defined as:
</bodyText>
<equation confidence="0.975160666666667">
m
F(zij, ¯α) = α0log(pzij) + E αkfk(zij)
k=1
</equation>
<bodyText confidence="0.999897666666667">
where α¯ is a vector of real-valued parameters. Given
a new test example, the output of the model is the
output node list reranked by F(zij, ¯α). To learn the
parameter weights ¯α, we here applied a boosting
method (Collins and Koo, 2005) (see also (Minkov
et al., 2006)).
</bodyText>
<subsubsectionHeader confidence="0.479615">
4.2.1 Features
</subsubsectionHeader>
<bodyText confidence="0.999834818181818">
We evaluate the following feature templates.
Edge label sequence features indicate whether a par-
ticular sequence of edge labels ii occurred, in a
particular order, within the set of paths leading to
the target node zij. Lexical unigram feature indi-
cate whether a word mention whose lexical value
is tk was traversed in the set of paths leading to
zij. Finally, the Source-count feature indicates the
number of different source query nodes that zij was
reached from. The intuition behind this last fea-
ture is that nodes linked to multiple query nodes,
where applicable, are more relevant. For exam-
ple, for the query term “girl” in the graph depicted
in Figure 1, the target node “boys” is described
by the features (denoted as feature-name.feature-
value): sequence.nsubj.nsubj-inv (where mention
and as-term edges are omitted) , lexical.’“like” etc.
In this work, the features encoded are binary.
However, features can be assugned numeric weights
that corresponds to the probability of the indicator
being true for any path between x and zij (Cohen
and Minkov, 2006).
</bodyText>
<sectionHeader confidence="0.925137" genericHeader="method">
5 Path-Constrained Graph Walk
</sectionHeader>
<bodyText confidence="0.99995978125">
While node reranking allows the incorporation of
high-level features that describe the traversed paths,
it is desirable to incorporate such information ear-
lier in the graph walk process. In this paper, we
suggest a variant of a graph-walk, which is con-
strained by path information. Assume that prelim-
inary knowledge is available that indicates the prob-
ability of reaching a relevant node after following a
particular edge type sequence (path) from the query
distribution VQ to some node x. Rather than fix the
edge weights O, we can evaluate the weights of the
outgoing edges from node x dynamically, given the
history of the walk (the path) up to this node. This
should result in gains in accuracy, as paths that lead
mostly to irrelevant nodes can be eliminated in the
graph walk process. In addition, scalability gains
are expected, for the same reason.
We suggest a path-constrained graph walk algo-
rithm, where path information is maintained in a
compact path-tree structure constructed based on
training examples. Each vertex in the path tree de-
notes a particular walk history. In applying the graph
walk, the nodes traversed are represented as a set of
node pairs, comprised of the graph node and the cor-
responding vertices in the path tree. The outgoing
edge weights from each node pair will be estimated
according to the respective vertex in the path tree.
This approach needs to address two subtasks: learn-
ing of the path-tree; and updating of the graph walk
paradigm to co-sample from the graph and the path
tree. We next describe these two components in de-
tail.
</bodyText>
<subsectionHeader confidence="0.81934">
The Path-Tree
</subsectionHeader>
<bodyText confidence="0.999921888888889">
We construct a path-tree T using a training set
of N example queries. Let a path p be a sequence
of k &lt; K edge types (where K is the maximum
number of graph walk steps). For each training ex-
ample, we recover all of the connecting paths lead-
ing to the top M (correct and incorrect) nodes. We
consider only acyclic paths. Let each path p be as-
sociated with its count, within the paths leading to
the correct nodes, denoted as C+P . Similarly, the
</bodyText>
<page confidence="0.991439">
910
</page>
<figureCaption confidence="0.999588">
Figure 2: An example path-tree.
</figureCaption>
<bodyText confidence="0.98083975">
count within paths leading to the negatively labelled
nodes is denoted C−p . The full set of paths observed
is then represented as a tree.5 The leaves of the
tree are assigned a Laplace-smoothed probability:
</bodyText>
<equation confidence="0.9932025">
C+p +1
Pr (p) _ — CP +C−p +2.
</equation>
<bodyText confidence="0.999812571428571">
Given path probabilities, they are propagated
backwards to all tree vertices, applying the MAX
operator.6 Consider the example given in Figure 2.
The path-tree in the figure includes three paths (con-
structed from edge types k,l, m, n). The top part
of the figure gives the paths’ associated counts, and
the bottom part of the figure gives the derived outgo-
ing edge probabilities at each vertex. This path-tree
specifies, for example, that given an edge of type l
was traversed from the root, the probability of reach-
ing a correct target node is 0.9 if an edge of type n
is followed, whereas the respective probability if an
edge of type m is followed is estimated at a lower
0.2.
</bodyText>
<sectionHeader confidence="0.540111" genericHeader="method">
A Concurrent Graph-walk
</sectionHeader>
<bodyText confidence="0.999931555555555">
Given a generated path tree, we apply path-
constrained graph walks that adhere both to the
topology of the graph G, and to the path tree T.
Walk histories of each node x visited in the walk
are compactly represented as pairs &lt; t, x &gt;, where
t denotes the relevant vertex in the path tree. For
example, suppose that after one walk step, the main-
tained node-history pairs include &lt; T(l), x1 &gt; and
&lt; T (m), x2 &gt;. If x3 is reached in the next walk step
</bodyText>
<footnote confidence="0.8291066">
5The conversion to a tree is straight-forward, where identical
path prefixes are merged.
6Another possibility is to average the downstream cumula-
tive counts at each vertex. The MAX operation gave better re-
sults in our experiments.
</footnote>
<figure confidence="0.6546079375">
Given: graph G, path-tree T, query distribution V0,
number of steps K
Initialize: for each xi ∈ V0, assign a pair
&lt; root(T), xi &gt;
Repeat for steps k = 0 to K:
For each &lt; ti, xi &gt;∈ Vk:
Let L be the set of outgoing edge labels from xi, in G.
For each lm ∈ L:
For each xj ∈ G s.t., xi −→ xj, add &lt; tj, xj &gt; to
l—
Vk+1, where tj ∈ T, s.t. ti −→ tj, with probability
l—
Pr(xi|Vk) × Pr(lm|ti, T). (The latter probabilities
should be normalized with respect to xi.)
If ti is a terminal node in T, emit xi with probability
Pr(xi|Vk) × Pr(ti|T).
</figure>
<figureCaption confidence="0.998996">
Figure 3: Pseudo-code for path-constrained graph walk
</figureCaption>
<bodyText confidence="0.9985292">
from both x1 and x2 over paths included in the path-
tree, it will be represented by multiple node pairs,
e.g., &lt; T(l → n), x3 &gt; and &lt; T(m → l, x3 &gt;.
A pseudo-code for a path-constrained graph walk is
given in Figure 3. It is straight-forward to discard
paths in T that are associated with a lower proba-
bility than some threshold. A threshold of 0.5, for
example, implies that only paths that led to a major-
ity of positively labelled nodes in the training set are
followed.
</bodyText>
<sectionHeader confidence="0.99996" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999966117647059">
Graph walks over typed graphs have been applied
to derive semantic similarity for NLP problems us-
ing WordNet as a primary information source. For
instance, Hughes and Ramage (2007) constructed a
graph which represented various types of word re-
lations from WordNet, and compared random-walk
similarity to similarity assessments from human-
subject trials. Random-walk similarity has also been
used for lexical smoothing for prepositional word
attachment (Toutanova et al., 2004) and query ex-
pansion (Collins-Thompson and Callan, 2005). In
contrast to these works, our graph representation de-
scribes parsed text and has not been (consciously)
engineered for a particular task. Instead, we in-
clude learning techniques to optimize the graph-
walk based similarity measure. The learning meth-
ods described in this paper can be readily applied to
</bodyText>
<page confidence="0.99589">
911
</page>
<bodyText confidence="0.9999156">
other directed and labelled entity-relation graphs.7
The graph representation described in this paper
is perhaps most related to syntax-based vector space
models, which derive a notion of semantic similar-
ity from statistics associated with a parsed corpus
(Grefenstette, 1994; Lin, 1998; Pad´o and Lapata,
2007). In most cases, these models construct vectors
to represent each word wi, where each element in the
vector for wi corresponds to particular “context” c,
and represents a count or an indication of whether
wi occurred in context c. A “context” can refer to
simple co-occurrence with another word wj, to a
particular syntactic relation to another word (e.g., a
relation of “direct object” to wj), etc. Given these
word vectors, inter-word similarity is evaluated us-
ing some appropriate similarity measure for the vec-
tor space, such as cosine vector similarity, or Lin’s
similarity (Lin, 1998).
Recently, Pad´o and Lapata (Pad´o and Lapata,
2007) have suggested an extended syntactic vector
space model called dependency vectors, in which
rather than simple counts, the components of a
word vector of contexts consist of weighted scores,
which combine both co-occurrence frequency and
the importance of a context, based on properties of
the connecting dependency paths. They considered
two different weighting schemes: a length weight-
ing scheme, assigning lower weight to longer con-
necting paths; and an obliqueness weighting hierar-
chy (Keenan and Comrie, 1977), assigning higher
weight to paths that include grammatically salient
relations. In an evaluation of word pair similar-
ity based on statistics from a corpus of about 100
million words, they show improvements over sev-
eral previous vector space models. Below we will
compare our framework to that of Pad´o and Lap-
ata. One important difference is that while Pad´o and
Lapata make manual choices (regarding the set of
paths considered and the weighting scheme), we ap-
ply learning to adjust the analogous parameters.
</bodyText>
<sectionHeader confidence="0.903671" genericHeader="method">
7 Extraction of Coordinate Terms
</sectionHeader>
<bodyText confidence="0.999646333333333">
We evaluate the text representation schema and the
proposed set of graph-based similarity measures on
the task of coordinate term extraction. In particular,
</bodyText>
<footnote confidence="0.94905">
7We refer the reader to the TextGraph workshop proceed-
ings, http://textgraphs.org.
</footnote>
<bodyText confidence="0.999908595238096">
we evaluate the extraction of named entities, includ-
ing city names and person names from newswire
data, using word similarity measures. Coordinate
terms reflect a particular type of word similarity
(relatedness), and are therefore an appropriate test
case for our framework. While coordinate term ex-
traction is often addressed by a rule-based (tem-
plates) approach (Hearst, 1992), this approach was
designed for very large corpora such as the Web,
where the availability of many redundant documents
allows use of high-precision and low-recall rules.
In this paper we focus on relatively small corpora.
Small limited text collections may correspond to
documents residing on a personal desktop, email
collections, discussion groups and other specialized
sets of documents.
The task defined in the experiments is to retrieve
a ranked list of city or person names given a small
set of seeds. This task is implemented in the graph
as a query, where we let the query distribution UQ be
uniform over the given seeds (and zero elsewhere).
Ideally, the resulting ranked list will be populated
with many additional city, or person, names.
We compare graph walks to dependency vec-
tors (DV) (Pad´o and Lapata, 2007),8 as well as to
a vector-based bag-of-words co-occurrence model.
DV is a state-of-the-art syntactic vector-based model
(see Section 6). The co-occurrence model represents
a more traditional approach, where text is processed
as a stream rather than syntactic structures. In ap-
plying the vector-space based methods, we compute
a similarity score between every candidate from the
corpus and each of the query terms, and then aver-
age these scores (as the query distributions are uni-
form) to construct a ranked list. For efficiency, in
the vector-based models we limit the considered set
of candidates to named entities. Similarly, the graph
walk results are filtered to include named entities.9
Corpora. As the experimental corpora, we use
the training set portion of the MUC-6 dataset (MUC,
1995) as well as articles from the Associated Press
(AP) extracted from the AQUAINT corpus (Bilotti
</bodyText>
<footnote confidence="0.999822166666667">
8We used the code from http://www.coli.uni-
saarland.de/ pado/dv.html, and converted the underlying
syntactic patterns to the Stanford dependency parser conven-
tions.
9In general, graph walk results can be filtered by various
word properties, e.g., capitalization pattern, or part-of-speech.
</footnote>
<page confidence="0.987344">
912
</page>
<table confidence="0.996144666666667">
Corpus words nodes edges unique NEs
MUC 140K 82K 244K 3K
MUC+AP 2,440K 1,030K 3,550K 36K
</table>
<tableCaption confidence="0.999881">
Table 1: Corpus statistics
</tableCaption>
<bodyText confidence="0.999981684210526">
et al., 2007), all parsed using the Stanford depen-
dency parser (de Marneffe et al., 2006).10 The MUC
corpus provides true named entity tags, while the
AQUAINT corpus includes automatically generated,
noisy, named entity tags. Statistics on the experi-
mental corpora and their corresponding graph rep-
resentation are detailed in Table 1. As shown, the
MUC corpus contains about 140 thousand words,
whereas the MUC+AP experimental corpus is sub-
stantially larger, containing about 2.5 million words.
We generated 10 queries, each comprised of 4 city
names selected randomly according to the distribu-
tion of city name mentions in MUC-6. Similarly,
we generated a set of 10 queries that include 4 per-
son names selected randomly from the MUC corpus.
(The MUC corpus was appended to AP, so that the
same query sets are applicable in both cases.) For
each task, we use 5 queries for training and tuning
and the remaining queries for testing.
</bodyText>
<sectionHeader confidence="0.979234" genericHeader="evaluation">
8 Experimental Results
</sectionHeader>
<bodyText confidence="0.9999372">
Experimental setup. We evaluated cross-validation
performance over the training queries in terms of
mean average precision for varying walk lengths K.
We found that beyond K = 6 improvements were
small (and in fact deteriorated for K = 9). We there-
fore set K = 6. Weight tuning was trained using
the training queries and two dozens of target nodes
overall. In reranking, we set a feature count cutoff
of 3, in order to avoid over-fitting. Reranking was
applied to the top 200 ranked nodes output by the
graph walk using the tuned edge weights. Finally,
path-trees were constructed using the top 20 correct
nodes and 20 incorrect nodes retrieved by the uni-
formly weighted graph walk. In the experiments,
we apply a threshold of 0.5 to the path constrained
graph walk method.
We note that for learning, true labels were used for
the fully annotated MUC corpus (we hand labelled
all of the named entities of type location in the cor-
pus as to whether they were city names). However,
</bodyText>
<footnote confidence="0.7382245">
10http://nlp.stanford.edu/software/lex-parser.shtml; sen-
tences longer than 70 words omitted.
</footnote>
<bodyText confidence="0.999979065217391">
noisy negative examples were considered for the
larger automatically annotated AP corpus. (Specif-
ically, for cities, we only considered city names in-
cluded in the MUC corpus as correct answers.)
A co-occurrence vector-space model was applied
using a window of two tokens to the right and to
the left of the focus word. Inter-word similarity
was evaluated in this model using cosine similar-
ity, where the underlying co-occurrence counts were
normalized by log-likelihood ratio (Pad´o and Lap-
ata, 2007). The parameters of the DV method were
set based on a cross validation evaluation (using the
MUC+AP corpus). The medium set of dependency
paths and the oblique edge weighting scheme were
found to perform best. We experimented with co-
sine as well as Lin similarity measure in combina-
tion with the dependency vectors representation. Fi-
nally, given the large number of candidates in the
MUC+AP corpus (Table 1), we show the results of
applying the considered vector-space models to the
top, high-quality, entities retrieved with reranking
for this corpus.11
Test set results. Figure 4 gives results for the city
name (top) and the person name (bottom) extraction
tasks. The left part of the figure shows results us-
ing the MUC corpus, and its right part – using the
MUC+AP corpus. The curves show precision as a
function of rank in the ranked list, up to rank 100.
(For this evaluation, we hand-labeled all the top-
ranked results as to whether they are city names or
person names.) Included in the figure are the curves
of the graph-walk method with uniform weights
(G:Uw), learned weights (G:Lw), graph-walk with
reranking (Rerank) and a path-constrained graph-
walk (PCW). Also given are the results of the co-
occurrence model (CO), and the syntactic vector-
space DV model, using the Lin similarity measure
(DV:Lin). Performance of the DV model using co-
sine similarity was found comparable or inferior to
using the Lin measure, and is omitted from the fig-
ure for clarity.
Several trends can be observed from the results.
With respect to the graph walk methods, the graph
walk using the learned edge weights consistently
outperforms the graph walk with uniform weights.
Reranking and the path-constrained graph walk,
</bodyText>
<footnote confidence="0.722474">
11We process the union of the top 200 results per each query.
</footnote>
<page confidence="0.997381">
913
</page>
<figure confidence="0.998369131578947">
MUC MUC+AP
Precision
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
1
Precision
0.8
0.6
0.4
0.2
0
1
G:Uw
G:Lw
CO
DV:Lin
PCW
Rerank
0 10 20 30 40 50 60 70 80 90 100 0 10 20 30 40 50 60 70 80 90 100
Rank Rank
Precision 1 Precision 1
0.9 0.8
0.8 0.6
0.7 0.4
0.6 0.2
0.5 0
0.4
0.3
0.2
0.1
0 10 20 30 40 50 60 70 80 90 100 0 10 20 30 40 50 60 70 80 90 100
Rank Rank
</figure>
<figureCaption confidence="0.999611">
Figure 4: Test results: Precision at the top 100 ranks, for the city name extraction task (top) and person name extraction
task (bottom).
</figureCaption>
<bodyText confidence="0.999969333333333">
however, yield superior results. Both of these learn-
ing methods utilize a richer set of features than the
graph walk and weight tuning, which can consider
only local information. In particular, while the graph
walk paradigm assigns lower importance to longer
connecting paths (as described in Section 3), rerank-
ing and the path-constrained walker allow to dis-
card short yet irrelevant paths, and by that eliminate
noise at the top ranks of the retrieved list. In gen-
eral, the results show that edge sequences carry ad-
ditional meaning compared with the individual edge
label segments traversed.
Out of the vector-based models, the co-
occurrence model is preferable for the city name
extraction task, and the syntactic dependency vec-
tors model gives substantially better performance
for person name extraction. We conjecture that city
name mentions are less structured in the underlying
text. In addition, the syntactic weighting scheme of
the DV model is probably not optimal for the case of
city names. For example, a conjunction relation was
found highly indicative for city names (see below).
However, this relation is not emphasized by the DV
weighting schema. As expected, the performance of
the vector-based models improves for larger corpora
(Terra and Clarke, 2003). These models demonstrate
good performance for the larger MUC+AP corpus,
but only mediocre performance for the smaller MUC
corpus.
Contrasting the graph-based methods with the
vector-based models, the difference in performance
in favor of reranking and PCW, especially for the
smaller corpus, can be attributed to two factors. The
first factor is learning, which optimizes performance
for the underlying data. A second factor is the incor-
poration of non-local information, encoding proper-
ties of the traversed paths.
Models. Following is a short description of the
models learned by the different methods and tasks.
Weight tuning assigned high weights to edge types
such as conj-and, prep-in and prep-from, nn, ap-
pos and amod for the city extraction task. For per-
</bodyText>
<page confidence="0.991013">
914
</page>
<figure confidence="0.998528153846154">
20
18
16
14
12
10
8
6
4
2
Number of graph nodes visited [log_2]
0 1 2 3 4 5 6
Walk steps
</figure>
<figureCaption confidence="0.9834935">
Figure 5: The graph walk exponential spread is bounded
by the path constrained walk.
</figureCaption>
<bodyText confidence="0.999921192307692">
son extraction, prominent edge types included subj,
obj, poss and nn. (The latter preferences are sim-
ilar to the linguistically motivated weights of DV.)
High weight features assigned by reranking for city
name extraction included, for example, lexical fea-
tures such as “based” and “downtown”, and edge bi-
grams such as “prep-in-Inverse—*conj-and” or “nn-
Inverse—*nn”. Positive highly predictive paths in
the constructed path tree included many symmetric
paths, such as ...—*conj andInverse...—*.conj and...,
...—*prep inInverse...—*.prep in..., for the city name
extraction task.
Scalability. Figure 5 shows the number of graph
nodes maintained in each step of the graph walk
(logarithm scale) for a typical city extraction query
and the MUC+AP corpus. As shown by the solid
line, the number of graph nodes visited using the
weighted graph walk paradigm grows exponentially
with the length of the walk. Applying a path-
constrained walk with a threshold of 0 (PCW:0) re-
duces the maximal number of nodes expanded (as
paths not observed in the training set are discarded).
As shown, increasing the threshold leads to signifi-
cant gains in scalability. Overall, query processing
time averaged at a few minutes, using a commodity
PC.
</bodyText>
<sectionHeader confidence="0.987652" genericHeader="conclusions">
9 Conclusion and Future Directions
</sectionHeader>
<bodyText confidence="0.999767021276596">
In this paper we make several contributions. First,
we have explored a novel but natural representation
for a corpus of dependency-parsed text, as a labelled
directed graph. We have evaluated the task of coor-
dinate term extraction using this representation, and
shown that this task can be performed using similar-
ity queries in a general-purpose graph-walk based
query language. Further, we have successfully ap-
plied learning techniques that tune weights assigned
to different dependency relations, and re-score can-
didates using features derived from the graph walk.
Another orthogonal contribution of this paper is
a path-constrained graph walk variant, where the
graph walk is guided by high level knowledge about
meaningful paths, learned from training examples.
This method was shown to yield improved perfor-
mance for the suggested graph representation, and
improved scalability compared with the local graph
walk. The method is general, and can be readily ap-
plied in similar settings.
Empirical evaluation of the coordinate term ex-
traction task shows that the graph-based framework
performs better than vector-space models for the
smaller corpus, and comparably otherwise. Over-
all, we find that the suggested model is suitable for
deep (syntactic) processing of small specialized cor-
pora. In preliminary experiments where we evalu-
ated this framework on the task of extracting general
word synonyms, using a relatively large corpus of
15 million words, we found the graph-walk perfor-
mance to be better than DV using cosine similarity
measures, but second to DV using Lin’s similarity
measure. While this set of results is incomplete, we
find that it is consistent with the results reported in
this paper.
The framework presented can be enhanced in sev-
eral ways. For instance, WordNet edges and mor-
phology relations can be readily encoded in the
graph. We believe that this framework can be ap-
plied for the extraction of more specialized no-
tions of word relatedness, as in relation extraction
(Bunescu and Mooney, 2005). The path-constrained
graph walk method proposed may be enhanced by
learning edge probabilities, using a rich set of fea-
tures. We are also interested in exploring a possi-
ble relation between the path-constrained walk ap-
proach and reinforcement learning.
</bodyText>
<sectionHeader confidence="0.998843" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.996298">
The authors wish to thank the anonymous reviewers
and Hanghang Tong for useful advice. This material
is based upon work supported by Yahoo! Research.
</bodyText>
<figure confidence="0.9294365">
G:U
PCW:0
PCW:0.5
PCW:0.8
</figure>
<page confidence="0.994782">
915
</page>
<sectionHeader confidence="0.995444" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999936116666667">
Matthew W. Bilotti, Paul Ogilvie, Jamie Callan, and Eric
Nyberg. 2007. Structured retrieval for question an-
swering. In SIGIR.
Razvan C. Bunescu and Raymond J. Mooney. 2005. A
shortest path dependency kernel for relation extrac-
tion. In HLT-EMNLP.
William W. Cohen and Einat Minkov. 2006. A graph-
search framework for associating gene identifiers with
documents. BMC Bioinformatics, 7(440).
Michael Collins and Terry Koo. 2005. Discrimina-
tive reranking for natural language parsing. Compu-
tational Linguistics, 31(1):25–69.
Kevyn Collins-Thompson and Jamie Callan. 2005.
Query expansion using random walk models. In
CIKM.
Michael Collins. 2002. Ranking algorithms for named-
entity extraction: Boosting and the voted perceptron.
In ACL.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
LREC.
Michelangelo Diligenti, Marco Gori, and Marco Mag-
gini. 2005. Learning web page scores by error back-
propagation. In IJCAI.
Gregory Grefenstette. 1994. Explorations in Automatic
Thesaurus Discovery. Kluwer Academic Publishers,
Dordrecht.
Marti Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In COLING.
Thad Hughes and Daniel Ramage. 2007. Lexical seman-
tic relatedness with random graph walks. In EMNLP.
Edward Keenan and Bernard Comrie. 1977. Noun
phrase accessibility and universal grammar. Linguis-
tic Inquiry, 8.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In COLING-ACL.
Einat Minkov and William W. Cohen. 2007. Learning to
rank typed graph walks: Local and global approaches.
In WebKDD/KDD-SNA workshop.
Einat Minkov, William W. Cohen, and Andrew Y. Ng.
2006. Contextual search and name disambiguation in
email using graphs. In SIGIR.
1995. Proceedings of the sixth message understanding
conference (muc-6). In Morgan Kaufmann Publish-
ers, Inc. Columbia, Maryland.
Sebastian Pad´o and Mirella Lapata. 2007. Dependency-
based construction of semantic space models. Compu-
tational Linguistics, 33(2).
Philip Resnik and Mona Diab. 2000. Measuring verb
similarity. In CogSci.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005.
Learning syntactic patterns for automatic hypernym
discovery. In NIPS.
Egidio Terra and C. L. A. Clarke. 2003. Frequency
estimates for statistical word similarity measures. In
NAACL.
Kristina Toutanova, Christopher D. Manning, and An-
drew Y. Ng. 2004. Learning random walk models for
inducing word dependency distributions. In ICML.
</reference>
<page confidence="0.9986">
916
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.553358">
<title confidence="0.9552005">Learning Graph Walk Based Similarity Measures for Parsed Text Language Technologies</title>
<affiliation confidence="0.952826">Carnegie Mellon</affiliation>
<address confidence="0.957735">Pittsburgh, PA 15213,</address>
<email confidence="0.99952">einat@cs.cmu.edu</email>
<author confidence="0.956244">W William</author>
<affiliation confidence="0.8647545">Machine Learning Carnegie Mellon</affiliation>
<address confidence="0.958935">Pittsburgh, PA 15213,</address>
<email confidence="0.999289">wcohen@cs.cmu.edu</email>
<abstract confidence="0.999764842105263">We consider a parsed text corpus as an instance of a labelled directed graph, where nodes represent words and weighted directed edges represent the syntactic relations between them. We show that graph walks, combined with existing techniques of supervised learning, can be used to derive a task-specific word similarity measure in this graph. We also a new walk method, in which the graph walk process is guided by high-level knowledge about meaningful edge sequences (paths). Empirical evaluation on the task of named entity coordinate term extraction shows that this framework is preferable to vector-based models for smallsized corpora. It is also shown that the pathconstrained graph walk algorithm yields both performance and scalability gains.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Matthew W Bilotti</author>
<author>Paul Ogilvie</author>
<author>Jamie Callan</author>
<author>Eric Nyberg</author>
</authors>
<title>Structured retrieval for question answering.</title>
<date>2007</date>
<booktitle>In SIGIR.</booktitle>
<marker>Bilotti, Ogilvie, Callan, Nyberg, 2007</marker>
<rawString>Matthew W. Bilotti, Paul Ogilvie, Jamie Callan, and Eric Nyberg. 2007. Structured retrieval for question answering. In SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan C Bunescu</author>
<author>Raymond J Mooney</author>
</authors>
<title>A shortest path dependency kernel for relation extraction.</title>
<date>2005</date>
<booktitle>In HLT-EMNLP.</booktitle>
<marker>Bunescu, Mooney, 2005</marker>
<rawString>Razvan C. Bunescu and Raymond J. Mooney. 2005. A shortest path dependency kernel for relation extraction. In HLT-EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William W Cohen</author>
<author>Einat Minkov</author>
</authors>
<title>A graphsearch framework for associating gene identifiers with documents.</title>
<date>2006</date>
<journal>BMC Bioinformatics,</journal>
<volume>7</volume>
<issue>440</issue>
<contexts>
<context position="13913" citStr="Cohen and Minkov, 2006" startWordPosition="2265" endWordPosition="2268">at zij was reached from. The intuition behind this last feature is that nodes linked to multiple query nodes, where applicable, are more relevant. For example, for the query term “girl” in the graph depicted in Figure 1, the target node “boys” is described by the features (denoted as feature-name.featurevalue): sequence.nsubj.nsubj-inv (where mention and as-term edges are omitted) , lexical.’“like” etc. In this work, the features encoded are binary. However, features can be assugned numeric weights that corresponds to the probability of the indicator being true for any path between x and zij (Cohen and Minkov, 2006). 5 Path-Constrained Graph Walk While node reranking allows the incorporation of high-level features that describe the traversed paths, it is desirable to incorporate such information earlier in the graph walk process. In this paper, we suggest a variant of a graph-walk, which is constrained by path information. Assume that preliminary knowledge is available that indicates the probability of reaching a relevant node after following a particular edge type sequence (path) from the query distribution VQ to some node x. Rather than fix the edge weights O, we can evaluate the weights of the outgoin</context>
</contexts>
<marker>Cohen, Minkov, 2006</marker>
<rawString>William W. Cohen and Einat Minkov. 2006. A graphsearch framework for associating gene identifiers with documents. BMC Bioinformatics, 7(440).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Terry Koo</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="11294" citStr="Collins and Koo, 2005" startWordPosition="1818" endWordPosition="1821">core according to the example’s labels, pOpt z .4 Specifically, pOpt z is set to 1 in case that the node z is relevant or 0 otherwise. The error is averaged over a set of example instantiations of size N. The cost function is minimized by gradient descent where the derivative of the error with respect to an edge weight O` is derived by decomposing the walk into single time steps, and considering the contribution of each node traversed to the final node score. 4.2 Node Reranking Reranking of the top candidates in a ranked list has been successfully applied to multiple NLP tasks (Collins, 2002; Collins and Koo, 2005). In essence, discriminative reranking allows the re-ordering of results obtained by methods that perform some form of local search, using features that encode higher level information. 4For every example query, a handful of the retrieved nodes are considered, including both relevant and irrelevant nodes. 1 N z∈N 1 N z∈N E= ez = 1 2 909 A number of features describing the set of paths from VQ can be conveniently computed in the process of executing the graph walk, and it has been shown that reranking using these features can improve results significantly. It has also been shown that reranking </context>
<context position="12799" citStr="Collins and Koo, 2005" startWordPosition="2085" endWordPosition="2088">the corresponding output ranked list of li nodes. Let zij be the output node ranked at rank j in li, and let pzij be the probability assigned to zij by the graph walk. Each output node zij is represented through m features, which are computed by pre-defined feature functions f1, ... , fm. The ranking function for node zij is defined as: m F(zij, ¯α) = α0log(pzij) + E αkfk(zij) k=1 where α¯ is a vector of real-valued parameters. Given a new test example, the output of the model is the output node list reranked by F(zij, ¯α). To learn the parameter weights ¯α, we here applied a boosting method (Collins and Koo, 2005) (see also (Minkov et al., 2006)). 4.2.1 Features We evaluate the following feature templates. Edge label sequence features indicate whether a particular sequence of edge labels ii occurred, in a particular order, within the set of paths leading to the target node zij. Lexical unigram feature indicate whether a word mention whose lexical value is tk was traversed in the set of paths leading to zij. Finally, the Source-count feature indicates the number of different source query nodes that zij was reached from. The intuition behind this last feature is that nodes linked to multiple query nodes,</context>
</contexts>
<marker>Collins, Koo, 2005</marker>
<rawString>Michael Collins and Terry Koo. 2005. Discriminative reranking for natural language parsing. Computational Linguistics, 31(1):25–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevyn Collins-Thompson</author>
<author>Jamie Callan</author>
</authors>
<title>Query expansion using random walk models.</title>
<date>2005</date>
<booktitle>In CIKM.</booktitle>
<contexts>
<context position="1758" citStr="Collins-Thompson and Callan, 2005" startWordPosition="261" endWordPosition="264">sures have been used for a variety of language processing applications. In this paper we assume directed graphs, where typed nodes denote entities and labelled directed and weighted edges denote the relations between them. In this framework, graph walks can be applied to draw a measure of similarity between the graph nodes. Previous works have applied graph walks to draw a notion of semantic similarity over such graphs that were carefully designed and manually tuned, based on WordNet reations (Toutanova ∗Current address: Nokia Research Center Cambridge, Cambridge, MA 02142, USA. et al., 2004; Collins-Thompson and Callan, 2005; Hughes and Ramage, 2007). While these and other researchers have used WordNet to evaluate similarity between words, there has been much interest in extracting such a measure from text corpora (e.g., (Snow et al., 2005; Pad´o and Lapata, 2007)). In this paper, we suggest processing dependency parse trees within the general framework of directed labelled graphs. We construct a graph that directly represents a corpus of structured (parsed) text. In the suggested graph scheme, nodes denote words and weighted edges represent the dependency relations between them. We apply graph walks to derive an</context>
<context position="19243" citStr="Collins-Thompson and Callan, 2005" startWordPosition="3216" endWordPosition="3219">s that led to a majority of positively labelled nodes in the training set are followed. 6 Related Work Graph walks over typed graphs have been applied to derive semantic similarity for NLP problems using WordNet as a primary information source. For instance, Hughes and Ramage (2007) constructed a graph which represented various types of word relations from WordNet, and compared random-walk similarity to similarity assessments from humansubject trials. Random-walk similarity has also been used for lexical smoothing for prepositional word attachment (Toutanova et al., 2004) and query expansion (Collins-Thompson and Callan, 2005). In contrast to these works, our graph representation describes parsed text and has not been (consciously) engineered for a particular task. Instead, we include learning techniques to optimize the graphwalk based similarity measure. The learning methods described in this paper can be readily applied to 911 other directed and labelled entity-relation graphs.7 The graph representation described in this paper is perhaps most related to syntax-based vector space models, which derive a notion of semantic similarity from statistics associated with a parsed corpus (Grefenstette, 1994; Lin, 1998; Pad</context>
</contexts>
<marker>Collins-Thompson, Callan, 2005</marker>
<rawString>Kevyn Collins-Thompson and Jamie Callan. 2005. Query expansion using random walk models. In CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Ranking algorithms for namedentity extraction: Boosting and the voted perceptron.</title>
<date>2002</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="11270" citStr="Collins, 2002" startWordPosition="1816" endWordPosition="1817">nd some ideal score according to the example’s labels, pOpt z .4 Specifically, pOpt z is set to 1 in case that the node z is relevant or 0 otherwise. The error is averaged over a set of example instantiations of size N. The cost function is minimized by gradient descent where the derivative of the error with respect to an edge weight O` is derived by decomposing the walk into single time steps, and considering the contribution of each node traversed to the final node score. 4.2 Node Reranking Reranking of the top candidates in a ranked list has been successfully applied to multiple NLP tasks (Collins, 2002; Collins and Koo, 2005). In essence, discriminative reranking allows the re-ordering of results obtained by methods that perform some form of local search, using features that encode higher level information. 4For every example query, a handful of the retrieved nodes are considered, including both relevant and irrelevant nodes. 1 N z∈N 1 N z∈N E= ez = 1 2 909 A number of features describing the set of paths from VQ can be conveniently computed in the process of executing the graph walk, and it has been shown that reranking using these features can improve results significantly. It has also be</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Ranking algorithms for namedentity extraction: Boosting and the voted perceptron. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In LREC.</booktitle>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michelangelo Diligenti</author>
<author>Marco Gori</author>
<author>Marco Maggini</author>
</authors>
<title>Learning web page scores by error backpropagation.</title>
<date>2005</date>
<booktitle>In IJCAI.</booktitle>
<contexts>
<context position="10448" citStr="Diligenti et al., 2005" startWordPosition="1663" endWordPosition="1666">t Tuning There are several motivations for learning the graph weights O in this domain. First, some dependency relations – foremost, subject and object – are in general more salient than others (Lin, 1998; Pad´o and Lapata, 2007). In addition, dependency relations may have varying importance per different notions of word similarity (e.g., noun vs. verb similarity (Resnik and Diab, 2000)). Weight tuning allows the adaption of edge weights to each task (i.e., distribution of queries). The weight tuning method implemented in this work is based on an error backpropagation hill climbing algorithm (Diligenti et al., 2005). The algorithm minimizes the following cost function: (pz − pOpt z )2 where ez is the error for a target node z defined as the squared difference between the final score assigned to z by the graph walk, pz, and some ideal score according to the example’s labels, pOpt z .4 Specifically, pOpt z is set to 1 in case that the node z is relevant or 0 otherwise. The error is averaged over a set of example instantiations of size N. The cost function is minimized by gradient descent where the derivative of the error with respect to an edge weight O` is derived by decomposing the walk into single time </context>
</contexts>
<marker>Diligenti, Gori, Maggini, 2005</marker>
<rawString>Michelangelo Diligenti, Marco Gori, and Marco Maggini. 2005. Learning web page scores by error backpropagation. In IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>Explorations in Automatic Thesaurus Discovery.</title>
<date>1994</date>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="19827" citStr="Grefenstette, 1994" startWordPosition="3307" endWordPosition="3308">Collins-Thompson and Callan, 2005). In contrast to these works, our graph representation describes parsed text and has not been (consciously) engineered for a particular task. Instead, we include learning techniques to optimize the graphwalk based similarity measure. The learning methods described in this paper can be readily applied to 911 other directed and labelled entity-relation graphs.7 The graph representation described in this paper is perhaps most related to syntax-based vector space models, which derive a notion of semantic similarity from statistics associated with a parsed corpus (Grefenstette, 1994; Lin, 1998; Pad´o and Lapata, 2007). In most cases, these models construct vectors to represent each word wi, where each element in the vector for wi corresponds to particular “context” c, and represents a count or an indication of whether wi occurred in context c. A “context” can refer to simple co-occurrence with another word wj, to a particular syntactic relation to another word (e.g., a relation of “direct object” to wj), etc. Given these word vectors, inter-word similarity is evaluated using some appropriate similarity measure for the vector space, such as cosine vector similarity, or Li</context>
</contexts>
<marker>Grefenstette, 1994</marker>
<rawString>Gregory Grefenstette. 1994. Explorations in Automatic Thesaurus Discovery. Kluwer Academic Publishers, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="22183" citStr="Hearst, 1992" startWordPosition="3673" endWordPosition="3674">aluate the text representation schema and the proposed set of graph-based similarity measures on the task of coordinate term extraction. In particular, 7We refer the reader to the TextGraph workshop proceedings, http://textgraphs.org. we evaluate the extraction of named entities, including city names and person names from newswire data, using word similarity measures. Coordinate terms reflect a particular type of word similarity (relatedness), and are therefore an appropriate test case for our framework. While coordinate term extraction is often addressed by a rule-based (templates) approach (Hearst, 1992), this approach was designed for very large corpora such as the Web, where the availability of many redundant documents allows use of high-precision and low-recall rules. In this paper we focus on relatively small corpora. Small limited text collections may correspond to documents residing on a personal desktop, email collections, discussion groups and other specialized sets of documents. The task defined in the experiments is to retrieve a ranked list of city or person names given a small set of seeds. This task is implemented in the graph as a query, where we let the query distribution UQ be</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thad Hughes</author>
<author>Daniel Ramage</author>
</authors>
<title>Lexical semantic relatedness with random graph walks.</title>
<date>2007</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="1784" citStr="Hughes and Ramage, 2007" startWordPosition="265" endWordPosition="268">of language processing applications. In this paper we assume directed graphs, where typed nodes denote entities and labelled directed and weighted edges denote the relations between them. In this framework, graph walks can be applied to draw a measure of similarity between the graph nodes. Previous works have applied graph walks to draw a notion of semantic similarity over such graphs that were carefully designed and manually tuned, based on WordNet reations (Toutanova ∗Current address: Nokia Research Center Cambridge, Cambridge, MA 02142, USA. et al., 2004; Collins-Thompson and Callan, 2005; Hughes and Ramage, 2007). While these and other researchers have used WordNet to evaluate similarity between words, there has been much interest in extracting such a measure from text corpora (e.g., (Snow et al., 2005; Pad´o and Lapata, 2007)). In this paper, we suggest processing dependency parse trees within the general framework of directed labelled graphs. We construct a graph that directly represents a corpus of structured (parsed) text. In the suggested graph scheme, nodes denote words and weighted edges represent the dependency relations between them. We apply graph walks to derive an inter-word similarity mea</context>
<context position="18892" citStr="Hughes and Ramage (2007)" startWordPosition="3167" endWordPosition="3170">the pathtree, it will be represented by multiple node pairs, e.g., &lt; T(l → n), x3 &gt; and &lt; T(m → l, x3 &gt;. A pseudo-code for a path-constrained graph walk is given in Figure 3. It is straight-forward to discard paths in T that are associated with a lower probability than some threshold. A threshold of 0.5, for example, implies that only paths that led to a majority of positively labelled nodes in the training set are followed. 6 Related Work Graph walks over typed graphs have been applied to derive semantic similarity for NLP problems using WordNet as a primary information source. For instance, Hughes and Ramage (2007) constructed a graph which represented various types of word relations from WordNet, and compared random-walk similarity to similarity assessments from humansubject trials. Random-walk similarity has also been used for lexical smoothing for prepositional word attachment (Toutanova et al., 2004) and query expansion (Collins-Thompson and Callan, 2005). In contrast to these works, our graph representation describes parsed text and has not been (consciously) engineered for a particular task. Instead, we include learning techniques to optimize the graphwalk based similarity measure. The learning me</context>
</contexts>
<marker>Hughes, Ramage, 2007</marker>
<rawString>Thad Hughes and Daniel Ramage. 2007. Lexical semantic relatedness with random graph walks. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Keenan</author>
<author>Bernard Comrie</author>
</authors>
<title>Noun phrase accessibility and universal grammar.</title>
<date>1977</date>
<journal>Linguistic Inquiry,</journal>
<volume>8</volume>
<contexts>
<context position="21021" citStr="Keenan and Comrie, 1977" startWordPosition="3492" endWordPosition="3495">osine vector similarity, or Lin’s similarity (Lin, 1998). Recently, Pad´o and Lapata (Pad´o and Lapata, 2007) have suggested an extended syntactic vector space model called dependency vectors, in which rather than simple counts, the components of a word vector of contexts consist of weighted scores, which combine both co-occurrence frequency and the importance of a context, based on properties of the connecting dependency paths. They considered two different weighting schemes: a length weighting scheme, assigning lower weight to longer connecting paths; and an obliqueness weighting hierarchy (Keenan and Comrie, 1977), assigning higher weight to paths that include grammatically salient relations. In an evaluation of word pair similarity based on statistics from a corpus of about 100 million words, they show improvements over several previous vector space models. Below we will compare our framework to that of Pad´o and Lapata. One important difference is that while Pad´o and Lapata make manual choices (regarding the set of paths considered and the weighting scheme), we apply learning to adjust the analogous parameters. 7 Extraction of Coordinate Terms We evaluate the text representation schema and the propo</context>
</contexts>
<marker>Keenan, Comrie, 1977</marker>
<rawString>Edward Keenan and Bernard Comrie. 1977. Noun phrase accessibility and universal grammar. Linguistic Inquiry, 8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In COLING-ACL.</booktitle>
<contexts>
<context position="10029" citStr="Lin, 1998" startWordPosition="1601" endWordPosition="1602"> by Minkov and 2In this paper, we consider either uniform edge weights; or, learn the set of weights O from examples. 3We tune K empirically and set γ = 0.5, as in (Minkov and Cohen, 2007). Cohen (Minkov and Cohen, 2007): a hill-climbing method that tunes the graph weights; and a reranking method. We also specify the feature set to be used by the reranking method in the domain of parsed text. 4.1 Weight Tuning There are several motivations for learning the graph weights O in this domain. First, some dependency relations – foremost, subject and object – are in general more salient than others (Lin, 1998; Pad´o and Lapata, 2007). In addition, dependency relations may have varying importance per different notions of word similarity (e.g., noun vs. verb similarity (Resnik and Diab, 2000)). Weight tuning allows the adaption of edge weights to each task (i.e., distribution of queries). The weight tuning method implemented in this work is based on an error backpropagation hill climbing algorithm (Diligenti et al., 2005). The algorithm minimizes the following cost function: (pz − pOpt z )2 where ez is the error for a target node z defined as the squared difference between the final score assigned t</context>
<context position="19838" citStr="Lin, 1998" startWordPosition="3309" endWordPosition="3310"> Callan, 2005). In contrast to these works, our graph representation describes parsed text and has not been (consciously) engineered for a particular task. Instead, we include learning techniques to optimize the graphwalk based similarity measure. The learning methods described in this paper can be readily applied to 911 other directed and labelled entity-relation graphs.7 The graph representation described in this paper is perhaps most related to syntax-based vector space models, which derive a notion of semantic similarity from statistics associated with a parsed corpus (Grefenstette, 1994; Lin, 1998; Pad´o and Lapata, 2007). In most cases, these models construct vectors to represent each word wi, where each element in the vector for wi corresponds to particular “context” c, and represents a count or an indication of whether wi occurred in context c. A “context” can refer to simple co-occurrence with another word wj, to a particular syntactic relation to another word (e.g., a relation of “direct object” to wj), etc. Given these word vectors, inter-word similarity is evaluated using some appropriate similarity measure for the vector space, such as cosine vector similarity, or Lin’s similar</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Einat Minkov</author>
<author>William W Cohen</author>
</authors>
<title>Learning to rank typed graph walks: Local and global approaches.</title>
<date>2007</date>
<booktitle>In WebKDD/KDD-SNA workshop.</booktitle>
<contexts>
<context position="2822" citStr="Minkov and Cohen, 2007" startWordPosition="423" endWordPosition="426">) text. In the suggested graph scheme, nodes denote words and weighted edges represent the dependency relations between them. We apply graph walks to derive an inter-word similarity measure. We further apply learning techniques, adapted to this framework, to improve the derived corpusbased similarity measure. The learning methods applied include existing learning techniques, namely edge weight tuning, where weights are associated with the edge types, and discriminative reranking of graph nodes, using features that describe the possible paths between a graph node and the initial “query nodes” (Minkov and Cohen, 2007). In addition, we outline in this paper a novel method for learning path-constrained graph walks. While reranking allows use of high-level features that describe properties of the traversed paths, the suggested algorithm incorporates this information in the graph walk process. More specifically, we allow the probability flow in the graph to be conditioned on the history of the walk. We show that this method results in improved performance as it directs probability flow to meaningful paths. In addition, it leads 907 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Pro</context>
<context position="8036" citStr="Minkov and Cohen, 2007" startWordPosition="1243" endWordPosition="1246"> more meaningful. In the next section we show that 908 the graph walk paradigm addresses both of these requirements. Further, different edge types, as well as the paths traversed, are expected to have varying importance in different types of word similarity (for example, verbs and nouns are associated with different connectivity patterns). These issues are addressed using learning. 3 Graph Walks and Similarity Queries This section provides a quick overview of the graph walk induced similarity measure. For details, the reader is referred to previous publications (e.g., (Toutanova et al., 2004; Minkov and Cohen, 2007)). In summary, similarity between two nodes in the graph is defined by a weighted graph walk process, where an edge of type E is assigned an edge weight, O`, determined by its type.2 The transition probability of reaching node y from node x over a single time step, Pr(x −→ y), is defined as the weight of their connecting edge, Ol, normalized by the total outgoing weight from x. Given these transition probabilities, and starting from an initial distribution Vq of interest (a query), we perform a graph walk for a finite number of steps K. Further, at each step of the walk, a proportion -y of the</context>
<context position="9608" citStr="Minkov and Cohen, 2007" startWordPosition="1527" endWordPosition="1530">inal distribution R. In this multi-step walk, nodes that are reached from the query nodes by many shorter paths will be assigned a higher score than nodes connected over fewer longer paths. 4 Learning We consider a supervised setting, where we are given a dataset of example queries and labels over the graph nodes, indicating which nodes are relevant to which query. For completeness, we describe here two methods previously described by Minkov and 2In this paper, we consider either uniform edge weights; or, learn the set of weights O from examples. 3We tune K empirically and set γ = 0.5, as in (Minkov and Cohen, 2007). Cohen (Minkov and Cohen, 2007): a hill-climbing method that tunes the graph weights; and a reranking method. We also specify the feature set to be used by the reranking method in the domain of parsed text. 4.1 Weight Tuning There are several motivations for learning the graph weights O in this domain. First, some dependency relations – foremost, subject and object – are in general more salient than others (Lin, 1998; Pad´o and Lapata, 2007). In addition, dependency relations may have varying importance per different notions of word similarity (e.g., noun vs. verb similarity (Resnik and Diab,</context>
<context position="11952" citStr="Minkov and Cohen, 2007" startWordPosition="1929" endWordPosition="1932">ing allows the re-ordering of results obtained by methods that perform some form of local search, using features that encode higher level information. 4For every example query, a handful of the retrieved nodes are considered, including both relevant and irrelevant nodes. 1 N z∈N 1 N z∈N E= ez = 1 2 909 A number of features describing the set of paths from VQ can be conveniently computed in the process of executing the graph walk, and it has been shown that reranking using these features can improve results significantly. It has also been shown that reranking is complementary to weight tuning (Minkov and Cohen, 2007), in the sense that the two techniques can be usefully combined by tuning weights, and then reranking the results. In the reranking approach, for every training example i (1 &lt; i &lt; N), the reranking algorithm is provided with the corresponding output ranked list of li nodes. Let zij be the output node ranked at rank j in li, and let pzij be the probability assigned to zij by the graph walk. Each output node zij is represented through m features, which are computed by pre-defined feature functions f1, ... , fm. The ranking function for node zij is defined as: m F(zij, ¯α) = α0log(pzij) + E αkfk(</context>
</contexts>
<marker>Minkov, Cohen, 2007</marker>
<rawString>Einat Minkov and William W. Cohen. 2007. Learning to rank typed graph walks: Local and global approaches. In WebKDD/KDD-SNA workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Einat Minkov</author>
<author>William W Cohen</author>
<author>Andrew Y Ng</author>
</authors>
<title>Contextual search and name disambiguation in email using graphs.</title>
<date>2006</date>
<booktitle>In SIGIR.</booktitle>
<contexts>
<context position="12831" citStr="Minkov et al., 2006" startWordPosition="2091" endWordPosition="2094">st of li nodes. Let zij be the output node ranked at rank j in li, and let pzij be the probability assigned to zij by the graph walk. Each output node zij is represented through m features, which are computed by pre-defined feature functions f1, ... , fm. The ranking function for node zij is defined as: m F(zij, ¯α) = α0log(pzij) + E αkfk(zij) k=1 where α¯ is a vector of real-valued parameters. Given a new test example, the output of the model is the output node list reranked by F(zij, ¯α). To learn the parameter weights ¯α, we here applied a boosting method (Collins and Koo, 2005) (see also (Minkov et al., 2006)). 4.2.1 Features We evaluate the following feature templates. Edge label sequence features indicate whether a particular sequence of edge labels ii occurred, in a particular order, within the set of paths leading to the target node zij. Lexical unigram feature indicate whether a word mention whose lexical value is tk was traversed in the set of paths leading to zij. Finally, the Source-count feature indicates the number of different source query nodes that zij was reached from. The intuition behind this last feature is that nodes linked to multiple query nodes, where applicable, are more rele</context>
</contexts>
<marker>Minkov, Cohen, Ng, 2006</marker>
<rawString>Einat Minkov, William W. Cohen, and Andrew Y. Ng. 2006. Contextual search and name disambiguation in email using graphs. In SIGIR.</rawString>
</citation>
<citation valid="true">
<date>1995</date>
<booktitle>Proceedings of the sixth message understanding conference (muc-6). In</booktitle>
<publisher>Morgan Kaufmann Publishers, Inc.</publisher>
<location>Columbia, Maryland.</location>
<marker>1995</marker>
<rawString>1995. Proceedings of the sixth message understanding conference (muc-6). In Morgan Kaufmann Publishers, Inc. Columbia, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
<author>Mirella Lapata</author>
</authors>
<title>Dependencybased construction of semantic space models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<marker>Pad´o, Lapata, 2007</marker>
<rawString>Sebastian Pad´o and Mirella Lapata. 2007. Dependencybased construction of semantic space models. Computational Linguistics, 33(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
<author>Mona Diab</author>
</authors>
<title>Measuring verb similarity.</title>
<date>2000</date>
<booktitle>In CogSci.</booktitle>
<contexts>
<context position="10214" citStr="Resnik and Diab, 2000" startWordPosition="1626" endWordPosition="1629">and Cohen, 2007). Cohen (Minkov and Cohen, 2007): a hill-climbing method that tunes the graph weights; and a reranking method. We also specify the feature set to be used by the reranking method in the domain of parsed text. 4.1 Weight Tuning There are several motivations for learning the graph weights O in this domain. First, some dependency relations – foremost, subject and object – are in general more salient than others (Lin, 1998; Pad´o and Lapata, 2007). In addition, dependency relations may have varying importance per different notions of word similarity (e.g., noun vs. verb similarity (Resnik and Diab, 2000)). Weight tuning allows the adaption of edge weights to each task (i.e., distribution of queries). The weight tuning method implemented in this work is based on an error backpropagation hill climbing algorithm (Diligenti et al., 2005). The algorithm minimizes the following cost function: (pz − pOpt z )2 where ez is the error for a target node z defined as the squared difference between the final score assigned to z by the graph walk, pz, and some ideal score according to the example’s labels, pOpt z .4 Specifically, pOpt z is set to 1 in case that the node z is relevant or 0 otherwise. The err</context>
</contexts>
<marker>Resnik, Diab, 2000</marker>
<rawString>Philip Resnik and Mona Diab. 2000. Measuring verb similarity. In CogSci.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Learning syntactic patterns for automatic hypernym discovery.</title>
<date>2005</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="1977" citStr="Snow et al., 2005" startWordPosition="296" endWordPosition="299">ework, graph walks can be applied to draw a measure of similarity between the graph nodes. Previous works have applied graph walks to draw a notion of semantic similarity over such graphs that were carefully designed and manually tuned, based on WordNet reations (Toutanova ∗Current address: Nokia Research Center Cambridge, Cambridge, MA 02142, USA. et al., 2004; Collins-Thompson and Callan, 2005; Hughes and Ramage, 2007). While these and other researchers have used WordNet to evaluate similarity between words, there has been much interest in extracting such a measure from text corpora (e.g., (Snow et al., 2005; Pad´o and Lapata, 2007)). In this paper, we suggest processing dependency parse trees within the general framework of directed labelled graphs. We construct a graph that directly represents a corpus of structured (parsed) text. In the suggested graph scheme, nodes denote words and weighted edges represent the dependency relations between them. We apply graph walks to derive an inter-word similarity measure. We further apply learning techniques, adapted to this framework, to improve the derived corpusbased similarity measure. The learning methods applied include existing learning techniques, </context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2005</marker>
<rawString>Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005. Learning syntactic patterns for automatic hypernym discovery. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Egidio Terra</author>
<author>C L A Clarke</author>
</authors>
<title>Frequency estimates for statistical word similarity measures.</title>
<date>2003</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="30362" citStr="Terra and Clarke, 2003" startWordPosition="5032" endWordPosition="5035"> is preferable for the city name extraction task, and the syntactic dependency vectors model gives substantially better performance for person name extraction. We conjecture that city name mentions are less structured in the underlying text. In addition, the syntactic weighting scheme of the DV model is probably not optimal for the case of city names. For example, a conjunction relation was found highly indicative for city names (see below). However, this relation is not emphasized by the DV weighting schema. As expected, the performance of the vector-based models improves for larger corpora (Terra and Clarke, 2003). These models demonstrate good performance for the larger MUC+AP corpus, but only mediocre performance for the smaller MUC corpus. Contrasting the graph-based methods with the vector-based models, the difference in performance in favor of reranking and PCW, especially for the smaller corpus, can be attributed to two factors. The first factor is learning, which optimizes performance for the underlying data. A second factor is the incorporation of non-local information, encoding properties of the traversed paths. Models. Following is a short description of the models learned by the different me</context>
</contexts>
<marker>Terra, Clarke, 2003</marker>
<rawString>Egidio Terra and C. L. A. Clarke. 2003. Frequency estimates for statistical word similarity measures. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Learning random walk models for inducing word dependency distributions.</title>
<date>2004</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="8011" citStr="Toutanova et al., 2004" startWordPosition="1239" endWordPosition="1242"> paths may be in general more meaningful. In the next section we show that 908 the graph walk paradigm addresses both of these requirements. Further, different edge types, as well as the paths traversed, are expected to have varying importance in different types of word similarity (for example, verbs and nouns are associated with different connectivity patterns). These issues are addressed using learning. 3 Graph Walks and Similarity Queries This section provides a quick overview of the graph walk induced similarity measure. For details, the reader is referred to previous publications (e.g., (Toutanova et al., 2004; Minkov and Cohen, 2007)). In summary, similarity between two nodes in the graph is defined by a weighted graph walk process, where an edge of type E is assigned an edge weight, O`, determined by its type.2 The transition probability of reaching node y from node x over a single time step, Pr(x −→ y), is defined as the weight of their connecting edge, Ol, normalized by the total outgoing weight from x. Given these transition probabilities, and starting from an initial distribution Vq of interest (a query), we perform a graph walk for a finite number of steps K. Further, at each step of the wal</context>
<context position="19187" citStr="Toutanova et al., 2004" startWordPosition="3208" endWordPosition="3211">d of 0.5, for example, implies that only paths that led to a majority of positively labelled nodes in the training set are followed. 6 Related Work Graph walks over typed graphs have been applied to derive semantic similarity for NLP problems using WordNet as a primary information source. For instance, Hughes and Ramage (2007) constructed a graph which represented various types of word relations from WordNet, and compared random-walk similarity to similarity assessments from humansubject trials. Random-walk similarity has also been used for lexical smoothing for prepositional word attachment (Toutanova et al., 2004) and query expansion (Collins-Thompson and Callan, 2005). In contrast to these works, our graph representation describes parsed text and has not been (consciously) engineered for a particular task. Instead, we include learning techniques to optimize the graphwalk based similarity measure. The learning methods described in this paper can be readily applied to 911 other directed and labelled entity-relation graphs.7 The graph representation described in this paper is perhaps most related to syntax-based vector space models, which derive a notion of semantic similarity from statistics associated </context>
</contexts>
<marker>Toutanova, Manning, Ng, 2004</marker>
<rawString>Kristina Toutanova, Christopher D. Manning, and Andrew Y. Ng. 2004. Learning random walk models for inducing word dependency distributions. In ICML.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>