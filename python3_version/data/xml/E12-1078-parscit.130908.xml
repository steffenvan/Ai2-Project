<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005647">
<title confidence="0.9890525">
To what extent does sentence-internal realisation reflect discourse
context? A study on word order
</title>
<author confidence="0.986598">
Sina Zarriell Jonas Kuhn Aoife Cahill
</author>
<affiliation confidence="0.976719">
Institut f¨ur maschinelle Sprachverarbeitung Educational Testing Service
University of Stuttgart, Germany Princeton, NJ 08541, USA
</affiliation>
<email confidence="0.995715">
zarriesa,jonas@ims.uni-stuttgart.de acahill@ets.org
</email>
<sectionHeader confidence="0.995572" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999289">
We compare the impact of sentence-
internal vs. sentence-external features on
word order prediction in two generation
settings: starting out from a discrimina-
tive surface realisation ranking model for
an LFG grammar of German, we enrich
the feature set with lexical chain features
from the discourse context which can be
robustly detected and reflect rough gram-
matical correlates of notions from theoreti-
cal approaches to discourse coherence. In a
more controlled setting, we develop a con-
stituent ordering classifier that is trained
on a German treebank with gold corefer-
ence annotation. Surprisingly, in both set-
tings, the sentence-external features per-
form poorly compared to the sentence-
internal ones, and do not improve over
a baseline model capturing the syntactic
functions of the constituents.
</bodyText>
<sectionHeader confidence="0.999131" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999942538461539">
The task of surface realization, especially in a rel-
atively free word order language like German, is
only partially determined by hard syntactic con-
straints. The space of alternative realizations that
are strictly speaking grammatical is typically con-
siderable. Nevertheless, for any given choice of
lexical items and prior discourse context, only a
few realizations will come across as natural and
will contribute to a coherent text. Hence, any NLP
application involving a non-trivial generation step
is confronted with the issue of soft constraints on
grammatical alternatives in one way or another.
There are countless approaches to modelling
these soft constraints, taking into account their
interaction with various aspects of the discourse
context (givenness or salience of particular refer-
ents, prior mentioning of particular concepts).
Since so many factors are involved and there is
further interaction with subtle semantic and prag-
matic differentiations, lexical choice, stylistics
and presumably processing factors, theoretical ac-
counts making reliable predictions for real cor-
pus examples have for a long time proven elusive.
As for German, only quite recently, a number of
corpus-based studies (Filippova and Strube, 2007;
Speyer, 2005; Dipper and Zinsmeister, 2009) have
made some good progress towards a coherence-
oriented account of at least the left edge of the
German clause structure, the Vorfeld constituent.
What makes the technological application of
theoretical insights even harder is that for most
relevant factors, automatic recognition cannot be
performed with high accuracy (e.g., a coreference
accuracy in the 70’s means there is a good deal
of noise) and for the higher-level notions such
as the information-structural focus, interannotator
agreement on real corpus data tends to be much
lower than for core-grammatical notions (Poesio
and Artstein, 2005; Ritz et al., 2008).
On the other hand, many of the relevant dis-
course factors are reflected indirectly in proper-
ties of the sentence-internal material. Most no-
tably, knowing the shape of referring expressions
narrows down many aspects of givenness and
salience of its referent; pronominal realizations
indicate givenness, and in German there are even
two variants of the personal pronoun (er and der)
for distinguishing salience. So, if the genera-
tion task is set in such a way that the actual lex-
ical choice, including functional categories such
as determiners, is fully fixed (which is of course
not always the case), one can take advantage of
</bodyText>
<page confidence="0.94334">
767
</page>
<note confidence="0.9765315">
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 767–776,
Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999953105263158">
these reflexes. This explains in part the fairly high
baseline performance of n-gram language mod-
els in the surface realization task. And the effect
can indeed be taken much further: the discrimi-
native training experiments of Cahill and Riester
(2009) show how effective it is to systematically
take advantage of asymmetry patterns in the mor-
phosyntactic reflexes of the discourse notion of
information status (i.e., using a feature set with
well-chosen purely sentence-bound features).
These observations give rise to the question: in
the light of the difficulty in obtaining reliable dis-
course information on the one hand and the effec-
tiveness of exploiting the reflexes of discourse in
the sentence-internal material on the other – can
we nevertheless expect to gain something from
adding sentence-external feature information?
We propose two scenarios for adressing this
question: first, we choose an approximative ac-
cess to context information and relations between
discourse referents – lexical reiteration of head
words, combined with information about their
grammatical relation and topological positioning
in prior sentences. We apply these features in a
rich sentence-internal surface realisation ranking
model for German. Secondly, we choose a more
controlled scenario: we train a constituent order-
ing classifier based on a feature model that cap-
tures properties of discourse referents in terms of
manually annotated coreference relations. As we
get the same effect in both setups – the sentence-
external features do not improve over a baseline
that captures basic morphosyntactic properties of
the constituents – we conclude that sentence-
internal realisation is actually a relatively accurate
predictor of discourse context, even more accurate
than information that can be obtained from coref-
erence and lexical chain relations.
</bodyText>
<sectionHeader confidence="0.999766" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999930877551021">
In the generation literature, most works on ex-
ploiting sentence-external discourse information
are set in a summarisation or content ordering
framework. Barzilay and Lee (2004) propose an
account for constraints on topic selection based on
probabilistic content models. Barzilay and Lapata
(2008) propose an entity grid model which repre-
sents the distribution of referents in a discourse
for sentence ordering. Karamanis et al. (2009)
use Centering-based metrics to assess coherence
in an information ordering system. Clarke and La-
pata (2010) have improved a sentence compres-
sion system by capturing prominence of phrases
or referents in terms of lexical chain information
inspired by Morris and Hirst (1991) and Center-
ing (Grosz et al., 1995). In their system, discourse
context is represented in terms of hard constraints
modelling whether a certain constituent can be
deleted or not.
In the linearisation or surface realisation do-
main, there is a considerable body of work ap-
proximating information structure in terms of
sentence-internal realisation (Ringger et al., 2004;
Filippova and Strube, 2009; Velldal and Oepen,
2005; Cahill et al., 2007). Cahill and Riester
(2009) improve realisation ranking for German –
which mainly deals with word order variation – by
representing precedence patterns of constituents
in terms of asymmetries in their morphosyntac-
tic properties. As a simple example, a pattern ex-
ploited by Cahill and Riester (2009) is the ten-
dency of definite elements tend to precede indef-
inites, which, on a discourse level, reflects that
given entities in a sentence tend to precede new
entities.
Other work on German surface realisation has
highlighted the role of the initial position in the
German sentence, the so-called Vorfeld (or “pre-
field”). Filippova and Strube (2007) show that
once the Vorfeld (i.e. the constituent that precedes
the finite verb) is correctly determined, the pre-
diction of the order in the Mittelfeld (i.e. the con-
stituents that follow the finite verb) is very easy.
Cheung and Penn (2010) extend the approach
of Filippova and Strube (2007) and augment a
sentence-internal constituent ordering model with
sentence-external features inspired from the en-
tity grid model proposed by Barzilay and Lapata
(2008).
</bodyText>
<sectionHeader confidence="0.991772" genericHeader="method">
3 Motivation
</sectionHeader>
<bodyText confidence="0.999965818181818">
While there would be many ways to construe
or represent discourse context (e.g. in terms of
the global discourse or information structure), we
concentrate on capturing local coherence through
the distribution of discourse referents in a text.
These discourse referents basically correspond to
the constituents that our surface realisation model
has to put in the right order. As the order of refer-
ents or constituents is arguably influenced by the
information structure of a sentence given the pre-
vious text, our main assumption was that infor-
</bodyText>
<page confidence="0.988937">
768
</page>
<figure confidence="0.8613363">
(1) a. Kurze Zeit sp¨ater erkl¨arte ein Anrufer bei Nachrichtenagenturen in Pakistan , die Gruppe Gamaa bekenne sich.
Shortly after, a caller declared at the news agencies in Pakistan, that the group Gamaa avowes itself.
b. Diese Gruppe wird f¨ur einen Großteil der Gewalttaten verantwortlich gemacht , die seit dreieinhalb Jahren in
¨Agypten ver¨ubt worden sind .
This group is made responsible for most of the violent acts that have been committed in Egypt in the last three and
a half years.
(2) a. Belgien w¨unscht, dass sich WEU und NATO dar¨uber einigen.
Belgium wants that WEU and NATO agree on that.
b. Belgien sieht in der NATO die beste milit¨arische Struktur in Europa.
Belgium sees the best military structure of Europe in the NATO.
</figure>
<listItem confidence="0.931413">
(3) a. Frauen vom Land k¨ampften aktiv darum , ein Staudammprojekt zu verhindern.
Women from the countryside fighted actively to block the dam project.
b. Auch in den St¨adten f¨anden sich immer mehr Frauen in Selbsthilfeorganisationen zusammen.
</listItem>
<bodyText confidence="0.997244866666667">
Also in the cities, more and more women team up in self-help organisations.
mation about the prior mentioning of a referent
would be helpful for predicting the position of this
referent in a sentence.
The idea that the occurence of discourse refer-
ents in a text is a central aspect of discourse struc-
ture has been systematically pursued by Centering
Theory (Grosz et al., 1995). Its most important
notions are related to the realisation of discourse
referents (i.e. described as “centers”) and the way
the centers are arranged in a sequence of utter-
ances to make this sequence a coherent discourse.
Another important concept is the “ranking” of dis-
course referents which basically determines the
prominence of a referent in a certain sentence and
is driven by several factors (e.g. their grammati-
cal function). For free word order languages like
German, word order has been proposed as one of
the factors that account for the ranking (Poesio et
al., 2004). In a similar spirit, Morris and Hirst
(1991) have proposed that chains of (related) lex-
ical items in a text are an important indicator of
text structure.
Our main hypothesis was that it is possible to
exploit these intuitions from Centering Theory
and the idea of lexical chains for word order pre-
diction. Thus, we expected that it would be easier
to predict the position of a referent in a sentence
if we have not only given its realisation in the cur-
rent utterance but also its prominence in the previ-
ous discourse. Especially, we expected this intu-
ition to hold for cases where the morpho-syntactic
realisation of a constituent does not provide many
clues. This is illustrated in Examples (1) and (2)
which both exemplify the reiteration of a lexical
item in two subsequent sentences, (reiteration is
one type of lexical chain discussed in Morris and
Hirst (1991)). In Example (1), the second instance
of the noun ‘group’ is modified by a demonstra-
tive pronoun such that its “known” and prominent
discourse status is overt in the morpho-syntactic
realisation. In Example (2), both instances of
“Belgium” are realised as bare proper nouns with-
out an overt morphosyntactic clue indicating their
discourse status.
Beyond the simple presence of reitered items in
sequences of sentences, we expected that it would
be useful to look at the position and syntactic
function of the previous mentions of a discourse
referent. In Example (1), the reiterated item is first
introduced in an embedded sentence and realised
in the Vorfeld in the second utterance. In terms
of centering, this transition would correspond to
a topic shift. In Example (2), both instances are
realised in the Vorfeld, such that the topic of the
first sentence is carried over to the next.
In Example (3), we illustrate a further type of
lexical reiteration. In this case, two identical head
nouns are realised in subsequent sentences, even
though they refer to two different discourse refer-
ents. While this type of lexical chain is described
as “reiteration without identity of referents” by
Morris and Hirst (1991), it would not be captured
in Centering since this is not a case of strict coref-
erence. On the other hand, lexical chains do not
capture types of reiterated discourse referents that
have distinct morpho-syntactic realisations, e.g.
nouns and pronouns.
Originally, we had the hypothesis that strict
corefence information is more useful and accurate
for word order prediction than rather loose lexi-
cal chains which conflate several types of referen-
tial and lexical relations. However, the advantage
of chains, especially chains of reiteration, is that
they can be easily detected in any corpus text and
</bodyText>
<page confidence="0.988836">
769
</page>
<bodyText confidence="0.9999365">
that they might capture “topics” of sentences be-
yond the identity of referents. Thus, we started
out from the idea of lexical chains and added cor-
responding features in a statistical ranking model
for surface realisation of German (Section 4). As
this strategy did not work out, we wanted to assess
whether an ideal coreference annotation would be
helpful at all for predicting word order. In a sec-
ond experiment, we use a corpus which is manu-
ally annotated for coreference (Section 5).
</bodyText>
<sectionHeader confidence="0.9567115" genericHeader="method">
4 Experiment 1: Realisation Ranking
with Lexical Chains
</sectionHeader>
<bodyText confidence="0.9999059">
In this Section, we present an experiment that in-
vestigates sentence-external context in a surface
realisation task. The sentence-external context is
represented in terms of lexical chain features and
compared to sentence-internal models which are
based on morphosyntactic features. The experi-
ment thus targets a generation scenario where no
coreference information is available and aims at
assessing whether relatively naive context infor-
mation is also useful.
</bodyText>
<subsectionHeader confidence="0.995226">
4.1 System Description
</subsectionHeader>
<bodyText confidence="0.999476947368421">
We carry out our first experiment in a regener-
ation set-up with two components: a) a large-
scale hand-crafted Lexical Functional Grammar
(LFG) for German (Rohrer and Forst, 2006), used
to parse and regenerate a corpus sentence, b)
a stochastic ranker that selects the most appro-
priate regenerated sentence in context according
to an underlying, linguistically motivated feature
model. In contrast to fully statistical linearisation
methods, our system first generates the full set
of sentences that correspond to the grammatically
well-formed realisations of the intermediate syn-
tactic representation.&apos; This representation is an
f-structure, which underspecifies the order of con-
stituents and, to some extent, their morphological
realisation, such that the output sentences contain
all possible combinations of word order permu-
tations and morphological variants. Depending
on the length and structure of the original corpus
sentence, the set of regenerated sentences can be
huge (see Cahill et al. (2007) for details on regen-
erating the German treebank TIGER).
&apos;There are occasional mistakes in the grammar which
sometimes lead to ungrammatical strings being generated,
but this is rare.
The realisation ranking component is an SVM
ranking model implemented with SVMrank,
a Support Vector Machine-based learning tool
(Joachims, 2006). During training, each sentence
is annotated with a rank and a set of features ex-
tracted from the F-structure, its surface string and
external resources (e.g. a language model). If
the sentence matches the original corpus string,
its rank will be highest, the assumption being that
the original sentence corresponds to the optimal
realisation in context. The output of generation,
the top-ranked sentence, is evaluated against the
original corpus sentence.
</bodyText>
<subsectionHeader confidence="0.993911">
4.2 The Feature Models
</subsectionHeader>
<bodyText confidence="0.9991585">
As the aim of this experiment is to better un-
derstand the nature of sentence-internal features
reflecting discourse context and compare them
to sentence-external ones, we build several fea-
ture models which capture different aspects of the
constituents in a given sentence. The sentence-
internal features describe the morphosyntacic re-
alisation of constituents, for instance their func-
tion (“subject”, “object”), and can be straightfor-
wardly extracted from the f-structure. These fea-
tures are then combined into discriminative prece-
dence features, for instance “subject-precedes-
object”. We implement the following types of
morphosyntactic features:
</bodyText>
<listItem confidence="0.999543090909091">
• syntactic function (arguments and adjuncts)
• modification (e.g. nouns modified by relative
clauses, genitive etc.)
• syntactic category (e.g. adverbs, proper
nouns, phrasal arguments)
• definiteness for nouns
• number and person for nominal elements
• types of pronouns (e.g. demonstrative, re-
flexive)
• constituent span and number of embedded
nodes in the tree
</listItem>
<bodyText confidence="0.9997433">
In addition, we also include language model
scores in our ranking model. In Section 4.4,
we report on results for several subsets of these
features where “BaseSyn” refers to a model that
only includes the syntactic function features and
“FullMorphSyn” includes all features mentioned
above.
For extracting the lexical chains, we check for
any overlapping nouns in the n sentences previ-
ous to the current one being generated. We check
</bodyText>
<page confidence="0.968587">
770
</page>
<table confidence="0.9986287">
Rank Sentence and Features
1 % Diese Gruppe wird f¨ur einen Großteil der Gewalttaten verantwortlich gemacht.
% This group is for a major part of the violent acts responsible made.
subject-&lt;-pp-object, demonstrative-&lt;-indefinite, overlap-&lt;-no-overlap, overlap-in-vorfeld, lm:-7.89
3 % F¨ur einen Großteil der Gewalttaten wird diese Gruppe verantwortlich gemacht.
% Fora major part of the violent acts is this group responsible made.
pp-object-&lt;-subject, indefinite-&lt;-demonstrative, no-overlap-&lt;-overlap, no-overlap-in-vorfeld, lm:-10.33
3 % Verantwortlich gemacht wird diese Gruppe f¨ur einen Großteil der Gewalttaten.
% Responsible made is this group for a major part of the violent acts.
subject-&lt;-pp-object, demonstrative-&lt;-indefinite, overlap-&lt;-no-overlap, lm:-9.41
</table>
<figureCaption confidence="0.996641">
Figure 1: Made-up training example for realisation ranking with precedence features
</figureCaption>
<bodyText confidence="0.999749518518518">
proper and common nouns, considering full and
partial overlaps as shown in Examples (1) and
(2), where the (a) example is the previous sen-
tence in the corpus. For each overlap, we record
the following properties: (i) function in the previ-
ous sentence, (ii) position in the previous sentence
(e.g. Vorfeld), (iii) distance between sentences,
(iv) total number of overlaps.
These overlap features are then also
combined in terms of precedence, e.g.
“has subject overlap:3-precedes-no overlap”,
meaning that in the current sentence a noun
that was previously mentioned in a subject 3
sentences ago precedes a noun that was not
mentioned before.
In Figure 1, we give an example of a set of gen-
eration alternatives and their (partial) feature rep-
resentation for the sentence (1-b). Precedence is
indicated by ”&lt;”.
Basically, our sentence-external feature model
is built on the intuition that lexical chains or over-
laps approximate discourse status in a way which
is similar to sentence-internal morphosyntactic
properties. Thus, we would expect that overlaps
indicate givenness, salience or prominence and
that asymmetries between overlapping and non-
overlapping entities are helpful in the ranking.
</bodyText>
<subsectionHeader confidence="0.995518">
4.3 Data
</subsectionHeader>
<bodyText confidence="0.999626454545455">
All our models are trained on 7,039 sentences
(subdivided into 1259 texts) from the TIGER
Treebank of German newspaper text (Brants et al.,
2002). We tune the parameters of our SVM model
on a development set of 55 sentences and report
the final results for our unseen test set of 240 sen-
tences. Table 1 shows how many sentences in our
training, development and test sets have at least
one textually overlapping phrase in the previous
1–10 sentences.
We choose the TIGER treebank, which has no
</bodyText>
<table confidence="0.99942625">
# Sentences % Sentences with overlap
in context Training Dev Test
1 20.96 23.64 20.42
2 35.42 40.74 35.00
3 45.58 50.00 53.33
4 52.66 53.70 58.75
5 57.45 58.18 64.58
6 61.42 57.41 68.75
7 64.58 61.11 70.83
8 67.05 62.96 72.08
9 69.20 64.81 74.17
10 71.16 70.37 75.83
</table>
<tableCaption confidence="0.993572">
Table 1: The percentage of sentences that have at least
one overlapping entity in the previous n sentences
</tableCaption>
<bodyText confidence="0.999754791666667">
coreference annotation, since we already have a
number of resources available to match the syn-
tactic analyses produced by our grammar against
the analyses in the treebank. Thus, in our regen-
eration system, we parse the sentences with the
grammar, and choose the parsed f-structures that
are compatible with the manual annotation in the
TIGER treebank as is done in Cahill et al. (2007).
This compatibility check eliminates noise which
would be introduced by generating from incorrect
parses (e.g. incorrect PP-attachments typically re-
sult in unnatural and non-equivalent surface reali-
sations).
For comparing the string chosen by the mod-
els against the original corpus sentence, we use
BLEU, NIST and exact match. Exact match is
a strict measure that only credits the system if it
chooses the exact same string as the original cor-
pus string. BLEU and NIST are more relaxed
measures that compare the strings on the n-gram
level. Finally, we report accuracy scores for the
Vorfeld position (VF) corresponding to the per-
centage of sentences generated with a correct Vor-
feld.
</bodyText>
<page confidence="0.995065">
771
</page>
<table confidence="0.990464916666667">
S� BLEU NIST Exact VF
0 0.766 11.885 50.19 64.0
1 0.765 11.756 49.78 64.0
2 0.765 11.886 50.01 64.1
3 0.765 11.885 50.08 63.8
4 0.761 11.723 49.43 63.2
5 0.765 11.884 49.71 64.2
6 0.768 11.892 50.42 64.6
7 0.765 11.885 50.01 64.5
8 0.764 11.884 49.78 64.3
9 0.765 11.888 49.82 63.6
10 0.764 11.889 49.7 63.5
</table>
<tableCaption confidence="0.881096">
Table 2: Tenfold-crossvalidation for feature model
FullMorphSyn and different context windows (Sc.)
</tableCaption>
<table confidence="0.999876142857143">
Model BLEU VF
Language Model 0.702 51.2
Language Model + Context S. = 5 0.715 54.3
BaseSyn 0.757 62.0
BaseSyn + Context S. = 5 0.760 63.0
FullMorphSyn 0.766 64.0
FullMorphSyn + Context S. = 5 0.763 64.2
</table>
<tableCaption confidence="0.995005">
Table 3: Evaluation for different feature models; ‘Lan-
guage Model’: ranking based on language model
scores, ‘BaseSyn’: precedence between constituent
functions, ‘FullMorphSyn’: entire set of sentence-
internal features.
</tableCaption>
<subsectionHeader confidence="0.69897">
4.4 Results
</subsectionHeader>
<bodyText confidence="0.999984866666667">
In Table 2, we report the performance of the full
sentence-internal feature model combined with
context windows from zero to ten. The scores
have been obtained from tenfold-crossvalidation.
For none of the context windows, the model out-
performs the baseline with a zero context which
has no sentence-external features. In Table 3,
we compare the performance of several feature
models corresponding to subsets of the features
used so far which are combined with sentence-
external features respectively. We note that the
function precedence features (i.e. the ‘BaseSyn’
model) are very powerful, leading to a major im-
provement compared to a language model. The
sentence-external features lead to an improvement
when combined with the language-model based
ranking. However, this improvement is leveled
out in the BaseSyn model.
On the one hand, the fact that the lexical chain
features improve a language-model based ranking
suggests these features are, to some extent, pre-
dictive for certain patterns of German word order.
On the other hand, the fact that they don’t improve
over an informed sentence-internal baseline sug-
gests that these patterns are equally well captured
by morphosyntactic features. However, we cannot
exclude the possibility that the chain features are
too noisy as they conflate several types of lexical
and coreferential relations. This will be adressed
in the following experiment.
</bodyText>
<sectionHeader confidence="0.9602065" genericHeader="method">
5 Experiment 2: Constituent Ordering
with Centering-inspired Features
</sectionHeader>
<bodyText confidence="0.999907888888889">
We now look at a simpler generation setup where
we concentrate on the ordering of constituents in
the German Vorfeld and Mittelfeld. This strat-
egy has also been adopted in previous investiga-
tions of German word order: Filippova and Strube
(2007) show that once the German Vorfeld is cor-
rectly chosen, the prediction accuracy for the Mit-
telfeld (the constituents following the finite verb)
is in the 90s.
In order to eliminate noise introduced from po-
tentially heterogeneous chain features, we look at
coreference features and, again, compare them to
sentence-internal morphosyntactic features. We
target a generation scenario where coreference in-
formation is available. The aim is to establish an
upper bound concerning the quality improvement
for word order prediction by recurring to manual
corefence annotation.
</bodyText>
<subsectionHeader confidence="0.993666">
5.1 Data and Setup
</subsectionHeader>
<bodyText confidence="0.9999835">
We carry out the constituent ordering experiment
on the T¨uba-D/Z treebank (v5) of German news-
paper articles (Telljohann et al., 2006). It com-
prises about 800k tokens in 45k sentences. We
choose this corpus because it is not only annotated
with syntactic analyses but also with coreference
relations (Naumann, 2006). The syntactic annota-
tion format differs from the TIGER treebank used
in the previous experiment, for instance, it ex-
plicitely represents the Vorfeld and Mittelfeld as
phrasal nodes in the tree. This format is very con-
venient for the extraction of constituents in the re-
spective positions.
The T¨uba-D/Z coreference annotation distin-
guishes several relations between discourse ref-
erents, most importantly “coreferential relation”
and “anaphoric relation” where the first denotes
a relation between noun phrases that refer to the
same entity, and the latter refers to a link between
a pronoun and a contextual antecedent, see Nau-
mann (2006) for further detail. We expected the
coreferential relation to be particularly useful, as
</bodyText>
<page confidence="0.993931">
772
</page>
<bodyText confidence="0.999960423076923">
it cannot always be read off the morphosyntac-
tic realisation of a noun phrase, whereas pronouns
are almost always used in an anaphoric relation.
The constituent ordering model is implemented
as a classifier that is given a set of constituents
and predicts the constituent that is most likely to
be realised in the Vorfeld.
The set of candidate constituents is determined
from the tree of the original corpus sentence. We
will assume that all constituents under a Vorfeld
and Mittelfeld node can be freely reordered. Thus,
we do not check whether the word order variants
we look at are actually grammatical assuming that
most of them are. In this sense, this experiment
is close to fully statistical generation approaches.
As a further simplification, we do not look at mor-
phological generation variants of the constituents
or their head verb.
The classifier is implemented with SVMrank
again. In contrast to the previous experiment
where we learned to rank sentences, the classi-
fier now learns to rank constituents. The con-
stituents have been extracted using the tool de-
scribed in Bouma (2010). The final data set com-
prises 48.513 candidate sets of freely orderable
constituents.
</bodyText>
<subsectionHeader confidence="0.999406">
5.2 Centering-inspired Feature Model
</subsectionHeader>
<bodyText confidence="0.999624608695652">
To compare the discourse context model against a
sentence-based model, we implemented a number
of sentence-internal features that are very similar
to the features used in the previous experiment.
Since we extract them from the syntactic annota-
tion instead of f-structures, some labels and fea-
ture names will be different, however, the design
of the sentence-internal model is identical to the
previous one in Section 4.
The sentence-external features differ in some
aspects from Section 4, since we extract coref-
erence relations of several types (see (Naumann,
2006) for the anaphoric relations annotated in the
Tueba-D/Z). For each type of coreference link,
we extract the following properties: (i) function
of the antecedent, (ii) position of the antecedent,
(iii) distance between sentences, (iv) type of rela-
tion. We also distinguish coreference links anno-
tated for the whole phrase (“head link”) and links
that are annotated for an element embedded by the
constituent (“contained link”). The two types are
illustrated in Examples (4) and (5). Note that both
cases would not have been captured in the lexical
</bodyText>
<table confidence="0.999692">
# VF # MF
Backward Center 3.5% 5.1%
Forward Center 6.8% 6.8%
Coref Link 30.5% 23.4%
</table>
<tableCaption confidence="0.987114">
Table 4: Backward and forward centers and their posi-
tions
</tableCaption>
<bodyText confidence="0.9992645">
chain model since there is no lexical overlap be-
tween the realisations of the discourse referents.
These types of coreference features implicitly
carry the information that would also be consid-
ered in a Centering formalisation of discourse
context. In addition to these, we designed features
that explicitly describe centers as these might
have a higher weight. In line with Clarke and
Lapata (2010), we compute backward (CB) and
forward centers (CF) in the following way:
</bodyText>
<listItem confidence="0.996816777777778">
1. Extract all entities from the current sentence
and the previous sentence.
2. Rank the entities of the previous sentence ac-
cording to their function (subject &lt; direct
object &lt; indirect object ...).
3. Find the highest ranked entity in the previous
sentence that has a link to an entity in the
current sentence, this entity is the CB of the
sentence.
</listItem>
<bodyText confidence="0.999624785714286">
In the same way, we mark entities as forward
centers that are ranked highest in the current sen-
tence and have a link to an entity in the following
sentence.2 In Table 4, we report the percentage of
sentences that have backward and forward centers
in the Vorfeld or Mittelfeld. While the percentage
of sentences that realise a backward center is quite
low, the overall proportion of sentences contain-
ing some type of coreference link is in a dimen-
sion such that the learner could definitely pick up
some predictive patterns. Going by the relative
frequencies, coreferential constituents have a bias
towards appearing in the Vorfeld rather than in the
Mittelfeld.
</bodyText>
<subsectionHeader confidence="0.803839">
5.3 Results
</subsectionHeader>
<bodyText confidence="0.99998525">
First, we build three coreference-based con-
stituent classifiers on their entire training set and
compare them to their sentence-internal baseline.
The most simple baseline records the category of
</bodyText>
<footnote confidence="0.953927333333333">
2In Centering, all entities in a given utterance can be seen
as forward centers, however we thought that this implemen-
tation would be more useful.
</footnote>
<page confidence="0.993483">
773
</page>
<figure confidence="0.5301649">
(4) a. Die Rechnung geht an die AWO.
The bill goes to the AWO.
b. [Hintergrund der gegenseitigen Vorw¨urfe in der Arbeiterwohlfahrt] sind offenbar scharfe Konkurrenzen zwischen
Bremern und Bremerhavenern.
Apparently, [the background of the mutual accusations at the labour welfare] are rivalries between people from
Bremen and Bremerhaven.
(5) a. Dies ist die Behauptung, mit der Bremens H¨afensenator die Skeptiker davon ¨uberzeugt hat, [...].
This is the claim, which Bremen’s harbour senator used to convince doubters, [...].
b. F¨ur diese Behauptung hat Beckmeyer bisher keinen Nachweis geliefert. So far, Beckmeyer has not given a prove of
this claim.
</figure>
<table confidence="0.991509571428571">
Model VF
ConstituentLength + HeadPos 47.48%
ConstituentLength + HeadPos + Coref 51.30%
BaseSyn 54.82%
BaseSyn + Coref 56.21%
FullMorphSyn 57.24%
FullMorphSyn + Coref 57.40%
</table>
<tableCaption confidence="0.9889395">
Table 5: Results from Vorfeld classification, training
and evaluation on entire treebank
</tableCaption>
<bodyText confidence="0.999745193548387">
the constituent head and the number of words that
the constituent spans. Additionally, in parallel to
the experiment in Section 4, we build a “BaseSyn”
model which has the syntactic function features,
and a “FullMorphSyn” model which comprises
the entire set of sentence-internal features. To
each of these baseline, we add the coreference
features. The results are reported in Table 5.
In this experiment, we find an effect of
the sentence-external features over the simple
sentence-internal baselines. However, in the fully
spelled-out, sentence-internal model, the effect
is, again, minimal. Moreover, for each base-
line, we obtain higher improvements by adding
further sentence-internal features than by adding
sentence-external ones the accuracy of the sim-
ple baseline (47.48%) improves by 7.34 points
through adding function features (the accuracy
of BaseSyn is 54.82%) and by only 3.48 points
through adding coreference features.
We run a second experiment in order to so see
whether the better performance of the sentence-
internal features is related to their coverage. We
build and evaluate the same set of classifiers on
the subset of sentences that contain at least one
coreference link for one of its constituents (see
Table 4 for the distribution of coreference links
in our data). The results are given in Table 6. In
this experiment, the coreference features improve
over all sentence-internal baselines including the
‘FullMorphSyn’ model.
</bodyText>
<table confidence="0.999206">
Model VF
ConstituentLength + HeadPos 46.61%
ConstituentLength + HeadPos + Coref 52.23%
BaseSyn 54.63%
BaseSyn + Coref 56.67%
FullMorphSyn 55.36%
FullMorphSyn + Coref 57.93%
</table>
<tableCaption confidence="0.957125666666667">
Table 6: Results from Vorfeld classification, training
and evaluation on sentences that contain a coreference
link
</tableCaption>
<subsectionHeader confidence="0.775443">
5.4 Discussion
</subsectionHeader>
<bodyText confidence="0.999997034482759">
The results presented in this Section consis-
tently complete the picture that emerged from
the experiments in Section 4. Even if we have
high quality information about discourse con-
text in terms of relations between referents, a
non-trivial sentence-internal model for word or-
der prediction can be hardly improved. This
suggests that sentence-internal approximations of
discourse context provide a fairly good way of
dealing with local coherence in a linearisation
task. It is also interesting that the sentence-
external features improve over simple baselines,
but get leveled out in rich sentence-internal fea-
ture models. From this, we conclude that the
sentence-external features we implemented are to
some extent predictive for word order, but that
they can be covered by sentence-internal features
as well.
Our second evaluation concentrating on the
sentences that have coreference information
shows that the better performance of the sentence-
internal features is also related to their cover-
age. These results confirm our initial intuition
that coreference information can add to the pre-
dictive power of the morpho-syntactic features in
certain contexts. This positive effect disappears
when sentences with and without coreferential
constituents are taken together. For future work,
it would be promising to investigate whether the
</bodyText>
<page confidence="0.993393">
774
</page>
<bodyText confidence="0.99899875">
positive impact of coreference features can be
strengthened if the coreference annotation scheme
is more exhaustive, including, e.g., bridging and
event anaphora.
</bodyText>
<sectionHeader confidence="0.998614" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999915">
We have carried out a number of experiments that
show that sentence-internal models for word order
are hardly improved by features which explicitely
represent the preceding context of a sentence in
terms of lexical and referential relations between
discourse entities. This suggests that sentence-
internal realisation implicitly carries a lot of im-
formation about discourse context. On average,
the morphosyntactic properties of constituents in
a text are better approximates of their discourse
status than actual coreference relations.
This result feeds into a number of research
questions concerning the representation of dis-
course and its application in generation systems.
Although we should certainly not expect a com-
putational model to achieve a perfect accuracy in
the constituent ordering task – even humans only
agree to a certain extent in rating word order vari-
ants (Belz and Reiter, 2006; Cahill, 2009) – the
average accuracy in the 60’s for prediction of Vor-
feld occupance is still moderate. An obvious di-
rection would be to further investigate more com-
plex representations of discourse that take into ac-
count the relations between utterances, such as
topic shifts. Moreover, it is not clear whether the
effects we find for linearisation in this paper carry
over to other levels of generation such as tacti-
cal generation where syntactic functions are not
fully specified. In a broader perspective, our re-
sults underline the need for better formalisations
of discourse that can be translated into features for
large-scale applications such as generation.
</bodyText>
<sectionHeader confidence="0.997487" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999851">
This work was funded by the Collaborative Re-
search Centre (SFB 732) at the University of
Stuttgart.
</bodyText>
<sectionHeader confidence="0.998977" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999785650793651">
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Com-
putational Linguistics, 34:1–34.
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models with applications
to generation and summarization. In Proceedings of
HLT-NAACL 2004, Boston,MA.
Anja Belz and Ehud Reiter. 2006. Comparing auto-
matic and human evaluation of NLG systems. In
Proceedings of EACL 2006, pages 313–320, Trento,
Italy.
Gerlof Bouma. 2010. Syntactic tree queries in prolog.
In Proceedings of the Fourth Linguistic Annotation
Workshop, ACL 2010.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
Treebank. In Proceedings of the Workshop on Tree-
banks and Linguistic Theories.
Aoife Cahill and Arndt Riester. 2009. Incorporat-
ing information status into generation ranking. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP, pages 817–825, Suntec, Singapore,
August. Association for Computational Linguistics.
Aoife Cahill, Martin Forst, and Christian Rohrer.
2007. Stochastic Realisation Ranking for a Free
Word Order Language. In Proceedings of the
Eleventh European Workshop on Natural Language
Generation, pages 17–24, Saarbr¨ucken, Germany.
DFKI GmbH.
Aoife Cahill. 2009. Correlating human and automatic
evaluation of a german surface realiser. In Proceed-
ings of the ACL-IJCNLP 2009 Conference Short Pa-
pers, pages 97–100, Suntec, Singapore, August. As-
sociation for Computational Linguistics.
Jackie C.K. Cheung and Gerald Penn. 2010. Entity-
based local coherence modelling using topological
fields. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguistics
(ACL 2010). Association for Computational Lin-
guistics.
James Clarke and Mirella Lapata. 2010. Discourse
constraints for document compression. Computa-
tional Linguistics, 36(3):411–441.
Stefanie Dipper and Heike Zinsmeister. 2009. The
role of the German Vorfeld for local coherence. In
Christian Chiarcos, Richard Eckart de Castilho, and
Manfred Stede, editors, Von der Form zur Bedeu-
tung: Texte automatisch verarbeiten/From Form to
Meaning: Processing Texts Automatically, pages
69–79. Narr, T¨ubingen.
Katja Filippova and Michael Strube. 2007. The ger-
man vorfeld and local coherence. Journal of Logic,
Language and Information, 16:465–485.
Katja Filippova and Michael Strube. 2009. Tree Lin-
earization in English: Improving Language Model
Based Approaches. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, Companion Volume:
Short Papers, pages 225–228, Boulder, Colorado,
June. Association for Computational Linguistics.
</reference>
<page confidence="0.980445">
775
</page>
<reference confidence="0.999476727272727">
Barbara J. Grosz, Aravind Joshi, and Scott Weinstein.
1995. Centering: A framework for modeling the
local coherence of discourse. Computational Lin-
guistics, 21(2):203–225.
Thorsten Joachims. 2006. Training linear SVMs in
linear time. In Proceedings of the ACM Conference
on Knowledge Discovery and Data Mining (KDD),
pages 217–226.
Nikiforos Karamanis, Massimo Poesioand Chris Mel-
lish, and Jon Oberlander. 2009. Evaluating center-
ing for information ordering using corpora. Com-
putational Linguistics, 35(1).
Jane Morris and Graeme Hirst. 1991. Lexical cohe-
sion, the thesaurus, and the structure of text. Com-
putational Linguistics, 17(1):21–225.
Karin Naumann. 2006. Manual for the annotation of
in-document referential relations. Technical report,
Seminar f¨ur Sprachwissenschaft, Abt. Computerlin-
guistik, Universit¨at T¨ubingen.
Massimo Poesio and Ron Artstein. 2005. The relia-
bility of anaphoric annotation, reconsidered: Taking
ambiguity into account. In Proc. of ACL Workshop
on Frontiers in Corpus Annotation.
Massimo Poesio, Rosemary Stevenson, Barbara di Eu-
genio, and Janet Hitzeman. 2004. Centering: A
parametric theory and its instantiations. Computa-
tional Linguistics, 30(3):309–363.
Eric K. Ringger, Michael Gamon, Robert C. Moore,
David Rojas, Martine Smets, and Simon Corston-
Oliver. 2004. Linguistically Informed Statisti-
cal Models of Constituent Structure for Ordering
in Sentence Realization. In Proceedings of the
2004 International Conference on Computational
Linguistics, Geneva, Switzerland.
Julia Ritz, Stefanie Dipper, and Michael G¨otze. 2008.
Annotation of information structure: An evaluation
across different types of texts. In Proceedings of the
the 6th LREC conference.
Christian Rohrer and Martin Forst. 2006. Improv-
ing Coverage and Parsing Quality of a Large-Scale
LFG for German. In Proceedings of the Fifth In-
ternational Conference on Language Resources and
Evaluation (LREC), Genoa, Italy.
Augustin Speyer. 2005. Competing constraints on
vorfeldbesetzung in german. In Proceedings of the
Constraints in Discourse Workshop, pages 79–87.
Heike Telljohann, Erhard Hinrichs, Sandra K¨ubler,
and Heike Zinsmeister. 2006. Stylebook for the
t¨ubingen treebank of written german (t¨uba-d/z).
revised version. Technical report, Seminar f¨ur
Sprachwissenschaft, Universit¨at T¨ubingen.
Erik Velldal and Stephan Oepen. 2005. Maximum
entropy models for realization ranking. In Proceed-
ings of the 10th Machine Translation Summit, pages
109–116, Thailand.
</reference>
<page confidence="0.998513">
776
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.521400">
<note confidence="0.9463">To what extent does sentence-internal realisation reflect</note>
<title confidence="0.788056">context? A study on word order</title>
<author confidence="0.96087">Sina Zarriell Jonas Kuhn Aoife Cahill</author>
<affiliation confidence="0.993086">Institut f¨ur maschinelle Sprachverarbeitung Educational Testing Service</affiliation>
<address confidence="0.745188">University of Stuttgart, Germany Princeton, NJ 08541, USA</address>
<email confidence="0.943281">zarriesa,jonas@ims.uni-stuttgart.deacahill@ets.org</email>
<abstract confidence="0.999522952380952">We compare the impact of sentenceinternal vs. sentence-external features on word order prediction in two generation settings: starting out from a discriminative surface realisation ranking model for an LFG grammar of German, we enrich the feature set with lexical chain features from the discourse context which can be robustly detected and reflect rough grammatical correlates of notions from theoretical approaches to discourse coherence. In a more controlled setting, we develop a constituent ordering classifier that is trained on a German treebank with gold coreference annotation. Surprisingly, in both settings, the sentence-external features perform poorly compared to the sentenceinternal ones, and do not improve over a baseline model capturing the syntactic functions of the constituents.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Mirella Lapata</author>
</authors>
<title>Modeling local coherence: An entity-based approach.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<pages>34--1</pages>
<contexts>
<context position="6039" citStr="Barzilay and Lapata (2008)" startWordPosition="898" endWordPosition="901">do not improve over a baseline that captures basic morphosyntactic properties of the constituents – we conclude that sentenceinternal realisation is actually a relatively accurate predictor of discourse context, even more accurate than information that can be obtained from coreference and lexical chain relations. 2 Related Work In the generation literature, most works on exploiting sentence-external discourse information are set in a summarisation or content ordering framework. Barzilay and Lee (2004) propose an account for constraints on topic selection based on probabilistic content models. Barzilay and Lapata (2008) propose an entity grid model which represents the distribution of referents in a discourse for sentence ordering. Karamanis et al. (2009) use Centering-based metrics to assess coherence in an information ordering system. Clarke and Lapata (2010) have improved a sentence compression system by capturing prominence of phrases or referents in terms of lexical chain information inspired by Morris and Hirst (1991) and Centering (Grosz et al., 1995). In their system, discourse context is represented in terms of hard constraints modelling whether a certain constituent can be deleted or not. In the li</context>
<context position="7997" citStr="Barzilay and Lapata (2008)" startWordPosition="1206" endWordPosition="1209">on German surface realisation has highlighted the role of the initial position in the German sentence, the so-called Vorfeld (or “prefield”). Filippova and Strube (2007) show that once the Vorfeld (i.e. the constituent that precedes the finite verb) is correctly determined, the prediction of the order in the Mittelfeld (i.e. the constituents that follow the finite verb) is very easy. Cheung and Penn (2010) extend the approach of Filippova and Strube (2007) and augment a sentence-internal constituent ordering model with sentence-external features inspired from the entity grid model proposed by Barzilay and Lapata (2008). 3 Motivation While there would be many ways to construe or represent discourse context (e.g. in terms of the global discourse or information structure), we concentrate on capturing local coherence through the distribution of discourse referents in a text. These discourse referents basically correspond to the constituents that our surface realisation model has to put in the right order. As the order of referents or constituents is arguably influenced by the information structure of a sentence given the previous text, our main assumption was that infor768 (1) a. Kurze Zeit sp¨ater erkl¨arte ei</context>
</contexts>
<marker>Barzilay, Lapata, 2008</marker>
<rawString>Regina Barzilay and Mirella Lapata. 2008. Modeling local coherence: An entity-based approach. Computational Linguistics, 34:1–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Catching the drift: Probabilistic content models with applications to generation and summarization.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT-NAACL 2004,</booktitle>
<location>Boston,MA.</location>
<contexts>
<context position="5919" citStr="Barzilay and Lee (2004)" startWordPosition="881" endWordPosition="884">f manually annotated coreference relations. As we get the same effect in both setups – the sentenceexternal features do not improve over a baseline that captures basic morphosyntactic properties of the constituents – we conclude that sentenceinternal realisation is actually a relatively accurate predictor of discourse context, even more accurate than information that can be obtained from coreference and lexical chain relations. 2 Related Work In the generation literature, most works on exploiting sentence-external discourse information are set in a summarisation or content ordering framework. Barzilay and Lee (2004) propose an account for constraints on topic selection based on probabilistic content models. Barzilay and Lapata (2008) propose an entity grid model which represents the distribution of referents in a discourse for sentence ordering. Karamanis et al. (2009) use Centering-based metrics to assess coherence in an information ordering system. Clarke and Lapata (2010) have improved a sentence compression system by capturing prominence of phrases or referents in terms of lexical chain information inspired by Morris and Hirst (1991) and Centering (Grosz et al., 1995). In their system, discourse cont</context>
</contexts>
<marker>Barzilay, Lee, 2004</marker>
<rawString>Regina Barzilay and Lillian Lee. 2004. Catching the drift: Probabilistic content models with applications to generation and summarization. In Proceedings of HLT-NAACL 2004, Boston,MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anja Belz</author>
<author>Ehud Reiter</author>
</authors>
<title>Comparing automatic and human evaluation of NLG systems.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL</booktitle>
<pages>313--320</pages>
<location>Trento, Italy.</location>
<contexts>
<context position="35086" citStr="Belz and Reiter, 2006" startWordPosition="5451" endWordPosition="5454">gests that sentenceinternal realisation implicitly carries a lot of imformation about discourse context. On average, the morphosyntactic properties of constituents in a text are better approximates of their discourse status than actual coreference relations. This result feeds into a number of research questions concerning the representation of discourse and its application in generation systems. Although we should certainly not expect a computational model to achieve a perfect accuracy in the constituent ordering task – even humans only agree to a certain extent in rating word order variants (Belz and Reiter, 2006; Cahill, 2009) – the average accuracy in the 60’s for prediction of Vorfeld occupance is still moderate. An obvious direction would be to further investigate more complex representations of discourse that take into account the relations between utterances, such as topic shifts. Moreover, it is not clear whether the effects we find for linearisation in this paper carry over to other levels of generation such as tactical generation where syntactic functions are not fully specified. In a broader perspective, our results underline the need for better formalisations of discourse that can be transl</context>
</contexts>
<marker>Belz, Reiter, 2006</marker>
<rawString>Anja Belz and Ehud Reiter. 2006. Comparing automatic and human evaluation of NLG systems. In Proceedings of EACL 2006, pages 313–320, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerlof Bouma</author>
</authors>
<title>Syntactic tree queries in prolog.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourth Linguistic Annotation Workshop, ACL</booktitle>
<contexts>
<context position="26774" citStr="Bouma (2010)" startWordPosition="4170" endWordPosition="4171">feld node can be freely reordered. Thus, we do not check whether the word order variants we look at are actually grammatical assuming that most of them are. In this sense, this experiment is close to fully statistical generation approaches. As a further simplification, we do not look at morphological generation variants of the constituents or their head verb. The classifier is implemented with SVMrank again. In contrast to the previous experiment where we learned to rank sentences, the classifier now learns to rank constituents. The constituents have been extracted using the tool described in Bouma (2010). The final data set comprises 48.513 candidate sets of freely orderable constituents. 5.2 Centering-inspired Feature Model To compare the discourse context model against a sentence-based model, we implemented a number of sentence-internal features that are very similar to the features used in the previous experiment. Since we extract them from the syntactic annotation instead of f-structures, some labels and feature names will be different, however, the design of the sentence-internal model is identical to the previous one in Section 4. The sentence-external features differ in some aspects fr</context>
</contexts>
<marker>Bouma, 2010</marker>
<rawString>Gerlof Bouma. 2010. Syntactic tree queries in prolog. In Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Brants</author>
<author>Stefanie Dipper</author>
<author>Silvia Hansen</author>
<author>Wolfgang Lezius</author>
<author>George Smith</author>
</authors>
<title>The TIGER Treebank.</title>
<date>2002</date>
<booktitle>In Proceedings of the Workshop on Treebanks and Linguistic Theories.</booktitle>
<contexts>
<context position="19687" citStr="Brants et al., 2002" startWordPosition="3025" endWordPosition="3028">) feature representation for the sentence (1-b). Precedence is indicated by ”&lt;”. Basically, our sentence-external feature model is built on the intuition that lexical chains or overlaps approximate discourse status in a way which is similar to sentence-internal morphosyntactic properties. Thus, we would expect that overlaps indicate givenness, salience or prominence and that asymmetries between overlapping and nonoverlapping entities are helpful in the ranking. 4.3 Data All our models are trained on 7,039 sentences (subdivided into 1259 texts) from the TIGER Treebank of German newspaper text (Brants et al., 2002). We tune the parameters of our SVM model on a development set of 55 sentences and report the final results for our unseen test set of 240 sentences. Table 1 shows how many sentences in our training, development and test sets have at least one textually overlapping phrase in the previous 1–10 sentences. We choose the TIGER treebank, which has no # Sentences % Sentences with overlap in context Training Dev Test 1 20.96 23.64 20.42 2 35.42 40.74 35.00 3 45.58 50.00 53.33 4 52.66 53.70 58.75 5 57.45 58.18 64.58 6 61.42 57.41 68.75 7 64.58 61.11 70.83 8 67.05 62.96 72.08 9 69.20 64.81 74.17 10 71.</context>
</contexts>
<marker>Brants, Dipper, Hansen, Lezius, Smith, 2002</marker>
<rawString>Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang Lezius, and George Smith. 2002. The TIGER Treebank. In Proceedings of the Workshop on Treebanks and Linguistic Theories.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aoife Cahill</author>
<author>Arndt Riester</author>
</authors>
<title>Incorporating information status into generation ranking.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>817--825</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="4140" citStr="Cahill and Riester (2009)" startWordPosition="617" endWordPosition="620"> actual lexical choice, including functional categories such as determiners, is fully fixed (which is of course not always the case), one can take advantage of 767 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 767–776, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics these reflexes. This explains in part the fairly high baseline performance of n-gram language models in the surface realization task. And the effect can indeed be taken much further: the discriminative training experiments of Cahill and Riester (2009) show how effective it is to systematically take advantage of asymmetry patterns in the morphosyntactic reflexes of the discourse notion of information status (i.e., using a feature set with well-chosen purely sentence-bound features). These observations give rise to the question: in the light of the difficulty in obtaining reliable discourse information on the one hand and the effectiveness of exploiting the reflexes of discourse in the sentence-internal material on the other – can we nevertheless expect to gain something from adding sentence-external feature information? We propose two scena</context>
<context position="6920" citStr="Cahill and Riester (2009)" startWordPosition="1035" endWordPosition="1038">oved a sentence compression system by capturing prominence of phrases or referents in terms of lexical chain information inspired by Morris and Hirst (1991) and Centering (Grosz et al., 1995). In their system, discourse context is represented in terms of hard constraints modelling whether a certain constituent can be deleted or not. In the linearisation or surface realisation domain, there is a considerable body of work approximating information structure in terms of sentence-internal realisation (Ringger et al., 2004; Filippova and Strube, 2009; Velldal and Oepen, 2005; Cahill et al., 2007). Cahill and Riester (2009) improve realisation ranking for German – which mainly deals with word order variation – by representing precedence patterns of constituents in terms of asymmetries in their morphosyntactic properties. As a simple example, a pattern exploited by Cahill and Riester (2009) is the tendency of definite elements tend to precede indefinites, which, on a discourse level, reflects that given entities in a sentence tend to precede new entities. Other work on German surface realisation has highlighted the role of the initial position in the German sentence, the so-called Vorfeld (or “prefield”). Filippo</context>
</contexts>
<marker>Cahill, Riester, 2009</marker>
<rawString>Aoife Cahill and Arndt Riester. 2009. Incorporating information status into generation ranking. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 817–825, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aoife Cahill</author>
<author>Martin Forst</author>
<author>Christian Rohrer</author>
</authors>
<title>Stochastic Realisation Ranking for a Free Word Order Language.</title>
<date>2007</date>
<booktitle>In Proceedings of the Eleventh European Workshop on Natural Language Generation,</booktitle>
<pages>17--24</pages>
<publisher>DFKI GmbH.</publisher>
<location>Saarbr¨ucken, Germany.</location>
<contexts>
<context position="6893" citStr="Cahill et al., 2007" startWordPosition="1031" endWordPosition="1034">apata (2010) have improved a sentence compression system by capturing prominence of phrases or referents in terms of lexical chain information inspired by Morris and Hirst (1991) and Centering (Grosz et al., 1995). In their system, discourse context is represented in terms of hard constraints modelling whether a certain constituent can be deleted or not. In the linearisation or surface realisation domain, there is a considerable body of work approximating information structure in terms of sentence-internal realisation (Ringger et al., 2004; Filippova and Strube, 2009; Velldal and Oepen, 2005; Cahill et al., 2007). Cahill and Riester (2009) improve realisation ranking for German – which mainly deals with word order variation – by representing precedence patterns of constituents in terms of asymmetries in their morphosyntactic properties. As a simple example, a pattern exploited by Cahill and Riester (2009) is the tendency of definite elements tend to precede indefinites, which, on a discourse level, reflects that given entities in a sentence tend to precede new entities. Other work on German surface realisation has highlighted the role of the initial position in the German sentence, the so-called Vorfe</context>
<context position="15226" citStr="Cahill et al. (2007)" startWordPosition="2368" endWordPosition="2371"> In contrast to fully statistical linearisation methods, our system first generates the full set of sentences that correspond to the grammatically well-formed realisations of the intermediate syntactic representation.&apos; This representation is an f-structure, which underspecifies the order of constituents and, to some extent, their morphological realisation, such that the output sentences contain all possible combinations of word order permutations and morphological variants. Depending on the length and structure of the original corpus sentence, the set of regenerated sentences can be huge (see Cahill et al. (2007) for details on regenerating the German treebank TIGER). &apos;There are occasional mistakes in the grammar which sometimes lead to ungrammatical strings being generated, but this is rare. The realisation ranking component is an SVM ranking model implemented with SVMrank, a Support Vector Machine-based learning tool (Joachims, 2006). During training, each sentence is annotated with a rank and a set of features extracted from the F-structure, its surface string and external resources (e.g. a language model). If the sentence matches the original corpus string, its rank will be highest, the assumption</context>
<context position="20794" citStr="Cahill et al. (2007)" startWordPosition="3220" endWordPosition="3223"> 58.75 5 57.45 58.18 64.58 6 61.42 57.41 68.75 7 64.58 61.11 70.83 8 67.05 62.96 72.08 9 69.20 64.81 74.17 10 71.16 70.37 75.83 Table 1: The percentage of sentences that have at least one overlapping entity in the previous n sentences coreference annotation, since we already have a number of resources available to match the syntactic analyses produced by our grammar against the analyses in the treebank. Thus, in our regeneration system, we parse the sentences with the grammar, and choose the parsed f-structures that are compatible with the manual annotation in the TIGER treebank as is done in Cahill et al. (2007). This compatibility check eliminates noise which would be introduced by generating from incorrect parses (e.g. incorrect PP-attachments typically result in unnatural and non-equivalent surface realisations). For comparing the string chosen by the models against the original corpus sentence, we use BLEU, NIST and exact match. Exact match is a strict measure that only credits the system if it chooses the exact same string as the original corpus string. BLEU and NIST are more relaxed measures that compare the strings on the n-gram level. Finally, we report accuracy scores for the Vorfeld positio</context>
</contexts>
<marker>Cahill, Forst, Rohrer, 2007</marker>
<rawString>Aoife Cahill, Martin Forst, and Christian Rohrer. 2007. Stochastic Realisation Ranking for a Free Word Order Language. In Proceedings of the Eleventh European Workshop on Natural Language Generation, pages 17–24, Saarbr¨ucken, Germany. DFKI GmbH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aoife Cahill</author>
</authors>
<title>Correlating human and automatic evaluation of a german surface realiser.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers,</booktitle>
<pages>97--100</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="35101" citStr="Cahill, 2009" startWordPosition="5455" endWordPosition="5456">rnal realisation implicitly carries a lot of imformation about discourse context. On average, the morphosyntactic properties of constituents in a text are better approximates of their discourse status than actual coreference relations. This result feeds into a number of research questions concerning the representation of discourse and its application in generation systems. Although we should certainly not expect a computational model to achieve a perfect accuracy in the constituent ordering task – even humans only agree to a certain extent in rating word order variants (Belz and Reiter, 2006; Cahill, 2009) – the average accuracy in the 60’s for prediction of Vorfeld occupance is still moderate. An obvious direction would be to further investigate more complex representations of discourse that take into account the relations between utterances, such as topic shifts. Moreover, it is not clear whether the effects we find for linearisation in this paper carry over to other levels of generation such as tactical generation where syntactic functions are not fully specified. In a broader perspective, our results underline the need for better formalisations of discourse that can be translated into featu</context>
</contexts>
<marker>Cahill, 2009</marker>
<rawString>Aoife Cahill. 2009. Correlating human and automatic evaluation of a german surface realiser. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 97–100, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jackie C K Cheung</author>
<author>Gerald Penn</author>
</authors>
<title>Entitybased local coherence modelling using topological fields.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL 2010). Association for Computational Linguistics.</booktitle>
<contexts>
<context position="7780" citStr="Cheung and Penn (2010)" startWordPosition="1175" endWordPosition="1178">ited by Cahill and Riester (2009) is the tendency of definite elements tend to precede indefinites, which, on a discourse level, reflects that given entities in a sentence tend to precede new entities. Other work on German surface realisation has highlighted the role of the initial position in the German sentence, the so-called Vorfeld (or “prefield”). Filippova and Strube (2007) show that once the Vorfeld (i.e. the constituent that precedes the finite verb) is correctly determined, the prediction of the order in the Mittelfeld (i.e. the constituents that follow the finite verb) is very easy. Cheung and Penn (2010) extend the approach of Filippova and Strube (2007) and augment a sentence-internal constituent ordering model with sentence-external features inspired from the entity grid model proposed by Barzilay and Lapata (2008). 3 Motivation While there would be many ways to construe or represent discourse context (e.g. in terms of the global discourse or information structure), we concentrate on capturing local coherence through the distribution of discourse referents in a text. These discourse referents basically correspond to the constituents that our surface realisation model has to put in the right</context>
</contexts>
<marker>Cheung, Penn, 2010</marker>
<rawString>Jackie C.K. Cheung and Gerald Penn. 2010. Entitybased local coherence modelling using topological fields. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL 2010). Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Clarke</author>
<author>Mirella Lapata</author>
</authors>
<title>Discourse constraints for document compression.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>3</issue>
<contexts>
<context position="6285" citStr="Clarke and Lapata (2010)" startWordPosition="935" endWordPosition="939">that can be obtained from coreference and lexical chain relations. 2 Related Work In the generation literature, most works on exploiting sentence-external discourse information are set in a summarisation or content ordering framework. Barzilay and Lee (2004) propose an account for constraints on topic selection based on probabilistic content models. Barzilay and Lapata (2008) propose an entity grid model which represents the distribution of referents in a discourse for sentence ordering. Karamanis et al. (2009) use Centering-based metrics to assess coherence in an information ordering system. Clarke and Lapata (2010) have improved a sentence compression system by capturing prominence of phrases or referents in terms of lexical chain information inspired by Morris and Hirst (1991) and Centering (Grosz et al., 1995). In their system, discourse context is represented in terms of hard constraints modelling whether a certain constituent can be deleted or not. In the linearisation or surface realisation domain, there is a considerable body of work approximating information structure in terms of sentence-internal realisation (Ringger et al., 2004; Filippova and Strube, 2009; Velldal and Oepen, 2005; Cahill et al</context>
<context position="28552" citStr="Clarke and Lapata (2010)" startWordPosition="4449" endWordPosition="4452">ples (4) and (5). Note that both cases would not have been captured in the lexical # VF # MF Backward Center 3.5% 5.1% Forward Center 6.8% 6.8% Coref Link 30.5% 23.4% Table 4: Backward and forward centers and their positions chain model since there is no lexical overlap between the realisations of the discourse referents. These types of coreference features implicitly carry the information that would also be considered in a Centering formalisation of discourse context. In addition to these, we designed features that explicitly describe centers as these might have a higher weight. In line with Clarke and Lapata (2010), we compute backward (CB) and forward centers (CF) in the following way: 1. Extract all entities from the current sentence and the previous sentence. 2. Rank the entities of the previous sentence according to their function (subject &lt; direct object &lt; indirect object ...). 3. Find the highest ranked entity in the previous sentence that has a link to an entity in the current sentence, this entity is the CB of the sentence. In the same way, we mark entities as forward centers that are ranked highest in the current sentence and have a link to an entity in the following sentence.2 In Table 4, we r</context>
</contexts>
<marker>Clarke, Lapata, 2010</marker>
<rawString>James Clarke and Mirella Lapata. 2010. Discourse constraints for document compression. Computational Linguistics, 36(3):411–441.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefanie Dipper</author>
<author>Heike Zinsmeister</author>
</authors>
<title>The role of the German Vorfeld for local coherence.</title>
<date>2009</date>
<booktitle>Von der Form zur Bedeutung: Texte automatisch verarbeiten/From Form to Meaning: Processing Texts Automatically,</booktitle>
<pages>69--79</pages>
<editor>In Christian Chiarcos, Richard Eckart de Castilho, and Manfred Stede, editors,</editor>
<location>Narr, T¨ubingen.</location>
<contexts>
<context position="2423" citStr="Dipper and Zinsmeister, 2009" startWordPosition="348" endWordPosition="351">e soft constraints, taking into account their interaction with various aspects of the discourse context (givenness or salience of particular referents, prior mentioning of particular concepts). Since so many factors are involved and there is further interaction with subtle semantic and pragmatic differentiations, lexical choice, stylistics and presumably processing factors, theoretical accounts making reliable predictions for real corpus examples have for a long time proven elusive. As for German, only quite recently, a number of corpus-based studies (Filippova and Strube, 2007; Speyer, 2005; Dipper and Zinsmeister, 2009) have made some good progress towards a coherenceoriented account of at least the left edge of the German clause structure, the Vorfeld constituent. What makes the technological application of theoretical insights even harder is that for most relevant factors, automatic recognition cannot be performed with high accuracy (e.g., a coreference accuracy in the 70’s means there is a good deal of noise) and for the higher-level notions such as the information-structural focus, interannotator agreement on real corpus data tends to be much lower than for core-grammatical notions (Poesio and Artstein, </context>
</contexts>
<marker>Dipper, Zinsmeister, 2009</marker>
<rawString>Stefanie Dipper and Heike Zinsmeister. 2009. The role of the German Vorfeld for local coherence. In Christian Chiarcos, Richard Eckart de Castilho, and Manfred Stede, editors, Von der Form zur Bedeutung: Texte automatisch verarbeiten/From Form to Meaning: Processing Texts Automatically, pages 69–79. Narr, T¨ubingen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Filippova</author>
<author>Michael Strube</author>
</authors>
<title>The german vorfeld and local coherence.</title>
<date>2007</date>
<journal>Journal of Logic, Language and Information,</journal>
<pages>16--465</pages>
<contexts>
<context position="2378" citStr="Filippova and Strube, 2007" startWordPosition="342" endWordPosition="345">are countless approaches to modelling these soft constraints, taking into account their interaction with various aspects of the discourse context (givenness or salience of particular referents, prior mentioning of particular concepts). Since so many factors are involved and there is further interaction with subtle semantic and pragmatic differentiations, lexical choice, stylistics and presumably processing factors, theoretical accounts making reliable predictions for real corpus examples have for a long time proven elusive. As for German, only quite recently, a number of corpus-based studies (Filippova and Strube, 2007; Speyer, 2005; Dipper and Zinsmeister, 2009) have made some good progress towards a coherenceoriented account of at least the left edge of the German clause structure, the Vorfeld constituent. What makes the technological application of theoretical insights even harder is that for most relevant factors, automatic recognition cannot be performed with high accuracy (e.g., a coreference accuracy in the 70’s means there is a good deal of noise) and for the higher-level notions such as the information-structural focus, interannotator agreement on real corpus data tends to be much lower than for co</context>
<context position="7540" citStr="Filippova and Strube (2007)" startWordPosition="1134" endWordPosition="1137"> (2009) improve realisation ranking for German – which mainly deals with word order variation – by representing precedence patterns of constituents in terms of asymmetries in their morphosyntactic properties. As a simple example, a pattern exploited by Cahill and Riester (2009) is the tendency of definite elements tend to precede indefinites, which, on a discourse level, reflects that given entities in a sentence tend to precede new entities. Other work on German surface realisation has highlighted the role of the initial position in the German sentence, the so-called Vorfeld (or “prefield”). Filippova and Strube (2007) show that once the Vorfeld (i.e. the constituent that precedes the finite verb) is correctly determined, the prediction of the order in the Mittelfeld (i.e. the constituents that follow the finite verb) is very easy. Cheung and Penn (2010) extend the approach of Filippova and Strube (2007) and augment a sentence-internal constituent ordering model with sentence-external features inspired from the entity grid model proposed by Barzilay and Lapata (2008). 3 Motivation While there would be many ways to construe or represent discourse context (e.g. in terms of the global discourse or information </context>
<context position="24043" citStr="Filippova and Strube (2007)" startWordPosition="3736" endWordPosition="3739">ntence-internal baseline suggests that these patterns are equally well captured by morphosyntactic features. However, we cannot exclude the possibility that the chain features are too noisy as they conflate several types of lexical and coreferential relations. This will be adressed in the following experiment. 5 Experiment 2: Constituent Ordering with Centering-inspired Features We now look at a simpler generation setup where we concentrate on the ordering of constituents in the German Vorfeld and Mittelfeld. This strategy has also been adopted in previous investigations of German word order: Filippova and Strube (2007) show that once the German Vorfeld is correctly chosen, the prediction accuracy for the Mittelfeld (the constituents following the finite verb) is in the 90s. In order to eliminate noise introduced from potentially heterogeneous chain features, we look at coreference features and, again, compare them to sentence-internal morphosyntactic features. We target a generation scenario where coreference information is available. The aim is to establish an upper bound concerning the quality improvement for word order prediction by recurring to manual corefence annotation. 5.1 Data and Setup We carry ou</context>
</contexts>
<marker>Filippova, Strube, 2007</marker>
<rawString>Katja Filippova and Michael Strube. 2007. The german vorfeld and local coherence. Journal of Logic, Language and Information, 16:465–485.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Filippova</author>
<author>Michael Strube</author>
</authors>
<title>Tree Linearization in English: Improving Language Model Based Approaches.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers,</booktitle>
<pages>225--228</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="6846" citStr="Filippova and Strube, 2009" startWordPosition="1023" endWordPosition="1026">rence in an information ordering system. Clarke and Lapata (2010) have improved a sentence compression system by capturing prominence of phrases or referents in terms of lexical chain information inspired by Morris and Hirst (1991) and Centering (Grosz et al., 1995). In their system, discourse context is represented in terms of hard constraints modelling whether a certain constituent can be deleted or not. In the linearisation or surface realisation domain, there is a considerable body of work approximating information structure in terms of sentence-internal realisation (Ringger et al., 2004; Filippova and Strube, 2009; Velldal and Oepen, 2005; Cahill et al., 2007). Cahill and Riester (2009) improve realisation ranking for German – which mainly deals with word order variation – by representing precedence patterns of constituents in terms of asymmetries in their morphosyntactic properties. As a simple example, a pattern exploited by Cahill and Riester (2009) is the tendency of definite elements tend to precede indefinites, which, on a discourse level, reflects that given entities in a sentence tend to precede new entities. Other work on German surface realisation has highlighted the role of the initial posit</context>
</contexts>
<marker>Filippova, Strube, 2009</marker>
<rawString>Katja Filippova and Michael Strube. 2009. Tree Linearization in English: Improving Language Model Based Approaches. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers, pages 225–228, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Aravind Joshi</author>
<author>Scott Weinstein</author>
</authors>
<title>Centering: A framework for modeling the local coherence of discourse.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="6486" citStr="Grosz et al., 1995" startWordPosition="969" endWordPosition="972"> content ordering framework. Barzilay and Lee (2004) propose an account for constraints on topic selection based on probabilistic content models. Barzilay and Lapata (2008) propose an entity grid model which represents the distribution of referents in a discourse for sentence ordering. Karamanis et al. (2009) use Centering-based metrics to assess coherence in an information ordering system. Clarke and Lapata (2010) have improved a sentence compression system by capturing prominence of phrases or referents in terms of lexical chain information inspired by Morris and Hirst (1991) and Centering (Grosz et al., 1995). In their system, discourse context is represented in terms of hard constraints modelling whether a certain constituent can be deleted or not. In the linearisation or surface realisation domain, there is a considerable body of work approximating information structure in terms of sentence-internal realisation (Ringger et al., 2004; Filippova and Strube, 2009; Velldal and Oepen, 2005; Cahill et al., 2007). Cahill and Riester (2009) improve realisation ranking for German – which mainly deals with word order variation – by representing precedence patterns of constituents in terms of asymmetries i</context>
<context position="9927" citStr="Grosz et al., 1995" startWordPosition="1522" endWordPosition="1525">a. Frauen vom Land k¨ampften aktiv darum , ein Staudammprojekt zu verhindern. Women from the countryside fighted actively to block the dam project. b. Auch in den St¨adten f¨anden sich immer mehr Frauen in Selbsthilfeorganisationen zusammen. Also in the cities, more and more women team up in self-help organisations. mation about the prior mentioning of a referent would be helpful for predicting the position of this referent in a sentence. The idea that the occurence of discourse referents in a text is a central aspect of discourse structure has been systematically pursued by Centering Theory (Grosz et al., 1995). Its most important notions are related to the realisation of discourse referents (i.e. described as “centers”) and the way the centers are arranged in a sequence of utterances to make this sequence a coherent discourse. Another important concept is the “ranking” of discourse referents which basically determines the prominence of a referent in a certain sentence and is driven by several factors (e.g. their grammatical function). For free word order languages like German, word order has been proposed as one of the factors that account for the ranking (Poesio et al., 2004). In a similar spirit,</context>
</contexts>
<marker>Grosz, Joshi, Weinstein, 1995</marker>
<rawString>Barbara J. Grosz, Aravind Joshi, and Scott Weinstein. 1995. Centering: A framework for modeling the local coherence of discourse. Computational Linguistics, 21(2):203–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Training linear SVMs in linear time.</title>
<date>2006</date>
<booktitle>In Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD),</booktitle>
<pages>217--226</pages>
<contexts>
<context position="15555" citStr="Joachims, 2006" startWordPosition="2418" endWordPosition="2419">rphological realisation, such that the output sentences contain all possible combinations of word order permutations and morphological variants. Depending on the length and structure of the original corpus sentence, the set of regenerated sentences can be huge (see Cahill et al. (2007) for details on regenerating the German treebank TIGER). &apos;There are occasional mistakes in the grammar which sometimes lead to ungrammatical strings being generated, but this is rare. The realisation ranking component is an SVM ranking model implemented with SVMrank, a Support Vector Machine-based learning tool (Joachims, 2006). During training, each sentence is annotated with a rank and a set of features extracted from the F-structure, its surface string and external resources (e.g. a language model). If the sentence matches the original corpus string, its rank will be highest, the assumption being that the original sentence corresponds to the optimal realisation in context. The output of generation, the top-ranked sentence, is evaluated against the original corpus sentence. 4.2 The Feature Models As the aim of this experiment is to better understand the nature of sentence-internal features reflecting discourse con</context>
</contexts>
<marker>Joachims, 2006</marker>
<rawString>Thorsten Joachims. 2006. Training linear SVMs in linear time. In Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), pages 217–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikiforos Karamanis</author>
<author>Massimo Poesioand Chris Mellish</author>
<author>Jon Oberlander</author>
</authors>
<title>Evaluating centering for information ordering using corpora.</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<volume>35</volume>
<issue>1</issue>
<contexts>
<context position="6177" citStr="Karamanis et al. (2009)" startWordPosition="920" endWordPosition="923">tion is actually a relatively accurate predictor of discourse context, even more accurate than information that can be obtained from coreference and lexical chain relations. 2 Related Work In the generation literature, most works on exploiting sentence-external discourse information are set in a summarisation or content ordering framework. Barzilay and Lee (2004) propose an account for constraints on topic selection based on probabilistic content models. Barzilay and Lapata (2008) propose an entity grid model which represents the distribution of referents in a discourse for sentence ordering. Karamanis et al. (2009) use Centering-based metrics to assess coherence in an information ordering system. Clarke and Lapata (2010) have improved a sentence compression system by capturing prominence of phrases or referents in terms of lexical chain information inspired by Morris and Hirst (1991) and Centering (Grosz et al., 1995). In their system, discourse context is represented in terms of hard constraints modelling whether a certain constituent can be deleted or not. In the linearisation or surface realisation domain, there is a considerable body of work approximating information structure in terms of sentence-i</context>
</contexts>
<marker>Karamanis, Mellish, Oberlander, 2009</marker>
<rawString>Nikiforos Karamanis, Massimo Poesioand Chris Mellish, and Jon Oberlander. 2009. Evaluating centering for information ordering using corpora. Computational Linguistics, 35(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane Morris</author>
<author>Graeme Hirst</author>
</authors>
<title>Lexical cohesion, the thesaurus, and the structure of text.</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<volume>17</volume>
<issue>1</issue>
<contexts>
<context position="6451" citStr="Morris and Hirst (1991)" startWordPosition="962" endWordPosition="965">formation are set in a summarisation or content ordering framework. Barzilay and Lee (2004) propose an account for constraints on topic selection based on probabilistic content models. Barzilay and Lapata (2008) propose an entity grid model which represents the distribution of referents in a discourse for sentence ordering. Karamanis et al. (2009) use Centering-based metrics to assess coherence in an information ordering system. Clarke and Lapata (2010) have improved a sentence compression system by capturing prominence of phrases or referents in terms of lexical chain information inspired by Morris and Hirst (1991) and Centering (Grosz et al., 1995). In their system, discourse context is represented in terms of hard constraints modelling whether a certain constituent can be deleted or not. In the linearisation or surface realisation domain, there is a considerable body of work approximating information structure in terms of sentence-internal realisation (Ringger et al., 2004; Filippova and Strube, 2009; Velldal and Oepen, 2005; Cahill et al., 2007). Cahill and Riester (2009) improve realisation ranking for German – which mainly deals with word order variation – by representing precedence patterns of con</context>
<context position="10551" citStr="Morris and Hirst (1991)" startWordPosition="1625" endWordPosition="1628"> Its most important notions are related to the realisation of discourse referents (i.e. described as “centers”) and the way the centers are arranged in a sequence of utterances to make this sequence a coherent discourse. Another important concept is the “ranking” of discourse referents which basically determines the prominence of a referent in a certain sentence and is driven by several factors (e.g. their grammatical function). For free word order languages like German, word order has been proposed as one of the factors that account for the ranking (Poesio et al., 2004). In a similar spirit, Morris and Hirst (1991) have proposed that chains of (related) lexical items in a text are an important indicator of text structure. Our main hypothesis was that it is possible to exploit these intuitions from Centering Theory and the idea of lexical chains for word order prediction. Thus, we expected that it would be easier to predict the position of a referent in a sentence if we have not only given its realisation in the current utterance but also its prominence in the previous discourse. Especially, we expected this intuition to hold for cases where the morpho-syntactic realisation of a constituent does not prov</context>
<context position="12590" citStr="Morris and Hirst (1991)" startWordPosition="1964" endWordPosition="1967">rst introduced in an embedded sentence and realised in the Vorfeld in the second utterance. In terms of centering, this transition would correspond to a topic shift. In Example (2), both instances are realised in the Vorfeld, such that the topic of the first sentence is carried over to the next. In Example (3), we illustrate a further type of lexical reiteration. In this case, two identical head nouns are realised in subsequent sentences, even though they refer to two different discourse referents. While this type of lexical chain is described as “reiteration without identity of referents” by Morris and Hirst (1991), it would not be captured in Centering since this is not a case of strict coreference. On the other hand, lexical chains do not capture types of reiterated discourse referents that have distinct morpho-syntactic realisations, e.g. nouns and pronouns. Originally, we had the hypothesis that strict corefence information is more useful and accurate for word order prediction than rather loose lexical chains which conflate several types of referential and lexical relations. However, the advantage of chains, especially chains of reiteration, is that they can be easily detected in any corpus text and</context>
</contexts>
<marker>Morris, Hirst, 1991</marker>
<rawString>Jane Morris and Graeme Hirst. 1991. Lexical cohesion, the thesaurus, and the structure of text. Computational Linguistics, 17(1):21–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karin Naumann</author>
</authors>
<title>Manual for the annotation of in-document referential relations.</title>
<date>2006</date>
<tech>Technical report, Seminar f¨ur</tech>
<institution>Sprachwissenschaft, Abt. Computerlinguistik, Universit¨at T¨ubingen.</institution>
<contexts>
<context position="24947" citStr="Naumann, 2006" startWordPosition="3877" endWordPosition="3878">ompare them to sentence-internal morphosyntactic features. We target a generation scenario where coreference information is available. The aim is to establish an upper bound concerning the quality improvement for word order prediction by recurring to manual corefence annotation. 5.1 Data and Setup We carry out the constituent ordering experiment on the T¨uba-D/Z treebank (v5) of German newspaper articles (Telljohann et al., 2006). It comprises about 800k tokens in 45k sentences. We choose this corpus because it is not only annotated with syntactic analyses but also with coreference relations (Naumann, 2006). The syntactic annotation format differs from the TIGER treebank used in the previous experiment, for instance, it explicitely represents the Vorfeld and Mittelfeld as phrasal nodes in the tree. This format is very convenient for the extraction of constituents in the respective positions. The T¨uba-D/Z coreference annotation distinguishes several relations between discourse referents, most importantly “coreferential relation” and “anaphoric relation” where the first denotes a relation between noun phrases that refer to the same entity, and the latter refers to a link between a pronoun and a c</context>
<context position="27464" citStr="Naumann, 2006" startWordPosition="4275" endWordPosition="4276">stituents. 5.2 Centering-inspired Feature Model To compare the discourse context model against a sentence-based model, we implemented a number of sentence-internal features that are very similar to the features used in the previous experiment. Since we extract them from the syntactic annotation instead of f-structures, some labels and feature names will be different, however, the design of the sentence-internal model is identical to the previous one in Section 4. The sentence-external features differ in some aspects from Section 4, since we extract coreference relations of several types (see (Naumann, 2006) for the anaphoric relations annotated in the Tueba-D/Z). For each type of coreference link, we extract the following properties: (i) function of the antecedent, (ii) position of the antecedent, (iii) distance between sentences, (iv) type of relation. We also distinguish coreference links annotated for the whole phrase (“head link”) and links that are annotated for an element embedded by the constituent (“contained link”). The two types are illustrated in Examples (4) and (5). Note that both cases would not have been captured in the lexical # VF # MF Backward Center 3.5% 5.1% Forward Center 6.</context>
</contexts>
<marker>Naumann, 2006</marker>
<rawString>Karin Naumann. 2006. Manual for the annotation of in-document referential relations. Technical report, Seminar f¨ur Sprachwissenschaft, Abt. Computerlinguistik, Universit¨at T¨ubingen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimo Poesio</author>
<author>Ron Artstein</author>
</authors>
<title>The reliability of anaphoric annotation, reconsidered: Taking ambiguity into account.</title>
<date>2005</date>
<booktitle>In Proc. of ACL Workshop on Frontiers in Corpus Annotation.</booktitle>
<contexts>
<context position="3027" citStr="Poesio and Artstein, 2005" startWordPosition="440" endWordPosition="443">nd Zinsmeister, 2009) have made some good progress towards a coherenceoriented account of at least the left edge of the German clause structure, the Vorfeld constituent. What makes the technological application of theoretical insights even harder is that for most relevant factors, automatic recognition cannot be performed with high accuracy (e.g., a coreference accuracy in the 70’s means there is a good deal of noise) and for the higher-level notions such as the information-structural focus, interannotator agreement on real corpus data tends to be much lower than for core-grammatical notions (Poesio and Artstein, 2005; Ritz et al., 2008). On the other hand, many of the relevant discourse factors are reflected indirectly in properties of the sentence-internal material. Most notably, knowing the shape of referring expressions narrows down many aspects of givenness and salience of its referent; pronominal realizations indicate givenness, and in German there are even two variants of the personal pronoun (er and der) for distinguishing salience. So, if the generation task is set in such a way that the actual lexical choice, including functional categories such as determiners, is fully fixed (which is of course </context>
</contexts>
<marker>Poesio, Artstein, 2005</marker>
<rawString>Massimo Poesio and Ron Artstein. 2005. The reliability of anaphoric annotation, reconsidered: Taking ambiguity into account. In Proc. of ACL Workshop on Frontiers in Corpus Annotation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimo Poesio</author>
<author>Rosemary Stevenson</author>
<author>Barbara di Eugenio</author>
<author>Janet Hitzeman</author>
</authors>
<title>Centering: A parametric theory and its instantiations.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>3</issue>
<marker>Poesio, Stevenson, di Eugenio, Hitzeman, 2004</marker>
<rawString>Massimo Poesio, Rosemary Stevenson, Barbara di Eugenio, and Janet Hitzeman. 2004. Centering: A parametric theory and its instantiations. Computational Linguistics, 30(3):309–363.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric K Ringger</author>
<author>Michael Gamon</author>
<author>Robert C Moore</author>
<author>David Rojas</author>
<author>Martine Smets</author>
<author>Simon CorstonOliver</author>
</authors>
<title>Linguistically Informed Statistical Models of Constituent Structure for Ordering in Sentence Realization.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 International Conference on Computational Linguistics,</booktitle>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="6818" citStr="Ringger et al., 2004" startWordPosition="1019" endWordPosition="1022">metrics to assess coherence in an information ordering system. Clarke and Lapata (2010) have improved a sentence compression system by capturing prominence of phrases or referents in terms of lexical chain information inspired by Morris and Hirst (1991) and Centering (Grosz et al., 1995). In their system, discourse context is represented in terms of hard constraints modelling whether a certain constituent can be deleted or not. In the linearisation or surface realisation domain, there is a considerable body of work approximating information structure in terms of sentence-internal realisation (Ringger et al., 2004; Filippova and Strube, 2009; Velldal and Oepen, 2005; Cahill et al., 2007). Cahill and Riester (2009) improve realisation ranking for German – which mainly deals with word order variation – by representing precedence patterns of constituents in terms of asymmetries in their morphosyntactic properties. As a simple example, a pattern exploited by Cahill and Riester (2009) is the tendency of definite elements tend to precede indefinites, which, on a discourse level, reflects that given entities in a sentence tend to precede new entities. Other work on German surface realisation has highlighted t</context>
</contexts>
<marker>Ringger, Gamon, Moore, Rojas, Smets, CorstonOliver, 2004</marker>
<rawString>Eric K. Ringger, Michael Gamon, Robert C. Moore, David Rojas, Martine Smets, and Simon CorstonOliver. 2004. Linguistically Informed Statistical Models of Constituent Structure for Ordering in Sentence Realization. In Proceedings of the 2004 International Conference on Computational Linguistics, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Ritz</author>
<author>Stefanie Dipper</author>
<author>Michael G¨otze</author>
</authors>
<title>Annotation of information structure: An evaluation across different types of texts.</title>
<date>2008</date>
<booktitle>In Proceedings of the the 6th LREC conference.</booktitle>
<marker>Ritz, Dipper, G¨otze, 2008</marker>
<rawString>Julia Ritz, Stefanie Dipper, and Michael G¨otze. 2008. Annotation of information structure: An evaluation across different types of texts. In Proceedings of the the 6th LREC conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Rohrer</author>
<author>Martin Forst</author>
</authors>
<title>Improving Coverage and Parsing Quality of a Large-Scale LFG for German.</title>
<date>2006</date>
<booktitle>In Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC),</booktitle>
<location>Genoa, Italy.</location>
<contexts>
<context position="14400" citStr="Rohrer and Forst, 2006" startWordPosition="2249" endWordPosition="2252">nt that investigates sentence-external context in a surface realisation task. The sentence-external context is represented in terms of lexical chain features and compared to sentence-internal models which are based on morphosyntactic features. The experiment thus targets a generation scenario where no coreference information is available and aims at assessing whether relatively naive context information is also useful. 4.1 System Description We carry out our first experiment in a regeneration set-up with two components: a) a largescale hand-crafted Lexical Functional Grammar (LFG) for German (Rohrer and Forst, 2006), used to parse and regenerate a corpus sentence, b) a stochastic ranker that selects the most appropriate regenerated sentence in context according to an underlying, linguistically motivated feature model. In contrast to fully statistical linearisation methods, our system first generates the full set of sentences that correspond to the grammatically well-formed realisations of the intermediate syntactic representation.&apos; This representation is an f-structure, which underspecifies the order of constituents and, to some extent, their morphological realisation, such that the output sentences cont</context>
</contexts>
<marker>Rohrer, Forst, 2006</marker>
<rawString>Christian Rohrer and Martin Forst. 2006. Improving Coverage and Parsing Quality of a Large-Scale LFG for German. In Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC), Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Augustin Speyer</author>
</authors>
<title>Competing constraints on vorfeldbesetzung in german.</title>
<date>2005</date>
<booktitle>In Proceedings of the Constraints in Discourse Workshop,</booktitle>
<pages>79--87</pages>
<contexts>
<context position="2392" citStr="Speyer, 2005" startWordPosition="346" endWordPosition="347">modelling these soft constraints, taking into account their interaction with various aspects of the discourse context (givenness or salience of particular referents, prior mentioning of particular concepts). Since so many factors are involved and there is further interaction with subtle semantic and pragmatic differentiations, lexical choice, stylistics and presumably processing factors, theoretical accounts making reliable predictions for real corpus examples have for a long time proven elusive. As for German, only quite recently, a number of corpus-based studies (Filippova and Strube, 2007; Speyer, 2005; Dipper and Zinsmeister, 2009) have made some good progress towards a coherenceoriented account of at least the left edge of the German clause structure, the Vorfeld constituent. What makes the technological application of theoretical insights even harder is that for most relevant factors, automatic recognition cannot be performed with high accuracy (e.g., a coreference accuracy in the 70’s means there is a good deal of noise) and for the higher-level notions such as the information-structural focus, interannotator agreement on real corpus data tends to be much lower than for core-grammatical</context>
</contexts>
<marker>Speyer, 2005</marker>
<rawString>Augustin Speyer. 2005. Competing constraints on vorfeldbesetzung in german. In Proceedings of the Constraints in Discourse Workshop, pages 79–87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heike Telljohann</author>
<author>Erhard Hinrichs</author>
<author>Sandra K¨ubler</author>
<author>Heike Zinsmeister</author>
</authors>
<title>Stylebook for the t¨ubingen treebank of written german (t¨uba-d/z). revised version.</title>
<date>2006</date>
<tech>Technical report, Seminar f¨ur Sprachwissenschaft, Universit¨at T¨ubingen.</tech>
<marker>Telljohann, Hinrichs, K¨ubler, Zinsmeister, 2006</marker>
<rawString>Heike Telljohann, Erhard Hinrichs, Sandra K¨ubler, and Heike Zinsmeister. 2006. Stylebook for the t¨ubingen treebank of written german (t¨uba-d/z). revised version. Technical report, Seminar f¨ur Sprachwissenschaft, Universit¨at T¨ubingen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik Velldal</author>
<author>Stephan Oepen</author>
</authors>
<title>Maximum entropy models for realization ranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 10th Machine Translation Summit,</booktitle>
<pages>109--116</pages>
<contexts>
<context position="6871" citStr="Velldal and Oepen, 2005" startWordPosition="1027" endWordPosition="1030">ring system. Clarke and Lapata (2010) have improved a sentence compression system by capturing prominence of phrases or referents in terms of lexical chain information inspired by Morris and Hirst (1991) and Centering (Grosz et al., 1995). In their system, discourse context is represented in terms of hard constraints modelling whether a certain constituent can be deleted or not. In the linearisation or surface realisation domain, there is a considerable body of work approximating information structure in terms of sentence-internal realisation (Ringger et al., 2004; Filippova and Strube, 2009; Velldal and Oepen, 2005; Cahill et al., 2007). Cahill and Riester (2009) improve realisation ranking for German – which mainly deals with word order variation – by representing precedence patterns of constituents in terms of asymmetries in their morphosyntactic properties. As a simple example, a pattern exploited by Cahill and Riester (2009) is the tendency of definite elements tend to precede indefinites, which, on a discourse level, reflects that given entities in a sentence tend to precede new entities. Other work on German surface realisation has highlighted the role of the initial position in the German sentenc</context>
</contexts>
<marker>Velldal, Oepen, 2005</marker>
<rawString>Erik Velldal and Stephan Oepen. 2005. Maximum entropy models for realization ranking. In Proceedings of the 10th Machine Translation Summit, pages 109–116, Thailand.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>