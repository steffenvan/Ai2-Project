<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000454">
<title confidence="0.9979425">
Shared Logistic Normal Distributions for Soft Parameter Tying
in Unsupervised Grammar Induction
</title>
<author confidence="0.945335">
Shay B. Cohen and Noah A. Smith
</author>
<affiliation confidence="0.91158475">
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.998873">
{scohen,nasmith}@cs.cmu.edu
</email>
<sectionHeader confidence="0.997389" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99973075">
We present a family of priors over probabilis-
tic grammar weights, called the shared logistic
normal distribution. This family extends the
partitioned logistic normal distribution, en-
abling factored covariance between the prob-
abilities of different derivation events in the
probabilistic grammar, providing a new way
to encode prior knowledge about an unknown
grammar. We describe a variational EM al-
gorithm for learning a probabilistic grammar
based on this family of priors. We then experi-
ment with unsupervised dependency grammar
induction and show significant improvements
using our model for both monolingual learn-
ing and bilingual learning with a non-parallel,
multilingual corpus.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999883055555556">
Probabilistic grammars have become an important
tool in natural language processing. They are most
commonly used for parsing and linguistic analy-
sis (Charniak and Johnson, 2005; Collins, 2003),
but are now commonly seen in applications like ma-
chine translation (Wu, 1997) and question answer-
ing (Wang et al., 2007). An attractive property of
probabilistic grammars is that they permit the use
of well-understood parameter estimation methods
for learning—both from labeled and unlabeled data.
Here we tackle the unsupervised grammar learning
problem, specifically for unlexicalized context-free
dependency grammars, using an empirical Bayesian
approach with a novel family of priors.
There has been an increased interest recently
in employing Bayesian modeling for probabilistic
grammars in different settings, ranging from putting
priors over grammar probabilities (Johnson et al.,
</bodyText>
<page confidence="0.979039">
74
</page>
<bodyText confidence="0.999893911764706">
2007) to putting non-parametric priors over deriva-
tions (Johnson et al., 2006) to learning the set of
states in a grammar (Finkel et al., 2007; Liang et al.,
2007). Bayesian methods offer an elegant frame-
work for combining prior knowledge with data.
The main challenge in Bayesian grammar learning
is efficiently approximating probabilistic inference,
which is generally intractable. Most commonly vari-
ational (Johnson, 2007; Kurihara and Sato, 2006)
or sampling techniques are applied (Johnson et al.,
2006).
Because probabilistic grammars are built out of
multinomial distributions, the Dirichlet family (or,
more precisely, a collection of Dirichlets) is a natural
candidate for probabilistic grammars because of its
conjugacy to the multinomial family. Conjugacy im-
plies a clean form for the posterior distribution over
grammar probabilities (given the data and the prior),
bestowing computational tractability.
Following work by Blei and Lafferty (2006) for
topic models, Cohen et al. (2008) proposed an alter-
native to Dirichlet priors for probabilistic grammars,
based on the logistic normal (LN) distribution over
the probability simplex. Cohen et al. used this prior
to softly tie grammar weights through the covariance
parameters of the LN. The prior encodes informa-
tion about which grammar rules’ weights are likely
to covary, a more intuitive and expressive represen-
tation of knowledge than offered by Dirichlet distri-
butions.1
The contribution of this paper is two-fold. First,
from the modeling perspective, we present a gen-
eralization of the LN prior of Cohen et al. (2008),
showing how to extend the use of the LN prior to
</bodyText>
<footnote confidence="0.969503333333333">
1Although the task, underlying model, and weights being
tied were different, Eisner (2002) also showed evidence for the
efficacy of parameter tying in grammar learning.
</footnote>
<note confidence="0.9419595">
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 74–82,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999568043478261">
tie between any grammar weights in a probabilistic
grammar (instead of only allowing weights within
the same multinomial distribution to covary). Sec-
ond, from the experimental perspective, we show
how such flexibility in parameter tying can help in
unsupervised grammar learning in the well-known
monolingual setting and in a new bilingual setting
where grammars for two languages are learned at
once (without parallel corpora).
Our method is based on a distribution which we
call the shared logistic normal distribution, which
is a distribution over a collection of multinomials
from different probability simplexes. We provide a
variational EM algorithm for inference.
The rest of this paper is organized as follows. In
§2, we give a brief explanation of probabilistic gram-
mars and introduce some notation for the specific
type of dependency grammar used in this paper, due
to Klein and Manning (2004). In §3, we present our
model and a variational inference algorithm for it. In
§4, we report on experiments for both monolingual
settings and a bilingual setting and discuss them. We
discuss future work (§5) and conclude in §6.
</bodyText>
<sectionHeader confidence="0.953355" genericHeader="introduction">
2 Probabilistic Grammars and
Dependency Grammar Induction
</sectionHeader>
<bodyText confidence="0.99999362962963">
A probabilistic grammar defines a probability dis-
tribution over grammatical derivations generated
through a step-by-step process. HMMs, for exam-
ple, can be understood as a random walk through
a probabilistic finite-state network, with an output
symbol sampled at each state. Each “step” of the
walk and each symbol emission corresponds to one
derivation step. PCFGs generate phrase-structure
trees by recursively rewriting nonterminal symbols
as sequences of “child” symbols (each itself either
a nonterminal symbol or a terminal symbol analo-
gous to the emissions of an HMM). Each step or
emission of an HMM and each rewriting operation
of a PCFG is conditionally independent of the other
rewriting operations given a single structural ele-
ment (one HMM or PCFG state); this Markov prop-
erty permits efficient inference for the probability
distribution defined by the probabilistic grammar.
In general, a probabilistic grammar defines the
joint probability of a string x and a grammatical
where fk,i is a function that “counts” the number
of times the kth distribution’s ith event occurs in
the derivation. The θ are a collection of K multi-
nomials (θ1, ..., θK), the kth of which includes Nk
events. Note that there may be many derivations y
for a given string x—perhaps even infinitely many
in some kinds of grammars.
</bodyText>
<sectionHeader confidence="0.592499" genericHeader="method">
2.1 Dependency Model with Valence
</sectionHeader>
<bodyText confidence="0.9977281875">
HMMs and PCFGs are the best-known probabilis-
tic grammars, but there are many others. In this
paper, we use the “dependency model with va-
lence” (DMV), due to Klein and Manning (2004).
DMV defines a probabilistic grammar for unla-
beled, projective dependency structures. Klein and
Manning (2004) achieved their best results with a
combination of DMV with a model known as the
“constituent-context model” (CCM). We do not ex-
periment with CCM in this paper, because it does
not fit directly in a Bayesian setting (it is highly defi-
cient) and because state-of-the-art unsupervised de-
pendency parsing results have been achieved with
DMV alone (Smith, 2006).
Using the notation above, DMV defines x =
(x1,x2, ..., xn) to be a sentence. x0 is a special
“wall” symbol, $, on the left of every sentence. A
tree y is defined by a pair of functions yleft and
yright (both 10, 1, 2,..., n} __+ 2{1,2,...,n}) that map
each word to its sets of left and right dependents,
respectively. Here, the graph is constrained to be a
projective tree rooted at x0 = $: each word except $
has a single parent, and there are no cycles or cross-
ing dependencies. yleft(0) is taken to be empty, and
yright(0) contains the sentence’s single head. Let
y(i) denote the subtree rooted at position i. The
probability P(y(i) I xi, θ) of generating this sub-
tree, given its head word xi, is defined recursively,
as described in Fig. 1 (Eq. 2).
The probability of the entire tree is given by
p(x, y θ) = P(y(0) 1$, θ). The θ are the multi-
nomial distributions 0s(· , ·,·) and 0c(· 1 ·, ·). To
</bodyText>
<equation confidence="0.982614444444444">
Nk
i=1
K
H
k=1
derivation y:
p(x, y 1 θ) =
k ,0ki(X,Y)
0 (1)
Nk
i=1
K
k=1
= exp
fk,i(x, y) log 0k,i
75
P(y(i)  |xi, 0) = HDE{left,right} θs(stop  |xi, D, [yD(i) = ∅]) (2)
× HjEYD(i) θs(¬stop  |xi, D, firstY(j)) × θc(xj  |xi, D) × P(y(j)  |xj, 0)
</equation>
<figureCaption confidence="0.995494">
Figure 1: The “dependency model with valence” recursive equation. firsty(j) is a predicate defined to be true iff xj is
the closest child (on either side) to its parent xi. The probability of the tree p(x, y  |0) = P(y(0)  |$, 0).
</figureCaption>
<bodyText confidence="0.9999501">
follow the general setting of Eq. 1, we index these
distributions as 01, ..., 0K.
Headden et al. (2009) extended DMV so that the
distributions θe condition on the valence as well,
with smoothing, and showed significant improve-
ments for short sentences. Our experiments found
that these improvements do not hold on longer sen-
tences. Here we experiment only with DMV, but
note that our techniques are also applicable to richer
probabilistic grammars like that of Headden et al.
</bodyText>
<subsectionHeader confidence="0.999286">
2.2 Learning DMV
</subsectionHeader>
<bodyText confidence="0.9999891">
Klein and Manning (2004) learned the DMV prob-
abilities 0 from a corpus of part-of-speech-tagged
sentences using the EM algorithm. EM manipulates
0 to locally optimize the likelihood of the observed
portion of the data (here, x), marginalizing out the
hidden portions (here, y). The likelihood surface
is not globally concave, so EM only locally opti-
mizes the surface. Klein and Manning’s initializa-
tion, though reasonable and language-independent,
was an important factor in performance.
Various alternatives to EM were explored by
Smith (2006), achieving substantially more accu-
rate parsing models by altering the objective func-
tion. Smith’s methods did require substantial hyper-
parameter tuning, and the best results were obtained
using small annotated development sets to choose
hyperparameters. In this paper, we consider only
fully unsupervised methods, though we the Bayesian
ideas explored here might be merged with the bias-
ing approaches of Smith (2006) for further benefit.
</bodyText>
<sectionHeader confidence="0.917812" genericHeader="method">
3 Parameter Tying in the Bayesian Setting
</sectionHeader>
<bodyText confidence="0.999967777777778">
As stated above, 0 comprises a collection of multi-
nomials that weights the grammar. Taking the
Bayesian approach, we wish to place a prior on those
multinomials, and the Dirichlet family is a natural
candidate for such a prior because of its conjugacy,
which makes inference algorithms easier to derive.
For example, if we make a “mean-field assumption,”
with respect to hidden structure and weights, the
variational algorithm for approximately inferring the
distribution over 0 and trees y resembles the tradi-
tional EM algorithm very closely (Johnson, 2007).
In fact, variational inference in this case takes an ac-
tion similar to smoothing the counts using the exp-Ψ
function during the E-step. Variational inference can
be embedded in an empirical Bayes setting, in which
we optimize the variational bound with respect to the
hyperparameters as well, repeating the process until
convergence.
</bodyText>
<subsectionHeader confidence="0.997525">
3.1 Logistic Normal Distributions
</subsectionHeader>
<bodyText confidence="0.999962555555556">
While Dirichlet priors over grammar probabilities
make learning algorithms easy, they are limiting.
In particular, as noted by Blei and Lafferty (2006),
there is no explicit flexible way for the Dirichlet’s
parameters to encode beliefs about covariance be-
tween the probabilities of two events. To illustrate
this point, we describe how a multinomial 0 of di-
mension d is generated from a Dirichlet distribution
with parameters α = hα1,..., αdi:
</bodyText>
<listItem confidence="0.999816">
1. Generate ηj ∼ F(αj,1) independently for j ∈
{1, ..., d}.
2. θj ← ηj/ i ηi.
</listItem>
<bodyText confidence="0.999574857142857">
where F(α,1) is a Gamma distribution with shape α
and scale 1.
Correlation among θi and θj, i =6 j, cannot be
modeled directly, only through the normalization
in step 2. In contrast, LN distributions (Aitchison,
1986) provide a natural way to model such correla-
tion. The LN draws a multinomial 0 as follows:
</bodyText>
<listItem confidence="0.9983355">
1. Generate ri ∼ Normal(µ, E).
2. θj ← exp(ηj)/ Ei exp(ηi).
</listItem>
<page confidence="0.426715">
76
</page>
<equation confidence="0.986105171428571">
I1 = {1:2,3:6,7:9} = {
I2 = {1:2,3:6} = {
I3 = {1:4,5:7} = {
IN = {1:2} = {
I1,1, I1,2, I1,L1 }
I2,1, I2,L2 }
I3,1, I3,L3 }
I4,L4 }
J1 J2 JK
} partition struct. 8
η1 = hη1,1, η1,2, η1,3, η1,4, η1,5, η1,6, η1,7, η1,8, η1,11i
η2 = hη2,1,η2,2, η2,3,η2,4,η2,5,η2,�2i
η3 = hη3,1,η3,2,η3,3,η3,4, η3,5,η3,6,η3,�3i
η4 = hη4,1, η4,84i
∼ Normal(µ1, E1) I sample η
∼ Normal(µ2,E2)
∼ Normal(µ3,E3)
∼ Normal(µ4, E4)
˜η1 = 1 3hη1,1 + η2,1 + η4,1, η1,2 + η2,2 + η4,2i η1,5 + η2,5
˜η2 = 1 3hη1,3 + η2,3 + η3,1 , η1,4 + η2,4 + η3,2,
˜η3 = 12hη1,7 + η3,5, η1,8 + η3,6, η1,9 + η3,7i
+ η3,3, η1,6 + η2,6 + η3,4i } combine η
θ1 =
θ2 =
θ3 =
N1
(exp˜η1) Ei0=1 exp˜η1,i0
N2
(exp η2) Ei0=1 exp ˜η2,i0
3
(exp ˜η3)EiN0 = 1 exp ˜η3,i0
I softmax
Figure 2: An example of a shared logistic normal distribution, illustrating Def. 1. N = 4 experts are used to sample
K = 3 multinomials; L1 = 3, L2 = 2, L3 = 2, L4 = 1, `1 = 9, `2 = 6, `3 = 7, `4 = 2, N1 = 2, N2 = 4, and
N3 = 3. This figure is best viewed in color.
</equation>
<bodyText confidence="0.999210895833333">
Blei and Lafferty (2006) defined correlated topic
models by replacing the Dirichlet in latent Dirich-
let allocation models (Blei et al., 2003) with a LN
distribution. Cohen et al. (2008) compared Dirichlet
and LN distributions for learning DMV using em-
pirical Bayes, finding substantial improvements for
English using the latter.
In that work, we obtained improvements even
without specifying exactly which grammar proba-
bilities covaried. While empirical Bayes learning
permits these covariances to be discovered without
supervision, we found that by initializing the covari-
ance to encode beliefs about which grammar prob-
abilities should covary, further improvements were
possible. Specifically, we grouped the Penn Tree-
bank part-of-speech tags into coarse groups based
on the treebank annotation guidelines and biased
the initial covariance matrix for each child distri-
bution θc(·  |·, ·) so that the probabilities of child
tags from the same coarse group covaried. For ex-
ample, the probability that a past-tense verb (VBD)
has a singular noun (NN) as a right child may be
correlated with the probability that it has a plu-
ral noun (NNS) as a right child. Hence linguistic
knowledge—specifically, a coarse grouping of word
classes—can be encoded in the prior.
A per-distribution LN distribution only permits
probabilities within a multinomial to covary. We
will generalize the LN to permit covariance among
any probabilities in θ, throughout the model. For
example, the probability of a past-tense verb (VBD)
having a noun as a right child might correlate with
the probability that other kinds of verbs (VBZ, VBN,
etc.) have a noun as a right child.
The partitioned logistic normal distribution
(PLN) is a generalization of the LN distribution
that takes the first step towards our goal (Aitchison,
1986). Generating from PLN involves drawing a
random vector from a multivariate normal distribu-
tion, but the logistic transformation is applied to dif-
ferent parts of the vector, leading to sampled multi-
nomial distributions of the required lengths from
different probability simplices. This is in principle
what is required for arbitrary covariance between
grammar probabilities, except that DMV has O(t2)
weights for a part-of-speech vocabulary of size t, re-
quiring a very large multivariate normal distribution
with O(t4) covariance parameters.
</bodyText>
<page confidence="0.992061">
77
</page>
<subsectionHeader confidence="0.996115">
3.2 Shared Logistic Normal Distributions
</subsectionHeader>
<bodyText confidence="0.985946895522388">
To solve this problem, we suggest a refinement of
the class of PLN distributions. Instead of using a
single normal vector for all of the multinomials, we
use several normal vectors, partition each one and
then recombine parts which correspond to the same
multinomial, as a mixture. Next, we apply the lo-
gisitic transformation on the mixed vectors (each of
which is normally distributed as well). Fig. 2 gives
an example of a non-trivial case of using a SLN
distribution, where three multinomials are generated
from four normal experts.
We now formalize this notion. For a natural num-
ber N, we denote by 1:N the set {1, ..., N}. For a
vector in v E RN and a set I C_ 1:N, we denote
by vI to be the vector created from v by using the
coordinates in I. Recall that K is the number of
multinomials in the probabilistic grammar, and Nk
is the number of events in the kth multinomial.
Definition 1. We define a shared logistic nor-
mal distribution with N “experts” over a collec-
tion of K multinomial distributions. Let ηn —
Normal(µn, En) be a set of multivariate normal
variables for n E 1:N, where the length of ηn
is denoted `n. Let In = {In,j}Ln
j=1 be a parti-
tion of 1:`n into Ln sets, such that ULn
j=1In,j =
1:`n and In,j n In,j, = 0 for j =� j&apos;. Let Jk
for k E 1:K be a collection of (disjoint) sub-
sets of {In,j I n E 1:N, j E 1:`n, IIn,j =
Nk}, such that all sets in Jk are of the same size,
Nk. Let ηk =|Jk |F-In,j∈Jk ηn,In,j, and θk i =
exp(˜ηk,i) /Ei, exp(˜ηk,i,). We then say θ distributes
according to the shared logistic normal distribution
with partition structure S = ({In}N n=1, {Jk}Kk=1)
and normal experts {(µn, En)}Nn=1 and denote it by
θ — SLN(µ, E, S).
The partitioned LN distribution in Aitchison
(1986) can be formulated as a shared LN distribution
where N = 1. The LN collection used by Cohen et
al. (2008) is the special case where N = K, each
Ln = 1, each `k = Nk, and each Jk = {Ik,1}.
The covariance among arbitrary θk,i is not defined
directly; it is implied by the definition of the nor-
mal experts ηn,In,j, for each In,j E Jk. We note
that a SLN can be represented as a PLN by relying
on the distributivity of the covariance operator, and
merging all the partition structure into one (perhaps
sparse) covariance matrix. However, if we are inter-
ested in keeping a factored structure on the covari-
ance matrices which generate the grammar weights,
we cannot represent every SLN as a PLN.
It is convenient to think of each ηi,j as a weight
associated with a unique event’s probability, a cer-
tain outcome of a certain multinomial in the prob-
abilistic grammar. By letting different ηi,j covary
with each other, we loosen the relationships among
θk,j and permit the model—at least in principle—
to learn patterns from the data. Def. 1 also implies
that we multiply several multinomials together in a
product-of-experts style (Hinton, 1999), because the
exponential of a mixture of normals becomes a prod-
uct of (unnormalized) probabilities.
Our extension to the model in Cohen et al. (2008)
follows naturally after we have defined the shared
LN distribution. The generative story for this model
is as follows:
</bodyText>
<listItem confidence="0.998088">
1. Generate θ — SLN(µ, E, S), where θ is a col-
lection of vectors θk, k = 1, ..., K.
2. Generate x and y from p(x, y θ) (i.e., sample
from the probabilistic grammar).
</listItem>
<subsectionHeader confidence="0.907295">
3.3 Inference
</subsectionHeader>
<bodyText confidence="0.99505795">
In this work, the partition structure S is known, the
sentences x are observed, the trees y and the gram-
mar weights θ are hidden, and the parameters of the
shared LN distribution µ and E are learned.2
Our inference algorithm aims to find the poste-
rior over the grammar probabilities θ and the hidden
structures (grammar trees y). To do that, we use
variational approximation techniques (Jordan et al.,
1999), which treat the problem of finding the pos-
terior as an optimization problem aimed to find the
best approximation q(θ, y) of the posterior p(θ, y �
x, µ, E, S). The posterior q needs to be constrained
to be within a family of tractable and manageable
distributions, yet rich enough to represent good ap-
proximations of the true posterior. “Best approx-
imation” is defined as the KL divergence between
q(θ, y) and p(θ, y I x, µ, E, S).
Our variational inference algorithm uses a mean-
field assumption: q(θ, y) = q(θ)q(y). The distri-
bution q(θ) is assumed to be a LN distribution with
</bodyText>
<footnote confidence="0.993078">
2In future work, we might aim to learn S.
</footnote>
<page confidence="0.981894">
78
</page>
<equation confidence="0.94374">
log p(x  |µ, E, S) ≥ (EN1 Eq [log p(ηk  |µk, Σk)]) + (Ek=K 1 ENk1˜fk,i˜ψk,i) + H(q) (3)
• v •
B
˜fk,i Ey q(y)fk,i(x, y) (4)
&apos;/, o˜C˜1 Nk ˜C (˜σ� i)2
4 0 = µk,i − log ζk + 1 − ζk &amp;quot;,=1 exp µk,i + 2 1 (5)
PC o 1 (6)
µk — Jk  |EIr,j ∈Jk Ar,Ir,j
(˜σCk )2 o Jk|2 EIr,j∈Jk σr,Ir,j (7)
</equation>
<figureCaption confidence="0.975531666666667">
Figure 3: Variational inference bound. Eq. 3 is the bound itself, using notation defined in Eqs. 4–7 for clarity. Eq. 4
defines expected counts of the grammar events under the variational distribution q(y), calculated using dynamic pro-
gramming. Eq. 5 describes the weights for the weighted grammar defined by q(y). Eq. 6 and Eq. 7 describe the mean
and the variance, respectively, for the multivariate normal eventually used with the weighted grammar. These values
are based on the parameterization of q(θ) by ˜µij and ˜σZ �. An additional set of variational parameters is ζk, which
helps resolve the non-conjugacy of the LN distribution through a first order Taylor approximation.
</figureCaption>
<bodyText confidence="0.998947423076923">
all off-diagonal covariances fixed at zero (i.e., the
variational parameters consist of a single mean ˜µk,i
and a single variance ˜σ2k,i for each θk,i). There is
an additional variational parameter, ˜ζk per multino-
mial, which is the result of an additional variational
approximation because of the lack of conjugacy of
the LN distribution to the multinomial distribution.
The distribution q(y) is assumed to be defined by a
DMV with unnormalized probabilities ˜ψ.
Inference optimizes the bound B given in Fig. 3
(Eq. 3) with respect to the variational parameters.
Our variational inference algorithm is derived simi-
larly to that of Cohen et al. (2008). Because we wish
to learn the values of µ and E, we embed variational
inference as the E step within a variational EM algo-
rithm, shown schematically in Fig. 4. In our exper-
iments, we use this variational EM algorithm on a
training set, and then use the normal experts’ means
to get a point estimate for θ, the grammar weights.
This is called empirical Bayesian estimation. Our
approach differs from maximum a posteriori (MAP)
estimation, since we re-estimate the parameters of
the normal experts. Exact MAP estimation is prob-
ably not feasible; a variational algorithm like ours
might be applied, though better performance is ex-
pected from adjusting the SLN to fit the data.
</bodyText>
<sectionHeader confidence="0.999721" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999900478260869">
Our experiments involve data from two treebanks:
the Wall Street Journal Penn treebank (Marcus et
al., 1993) and the Chinese treebank (Xue et al.,
2004). In both cases, following standard practice,
sentences were stripped of words and punctuation,
leaving part-of-speech tags for the unsupervised in-
duction of dependency structure. For English, we
train on §2–21, tune on §22 (without using annotated
data), and report final results on §23. For Chinese,
we train on §1–270, use §301–1151 for development
and report testing results on §271–300.3
To evaluate performance, we report the fraction
of words whose predicted parent matches the gold
standard corpus. This performance measure is also
known as attachment accuracy. We considered two
parsing methods after extracting a point estimate
for the grammar: the most probable “Viterbi” parse
(argmaxy p(y  |x, θ)) and the minimum Bayes risk
(MBR) parse (argminy Ep(y,|x,θ)[`(y; x, y&apos;)]) with
dependency attachment error as the loss function
(Goodman, 1996). Performance with MBR parsing
is consistently higher than its Viterbi counterpart, so
we report only performance with MBR parsing.
</bodyText>
<subsectionHeader confidence="0.994408">
4.1 Nouns, Verbs, and Adjectives
</subsectionHeader>
<bodyText confidence="0.9494055">
In this paper, we use a few simple heuristics to de-
cide which partition structure S to use. Our heuris-
</bodyText>
<footnote confidence="0.9029795">
3Unsupervised training for these datasets can be costly,
and requires iteratively running a cubic-time inside-outside dy-
namic programming algorithm, so we follow Klein and Man-
ning (2004) in restricting the training set to sentences of ten or
fewer words in length. Short sentences are also less structurally
ambiguous and may therefore be easier to learn from.
</footnote>
<page confidence="0.997739">
79
</page>
<bodyText confidence="0.663505666666667">
Input: initial parameters µ(°), E(°), partition
structure S, observed data x, number of
iterations T
</bodyText>
<equation confidence="0.927887578947369">
Output: learned parameters µ, E
t i 1 ;
while t &lt; T do
E-step (for B = 1, ..., M) do: repeat
optimize B w.r.t. ˜µ`,(t)
r , r = 1, ..., N;
optimize B w.r.t. ˜σ`,(t)
r , r = 1, ..., N;
update ˜ζ`,(t)
r , r = 1, ..., N;
`, (t)
updater , r = 1, ..., N;
compute counts ˜f`,(t)
r , r = 1, ..., N;
until convergence of B ;
M-step: optimize B w.r.t. µ(t) and E(t);
t i t + 1;
end
return µ(T), E(T)
</equation>
<figureCaption confidence="0.991852833333333">
Figure 4: Main details of the variational inference EM
algorithm with empirical Bayes estimation of µ and E.
B is the bound defined in Fig. 3 (Eq. 3). N is the number
of normal experts for the SLN distribution defining the
prior. M is the number of training examples. The full
algorithm is given in Cohen and Smith (2009).
</figureCaption>
<bodyText confidence="0.999698066666667">
tics rely mainly on the centrality of content words:
nouns, verbs, and adjectives. For example, in the En-
glish treebank, the most common attachment errors
(with the LN prior from Cohen et al., 2008) happen
with a noun (25.9%) or a verb (16.9%) parent. In
the Chinese treebank, the most common attachment
errors happen with noun (36.0%) and verb (21.2%)
parents as well. The errors being governed by such
attachments are the direct result of nouns and verbs
being the most common parents in these data sets.
Following this observation, we compare four dif-
ferent settings in our experiments (all SLN settings
include one normal expert for each multinomial on
its own, equivalent to the regular LN setting from
Cohen et al.):
</bodyText>
<listItem confidence="0.940765">
• TIEV: We add normal experts that tie all proba-
</listItem>
<bodyText confidence="0.971394888888889">
bilities corresponding to a verbal parent (any par-
ent, using the coarse tags of Cohen et al., 2008).
Let V be the set of part-of-speech tags which be-
long to the verb category. For each direction D
(left or right), the set of multinomials of the form
Oc(·  |v, D), for v E V , all share a normal expert.
For each direction D and each boolean value B
of the predicate firsty(·), the set of multinomials
Os(·  |x, D, v), for v E V share a normal expert.
</bodyText>
<listItem confidence="0.99895425">
• TIEN: This is the same as TIEV, only for nominal
parents.
• TIEV&amp;N: Tie both verbs and nouns (in separate
partitions). This is equivalent to taking the union
of the partition structures of the above two set-
tings.
• TIEA: This is the same as TIEV, only for adjecti-
val parents.
</listItem>
<bodyText confidence="0.999967166666667">
Since inference for a model with parameter tying
can be computationally intensive, we first run the in-
ference algorithm without parameter tying, and then
add parameter tying to the rest of the inference algo-
rithm’s execution until convergence.
Initialization is important for the inference al-
gorithm, because the variational bound is a non-
concave function. For the expected values of the
normal experts, we use the initializer from Klein and
Manning (2004). For the covariance matrices, we
follow the setting in Cohen et al. (2008) in our ex-
periments also described in §3.1. For each treebank,
we divide the tags into twelve disjoint tag families.4
The covariance matrices for all dependency distri-
butions were initialized with 1 on the diagonal, 0.5
between tags which belong to the same family, and
0 otherwise. This initializer has been shown to be
more successful than an identity covariance matrix.
</bodyText>
<subsectionHeader confidence="0.996119">
4.2 Monolingual Experiments
</subsectionHeader>
<bodyText confidence="0.999987571428571">
We begin our experiments with a monolingual set-
ting, where we learn grammars for English and Chi-
nese (separately) using the settings described above.
The attachment accuracy for this set of experi-
ments is described in Table 1. The baselines include
right attachment (where each word is attached to the
word to its right), MLE via EM (Klein and Man-
ning, 2004), and empirical Bayes with Dirichlet and
LN priors (Cohen et al., 2008). We also include a
“ceiling” (DMV trained using supervised MLE from
the training sentences’ trees). For English, we see
that tying nouns, verbs or adjectives improves per-
formance compared to the LN baseline. Tying both
nouns and verbs improves performance a bit more.
</bodyText>
<footnote confidence="0.486979666666667">
4These are simply coarser tags: adjective, adverb, conjunc-
tion, foreign word, interjection, noun, number, particle, prepo-
sition, pronoun, proper noun, verb.
</footnote>
<page confidence="0.974535">
80
</page>
<table confidence="0.999631">
attachment acc. (%)
&lt; 10 &lt; 20 all
Chinese English Attach-Right 38.4 33.4 31.7
EM (K&amp;M, 2004) 46.1 39.9 35.9
Dirichlet 46.1 40.6 36.9
LN (CG&amp;S, 2008) 59.4 45.9 40.5
SLN, TIEV 60.2 46.2 40.0
SLN, TIEN 60.2 46.7 40.9
SLN, TIEV&amp;N 61.3 47.4 41.4
SLN, TIEA 59.9 45.8 40.9
Biling. SLN, TIEV †61.6 47.6 41.7
Biling. SLN, TIEN †61.8 48.1 †42.1
Biling. SLN, TIEV&amp;N 62.0 †48.0 42.2
Biling. SLN, TIEA 61.3 47.6 41.7
Supervised MLE 84.5 74.9 68.8
Attach-Right 34.9 34.6 34.6
EM (K&amp;M, 2004) 38.3 36.1 32.7
Dirichlet 38.3 35.9 32.4
LN 50.1 40.5 35.8
SLN, TIEV †51.9 42.0 35.8
SLN, TIEN 43.0 38.4 33.7
SLN, TIEV&amp;N 45.0 39.2 34.2
SLN, TIEA 47.4 40.4 35.2
Biling. SLN, TIEV †51.9 42.0 35.8
Biling. SLN, TIEN 48.0 38.9 33.8
Biling. SLN, TIEV&amp;N †51.5 †41.7 35.3
Biling. SLN, TIEA 52.0 41.3 35.2
Supervised MLE 84.3 66.1 57.6
</table>
<tableCaption confidence="0.999395">
Table 1: Attachment accuracy of different models, on test
</tableCaption>
<bodyText confidence="0.994007833333333">
data from the Penn Treebank and the Chinese Treebank
of varying levels of difficulty imposed through a length
filter. Attach-Right attaches each word to the word on
its right and the last word to $. Bold marks best overall
accuracy per length bound, and † marks figures that are
not significantly worse (binomial sign test, p &lt; 0.05).
</bodyText>
<subsectionHeader confidence="0.998325">
4.3 Bilingual Experiments
</subsectionHeader>
<bodyText confidence="0.9997479">
Leveraging information from one language for the
task of disambiguating another language has re-
ceived considerable attention (Dagan, 1991; Smith
and Smith, 2004; Snyder and Barzilay, 2008; Bur-
kett and Klein, 2008). Usually such a setting re-
quires a parallel corpus or other annotated data that
ties between those two languages.5
Our bilingual experiments use the English and
Chinese treebanks, which are not parallel corpora,
to train parsers for both languages jointly. Shar-
</bodyText>
<footnote confidence="0.7426575">
5Haghighi et al. (2008) presented a technique to learn bilin-
gual lexicons from two non-parallel monolingual corpora.
</footnote>
<bodyText confidence="0.99997108">
ing information between those two models is done
by softly tying grammar weights in the two hidden
grammars.
We first merge the models for English and Chi-
nese by taking a union of the multinomial fami-
lies of each and the corresponding prior parame-
ters. We then add a normal expert that ties be-
tween the parts of speech in the respective parti-
tion structures for both grammars together. Parts
of speech are matched through the single coarse
tagset (footnote 4). For example, with TIEV, let
V = VEng U VChi be the set of part-of-speech tags
which belong to the verb category for either tree-
bank. Then, we tie parameters for all part-of-speech
tags in V . We tested this joint model for each of
TIEV, TIEN, TIEV&amp;N, and TIEA. After running
the inference algorithm which learns the two mod-
els jointly, we use unseen data to test each learned
model separately.
Table 1 includes the results for these experiments.
The performance on English improved significantly
in the bilingual setting, achieving highest perfor-
mance with TIEV&amp;N. Performance with Chinese is
also the highest in the bilingual setting, with TIEA
and TIEV&amp;N.
</bodyText>
<sectionHeader confidence="0.999838" genericHeader="method">
5 Future Work
</sectionHeader>
<bodyText confidence="0.9999675">
In future work we plan to lexicalize the model, in-
cluding a Bayesian grammar prior that accounts for
the syntactic patterns of words. Nonparametric mod-
els (Teh, 2006) may be appropriate. We also believe
that Bayesian discovery of cross-linguistic patterns
is an exciting topic worthy of further exploration.
</bodyText>
<sectionHeader confidence="0.999384" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99995775">
We described a Bayesian model that allows soft pa-
rameter tying among any weights in a probabilistic
grammar. We used this model to improve unsuper-
vised parsing accuracy on two different languages,
English and Chinese, achieving state-of-the-art re-
sults. We also showed how our model can be effec-
tively used to simultaneously learn grammars in two
languages from non-parallel multilingual data.
</bodyText>
<sectionHeader confidence="0.999345" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.842116666666667">
This research was supported by NSF IIS-0836431. The
authors thank the anonymous reviewers and Sylvia Reb-
holz for helpful comments.
</bodyText>
<page confidence="0.998251">
81
</page>
<sectionHeader confidence="0.998333" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999964694117648">
J. Aitchison. 1986. The Statistical Analysis of Composi-
tional Data. Chapman and Hall, London.
D. M. Blei and J. D. Lafferty. 2006. Correlated topic
models. In Proc. of NIPS.
D. M. Blei, A. Ng, and M. Jordan. 2003. Latent Dirich-
let allocation. Journal of Machine Learning Research,
3:993–1022.
D. Burkett and D. Klein. 2008. Two languages are better
than one (for syntactic parsing). In Proc. of EMNLP.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and maxent discriminative reranking. In
Proc. of ACL.
S. B. Cohen and N. A. Smith. 2009. Inference for proba-
bilistic grammars with shared logistic normal distribu-
tions. Technical report, Carnegie Mellon University.
S. B. Cohen, K. Gimpel, and N. A. Smith. 2008. Logistic
normal priors for unsupervised probabilistic grammar
induction. In NIPS.
M. Collins. 2003. Head-driven statistical models for nat-
ural language processing. Computational Linguistics,
29:589–637.
I. Dagan. 1991. Two languages are more informative
than one. In Proc. of ACL.
J. Eisner. 2002. Transformational priors over grammars.
In Proc. of EMNLP.
J. R. Finkel, T. Grenager, and C. D. Manning. 2007. The
infinite tree. In Proc. of ACL.
J. Goodman. 1996. Parsing algorithms and metrics. In
Proc. of ACL.
A. Haghighi, P. Liang, T. Berg-Kirkpatrick, and D. Klein.
2008. Learning bilingual lexicons from monolingual
corpora. In Proc. of ACL.
W. P. Headden, M. Johnson, and D. McClosky. 2009.
Improving unsupervised dependency parsing with
richer contexts and smoothing. In Proc. of NAACL-
HLT.
G. E. Hinton. 1999. Products of experts. In Proc. of
ICANN.
M. Johnson, T. L. Griffiths, and S. Goldwater. 2006.
Adaptor grammars: A framework for specifying com-
positional nonparameteric Bayesian models. In NIPS.
M. Johnson, T. L. Griffiths, and S. Goldwater. 2007.
Bayesian inference for PCFGs via Markov chain
Monte Carlo. In Proc. of NAACL.
M. Johnson. 2007. Why doesn’t EM find good HMM
POS-taggers? In Proc. EMNLP-CoNLL.
M. I. Jordan, Z. Ghahramani, T. S. Jaakola, and L. K.
Saul. 1999. An introduction to variational methods
for graphical models. Machine Learning, 37(2):183–
233.
D. Klein and C. D. Manning. 2004. Corpus-based induc-
tion of syntactic structure: Models of dependency and
constituency. In Proc. of ACL.
K. Kurihara and T. Sato. 2006. Variational Bayesian
grammar induction for natural language. In Proc. of
ICGI.
P. Liang, S. Petrov, M. Jordan, and D. Klein. 2007. The
infinite PCFG using hierarchical Dirichlet processes.
In Proc. of EMNLP.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn treebank. Computational Linguistics,
19:313–330.
D. A. Smith and N. A. Smith. 2004. Bilingual parsing
with factored estimation: Using English to parse Ko-
rean. In Proc. of EMNLP, pages 49–56.
N. A. Smith. 2006. Novel Estimation Methods for Unsu-
pervised Discovery of Latent Structure in Natural Lan-
guage Text. Ph.D. thesis, Johns Hopkins University.
B. Snyder and R. Barzilay. 2008. Unsupervised multi-
lingual learning for morphological segmentation. In
Proc. of ACL.
Y. W. Teh. 2006. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Proc. of
COLING-ACL.
M. Wang, N. A. Smith, and T. Mitamura. 2007. What
is the Jeopardy model? a quasi-synchronous grammar
for question answering. In Proc. of EMNLP.
D. Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Comp. Ling., 23(3):377–404.
N. Xue, F. Xia, F.-D. Chiou, and M. Palmer. 2004. The
Penn Chinese Treebank: Phrase structure annotation
of a large corpus. Natural Language Engineering,
10(4):1–30.
</reference>
<page confidence="0.999129">
82
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.432310">
<title confidence="0.76656625">Shared Logistic Normal Distributions for Soft Parameter in Unsupervised Grammar Induction B. Cohen A. Language Technologies</title>
<affiliation confidence="0.997045">School of Computer Carnegie Mellon</affiliation>
<address confidence="0.993022">Pittsburgh, PA 15213,</address>
<abstract confidence="0.994928411764706">We present a family of priors over probabilistic grammar weights, called the shared logistic normal distribution. This family extends the partitioned logistic normal distribution, enabling factored covariance between the probabilities of different derivation events in the probabilistic grammar, providing a new way to encode prior knowledge about an unknown grammar. We describe a variational EM algorithm for learning a probabilistic grammar based on this family of priors. We then experiment with unsupervised dependency grammar induction and show significant improvements using our model for both monolingual learnand with a non-parallel, multilingual corpus.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Aitchison</author>
</authors>
<title>The Statistical Analysis of Compositional Data.</title>
<date>1986</date>
<publisher>Chapman and Hall,</publisher>
<location>London.</location>
<contexts>
<context position="11607" citStr="Aitchison, 1986" startWordPosition="1847" endWordPosition="1848"> by Blei and Lafferty (2006), there is no explicit flexible way for the Dirichlet’s parameters to encode beliefs about covariance between the probabilities of two events. To illustrate this point, we describe how a multinomial 0 of dimension d is generated from a Dirichlet distribution with parameters α = hα1,..., αdi: 1. Generate ηj ∼ F(αj,1) independently for j ∈ {1, ..., d}. 2. θj ← ηj/ i ηi. where F(α,1) is a Gamma distribution with shape α and scale 1. Correlation among θi and θj, i =6 j, cannot be modeled directly, only through the normalization in step 2. In contrast, LN distributions (Aitchison, 1986) provide a natural way to model such correlation. The LN draws a multinomial 0 as follows: 1. Generate ri ∼ Normal(µ, E). 2. θj ← exp(ηj)/ Ei exp(ηi). 76 I1 = {1:2,3:6,7:9} = { I2 = {1:2,3:6} = { I3 = {1:4,5:7} = { IN = {1:2} = { I1,1, I1,2, I1,L1 } I2,1, I2,L2 } I3,1, I3,L3 } I4,L4 } J1 J2 JK } partition struct. 8 η1 = hη1,1, η1,2, η1,3, η1,4, η1,5, η1,6, η1,7, η1,8, η1,11i η2 = hη2,1,η2,2, η2,3,η2,4,η2,5,η2,�2i η3 = hη3,1,η3,2,η3,3,η3,4, η3,5,η3,6,η3,�3i η4 = hη4,1, η4,84i ∼ Normal(µ1, E1) I sample η ∼ Normal(µ2,E2) ∼ Normal(µ3,E3) ∼ Normal(µ4, E4) ˜η1 = 1 3hη1,1 + η2,1 + η4,1, η1,2 + η2,2 +</context>
<context position="14546" citStr="Aitchison, 1986" startWordPosition="2376" endWordPosition="2377">fically, a coarse grouping of word classes—can be encoded in the prior. A per-distribution LN distribution only permits probabilities within a multinomial to covary. We will generalize the LN to permit covariance among any probabilities in θ, throughout the model. For example, the probability of a past-tense verb (VBD) having a noun as a right child might correlate with the probability that other kinds of verbs (VBZ, VBN, etc.) have a noun as a right child. The partitioned logistic normal distribution (PLN) is a generalization of the LN distribution that takes the first step towards our goal (Aitchison, 1986). Generating from PLN involves drawing a random vector from a multivariate normal distribution, but the logistic transformation is applied to different parts of the vector, leading to sampled multinomial distributions of the required lengths from different probability simplices. This is in principle what is required for arbitrary covariance between grammar probabilities, except that DMV has O(t2) weights for a part-of-speech vocabulary of size t, requiring a very large multivariate normal distribution with O(t4) covariance parameters. 77 3.2 Shared Logistic Normal Distributions To solve this p</context>
<context position="16851" citStr="Aitchison (1986)" startWordPosition="2788" endWordPosition="2789">of ηn is denoted `n. Let In = {In,j}Ln j=1 be a partition of 1:`n into Ln sets, such that ULn j=1In,j = 1:`n and In,j n In,j, = 0 for j =� j&apos;. Let Jk for k E 1:K be a collection of (disjoint) subsets of {In,j I n E 1:N, j E 1:`n, IIn,j = Nk}, such that all sets in Jk are of the same size, Nk. Let ηk =|Jk |F-In,j∈Jk ηn,In,j, and θk i = exp(˜ηk,i) /Ei, exp(˜ηk,i,). We then say θ distributes according to the shared logistic normal distribution with partition structure S = ({In}N n=1, {Jk}Kk=1) and normal experts {(µn, En)}Nn=1 and denote it by θ — SLN(µ, E, S). The partitioned LN distribution in Aitchison (1986) can be formulated as a shared LN distribution where N = 1. The LN collection used by Cohen et al. (2008) is the special case where N = K, each Ln = 1, each `k = Nk, and each Jk = {Ik,1}. The covariance among arbitrary θk,i is not defined directly; it is implied by the definition of the normal experts ηn,In,j, for each In,j E Jk. We note that a SLN can be represented as a PLN by relying on the distributivity of the covariance operator, and merging all the partition structure into one (perhaps sparse) covariance matrix. However, if we are interested in keeping a factored structure on the covari</context>
</contexts>
<marker>Aitchison, 1986</marker>
<rawString>J. Aitchison. 1986. The Statistical Analysis of Compositional Data. Chapman and Hall, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Blei</author>
<author>J D Lafferty</author>
</authors>
<title>Correlated topic models.</title>
<date>2006</date>
<booktitle>In Proc. of NIPS.</booktitle>
<contexts>
<context position="2822" citStr="Blei and Lafferty (2006)" startWordPosition="399" endWordPosition="402">listic inference, which is generally intractable. Most commonly variational (Johnson, 2007; Kurihara and Sato, 2006) or sampling techniques are applied (Johnson et al., 2006). Because probabilistic grammars are built out of multinomial distributions, the Dirichlet family (or, more precisely, a collection of Dirichlets) is a natural candidate for probabilistic grammars because of its conjugacy to the multinomial family. Conjugacy implies a clean form for the posterior distribution over grammar probabilities (given the data and the prior), bestowing computational tractability. Following work by Blei and Lafferty (2006) for topic models, Cohen et al. (2008) proposed an alternative to Dirichlet priors for probabilistic grammars, based on the logistic normal (LN) distribution over the probability simplex. Cohen et al. used this prior to softly tie grammar weights through the covariance parameters of the LN. The prior encodes information about which grammar rules’ weights are likely to covary, a more intuitive and expressive representation of knowledge than offered by Dirichlet distributions.1 The contribution of this paper is two-fold. First, from the modeling perspective, we present a generalization of the LN</context>
<context position="11019" citStr="Blei and Lafferty (2006)" startWordPosition="1742" endWordPosition="1745">the distribution over 0 and trees y resembles the traditional EM algorithm very closely (Johnson, 2007). In fact, variational inference in this case takes an action similar to smoothing the counts using the exp-Ψ function during the E-step. Variational inference can be embedded in an empirical Bayes setting, in which we optimize the variational bound with respect to the hyperparameters as well, repeating the process until convergence. 3.1 Logistic Normal Distributions While Dirichlet priors over grammar probabilities make learning algorithms easy, they are limiting. In particular, as noted by Blei and Lafferty (2006), there is no explicit flexible way for the Dirichlet’s parameters to encode beliefs about covariance between the probabilities of two events. To illustrate this point, we describe how a multinomial 0 of dimension d is generated from a Dirichlet distribution with parameters α = hα1,..., αdi: 1. Generate ηj ∼ F(αj,1) independently for j ∈ {1, ..., d}. 2. θj ← ηj/ i ηi. where F(α,1) is a Gamma distribution with shape α and scale 1. Correlation among θi and θj, i =6 j, cannot be modeled directly, only through the normalization in step 2. In contrast, LN distributions (Aitchison, 1986) provide a n</context>
<context position="12768" citStr="Blei and Lafferty (2006)" startWordPosition="2096" endWordPosition="2099">3) ∼ Normal(µ4, E4) ˜η1 = 1 3hη1,1 + η2,1 + η4,1, η1,2 + η2,2 + η4,2i η1,5 + η2,5 ˜η2 = 1 3hη1,3 + η2,3 + η3,1 , η1,4 + η2,4 + η3,2, ˜η3 = 12hη1,7 + η3,5, η1,8 + η3,6, η1,9 + η3,7i + η3,3, η1,6 + η2,6 + η3,4i } combine η θ1 = θ2 = θ3 = N1 (exp˜η1) Ei0=1 exp˜η1,i0 N2 (exp η2) Ei0=1 exp ˜η2,i0 3 (exp ˜η3)EiN0 = 1 exp ˜η3,i0 I softmax Figure 2: An example of a shared logistic normal distribution, illustrating Def. 1. N = 4 experts are used to sample K = 3 multinomials; L1 = 3, L2 = 2, L3 = 2, L4 = 1, `1 = 9, `2 = 6, `3 = 7, `4 = 2, N1 = 2, N2 = 4, and N3 = 3. This figure is best viewed in color. Blei and Lafferty (2006) defined correlated topic models by replacing the Dirichlet in latent Dirichlet allocation models (Blei et al., 2003) with a LN distribution. Cohen et al. (2008) compared Dirichlet and LN distributions for learning DMV using empirical Bayes, finding substantial improvements for English using the latter. In that work, we obtained improvements even without specifying exactly which grammar probabilities covaried. While empirical Bayes learning permits these covariances to be discovered without supervision, we found that by initializing the covariance to encode beliefs about which grammar probabil</context>
</contexts>
<marker>Blei, Lafferty, 2006</marker>
<rawString>D. M. Blei and J. D. Lafferty. 2006. Correlated topic models. In Proc. of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Blei</author>
<author>A Ng</author>
<author>M Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="12885" citStr="Blei et al., 2003" startWordPosition="2114" endWordPosition="2117">4 + η3,2, ˜η3 = 12hη1,7 + η3,5, η1,8 + η3,6, η1,9 + η3,7i + η3,3, η1,6 + η2,6 + η3,4i } combine η θ1 = θ2 = θ3 = N1 (exp˜η1) Ei0=1 exp˜η1,i0 N2 (exp η2) Ei0=1 exp ˜η2,i0 3 (exp ˜η3)EiN0 = 1 exp ˜η3,i0 I softmax Figure 2: An example of a shared logistic normal distribution, illustrating Def. 1. N = 4 experts are used to sample K = 3 multinomials; L1 = 3, L2 = 2, L3 = 2, L4 = 1, `1 = 9, `2 = 6, `3 = 7, `4 = 2, N1 = 2, N2 = 4, and N3 = 3. This figure is best viewed in color. Blei and Lafferty (2006) defined correlated topic models by replacing the Dirichlet in latent Dirichlet allocation models (Blei et al., 2003) with a LN distribution. Cohen et al. (2008) compared Dirichlet and LN distributions for learning DMV using empirical Bayes, finding substantial improvements for English using the latter. In that work, we obtained improvements even without specifying exactly which grammar probabilities covaried. While empirical Bayes learning permits these covariances to be discovered without supervision, we found that by initializing the covariance to encode beliefs about which grammar probabilities should covary, further improvements were possible. Specifically, we grouped the Penn Treebank part-of-speech ta</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>D. M. Blei, A. Ng, and M. Jordan. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Burkett</author>
<author>D Klein</author>
</authors>
<title>Two languages are better than one (for syntactic parsing).</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="28942" citStr="Burkett and Klein, 2008" startWordPosition="4885" endWordPosition="4889"> Attachment accuracy of different models, on test data from the Penn Treebank and the Chinese Treebank of varying levels of difficulty imposed through a length filter. Attach-Right attaches each word to the word on its right and the last word to $. Bold marks best overall accuracy per length bound, and † marks figures that are not significantly worse (binomial sign test, p &lt; 0.05). 4.3 Bilingual Experiments Leveraging information from one language for the task of disambiguating another language has received considerable attention (Dagan, 1991; Smith and Smith, 2004; Snyder and Barzilay, 2008; Burkett and Klein, 2008). Usually such a setting requires a parallel corpus or other annotated data that ties between those two languages.5 Our bilingual experiments use the English and Chinese treebanks, which are not parallel corpora, to train parsers for both languages jointly. Shar5Haghighi et al. (2008) presented a technique to learn bilingual lexicons from two non-parallel monolingual corpora. ing information between those two models is done by softly tying grammar weights in the two hidden grammars. We first merge the models for English and Chinese by taking a union of the multinomial families of each and the </context>
</contexts>
<marker>Burkett, Klein, 2008</marker>
<rawString>D. Burkett and D. Klein. 2008. Two languages are better than one (for syntactic parsing). In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Coarse-to-fine nbest parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="1155" citStr="Charniak and Johnson, 2005" startWordPosition="159" endWordPosition="162">vation events in the probabilistic grammar, providing a new way to encode prior knowledge about an unknown grammar. We describe a variational EM algorithm for learning a probabilistic grammar based on this family of priors. We then experiment with unsupervised dependency grammar induction and show significant improvements using our model for both monolingual learning and bilingual learning with a non-parallel, multilingual corpus. 1 Introduction Probabilistic grammars have become an important tool in natural language processing. They are most commonly used for parsing and linguistic analysis (Charniak and Johnson, 2005; Collins, 2003), but are now commonly seen in applications like machine translation (Wu, 1997) and question answering (Wang et al., 2007). An attractive property of probabilistic grammars is that they permit the use of well-understood parameter estimation methods for learning—both from labeled and unlabeled data. Here we tackle the unsupervised grammar learning problem, specifically for unlexicalized context-free dependency grammars, using an empirical Bayesian approach with a novel family of priors. There has been an increased interest recently in employing Bayesian modeling for probabilisti</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>E. Charniak and M. Johnson. 2005. Coarse-to-fine nbest parsing and maxent discriminative reranking. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S B Cohen</author>
<author>N A Smith</author>
</authors>
<title>Inference for probabilistic grammars with shared logistic normal distributions.</title>
<date>2009</date>
<tech>Technical report,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="24215" citStr="Cohen and Smith (2009)" startWordPosition="4077" endWordPosition="4080">eat optimize B w.r.t. ˜µ`,(t) r , r = 1, ..., N; optimize B w.r.t. ˜σ`,(t) r , r = 1, ..., N; update ˜ζ`,(t) r , r = 1, ..., N; `, (t) updater , r = 1, ..., N; compute counts ˜f`,(t) r , r = 1, ..., N; until convergence of B ; M-step: optimize B w.r.t. µ(t) and E(t); t i t + 1; end return µ(T), E(T) Figure 4: Main details of the variational inference EM algorithm with empirical Bayes estimation of µ and E. B is the bound defined in Fig. 3 (Eq. 3). N is the number of normal experts for the SLN distribution defining the prior. M is the number of training examples. The full algorithm is given in Cohen and Smith (2009). tics rely mainly on the centrality of content words: nouns, verbs, and adjectives. For example, in the English treebank, the most common attachment errors (with the LN prior from Cohen et al., 2008) happen with a noun (25.9%) or a verb (16.9%) parent. In the Chinese treebank, the most common attachment errors happen with noun (36.0%) and verb (21.2%) parents as well. The errors being governed by such attachments are the direct result of nouns and verbs being the most common parents in these data sets. Following this observation, we compare four different settings in our experiments (all SLN </context>
</contexts>
<marker>Cohen, Smith, 2009</marker>
<rawString>S. B. Cohen and N. A. Smith. 2009. Inference for probabilistic grammars with shared logistic normal distributions. Technical report, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S B Cohen</author>
<author>K Gimpel</author>
<author>N A Smith</author>
</authors>
<title>Logistic normal priors for unsupervised probabilistic grammar induction.</title>
<date>2008</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="2860" citStr="Cohen et al. (2008)" startWordPosition="406" endWordPosition="409">table. Most commonly variational (Johnson, 2007; Kurihara and Sato, 2006) or sampling techniques are applied (Johnson et al., 2006). Because probabilistic grammars are built out of multinomial distributions, the Dirichlet family (or, more precisely, a collection of Dirichlets) is a natural candidate for probabilistic grammars because of its conjugacy to the multinomial family. Conjugacy implies a clean form for the posterior distribution over grammar probabilities (given the data and the prior), bestowing computational tractability. Following work by Blei and Lafferty (2006) for topic models, Cohen et al. (2008) proposed an alternative to Dirichlet priors for probabilistic grammars, based on the logistic normal (LN) distribution over the probability simplex. Cohen et al. used this prior to softly tie grammar weights through the covariance parameters of the LN. The prior encodes information about which grammar rules’ weights are likely to covary, a more intuitive and expressive representation of knowledge than offered by Dirichlet distributions.1 The contribution of this paper is two-fold. First, from the modeling perspective, we present a generalization of the LN prior of Cohen et al. (2008), showing</context>
<context position="12929" citStr="Cohen et al. (2008)" startWordPosition="2122" endWordPosition="2125">, η1,9 + η3,7i + η3,3, η1,6 + η2,6 + η3,4i } combine η θ1 = θ2 = θ3 = N1 (exp˜η1) Ei0=1 exp˜η1,i0 N2 (exp η2) Ei0=1 exp ˜η2,i0 3 (exp ˜η3)EiN0 = 1 exp ˜η3,i0 I softmax Figure 2: An example of a shared logistic normal distribution, illustrating Def. 1. N = 4 experts are used to sample K = 3 multinomials; L1 = 3, L2 = 2, L3 = 2, L4 = 1, `1 = 9, `2 = 6, `3 = 7, `4 = 2, N1 = 2, N2 = 4, and N3 = 3. This figure is best viewed in color. Blei and Lafferty (2006) defined correlated topic models by replacing the Dirichlet in latent Dirichlet allocation models (Blei et al., 2003) with a LN distribution. Cohen et al. (2008) compared Dirichlet and LN distributions for learning DMV using empirical Bayes, finding substantial improvements for English using the latter. In that work, we obtained improvements even without specifying exactly which grammar probabilities covaried. While empirical Bayes learning permits these covariances to be discovered without supervision, we found that by initializing the covariance to encode beliefs about which grammar probabilities should covary, further improvements were possible. Specifically, we grouped the Penn Treebank part-of-speech tags into coarse groups based on the treebank </context>
<context position="16956" citStr="Cohen et al. (2008)" startWordPosition="2807" endWordPosition="2810">= 1:`n and In,j n In,j, = 0 for j =� j&apos;. Let Jk for k E 1:K be a collection of (disjoint) subsets of {In,j I n E 1:N, j E 1:`n, IIn,j = Nk}, such that all sets in Jk are of the same size, Nk. Let ηk =|Jk |F-In,j∈Jk ηn,In,j, and θk i = exp(˜ηk,i) /Ei, exp(˜ηk,i,). We then say θ distributes according to the shared logistic normal distribution with partition structure S = ({In}N n=1, {Jk}Kk=1) and normal experts {(µn, En)}Nn=1 and denote it by θ — SLN(µ, E, S). The partitioned LN distribution in Aitchison (1986) can be formulated as a shared LN distribution where N = 1. The LN collection used by Cohen et al. (2008) is the special case where N = K, each Ln = 1, each `k = Nk, and each Jk = {Ik,1}. The covariance among arbitrary θk,i is not defined directly; it is implied by the definition of the normal experts ηn,In,j, for each In,j E Jk. We note that a SLN can be represented as a PLN by relying on the distributivity of the covariance operator, and merging all the partition structure into one (perhaps sparse) covariance matrix. However, if we are interested in keeping a factored structure on the covariance matrices which generate the grammar weights, we cannot represent every SLN as a PLN. It is convenien</context>
<context position="21079" citStr="Cohen et al. (2008)" startWordPosition="3538" endWordPosition="3541">ed at zero (i.e., the variational parameters consist of a single mean ˜µk,i and a single variance ˜σ2k,i for each θk,i). There is an additional variational parameter, ˜ζk per multinomial, which is the result of an additional variational approximation because of the lack of conjugacy of the LN distribution to the multinomial distribution. The distribution q(y) is assumed to be defined by a DMV with unnormalized probabilities ˜ψ. Inference optimizes the bound B given in Fig. 3 (Eq. 3) with respect to the variational parameters. Our variational inference algorithm is derived similarly to that of Cohen et al. (2008). Because we wish to learn the values of µ and E, we embed variational inference as the E step within a variational EM algorithm, shown schematically in Fig. 4. In our experiments, we use this variational EM algorithm on a training set, and then use the normal experts’ means to get a point estimate for θ, the grammar weights. This is called empirical Bayesian estimation. Our approach differs from maximum a posteriori (MAP) estimation, since we re-estimate the parameters of the normal experts. Exact MAP estimation is probably not feasible; a variational algorithm like ours might be applied, tho</context>
<context position="24415" citStr="Cohen et al., 2008" startWordPosition="4111" endWordPosition="4114">ntil convergence of B ; M-step: optimize B w.r.t. µ(t) and E(t); t i t + 1; end return µ(T), E(T) Figure 4: Main details of the variational inference EM algorithm with empirical Bayes estimation of µ and E. B is the bound defined in Fig. 3 (Eq. 3). N is the number of normal experts for the SLN distribution defining the prior. M is the number of training examples. The full algorithm is given in Cohen and Smith (2009). tics rely mainly on the centrality of content words: nouns, verbs, and adjectives. For example, in the English treebank, the most common attachment errors (with the LN prior from Cohen et al., 2008) happen with a noun (25.9%) or a verb (16.9%) parent. In the Chinese treebank, the most common attachment errors happen with noun (36.0%) and verb (21.2%) parents as well. The errors being governed by such attachments are the direct result of nouns and verbs being the most common parents in these data sets. Following this observation, we compare four different settings in our experiments (all SLN settings include one normal expert for each multinomial on its own, equivalent to the regular LN setting from Cohen et al.): • TIEV: We add normal experts that tie all probabilities corresponding to a</context>
<context position="26246" citStr="Cohen et al. (2008)" startWordPosition="4435" endWordPosition="4438"> the above two settings. • TIEA: This is the same as TIEV, only for adjectival parents. Since inference for a model with parameter tying can be computationally intensive, we first run the inference algorithm without parameter tying, and then add parameter tying to the rest of the inference algorithm’s execution until convergence. Initialization is important for the inference algorithm, because the variational bound is a nonconcave function. For the expected values of the normal experts, we use the initializer from Klein and Manning (2004). For the covariance matrices, we follow the setting in Cohen et al. (2008) in our experiments also described in §3.1. For each treebank, we divide the tags into twelve disjoint tag families.4 The covariance matrices for all dependency distributions were initialized with 1 on the diagonal, 0.5 between tags which belong to the same family, and 0 otherwise. This initializer has been shown to be more successful than an identity covariance matrix. 4.2 Monolingual Experiments We begin our experiments with a monolingual setting, where we learn grammars for English and Chinese (separately) using the settings described above. The attachment accuracy for this set of experimen</context>
</contexts>
<marker>Cohen, Gimpel, Smith, 2008</marker>
<rawString>S. B. Cohen, K. Gimpel, and N. A. Smith. 2008. Logistic normal priors for unsupervised probabilistic grammar induction. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-driven statistical models for natural language processing.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<pages>29--589</pages>
<contexts>
<context position="1171" citStr="Collins, 2003" startWordPosition="163" endWordPosition="164">listic grammar, providing a new way to encode prior knowledge about an unknown grammar. We describe a variational EM algorithm for learning a probabilistic grammar based on this family of priors. We then experiment with unsupervised dependency grammar induction and show significant improvements using our model for both monolingual learning and bilingual learning with a non-parallel, multilingual corpus. 1 Introduction Probabilistic grammars have become an important tool in natural language processing. They are most commonly used for parsing and linguistic analysis (Charniak and Johnson, 2005; Collins, 2003), but are now commonly seen in applications like machine translation (Wu, 1997) and question answering (Wang et al., 2007). An attractive property of probabilistic grammars is that they permit the use of well-understood parameter estimation methods for learning—both from labeled and unlabeled data. Here we tackle the unsupervised grammar learning problem, specifically for unlexicalized context-free dependency grammars, using an empirical Bayesian approach with a novel family of priors. There has been an increased interest recently in employing Bayesian modeling for probabilistic grammars in di</context>
</contexts>
<marker>Collins, 2003</marker>
<rawString>M. Collins. 2003. Head-driven statistical models for natural language processing. Computational Linguistics, 29:589–637.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
</authors>
<title>Two languages are more informative than one.</title>
<date>1991</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="28866" citStr="Dagan, 1991" startWordPosition="4875" endWordPosition="4876">SLN, TIEA 52.0 41.3 35.2 Supervised MLE 84.3 66.1 57.6 Table 1: Attachment accuracy of different models, on test data from the Penn Treebank and the Chinese Treebank of varying levels of difficulty imposed through a length filter. Attach-Right attaches each word to the word on its right and the last word to $. Bold marks best overall accuracy per length bound, and † marks figures that are not significantly worse (binomial sign test, p &lt; 0.05). 4.3 Bilingual Experiments Leveraging information from one language for the task of disambiguating another language has received considerable attention (Dagan, 1991; Smith and Smith, 2004; Snyder and Barzilay, 2008; Burkett and Klein, 2008). Usually such a setting requires a parallel corpus or other annotated data that ties between those two languages.5 Our bilingual experiments use the English and Chinese treebanks, which are not parallel corpora, to train parsers for both languages jointly. Shar5Haghighi et al. (2008) presented a technique to learn bilingual lexicons from two non-parallel monolingual corpora. ing information between those two models is done by softly tying grammar weights in the two hidden grammars. We first merge the models for Englis</context>
</contexts>
<marker>Dagan, 1991</marker>
<rawString>I. Dagan. 1991. Two languages are more informative than one. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
</authors>
<title>Transformational priors over grammars.</title>
<date>2002</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="3592" citStr="Eisner (2002)" startWordPosition="526" endWordPosition="527"> over the probability simplex. Cohen et al. used this prior to softly tie grammar weights through the covariance parameters of the LN. The prior encodes information about which grammar rules’ weights are likely to covary, a more intuitive and expressive representation of knowledge than offered by Dirichlet distributions.1 The contribution of this paper is two-fold. First, from the modeling perspective, we present a generalization of the LN prior of Cohen et al. (2008), showing how to extend the use of the LN prior to 1Although the task, underlying model, and weights being tied were different, Eisner (2002) also showed evidence for the efficacy of parameter tying in grammar learning. Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 74–82, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics tie between any grammar weights in a probabilistic grammar (instead of only allowing weights within the same multinomial distribution to covary). Second, from the experimental perspective, we show how such flexibility in parameter tying can help in unsupervised grammar learning in the well-known monolingual setting and in a new b</context>
</contexts>
<marker>Eisner, 2002</marker>
<rawString>J. Eisner. 2002. Transformational priors over grammars. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Finkel</author>
<author>T Grenager</author>
<author>C D Manning</author>
</authors>
<title>The infinite tree.</title>
<date>2007</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="2006" citStr="Finkel et al., 2007" startWordPosition="284" endWordPosition="287">ood parameter estimation methods for learning—both from labeled and unlabeled data. Here we tackle the unsupervised grammar learning problem, specifically for unlexicalized context-free dependency grammars, using an empirical Bayesian approach with a novel family of priors. There has been an increased interest recently in employing Bayesian modeling for probabilistic grammars in different settings, ranging from putting priors over grammar probabilities (Johnson et al., 74 2007) to putting non-parametric priors over derivations (Johnson et al., 2006) to learning the set of states in a grammar (Finkel et al., 2007; Liang et al., 2007). Bayesian methods offer an elegant framework for combining prior knowledge with data. The main challenge in Bayesian grammar learning is efficiently approximating probabilistic inference, which is generally intractable. Most commonly variational (Johnson, 2007; Kurihara and Sato, 2006) or sampling techniques are applied (Johnson et al., 2006). Because probabilistic grammars are built out of multinomial distributions, the Dirichlet family (or, more precisely, a collection of Dirichlets) is a natural candidate for probabilistic grammars because of its conjugacy to the multi</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2007</marker>
<rawString>J. R. Finkel, T. Grenager, and C. D. Manning. 2007. The infinite tree. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goodman</author>
</authors>
<title>Parsing algorithms and metrics.</title>
<date>1996</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="22772" citStr="Goodman, 1996" startWordPosition="3810" endWordPosition="3811"> annotated data), and report final results on §23. For Chinese, we train on §1–270, use §301–1151 for development and report testing results on §271–300.3 To evaluate performance, we report the fraction of words whose predicted parent matches the gold standard corpus. This performance measure is also known as attachment accuracy. We considered two parsing methods after extracting a point estimate for the grammar: the most probable “Viterbi” parse (argmaxy p(y |x, θ)) and the minimum Bayes risk (MBR) parse (argminy Ep(y,|x,θ)[`(y; x, y&apos;)]) with dependency attachment error as the loss function (Goodman, 1996). Performance with MBR parsing is consistently higher than its Viterbi counterpart, so we report only performance with MBR parsing. 4.1 Nouns, Verbs, and Adjectives In this paper, we use a few simple heuristics to decide which partition structure S to use. Our heuris3Unsupervised training for these datasets can be costly, and requires iteratively running a cubic-time inside-outside dynamic programming algorithm, so we follow Klein and Manning (2004) in restricting the training set to sentences of ten or fewer words in length. Short sentences are also less structurally ambiguous and may therefo</context>
</contexts>
<marker>Goodman, 1996</marker>
<rawString>J. Goodman. 1996. Parsing algorithms and metrics. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Haghighi</author>
<author>P Liang</author>
<author>T Berg-Kirkpatrick</author>
<author>D Klein</author>
</authors>
<title>Learning bilingual lexicons from monolingual corpora.</title>
<date>2008</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="29227" citStr="Haghighi et al. (2008)" startWordPosition="4930" endWordPosition="4934">ength bound, and † marks figures that are not significantly worse (binomial sign test, p &lt; 0.05). 4.3 Bilingual Experiments Leveraging information from one language for the task of disambiguating another language has received considerable attention (Dagan, 1991; Smith and Smith, 2004; Snyder and Barzilay, 2008; Burkett and Klein, 2008). Usually such a setting requires a parallel corpus or other annotated data that ties between those two languages.5 Our bilingual experiments use the English and Chinese treebanks, which are not parallel corpora, to train parsers for both languages jointly. Shar5Haghighi et al. (2008) presented a technique to learn bilingual lexicons from two non-parallel monolingual corpora. ing information between those two models is done by softly tying grammar weights in the two hidden grammars. We first merge the models for English and Chinese by taking a union of the multinomial families of each and the corresponding prior parameters. We then add a normal expert that ties between the parts of speech in the respective partition structures for both grammars together. Parts of speech are matched through the single coarse tagset (footnote 4). For example, with TIEV, let V = VEng U VChi b</context>
</contexts>
<marker>Haghighi, Liang, Berg-Kirkpatrick, Klein, 2008</marker>
<rawString>A. Haghighi, P. Liang, T. Berg-Kirkpatrick, and D. Klein. 2008. Learning bilingual lexicons from monolingual corpora. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W P Headden</author>
<author>M Johnson</author>
<author>D McClosky</author>
</authors>
<title>Improving unsupervised dependency parsing with richer contexts and smoothing.</title>
<date>2009</date>
<booktitle>In Proc. of NAACLHLT.</booktitle>
<contexts>
<context position="8525" citStr="Headden et al. (2009)" startWordPosition="1358" endWordPosition="1361"> distributions 0s(· , ·,·) and 0c(· 1 ·, ·). To Nk i=1 K H k=1 derivation y: p(x, y 1 θ) = k ,0ki(X,Y) 0 (1) Nk i=1 K k=1 = exp fk,i(x, y) log 0k,i 75 P(y(i) |xi, 0) = HDE{left,right} θs(stop |xi, D, [yD(i) = ∅]) (2) × HjEYD(i) θs(¬stop |xi, D, firstY(j)) × θc(xj |xi, D) × P(y(j) |xj, 0) Figure 1: The “dependency model with valence” recursive equation. firsty(j) is a predicate defined to be true iff xj is the closest child (on either side) to its parent xi. The probability of the tree p(x, y |0) = P(y(0) |$, 0). follow the general setting of Eq. 1, we index these distributions as 01, ..., 0K. Headden et al. (2009) extended DMV so that the distributions θe condition on the valence as well, with smoothing, and showed significant improvements for short sentences. Our experiments found that these improvements do not hold on longer sentences. Here we experiment only with DMV, but note that our techniques are also applicable to richer probabilistic grammars like that of Headden et al. 2.2 Learning DMV Klein and Manning (2004) learned the DMV probabilities 0 from a corpus of part-of-speech-tagged sentences using the EM algorithm. EM manipulates 0 to locally optimize the likelihood of the observed portion of t</context>
</contexts>
<marker>Headden, Johnson, McClosky, 2009</marker>
<rawString>W. P. Headden, M. Johnson, and D. McClosky. 2009. Improving unsupervised dependency parsing with richer contexts and smoothing. In Proc. of NAACLHLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G E Hinton</author>
</authors>
<title>Products of experts.</title>
<date>1999</date>
<booktitle>In Proc. of ICANN.</booktitle>
<contexts>
<context position="17988" citStr="Hinton, 1999" startWordPosition="2990" endWordPosition="2991"> However, if we are interested in keeping a factored structure on the covariance matrices which generate the grammar weights, we cannot represent every SLN as a PLN. It is convenient to think of each ηi,j as a weight associated with a unique event’s probability, a certain outcome of a certain multinomial in the probabilistic grammar. By letting different ηi,j covary with each other, we loosen the relationships among θk,j and permit the model—at least in principle— to learn patterns from the data. Def. 1 also implies that we multiply several multinomials together in a product-of-experts style (Hinton, 1999), because the exponential of a mixture of normals becomes a product of (unnormalized) probabilities. Our extension to the model in Cohen et al. (2008) follows naturally after we have defined the shared LN distribution. The generative story for this model is as follows: 1. Generate θ — SLN(µ, E, S), where θ is a collection of vectors θk, k = 1, ..., K. 2. Generate x and y from p(x, y θ) (i.e., sample from the probabilistic grammar). 3.3 Inference In this work, the partition structure S is known, the sentences x are observed, the trees y and the grammar weights θ are hidden, and the parameters o</context>
</contexts>
<marker>Hinton, 1999</marker>
<rawString>G. E. Hinton. 1999. Products of experts. In Proc. of ICANN.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
<author>T L Griffiths</author>
<author>S Goldwater</author>
</authors>
<title>Adaptor grammars: A framework for specifying compositional nonparameteric Bayesian models.</title>
<date>2006</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="1942" citStr="Johnson et al., 2006" startWordPosition="271" endWordPosition="274">probabilistic grammars is that they permit the use of well-understood parameter estimation methods for learning—both from labeled and unlabeled data. Here we tackle the unsupervised grammar learning problem, specifically for unlexicalized context-free dependency grammars, using an empirical Bayesian approach with a novel family of priors. There has been an increased interest recently in employing Bayesian modeling for probabilistic grammars in different settings, ranging from putting priors over grammar probabilities (Johnson et al., 74 2007) to putting non-parametric priors over derivations (Johnson et al., 2006) to learning the set of states in a grammar (Finkel et al., 2007; Liang et al., 2007). Bayesian methods offer an elegant framework for combining prior knowledge with data. The main challenge in Bayesian grammar learning is efficiently approximating probabilistic inference, which is generally intractable. Most commonly variational (Johnson, 2007; Kurihara and Sato, 2006) or sampling techniques are applied (Johnson et al., 2006). Because probabilistic grammars are built out of multinomial distributions, the Dirichlet family (or, more precisely, a collection of Dirichlets) is a natural candidate </context>
</contexts>
<marker>Johnson, Griffiths, Goldwater, 2006</marker>
<rawString>M. Johnson, T. L. Griffiths, and S. Goldwater. 2006. Adaptor grammars: A framework for specifying compositional nonparameteric Bayesian models. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
<author>T L Griffiths</author>
<author>S Goldwater</author>
</authors>
<title>Bayesian inference for PCFGs via Markov chain Monte Carlo.</title>
<date>2007</date>
<booktitle>In Proc. of NAACL.</booktitle>
<marker>Johnson, Griffiths, Goldwater, 2007</marker>
<rawString>M. Johnson, T. L. Griffiths, and S. Goldwater. 2007. Bayesian inference for PCFGs via Markov chain Monte Carlo. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
</authors>
<title>Why doesn’t EM find good HMM POS-taggers?</title>
<date>2007</date>
<booktitle>In Proc. EMNLP-CoNLL.</booktitle>
<contexts>
<context position="2288" citStr="Johnson, 2007" startWordPosition="325" endWordPosition="326">n an increased interest recently in employing Bayesian modeling for probabilistic grammars in different settings, ranging from putting priors over grammar probabilities (Johnson et al., 74 2007) to putting non-parametric priors over derivations (Johnson et al., 2006) to learning the set of states in a grammar (Finkel et al., 2007; Liang et al., 2007). Bayesian methods offer an elegant framework for combining prior knowledge with data. The main challenge in Bayesian grammar learning is efficiently approximating probabilistic inference, which is generally intractable. Most commonly variational (Johnson, 2007; Kurihara and Sato, 2006) or sampling techniques are applied (Johnson et al., 2006). Because probabilistic grammars are built out of multinomial distributions, the Dirichlet family (or, more precisely, a collection of Dirichlets) is a natural candidate for probabilistic grammars because of its conjugacy to the multinomial family. Conjugacy implies a clean form for the posterior distribution over grammar probabilities (given the data and the prior), bestowing computational tractability. Following work by Blei and Lafferty (2006) for topic models, Cohen et al. (2008) proposed an alternative to </context>
<context position="10498" citStr="Johnson, 2007" startWordPosition="1666" endWordPosition="1667">ther benefit. 3 Parameter Tying in the Bayesian Setting As stated above, 0 comprises a collection of multinomials that weights the grammar. Taking the Bayesian approach, we wish to place a prior on those multinomials, and the Dirichlet family is a natural candidate for such a prior because of its conjugacy, which makes inference algorithms easier to derive. For example, if we make a “mean-field assumption,” with respect to hidden structure and weights, the variational algorithm for approximately inferring the distribution over 0 and trees y resembles the traditional EM algorithm very closely (Johnson, 2007). In fact, variational inference in this case takes an action similar to smoothing the counts using the exp-Ψ function during the E-step. Variational inference can be embedded in an empirical Bayes setting, in which we optimize the variational bound with respect to the hyperparameters as well, repeating the process until convergence. 3.1 Logistic Normal Distributions While Dirichlet priors over grammar probabilities make learning algorithms easy, they are limiting. In particular, as noted by Blei and Lafferty (2006), there is no explicit flexible way for the Dirichlet’s parameters to encode be</context>
</contexts>
<marker>Johnson, 2007</marker>
<rawString>M. Johnson. 2007. Why doesn’t EM find good HMM POS-taggers? In Proc. EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M I Jordan</author>
<author>Z Ghahramani</author>
<author>T S Jaakola</author>
<author>L K Saul</author>
</authors>
<title>An introduction to variational methods for graphical models.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>37</volume>
<issue>2</issue>
<pages>233</pages>
<contexts>
<context position="18845" citStr="Jordan et al., 1999" startWordPosition="3141" endWordPosition="3144">ry for this model is as follows: 1. Generate θ — SLN(µ, E, S), where θ is a collection of vectors θk, k = 1, ..., K. 2. Generate x and y from p(x, y θ) (i.e., sample from the probabilistic grammar). 3.3 Inference In this work, the partition structure S is known, the sentences x are observed, the trees y and the grammar weights θ are hidden, and the parameters of the shared LN distribution µ and E are learned.2 Our inference algorithm aims to find the posterior over the grammar probabilities θ and the hidden structures (grammar trees y). To do that, we use variational approximation techniques (Jordan et al., 1999), which treat the problem of finding the posterior as an optimization problem aimed to find the best approximation q(θ, y) of the posterior p(θ, y � x, µ, E, S). The posterior q needs to be constrained to be within a family of tractable and manageable distributions, yet rich enough to represent good approximations of the true posterior. “Best approximation” is defined as the KL divergence between q(θ, y) and p(θ, y I x, µ, E, S). Our variational inference algorithm uses a meanfield assumption: q(θ, y) = q(θ)q(y). The distribution q(θ) is assumed to be a LN distribution with 2In future work, we</context>
</contexts>
<marker>Jordan, Ghahramani, Jaakola, Saul, 1999</marker>
<rawString>M. I. Jordan, Z. Ghahramani, T. S. Jaakola, and L. K. Saul. 1999. An introduction to variational methods for graphical models. Machine Learning, 37(2):183– 233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Corpus-based induction of syntactic structure: Models of dependency and constituency.</title>
<date>2004</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="4764" citStr="Klein and Manning (2004)" startWordPosition="703" endWordPosition="706">in the well-known monolingual setting and in a new bilingual setting where grammars for two languages are learned at once (without parallel corpora). Our method is based on a distribution which we call the shared logistic normal distribution, which is a distribution over a collection of multinomials from different probability simplexes. We provide a variational EM algorithm for inference. The rest of this paper is organized as follows. In §2, we give a brief explanation of probabilistic grammars and introduce some notation for the specific type of dependency grammar used in this paper, due to Klein and Manning (2004). In §3, we present our model and a variational inference algorithm for it. In §4, we report on experiments for both monolingual settings and a bilingual setting and discuss them. We discuss future work (§5) and conclude in §6. 2 Probabilistic Grammars and Dependency Grammar Induction A probabilistic grammar defines a probability distribution over grammatical derivations generated through a step-by-step process. HMMs, for example, can be understood as a random walk through a probabilistic finite-state network, with an output symbol sampled at each state. Each “step” of the walk and each symbol</context>
<context position="6582" citStr="Klein and Manning (2004)" startWordPosition="997" endWordPosition="1000">bilistic grammar defines the joint probability of a string x and a grammatical where fk,i is a function that “counts” the number of times the kth distribution’s ith event occurs in the derivation. The θ are a collection of K multinomials (θ1, ..., θK), the kth of which includes Nk events. Note that there may be many derivations y for a given string x—perhaps even infinitely many in some kinds of grammars. 2.1 Dependency Model with Valence HMMs and PCFGs are the best-known probabilistic grammars, but there are many others. In this paper, we use the “dependency model with valence” (DMV), due to Klein and Manning (2004). DMV defines a probabilistic grammar for unlabeled, projective dependency structures. Klein and Manning (2004) achieved their best results with a combination of DMV with a model known as the “constituent-context model” (CCM). We do not experiment with CCM in this paper, because it does not fit directly in a Bayesian setting (it is highly deficient) and because state-of-the-art unsupervised dependency parsing results have been achieved with DMV alone (Smith, 2006). Using the notation above, DMV defines x = (x1,x2, ..., xn) to be a sentence. x0 is a special “wall” symbol, $, on the left of ever</context>
<context position="8939" citStr="Klein and Manning (2004)" startWordPosition="1425" endWordPosition="1428">e closest child (on either side) to its parent xi. The probability of the tree p(x, y |0) = P(y(0) |$, 0). follow the general setting of Eq. 1, we index these distributions as 01, ..., 0K. Headden et al. (2009) extended DMV so that the distributions θe condition on the valence as well, with smoothing, and showed significant improvements for short sentences. Our experiments found that these improvements do not hold on longer sentences. Here we experiment only with DMV, but note that our techniques are also applicable to richer probabilistic grammars like that of Headden et al. 2.2 Learning DMV Klein and Manning (2004) learned the DMV probabilities 0 from a corpus of part-of-speech-tagged sentences using the EM algorithm. EM manipulates 0 to locally optimize the likelihood of the observed portion of the data (here, x), marginalizing out the hidden portions (here, y). The likelihood surface is not globally concave, so EM only locally optimizes the surface. Klein and Manning’s initialization, though reasonable and language-independent, was an important factor in performance. Various alternatives to EM were explored by Smith (2006), achieving substantially more accurate parsing models by altering the objective</context>
<context position="23225" citStr="Klein and Manning (2004)" startWordPosition="3878" endWordPosition="3882">” parse (argmaxy p(y |x, θ)) and the minimum Bayes risk (MBR) parse (argminy Ep(y,|x,θ)[`(y; x, y&apos;)]) with dependency attachment error as the loss function (Goodman, 1996). Performance with MBR parsing is consistently higher than its Viterbi counterpart, so we report only performance with MBR parsing. 4.1 Nouns, Verbs, and Adjectives In this paper, we use a few simple heuristics to decide which partition structure S to use. Our heuris3Unsupervised training for these datasets can be costly, and requires iteratively running a cubic-time inside-outside dynamic programming algorithm, so we follow Klein and Manning (2004) in restricting the training set to sentences of ten or fewer words in length. Short sentences are also less structurally ambiguous and may therefore be easier to learn from. 79 Input: initial parameters µ(°), E(°), partition structure S, observed data x, number of iterations T Output: learned parameters µ, E t i 1 ; while t &lt; T do E-step (for B = 1, ..., M) do: repeat optimize B w.r.t. ˜µ`,(t) r , r = 1, ..., N; optimize B w.r.t. ˜σ`,(t) r , r = 1, ..., N; update ˜ζ`,(t) r , r = 1, ..., N; `, (t) updater , r = 1, ..., N; compute counts ˜f`,(t) r , r = 1, ..., N; until convergence of B ; M-ste</context>
<context position="26171" citStr="Klein and Manning (2004)" startWordPosition="4422" endWordPosition="4425">rtitions). This is equivalent to taking the union of the partition structures of the above two settings. • TIEA: This is the same as TIEV, only for adjectival parents. Since inference for a model with parameter tying can be computationally intensive, we first run the inference algorithm without parameter tying, and then add parameter tying to the rest of the inference algorithm’s execution until convergence. Initialization is important for the inference algorithm, because the variational bound is a nonconcave function. For the expected values of the normal experts, we use the initializer from Klein and Manning (2004). For the covariance matrices, we follow the setting in Cohen et al. (2008) in our experiments also described in §3.1. For each treebank, we divide the tags into twelve disjoint tag families.4 The covariance matrices for all dependency distributions were initialized with 1 on the diagonal, 0.5 between tags which belong to the same family, and 0 otherwise. This initializer has been shown to be more successful than an identity covariance matrix. 4.2 Monolingual Experiments We begin our experiments with a monolingual setting, where we learn grammars for English and Chinese (separately) using the </context>
</contexts>
<marker>Klein, Manning, 2004</marker>
<rawString>D. Klein and C. D. Manning. 2004. Corpus-based induction of syntactic structure: Models of dependency and constituency. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kurihara</author>
<author>T Sato</author>
</authors>
<title>Variational Bayesian grammar induction for natural language.</title>
<date>2006</date>
<booktitle>In Proc. of ICGI.</booktitle>
<contexts>
<context position="2314" citStr="Kurihara and Sato, 2006" startWordPosition="327" endWordPosition="330">interest recently in employing Bayesian modeling for probabilistic grammars in different settings, ranging from putting priors over grammar probabilities (Johnson et al., 74 2007) to putting non-parametric priors over derivations (Johnson et al., 2006) to learning the set of states in a grammar (Finkel et al., 2007; Liang et al., 2007). Bayesian methods offer an elegant framework for combining prior knowledge with data. The main challenge in Bayesian grammar learning is efficiently approximating probabilistic inference, which is generally intractable. Most commonly variational (Johnson, 2007; Kurihara and Sato, 2006) or sampling techniques are applied (Johnson et al., 2006). Because probabilistic grammars are built out of multinomial distributions, the Dirichlet family (or, more precisely, a collection of Dirichlets) is a natural candidate for probabilistic grammars because of its conjugacy to the multinomial family. Conjugacy implies a clean form for the posterior distribution over grammar probabilities (given the data and the prior), bestowing computational tractability. Following work by Blei and Lafferty (2006) for topic models, Cohen et al. (2008) proposed an alternative to Dirichlet priors for proba</context>
</contexts>
<marker>Kurihara, Sato, 2006</marker>
<rawString>K. Kurihara and T. Sato. 2006. Variational Bayesian grammar induction for natural language. In Proc. of ICGI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>S Petrov</author>
<author>M Jordan</author>
<author>D Klein</author>
</authors>
<title>The infinite PCFG using hierarchical Dirichlet processes.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="2027" citStr="Liang et al., 2007" startWordPosition="288" endWordPosition="291">ion methods for learning—both from labeled and unlabeled data. Here we tackle the unsupervised grammar learning problem, specifically for unlexicalized context-free dependency grammars, using an empirical Bayesian approach with a novel family of priors. There has been an increased interest recently in employing Bayesian modeling for probabilistic grammars in different settings, ranging from putting priors over grammar probabilities (Johnson et al., 74 2007) to putting non-parametric priors over derivations (Johnson et al., 2006) to learning the set of states in a grammar (Finkel et al., 2007; Liang et al., 2007). Bayesian methods offer an elegant framework for combining prior knowledge with data. The main challenge in Bayesian grammar learning is efficiently approximating probabilistic inference, which is generally intractable. Most commonly variational (Johnson, 2007; Kurihara and Sato, 2006) or sampling techniques are applied (Johnson et al., 2006). Because probabilistic grammars are built out of multinomial distributions, the Dirichlet family (or, more precisely, a collection of Dirichlets) is a natural candidate for probabilistic grammars because of its conjugacy to the multinomial family. Conjug</context>
</contexts>
<marker>Liang, Petrov, Jordan, Klein, 2007</marker>
<rawString>P. Liang, S. Petrov, M. Jordan, and D. Klein. 2007. The infinite PCFG using hierarchical Dirichlet processes. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="21876" citStr="Marcus et al., 1993" startWordPosition="3672" endWordPosition="3675">ts, we use this variational EM algorithm on a training set, and then use the normal experts’ means to get a point estimate for θ, the grammar weights. This is called empirical Bayesian estimation. Our approach differs from maximum a posteriori (MAP) estimation, since we re-estimate the parameters of the normal experts. Exact MAP estimation is probably not feasible; a variational algorithm like ours might be applied, though better performance is expected from adjusting the SLN to fit the data. 4 Experiments Our experiments involve data from two treebanks: the Wall Street Journal Penn treebank (Marcus et al., 1993) and the Chinese treebank (Xue et al., 2004). In both cases, following standard practice, sentences were stripped of words and punctuation, leaving part-of-speech tags for the unsupervised induction of dependency structure. For English, we train on §2–21, tune on §22 (without using annotated data), and report final results on §23. For Chinese, we train on §1–270, use §301–1151 for development and report testing results on §271–300.3 To evaluate performance, we report the fraction of words whose predicted parent matches the gold standard corpus. This performance measure is also known as attachm</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn treebank. Computational Linguistics, 19:313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Smith</author>
<author>N A Smith</author>
</authors>
<title>Bilingual parsing with factored estimation: Using English to parse Korean.</title>
<date>2004</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>49--56</pages>
<contexts>
<context position="28889" citStr="Smith and Smith, 2004" startWordPosition="4877" endWordPosition="4880">0 41.3 35.2 Supervised MLE 84.3 66.1 57.6 Table 1: Attachment accuracy of different models, on test data from the Penn Treebank and the Chinese Treebank of varying levels of difficulty imposed through a length filter. Attach-Right attaches each word to the word on its right and the last word to $. Bold marks best overall accuracy per length bound, and † marks figures that are not significantly worse (binomial sign test, p &lt; 0.05). 4.3 Bilingual Experiments Leveraging information from one language for the task of disambiguating another language has received considerable attention (Dagan, 1991; Smith and Smith, 2004; Snyder and Barzilay, 2008; Burkett and Klein, 2008). Usually such a setting requires a parallel corpus or other annotated data that ties between those two languages.5 Our bilingual experiments use the English and Chinese treebanks, which are not parallel corpora, to train parsers for both languages jointly. Shar5Haghighi et al. (2008) presented a technique to learn bilingual lexicons from two non-parallel monolingual corpora. ing information between those two models is done by softly tying grammar weights in the two hidden grammars. We first merge the models for English and Chinese by taking</context>
</contexts>
<marker>Smith, Smith, 2004</marker>
<rawString>D. A. Smith and N. A. Smith. 2004. Bilingual parsing with factored estimation: Using English to parse Korean. In Proc. of EMNLP, pages 49–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N A Smith</author>
</authors>
<title>Novel Estimation Methods for Unsupervised Discovery of Latent Structure in Natural Language Text.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>Johns Hopkins University.</institution>
<contexts>
<context position="7050" citStr="Smith, 2006" startWordPosition="1073" endWordPosition="1074">babilistic grammars, but there are many others. In this paper, we use the “dependency model with valence” (DMV), due to Klein and Manning (2004). DMV defines a probabilistic grammar for unlabeled, projective dependency structures. Klein and Manning (2004) achieved their best results with a combination of DMV with a model known as the “constituent-context model” (CCM). We do not experiment with CCM in this paper, because it does not fit directly in a Bayesian setting (it is highly deficient) and because state-of-the-art unsupervised dependency parsing results have been achieved with DMV alone (Smith, 2006). Using the notation above, DMV defines x = (x1,x2, ..., xn) to be a sentence. x0 is a special “wall” symbol, $, on the left of every sentence. A tree y is defined by a pair of functions yleft and yright (both 10, 1, 2,..., n} __+ 2{1,2,...,n}) that map each word to its sets of left and right dependents, respectively. Here, the graph is constrained to be a projective tree rooted at x0 = $: each word except $ has a single parent, and there are no cycles or crossing dependencies. yleft(0) is taken to be empty, and yright(0) contains the sentence’s single head. Let y(i) denote the subtree rooted </context>
<context position="9459" citStr="Smith (2006)" startWordPosition="1506" endWordPosition="1507">probabilistic grammars like that of Headden et al. 2.2 Learning DMV Klein and Manning (2004) learned the DMV probabilities 0 from a corpus of part-of-speech-tagged sentences using the EM algorithm. EM manipulates 0 to locally optimize the likelihood of the observed portion of the data (here, x), marginalizing out the hidden portions (here, y). The likelihood surface is not globally concave, so EM only locally optimizes the surface. Klein and Manning’s initialization, though reasonable and language-independent, was an important factor in performance. Various alternatives to EM were explored by Smith (2006), achieving substantially more accurate parsing models by altering the objective function. Smith’s methods did require substantial hyperparameter tuning, and the best results were obtained using small annotated development sets to choose hyperparameters. In this paper, we consider only fully unsupervised methods, though we the Bayesian ideas explored here might be merged with the biasing approaches of Smith (2006) for further benefit. 3 Parameter Tying in the Bayesian Setting As stated above, 0 comprises a collection of multinomials that weights the grammar. Taking the Bayesian approach, we wi</context>
</contexts>
<marker>Smith, 2006</marker>
<rawString>N. A. Smith. 2006. Novel Estimation Methods for Unsupervised Discovery of Latent Structure in Natural Language Text. Ph.D. thesis, Johns Hopkins University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Snyder</author>
<author>R Barzilay</author>
</authors>
<title>Unsupervised multilingual learning for morphological segmentation.</title>
<date>2008</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="28916" citStr="Snyder and Barzilay, 2008" startWordPosition="4881" endWordPosition="4884">MLE 84.3 66.1 57.6 Table 1: Attachment accuracy of different models, on test data from the Penn Treebank and the Chinese Treebank of varying levels of difficulty imposed through a length filter. Attach-Right attaches each word to the word on its right and the last word to $. Bold marks best overall accuracy per length bound, and † marks figures that are not significantly worse (binomial sign test, p &lt; 0.05). 4.3 Bilingual Experiments Leveraging information from one language for the task of disambiguating another language has received considerable attention (Dagan, 1991; Smith and Smith, 2004; Snyder and Barzilay, 2008; Burkett and Klein, 2008). Usually such a setting requires a parallel corpus or other annotated data that ties between those two languages.5 Our bilingual experiments use the English and Chinese treebanks, which are not parallel corpora, to train parsers for both languages jointly. Shar5Haghighi et al. (2008) presented a technique to learn bilingual lexicons from two non-parallel monolingual corpora. ing information between those two models is done by softly tying grammar weights in the two hidden grammars. We first merge the models for English and Chinese by taking a union of the multinomial</context>
</contexts>
<marker>Snyder, Barzilay, 2008</marker>
<rawString>B. Snyder and R. Barzilay. 2008. Unsupervised multilingual learning for morphological segmentation. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y W Teh</author>
</authors>
<title>A hierarchical Bayesian language model based on Pitman-Yor processes.</title>
<date>2006</date>
<booktitle>In Proc. of COLING-ACL.</booktitle>
<contexts>
<context position="30620" citStr="Teh, 2006" startWordPosition="5169" endWordPosition="5170">of TIEV, TIEN, TIEV&amp;N, and TIEA. After running the inference algorithm which learns the two models jointly, we use unseen data to test each learned model separately. Table 1 includes the results for these experiments. The performance on English improved significantly in the bilingual setting, achieving highest performance with TIEV&amp;N. Performance with Chinese is also the highest in the bilingual setting, with TIEA and TIEV&amp;N. 5 Future Work In future work we plan to lexicalize the model, including a Bayesian grammar prior that accounts for the syntactic patterns of words. Nonparametric models (Teh, 2006) may be appropriate. We also believe that Bayesian discovery of cross-linguistic patterns is an exciting topic worthy of further exploration. 6 Conclusion We described a Bayesian model that allows soft parameter tying among any weights in a probabilistic grammar. We used this model to improve unsupervised parsing accuracy on two different languages, English and Chinese, achieving state-of-the-art results. We also showed how our model can be effectively used to simultaneously learn grammars in two languages from non-parallel multilingual data. Acknowledgments This research was supported by NSF </context>
</contexts>
<marker>Teh, 2006</marker>
<rawString>Y. W. Teh. 2006. A hierarchical Bayesian language model based on Pitman-Yor processes. In Proc. of COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Wang</author>
<author>N A Smith</author>
<author>T Mitamura</author>
</authors>
<title>What is the Jeopardy model? a quasi-synchronous grammar for question answering.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="1293" citStr="Wang et al., 2007" startWordPosition="182" endWordPosition="185">lgorithm for learning a probabilistic grammar based on this family of priors. We then experiment with unsupervised dependency grammar induction and show significant improvements using our model for both monolingual learning and bilingual learning with a non-parallel, multilingual corpus. 1 Introduction Probabilistic grammars have become an important tool in natural language processing. They are most commonly used for parsing and linguistic analysis (Charniak and Johnson, 2005; Collins, 2003), but are now commonly seen in applications like machine translation (Wu, 1997) and question answering (Wang et al., 2007). An attractive property of probabilistic grammars is that they permit the use of well-understood parameter estimation methods for learning—both from labeled and unlabeled data. Here we tackle the unsupervised grammar learning problem, specifically for unlexicalized context-free dependency grammars, using an empirical Bayesian approach with a novel family of priors. There has been an increased interest recently in employing Bayesian modeling for probabilistic grammars in different settings, ranging from putting priors over grammar probabilities (Johnson et al., 74 2007) to putting non-parametr</context>
</contexts>
<marker>Wang, Smith, Mitamura, 2007</marker>
<rawString>M. Wang, N. A. Smith, and T. Mitamura. 2007. What is the Jeopardy model? a quasi-synchronous grammar for question answering. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Comp. Ling.,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="1250" citStr="Wu, 1997" startWordPosition="176" endWordPosition="177">ar. We describe a variational EM algorithm for learning a probabilistic grammar based on this family of priors. We then experiment with unsupervised dependency grammar induction and show significant improvements using our model for both monolingual learning and bilingual learning with a non-parallel, multilingual corpus. 1 Introduction Probabilistic grammars have become an important tool in natural language processing. They are most commonly used for parsing and linguistic analysis (Charniak and Johnson, 2005; Collins, 2003), but are now commonly seen in applications like machine translation (Wu, 1997) and question answering (Wang et al., 2007). An attractive property of probabilistic grammars is that they permit the use of well-understood parameter estimation methods for learning—both from labeled and unlabeled data. Here we tackle the unsupervised grammar learning problem, specifically for unlexicalized context-free dependency grammars, using an empirical Bayesian approach with a novel family of priors. There has been an increased interest recently in employing Bayesian modeling for probabilistic grammars in different settings, ranging from putting priors over grammar probabilities (Johns</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>D. Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Comp. Ling., 23(3):377–404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Xue</author>
<author>F Xia</author>
<author>F-D Chiou</author>
<author>M Palmer</author>
</authors>
<title>The Penn Chinese Treebank: Phrase structure annotation of a large corpus.</title>
<date>2004</date>
<journal>Natural Language Engineering,</journal>
<volume>10</volume>
<issue>4</issue>
<contexts>
<context position="21920" citStr="Xue et al., 2004" startWordPosition="3680" endWordPosition="3683">raining set, and then use the normal experts’ means to get a point estimate for θ, the grammar weights. This is called empirical Bayesian estimation. Our approach differs from maximum a posteriori (MAP) estimation, since we re-estimate the parameters of the normal experts. Exact MAP estimation is probably not feasible; a variational algorithm like ours might be applied, though better performance is expected from adjusting the SLN to fit the data. 4 Experiments Our experiments involve data from two treebanks: the Wall Street Journal Penn treebank (Marcus et al., 1993) and the Chinese treebank (Xue et al., 2004). In both cases, following standard practice, sentences were stripped of words and punctuation, leaving part-of-speech tags for the unsupervised induction of dependency structure. For English, we train on §2–21, tune on §22 (without using annotated data), and report final results on §23. For Chinese, we train on §1–270, use §301–1151 for development and report testing results on §271–300.3 To evaluate performance, we report the fraction of words whose predicted parent matches the gold standard corpus. This performance measure is also known as attachment accuracy. We considered two parsing meth</context>
</contexts>
<marker>Xue, Xia, Chiou, Palmer, 2004</marker>
<rawString>N. Xue, F. Xia, F.-D. Chiou, and M. Palmer. 2004. The Penn Chinese Treebank: Phrase structure annotation of a large corpus. Natural Language Engineering, 10(4):1–30.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>