<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008100">
<title confidence="0.963378">
Trait-Based Hypothesis Selection For Machine Translation
</title>
<author confidence="0.773812">
Jacob Devlin and Spyros Matsoukas
</author>
<affiliation confidence="0.620939">
Raytheon BBN Technologies, 10 Moulton St, Cambridge, MA 02138, USA
</affiliation>
<email confidence="0.997629">
{jdevlin,smatsouk}@bbn.com
</email>
<sectionHeader confidence="0.99664" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.988652764705882">
In the area of machine translation (MT) sys-
tem combination, previous work on generat-
ing input hypotheses has focused on varying a
core aspect of the MT system, such as the de-
coding algorithm or alignment algorithm. In
this paper, we propose a new method for gen-
erating diverse hypotheses from a single MT
system using traits. These traits are simple
properties of the MT output such as “aver-
age output length” and “average rule length.”
Our method is designed to select hypotheses
which vary in trait value but do not signif-
icantly degrade in BLEU score. These hy-
potheses can be combined using standard sys-
tem combination techniques to produce a 1.2-
1.5 BLEU gain on the Arabic-English NIST
MT06/MT08 translation task.
</bodyText>
<sectionHeader confidence="0.998737" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999752428571428">
In Machine Translation (MT), the output from mul-
tiple decoding systems can be used to create a new
output which is better than any single input system,
using a procedure known as system combination.
Normally, the input systems are generated by
varying some important aspect of the MT system,
such as the alignment algorithm (Xu and Rosti,
</bodyText>
<footnote confidence="0.9786005">
This work was supported by DARPA/I2O Contract No.
HR0011-12-C-0014 under the BOLT program (Approved for
Public Release, Distribution Unlimited). The views, opinions,
and/or findings contained in this article are those of the au-
thor and should not be interpreted as representing the official
views or policies, either expressed or implied, of the Defense
Advanced Research Projects Agency or the Department of De-
fense.
</footnote>
<bodyText confidence="0.999541575757576">
2010) or tokenization algorithm (de Gispert et al.,
2009). Unfortunately, creating novel algorithms to
perform some important aspect of MT decoding is
obviously quite challenging. Thus, it is difficult to
increase the number of input systems in a meaning-
ful way.
In this paper, we show it is possible to create
diverse input hypotheses for combination without
making any algorithmic changes. Instead, we use
traits, which are very simple attributes of the MT
output, such as “output length” and “average rule
length.” Our basic procedure is to intelligently se-
lect hypotheses from our decoding forest which vary
in trait value, but have minimal BLEU degradation
compared to our baseline. We then combine these
to produce a substantial gain. Note that all of the
hypotheses are generated from a single decode of a
single input system.
Additionally, our method is completely compati-
ble with multi-system combination, since our proce-
dure can be applied to each input system, and then
these systems can be combined as normal.
Methods for automatically creating diverse hy-
potheses from a single system have been explored
in speech recognition (Siohan et al., 2005), but we
know of no analogous work applied to machine
translation. Our procedure does share some surface
similarities with techniques such as variational de-
coding (VD) (Li et al., 2009), but the goal in those
techniques is to find output which is consistent with
the entire forest, rather than to select hypotheses
with particular attributes. In fact, VD can be applied
in conjunction by running VD on the rescored forest
</bodyText>
<page confidence="0.892969666666667">
528
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 528–532,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</page>
<bodyText confidence="0.8657">
for each trait condition.1
</bodyText>
<sectionHeader confidence="0.532065" genericHeader="method">
2 Description of MT System
</sectionHeader>
<bodyText confidence="0.999552142857143">
Our machine translation system is a string-to-
dependency hierarchical decoder based on (Shen et
al., 2008) and (Chiang, 2007). Bottom-up chart
parsing is performed to produce a shared forest of
derivations. The decoder uses a log-linear transla-
tion model, so the score of derivation d is defined
as:
</bodyText>
<equation confidence="0.9998845">
Sd(w) _ M 1] wi Fri (1)
i=1 rER(d)
</equation>
<bodyText confidence="0.999967928571429">
where R(d) is the set of translation rules that make
up derivation d, m is the number of features, Fri
is the score of the ith feature in rule r, and wi is
the weight of feature i. This weight vector is opti-
mized discriminatively to maximize BLEU score on
a tuning set, using the Expected-BLEU optimization
procedure (Rosti et al., 2010).
Our decoder uses all of the standard statistical MT
features, such as the language model, rule probabil-
ities, and lexical probabilities. Additionally, we use
50,000 sparse, binary-valued features such as “Is the
bi-gram ‘united states’ present in the output?”, based
on (Chiang et al., 2009). We use a 3-gram LM for
decoding and a 5-gram LM for rescoring.
</bodyText>
<sectionHeader confidence="0.986349" genericHeader="method">
3 Trait Features
</sectionHeader>
<bodyText confidence="0.99688">
An MT trait represents a high-level property of the
MT output.
The traits used in this paper are:
</bodyText>
<listItem confidence="0.95040175">
• Null Source Words – The percentage of source
content words which align to null, i.e., are not
translated.
• Source Reorder – The percentage of source
terminals/non-terminals which cross alignment
links inside their decoding rule.
• Ngram Frequency – The percentage of target 3-
grams which are seen more than 10 times in the
monolingual training.
• Rule Frequency – The percentage of rules
which are seen more than 3 times in the par-
allel training.
1We do not use VD here because we have not found it to be
beneficial to our system.
• Rule Length – The average number of target
words per rule.
• Output Length – The ratio of the number of tar-
get words in the MT output divided by the num-
ber of source words in the input.
• High Lex Prob – The percentage of source
</listItem>
<bodyText confidence="0.988039315789474">
words which have a lexical translation proba-
bility greater than 0.1.
Each trait can be represented as the ratio of two lin-
ear decoding features. For example, for the Output
Length trait, the “numerator” feature is the number
of target words in the hypothesis, while the “denom-
inator” feature is the number of source words in the
input sentence. We can sum these feature scores
over a test set, and the resulting quotient is the Out-
put Length for that set.
Intuitively, each trait is associated with a par-
ticular tradeoff, such as fluency/adequacy or preci-
sion/recall. For example, when MT performance is
maximized, shorter output tends to have higher pre-
cision but lower recall than longer output. For the
Ngram Frequency trait, a greater percentage of high-
frequency n-grams tends to result in more fluent but
less adequate output. Similar intuitive justifications
should be evident for the remaining traits.
</bodyText>
<sectionHeader confidence="0.954971" genericHeader="method">
4 Hypothesis Generation
</sectionHeader>
<bodyText confidence="0.9999865625">
The main goal of this work is to generate additional
hypotheses which vary in trait values, while mini-
mizing degradation to the BLEU score. So, imagine
that we have some baseline MT output. Then, we
want to generate a second set of hypotheses which
have maximal BLEU score, subject to the constraint
that the output must be 5% shorter.2
The question then becomes how to figure out
which 5% of words should be removed. Rather than
attempting to do this with a new algorithm, we sim-
ply let our existing MT models do it for us, using
our standard optimization procedure. This is the es-
sential purpose of the trait features – using the Out-
put Length feature, the optimizer has a “knob” with
which it can control the trait value independently
of everything else.3 Thus, the new hypotheses that
</bodyText>
<footnote confidence="0.97528625">
2Note that the trait value is always aggregated over the entire
set, and not computed sentence-by-sentence.
3A feature representing the number of words already exists
in our baseline system, but no such feature exists for the other 6
</footnote>
<page confidence="0.996111">
529
</page>
<bodyText confidence="0.999057">
we select are “optimal” in terms of our existing MT
model probabilities, but have trait values which vary
from the baseline in a precise way.
</bodyText>
<subsectionHeader confidence="0.993206">
4.1 Optimization Function
</subsectionHeader>
<bodyText confidence="0.999987625">
Our normal optimization procedure uses n-
best-based Expected-BLEU tuning (Rosti et al.,
2010), which is a differentiable approximation of
Maximum-BLEU tuning. To “target” a particular
trait value, we add a second term equal to the
squared error between the current trait value and
the target trait value. Our modified optimization
function which we seek to maximize is then:
</bodyText>
<equation confidence="0.985124">
2
Obj(w) = ExpBLEU(w) − α (D(w) − T-y)
</equation>
<bodyText confidence="0.999982266666667">
where w is the MT feature weight vector, α is the
weight of the trait term, -y is the baseline value of
the trait, and T is the “target” trait multiplier, N(V)
is the expected-value of the numerator feature, and
D(w) is the expected value of the denominator fea-
ture.
To give an example, imagine that for our baseline
tune set the Output Length ratio is 1.2, and we want
to create a hypothesis set with 5% fewer words. In
that case, we would set -y = 1.2 and T = 0.95, so the
target trait value is 1.14. We fix the free parameter
α to 10, which forces the optimized trait value to be
very close to the target.4
The trait-value functions N(w) and D(w) are
computed as standard expected value functions, e.g.:
</bodyText>
<equation confidence="0.979136">
N(w) = � � pij(w)Nij
i j
</equation>
<bodyText confidence="0.999733">
where pij(w) is the posterior probability of the jth
hypothesis of sentence i, and Nij is the value of the
numerator feature for hypothesis ij.5
</bodyText>
<subsectionHeader confidence="0.997903">
4.2 Meta-Optimization
</subsectionHeader>
<bodyText confidence="0.9347718">
It is somewhat problematic to use a fixed multiplier
T on all of the traits, since on some traits it may
cause a larger degradation than others. So, we take
the reverse approach – for some targeted BLEU loss
traits.
</bodyText>
<footnote confidence="0.92699475">
4Note that the ExpBLEU(w) is raw BLEU not BLEU per-
centage, i.e., it’s 0.4528 not 45.28
5pzj(w) is computed the same way as in ExpBLEU(w).
See (Rosti et al., 2010) for details.
</footnote>
<bodyText confidence="0.999942777777778">
Q, we find the maximum (or minimum) value of T
which causes a loss no greater than Q, as computed
on a held-out portion of the tune set.6 Here, we find
the maximum and minimum trait value for Q = 0.5
and Q = 2.0, resulting in 4 sets of weights per trait.
We can find the optimal T for each Q by perform-
ing a binary search on T, where we run our optimiza-
tion procedure and then compute the BLEU loss at
each iteration.
</bodyText>
<subsectionHeader confidence="0.97782">
4.3 Forest-Based Optimization
</subsectionHeader>
<bodyText confidence="0.999991653846154">
Since we have 7 traits, and we generate 4 sets of
weights per trait, we have 28 “systems” to combine.
Obviously, running 28 full decodes on each new test
sentence is highly undesirable.
We resolve this issue by using our baseline deriva-
tion forest for both optimization and hypothesis gen-
eration. We perform a single round of decoding to
generate a forest, and then perform iterative n-best
optimization by rescoring the forest rather than re-
decoding from scratch. 7 We constrain the 50,000
sparse feature weights to be fixed at their baseline
values, to prevent over-fitting.
Once the weight sets are generated, the hypothe-
ses for each trait condition can be generated by
rescoring the forest inside of the decoder. Therefore,
all 28 trait hypotheses can be generated for almost
no cost over a single decode.
It should be noted we have found it beneficial to
relax our MT pruning parameters in order to cre-
ate a larger forest. This results in decoding which
is roughly 2x-3x as slow as the baseline, and re-
quires storing the larger forest in memory. However,
we have found that the procedure still works well
even with the standard pruning parameters. Addi-
tionally, we are investigating methods for diversify-
ing the forest with less of a slowdown to decoding.
</bodyText>
<sectionHeader confidence="0.985163" genericHeader="method">
5 Combination
</sectionHeader>
<bodyText confidence="0.999554">
Once the different trait hypotheses have been gen-
erated, system combination can be performed using
any method.
Here, we use a confusion network decoder based
on (Rosti et al., 2010). The basic procedure is to
</bodyText>
<footnote confidence="0.99857425">
6For example if the held-out baseline BLEU is 40.0 and ,Q =
0.5, the BLEU after trait optimization can be no less than 39.5.
7Forest-based optimization such as (Pauls et al., 2009) could
be used instead.
</footnote>
<page confidence="0.995165">
530
</page>
<bodyText confidence="0.999853">
select one hypothesis as the “skeleton” and then in-
crementally align the remaining hypotheses to create
a confusion network. The confusion network is de-
coded using an arc-level confidence score for each
input system and a language model, the weights for
which are estimated discriminatively to maximize
BLEU.
</bodyText>
<sectionHeader confidence="0.999769" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.986432533333333">
We present MT results in Table 1. Our experimen-
tal setup is compatible with the NIST MT08 con-
strained track. We trained our translation model on
35 million words of parallel data and our language
model on 3.8 billion words of monolingual data. We
use a portion of MT02-05 for tuning the MT baseline
and the trait systems, and another portion of MT02-
05 for tuning system combination.
We present results on Arabic-English MT06-
newswire and MT08-eval. The systems were tuned
and evaluated using IBM-BLEU. Our baseline sys-
tem is 1.5 BLEU better than the best result from the
NIST M08 evaluation.
For the Trait Feats condition, we simply added the
numerator and denominator features for all 7 traits to
the baseline system and re-optimized.8 Somewhat
surprisingly, this produces an 0.5-0.7 BLEU gain on
its own. In this condition, although we do not target
any particular trait values, the optimizer will natu-
rally fine-tune the trait values to whatever is optimal
for BLEU score. For example, the MT08 baseline
value of Source Reorder is 0.307, while for the Trait
Feats it is 0.330, so the system determined it is “op-
timal” to have 7.5% (0.330/0.307) more re-ordering
than the baseline.
For the Trait Comb condition, we generated 28
trait hypothesis sets using the decoding forest from
the Trait Feats condition. We combined these with
the Trait Feats output using consensus network de-
coding. This produces an additional 0.8 BLEU gain,
resulting in a 1.2-1.5 BLEU gain over the baseline.
We also present another condition, n-best Comb,
where we perform confusion network combination
on the 28-best hypotheses from Trait Feats. This
represents the simplest and most trivial method of
hypothesis selection. We observe no gain in BLEU
on this condition. Other simple methods of hy-
8Including the 50k sparse features.
potheses selection, such as optimizing systems to be
“different” from one another (i.e., have high inter-
system TER), also produced no gain over the single
system. We include these results simply to demon-
strate that it is not trivial to select hypotheses from a
single system which produce a significant improve-
ment in from system combination.
</bodyText>
<table confidence="0.9996705">
MT06 nw MT08 eval
BLEU Len BLEU Len
Baseline 55.11 99.1% 46.75 96.1%
Trait Feats 55.79* 99.3% 47.23* 96.0%
+n-best Comb 55.65 99.3% 47.24 96.2%
+Trait Comb 56.65** 99.3% 48.00** 96.2%
</table>
<tableCaption confidence="0.7023646">
Table 1: Results on Arabic-English MT. * = Significant
improvement at 95% confidence, as defined by (Koehn,
2004). ** = Significant improvement at 99.9% confi-
dence. BLEU = IBM-BLEU score. Len = Hypothesis-
to-reference length ratio.
</tableCaption>
<sectionHeader confidence="0.989817" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999988916666667">
We demonstrated a method of intelligently select-
ing hypotheses from a decoding forest which can be
combined with the baseline hypotheses to produce
a significant gain in BLEU score. In the future, we
plan to explore more trait types and alternate meth-
ods of system combination.
One possible application of this work is in fielded
translation systems. Because our method produces
high-quality complementary hypotheses at a low
computational cost, the system could present these
to the user as alternate translations. Going further,
a user could prefer a particular output type, such as
the fluency-tuned condition, and set that to be their
default translation.
The major open question is how our trait-based
combination interacts with multi-system combina-
tion. Imagine there are three different types of de-
coders which can be combined to produce some gain
in the baseline condition. If you independently im-
prove all three using trait-based combination, will
the relative gain from multi-system combination be
reduced? Or can you jointly combine all of the trait
hypotheses and get an even greater relative gain? We
plan to thoroughly explore this in the future.
</bodyText>
<page confidence="0.996703">
531
</page>
<sectionHeader confidence="0.993875" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99985190625">
D. Chiang, K. Knight, and W. Wang. 2009. 11,001 new
features for statistical machine translation. In NAACL,
pages 218–226.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201–228.
A. de Gispert, S. Virpioja, M. Kurimo, and W. Byrne.
2009. Minimum Bayes risk combination of translation
hypotheses from alternative morphological decompo-
sitions. In NAACL, pages 73–76.
P. Koehn. 2004. Pharaoh: A beam search decoder
for phrase-based statistical machine translation mod-
els. In AMTA, pages 115–124.
Z. Li, J. Eisner, and S. Khudanpur. 2009. Variational
decoding for statistical machine translation. In ACL,
pages 593–601.
A. Pauls, J. DeNero, and D. Klein. 2009. Consensus
training for consensus decoding in machine transla-
tion. In EMNLP, pages 1418–1427.
A. Rosti, B. Zhang, S. Matsoukas, and R. Schwartz.
2010. BBN system description for WMT10 system
combination task. In WMT/MetricsMATR, pages 321–
326.
L. Shen, J. Xu, and R. Weischedel. 2008. A new string-
to-dependency machine translation algorithm with a
target dependency language model. In ACL-HLT,
pages 577–585.
O. Siohan, B. Ramabhadran, and B. Kingsbury. 2005.
Constructing ensembles of ASR systems using ran-
domized decision trees. In ICASSP.
J. Xu and A. Rosti. 2010. Combining unsupervised and
supervised alignments for MT: An empirical study. In
EMNLP, pages 667–673.
</reference>
<page confidence="0.997223">
532
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.736916">
<title confidence="0.999163">Trait-Based Hypothesis Selection For Machine Translation</title>
<author confidence="0.977033">Devlin Matsoukas</author>
<address confidence="0.781867">Raytheon BBN Technologies, 10 Moulton St, Cambridge, MA 02138, USA</address>
<abstract confidence="0.996479">In the area of machine translation (MT) system combination, previous work on generating input hypotheses has focused on varying a core aspect of the MT system, such as the decoding algorithm or alignment algorithm. In this paper, we propose a new method for gendiverse hypotheses from a using These traits are simple properties of the MT output such as “average output length” and “average rule length.” Our method is designed to select hypotheses which vary in trait value but do not significantly degrade in BLEU score. These hypotheses can be combined using standard system combination techniques to produce a 1.2- 1.5 BLEU gain on the Arabic-English NIST MT06/MT08 translation task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Chiang</author>
<author>K Knight</author>
<author>W Wang</author>
</authors>
<title>11,001 new features for statistical machine translation.</title>
<date>2009</date>
<booktitle>In NAACL,</booktitle>
<pages>218--226</pages>
<contexts>
<context position="4513" citStr="Chiang et al., 2009" startWordPosition="722" endWordPosition="725"> is the set of translation rules that make up derivation d, m is the number of features, Fri is the score of the ith feature in rule r, and wi is the weight of feature i. This weight vector is optimized discriminatively to maximize BLEU score on a tuning set, using the Expected-BLEU optimization procedure (Rosti et al., 2010). Our decoder uses all of the standard statistical MT features, such as the language model, rule probabilities, and lexical probabilities. Additionally, we use 50,000 sparse, binary-valued features such as “Is the bi-gram ‘united states’ present in the output?”, based on (Chiang et al., 2009). We use a 3-gram LM for decoding and a 5-gram LM for rescoring. 3 Trait Features An MT trait represents a high-level property of the MT output. The traits used in this paper are: • Null Source Words – The percentage of source content words which align to null, i.e., are not translated. • Source Reorder – The percentage of source terminals/non-terminals which cross alignment links inside their decoding rule. • Ngram Frequency – The percentage of target 3- grams which are seen more than 10 times in the monolingual training. • Rule Frequency – The percentage of rules which are seen more than 3 t</context>
</contexts>
<marker>Chiang, Knight, Wang, 2009</marker>
<rawString>D. Chiang, K. Knight, and W. Wang. 2009. 11,001 new features for statistical machine translation. In NAACL, pages 218–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="3673" citStr="Chiang, 2007" startWordPosition="579" endWordPosition="580">ques is to find output which is consistent with the entire forest, rather than to select hypotheses with particular attributes. In fact, VD can be applied in conjunction by running VD on the rescored forest 528 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 528–532, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics for each trait condition.1 2 Description of MT System Our machine translation system is a string-todependency hierarchical decoder based on (Shen et al., 2008) and (Chiang, 2007). Bottom-up chart parsing is performed to produce a shared forest of derivations. The decoder uses a log-linear translation model, so the score of derivation d is defined as: Sd(w) _ M 1] wi Fri (1) i=1 rER(d) where R(d) is the set of translation rules that make up derivation d, m is the number of features, Fri is the score of the ith feature in rule r, and wi is the weight of feature i. This weight vector is optimized discriminatively to maximize BLEU score on a tuning set, using the Expected-BLEU optimization procedure (Rosti et al., 2010). Our decoder uses all of the standard statistical MT</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>D. Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A de Gispert</author>
<author>S Virpioja</author>
<author>M Kurimo</author>
<author>W Byrne</author>
</authors>
<title>Minimum Bayes risk combination of translation hypotheses from alternative morphological decompositions.</title>
<date>2009</date>
<booktitle>In NAACL,</booktitle>
<pages>73--76</pages>
<marker>de Gispert, Virpioja, Kurimo, Byrne, 2009</marker>
<rawString>A. de Gispert, S. Virpioja, M. Kurimo, and W. Byrne. 2009. Minimum Bayes risk combination of translation hypotheses from alternative morphological decompositions. In NAACL, pages 73–76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Pharaoh: A beam search decoder for phrase-based statistical machine translation models.</title>
<date>2004</date>
<booktitle>In AMTA,</booktitle>
<pages>115--124</pages>
<contexts>
<context position="14203" citStr="Koehn, 2004" startWordPosition="2401" endWordPosition="2402">optimizing systems to be “different” from one another (i.e., have high intersystem TER), also produced no gain over the single system. We include these results simply to demonstrate that it is not trivial to select hypotheses from a single system which produce a significant improvement in from system combination. MT06 nw MT08 eval BLEU Len BLEU Len Baseline 55.11 99.1% 46.75 96.1% Trait Feats 55.79* 99.3% 47.23* 96.0% +n-best Comb 55.65 99.3% 47.24 96.2% +Trait Comb 56.65** 99.3% 48.00** 96.2% Table 1: Results on Arabic-English MT. * = Significant improvement at 95% confidence, as defined by (Koehn, 2004). ** = Significant improvement at 99.9% confidence. BLEU = IBM-BLEU score. Len = Hypothesisto-reference length ratio. 7 Conclusions and Future Work We demonstrated a method of intelligently selecting hypotheses from a decoding forest which can be combined with the baseline hypotheses to produce a significant gain in BLEU score. In the future, we plan to explore more trait types and alternate methods of system combination. One possible application of this work is in fielded translation systems. Because our method produces high-quality complementary hypotheses at a low computational cost, the sy</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>P. Koehn. 2004. Pharaoh: A beam search decoder for phrase-based statistical machine translation models. In AMTA, pages 115–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Li</author>
<author>J Eisner</author>
<author>S Khudanpur</author>
</authors>
<title>Variational decoding for statistical machine translation.</title>
<date>2009</date>
<booktitle>In ACL,</booktitle>
<pages>593--601</pages>
<contexts>
<context position="3030" citStr="Li et al., 2009" startWordPosition="480" endWordPosition="483">antial gain. Note that all of the hypotheses are generated from a single decode of a single input system. Additionally, our method is completely compatible with multi-system combination, since our procedure can be applied to each input system, and then these systems can be combined as normal. Methods for automatically creating diverse hypotheses from a single system have been explored in speech recognition (Siohan et al., 2005), but we know of no analogous work applied to machine translation. Our procedure does share some surface similarities with techniques such as variational decoding (VD) (Li et al., 2009), but the goal in those techniques is to find output which is consistent with the entire forest, rather than to select hypotheses with particular attributes. In fact, VD can be applied in conjunction by running VD on the rescored forest 528 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 528–532, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics for each trait condition.1 2 Description of MT System Our machine translation system is a string-todependency hierarchical decoder base</context>
</contexts>
<marker>Li, Eisner, Khudanpur, 2009</marker>
<rawString>Z. Li, J. Eisner, and S. Khudanpur. 2009. Variational decoding for statistical machine translation. In ACL, pages 593–601.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Pauls</author>
<author>J DeNero</author>
<author>D Klein</author>
</authors>
<title>Consensus training for consensus decoding in machine translation.</title>
<date>2009</date>
<booktitle>In EMNLP,</booktitle>
<pages>1418--1427</pages>
<contexts>
<context position="11403" citStr="Pauls et al., 2009" startWordPosition="1940" endWordPosition="1943">mory. However, we have found that the procedure still works well even with the standard pruning parameters. Additionally, we are investigating methods for diversifying the forest with less of a slowdown to decoding. 5 Combination Once the different trait hypotheses have been generated, system combination can be performed using any method. Here, we use a confusion network decoder based on (Rosti et al., 2010). The basic procedure is to 6For example if the held-out baseline BLEU is 40.0 and ,Q = 0.5, the BLEU after trait optimization can be no less than 39.5. 7Forest-based optimization such as (Pauls et al., 2009) could be used instead. 530 select one hypothesis as the “skeleton” and then incrementally align the remaining hypotheses to create a confusion network. The confusion network is decoded using an arc-level confidence score for each input system and a language model, the weights for which are estimated discriminatively to maximize BLEU. 6 Results We present MT results in Table 1. Our experimental setup is compatible with the NIST MT08 constrained track. We trained our translation model on 35 million words of parallel data and our language model on 3.8 billion words of monolingual data. We use a </context>
</contexts>
<marker>Pauls, DeNero, Klein, 2009</marker>
<rawString>A. Pauls, J. DeNero, and D. Klein. 2009. Consensus training for consensus decoding in machine translation. In EMNLP, pages 1418–1427.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rosti</author>
<author>B Zhang</author>
<author>S Matsoukas</author>
<author>R Schwartz</author>
</authors>
<title>BBN system description for WMT10 system combination task.</title>
<date>2010</date>
<booktitle>In WMT/MetricsMATR,</booktitle>
<pages>321--326</pages>
<contexts>
<context position="4220" citStr="Rosti et al., 2010" startWordPosition="677" endWordPosition="680">ncy hierarchical decoder based on (Shen et al., 2008) and (Chiang, 2007). Bottom-up chart parsing is performed to produce a shared forest of derivations. The decoder uses a log-linear translation model, so the score of derivation d is defined as: Sd(w) _ M 1] wi Fri (1) i=1 rER(d) where R(d) is the set of translation rules that make up derivation d, m is the number of features, Fri is the score of the ith feature in rule r, and wi is the weight of feature i. This weight vector is optimized discriminatively to maximize BLEU score on a tuning set, using the Expected-BLEU optimization procedure (Rosti et al., 2010). Our decoder uses all of the standard statistical MT features, such as the language model, rule probabilities, and lexical probabilities. Additionally, we use 50,000 sparse, binary-valued features such as “Is the bi-gram ‘united states’ present in the output?”, based on (Chiang et al., 2009). We use a 3-gram LM for decoding and a 5-gram LM for rescoring. 3 Trait Features An MT trait represents a high-level property of the MT output. The traits used in this paper are: • Null Source Words – The percentage of source content words which align to null, i.e., are not translated. • Source Reorder – </context>
<context position="7679" citStr="Rosti et al., 2010" startWordPosition="1273" endWordPosition="1276"> has a “knob” with which it can control the trait value independently of everything else.3 Thus, the new hypotheses that 2Note that the trait value is always aggregated over the entire set, and not computed sentence-by-sentence. 3A feature representing the number of words already exists in our baseline system, but no such feature exists for the other 6 529 we select are “optimal” in terms of our existing MT model probabilities, but have trait values which vary from the baseline in a precise way. 4.1 Optimization Function Our normal optimization procedure uses nbest-based Expected-BLEU tuning (Rosti et al., 2010), which is a differentiable approximation of Maximum-BLEU tuning. To “target” a particular trait value, we add a second term equal to the squared error between the current trait value and the target trait value. Our modified optimization function which we seek to maximize is then: 2 Obj(w) = ExpBLEU(w) − α (D(w) − T-y) where w is the MT feature weight vector, α is the weight of the trait term, -y is the baseline value of the trait, and T is the “target” trait multiplier, N(V) is the expected-value of the numerator feature, and D(w) is the expected value of the denominator feature. To give an e</context>
<context position="9277" citStr="Rosti et al., 2010" startWordPosition="1565" endWordPosition="1568">uted as standard expected value functions, e.g.: N(w) = � � pij(w)Nij i j where pij(w) is the posterior probability of the jth hypothesis of sentence i, and Nij is the value of the numerator feature for hypothesis ij.5 4.2 Meta-Optimization It is somewhat problematic to use a fixed multiplier T on all of the traits, since on some traits it may cause a larger degradation than others. So, we take the reverse approach – for some targeted BLEU loss traits. 4Note that the ExpBLEU(w) is raw BLEU not BLEU percentage, i.e., it’s 0.4528 not 45.28 5pzj(w) is computed the same way as in ExpBLEU(w). See (Rosti et al., 2010) for details. Q, we find the maximum (or minimum) value of T which causes a loss no greater than Q, as computed on a held-out portion of the tune set.6 Here, we find the maximum and minimum trait value for Q = 0.5 and Q = 2.0, resulting in 4 sets of weights per trait. We can find the optimal T for each Q by performing a binary search on T, where we run our optimization procedure and then compute the BLEU loss at each iteration. 4.3 Forest-Based Optimization Since we have 7 traits, and we generate 4 sets of weights per trait, we have 28 “systems” to combine. Obviously, running 28 full decodes o</context>
<context position="11195" citStr="Rosti et al., 2010" startWordPosition="1903" endWordPosition="1906">found it beneficial to relax our MT pruning parameters in order to create a larger forest. This results in decoding which is roughly 2x-3x as slow as the baseline, and requires storing the larger forest in memory. However, we have found that the procedure still works well even with the standard pruning parameters. Additionally, we are investigating methods for diversifying the forest with less of a slowdown to decoding. 5 Combination Once the different trait hypotheses have been generated, system combination can be performed using any method. Here, we use a confusion network decoder based on (Rosti et al., 2010). The basic procedure is to 6For example if the held-out baseline BLEU is 40.0 and ,Q = 0.5, the BLEU after trait optimization can be no less than 39.5. 7Forest-based optimization such as (Pauls et al., 2009) could be used instead. 530 select one hypothesis as the “skeleton” and then incrementally align the remaining hypotheses to create a confusion network. The confusion network is decoded using an arc-level confidence score for each input system and a language model, the weights for which are estimated discriminatively to maximize BLEU. 6 Results We present MT results in Table 1. Our experim</context>
</contexts>
<marker>Rosti, Zhang, Matsoukas, Schwartz, 2010</marker>
<rawString>A. Rosti, B. Zhang, S. Matsoukas, and R. Schwartz. 2010. BBN system description for WMT10 system combination task. In WMT/MetricsMATR, pages 321– 326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Shen</author>
<author>J Xu</author>
<author>R Weischedel</author>
</authors>
<title>A new stringto-dependency machine translation algorithm with a target dependency language model.</title>
<date>2008</date>
<booktitle>In ACL-HLT,</booktitle>
<pages>577--585</pages>
<contexts>
<context position="3654" citStr="Shen et al., 2008" startWordPosition="574" endWordPosition="577">the goal in those techniques is to find output which is consistent with the entire forest, rather than to select hypotheses with particular attributes. In fact, VD can be applied in conjunction by running VD on the rescored forest 528 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 528–532, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics for each trait condition.1 2 Description of MT System Our machine translation system is a string-todependency hierarchical decoder based on (Shen et al., 2008) and (Chiang, 2007). Bottom-up chart parsing is performed to produce a shared forest of derivations. The decoder uses a log-linear translation model, so the score of derivation d is defined as: Sd(w) _ M 1] wi Fri (1) i=1 rER(d) where R(d) is the set of translation rules that make up derivation d, m is the number of features, Fri is the score of the ith feature in rule r, and wi is the weight of feature i. This weight vector is optimized discriminatively to maximize BLEU score on a tuning set, using the Expected-BLEU optimization procedure (Rosti et al., 2010). Our decoder uses all of the stan</context>
</contexts>
<marker>Shen, Xu, Weischedel, 2008</marker>
<rawString>L. Shen, J. Xu, and R. Weischedel. 2008. A new stringto-dependency machine translation algorithm with a target dependency language model. In ACL-HLT, pages 577–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Siohan</author>
<author>B Ramabhadran</author>
<author>B Kingsbury</author>
</authors>
<title>Constructing ensembles of ASR systems using randomized decision trees.</title>
<date>2005</date>
<booktitle>In ICASSP.</booktitle>
<contexts>
<context position="2845" citStr="Siohan et al., 2005" startWordPosition="450" endWordPosition="453">s to intelligently select hypotheses from our decoding forest which vary in trait value, but have minimal BLEU degradation compared to our baseline. We then combine these to produce a substantial gain. Note that all of the hypotheses are generated from a single decode of a single input system. Additionally, our method is completely compatible with multi-system combination, since our procedure can be applied to each input system, and then these systems can be combined as normal. Methods for automatically creating diverse hypotheses from a single system have been explored in speech recognition (Siohan et al., 2005), but we know of no analogous work applied to machine translation. Our procedure does share some surface similarities with techniques such as variational decoding (VD) (Li et al., 2009), but the goal in those techniques is to find output which is consistent with the entire forest, rather than to select hypotheses with particular attributes. In fact, VD can be applied in conjunction by running VD on the rescored forest 528 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 528–532, Montr´eal, Canada, June 3-8, 2012.</context>
</contexts>
<marker>Siohan, Ramabhadran, Kingsbury, 2005</marker>
<rawString>O. Siohan, B. Ramabhadran, and B. Kingsbury. 2005. Constructing ensembles of ASR systems using randomized decision trees. In ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Xu</author>
<author>A Rosti</author>
</authors>
<title>Combining unsupervised and supervised alignments for MT: An empirical study.</title>
<date>2010</date>
<booktitle>In EMNLP,</booktitle>
<pages>667--673</pages>
<marker>Xu, Rosti, 2010</marker>
<rawString>J. Xu and A. Rosti. 2010. Combining unsupervised and supervised alignments for MT: An empirical study. In EMNLP, pages 667–673.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>