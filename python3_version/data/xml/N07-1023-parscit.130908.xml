<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000025">
<title confidence="0.995163">
Lexicalized Markov Grammars for Sentence Compression*
</title>
<author confidence="0.995762">
Michel Galley and Kathleen R. McKeown
</author>
<affiliation confidence="0.999464">
Columbia University
Department of Computer Science
</affiliation>
<address confidence="0.98619">
New York, NY 10027, USA
</address>
<email confidence="0.999416">
{galley,kathy}@cs.columbia.edu
</email>
<sectionHeader confidence="0.994809" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999719444444445">
We present a sentence compression system based
on synchronous context-free grammars (SCFG),
following the successful noisy-channel approach
of (Knight and Marcu, 2000). We define a head-
driven Markovization formulation of SCFG dele-
tion rules, which allows us to lexicalize probabili-
ties of constituent deletions. We also use a robust
approach for tree-to-tree alignment between arbi-
trary document-abstract parallel corpora, which lets
us train lexicalized models with much more data
than previous approaches relying exclusively on
scarcely available document-compression corpora.
Finally, we evaluate different Markovized models,
and find that our selected best model is one that ex-
ploits head-modifier bilexicalization to accurately
distinguish adjuncts from complements, and that
produces sentences that were judged more gram-
matical than those generated by previous work.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999899833333333">
Sentence compression addresses the problem of re-
moving words or phrases that are not necessary
in the generated output of, for instance, summa-
rization and question answering systems. Given
the need to ensure grammatical sentences, a num-
ber of researchers have used syntax-directed ap-
proaches that perform transformations on the out-
put of syntactic parsers (Jing, 2000; Dorr et al.,
2003). Some of them (Knight and Marcu, 2000;
Turner and Charniak, 2005) take an empirical ap-
proach, relying on formalisms equivalent to proba-
bilistic synchronous context-free grammars (SCFG)
</bodyText>
<footnote confidence="0.878946285714286">
*This material is based on research supported in part
by the U.S. National Science Foundation (NSF) under Grant
No. IIS-05-34871 and the Defense Advanced Research Projects
Agency (DARPA) under Contract No. HR0011-06-C-0023.
Any opinions, findings and conclusions or recommendations
expressed in this material are those of the authors and do not
necessarily reflect the views of the NSF or DARPA.
</footnote>
<bodyText confidence="0.999762828571429">
(Lewis and Stearns, 1968; Aho and Ullman, 1969) to
extract compression rules from aligned Penn Tree-
bank (PTB) trees. While their approach proved suc-
cessful, their reliance on standard maximum like-
lihood estimators for SCFG productions results in
considerable sparseness issues, especially given the
relative flat structure of PTB trees; in practice, many
SCFG productions are seen only once. This problem
is exacerbated for the compression task, which has
only scarce training material available.
In this paper, we present a head-driven
Markovization of SCFG compression rules, an
approach that was successfully used in syntactic
parsing (Collins, 1999; Klein and Manning, 2003)
to alleviate issues intrinsic to relative frequency
estimation of treebank productions. Markovization
for sentence compression provides several benefits,
including the ability to condition deletions on
a flexible amount of syntactic context, to treat
head-modifier dependencies independently, and to
lexicalize SCFG productions.
Another part of our effort focuses on better align-
ment models for extracting SCFG compression rules
from parallel data, and to improve upon (Knight
and Marcu, 2000), who could only exploit 1.75% of
the Ziff-Davis corpus because of stringent assump-
tions about human abstractive behavior. To alleviate
their restrictions, we rely on a robust approach for
aligning trees of arbitrary document-abstract sen-
tence pairs. After accounting for sentence pairs with
both substitutions and deletions, we reached a reten-
tion of more than 25% of the Ziff-Davis data, which
greatly benefited the lexical probabilities incorpo-
rated into our Markovized SCFGs.
Our work provides three main contributions:
</bodyText>
<page confidence="0.970697">
180
</page>
<note confidence="0.7597645">
Proceedings of NAACL HLT 2007, pages 180–187,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<listItem confidence="0.7799434">
(1) Our lexicalized head-driven Markovization
yields more robust probability estimates, and our
compressions outperform (Knight and Marcu, 2000)
according to automatic and human evaluation.
(2) We provide a comprehensive analysis of the im-
</listItem>
<bodyText confidence="0.937766857142857">
pact of different Markov orders for sentence com-
pression, similarly to a study done for PCFGs (Klein
and Manning, 2003). (3) We provide a framework
for exploiting document-abstract sentence pairs that
are not purely compressive, and augment the avail-
able training resources for syntax-directed sentence
compression systems.
</bodyText>
<sectionHeader confidence="0.998737" genericHeader="introduction">
2 Synchronous Grammars for Sentence
Compression
</sectionHeader>
<bodyText confidence="0.999961">
One successful syntax-driven approach (Knight and
Marcu, 2000, henceforth K&amp;M) relies on syn-
chronous context-free grammars (SCFG) (Lewis
and Stearns, 1968; Aho and Ullman, 1969). SCFGs
can be informally defined as context-free grammars
(CFGs) whose productions have two right-hand side
strings instead of one, namely source and target
right-hand side. In the case of sentence compres-
sion, we restrict the target side to be a sub-sequence
of the source side (possibly identical), and we will
call this restricted grammar a deletion SCFG. For in-
stance, a deletion SCFG rule that removes an adver-
bial phrase (ADVP) between an noun phrase (NP)
and a verb phrase (VP) may be written as follows:
</bodyText>
<equation confidence="0.731451">
S —* (NP ADVP VP, NP VP)
</equation>
<bodyText confidence="0.999714214285714">
In a sentence compression framework similar to
the one presented by K&amp;M, we build SCFGs that
are fully trainable from a corpus of document and
reduced sentences. Such an approach comprises
two subproblems: (1) transform tree pairs into syn-
chronous grammar derivations; (2) based on these
derivations, assign probabilities to deletion SCFG
productions, and more generally, to compressions
produced by such grammars. Since the main point of
our paper lies in the exploration of better probability
estimates through Markovization and lexicalization
of SCFGs, we first address the latter problem, and
discuss the task of building synchronous derivations
only later in Section 4.
</bodyText>
<subsectionHeader confidence="0.985163">
2.1 Stochastic Synchronous Grammars
</subsectionHeader>
<bodyText confidence="0.99987625">
The overall goal of a sentence compression system is
to transform a given input sentence f into a concise
and grammatical sentence c E C, which is a sub-
sequence of f. Similarly to K&amp;M and many suc-
cessful syntactic parsers (Collins, 1999; Klein and
Manning, 2003), our sentence compression system
is generative, and attempts to find the optimal com-
pression c� by estimating the following function:1
</bodyText>
<equation confidence="0.982468">
c� = arg max { p(c f)� = arg max �p(f, c) } (1)
cEC l cEC JJ
</equation>
<bodyText confidence="0.988129">
If τ(f, c) is the set of all tree pairs that yield (f, c)
according to some underlying SCFG, we can esti-
mate the probability of the sentence pair using:
</bodyText>
<equation confidence="0.9963205">
p(f, c) = X P(πf, πc) (2)
(πf,πc)Eτ(f,c)
</equation>
<bodyText confidence="0.999943818181818">
We note that, in practice (and as in K&amp;M), Equa-
tion 2 is often approximated by restricting τ(f, c)
to a unique full tree orf, the best hypothesis of an
off-the-shelf syntactic parser. This implies that each
possible compression c is the target-side yield of at
most one SCFG derivation.
As in standard PCFG history-based models, the
probability of the entire structure (Equation 2) is fac-
tored into probabilities of grammar productions. If
θ is a derivation θ = r1 o · · · o rj · · · o rJ, where
rj denotes the SCFG rule lj —* (αjf, αjc), we get:
</bodyText>
<equation confidence="0.992485">
p(πf, πc) = YJ p(αjf, αjc lj) (3)
j�1
</equation>
<bodyText confidence="0.992541">
The question we will now address is how to esti-
mate the probability p(αj f, αjc lj) of each SCFG pro-
duction.
</bodyText>
<subsectionHeader confidence="0.999754">
2.2 Lexicalized Head-Driven Markovization of
Synchronous Grammars
</subsectionHeader>
<bodyText confidence="0.989880625">
A main issue in our enterprise is to reliably estimate
productions of deletion SCFGs. In a sentence com-
pression framework as the one presented by K&amp;M,
we use aligned trees of the form of the Penn Tree-
bank (PTB) (Marcus et al., 1994) to acquire and
score SCFG productions. However, the use of the
PTB structure faces many challenges also encoun-
tered in probabilistic parsing.
</bodyText>
<footnote confidence="0.730290333333333">
1In their noisy-channel approach, K&amp;M further break down
p(c, f) into p(f|c) · p(c), which we refrain from doing for rea-
sons that will become obvious later.
</footnote>
<page confidence="0.997962">
181
</page>
<bodyText confidence="0.999887976190477">
Firstly, PTB tree structures are relatively flat, par-
ticularly within noun phrases. For instance, adjec-
tive phrases (ADJP)—which are generally good can-
didates for deletions—appear in 90 different NP-
rooted SCFG productions in Ziff-Davis,2 61 of
which appear only once, e.g., NP , (DT ADJP JJ
NN NN, DT JJ NN NN). While it may seem ad-
vantageous to maintain many constituents within the
same domain of locality of an SCFG production, as
we may hope to exploit its large syntactic context to
condition deletions more accurately, the sparsity of
such productions make them poor candidates for rel-
ative frequency estimation, especially in a task with
limited quantities of training material. Indeed, our
base training corpus described in Section 4 contains
only 951 SCFG productions, 593 appearing once.
Secondly, syntactic categories in the PTB are par-
ticularly coarse grained, and lead to many incorrect
context-free assumptions. Some important distinc-
tions, such as between arguments and adjuncts, are
beyond the scope of the PTB annotation, and it is
often difficult to determine out of context whether a
given constituent can safely be deleted from a right-
hand side.
One first type of annotation that can effectively be
added to each syntactic category is its lexical head
and head part-of-speech (POS), following work in
syntactic parsing (Collins, 1999). This type of an-
notation is particular beneficial in the case of, e.g.,
prepositional phrases (PP), which may be either
complement or adjunct. As in the case of Figure 1
(in which adjuncts appear in italic), knowing that the
PP headed by “from” appears in a VP headed by
“fell” helps us to determine that the PP is a com-
plement to the verb “fell”, and that it should pre-
sumably not be deleted. Conversely, the PP headed
by “because” modifying the same verb is an adjunct,
and can safely be deleted if unimportant.3 Also, as
discussed in (Klein and Manning, 2003), POS an-
notation can be useful as a means of backing off
to more frequently occurring head-modifier POS oc-
currences (e.g., VBD-IN) when specific bilexical co-
</bodyText>
<footnote confidence="0.704955142857143">
2Details about the SCFG extraction procedure are given in
Section 4. In short, we refer here to a grammar generated from
823 sentence pairs.
3The PP headed by “from” is an optional argument, and thus
may still be deleted. Our point is that lexical information in gen-
eral should help give lower scores to deletions of constituents
that are grammatically more prominent.
</footnote>
<figure confidence="0.602516">
S
NP ADVP VP .
</figure>
<figureCaption confidence="0.999442">
Figure 1: Penn Treebank tree with adjuncts in italic.
</figureCaption>
<bodyText confidence="0.983092135135135">
occurrences are sparsely seen (e.g., “fell”-“from”).
At a lower level, lexicalization is clearly desirable
for pre-terminals. Indeed, current SCFG models
such as K&amp;M have no direct way of preventing
highly improbable single word removals, such as
deletions of adverbs “never” or “nowhere”, which
may turn a negative statement into a positive one.4
A second type of annotation that can be added to
syntactic categories is the so-called parent annota-
tion (Johnson, 1998), which was effectively used in
syntactic parsing to break unreasonable context-free
assumptions. For instance, a PP with a VP parent
is marked as PPˆVP. It is reasonable to assume that,
e.g., that constituents deep inside a PP have more
chances to be removed than otherwise expected, and
one may seek to increase the amount of vertical
context that is available for conditioning each con-
stituent deletion.
To achieve the above desiderata for better SCFG
probability estimates—i.e., reduce the amount of
sister annotation within each SCFG production, by
conditioning deletions on a context smaller than an
entire right-hand side, and at the same time in-
crease the amount of ancestor and descendent an-
notation through parent (or ancestor) annotation and
lexicalization—we follow the approach of (Collins,
1999; Klein and Manning, 2003), i.e., factor-
ize n-ary grammar productions into products of n
right-hand side probabilities, a technique sometimes
called Markovization.
Markovization is generally head-driven, i.e., re-
flects a decomposition centered around the head of
each CFG production:
l , AL- ··· L&apos;HR&apos; ··· RnA (4)
4K&amp;M incorporate lexical probabilities through n-gram
models, but such language models are obviously not good for
preventing such unreasonable deletions.
</bodyText>
<figure confidence="0.998511115384615">
RB
PP
NN
.
NP
IN
also
Earning
because
NN
NN
NN
JJ
microchip
demand
period
year-ago
VBD
fell IN
from DT
the
NP
PP
IN
of VBG
slowing
</figure>
<page confidence="0.986823">
182
</page>
<bodyText confidence="0.999975363636364">
where H is the head, L1,..., Lm the left modi-
fiers, R1, ... , Rn are right modifiers, and A termi-
nation symbols needed for accurate probability es-
timations (e.g., to capture the fact that certain con-
stituents are more likely than others to be the right-
most constituent); for simplicity, we will ignore A
in later discussions. For a given SCFG production
l → hαf, αci, we ask, given the source RHS αf
that is assumed given (e.g., provided by a syntactic
parser), which of its RHS elements are also present
in αc. That is, we write:
</bodyText>
<equation confidence="0.654573">
p(αc|αf, l) = (5)
</equation>
<bodyText confidence="0.991512696969697">
p(km l ,··· , k1l , kh, k1r, ··· , knr |αf, l)
where kh, kil, kjr (‘k’ for keep) are binary variables
that are true if and only if constituents H, Li, Rj (re-
spectively) of the source RHS αf are present in the
target side αc. Note that the conditional probabil-
ity in Equation 5 enables us to estimate Equation 3,
since p(αf, αc|l) = p(αc|αf, l) · p(αf|l). We can
rely on a state-of-the-art probabilistic parser to ef-
fectively compute either p(αf|l) or the probability
of the entire tree πf, and need not worry about esti-
mating this term. In the case of sentence compres-
sion from the one-best hypothesis of the parser, we
can ignore p(αf|l) altogether, since πf is the same
for all compressions.
We can rewrite Equation 5 exactly using a head-
driven infinite-horizon Markovization:
where A = (k1l , · · · , kml ) is a term needed by the
chain rule. One key issue is to make linguistically
plausible assumptions to determine which condi-
tioning variables in the terms should be deleted. Fol-
lowing our discussion in the first part of this section,
we may start by making an order-s Markov approx-
imation centered around the head, i.e., we condi-
tion each binary variable (e.g., kir) on a context of
up to s sister constituents between the current con-
stituent and the head (e.g., (Ri−s, ... , Ri)). In or-
der to incorporate bilexical dependencies between
the head and each modifier, we also condition all
modifier probabilities on head variables H (and kh).
These assumptions are overall quite similar to the
ones made in Markovized parsing models. If we as-
sume that all other conditioning variables in Equa-
tion 6 are irrelevant, we write:
</bodyText>
<equation confidence="0.989248444444444">
p(αc|αf, l) = ph(kh|H, l) (7)
H ·pl(ki l|Li−s,...,Li,ki−s
l ,...,ki−1
l , H, kh, l)
i=1...m
pr(kir|Ri−s, ..., Ri, ki−s
r , ..., ki−1
r , H, kh, l)
i=1...n
</equation>
<bodyText confidence="0.99679212">
Note that it is important to condition deletions on
both constituent histories (Ri−s, ... , Ri) and non-
deletion histories (ki−s
r , ... , ki−1
r ); otherwise we
would be unable to perform deletions that must op-
erate jointly, as in production S → hADVP COMMA
NP VP, NP VPi (in which the ADVP should not be
deleted without the comma). Without binary his-
tories, we often observed superfluous punctuation
symbols and dangling coordinate conjunctions ap-
pearing in our outputs.
Finally, we label l with an order-v ancestor anno-
tation, e.g., for the VP in Figure 1, l = E for v = 0,
l =VPˆS for v = 2, and so on. We also replace H
and modifiers Li and Ri by lexicalized entries, e.g.,
H =(VP,VBD,fell) and Ri =(PP,IN,from). Note
that to estimate pl(kil |· · · ), we only lexicalize Li
and H, and none of the other conditioning modifiers,
since this would, of course, introduce too many con-
ditioning variables (the same goes for pr(kir |· · · )).
The question of how much sister and vertical (s and
v) context is needed for effective sentence compres-
sion, and whether to use lexical or POS annotation,
will be evaluated in detail in Section 5.
</bodyText>
<sectionHeader confidence="0.988393" genericHeader="method">
3 The Data
</sectionHeader>
<bodyText confidence="0.999992818181818">
To acquire SCFG productions, we used Ziff-Davis,
a corpus of technical articles and human abstractive
summaries. Articles and summaries are paired by
document, so the first step was to perform sentence
alignment. In the particular case of sentence com-
pression, a simple approach is to just consider com-
pression pairs (f,c), where c is a substring of f. K&amp;M
identified only 1,087 such paired sentences in the en-
tire corpus, which represents a recall of 1.75%.
For our empirical evaluations, we split the data as
follows: among the 1,055 sentences that were taken
</bodyText>
<equation confidence="0.5209245">
p(αc|αf, l) = p(kh|αf, l) (6)
H· p(kil|k1l ,··· ,ki−1
i=1...m l , kh, αf,l)
H· p(kir|k1r,··· ,ki−1
i=1...n r ,kh,A,αf,l)
H·
</equation>
<page confidence="0.988168">
183
</page>
<bodyText confidence="0.999955428571429">
to train systems described in K&amp;M, we selected the
first 32 sentence pairs to be an auxiliary test corpus
(for future work), the next 200 sentences to be our
development corpus, and the remaining 823 to be
our base training corpus (ZD-0), which will be aug-
mented with additional data as explained in the next
section. We feel it is important to use a relatively
large development corpus, since we will provide in
Section 5 detailed analyses of model selection on
the development set (e.g., by evaluating different
Markov structures), and we want these findings to
be as significant as possible. Finally, we used the
same test data as K&amp;M for human evaluation pur-
poses (32 sentence pairs).
</bodyText>
<sectionHeader confidence="0.9883595" genericHeader="method">
4 Tree Alignment and Synchronous Gram-
mar Inference
</sectionHeader>
<bodyText confidence="0.999984870967742">
We now describe methods to train SCFG models
from sentence pairs. Given a tree pair (f, c), whose
respective parses (7rf, 7rc) were generated by the
parser described in (Charniak and Johnson, 2005),
the goal is to transform the tree pair into SCFG
derivations, in order to build relative frequency es-
timates for our Markovized models from observed
SCFG productions. Clearly, the two trees may
sometimes be structurally quite different (e.g., a
given PP may attach to an NP in 7rf, while attach-
ing to VP in 7rc), and it is not always possible to
build an SCFG derivation given the constraints in
(7rf, 7rc). The approach taken by K&amp;M is to analyze
both trees and count an SCFG rule whenever two
nodes are “deemed to correspond”, i.e., roots are the
same, and α, is a sub-sequence of αf. This leads
to a quite restricted number of different productions
on our base training set (ZD-0): 823 different pro-
ductions were extracted, 593 of which appear only
once. This first approach has serious limitations;
the assumption that sentence compression appropri-
ately models human abstractive data is particularly
problematic. This considerably limits the amount
of training data that can be exploited in Ziff-Davis
(which contains overall more than 4,000 documents-
abstract pairs), and this makes it very difficult to
train lexicalized models.
An approach to slightly loosen this assumption
is to consider document-abstract sentence pairs in
which the condensed version contains one or more
substitutions or insertions. Consider for example
</bodyText>
<equation confidence="0.984690272727272">
S[1]
NP[2] VP .[
DT[3] JJ[5] NN[7] VP CC VP[9] .
The[4] second[6] computer[8] VBD PRT and VBD[10] PP[12]
started RP ran[11] IN[13] NP[15]
up without[14] NN[16]
S[1]
incident[17]
NP[2] VP[9] .[18]
without[14] NN[16]
incident[17]
</equation>
<figureCaption confidence="0.81397375">
Figure 2: Full sentence and its revision. While the latter is not a
compression of the former, it could still be used to gather statis-
tics to train a sentence compression system, e.g., to learn the
reduction of a VP coordination.
</figureCaption>
<bodyText confidence="0.999454">
the tree pair in Figure 2: the two sentences are syn-
tactically very close, but the substitution of “com-
puter” with “unit” makes this sentence pair unus-
able in the framework presented in K&amp;M. Arguably,
there should be ways to exploit abstract sentences
that are slightly reworded in addition to being com-
pressed. To use sentence pairs with insertions and
substitutions, we must find a way to align tree pairs
in order to identify SCFG productions. More specif-
ically, we must define a constituent alignment be-
tween the paired abstract and document sentences,
which determine how the two trees are synchronized
in a derivation. Obtaining this alignment is no triv-
ial matter as the number of non-deleting edits in-
creases. To address this, we synchronized tree pairs
by finding the constituent alignment that minimizes
the edit distance between the two trees, i.e., mini-
mize the number of terminals and non-terminals in-
sertions, substitutions and deletions.5 While criteria
</bodyText>
<footnote confidence="0.733834">
5The minimization problem is known to be NP hard, so we
used an approximation algorithm (Zhang and Shasha,1989) that
</footnote>
<figure confidence="0.964429181818182">
.[19]
VBD[10]
PP[12]
NN[7]
DT[3]
JJ[5]
The[4]
ran[11] IN[13]
second[6]
NP[15]
unit[8]
</figure>
<page confidence="0.99594">
184
</page>
<bodyText confidence="0.999511763157895">
other than minimum tree edit distance may be effec-
tive, we found—after manual inspections of align-
ments between sentences with less than five non-
deleting edits—that this method generally produces
good alignments. A sample alignment is provided in
Figure 2. Once a constituent alignment is available,
it is then trivial to extract all deletion SCFG rules
available in a tree pair, e.g., NP —* (DT JJ NN, DT
JJ NN) in the figure.
We also exploited more general tree productions
known as synchronous tree substitution grammar
(STSG) rules, in an approach quite similar to (Turner
and Charniak, 2005). For instance, the STSG rule
rooted at S can be decomposed into two SCFG pro-
ductions if we allow unary rules such as VP —* VP to
be freely added to the compressed tree. More specif-
ically, we decompose any STSG rule that has in its
target (compressed) RHS a single context free pro-
duction, and that contains in its source (full) RHS
a single context free production adjoined with any
number of tree adjoining grammar (TAG) auxiliary
trees (Joshi et al., 1975). In the figure, the initial tree
is S —* NP VP, and the adjoined (auxiliary) tree is
VP —* VP CC VP.6 We found this approach quite
helpful, since most useful compressions that mimic
TAG adjoining operations are missed by the extrac-
tion procedure of K&amp;M.
Since we found that exploiting sentence pairs con-
taining insertions had adverse consequences in terms
of compression accuracies, we only report experi-
ments with sentence pairs containing no insertions.
We gathered sentence pairs with up to six substi-
tutions using minimum edit distance matching (we
will refer to these sets as ZD-0 to ZD-6). With a
limit of up to six substitutions (ZD-6), we were able
to train our models on 16,787 sentences, which rep-
resents about 25% of the total number of summary
sentences of the Ziff-Davis corpus.
</bodyText>
<sectionHeader confidence="0.998745" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.996058">
All experiments presented in this section are per-
formed on the Ziff-Davis corpus. We note first that
all probability estimates of our Markovized gram-
</bodyText>
<footnote confidence="0.937362">
runs in polynomial time.
6To determine whether a given one-level tree is an auxiliary,
we simply check the following properties: all its leaves but one
(the “foot node”) must be nodes attached to deleted subtrees
(e.g., VP and CC in the figure), and the foot node (VP[9]) must
have the same syntactic category as the root node.
</footnote>
<bodyText confidence="0.94358926">
mars are smoothed. Indeed, incorporating lexical
dependencies within models trained on data sets as
small as 16,000 sentence pairs would be quite fu-
tile without incorporating robust smoothing tech-
niques. Different smoothing techniques were eval-
uated with our models, and we found that interpo-
lated Witten-Bell discounting was the method that
performed best. We used relative frequency es-
timates for each of the models presented in Sec-
tion 2.2 (i.e., ph, pl, pr), and trained pl separately
from pr. We interpolated our most specific models
(lexical heads, POS tags, ancestor and sister annota-
tion) with lower-order models.7
Automatic evaluation on development sets is per-
formed using word-level classification accuracy, i.e.,
the number of words correctly classified as being
either deleted or not deleted, divided by the to-
tal number of words. In our first evaluation, we
experimented with different horizontal and vertical
Markovizations (Table 1). First, it appears that ver-
tical annotation is moderately helpful. It provides
gains in accuracy ranging from .5% to .9% for v = 1
over a simpler models (v = 0), but higher orders
(v &gt; 1) have a tendency to decrease performance.
On the other hand, sister annotation of order 1 is
much more critical, and provides 4.1% improvement
over a simpler model (s = 0, v = 0). Manual exami-
nations of compression outputs confirmed this anal-
ysis: without sister annotation, deletion of punctu-
ation and function words (determiners, coordinate
conjunctions, etc.) is often inaccurate, and compres-
sions clearly lack fluency. This annotation is also
helpful for phrasal deletions; for instance, we found
that PPs are deleted in 31.4% of cases in Ziff-Davis
if they do not immediately follow the head con-
stituent, but this percentage drops to 11.1% for PPs
that immediately follow the head. It seems, how-
ever, that increasing sister annotation beyond s &gt; 1
only provide limited improvements.
In our second evaluation reported in Table 2, we
7We relied on the SRI language modeling (SRILM) toolkit
library for all smoothing experiments. We used the following
order in our deleted interpolation of ph: lexical head, head POS,
ancestor annotation, and head category. For pl and pr, we re-
moved first: lexical head, lexical head of the modifier, head
POS, head POS of the modifier, sister annotation (Li deleted
before kil), kh, category of the head, category of the modifier.
We experimented with different deletion interpolation order-
ings, and this ordering appears to work quite well in practice,
and was used in all experiments reported in this paper.
</bodyText>
<page confidence="0.997784">
185
</page>
<bodyText confidence="0.99414">
assessed the usefulness of lexical and POS anno-
tation (setting s and v to 0). In the table, we use
M to denote any of the modifiers Li or Ri, and
c, t, w respectively represent syntactic constituent,
POS, and lexical conditioning. While POS annota-
tion is clearly advantageous compared to using only
syntactic categories, adding lexical variables to the
model also helps. As is shown in the table, it is es-
pecially important to know the lexical head of the
modifier we are attempting to delete. The addition of
wm to conditioning variables provides an improve-
ment of 1.3% (from 66.5% to 67.8%) on our op-
timal Ziff-Davis training corpus (ZD-6). Further-
more, bilexical head-modifier dependencies provide
a relatively small improvement of .5% (from 69.8%
to 70.3%) over the best model that does not incor-
porate the lexical head wh. Note that lexical con-
ditioning also helps in the case where the training
data is relatively small (ZD-0), though differences
are less significant, and bilexical dependencies actu-
ally hurt performance. In subsequent experiments,
we experimented with different Markovizations and
lexical dependency combination, and finally settled
with a model (s = 1 and v = 1) incorporating all
conditioning variables listed in the last line of Ta-
ble 2. This final tuning was combined with human
inspection of generated outputs, since certain modi-
fications that positively impacted output quality sel-
dom changed accuracies.
We finally took the best configuration selected
above, and evaluated our model against the noisy-
channel model of K&amp;M on the 32 test sentences se-
lected by them. We performed both automatic and
human evaluation against the output produced by
Knight and Marcu’s original implementation of their
noisy channel model (Table 3). In the former case,
we also provide Simple String Accuracies (SSA).8
For human evaluation, we hired six native-speaker
judges who scored grammaticality and content (im-
portance) with scores from 1 to 5, using instructions
as described in K&amp;M. Both types of evaluations fa-
vored our Markovized model against the noisy chan-
nel model.
Table 4 shows several outputs of our system
</bodyText>
<footnote confidence="0.734312">
8SSA is defined as: SSA = 1 − (I + D + S)/R. The
numerator terms are respectively the number of inserts, deletes,
and substitutions, and R is the length of the reference compres-
sion.
</footnote>
<table confidence="0.998390166666667">
Vertical Horizontal Order
Order s = 0 s = 1 s = 2 s = 3
v = 0 63 67.1 67.2 67.2
v = 1 63.9 67.6 67.7 67.7
v = 2 65.7 66.6 66.9 66.9
v = 3 65.2 66.8 67.1 67
</table>
<tableCaption confidence="0.995512">
Table 1: Markovizations accuracies on Ziff-Davis devel set.
</tableCaption>
<table confidence="0.999873625">
Conditioning Variables ZD-0 ZD-3 ZD-6
M =cm H=ch 62.2 62.4 64.4
M =(cm, tm) H =ch 63.0 63.4 66.5
M =(cm, wm) H =ch 64.2 65.2 66.7
M =(cm, tm, wm) H =ch 63.8 65.8 67.8
M =(cm, tm, wm) H =(ch, th) 66.7 68.6 69.8
M =(cm, tm, wm) H =(ch, wh) 66.9 68.9 70.3
M =(cm, tm, wm) H =(ch, th, wh) 66.3 69.1 69.8
</table>
<tableCaption confidence="0.9950535">
Table 2: Accuracies on Ziff-Davis devel set with different head-
modifier annotations.
</tableCaption>
<table confidence="0.9996325">
Models Acc SSA Grammar Content Len(%)
NoisyC 61.3 14.6 4.37 ± 0.5 3.87 ± 1.2 70.4
Markov 67.9 31.7 4.68 ± 0.4 4.22 ± 0.4 62.7
Human - - 4.95 ± 0.1 4.43 ± 0.3 53.3
</table>
<tableCaption confidence="0.999965">
Table 3: Accuracies on Ziff-Davis test set.
</tableCaption>
<bodyText confidence="0.999965333333333">
(Markov) that significantly differed from the output
of the noisy channel model (NoisyC), which con-
firms our finding that Markovized models can pro-
duce quite grammatical output. Our compression for
the first sentence underlines one of the advantages of
constituent-based classifiers, which have the ability
of deleting a very long phrase (here, a PP) at once.
The three next sentences display some advantages
of our approach over the K&amp;M model: here, the lat-
ter model performs deletion with too little lexico-
syntactic information, and accidentally removes cer-
tain modifiers that are sometimes, but not always,
good candidates for deletions (e.g., ADJP in Sen-
tence 2, PP in sentences 3 and 4). On the other hand,
our model keeps these constituent intact. Finally, the
fifth and last example is one of the only three cases
(among the 32 sentences) where our model produced
a sentence we judged clearly ungrammatical. After
inspection, we found that our parser assigned par-
ticularly errorful trees to those inputs, which may
partially explain these ungrammatical outputs.
</bodyText>
<sectionHeader confidence="0.999976" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.99999">
A relatively large body of work addressed the prob-
lem of sentence compression. One successful recent
approach (McDonald, 2006) combines a discrimi-
native framework with a set of features that cap-
ture information similar to the K&amp;M model. Mc-
</bodyText>
<page confidence="0.978846">
186
</page>
<bodyText confidence="0.797745066666667">
Input Many debugging features, including user-defined breakpoints
NoisyC and variable-watching and message-watching windows, have
Markov been added.
Human Many debugging features, including user-defined points and
variable-watching and message-watching windows, have been
added.
Many debugging features have been added.
Many debugging features have been added.
Input The chemical etching process usedforglare protection is effec-
NoisyC tive and will help ifyour office has the fluorescent-light overkill
Markov that ’s typical in offices.
Human The process used for glare protection is and will help if your
office has the overkill
The chemical etching process used for glare protection is ef-
fective.
Glare protection is effective.
Input The utilities will be bundled with Quickdex II in a $90 pack-
NoisyC age called super quickdex, which is expected to ship in late
Markov summer.
Human The utilities will be bundled
The utilities will be bundled with Quickdex II.
The utilities will be bundled with Quickdex II.
Input The discounted package for the SparcServer 470 is priced at
NoisyC $89,900, down from the regular $107,795.
Markov The package for the 470 is priced
Human The discounted package for the SparcServer 470 is at $89,900.
The SparcServer 470 is priced at $89,900, down from the reg-
ular $107,795.
Input Prices range from $5,000for a microvax 2000 to $179,000for
NoisyC the vax 8000 or higher series.
</bodyText>
<table confidence="0.95826075">
Markov Prices range from $5,000 for a 2000 to $179,000 for the vax
Human 8000 or higher series.
Prices range from $5,000 for a microvax for the vax.
Prices range from $5,000 to $179,000.
</table>
<tableCaption confidence="0.999463">
Table 4: Compressions of sample test sentences.
</tableCaption>
<bodyText confidence="0.999879862068966">
Donald’s features include compression bigrams, as
well as soft syntactic evidence extracted from parse
trees and dependency trees. The strength of McDon-
ald’s approach partially stems from its robustness
against redundant and noisy features, since each fea-
ture is weighted proportionally to its discriminative
power, and his approach is thus hardly penalized
by uninformative features. In contrast, our work
puts much more emphasis on feature analysis than
on efficient optimization, and relies on a statisti-
cal framework (maximum-likelihood estimates) that
strives for careful feature selection and combination.
It also describes and evaluates models incorporating
syntactic evidence that is new to the sentence com-
pression literature, such as head-modifier bilexical
dependencies, and nth-order sister and vertical an-
notation. We think this work leads to a better un-
derstanding of what type of syntactic and lexical ev-
idence makes sentence compression work. Further-
more, our work leaves the door open to uses of our
factored model in a constituent-based or word-based
discriminative framework, in which each elemen-
tary lexico-syntactic structure of this paper can be
discriminatively weighted to directly optimize com-
pression quality. Since McDonald’s approach does
not incorporate SCFG deletion rules, and conditions
deletions on less lexico-syntactic context, we believe
this will lead to levels of performance superior to
both papers.
</bodyText>
<sectionHeader confidence="0.999075" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.9999849">
We presented a sentence compression system based
on SCFG deletion rules, for which we defined
a head-driven Markovization formulation. This
Markovization enabled us to incorporate lexical con-
ditioning variables into our models. We empirically
evaluated different Markov structures, and obtained
a best system that generates particularly grammati-
cal sentences according to a human evaluation. Our
sentence compression system is freely available for
research and educational purposes.
</bodyText>
<sectionHeader confidence="0.999022" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999643">
We would like to thank Owen Rambow, Michael
Collins, Julia Hirschberg, and Daniel Ellis for their
helpful comments and suggestions.
</bodyText>
<sectionHeader confidence="0.998712" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999920387096774">
A. Aho and J. Ullman. 1969. Syntax directed translations and
the pushdown assembler. 3:37–56.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-best pars-
ing and maxent discriminative reranking. In Proc. of ACL.
M. Collins. 1999. Head-driven Statistical Models for Natural
Language Parsing. Ph.D. thesis, U. of Pennsylvania.
B. Dorr, D. Zajic, and R. Schwartz. 2003. Hedge: A parse-and-
trim approach to headline generation. In Proc. of DUC.
H. Jing. 2000. Sentence reduction for automatic text summa-
rization. In Proc. of NAACL, pages 310–315.
M. Johnson. 1998. PCFG models of linguistic tree representa-
tions. Computational Linguistics, 24(4):613–632.
A. Joshi, L. Levy, and M. Takahashi. 1975. Tree adjunct gram-
mar. Journal of Computer and System Science, 21(2).
D. Klein and C. Manning. 2003. Accurate unlexicalized pars-
ing. In Proc. of ACL.
K. Knight and D. Marcu. 2000. Statistics-based summarization
— step one: Sentence compression. In Proc. of AAAI.
P. Lewis and R. Stearns. 1968. Syntax-directed transduction.
In Journal of the Association for Computing Machinery, vol-
ume 15, pages 465–488.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1994. Build-
ing a large annotated corpus of english: The penn treebank.
Computational Linguistics, 19(2):313–330.
R. McDonald. 2006. Discriminative sentence compression
with soft syntactic constraints. In Proc. of EACL.
J. Turner and E. Charniak. 2005. Supervised and unsupervised
learning for sentence compression. In Proc. of ACL.
K. Zhang and D. Shasha. 1989. Simple fast algorithms for the
editing distance between trees and related problems. SIAM
J. Comput., 18(6):1245–1262.
</reference>
<page confidence="0.997933">
187
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.885105">
<title confidence="0.998854">Markov Grammars for Sentence</title>
<author confidence="0.996212">R Galley</author>
<affiliation confidence="0.974663">Columbia Department of Computer</affiliation>
<address confidence="0.970155">New York, NY 10027,</address>
<email confidence="0.999652">galley@cs.columbia.edu</email>
<email confidence="0.999652">kathy@cs.columbia.edu</email>
<abstract confidence="0.99810252631579">We present a sentence compression system based on synchronous context-free grammars (SCFG), following the successful noisy-channel approach of (Knight and Marcu, 2000). We define a headdriven Markovization formulation of SCFG deletion rules, which allows us to lexicalize probabilities of constituent deletions. We also use a robust approach for tree-to-tree alignment between arbitrary document-abstract parallel corpora, which lets us train lexicalized models with much more data than previous approaches relying exclusively on scarcely available document-compression corpora. Finally, we evaluate different Markovized models, and find that our selected best model is one that exploits head-modifier bilexicalization to accurately distinguish adjuncts from complements, and that produces sentences that were judged more grammatical than those generated by previous work.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Aho</author>
<author>J Ullman</author>
</authors>
<title>Syntax directed translations and the pushdown assembler.</title>
<date>1969</date>
<pages>3--37</pages>
<contexts>
<context position="2111" citStr="Aho and Ullman, 1969" startWordPosition="300" endWordPosition="303">f them (Knight and Marcu, 2000; Turner and Charniak, 2005) take an empirical approach, relying on formalisms equivalent to probabilistic synchronous context-free grammars (SCFG) *This material is based on research supported in part by the U.S. National Science Foundation (NSF) under Grant No. IIS-05-34871 and the Defense Advanced Research Projects Agency (DARPA) under Contract No. HR0011-06-C-0023. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the NSF or DARPA. (Lewis and Stearns, 1968; Aho and Ullman, 1969) to extract compression rules from aligned Penn Treebank (PTB) trees. While their approach proved successful, their reliance on standard maximum likelihood estimators for SCFG productions results in considerable sparseness issues, especially given the relative flat structure of PTB trees; in practice, many SCFG productions are seen only once. This problem is exacerbated for the compression task, which has only scarce training material available. In this paper, we present a head-driven Markovization of SCFG compression rules, an approach that was successfully used in syntactic parsing (Collins,</context>
<context position="4674" citStr="Aho and Ullman, 1969" startWordPosition="667" endWordPosition="670"> and human evaluation. (2) We provide a comprehensive analysis of the impact of different Markov orders for sentence compression, similarly to a study done for PCFGs (Klein and Manning, 2003). (3) We provide a framework for exploiting document-abstract sentence pairs that are not purely compressive, and augment the available training resources for syntax-directed sentence compression systems. 2 Synchronous Grammars for Sentence Compression One successful syntax-driven approach (Knight and Marcu, 2000, henceforth K&amp;M) relies on synchronous context-free grammars (SCFG) (Lewis and Stearns, 1968; Aho and Ullman, 1969). SCFGs can be informally defined as context-free grammars (CFGs) whose productions have two right-hand side strings instead of one, namely source and target right-hand side. In the case of sentence compression, we restrict the target side to be a sub-sequence of the source side (possibly identical), and we will call this restricted grammar a deletion SCFG. For instance, a deletion SCFG rule that removes an adverbial phrase (ADVP) between an noun phrase (NP) and a verb phrase (VP) may be written as follows: S —* (NP ADVP VP, NP VP) In a sentence compression framework similar to the one present</context>
</contexts>
<marker>Aho, Ullman, 1969</marker>
<rawString>A. Aho and J. Ullman. 1969. Syntax directed translations and the pushdown assembler. 3:37–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Coarse-to-fine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proc. of ACL. M. Collins.</booktitle>
<tech>Ph.D. thesis,</tech>
<institution>U. of Pennsylvania.</institution>
<contexts>
<context position="17371" citStr="Charniak and Johnson, 2005" startWordPosition="2852" endWordPosition="2855">ection. We feel it is important to use a relatively large development corpus, since we will provide in Section 5 detailed analyses of model selection on the development set (e.g., by evaluating different Markov structures), and we want these findings to be as significant as possible. Finally, we used the same test data as K&amp;M for human evaluation purposes (32 sentence pairs). 4 Tree Alignment and Synchronous Grammar Inference We now describe methods to train SCFG models from sentence pairs. Given a tree pair (f, c), whose respective parses (7rf, 7rc) were generated by the parser described in (Charniak and Johnson, 2005), the goal is to transform the tree pair into SCFG derivations, in order to build relative frequency estimates for our Markovized models from observed SCFG productions. Clearly, the two trees may sometimes be structurally quite different (e.g., a given PP may attach to an NP in 7rf, while attaching to VP in 7rc), and it is not always possible to build an SCFG derivation given the constraints in (7rf, 7rc). The approach taken by K&amp;M is to analyze both trees and count an SCFG rule whenever two nodes are “deemed to correspond”, i.e., roots are the same, and α, is a sub-sequence of αf. This leads </context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>E. Charniak and M. Johnson. 2005. Coarse-to-fine n-best parsing and maxent discriminative reranking. In Proc. of ACL. M. Collins. 1999. Head-driven Statistical Models for Natural Language Parsing. Ph.D. thesis, U. of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Dorr</author>
<author>D Zajic</author>
<author>R Schwartz</author>
</authors>
<title>Hedge: A parse-andtrim approach to headline generation.</title>
<date>2003</date>
<booktitle>In Proc. of DUC.</booktitle>
<contexts>
<context position="1482" citStr="Dorr et al., 2003" startWordPosition="205" endWordPosition="208">est model is one that exploits head-modifier bilexicalization to accurately distinguish adjuncts from complements, and that produces sentences that were judged more grammatical than those generated by previous work. 1 Introduction Sentence compression addresses the problem of removing words or phrases that are not necessary in the generated output of, for instance, summarization and question answering systems. Given the need to ensure grammatical sentences, a number of researchers have used syntax-directed approaches that perform transformations on the output of syntactic parsers (Jing, 2000; Dorr et al., 2003). Some of them (Knight and Marcu, 2000; Turner and Charniak, 2005) take an empirical approach, relying on formalisms equivalent to probabilistic synchronous context-free grammars (SCFG) *This material is based on research supported in part by the U.S. National Science Foundation (NSF) under Grant No. IIS-05-34871 and the Defense Advanced Research Projects Agency (DARPA) under Contract No. HR0011-06-C-0023. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the NSF or DARPA. (Lewis and Stearns</context>
</contexts>
<marker>Dorr, Zajic, Schwartz, 2003</marker>
<rawString>B. Dorr, D. Zajic, and R. Schwartz. 2003. Hedge: A parse-andtrim approach to headline generation. In Proc. of DUC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Jing</author>
</authors>
<title>Sentence reduction for automatic text summarization.</title>
<date>2000</date>
<booktitle>In Proc. of NAACL,</booktitle>
<pages>310--315</pages>
<contexts>
<context position="1462" citStr="Jing, 2000" startWordPosition="203" endWordPosition="204">r selected best model is one that exploits head-modifier bilexicalization to accurately distinguish adjuncts from complements, and that produces sentences that were judged more grammatical than those generated by previous work. 1 Introduction Sentence compression addresses the problem of removing words or phrases that are not necessary in the generated output of, for instance, summarization and question answering systems. Given the need to ensure grammatical sentences, a number of researchers have used syntax-directed approaches that perform transformations on the output of syntactic parsers (Jing, 2000; Dorr et al., 2003). Some of them (Knight and Marcu, 2000; Turner and Charniak, 2005) take an empirical approach, relying on formalisms equivalent to probabilistic synchronous context-free grammars (SCFG) *This material is based on research supported in part by the U.S. National Science Foundation (NSF) under Grant No. IIS-05-34871 and the Defense Advanced Research Projects Agency (DARPA) under Contract No. HR0011-06-C-0023. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the NSF or DARPA</context>
</contexts>
<marker>Jing, 2000</marker>
<rawString>H. Jing. 2000. Sentence reduction for automatic text summarization. In Proc. of NAACL, pages 310–315.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
</authors>
<title>PCFG models of linguistic tree representations.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>4</issue>
<contexts>
<context position="10852" citStr="Johnson, 1998" startWordPosition="1717" endWordPosition="1718">r scores to deletions of constituents that are grammatically more prominent. S NP ADVP VP . Figure 1: Penn Treebank tree with adjuncts in italic. occurrences are sparsely seen (e.g., “fell”-“from”). At a lower level, lexicalization is clearly desirable for pre-terminals. Indeed, current SCFG models such as K&amp;M have no direct way of preventing highly improbable single word removals, such as deletions of adverbs “never” or “nowhere”, which may turn a negative statement into a positive one.4 A second type of annotation that can be added to syntactic categories is the so-called parent annotation (Johnson, 1998), which was effectively used in syntactic parsing to break unreasonable context-free assumptions. For instance, a PP with a VP parent is marked as PPˆVP. It is reasonable to assume that, e.g., that constituents deep inside a PP have more chances to be removed than otherwise expected, and one may seek to increase the amount of vertical context that is available for conditioning each constituent deletion. To achieve the above desiderata for better SCFG probability estimates—i.e., reduce the amount of sister annotation within each SCFG production, by conditioning deletions on a context smaller th</context>
</contexts>
<marker>Johnson, 1998</marker>
<rawString>M. Johnson. 1998. PCFG models of linguistic tree representations. Computational Linguistics, 24(4):613–632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Joshi</author>
<author>L Levy</author>
<author>M Takahashi</author>
</authors>
<title>Tree adjunct grammar.</title>
<date>1975</date>
<journal>Journal of Computer and System Science,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="21393" citStr="Joshi et al., 1975" startWordPosition="3513" endWordPosition="3516">exploited more general tree productions known as synchronous tree substitution grammar (STSG) rules, in an approach quite similar to (Turner and Charniak, 2005). For instance, the STSG rule rooted at S can be decomposed into two SCFG productions if we allow unary rules such as VP —* VP to be freely added to the compressed tree. More specifically, we decompose any STSG rule that has in its target (compressed) RHS a single context free production, and that contains in its source (full) RHS a single context free production adjoined with any number of tree adjoining grammar (TAG) auxiliary trees (Joshi et al., 1975). In the figure, the initial tree is S —* NP VP, and the adjoined (auxiliary) tree is VP —* VP CC VP.6 We found this approach quite helpful, since most useful compressions that mimic TAG adjoining operations are missed by the extraction procedure of K&amp;M. Since we found that exploiting sentence pairs containing insertions had adverse consequences in terms of compression accuracies, we only report experiments with sentence pairs containing no insertions. We gathered sentence pairs with up to six substitutions using minimum edit distance matching (we will refer to these sets as ZD-0 to ZD-6). Wit</context>
</contexts>
<marker>Joshi, Levy, Takahashi, 1975</marker>
<rawString>A. Joshi, L. Levy, and M. Takahashi. 1975. Tree adjunct grammar. Journal of Computer and System Science, 21(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="2742" citStr="Klein and Manning, 2003" startWordPosition="393" endWordPosition="396">ract compression rules from aligned Penn Treebank (PTB) trees. While their approach proved successful, their reliance on standard maximum likelihood estimators for SCFG productions results in considerable sparseness issues, especially given the relative flat structure of PTB trees; in practice, many SCFG productions are seen only once. This problem is exacerbated for the compression task, which has only scarce training material available. In this paper, we present a head-driven Markovization of SCFG compression rules, an approach that was successfully used in syntactic parsing (Collins, 1999; Klein and Manning, 2003) to alleviate issues intrinsic to relative frequency estimation of treebank productions. Markovization for sentence compression provides several benefits, including the ability to condition deletions on a flexible amount of syntactic context, to treat head-modifier dependencies independently, and to lexicalize SCFG productions. Another part of our effort focuses on better alignment models for extracting SCFG compression rules from parallel data, and to improve upon (Knight and Marcu, 2000), who could only exploit 1.75% of the Ziff-Davis corpus because of stringent assumptions about human abstr</context>
<context position="4244" citStr="Klein and Manning, 2003" startWordPosition="609" endWordPosition="612">data, which greatly benefited the lexical probabilities incorporated into our Markovized SCFGs. Our work provides three main contributions: 180 Proceedings of NAACL HLT 2007, pages 180–187, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics (1) Our lexicalized head-driven Markovization yields more robust probability estimates, and our compressions outperform (Knight and Marcu, 2000) according to automatic and human evaluation. (2) We provide a comprehensive analysis of the impact of different Markov orders for sentence compression, similarly to a study done for PCFGs (Klein and Manning, 2003). (3) We provide a framework for exploiting document-abstract sentence pairs that are not purely compressive, and augment the available training resources for syntax-directed sentence compression systems. 2 Synchronous Grammars for Sentence Compression One successful syntax-driven approach (Knight and Marcu, 2000, henceforth K&amp;M) relies on synchronous context-free grammars (SCFG) (Lewis and Stearns, 1968; Aho and Ullman, 1969). SCFGs can be informally defined as context-free grammars (CFGs) whose productions have two right-hand side strings instead of one, namely source and target right-hand s</context>
<context position="6185" citStr="Klein and Manning, 2003" startWordPosition="913" endWordPosition="916">s, and more generally, to compressions produced by such grammars. Since the main point of our paper lies in the exploration of better probability estimates through Markovization and lexicalization of SCFGs, we first address the latter problem, and discuss the task of building synchronous derivations only later in Section 4. 2.1 Stochastic Synchronous Grammars The overall goal of a sentence compression system is to transform a given input sentence f into a concise and grammatical sentence c E C, which is a subsequence of f. Similarly to K&amp;M and many successful syntactic parsers (Collins, 1999; Klein and Manning, 2003), our sentence compression system is generative, and attempts to find the optimal compression c� by estimating the following function:1 c� = arg max { p(c f)� = arg max �p(f, c) } (1) cEC l cEC JJ If τ(f, c) is the set of all tree pairs that yield (f, c) according to some underlying SCFG, we can estimate the probability of the sentence pair using: p(f, c) = X P(πf, πc) (2) (πf,πc)Eτ(f,c) We note that, in practice (and as in K&amp;M), Equation 2 is often approximated by restricting τ(f, c) to a unique full tree orf, the best hypothesis of an off-the-shelf syntactic parser. This implies that each po</context>
<context position="9789" citStr="Klein and Manning, 2003" startWordPosition="1541" endWordPosition="1544">ch (POS), following work in syntactic parsing (Collins, 1999). This type of annotation is particular beneficial in the case of, e.g., prepositional phrases (PP), which may be either complement or adjunct. As in the case of Figure 1 (in which adjuncts appear in italic), knowing that the PP headed by “from” appears in a VP headed by “fell” helps us to determine that the PP is a complement to the verb “fell”, and that it should presumably not be deleted. Conversely, the PP headed by “because” modifying the same verb is an adjunct, and can safely be deleted if unimportant.3 Also, as discussed in (Klein and Manning, 2003), POS annotation can be useful as a means of backing off to more frequently occurring head-modifier POS occurrences (e.g., VBD-IN) when specific bilexical co2Details about the SCFG extraction procedure are given in Section 4. In short, we refer here to a grammar generated from 823 sentence pairs. 3The PP headed by “from” is an optional argument, and thus may still be deleted. Our point is that lexical information in general should help give lower scores to deletions of constituents that are grammatically more prominent. S NP ADVP VP . Figure 1: Penn Treebank tree with adjuncts in italic. occur</context>
<context position="11686" citStr="Klein and Manning, 2003" startWordPosition="1845" endWordPosition="1848"> deep inside a PP have more chances to be removed than otherwise expected, and one may seek to increase the amount of vertical context that is available for conditioning each constituent deletion. To achieve the above desiderata for better SCFG probability estimates—i.e., reduce the amount of sister annotation within each SCFG production, by conditioning deletions on a context smaller than an entire right-hand side, and at the same time increase the amount of ancestor and descendent annotation through parent (or ancestor) annotation and lexicalization—we follow the approach of (Collins, 1999; Klein and Manning, 2003), i.e., factorize n-ary grammar productions into products of n right-hand side probabilities, a technique sometimes called Markovization. Markovization is generally head-driven, i.e., reflects a decomposition centered around the head of each CFG production: l , AL- ··· L&apos;HR&apos; ··· RnA (4) 4K&amp;M incorporate lexical probabilities through n-gram models, but such language models are obviously not good for preventing such unreasonable deletions. RB PP NN . NP IN also Earning because NN NN NN JJ microchip demand period year-ago VBD fell IN from DT the NP PP IN of VBG slowing 182 where H is the head, L1</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>D. Klein and C. Manning. 2003. Accurate unlexicalized parsing. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
<author>D Marcu</author>
</authors>
<title>Statistics-based summarization — step one: Sentence compression.</title>
<date>2000</date>
<booktitle>In Proc. of AAAI.</booktitle>
<contexts>
<context position="1520" citStr="Knight and Marcu, 2000" startWordPosition="212" endWordPosition="215">d-modifier bilexicalization to accurately distinguish adjuncts from complements, and that produces sentences that were judged more grammatical than those generated by previous work. 1 Introduction Sentence compression addresses the problem of removing words or phrases that are not necessary in the generated output of, for instance, summarization and question answering systems. Given the need to ensure grammatical sentences, a number of researchers have used syntax-directed approaches that perform transformations on the output of syntactic parsers (Jing, 2000; Dorr et al., 2003). Some of them (Knight and Marcu, 2000; Turner and Charniak, 2005) take an empirical approach, relying on formalisms equivalent to probabilistic synchronous context-free grammars (SCFG) *This material is based on research supported in part by the U.S. National Science Foundation (NSF) under Grant No. IIS-05-34871 and the Defense Advanced Research Projects Agency (DARPA) under Contract No. HR0011-06-C-0023. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the NSF or DARPA. (Lewis and Stearns, 1968; Aho and Ullman, 1969) to extra</context>
<context position="3236" citStr="Knight and Marcu, 2000" startWordPosition="461" endWordPosition="464">n of SCFG compression rules, an approach that was successfully used in syntactic parsing (Collins, 1999; Klein and Manning, 2003) to alleviate issues intrinsic to relative frequency estimation of treebank productions. Markovization for sentence compression provides several benefits, including the ability to condition deletions on a flexible amount of syntactic context, to treat head-modifier dependencies independently, and to lexicalize SCFG productions. Another part of our effort focuses on better alignment models for extracting SCFG compression rules from parallel data, and to improve upon (Knight and Marcu, 2000), who could only exploit 1.75% of the Ziff-Davis corpus because of stringent assumptions about human abstractive behavior. To alleviate their restrictions, we rely on a robust approach for aligning trees of arbitrary document-abstract sentence pairs. After accounting for sentence pairs with both substitutions and deletions, we reached a retention of more than 25% of the Ziff-Davis data, which greatly benefited the lexical probabilities incorporated into our Markovized SCFGs. Our work provides three main contributions: 180 Proceedings of NAACL HLT 2007, pages 180–187, Rochester, NY, April 2007.</context>
<context position="4558" citStr="Knight and Marcu, 2000" startWordPosition="650" endWordPosition="653">ds more robust probability estimates, and our compressions outperform (Knight and Marcu, 2000) according to automatic and human evaluation. (2) We provide a comprehensive analysis of the impact of different Markov orders for sentence compression, similarly to a study done for PCFGs (Klein and Manning, 2003). (3) We provide a framework for exploiting document-abstract sentence pairs that are not purely compressive, and augment the available training resources for syntax-directed sentence compression systems. 2 Synchronous Grammars for Sentence Compression One successful syntax-driven approach (Knight and Marcu, 2000, henceforth K&amp;M) relies on synchronous context-free grammars (SCFG) (Lewis and Stearns, 1968; Aho and Ullman, 1969). SCFGs can be informally defined as context-free grammars (CFGs) whose productions have two right-hand side strings instead of one, namely source and target right-hand side. In the case of sentence compression, we restrict the target side to be a sub-sequence of the source side (possibly identical), and we will call this restricted grammar a deletion SCFG. For instance, a deletion SCFG rule that removes an adverbial phrase (ADVP) between an noun phrase (NP) and a verb phrase (VP</context>
</contexts>
<marker>Knight, Marcu, 2000</marker>
<rawString>K. Knight and D. Marcu. 2000. Statistics-based summarization — step one: Sentence compression. In Proc. of AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Lewis</author>
<author>R Stearns</author>
</authors>
<title>Syntax-directed transduction.</title>
<date>1968</date>
<journal>In Journal of the Association for Computing Machinery,</journal>
<volume>15</volume>
<pages>465--488</pages>
<contexts>
<context position="2088" citStr="Lewis and Stearns, 1968" startWordPosition="296" endWordPosition="299">orr et al., 2003). Some of them (Knight and Marcu, 2000; Turner and Charniak, 2005) take an empirical approach, relying on formalisms equivalent to probabilistic synchronous context-free grammars (SCFG) *This material is based on research supported in part by the U.S. National Science Foundation (NSF) under Grant No. IIS-05-34871 and the Defense Advanced Research Projects Agency (DARPA) under Contract No. HR0011-06-C-0023. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the NSF or DARPA. (Lewis and Stearns, 1968; Aho and Ullman, 1969) to extract compression rules from aligned Penn Treebank (PTB) trees. While their approach proved successful, their reliance on standard maximum likelihood estimators for SCFG productions results in considerable sparseness issues, especially given the relative flat structure of PTB trees; in practice, many SCFG productions are seen only once. This problem is exacerbated for the compression task, which has only scarce training material available. In this paper, we present a head-driven Markovization of SCFG compression rules, an approach that was successfully used in synt</context>
<context position="4651" citStr="Lewis and Stearns, 1968" startWordPosition="663" endWordPosition="666">0) according to automatic and human evaluation. (2) We provide a comprehensive analysis of the impact of different Markov orders for sentence compression, similarly to a study done for PCFGs (Klein and Manning, 2003). (3) We provide a framework for exploiting document-abstract sentence pairs that are not purely compressive, and augment the available training resources for syntax-directed sentence compression systems. 2 Synchronous Grammars for Sentence Compression One successful syntax-driven approach (Knight and Marcu, 2000, henceforth K&amp;M) relies on synchronous context-free grammars (SCFG) (Lewis and Stearns, 1968; Aho and Ullman, 1969). SCFGs can be informally defined as context-free grammars (CFGs) whose productions have two right-hand side strings instead of one, namely source and target right-hand side. In the case of sentence compression, we restrict the target side to be a sub-sequence of the source side (possibly identical), and we will call this restricted grammar a deletion SCFG. For instance, a deletion SCFG rule that removes an adverbial phrase (ADVP) between an noun phrase (NP) and a verb phrase (VP) may be written as follows: S —* (NP ADVP VP, NP VP) In a sentence compression framework sim</context>
</contexts>
<marker>Lewis, Stearns, 1968</marker>
<rawString>P. Lewis and R. Stearns. 1968. Syntax-directed transduction. In Journal of the Association for Computing Machinery, volume 15, pages 465–488.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="7568" citStr="Marcus et al., 1994" startWordPosition="1173" endWordPosition="1176">uation 2) is factored into probabilities of grammar productions. If θ is a derivation θ = r1 o · · · o rj · · · o rJ, where rj denotes the SCFG rule lj —* (αjf, αjc), we get: p(πf, πc) = YJ p(αjf, αjc lj) (3) j�1 The question we will now address is how to estimate the probability p(αj f, αjc lj) of each SCFG production. 2.2 Lexicalized Head-Driven Markovization of Synchronous Grammars A main issue in our enterprise is to reliably estimate productions of deletion SCFGs. In a sentence compression framework as the one presented by K&amp;M, we use aligned trees of the form of the Penn Treebank (PTB) (Marcus et al., 1994) to acquire and score SCFG productions. However, the use of the PTB structure faces many challenges also encountered in probabilistic parsing. 1In their noisy-channel approach, K&amp;M further break down p(c, f) into p(f|c) · p(c), which we refrain from doing for reasons that will become obvious later. 181 Firstly, PTB tree structures are relatively flat, particularly within noun phrases. For instance, adjective phrases (ADJP)—which are generally good candidates for deletions—appear in 90 different NProoted SCFG productions in Ziff-Davis,2 61 of which appear only once, e.g., NP , (DT ADJP JJ NN NN</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1994</marker>
<rawString>M. Marcus, B. Santorini, and M. Marcinkiewicz. 1994. Building a large annotated corpus of english: The penn treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
</authors>
<title>Discriminative sentence compression with soft syntactic constraints.</title>
<date>2006</date>
<booktitle>In Proc. of EACL.</booktitle>
<contexts>
<context position="29598" citStr="McDonald, 2006" startWordPosition="4898" endWordPosition="4899">ut not always, good candidates for deletions (e.g., ADJP in Sentence 2, PP in sentences 3 and 4). On the other hand, our model keeps these constituent intact. Finally, the fifth and last example is one of the only three cases (among the 32 sentences) where our model produced a sentence we judged clearly ungrammatical. After inspection, we found that our parser assigned particularly errorful trees to those inputs, which may partially explain these ungrammatical outputs. 6 Related Work A relatively large body of work addressed the problem of sentence compression. One successful recent approach (McDonald, 2006) combines a discriminative framework with a set of features that capture information similar to the K&amp;M model. Mc186 Input Many debugging features, including user-defined breakpoints NoisyC and variable-watching and message-watching windows, have Markov been added. Human Many debugging features, including user-defined points and variable-watching and message-watching windows, have been added. Many debugging features have been added. Many debugging features have been added. Input The chemical etching process usedforglare protection is effecNoisyC tive and will help ifyour office has the fluores</context>
</contexts>
<marker>McDonald, 2006</marker>
<rawString>R. McDonald. 2006. Discriminative sentence compression with soft syntactic constraints. In Proc. of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Turner</author>
<author>E Charniak</author>
</authors>
<title>Supervised and unsupervised learning for sentence compression.</title>
<date>2005</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="1548" citStr="Turner and Charniak, 2005" startWordPosition="216" endWordPosition="219">ion to accurately distinguish adjuncts from complements, and that produces sentences that were judged more grammatical than those generated by previous work. 1 Introduction Sentence compression addresses the problem of removing words or phrases that are not necessary in the generated output of, for instance, summarization and question answering systems. Given the need to ensure grammatical sentences, a number of researchers have used syntax-directed approaches that perform transformations on the output of syntactic parsers (Jing, 2000; Dorr et al., 2003). Some of them (Knight and Marcu, 2000; Turner and Charniak, 2005) take an empirical approach, relying on formalisms equivalent to probabilistic synchronous context-free grammars (SCFG) *This material is based on research supported in part by the U.S. National Science Foundation (NSF) under Grant No. IIS-05-34871 and the Defense Advanced Research Projects Agency (DARPA) under Contract No. HR0011-06-C-0023. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the NSF or DARPA. (Lewis and Stearns, 1968; Aho and Ullman, 1969) to extract compression rules from al</context>
<context position="20934" citStr="Turner and Charniak, 2005" startWordPosition="3430" endWordPosition="3433"> second[6] NP[15] unit[8] 184 other than minimum tree edit distance may be effective, we found—after manual inspections of alignments between sentences with less than five nondeleting edits—that this method generally produces good alignments. A sample alignment is provided in Figure 2. Once a constituent alignment is available, it is then trivial to extract all deletion SCFG rules available in a tree pair, e.g., NP —* (DT JJ NN, DT JJ NN) in the figure. We also exploited more general tree productions known as synchronous tree substitution grammar (STSG) rules, in an approach quite similar to (Turner and Charniak, 2005). For instance, the STSG rule rooted at S can be decomposed into two SCFG productions if we allow unary rules such as VP —* VP to be freely added to the compressed tree. More specifically, we decompose any STSG rule that has in its target (compressed) RHS a single context free production, and that contains in its source (full) RHS a single context free production adjoined with any number of tree adjoining grammar (TAG) auxiliary trees (Joshi et al., 1975). In the figure, the initial tree is S —* NP VP, and the adjoined (auxiliary) tree is VP —* VP CC VP.6 We found this approach quite helpful, </context>
</contexts>
<marker>Turner, Charniak, 2005</marker>
<rawString>J. Turner and E. Charniak. 2005. Supervised and unsupervised learning for sentence compression. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Zhang</author>
<author>D Shasha</author>
</authors>
<title>Simple fast algorithms for the editing distance between trees and related problems.</title>
<date>1989</date>
<journal>SIAM J. Comput.,</journal>
<volume>18</volume>
<issue>6</issue>
<marker>Zhang, Shasha, 1989</marker>
<rawString>K. Zhang and D. Shasha. 1989. Simple fast algorithms for the editing distance between trees and related problems. SIAM J. Comput., 18(6):1245–1262.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>