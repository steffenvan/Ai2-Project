<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000014">
<title confidence="0.996589">
A Model of Lexical Attraction and Repulsion*
</title>
<author confidence="0.998749">
Doug Beeferman Adam Berger John Lafferty
</author>
<affiliation confidence="0.848068">
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213 USA
</affiliation>
<email confidence="0.946604">
&lt;dougb,aberger,laffertyncs.cmu.edu
</email>
<sectionHeader confidence="0.990653" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999865318181818">
This paper introduces new methods based
on exponential families for modeling the
correlations between words in text and
speech. While previous work assumed the
effects of word co-occurrence statistics to
be constant over a window of several hun-
dred words, we show that their influence
is nonstationary on a much smaller time
scale. Empirical data drawn from En-
glish and Japanese text, as well as conver-
sational speech, reveals that the &amp;quot;attrac-
tion&amp;quot; between words decays exponentially,
while stylistic and syntactic contraints cre-
ate a &amp;quot;repulsion&amp;quot; between words that dis-
courages close co-occurrence. We show
that these characteristics are well described
by simple mixture models based on two-
stage exponential distributions which can
be trained using the EM algorithm. The
resulting distance distributions can then be
incorporated as penalizing features in an
exponential language model.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.98698626923077">
One of the fundamental characteristics of language,
viewed as a stochastic process, is that it is highly
nonstationary. Throughout a written document
and during the course of spoken conversation, the
topic evolves, effecting local statistics on word oc-
currences. The standard trigram model disregards
this nonstationarity, as does any stochastic grammar
which assigns probabilities to sentences in a context-
independent fashion.
&apos;Research supported in part by NSF grant IRI-
9314969, DARPA AASERT award DAAH04-95-1-0475,
and the ATR Interpreting Telecommunications Research
Laboratories.
Stationary models are used to describe such a dy-
namic source for at least two reasons. The first is
convenience: stationary models require a relatively
small amount of computation to train and to apply.
The second is ignorance: we know so little about
how to model effectively the nonstationary charac-
teristics of language that we have for the most part
completely neglected the problem. From a theoreti-
cal standpoint, we appeal to the Shannon-McMillan-
Breiman theorem (Cover and Thomas, 1991) when-
ever computing perplexities on test data; yet this
result only rigorously applies to stationary and er-
godic sources.
To allow a language model to adapt to its recent
context, some researchers have used techniques to
update trigram statistics in a dynamic fashion by
creating a cache of the most recently seen n-grams
which is smoothed together (typically by linear in-
terpolation) with the static model; see for example
(Jelinek et al., 1991; Kuhn and de Mori, 1990). An-
other approach, using maximum entropy methods
similar to those that we present here, introduces a
parameter for trigger pairs of mutually informative
words, so that the occurrence of certain words in re-
cent context boosts the probability of the words that
they trigger (Rosenfeld, 1996). Triggers have also
been incorporated through different methods (Kuhn
and de Mori, 1990; Ney, Essen, and Kneser, 1994).
All of these techniques treat the recent context as a
&amp;quot;bag of words,&amp;quot; so that a word that appears, say, five
positions back makes the same contribution to pre-
diction as words at distances of 50 or 500 positions
back in the history.
In this paper we introduce new modeling tech-
niques based on exponential families for captur-
ing the long-range correlations between occurrences
of words in text and speech. We show how for
both written text and conversational speech, the
empirical distribution of the distance between trig-
</bodyText>
<page confidence="0.998504">
373
</page>
<bodyText confidence="0.999967266666667">
ger words exhibits a striking behavior in which the
&amp;quot;attraction&amp;quot; between words decays exponentially,
while stylistic and syntactic constraints create a &amp;quot;re-
pulsion&amp;quot; between words that discourages close co-
occurrence.
We have discovered that this observed behavior
is well described by simple mixture models based on
two-stage exponential distributions. Though in com-
mon use in queueing theory, such distributions have
not, to our knowledge, been previously exploited
in speech and language processing. It is remark-
able that the behavior of a highly complex stochas-
tic process such as the separation between word co-
occurrences is well modeled by such a simple para-
metric family, just as it is surprising that Zipf&apos;s law
can so simply capture the distribution of word fre-
quencies in most languages.
In the following section we present examples of the
empirical evidence for the effects of distance. In Sec-
tion 3 we outline the class of statistical models that
we propose to model this data. After completing
this work we learned of a related paper (Niesler and
Woodland, 1997) which constructs similar models.
In Section 4 we present a parameter estimation algo-
rithm, based on the EM algorithm, for determining
the maximum likelihood estimates within the class.
In Section 5 we explain how distance models can be
incorporated into an exponential language model,
and present sample perplexity results we have ob-
tained using this class of models.
</bodyText>
<sectionHeader confidence="0.946691" genericHeader="method">
2 The Empirical Evidence
</sectionHeader>
<bodyText confidence="0.99988165">
The work described in this paper began with the
goal of building a statistical language model using
a static trigram model as a &amp;quot;prior,&amp;quot; or default dis-
tribution, and adding certain features to a family of
conditional exponential models to capture some of
the nonstationary features of text. The features we
used were simple &amp;quot;trigger pairs&amp;quot; of words that were
chosen on the basis of mutual information. Figure 1
provides a small sample of the 41,263 (s,t) trigger
pairs used in most of the experiments we will de-
scribe.
In earlier work, for example (Rosenfeld, 1996), the
distance between the words of a trigger pair (s, t)
plays no role in the model, meaning that the &amp;quot;boost&amp;quot;
in probability which t receives following its trigger s
is independent of how long ago s occurred, so long
as s appeared somewhere in the history H, a fixed-
length window of words preceding t. It is reasonable
to expect, however, that the relevance of a word s to
the identity of the next word should decay as s falls
</bodyText>
<figure confidence="0.964110625">
S -t
Ms. her
changes revisions
energy gas
committee representative
board board
lieutenant colonel
AIDS AIDS
Soviet missiles
underwater diving
patients drugs
television airwaves
Voyager Neptune
medical surgical
I me
Gulf Gulf
</figure>
<figureCaption confidence="0.988803">
Figure 1: A sample of the 41,263 trigger pairs ex-
tracted from the 38 million word Wall Street Journal
corpus.
</figureCaption>
<table confidence="0.951715">
.9 t
NIA OAIN
UN Security Council
Xii * t3 7 v 1-34
electricity kilowatt
in* ii`2-14EK
election small electoral district
tAt ea
silk cocoon
itta 215t
court imprisonment
Hungary Bulgaria
El 7ox no
Japan Air to fly cargo
M *lff1
sentence proposed punishment
WO tf*
transplant organ
A44: tik
forest wastepaper
= v -.--— *A 1-
computer host
</table>
<figureCaption confidence="0.9727145">
Figure 2: A sample of triggers extracted from the
33 million word Nikkei corpus.
</figureCaption>
<bodyText confidence="0.999794428571429">
further and further back into the context. Indeed,
there are tables in (Rosenfeld, 1996) which suggest
that this is so, and distance-dependent &amp;quot;memory
weights&amp;quot; are proposed in (Ney, Essen, and Kneser,
1994). We decided to investigate the effect of dis-
tance in more detail, and were surprised by what
we found.
</bodyText>
<page confidence="0.996113">
374
</page>
<table confidence="0.983181454545455">
0.014 0015
0.015 0.01
0.01 0.1041
0.005 0.001
0.005
0.034
0.002
0.004 •)&amp;quot;,
0.002 • 744 .
&amp;quot;f4414*44.04szoamit•mmo
50 100 so 203 250 100 350 403 50 103 ISO 203 340 301 360 401
</table>
<figureCaption confidence="0.579504333333333">
Figure 3: The observed distance distributions—collected from five million words of the Wall Street Journal
corpus—for one of the non-self trigger groups (left) and one of the self trigger groups (right). For a given
distance 0 &lt; k &lt; 400 on the x-axis, the value on the y-axis is the empirical probability that two trigger words
within the group are separated by exactly k + 2 words, conditional on the event that they co-occur within
a 400 word window. (We exclude separation of one or two words because of our use of distance models to
improve upon trigrams.)
</figureCaption>
<bodyText confidence="0.999887466666667">
The set of 41,263 trigger pairs was partitioned
into 20 groups of non-self triggers (s,t), s t, such
as (Soviet, Kremlin &apos; s), and 20 groups of self trig-
gers (s, s), such as (business, business). Figure 3
displays the empirical probability that a word t ap-
pears for the first time k words after the appearance
of its mate s in a trigger pair (s, t), for two repre-
sentative groups.
The curves are striking in both their similarities
and their differences. Both curves seem to have more
or less flattened out by N = 400, which allows us to
make the approximating assumption (of great prac-
tical importance) that word-triggering effects may
be neglected after several hundred words. The most
prominent distinction between the two curves is the
peak near k = 25 in the self trigger plots; the non-
self trigger plots suggest a monotonic decay. The
shape of the self trigger curve, in particular the rise
between k = 1 and k r.s. 25, reflects the stylistic and
syntactic injunctions against repeating a word too
soon. This effect, which we term the lexical exclu-
sion principle, does not appear for non-self triggers.
In general, the lexical exclusion principle seems to
be more in effect for uncommon words, and thus the
peak for such words is shifted further to the right.
While the details of the curves vary depending on
the particular triggers, this behavior appears to be
universal. For triggers that appear too few times in
the data for this behavior to exhibit itself, the curves
emerge when the counts are pooled with those from
a collection of other rare words. An example of this
law of large numbers is shown in Figure 4.
These empirical phenomena are not restricted to
the Wall Street Journal corpus. In fact, we have ob-
served similar behavior in conversational speech and
Japanese text. The corresponding data for self trig-
gers in the Switchboard data (Godfrey, Holliman,
and McDaniel, 1992), for instance, exhibits the same
bump in p(k) for small k, though the peak is closer
to zero. The lexical exclusion principle, then, seems
to be less applicable when two people are convers-
ing, perhaps because the stylistic concerns of written
communication are not as important in conversation.
Several examples from the Switchboard and Nikkei
corpora are shown in Figure 5.
</bodyText>
<sectionHeader confidence="0.998778" genericHeader="method">
3 Exponential Models of Distance
</sectionHeader>
<bodyText confidence="0.999946857142857">
The empirical data presented in the previous section
exhibits three salient characteristics. First is the de-
cay of the probability of a word t as the distance
k from the most recent occurrence of its mate s in-
creases. The most important (continuous-time) dis-
tribution with this property is the single-parameter
exponential family
</bodyText>
<equation confidence="0.962754">
Mx) =
</equation>
<bodyText confidence="0.999928666666667">
(We&apos;ll begin by showing the continuous analogues
of the discrete formulas we actually use, since they
are simpler in appearance.) This family is uniquely
characterized by the memoryless property that the
probability of waiting an additional length of time
At is independent of the time elapsed so far, and
</bodyText>
<page confidence="0.98813">
375
</page>
<figure confidence="0.734442">
tt•gtoiva...Q.E.d.r.1
</figure>
<figureCaption confidence="0.9349795">
Figure 4: The law of large numbers emerging for distance distributions. Each plot shows the empirical
distance curve for a collection of self triggers, each of which appears fewer than 100 times in the entire 38
million word Wall Street Journal corpus. The plots include statistics for 10, 50, 500, and all 2779 of the self
triggers which occurred no more than 100 times each.
</figureCaption>
<figure confidence="0.996504303030303">
000 0014
0.0111 0612
02214 • I
0012
001 0.60
0.101. 0.201
000
0201
0202
6002
6062
60 100 10 00 260 MO 310 .0 103 1.0 20, no 010 260 407
06111
020st 0010
0.00
0211
002
0.616
002 ••• 0.01 *),
• 0200 0
0209
0.201
op.
260 303 0 0 100 SO 203 20,
am
atns
0221
020
100
160
203
260
203
</figure>
<figureCaption confidence="0.9525578">
Figure 5: Empirical distance distributions of triggers in the Japanese Nikkei corpus, and the Switchboard
corpus of conversational speech. Upper row: All non-self (left) and self triggers (middle) appearing fewer
than 100 times in the Nikkei corpus, and the curve for the possessive particle CO (right). Bottom row:
self trigger UK (left), YOU-KNOW (middle), and all self triggers appearing fewer than 100 times in the entire
Switchboard corpus (right).
</figureCaption>
<bodyText confidence="0.997272">
the distribution p, has mean 1/p and variance 1/p2.
This distribution is a good candidate for modeling
non-self triggers.
</bodyText>
<figure confidence="0.6686">
C
</figure>
<figureCaption confidence="0.999729">
Figure 6: A two-stage queue
</figureCaption>
<bodyText confidence="0.9998904">
The second characteristic is the bump between 0
and 25 words for self triggers. This behavior appears
when two exponential distributions are arranged in
serial, and such distributions are an important tool
in the &amp;quot;method of stages&amp;quot; in queueing theory (Klein-
rock, 1975). The time it takes to travel through two
service facilities arranged in serial, where the first
provides exponential service with rate pi and the
second provides exponential service with rate p2, is
simply the convolution of the two exponentials:
</bodyText>
<equation confidence="0.943675">
e-Pite(&apos;-&apos;)dt
PPI,m2(x) P1142 f
ii1P2 p —&apos;s
- e0 x ) P1 0 1L2
P2 - /41
</equation>
<bodyText confidence="0.9999014">
The mean and variance of the two-stage exponen-
tial p,, are 1/pi + 1/p2 and 1/p? + 1/4 respec-
tively. As pi (or, by symmetry, p2) gets large, the
peak shifts towards zero and the distribution ap-
proaches the single-parameter exponential pm, (by
</bodyText>
<page confidence="0.996303">
376
</page>
<figureCaption confidence="0.708105666666667">
symmetry, poi). A sequence of two-stage models is
shown in Figure 7.
Figure 7: A sequence of two-stage exponential mod-
</figureCaption>
<bodyText confidence="0.964618833333333">
els PAI,p,(x) with pi = 0.01, 0.02, 0.06, 0.2, oo and
= 0.01.
The two-stage exponential is a good candidate for
distance modeling because of its mathematical prop-
erties, but it is also well-motivated for linguistic rea-
sons. The first queue in the two-stage model rep-
resents the stylistic and syntactic constraints that
prevent a word from being repeated too soon. After
this waiting period, the distribution falls off expo-
nentially, with the memoryless property. For non-
self triggers, the first queue has a waiting time of
zero, corresponding to the absence of linguistic con-
straints against using t soon after s when the words
s and t are different. Thus, we are directly model-
ing the &amp;quot;lexical exclusion&amp;quot; effect and long-distance
decay that have been observed empirically.
The third artifact of the empirical data is the ten-
dency of the curves to approach a constant, positive
value for large distances. While the exponential dis-
tribution quickly approaches zero, the empirical data
settles down to a nonzero steady-state value.
Together these three features suggest modeling
distance with a three-parameter family of distribu-
tions:
</bodyText>
<equation confidence="0.967971">
Ppl,p2,c(x)= --1 (pp,,p,(x) c)
</equation>
<bodyText confidence="0.999825333333333">
where c &gt; 0 and 7 is a normalizing constant.
Rather than a continuous-time exponential, we use
the discrete-time analogue
</bodyText>
<equation confidence="0.977121">
pp(k) = (1- 14) e-18k
</equation>
<bodyText confidence="0.8229475">
In this case, the two-stage model becomes the
discrete-time convolution
</bodyText>
<equation confidence="0.9387045">
ppi,.(k) = E (i) Pm2(k - t)
t=0
</equation>
<bodyText confidence="0.983087">
Remark. It should be pointed out that there is
another parametric family that is an excellent can-
didate for distance models, based on the first two
features noted above. This is the Gamma distribu-
tion
</bodyText>
<equation confidence="0.99162">
pa,p(x) = — e-flx .
F(a)
</equation>
<bodyText confidence="0.999975142857143">
This distribution has mean a/16 and variance a/02
and thus can afford greater flexibility in fitting the
empirical data. For Bayesian analysis, this distribu-
tion is appropriate as the conjugate prior for the ex-
ponential parameter p (Gelman et al., 1995). Using
this family, however, sacrifices the linguistic inter-
pretation of the two-stage model.
</bodyText>
<sectionHeader confidence="0.981811" genericHeader="method">
4 Estimating the Parameters
</sectionHeader>
<bodyText confidence="0.9987685">
In this section we present a solution to the problem
of estimating the parameters of the distance models
introduced in the previous section. We use the max-
imum likelihood criterion to fit the curves. Thus, if
Ee represents the parameters of our model, and
/3(k) is the empirical probability that two triggers
appear a distance of k words apart, then we seek to
maximize the log-likelihood
</bodyText>
<equation confidence="0.9816175">
L(0) = Ep(k)log pe(k) .
k&gt;0
</equation>
<bodyText confidence="0.983920833333333">
First suppose that {pe}oEe is the family of continu-
ous one-stage exponential models pp(k) = Pk .
In this case the maximum likelihood problem is
straightforward: the mean is the sufficient statistic
for this exponential family, and its maximum likeli-
hood estimate is determined by
</bodyText>
<equation confidence="0.879632166666667">
1 1
= Ek k 13(k) = E [k]
In the case where we instead use the discrete model
pp(k) = (1- P) Pk , a little algebra shows that
the maximum likelihood estimate is then
Ef [k]) •
</equation>
<bodyText confidence="0.997944666666667">
Now suppose that our parametric family {p9}0Eo
is the collection of two-stage exponential models; the
log-likelihood in this case becomes
</bodyText>
<equation confidence="0.988887">
C(ill = 13(k) log E p ts2(k - j) .
k&gt;0 1=0
</equation>
<bodyText confidence="0.9750095">
Here it is not obvious how to proceed to obtain the
maximum likelihood estimates. The difficulty is that
there is a sum inside the logarithm, and direct dif-
ferentiation results in coupled equations for pi and
</bodyText>
<figure confidence="0.99558775">
0.01
0.000
0.030
0.401
0.005
0.405
0.004
0.003
0.032
0.001
100 150 200 250 NO 350 400 450 KO
p = log (1 + 1
</figure>
<page confidence="0.991032">
377
</page>
<bodyText confidence="0.999691090909091">
P2. Our solution to this problem is to view the con-
volving index j as a hidden variable and apply the
EM algorithm (Dempster, Laird, and Rubin, 1977).
Recall that the interpretation of j is the time used
to pass through the first queue; that is, the number
of words used to satisfy the linguistic constraints of
lexical exclusion. This value is hidden given only the
total time k required to pass through both queues.
Applying the standard EM argument, the dif-
ference in log-likelihood for two parameter pairs
(pc, p12) and (pi, p2) can be bounded from below as
</bodyText>
<equation confidence="0.9984304">
£041) — C(p) = E i5(k) log ( &apos;
pp, pi (k))
k&gt;0 PNi,N2(k)
Psii,p2k^,, .1)
A(141,12)
</equation>
<bodyText confidence="0.701675">
where
</bodyText>
<equation confidence="0.9269534">
Poi ,p3(k .i) = Pol,(..7)Pp2(k — :7)
and
ppi,p2ci k) = PAI,i12(k
Ppia.12(k) •
auxiliary function A can be written as
log (1 — e—P11) + log (1 — e—Pi2)
- EI-5(k)Eip1,.(i k)
k&gt;0 j=0
- Ei3(k)E(k —i)Ppi,p2(j1k)
k&gt;0 j=0
+ constant(p) .
Differentiating A(p1 , p) with respect to p, we get
the EM updates
= log (1 + 1
k •
Ek&gt;0 P(k)Ej=0 3 731,1)A2(i 1k)
p&apos;2 = log ( +
1
Ic
k&gt;0 13(k)Ej=0(k i)PP1,142(i 1k)
</equation>
<bodyText confidence="0.950048272727273">
Remark. It appears that the above updates re-
quire 0(N2) operations if a window of N words
is maintained in the history. However, us-
ing formulas for the geometric series, such as
Er_o kxk = x/(1 — x)2, we can write the expec-
tation Ejk.=0 jppi,p,(j1k) in closed form. Thus, the
updates can be calculated in linear time.
Finally, suppose that our parametric family
{po}eEe is the three-parameter collection of two-.
stage exponential models together with an additive
constant:
</bodyText>
<equation confidence="0.996063">
Pisi,p2,c(k) = 7-1(Pp1,p2(k)+ c) •
</equation>
<bodyText confidence="0.998915">
Here again, the maximum likelihood problem can
be solved by introducing a hidden variable. In par-
ticular, by setting a = we can express this
model as a mixture of a two-stage exponential and
a uniform distribution:
</bodyText>
<equation confidence="0.9769455">
1
= (1 — a) Ppt,p2(k) + a (T) •
</equation>
<bodyText confidence="0.7921886">
Thus, we can again apply the EM algorithm to de-
termine the mixing parameter a. This is a standard
application of the EM algorithm, and the details are
omitted.
In summary, we have shown how the EM algo-
rithm can be applied to determine maximum like-
lihood estimates of the three-parameter family of
distance models {ppi,p2,„} of distance models. In
Figure 8 we display typical examples of this training
algorithm at work.
</bodyText>
<sectionHeader confidence="0.970184" genericHeader="method">
5 A Nonstationary Language Model
</sectionHeader>
<bodyText confidence="0.999961666666667">
To incorporate triggers and distance models into
a long-distance language model, we begin by
constructing a standard, static backoff trigram
model (Katz, 1987), which we will denote as
q(wolw--1,w-2). For the purposes of building a
model for the Wall Street Journal data, this trigram
model is quickly trained on the entire 38-million
word corpus. We then build a family of conditional
exponential models of the general form
</bodyText>
<equation confidence="0.989134">
p(w I H) =
1
z (H) exp (E Ai fi(H , w)) q(w I w_i, w_2)
</equation>
<bodyText confidence="0.999175">
where H = w—i, w-2, • w—N is the word history,
and Z(H) is the normalization constant
</bodyText>
<equation confidence="0.921145">
Z(H) = Eexp Aifi(H, w)) q(w w—i w_2)
</equation>
<bodyText confidence="0.999147333333333">
The functions ft, which depend both on the word
history H and the word being predicted, are called
features, and each feature fi is assigned a weight Ai.
In the models that we built, feature fi is an indicator
function, testing for the occurrence of a trigger pair
(si, ti):
</bodyText>
<equation confidence="0.866636">
fi(H,w) = 01
</equation>
<bodyText confidence="0.9997875">
The use of the trigram model as a default dis-
tribution (Csiszar, 1996) in this manner is new in
language modeling. (One might also use the term
prior, although q(w H) is not a prior in the strict
Bayesian sense.) Previous work using maximum en-
tropy methods incorporated trigram constraints as
</bodyText>
<figure confidence="0.990477666666667">
k&gt;0 j=0
Thus, the
1
if si E H and w = ti
otherwise.
378
0.012
0.01
0.206
0.006
0.036
0.032
SO
150
100
203
250
250
350
400
•
&apos; . . •
• -14.4.4214-i4.24444.6.40,N,,
•
</figure>
<figureCaption confidence="0.666776666666667">
Figure 8: The same empirical distance distributions of Figure 2 fit to the three-parameter mixture model
pi„,i„,„,, using the EM algorithm. The dashed line is the fitted curve. For the non-self trigger plot pi = 7,
P2 = 0.0148, and a = 0.253. For the self trigger plot Pi = 0.29, p2 = 0.0168, and a = 0.224.
</figureCaption>
<bodyText confidence="0.999953545454545">
explicit features (Rosenfeld, 1996), using the uni-
form distribution as the default model. There are
several advantages to incorporating trigrams in this
way. The trigram component can be efficiently con-
structed over a large volume of data, using standard
software or including the various sophisticated tech-
niques for smoothing that have been developed. Fur-
thermore, the normalization Z(H) can be computed
more efficiently when trigrams appear in the default
distribution. For example, in the case of trigger fea-
tures, since
</bodyText>
<equation confidence="0.864756">
Z(H) = 1 + E 6(si E H)(eA• — 1)g(ti I w w _2)
</equation>
<bodyText confidence="0.999772428571429">
the normalization involves only a sum over those
words that are actively triggered. Finally, assuming
robust estimates for the parameters Ai, the resulting
model is essentially guaranteed to be superior to the
trigram model. The training algorithm we use for
estimating the parameters is the Improved Iterative
Scaling (HS) algorithm introduced in (Della Pietra,
Della Pietra, and Lafferty, 1997).
To include distance models in the word predic-
tions, we treat the distribution on the separation k
between si and ti in a trigger pair (si,ti) as a prior.
Suppose first that our distance model is a simple
one-parameter exponential, p(k I si E H, w = ti) =
pi ek . Using Bayes&apos; theorem, we can then write
</bodyText>
<equation confidence="0.821646">
p(w = ti 181 E H, si = w_k)
p(w = ti I si E H) p(k I si E H ,w = ti)
p(k I si EH)
oc •
</equation>
<bodyText confidence="0.999905545454545">
Thus, the distance dependence is incorporated as a
penalizing feature, the effect of which is to discour-
age a large separation between si and ti. A simi-
lar interpretation holds when the two-stage mixture
models are used to model distance, but the
formulas are more complicated.
In this fashion, we first trained distance models
using the algorithm outlined in Section 4. We then
incorporated the distance models as penalizing fea-
tures, whose parameters remained fixed, and pro-
ceeded to train the trigger parameters Ai using the
IIS algorithm. Sample perplexity results are tabu-
lated in Figure 9.
One important aspect of these results is that be-
cause a smoothed trigram model is used as a de-
fault distribution, we are able to bucket the trigger
features and estimate their parameters on a modest
amount of data. The resulting calculation takes only
several hours on a standard workstation, in com-
parison to the machine-months of computation that
previous language models of this type required.
The use of distance penalties gives only a small
improvement, in terms of perplexity, over the base-
line trigger model. However, we have found that
the benefits of distance modeling can be sensitive to
configuration of the trigger model. For example, in
the results reported in Table 9, a trigger is only al-
lowed to be active once in any given context. By
instead allowing multiple occurrences of a trigger s
to contribute to the prediction of its mate t, both
the perplexity reduction over the baseline trigram
and the relative improvements due to distance mod-
eling are increased.
</bodyText>
<page confidence="0.993972">
379
</page>
<table confidence="0.512840571428571">
Experiment Perplexity Reduction
Baseline: trigrams trained on 5M words 170 —
Trigram prior + 41,263 triggers 145 14.7%
Same as above + distance modeling 142 16.5%
Baseline: trigrams trained on 38M words 107 —
Trigram prior + 41,263 triggers 92 14.0%
Same as above + distance modeling 90 15.9%
</table>
<figureCaption confidence="0.949734">
Figure 9: Models constructed using trigram priors. Training the larger model required about 10 hours on a
DEC Alpha workstation.
</figureCaption>
<sectionHeader confidence="0.997232" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999991941176471">
We have presented empirical evidence showing that
the distribution of the distance between word pairs
that have high mutual information exhibits a strik-
ing behavior that is well modeled by a three-
parameter family of exponential models. The prop-
erties of these co-occurrence statistics appear to be
exhibited universally in both text and conversational
speech. We presented a training algorithm for this
class of distance models based on a novel applica-
tion of the EM algorithm. Using a standard backoff
trigram model as a default distribution, we built a
class of exponential language models which use non-
stationary features based on trigger words to allow
the model to adapt to the recent context, and then
incorporated the distance models as penalizing fea-
tures. The use of distance modeling results in an
improvement over the baseline trigger model.
</bodyText>
<sectionHeader confidence="0.975546" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.99991525">
We are grateful to Fujitsu Laboratories, and in par-
ticular to Akira Ushioda, for providing access to the
Nikkei corpus within Fujitsu Laboratories, and as-
sistance in extracting Japanese trigger pairs.
</bodyText>
<sectionHeader confidence="0.998775" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998840177777778">
Berger, A., S. Della Pietra, and V. Della Pietra. 1996. A
maximum entropy approach to natural language pro-
cessing. Computational Linguistics, 22(1):39-71.
Cover, T.M. and J.A. Thomas. 1991. Elements of In-
formation Theory. John Wiley.
Csiszax, I. 1996. Maxent, mathematics, and information
theory. In K. Hanson and R. Silver, editors, Max-
imum Entropy and Bayesian Methods. Kluwer Aca-
demic Publishers.
DeUa Pietra, S., V. Della Pietra, and J. Lafferty. 1997.
Inducing features of random fields. IEEE Trans.
on Pattern Analysis and Machine Intelligence, 19(3),
March.
Dempster, A.P., N.M. Laird, and D.B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society,
39(B):1-38.
Gelman, A., J. Carlin, H. Stern, and D. Rubin. 1995.
Bayesian Data Analysis. Chapman Sz Hall, London.
Godfrey, J., E. Badman, and J. McDaniel. 1992.
SWITCHBOARD: Telephone speech corpus for re-
search and development. In Proc. ICASSP-92.
Jelinek, F., B. Merialdo, S. Roukos, and M. Strauss.
1991. A dynamic language model for speech recog-
nition. In Proceedings of the DARPA Speech and Nat-
ural Language Workshop, pages 293-295, February.
Katz, S. 1987. Estimation of probabilities from sparse
data for the langauge model component of a. speech
recognizer. IEEE Transactions on Acoustics, Speech
and Signal Processing, ASSP-35(3):400-401, March.
Kleinrock, L. 1975. Queueing Systems. Volume I: The-
ory. Wiley, New York.
Kuhn, R. and R. de Mori. 1990. A cache-based nat-
ural language model for speech recognition. IEEE
Trans. on Pattern Analysis and Machine Intelligence,
12:570-583.
Ney, H., U. Essen, and R. Kneser. 1994. On structur-
ing probabilistic dependencies in stochastic language
modeling. Computer Speech and Language, 8:1-38.
Niesler, T. and P. Woodland. 1997. Modelling word-
pair relations in a category-based language model. In
Proceedings of ICASSP-97, Munich, Germany, April.
Rosenfeld, R. 1996. A maximum entropy approach
to adaptive statistical language modeling. Computer
Speech and Language, 10:187-228.
</reference>
<page confidence="0.998254">
380
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.563783">
<title confidence="0.999391">A Model of Lexical Attraction and Repulsion*</title>
<author confidence="0.999964">Doug Beeferman Adam Berger John Lafferty</author>
<affiliation confidence="0.9999445">School of Computer Science Carnegie Mellon University</affiliation>
<address confidence="0.569447">Pittsburgh, PA 15213 USA</address>
<email confidence="0.999733"><dougb,aberger,laffertyncs.cmu.edu</email>
<abstract confidence="0.999572043478261">This paper introduces new methods based on exponential families for modeling the correlations between words in text and speech. While previous work assumed the effects of word co-occurrence statistics to be constant over a window of several hundred words, we show that their influence is nonstationary on a much smaller time scale. Empirical data drawn from English and Japanese text, as well as conversational speech, reveals that the &amp;quot;attraction&amp;quot; between words decays exponentially, while stylistic and syntactic contraints create a &amp;quot;repulsion&amp;quot; between words that discourages close co-occurrence. We show that these characteristics are well described by simple mixture models based on twostage exponential distributions which can be trained using the EM algorithm. The resulting distance distributions can then be incorporated as penalizing features in an exponential language model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Berger</author>
<author>S Della Pietra</author>
<author>V Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<pages>22--1</pages>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Berger, A., S. Della Pietra, and V. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39-71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T M Cover</author>
<author>J A Thomas</author>
</authors>
<title>Elements of Information Theory.</title>
<date>1991</date>
<publisher>John Wiley.</publisher>
<contexts>
<context position="2187" citStr="Cover and Thomas, 1991" startWordPosition="320" endWordPosition="323">t by NSF grant IRI9314969, DARPA AASERT award DAAH04-95-1-0475, and the ATR Interpreting Telecommunications Research Laboratories. Stationary models are used to describe such a dynamic source for at least two reasons. The first is convenience: stationary models require a relatively small amount of computation to train and to apply. The second is ignorance: we know so little about how to model effectively the nonstationary characteristics of language that we have for the most part completely neglected the problem. From a theoretical standpoint, we appeal to the Shannon-McMillanBreiman theorem (Cover and Thomas, 1991) whenever computing perplexities on test data; yet this result only rigorously applies to stationary and ergodic sources. To allow a language model to adapt to its recent context, some researchers have used techniques to update trigram statistics in a dynamic fashion by creating a cache of the most recently seen n-grams which is smoothed together (typically by linear interpolation) with the static model; see for example (Jelinek et al., 1991; Kuhn and de Mori, 1990). Another approach, using maximum entropy methods similar to those that we present here, introduces a parameter for trigger pairs </context>
</contexts>
<marker>Cover, Thomas, 1991</marker>
<rawString>Cover, T.M. and J.A. Thomas. 1991. Elements of Information Theory. John Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Csiszax</author>
</authors>
<title>Maxent, mathematics, and information theory.</title>
<date>1996</date>
<booktitle>Maximum Entropy and Bayesian Methods.</booktitle>
<editor>In K. Hanson and R. Silver, editors,</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<marker>Csiszax, 1996</marker>
<rawString>Csiszax, I. 1996. Maxent, mathematics, and information theory. In K. Hanson and R. Silver, editors, Maximum Entropy and Bayesian Methods. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>DeUa Pietra</author>
<author>V Della Pietra S</author>
<author>J Lafferty</author>
</authors>
<title>Inducing features of random fields.</title>
<date>1997</date>
<journal>IEEE Trans. on Pattern Analysis and Machine Intelligence,</journal>
<volume>19</volume>
<issue>3</issue>
<marker>Pietra, S, Lafferty, 1997</marker>
<rawString>DeUa Pietra, S., V. Della Pietra, and J. Lafferty. 1997. Inducing features of random fields. IEEE Trans. on Pattern Analysis and Machine Intelligence, 19(3), March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society,</journal>
<pages>39--1</pages>
<contexts>
<context position="16575" citStr="Dempster, Laird, and Rubin, 1977" startWordPosition="2768" endWordPosition="2772">ur parametric family {p9}0Eo is the collection of two-stage exponential models; the log-likelihood in this case becomes C(ill = 13(k) log E p ts2(k - j) . k&gt;0 1=0 Here it is not obvious how to proceed to obtain the maximum likelihood estimates. The difficulty is that there is a sum inside the logarithm, and direct differentiation results in coupled equations for pi and 0.01 0.000 0.030 0.401 0.005 0.405 0.004 0.003 0.032 0.001 100 150 200 250 NO 350 400 450 KO p = log (1 + 1 377 P2. Our solution to this problem is to view the convolving index j as a hidden variable and apply the EM algorithm (Dempster, Laird, and Rubin, 1977). Recall that the interpretation of j is the time used to pass through the first queue; that is, the number of words used to satisfy the linguistic constraints of lexical exclusion. This value is hidden given only the total time k required to pass through both queues. Applying the standard EM argument, the difference in log-likelihood for two parameter pairs (pc, p12) and (pi, p2) can be bounded from below as £041) — C(p) = E i5(k) log ( &apos; pp, pi (k)) k&gt;0 PNi,N2(k) Psii,p2k^,, .1) A(141,12) where Poi ,p3(k .i) = Pol,(..7)Pp2(k — :7) and ppi,p2ci k) = PAI,i12(k Ppia.12(k) • auxiliary function </context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>Dempster, A.P., N.M. Laird, and D.B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, 39(B):1-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gelman</author>
<author>J Carlin</author>
<author>H Stern</author>
<author>D Rubin</author>
</authors>
<title>Bayesian Data Analysis.</title>
<date>1995</date>
<publisher>Chapman Sz Hall,</publisher>
<location>London.</location>
<contexts>
<context position="14930" citStr="Gelman et al., 1995" startWordPosition="2474" endWordPosition="2477">-time analogue pp(k) = (1- 14) e-18k In this case, the two-stage model becomes the discrete-time convolution ppi,.(k) = E (i) Pm2(k - t) t=0 Remark. It should be pointed out that there is another parametric family that is an excellent candidate for distance models, based on the first two features noted above. This is the Gamma distribution pa,p(x) = — e-flx . F(a) This distribution has mean a/16 and variance a/02 and thus can afford greater flexibility in fitting the empirical data. For Bayesian analysis, this distribution is appropriate as the conjugate prior for the exponential parameter p (Gelman et al., 1995). Using this family, however, sacrifices the linguistic interpretation of the two-stage model. 4 Estimating the Parameters In this section we present a solution to the problem of estimating the parameters of the distance models introduced in the previous section. We use the maximum likelihood criterion to fit the curves. Thus, if Ee represents the parameters of our model, and /3(k) is the empirical probability that two triggers appear a distance of k words apart, then we seek to maximize the log-likelihood L(0) = Ep(k)log pe(k) . k&gt;0 First suppose that {pe}oEe is the family of continuous one-s</context>
</contexts>
<marker>Gelman, Carlin, Stern, Rubin, 1995</marker>
<rawString>Gelman, A., J. Carlin, H. Stern, and D. Rubin. 1995. Bayesian Data Analysis. Chapman Sz Hall, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Godfrey</author>
<author>E Badman</author>
<author>J McDaniel</author>
</authors>
<title>SWITCHBOARD: Telephone speech corpus for research and development.</title>
<date>1992</date>
<booktitle>In Proc. ICASSP-92.</booktitle>
<marker>Godfrey, Badman, McDaniel, 1992</marker>
<rawString>Godfrey, J., E. Badman, and J. McDaniel. 1992. SWITCHBOARD: Telephone speech corpus for research and development. In Proc. ICASSP-92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
<author>B Merialdo</author>
<author>S Roukos</author>
<author>M Strauss</author>
</authors>
<title>A dynamic language model for speech recognition.</title>
<date>1991</date>
<booktitle>In Proceedings of the DARPA Speech and Natural Language Workshop,</booktitle>
<pages>293--295</pages>
<contexts>
<context position="2632" citStr="Jelinek et al., 1991" startWordPosition="393" endWordPosition="396">guage that we have for the most part completely neglected the problem. From a theoretical standpoint, we appeal to the Shannon-McMillanBreiman theorem (Cover and Thomas, 1991) whenever computing perplexities on test data; yet this result only rigorously applies to stationary and ergodic sources. To allow a language model to adapt to its recent context, some researchers have used techniques to update trigram statistics in a dynamic fashion by creating a cache of the most recently seen n-grams which is smoothed together (typically by linear interpolation) with the static model; see for example (Jelinek et al., 1991; Kuhn and de Mori, 1990). Another approach, using maximum entropy methods similar to those that we present here, introduces a parameter for trigger pairs of mutually informative words, so that the occurrence of certain words in recent context boosts the probability of the words that they trigger (Rosenfeld, 1996). Triggers have also been incorporated through different methods (Kuhn and de Mori, 1990; Ney, Essen, and Kneser, 1994). All of these techniques treat the recent context as a &amp;quot;bag of words,&amp;quot; so that a word that appears, say, five positions back makes the same contribution to predictio</context>
</contexts>
<marker>Jelinek, Merialdo, Roukos, Strauss, 1991</marker>
<rawString>Jelinek, F., B. Merialdo, S. Roukos, and M. Strauss. 1991. A dynamic language model for speech recognition. In Proceedings of the DARPA Speech and Natural Language Workshop, pages 293-295, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Katz</author>
</authors>
<title>Estimation of probabilities from sparse data for the langauge model component of a. speech recognizer.</title>
<date>1987</date>
<journal>IEEE Transactions on Acoustics, Speech and Signal Processing,</journal>
<pages>35--3</pages>
<contexts>
<context position="18843" citStr="Katz, 1987" startWordPosition="3171" endWordPosition="3172">hus, we can again apply the EM algorithm to determine the mixing parameter a. This is a standard application of the EM algorithm, and the details are omitted. In summary, we have shown how the EM algorithm can be applied to determine maximum likelihood estimates of the three-parameter family of distance models {ppi,p2,„} of distance models. In Figure 8 we display typical examples of this training algorithm at work. 5 A Nonstationary Language Model To incorporate triggers and distance models into a long-distance language model, we begin by constructing a standard, static backoff trigram model (Katz, 1987), which we will denote as q(wolw--1,w-2). For the purposes of building a model for the Wall Street Journal data, this trigram model is quickly trained on the entire 38-million word corpus. We then build a family of conditional exponential models of the general form p(w I H) = 1 z (H) exp (E Ai fi(H , w)) q(w I w_i, w_2) where H = w—i, w-2, • w—N is the word history, and Z(H) is the normalization constant Z(H) = Eexp Aifi(H, w)) q(w w—i w_2) The functions ft, which depend both on the word history H and the word being predicted, are called features, and each feature fi is assigned a weight Ai. I</context>
</contexts>
<marker>Katz, 1987</marker>
<rawString>Katz, S. 1987. Estimation of probabilities from sparse data for the langauge model component of a. speech recognizer. IEEE Transactions on Acoustics, Speech and Signal Processing, ASSP-35(3):400-401, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Kleinrock</author>
</authors>
<title>Queueing Systems. Volume I: Theory.</title>
<date>1975</date>
<publisher>Wiley,</publisher>
<location>New York.</location>
<contexts>
<context position="12355" citStr="Kleinrock, 1975" startWordPosition="2043" endWordPosition="2045">curve for the possessive particle CO (right). Bottom row: self trigger UK (left), YOU-KNOW (middle), and all self triggers appearing fewer than 100 times in the entire Switchboard corpus (right). the distribution p, has mean 1/p and variance 1/p2. This distribution is a good candidate for modeling non-self triggers. C Figure 6: A two-stage queue The second characteristic is the bump between 0 and 25 words for self triggers. This behavior appears when two exponential distributions are arranged in serial, and such distributions are an important tool in the &amp;quot;method of stages&amp;quot; in queueing theory (Kleinrock, 1975). The time it takes to travel through two service facilities arranged in serial, where the first provides exponential service with rate pi and the second provides exponential service with rate p2, is simply the convolution of the two exponentials: e-Pite(&apos;-&apos;)dt PPI,m2(x) P1142 f ii1P2 p —&apos;s - e0 x ) P1 0 1L2 P2 - /41 The mean and variance of the two-stage exponential p,, are 1/pi + 1/p2 and 1/p? + 1/4 respectively. As pi (or, by symmetry, p2) gets large, the peak shifts towards zero and the distribution approaches the single-parameter exponential pm, (by 376 symmetry, poi). A sequence of two-s</context>
</contexts>
<marker>Kleinrock, 1975</marker>
<rawString>Kleinrock, L. 1975. Queueing Systems. Volume I: Theory. Wiley, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kuhn</author>
<author>R de Mori</author>
</authors>
<title>A cache-based natural language model for speech recognition.</title>
<date>1990</date>
<journal>IEEE Trans. on Pattern Analysis and Machine Intelligence,</journal>
<pages>12--570</pages>
<marker>Kuhn, de Mori, 1990</marker>
<rawString>Kuhn, R. and R. de Mori. 1990. A cache-based natural language model for speech recognition. IEEE Trans. on Pattern Analysis and Machine Intelligence, 12:570-583.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ney</author>
<author>U Essen</author>
<author>R Kneser</author>
</authors>
<title>On structuring probabilistic dependencies in stochastic language modeling.</title>
<date>1994</date>
<journal>Computer Speech and Language,</journal>
<pages>8--1</pages>
<contexts>
<context position="3065" citStr="Ney, Essen, and Kneser, 1994" startWordPosition="461" endWordPosition="465">a dynamic fashion by creating a cache of the most recently seen n-grams which is smoothed together (typically by linear interpolation) with the static model; see for example (Jelinek et al., 1991; Kuhn and de Mori, 1990). Another approach, using maximum entropy methods similar to those that we present here, introduces a parameter for trigger pairs of mutually informative words, so that the occurrence of certain words in recent context boosts the probability of the words that they trigger (Rosenfeld, 1996). Triggers have also been incorporated through different methods (Kuhn and de Mori, 1990; Ney, Essen, and Kneser, 1994). All of these techniques treat the recent context as a &amp;quot;bag of words,&amp;quot; so that a word that appears, say, five positions back makes the same contribution to prediction as words at distances of 50 or 500 positions back in the history. In this paper we introduce new modeling techniques based on exponential families for capturing the long-range correlations between occurrences of words in text and speech. We show how for both written text and conversational speech, the empirical distribution of the distance between trig373 ger words exhibits a striking behavior in which the &amp;quot;attraction&amp;quot; between </context>
<context position="7009" citStr="Ney, Essen, and Kneser, 1994" startWordPosition="1119" endWordPosition="1123">pus. .9 t NIA OAIN UN Security Council Xii * t3 7 v 1-34 electricity kilowatt in* ii`2-14EK election small electoral district tAt ea silk cocoon itta 215t court imprisonment Hungary Bulgaria El 7ox no Japan Air to fly cargo M *lff1 sentence proposed punishment WO tf* transplant organ A44: tik forest wastepaper = v -.--— *A 1- computer host Figure 2: A sample of triggers extracted from the 33 million word Nikkei corpus. further and further back into the context. Indeed, there are tables in (Rosenfeld, 1996) which suggest that this is so, and distance-dependent &amp;quot;memory weights&amp;quot; are proposed in (Ney, Essen, and Kneser, 1994). We decided to investigate the effect of distance in more detail, and were surprised by what we found. 374 0.014 0015 0.015 0.01 0.01 0.1041 0.005 0.001 0.005 0.034 0.002 0.004 •)&amp;quot;, 0.002 • 744 . &amp;quot;f4414*44.04szoamit•mmo 50 100 so 203 250 100 350 403 50 103 ISO 203 340 301 360 401 Figure 3: The observed distance distributions—collected from five million words of the Wall Street Journal corpus—for one of the non-self trigger groups (left) and one of the self trigger groups (right). For a given distance 0 &lt; k &lt; 400 on the x-axis, the value on the y-axis is the empirical probability that two tri</context>
</contexts>
<marker>Ney, Essen, Kneser, 1994</marker>
<rawString>Ney, H., U. Essen, and R. Kneser. 1994. On structuring probabilistic dependencies in stochastic language modeling. Computer Speech and Language, 8:1-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Niesler</author>
<author>P Woodland</author>
</authors>
<title>Modelling wordpair relations in a category-based language model.</title>
<date>1997</date>
<booktitle>In Proceedings of ICASSP-97,</booktitle>
<location>Munich, Germany,</location>
<contexts>
<context position="4664" citStr="Niesler and Woodland, 1997" startWordPosition="724" endWordPosition="727">reviously exploited in speech and language processing. It is remarkable that the behavior of a highly complex stochastic process such as the separation between word cooccurrences is well modeled by such a simple parametric family, just as it is surprising that Zipf&apos;s law can so simply capture the distribution of word frequencies in most languages. In the following section we present examples of the empirical evidence for the effects of distance. In Section 3 we outline the class of statistical models that we propose to model this data. After completing this work we learned of a related paper (Niesler and Woodland, 1997) which constructs similar models. In Section 4 we present a parameter estimation algorithm, based on the EM algorithm, for determining the maximum likelihood estimates within the class. In Section 5 we explain how distance models can be incorporated into an exponential language model, and present sample perplexity results we have obtained using this class of models. 2 The Empirical Evidence The work described in this paper began with the goal of building a statistical language model using a static trigram model as a &amp;quot;prior,&amp;quot; or default distribution, and adding certain features to a family of c</context>
</contexts>
<marker>Niesler, Woodland, 1997</marker>
<rawString>Niesler, T. and P. Woodland. 1997. Modelling wordpair relations in a category-based language model. In Proceedings of ICASSP-97, Munich, Germany, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rosenfeld</author>
</authors>
<title>A maximum entropy approach to adaptive statistical language modeling.</title>
<date>1996</date>
<journal>Computer Speech and Language,</journal>
<pages>10--187</pages>
<contexts>
<context position="2947" citStr="Rosenfeld, 1996" startWordPosition="446" endWordPosition="447">del to adapt to its recent context, some researchers have used techniques to update trigram statistics in a dynamic fashion by creating a cache of the most recently seen n-grams which is smoothed together (typically by linear interpolation) with the static model; see for example (Jelinek et al., 1991; Kuhn and de Mori, 1990). Another approach, using maximum entropy methods similar to those that we present here, introduces a parameter for trigger pairs of mutually informative words, so that the occurrence of certain words in recent context boosts the probability of the words that they trigger (Rosenfeld, 1996). Triggers have also been incorporated through different methods (Kuhn and de Mori, 1990; Ney, Essen, and Kneser, 1994). All of these techniques treat the recent context as a &amp;quot;bag of words,&amp;quot; so that a word that appears, say, five positions back makes the same contribution to prediction as words at distances of 50 or 500 positions back in the history. In this paper we introduce new modeling techniques based on exponential families for capturing the long-range correlations between occurrences of words in text and speech. We show how for both written text and conversational speech, the empirical </context>
<context position="5623" citStr="Rosenfeld, 1996" startWordPosition="884" endWordPosition="885">ing this class of models. 2 The Empirical Evidence The work described in this paper began with the goal of building a statistical language model using a static trigram model as a &amp;quot;prior,&amp;quot; or default distribution, and adding certain features to a family of conditional exponential models to capture some of the nonstationary features of text. The features we used were simple &amp;quot;trigger pairs&amp;quot; of words that were chosen on the basis of mutual information. Figure 1 provides a small sample of the 41,263 (s,t) trigger pairs used in most of the experiments we will describe. In earlier work, for example (Rosenfeld, 1996), the distance between the words of a trigger pair (s, t) plays no role in the model, meaning that the &amp;quot;boost&amp;quot; in probability which t receives following its trigger s is independent of how long ago s occurred, so long as s appeared somewhere in the history H, a fixedlength window of words preceding t. It is reasonable to expect, however, that the relevance of a word s to the identity of the next word should decay as s falls S -t Ms. her changes revisions energy gas committee representative board board lieutenant colonel AIDS AIDS Soviet missiles underwater diving patients drugs television airw</context>
<context position="6892" citStr="Rosenfeld, 1996" startWordPosition="1104" endWordPosition="1105">Figure 1: A sample of the 41,263 trigger pairs extracted from the 38 million word Wall Street Journal corpus. .9 t NIA OAIN UN Security Council Xii * t3 7 v 1-34 electricity kilowatt in* ii`2-14EK election small electoral district tAt ea silk cocoon itta 215t court imprisonment Hungary Bulgaria El 7ox no Japan Air to fly cargo M *lff1 sentence proposed punishment WO tf* transplant organ A44: tik forest wastepaper = v -.--— *A 1- computer host Figure 2: A sample of triggers extracted from the 33 million word Nikkei corpus. further and further back into the context. Indeed, there are tables in (Rosenfeld, 1996) which suggest that this is so, and distance-dependent &amp;quot;memory weights&amp;quot; are proposed in (Ney, Essen, and Kneser, 1994). We decided to investigate the effect of distance in more detail, and were surprised by what we found. 374 0.014 0015 0.015 0.01 0.01 0.1041 0.005 0.001 0.005 0.034 0.002 0.004 •)&amp;quot;, 0.002 • 744 . &amp;quot;f4414*44.04szoamit•mmo 50 100 so 203 250 100 350 403 50 103 ISO 203 340 301 360 401 Figure 3: The observed distance distributions—collected from five million words of the Wall Street Journal corpus—for one of the non-self trigger groups (left) and one of the self trigger groups (righ</context>
<context position="20377" citStr="Rosenfeld, 1996" startWordPosition="3460" endWordPosition="3461">the strict Bayesian sense.) Previous work using maximum entropy methods incorporated trigram constraints as k&gt;0 j=0 Thus, the 1 if si E H and w = ti otherwise. 378 0.012 0.01 0.206 0.006 0.036 0.032 SO 150 100 203 250 250 350 400 • &apos; . . • • -14.4.4214-i4.24444.6.40,N,, • Figure 8: The same empirical distance distributions of Figure 2 fit to the three-parameter mixture model pi„,i„,„,, using the EM algorithm. The dashed line is the fitted curve. For the non-self trigger plot pi = 7, P2 = 0.0148, and a = 0.253. For the self trigger plot Pi = 0.29, p2 = 0.0168, and a = 0.224. explicit features (Rosenfeld, 1996), using the uniform distribution as the default model. There are several advantages to incorporating trigrams in this way. The trigram component can be efficiently constructed over a large volume of data, using standard software or including the various sophisticated techniques for smoothing that have been developed. Furthermore, the normalization Z(H) can be computed more efficiently when trigrams appear in the default distribution. For example, in the case of trigger features, since Z(H) = 1 + E 6(si E H)(eA• — 1)g(ti I w w _2) the normalization involves only a sum over those words that are </context>
</contexts>
<marker>Rosenfeld, 1996</marker>
<rawString>Rosenfeld, R. 1996. A maximum entropy approach to adaptive statistical language modeling. Computer Speech and Language, 10:187-228.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>