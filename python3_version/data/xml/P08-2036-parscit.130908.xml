<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.023913">
<title confidence="0.981488">
Smoothing a Tera-word Language Model
</title>
<author confidence="0.980406">
Deniz Yuret
</author>
<affiliation confidence="0.962621">
Koc¸ University
</affiliation>
<email confidence="0.995927">
dyuret@ku.edu.tr
</email>
<sectionHeader confidence="0.995608" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998410066666667">
Frequency counts from very large corpora,
such as the Web 1T dataset, have recently be-
come available for language modeling. Omis-
sion of low frequency n-gram counts is a prac-
tical necessity for datasets of this size. Naive
implementations of standard smoothing meth-
ods do not realize the full potential of such
large datasets with missing counts. In this pa-
per I present a new smoothing algorithm that
combines the Dirichlet prior form of (Mackay
and Peto, 1995) with the modified back-off es-
timates of (Kneser and Ney, 1995) that leads to
a 31% perplexity reduction on the Brown cor-
pus compared to a baseline implementation of
Kneser-Ney discounting.
</bodyText>
<sectionHeader confidence="0.998996" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999936823529412">
Language models, i.e. models that assign probabili-
ties to sequences of words, have been proven useful
in a variety of applications including speech recog-
nition and machine translation (Bahl et al., 1983;
Brown et al., 1990). More recently, good results
on lexical substitution and word sense disambigua-
tion using language models have also been reported
(Yuret, 2007).
The recently introduced Web 1T 5-gram dataset
(Brants and Franz, 2006) contains the counts of
word sequences up to length five in a 1012 word cor-
pus derived from publicly accessible Web pages. As
this corpus is several orders of magnitude larger than
the ones used in previous language modeling stud-
ies, it holds the promise to provide more accurate
domain independent probability estimates. How-
ever, naive application of the well-known smooth-
ing methods do not realize the full potential of this
dataset.
In this paper I present experiments with modifica-
tions and combinations of various smoothing meth-
ods using the Web 1T dataset for model building and
the Brown corpus for evaluation. I describe a new
smoothing method, Dirichlet-Kneser-Ney (DKN),
that combines the Bayesian intuition of MacKay and
Peto (1995) and the improved back-off estimation of
Kneser and Ney (1995) and gives significantly better
results than the baseline Kneser-Ney discounting.
The next section describes the general structure
of n-gram models and smoothing. Section 3 de-
scribes the data sets and the experimental methodol-
ogy used. Section 4 presents experiments with adap-
tations of various smoothing methods. Section 5 de-
scribes the new algorithm.
</bodyText>
<sectionHeader confidence="0.92275" genericHeader="method">
2 N-gram Models and Smoothing
</sectionHeader>
<bodyText confidence="0.999278375">
N-gram models are the most commonly used lan-
guage modeling tools. They estimate the probability
of each word using the context made up of the previ-
ous n −1 words. Let abc represent an n-gram where
a is the first word, c is the last word, and b repre-
sents zero or more words in between. One way to
estimate Pr(c|ab) is to look at the number of times
word c has followed the previous n − 1 words ab,
</bodyText>
<equation confidence="0.995081666666667">
C(abc)
Pr(c|ab) = (1)
C(ab)
</equation>
<bodyText confidence="0.998876333333333">
where C(x) denotes the number of times x has been
observed in the training corpus. This is the max-
imum likelihood (ML) estimate. Unfortunately it
</bodyText>
<page confidence="0.995131">
141
</page>
<reference confidence="0.223747">
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 141–144,
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</reference>
<bodyText confidence="0.999448777777778">
does not work very well because it assigns zero
probability to n-grams that have not been observed
in the training corpus. To avoid the zero probabil-
ities, we take some probability mass from the ob-
served n-grams and distribute it to unobserved n-
grams. Such redistribution is known as smoothing
or discounting.
Most existing smoothing methods can be ex-
pressed in one of the following two forms:
</bodyText>
<equation confidence="0.99966475">
Pr(c|ab) = α(c|ab) + -y(ab) Pr(c|b) (2)
�
Q(c|ab) if C(abc) &gt; 0
-y(ab) Pr(c|b) otherwise (3)
</equation>
<bodyText confidence="0.999754230769231">
Equation 2 describes the so-called interpolated
models and Equation 3 describes the back-off mod-
els. The highest order distributions α(c|ab) and
Q(c|ab) are typically discounted to be less than the
ML estimate so we have some leftover probability
for the c words unseen in the context ab. Different
methods mainly differ on how they discount the ML
estimate. The back-off weights -y(ab) are computed
to make sure the probabilities are normalized. The
interpolated models always incorporate the lower or-
der distribution Pr(c|b) whereas the back-off models
consider it only when the n-gram abc has not been
observed in the training data.
</bodyText>
<sectionHeader confidence="0.977946" genericHeader="method">
3 Data and Method
</sectionHeader>
<bodyText confidence="0.999921523809524">
All the models in this paper are interpolated mod-
els built using the counts obtained from the Web 1T
dataset and evaluated on the million word Brown
corpus using cross entropy (bits per token). The low-
est order model is taken to be the word frequencies
in the Web 1T corpus. The Brown corpus was re-
tokenized to match the tokenization style of the Web
1T dataset resulting in 1,186,262 tokens in 52,108
sentences. The Web 1T dataset has a 13 million
word vocabulary consisting of words that appear 100
times or more in its corpus. 769 sentences in Brown
that contained words outside this vocabulary were
eliminated leaving 1,162,052 tokens in 51,339 sen-
tences. Capitalization and punctuation were left in-
tact. The n-gram patterns of the Brown corpus were
extracted and the necessary counts were collected
from the Web 1T dataset in one pass. The end-of-
sentence tags were not included in the entropy cal-
culation. For parameter optimization, numerical op-
timization was performed on a 1,000 sentence ran-
dom sample of Brown.
</bodyText>
<sectionHeader confidence="0.998711" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999983555555556">
In this section, I describe several smoothing meth-
ods and give their performance on the Brown corpus.
Each subsection describes a single idea and its im-
pact on the performance. All methods use interpo-
lated models expressed by α(c|ab) and -y(ab) based
on Equation 2. The Web 1T dataset does not include
n-grams with counts less than 40, and I note the spe-
cific implementation decisions due to the missing
counts where appropriate.
</bodyText>
<subsectionHeader confidence="0.996851">
4.1 Absolute Discounting
</subsectionHeader>
<bodyText confidence="0.999978875">
Absolute discounting subtracts a fixed constant D
from each nonzero count to allocate probability for
unseen words. A different D constant is chosen for
each n-gram order. Note that in the original study, D
is taken to be between 0 and 1, but because the Web
1T dataset does not include n-grams with counts less
than 40, the optimized D constants in our case range
from 0 to 40. The interpolated form is:
</bodyText>
<equation confidence="0.994565166666667">
max(0, C(abc) − D)
α(c|ab) = (4)
C(ab*)
N(ab*)D
-y(ab) =
C(ab*)
</equation>
<bodyText confidence="0.996885818181818">
The * represents a wildcard matching any word and
C(ab*) is the total count of n-grams that start with
the n − 1 words ab. If we had complete counts,
we would have C(ab*) = E, C(abc) = C(ab).
However because of the missing counts in general
C(ab*) &lt; C(ab) and we need to use the former for
proper normalization. N(ab*) denotes the number
of distinct words following ab in the training data.
Absolute discounting achieves its best performance
with a 3-gram model and gives 8.53 bits of cross en-
tropy on the Brown corpus.
</bodyText>
<subsectionHeader confidence="0.988662">
4.2 Kneser-Ney
</subsectionHeader>
<bodyText confidence="0.999242333333333">
Kneser-Ney discounting (Kneser and Ney, 1995)
has been reported as the best performing smooth-
ing method in several comparative studies (Chen and
Goodman, 1999; Goodman, 2001). The α(c|ab)
and -y(ab) expressions are identical to absolute dis-
counting (Equation 4) for the highest order n-grams.
</bodyText>
<equation confidence="0.987804">
Pr(c|ab) =
</equation>
<page confidence="0.984141">
142
</page>
<bodyText confidence="0.9882215">
However, a modified estimate is used for lower order
n-grams used for back-off. The interpolated form is:
</bodyText>
<equation confidence="0.9999935">
Pr(c|ab) = α(c|ab) + &apos;y(ab)Pr0(c|b) (5)
Pr0(c|ab) = α0(c|ab) + &apos;y0(ab)Pr0(c|b)
</equation>
<bodyText confidence="0.9998572">
Specifically, the modified estimate Pr0(c|b) for a
lower order n-gram is taken to be proportional to the
number of unique words that precede the n-gram in
the training data. The α0 and &apos;y0 expressions for the
modified lower order distributions are:
</bodyText>
<equation confidence="0.9703955">
α0(c|b) = max(0, N(*bc) − D) (6)
N(*b*)
&apos;y0(b)
= R(*b*)D
</equation>
<subsectionHeader confidence="0.989273">
4.3 Correcting for Missing Counts
</subsectionHeader>
<bodyText confidence="0.999995125">
Kneser-Ney takes the back-off probability of a lower
order n-gram to be proportional to the number of
unique words that precede the n-gram in the training
data. Unfortunately this number is not exactly equal
to the N(*bc) value given in the Web 1T dataset be-
cause the dataset does not include low count abc n-
grams. To correct for the missing counts I used the
following modified estimates:
</bodyText>
<equation confidence="0.999985">
N0(*bc) = N(*bc) + S(C(bc) − C(*bc))
N0(*b*) = N(*b*) + S(C(b*) − C(*b*))
</equation>
<bodyText confidence="0.9992042">
The difference between C(bc) and C(*bc) is due
to the words preceding bc less than 40 times. We
can estimate their number to be a fraction of this
difference. S is an estimate of the type token ra-
tio of these low count words. Its valid range is be-
tween 1/40 and 1, and it can be optimized along with
the other parameters. The reader can confirm that
E, N0(*bc) = N0(*b*) and |c : N0(*bc) &gt; 0 |=
N(b*). The expression for the Kneser-Ney back-off
estimate becomes
</bodyText>
<equation confidence="0.9995272">
α0(c|b) = max(0, N0(*bc) − D) (7)
N0(*b*)
N(b*)D
=
N0(*b*)
</equation>
<bodyText confidence="0.999641">
Using the corrected N0 counts instead of the plain N
counts achieves its best performance with a 4-gram
model and gives 8.23 bits on Brown.
</bodyText>
<subsectionHeader confidence="0.992587">
4.4 Dirichlet Form
</subsectionHeader>
<bodyText confidence="0.999461333333333">
MacKay and Peto (1995) show that based on Dirich-
let priors a reasonable form for a smoothed distribu-
tion can be expressed as
</bodyText>
<equation confidence="0.998219833333334">
C(abc)
α(c|ab) = (8)
C(ab*) + A
A
&apos;y(ab) =
C(ab*) + A
</equation>
<bodyText confidence="0.999208">
The parameter A can be interpreted as the extra
counts added to the given distribution and these ex-
tra counts are distributed as the lower order model.
Chen and Goodman (1996) suggest that these ex-
tra counts should be proportional to the number of
words with exactly one count in the given context
based on the Good-Turing estimate. The Web 1T
dataset does not include one-count n-grams. A rea-
sonable alternative is to take A to be proportional
to the missing count due to low-count n-grams:
</bodyText>
<equation confidence="0.854228">
C(ab) − C(ab*).
A(ab) = max(1, K(C(ab) − C(ab*)))
</equation>
<bodyText confidence="0.99976375">
A different K constant is chosen for each n-gram
order. Using this formulation as an interpolated 5-
gram language model gives a cross entropy of 8.05
bits on Brown.
</bodyText>
<subsectionHeader confidence="0.998611">
4.5 Dirichlet with KN Back-Off
</subsectionHeader>
<bodyText confidence="0.999802555555556">
Using a modified back-off distribution for lower or-
der n-grams gave us a big boost in the baseline re-
sults from 8.53 bits for absolute discounting to 8.23
bits for Kneser-Ney. The same idea can be applied
to the missing-count estimate. We can use Equa-
tion 8 for the highest order n-grams and Equation 7
for lower order n-grams used for back-off. Such a
5-gram model gives a cross entropy of 7.96 bits on
the Brown corpus.
</bodyText>
<sectionHeader confidence="0.926123" genericHeader="method">
5 A New Smoothing Method: DKN
</sectionHeader>
<bodyText confidence="0.997317">
In this section, I describe a new smoothing method
that combines the Dirichlet form of MacKay and
</bodyText>
<equation confidence="0.672128">
N( *b*)
</equation>
<bodyText confidence="0.999793714285714">
where R(*b*) = |c : N(*bc) &gt; 0 |denotes the num-
ber of distinct words observed on the right hand side
of the *b* pattern. A different D constant is chosen
for each n-gram order. The lowest order model is
taken to be Pr(c) = N(*c)/N(**). The best results
for Kneser-Ney are achieved with a 4-gram model
and its performance on Brown is 8.40 bits.
</bodyText>
<equation confidence="0.484993">
&apos;y0(b)
</equation>
<page confidence="0.995469">
143
</page>
<bodyText confidence="0.999914954545455">
Peto (1995) and the modified back-off distribution
of Kneser and Ney (1995). We will call this new
method Dirichlet-Kneser-Ney, or DKN for short.
The important idea in Kneser-Ney is to let the prob-
ability of a back-off n-gram be proportional to the
number of unique words that precede it. However
we do not need to use the absolute discount form for
the estimates. We can use the Dirichlet prior form
for the lower order back-off distributions as well as
the highest order distribution. The extra counts A
in the Dirichlet form are taken to be proportional
to the missing counts, and the coefficient of pro-
portionality K is optimized for each n-gram order.
Where complete counts are available, A should be
taken to be proportional to the number of one-count
n-grams instead. This smoothing method with a 5-
gram model gives a cross entropy of 7.86 bits on
the Brown corpus achieving a perplexity reduction
of 31% compared to the naive implementation of
Kneser-Ney.
The relevant equations are repeated below for the
reader’s convenience.
</bodyText>
<sectionHeader confidence="0.994157" genericHeader="conclusions">
6 Summary and Discussion
</sectionHeader>
<bodyText confidence="0.999941818181818">
Frequency counts based on very large corpora can
provide accurate domain independent probability es-
timates for language modeling. I presented adapta-
tions of several smoothing methods that can prop-
erly handle the missing counts that may exist in
such datasets. I described a new smoothing method,
DKN, combining the Bayesian intuition of MacKay
and Peto (1995) and the modified back-off distri-
bution of Kneser and Ney (1995) which achieves a
significant perplexity reduction compared to a naive
implementation of Kneser-Ney smoothing. This
is a surprizing result because Chen and Goodman
(1999) partly attribute the performance of Kneser-
Ney to the use of absolute discounting. The re-
lationship between Kneser-Ney smoothing to the
Bayesian approach have been explored in (Goldwa-
ter et al., 2006; Teh, 2006) using Pitman-Yor pro-
cesses. These models still suggest discount-based
interpolation with type frequencies whereas DKN
uses Dirichlet smoothing throughout. The condi-
tions under which the Dirichlet form is superior is
a topic for future research.
</bodyText>
<sectionHeader confidence="0.998939" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999543405405406">
Lalit R. Bahl, Frederick Jelinek, and Robert L. Mercer.
1983. A maximum likelihood approach to continu-
ous speech recognition. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 5(2):179–190.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
version 1. Linguistic Data Consortium, Philadelphia.
LDC2006T13.
Peter F. Brown, John Cocke, Stephen A. Della Pietra,
Vincent J. Della Pietra, Frederick Jelinek, John D. Laf-
ferty, Robert L. Mercer, and Paul S. Roossin. 1990. A
statistical approach to machine translation. Computa-
tional Linguistics, 16(2):79–85.
Stanley F. Chen and Joshua Goodman. 1996. An empir-
ical study of smoothing techniques for language mod-
eling. In Proceedings of the 34th Annual Meeting of
the ACL.
Stanley F. Chen and Joshua Goodman. 1999. An empir-
ical study of smoothing techniques for language mod-
eling. Computer Speech and Language.
S. Goldwater, T.L. Griffiths, and M. Johnson. 2006. In-
terpolating between types and tokens by estimating
power-law generators. In Advances in Neural Infor-
mation Processing Systems, volume 18. MIT Press.
Joshua Goodman. 2001. A bit of progress in language
modeling. Computer Speech and Language.
R. Kneser and H. Ney. 1995. Improved backing-off for
m-gram language modeling. In International Confer-
ence on Acoustics, Speech, and Signal Processing.
David J. C. Mackay and Linda C. Bauman Peto. 1995. A
hierarchical Dirichlet language model. Natural Lan-
guage Engineering, 1(3):1–19.
Y.W. Teh. 2006. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Proceed-
ings of the ACL, pages 985–992.
Deniz Yuret. 2007. KU: Word sense disambiguation
by substitution. In SemEval-2007: 4th International
Workshop on Semantic Evaluations.
</reference>
<equation confidence="0.998982230769231">
Pr(c|ab) = α(c|ab) + ry(ab)Pr0(c|b)
Pr0(c|ab) = α0(c|ab) + ry0(ab)Pr0(c|b)
C(bc)
α(c|b) = C(b*) + A(b)
A(b)
�(b) =
C(b*) + A(b)
N0(*bc)
α0(c|b) = N0(*b*) + A(b)
A(b)
&apos;Y0(b) = N0(*b*) + A(b)
A(b) = max(1, K(C(b) − C(b*)))
or max(1, K|c : C(bc) = 1|)
</equation>
<page confidence="0.996932">
144
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.807692">
<title confidence="0.999752">Smoothing a Tera-word Language Model</title>
<author confidence="0.983475">Deniz Yuret</author>
<affiliation confidence="0.983313">University</affiliation>
<email confidence="0.961229">dyuret@ku.edu.tr</email>
<abstract confidence="0.9908563125">Frequency counts from very large corpora, such as the Web 1T dataset, have recently become available for language modeling. Omission of low frequency n-gram counts is a practical necessity for datasets of this size. Naive implementations of standard smoothing methods do not realize the full potential of such large datasets with missing counts. In this paper I present a new smoothing algorithm that combines the Dirichlet prior form of (Mackay and Peto, 1995) with the modified back-off estimates of (Kneser and Ney, 1995) that leads to a 31% perplexity reduction on the Brown corpus compared to a baseline implementation of Kneser-Ney discounting.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<booktitle>Proceedings of ACL-08: HLT, Short Papers (Companion Volume),</booktitle>
<pages>141--144</pages>
<marker></marker>
<rawString>Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 141–144,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Columbus</author>
</authors>
<title>c�2008 Association for Computational Linguistics Lalit</title>
<date>2008</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>5</volume>
<issue>2</issue>
<location>Ohio, USA,</location>
<marker>Columbus, 2008</marker>
<rawString>Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics Lalit R. Bahl, Frederick Jelinek, and Robert L. Mercer. 1983. A maximum likelihood approach to continuous speech recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 5(2):179–190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Alex Franz</author>
</authors>
<title>Web 1T 5-gram version 1. Linguistic Data Consortium,</title>
<date>2006</date>
<location>Philadelphia. LDC2006T13.</location>
<contexts>
<context position="1195" citStr="Brants and Franz, 2006" startWordPosition="186" endWordPosition="189">ack-off estimates of (Kneser and Ney, 1995) that leads to a 31% perplexity reduction on the Brown corpus compared to a baseline implementation of Kneser-Ney discounting. 1 Introduction Language models, i.e. models that assign probabilities to sequences of words, have been proven useful in a variety of applications including speech recognition and machine translation (Bahl et al., 1983; Brown et al., 1990). More recently, good results on lexical substitution and word sense disambiguation using language models have also been reported (Yuret, 2007). The recently introduced Web 1T 5-gram dataset (Brants and Franz, 2006) contains the counts of word sequences up to length five in a 1012 word corpus derived from publicly accessible Web pages. As this corpus is several orders of magnitude larger than the ones used in previous language modeling studies, it holds the promise to provide more accurate domain independent probability estimates. However, naive application of the well-known smoothing methods do not realize the full potential of this dataset. In this paper I present experiments with modifications and combinations of various smoothing methods using the Web 1T dataset for model building and the Brown corpu</context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram version 1. Linguistic Data Consortium, Philadelphia. LDC2006T13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>John Cocke</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Frederick Jelinek</author>
<author>John D Lafferty</author>
<author>Robert L Mercer</author>
<author>Paul S Roossin</author>
</authors>
<title>A statistical approach to machine translation.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>2</issue>
<contexts>
<context position="980" citStr="Brown et al., 1990" startWordPosition="154" endWordPosition="157"> not realize the full potential of such large datasets with missing counts. In this paper I present a new smoothing algorithm that combines the Dirichlet prior form of (Mackay and Peto, 1995) with the modified back-off estimates of (Kneser and Ney, 1995) that leads to a 31% perplexity reduction on the Brown corpus compared to a baseline implementation of Kneser-Ney discounting. 1 Introduction Language models, i.e. models that assign probabilities to sequences of words, have been proven useful in a variety of applications including speech recognition and machine translation (Bahl et al., 1983; Brown et al., 1990). More recently, good results on lexical substitution and word sense disambiguation using language models have also been reported (Yuret, 2007). The recently introduced Web 1T 5-gram dataset (Brants and Franz, 2006) contains the counts of word sequences up to length five in a 1012 word corpus derived from publicly accessible Web pages. As this corpus is several orders of magnitude larger than the ones used in previous language modeling studies, it holds the promise to provide more accurate domain independent probability estimates. However, naive application of the well-known smoothing methods </context>
</contexts>
<marker>Brown, Cocke, Pietra, Pietra, Jelinek, Lafferty, Mercer, Roossin, 1990</marker>
<rawString>Peter F. Brown, John Cocke, Stephen A. Della Pietra, Vincent J. Della Pietra, Frederick Jelinek, John D. Lafferty, Robert L. Mercer, and Paul S. Roossin. 1990. A statistical approach to machine translation. Computational Linguistics, 16(2):79–85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the ACL.</booktitle>
<marker>Chen, Goodman, 1996</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1996. An empirical study of smoothing techniques for language modeling. In Proceedings of the 34th Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling. Computer Speech and Language.</title>
<date>1999</date>
<marker>Chen, Goodman, 1999</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1999. An empirical study of smoothing techniques for language modeling. Computer Speech and Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Goldwater</author>
<author>T L Griffiths</author>
<author>M Johnson</author>
</authors>
<title>Interpolating between types and tokens by estimating power-law generators.</title>
<date>2006</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<volume>18</volume>
<publisher>MIT Press.</publisher>
<marker>Goldwater, Griffiths, Johnson, 2006</marker>
<rawString>S. Goldwater, T.L. Griffiths, and M. Johnson. 2006. Interpolating between types and tokens by estimating power-law generators. In Advances in Neural Information Processing Systems, volume 18. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>A bit of progress in language modeling. Computer Speech and Language.</title>
<date>2001</date>
<marker>Goodman, 2001</marker>
<rawString>Joshua Goodman. 2001. A bit of progress in language modeling. Computer Speech and Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kneser</author>
<author>H Ney</author>
</authors>
<title>Improved backing-off for m-gram language modeling.</title>
<date>1995</date>
<booktitle>In International Conference on Acoustics, Speech, and Signal Processing.</booktitle>
<contexts>
<context position="2000" citStr="Kneser and Ney (1995)" startWordPosition="316" endWordPosition="319"> ones used in previous language modeling studies, it holds the promise to provide more accurate domain independent probability estimates. However, naive application of the well-known smoothing methods do not realize the full potential of this dataset. In this paper I present experiments with modifications and combinations of various smoothing methods using the Web 1T dataset for model building and the Brown corpus for evaluation. I describe a new smoothing method, Dirichlet-Kneser-Ney (DKN), that combines the Bayesian intuition of MacKay and Peto (1995) and the improved back-off estimation of Kneser and Ney (1995) and gives significantly better results than the baseline Kneser-Ney discounting. The next section describes the general structure of n-gram models and smoothing. Section 3 describes the data sets and the experimental methodology used. Section 4 presents experiments with adaptations of various smoothing methods. Section 5 describes the new algorithm. 2 N-gram Models and Smoothing N-gram models are the most commonly used language modeling tools. They estimate the probability of each word using the context made up of the previous n −1 words. Let abc represent an n-gram where a is the first word,</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>R. Kneser and H. Ney. 1995. Improved backing-off for m-gram language modeling. In International Conference on Acoustics, Speech, and Signal Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David J C Mackay</author>
<author>Linda C Bauman Peto</author>
</authors>
<title>A hierarchical Dirichlet language model.</title>
<date>1995</date>
<journal>Natural Language Engineering,</journal>
<volume>1</volume>
<issue>3</issue>
<marker>Mackay, Peto, 1995</marker>
<rawString>David J. C. Mackay and Linda C. Bauman Peto. 1995. A hierarchical Dirichlet language model. Natural Language Engineering, 1(3):1–19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y W Teh</author>
</authors>
<title>A hierarchical Bayesian language model based on Pitman-Yor processes.</title>
<date>2006</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>985--992</pages>
<marker>Teh, 2006</marker>
<rawString>Y.W. Teh. 2006. A hierarchical Bayesian language model based on Pitman-Yor processes. In Proceedings of the ACL, pages 985–992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deniz Yuret</author>
</authors>
<title>KU: Word sense disambiguation by substitution.</title>
<date>2007</date>
<booktitle>In SemEval-2007: 4th International Workshop on Semantic Evaluations.</booktitle>
<contexts>
<context position="1123" citStr="Yuret, 2007" startWordPosition="177" endWordPosition="178">let prior form of (Mackay and Peto, 1995) with the modified back-off estimates of (Kneser and Ney, 1995) that leads to a 31% perplexity reduction on the Brown corpus compared to a baseline implementation of Kneser-Ney discounting. 1 Introduction Language models, i.e. models that assign probabilities to sequences of words, have been proven useful in a variety of applications including speech recognition and machine translation (Bahl et al., 1983; Brown et al., 1990). More recently, good results on lexical substitution and word sense disambiguation using language models have also been reported (Yuret, 2007). The recently introduced Web 1T 5-gram dataset (Brants and Franz, 2006) contains the counts of word sequences up to length five in a 1012 word corpus derived from publicly accessible Web pages. As this corpus is several orders of magnitude larger than the ones used in previous language modeling studies, it holds the promise to provide more accurate domain independent probability estimates. However, naive application of the well-known smoothing methods do not realize the full potential of this dataset. In this paper I present experiments with modifications and combinations of various smoothing</context>
</contexts>
<marker>Yuret, 2007</marker>
<rawString>Deniz Yuret. 2007. KU: Word sense disambiguation by substitution. In SemEval-2007: 4th International Workshop on Semantic Evaluations.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>