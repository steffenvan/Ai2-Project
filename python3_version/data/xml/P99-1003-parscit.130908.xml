<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.838654">
<title confidence="0.977104">
The Lexical Component of Natural Language
Processing
</title>
<author confidence="0.929934">
George A. Miller
</author>
<affiliation confidence="0.9626065">
Cognitive Science Laboratory
Princeton University
</affiliation>
<sectionHeader confidence="0.951775" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999899263157895">
Computational linguistics is generally considered to be the branch
of engineering that uses computers to do useful things with linguistic
signals, but it can also be viewed as an extended test of computational
theories of human cognition; it is this latter perspective that psychol-
ogists find most interesting. Language provides a critical test for the
hypothesis that physical symbol systems are adequate to perform all
human cognitive functions. As yet, no adequate system for natural
language processing has approached human levels of performance.
Of the various problems that natural language processing has re-
vealed, polysemy is probably the most frustrating. People deal with
polysemy so easily that potential abiguities are overlooked, whereas
computers must work hard to do far less well. A linguistic approach
generally involves a parser, a lexicon, and some ad hoc rules for using
linguistic context to identify the context-appropriate sense. A statisti-
cal approach generally involves the use of word co-occurrence statistics
to create a semantic hyperspace where each word, regardless of its pol-
ysemy, is represented as a single vector. Each approach has strengths
and limitations; some combination is often proposed. Various possibil-
ities will be discussed in terms of their psychological plausibility.
</bodyText>
<page confidence="0.999081">
21
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.872405">
<title confidence="0.9983425">The Lexical Component of Natural Language Processing</title>
<author confidence="0.999928">George A Miller</author>
<affiliation confidence="0.9999535">Cognitive Science Laboratory Princeton University</affiliation>
<abstract confidence="0.9998496">Computational linguistics is generally considered to be the branch of engineering that uses computers to do useful things with linguistic signals, but it can also be viewed as an extended test of computational theories of human cognition; it is this latter perspective that psychologists find most interesting. Language provides a critical test for the hypothesis that physical symbol systems are adequate to perform all human cognitive functions. As yet, no adequate system for natural language processing has approached human levels of performance. Of the various problems that natural language processing has revealed, polysemy is probably the most frustrating. People deal with polysemy so easily that potential abiguities are overlooked, whereas computers must work hard to do far less well. A linguistic approach generally involves a parser, a lexicon, and some ad hoc rules for using linguistic context to identify the context-appropriate sense. A statistical approach generally involves the use of word co-occurrence statistics to create a semantic hyperspace where each word, regardless of its polysemy, is represented as a single vector. Each approach has strengths and limitations; some combination is often proposed. Various possibilities will be discussed in terms of their psychological plausibility.</abstract>
<intro confidence="0.877438">21</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>