<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000070">
<title confidence="0.996603">
A Term Recognition Approach to Acronym Recognition
</title>
<author confidence="0.885762">
Naoaki Okazaki *
</author>
<affiliation confidence="0.804095">
Graduate School of Information
Science and Technology
The University of Tokyo
</affiliation>
<address confidence="0.9683135">
7-3-1 Hongo, Bunkyo-ku, Tokyo
113-8656 Japan
</address>
<email confidence="0.999273">
okazaki@mi.ci.i.u-tokyo.ac.jp
</email>
<author confidence="0.987314">
Sophia Ananiadou
</author>
<affiliation confidence="0.990770666666667">
National Centre for Text Mining
School of Informatics
Manchester University
</affiliation>
<address confidence="0.990497">
PO Box 88, Sackville Street, Manchester
M60 1QD United Kingdom
</address>
<email confidence="0.998236">
Sophia.Ananiadou@manchester.ac.uk
</email>
<sectionHeader confidence="0.995608" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999968842105263">
We present a term recognition approach
to extract acronyms and their definitions
from a large text collection. Parentheti-
cal expressions appearing in a text collec-
tion are identified as potential acronyms.
Assuming terms appearing frequently in
the proximity of an acronym to be
the expanded forms (definitions) of the
acronyms, we apply a term recognition
method to enumerate such candidates and
to measure the likelihood scores of the
expanded forms. Based on the list of
the expanded forms and their likelihood
scores, the proposed algorithm determines
the final acronym-definition pairs. The
proposed method combined with a letter
matching algorithm achieved 78% preci-
sion and 85% recall on an evaluation cor-
pus with 4,212 acronym-definition pairs.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9989493">
In the biomedical literature the amount of terms
(names of genes, proteins, chemical compounds,
drugs, organisms, etc) is increasing at an astound-
ing rate. Existing terminological resources and
scientific databases (such as Swiss-Prot1, SGD2,
FlyBase3, and UniProt4) cannot keep up-to-date
with the growth of neologisms (Pustejovsky et al.,
2001). Although curation teams maintain termino-
logical resources, integrating neologisms is very
difficult if not based on systematic extraction and
</bodyText>
<affiliation confidence="0.184034">
*Research Fellow of the Japan Society for the Promotion
of Science (JSPS)
</affiliation>
<footnote confidence="0.99993">
1http://www.ebi.ac.uk/swissprot/
2http://www.yeastgenome.org/
3http://www.flybase.org/
4http://www.ebi.ac.uk/GOA/
</footnote>
<bodyText confidence="0.999964">
collection of terminology from literature. Term
identification in literature is one of the major bot-
tlenecks in processing information in biology as it
faces many challenges (Ananiadou and Nenadic,
2006; Friedman et al., 2001; Bodenreider, 2004).
The major challenges are due to term variation,
e.g. spelling, morphological, syntactic, semantic
variations (one term having different termforms),
term synonymy and homonymy, which are all cen-
tral concerns of any term management system.
Acronyms are among the most productive type
of term variation. Acronyms (e.g. RARA)
are compressed forms of terms, and are used
as substitutes of the fully expanded termforms
(e.g., retinoic acid receptor alpha). Chang and
Sch¨utze (2006) reported that, in MEDLINE ab-
stracts, 64,242 new acronyms were introduced in
2004 with the estimated number being 800,000.
Wren et al. (2005) reported that 5,477 documents
could be retrieved by using the acronym JNK
while only 3,773 documents could be retrieved by
using its full term, c-jun N-terminal kinase.
In practice, there are no rules or exact patterns
for the creation of acronyms. Moreover, acronyms
are ambiguous, i.e., the same acronym may re-
fer to different concepts (GR abbreviates both glu-
cocorticoid receptor and glutathione reductase).
Acronyms also have variant forms (e.g. NF kappa
B, NF kB, NF-KB, NF-kappaB, NFKB factor for
nuclear factor-kappa B). Ambiguity and variation
present a challenge for any text mining system,
since acronyms have not only to be recognised, but
their variants have to be linked to the same canon-
ical form and be disambiguated.
Thus, discovering acronyms and relating them
to their expanded forms is important for terminol-
ogy management. In this paper, we present a term
recognition approach to construct an acronym dic-
</bodyText>
<page confidence="0.988919">
643
</page>
<note confidence="0.7240495">
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 643–650,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999673125">
tionary from a large text collection. The proposed
method focuses on terms appearing frequently in
the proximity of an acronym and measures the
likelihood scores of such terms to be the expanded
forms of the acronyms. We also describe an algo-
rithm to combine the proposed method with a con-
ventional letter-based method for acronym recog-
nition.
</bodyText>
<sectionHeader confidence="0.999781" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9999206">
The goal of acronym identification is to extract
pairs of short forms (acronyms) and long forms
(their expanded forms or definitions) occurring in
text5. Currently, most methods are based on let-
ter matching of the acronym-definition pair, e.g.,
hidden markov model (HMM), to identify short/-
long form candidates. Existing methods of short-
/long form recognition are divided into pattern
matching approaches, e.g., exploring an efficient
set of heuristics/rules (Adar, 2004; Ao and Takagi,
2005; Schwartz and Hearst, 2003; Wren and Gar-
ner, 2002; Yu et al., 2002), and pattern mining ap-
proaches, e.g., Longest Common Substring (LCS)
formalization (Chang and Sch¨utze, 2006; Taghva
and Gilbreth, 1999).
Schwartz and Hearst (2003) implemented an al-
gorithm for identifying acronyms by using paren-
thetical expressions as a marker of a short form.
A character matching technique was used, i.e. all
letters and digits in a short form had to appear in
the corresponding long form in the same order, to
determine its long form. Even though the core al-
gorithm was very simple, the authors report 99%
precision and 84% recall on the Medstract gold
standard6.
However, the letter-matching approach is af-
fected by the expressions in the source text and
sometimes finds incorrect long forms such as
acquired syndrome and a patient with human
immunodeficiency syndrome7 instead of the cor-
rect one, acquired immune deficiency syndrome
for the acronym AIDS. This approach also en-
counters difficulties finding a long form whose
short form is arranged in a different word order,
e.g., beta 2 adrenergic receptor (ADRB2). To
</bodyText>
<footnote confidence="0.973131375">
5This paper uses the terms “short form” and “long form”
hereafter. “Long form” is what others call “definition”,
“meaning”, “expansion”, and “expanded form” of acronym.
6http://www.medstract.org/
7These examples are obtained from the actual MED-
LINE abstracts submitted to Schwartz and Hearst’s algorithm
(2003). An author does not always write a proper definition
with a parenthetic expression.
</footnote>
<bodyText confidence="0.999870088888889">
improve the accuracy of long/short form recogni-
tion, some methods measure the appropriateness
of these candidates based on a set of rules (Ao and
Takagi, 2005), scoring functions (Adar, 2004), sta-
tistical analysis (Hisamitsu and Niwa, 2001; Liu
and Friedman, 2003) and machine learning ap-
proaches (Chang and Sch¨utze, 2006; Pakhomov,
2002; Nadeau and Turney, 2005).
Chang and Sch¨utze (2006) present an algorithm
for matching short/long forms with a statistical
learning method. They discover a list of abbrevia-
tion candidates based on parentheses and enumer-
ate possible short/long form candidates by a dy-
namic programming algorithm. The likelihood of
the recognized candidates is estimated as the prob-
ability calculated from a logistic regression with
nine features such as the percentage of long-form
letters aligned at the beginning of a word. Their
method achieved 80% precision and 83% recall on
the Medstract corpus.
Hisamitsu and Niwa (2001) propose a method
for extracting useful parenthetical expressions
from Japanese newspaper articles. Their method
measures the co-occurrence strength between the
inner and outer phrases of a parenthetical expres-
sion by using statistical measures such as mutual
information, x2 test with Yate’s correction, Dice
coefficient, log-likelihood ratio, etc. Their method
deals with generic parenthetical expressions (e.g.,
abbreviation, non abbreviation paraphrase, supple-
mentary comments), not focusing exclusively on
acronym recognition.
Liu and Friedman (2003) proposed a method
based on mining collocations occurring before the
parenthetical expressions. Their method creates a
list of potential long forms from collocations ap-
pearing more than once in a text collection and
eliminates unlikely candidates with three rules,
e.g., “remove a set of candidates T,,, formed by
adding a prefix word to a candidate w if the num-
ber of such candidates T,,, is greater than 3”. Their
approach cannot recognise expanded forms occur-
ring only once in the corpus. They reported a pre-
cision of 96.3% and a recall of 88.5% for abbrevi-
ations recognition on their test corpus.
</bodyText>
<sectionHeader confidence="0.999788" genericHeader="method">
3 Methodology
</sectionHeader>
<subsectionHeader confidence="0.999428">
3.1 Term-based long-form identification
</subsectionHeader>
<bodyText confidence="0.999880333333333">
We propose a method for identifying the long
forms of an acronym based on a term extrac-
tion technique. We focus on terms appearing fre-
</bodyText>
<page confidence="0.998294">
644
</page>
<bodyText confidence="0.950991230769231">
quently in the proximity of an acronym in a text
collection. More specifically, if a word sequence
co-occurs frequently with a specific acronym and
not with other surrounding words, we assume that
there is a relationship8 between the acronym and
the word sequence.
Figure 1 illustrates our hypothesis taking the
acronym TTF-1 as an example. The tree consists
of expressions collected from all sentences with
the acronym in parentheses and appearing before
the acronym. A node represents a word, and a path
from any node to TTF-1 represents a long-form
candidate9. The figure above each node shows
the co-occurrence frequency of the corresponding
long-form candidate. For example, long-form can-
didates 1, factor 1, transcription factor 1, and thy-
roid transcription factor 1 co-occur 218, 216, 213,
and 209 times respectively with the acronym TTF-
1 in the text collection.
Even though long-form candidates 1, factor
1 and transcription factor 1 co-occur frequently
with the acronym TTF-1, we note that they
also co-occur frequently with the word thyroid.
Meanwhile, the candidate thyroid transcription
factor 1 is used in a number of contexts (e.g.,
expression of thyroid transcription factor 1,
expressed thyroid transcription factor 1, gene
encoding thyroid transcription factor 1, etc.).
Therefore, we observe this to be the strongest
relationship between acronym TTF-1 and its
8A sequence of words that co-occurs with an acronym
does not always imply the acronym-definition relation. For
example, the acronym 5-HT co-occurs frequently with the
term serotonin, but their relation is interpreted as a synony-
mous relation.
9The words with function words (e.g., expression of, reg-
ulation of the, etc.) are combined into a node. This is due
to the requirement for a long-form candidate discussed later
(Section 3.3).
</bodyText>
<figure confidence="0.645434">
Acronyms and
expanded forms
</figure>
<figureCaption confidence="0.999392">
Figure 2: System diagram of acronym recognition
</figureCaption>
<bodyText confidence="0.999379285714286">
long-form candidate thyroid transcription factor 1
in the tree. We apply a number of validation rules
(described later) to the candidate pair to make
sure that it has an acronym-definition relation. In
this example, the candidate pair is likely to be
an acronym-definition relation because the long
form thyroid transcription factor 1 contains all
alphanumeric letters in the short form TTF-1.
Figure 1 also shows another notable character-
istic of long-form recognition. Assuming that the
term thyroid transcription factor 1 has an acronym
TTF-1, we can disregard candidates such as tran-
scription factor 1, factor 1, and 1 since they lack
the necessary elements (e.g., thyroid for all can-
didates; thyroid transcription for candidates fac-
tor 1 and 1; etc.) to produce the acronym TTF-
1. Similarly, we can disregard candidates such
as expression of thyroid transcription factor 1 and
encoding thyroid transcription factor 1 since they
contain unnecessary elements (i.e., expression of
and encoding) attached to the long-form. Hence,
once thyroid transcription factor 1 is chosen as
the most likely long form of the acronym TTF-
1, we prune the unlikely candidates: nested can-
didates (e.g., transcription factor 1); expansions
(e.g., expression of thyroid transcription factor 1);
and insertions (e.g., thyroid specific transcription
factor 1).
</bodyText>
<subsectionHeader confidence="0.999991">
3.2 Extracting acronyms and their contexts
</subsectionHeader>
<bodyText confidence="0.9975464">
Before describing in detail the formalization of
long-form identification, we explain the whole
process of acronym recognition. We divide the
acronym extraction task into three steps (Figure
2):
</bodyText>
<listItem confidence="0.9824912">
1. Short-form mining: identifying and extract-
ing short forms (i.e., acronyms) in a collec-
tion of documents
2. Long-form mining: generating a list of
ranked long-form candidates for each short
</listItem>
<figure confidence="0.992290452380952">
11
5
factor
1
one
expression of
3 3
gene encoding
1
co-expression of
regulation of the1
1
containing
1
expressed
1
stained for
1
identification of
216 218
213
209
factor 1 (TTF-1)
thyroid transcription
1
transription
1
transciption
1
thyroid nuclear
* These candidates are spelling mistakes
found in the MEDLINE abstracts.
thyroid
thyroid
1
protein
1
3
thyroid
1
tissue
specific4 ...... factor2 nkx22
</figure>
<figureCaption confidence="0.981871">
Figure 1: Long-form candidates for TTF-1.
</figureCaption>
<figure confidence="0.998722">
1
examined
1
explore
1
increased
1
studied
1
its
All sentences with
any acronyms
Short-form
mining
A large
collection of
text
Raw text
Contextual
sentences
for acronyms
Sentences with
a specific acronym
Acronym
dictionary
Long-form
validation
Long-form
mining
</figure>
<page confidence="0.977141">
645
</page>
<table confidence="0.9996049375">
Acronym Contextual sentence
... .................
HML Hard metal lung diseases (HML) are rare, and complex
to diagnose.
HMM Heavy meromyosin (HMM) from conditioned hearts
had a higher Ca++-ATPase activity than from controls.
HMM Heavy meromyosin (HMM) and myosin subfragment 1
(S1) were prepared from myosin by using low concen-
trations of alpha-chymotrypsin.
HMM Hidden Markov model (HMM) techniques are used to
model families of biological sequences.
HMM Hexamethylmelamine (HMM) is a cytotoxic agent
demonstrated to have broad antitumor activity.
HMN Hereditary metabolic neuropathies (HMN) are marked
by inherited enzyme or other metabolic defects.
... ...........................
</table>
<tableCaption confidence="0.9903925">
Table 1: An example of extracted acronyms and
their contextual sentences.
</tableCaption>
<bodyText confidence="0.974865909090909">
form by using a term extraction technique
3. Long-form validation: extracting short/long
form pairs recognized as having an acronym-
definition relation and eliminating unneces-
sary candidates.
The first step, short-form mining, enumerates all
short forms in a target text which are likely to be
acronyms. Most studies make use of the follow-
ing pattern to find candidate acronyms (Wren and
Garner, 2002; Schwartz and Hearst, 2003):
long form ’(’ short form ’)’
Just as the heuristic rules described in Schwartz
and Hearst (Schwartz and Hearst, 2003), we con-
sider short forms to be valid only if they consist of
at most two words; their length is between two to
ten characters; they contain at least an alphabetic
letter; and the first character is alphanumeric. All
sentences containing a short form in parenthesis
are inserted into a database, which returns all con-
textual sentences for a short form to be processed
in the next step. Table 1 shows an example of the
database content.
</bodyText>
<subsectionHeader confidence="0.932364">
3.3 Formalizing long-form mining as a term
extraction problem
</subsectionHeader>
<bodyText confidence="0.999974272727273">
The second step, long-form mining, generates a
list of long-form candidates and their likelihood
scores for each short form. As mentioned previ-
ously, we focus on words or word sequences that
co-occur frequently with a specific acronym and
not with any other surrounding words. We deal
with the problem of extracting long-form candi-
dates from contextual sentences for an acronym
in a similar manner as the term recognition task
which extracts terms from the given text. For that
purpose, we used a modified version of the C-
value method (Frantzi and Ananiadou, 1999).
C-value is a domain-independent method for
automatic term recognition (ATR) which com-
bines linguistic and statistical information, empha-
sis being placed on the statistical part. The lin-
guistic analysis enumerates all candidate terms in
a given text by applying part-of-speech tagging,
candidate extraction (e.g., extracting sequences
of adjectives/nouns based on part-of-speech tags),
and a stop-list. The statistical analysis assigns
a termhood (likelihood to be a term) to a candi-
date term by using the following features: the fre-
quency of occurrence of the candidate term; the
frequency of the candidate term as part of other
longer candidate terms; the number of these longer
candidate terms; and the length of the candidate
term.
The C-value approach is characterized by the
extraction of nested terms which gives preference
to terms appearing frequently in a given text but
not as a part of specific longer terms. This is a de-
sirable feature for acronym recognition to identify
long-form candidates in contextual sentences. The
rest of this subsection describes the method to ex-
tract long-form candidates and to assign scores to
the candidates based on the C-value approach.
Given a contextual sentence as shown in Ta-
ble 1, we tokenize a contextual sentence by
non-alphanumeric characters (e.g., space, hyphen,
colon, etc.) and apply Porter’s stemming algo-
rithm (Porter, 1980) to obtain a sequence of nor-
malized words. We use the following pattern to
extract long-form candidates from the sequence:
</bodyText>
<equation confidence="0.832845">
[:WORD:].*$ (1)
</equation>
<bodyText confidence="0.9998375">
Therein: [:WORD:] matches a non-function
word; .* matches an empty string or any word(s)
of any length; and $ matches a short form of the
target acronym. The extraction pattern accepts a
word or word sequence if the word or word se-
quence begins with any non-function word, and
ends with any word just before the corresponding
short form in the contextual sentence. We have
defined 113 function words such as a, the, of, we,
and be in an external dictionary so that long-form
candidates cannot begin with these words.
Let us take the example of a contextual sen-
tence, “we studied the expression of thyroid tran-
scription factor-1 (TTF-1)”. We extract the fol-
lowing substrings as long form candidates (words
are stemmed): 1; factor 1; transcript factor 1; thy-
roid transcriptfactor 1; expression of thyroid tran-
script factor 1; and studi the expression of thyroid
</bodyText>
<page confidence="0.998059">
646
</page>
<table confidence="0.817425666666667">
Candidate Length Freq Score Valid
Valid = I o: valid, m: letter match, L: lacks necessary letters, E: expansion,
N: nested, B: below the threshold I
</table>
<tableCaption confidence="0.97892">
Table 2: Long-form candidates for ADM.
</tableCaption>
<bodyText confidence="0.973554714285714">
transcript factor 1. Substrings such as of thyroid
transcript factor 1 (which begins with a function
word) and thyroid transcript (which ends prema-
turely before the short form) are not selected as
long-form candidates.
We define the likelihood LF(w) for candidate w
to be the long form of an acronym:
</bodyText>
<equation confidence="0.972358666666667">
� freq(t)
LF(w) = freq(w)− freq(t)� (2)
tETw freq(Tw)&apos;
</equation>
<bodyText confidence="0.97253752173913">
Therein: w is a long-form candidate; freq(x) de-
notes the frequency of occurrence of a candidate
x in the contextual sentences (i.e., co-occurrence
frequency with a short form); Tw is a set of nested
candidates, long-form candidates each of which
consists of a preceding word followed by the can-
didate w; and freq(Tw) represents the total fre-
quency of such candidates Tw.
The first term is equivalent to the co-occurrence
frequency of a long-form candidate with a short
form. The second term discounts the co-
occurrence frequency based on the frequency dis-
tribution of nested candidates. Given a long-form
candidate t E Tw, freq(t) resents the occurrence
freq(Tw) p
probability of candidate t in the nested candidate
set Tw. Therefore, the second term of the formula
calculates the expectation of the frequency of oc-
currence of a nested candidate accounting for the
frequency of candidate w.
Table 2 shows a list of long-form candidates for
acronym ADM extracted from 7,306,153 MED-
LINE abstracts10. The long-form mining step
</bodyText>
<page confidence="0.660142">
10 52GB XML files (from medline05n0001.xml to
medline05n0500.xml)
</page>
<bodyText confidence="0.999661777777778">
extracted 10,216 unique long-form candidates
from 1,319 contextual sentences containing the
acronym ADM in parentheses. Table 2 arranges
long-form candidates with their scores in de-
sending order. Long-form candidates adriamycin
and adrenomedullin co-occur frequently with the
acronym ADM.
Note the huge difference in scores between
the candidates abductor digiti minimi and minimi.
Even though the candidate minimi co-occurs more
frequently (83 times) than abductor digiti minimi
(78 times), the co-occurrence frequency is mostly
derived from the longer candidate, i.e., digiti min-
imi. In this case, the second term of Formula
2, the occurrence-frequency expectation of expan-
sions for minimi (e.g., digiti minimi), will have a
high value and will therefore lower the score of
candidate minimi. This is also true for the can-
didate digiti minimi, i.e., the score of candidate
digiti minimi is lowered by the longer candidate
abductor digiti minimi. In contrast, the candidate
abductor digiti minimi preserves its co-occurrence
frequency since the second term of the formula is
low, which means that each expansion (e.g, brevis
and abductor digiti minimi, right abductor digiti
minimi, ...) is expected to have a low frequency of
occurrence.
</bodyText>
<subsectionHeader confidence="0.993736">
3.4 Validation rules for long-form candidates
</subsectionHeader>
<bodyText confidence="0.999978090909091">
The final step of Figure 2 validates the extracted
long-form candidates to generate a final set of
short/long form pairs. According to the score
in Table 2, adriamycin is the most likely long-
form for acronym ADM. Since the long-form
candidate adriamycin contains all letters in the
acronym ADM, it is considered as an authentic
long-form (marked as ’o’ in the Valid field). This
is also true for the second and third candidate
(adrenomedullin and abductor digiti minimi).
The fourth candidate doxorubicin looks inter-
esting, i.e., the proposed method assigns a high
score to the candidate even though it lacks the let-
ters a and m, which are necessary to form the cor-
responding short form. This is because doxoru-
bicin is a synonymous term for adriamycin and de-
scribed directly with its acronym ADM. In this pa-
per, we deal with the acronym-definition relation
although the proposed method would be applica-
ble to mining other types of relations marked by
parenthetical expressions. Hence, we introduce a
constraint that a long form must cover all alphanu-
</bodyText>
<table confidence="0.871186578947369">
adriamycin 1 727 721.4 o
adrenomedullin 1 247 241.7 o
abductor digiti minimi 3 78 74.9 o
doxorubicin 1 56 54.6 L
effect of adriamycin 3 25 23.6 E
adrenodemedullated 1 19 17.7 o
acellular dermal matrix 3 17 15.9 o
peptide adrenomedullin 2 17 15.1 E
effects of adrenomedullin 3 15 13.2 E
resistance to adriamycin 3 15 13.2 E
amyopathic dermatomyositis 2 14 12.8 o
vincristine (vcr) and adriamycin 4 11 10.0 E
drug adriamycin 2 14 10.0 E
brevis and abductor digiti minimi 5 11 9.8 E
minimi 1 83 5.8 N
digiti minimi 2 80 3.9 N
right abductor digiti minimi 4 4 2.5 E
automated digital microscopy 3 1 0.0 m
adrenomedullin concentration 2 1 0.0 N
</table>
<page confidence="0.751913">
647
</page>
<table confidence="0.743064">
# [Variables]
# sf : the target short−form .
# candidates: long−form candidates.
# result: the l i s t of decisive long−forms .
# threshold: the threshold of cut−off .
# Sort long−form candidates in descending order
candidates. sort( # of scores.
key=lambda lf:lf.score, reverse=True)
# Initialize result l i s t as empty.
</table>
<figure confidence="0.972628823529412">
result = [ ]
# Pick up a long form one by one from candidates.
for l f in candidates:
# Apply a cut−off based on termhood score.
# Allow candidates with letter matching ( a )
i f lf.score &lt; threshold and not lf.match:
continue
# A long−form must contain all letters ( b )
i f letter recall(sf, l f ) &lt; 1:
continue
# Apply pruning of redundant long form ( c )
i f redundant( result, l f ) :
continue
# Insert this long form to the result list.
result.append(lf)
# Output the decisive long−forms .
print result
</figure>
<figureCaption confidence="0.999952">
Figure 3: Pseudo-code for long-form validation.
</figureCaption>
<bodyText confidence="0.999071">
meric letters in the short form.
The fifth candidate effect of adriamycin is an
expansion of a long form adriamycin, which has
a higher score than effect of adriamycin. As we
discussed previously, the candidate effect of adri-
amycin is skipped since it contains unnecessary
word(s) to form an acronym. Similarly, we prune
the candidate minimi because it forms a part of an-
other long form abductor digiti minimi, which has
a higher score than the candidate minimi. The like-
lihood score LF(w) determines the most appro-
priate long-form among similar candidates sharing
the same words or lacking some words.
We do not include candidates with scores be-
low a given threshold. Therefore, the proposed
method cannot extract candidates appearing rarely
in the text collection. It depends on the applica-
tion and considerations of the trade-off between
precision and recall, whether or not an acronym
recognition system should extract such rare long
forms. When integrating the proposed method
with e.g., Schwartz and Hearst’s algorithm, we
treat candidates recognized by the external method
as if they pass the score cut-off. In Table 2, for
example, candidate automated digital microscopy
is inserted into the result set whereas candidate
adrenomedullin concentration is skipped since it
is nested by candidate adrenomedullin.
Figure 3 is a pseudo-code for the long-form val-
idation algorithm described above. A long-form
</bodyText>
<table confidence="0.999610208333333">
Rank Parenthetic phrase # contextual # unique
sentence long-forms
1 CT 30,982 171
2 PCR 25,387 39
3 HIV 19,566 13
4 LPS 18,071 51
5 MRI 16,966 18
6 ELISA 16,527 25
7 SD 15,760 165
8 BP 14,860 145
9 DA 14,518 129
10 CSF 14,035 34
11 CNS 13,573 47
12 IL 13,423 60
13 PKC 13,414 11
14 TNF-ALPHA 12,228 14
15 HPLC 12,211 16
16 ER 12,155 140
17 RT-PCR 12,153 21
18 TNF 12,145 13
19 LDL 11,960 24
20 5-HT 11,836 20
.. .... ... ..
— (overall 50 acronyms) 600,375 4,212
</table>
<tableCaption confidence="0.99993">
Table 3: Statistics on our evaluation corpus.
</tableCaption>
<bodyText confidence="0.996246714285714">
candidate is considered valid if the following con-
ditions are met: (a) it has a score greater than
a threshold or is nominated by a letter-matching
algorithm; (b) it contains all letters in the corre-
sponding short form; and (c) it is not nested, ex-
pansion, or insertion of the previously chosen long
forms.
</bodyText>
<sectionHeader confidence="0.996798" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.987618384615385">
Several evaluation corpora for acronym recogni-
tion are available. The Medstract Gold Standard
Evaluation Corpus, which consists of 166 alias
pairs annotated to 201 MEDLINE abstracts, is
widely used for evaluation (Chang and Sch¨utze,
2006; Schwartz and Hearst, 2003). However, the
amount of the text in the corpus is insufficient for
the proposed method, which makes use of statisti-
cal features in a text collection. Therefore, we pre-
pared an evaluation corpus with a large text collec-
tion and examined how the proposed algorithm ex-
tracts short/long forms precisely and comprehen-
sively.
We applied the short-form mining described
in Section 3 to 7,306,153 MEDLINE abstracts10.
Out of 921,349 unique short-forms recognized by
the short-form mining, top 50 acronyms11 appear-
ing frequently in the abstracts were chosen for our
11We have excluded several parenthetical expressions such
as II (99,378 occurrences), OH (37,452 occurrences), and
P&lt;0.05 (23,678 occurrences). Even though they are enclosed
within parentheses, they do not introduce acronyms. We have
also excluded a few acronyms such as RA (18,655 occur-
rences) and AD (15,540 occurrences) because they have many
variations of their expanded forms to prepare the evaluation
corpus manually.
</bodyText>
<page confidence="0.995822">
648
</page>
<bodyText confidence="0.995710423076923">
evaluation corpus. We asked an expert in bio-
informatics to extract long forms from 600,375
contextual sentences with the following criteria:
a long form with minimum necessary elements
(words) to produce its acronym is accepted; a long
form with unnecessary elements, e.g., magnetic
resonance imaging unit (MRI) or computed x-ray
tomography (CT), is not accepted; a misspelled
long-form, e.g., hidden markvov model (HMM),
is accepted (to separate the acronym-recognition
task from a spelling-correction task). Table 3
shows the top 20 acronyms in our evaluation cor-
pus, the number of their contextual sentences, and
the number of unique long-forms extracted.
Using this evaluation corpus as a gold standard,
we examined precision, recall, and f-measure12 of
long forms recognized by the proposed algorithm
and baseline systems. We compared five sys-
tems: the proposed algorithm with Schwartz and
Hearst’s algorithm integrated (PM+SH); the pro-
posed algorithm without any letter-matching algo-
rithm integrated (PM); the proposed algorithm but
using the original C-value measure for long-form
likelihood scores (CV+SH); the proposed algo-
rithm but using co-occurrence frequency for long-
form likelihood scores (FQ+SH); and Schwartz
and Hearst’s algorithm (SH). The threshold for the
proposed algorithm was set to four.
Table 4 shows the evaluation result. The best-
performing configuration of algorithms (PM+SH)
achieved 78% precision and 85% recall. The
Schwartz and Hearst’s (SH) algorithm obtained a
good recall (93%) but misrecognized a number
of long-forms (56% precision), e.g., the kinetics
of serum tumour necrosis alpha (TNF-ALPHA)
and infected mice lacking the gamma interferon
(IFN-GAMMA). The SH algorithm cannot gather
variations of long forms for an acronym, e.g.,
ACE as angiotensin-converting enzyme level, an-
giotensin i-converting enzyme gene, angiotensin-
1-converting enzyme, angiotensin-converting, an-
giotensin converting activity, etc. The proposed
method combined with the Schwartz and Hearst’s
algorithm remedied these misrecognitions based
on the likelihood scores and the long-form vali-
dation algorithm. The PM+SH also outperformed
other likelihood measures, CV+SH and FQ+SH.
12We count the number of unique long forms, i.e., count
once even if short/long form pair (HMM, hidden markov
model) occurs more than once in the text collection. The
Porter’s stemming algorithm was applied to long forms be-
fore comparing them with the gold standard.
</bodyText>
<table confidence="0.999711833333333">
Method Precision Recall F-measure
PM+SH 0.783 0.849 0.809
CV+SH 0.722 0.838 0.765
FQ+SH 0.716 0.800 0.747
SH 0.555 0.933 0.681
PM 0.815 0.140 0.216
</table>
<tableCaption confidence="0.8708375">
Table 4: Evaluation result of long-form recogni-
tion.
</tableCaption>
<bodyText confidence="0.970110523809524">
The proposed algorithm without Schwartz and
Hearst’s algorithm (PM) identified long forms the
most precisely (81% precision) but misses a num-
ber of long forms in the text collection (14% re-
call). The result suggested that the proposed likeli-
hood measure performed well to extract frequently
used long-forms in a large text collection, but
could not extract rare acronym-definition pairs.
We also found the case where PM missed a set of
long forms for acronym ER which end with rate,
e.g., eating rate, elimination rate, embolic rate,
etc. This was because the word rate was used with
a variety of expansions (i.e., the likelihood score
for rate was not reduced much) while it can be
also interpreted as the long form of the acronym.
Even though the Medstract corpus is insuffi-
cient for evaluating the proposed method, we ex-
amined the number of long/short pairs extracted
from 7,306,153 MEDLINE abstracts and also ap-
pearing in the Medstract corpus. We can neither
calculate the precision from this experiment nor
compare the recall directly with other acronym
recognition methods since the size of the source
texts is different. Out of 166 pairs in Medstract
corpus, 123 (74%) pairs were exactly covered by
the proposed method, and 15 (83% in total) pairs
were partially covered13. The algorithm missed 28
pairs because: 17 (10%) pairs in the corpus were
not acronyms but more generic aliases, e.g., alpha
tocopherol (Vitamin E); 4 (2%) pairs in the cor-
pus were incorrectly annotated (e.g, long form in
the corpus embryo fibroblasts lacks word mouse to
form acronym MEFS); and 7 (4%) long forms are
missed by the algorithm, e.g., the algorithm recog-
nized pair protein kinase (PKR) while the correct
pair in the corpus is RNA-activated protein kinase
(PKR).
13Medstract corpus leaves unnecessary elements attached
to some long-forms such as general transcription factor iib
(TFIIB), whereas the proposed algorithm may drop the un-
necessary elements (i.e. general) based on the frequency. We
regard such cases as partly correct.
</bodyText>
<page confidence="0.998899">
649
</page>
<sectionHeader confidence="0.998961" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999842611111111">
In this paper we described a term recognition ap-
proach to extract acronyms and their definitions
from a large text collection. The main contribution
of this study has been to show the usefulness of
statistical information for recognizing acronyms in
large text collections. The proposed method com-
bined with a letter matching algorithm achieved
78% precision and 85% recall on the evaluation
corpus with 4,212 acronym-definition pairs.
A future direction of this study would be to
incorporate other types of relations expressed
with parenthesis such as synonym, paraphrase,
etc. Although this study dealt with the acronym-
definition relation only, modelling these relations
will also contribute to the accuracy of the acronym
recognition, establishing a methodology to distin-
guish the acronym-definition relation from other
types of relations.
</bodyText>
<sectionHeader confidence="0.999091" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999887024390244">
Eytan Adar. 2004. SaRAD: A simple and robust ab-
breviation dictionary. Bioinformatics, 20(4):527–
533.
Sophia Ananiadou and Goran Nenadic. 2006. Auto-
matic terminology management in biomedicine. In
Sophia Ananiadou and John McNaught, editors, Text
Mining for Biology and Biomedicine, pages 67–97.
Artech House, Inc.
Hiroko Ao and Toshihisa Takagi. 2005. ALICE: An
algorithm to extract abbreviations from MEDLINE.
Journal of the American Medical Informatics Asso-
ciation, 12(5):576–586.
Olivier Bodenreider. 2004. The Unified Medical Lan-
guage System (UMLS): Integrating biomedical ter-
minology. Nucleic Acids Research, 32:267–270.
Jeffrey T. Chang and Hinrich Sch¨utze. 2006. Abbre-
viations in biomedical text. In S. Ananiadou and
J. McNaught, editors, Text Mining for Biology and
Biomedicine, pages 99–119. Artech House, Inc.
Katerina T. Frantzi and Sophia Ananiadou. 1999. The
C-value / NC-value domain independent method for
multi-word term extraction. Journal ofNatural Lan-
guage Processing, 6(3):145–179.
Carol Friedman, Hongfang Liu, Lyuda Shagina,
Stephen Johnson, and George Hripcsak. 2001.
Evaluating the UMLS as a source of lexical knowl-
edge for medical language processing. In AMIA
Symposium, pages 189–193.
Toru Hisamitsu and Yoshiki Niwa. 2001. Extract-
ing useful terms from parenthetical expression by
combining simple rules and statistical measures: A
comparative evaluation of bigram statistics. In Di-
dier Bourigault, Christian Jacquemin, and Marie-
C L’Homme, editors, Recent Advances in Compu-
tational Terminology, pages 209–224. John Ben-
jamins.
Hongfang Liu and Carol Friedman. 2003. Mining
terminological knowledge in large biomedical cor-
pora. In 8th Pacific Symposium on Biocomputing
(PSB 2003), pages 415–426.
David Nadeau and Peter D. Turney. 2005. A su-
pervised learning approach to acronym identifica-
tion. In 8th Canadian Conference on Artificial In-
telligence (AI’2005) (LNAI 3501), page 10 pages.
Serguei Pakhomov. 2002. Semi-supervised maximum
entropy based approach to acronym and abbrevia-
tion normalization in medical texts. In 40th Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 160–167.
Youngja Park and Roy J. Byrd. 2001. Hybrid text min-
ing for finding abbreviations and their definitions. In
2001 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 126–133.
Martin F. Porter. 1980. An algorithm for suffix strip-
ping. Program, 14(3):130–137.
James Pustejovsky, Jos´e Casta˜no, Brent Cochran, Ma-
ciej Kotecki, and Michael Morrell. 2001. Au-
tomatic extraction of acronym meaning pairs from
MEDLINE databases. MEDINFO 2001, pages 371–
375.
Ariel S. Schwartz and Marti A. Hearst. 2003. A simple
algorithm for identifying abbreviation definitions in
biomedical text. In Pacific Symposium on Biocom-
puting (PSB 2003), number 8, pages 451–462.
Kazem Taghva and Jeff Gilbreth. 1999. Recogniz-
ing acronyms and their definitions. International
Journal on Document Analysis and Recognition (IJ-
DAR), 1(4):191–198.
Jonathan D. Wren and Harold R. Garner. 2002.
Heuristics for identification of acronym-definition
patterns within text: towards an automated con-
struction of comprehensive acronym-definition dic-
tionaries. Methods of Information in Medicine,
41(5):426–434.
Jonathan D. Wren, Jeffrey T. Chang, James Puste-
jovsky, Eytan Adar, Harold R. Garner, and Russ B.
Altman. 2005. Biomedical term mapping
databases. Database Issue, 33:D289–D293.
Hong Yu, George Hripcsak, and Carol Friedman. 2002.
Mapping abbreviations to full forms in biomedical
articles. Journal of the American Medical Informat-
ics Association, 9(3):262–272.
</reference>
<page confidence="0.997962">
650
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.550994">
<title confidence="0.998557">A Term Recognition Approach to Acronym Recognition</title>
<author confidence="0.927376">Okazaki</author>
<affiliation confidence="0.992058">Graduate School of Information Science and Technology The University of Tokyo</affiliation>
<address confidence="0.9801425">7-3-1 Hongo, Bunkyo-ku, Tokyo 113-8656 Japan</address>
<email confidence="0.948972">okazaki@mi.ci.i.u-tokyo.ac.jp</email>
<author confidence="0.978882">Sophia Ananiadou</author>
<affiliation confidence="0.993054">National Centre for Text Mining School of Informatics Manchester University</affiliation>
<address confidence="0.933874">PO Box 88, Sackville Street, Manchester M60 1QD United Kingdom</address>
<email confidence="0.970957">Sophia.Ananiadou@manchester.ac.uk</email>
<abstract confidence="0.98927535">We present a term recognition approach to extract acronyms and their definitions from a large text collection. Parenthetical expressions appearing in a text collection are identified as potential acronyms. Assuming terms appearing frequently in the proximity of an acronym to be the expanded forms (definitions) of the acronyms, we apply a term recognition method to enumerate such candidates and to measure the likelihood scores of the expanded forms. Based on the list of the expanded forms and their likelihood scores, the proposed algorithm determines the final acronym-definition pairs. The proposed method combined with a letter matching algorithm achieved 78% precision and 85% recall on an evaluation corpus with 4,212 acronym-definition pairs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eytan Adar</author>
</authors>
<title>SaRAD: A simple and robust abbreviation dictionary.</title>
<date>2004</date>
<journal>Bioinformatics,</journal>
<volume>20</volume>
<issue>4</issue>
<pages>533</pages>
<contexts>
<context position="4641" citStr="Adar, 2004" startWordPosition="679" endWordPosition="680">also describe an algorithm to combine the proposed method with a conventional letter-based method for acronym recognition. 2 Related Work The goal of acronym identification is to extract pairs of short forms (acronyms) and long forms (their expanded forms or definitions) occurring in text5. Currently, most methods are based on letter matching of the acronym-definition pair, e.g., hidden markov model (HMM), to identify short/- long form candidates. Existing methods of short/long form recognition are divided into pattern matching approaches, e.g., exploring an efficient set of heuristics/rules (Adar, 2004; Ao and Takagi, 2005; Schwartz and Hearst, 2003; Wren and Garner, 2002; Yu et al., 2002), and pattern mining approaches, e.g., Longest Common Substring (LCS) formalization (Chang and Sch¨utze, 2006; Taghva and Gilbreth, 1999). Schwartz and Hearst (2003) implemented an algorithm for identifying acronyms by using parenthetical expressions as a marker of a short form. A character matching technique was used, i.e. all letters and digits in a short form had to appear in the corresponding long form in the same order, to determine its long form. Even though the core algorithm was very simple, the au</context>
<context position="6359" citStr="Adar, 2004" startWordPosition="950" endWordPosition="951">gic receptor (ADRB2). To 5This paper uses the terms “short form” and “long form” hereafter. “Long form” is what others call “definition”, “meaning”, “expansion”, and “expanded form” of acronym. 6http://www.medstract.org/ 7These examples are obtained from the actual MEDLINE abstracts submitted to Schwartz and Hearst’s algorithm (2003). An author does not always write a proper definition with a parenthetic expression. improve the accuracy of long/short form recognition, some methods measure the appropriateness of these candidates based on a set of rules (Ao and Takagi, 2005), scoring functions (Adar, 2004), statistical analysis (Hisamitsu and Niwa, 2001; Liu and Friedman, 2003) and machine learning approaches (Chang and Sch¨utze, 2006; Pakhomov, 2002; Nadeau and Turney, 2005). Chang and Sch¨utze (2006) present an algorithm for matching short/long forms with a statistical learning method. They discover a list of abbreviation candidates based on parentheses and enumerate possible short/long form candidates by a dynamic programming algorithm. The likelihood of the recognized candidates is estimated as the probability calculated from a logistic regression with nine features such as the percentage o</context>
</contexts>
<marker>Adar, 2004</marker>
<rawString>Eytan Adar. 2004. SaRAD: A simple and robust abbreviation dictionary. Bioinformatics, 20(4):527– 533.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sophia Ananiadou</author>
<author>Goran Nenadic</author>
</authors>
<title>Automatic terminology management in biomedicine.</title>
<date>2006</date>
<booktitle>In Sophia Ananiadou and John McNaught, editors, Text Mining for Biology and Biomedicine,</booktitle>
<pages>67--97</pages>
<publisher>Artech House, Inc.</publisher>
<contexts>
<context position="2068" citStr="Ananiadou and Nenadic, 2006" startWordPosition="279" endWordPosition="282">and UniProt4) cannot keep up-to-date with the growth of neologisms (Pustejovsky et al., 2001). Although curation teams maintain terminological resources, integrating neologisms is very difficult if not based on systematic extraction and *Research Fellow of the Japan Society for the Promotion of Science (JSPS) 1http://www.ebi.ac.uk/swissprot/ 2http://www.yeastgenome.org/ 3http://www.flybase.org/ 4http://www.ebi.ac.uk/GOA/ collection of terminology from literature. Term identification in literature is one of the major bottlenecks in processing information in biology as it faces many challenges (Ananiadou and Nenadic, 2006; Friedman et al., 2001; Bodenreider, 2004). The major challenges are due to term variation, e.g. spelling, morphological, syntactic, semantic variations (one term having different termforms), term synonymy and homonymy, which are all central concerns of any term management system. Acronyms are among the most productive type of term variation. Acronyms (e.g. RARA) are compressed forms of terms, and are used as substitutes of the fully expanded termforms (e.g., retinoic acid receptor alpha). Chang and Sch¨utze (2006) reported that, in MEDLINE abstracts, 64,242 new acronyms were introduced in 20</context>
</contexts>
<marker>Ananiadou, Nenadic, 2006</marker>
<rawString>Sophia Ananiadou and Goran Nenadic. 2006. Automatic terminology management in biomedicine. In Sophia Ananiadou and John McNaught, editors, Text Mining for Biology and Biomedicine, pages 67–97. Artech House, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroko Ao</author>
<author>Toshihisa Takagi</author>
</authors>
<title>ALICE: An algorithm to extract abbreviations from MEDLINE.</title>
<date>2005</date>
<journal>Journal of the American Medical Informatics Association,</journal>
<volume>12</volume>
<issue>5</issue>
<contexts>
<context position="4662" citStr="Ao and Takagi, 2005" startWordPosition="681" endWordPosition="684">e an algorithm to combine the proposed method with a conventional letter-based method for acronym recognition. 2 Related Work The goal of acronym identification is to extract pairs of short forms (acronyms) and long forms (their expanded forms or definitions) occurring in text5. Currently, most methods are based on letter matching of the acronym-definition pair, e.g., hidden markov model (HMM), to identify short/- long form candidates. Existing methods of short/long form recognition are divided into pattern matching approaches, e.g., exploring an efficient set of heuristics/rules (Adar, 2004; Ao and Takagi, 2005; Schwartz and Hearst, 2003; Wren and Garner, 2002; Yu et al., 2002), and pattern mining approaches, e.g., Longest Common Substring (LCS) formalization (Chang and Sch¨utze, 2006; Taghva and Gilbreth, 1999). Schwartz and Hearst (2003) implemented an algorithm for identifying acronyms by using parenthetical expressions as a marker of a short form. A character matching technique was used, i.e. all letters and digits in a short form had to appear in the corresponding long form in the same order, to determine its long form. Even though the core algorithm was very simple, the authors report 99% prec</context>
<context position="6327" citStr="Ao and Takagi, 2005" startWordPosition="944" endWordPosition="947">ifferent word order, e.g., beta 2 adrenergic receptor (ADRB2). To 5This paper uses the terms “short form” and “long form” hereafter. “Long form” is what others call “definition”, “meaning”, “expansion”, and “expanded form” of acronym. 6http://www.medstract.org/ 7These examples are obtained from the actual MEDLINE abstracts submitted to Schwartz and Hearst’s algorithm (2003). An author does not always write a proper definition with a parenthetic expression. improve the accuracy of long/short form recognition, some methods measure the appropriateness of these candidates based on a set of rules (Ao and Takagi, 2005), scoring functions (Adar, 2004), statistical analysis (Hisamitsu and Niwa, 2001; Liu and Friedman, 2003) and machine learning approaches (Chang and Sch¨utze, 2006; Pakhomov, 2002; Nadeau and Turney, 2005). Chang and Sch¨utze (2006) present an algorithm for matching short/long forms with a statistical learning method. They discover a list of abbreviation candidates based on parentheses and enumerate possible short/long form candidates by a dynamic programming algorithm. The likelihood of the recognized candidates is estimated as the probability calculated from a logistic regression with nine f</context>
</contexts>
<marker>Ao, Takagi, 2005</marker>
<rawString>Hiroko Ao and Toshihisa Takagi. 2005. ALICE: An algorithm to extract abbreviations from MEDLINE. Journal of the American Medical Informatics Association, 12(5):576–586.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier Bodenreider</author>
</authors>
<title>The Unified Medical Language System (UMLS): Integrating biomedical terminology.</title>
<date>2004</date>
<journal>Nucleic Acids Research,</journal>
<pages>32--267</pages>
<contexts>
<context position="2111" citStr="Bodenreider, 2004" startWordPosition="287" endWordPosition="288"> of neologisms (Pustejovsky et al., 2001). Although curation teams maintain terminological resources, integrating neologisms is very difficult if not based on systematic extraction and *Research Fellow of the Japan Society for the Promotion of Science (JSPS) 1http://www.ebi.ac.uk/swissprot/ 2http://www.yeastgenome.org/ 3http://www.flybase.org/ 4http://www.ebi.ac.uk/GOA/ collection of terminology from literature. Term identification in literature is one of the major bottlenecks in processing information in biology as it faces many challenges (Ananiadou and Nenadic, 2006; Friedman et al., 2001; Bodenreider, 2004). The major challenges are due to term variation, e.g. spelling, morphological, syntactic, semantic variations (one term having different termforms), term synonymy and homonymy, which are all central concerns of any term management system. Acronyms are among the most productive type of term variation. Acronyms (e.g. RARA) are compressed forms of terms, and are used as substitutes of the fully expanded termforms (e.g., retinoic acid receptor alpha). Chang and Sch¨utze (2006) reported that, in MEDLINE abstracts, 64,242 new acronyms were introduced in 2004 with the estimated number being 800,000.</context>
</contexts>
<marker>Bodenreider, 2004</marker>
<rawString>Olivier Bodenreider. 2004. The Unified Medical Language System (UMLS): Integrating biomedical terminology. Nucleic Acids Research, 32:267–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey T Chang</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Abbreviations in biomedical text.</title>
<date>2006</date>
<booktitle>Text Mining for Biology and Biomedicine,</booktitle>
<pages>99--119</pages>
<editor>In S. Ananiadou and J. McNaught, editors,</editor>
<publisher>Artech House, Inc.</publisher>
<marker>Chang, Sch¨utze, 2006</marker>
<rawString>Jeffrey T. Chang and Hinrich Sch¨utze. 2006. Abbreviations in biomedical text. In S. Ananiadou and J. McNaught, editors, Text Mining for Biology and Biomedicine, pages 99–119. Artech House, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katerina T Frantzi</author>
<author>Sophia Ananiadou</author>
</authors>
<title>The C-value / NC-value domain independent method for multi-word term extraction.</title>
<date>1999</date>
<journal>Journal ofNatural Language Processing,</journal>
<volume>6</volume>
<issue>3</issue>
<contexts>
<context position="15195" citStr="Frantzi and Ananiadou, 1999" startWordPosition="2317" endWordPosition="2320">. 3.3 Formalizing long-form mining as a term extraction problem The second step, long-form mining, generates a list of long-form candidates and their likelihood scores for each short form. As mentioned previously, we focus on words or word sequences that co-occur frequently with a specific acronym and not with any other surrounding words. We deal with the problem of extracting long-form candidates from contextual sentences for an acronym in a similar manner as the term recognition task which extracts terms from the given text. For that purpose, we used a modified version of the Cvalue method (Frantzi and Ananiadou, 1999). C-value is a domain-independent method for automatic term recognition (ATR) which combines linguistic and statistical information, emphasis being placed on the statistical part. The linguistic analysis enumerates all candidate terms in a given text by applying part-of-speech tagging, candidate extraction (e.g., extracting sequences of adjectives/nouns based on part-of-speech tags), and a stop-list. The statistical analysis assigns a termhood (likelihood to be a term) to a candidate term by using the following features: the frequency of occurrence of the candidate term; the frequency of the c</context>
</contexts>
<marker>Frantzi, Ananiadou, 1999</marker>
<rawString>Katerina T. Frantzi and Sophia Ananiadou. 1999. The C-value / NC-value domain independent method for multi-word term extraction. Journal ofNatural Language Processing, 6(3):145–179.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carol Friedman</author>
<author>Hongfang Liu</author>
<author>Lyuda Shagina</author>
<author>Stephen Johnson</author>
<author>George Hripcsak</author>
</authors>
<title>Evaluating the UMLS as a source of lexical knowledge for medical language processing.</title>
<date>2001</date>
<booktitle>In AMIA Symposium,</booktitle>
<pages>189--193</pages>
<contexts>
<context position="2091" citStr="Friedman et al., 2001" startWordPosition="283" endWordPosition="286">to-date with the growth of neologisms (Pustejovsky et al., 2001). Although curation teams maintain terminological resources, integrating neologisms is very difficult if not based on systematic extraction and *Research Fellow of the Japan Society for the Promotion of Science (JSPS) 1http://www.ebi.ac.uk/swissprot/ 2http://www.yeastgenome.org/ 3http://www.flybase.org/ 4http://www.ebi.ac.uk/GOA/ collection of terminology from literature. Term identification in literature is one of the major bottlenecks in processing information in biology as it faces many challenges (Ananiadou and Nenadic, 2006; Friedman et al., 2001; Bodenreider, 2004). The major challenges are due to term variation, e.g. spelling, morphological, syntactic, semantic variations (one term having different termforms), term synonymy and homonymy, which are all central concerns of any term management system. Acronyms are among the most productive type of term variation. Acronyms (e.g. RARA) are compressed forms of terms, and are used as substitutes of the fully expanded termforms (e.g., retinoic acid receptor alpha). Chang and Sch¨utze (2006) reported that, in MEDLINE abstracts, 64,242 new acronyms were introduced in 2004 with the estimated n</context>
</contexts>
<marker>Friedman, Liu, Shagina, Johnson, Hripcsak, 2001</marker>
<rawString>Carol Friedman, Hongfang Liu, Lyuda Shagina, Stephen Johnson, and George Hripcsak. 2001. Evaluating the UMLS as a source of lexical knowledge for medical language processing. In AMIA Symposium, pages 189–193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Toru Hisamitsu</author>
<author>Yoshiki Niwa</author>
</authors>
<title>Extracting useful terms from parenthetical expression by combining simple rules and statistical measures: A comparative evaluation of bigram statistics.</title>
<date>2001</date>
<booktitle>Recent Advances in Computational Terminology,</booktitle>
<pages>209--224</pages>
<editor>In Didier Bourigault, Christian Jacquemin, and MarieC L’Homme, editors,</editor>
<publisher>John Benjamins.</publisher>
<contexts>
<context position="6407" citStr="Hisamitsu and Niwa, 2001" startWordPosition="955" endWordPosition="958">r uses the terms “short form” and “long form” hereafter. “Long form” is what others call “definition”, “meaning”, “expansion”, and “expanded form” of acronym. 6http://www.medstract.org/ 7These examples are obtained from the actual MEDLINE abstracts submitted to Schwartz and Hearst’s algorithm (2003). An author does not always write a proper definition with a parenthetic expression. improve the accuracy of long/short form recognition, some methods measure the appropriateness of these candidates based on a set of rules (Ao and Takagi, 2005), scoring functions (Adar, 2004), statistical analysis (Hisamitsu and Niwa, 2001; Liu and Friedman, 2003) and machine learning approaches (Chang and Sch¨utze, 2006; Pakhomov, 2002; Nadeau and Turney, 2005). Chang and Sch¨utze (2006) present an algorithm for matching short/long forms with a statistical learning method. They discover a list of abbreviation candidates based on parentheses and enumerate possible short/long form candidates by a dynamic programming algorithm. The likelihood of the recognized candidates is estimated as the probability calculated from a logistic regression with nine features such as the percentage of long-form letters aligned at the beginning of </context>
</contexts>
<marker>Hisamitsu, Niwa, 2001</marker>
<rawString>Toru Hisamitsu and Yoshiki Niwa. 2001. Extracting useful terms from parenthetical expression by combining simple rules and statistical measures: A comparative evaluation of bigram statistics. In Didier Bourigault, Christian Jacquemin, and MarieC L’Homme, editors, Recent Advances in Computational Terminology, pages 209–224. John Benjamins.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongfang Liu</author>
<author>Carol Friedman</author>
</authors>
<title>Mining terminological knowledge in large biomedical corpora.</title>
<date>2003</date>
<booktitle>In 8th Pacific Symposium on Biocomputing (PSB</booktitle>
<pages>415--426</pages>
<contexts>
<context position="6432" citStr="Liu and Friedman, 2003" startWordPosition="959" endWordPosition="962">rm” and “long form” hereafter. “Long form” is what others call “definition”, “meaning”, “expansion”, and “expanded form” of acronym. 6http://www.medstract.org/ 7These examples are obtained from the actual MEDLINE abstracts submitted to Schwartz and Hearst’s algorithm (2003). An author does not always write a proper definition with a parenthetic expression. improve the accuracy of long/short form recognition, some methods measure the appropriateness of these candidates based on a set of rules (Ao and Takagi, 2005), scoring functions (Adar, 2004), statistical analysis (Hisamitsu and Niwa, 2001; Liu and Friedman, 2003) and machine learning approaches (Chang and Sch¨utze, 2006; Pakhomov, 2002; Nadeau and Turney, 2005). Chang and Sch¨utze (2006) present an algorithm for matching short/long forms with a statistical learning method. They discover a list of abbreviation candidates based on parentheses and enumerate possible short/long form candidates by a dynamic programming algorithm. The likelihood of the recognized candidates is estimated as the probability calculated from a logistic regression with nine features such as the percentage of long-form letters aligned at the beginning of a word. Their method achi</context>
<context position="7671" citStr="Liu and Friedman (2003)" startWordPosition="1136" endWordPosition="1139">on and 83% recall on the Medstract corpus. Hisamitsu and Niwa (2001) propose a method for extracting useful parenthetical expressions from Japanese newspaper articles. Their method measures the co-occurrence strength between the inner and outer phrases of a parenthetical expression by using statistical measures such as mutual information, x2 test with Yate’s correction, Dice coefficient, log-likelihood ratio, etc. Their method deals with generic parenthetical expressions (e.g., abbreviation, non abbreviation paraphrase, supplementary comments), not focusing exclusively on acronym recognition. Liu and Friedman (2003) proposed a method based on mining collocations occurring before the parenthetical expressions. Their method creates a list of potential long forms from collocations appearing more than once in a text collection and eliminates unlikely candidates with three rules, e.g., “remove a set of candidates T,,, formed by adding a prefix word to a candidate w if the number of such candidates T,,, is greater than 3”. Their approach cannot recognise expanded forms occurring only once in the corpus. They reported a precision of 96.3% and a recall of 88.5% for abbreviations recognition on their test corpus.</context>
</contexts>
<marker>Liu, Friedman, 2003</marker>
<rawString>Hongfang Liu and Carol Friedman. 2003. Mining terminological knowledge in large biomedical corpora. In 8th Pacific Symposium on Biocomputing (PSB 2003), pages 415–426.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Nadeau</author>
<author>Peter D Turney</author>
</authors>
<title>A supervised learning approach to acronym identification.</title>
<date>2005</date>
<booktitle>In 8th Canadian Conference on Artificial Intelligence (AI’2005) (LNAI 3501),</booktitle>
<pages>10</pages>
<contexts>
<context position="6532" citStr="Nadeau and Turney, 2005" startWordPosition="974" endWordPosition="977">, and “expanded form” of acronym. 6http://www.medstract.org/ 7These examples are obtained from the actual MEDLINE abstracts submitted to Schwartz and Hearst’s algorithm (2003). An author does not always write a proper definition with a parenthetic expression. improve the accuracy of long/short form recognition, some methods measure the appropriateness of these candidates based on a set of rules (Ao and Takagi, 2005), scoring functions (Adar, 2004), statistical analysis (Hisamitsu and Niwa, 2001; Liu and Friedman, 2003) and machine learning approaches (Chang and Sch¨utze, 2006; Pakhomov, 2002; Nadeau and Turney, 2005). Chang and Sch¨utze (2006) present an algorithm for matching short/long forms with a statistical learning method. They discover a list of abbreviation candidates based on parentheses and enumerate possible short/long form candidates by a dynamic programming algorithm. The likelihood of the recognized candidates is estimated as the probability calculated from a logistic regression with nine features such as the percentage of long-form letters aligned at the beginning of a word. Their method achieved 80% precision and 83% recall on the Medstract corpus. Hisamitsu and Niwa (2001) propose a metho</context>
</contexts>
<marker>Nadeau, Turney, 2005</marker>
<rawString>David Nadeau and Peter D. Turney. 2005. A supervised learning approach to acronym identification. In 8th Canadian Conference on Artificial Intelligence (AI’2005) (LNAI 3501), page 10 pages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Serguei Pakhomov</author>
</authors>
<title>Semi-supervised maximum entropy based approach to acronym and abbreviation normalization in medical texts.</title>
<date>2002</date>
<booktitle>In 40th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>160--167</pages>
<contexts>
<context position="6506" citStr="Pakhomov, 2002" startWordPosition="972" endWordPosition="973">ng”, “expansion”, and “expanded form” of acronym. 6http://www.medstract.org/ 7These examples are obtained from the actual MEDLINE abstracts submitted to Schwartz and Hearst’s algorithm (2003). An author does not always write a proper definition with a parenthetic expression. improve the accuracy of long/short form recognition, some methods measure the appropriateness of these candidates based on a set of rules (Ao and Takagi, 2005), scoring functions (Adar, 2004), statistical analysis (Hisamitsu and Niwa, 2001; Liu and Friedman, 2003) and machine learning approaches (Chang and Sch¨utze, 2006; Pakhomov, 2002; Nadeau and Turney, 2005). Chang and Sch¨utze (2006) present an algorithm for matching short/long forms with a statistical learning method. They discover a list of abbreviation candidates based on parentheses and enumerate possible short/long form candidates by a dynamic programming algorithm. The likelihood of the recognized candidates is estimated as the probability calculated from a logistic regression with nine features such as the percentage of long-form letters aligned at the beginning of a word. Their method achieved 80% precision and 83% recall on the Medstract corpus. Hisamitsu and N</context>
</contexts>
<marker>Pakhomov, 2002</marker>
<rawString>Serguei Pakhomov. 2002. Semi-supervised maximum entropy based approach to acronym and abbreviation normalization in medical texts. In 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Youngja Park</author>
<author>Roy J Byrd</author>
</authors>
<title>Hybrid text mining for finding abbreviations and their definitions.</title>
<date>2001</date>
<booktitle>In 2001 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>126--133</pages>
<marker>Park, Byrd, 2001</marker>
<rawString>Youngja Park and Roy J. Byrd. 2001. Hybrid text mining for finding abbreviations and their definitions. In 2001 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 126–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin F Porter</author>
</authors>
<title>An algorithm for suffix stripping.</title>
<date>1980</date>
<journal>Program,</journal>
<volume>14</volume>
<issue>3</issue>
<contexts>
<context position="16579" citStr="Porter, 1980" startWordPosition="2534" endWordPosition="2535">zed by the extraction of nested terms which gives preference to terms appearing frequently in a given text but not as a part of specific longer terms. This is a desirable feature for acronym recognition to identify long-form candidates in contextual sentences. The rest of this subsection describes the method to extract long-form candidates and to assign scores to the candidates based on the C-value approach. Given a contextual sentence as shown in Table 1, we tokenize a contextual sentence by non-alphanumeric characters (e.g., space, hyphen, colon, etc.) and apply Porter’s stemming algorithm (Porter, 1980) to obtain a sequence of normalized words. We use the following pattern to extract long-form candidates from the sequence: [:WORD:].*$ (1) Therein: [:WORD:] matches a non-function word; .* matches an empty string or any word(s) of any length; and $ matches a short form of the target acronym. The extraction pattern accepts a word or word sequence if the word or word sequence begins with any non-function word, and ends with any word just before the corresponding short form in the contextual sentence. We have defined 113 function words such as a, the, of, we, and be in an external dictionary so t</context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>Martin F. Porter. 1980. An algorithm for suffix stripping. Program, 14(3):130–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
<author>Jos´e Casta˜no</author>
<author>Brent Cochran</author>
<author>Maciej Kotecki</author>
<author>Michael Morrell</author>
</authors>
<title>Automatic extraction of acronym meaning pairs from MEDLINE databases. MEDINFO</title>
<date>2001</date>
<pages>371--375</pages>
<marker>Pustejovsky, Casta˜no, Cochran, Kotecki, Morrell, 2001</marker>
<rawString>James Pustejovsky, Jos´e Casta˜no, Brent Cochran, Maciej Kotecki, and Michael Morrell. 2001. Automatic extraction of acronym meaning pairs from MEDLINE databases. MEDINFO 2001, pages 371– 375.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ariel S Schwartz</author>
<author>Marti A Hearst</author>
</authors>
<title>A simple algorithm for identifying abbreviation definitions in biomedical text.</title>
<date>2003</date>
<booktitle>In Pacific Symposium on Biocomputing (PSB 2003), number 8,</booktitle>
<pages>451--462</pages>
<contexts>
<context position="4689" citStr="Schwartz and Hearst, 2003" startWordPosition="685" endWordPosition="688">bine the proposed method with a conventional letter-based method for acronym recognition. 2 Related Work The goal of acronym identification is to extract pairs of short forms (acronyms) and long forms (their expanded forms or definitions) occurring in text5. Currently, most methods are based on letter matching of the acronym-definition pair, e.g., hidden markov model (HMM), to identify short/- long form candidates. Existing methods of short/long form recognition are divided into pattern matching approaches, e.g., exploring an efficient set of heuristics/rules (Adar, 2004; Ao and Takagi, 2005; Schwartz and Hearst, 2003; Wren and Garner, 2002; Yu et al., 2002), and pattern mining approaches, e.g., Longest Common Substring (LCS) formalization (Chang and Sch¨utze, 2006; Taghva and Gilbreth, 1999). Schwartz and Hearst (2003) implemented an algorithm for identifying acronyms by using parenthetical expressions as a marker of a short form. A character matching technique was used, i.e. all letters and digits in a short form had to appear in the corresponding long form in the same order, to determine its long form. Even though the core algorithm was very simple, the authors report 99% precision and 84% recall on the</context>
<context position="14014" citStr="Schwartz and Hearst, 2003" startWordPosition="2118" endWordPosition="2121">tary metabolic neuropathies (HMN) are marked by inherited enzyme or other metabolic defects. ... ........................... Table 1: An example of extracted acronyms and their contextual sentences. form by using a term extraction technique 3. Long-form validation: extracting short/long form pairs recognized as having an acronymdefinition relation and eliminating unnecessary candidates. The first step, short-form mining, enumerates all short forms in a target text which are likely to be acronyms. Most studies make use of the following pattern to find candidate acronyms (Wren and Garner, 2002; Schwartz and Hearst, 2003): long form ’(’ short form ’)’ Just as the heuristic rules described in Schwartz and Hearst (Schwartz and Hearst, 2003), we consider short forms to be valid only if they consist of at most two words; their length is between two to ten characters; they contain at least an alphabetic letter; and the first character is alphanumeric. All sentences containing a short form in parenthesis are inserted into a database, which returns all contextual sentences for a short form to be processed in the next step. Table 1 shows an example of the database content. 3.3 Formalizing long-form mining as a term ex</context>
<context position="25579" citStr="Schwartz and Hearst, 2003" startWordPosition="4046" endWordPosition="4049">3: Statistics on our evaluation corpus. candidate is considered valid if the following conditions are met: (a) it has a score greater than a threshold or is nominated by a letter-matching algorithm; (b) it contains all letters in the corresponding short form; and (c) it is not nested, expansion, or insertion of the previously chosen long forms. 4 Evaluation Several evaluation corpora for acronym recognition are available. The Medstract Gold Standard Evaluation Corpus, which consists of 166 alias pairs annotated to 201 MEDLINE abstracts, is widely used for evaluation (Chang and Sch¨utze, 2006; Schwartz and Hearst, 2003). However, the amount of the text in the corpus is insufficient for the proposed method, which makes use of statistical features in a text collection. Therefore, we prepared an evaluation corpus with a large text collection and examined how the proposed algorithm extracts short/long forms precisely and comprehensively. We applied the short-form mining described in Section 3 to 7,306,153 MEDLINE abstracts10. Out of 921,349 unique short-forms recognized by the short-form mining, top 50 acronyms11 appearing frequently in the abstracts were chosen for our 11We have excluded several parenthetical e</context>
</contexts>
<marker>Schwartz, Hearst, 2003</marker>
<rawString>Ariel S. Schwartz and Marti A. Hearst. 2003. A simple algorithm for identifying abbreviation definitions in biomedical text. In Pacific Symposium on Biocomputing (PSB 2003), number 8, pages 451–462.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazem Taghva</author>
<author>Jeff Gilbreth</author>
</authors>
<title>Recognizing acronyms and their definitions.</title>
<date>1999</date>
<journal>International Journal on Document Analysis and Recognition (IJDAR),</journal>
<volume>1</volume>
<issue>4</issue>
<contexts>
<context position="4867" citStr="Taghva and Gilbreth, 1999" startWordPosition="713" endWordPosition="716">cronyms) and long forms (their expanded forms or definitions) occurring in text5. Currently, most methods are based on letter matching of the acronym-definition pair, e.g., hidden markov model (HMM), to identify short/- long form candidates. Existing methods of short/long form recognition are divided into pattern matching approaches, e.g., exploring an efficient set of heuristics/rules (Adar, 2004; Ao and Takagi, 2005; Schwartz and Hearst, 2003; Wren and Garner, 2002; Yu et al., 2002), and pattern mining approaches, e.g., Longest Common Substring (LCS) formalization (Chang and Sch¨utze, 2006; Taghva and Gilbreth, 1999). Schwartz and Hearst (2003) implemented an algorithm for identifying acronyms by using parenthetical expressions as a marker of a short form. A character matching technique was used, i.e. all letters and digits in a short form had to appear in the corresponding long form in the same order, to determine its long form. Even though the core algorithm was very simple, the authors report 99% precision and 84% recall on the Medstract gold standard6. However, the letter-matching approach is affected by the expressions in the source text and sometimes finds incorrect long forms such as acquired syndr</context>
</contexts>
<marker>Taghva, Gilbreth, 1999</marker>
<rawString>Kazem Taghva and Jeff Gilbreth. 1999. Recognizing acronyms and their definitions. International Journal on Document Analysis and Recognition (IJDAR), 1(4):191–198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan D Wren</author>
<author>Harold R Garner</author>
</authors>
<title>Heuristics for identification of acronym-definition patterns within text: towards an automated construction of comprehensive acronym-definition dictionaries.</title>
<date>2002</date>
<journal>Methods of Information in Medicine,</journal>
<volume>41</volume>
<issue>5</issue>
<contexts>
<context position="4712" citStr="Wren and Garner, 2002" startWordPosition="689" endWordPosition="693">th a conventional letter-based method for acronym recognition. 2 Related Work The goal of acronym identification is to extract pairs of short forms (acronyms) and long forms (their expanded forms or definitions) occurring in text5. Currently, most methods are based on letter matching of the acronym-definition pair, e.g., hidden markov model (HMM), to identify short/- long form candidates. Existing methods of short/long form recognition are divided into pattern matching approaches, e.g., exploring an efficient set of heuristics/rules (Adar, 2004; Ao and Takagi, 2005; Schwartz and Hearst, 2003; Wren and Garner, 2002; Yu et al., 2002), and pattern mining approaches, e.g., Longest Common Substring (LCS) formalization (Chang and Sch¨utze, 2006; Taghva and Gilbreth, 1999). Schwartz and Hearst (2003) implemented an algorithm for identifying acronyms by using parenthetical expressions as a marker of a short form. A character matching technique was used, i.e. all letters and digits in a short form had to appear in the corresponding long form in the same order, to determine its long form. Even though the core algorithm was very simple, the authors report 99% precision and 84% recall on the Medstract gold standar</context>
<context position="13986" citStr="Wren and Garner, 2002" startWordPosition="2114" endWordPosition="2117">or activity. HMN Hereditary metabolic neuropathies (HMN) are marked by inherited enzyme or other metabolic defects. ... ........................... Table 1: An example of extracted acronyms and their contextual sentences. form by using a term extraction technique 3. Long-form validation: extracting short/long form pairs recognized as having an acronymdefinition relation and eliminating unnecessary candidates. The first step, short-form mining, enumerates all short forms in a target text which are likely to be acronyms. Most studies make use of the following pattern to find candidate acronyms (Wren and Garner, 2002; Schwartz and Hearst, 2003): long form ’(’ short form ’)’ Just as the heuristic rules described in Schwartz and Hearst (Schwartz and Hearst, 2003), we consider short forms to be valid only if they consist of at most two words; their length is between two to ten characters; they contain at least an alphabetic letter; and the first character is alphanumeric. All sentences containing a short form in parenthesis are inserted into a database, which returns all contextual sentences for a short form to be processed in the next step. Table 1 shows an example of the database content. 3.3 Formalizing l</context>
</contexts>
<marker>Wren, Garner, 2002</marker>
<rawString>Jonathan D. Wren and Harold R. Garner. 2002. Heuristics for identification of acronym-definition patterns within text: towards an automated construction of comprehensive acronym-definition dictionaries. Methods of Information in Medicine, 41(5):426–434.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan D Wren</author>
<author>Jeffrey T Chang</author>
<author>James Pustejovsky</author>
<author>Eytan Adar</author>
<author>Harold R Garner</author>
<author>Russ B Altman</author>
</authors>
<title>Biomedical term mapping databases. Database Issue,</title>
<date>2005</date>
<contexts>
<context position="2730" citStr="Wren et al. (2005)" startWordPosition="379" endWordPosition="382"> The major challenges are due to term variation, e.g. spelling, morphological, syntactic, semantic variations (one term having different termforms), term synonymy and homonymy, which are all central concerns of any term management system. Acronyms are among the most productive type of term variation. Acronyms (e.g. RARA) are compressed forms of terms, and are used as substitutes of the fully expanded termforms (e.g., retinoic acid receptor alpha). Chang and Sch¨utze (2006) reported that, in MEDLINE abstracts, 64,242 new acronyms were introduced in 2004 with the estimated number being 800,000. Wren et al. (2005) reported that 5,477 documents could be retrieved by using the acronym JNK while only 3,773 documents could be retrieved by using its full term, c-jun N-terminal kinase. In practice, there are no rules or exact patterns for the creation of acronyms. Moreover, acronyms are ambiguous, i.e., the same acronym may refer to different concepts (GR abbreviates both glucocorticoid receptor and glutathione reductase). Acronyms also have variant forms (e.g. NF kappa B, NF kB, NF-KB, NF-kappaB, NFKB factor for nuclear factor-kappa B). Ambiguity and variation present a challenge for any text mining system,</context>
</contexts>
<marker>Wren, Chang, Pustejovsky, Adar, Garner, Altman, 2005</marker>
<rawString>Jonathan D. Wren, Jeffrey T. Chang, James Pustejovsky, Eytan Adar, Harold R. Garner, and Russ B. Altman. 2005. Biomedical term mapping databases. Database Issue, 33:D289–D293.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hong Yu</author>
<author>George Hripcsak</author>
<author>Carol Friedman</author>
</authors>
<title>Mapping abbreviations to full forms in biomedical articles.</title>
<date>2002</date>
<journal>Journal of the American Medical Informatics Association,</journal>
<volume>9</volume>
<issue>3</issue>
<contexts>
<context position="4730" citStr="Yu et al., 2002" startWordPosition="694" endWordPosition="697">r-based method for acronym recognition. 2 Related Work The goal of acronym identification is to extract pairs of short forms (acronyms) and long forms (their expanded forms or definitions) occurring in text5. Currently, most methods are based on letter matching of the acronym-definition pair, e.g., hidden markov model (HMM), to identify short/- long form candidates. Existing methods of short/long form recognition are divided into pattern matching approaches, e.g., exploring an efficient set of heuristics/rules (Adar, 2004; Ao and Takagi, 2005; Schwartz and Hearst, 2003; Wren and Garner, 2002; Yu et al., 2002), and pattern mining approaches, e.g., Longest Common Substring (LCS) formalization (Chang and Sch¨utze, 2006; Taghva and Gilbreth, 1999). Schwartz and Hearst (2003) implemented an algorithm for identifying acronyms by using parenthetical expressions as a marker of a short form. A character matching technique was used, i.e. all letters and digits in a short form had to appear in the corresponding long form in the same order, to determine its long form. Even though the core algorithm was very simple, the authors report 99% precision and 84% recall on the Medstract gold standard6. However, the l</context>
</contexts>
<marker>Yu, Hripcsak, Friedman, 2002</marker>
<rawString>Hong Yu, George Hripcsak, and Carol Friedman. 2002. Mapping abbreviations to full forms in biomedical articles. Journal of the American Medical Informatics Association, 9(3):262–272.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>