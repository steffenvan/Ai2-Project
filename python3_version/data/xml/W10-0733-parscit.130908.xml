<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004902">
<title confidence="0.985332">
Using Mechanical Turk to Build Machine Translation Evaluation Sets
</title>
<author confidence="0.983883">
Michael Bloodgood
</author>
<affiliation confidence="0.930404">
Human Language Technology
Center of Excellence
Johns Hopkins University
</affiliation>
<email confidence="0.997809">
bloodgood@jhu.edu
</email>
<author confidence="0.919036">
Chris Callison-Burch
</author>
<affiliation confidence="0.691850666666667">
Center for Language and
Speech Processing
Johns Hopkins University
</affiliation>
<email confidence="0.998715">
ccb@cs.jhu.edu
</email>
<sectionHeader confidence="0.995636" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999172133333333">
Building machine translation (MT) test sets is
a relatively expensive task. As MT becomes
increasingly desired for more and more lan-
guage pairs and more and more domains, it
becomes necessary to build test sets for each
case. In this paper, we investigate using Ama-
zon’s Mechanical Turk (MTurk) to make MT
test sets cheaply. We find that MTurk can
be used to make test sets much cheaper than
professionally-produced test sets. More im-
portantly, in experiments with multiple MT
systems, we find that the MTurk-produced
test sets yield essentially the same conclu-
sions regarding system performance as the
professionally-produced test sets yield.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999857791666667">
Machine translation (MT) research is empirically
evaluated by comparing system output against refer-
ence human translations, typically using automatic
evaluation metrics. One method for establishing a
translation test set is to hold out part of the training
set to be used for testing. However, this practice typ-
ically overestimates system quality when compared
to evaluating on a test set drawn from a different do-
main. Therefore, it’s necessary to make new test sets
not only for new language pairs but also for new do-
mains.
Creating reasonable sized test sets for new do-
mains can be expensive. For example, the Workshop
on Statistical Machine Translation (WMT) uses a
mix of non-professional and professional translators
to create the test sets for its annual shared translation
tasks (Callison-Burch et al., 2008; Callison-Burch
et al., 2009). For WMT09, the total cost of creat-
ing the test sets consisting of roughly 80,000 words
across 3027 sentences in seven European languages
was approximately $39,800 USD, or slightly more
than $0.08 USD/word. For WMT08, creating test
sets consisting of 2,051 sentences in six languages
was approximately $26,500 USD or slightly more
than $0.10 USD/word.
In this paper we examine the use of Amazon’s
Mechanical Turk (MTurk) to create translation test
sets for statistical machine translation research.
Snow et al. (2008) showed that MTurk can be useful
for creating data for a variety of NLP tasks, and that
a combination of judgments from non-experts can
attain expert-level quality in many cases. Callison-
Burch (2009) showed that MTurk could be used for
low-cost manual evaluation of machine translation
quality, and suggested that it might be possible to
use MTurk to create MT test sets after an initial pi-
lot study where turkers (the people who complete
the work assignments posted on MTurk) produced
translations of 50 sentences in five languages.
This paper explores this in more detail by ask-
ing turkers to translate the Urdu sentences of the
Urdu-English test set used in the 2009 NIST Ma-
chine Translation Evaluation Workshop. We evalu-
ate multiple MT systems on both the professionally-
produced NIST2009 test set and our MTurk-
produced test set and find that the MTurk-produced
test set yields essentially the same conclusions about
system performance as the NIST2009 set yields.
</bodyText>
<page confidence="0.965632">
208
</page>
<note confidence="0.890081">
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 208–211,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.819983" genericHeader="method">
2 Gathering the Translations via
</sectionHeader>
<subsectionHeader confidence="0.665184">
Mechanical Turk
</subsectionHeader>
<bodyText confidence="0.999963027027027">
The NIST2009 Urdu-English test set1 is a pro-
fessionally produced machine translation evalua-
tion set, containing four human-produced reference
translations for each of 1792 Urdu sentences. We
posted the 1792 Urdu sentences on MTurk and asked
for translations into English. We charged $0.10 USD
per translation, giving us a total translation cost of
$179.20 USD. A challenge we encountered during
this data collection was that many turkers would
cheat, giving us fake translations. We noticed that
many turkers were pasting the Urdu into an online
machine translation system and giving us the output
as their response even though our instructions said
not to do this. We manually monitored for this and
rejected these responses and blocked these workers
from computing any of our future work assignments.
In the future, we plan to combat this in a more prin-
cipled manner by converting our Urdu sentences into
an image and posting the images. This way, the
cheating turkers will not be able to cut and paste into
a machine translation system.
We also noticed that many of the translations had
simple mistakes such as misspellings and typos. We
wanted to investigate whether these would decrease
the value of our test set so we did a second phase
of data collection where we posted the translations
we gathered and asked turkers (likely to be com-
pletely different people than the ones who provided
the initial translations) to correct simple grammar
mistakes, misspellings, and typos. For this post-
editing phase, we paid $0.25 USD per ten sentences,
giving a total post-editing cost of $44.80 USD.
In summary, we built two sets of reference trans-
lations, one with no editing, and one with post-
editing. In the next section, we present the results
of experiments that test how effective these test sets
are for evaluating MT systems.
</bodyText>
<sectionHeader confidence="0.995667" genericHeader="method">
3 Experimental Results
</sectionHeader>
<bodyText confidence="0.999352666666667">
A main purpose of an MT test set is to evaluate vari-
ous MT systems’ performances relative to each other
and assist in drawing conclusions about the relative
</bodyText>
<footnote confidence="0.873872">
1http://www.itl.nist.gov/iad/894.01/tests/mt/2009/
ResultsRelease/currentUrdu.html
</footnote>
<bodyText confidence="0.999979325581395">
quality of the translations produced by the systems.2
Therefore, if a given system, say System A, out-
performs another given system, say System B, on
a high-quality professionally-produced test set, then
we would want to see that System A also outper-
forms System B on our MTurk-produced test set. It
is also desirable that the magnitudes of the differ-
ences in performance between systems also be main-
tained.
In order to measure the differences in perfor-
mance, using the differences in the absolute mag-
nitudes of the BLEU scores will not work well be-
cause the magnitudes of the BLEU scores are af-
fected by many factors of the test set being used,
such as the number of reference translations per for-
eign sentence. For determining performance differ-
ences between systems and especially for compar-
ing them across different test sets, we use percentage
of baseline performance. To compute percentage of
baseline performance, we designate one system as
the baseline system and use percentage of that base-
line system’s performance. For example, Table 1
shows both absolute BLEU scores and percentage
performance for three MT systems when tested on
five different test sets. The first test set in the table
is the NIST-2009 set with all four reference trans-
lations per Urdu sentence. The next four test sets
use only a single reference translation per Urdu sen-
tence (ref 1 uses the first reference translation only,
ref 2 the second only, etc.). Note that the BLEU
scores for the single-reference translation test sets
are much lower than for the test set with all four ref-
erence translations and the difference in the absolute
magnitudes of the BLEU scores between the three
different systems are different for the different test
sets. However, the percentage performance of the
MT systems is maintained (both the ordering of the
systems and the amount of the difference between
them) across the different test sets.
We evaluated three different MT systems on the
NIST2009 test set and on our two MTurk-produced
test sets (MTurk-NoEditing and MTurk-Edited).
Two of the MT systems (ISI Syntax (Galley et al.,
</bodyText>
<footnote confidence="0.9371076">
2Another useful purpose would be to get some absolute
sense of the quality of the translations but that seems out of
reach currently as the values of BLEU scores (the defacto stan-
dard evaluation metric) are difficult to map to precise levels of
translation quality.
</footnote>
<page confidence="0.977516">
209
</page>
<table confidence="0.999931235294118">
Eval ISI JHU Joshua
Set (Syntax) (Syntax) (Hier.)
NIST-2009 33.10 32.77 26.65
(4 refs)
100% 99.00% 80.51%
NIST-2009 17.22 16.98 14.25
(ref 1)
100% 98.61% 82.75%
NIST-2009 17.76 17.14 14.69
(ref 2)
100% 96.51% 82.71%
NIST-2009 16.94 16.54 13.80
(ref 3)
100% 97.64% 81.46%
NIST-2009 13.63 13.67 11.05
(ref 4)
100% 100.29% 81.07%
</table>
<tableCaption confidence="0.8806525">
Table 1: This table shows three MT systems evaluated
on five different test sets. For each system-test set pair,
two numbers are displayed. The top number is the BLEU
score for that system when using that test set. For ex-
ample, ISI-Syntax tested on the NIST-2009 test set has
a BLEU score of 33.10. The bottom number is the per-
centage of baseline system performance that is achieved.
ISI-Syntax (the highest-performing system on NIST2009
</tableCaption>
<bodyText confidence="0.975524444444444">
to our knowledge) is used as the baseline. Thus, it will
always have 100% as the percentage performance for all
of the test sets. To illustrate computing the percentage
performance for the other systems, consider for JHU-
Syntax tested on NIST2009, that its BLEU score of 32.77
divided by the BLEU score of the baseline system is
32.77/33.10;:t--i 99.00%
2004; Galley et al., 2006) and JHU Syntax (Li et al.,
2009) augmented with (Zollmann and Venugopal,
2006)) were chosen because they represent state-
of-the-art performance, having achieved the highest
scores on NIST2009 to our knowledge. They also
have very similar performance on NIST2009 so we
want to see if that similar performance is maintained
as we evaluate on our MTurk-produced test sets.
The third MT system (Joshua-Hierarchical) (Li et
al., 2009), an open source implementation of (Chi-
ang, 2007), was chosen because though it is a com-
petitive system, it had clear, markedly lower perfor-
mance on NIST2009 than the other two systems and
we want to see if that difference in performance is
also maintained if we were to shift evaluation to our
MTurk-produced test sets.
Table 2 shows the results. There are a number
of observations to make. One is that the absolute
magnitude of the BLEU scores is much lower for
all systems on the MTurk-produced test sets than on
</bodyText>
<table confidence="0.999633818181818">
Eval ISI JHU Joshua
Set (Syntax) (Syntax) (Hier.)
NIST- 33.10 32.77 26.65
2009
100% 99.00% 80.51%
MTurk- 13.81 13.93 11.10
NoEditing
100% 100.87% 80.38%
MTurk- 14.16 14.23 11.68
Edited
100% 100.49% 82.49%
</table>
<tableCaption confidence="0.998718">
Table 2: This table shows three MT systems evaluated us-
</tableCaption>
<bodyText confidence="0.956178545454545">
ing the official NIST2009 test set and the two test sets we
constructed (MTurk-NoEditing and MTurk-Edited). For
each system-test set pair, two numbers are displayed. The
top number is the BLEU score for that system when using
that test set. For example, ISI-Syntax tested on the NIST-
2009 test set has a BLEU score of 33.10. The bottom
number is the percentage of baseline system performance
that is achieved. ISI-Syntax (the highest-performing sys-
tem on NIST2009 to our knowledge) is used as the base-
line.
the NIST2009 test set. This is primarily because the
NIST2009 set had four translations per foreign sen-
tence whereas the MTurk-produced sets only have
one translation per foreign sentence. Due to this
different scale of BLEU scores, we compare perfor-
mances using percentage of baseline performance.
We use the ISI Syntax system as the baseline since
it achieved the highest results on NIST2009. The
main observation of the results in Table 2 is that
both the relative performance of the various MT sys-
tems and the amount of the differences in perfor-
mance (in terms of percentage performance of the
baseline) are maintained when we use the MTurk-
produced test sets as when we use the NIST2009 test
set. In particular, we can see that whether using the
NIST2009 test set or the MTurk-produced test sets,
one would conclude that ISI Syntax and JHU Syn-
tax perform about the same and Joshua-Hierarchical
delivers about 80% of the performance of the two
syntax systems. The post-edited test set did not
yield different conclusions than the non-edited test
set yielded so the value of post-editing for test set
creation remains an open question.
</bodyText>
<sectionHeader confidence="0.994753" genericHeader="conclusions">
4 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.998084">
In conclusion, we have shown that it is feasible to
use MTurk to build MT evaluation sets at a sig-
</bodyText>
<page confidence="0.995532">
210
</page>
<bodyText confidence="0.99993665625">
nificantly reduced cost. But the large cost sav-
ings does not hamper the utility of the test set for
evaluating systems’ translation quality. In exper-
iments, MTurk-produced test sets lead to essen-
tially the same conclusions about multiple MT sys-
tems’ translation quality as much more expensive
professionally-produced MT test sets.
It’s important to be able to build MT test sets
quickly and cheaply because we need new ones for
new domains (as discussed in Section 1). Now that
we have shown the feasibility of using MTurk to
build MT test sets, in the future we plan to build
new MT test sets for specific domains (e.g., enter-
tainment, science, etc.) and release them to the com-
munity to spur work on domain-adaptation for MT.
We also envision using MTurk to collect addi-
tional training data to tune an MT system for a new
domain. It’s been shown that active learning can be
used to reduce training data annotation burdens for
a variety of NLP tasks (see, e.g., (Bloodgood and
Vijay-Shanker, 2009)). Therefore, in future work,
we plan to use MTurk combined with an active
learning approach to gather new data in the new do-
main to investigate improving MT performance for
specialized domains. But we’ll need new test sets in
the specialized domains to be able to evaluate the ef-
fectiveness of this line of research and therefore, we
will need to be able to build new test sets. In light of
the findings we presented in this paper, it seems we
can build those test sets using MTurk for relatively
low costs without sacrificing much in their utility for
evaluating MT systems.
</bodyText>
<sectionHeader confidence="0.994952" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999859">
This research was supported by the EuroMatrix-
Plus project funded by the European Commission,
by the DARPA GALE program under Contract No.
HR0011-06-2-0001, and the NSF under grant IIS-
0713448. Thanks to Amazon Mechanical Turk for
providing a $100 credit.
</bodyText>
<sectionHeader confidence="0.986734" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.951453389830509">
Michael Bloodgood and K Vijay-Shanker. 2009. Taking
into account the differences between actively and pas-
sively acquired data: The case of active learning with
support vector machines for imbalanced datasets. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 137–140, Boulder, Colorado, June. Association
for Computational Linguistics.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2008. Further
meta-evaluation of machine translation. In Proceed-
ings of the Third Workshop on Statistical Machine
Translation (WMT08), Colmbus, Ohio.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In Pro-
ceedings of the Fourth Workshop on Statistical Ma-
chine Translation (WMT09), March.
Chris Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using Amazon’s Me-
chanical Turk. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language Process-
ing, pages 286–295, Singapore, August. Association
for Computational Linguistics.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201–228.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What’s in a translation rule? In Pro-
ceedings of the Human Language Technology Con-
ference of the North American chapter of the Asso-
ciation for Computational Linguistics (HLT/NAACL-
2004), Boston, Massachusetts.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of the 21st International Conference on Com-
putational Linguistics and 44th Annual Meeting of
the Association for Computational Linguistics (ACL-
CoLing-2006), Sydney, Australia.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
Thornton, Jonathan Weese, and Omar Zaidan. 2009.
Joshua: An open source toolkit for parsing-based ma-
chine translation. In Proceedings of the Fourth Work-
shop on Statistical Machine Translation, pages 135–
139, Athens, Greece, March. Association for Compu-
tational Linguistics.
Rion Snow, Brendan O’Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast - but is it
good? Evaluating non-expert annotations for natu-
ral language tasks. In Proceedings of EMNLP-2008,
Honolulu, Hawaii.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of the NAACL-2006 Workshop on Statis-
tical Machine Translation (WMT-06), New York, New
York.
</reference>
<page confidence="0.998791">
211
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.087226">
<title confidence="0.999972">Using Mechanical Turk to Build Machine Translation Evaluation Sets</title>
<author confidence="0.901137">Michael</author>
<affiliation confidence="0.65931">Human Language Center of Johns Hopkins</affiliation>
<email confidence="0.998447">bloodgood@jhu.edu</email>
<author confidence="0.686069">Chris</author>
<affiliation confidence="0.755138">Center for Language</affiliation>
<title confidence="0.647336">Speech</title>
<author confidence="0.872724">Johns Hopkins</author>
<email confidence="0.999626">ccb@cs.jhu.edu</email>
<abstract confidence="0.997791375">Building machine translation (MT) test sets is a relatively expensive task. As MT becomes increasingly desired for more and more language pairs and more and more domains, it becomes necessary to build test sets for each case. In this paper, we investigate using Amazon’s Mechanical Turk (MTurk) to make MT test sets cheaply. We find that MTurk can be used to make test sets much cheaper than professionally-produced test sets. More importantly, in experiments with multiple MT systems, we find that the MTurk-produced test sets yield essentially the same conclusions regarding system performance as the professionally-produced test sets yield.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Michael Bloodgood</author>
<author>K Vijay-Shanker</author>
</authors>
<title>Taking into account the differences between actively and passively acquired data: The case of active learning with support vector machines for imbalanced datasets.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>137--140</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="13125" citStr="Bloodgood and Vijay-Shanker, 2009" startWordPosition="2143" endWordPosition="2146">d MT test sets quickly and cheaply because we need new ones for new domains (as discussed in Section 1). Now that we have shown the feasibility of using MTurk to build MT test sets, in the future we plan to build new MT test sets for specific domains (e.g., entertainment, science, etc.) and release them to the community to spur work on domain-adaptation for MT. We also envision using MTurk to collect additional training data to tune an MT system for a new domain. It’s been shown that active learning can be used to reduce training data annotation burdens for a variety of NLP tasks (see, e.g., (Bloodgood and Vijay-Shanker, 2009)). Therefore, in future work, we plan to use MTurk combined with an active learning approach to gather new data in the new domain to investigate improving MT performance for specialized domains. But we’ll need new test sets in the specialized domains to be able to evaluate the effectiveness of this line of research and therefore, we will need to be able to build new test sets. In light of the findings we presented in this paper, it seems we can build those test sets using MTurk for relatively low costs without sacrificing much in their utility for evaluating MT systems. Acknowledgements This r</context>
</contexts>
<marker>Bloodgood, Vijay-Shanker, 2009</marker>
<rawString>Michael Bloodgood and K Vijay-Shanker. 2009. Taking into account the differences between actively and passively acquired data: The case of active learning with support vector machines for imbalanced datasets. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 137–140, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Cameron Fordyce</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Josh Schroeder</author>
</authors>
<title>Further meta-evaluation of machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation (WMT08), Colmbus,</booktitle>
<location>Ohio.</location>
<contexts>
<context position="1761" citStr="Callison-Burch et al., 2008" startWordPosition="264" endWordPosition="267">ablishing a translation test set is to hold out part of the training set to be used for testing. However, this practice typically overestimates system quality when compared to evaluating on a test set drawn from a different domain. Therefore, it’s necessary to make new test sets not only for new language pairs but also for new domains. Creating reasonable sized test sets for new domains can be expensive. For example, the Workshop on Statistical Machine Translation (WMT) uses a mix of non-professional and professional translators to create the test sets for its annual shared translation tasks (Callison-Burch et al., 2008; Callison-Burch et al., 2009). For WMT09, the total cost of creating the test sets consisting of roughly 80,000 words across 3027 sentences in seven European languages was approximately $39,800 USD, or slightly more than $0.08 USD/word. For WMT08, creating test sets consisting of 2,051 sentences in six languages was approximately $26,500 USD or slightly more than $0.10 USD/word. In this paper we examine the use of Amazon’s Mechanical Turk (MTurk) to create translation test sets for statistical machine translation research. Snow et al. (2008) showed that MTurk can be useful for creating data f</context>
</contexts>
<marker>Callison-Burch, Fordyce, Koehn, Monz, Schroeder, 2008</marker>
<rawString>Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and Josh Schroeder. 2008. Further meta-evaluation of machine translation. In Proceedings of the Third Workshop on Statistical Machine Translation (WMT08), Colmbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Josh Schroeder</author>
</authors>
<date>2009</date>
<booktitle>Findings of the 2009 Workshop on Statistical Machine Translation. In Proceedings of the Fourth Workshop on Statistical Machine Translation (WMT09),</booktitle>
<contexts>
<context position="1791" citStr="Callison-Burch et al., 2009" startWordPosition="268" endWordPosition="271">set is to hold out part of the training set to be used for testing. However, this practice typically overestimates system quality when compared to evaluating on a test set drawn from a different domain. Therefore, it’s necessary to make new test sets not only for new language pairs but also for new domains. Creating reasonable sized test sets for new domains can be expensive. For example, the Workshop on Statistical Machine Translation (WMT) uses a mix of non-professional and professional translators to create the test sets for its annual shared translation tasks (Callison-Burch et al., 2008; Callison-Burch et al., 2009). For WMT09, the total cost of creating the test sets consisting of roughly 80,000 words across 3027 sentences in seven European languages was approximately $39,800 USD, or slightly more than $0.08 USD/word. For WMT08, creating test sets consisting of 2,051 sentences in six languages was approximately $26,500 USD or slightly more than $0.10 USD/word. In this paper we examine the use of Amazon’s Mechanical Turk (MTurk) to create translation test sets for statistical machine translation research. Snow et al. (2008) showed that MTurk can be useful for creating data for a variety of NLP tasks, and</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Schroeder, 2009</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, and Josh Schroeder. 2009. Findings of the 2009 Workshop on Statistical Machine Translation. In Proceedings of the Fourth Workshop on Statistical Machine Translation (WMT09), March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
</authors>
<title>Fast, cheap, and creative: Evaluating translation quality using Amazon’s Mechanical Turk.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>286--295</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Callison-Burch, 2009</marker>
<rawString>Chris Callison-Burch. 2009. Fast, cheap, and creative: Evaluating translation quality using Amazon’s Mechanical Turk. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 286–295, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="9624" citStr="Chiang, 2007" startWordPosition="1544" endWordPosition="1546">9, that its BLEU score of 32.77 divided by the BLEU score of the baseline system is 32.77/33.10;:t--i 99.00% 2004; Galley et al., 2006) and JHU Syntax (Li et al., 2009) augmented with (Zollmann and Venugopal, 2006)) were chosen because they represent stateof-the-art performance, having achieved the highest scores on NIST2009 to our knowledge. They also have very similar performance on NIST2009 so we want to see if that similar performance is maintained as we evaluate on our MTurk-produced test sets. The third MT system (Joshua-Hierarchical) (Li et al., 2009), an open source implementation of (Chiang, 2007), was chosen because though it is a competitive system, it had clear, markedly lower performance on NIST2009 than the other two systems and we want to see if that difference in performance is also maintained if we were to shift evaluation to our MTurk-produced test sets. Table 2 shows the results. There are a number of observations to make. One is that the absolute magnitude of the BLEU scores is much lower for all systems on the MTurk-produced test sets than on Eval ISI JHU Joshua Set (Syntax) (Syntax) (Hier.) NIST- 33.10 32.77 26.65 2009 100% 99.00% 80.51% MTurk- 13.81 13.93 11.10 NoEditing </context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American chapter of the Association for Computational Linguistics (HLT/NAACL2004),</booktitle>
<location>Boston, Massachusetts.</location>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proceedings of the Human Language Technology Conference of the North American chapter of the Association for Computational Linguistics (HLT/NAACL2004), Boston, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (ACLCoLing-2006),</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="9146" citStr="Galley et al., 2006" startWordPosition="1468" endWordPosition="1471">ng that test set. For example, ISI-Syntax tested on the NIST-2009 test set has a BLEU score of 33.10. The bottom number is the percentage of baseline system performance that is achieved. ISI-Syntax (the highest-performing system on NIST2009 to our knowledge) is used as the baseline. Thus, it will always have 100% as the percentage performance for all of the test sets. To illustrate computing the percentage performance for the other systems, consider for JHUSyntax tested on NIST2009, that its BLEU score of 32.77 divided by the BLEU score of the baseline system is 32.77/33.10;:t--i 99.00% 2004; Galley et al., 2006) and JHU Syntax (Li et al., 2009) augmented with (Zollmann and Venugopal, 2006)) were chosen because they represent stateof-the-art performance, having achieved the highest scores on NIST2009 to our knowledge. They also have very similar performance on NIST2009 so we want to see if that similar performance is maintained as we evaluate on our MTurk-produced test sets. The third MT system (Joshua-Hierarchical) (Li et al., 2009), an open source implementation of (Chiang, 2007), was chosen because though it is a competitive system, it had clear, markedly lower performance on NIST2009 than the othe</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (ACLCoLing-2006), Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Chris Callison-Burch</author>
<author>Chris Dyer</author>
<author>Juri Ganitkevitch</author>
<author>Sanjeev Khudanpur</author>
<author>Lane Schwartz</author>
<author>Wren Thornton</author>
<author>Jonathan Weese</author>
<author>Omar Zaidan</author>
</authors>
<title>Joshua: An open source toolkit for parsing-based machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Machine Translation,</booktitle>
<pages>135--139</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Athens, Greece,</location>
<contexts>
<context position="9179" citStr="Li et al., 2009" startWordPosition="1475" endWordPosition="1478">ntax tested on the NIST-2009 test set has a BLEU score of 33.10. The bottom number is the percentage of baseline system performance that is achieved. ISI-Syntax (the highest-performing system on NIST2009 to our knowledge) is used as the baseline. Thus, it will always have 100% as the percentage performance for all of the test sets. To illustrate computing the percentage performance for the other systems, consider for JHUSyntax tested on NIST2009, that its BLEU score of 32.77 divided by the BLEU score of the baseline system is 32.77/33.10;:t--i 99.00% 2004; Galley et al., 2006) and JHU Syntax (Li et al., 2009) augmented with (Zollmann and Venugopal, 2006)) were chosen because they represent stateof-the-art performance, having achieved the highest scores on NIST2009 to our knowledge. They also have very similar performance on NIST2009 so we want to see if that similar performance is maintained as we evaluate on our MTurk-produced test sets. The third MT system (Joshua-Hierarchical) (Li et al., 2009), an open source implementation of (Chiang, 2007), was chosen because though it is a competitive system, it had clear, markedly lower performance on NIST2009 than the other two systems and we want to see </context>
</contexts>
<marker>Li, Callison-Burch, Dyer, Ganitkevitch, Khudanpur, Schwartz, Thornton, Weese, Zaidan, 2009</marker>
<rawString>Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren Thornton, Jonathan Weese, and Omar Zaidan. 2009. Joshua: An open source toolkit for parsing-based machine translation. In Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 135– 139, Athens, Greece, March. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Brendan O’Connor</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Cheap and fast - but is it good? Evaluating non-expert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP-2008,</booktitle>
<location>Honolulu, Hawaii.</location>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>Rion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew Y. Ng. 2008. Cheap and fast - but is it good? Evaluating non-expert annotations for natural language tasks. In Proceedings of EMNLP-2008, Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Ashish Venugopal</author>
</authors>
<title>Syntax augmented machine translation via chart parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the NAACL-2006 Workshop on Statistical Machine Translation (WMT-06),</booktitle>
<location>New York, New York.</location>
<contexts>
<context position="9225" citStr="Zollmann and Venugopal, 2006" startWordPosition="1481" endWordPosition="1484"> set has a BLEU score of 33.10. The bottom number is the percentage of baseline system performance that is achieved. ISI-Syntax (the highest-performing system on NIST2009 to our knowledge) is used as the baseline. Thus, it will always have 100% as the percentage performance for all of the test sets. To illustrate computing the percentage performance for the other systems, consider for JHUSyntax tested on NIST2009, that its BLEU score of 32.77 divided by the BLEU score of the baseline system is 32.77/33.10;:t--i 99.00% 2004; Galley et al., 2006) and JHU Syntax (Li et al., 2009) augmented with (Zollmann and Venugopal, 2006)) were chosen because they represent stateof-the-art performance, having achieved the highest scores on NIST2009 to our knowledge. They also have very similar performance on NIST2009 so we want to see if that similar performance is maintained as we evaluate on our MTurk-produced test sets. The third MT system (Joshua-Hierarchical) (Li et al., 2009), an open source implementation of (Chiang, 2007), was chosen because though it is a competitive system, it had clear, markedly lower performance on NIST2009 than the other two systems and we want to see if that difference in performance is also main</context>
</contexts>
<marker>Zollmann, Venugopal, 2006</marker>
<rawString>Andreas Zollmann and Ashish Venugopal. 2006. Syntax augmented machine translation via chart parsing. In Proceedings of the NAACL-2006 Workshop on Statistical Machine Translation (WMT-06), New York, New York.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>