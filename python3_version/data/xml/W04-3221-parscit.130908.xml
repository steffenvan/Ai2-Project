<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000005">
<title confidence="0.929012">
Attribute-Based and Value-Based Clustering: An Evaluation
</title>
<author confidence="0.941854">
Abdulrahman ALMUHAREB and Massimo POESIO
</author>
<affiliation confidence="0.915155">
Department of Computer Science and Centre for Cognitive Science
University of Essex
Colchester, United Kingdom, CO4 3SQ
</affiliation>
<email confidence="0.992246">
aalmuh@essex.ac.uk poesio@essex.ac.uk
</email>
<sectionHeader confidence="0.992322" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999954882352941">
In most research on concept acquisition from
corpora, concepts are modeled as vectors of
relations extracted from syntactic structures.
In the case of modifiers, these relations often
specify values of attributes, as in (attr
red); this is unlike what typically proposed in
theories of knowledge representation, where
concepts are typically defined in terms of their
attributes (e.g., color). We compared
models of concepts based on values with
models based on attributes, using lexical
clustering as the basis for comparison. We
find that attribute-based models work better
than value-based ones, and result in shorter
descriptions; but that mixed models including
both the best attributes and the best values
work best of all.
</bodyText>
<sectionHeader confidence="0.998693" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999901254901961">
In most recent research on concept acquisition
from corpora (e.g., for lexicon construction),
concepts are viewed as vectors of relations, or
properties, extracted from syntactic structures
(Grefenstette, 1993; Lin, 1998; Curran and Moens,
2002; Kilgarriff, 2003, and many others). These
properties often specify values of attributes such as
color, shape, or size: for example, the vector used
by Lin (1998) for the concept dog includes the
property (dog adj-mod brown). (We will
use the term values here to refer to any modifier.)
To our knowledge, however, no attempt has been
made by computational linguists to use the
attributes themselves in such vectors: i.e., to learn
that the description of the concept dog includes
elements such as (dog color) or (dog
size). This is surprising when considering that
most models of concepts in the AI literature are
based on such attributes (Brachman and Levesque,
1985).
Two problems need to be addressed when trying
to identify concept attributes. The first problem is
that values are easier to extract. We found,
however, that patterns like the X of the dog,
already used in (Berland and Charniak, 1999;
Poesio et al, 2002) to find part-of relations (using
techniques derived from those used in (Hearst,
1998; Caraballo, 1999) to find hyponymy
relations) are quite effective at finding attributes.
A second problem might be that instances of such
patterns are less frequent than those used to extract
values, even in large corpora such as the British
National Corpus (BNC). But this problem, as well,
is less serious when using the Web as a corpus
(Kilgarriff and Schuetze, 2003; Keller and Lapata,
2003; Markert et al, submitted).
We report on two experiments whose goal was
to test whether identifying attributes leads to better
lexical descriptions of concepts. We do this by
comparing the results obtained by using attributes
or more general modifiers – that we will simply
call values – as elements of concept vectors used
to identify concept similarities via clustering. In
Section 2, we discuss how Web data were used to
build attribute- and value- based concept vectors,
and our clustering and evaluation methods. In
Section 3, we discuss a first experiment using the
set of concepts used in (Lund and Burgess, 1996).
In Section 4, we discuss a second experiment using
214 concepts from WordNet (Fellbaum, 1998). In
Section 5 we return to the notion of attribute.
</bodyText>
<sectionHeader confidence="0.999142" genericHeader="introduction">
2 Methods
</sectionHeader>
<subsectionHeader confidence="0.9871525">
2.1 Using Text Patterns to Build Concept
Descriptions
</subsectionHeader>
<bodyText confidence="0.999953935483871">
Our techniques for extracting concept
descriptions are simpler than those used in other
work in at least two respects. First of all, we only
extracted values expressed as nominal modifiers,
ignoring properties expressed by verbal
constructions in which the concept occurred as an
argument (e.g., Lin’s (dog obj-of have)).
(We originally made this simplification to
concentrate on the comparison between attributes
and values (many verbal relations express more
complex properties), but found that the resulting
descriptions were still adequate for clustering.)
Secondly, our data were not parsed or POS-tagged
prior to extracting concept properties; our patterns
are word-based. Full parsing is essential when
complete descriptions are built (see below) and
allows the specification of much more general
patterns (e.g., matching descriptions modified in a
variety of ways, see below), but is computationally
much more expensive, particularly when Web data
are used, as done here. We also found that when
using the Web, simple text patterns not requiring
parsing or POS tagging were sufficient to extract
large numbers of instances of properties with a
good degree of precision.
Our methods for extracting &apos;values&apos; are
analogous to those used in the previous literature,
apart from the two simplifications just mentioned:
i.e., we just consider every nominal modifier as
expressing a potential property. The pattern we
use to extract values is as follows:
</bodyText>
<listItem confidence="0.996026">
• &amp;quot;[a|an|the] * C [is|was]&amp;quot;
</listItem>
<bodyText confidence="0.9985015">
where C is a concept, and the wildcard (*) stands
for an unspecified value. The restriction to
instances containing is or was to ensure that the C
actually stands for a concept (i.e., avoiding
modifiers) proved adequate to ensure precision. An
example of text matching this pattern is:
</bodyText>
<listItem confidence="0.931535">
• ... an inexpensive car is ...
</listItem>
<bodyText confidence="0.999900823529412">
The pattern we use for extracting concept
attributes is based on linguistic tests for attributes
already discussed, e.g., in (Woods, 1975).
According to Woods, A is an attribute of C if we
can say [V is a/the A of C]: e.g., brown is a color
of dogs. If no V can be found which is a value of
A, then A can not be an attribute for the concept C.
This test only selects attributes that have values,
and is designed to exclude other functions defined
over concepts, such as parts. But some of these
functions can be (and have been) viewed as
defining attributes of concepts as well; so for the
moment we used more general patterns identifying
all relational nouns taking a particular concept as
arguments. (We return on the issue of the
characterization of attributes below.) Our pattern
for attributes is shown below:
</bodyText>
<listItem confidence="0.99747">
• &amp;quot;the * of the C [is|was]&amp;quot;
</listItem>
<bodyText confidence="0.9990805">
where again C is a concept, but the wildcard
denotes an unspecified attribute. Again, is/was is
used to increase precision. An example of text
matching this pattern is:
</bodyText>
<listItem confidence="0.830161">
• ... the price of the car was ...
</listItem>
<bodyText confidence="0.999770384615385">
Both of the patterns we use satisfy Hearst&apos;s
desiderata for good patterns (Hearst, 1998): they
are (i) frequent, (ii) precise, and (iii) easy to
recognize. Patterns similar to our attribute pattern
were used by Berland and Charniak (1999) and
Poesio et al (2002) to find object parts only; after
collecting their data, Berland and Charniak filtered
out words ending with &amp;quot;ness&amp;quot;, &amp;quot;ing&amp;quot;, and &amp;quot;ity&amp;quot;,
because these express qualities of objects, and used
a ranking method to rank the remaining words.
(An accuracy of 55% for the top 50 proposed parts
was reported.) We found that these patterns can be
used to collect other sorts of &apos;attributes&apos;, as well.
</bodyText>
<subsectionHeader confidence="0.999827">
2.2 Web Data Collection through Google
</subsectionHeader>
<bodyText confidence="0.997820538461538">
In recent years there has been growing evidence
that using the Web as a corpus greatly reduces the
problem of data sparseness, and its size more than
compensates the lack of balance (e.g., (Keller and
Lapata, 2003)). The benefits from using the Web
over even large corpora like the BNC for
extracting semantic relations, particularly when
using simple text patterns, were informally pointed
out in (Poesio, 2003) and demonstrated more
systematically by Markert et al (submitted). These
findings were confirmed by our experiments. A
comparison of numbers of instances of some
patterns using the Web and the BNC is shown in
</bodyText>
<tableCaption confidence="0.865956">
Table 1.
</tableCaption>
<figureCaption confidence="0.752644444444445">
Pattern Web BNC
Attribute &amp;quot;the * of the *&amp;quot; 23,100,000 208,155
&amp;quot;the * of the * is&amp;quot; 10,900,000 3,627
&amp;quot;the * of the car is&amp;quot; 26,400 5
&amp;quot;the * of the hat is&amp;quot; 2,770 1
Value &amp;quot;the fast * is&amp;quot; 38,100 3
&amp;quot;an electronic * is&amp;quot; 120,000 5
&amp;quot;the * car is&amp;quot; 84,500 24
&amp;quot;the * hat is&amp;quot; 17,100 1
</figureCaption>
<tableCaption confidence="0.833632333333333">
Table 1: Comparison of frequencies of some
patterns in BNC and the Web. Web frequency is
based on Google counts
</tableCaption>
<bodyText confidence="0.99539025">
We collect our data from the Web using the
Google search engine, accessed via the freely
available Google Web API1. The API only allows
to retrieve the first 1,000 results per search request;
to overcome this restriction, we use the daterage
feature of the Google search request. This feature
allows the user to fragment the search space into a
number of periods, hence retrieving only pages that
have been updated during a specified period. In the
two experiments presented here, we aimed to
collect up to 10,000 matches per search request
using the daterage feature: we divided the search
space into 100 days starting from January, 1990
until mid 2004. (The procedure we used does not
guarantee collecting all the instances in the
accessed periods, because if there are more than
</bodyText>
<footnote confidence="0.9713945">
1 Google Web API is available on the Web at
http://www.google.com/apis/
</footnote>
<bodyText confidence="0.978402428571429">
1,000 instances in one period, then only the first
1,000 instances will be collected.) 2
Our requests to Google take the general form &amp;quot;s1
* s2&amp;quot; (including the double quotes), where s1 and s2
are two strings and the wildcard denotes an
unspecified single word. For example, the search
request &amp;quot;a * car is&amp;quot; catches instances such as: [a
red car is], [a small car is], and [a sport car is]. It
is worth mentioning that Google does not pay
attention to punctuation marks; this is one area in
which parsing would help.
When receiving results from Google, we do not
access the actual Web pages, but instead we
process the snippets that are returned by Google.3
</bodyText>
<subsectionHeader confidence="0.974258">
2.3 Clustering Methods
</subsectionHeader>
<bodyText confidence="0.999970782608696">
The task that we use to compare concept
descriptions is lexical acquisition via clustering.
We experimented with clustering systems such as
COBWEB (Fisher, 1987) and SUBDUE (Cook and
Holder, 2000) before settling on CLUTO 2.1
(Karypis, 2002). CLUTO is a general-purpose
clustering tool that implements three different
clustering algorithms: partitional, agglomerative,
and graph partitioning algorithms. CLUTO
produces both flat and hierarchical clusters. It uses
a hard clustering technique, where each concept
can be assigned to only one cluster. The software
allows to choose a similarity metric between a set
including extended Jaccard and cosine. CLUTO
was optimized to cluster data of large sizes in a
reasonable time. The software also provides
analysis and visualization tools.
In this paper, we use extended Jaccard, which
was found to produce more accurate results than
the cosine function in similar tasks (Karypis, 2002;
Curran and Moens, 2003). In CLUTO, the
extended Jaccard function works only with the
graph partitioning algorithm.
</bodyText>
<subsectionHeader confidence="0.992513">
2.4 Evaluation Measures
</subsectionHeader>
<bodyText confidence="0.999729">
We used two types of measures to evaluate the
clusters produced by CLUTO using the concept
descriptions discussed above, both of which
compare the clusters produced by the system to
model clusters. Accuracy is computed by dividing
the number of correctly clustered concepts by the
total number of concepts. The number of correctly
clustered concepts is determined by examining
</bodyText>
<footnote confidence="0.951722">
2 Also, registered users of the API can send up to 1,000
requests per day, but our daily limit was increased by
Google to 20,000 requests per day.
3 Snippets are text excerpts captured from the actual
web pages with embedded HTML tags. We process the
snippets by removing the HTML tags and extracting the
targeted piece of text that was specified in the request.
</footnote>
<bodyText confidence="0.999866842105263">
each system cluster, finding the class of each
concept in the model clusters, and determining the
majority class. The cluster is then labeled with this
class; the concepts belonging to it are taken to be
correctly clustered, whereas the remaining
concepts are judged to be incorrectly clustered.
In the contingency table evaluation (Swets,
1969; Hatzivassiloglou and McKeown, 1993), the
clusters are converted into two lists (one for the
system clusters and one for the model clusters) of
yes-no answers to the question &amp;quot;Does the pair of
concepts occur in the same cluster?&amp;quot; for each pair
of concepts. A contingency table is then built,
from which recall (R), precision (P), fallout, and F
measures can be computed. For example, if the
model clusters are: (A, B, C) and (D), and the
system clusters are: (A, B) and (C, D), the yes-no
lists are as in Table 2, and the contingency table is
as in Table 3.
</bodyText>
<table confidence="0.945467071428571">
Question Model System
Answer Answer
Does the pair (A, B) occur in Yes Yes
the same cluster?
Does the pair (A, C) occur in Yes No
the same cluster?
Does the pair (A, D) occur in No No
the same cluster?
Does the pair (B, C) occur in Yes No
the same cluster?
Does the pair (B, D) occur in No No
the same cluster?
Does the pair (C, D) occur in No Yes
the same cluster?
</table>
<tableCaption confidence="0.9541625">
Table 2: Model and the system answers for the
co-occurrence question
</tableCaption>
<table confidence="0.99978925">
System Answer Model Answer
Yes No
Yes a 1 b 1
No c 2 d 2
</table>
<tableCaption confidence="0.999827">
Table 3: The contingency table
</tableCaption>
<figure confidence="0.9881302">
a
P = = 0.50
a+b
0.33 F =2×R×P w0.40
R+P
</figure>
<sectionHeader confidence="0.572715" genericHeader="method">
3 First Experiment: Using a Set of Concepts
from Lund and Burgess
</sectionHeader>
<bodyText confidence="0.99853525">
One limitation of using Google is that even with
an increased daily limit of 20,000, it wouldn’t
really be feasible to attempt to cluster, say, all of
WordNet 100,000 noun concepts. For this reason,
</bodyText>
<figure confidence="0.978525833333333">
a 0.33
R = w
a +c
b
Fallout = w
b+d
</figure>
<bodyText confidence="0.9990396">
we used much smaller sets of concepts in our two
experiments. The first set allowed us to compare
our results with those obtained by Lund and
Burgess (1996); the second set consisted of a larger
number of concepts from WordNet.
Lund and Burgess (1996) used a set of 34
concepts belonging to 3 different classes (animals,
body parts, and geographical locations) to evaluate
their method for acquiring lexical representations,
HAL (Hyperspace Analogue to Language). Lund
and Burgess were able to correctly cluster all of the
concepts except for one body part, tooth, which
was incorrectly clustered with animals. In this first
experiment, we used the 34 Lund and Burgess
concepts plus Italy, horse, and tongue (37 in total)
to compare value-based and attribute-based
description when used for clustering, using concept
descriptions collected using the methods described
above.
The input to clustering is a frequency table with
concepts as rows and values, attributes, or both
attributes and values as columns. Each cell in the
table contains the frequency of co-occurrence
between the concept and corresponding value or
attribute. Before clustering, the frequencies are
transformed into weighted values using the t test
(Manning and Schutze, 1999). (The t test was
found by Curran and Moens (2002) to be the best
weighting method.) The t test formula we used for
attributes is shown below:
</bodyText>
<subsectionHeader confidence="0.572464">
r C(concepti )x C(attributej )1
</subsectionHeader>
<bodyText confidence="0.706502">
I` N 2 J
</bodyText>
<subsectionHeader confidence="0.663994">
C(concept
</subsectionHeader>
<bodyText confidence="0.880816">
N
where N is the total number of relations, and C is a
count function. The values formula is similar.
We use the CLUTO vcluster command for
clustering, with parameters: similarity function =
Extended Jaccard Coefficient, clustering method =
Graph Partitioning, no. of clusters = 3.
</bodyText>
<table confidence="0.999557875">
Used Data Vector Size4
500 1522 3044 4753 4969
Values 64.86% 94.59% - - 94.59%
Only
Attributes 97.30% 97.30% - 97.30% -
Only
Attributes1522 - - 100.00% - -
and Values1522
</table>
<tableCaption confidence="0.984599333333333">
Table 4: Clustering accuracy with values,
attributes, and their combination, using different
vector sizes
</tableCaption>
<footnote confidence="0.4659185">
4 Here, we choose the top k features by their overall
frequency.
</footnote>
<bodyText confidence="0.99959288">
Table 4 shows the accuracy of the produced
clusters when using values, attributes, and the
combination with different vector sizes. The
results show that with concept descriptions of
length 500, attributes (97.30%) are much more
accurate than values (64.86%). With vectors of
size 1522, the accuracy with attributes remains the
same, while the accuracy with values improves,
but is still lower than the accuracy with attributes
(94.59%). This indicates that attributes have more
discriminatory power than values: an attribute
vector of size 500 is sufficient to produce a more
accurate result than using a value vector of three
times the size. But perhaps the most interesting
result is that even though further increasing the
size of pure attribute- and value- descriptions (to
4753 and 4969, respectively) does not improve
accuracy, perfect accuracy can be obtained by
using vectors of length 3044, including the 1522
best attributes and the 1522 best values. This
suggests that while attributes are a good way of
generalizing across properties, not all properties of
concepts can be viewed as attribute/value pairs
(section 5; also (Poesio and Almuhareb,
submitted)).
</bodyText>
<sectionHeader confidence="0.978421" genericHeader="method">
4 Second Experiment: Using a Set of
Concepts from WordNet
</sectionHeader>
<bodyText confidence="0.9998319">
In order to get a more realistic evaluation and a
better comparison with work such as (Lin, 1998;
Curran and Moens, 2002), we also ran a second
experiment using a larger set of concepts from the
WordNet noun hierarchy (Fellbaum, 1998). We
chose 214 relatively common concepts from 13
different classes covering a variety of
subhierarchies (see Appendix A). Each class
contains a set of concepts that share a common
hypernym in the WordNet hierarchy.
</bodyText>
<table confidence="0.999381">
Systems Answer Model Answer
Yes No
Boolean Yes 1294 503
No 387 20607
Frequency Yes 1117 950
No 564 20160
</table>
<tableCaption confidence="0.9486225">
Table 5: The contingency table based on boolean
and frequency for the combined attributes and
</tableCaption>
<bodyText confidence="0.926693666666667">
values
The frequencies for attributes and values were
again collected as in the first experiment.
However, these data were used in a different way.
In determining the weight, we performed the t test5
on boolean values instead of the original
</bodyText>
<equation confidence="0.557371384615385">
5 We consider only positive values of t.
C(concept
, attribute )
i j �
N
ti,j �
i
attribute
,
j
)
2
(1)
</equation>
<bodyText confidence="0.999373">
frequencies6, treating all positive frequencies as 1
and everything else as 0. This eliminates the effect
of variations in frequencies in the original data, the
intuition being that frequencies do not add to the
semantics of concepts: what we are interested in is
the fact that a concept has a given attribute/value,
regardless of how many times we have
encountered this fact. This approach is similar to
the approach adopted in (Hearst, 1998); see also
(Curran and Moens, 2002) for a comparison of
methods dealing with concept vectors based on
raw frequencies or boolean values. The
transformed table is a binary table that contains
only zeros and ones in its cells. Table 5 shows the
contingency table for clusters produced based on
boolean and frequency for the combined data of
attributes and values; it shows that boolean data is
more accurate in the four cases.
For clustering, as well, we used CLUTO in a
different way. Instead of asking CLUTO to
compute the similarities between the concepts, we
computed them ourselves, using the version of the
extended Jaccard similarity function used by
Curran and Moens, as this version produces better
results than the one used in CLUTO. The two
versions of the extended Jaccard function are
shown below:
</bodyText>
<equation confidence="0.5250176">
E (tm,i x tn,i)
sim (concept
Curran&amp;Moens
E (tm,i xtn,i)
i
</equation>
<bodyText confidence="0.970931333333333">
where tm,i and tn,i are the weighted co-occurrence
values between concept m and concept n with
attribute/value i, and computed as in equation (1).
</bodyText>
<table confidence="0.998353444444445">
Used Measures
Data7
Accuracy Recall Precision Fallout F
Values 71.96% 58.48% 52.91% 04.14% 55.55%
Only
Attributes 64.02% 59.90% 53.54% 04.14% 56.54%
Only
Attributes 85.51% 76.98% 72.01% 02.38% 74.41%
And Values
</table>
<tableCaption confidence="0.8230675">
Table 6: Clustering evaluation based on values,
attributes, and the combination
</tableCaption>
<bodyText confidence="0.994631333333333">
We compute the similarity between each pair of
concepts, produce a similarity matrix and send it to
CLUTO for clustering. We then call the scluster
</bodyText>
<footnote confidence="0.86591375">
6 In equation (1), this will effect only C(concepti,
attributej), other counts will not be effected.
7 Here, we use full size vectors that contain all the
features.
</footnote>
<bodyText confidence="0.998914142857143">
command of CLUTO with the following
parameters: clustering method = Graph
Partitioning, no. of clusters = 13. The results of the
evaluation are shown in Table 6.
Value-based concept descriptions resulted in
better clusters than attribute-based when measured
using Accuracy (71.96% vs. 64.02%), but the other
measures all indicate that attributes work slightly
better than values: e.g., F=55.55% for values,
56.64% for attributes. The reason for this
difference is that the Accuracy measure simply
evaluates if each concept is assigned to its correct
cluster, while the remaining measures concern
about the relation between each pair of concepts
(i.e., if they were assigned to the same cluster or
not). But, just as in Experiment 1, the best results
by any measure are again obtained when using
concept descriptions containing the best &apos;attributes&apos;
and the best &apos;values&apos;; this time, however, the
difference is much more significant: Accuracy is
85.51%, F is 74.41%.
</bodyText>
<table confidence="0.999001">
System Cluster Model Cluster
Building Disease Vehicle Feeling Body Part Fruit Creator Publication Animal Furniture Cloth F. Relation Time
1 0 2 0 11 0 0 0 0 0 0 0 0 0
2 0 0 13 0 0 0 0 0 0 0 0 0 0
3 0 0 0 0 0 0 0 0 0 0 0 0 17
4 0 0 0 0 0 0 2 0 18 0 0 6 0
5 2 0 0 1 0 16 0 0 1 0 0 1 0
6 1 16 0 0 0 0 0 0 0 0 0 0 1
7 0 0 0 0 0 0 15 0 0 0 0 0 0
8 0 0 0 0 0 0 0 15 0 0 0 0 0
9 0 0 0 0 16 0 0 0 0 4 0 0 1
10 0 0 0 0 0 0 0 0 1 0 0 9 0
11 1 0 1 0 0 0 0 0 0 6 0 0 0
12 0 0 0 0 0 0 0 0 0 2 16 0 0
13 15 0 0 1 0 0 0 1 0 2 0 0 0
</table>
<tableCaption confidence="0.999559">
Table 7: The confusion matrix for the clusters
</tableCaption>
<bodyText confidence="0.9554134375">
produced using both attributes and values
Table 7 shows the confusion matrix for the
clusters produced using both attributes and values.
A close inspection of these clusters reveals that
&apos;furniture&apos; concepts were the less homogeneous
because they were scattered among four different
clusters. There are 14 &apos;furniture&apos; concepts; six of
them (bookcase, cabinet, couch, cradle, desk and
wardrobe) were grouped in a separate cluster
which also contains two more concepts (pickup
and greenhouse). Four of the concepts (bed,
lamp, seat, and table) were clustered with &apos;body
part&apos; concepts. Two of the concepts (dresser and
sofa) were clustered with &apos;cloth&apos; concepts, and the
remaining two concepts (chair and lounge) were
clustered with &apos;building&apos; concepts.
</bodyText>
<figure confidence="0.61617452631579">
+
))
E
− (tm,i x tn ,i
(tm ,i
tn ,i
i
i
)
,concept
m
n
�
)
(tm,i + tn ,i
E
i
sim (concept m ,concept n ) �
CLUTO
</figure>
<bodyText confidence="0.999889423076923">
Two points should be noted about the furniture
concepts. First, at least two concepts (seat and
lounge) have more than one sense in WordNet.
Seat was clustered with body part concepts, which
is acceptable if we think of seat as &amp;quot;the fleshy part
of the human body that you sit on&amp;quot; (WordNet,
sense 2). The same for lounge, which was
clustered with buildings, which is consistent with
its second sense in WordNet: &amp;quot;a public room (as in
a hotel or airport) with seating where people can
wait&amp;quot;. This indicates that techniques for
differentiating between different senses are needed
– e.g., using a soft clustering technique as in
(Pereira et al, 1993) instead of a hard clustering
technique. Second, furniture concepts may not
have a common prototype that is shared by all of
the member concepts. This is a well known
problem in the prototype theory of concepts
(Laurence and Margolis, 1999).
The greater compactness of attribute-based
representations vs. value-based ones was more
evident in this second experiment. We collected
51,045 distinct values and 8,934 distinct attributes;
the total number of value-concept relations is
1,026,335, compared to 422,621 attribute-concept
relations.
</bodyText>
<sectionHeader confidence="0.69723" genericHeader="evaluation">
5 Attributes and Values: A discussion
</sectionHeader>
<bodyText confidence="0.994942058823529">
Although our results suggest that trying to
identify attributes is beneficial, the notion of
&apos;attribute&apos; is not completely clear, and has been
used in widely different ways in Knowledge
Representation literature. An attempt of defining
the notion has been made by Guarino (1992), who
classifies attributes into relational and non-
relational attributes. Relational attributes include
qualities such as color and position, and relational
roles such as son and spouse. Non-relational
attributes include parts such as wheel and engine.
The Qualia Structure of the Generative Lexicon
(Pustejovsky, 1991) is another attempt at
identifying &amp;quot;the essential attributes of an object as
defined by the lexical item&amp;quot;. Pustejovsky
identifies four roles: Constitutive Role (Guarino&apos;s
parts), Formal Role (Guarino&apos;s qualities), Agentive
Role (Guarino&apos;s relational roles), and Telic Role
(not included in Guarino&apos;s classification).
Our analysis of the attribute data shows that the
attributes we found can be mapped in the four roles
of the Qualia structure. Table 8 shows how we
manually mapped the top 50 attributes of the
concept car to the Qualia roles and the Guarino&apos;s
classes. This mapping is not trivial (e.g., a path is
not part of a car, and design can be regarded as a
quality), but a variety of tests may help:
Morphological and Ontological Tests:
Dixon (1991) proposed a semantic classification
for nouns. According to Dixon, parts are concrete
concepts and mostly basic noun roots or rarely
derived from verbs, while qualities are abstract
concepts and many of them are basic noun roots or
derived from adjectives, some derived from stems,
and few derived from verbs. Our observations also
suggest that telic attributes are usually derived
from verbs.
Attributes Test: Since attributes can also be
viewed as concepts (e.g., in WordNet), they
themselves should have some shared attributes.
For example: since parts are concrete objects they
should share attributes such as size, length, and
geometry. Also, since qualities usually can be
assigned values (e.g. age (25)), then they should
share attributes such as range and average.
Question Type Test: Different types of
attributes tend to occur with different types of
questions. For example, relational role attributes
tend to occur with who-questions like &amp;quot;Who is the
driver of the car?&amp;quot; and &amp;quot;Who is the manufacturer of
the car?&amp;quot;
</bodyText>
<table confidence="0.999446166666667">
Guarino Qualia Car Attributes
Class Role
Part Constitutive front, rear, interior, inside, side,
Role body, trunk, exterior, underside,
hood, back, nose, roof, engine,
frame, floor, rest, silhouette,
backseat, wheelbase, battery,
chassis, path
Quality Formal speed, value, weight, price,
Role velocity, color, condition,
momentum, convenience,
propulsion, look, inertia, state,
model, history, balance, motion,
performance
Relational Agentive driver, owner
Role Role
- Telic handling, use, search, design,
Role8 benefit
</table>
<tableCaption confidence="0.872063">
Table 8: The classification of the top 50
attributes of the concept car
</tableCaption>
<bodyText confidence="0.891327846153846">
In future work, we plan to use some of these
tests to classify attributes, and possibly filter some
of them; this might improve the discrimination
power of attributes. Also, concepts may share
certain Qualia, but differ in other respects: for
example, the chair concept and the man concept
share some parts (e.g., arm, back, leg, and seat)
and even some qualities (e.g., color, size, and
shape) but differ in other levels (i.e., Agentive
Role, and Telic Role).
8 Telic roles define purposes, functions, and activities
that are related to the concept. Some valid telic roles for
the concept car would be: driving, selling, and buying.
</bodyText>
<sectionHeader confidence="0.997957" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999929555555556">
Simple text patterns were used to automatically
extract both basic value-based and attribute-based
concept descriptions for clustering purposes. Our
preliminary results suggest, first of all, that when
large amounts of data such as the Web are
accessed, these simple patterns may be sufficient to
compute descriptions rich enough to discriminate
quite well, at least with small sets of concepts
belonging to clearly distinct classes. Secondly, we
found that even though attributes are fewer than
values, attribute-based descriptions need not be as
long as value-based ones to achieve as good or
better results. Finally, we found that the best
descriptions included both attributes and more
general properties. We plan to extend this work
both by refining our notion of attribute and by
using more sophisticated patterns working off the
output of a parser.
</bodyText>
<sectionHeader confidence="0.998513" genericHeader="acknowledgments">
7 Acknowledgement
</sectionHeader>
<bodyText confidence="0.9975305">
Abdulrahman Almuhareb is supported by King
Abdulaziz City for Science and Technology
(KACST), Riyadh, Saudi Arabia. We want to
thank Google for making their Web API available
to the research community and George Karypis for
the CLUTO clustering toolkit.
</bodyText>
<sectionHeader confidence="0.998701" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997545320000001">
M. Berland and E. Charniak. 1999. Finding parts in
very large corpora. In Proc. of the 37th ACL,
pages 57–64, University of Maryland.
R. J. Brachman and H. J. Levesque, editors. 1985.
Reading in Knowledge Representation. Morgan
Kaufmann, California.
S. A. Caraballo. 1999. Automatic construction of a
hypernym-labeled noun hierarchy from text. In
Proc. of the 37th ACL.
D. J. Cook and L. B. Holder. 2000. Graph-based
data mining. IEEE Intelligent Systems, 15(2), 32-
41.
J. R. Curran and M. Moens. 2002. Improvements
in automatic thesaurus extraction. In Proc. of the
ACL Workshop on Unsupervised Lexical
Acquisition, pages 59–66.
R. M. W. Dixon. 1991. A New Approach to
English Grammar, on Semantic Principles.
Clarendon Press, Oxford.
C. Fellbaum, editor. 1998. WordNet: An electronic
lexical database. The MIT Press.
D. H. Fisher. 1987. Knowledge acquisition via
incremental conceptual clustering. Machine
Learning, 2:139–172.
G. Grefenstette. 1993. SEXTANT: Extracting
semantics from raw text implementation details.
Heuristics: The Journal of Knowledge
Engineering.
N. Guarino. 1992. Concepts, attributes and
arbitrary relations: some linguistic and
ontological criteria for structuring knowledge
base. Data and Knowledge Engineering, 8, pages
249–261.
V. Hatzivassiloglou and K. McKeown. 1993.
Towards the automatic identification of
adjectival scales: clustering adjectives according
to meaning. In Proc. of the 31st ACL, pages 172–
182.
M. A. Hearst. 1998. Automated discovery of
WordNet relations. In C. Fellbaum, editor,
WordNet: An Electronic Lexical Database. MIT
Press.
G. Karypis. 2002. CLUTO: A clustering toolkit.
Technical Report 02-017, University of
Minnesota. Available at URL: http://www-
users.cs.umn.edu/~karypis/cluto/.
F. Keller and M. Lapata. 2003. Using the Web to
obtain frequencies for unseen bigrams.
Computational Linguistics, 29(3).
A. Kilgarriff and H. Schuetze. 2003. Introduction
to the special issue of Computational Linguistics
on the web as a corpus. Computational
Linguistics.
A. Kilgarriff. 2003. Thesauruses for Natural
Language Processing. In Proc. of the IEEE 2003
International Conference on Natural Language
Processing and Knowledge Engineering (NLP-
KE&apos;03), Beijing.
S. Laurence and E. Margolis. 1999. Concepts and
Cognitive Science. In E. Margolis and S.
Laurence, editors, Concepts: Core Readings.
Cambridge, MA., Bradford Books/MIT Press,
pages 3-81.
D. Lin. 1998. Automatic retrieval and clustering of
similar words. In Proc. of COLING-ACL, 768-
774.
K. Lund and C. Burgess. 1996. Producing high-
dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods,
Instrumentation, and Computers, 28, 203-208.
C. D. Manning and H. Schuetze. 1999.
Foundations of Statistical NLP. MIT Press.
K. Markert, M. Nissim, and N. Modjeska. 2004.
Comparing Knowledge Sources for Nominal
Anaphora Resolution. Submitted.
F. Pereira, N. Tishby, and L. Lee. 1993.
Distributional clustering of English words. In
Proc. of the 31st ACL, pages 183-190,
Columbus, Ohio.
M. Poesio and A. Almuhareb. 2004. Feature-based
vs. Property-based KR: An Empirical
Perspective. Submitted.
M. Poesio, T. Ishikawa, S. Walde, and R. Vieira.
2002. Acquiring lexical knowledge for anaphora
resolution. In Proc. of LREC, Las Palmas, June.
M. Poesio. 2003. Associative descriptions and
salience. In Proc. of the EACL Workshop on
Computational Treatments of Anaphora,
Budapest.
J. Pustejovsky. 1991. The generative lexicon.
Computational Linguistics, 17(4), pages 409-
441.
J. A. Swets. 1969. Effectiveness of Information
Retrieval Methods. American Documentation,
20, pages 72-89.
W. A. Woods. 1975. What’s in a link: Foundations
for semantic networks. In Daniel G. Bobrow and
Alan M. Collins, editors, Representation and
Understanding: Studies in Cognitive Science,
pages 35-82. Academic Press, New York.
</reference>
<sectionHeader confidence="0.676161" genericHeader="references">
Appendix A. The 214 Concepts from the 13 WordNet Classes Used in Experiment 2
</sectionHeader>
<bodyText confidence="0.982866925925926">
Class Concepts
Animal bear, bull, camel, cat, cow, deer, dog, elephant, horse, kitten, lion, monkey, mouse,
oyster, puppy, rat, sheep, tiger, turtle, zebra
Building abattoir, center, clubhouse, dormitory, greenhouse, hall, hospital, hotel, house, inn,
library, nursery, restaurant, school, skyscraper, tavern, theater, villa, whorehouse
Cloth pants, blouse, coat, costume, gloves, hat, jacket, jeans, neckpiece, pajamas, robe, scarf,
shirt, suit, trousers, uniform
Creator architect, artist, builder, constructor, craftsman, designer, developer, farmer, inventor,
maker, manufacture, musician, originator, painter, photographer, producer, tailor
Disease acne, anthrax, arthritis, asthma, cancer, cholera, cirrhosis, diabetes, eczema, flu,
glaucoma, hepatitis, leukemia, malnutrition, meningitis, plague, rheumatism, smallpox
Feeling anger, desire, fear, happiness, joy, love, pain, passion, pleasure, sadness, sensitivity,
shame, wonder
Fruit apple, banana, berry, cherry, grape, kiwi, lemon, mango, melon, olive, orange, peach,
pear, pineapple, strawberry, watermelon
Furniture bed, bookcase, cabinet, chair, couch, cradle, desk, dresser, lamp, lounge, seat, sofa, table,
wardrobe
Body Part ankle, arm, ear, eye, face, finger, foot, hand, head, leg, nose, shoulder, toe, tongue, tooth,
wrist
Publication atlas, book, booklet, brochure, catalog, cookbook, dictionary, encyclopedia, handbook,
journal, magazine, manual, phonebook, reference, textbook, workbook
Family boy, child, cousin, daughter, father, girl, grandchild, grandfather, grandmother, husband,
Relation kid, mother, offspring, sibling, son, wife
Time century, decade, era, evening, fall, hour, month, morning, night, overtime, quarter,
season, semester, spring, summer, week, weekend, winter, year
Vehicle aircraft, airplane, automobile, bicycle, boat, car, cruiser, helicopter, motorcycle, pickup,
rocket, ship, truck, van
</bodyText>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.919847">
<title confidence="0.999288">Attribute-Based and Value-Based Clustering: An Evaluation</title>
<author confidence="0.997818">Abdulrahman ALMUHAREB</author>
<author confidence="0.997818">Massimo POESIO</author>
<affiliation confidence="0.9997515">Department of Computer Science and Centre for Cognitive University of Essex</affiliation>
<address confidence="0.963233">Colchester, United Kingdom, CO4 3SQ</address>
<email confidence="0.970179">aalmuh@essex.ac.ukpoesio@essex.ac.uk</email>
<abstract confidence="0.999202444444445">In most research on concept acquisition from corpora, concepts are modeled as vectors of relations extracted from syntactic structures. In the case of modifiers, these relations often attributes, as in this is unlike what typically proposed in theories of knowledge representation, where concepts are typically defined in terms of their We compared models of concepts based on values with models based on attributes, using lexical clustering as the basis for comparison. We find that attribute-based models work better than value-based ones, and result in shorter but that including both the best attributes and the best values work best of all.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Berland</author>
<author>E Charniak</author>
</authors>
<title>Finding parts in very large corpora.</title>
<date>1999</date>
<booktitle>In Proc. of the 37th ACL,</booktitle>
<pages>57--64</pages>
<institution>University of Maryland.</institution>
<contexts>
<context position="2158" citStr="Berland and Charniak, 1999" startWordPosition="324" endWordPosition="327">odifier.) To our knowledge, however, no attempt has been made by computational linguists to use the attributes themselves in such vectors: i.e., to learn that the description of the concept dog includes elements such as (dog color) or (dog size). This is surprising when considering that most models of concepts in the AI literature are based on such attributes (Brachman and Levesque, 1985). Two problems need to be addressed when trying to identify concept attributes. The first problem is that values are easier to extract. We found, however, that patterns like the X of the dog, already used in (Berland and Charniak, 1999; Poesio et al, 2002) to find part-of relations (using techniques derived from those used in (Hearst, 1998; Caraballo, 1999) to find hyponymy relations) are quite effective at finding attributes. A second problem might be that instances of such patterns are less frequent than those used to extract values, even in large corpora such as the British National Corpus (BNC). But this problem, as well, is less serious when using the Web as a corpus (Kilgarriff and Schuetze, 2003; Keller and Lapata, 2003; Markert et al, submitted). We report on two experiments whose goal was to test whether identifyin</context>
<context position="6578" citStr="Berland and Charniak (1999)" startWordPosition="1051" endWordPosition="1054">s taking a particular concept as arguments. (We return on the issue of the characterization of attributes below.) Our pattern for attributes is shown below: • &amp;quot;the * of the C [is|was]&amp;quot; where again C is a concept, but the wildcard denotes an unspecified attribute. Again, is/was is used to increase precision. An example of text matching this pattern is: • ... the price of the car was ... Both of the patterns we use satisfy Hearst&apos;s desiderata for good patterns (Hearst, 1998): they are (i) frequent, (ii) precise, and (iii) easy to recognize. Patterns similar to our attribute pattern were used by Berland and Charniak (1999) and Poesio et al (2002) to find object parts only; after collecting their data, Berland and Charniak filtered out words ending with &amp;quot;ness&amp;quot;, &amp;quot;ing&amp;quot;, and &amp;quot;ity&amp;quot;, because these express qualities of objects, and used a ranking method to rank the remaining words. (An accuracy of 55% for the top 50 proposed parts was reported.) We found that these patterns can be used to collect other sorts of &apos;attributes&apos;, as well. 2.2 Web Data Collection through Google In recent years there has been growing evidence that using the Web as a corpus greatly reduces the problem of data sparseness, and its size more tha</context>
</contexts>
<marker>Berland, Charniak, 1999</marker>
<rawString>M. Berland and E. Charniak. 1999. Finding parts in very large corpora. In Proc. of the 37th ACL, pages 57–64, University of Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Brachman</author>
<author>H J</author>
</authors>
<date>1985</date>
<booktitle>Reading in Knowledge Representation.</booktitle>
<editor>Levesque, editors.</editor>
<publisher>Morgan Kaufmann,</publisher>
<location>California.</location>
<marker>Brachman, J, 1985</marker>
<rawString>R. J. Brachman and H. J. Levesque, editors. 1985. Reading in Knowledge Representation. Morgan Kaufmann, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S A Caraballo</author>
</authors>
<title>Automatic construction of a hypernym-labeled noun hierarchy from text.</title>
<date>1999</date>
<booktitle>In Proc. of the 37th ACL.</booktitle>
<contexts>
<context position="2282" citStr="Caraballo, 1999" startWordPosition="345" endWordPosition="346">ors: i.e., to learn that the description of the concept dog includes elements such as (dog color) or (dog size). This is surprising when considering that most models of concepts in the AI literature are based on such attributes (Brachman and Levesque, 1985). Two problems need to be addressed when trying to identify concept attributes. The first problem is that values are easier to extract. We found, however, that patterns like the X of the dog, already used in (Berland and Charniak, 1999; Poesio et al, 2002) to find part-of relations (using techniques derived from those used in (Hearst, 1998; Caraballo, 1999) to find hyponymy relations) are quite effective at finding attributes. A second problem might be that instances of such patterns are less frequent than those used to extract values, even in large corpora such as the British National Corpus (BNC). But this problem, as well, is less serious when using the Web as a corpus (Kilgarriff and Schuetze, 2003; Keller and Lapata, 2003; Markert et al, submitted). We report on two experiments whose goal was to test whether identifying attributes leads to better lexical descriptions of concepts. We do this by comparing the results obtained by using attribu</context>
</contexts>
<marker>Caraballo, 1999</marker>
<rawString>S. A. Caraballo. 1999. Automatic construction of a hypernym-labeled noun hierarchy from text. In Proc. of the 37th ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D J Cook</author>
<author>L B Holder</author>
</authors>
<title>Graph-based data mining.</title>
<date>2000</date>
<journal>IEEE Intelligent Systems,</journal>
<volume>15</volume>
<issue>2</issue>
<pages>32--41</pages>
<contexts>
<context position="9772" citStr="Cook and Holder, 2000" startWordPosition="1602" endWordPosition="1605">e word. For example, the search request &amp;quot;a * car is&amp;quot; catches instances such as: [a red car is], [a small car is], and [a sport car is]. It is worth mentioning that Google does not pay attention to punctuation marks; this is one area in which parsing would help. When receiving results from Google, we do not access the actual Web pages, but instead we process the snippets that are returned by Google.3 2.3 Clustering Methods The task that we use to compare concept descriptions is lexical acquisition via clustering. We experimented with clustering systems such as COBWEB (Fisher, 1987) and SUBDUE (Cook and Holder, 2000) before settling on CLUTO 2.1 (Karypis, 2002). CLUTO is a general-purpose clustering tool that implements three different clustering algorithms: partitional, agglomerative, and graph partitioning algorithms. CLUTO produces both flat and hierarchical clusters. It uses a hard clustering technique, where each concept can be assigned to only one cluster. The software allows to choose a similarity metric between a set including extended Jaccard and cosine. CLUTO was optimized to cluster data of large sizes in a reasonable time. The software also provides analysis and visualization tools. In this pa</context>
</contexts>
<marker>Cook, Holder, 2000</marker>
<rawString>D. J. Cook and L. B. Holder. 2000. Graph-based data mining. IEEE Intelligent Systems, 15(2), 32-41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Curran</author>
<author>M Moens</author>
</authors>
<title>Improvements in automatic thesaurus extraction.</title>
<date>2002</date>
<booktitle>In Proc. of the ACL Workshop on Unsupervised Lexical Acquisition,</booktitle>
<pages>59--66</pages>
<contexts>
<context position="1254" citStr="Curran and Moens, 2002" startWordPosition="174" endWordPosition="177"> attributes (e.g., color). We compared models of concepts based on values with models based on attributes, using lexical clustering as the basis for comparison. We find that attribute-based models work better than value-based ones, and result in shorter descriptions; but that mixed models including both the best attributes and the best values work best of all. 1 Introduction In most recent research on concept acquisition from corpora (e.g., for lexicon construction), concepts are viewed as vectors of relations, or properties, extracted from syntactic structures (Grefenstette, 1993; Lin, 1998; Curran and Moens, 2002; Kilgarriff, 2003, and many others). These properties often specify values of attributes such as color, shape, or size: for example, the vector used by Lin (1998) for the concept dog includes the property (dog adj-mod brown). (We will use the term values here to refer to any modifier.) To our knowledge, however, no attempt has been made by computational linguists to use the attributes themselves in such vectors: i.e., to learn that the description of the concept dog includes elements such as (dog color) or (dog size). This is surprising when considering that most models of concepts in the AI </context>
<context position="14450" citStr="Curran and Moens (2002)" startWordPosition="2392" endWordPosition="2395">cepts plus Italy, horse, and tongue (37 in total) to compare value-based and attribute-based description when used for clustering, using concept descriptions collected using the methods described above. The input to clustering is a frequency table with concepts as rows and values, attributes, or both attributes and values as columns. Each cell in the table contains the frequency of co-occurrence between the concept and corresponding value or attribute. Before clustering, the frequencies are transformed into weighted values using the t test (Manning and Schutze, 1999). (The t test was found by Curran and Moens (2002) to be the best weighting method.) The t test formula we used for attributes is shown below: r C(concepti )x C(attributej )1 I` N 2 J C(concept N where N is the total number of relations, and C is a count function. The values formula is similar. We use the CLUTO vcluster command for clustering, with parameters: similarity function = Extended Jaccard Coefficient, clustering method = Graph Partitioning, no. of clusters = 3. Used Data Vector Size4 500 1522 3044 4753 4969 Values 64.86% 94.59% - - 94.59% Only Attributes 97.30% 97.30% - 97.30% - Only Attributes1522 - - 100.00% - - and Values1522 Tab</context>
<context position="16568" citStr="Curran and Moens, 2002" startWordPosition="2736" endWordPosition="2739">size of pure attribute- and value- descriptions (to 4753 and 4969, respectively) does not improve accuracy, perfect accuracy can be obtained by using vectors of length 3044, including the 1522 best attributes and the 1522 best values. This suggests that while attributes are a good way of generalizing across properties, not all properties of concepts can be viewed as attribute/value pairs (section 5; also (Poesio and Almuhareb, submitted)). 4 Second Experiment: Using a Set of Concepts from WordNet In order to get a more realistic evaluation and a better comparison with work such as (Lin, 1998; Curran and Moens, 2002), we also ran a second experiment using a larger set of concepts from the WordNet noun hierarchy (Fellbaum, 1998). We chose 214 relatively common concepts from 13 different classes covering a variety of subhierarchies (see Appendix A). Each class contains a set of concepts that share a common hypernym in the WordNet hierarchy. Systems Answer Model Answer Yes No Boolean Yes 1294 503 No 387 20607 Frequency Yes 1117 950 No 564 20160 Table 5: The contingency table based on boolean and frequency for the combined attributes and values The frequencies for attributes and values were again collected as</context>
<context position="17920" citStr="Curran and Moens, 2002" startWordPosition="2968" endWordPosition="2971">n boolean values instead of the original 5 We consider only positive values of t. C(concept , attribute ) i j � N ti,j � i attribute , j ) 2 (1) frequencies6, treating all positive frequencies as 1 and everything else as 0. This eliminates the effect of variations in frequencies in the original data, the intuition being that frequencies do not add to the semantics of concepts: what we are interested in is the fact that a concept has a given attribute/value, regardless of how many times we have encountered this fact. This approach is similar to the approach adopted in (Hearst, 1998); see also (Curran and Moens, 2002) for a comparison of methods dealing with concept vectors based on raw frequencies or boolean values. The transformed table is a binary table that contains only zeros and ones in its cells. Table 5 shows the contingency table for clusters produced based on boolean and frequency for the combined data of attributes and values; it shows that boolean data is more accurate in the four cases. For clustering, as well, we used CLUTO in a different way. Instead of asking CLUTO to compute the similarities between the concepts, we computed them ourselves, using the version of the extended Jaccard similar</context>
</contexts>
<marker>Curran, Moens, 2002</marker>
<rawString>J. R. Curran and M. Moens. 2002. Improvements in automatic thesaurus extraction. In Proc. of the ACL Workshop on Unsupervised Lexical Acquisition, pages 59–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M W Dixon</author>
</authors>
<title>A New Approach to English Grammar, on Semantic Principles.</title>
<date>1991</date>
<publisher>Clarendon Press,</publisher>
<location>Oxford.</location>
<contexts>
<context position="24477" citStr="Dixon (1991)" startWordPosition="4139" endWordPosition="4140">stitutive Role (Guarino&apos;s parts), Formal Role (Guarino&apos;s qualities), Agentive Role (Guarino&apos;s relational roles), and Telic Role (not included in Guarino&apos;s classification). Our analysis of the attribute data shows that the attributes we found can be mapped in the four roles of the Qualia structure. Table 8 shows how we manually mapped the top 50 attributes of the concept car to the Qualia roles and the Guarino&apos;s classes. This mapping is not trivial (e.g., a path is not part of a car, and design can be regarded as a quality), but a variety of tests may help: Morphological and Ontological Tests: Dixon (1991) proposed a semantic classification for nouns. According to Dixon, parts are concrete concepts and mostly basic noun roots or rarely derived from verbs, while qualities are abstract concepts and many of them are basic noun roots or derived from adjectives, some derived from stems, and few derived from verbs. Our observations also suggest that telic attributes are usually derived from verbs. Attributes Test: Since attributes can also be viewed as concepts (e.g., in WordNet), they themselves should have some shared attributes. For example: since parts are concrete objects they should share attri</context>
</contexts>
<marker>Dixon, 1991</marker>
<rawString>R. M. W. Dixon. 1991. A New Approach to English Grammar, on Semantic Principles. Clarendon Press, Oxford.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An electronic lexical database.</title>
<date>1998</date>
<editor>C. Fellbaum, editor.</editor>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="1417" citStr="(1998)" startWordPosition="203" endWordPosition="203">bute-based models work better than value-based ones, and result in shorter descriptions; but that mixed models including both the best attributes and the best values work best of all. 1 Introduction In most recent research on concept acquisition from corpora (e.g., for lexicon construction), concepts are viewed as vectors of relations, or properties, extracted from syntactic structures (Grefenstette, 1993; Lin, 1998; Curran and Moens, 2002; Kilgarriff, 2003, and many others). These properties often specify values of attributes such as color, shape, or size: for example, the vector used by Lin (1998) for the concept dog includes the property (dog adj-mod brown). (We will use the term values here to refer to any modifier.) To our knowledge, however, no attempt has been made by computational linguists to use the attributes themselves in such vectors: i.e., to learn that the description of the concept dog includes elements such as (dog color) or (dog size). This is surprising when considering that most models of concepts in the AI literature are based on such attributes (Brachman and Levesque, 1985). Two problems need to be addressed when trying to identify concept attributes. The first prob</context>
</contexts>
<marker>1998</marker>
<rawString>C. Fellbaum, editor. 1998. WordNet: An electronic lexical database. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D H Fisher</author>
</authors>
<title>Knowledge acquisition via incremental conceptual clustering.</title>
<date>1987</date>
<booktitle>Machine Learning,</booktitle>
<pages>2--139</pages>
<contexts>
<context position="9737" citStr="Fisher, 1987" startWordPosition="1598" endWordPosition="1599">notes an unspecified single word. For example, the search request &amp;quot;a * car is&amp;quot; catches instances such as: [a red car is], [a small car is], and [a sport car is]. It is worth mentioning that Google does not pay attention to punctuation marks; this is one area in which parsing would help. When receiving results from Google, we do not access the actual Web pages, but instead we process the snippets that are returned by Google.3 2.3 Clustering Methods The task that we use to compare concept descriptions is lexical acquisition via clustering. We experimented with clustering systems such as COBWEB (Fisher, 1987) and SUBDUE (Cook and Holder, 2000) before settling on CLUTO 2.1 (Karypis, 2002). CLUTO is a general-purpose clustering tool that implements three different clustering algorithms: partitional, agglomerative, and graph partitioning algorithms. CLUTO produces both flat and hierarchical clusters. It uses a hard clustering technique, where each concept can be assigned to only one cluster. The software allows to choose a similarity metric between a set including extended Jaccard and cosine. CLUTO was optimized to cluster data of large sizes in a reasonable time. The software also provides analysis </context>
</contexts>
<marker>Fisher, 1987</marker>
<rawString>D. H. Fisher. 1987. Knowledge acquisition via incremental conceptual clustering. Machine Learning, 2:139–172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Grefenstette</author>
</authors>
<title>SEXTANT: Extracting semantics from raw text implementation details. Heuristics:</title>
<date>1993</date>
<journal>The Journal of Knowledge Engineering.</journal>
<contexts>
<context position="1219" citStr="Grefenstette, 1993" startWordPosition="170" endWordPosition="171">cally defined in terms of their attributes (e.g., color). We compared models of concepts based on values with models based on attributes, using lexical clustering as the basis for comparison. We find that attribute-based models work better than value-based ones, and result in shorter descriptions; but that mixed models including both the best attributes and the best values work best of all. 1 Introduction In most recent research on concept acquisition from corpora (e.g., for lexicon construction), concepts are viewed as vectors of relations, or properties, extracted from syntactic structures (Grefenstette, 1993; Lin, 1998; Curran and Moens, 2002; Kilgarriff, 2003, and many others). These properties often specify values of attributes such as color, shape, or size: for example, the vector used by Lin (1998) for the concept dog includes the property (dog adj-mod brown). (We will use the term values here to refer to any modifier.) To our knowledge, however, no attempt has been made by computational linguists to use the attributes themselves in such vectors: i.e., to learn that the description of the concept dog includes elements such as (dog color) or (dog size). This is surprising when considering that</context>
</contexts>
<marker>Grefenstette, 1993</marker>
<rawString>G. Grefenstette. 1993. SEXTANT: Extracting semantics from raw text implementation details. Heuristics: The Journal of Knowledge Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Guarino</author>
</authors>
<title>Concepts, attributes and arbitrary relations: some linguistic and ontological criteria for structuring knowledge base.</title>
<date>1992</date>
<journal>Data and Knowledge Engineering,</journal>
<volume>8</volume>
<pages>249--261</pages>
<contexts>
<context position="23401" citStr="Guarino (1992)" startWordPosition="3972" endWordPosition="3973">The greater compactness of attribute-based representations vs. value-based ones was more evident in this second experiment. We collected 51,045 distinct values and 8,934 distinct attributes; the total number of value-concept relations is 1,026,335, compared to 422,621 attribute-concept relations. 5 Attributes and Values: A discussion Although our results suggest that trying to identify attributes is beneficial, the notion of &apos;attribute&apos; is not completely clear, and has been used in widely different ways in Knowledge Representation literature. An attempt of defining the notion has been made by Guarino (1992), who classifies attributes into relational and nonrelational attributes. Relational attributes include qualities such as color and position, and relational roles such as son and spouse. Non-relational attributes include parts such as wheel and engine. The Qualia Structure of the Generative Lexicon (Pustejovsky, 1991) is another attempt at identifying &amp;quot;the essential attributes of an object as defined by the lexical item&amp;quot;. Pustejovsky identifies four roles: Constitutive Role (Guarino&apos;s parts), Formal Role (Guarino&apos;s qualities), Agentive Role (Guarino&apos;s relational roles), and Telic Role (not inc</context>
</contexts>
<marker>Guarino, 1992</marker>
<rawString>N. Guarino. 1992. Concepts, attributes and arbitrary relations: some linguistic and ontological criteria for structuring knowledge base. Data and Knowledge Engineering, 8, pages 249–261.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Hatzivassiloglou</author>
<author>K McKeown</author>
</authors>
<title>Towards the automatic identification of adjectival scales: clustering adjectives according to meaning.</title>
<date>1993</date>
<booktitle>In Proc. of the 31st ACL,</booktitle>
<pages>172--182</pages>
<contexts>
<context position="11769" citStr="Hatzivassiloglou and McKeown, 1993" startWordPosition="1913" endWordPosition="1916">by Google to 20,000 requests per day. 3 Snippets are text excerpts captured from the actual web pages with embedded HTML tags. We process the snippets by removing the HTML tags and extracting the targeted piece of text that was specified in the request. each system cluster, finding the class of each concept in the model clusters, and determining the majority class. The cluster is then labeled with this class; the concepts belonging to it are taken to be correctly clustered, whereas the remaining concepts are judged to be incorrectly clustered. In the contingency table evaluation (Swets, 1969; Hatzivassiloglou and McKeown, 1993), the clusters are converted into two lists (one for the system clusters and one for the model clusters) of yes-no answers to the question &amp;quot;Does the pair of concepts occur in the same cluster?&amp;quot; for each pair of concepts. A contingency table is then built, from which recall (R), precision (P), fallout, and F measures can be computed. For example, if the model clusters are: (A, B, C) and (D), and the system clusters are: (A, B) and (C, D), the yes-no lists are as in Table 2, and the contingency table is as in Table 3. Question Model System Answer Answer Does the pair (A, B) occur in Yes Yes the </context>
</contexts>
<marker>Hatzivassiloglou, McKeown, 1993</marker>
<rawString>V. Hatzivassiloglou and K. McKeown. 1993. Towards the automatic identification of adjectival scales: clustering adjectives according to meaning. In Proc. of the 31st ACL, pages 172– 182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Hearst</author>
</authors>
<title>Automated discovery of WordNet relations.</title>
<date>1998</date>
<editor>In C. Fellbaum, editor,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="2264" citStr="Hearst, 1998" startWordPosition="343" endWordPosition="344">s in such vectors: i.e., to learn that the description of the concept dog includes elements such as (dog color) or (dog size). This is surprising when considering that most models of concepts in the AI literature are based on such attributes (Brachman and Levesque, 1985). Two problems need to be addressed when trying to identify concept attributes. The first problem is that values are easier to extract. We found, however, that patterns like the X of the dog, already used in (Berland and Charniak, 1999; Poesio et al, 2002) to find part-of relations (using techniques derived from those used in (Hearst, 1998; Caraballo, 1999) to find hyponymy relations) are quite effective at finding attributes. A second problem might be that instances of such patterns are less frequent than those used to extract values, even in large corpora such as the British National Corpus (BNC). But this problem, as well, is less serious when using the Web as a corpus (Kilgarriff and Schuetze, 2003; Keller and Lapata, 2003; Markert et al, submitted). We report on two experiments whose goal was to test whether identifying attributes leads to better lexical descriptions of concepts. We do this by comparing the results obtaine</context>
<context position="6428" citStr="Hearst, 1998" startWordPosition="1029" endWordPosition="1030">been) viewed as defining attributes of concepts as well; so for the moment we used more general patterns identifying all relational nouns taking a particular concept as arguments. (We return on the issue of the characterization of attributes below.) Our pattern for attributes is shown below: • &amp;quot;the * of the C [is|was]&amp;quot; where again C is a concept, but the wildcard denotes an unspecified attribute. Again, is/was is used to increase precision. An example of text matching this pattern is: • ... the price of the car was ... Both of the patterns we use satisfy Hearst&apos;s desiderata for good patterns (Hearst, 1998): they are (i) frequent, (ii) precise, and (iii) easy to recognize. Patterns similar to our attribute pattern were used by Berland and Charniak (1999) and Poesio et al (2002) to find object parts only; after collecting their data, Berland and Charniak filtered out words ending with &amp;quot;ness&amp;quot;, &amp;quot;ing&amp;quot;, and &amp;quot;ity&amp;quot;, because these express qualities of objects, and used a ranking method to rank the remaining words. (An accuracy of 55% for the top 50 proposed parts was reported.) We found that these patterns can be used to collect other sorts of &apos;attributes&apos;, as well. 2.2 Web Data Collection through Googl</context>
<context position="17885" citStr="Hearst, 1998" startWordPosition="2964" endWordPosition="2965">e performed the t test5 on boolean values instead of the original 5 We consider only positive values of t. C(concept , attribute ) i j � N ti,j � i attribute , j ) 2 (1) frequencies6, treating all positive frequencies as 1 and everything else as 0. This eliminates the effect of variations in frequencies in the original data, the intuition being that frequencies do not add to the semantics of concepts: what we are interested in is the fact that a concept has a given attribute/value, regardless of how many times we have encountered this fact. This approach is similar to the approach adopted in (Hearst, 1998); see also (Curran and Moens, 2002) for a comparison of methods dealing with concept vectors based on raw frequencies or boolean values. The transformed table is a binary table that contains only zeros and ones in its cells. Table 5 shows the contingency table for clusters produced based on boolean and frequency for the combined data of attributes and values; it shows that boolean data is more accurate in the four cases. For clustering, as well, we used CLUTO in a different way. Instead of asking CLUTO to compute the similarities between the concepts, we computed them ourselves, using the vers</context>
</contexts>
<marker>Hearst, 1998</marker>
<rawString>M. A. Hearst. 1998. Automated discovery of WordNet relations. In C. Fellbaum, editor, WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Karypis</author>
</authors>
<title>CLUTO: A clustering toolkit.</title>
<date>2002</date>
<tech>Technical Report 02-017,</tech>
<institution>University of Minnesota.</institution>
<note>Available at URL: http://wwwusers.cs.umn.edu/~karypis/cluto/.</note>
<contexts>
<context position="9817" citStr="Karypis, 2002" startWordPosition="1611" endWordPosition="1612">catches instances such as: [a red car is], [a small car is], and [a sport car is]. It is worth mentioning that Google does not pay attention to punctuation marks; this is one area in which parsing would help. When receiving results from Google, we do not access the actual Web pages, but instead we process the snippets that are returned by Google.3 2.3 Clustering Methods The task that we use to compare concept descriptions is lexical acquisition via clustering. We experimented with clustering systems such as COBWEB (Fisher, 1987) and SUBDUE (Cook and Holder, 2000) before settling on CLUTO 2.1 (Karypis, 2002). CLUTO is a general-purpose clustering tool that implements three different clustering algorithms: partitional, agglomerative, and graph partitioning algorithms. CLUTO produces both flat and hierarchical clusters. It uses a hard clustering technique, where each concept can be assigned to only one cluster. The software allows to choose a similarity metric between a set including extended Jaccard and cosine. CLUTO was optimized to cluster data of large sizes in a reasonable time. The software also provides analysis and visualization tools. In this paper, we use extended Jaccard, which was found</context>
</contexts>
<marker>Karypis, 2002</marker>
<rawString>G. Karypis. 2002. CLUTO: A clustering toolkit. Technical Report 02-017, University of Minnesota. Available at URL: http://wwwusers.cs.umn.edu/~karypis/cluto/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Keller</author>
<author>M Lapata</author>
</authors>
<title>Using the Web to obtain frequencies for unseen bigrams.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>3</issue>
<contexts>
<context position="2659" citStr="Keller and Lapata, 2003" startWordPosition="406" endWordPosition="409">are easier to extract. We found, however, that patterns like the X of the dog, already used in (Berland and Charniak, 1999; Poesio et al, 2002) to find part-of relations (using techniques derived from those used in (Hearst, 1998; Caraballo, 1999) to find hyponymy relations) are quite effective at finding attributes. A second problem might be that instances of such patterns are less frequent than those used to extract values, even in large corpora such as the British National Corpus (BNC). But this problem, as well, is less serious when using the Web as a corpus (Kilgarriff and Schuetze, 2003; Keller and Lapata, 2003; Markert et al, submitted). We report on two experiments whose goal was to test whether identifying attributes leads to better lexical descriptions of concepts. We do this by comparing the results obtained by using attributes or more general modifiers – that we will simply call values – as elements of concept vectors used to identify concept similarities via clustering. In Section 2, we discuss how Web data were used to build attribute- and value- based concept vectors, and our clustering and evaluation methods. In Section 3, we discuss a first experiment using the set of concepts used in (Lu</context>
<context position="7244" citStr="Keller and Lapata, 2003" startWordPosition="1164" endWordPosition="1167">rts only; after collecting their data, Berland and Charniak filtered out words ending with &amp;quot;ness&amp;quot;, &amp;quot;ing&amp;quot;, and &amp;quot;ity&amp;quot;, because these express qualities of objects, and used a ranking method to rank the remaining words. (An accuracy of 55% for the top 50 proposed parts was reported.) We found that these patterns can be used to collect other sorts of &apos;attributes&apos;, as well. 2.2 Web Data Collection through Google In recent years there has been growing evidence that using the Web as a corpus greatly reduces the problem of data sparseness, and its size more than compensates the lack of balance (e.g., (Keller and Lapata, 2003)). The benefits from using the Web over even large corpora like the BNC for extracting semantic relations, particularly when using simple text patterns, were informally pointed out in (Poesio, 2003) and demonstrated more systematically by Markert et al (submitted). These findings were confirmed by our experiments. A comparison of numbers of instances of some patterns using the Web and the BNC is shown in Table 1. Pattern Web BNC Attribute &amp;quot;the * of the *&amp;quot; 23,100,000 208,155 &amp;quot;the * of the * is&amp;quot; 10,900,000 3,627 &amp;quot;the * of the car is&amp;quot; 26,400 5 &amp;quot;the * of the hat is&amp;quot; 2,770 1 Value &amp;quot;the fast * is&amp;quot; 3</context>
</contexts>
<marker>Keller, Lapata, 2003</marker>
<rawString>F. Keller and M. Lapata. 2003. Using the Web to obtain frequencies for unseen bigrams. Computational Linguistics, 29(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kilgarriff</author>
<author>H Schuetze</author>
</authors>
<title>Introduction to the special issue of Computational Linguistics on the web as a corpus. Computational Linguistics.</title>
<date>2003</date>
<contexts>
<context position="2634" citStr="Kilgarriff and Schuetze, 2003" startWordPosition="402" endWordPosition="405">e first problem is that values are easier to extract. We found, however, that patterns like the X of the dog, already used in (Berland and Charniak, 1999; Poesio et al, 2002) to find part-of relations (using techniques derived from those used in (Hearst, 1998; Caraballo, 1999) to find hyponymy relations) are quite effective at finding attributes. A second problem might be that instances of such patterns are less frequent than those used to extract values, even in large corpora such as the British National Corpus (BNC). But this problem, as well, is less serious when using the Web as a corpus (Kilgarriff and Schuetze, 2003; Keller and Lapata, 2003; Markert et al, submitted). We report on two experiments whose goal was to test whether identifying attributes leads to better lexical descriptions of concepts. We do this by comparing the results obtained by using attributes or more general modifiers – that we will simply call values – as elements of concept vectors used to identify concept similarities via clustering. In Section 2, we discuss how Web data were used to build attribute- and value- based concept vectors, and our clustering and evaluation methods. In Section 3, we discuss a first experiment using the se</context>
</contexts>
<marker>Kilgarriff, Schuetze, 2003</marker>
<rawString>A. Kilgarriff and H. Schuetze. 2003. Introduction to the special issue of Computational Linguistics on the web as a corpus. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kilgarriff</author>
</authors>
<title>Thesauruses for Natural Language Processing.</title>
<date>2003</date>
<booktitle>In Proc. of the IEEE 2003 International Conference on Natural Language Processing and Knowledge Engineering (NLPKE&apos;03),</booktitle>
<location>Beijing.</location>
<contexts>
<context position="1272" citStr="Kilgarriff, 2003" startWordPosition="178" endWordPosition="179">). We compared models of concepts based on values with models based on attributes, using lexical clustering as the basis for comparison. We find that attribute-based models work better than value-based ones, and result in shorter descriptions; but that mixed models including both the best attributes and the best values work best of all. 1 Introduction In most recent research on concept acquisition from corpora (e.g., for lexicon construction), concepts are viewed as vectors of relations, or properties, extracted from syntactic structures (Grefenstette, 1993; Lin, 1998; Curran and Moens, 2002; Kilgarriff, 2003, and many others). These properties often specify values of attributes such as color, shape, or size: for example, the vector used by Lin (1998) for the concept dog includes the property (dog adj-mod brown). (We will use the term values here to refer to any modifier.) To our knowledge, however, no attempt has been made by computational linguists to use the attributes themselves in such vectors: i.e., to learn that the description of the concept dog includes elements such as (dog color) or (dog size). This is surprising when considering that most models of concepts in the AI literature are bas</context>
</contexts>
<marker>Kilgarriff, 2003</marker>
<rawString>A. Kilgarriff. 2003. Thesauruses for Natural Language Processing. In Proc. of the IEEE 2003 International Conference on Natural Language Processing and Knowledge Engineering (NLPKE&apos;03), Beijing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Laurence</author>
<author>E Margolis</author>
</authors>
<title>Concepts and Cognitive Science.</title>
<date>1999</date>
<pages>3--81</pages>
<editor>In E. Margolis and S. Laurence, editors,</editor>
<publisher>Books/MIT Press,</publisher>
<location>Cambridge, MA., Bradford</location>
<contexts>
<context position="22785" citStr="Laurence and Margolis, 1999" startWordPosition="3883" endWordPosition="3886">body that you sit on&amp;quot; (WordNet, sense 2). The same for lounge, which was clustered with buildings, which is consistent with its second sense in WordNet: &amp;quot;a public room (as in a hotel or airport) with seating where people can wait&amp;quot;. This indicates that techniques for differentiating between different senses are needed – e.g., using a soft clustering technique as in (Pereira et al, 1993) instead of a hard clustering technique. Second, furniture concepts may not have a common prototype that is shared by all of the member concepts. This is a well known problem in the prototype theory of concepts (Laurence and Margolis, 1999). The greater compactness of attribute-based representations vs. value-based ones was more evident in this second experiment. We collected 51,045 distinct values and 8,934 distinct attributes; the total number of value-concept relations is 1,026,335, compared to 422,621 attribute-concept relations. 5 Attributes and Values: A discussion Although our results suggest that trying to identify attributes is beneficial, the notion of &apos;attribute&apos; is not completely clear, and has been used in widely different ways in Knowledge Representation literature. An attempt of defining the notion has been made b</context>
</contexts>
<marker>Laurence, Margolis, 1999</marker>
<rawString>S. Laurence and E. Margolis. 1999. Concepts and Cognitive Science. In E. Margolis and S. Laurence, editors, Concepts: Core Readings. Cambridge, MA., Bradford Books/MIT Press, pages 3-81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proc. of COLING-ACL,</booktitle>
<pages>768--774</pages>
<contexts>
<context position="1230" citStr="Lin, 1998" startWordPosition="172" endWordPosition="173">ms of their attributes (e.g., color). We compared models of concepts based on values with models based on attributes, using lexical clustering as the basis for comparison. We find that attribute-based models work better than value-based ones, and result in shorter descriptions; but that mixed models including both the best attributes and the best values work best of all. 1 Introduction In most recent research on concept acquisition from corpora (e.g., for lexicon construction), concepts are viewed as vectors of relations, or properties, extracted from syntactic structures (Grefenstette, 1993; Lin, 1998; Curran and Moens, 2002; Kilgarriff, 2003, and many others). These properties often specify values of attributes such as color, shape, or size: for example, the vector used by Lin (1998) for the concept dog includes the property (dog adj-mod brown). (We will use the term values here to refer to any modifier.) To our knowledge, however, no attempt has been made by computational linguists to use the attributes themselves in such vectors: i.e., to learn that the description of the concept dog includes elements such as (dog color) or (dog size). This is surprising when considering that most model</context>
<context position="16543" citStr="Lin, 1998" startWordPosition="2734" endWordPosition="2735">easing the size of pure attribute- and value- descriptions (to 4753 and 4969, respectively) does not improve accuracy, perfect accuracy can be obtained by using vectors of length 3044, including the 1522 best attributes and the 1522 best values. This suggests that while attributes are a good way of generalizing across properties, not all properties of concepts can be viewed as attribute/value pairs (section 5; also (Poesio and Almuhareb, submitted)). 4 Second Experiment: Using a Set of Concepts from WordNet In order to get a more realistic evaluation and a better comparison with work such as (Lin, 1998; Curran and Moens, 2002), we also ran a second experiment using a larger set of concepts from the WordNet noun hierarchy (Fellbaum, 1998). We chose 214 relatively common concepts from 13 different classes covering a variety of subhierarchies (see Appendix A). Each class contains a set of concepts that share a common hypernym in the WordNet hierarchy. Systems Answer Model Answer Yes No Boolean Yes 1294 503 No 387 20607 Frequency Yes 1117 950 No 564 20160 Table 5: The contingency table based on boolean and frequency for the combined attributes and values The frequencies for attributes and value</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>D. Lin. 1998. Automatic retrieval and clustering of similar words. In Proc. of COLING-ACL, 768-774.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lund</author>
<author>C Burgess</author>
</authors>
<title>Producing highdimensional semantic spaces from lexical cooccurrence.</title>
<date>1996</date>
<journal>Behavior Research Methods, Instrumentation, and Computers,</journal>
<volume>28</volume>
<pages>203--208</pages>
<contexts>
<context position="3280" citStr="Lund and Burgess, 1996" startWordPosition="508" endWordPosition="511">03; Markert et al, submitted). We report on two experiments whose goal was to test whether identifying attributes leads to better lexical descriptions of concepts. We do this by comparing the results obtained by using attributes or more general modifiers – that we will simply call values – as elements of concept vectors used to identify concept similarities via clustering. In Section 2, we discuss how Web data were used to build attribute- and value- based concept vectors, and our clustering and evaluation methods. In Section 3, we discuss a first experiment using the set of concepts used in (Lund and Burgess, 1996). In Section 4, we discuss a second experiment using 214 concepts from WordNet (Fellbaum, 1998). In Section 5 we return to the notion of attribute. 2 Methods 2.1 Using Text Patterns to Build Concept Descriptions Our techniques for extracting concept descriptions are simpler than those used in other work in at least two respects. First of all, we only extracted values expressed as nominal modifiers, ignoring properties expressed by verbal constructions in which the concept occurred as an argument (e.g., Lin’s (dog obj-of have)). (We originally made this simplification to concentrate on the comp</context>
<context position="13311" citStr="Lund and Burgess (1996)" startWordPosition="2216" endWordPosition="2219">swers for the co-occurrence question System Answer Model Answer Yes No Yes a 1 b 1 No c 2 d 2 Table 3: The contingency table a P = = 0.50 a+b 0.33 F =2×R×P w0.40 R+P 3 First Experiment: Using a Set of Concepts from Lund and Burgess One limitation of using Google is that even with an increased daily limit of 20,000, it wouldn’t really be feasible to attempt to cluster, say, all of WordNet 100,000 noun concepts. For this reason, a 0.33 R = w a +c b Fallout = w b+d we used much smaller sets of concepts in our two experiments. The first set allowed us to compare our results with those obtained by Lund and Burgess (1996); the second set consisted of a larger number of concepts from WordNet. Lund and Burgess (1996) used a set of 34 concepts belonging to 3 different classes (animals, body parts, and geographical locations) to evaluate their method for acquiring lexical representations, HAL (Hyperspace Analogue to Language). Lund and Burgess were able to correctly cluster all of the concepts except for one body part, tooth, which was incorrectly clustered with animals. In this first experiment, we used the 34 Lund and Burgess concepts plus Italy, horse, and tongue (37 in total) to compare value-based and attribu</context>
</contexts>
<marker>Lund, Burgess, 1996</marker>
<rawString>K. Lund and C. Burgess. 1996. Producing highdimensional semantic spaces from lexical cooccurrence. Behavior Research Methods, Instrumentation, and Computers, 28, 203-208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>H Schuetze</author>
</authors>
<date>1999</date>
<booktitle>Foundations of Statistical NLP.</booktitle>
<publisher>MIT Press.</publisher>
<marker>Manning, Schuetze, 1999</marker>
<rawString>C. D. Manning and H. Schuetze. 1999. Foundations of Statistical NLP. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Markert</author>
<author>M Nissim</author>
<author>N Modjeska</author>
</authors>
<title>Comparing Knowledge Sources for Nominal Anaphora Resolution.</title>
<date>2004</date>
<note>Submitted.</note>
<marker>Markert, Nissim, Modjeska, 2004</marker>
<rawString>K. Markert, M. Nissim, and N. Modjeska. 2004. Comparing Knowledge Sources for Nominal Anaphora Resolution. Submitted.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
<author>N Tishby</author>
<author>L Lee</author>
</authors>
<title>Distributional clustering of English words.</title>
<date>1993</date>
<booktitle>In Proc. of the 31st ACL,</booktitle>
<pages>183--190</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="22545" citStr="Pereira et al, 1993" startWordPosition="3843" endWordPosition="3846"> about the furniture concepts. First, at least two concepts (seat and lounge) have more than one sense in WordNet. Seat was clustered with body part concepts, which is acceptable if we think of seat as &amp;quot;the fleshy part of the human body that you sit on&amp;quot; (WordNet, sense 2). The same for lounge, which was clustered with buildings, which is consistent with its second sense in WordNet: &amp;quot;a public room (as in a hotel or airport) with seating where people can wait&amp;quot;. This indicates that techniques for differentiating between different senses are needed – e.g., using a soft clustering technique as in (Pereira et al, 1993) instead of a hard clustering technique. Second, furniture concepts may not have a common prototype that is shared by all of the member concepts. This is a well known problem in the prototype theory of concepts (Laurence and Margolis, 1999). The greater compactness of attribute-based representations vs. value-based ones was more evident in this second experiment. We collected 51,045 distinct values and 8,934 distinct attributes; the total number of value-concept relations is 1,026,335, compared to 422,621 attribute-concept relations. 5 Attributes and Values: A discussion Although our results s</context>
</contexts>
<marker>Pereira, Tishby, Lee, 1993</marker>
<rawString>F. Pereira, N. Tishby, and L. Lee. 1993. Distributional clustering of English words. In Proc. of the 31st ACL, pages 183-190, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Poesio</author>
<author>A Almuhareb</author>
</authors>
<title>Feature-based vs. Property-based KR: An Empirical Perspective.</title>
<date>2004</date>
<note>Submitted.</note>
<marker>Poesio, Almuhareb, 2004</marker>
<rawString>M. Poesio and A. Almuhareb. 2004. Feature-based vs. Property-based KR: An Empirical Perspective. Submitted.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Poesio</author>
<author>T Ishikawa</author>
<author>S Walde</author>
<author>R Vieira</author>
</authors>
<title>Acquiring lexical knowledge for anaphora resolution.</title>
<date>2002</date>
<booktitle>In Proc. of LREC,</booktitle>
<location>Las Palmas,</location>
<contexts>
<context position="2179" citStr="Poesio et al, 2002" startWordPosition="328" endWordPosition="331">however, no attempt has been made by computational linguists to use the attributes themselves in such vectors: i.e., to learn that the description of the concept dog includes elements such as (dog color) or (dog size). This is surprising when considering that most models of concepts in the AI literature are based on such attributes (Brachman and Levesque, 1985). Two problems need to be addressed when trying to identify concept attributes. The first problem is that values are easier to extract. We found, however, that patterns like the X of the dog, already used in (Berland and Charniak, 1999; Poesio et al, 2002) to find part-of relations (using techniques derived from those used in (Hearst, 1998; Caraballo, 1999) to find hyponymy relations) are quite effective at finding attributes. A second problem might be that instances of such patterns are less frequent than those used to extract values, even in large corpora such as the British National Corpus (BNC). But this problem, as well, is less serious when using the Web as a corpus (Kilgarriff and Schuetze, 2003; Keller and Lapata, 2003; Markert et al, submitted). We report on two experiments whose goal was to test whether identifying attributes leads to</context>
<context position="6602" citStr="Poesio et al (2002)" startWordPosition="1056" endWordPosition="1059"> arguments. (We return on the issue of the characterization of attributes below.) Our pattern for attributes is shown below: • &amp;quot;the * of the C [is|was]&amp;quot; where again C is a concept, but the wildcard denotes an unspecified attribute. Again, is/was is used to increase precision. An example of text matching this pattern is: • ... the price of the car was ... Both of the patterns we use satisfy Hearst&apos;s desiderata for good patterns (Hearst, 1998): they are (i) frequent, (ii) precise, and (iii) easy to recognize. Patterns similar to our attribute pattern were used by Berland and Charniak (1999) and Poesio et al (2002) to find object parts only; after collecting their data, Berland and Charniak filtered out words ending with &amp;quot;ness&amp;quot;, &amp;quot;ing&amp;quot;, and &amp;quot;ity&amp;quot;, because these express qualities of objects, and used a ranking method to rank the remaining words. (An accuracy of 55% for the top 50 proposed parts was reported.) We found that these patterns can be used to collect other sorts of &apos;attributes&apos;, as well. 2.2 Web Data Collection through Google In recent years there has been growing evidence that using the Web as a corpus greatly reduces the problem of data sparseness, and its size more than compensates the lack o</context>
</contexts>
<marker>Poesio, Ishikawa, Walde, Vieira, 2002</marker>
<rawString>M. Poesio, T. Ishikawa, S. Walde, and R. Vieira. 2002. Acquiring lexical knowledge for anaphora resolution. In Proc. of LREC, Las Palmas, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Poesio</author>
</authors>
<title>Associative descriptions and salience.</title>
<date>2003</date>
<booktitle>In Proc. of the EACL Workshop on Computational Treatments of Anaphora,</booktitle>
<location>Budapest.</location>
<contexts>
<context position="7442" citStr="Poesio, 2003" startWordPosition="1196" endWordPosition="1197">words. (An accuracy of 55% for the top 50 proposed parts was reported.) We found that these patterns can be used to collect other sorts of &apos;attributes&apos;, as well. 2.2 Web Data Collection through Google In recent years there has been growing evidence that using the Web as a corpus greatly reduces the problem of data sparseness, and its size more than compensates the lack of balance (e.g., (Keller and Lapata, 2003)). The benefits from using the Web over even large corpora like the BNC for extracting semantic relations, particularly when using simple text patterns, were informally pointed out in (Poesio, 2003) and demonstrated more systematically by Markert et al (submitted). These findings were confirmed by our experiments. A comparison of numbers of instances of some patterns using the Web and the BNC is shown in Table 1. Pattern Web BNC Attribute &amp;quot;the * of the *&amp;quot; 23,100,000 208,155 &amp;quot;the * of the * is&amp;quot; 10,900,000 3,627 &amp;quot;the * of the car is&amp;quot; 26,400 5 &amp;quot;the * of the hat is&amp;quot; 2,770 1 Value &amp;quot;the fast * is&amp;quot; 38,100 3 &amp;quot;an electronic * is&amp;quot; 120,000 5 &amp;quot;the * car is&amp;quot; 84,500 24 &amp;quot;the * hat is&amp;quot; 17,100 1 Table 1: Comparison of frequencies of some patterns in BNC and the Web. Web frequency is based on Google count</context>
</contexts>
<marker>Poesio, 2003</marker>
<rawString>M. Poesio. 2003. Associative descriptions and salience. In Proc. of the EACL Workshop on Computational Treatments of Anaphora, Budapest.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pustejovsky</author>
</authors>
<title>The generative lexicon.</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<volume>17</volume>
<issue>4</issue>
<pages>409--441</pages>
<contexts>
<context position="23720" citStr="Pustejovsky, 1991" startWordPosition="4016" endWordPosition="4017">Values: A discussion Although our results suggest that trying to identify attributes is beneficial, the notion of &apos;attribute&apos; is not completely clear, and has been used in widely different ways in Knowledge Representation literature. An attempt of defining the notion has been made by Guarino (1992), who classifies attributes into relational and nonrelational attributes. Relational attributes include qualities such as color and position, and relational roles such as son and spouse. Non-relational attributes include parts such as wheel and engine. The Qualia Structure of the Generative Lexicon (Pustejovsky, 1991) is another attempt at identifying &amp;quot;the essential attributes of an object as defined by the lexical item&amp;quot;. Pustejovsky identifies four roles: Constitutive Role (Guarino&apos;s parts), Formal Role (Guarino&apos;s qualities), Agentive Role (Guarino&apos;s relational roles), and Telic Role (not included in Guarino&apos;s classification). Our analysis of the attribute data shows that the attributes we found can be mapped in the four roles of the Qualia structure. Table 8 shows how we manually mapped the top 50 attributes of the concept car to the Qualia roles and the Guarino&apos;s classes. This mapping is not trivial (e.</context>
</contexts>
<marker>Pustejovsky, 1991</marker>
<rawString>J. Pustejovsky. 1991. The generative lexicon. Computational Linguistics, 17(4), pages 409-441.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Swets</author>
</authors>
<date>1969</date>
<journal>Effectiveness of Information Retrieval Methods. American Documentation,</journal>
<volume>20</volume>
<pages>72--89</pages>
<contexts>
<context position="11732" citStr="Swets, 1969" startWordPosition="1911" endWordPosition="1912">as increased by Google to 20,000 requests per day. 3 Snippets are text excerpts captured from the actual web pages with embedded HTML tags. We process the snippets by removing the HTML tags and extracting the targeted piece of text that was specified in the request. each system cluster, finding the class of each concept in the model clusters, and determining the majority class. The cluster is then labeled with this class; the concepts belonging to it are taken to be correctly clustered, whereas the remaining concepts are judged to be incorrectly clustered. In the contingency table evaluation (Swets, 1969; Hatzivassiloglou and McKeown, 1993), the clusters are converted into two lists (one for the system clusters and one for the model clusters) of yes-no answers to the question &amp;quot;Does the pair of concepts occur in the same cluster?&amp;quot; for each pair of concepts. A contingency table is then built, from which recall (R), precision (P), fallout, and F measures can be computed. For example, if the model clusters are: (A, B, C) and (D), and the system clusters are: (A, B) and (C, D), the yes-no lists are as in Table 2, and the contingency table is as in Table 3. Question Model System Answer Answer Does </context>
</contexts>
<marker>Swets, 1969</marker>
<rawString>J. A. Swets. 1969. Effectiveness of Information Retrieval Methods. American Documentation, 20, pages 72-89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Woods</author>
</authors>
<title>What’s in a link: Foundations for semantic networks.</title>
<date>1975</date>
<booktitle>Representation and Understanding: Studies in Cognitive Science,</booktitle>
<pages>35--82</pages>
<editor>In Daniel G. Bobrow and Alan M. Collins, editors,</editor>
<publisher>Academic Press,</publisher>
<location>New York.</location>
<contexts>
<context position="5431" citStr="Woods, 1975" startWordPosition="846" endWordPosition="847">we just consider every nominal modifier as expressing a potential property. The pattern we use to extract values is as follows: • &amp;quot;[a|an|the] * C [is|was]&amp;quot; where C is a concept, and the wildcard (*) stands for an unspecified value. The restriction to instances containing is or was to ensure that the C actually stands for a concept (i.e., avoiding modifiers) proved adequate to ensure precision. An example of text matching this pattern is: • ... an inexpensive car is ... The pattern we use for extracting concept attributes is based on linguistic tests for attributes already discussed, e.g., in (Woods, 1975). According to Woods, A is an attribute of C if we can say [V is a/the A of C]: e.g., brown is a color of dogs. If no V can be found which is a value of A, then A can not be an attribute for the concept C. This test only selects attributes that have values, and is designed to exclude other functions defined over concepts, such as parts. But some of these functions can be (and have been) viewed as defining attributes of concepts as well; so for the moment we used more general patterns identifying all relational nouns taking a particular concept as arguments. (We return on the issue of the chara</context>
</contexts>
<marker>Woods, 1975</marker>
<rawString>W. A. Woods. 1975. What’s in a link: Foundations for semantic networks. In Daniel G. Bobrow and Alan M. Collins, editors, Representation and Understanding: Studies in Cognitive Science, pages 35-82. Academic Press, New York.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>