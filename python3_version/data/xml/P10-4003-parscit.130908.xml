<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.035414">
<title confidence="0.976247">
BEETLE II: a system for tutoring and computational linguistics
experimentation
</title>
<author confidence="0.906523">
Myroslava O. Dzikovska and Johanna D. Moore
</author>
<affiliation confidence="0.997902">
School of Informatics, University of Edinburgh, Edinburgh, United Kingdom
</affiliation>
<email confidence="0.993465">
{m.dzikovska,j.moore}@ed.ac.uk
</email>
<author confidence="0.822323">
Natalie Steinhauser and Gwendolyn Campbell
</author>
<affiliation confidence="0.841046">
Naval Air Warfare Center Training Systems Division, Orlando, FL, USA
</affiliation>
<email confidence="0.99796">
{gwendolyn.campbell,natalie.steihauser}@navy.mil
</email>
<author confidence="0.994256">
Elaine Farrow
</author>
<affiliation confidence="0.957939">
Heriot-Watt University
Edinburgh, United Kingdom
</affiliation>
<email confidence="0.994785">
e.farrow@hw.ac.uk
</email>
<sectionHeader confidence="0.998581" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999871083333333">
We present BEETLE II, a tutorial dia-
logue system designed to accept unre-
stricted language input and support exper-
imentation with different tutorial planning
and dialogue strategies. Our first system
evaluation used two different tutorial poli-
cies and demonstrated that the system can
be successfully used to study the impact
of different approaches to tutoring. In the
future, the system can also be used to ex-
periment with a variety of natural language
interpretation and generation techniques.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999956619047619">
Over the last decade there has been a lot of inter-
est in developing tutorial dialogue systems that un-
derstand student explanations (Jordan et al., 2006;
Graesser et al., 1999; Aleven et al., 2001; Buckley
and Wolska, 2007; Nielsen et al., 2008; VanLehn
et al., 2007), because high percentages of self-
explanation and student contentful talk are known
to be correlated with better learning in human-
human tutoring (Chi et al., 1994; Litman et al.,
2009; Purandare and Litman, 2008; Steinhauser et
al., 2007). However, most existing systems use
pre-authored tutor responses for addressing stu-
dent errors. The advantage of this approach is that
tutors can devise remediation dialogues that are
highly tailored to specific misconceptions many
students share, providing step-by-step scaffolding
and potentially suggesting additional problems.
The disadvantage is a lack of adaptivity and gen-
erality: students often get the same remediation
for the same error regardless of their past perfor-
mance or dialogue context, as it is infeasible to
</bodyText>
<author confidence="0.422409">
Charles B. Callaway
</author>
<affiliation confidence="0.934018">
University of Haifa
</affiliation>
<address confidence="0.922208">
Mount Carmel, Haifa, Israel
</address>
<email confidence="0.989015">
ccallawa@gmail.com
</email>
<bodyText confidence="0.999909083333334">
author a different remediation dialogue for every
possible dialogue state. It also becomes more dif-
ficult to experiment with different tutorial policies
within the system due to the inherent completixites
in applying tutoring strategies consistently across
a large number of individual hand-authored reme-
diations.
The BEETLE II system architecture is designed
to overcome these limitations (Callaway et al.,
2007). It uses a deep parser and generator, to-
gether with a domain reasoner and a diagnoser,
to produce detailed analyses of student utterances
and generate feedback automatically. This allows
the system to consistently apply the same tutorial
policy across a range of questions. To some extent,
this comes at the expense of being able to address
individual student misconceptions. However, the
system’s modular setup and extensibility make it
a suitable testbed for both computational linguis-
tics algorithms and more general questions about
theories of learning.
A distinguishing feature of the system is that it
is based on an introductory electricity and elec-
tronics course developed by experienced instruc-
tional designers. The course was first created for
use in a human-human tutoring study, without tak-
ing into account possible limitations of computer
tutoring. The exercises were then transferred into
a computer system with only minor adjustments
(e.g., breaking down compound questions into in-
dividual questions). This resulted in a realistic tu-
toring setup, which presents interesting challenges
to language processing components, involving a
wide variety of language phenomena.
We demonstrate a version of the system that
has undergone a successful user evaluation in
</bodyText>
<page confidence="0.990922">
13
</page>
<note confidence="0.6030985">
Proceedings of the ACL 2010 System Demonstrations, pages 13–18,
Uppsala, Sweden, 13 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999194166666667">
2009. The evaluation results indicate that addi-
tional improvements to remediation strategies, and
especially to strategies dealing with interpretation
problems, are necessary for effective tutoring. At
the same time, the successful large-scale evalua-
tion shows that BEETLE II can be used as a plat-
form for future experimentation.
The rest of this paper discusses the BEETLE II
system architecture (Section 2), system evaluation
(Section 3), and the range of computational lin-
guistics problems that can be investigated using
BEETLE II (Section 4).
</bodyText>
<sectionHeader confidence="0.92714" genericHeader="method">
2 System Architecture
</sectionHeader>
<bodyText confidence="0.999931666666667">
The BEETLE II system delivers basic electricity
and electronics tutoring to students with no prior
knowledge of the subject. A screenshot of the sys-
tem is shown in Figure 1. The student interface in-
cludes an area to display reading material, a circuit
simulator, and a dialogue history window. All in-
teractions with the system are typed. Students read
pre-authored curriculum slides and carry out exer-
cises which involve experimenting with the circuit
simulator and explaining the observed behavior.
The system also asks some high-level questions,
such as “What is voltage?”.
The system architecture is shown in Figure 2.
The system uses a standard interpretation pipeline,
with domain-independent parsing and generation
components supported by domain specific reason-
ers for decision making. The architecture is dis-
cussed in detail in the rest of this section.
</bodyText>
<subsectionHeader confidence="0.973602">
2.1 Interpretation Components
</subsectionHeader>
<bodyText confidence="0.999961">
We use the TRIPS dialogue parser (Allen et al.,
2007) to parse the utterances. The parser provides
a domain-independent semantic representation in-
cluding high-level word senses and semantic role
labels. The contextual interpreter then uses a refer-
ence resolution approach similar to Byron (2002),
and an ontology mapping mechanism (Dzikovska
et al., 2008a) to produce a domain-specific seman-
tic representation of the student’s output. Utter-
ance content is represented as a set of extracted
objects and relations between them. Negation is
supported, together with a heuristic scoping algo-
rithm. The interpreter also performs basic ellipsis
resolution. For example, it can determine that in
the answer to the question “Which bulbs will be
on and which bulbs will be off in this diagram?”,
“off” can be taken to mean “all bulbs in the di-
agram will be off.” The resulting output is then
passed on to the domain reasoning and diagnosis
components.
</bodyText>
<subsectionHeader confidence="0.999789">
2.2 Domain Reasoning and Diagnosis
</subsectionHeader>
<bodyText confidence="0.99996865625">
The system uses a knowledge base implemented in
the KM representation language (Clark and Porter,
1999; Dzikovska et al., 2006) to represent the state
of the world. At present, the knowledge base rep-
resents 14 object types and supports the curricu-
lum containing over 200 questions and 40 differ-
ent circuits.
Student explanations are checked on two levels,
verifying factual and explanation correctness. For
example, for a question “Why is bulb A lit?”, if
the student says “it is in a closed path”, the system
checks two things: a) is the bulb indeed in a closed
path? and b) is being in a closed path a reason-
able explanation for the bulb being lit? Different
remediation strategies need to be used depending
on whether the student made a factual error (i.e.,
they misread the diagram and the bulb is not in a
closed path) or produced an incorrect explanation
(i.e., the bulb is indeed in a closed path, but they
failed to mention that a battery needs to be in the
same closed path for the bulb to light).
The knowledge base is used to check the fac-
tual correctness of the answers first, and then a di-
agnoser checks the explanation correctness. The
diagnoser, based on Dzikovska et al. (2008b), out-
puts a diagnosis which consists of lists of correct,
contradictory and non-mentioned objects and re-
lations from the student’s answer. At present, the
system uses a heuristic matching algorithm to clas-
sify relations into the appropriate category, though
in the future we may consider a classifier similar
to Nielsen et al. (2008).
</bodyText>
<subsectionHeader confidence="0.999398">
2.3 Tutorial Planner
</subsectionHeader>
<bodyText confidence="0.999974846153846">
The tutorial planner implements a set of generic
tutoring strategies, as well as a policy to choose
an appropriate strategy at each point of the inter-
action. It is designed so that different policies can
be defined for the system. The currently imple-
mented strategies are: acknowledging the correct
part of the answer; suggesting a slide to read with
background material; prompting for missing parts
of the answer; hinting (low- and high- specificity);
and giving away the answer. Two or more strate-
gies can be used together if necessary.
The hint selection mechanism generates hints
automatically. For a low specificity hint it selects
</bodyText>
<page confidence="0.974079">
14
</page>
<figure confidence="0.999135235294118">
Knowledge
Base
Diagnoser
Curriculum
Planner
Dialogue
Manager
GUI
Parser
Contextual
Interpreter
Tutoring
Tutorial
Planner
Content Planner
&amp; Generator
Interpretation
</figure>
<figureCaption confidence="0.9998885">
Figure 1: Screenshot of the BEETLE II system
Figure 2: System architecture diagram
</figureCaption>
<page confidence="0.984291">
15
</page>
<bodyText confidence="0.999976464285714">
an as-yet unmentioned object and hints at it, for
example, “Here’s a hint: Your answer should men-
tion a battery.” For high-specificity, it attempts to
hint at a two-place relation, for example, “Here’s
a hint: the battery is connected to something.”
The tutorial policy makes a high-level decision
as to which strategy to use (for example, “ac-
knowledge the correct part and give a high speci-
ficity hint”) based on the answer analysis and di-
alogue context. At present, the system takes into
consideration the number of incorrect answers re-
ceived in response to the current question and the
number of uninterpretable answers.1
In addition to a remediation policy, the tuto-
rial planner implements an error recovery policy
(Dzikovska et al., 2009). Since the system ac-
cepts unrestricted input, interpretation errors are
unavoidable. Our recovery policy is modeled on
the TargetedHelp (Hockey et al., 2003) policy used
in task-oriented dialogue. If the system cannot
find an interpretation for an utterance, it attempts
to produce a message that describes the problem
but without giving away the answer, for example,
“I’m sorry, I’m having a problem understanding. I
don’t know the word power.” The help message is
accompanied with a hint at the appropriate level,
also depending on the number of previous incor-
rect and non-interpretable answers.
</bodyText>
<subsectionHeader confidence="0.963129">
2.4 Generation
</subsectionHeader>
<bodyText confidence="0.999990909090909">
The strategy decision made by the tutorial plan-
ner, together with relevant semantic content from
the student’s answer (e.g., part of the answer to
confirm), is passed to content planning and gen-
eration. The system uses a domain-specific con-
tent planner to produce input to the surface realizer
based on the strategy decision, and a FUF/SURGE
(Elhadad and Robin, 1992) generation system to
produce the appropriate text. Templates are used
to generate some stock phrases such as “When you
are ready, go on to the next slide.”
</bodyText>
<subsectionHeader confidence="0.962657">
2.5 Dialogue Management
</subsectionHeader>
<bodyText confidence="0.966113307692308">
Interaction between components is coordinated by
the dialogue manager which uses the information-
state approach (Larsson and Traum, 2000). The
dialogue state is represented by a cumulative an-
swer analysis which tracks, over multiple turns,
the correct, incorrect, and not-yet-mentioned parts
1Other factors such as student confidence could be con-
sidered as well (Callaway et al., 2007).
of the answer. Once the complete answer has been
accumulated, the system accepts it and moves on.
Tutor hints can contribute parts of the answer to
the cumulative state as well, allowing the system
to jointly construct the solution with the student.
</bodyText>
<sectionHeader confidence="0.99871" genericHeader="evaluation">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.999986909090909">
The first experimental evaluation involving 81 par-
ticipants (undergraduates recruited from a South-
eastern University in the USA) was completed in
2009. Participants had little or no prior knowledge
of the domain. Each participant took a pre-test,
worked through a lesson with the system, took a
post-test, and completed a user satisfaction survey.
Each session lasted approximately 4 hours.
We implemented two different tutoring policies
in the system for this evaluation. The baseline
policy used an “accept and bottom out” strategy
for all student answers, regardless of their con-
tent. The students were always given the correct
answer, but the system made no attempt at reme-
diation, and never indicated whether the student
was understood (or correct). In comparison, the
full adaptive policy attempted to select an appro-
priate strategy based on student answer analysis
and dialogue context, as described above. Stu-
dents were randomly assigned to a condition. On
average, each student session contained 232 stu-
dent language turns in the fully adaptive condition
(5D = 25.6) and 156 turns in the baseline condi-
tion (5D = 2.02). Additional time was taken by
reading and interacting with the simulation envi-
ronment.
Out of 81 students, 76 successfully completed
the evaluation (data from 5 participants had to be
discarded due to system crashes). There was no
significant difference in learning gain between the
two conditions (baseline: M = 0.63, 5D = 0.26,
full: M = 0.57, 5D = 0.23, t-test p = 0.32),
and students gave higher satisfaction scores to the
baseline condition (baseline: M = 3.32, 5D =
0.65, full: M = 2.56, 5D = 0.65, t-test, p &lt;
0.01). We are currently analyzing the data to in-
vestigate the reasons for this outcome. However,
the evaluation demonstrated that the system can be
successfully deployed in experiments with naive
users and therefore can be used to study the im-
pact of different natural language processing algo-
rithms on learning gain and user satisfaction.
An example dialogue with the system, taken
from the evaluation corpus, is shown in Figure 3.
</bodyText>
<page confidence="0.994061">
16
</page>
<bodyText confidence="0.970077416666667">
Tutor: Why was bulb A on when switch Y
was open and switch Z was closed?
Student: because it had a closed path
Tutor: Right. There is a closed path. Keep
going.
Student: there was a closed path not effected by
the switch
Tutor: I’m sorry, I’m having a problem un-
derstanding. I don’t understand the
word ’effect’ here. Here’s a hint. Your
answer should mention a battery.
Student: it has a closed path with the battery
</bodyText>
<figureCaption confidence="0.9883708">
Tutor: You’re on the right track. Here’s the
answer. Bulb A is still contained in a
closed path with the battery.
Figure 3: Example interaction with the system
from our corpus
</figureCaption>
<bodyText confidence="0.9999745">
It shows three key system properties: after the stu-
dent’s first turn, the system rephrases its under-
standing of the correct part of the student answer
and prompts the student to supply the missing in-
formation. In the second turn, the student utter-
ance could not be interpreted and the system re-
sponds with a targeted help message and a hint
about the object that needs to be mentioned. Fi-
nally, in the last turn the system combines the in-
formation from the tutor’s hint and the student’s
answers and restates the complete answer since the
current answer was completed over multiple turns.
</bodyText>
<sectionHeader confidence="0.995413" genericHeader="conclusions">
4 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.9999721">
The BEETLE II system we present was built to
serve as a platform for research in computational
linguistics and tutoring, and can be used for task-
based evaluation of algorithms developed for other
domains. We are currently developing an annota-
tion scheme for the data we collected to identify
student paraphrases of correct answers. The an-
notated data will be used to evaluate the accuracy
of existing paraphrasing and textual entailment ap-
proaches and to investigate how to combine such
algorithms with the current deep linguistic analy-
sis to improve system robustness. We also plan
to annotate the data we collected for evidence of
misunderstandings, i.e., situations where the sys-
tem arrived at an incorrect interpretation of a stu-
dent utterance and took action on it. Such annota-
tion can provide useful input for statistical learn-
ing algorithms to detect and recover from misun-
derstandings.
In dialogue management and generation, the
key issue we are planning to investigate is that of
linguistic alignment. The analysis of the data we
have collected indicates that student satisfaction
may be affected if the system rephrases student
answers using different words (for example, using
better terminology) but doesn’t explicitly explain
the reason why different terminology is needed
(Dzikovska et al., 2010). Results from other sys-
tems show that measures of semantic coherence
between a student and a system were positively as-
sociated with higher learning gain (Ward and Lit-
man, 2006). Using a deep generator to automati-
cally generate system feedback gives us a level of
control over the output and will allow us to devise
experiments to study those issues in more detail.
From the point of view of tutoring research,
we are planning to use the system to answer
questions about the effectiveness of different ap-
proaches to tutoring, and the differences between
human-human and human-computer tutoring. Pre-
vious comparisons of human-human and human-
computer dialogue were limited to systems that
asked short-answer questions (Litman et al., 2006;
Ros´e and Torrey, 2005). Having a system that al-
lows more unrestricted language input will pro-
vide a more balanced comparison. We are also
planning experiments that will allow us to eval-
uate the effectiveness of individual strategies im-
plemented in the system by comparing system ver-
sions using different tutoring policies.
</bodyText>
<sectionHeader confidence="0.990763" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9997948">
This work has been supported in part by US Office
of Naval Research grants N000140810043 and
N0001410WX20278. We thank Katherine Harri-
son and Leanne Taylor for their help running the
evaluation.
</bodyText>
<sectionHeader confidence="0.996334" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.8965644">
V. Aleven, O. Popescu, and K. R. Koedinger. 2001.
Towards tutorial dialog to support self-explanation:
Adding natural language understanding to a cogni-
tive tutor. In Proceedings of the 101h International
Conference on Artificial Intelligence in Education
(AIED ’01)”.
James Allen, Myroslava Dzikovska, Mehdi Manshadi,
and Mary Swift. 2007. Deep linguistic processing
for spoken dialogue systems. In Proceedings of the
ACL-07 Workshop on Deep Linguistic Processing.
</reference>
<page confidence="0.995867">
17
</page>
<reference confidence="0.999631221238938">
Mark Buckley and Magdalena Wolska. 2007. To-
wards modelling and using common ground in tu-
torial dialogue. In Proceedings of DECALOG, the
2007 Workshop on the Semantics and Pragmatics of
Dialogue, pages 41–48.
Donna K. Byron. 2002. Resolving Pronominal Refer-
ence to Abstract Entities. Ph.D. thesis, University of
Rochester.
Charles B. Callaway, Myroslava Dzikovska, Elaine
Farrow, Manuel Marques-Pita, Colin Matheson, and
Johanna D. Moore. 2007. The Beetle and BeeD-
iff tutoring systems. In Proceedings of SLaTE’07
(Speech and Language Technology in Education).
Michelene T. H. Chi, Nicholas de Leeuw, Mei-Hung
Chiu, and Christian LaVancher. 1994. Eliciting
self-explanations improves understanding. Cogni-
tive Science, 18(3):439–477.
Peter Clark and Bruce Porter, 1999. KM (1.4): Users
Manual. http://www.cs.utexas.edu/users/mfkb/km.
Myroslava O. Dzikovska, Charles B. Callaway, and
Elaine Farrow. 2006. Interpretation and generation
in a knowledge-based tutorial system. In Proceed-
ings of EACL-06 workshop on knowledge and rea-
soning for language processing, Trento, Italy, April.
Myroslava O. Dzikovska, James F. Allen, and Mary D.
Swift. 2008a. Linking semantic and knowledge
representations in a multi-domain dialogue system.
Journal of Logic and Computation, 18(3):405–430.
Myroslava O. Dzikovska, Gwendolyn E. Campbell,
Charles B. Callaway, Natalie B. Steinhauser, Elaine
Farrow, Johanna D. Moore, Leslie A. Butler, and
Colin Matheson. 2008b. Diagnosing natural lan-
guage answers to support adaptive tutoring. In
Proceedings 21st International FLAIRS Conference,
Coconut Grove, Florida, May.
Myroslava O. Dzikovska, Charles B. Callaway, Elaine
Farrow, Johanna D. Moore, Natalie B. Steinhauser,
and Gwendolyn C. Campbell. 2009. Dealing with
interpretation errors in tutorial dialogue. In Pro-
ceedings of SIGDIAL-09, London, UK, Sep.
Myroslava O. Dzikovska, Johanna D. Moore, Natalie
Steinhauser, and Gwendolyn Campbell. 2010. The
impact of interpretation problems on tutorial dia-
logue. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics(ACL-
2010).
Michael Elhadad and Jacques Robin. 1992. Control-
ling content realization with functional unification
grammars. In R. Dale, E. Hovy, D. R¨osner, and
O. Stock, editors, Proceedings of the Sixth Interna-
tional Workshop on Natural Language Generation,
pages 89–104, Berlin, April. Springer-Verlag.
A. C. Graesser, P. Wiemer-Hastings, P. Wiemer-
Hastings, and R. Kreuz. 1999. Autotutor: A simula-
tion of a human tutor. Cognitive Systems Research,
1:35–51.
Beth Ann Hockey, Oliver Lemon, Ellen Campana,
Laura Hiatt, Gregory Aist, James Hieronymus,
Alexander Gruenstein, and John Dowding. 2003.
Targeted help for spoken dialogue systems: intelli-
gent feedback improves naive users’ performance.
In Proceedings of the tenth conference on European
chapter of the Association for Computational Lin-
guistics, pages 147–154, Morristown, NJ, USA.
Pamela Jordan, Maxim Makatchev, Umarani Pap-
puswamy, Kurt VanLehn, and Patricia Albacete.
2006. A natural language tutorial dialogue system
for physics. In Proceedings of the 19th International
FLAIRS conference.
Staffan Larsson and David Traum. 2000. Information
state and dialogue management in the TRINDI Dia-
logue Move Engine Toolkit. Natural Language En-
gineering, 6(3-4):323–340.
Diane Litman, Carolyn P. Ros´e, Kate Forbes-Riley,
Kurt VanLehn, Dumisizwe Bhembe, and Scott Sil-
liman. 2006. Spoken versus typed human and com-
puter dialogue tutoring. International Journal ofAr-
tificial Intelligence in Education, 16:145–170.
Diane Litman, Johanna Moore, Myroslava Dzikovska,
and Elaine Farrow. 2009. Generalizing tutorial dia-
logue results. In Proceedings of 14th International
Conference on Artificial Intelligence in Education
(AIED), Brighton, UK, July.
Rodney D. Nielsen, Wayne Ward, and James H. Mar-
tin. 2008. Learning to assess low-level conceptual
understanding. In Proceedings 21st International
FLAIRS Conference, Coconut Grove, Florida, May.
Amruta Purandare and Diane Litman. 2008. Content-
learning correlations in spoken tutoring dialogs at
word, turn and discourse levels. In Proceedings 21st
International FLAIRS Conference, Coconut Grove,
Florida, May.
C.P. Ros´e and C. Torrey. 2005. Interactivity versus ex-
pectation: Eliciting learning oriented behavior with
tutorial dialogue systems. In Proceedings of Inter-
act’05.
N. B. Steinhauser, L. A. Butler, and G. E. Campbell.
2007. Simulated tutors in immersive learning envi-
ronments: Empirically-derived design principles. In
Proceedings of the 2007Interservice/Industry Train-
ing, Simulation and Education Conference, Orlando,
FL.
Kurt VanLehn, Pamela Jordan, and Diane Litman.
2007. Developing pedagogically effective tutorial
dialogue tactics: Experiments and a testbed. In Pro-
ceedings of SLaTE Workshop on Speech and Lan-
guage Technology in Education, Farmington, PA,
October.
Arthur Ward and Diane Litman. 2006. Cohesion and
learning in a tutorial spoken dialog system. In Pro-
ceedings of 19th International FLAIRS (Florida Ar-
tificial Intelligence Research Society) Conference,
Melbourne Beach, FL.
</reference>
<page confidence="0.999292">
18
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.402965">
<title confidence="0.9335795">a system for tutoring and computational linguistics experimentation</title>
<author confidence="0.999381">O Dzikovska D Moore</author>
<affiliation confidence="0.845853">School of Informatics, University of Edinburgh, Edinburgh, United Kingdom Steinhauser Campbell</affiliation>
<address confidence="0.749685">Naval Air Warfare Center Training Systems Division, Orlando, FL, USA</address>
<author confidence="0.990283">Elaine Farrow</author>
<affiliation confidence="0.999992">Heriot-Watt University</affiliation>
<address confidence="0.977393">Edinburgh, United Kingdom</address>
<email confidence="0.999245">e.farrow@hw.ac.uk</email>
<abstract confidence="0.999197615384615">present a tutorial dialogue system designed to accept unrestricted language input and support experimentation with different tutorial planning and dialogue strategies. Our first system evaluation used two different tutorial policies and demonstrated that the system can be successfully used to study the impact of different approaches to tutoring. In the future, the system can also be used to experiment with a variety of natural language interpretation and generation techniques.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>V Aleven</author>
<author>O Popescu</author>
<author>K R Koedinger</author>
</authors>
<title>Towards tutorial dialog to support self-explanation: Adding natural language understanding to a cognitive tutor.</title>
<date>2001</date>
<booktitle>In Proceedings of the 101h International Conference on Artificial Intelligence in Education (AIED ’01)”.</booktitle>
<contexts>
<context position="1185" citStr="Aleven et al., 2001" startWordPosition="161" endWordPosition="164">ge input and support experimentation with different tutorial planning and dialogue strategies. Our first system evaluation used two different tutorial policies and demonstrated that the system can be successfully used to study the impact of different approaches to tutoring. In the future, the system can also be used to experiment with a variety of natural language interpretation and generation techniques. 1 Introduction Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations (Jordan et al., 2006; Graesser et al., 1999; Aleven et al., 2001; Buckley and Wolska, 2007; Nielsen et al., 2008; VanLehn et al., 2007), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring (Chi et al., 1994; Litman et al., 2009; Purandare and Litman, 2008; Steinhauser et al., 2007). However, most existing systems use pre-authored tutor responses for addressing student errors. The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and pot</context>
</contexts>
<marker>Aleven, Popescu, Koedinger, 2001</marker>
<rawString>V. Aleven, O. Popescu, and K. R. Koedinger. 2001. Towards tutorial dialog to support self-explanation: Adding natural language understanding to a cognitive tutor. In Proceedings of the 101h International Conference on Artificial Intelligence in Education (AIED ’01)”.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Allen</author>
<author>Myroslava Dzikovska</author>
<author>Mehdi Manshadi</author>
<author>Mary Swift</author>
</authors>
<title>Deep linguistic processing for spoken dialogue systems.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL-07 Workshop on Deep Linguistic Processing.</booktitle>
<contexts>
<context position="5457" citStr="Allen et al., 2007" startWordPosition="802" endWordPosition="805">m are typed. Students read pre-authored curriculum slides and carry out exercises which involve experimenting with the circuit simulator and explaining the observed behavior. The system also asks some high-level questions, such as “What is voltage?”. The system architecture is shown in Figure 2. The system uses a standard interpretation pipeline, with domain-independent parsing and generation components supported by domain specific reasoners for decision making. The architecture is discussed in detail in the rest of this section. 2.1 Interpretation Components We use the TRIPS dialogue parser (Allen et al., 2007) to parse the utterances. The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels. The contextual interpreter then uses a reference resolution approach similar to Byron (2002), and an ontology mapping mechanism (Dzikovska et al., 2008a) to produce a domain-specific semantic representation of the student’s output. Utterance content is represented as a set of extracted objects and relations between them. Negation is supported, together with a heuristic scoping algorithm. The interpreter also performs basic ellipsis resolution. Fo</context>
</contexts>
<marker>Allen, Dzikovska, Manshadi, Swift, 2007</marker>
<rawString>James Allen, Myroslava Dzikovska, Mehdi Manshadi, and Mary Swift. 2007. Deep linguistic processing for spoken dialogue systems. In Proceedings of the ACL-07 Workshop on Deep Linguistic Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Buckley</author>
<author>Magdalena Wolska</author>
</authors>
<title>Towards modelling and using common ground in tutorial dialogue.</title>
<date>2007</date>
<booktitle>In Proceedings of DECALOG, the 2007 Workshop on the Semantics and Pragmatics of Dialogue,</booktitle>
<pages>41--48</pages>
<contexts>
<context position="1211" citStr="Buckley and Wolska, 2007" startWordPosition="165" endWordPosition="168">experimentation with different tutorial planning and dialogue strategies. Our first system evaluation used two different tutorial policies and demonstrated that the system can be successfully used to study the impact of different approaches to tutoring. In the future, the system can also be used to experiment with a variety of natural language interpretation and generation techniques. 1 Introduction Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations (Jordan et al., 2006; Graesser et al., 1999; Aleven et al., 2001; Buckley and Wolska, 2007; Nielsen et al., 2008; VanLehn et al., 2007), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring (Chi et al., 1994; Litman et al., 2009; Purandare and Litman, 2008; Steinhauser et al., 2007). However, most existing systems use pre-authored tutor responses for addressing student errors. The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additi</context>
</contexts>
<marker>Buckley, Wolska, 2007</marker>
<rawString>Mark Buckley and Magdalena Wolska. 2007. Towards modelling and using common ground in tutorial dialogue. In Proceedings of DECALOG, the 2007 Workshop on the Semantics and Pragmatics of Dialogue, pages 41–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donna K Byron</author>
</authors>
<title>Resolving Pronominal Reference to Abstract Entities.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Rochester.</institution>
<contexts>
<context position="5699" citStr="Byron (2002)" startWordPosition="838" endWordPosition="839">. The system architecture is shown in Figure 2. The system uses a standard interpretation pipeline, with domain-independent parsing and generation components supported by domain specific reasoners for decision making. The architecture is discussed in detail in the rest of this section. 2.1 Interpretation Components We use the TRIPS dialogue parser (Allen et al., 2007) to parse the utterances. The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels. The contextual interpreter then uses a reference resolution approach similar to Byron (2002), and an ontology mapping mechanism (Dzikovska et al., 2008a) to produce a domain-specific semantic representation of the student’s output. Utterance content is represented as a set of extracted objects and relations between them. Negation is supported, together with a heuristic scoping algorithm. The interpreter also performs basic ellipsis resolution. For example, it can determine that in the answer to the question “Which bulbs will be on and which bulbs will be off in this diagram?”, “off” can be taken to mean “all bulbs in the diagram will be off.” The resulting output is then passed on to</context>
</contexts>
<marker>Byron, 2002</marker>
<rawString>Donna K. Byron. 2002. Resolving Pronominal Reference to Abstract Entities. Ph.D. thesis, University of Rochester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles B Callaway</author>
<author>Myroslava Dzikovska</author>
<author>Elaine Farrow</author>
<author>Manuel Marques-Pita</author>
<author>Colin Matheson</author>
<author>Johanna D Moore</author>
</authors>
<title>The Beetle and BeeDiff tutoring systems.</title>
<date>2007</date>
<booktitle>In Proceedings of SLaTE’07 (Speech and Language Technology in Education).</booktitle>
<contexts>
<context position="2523" citStr="Callaway et al., 2007" startWordPosition="358" endWordPosition="361"> the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to Charles B. Callaway University of Haifa Mount Carmel, Haifa, Israel ccallawa@gmail.com author a different remediation dialogue for every possible dialogue state. It also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand-authored remediations. The BEETLE II system architecture is designed to overcome these limitations (Callaway et al., 2007). It uses a deep parser and generator, together with a domain reasoner and a diagnoser, to produce detailed analyses of student utterances and generate feedback automatically. This allows the system to consistently apply the same tutorial policy across a range of questions. To some extent, this comes at the expense of being able to address individual student misconceptions. However, the system’s modular setup and extensibility make it a suitable testbed for both computational linguistics algorithms and more general questions about theories of learning. A distinguishing feature of the system is</context>
<context position="11111" citStr="Callaway et al., 2007" startWordPosition="1716" endWordPosition="1719">ategy decision, and a FUF/SURGE (Elhadad and Robin, 1992) generation system to produce the appropriate text. Templates are used to generate some stock phrases such as “When you are ready, go on to the next slide.” 2.5 Dialogue Management Interaction between components is coordinated by the dialogue manager which uses the informationstate approach (Larsson and Traum, 2000). The dialogue state is represented by a cumulative answer analysis which tracks, over multiple turns, the correct, incorrect, and not-yet-mentioned parts 1Other factors such as student confidence could be considered as well (Callaway et al., 2007). of the answer. Once the complete answer has been accumulated, the system accepts it and moves on. Tutor hints can contribute parts of the answer to the cumulative state as well, allowing the system to jointly construct the solution with the student. 3 Evaluation The first experimental evaluation involving 81 participants (undergraduates recruited from a Southeastern University in the USA) was completed in 2009. Participants had little or no prior knowledge of the domain. Each participant took a pre-test, worked through a lesson with the system, took a post-test, and completed a user satisfac</context>
</contexts>
<marker>Callaway, Dzikovska, Farrow, Marques-Pita, Matheson, Moore, 2007</marker>
<rawString>Charles B. Callaway, Myroslava Dzikovska, Elaine Farrow, Manuel Marques-Pita, Colin Matheson, and Johanna D. Moore. 2007. The Beetle and BeeDiff tutoring systems. In Proceedings of SLaTE’07 (Speech and Language Technology in Education).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michelene T H Chi</author>
<author>Nicholas de Leeuw</author>
<author>Mei-Hung Chiu</author>
<author>Christian LaVancher</author>
</authors>
<title>Eliciting self-explanations improves understanding.</title>
<date>1994</date>
<journal>Cognitive Science,</journal>
<volume>18</volume>
<issue>3</issue>
<marker>Chi, de Leeuw, Chiu, LaVancher, 1994</marker>
<rawString>Michelene T. H. Chi, Nicholas de Leeuw, Mei-Hung Chiu, and Christian LaVancher. 1994. Eliciting self-explanations improves understanding. Cognitive Science, 18(3):439–477.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Clark</author>
<author>Bruce Porter</author>
</authors>
<date>1999</date>
<note>KM (1.4): Users Manual. http://www.cs.utexas.edu/users/mfkb/km.</note>
<contexts>
<context position="6484" citStr="Clark and Porter, 1999" startWordPosition="964" endWordPosition="967">presented as a set of extracted objects and relations between them. Negation is supported, together with a heuristic scoping algorithm. The interpreter also performs basic ellipsis resolution. For example, it can determine that in the answer to the question “Which bulbs will be on and which bulbs will be off in this diagram?”, “off” can be taken to mean “all bulbs in the diagram will be off.” The resulting output is then passed on to the domain reasoning and diagnosis components. 2.2 Domain Reasoning and Diagnosis The system uses a knowledge base implemented in the KM representation language (Clark and Porter, 1999; Dzikovska et al., 2006) to represent the state of the world. At present, the knowledge base represents 14 object types and supports the curriculum containing over 200 questions and 40 different circuits. Student explanations are checked on two levels, verifying factual and explanation correctness. For example, for a question “Why is bulb A lit?”, if the student says “it is in a closed path”, the system checks two things: a) is the bulb indeed in a closed path? and b) is being in a closed path a reasonable explanation for the bulb being lit? Different remediation strategies need to be used de</context>
</contexts>
<marker>Clark, Porter, 1999</marker>
<rawString>Peter Clark and Bruce Porter, 1999. KM (1.4): Users Manual. http://www.cs.utexas.edu/users/mfkb/km.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Myroslava O Dzikovska</author>
<author>Charles B Callaway</author>
<author>Elaine Farrow</author>
</authors>
<title>Interpretation and generation in a knowledge-based tutorial system.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL-06 workshop on</booktitle>
<location>Trento, Italy,</location>
<contexts>
<context position="6509" citStr="Dzikovska et al., 2006" startWordPosition="968" endWordPosition="971">tracted objects and relations between them. Negation is supported, together with a heuristic scoping algorithm. The interpreter also performs basic ellipsis resolution. For example, it can determine that in the answer to the question “Which bulbs will be on and which bulbs will be off in this diagram?”, “off” can be taken to mean “all bulbs in the diagram will be off.” The resulting output is then passed on to the domain reasoning and diagnosis components. 2.2 Domain Reasoning and Diagnosis The system uses a knowledge base implemented in the KM representation language (Clark and Porter, 1999; Dzikovska et al., 2006) to represent the state of the world. At present, the knowledge base represents 14 object types and supports the curriculum containing over 200 questions and 40 different circuits. Student explanations are checked on two levels, verifying factual and explanation correctness. For example, for a question “Why is bulb A lit?”, if the student says “it is in a closed path”, the system checks two things: a) is the bulb indeed in a closed path? and b) is being in a closed path a reasonable explanation for the bulb being lit? Different remediation strategies need to be used depending on whether the st</context>
</contexts>
<marker>Dzikovska, Callaway, Farrow, 2006</marker>
<rawString>Myroslava O. Dzikovska, Charles B. Callaway, and Elaine Farrow. 2006. Interpretation and generation in a knowledge-based tutorial system. In Proceedings of EACL-06 workshop on knowledge and reasoning for language processing, Trento, Italy, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Myroslava O Dzikovska</author>
<author>James F Allen</author>
<author>Mary D Swift</author>
</authors>
<title>Linking semantic and knowledge representations in a multi-domain dialogue system.</title>
<date>2008</date>
<journal>Journal of Logic and Computation,</journal>
<volume>18</volume>
<issue>3</issue>
<contexts>
<context position="5758" citStr="Dzikovska et al., 2008" startWordPosition="845" endWordPosition="848">he system uses a standard interpretation pipeline, with domain-independent parsing and generation components supported by domain specific reasoners for decision making. The architecture is discussed in detail in the rest of this section. 2.1 Interpretation Components We use the TRIPS dialogue parser (Allen et al., 2007) to parse the utterances. The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels. The contextual interpreter then uses a reference resolution approach similar to Byron (2002), and an ontology mapping mechanism (Dzikovska et al., 2008a) to produce a domain-specific semantic representation of the student’s output. Utterance content is represented as a set of extracted objects and relations between them. Negation is supported, together with a heuristic scoping algorithm. The interpreter also performs basic ellipsis resolution. For example, it can determine that in the answer to the question “Which bulbs will be on and which bulbs will be off in this diagram?”, “off” can be taken to mean “all bulbs in the diagram will be off.” The resulting output is then passed on to the domain reasoning and diagnosis components. 2.2 Domain </context>
<context position="7574" citStr="Dzikovska et al. (2008" startWordPosition="1159" endWordPosition="1162">nd b) is being in a closed path a reasonable explanation for the bulb being lit? Different remediation strategies need to be used depending on whether the student made a factual error (i.e., they misread the diagram and the bulb is not in a closed path) or produced an incorrect explanation (i.e., the bulb is indeed in a closed path, but they failed to mention that a battery needs to be in the same closed path for the bulb to light). The knowledge base is used to check the factual correctness of the answers first, and then a diagnoser checks the explanation correctness. The diagnoser, based on Dzikovska et al. (2008b), outputs a diagnosis which consists of lists of correct, contradictory and non-mentioned objects and relations from the student’s answer. At present, the system uses a heuristic matching algorithm to classify relations into the appropriate category, though in the future we may consider a classifier similar to Nielsen et al. (2008). 2.3 Tutorial Planner The tutorial planner implements a set of generic tutoring strategies, as well as a policy to choose an appropriate strategy at each point of the interaction. It is designed so that different policies can be defined for the system. The current</context>
</contexts>
<marker>Dzikovska, Allen, Swift, 2008</marker>
<rawString>Myroslava O. Dzikovska, James F. Allen, and Mary D. Swift. 2008a. Linking semantic and knowledge representations in a multi-domain dialogue system. Journal of Logic and Computation, 18(3):405–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Myroslava O Dzikovska</author>
<author>Gwendolyn E Campbell</author>
<author>Charles B Callaway</author>
<author>Natalie B Steinhauser</author>
<author>Elaine Farrow</author>
<author>Johanna D Moore</author>
<author>Leslie A Butler</author>
<author>Colin Matheson</author>
</authors>
<title>Diagnosing natural language answers to support adaptive tutoring.</title>
<date>2008</date>
<booktitle>In Proceedings 21st International FLAIRS Conference, Coconut</booktitle>
<location>Grove, Florida,</location>
<contexts>
<context position="5758" citStr="Dzikovska et al., 2008" startWordPosition="845" endWordPosition="848">he system uses a standard interpretation pipeline, with domain-independent parsing and generation components supported by domain specific reasoners for decision making. The architecture is discussed in detail in the rest of this section. 2.1 Interpretation Components We use the TRIPS dialogue parser (Allen et al., 2007) to parse the utterances. The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels. The contextual interpreter then uses a reference resolution approach similar to Byron (2002), and an ontology mapping mechanism (Dzikovska et al., 2008a) to produce a domain-specific semantic representation of the student’s output. Utterance content is represented as a set of extracted objects and relations between them. Negation is supported, together with a heuristic scoping algorithm. The interpreter also performs basic ellipsis resolution. For example, it can determine that in the answer to the question “Which bulbs will be on and which bulbs will be off in this diagram?”, “off” can be taken to mean “all bulbs in the diagram will be off.” The resulting output is then passed on to the domain reasoning and diagnosis components. 2.2 Domain </context>
<context position="7574" citStr="Dzikovska et al. (2008" startWordPosition="1159" endWordPosition="1162">nd b) is being in a closed path a reasonable explanation for the bulb being lit? Different remediation strategies need to be used depending on whether the student made a factual error (i.e., they misread the diagram and the bulb is not in a closed path) or produced an incorrect explanation (i.e., the bulb is indeed in a closed path, but they failed to mention that a battery needs to be in the same closed path for the bulb to light). The knowledge base is used to check the factual correctness of the answers first, and then a diagnoser checks the explanation correctness. The diagnoser, based on Dzikovska et al. (2008b), outputs a diagnosis which consists of lists of correct, contradictory and non-mentioned objects and relations from the student’s answer. At present, the system uses a heuristic matching algorithm to classify relations into the appropriate category, though in the future we may consider a classifier similar to Nielsen et al. (2008). 2.3 Tutorial Planner The tutorial planner implements a set of generic tutoring strategies, as well as a policy to choose an appropriate strategy at each point of the interaction. It is designed so that different policies can be defined for the system. The current</context>
</contexts>
<marker>Dzikovska, Campbell, Callaway, Steinhauser, Farrow, Moore, Butler, Matheson, 2008</marker>
<rawString>Myroslava O. Dzikovska, Gwendolyn E. Campbell, Charles B. Callaway, Natalie B. Steinhauser, Elaine Farrow, Johanna D. Moore, Leslie A. Butler, and Colin Matheson. 2008b. Diagnosing natural language answers to support adaptive tutoring. In Proceedings 21st International FLAIRS Conference, Coconut Grove, Florida, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Myroslava O Dzikovska</author>
<author>Charles B Callaway</author>
<author>Elaine Farrow</author>
<author>Johanna D Moore</author>
<author>Natalie B Steinhauser</author>
<author>Gwendolyn C Campbell</author>
</authors>
<title>Dealing with interpretation errors in tutorial dialogue.</title>
<date>2009</date>
<booktitle>In Proceedings of SIGDIAL-09,</booktitle>
<location>London, UK,</location>
<contexts>
<context position="9565" citStr="Dzikovska et al., 2009" startWordPosition="1474" endWordPosition="1477">gh-specificity, it attempts to hint at a two-place relation, for example, “Here’s a hint: the battery is connected to something.” The tutorial policy makes a high-level decision as to which strategy to use (for example, “acknowledge the correct part and give a high specificity hint”) based on the answer analysis and dialogue context. At present, the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers.1 In addition to a remediation policy, the tutorial planner implements an error recovery policy (Dzikovska et al., 2009). Since the system accepts unrestricted input, interpretation errors are unavoidable. Our recovery policy is modeled on the TargetedHelp (Hockey et al., 2003) policy used in task-oriented dialogue. If the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, “I’m sorry, I’m having a problem understanding. I don’t know the word power.” The help message is accompanied with a hint at the appropriate level, also depending on the number of previous incorrect and non-interpretable answers. 2</context>
</contexts>
<marker>Dzikovska, Callaway, Farrow, Moore, Steinhauser, Campbell, 2009</marker>
<rawString>Myroslava O. Dzikovska, Charles B. Callaway, Elaine Farrow, Johanna D. Moore, Natalie B. Steinhauser, and Gwendolyn C. Campbell. 2009. Dealing with interpretation errors in tutorial dialogue. In Proceedings of SIGDIAL-09, London, UK, Sep.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Myroslava O Dzikovska</author>
<author>Johanna D Moore</author>
<author>Natalie Steinhauser</author>
<author>Gwendolyn Campbell</author>
</authors>
<title>The impact of interpretation problems on tutorial dialogue.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics(ACL2010).</booktitle>
<contexts>
<context position="15980" citStr="Dzikovska et al., 2010" startWordPosition="2524" endWordPosition="2527">rrived at an incorrect interpretation of a student utterance and took action on it. Such annotation can provide useful input for statistical learning algorithms to detect and recover from misunderstandings. In dialogue management and generation, the key issue we are planning to investigate is that of linguistic alignment. The analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words (for example, using better terminology) but doesn’t explicitly explain the reason why different terminology is needed (Dzikovska et al., 2010). Results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain (Ward and Litman, 2006). Using a deep generator to automatically generate system feedback gives us a level of control over the output and will allow us to devise experiments to study those issues in more detail. From the point of view of tutoring research, we are planning to use the system to answer questions about the effectiveness of different approaches to tutoring, and the differences between human-human and human-computer tutoring. Previ</context>
</contexts>
<marker>Dzikovska, Moore, Steinhauser, Campbell, 2010</marker>
<rawString>Myroslava O. Dzikovska, Johanna D. Moore, Natalie Steinhauser, and Gwendolyn Campbell. 2010. The impact of interpretation problems on tutorial dialogue. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics(ACL2010).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Elhadad</author>
<author>Jacques Robin</author>
</authors>
<title>Controlling content realization with functional unification grammars. In</title>
<date>1992</date>
<booktitle>Proceedings of the Sixth International Workshop on Natural Language Generation,</booktitle>
<pages>89--104</pages>
<editor>R. Dale, E. Hovy, D. R¨osner, and O. Stock, editors,</editor>
<publisher>Springer-Verlag.</publisher>
<location>Berlin,</location>
<contexts>
<context position="10546" citStr="Elhadad and Robin, 1992" startWordPosition="1629" endWordPosition="1632">or example, “I’m sorry, I’m having a problem understanding. I don’t know the word power.” The help message is accompanied with a hint at the appropriate level, also depending on the number of previous incorrect and non-interpretable answers. 2.4 Generation The strategy decision made by the tutorial planner, together with relevant semantic content from the student’s answer (e.g., part of the answer to confirm), is passed to content planning and generation. The system uses a domain-specific content planner to produce input to the surface realizer based on the strategy decision, and a FUF/SURGE (Elhadad and Robin, 1992) generation system to produce the appropriate text. Templates are used to generate some stock phrases such as “When you are ready, go on to the next slide.” 2.5 Dialogue Management Interaction between components is coordinated by the dialogue manager which uses the informationstate approach (Larsson and Traum, 2000). The dialogue state is represented by a cumulative answer analysis which tracks, over multiple turns, the correct, incorrect, and not-yet-mentioned parts 1Other factors such as student confidence could be considered as well (Callaway et al., 2007). of the answer. Once the complete </context>
</contexts>
<marker>Elhadad, Robin, 1992</marker>
<rawString>Michael Elhadad and Jacques Robin. 1992. Controlling content realization with functional unification grammars. In R. Dale, E. Hovy, D. R¨osner, and O. Stock, editors, Proceedings of the Sixth International Workshop on Natural Language Generation, pages 89–104, Berlin, April. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A C Graesser</author>
<author>P Wiemer-Hastings</author>
<author>P WiemerHastings</author>
<author>R Kreuz</author>
</authors>
<title>Autotutor: A simulation of a human tutor. Cognitive Systems Research,</title>
<date>1999</date>
<pages>1--35</pages>
<contexts>
<context position="1164" citStr="Graesser et al., 1999" startWordPosition="157" endWordPosition="160">ept unrestricted language input and support experimentation with different tutorial planning and dialogue strategies. Our first system evaluation used two different tutorial policies and demonstrated that the system can be successfully used to study the impact of different approaches to tutoring. In the future, the system can also be used to experiment with a variety of natural language interpretation and generation techniques. 1 Introduction Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations (Jordan et al., 2006; Graesser et al., 1999; Aleven et al., 2001; Buckley and Wolska, 2007; Nielsen et al., 2008; VanLehn et al., 2007), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring (Chi et al., 1994; Litman et al., 2009; Purandare and Litman, 2008; Steinhauser et al., 2007). However, most existing systems use pre-authored tutor responses for addressing student errors. The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-ste</context>
</contexts>
<marker>Graesser, Wiemer-Hastings, WiemerHastings, Kreuz, 1999</marker>
<rawString>A. C. Graesser, P. Wiemer-Hastings, P. WiemerHastings, and R. Kreuz. 1999. Autotutor: A simulation of a human tutor. Cognitive Systems Research, 1:35–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Ann Hockey</author>
<author>Oliver Lemon</author>
<author>Ellen Campana</author>
<author>Laura Hiatt</author>
<author>Gregory Aist</author>
<author>James Hieronymus</author>
<author>Alexander Gruenstein</author>
<author>John Dowding</author>
</authors>
<title>Targeted help for spoken dialogue systems: intelligent feedback improves naive users’ performance.</title>
<date>2003</date>
<booktitle>In Proceedings of the tenth conference on European chapter of the Association for Computational Linguistics,</booktitle>
<pages>147--154</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="9723" citStr="Hockey et al., 2003" startWordPosition="1497" endWordPosition="1500">h-level decision as to which strategy to use (for example, “acknowledge the correct part and give a high specificity hint”) based on the answer analysis and dialogue context. At present, the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers.1 In addition to a remediation policy, the tutorial planner implements an error recovery policy (Dzikovska et al., 2009). Since the system accepts unrestricted input, interpretation errors are unavoidable. Our recovery policy is modeled on the TargetedHelp (Hockey et al., 2003) policy used in task-oriented dialogue. If the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, “I’m sorry, I’m having a problem understanding. I don’t know the word power.” The help message is accompanied with a hint at the appropriate level, also depending on the number of previous incorrect and non-interpretable answers. 2.4 Generation The strategy decision made by the tutorial planner, together with relevant semantic content from the student’s answer (e.g., part of the answer </context>
</contexts>
<marker>Hockey, Lemon, Campana, Hiatt, Aist, Hieronymus, Gruenstein, Dowding, 2003</marker>
<rawString>Beth Ann Hockey, Oliver Lemon, Ellen Campana, Laura Hiatt, Gregory Aist, James Hieronymus, Alexander Gruenstein, and John Dowding. 2003. Targeted help for spoken dialogue systems: intelligent feedback improves naive users’ performance. In Proceedings of the tenth conference on European chapter of the Association for Computational Linguistics, pages 147–154, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pamela Jordan</author>
<author>Maxim Makatchev</author>
<author>Umarani Pappuswamy</author>
<author>Kurt VanLehn</author>
<author>Patricia Albacete</author>
</authors>
<title>A natural language tutorial dialogue system for physics.</title>
<date>2006</date>
<booktitle>In Proceedings of the 19th International FLAIRS conference.</booktitle>
<contexts>
<context position="1141" citStr="Jordan et al., 2006" startWordPosition="153" endWordPosition="156">ystem designed to accept unrestricted language input and support experimentation with different tutorial planning and dialogue strategies. Our first system evaluation used two different tutorial policies and demonstrated that the system can be successfully used to study the impact of different approaches to tutoring. In the future, the system can also be used to experiment with a variety of natural language interpretation and generation techniques. 1 Introduction Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations (Jordan et al., 2006; Graesser et al., 1999; Aleven et al., 2001; Buckley and Wolska, 2007; Nielsen et al., 2008; VanLehn et al., 2007), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring (Chi et al., 1994; Litman et al., 2009; Purandare and Litman, 2008; Steinhauser et al., 2007). However, most existing systems use pre-authored tutor responses for addressing student errors. The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share</context>
</contexts>
<marker>Jordan, Makatchev, Pappuswamy, VanLehn, Albacete, 2006</marker>
<rawString>Pamela Jordan, Maxim Makatchev, Umarani Pappuswamy, Kurt VanLehn, and Patricia Albacete. 2006. A natural language tutorial dialogue system for physics. In Proceedings of the 19th International FLAIRS conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Staffan Larsson</author>
<author>David Traum</author>
</authors>
<title>Information state and dialogue management in the TRINDI Dialogue Move Engine Toolkit.</title>
<date>2000</date>
<journal>Natural Language Engineering,</journal>
<pages>6--3</pages>
<contexts>
<context position="10863" citStr="Larsson and Traum, 2000" startWordPosition="1678" endWordPosition="1681"> with relevant semantic content from the student’s answer (e.g., part of the answer to confirm), is passed to content planning and generation. The system uses a domain-specific content planner to produce input to the surface realizer based on the strategy decision, and a FUF/SURGE (Elhadad and Robin, 1992) generation system to produce the appropriate text. Templates are used to generate some stock phrases such as “When you are ready, go on to the next slide.” 2.5 Dialogue Management Interaction between components is coordinated by the dialogue manager which uses the informationstate approach (Larsson and Traum, 2000). The dialogue state is represented by a cumulative answer analysis which tracks, over multiple turns, the correct, incorrect, and not-yet-mentioned parts 1Other factors such as student confidence could be considered as well (Callaway et al., 2007). of the answer. Once the complete answer has been accumulated, the system accepts it and moves on. Tutor hints can contribute parts of the answer to the cumulative state as well, allowing the system to jointly construct the solution with the student. 3 Evaluation The first experimental evaluation involving 81 participants (undergraduates recruited f</context>
</contexts>
<marker>Larsson, Traum, 2000</marker>
<rawString>Staffan Larsson and David Traum. 2000. Information state and dialogue management in the TRINDI Dialogue Move Engine Toolkit. Natural Language Engineering, 6(3-4):323–340.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diane Litman</author>
<author>Carolyn P Ros´e</author>
<author>Kate Forbes-Riley</author>
<author>Kurt VanLehn</author>
<author>Dumisizwe Bhembe</author>
<author>Scott Silliman</author>
</authors>
<title>Spoken versus typed human and computer dialogue tutoring.</title>
<date>2006</date>
<booktitle>International Journal ofArtificial Intelligence in Education,</booktitle>
<pages>16--145</pages>
<marker>Litman, Ros´e, Forbes-Riley, VanLehn, Bhembe, Silliman, 2006</marker>
<rawString>Diane Litman, Carolyn P. Ros´e, Kate Forbes-Riley, Kurt VanLehn, Dumisizwe Bhembe, and Scott Silliman. 2006. Spoken versus typed human and computer dialogue tutoring. International Journal ofArtificial Intelligence in Education, 16:145–170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diane Litman</author>
<author>Johanna Moore</author>
<author>Myroslava Dzikovska</author>
<author>Elaine Farrow</author>
</authors>
<title>Generalizing tutorial dialogue results.</title>
<date>2009</date>
<booktitle>In Proceedings of 14th International Conference on Artificial Intelligence in Education (AIED),</booktitle>
<location>Brighton, UK,</location>
<contexts>
<context position="1439" citStr="Litman et al., 2009" startWordPosition="203" endWordPosition="206">roaches to tutoring. In the future, the system can also be used to experiment with a variety of natural language interpretation and generation techniques. 1 Introduction Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations (Jordan et al., 2006; Graesser et al., 1999; Aleven et al., 2001; Buckley and Wolska, 2007; Nielsen et al., 2008; VanLehn et al., 2007), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring (Chi et al., 1994; Litman et al., 2009; Purandare and Litman, 2008; Steinhauser et al., 2007). However, most existing systems use pre-authored tutor responses for addressing student errors. The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems. The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to Charles B. Calla</context>
</contexts>
<marker>Litman, Moore, Dzikovska, Farrow, 2009</marker>
<rawString>Diane Litman, Johanna Moore, Myroslava Dzikovska, and Elaine Farrow. 2009. Generalizing tutorial dialogue results. In Proceedings of 14th International Conference on Artificial Intelligence in Education (AIED), Brighton, UK, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rodney D Nielsen</author>
<author>Wayne Ward</author>
<author>James H Martin</author>
</authors>
<title>Learning to assess low-level conceptual understanding.</title>
<date>2008</date>
<booktitle>In Proceedings 21st International FLAIRS Conference, Coconut</booktitle>
<location>Grove, Florida,</location>
<contexts>
<context position="1233" citStr="Nielsen et al., 2008" startWordPosition="169" endWordPosition="172">rent tutorial planning and dialogue strategies. Our first system evaluation used two different tutorial policies and demonstrated that the system can be successfully used to study the impact of different approaches to tutoring. In the future, the system can also be used to experiment with a variety of natural language interpretation and generation techniques. 1 Introduction Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations (Jordan et al., 2006; Graesser et al., 1999; Aleven et al., 2001; Buckley and Wolska, 2007; Nielsen et al., 2008; VanLehn et al., 2007), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring (Chi et al., 1994; Litman et al., 2009; Purandare and Litman, 2008; Steinhauser et al., 2007). However, most existing systems use pre-authored tutor responses for addressing student errors. The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems. The dis</context>
<context position="7909" citStr="Nielsen et al. (2008)" startWordPosition="1212" endWordPosition="1215">ut they failed to mention that a battery needs to be in the same closed path for the bulb to light). The knowledge base is used to check the factual correctness of the answers first, and then a diagnoser checks the explanation correctness. The diagnoser, based on Dzikovska et al. (2008b), outputs a diagnosis which consists of lists of correct, contradictory and non-mentioned objects and relations from the student’s answer. At present, the system uses a heuristic matching algorithm to classify relations into the appropriate category, though in the future we may consider a classifier similar to Nielsen et al. (2008). 2.3 Tutorial Planner The tutorial planner implements a set of generic tutoring strategies, as well as a policy to choose an appropriate strategy at each point of the interaction. It is designed so that different policies can be defined for the system. The currently implemented strategies are: acknowledging the correct part of the answer; suggesting a slide to read with background material; prompting for missing parts of the answer; hinting (low- and high- specificity); and giving away the answer. Two or more strategies can be used together if necessary. The hint selection mechanism generates</context>
</contexts>
<marker>Nielsen, Ward, Martin, 2008</marker>
<rawString>Rodney D. Nielsen, Wayne Ward, and James H. Martin. 2008. Learning to assess low-level conceptual understanding. In Proceedings 21st International FLAIRS Conference, Coconut Grove, Florida, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amruta Purandare</author>
<author>Diane Litman</author>
</authors>
<title>Contentlearning correlations in spoken tutoring dialogs at word, turn and discourse levels.</title>
<date>2008</date>
<booktitle>In Proceedings 21st International FLAIRS Conference, Coconut</booktitle>
<location>Grove, Florida,</location>
<contexts>
<context position="1467" citStr="Purandare and Litman, 2008" startWordPosition="207" endWordPosition="210">In the future, the system can also be used to experiment with a variety of natural language interpretation and generation techniques. 1 Introduction Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations (Jordan et al., 2006; Graesser et al., 1999; Aleven et al., 2001; Buckley and Wolska, 2007; Nielsen et al., 2008; VanLehn et al., 2007), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring (Chi et al., 1994; Litman et al., 2009; Purandare and Litman, 2008; Steinhauser et al., 2007). However, most existing systems use pre-authored tutor responses for addressing student errors. The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems. The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to Charles B. Callaway University of Haifa Moun</context>
</contexts>
<marker>Purandare, Litman, 2008</marker>
<rawString>Amruta Purandare and Diane Litman. 2008. Contentlearning correlations in spoken tutoring dialogs at word, turn and discourse levels. In Proceedings 21st International FLAIRS Conference, Coconut Grove, Florida, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C P Ros´e</author>
<author>C Torrey</author>
</authors>
<title>Interactivity versus expectation: Eliciting learning oriented behavior with tutorial dialogue systems.</title>
<date>2005</date>
<booktitle>In Proceedings of Interact’05.</booktitle>
<marker>Ros´e, Torrey, 2005</marker>
<rawString>C.P. Ros´e and C. Torrey. 2005. Interactivity versus expectation: Eliciting learning oriented behavior with tutorial dialogue systems. In Proceedings of Interact’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N B Steinhauser</author>
<author>L A Butler</author>
<author>G E Campbell</author>
</authors>
<title>Simulated tutors in immersive learning environments: Empirically-derived design principles.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007Interservice/Industry Training, Simulation and Education Conference,</booktitle>
<location>Orlando, FL.</location>
<contexts>
<context position="1494" citStr="Steinhauser et al., 2007" startWordPosition="211" endWordPosition="214">n also be used to experiment with a variety of natural language interpretation and generation techniques. 1 Introduction Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations (Jordan et al., 2006; Graesser et al., 1999; Aleven et al., 2001; Buckley and Wolska, 2007; Nielsen et al., 2008; VanLehn et al., 2007), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring (Chi et al., 1994; Litman et al., 2009; Purandare and Litman, 2008; Steinhauser et al., 2007). However, most existing systems use pre-authored tutor responses for addressing student errors. The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems. The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to Charles B. Callaway University of Haifa Mount Carmel, Haifa, Israel cca</context>
</contexts>
<marker>Steinhauser, Butler, Campbell, 2007</marker>
<rawString>N. B. Steinhauser, L. A. Butler, and G. E. Campbell. 2007. Simulated tutors in immersive learning environments: Empirically-derived design principles. In Proceedings of the 2007Interservice/Industry Training, Simulation and Education Conference, Orlando, FL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kurt VanLehn</author>
<author>Pamela Jordan</author>
<author>Diane Litman</author>
</authors>
<title>Developing pedagogically effective tutorial dialogue tactics: Experiments and a testbed.</title>
<date>2007</date>
<booktitle>In Proceedings of SLaTE Workshop on Speech and Language Technology in Education,</booktitle>
<location>Farmington, PA,</location>
<contexts>
<context position="1256" citStr="VanLehn et al., 2007" startWordPosition="173" endWordPosition="176"> and dialogue strategies. Our first system evaluation used two different tutorial policies and demonstrated that the system can be successfully used to study the impact of different approaches to tutoring. In the future, the system can also be used to experiment with a variety of natural language interpretation and generation techniques. 1 Introduction Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations (Jordan et al., 2006; Graesser et al., 1999; Aleven et al., 2001; Buckley and Wolska, 2007; Nielsen et al., 2008; VanLehn et al., 2007), because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring (Chi et al., 1994; Litman et al., 2009; Purandare and Litman, 2008; Steinhauser et al., 2007). However, most existing systems use pre-authored tutor responses for addressing student errors. The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems. The disadvantage is a lack of </context>
</contexts>
<marker>VanLehn, Jordan, Litman, 2007</marker>
<rawString>Kurt VanLehn, Pamela Jordan, and Diane Litman. 2007. Developing pedagogically effective tutorial dialogue tactics: Experiments and a testbed. In Proceedings of SLaTE Workshop on Speech and Language Technology in Education, Farmington, PA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arthur Ward</author>
<author>Diane Litman</author>
</authors>
<title>Cohesion and learning in a tutorial spoken dialog system.</title>
<date>2006</date>
<booktitle>In Proceedings of 19th International FLAIRS (Florida Artificial Intelligence Research Society) Conference,</booktitle>
<location>Melbourne Beach, FL.</location>
<contexts>
<context position="16157" citStr="Ward and Litman, 2006" startWordPosition="2553" endWordPosition="2557">over from misunderstandings. In dialogue management and generation, the key issue we are planning to investigate is that of linguistic alignment. The analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words (for example, using better terminology) but doesn’t explicitly explain the reason why different terminology is needed (Dzikovska et al., 2010). Results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain (Ward and Litman, 2006). Using a deep generator to automatically generate system feedback gives us a level of control over the output and will allow us to devise experiments to study those issues in more detail. From the point of view of tutoring research, we are planning to use the system to answer questions about the effectiveness of different approaches to tutoring, and the differences between human-human and human-computer tutoring. Previous comparisons of human-human and humancomputer dialogue were limited to systems that asked short-answer questions (Litman et al., 2006; Ros´e and Torrey, 2005). Having a syste</context>
</contexts>
<marker>Ward, Litman, 2006</marker>
<rawString>Arthur Ward and Diane Litman. 2006. Cohesion and learning in a tutorial spoken dialog system. In Proceedings of 19th International FLAIRS (Florida Artificial Intelligence Research Society) Conference, Melbourne Beach, FL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>