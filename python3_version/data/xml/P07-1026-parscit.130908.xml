<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.981129">
A Grammar-driven Convolution Tree Kernel for Se-
mantic Role Classification
</title>
<author confidence="0.8004225">
Min ZHANG1 Wanxiang CHE2 Ai Ti AW1 Chew Lim TAN3
Guodong ZHOU1,4 Ting LIU2 Sheng LI2
</author>
<affiliation confidence="0.866106">
1Institute for Infocomm Research
</affiliation>
<email confidence="0.924219">
{mzhang, aaiti}@i2r.a-star.edu.sg
</email>
<affiliation confidence="0.972099">
3National University of Singapore
</affiliation>
<email confidence="0.970068">
tancl@comp.nus.edu.sg
</email>
<affiliation confidence="0.667526">
2Harbin Institute of Technology
</affiliation>
<email confidence="0.9410185">
{car, tliu}@ir.hit.edu.cn
lisheng@hit.edu.cn
</email>
<address confidence="0.841328">
4 Soochow Univ., China 215006
</address>
<email confidence="0.99522">
gdzhou@suda.edu.cn
</email>
<sectionHeader confidence="0.996617" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999975884615385">
Convolution tree kernel has shown promis-
ing results in semantic role classification.
However, it only carries out hard matching,
which may lead to over-fitting and less ac-
curate similarity measure. To remove the
constraint, this paper proposes a grammar-
driven convolution tree kernel for semantic
role classification by introducing more lin-
guistic knowledge into the standard tree
kernel. The proposed grammar-driven tree
kernel displays two advantages over the pre-
vious one: 1) grammar-driven approximate
substructure matching and 2) grammar-
driven approximate tree node matching. The
two improvements enable the grammar-
driven tree kernel explore more linguistically
motivated structure features than the previ-
ous one. Experiments on the CoNLL-2005
SRL shared task show that the grammar-
driven tree kernel significantly outperforms
the previous non-grammar-driven one in
SRL. Moreover, we present a composite
kernel to integrate feature-based and tree
kernel-based methods. Experimental results
show that the composite kernel outperforms
the previously best-reported methods.
</bodyText>
<sectionHeader confidence="0.998558" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99996">
Given a sentence, the task of Semantic Role Label-
ing (SRL) consists of analyzing the logical forms
expressed by some target verbs or nouns and some
constituents of the sentence. In particular, for each
predicate (target verb or noun) all the constituents in
the sentence which fill semantic arguments (roles)
of the predicate have to be recognized. Typical se-
mantic roles include Agent, Patient, Instrument, etc.
and also adjuncts such as Locative, Temporal,
Manner, and Cause, etc. Generally, semantic role
identification and classification are regarded as two
key steps in semantic role labeling. Semantic role
identification involves classifying each syntactic
element in a sentence into either a semantic argu-
ment or a non-argument while semantic role classi-
fication involves classifying each semantic argument
identified into a specific semantic role. This paper
focuses on semantic role classification task with the
assumption that the semantic arguments have been
identified correctly.
Both feature-based and kernel-based learning
methods have been studied for semantic role classi-
fication (Carreras and Màrquez, 2004; Carreras and
Màrquez, 2005). In feature-based methods, a flat
feature vector is used to represent a predicate-
argument structure while, in kernel-based methods,
a kernel function is used to measure directly the
similarity between two predicate-argument struc-
tures. As we know, kernel methods are more effec-
tive in capturing structured features. Moschitti
(2004) and Che et al. (2006) used a convolution
tree kernel (Collins and Duffy, 2001) for semantic
role classification. The convolution tree kernel
takes sub-tree as its feature and counts the number
of common sub-trees as the similarity between two
predicate-arguments. This kernel has shown very
</bodyText>
<page confidence="0.946364">
200
</page>
<note confidence="0.925476">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 200–207,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.99900612195122">
promising results in SRL. However, as a general syntactic structured information. For example, in
learning algorithm, the tree kernel only carries out SRL, the Parse Tree Path feature is sensitive to
hard matching between any two sub-trees without small changes of the syntactic structures. Thus, a
considering any linguistic knowledge in kernel de- predicate argument pair will have two different
sign. This makes the kernel fail to handle similar Path features even if their paths differ only for one
phrase structures (e.g., “buy a car” vs. “buy a red node. This may result in data sparseness and model
car”) and near-synonymic grammar tags (e.g., the generalization problems.
POS variations between “high/JJ degree/NN” 1 and Kernel-based Methods for SRL: as an alternative,
“higher/JJR degree/NN”) 2. To some degree, it may kernel methods are more effective in modeling
lead to over-fitting and compromise performance. structured objects. This is because a kernel can
This paper reports our preliminary study in ad- measure the similarity between two structured ob-
dressing the above issue by introducing more lin- jects using the original representation of the objects
guistic knowledge into the convolution tree kernel. instead of explicitly enumerating their features.
To our knowledge, this is the first attempt in this Many kernels have been proposed and applied to
research direction. In detail, we propose a gram- the NLP study. In particular, Haussler (1999) pro-
mar-driven convolution tree kernel for semantic posed the well-known convolution kernels for a
role classification that can carry out more linguisti- discrete structure. In the context of it, more and
cally motivated substructure matching. Experimental more kernels for restricted syntaxes or specific do-
results show that the proposed method significantly mains (Collins and Duffy, 2001; Lodhi et al., 2002;
outperforms the standard convolution tree kernel on Zelenko et al., 2003; Zhang et al., 2006) are pro-
the data set of the CoNLL-2005 SRL shared task. posed and explored in the NLP domain.
The remainder of the paper is organized as fol- Of special interest here, Moschitti (2004) proposed
lows: Section 2 reviews the previous work and Sec- Predicate Argument Feature (PAF) kernel for SRL
tion 3 discusses our grammar-driven convolution under the framework of convolution tree kernel. He
tree kernel. Section 4 shows the experimental re- selected portions of syntactic parse trees as predicate-
sults. We conclude our work in Section 5. argument feature spaces, which include salient sub-
structures of predicate-arguments, to define convo-
lution kernels for the task of semantic role classifi-
cation. Under the same framework, Che et al. (2006)
proposed a hybrid convolution tree kernel, which
consists of two individual convolution kernels: a Path
kernel and a Constituent Structure kernel. Che et al.
(2006) showed that their method outperformed PAF
on the CoNLL-2005 SRL dataset.
The above two kernels are special instances of
convolution tree kernel for SRL. As discussed in
Section 1, convolution tree kernel only carries out
hard matching, so it fails to handle similar phrase
structures and near-synonymic grammar tags. This
paper presents a grammar-driven convolution tree
kernel to solve the two problems
</bodyText>
<sectionHeader confidence="0.744932" genericHeader="method">
3 Grammar-driven Convolution Tree
Kernel
</sectionHeader>
<subsectionHeader confidence="0.755431">
3.1 Convolution Tree Kernel
</subsectionHeader>
<bodyText confidence="0.997220315789474">
In convolution tree kernel (Collins and Duffy,
2001), a parse tree T is represented by a vector of
integer counts of each sub-tree type (regardless of
its ancestors): φ(T) = ( ..., # subtreei(T), ...), where
2 Previous Work
Feature-based Methods for SRL: most features
used in prior SRL research are generally extended
from Gildea and Jurafsky (2002), who used a linear
interpolation method and extracted basic flat fea-
tures from a parse tree to identify and classify the
constituents in the FrameNet (Baker et al., 1998).
Here, the basic features include Phrase Type, Parse
Tree Path, and Position. Most of the following work
focused on feature engineering (Xue and Palmer,
2004; Jiang et al., 2005) and machine learning
models (Nielsen and Pradhan, 2004; Pradhan et al.,
2005a). Some other work paid much attention to the
robust SRL (Pradhan et al., 2005b) and post infer-
ence (Punyakanok et al., 2004). These feature-
based methods are considered as the state of the art
methods for SRL. However, as we know, the stan-
dard flat features are less effective in modeling the
1 Please refer to http://www.cis.upenn.edu/~treebank/ for the
detailed definitions of the grammar tags used in the paper.
2 Some rewrite rules in English grammar are generalizations of
others: for example, “NP4 DET JJ NN” is a specialized ver-
sion of “NP4 DET NN”. The same applies to POS. The stan-
dard convolution tree kernel is unable to capture the two cases.
201
# subtreei(T) is the occurrence number of the ith
sub-tree type (subtreei) in T. Since the number of
different sub-trees is exponential with the parse tree
size, it is computationally infeasible to directly use
the feature vector φ(T) . To solve this computa-
tional issue, Collins and Duffy (2001) proposed the
following parse tree kernel to calculate the dot
product between the above high dimensional vec-
tors implicitly.
</bodyText>
<equation confidence="0.9988408125">
K T T
( , ) ( ), ( )
=&lt; φ φ
T T
1 2 1 2
∑ ∑
(( subtree i ) (
I ( )
n ⋅ ∑ I i ))
( )
n
i n N 1 n N subtree 2
1 1
∈ 2 2
∈
= ∑n1∈N1 ∑n2∈N2 ∆
</equation>
<bodyText confidence="0.540316833333333">
where N1 and N2 are the sets of nodes in trees T1 and
T2, respectively, and ( )
I subtreei n is a function that is
1 iff the subtreei occurs with root at node n and zero
otherwise, and ∆(n1, n2) is the number of the com-
mon subtrees rooted at n1 and n2, i.e.,
</bodyText>
<equation confidence="0.9910585">
∆ n n = ∑ I
( , ) ( 1)
n ⋅ I ( 2)
n
1 2 i subtreei subtree i
∆(n1, n2) can be further computed efficiently by the
following recursive rules:
Rule 1: if the productions (CFG rules) at n1 and
n2 are different, ∆(n1, n2) = 0;
Rule 2: else if both n1 and n2 are pre-terminals
(POS tags), ∆(n1,n2)=1×λ;
Rule 3: else,
∆ (n1 , n2) = λ∏ nc(n1) (1 + ∆(ch (n1 , j), ch(n2 , j))) ,
j=1
</equation>
<bodyText confidence="0.997954">
where nc(n1) is the child number of n1, ch(n,j) is
the jth child of node n and λ (0&lt;λ &lt;1) is the decay
factor in order to make the kernel value less vari-
able with respect to the subtree sizes. In addition,
the recursive Rule 3 holds because given two
nodes with the same children, one can construct
common sub-trees using these children and com-
mon sub-trees of further offspring. The time com-
plexity for computing this kernel is O( |N1  |⋅  |N2 |) .
</bodyText>
<subsectionHeader confidence="0.991084">
3.2 Grammar-driven Convolution Tree
Kernel
</subsectionHeader>
<bodyText confidence="0.996511136363637">
This Subsection introduces the two improvements
and defines our grammar-driven tree kernel.
Improvement 1: Grammar-driven approximate
matching between substructures. The conven-
tional tree kernel requires exact matching between
two contiguous phrase structures. This constraint
may be too strict. For example, the two phrase
structures “NP4DT JJ NN” (NP4a red car) and
“NP4DT NN” (NP-&gt;a car) are not identical, thus
they contribute nothing to the conventional kernel
although they should share the same semantic role
given a predicate. In this paper, we propose a
grammar-driven approximate matching mechanism
to capture the similarity between such kinds of
quasi-structures for SRL.
First, we construct reduced rule set by defining
optional nodes, for example, “NP-&gt;DT [JJ] NP” or
“VP-&gt; VB [ADVP] PP”, where [*] denotes op-
tional nodes. For convenience, we call “NP-&gt; DT
JJ NP” the original rule and “NP-&gt;DT [JJ] NP” the
reduced rule. Here, we define two grammar-driven
criteria to select optional nodes:
</bodyText>
<listItem confidence="0.888761333333334">
1) The reduced rules must be grammatical. It
means that the reduced rule should be a valid rule
in the original rule set. For example, “NP-&gt;DT [JJ]
NP” is valid only when “NP-&gt;DT NP” is a valid
rule in the original rule set while “NP-&gt;DT [JJ
NP]” may not be valid since “NP-&gt;DT” is not a
valid rule in the original rule set.
2) A valid reduced rule must keep the head
child of its corresponding original rule and has at
least two children. This can make the reduced rules
retain the underlying semantic meaning of their
corresponding original rules.
</listItem>
<bodyText confidence="0.997501333333333">
Given the reduced rule set, we can then formu-
late the approximate substructure matching mecha-
nism as follows:
</bodyText>
<equation confidence="0.983128375">
a i b j
+
M r r =∑ ( ( , ) )
i j
( , ) I T T λ
× (1)
1 2 i j T r r
, 1 2 1
</equation>
<bodyText confidence="0.920169833333333">
where r1 is a production rule, representing a sub-tree
of depth one3, and 1
Tr is the ith variation of the sub-
i
tree r1 by removing one ore more optional nodes4,
and likewise for r2 and 2
</bodyText>
<equation confidence="0.7674355">
Tr . IT (•, •) is a function
j
</equation>
<bodyText confidence="0.8371907">
that is 1 iff the two sub-trees are identical and zero
otherwise. λ1 (0≤ λ1≤1) is a small penalty to penal-
3 Eq.(1) is defined over sub-structure of depth one. The ap-
proximate matching between structures of depth more than one
can be achieved easily through the matching of sub-structures
of depth one in the recursively-defined convolution kernel. We
will discuss this issue when defining our kernel.
4 To make sure that the new kernel is a proper kernel, we have
to consider all the possible variations of the original sub-trees.
Training program converges only when using a proper kernel.
</bodyText>
<equation confidence="0.8844265">
&gt;
(n1 , n2)
</equation>
<page confidence="0.98791">
202
</page>
<bodyText confidence="0.999309">
ize optional nodes and the two parameters ai and
bj stand for the numbers of occurrence of removed
optional nodes in subtrees 1
</bodyText>
<equation confidence="0.526599333333333">
Tr and 2
i Tr , respectively.
j
</equation>
<bodyText confidence="0.882552346153846">
M(r1,r2) returns the similarity (ie., the kernel
value) between the two sub-trees r1 and r2 by sum-
ming up the similarities between all possible varia-
tions of the sub-trees r1 and r2.
Under the new approximate matching mecha-
nism, two structures are matchable (but with a small
penalty λ1 ) if the two structures are identical after
removing one or more optional nodes. In this case,
the above example phrase structures “NP-&gt;a red
car” and “NP-&gt;a car” are matchable with a pen-
alty λ1 in our new kernel. It means that one co-
occurrence of the two structures contributes λ1 to
our proposed kernel while it contributes zero to the
traditional one. Therefore, by this improvement, our
method would be able to explore more linguistically
appropriate features than the previous one (which is
formulated as IT (r1 , r2 ) ).
Improvement 2: Grammar-driven tree nodes ap-
proximate matching. The conventional tree kernel
needs an exact matching between two (termi-
nal/non-terminal) nodes. But, some similar POSs
may represent similar roles, such as NN (dog) and
NNS (dogs). In order to capture this phenomenon,
we allow approximate matching between node fea-
tures. The following illustrates some equivalent
node feature sets:
</bodyText>
<listItem confidence="0.999898">
• JJ, JJR, JJS
• VB, VBD, VBG, VBN, VBP, VBZ
•
</listItem>
<bodyText confidence="0.9756735">
where POSs in the same line can match each other
with a small penalty 0≤ λ2 ≤1. We call this case
node feature mutation. This improvement further
generalizes the conventional tree kernel to get bet-
ter coverage. The approximate node matching can
be formulated as:
</bodyText>
<equation confidence="0.856736307692308">
a i b j
+
i j
M f f
( , ) = ∑ ( ( , ) )
I f f λ
×
1 2 i j f
, 1 2 2
where f1 is a node feature, f 1 is the ith mutation
i
of f1 and ai is 0 iff f 1 and f1 are identical and 1 oth-
i
</equation>
<bodyText confidence="0.998551133333333">
erwise, and likewise for f2. If (•, •) is a function
that is 1 iff the two features are identical and zero
otherwise. Eq. (2) sums over all combinations of
feature mutations as the node feature similarity.
The same as Eq. (1), the reason for taking all the
possibilities into account in Eq. (2) is to make sure
that the new kernel is a proper kernel.
The above two improvements are grammar-
driven, i.e., the two improvements retain the under-
lying linguistic grammar constraints and keep se-
mantic meanings of original rules.
The Grammar-driven Kernel Definition: Given
the two improvements discussed above, we can de-
fine the new kernel by beginning with the feature
vector representation of a parse tree T as follows:
</bodyText>
<equation confidence="0.971408">
φ′(T) = (# subtree1(T), ..., # subtreen(T))
</equation>
<bodyText confidence="0.999863428571429">
where # subtreei(T) is the occurrence number of the
ith sub-tree type (subtreei) in T. Please note that,
different from the previous tree kernel, here we
loosen the condition for the occurrence of a subtree
by allowing both original and reduced rules (Im-
provement 1) and node feature mutations (Im-
provement 2). In other words, we modify the crite-
ria by which a subtree is said to occur. For example,
one occurrence of the rule “NP-&gt;DT JJ NP” shall
contribute 1 times to the feature “NP-&gt;DT JJ NP”
and λ1 times to the feature “NP-&gt;DT NP” in the
new kernel while it only contributes 1 times to the
feature “NP-&gt;DT JJ NP” in the previous one. Now
we can define the new grammar-driven kernel
</bodyText>
<equation confidence="0.996359">
KG (T1 , T2) as follows:
K T T
( , ) ( ), ( )
=&lt; ′ ′ &gt;
φ φ
T T
G 1 2 1 2
=∑ ∑
(( ′ i ) (
( ) ⋅ ∑ I n
′ i ))
( ) (3)
i n N subtree
I n 1 n N subtree 2
1 1
∈ 2 2
∈
=∑ ∑ ∆ ′ ( , )
n n
n N n N 1 2
1 1
∈ 2 2
∈
</equation>
<bodyText confidence="0.992893333333333">
where N1 and N2 are the sets of nodes in trees T1 and
T2, respectively. I′subtreei n is a function that is
( )λ •λ iff the subtreei occurs with root at node n
</bodyText>
<figure confidence="0.46275">
a b
1 2
</figure>
<bodyText confidence="0.9970045">
and zero otherwise, where a and b are the numbers
of removed optional nodes and mutated node fea-
tures, respectively. ∆′(n1,n2) is the number of the
common subtrees rooted at n1 and n2, i.e. ,
</bodyText>
<equation confidence="0.983421333333333">
∆′ n n = ∑ I ′ n ⋅ I ′ n (4)
( , ) ( 1 ) ( 2 )
1 2 i subtree i subtreei
</equation>
<bodyText confidence="0.999373">
Please note that the value of ∆′(n1, n2) is no longer
an integer as that in the conventional one since op-
tional nodes and node feature mutations are consid-
ered in the new kernel. ∆′(n1 , n2) can be further
computed by the following recursive rules:
</bodyText>
<figure confidence="0.300004">
(2)
</figure>
<page confidence="0.870284">
203
</page>
<equation confidence="0.9428995">
Rule A: if n1 and n2 are pre-terminals, then:
0′(n1,n2) = AxM(f1, f2) (5)
</equation>
<bodyText confidence="0.99247325">
where f1 and f2 are features of nodes n1 and n2 re-
spectively, and M(f 1 , f2) is defined at Eq. (2).
Rule B: else if both n1 and n2 are the same non-
terminals, then generate all variations of the subtrees
of depth one rooted by n1 and n2 (denoted by Tn1
and Tn 2 respectively) by removing different optional
nodes, then:
where
</bodyText>
<listItem confidence="0.9894550625">
• Tni1 and Tnj2 stand for the ith and jth variations in
sub-tree set Tn 1 and Tn2, respectively.
• IT (•, •) is a function that is 1 iff the two sub-
trees are identical and zero otherwise.
• ai and bj stand for the number of removed op-
tional nodes in subtrees 1
Tn and 2
i Tn , respectively.
j
• nc(n1, i) returns the child number of n1 in its ith
subtree variation Tni1.
• ch (n1, i, k) is the kth child of node n1 in its ith
variation subtree Tni1, and likewise for ch(n2, j, k) .
• Finally, the same as the previous tree kernel,
a (0&lt;A&lt;1) is the decay factor (see the discussion
in Subsection 3.1).
</listItem>
<equation confidence="0.246974">
Rule C: else 0′(n1, n2) = 0
</equation>
<bodyText confidence="0.98053692">
Rule A accounts for Improvement 2 while Rule
B accounts for Improvement 1. In Rule B, Eq. (6)
is able to carry out multi-layer sub-tree approxi-
mate matching due to the introduction of the recur-
sive part while Eq. (1) is only effective for sub-
trees of depth one. Moreover, we note that Eq. (4)
is a convolution kernel according to the definition
and the proof given in Haussler (1999), and Eqs (5)
and (6) reformulate Eq. (4) so that it can be com-
puted efficiently, in this way, our kernel defined by
Eq (3) is also a valid convolution kernel. Finally,
let us study the computational issue of the new
convolution tree kernel. Clearly, computing Eq. (6)
requires exponential time in its worst case. How-
ever, in practice, it may only need O( |N1  |•  |N2 |) .
This is because there are only 9.9% rules (647 out
of the total 6,534 rules in the parse trees) have op-
tional nodes and most of them have only one op-
tional node. In fact, the actual running time is even
much less and is close to linear in the size of the
trees since 0′(n1, n2) = 0 holds for many node
pairs (Collins and Duffy, 2001). In theory, we can
also design an efficient algorithm to compute Eq.
(6) using a dynamic programming algorithm (Mo-
schitti, 2006). We just leave it for our future work.
</bodyText>
<subsectionHeader confidence="0.999794">
3.3 Comparison with previous work
</subsectionHeader>
<bodyText confidence="0.999935277777778">
In above discussion, we show that the conventional
convolution tree kernel is a special case of the
grammar-driven tree kernel. From kernel function
viewpoint, our kernel can carry out not only exact
matching (as previous one described by Rules 2
and 3 in Subsection 3.1) but also approximate
matching (Eqs. (5) and (6) in Subsection 3.2). From
feature exploration viewpoint, although they ex-
plore the same sub-structure feature space (defined
recursively by the phrase parse rules), their feature
values are different since our kernel captures the
structure features in a more linguistically appropri-
ate way by considering more linguistic knowledge
in our kernel design.
Moschitti (2006) proposes a partial tree (PT)
kernel which can carry out partial matching be-
tween sub-trees. The PT kernel generates a much
larger feature space than both the conventional and
the grammar-driven kernels. In this point, one can
say that the grammar-driven tree kernel is a spe-
cialization of the PT kernel. However, the impor-
tant difference between them is that the PT kernel
is not grammar-driven, thus many non-
linguistically motivated structures are matched in
the PT kernel. This may potentially compromise
the performance since some of the over-generated
features may possibly be noisy due to the lack of
linguistic interpretation and constraint.
Kashima and Koyanagi (2003) proposed a con-
volution kernel over labeled order trees by general-
izing the standard convolution tree kernel. The la-
beled order tree kernel is much more flexible than
the PT kernel and can explore much larger sub-tree
features than the PT kernel. However, the same as
the PT kernel, the labeled order tree kernel is not
grammar-driven. Thus, it may face the same issues
</bodyText>
<equation confidence="0.996598571428571">
0′(n n) = x ∑(I (Ti Tj )
x rl
i
1 2
T n1 n2
nc ( , )
n i
1
k
=1
(1 + 0′(ch(n1, i, k), ch(n2, j, k)))
1 ai+bj
x �
(6)
</equation>
<page confidence="0.99661">
204
</page>
<bodyText confidence="0.9998946">
(such as over-generated features) as the PT kernel
when used in NLP applications.
Shen el al. (2003) proposed a lexicalized tree
kernel to utilize LTAG-based features in parse
reranking. Their methods need to obtain a LTAG
derivation tree for each parse tree before kernel
calculation. In contrast, we use the notion of op-
tional arguments to define our grammar-driven tree
kernel and use the empirical set of CFG rules to de-
termine which arguments are optional.
</bodyText>
<sectionHeader confidence="0.999443" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.98865">
4.1 Experimental Setting
</subsectionHeader>
<bodyText confidence="0.999885846153846">
Data: We use the CoNLL-2005 SRL shared task
data (Carreras and Màrquez, 2005) as our experi-
mental corpus. The data consists of sections of the
Wall Street Journal part of the Penn TreeBank
(Marcus et al., 1993), with information on predi-
cate-argument structures extracted from the Prop-
Bank corpus (Palmer et al., 2005). As defined by
the shared task, we use sections 02-21 for training,
section 24 for development and section 23 for test.
There are 35 roles in the data including 7 Core
(A0–A5, AA), 14 Adjunct (AM-) and 14 Reference
(R-) arguments. Table 1 lists counts of sentences
and arguments in the three data sets.
</bodyText>
<table confidence="0.998975">
Training Development Test
Sentences 39,832 1,346 2,416
Arguments 239,858 8,346 14,077
</table>
<tableCaption confidence="0.999932">
Table 1: Counts on the data set
</tableCaption>
<bodyText confidence="0.999771216216216">
We assume that the semantic role identification
has been done correctly. In this way, we can focus
on the classification task and evaluate it more accu-
rately. We evaluate the performance with Accu-
racy. SVM (Vapnik, 1998) is selected as our classi-
fier and the one vs. others strategy is adopted and
the one with the largest margin is selected as the
final answer. In our implementation, we use the bi-
nary SVMLight (Joachims, 1998) and modify the
Tree Kernel Tools (Moschitti, 2004) to a grammar-
driven one.
Kernel Setup: We use the Constituent, Predicate,
and Predicate-Constituent related features, which
are reported to get the best-reported performance
(Pradhan et al., 2005a), as the baseline features. We
use Che et al. (2006)’s hybrid convolution tree ker-
nel (the best-reported method for kernel-based
SRL) as our baseline kernel. It is defined as
Khybrid = BKpath + (1− B)Kcs (0≤ B≤ 1) (for the de-
tailed definitions of Kpath and Kcs , please refer to
Che et al. (2006)). Here, we use our grammar-
driven tree kernel to compute Kpath and Kcs , and we
call it grammar-driven hybrid tree kernel while Che
et al. (2006)’s is non-grammar-driven hybrid convo-
lution tree kernel.
We use a greedy strategy to fine-tune parameters.
Evaluation on the development set shows that our
kernel yields the best performance when A (decay
factor of tree kernel), A1 and A2 (two penalty factors
for the grammar-driven kernel), B (hybrid kernel
parameter) and c (a SVM training parameter to
balance training error and margin) are set to 0.4,
0.6, 0.3, 0.6 and 2.4, respectively. For other parame-
ters, we use default setting. In the CoNLL 2005
benchmark data, we get 647 rules with optional
nodes out of the total 6,534 grammar rules and de-
fine three equivalent node feature sets as below:
</bodyText>
<listItem confidence="0.999909333333333">
• JJ, JJR, JJS
• RB, RBR, RBS
• NN, NNS, NNP, NNPS, NAC, NX
</listItem>
<bodyText confidence="0.99990775">
Here, the verb feature set “VB, VBD, VBG, VBN,
VBP, VBZ” is removed since the voice information
is very indicative to the arguments of ARG0
(Agent, operator) and ARG1 (Thing operated).
</bodyText>
<sectionHeader confidence="0.659885" genericHeader="method">
Methods Accuracy (%)
</sectionHeader>
<table confidence="0.73589575">
Baseline: Non-grammar-driven 85.21
+Approximate Node Matching 86.27
+Approximate Substructure 87.12
Matching
Ours: Grammar-driven Substruc- 87.96
ture and Node Matching
Feature-based method with poly- 89.92
nomial kernel (d = 2)
</table>
<tableCaption confidence="0.989787">
Table 2: Performance comparison
</tableCaption>
<subsectionHeader confidence="0.974646">
4.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.9954238">
Table 2 compares the performances of different
methods on the test set. First, we can see that the
new grammar-driven hybrid convolution tree kernel
significantly outperforms ( 2
χ test with p=0.05) the
</bodyText>
<page confidence="0.995384">
205
</page>
<bodyText confidence="0.999904866666667">
non-grammar one with an absolute improvement of
2.75 (87.96-85.21) percentage, representing a rela-
tive error rate reduction of 18.6% (2.75/(100-85.21))
. It suggests that 1) the linguistically motivated
structure features are very useful for semantic role
classification and 2) the grammar-driven kernel is
much more effective in capturing such kinds of fea-
tures due to the consideration of linguistic knowl-
edge. Moreover, Table 2 shows that 1) both the
grammar-driven approximate node matching and the
grammar-driven approximate substructure matching
are very useful in modeling syntactic tree structures
for SRL since they contribute relative error rate re-
duction of 7.2% ((86.27-85.21)/(100-85.21)) and
12.9% ((87.12-85.21)/(100-85.21)), respectively; 2)
the grammar-driven approximate substructure
matching is more effective than the grammar-driven
approximate node matching. However, we find that
the performance of the grammar-driven kernel is
still a bit lower than the feature-based method. This
is not surprising since tree kernel methods only fo-
cus on modeling tree structure information. In this
paper, it captures the syntactic parse tree structure
features only while the features used in the feature-
based methods cover more knowledge sources.
In order to make full use of the syntactic structure
information and the other useful diverse flat fea-
tures, we present a composite kernel to combine the
grammar-driven hybrid kernel and feature-based
method with polynomial kernel:
</bodyText>
<equation confidence="0.759539">
Kcomp = YKhybrid + (1 − Y)Kpoly (0 &lt; Y&lt; 1)
</equation>
<bodyText confidence="0.999732357142857">
Evaluation on the development set shows that the
composite kernel yields the best performance when
y is set to 0.3. Using the same setting, the system
achieves the performance of 91.02% in Accuracy
in the same test set. It shows statistically significant
improvement (χ2 test with p= 0.10) over using the
standard features with the polynomial kernel (y = 0,
Accuracy = 89.92%) and using the grammar-driven
hybrid convolution tree kernel (y = 1, Accuracy =
87.96%). The main reason is that the tree kernel
can capture effectively more structure features
while the standard flat features can cover some
other useful features, such as Voice, SubCat, which
are hard to be covered by the tree kernel. The ex-
perimental results suggest that these two kinds of
methods are complementary to each other.
In order to further compare with other methods,
we also do experiments on the dataset of English
PropBank I (LDC2004T14). The training, develop-
ment and test sets follow the conventional split of
Sections 02-21, 00 and 23. Table 3 compares our
method with other previously best-reported methods
with the same setting as discussed previously. It
shows that our method outperforms the previous
best-reported one with a relative error rate reduction
of 10.8% (0.97/(100-91)). This further verifies the
effectiveness of the grammar-driven kernel method
for semantic role classification.
</bodyText>
<table confidence="0.4897622">
Method Accuracy (%)
Ours (Composite Kernel) 91.97
Moschitti (2006): PAF kernel only 87.7
Jiang et al. (2005): feature based 90.50
Pradhan et al. (2005a): feature based 91.0
</table>
<tableCaption confidence="0.9954385">
Table 3: Performance comparison between our
method and previous work
</tableCaption>
<table confidence="0.899469428571429">
Method Training Time
4 Sections 19 Sections
Ours: grammar- —8.1 hours —7.9 days
driven tree kernel
Moschitti (2006): —7.9 hours —7.1 days
non-grammar-driven
tree kernel
</table>
<tableCaption confidence="0.998054">
Table 4: Training time comparison
</tableCaption>
<bodyText confidence="0.988089125">
Table 4 reports the training times of the two ker-
nels. We can see that 1) the two kinds of convolu-
tion tree kernels have similar computing time. Al-
though computing the grammar-driven one requires
exponential time in its worst case, however, in
practice, it may only need O( |N1  |•  |N2 |) or lin-
ear and 2) it is very time-consuming to train a SVM
classifier in a large dataset.
</bodyText>
<sectionHeader confidence="0.994379" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.9999589">
In this paper, we propose a novel grammar-driven
convolution tree kernel for semantic role classifica-
tion. More linguistic knowledge is considered in
the new kernel design. The experimental results
verify that the grammar-driven kernel is more ef-
fective in capturing syntactic structure features than
the previous convolution tree kernel because it al-
lows grammar-driven approximate matching of
substructures and node features. We also discuss
the criteria to determine the optional nodes in a
</bodyText>
<page confidence="0.994395">
206
</page>
<bodyText confidence="0.9986356">
CFG rule in defining our grammar-driven convolu-
tion tree kernel.
The extension of our work is to improve the per-
formance of the entire semantic role labeling system
using the grammar-driven tree kernel, including all
four stages: pruning, semantic role identification,
classification and post inference. In addition, a
more interesting research topic is to study how to
integrate linguistic knowledge and tree kernel
methods to do feature selection for tree kernel-
based NLP applications (Suzuki et al., 2004). In
detail, a linguistics and statistics-based theory that
can suggest the effectiveness of different substruc-
ture features and whether they should be generated
or not by the tree kernels would be worked out.
</bodyText>
<sectionHeader confidence="0.999213" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999683253333333">
C. F. Baker, C. J. Fillmore, and J. B. Lowe. 1998. The
Berkeley FrameNet Project. COLING-ACL-1998
Xavier Carreras and Lluõs Màrquez. 2004. Introduction to
the CoNLL-2004 shared task: Semantic role labeling.
CoNLL-2004
Xavier Carreras and Lluõs Màrquez. 2005. Introduction to
the CoNLL-2005 shared task: Semantic role labeling.
CoNLL-2005
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings ofNAACL-2000
Wanxiang Che, Min Zhang, Ting Liu and Sheng Li.
2006. A hybrid convolution tree kernel for semantic
role labeling. COLING-ACL-2006(poster)
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. NIPS-2001
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245–288
David Haussler. 1999. Convolution kernels on discrete
structures. Technical Report UCSC-CRL-99-10
Zheng Ping Jiang, Jia Li and Hwee Tou Ng. 2005. Se-
mantic argument classification exploiting argument
interdependence. IJCAI-2005
T. Joachims. 1998. Text Categorization with Support
Vecor Machine: learning with many relevant fea-
tures. ECML-1998
Kashima H. and Koyanagi T. 2003. Kernels for Semi-
Structured Data. ICML-2003
Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello
Cristianini and Chris Watkins. 2002. Text classifica-
tion using string kernels. Journal of Machine Learn-
ing Research, 2:419–444
Mitchell P. Marcus, Mary Ann Marcinkiewicz and Bea-
trice Santorini. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313–330
Alessandro Moschitti. 2004. A study on convolution ker-
nels for shallow statistic parsing. ACL-2004
Alessandro Moschitti. 2006. Syntactic kernels for natu-
ral language learning: the semantic role labeling
case. HLT-NAACL-2006 (short paper)
Rodney D. Nielsen and Sameer Pradhan. 2004. Mixing
weak learners in semantic parsing. EMNLP-2004
Martha Palmer, Dan Gildea and Paul Kingsbury. 2005.
The proposition bank: An annotated corpus of seman-
tic roles. Computational Linguistics, 31(1)
Sameer Pradhan, Kadri Hacioglu, Valeri Krugler, Wayne
Ward, James H. Martin and Daniel Jurafsky. 2005a.
Support vector learning for semantic argument classi-
fication. Journal of Machine Learning
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James
Martin and Daniel Jurafsky. 2005b. Semantic role la-
beling using different syntactic views. ACL-2005
Vasin Punyakanok, Dan Roth, Wen-tau Yih and Dav Zi-
mak. 2004. Semantic role labeling via integer linear
programming inference. COLING-2004
Vasin Punyakanok, Dan Roth and Wen Tau Yih. 2005.
The necessity of syntactic parsing for semantic role
labeling. IJCAI-2005
Libin Shen, Anoop Sarkar and A. K. Joshi. 2003. Using
LTAG based features in parse reranking. EMNLP-03
Jun Suzuki, Hideki Isozaki and Eisaku Maede. 2004.
Convolution kernels with feature selection for Natu-
ral Language processing tasks. ACL-2004
Vladimir N. Vapnik. 1998. Statistical Learning Theory.
Wiley
Nianwen Xue and Martha Palmer. 2004. Calibrating
features for semantic role labeling. EMNLP-2004
Dmitry Zelenko, Chinatsu Aone, and Anthony Rich-
ardella. 2003. Kernel methods for relation extraction.
Machine Learning Research, 3:1083–1106
Min Zhang, Jie Zhang, Jian Su and Guodong Zhou.
2006. A Composite Kernel to Extract Relations be-
tween Entities with both Flat and Structured Fea-
tures. COLING-ACL-2006
</reference>
<page confidence="0.997968">
207
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.061868">
<title confidence="0.999421">A Grammar-driven Convolution Tree Kernel for Semantic Role Classification</title>
<author confidence="0.772619">Wanxiang Ai Ti Chew Lim Ting Sheng</author>
<affiliation confidence="0.983293">for Infocomm Research</affiliation>
<address confidence="0.794611">{mzhang, aaiti}@i2r.a-star.edu.sg</address>
<affiliation confidence="0.991963">University of Singapore</affiliation>
<email confidence="0.966613">tancl@comp.nus.edu.sg</email>
<affiliation confidence="0.985818">Institute of Technology</affiliation>
<address confidence="0.302849">{car, tliu}@ir.hit.edu.cn</address>
<email confidence="0.430406">lisheng@hit.edu.cn</email>
<address confidence="0.793018">Univ., China 215006</address>
<email confidence="0.88945">gdzhou@suda.edu.cn</email>
<abstract confidence="0.999387962962963">Convolution tree kernel has shown promising results in semantic role classification. However, it only carries out hard matching, which may lead to over-fitting and less accurate similarity measure. To remove the constraint, this paper proposes a grammardriven convolution tree kernel for semantic role classification by introducing more linguistic knowledge into the standard tree kernel. The proposed grammar-driven tree kernel displays two advantages over the previous one: 1) grammar-driven approximate substructure matching and 2) grammardriven approximate tree node matching. The two improvements enable the grammardriven tree kernel explore more linguistically motivated structure features than the previous one. Experiments on the CoNLL-2005 SRL shared task show that the grammardriven tree kernel significantly outperforms the previous non-grammar-driven one in SRL. Moreover, we present a composite kernel to integrate feature-based and tree kernel-based methods. Experimental results show that the composite kernel outperforms the previously best-reported methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C F Baker</author>
<author>C J Fillmore</author>
<author>J B Lowe</author>
</authors>
<title>The Berkeley FrameNet Project.</title>
<date>1998</date>
<tech>COLING-ACL-1998</tech>
<contexts>
<context position="7353" citStr="Baker et al., 1998" startWordPosition="1093" endWordPosition="1096">el to solve the two problems 3 Grammar-driven Convolution Tree Kernel 3.1 Convolution Tree Kernel In convolution tree kernel (Collins and Duffy, 2001), a parse tree T is represented by a vector of integer counts of each sub-tree type (regardless of its ancestors): φ(T) = ( ..., # subtreei(T), ...), where 2 Previous Work Feature-based Methods for SRL: most features used in prior SRL research are generally extended from Gildea and Jurafsky (2002), who used a linear interpolation method and extracted basic flat features from a parse tree to identify and classify the constituents in the FrameNet (Baker et al., 1998). Here, the basic features include Phrase Type, Parse Tree Path, and Position. Most of the following work focused on feature engineering (Xue and Palmer, 2004; Jiang et al., 2005) and machine learning models (Nielsen and Pradhan, 2004; Pradhan et al., 2005a). Some other work paid much attention to the robust SRL (Pradhan et al., 2005b) and post inference (Punyakanok et al., 2004). These featurebased methods are considered as the state of the art methods for SRL. However, as we know, the standard flat features are less effective in modeling the 1 Please refer to http://www.cis.upenn.edu/~treeba</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>C. F. Baker, C. J. Fillmore, and J. B. Lowe. 1998. The Berkeley FrameNet Project. COLING-ACL-1998</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Lluõs Màrquez</author>
</authors>
<title>Introduction to the CoNLL-2004 shared task: Semantic role labeling.</title>
<date>2004</date>
<booktitle>CoNLL-2004</booktitle>
<contexts>
<context position="2631" citStr="Carreras and Màrquez, 2004" startWordPosition="366" endWordPosition="369">ntic role identification and classification are regarded as two key steps in semantic role labeling. Semantic role identification involves classifying each syntactic element in a sentence into either a semantic argument or a non-argument while semantic role classification involves classifying each semantic argument identified into a specific semantic role. This paper focuses on semantic role classification task with the assumption that the semantic arguments have been identified correctly. Both feature-based and kernel-based learning methods have been studied for semantic role classification (Carreras and Màrquez, 2004; Carreras and Màrquez, 2005). In feature-based methods, a flat feature vector is used to represent a predicateargument structure while, in kernel-based methods, a kernel function is used to measure directly the similarity between two predicate-argument structures. As we know, kernel methods are more effective in capturing structured features. Moschitti (2004) and Che et al. (2006) used a convolution tree kernel (Collins and Duffy, 2001) for semantic role classification. The convolution tree kernel takes sub-tree as its feature and counts the number of common sub-trees as the similarity betwee</context>
</contexts>
<marker>Carreras, Màrquez, 2004</marker>
<rawString>Xavier Carreras and Lluõs Màrquez. 2004. Introduction to the CoNLL-2004 shared task: Semantic role labeling. CoNLL-2004</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Lluõs Màrquez</author>
</authors>
<title>Introduction to the CoNLL-2005 shared task: Semantic role labeling.</title>
<date>2005</date>
<journal>CoNLL-2005</journal>
<contexts>
<context position="2660" citStr="Carreras and Màrquez, 2005" startWordPosition="370" endWordPosition="373"> classification are regarded as two key steps in semantic role labeling. Semantic role identification involves classifying each syntactic element in a sentence into either a semantic argument or a non-argument while semantic role classification involves classifying each semantic argument identified into a specific semantic role. This paper focuses on semantic role classification task with the assumption that the semantic arguments have been identified correctly. Both feature-based and kernel-based learning methods have been studied for semantic role classification (Carreras and Màrquez, 2004; Carreras and Màrquez, 2005). In feature-based methods, a flat feature vector is used to represent a predicateargument structure while, in kernel-based methods, a kernel function is used to measure directly the similarity between two predicate-argument structures. As we know, kernel methods are more effective in capturing structured features. Moschitti (2004) and Che et al. (2006) used a convolution tree kernel (Collins and Duffy, 2001) for semantic role classification. The convolution tree kernel takes sub-tree as its feature and counts the number of common sub-trees as the similarity between two predicate-arguments. Th</context>
<context position="21472" citStr="Carreras and Màrquez, 2005" startWordPosition="3763" endWordPosition="3766">ch(n1, i, k), ch(n2, j, k))) 1 ai+bj x � (6) 204 (such as over-generated features) as the PT kernel when used in NLP applications. Shen el al. (2003) proposed a lexicalized tree kernel to utilize LTAG-based features in parse reranking. Their methods need to obtain a LTAG derivation tree for each parse tree before kernel calculation. In contrast, we use the notion of optional arguments to define our grammar-driven tree kernel and use the empirical set of CFG rules to determine which arguments are optional. 4 Experiments 4.1 Experimental Setting Data: We use the CoNLL-2005 SRL shared task data (Carreras and Màrquez, 2005) as our experimental corpus. The data consists of sections of the Wall Street Journal part of the Penn TreeBank (Marcus et al., 1993), with information on predicate-argument structures extracted from the PropBank corpus (Palmer et al., 2005). As defined by the shared task, we use sections 02-21 for training, section 24 for development and section 23 for test. There are 35 roles in the data including 7 Core (A0–A5, AA), 14 Adjunct (AM-) and 14 Reference (R-) arguments. Table 1 lists counts of sentences and arguments in the three data sets. Training Development Test Sentences 39,832 1,346 2,416 </context>
</contexts>
<marker>Carreras, Màrquez, 2005</marker>
<rawString>Xavier Carreras and Lluõs Màrquez. 2005. Introduction to the CoNLL-2005 shared task: Semantic role labeling. CoNLL-2005</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings ofNAACL-2000</booktitle>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings ofNAACL-2000</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wanxiang Che</author>
<author>Min Zhang</author>
<author>Ting Liu</author>
<author>Sheng Li</author>
</authors>
<title>A hybrid convolution tree kernel for semantic role labeling. COLING-ACL-2006(poster)</title>
<date>2006</date>
<contexts>
<context position="3015" citStr="Che et al. (2006)" startWordPosition="424" endWordPosition="427">e classification task with the assumption that the semantic arguments have been identified correctly. Both feature-based and kernel-based learning methods have been studied for semantic role classification (Carreras and Màrquez, 2004; Carreras and Màrquez, 2005). In feature-based methods, a flat feature vector is used to represent a predicateargument structure while, in kernel-based methods, a kernel function is used to measure directly the similarity between two predicate-argument structures. As we know, kernel methods are more effective in capturing structured features. Moschitti (2004) and Che et al. (2006) used a convolution tree kernel (Collins and Duffy, 2001) for semantic role classification. The convolution tree kernel takes sub-tree as its feature and counts the number of common sub-trees as the similarity between two predicate-arguments. This kernel has shown very 200 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 200–207, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics promising results in SRL. However, as a general syntactic structured information. For example, in learning algorithm, the tree kernel only </context>
<context position="6193" citStr="Che et al. (2006)" startWordPosition="910" endWordPosition="913">ganized as fol- Of special interest here, Moschitti (2004) proposed lows: Section 2 reviews the previous work and Sec- Predicate Argument Feature (PAF) kernel for SRL tion 3 discusses our grammar-driven convolution under the framework of convolution tree kernel. He tree kernel. Section 4 shows the experimental re- selected portions of syntactic parse trees as predicatesults. We conclude our work in Section 5. argument feature spaces, which include salient substructures of predicate-arguments, to define convolution kernels for the task of semantic role classification. Under the same framework, Che et al. (2006) proposed a hybrid convolution tree kernel, which consists of two individual convolution kernels: a Path kernel and a Constituent Structure kernel. Che et al. (2006) showed that their method outperformed PAF on the CoNLL-2005 SRL dataset. The above two kernels are special instances of convolution tree kernel for SRL. As discussed in Section 1, convolution tree kernel only carries out hard matching, so it fails to handle similar phrase structures and near-synonymic grammar tags. This paper presents a grammar-driven convolution tree kernel to solve the two problems 3 Grammar-driven Convolution T</context>
<context position="22864" citStr="Che et al. (2006)" startWordPosition="3996" endWordPosition="3999">fication task and evaluate it more accurately. We evaluate the performance with Accuracy. SVM (Vapnik, 1998) is selected as our classifier and the one vs. others strategy is adopted and the one with the largest margin is selected as the final answer. In our implementation, we use the binary SVMLight (Joachims, 1998) and modify the Tree Kernel Tools (Moschitti, 2004) to a grammardriven one. Kernel Setup: We use the Constituent, Predicate, and Predicate-Constituent related features, which are reported to get the best-reported performance (Pradhan et al., 2005a), as the baseline features. We use Che et al. (2006)’s hybrid convolution tree kernel (the best-reported method for kernel-based SRL) as our baseline kernel. It is defined as Khybrid = BKpath + (1− B)Kcs (0≤ B≤ 1) (for the detailed definitions of Kpath and Kcs , please refer to Che et al. (2006)). Here, we use our grammardriven tree kernel to compute Kpath and Kcs , and we call it grammar-driven hybrid tree kernel while Che et al. (2006)’s is non-grammar-driven hybrid convolution tree kernel. We use a greedy strategy to fine-tune parameters. Evaluation on the development set shows that our kernel yields the best performance when A (decay factor</context>
</contexts>
<marker>Che, Zhang, Liu, Li, 2006</marker>
<rawString>Wanxiang Che, Min Zhang, Ting Liu and Sheng Li. 2006. A hybrid convolution tree kernel for semantic role labeling. COLING-ACL-2006(poster)</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>Convolution kernels for natural language.</title>
<date>2001</date>
<pages>2001</pages>
<contexts>
<context position="3072" citStr="Collins and Duffy, 2001" startWordPosition="433" endWordPosition="436"> semantic arguments have been identified correctly. Both feature-based and kernel-based learning methods have been studied for semantic role classification (Carreras and Màrquez, 2004; Carreras and Màrquez, 2005). In feature-based methods, a flat feature vector is used to represent a predicateargument structure while, in kernel-based methods, a kernel function is used to measure directly the similarity between two predicate-argument structures. As we know, kernel methods are more effective in capturing structured features. Moschitti (2004) and Che et al. (2006) used a convolution tree kernel (Collins and Duffy, 2001) for semantic role classification. The convolution tree kernel takes sub-tree as its feature and counts the number of common sub-trees as the similarity between two predicate-arguments. This kernel has shown very 200 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 200–207, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics promising results in SRL. However, as a general syntactic structured information. For example, in learning algorithm, the tree kernel only carries out SRL, the Parse Tree Path feature is sensitive</context>
<context position="5335" citStr="Collins and Duffy, 2001" startWordPosition="772" endWordPosition="775">explicitly enumerating their features. To our knowledge, this is the first attempt in this Many kernels have been proposed and applied to research direction. In detail, we propose a gram- the NLP study. In particular, Haussler (1999) promar-driven convolution tree kernel for semantic posed the well-known convolution kernels for a role classification that can carry out more linguisti- discrete structure. In the context of it, more and cally motivated substructure matching. Experimental more kernels for restricted syntaxes or specific doresults show that the proposed method significantly mains (Collins and Duffy, 2001; Lodhi et al., 2002; outperforms the standard convolution tree kernel on Zelenko et al., 2003; Zhang et al., 2006) are prothe data set of the CoNLL-2005 SRL shared task. posed and explored in the NLP domain. The remainder of the paper is organized as fol- Of special interest here, Moschitti (2004) proposed lows: Section 2 reviews the previous work and Sec- Predicate Argument Feature (PAF) kernel for SRL tion 3 discusses our grammar-driven convolution under the framework of convolution tree kernel. He tree kernel. Section 4 shows the experimental re- selected portions of syntactic parse trees </context>
<context position="6884" citStr="Collins and Duffy, 2001" startWordPosition="1014" endWordPosition="1017">ndividual convolution kernels: a Path kernel and a Constituent Structure kernel. Che et al. (2006) showed that their method outperformed PAF on the CoNLL-2005 SRL dataset. The above two kernels are special instances of convolution tree kernel for SRL. As discussed in Section 1, convolution tree kernel only carries out hard matching, so it fails to handle similar phrase structures and near-synonymic grammar tags. This paper presents a grammar-driven convolution tree kernel to solve the two problems 3 Grammar-driven Convolution Tree Kernel 3.1 Convolution Tree Kernel In convolution tree kernel (Collins and Duffy, 2001), a parse tree T is represented by a vector of integer counts of each sub-tree type (regardless of its ancestors): φ(T) = ( ..., # subtreei(T), ...), where 2 Previous Work Feature-based Methods for SRL: most features used in prior SRL research are generally extended from Gildea and Jurafsky (2002), who used a linear interpolation method and extracted basic flat features from a parse tree to identify and classify the constituents in the FrameNet (Baker et al., 1998). Here, the basic features include Phrase Type, Parse Tree Path, and Position. Most of the following work focused on feature engine</context>
<context position="8565" citStr="Collins and Duffy (2001)" startWordPosition="1297" endWordPosition="1300">.edu/~treebank/ for the detailed definitions of the grammar tags used in the paper. 2 Some rewrite rules in English grammar are generalizations of others: for example, “NP4 DET JJ NN” is a specialized version of “NP4 DET NN”. The same applies to POS. The standard convolution tree kernel is unable to capture the two cases. 201 # subtreei(T) is the occurrence number of the ith sub-tree type (subtreei) in T. Since the number of different sub-trees is exponential with the parse tree size, it is computationally infeasible to directly use the feature vector φ(T) . To solve this computational issue, Collins and Duffy (2001) proposed the following parse tree kernel to calculate the dot product between the above high dimensional vectors implicitly. K T T ( , ) ( ), ( ) =&lt; φ φ T T 1 2 1 2 ∑ ∑ (( subtree i ) ( I ( ) n ⋅ ∑ I i )) ( ) n i n N 1 n N subtree 2 1 1 ∈ 2 2 ∈ = ∑n1∈N1 ∑n2∈N2 ∆ where N1 and N2 are the sets of nodes in trees T1 and T2, respectively, and ( ) I subtreei n is a function that is 1 iff the subtreei occurs with root at node n and zero otherwise, and ∆(n1, n2) is the number of the common subtrees rooted at n1 and n2, i.e., ∆ n n = ∑ I ( , ) ( 1) n ⋅ I ( 2) n 1 2 i subtreei subtree i ∆(n1, n2) can be</context>
<context position="18836" citStr="Collins and Duffy, 2001" startWordPosition="3319" endWordPosition="3322"> in this way, our kernel defined by Eq (3) is also a valid convolution kernel. Finally, let us study the computational issue of the new convolution tree kernel. Clearly, computing Eq. (6) requires exponential time in its worst case. However, in practice, it may only need O( |N1 |• |N2 |) . This is because there are only 9.9% rules (647 out of the total 6,534 rules in the parse trees) have optional nodes and most of them have only one optional node. In fact, the actual running time is even much less and is close to linear in the size of the trees since 0′(n1, n2) = 0 holds for many node pairs (Collins and Duffy, 2001). In theory, we can also design an efficient algorithm to compute Eq. (6) using a dynamic programming algorithm (Moschitti, 2006). We just leave it for our future work. 3.3 Comparison with previous work In above discussion, we show that the conventional convolution tree kernel is a special case of the grammar-driven tree kernel. From kernel function viewpoint, our kernel can carry out not only exact matching (as previous one described by Rules 2 and 3 in Subsection 3.1) but also approximate matching (Eqs. (5) and (6) in Subsection 3.2). From feature exploration viewpoint, although they explore</context>
</contexts>
<marker>Collins, Duffy, 2001</marker>
<rawString>Michael Collins and Nigel Duffy. 2001. Convolution kernels for natural language. NIPS-2001</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="7182" citStr="Gildea and Jurafsky (2002)" startWordPosition="1064" endWordPosition="1067">kernel only carries out hard matching, so it fails to handle similar phrase structures and near-synonymic grammar tags. This paper presents a grammar-driven convolution tree kernel to solve the two problems 3 Grammar-driven Convolution Tree Kernel 3.1 Convolution Tree Kernel In convolution tree kernel (Collins and Duffy, 2001), a parse tree T is represented by a vector of integer counts of each sub-tree type (regardless of its ancestors): φ(T) = ( ..., # subtreei(T), ...), where 2 Previous Work Feature-based Methods for SRL: most features used in prior SRL research are generally extended from Gildea and Jurafsky (2002), who used a linear interpolation method and extracted basic flat features from a parse tree to identify and classify the constituents in the FrameNet (Baker et al., 1998). Here, the basic features include Phrase Type, Parse Tree Path, and Position. Most of the following work focused on feature engineering (Xue and Palmer, 2004; Jiang et al., 2005) and machine learning models (Nielsen and Pradhan, 2004; Pradhan et al., 2005a). Some other work paid much attention to the robust SRL (Pradhan et al., 2005b) and post inference (Punyakanok et al., 2004). These featurebased methods are considered as </context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28(3):245–288</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Haussler</author>
</authors>
<title>Convolution kernels on discrete structures.</title>
<date>1999</date>
<tech>Technical Report UCSC-CRL-99-10</tech>
<contexts>
<context position="4945" citStr="Haussler (1999)" startWordPosition="718" endWordPosition="719">fective in modeling lead to over-fitting and compromise performance. structured objects. This is because a kernel can This paper reports our preliminary study in ad- measure the similarity between two structured obdressing the above issue by introducing more lin- jects using the original representation of the objects guistic knowledge into the convolution tree kernel. instead of explicitly enumerating their features. To our knowledge, this is the first attempt in this Many kernels have been proposed and applied to research direction. In detail, we propose a gram- the NLP study. In particular, Haussler (1999) promar-driven convolution tree kernel for semantic posed the well-known convolution kernels for a role classification that can carry out more linguisti- discrete structure. In the context of it, more and cally motivated substructure matching. Experimental more kernels for restricted syntaxes or specific doresults show that the proposed method significantly mains (Collins and Duffy, 2001; Lodhi et al., 2002; outperforms the standard convolution tree kernel on Zelenko et al., 2003; Zhang et al., 2006) are prothe data set of the CoNLL-2005 SRL shared task. posed and explored in the NLP domain. T</context>
<context position="18131" citStr="Haussler (1999)" startWordPosition="3184" endWordPosition="3185"> child of node n1 in its ith variation subtree Tni1, and likewise for ch(n2, j, k) . • Finally, the same as the previous tree kernel, a (0&lt;A&lt;1) is the decay factor (see the discussion in Subsection 3.1). Rule C: else 0′(n1, n2) = 0 Rule A accounts for Improvement 2 while Rule B accounts for Improvement 1. In Rule B, Eq. (6) is able to carry out multi-layer sub-tree approximate matching due to the introduction of the recursive part while Eq. (1) is only effective for subtrees of depth one. Moreover, we note that Eq. (4) is a convolution kernel according to the definition and the proof given in Haussler (1999), and Eqs (5) and (6) reformulate Eq. (4) so that it can be computed efficiently, in this way, our kernel defined by Eq (3) is also a valid convolution kernel. Finally, let us study the computational issue of the new convolution tree kernel. Clearly, computing Eq. (6) requires exponential time in its worst case. However, in practice, it may only need O( |N1 |• |N2 |) . This is because there are only 9.9% rules (647 out of the total 6,534 rules in the parse trees) have optional nodes and most of them have only one optional node. In fact, the actual running time is even much less and is close to</context>
</contexts>
<marker>Haussler, 1999</marker>
<rawString>David Haussler. 1999. Convolution kernels on discrete structures. Technical Report UCSC-CRL-99-10</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zheng Ping Jiang</author>
<author>Jia Li</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Semantic argument classification exploiting argument interdependence.</title>
<date>2005</date>
<pages>2005</pages>
<contexts>
<context position="7532" citStr="Jiang et al., 2005" startWordPosition="1122" endWordPosition="1125">ed by a vector of integer counts of each sub-tree type (regardless of its ancestors): φ(T) = ( ..., # subtreei(T), ...), where 2 Previous Work Feature-based Methods for SRL: most features used in prior SRL research are generally extended from Gildea and Jurafsky (2002), who used a linear interpolation method and extracted basic flat features from a parse tree to identify and classify the constituents in the FrameNet (Baker et al., 1998). Here, the basic features include Phrase Type, Parse Tree Path, and Position. Most of the following work focused on feature engineering (Xue and Palmer, 2004; Jiang et al., 2005) and machine learning models (Nielsen and Pradhan, 2004; Pradhan et al., 2005a). Some other work paid much attention to the robust SRL (Pradhan et al., 2005b) and post inference (Punyakanok et al., 2004). These featurebased methods are considered as the state of the art methods for SRL. However, as we know, the standard flat features are less effective in modeling the 1 Please refer to http://www.cis.upenn.edu/~treebank/ for the detailed definitions of the grammar tags used in the paper. 2 Some rewrite rules in English grammar are generalizations of others: for example, “NP4 DET JJ NN” is a sp</context>
<context position="27681" citStr="Jiang et al. (2005)" startWordPosition="4758" endWordPosition="4761">on the dataset of English PropBank I (LDC2004T14). The training, development and test sets follow the conventional split of Sections 02-21, 00 and 23. Table 3 compares our method with other previously best-reported methods with the same setting as discussed previously. It shows that our method outperforms the previous best-reported one with a relative error rate reduction of 10.8% (0.97/(100-91)). This further verifies the effectiveness of the grammar-driven kernel method for semantic role classification. Method Accuracy (%) Ours (Composite Kernel) 91.97 Moschitti (2006): PAF kernel only 87.7 Jiang et al. (2005): feature based 90.50 Pradhan et al. (2005a): feature based 91.0 Table 3: Performance comparison between our method and previous work Method Training Time 4 Sections 19 Sections Ours: grammar- —8.1 hours —7.9 days driven tree kernel Moschitti (2006): —7.9 hours —7.1 days non-grammar-driven tree kernel Table 4: Training time comparison Table 4 reports the training times of the two kernels. We can see that 1) the two kinds of convolution tree kernels have similar computing time. Although computing the grammar-driven one requires exponential time in its worst case, however, in practice, it may on</context>
</contexts>
<marker>Jiang, Li, Ng, 2005</marker>
<rawString>Zheng Ping Jiang, Jia Li and Hwee Tou Ng. 2005. Semantic argument classification exploiting argument interdependence. IJCAI-2005</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Text Categorization with Support Vecor Machine: learning with many relevant features.</title>
<date>1998</date>
<pages>1998</pages>
<contexts>
<context position="22564" citStr="Joachims, 1998" startWordPosition="3952" endWordPosition="3953">e 1 lists counts of sentences and arguments in the three data sets. Training Development Test Sentences 39,832 1,346 2,416 Arguments 239,858 8,346 14,077 Table 1: Counts on the data set We assume that the semantic role identification has been done correctly. In this way, we can focus on the classification task and evaluate it more accurately. We evaluate the performance with Accuracy. SVM (Vapnik, 1998) is selected as our classifier and the one vs. others strategy is adopted and the one with the largest margin is selected as the final answer. In our implementation, we use the binary SVMLight (Joachims, 1998) and modify the Tree Kernel Tools (Moschitti, 2004) to a grammardriven one. Kernel Setup: We use the Constituent, Predicate, and Predicate-Constituent related features, which are reported to get the best-reported performance (Pradhan et al., 2005a), as the baseline features. We use Che et al. (2006)’s hybrid convolution tree kernel (the best-reported method for kernel-based SRL) as our baseline kernel. It is defined as Khybrid = BKpath + (1− B)Kcs (0≤ B≤ 1) (for the detailed definitions of Kpath and Kcs , please refer to Che et al. (2006)). Here, we use our grammardriven tree kernel to compute</context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>T. Joachims. 1998. Text Categorization with Support Vecor Machine: learning with many relevant features. ECML-1998</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kashima</author>
<author>T Koyanagi</author>
</authors>
<date>2003</date>
<note>Kernels for SemiStructured Data. ICML-2003</note>
<contexts>
<context position="20403" citStr="Kashima and Koyanagi (2003)" startWordPosition="3566" endWordPosition="3569">rry out partial matching between sub-trees. The PT kernel generates a much larger feature space than both the conventional and the grammar-driven kernels. In this point, one can say that the grammar-driven tree kernel is a specialization of the PT kernel. However, the important difference between them is that the PT kernel is not grammar-driven, thus many nonlinguistically motivated structures are matched in the PT kernel. This may potentially compromise the performance since some of the over-generated features may possibly be noisy due to the lack of linguistic interpretation and constraint. Kashima and Koyanagi (2003) proposed a convolution kernel over labeled order trees by generalizing the standard convolution tree kernel. The labeled order tree kernel is much more flexible than the PT kernel and can explore much larger sub-tree features than the PT kernel. However, the same as the PT kernel, the labeled order tree kernel is not grammar-driven. Thus, it may face the same issues 0′(n n) = x ∑(I (Ti Tj ) x rl i 1 2 T n1 n2 nc ( , ) n i 1 k =1 (1 + 0′(ch(n1, i, k), ch(n2, j, k))) 1 ai+bj x � (6) 204 (such as over-generated features) as the PT kernel when used in NLP applications. Shen el al. (2003) proposed</context>
</contexts>
<marker>Kashima, Koyanagi, 2003</marker>
<rawString>Kashima H. and Koyanagi T. 2003. Kernels for SemiStructured Data. ICML-2003</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huma Lodhi</author>
<author>Craig Saunders</author>
<author>John Shawe-Taylor</author>
<author>Nello Cristianini</author>
<author>Chris Watkins</author>
</authors>
<title>Text classification using string kernels.</title>
<date>2002</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>2--419</pages>
<contexts>
<context position="5355" citStr="Lodhi et al., 2002" startWordPosition="776" endWordPosition="779">eir features. To our knowledge, this is the first attempt in this Many kernels have been proposed and applied to research direction. In detail, we propose a gram- the NLP study. In particular, Haussler (1999) promar-driven convolution tree kernel for semantic posed the well-known convolution kernels for a role classification that can carry out more linguisti- discrete structure. In the context of it, more and cally motivated substructure matching. Experimental more kernels for restricted syntaxes or specific doresults show that the proposed method significantly mains (Collins and Duffy, 2001; Lodhi et al., 2002; outperforms the standard convolution tree kernel on Zelenko et al., 2003; Zhang et al., 2006) are prothe data set of the CoNLL-2005 SRL shared task. posed and explored in the NLP domain. The remainder of the paper is organized as fol- Of special interest here, Moschitti (2004) proposed lows: Section 2 reviews the previous work and Sec- Predicate Argument Feature (PAF) kernel for SRL tion 3 discusses our grammar-driven convolution under the framework of convolution tree kernel. He tree kernel. Section 4 shows the experimental re- selected portions of syntactic parse trees as predicatesults. W</context>
</contexts>
<marker>Lodhi, Saunders, Shawe-Taylor, Cristianini, Watkins, 2002</marker>
<rawString>Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello Cristianini and Chris Watkins. 2002. Text classification using string kernels. Journal of Machine Learning Research, 2:419–444</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="21605" citStr="Marcus et al., 1993" startWordPosition="3787" endWordPosition="3790">l. (2003) proposed a lexicalized tree kernel to utilize LTAG-based features in parse reranking. Their methods need to obtain a LTAG derivation tree for each parse tree before kernel calculation. In contrast, we use the notion of optional arguments to define our grammar-driven tree kernel and use the empirical set of CFG rules to determine which arguments are optional. 4 Experiments 4.1 Experimental Setting Data: We use the CoNLL-2005 SRL shared task data (Carreras and Màrquez, 2005) as our experimental corpus. The data consists of sections of the Wall Street Journal part of the Penn TreeBank (Marcus et al., 1993), with information on predicate-argument structures extracted from the PropBank corpus (Palmer et al., 2005). As defined by the shared task, we use sections 02-21 for training, section 24 for development and section 23 for test. There are 35 roles in the data including 7 Core (A0–A5, AA), 14 Adjunct (AM-) and 14 Reference (R-) arguments. Table 1 lists counts of sentences and arguments in the three data sets. Training Development Test Sentences 39,832 1,346 2,416 Arguments 239,858 8,346 14,077 Table 1: Counts on the data set We assume that the semantic role identification has been done correctl</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P. Marcus, Mary Ann Marcinkiewicz and Beatrice Santorini. 1993. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19(2):313–330</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>A study on convolution kernels for shallow statistic parsing.</title>
<date>2004</date>
<pages>2004</pages>
<contexts>
<context position="2993" citStr="Moschitti (2004)" startWordPosition="421" endWordPosition="422">cuses on semantic role classification task with the assumption that the semantic arguments have been identified correctly. Both feature-based and kernel-based learning methods have been studied for semantic role classification (Carreras and Màrquez, 2004; Carreras and Màrquez, 2005). In feature-based methods, a flat feature vector is used to represent a predicateargument structure while, in kernel-based methods, a kernel function is used to measure directly the similarity between two predicate-argument structures. As we know, kernel methods are more effective in capturing structured features. Moschitti (2004) and Che et al. (2006) used a convolution tree kernel (Collins and Duffy, 2001) for semantic role classification. The convolution tree kernel takes sub-tree as its feature and counts the number of common sub-trees as the similarity between two predicate-arguments. This kernel has shown very 200 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 200–207, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics promising results in SRL. However, as a general syntactic structured information. For example, in learning algorithm,</context>
<context position="5634" citStr="Moschitti (2004)" startWordPosition="826" endWordPosition="827">convolution kernels for a role classification that can carry out more linguisti- discrete structure. In the context of it, more and cally motivated substructure matching. Experimental more kernels for restricted syntaxes or specific doresults show that the proposed method significantly mains (Collins and Duffy, 2001; Lodhi et al., 2002; outperforms the standard convolution tree kernel on Zelenko et al., 2003; Zhang et al., 2006) are prothe data set of the CoNLL-2005 SRL shared task. posed and explored in the NLP domain. The remainder of the paper is organized as fol- Of special interest here, Moschitti (2004) proposed lows: Section 2 reviews the previous work and Sec- Predicate Argument Feature (PAF) kernel for SRL tion 3 discusses our grammar-driven convolution under the framework of convolution tree kernel. He tree kernel. Section 4 shows the experimental re- selected portions of syntactic parse trees as predicatesults. We conclude our work in Section 5. argument feature spaces, which include salient substructures of predicate-arguments, to define convolution kernels for the task of semantic role classification. Under the same framework, Che et al. (2006) proposed a hybrid convolution tree kerne</context>
<context position="22615" citStr="Moschitti, 2004" startWordPosition="3960" endWordPosition="3961"> three data sets. Training Development Test Sentences 39,832 1,346 2,416 Arguments 239,858 8,346 14,077 Table 1: Counts on the data set We assume that the semantic role identification has been done correctly. In this way, we can focus on the classification task and evaluate it more accurately. We evaluate the performance with Accuracy. SVM (Vapnik, 1998) is selected as our classifier and the one vs. others strategy is adopted and the one with the largest margin is selected as the final answer. In our implementation, we use the binary SVMLight (Joachims, 1998) and modify the Tree Kernel Tools (Moschitti, 2004) to a grammardriven one. Kernel Setup: We use the Constituent, Predicate, and Predicate-Constituent related features, which are reported to get the best-reported performance (Pradhan et al., 2005a), as the baseline features. We use Che et al. (2006)’s hybrid convolution tree kernel (the best-reported method for kernel-based SRL) as our baseline kernel. It is defined as Khybrid = BKpath + (1− B)Kcs (0≤ B≤ 1) (for the detailed definitions of Kpath and Kcs , please refer to Che et al. (2006)). Here, we use our grammardriven tree kernel to compute Kpath and Kcs , and we call it grammar-driven hybr</context>
</contexts>
<marker>Moschitti, 2004</marker>
<rawString>Alessandro Moschitti. 2004. A study on convolution kernels for shallow statistic parsing. ACL-2004</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Syntactic kernels for natural language learning: the semantic role labeling case.</title>
<date>2006</date>
<note>HLT-NAACL-2006 (short paper)</note>
<contexts>
<context position="18965" citStr="Moschitti, 2006" startWordPosition="3341" endWordPosition="3343">onvolution tree kernel. Clearly, computing Eq. (6) requires exponential time in its worst case. However, in practice, it may only need O( |N1 |• |N2 |) . This is because there are only 9.9% rules (647 out of the total 6,534 rules in the parse trees) have optional nodes and most of them have only one optional node. In fact, the actual running time is even much less and is close to linear in the size of the trees since 0′(n1, n2) = 0 holds for many node pairs (Collins and Duffy, 2001). In theory, we can also design an efficient algorithm to compute Eq. (6) using a dynamic programming algorithm (Moschitti, 2006). We just leave it for our future work. 3.3 Comparison with previous work In above discussion, we show that the conventional convolution tree kernel is a special case of the grammar-driven tree kernel. From kernel function viewpoint, our kernel can carry out not only exact matching (as previous one described by Rules 2 and 3 in Subsection 3.1) but also approximate matching (Eqs. (5) and (6) in Subsection 3.2). From feature exploration viewpoint, although they explore the same sub-structure feature space (defined recursively by the phrase parse rules), their feature values are different since o</context>
<context position="27639" citStr="Moschitti (2006)" startWordPosition="4752" endWordPosition="4753"> other methods, we also do experiments on the dataset of English PropBank I (LDC2004T14). The training, development and test sets follow the conventional split of Sections 02-21, 00 and 23. Table 3 compares our method with other previously best-reported methods with the same setting as discussed previously. It shows that our method outperforms the previous best-reported one with a relative error rate reduction of 10.8% (0.97/(100-91)). This further verifies the effectiveness of the grammar-driven kernel method for semantic role classification. Method Accuracy (%) Ours (Composite Kernel) 91.97 Moschitti (2006): PAF kernel only 87.7 Jiang et al. (2005): feature based 90.50 Pradhan et al. (2005a): feature based 91.0 Table 3: Performance comparison between our method and previous work Method Training Time 4 Sections 19 Sections Ours: grammar- —8.1 hours —7.9 days driven tree kernel Moschitti (2006): —7.9 hours —7.1 days non-grammar-driven tree kernel Table 4: Training time comparison Table 4 reports the training times of the two kernels. We can see that 1) the two kinds of convolution tree kernels have similar computing time. Although computing the grammar-driven one requires exponential time in its w</context>
</contexts>
<marker>Moschitti, 2006</marker>
<rawString>Alessandro Moschitti. 2006. Syntactic kernels for natural language learning: the semantic role labeling case. HLT-NAACL-2006 (short paper)</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rodney D Nielsen</author>
<author>Sameer Pradhan</author>
</authors>
<title>Mixing weak learners in semantic parsing.</title>
<date>2004</date>
<pages>2004</pages>
<contexts>
<context position="7587" citStr="Nielsen and Pradhan, 2004" startWordPosition="1130" endWordPosition="1133"> type (regardless of its ancestors): φ(T) = ( ..., # subtreei(T), ...), where 2 Previous Work Feature-based Methods for SRL: most features used in prior SRL research are generally extended from Gildea and Jurafsky (2002), who used a linear interpolation method and extracted basic flat features from a parse tree to identify and classify the constituents in the FrameNet (Baker et al., 1998). Here, the basic features include Phrase Type, Parse Tree Path, and Position. Most of the following work focused on feature engineering (Xue and Palmer, 2004; Jiang et al., 2005) and machine learning models (Nielsen and Pradhan, 2004; Pradhan et al., 2005a). Some other work paid much attention to the robust SRL (Pradhan et al., 2005b) and post inference (Punyakanok et al., 2004). These featurebased methods are considered as the state of the art methods for SRL. However, as we know, the standard flat features are less effective in modeling the 1 Please refer to http://www.cis.upenn.edu/~treebank/ for the detailed definitions of the grammar tags used in the paper. 2 Some rewrite rules in English grammar are generalizations of others: for example, “NP4 DET JJ NN” is a specialized version of “NP4 DET NN”. The same applies to </context>
</contexts>
<marker>Nielsen, Pradhan, 2004</marker>
<rawString>Rodney D. Nielsen and Sameer Pradhan. 2004. Mixing weak learners in semantic parsing. EMNLP-2004</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Dan Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The proposition bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="21713" citStr="Palmer et al., 2005" startWordPosition="3803" endWordPosition="3806">s need to obtain a LTAG derivation tree for each parse tree before kernel calculation. In contrast, we use the notion of optional arguments to define our grammar-driven tree kernel and use the empirical set of CFG rules to determine which arguments are optional. 4 Experiments 4.1 Experimental Setting Data: We use the CoNLL-2005 SRL shared task data (Carreras and Màrquez, 2005) as our experimental corpus. The data consists of sections of the Wall Street Journal part of the Penn TreeBank (Marcus et al., 1993), with information on predicate-argument structures extracted from the PropBank corpus (Palmer et al., 2005). As defined by the shared task, we use sections 02-21 for training, section 24 for development and section 23 for test. There are 35 roles in the data including 7 Core (A0–A5, AA), 14 Adjunct (AM-) and 14 Reference (R-) arguments. Table 1 lists counts of sentences and arguments in the three data sets. Training Development Test Sentences 39,832 1,346 2,416 Arguments 239,858 8,346 14,077 Table 1: Counts on the data set We assume that the semantic role identification has been done correctly. In this way, we can focus on the classification task and evaluate it more accurately. We evaluate the per</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Dan Gildea and Paul Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1)</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Kadri Hacioglu</author>
<author>Valeri Krugler</author>
<author>Wayne Ward</author>
<author>James H Martin</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Support vector learning for semantic argument classification.</title>
<date>2005</date>
<journal>Journal of Machine Learning</journal>
<contexts>
<context position="7609" citStr="Pradhan et al., 2005" startWordPosition="1134" endWordPosition="1137">cestors): φ(T) = ( ..., # subtreei(T), ...), where 2 Previous Work Feature-based Methods for SRL: most features used in prior SRL research are generally extended from Gildea and Jurafsky (2002), who used a linear interpolation method and extracted basic flat features from a parse tree to identify and classify the constituents in the FrameNet (Baker et al., 1998). Here, the basic features include Phrase Type, Parse Tree Path, and Position. Most of the following work focused on feature engineering (Xue and Palmer, 2004; Jiang et al., 2005) and machine learning models (Nielsen and Pradhan, 2004; Pradhan et al., 2005a). Some other work paid much attention to the robust SRL (Pradhan et al., 2005b) and post inference (Punyakanok et al., 2004). These featurebased methods are considered as the state of the art methods for SRL. However, as we know, the standard flat features are less effective in modeling the 1 Please refer to http://www.cis.upenn.edu/~treebank/ for the detailed definitions of the grammar tags used in the paper. 2 Some rewrite rules in English grammar are generalizations of others: for example, “NP4 DET JJ NN” is a specialized version of “NP4 DET NN”. The same applies to POS. The standard conv</context>
<context position="22810" citStr="Pradhan et al., 2005" startWordPosition="3986" endWordPosition="3989">n done correctly. In this way, we can focus on the classification task and evaluate it more accurately. We evaluate the performance with Accuracy. SVM (Vapnik, 1998) is selected as our classifier and the one vs. others strategy is adopted and the one with the largest margin is selected as the final answer. In our implementation, we use the binary SVMLight (Joachims, 1998) and modify the Tree Kernel Tools (Moschitti, 2004) to a grammardriven one. Kernel Setup: We use the Constituent, Predicate, and Predicate-Constituent related features, which are reported to get the best-reported performance (Pradhan et al., 2005a), as the baseline features. We use Che et al. (2006)’s hybrid convolution tree kernel (the best-reported method for kernel-based SRL) as our baseline kernel. It is defined as Khybrid = BKpath + (1− B)Kcs (0≤ B≤ 1) (for the detailed definitions of Kpath and Kcs , please refer to Che et al. (2006)). Here, we use our grammardriven tree kernel to compute Kpath and Kcs , and we call it grammar-driven hybrid tree kernel while Che et al. (2006)’s is non-grammar-driven hybrid convolution tree kernel. We use a greedy strategy to fine-tune parameters. Evaluation on the development set shows that our k</context>
<context position="27723" citStr="Pradhan et al. (2005" startWordPosition="4765" endWordPosition="4768">2004T14). The training, development and test sets follow the conventional split of Sections 02-21, 00 and 23. Table 3 compares our method with other previously best-reported methods with the same setting as discussed previously. It shows that our method outperforms the previous best-reported one with a relative error rate reduction of 10.8% (0.97/(100-91)). This further verifies the effectiveness of the grammar-driven kernel method for semantic role classification. Method Accuracy (%) Ours (Composite Kernel) 91.97 Moschitti (2006): PAF kernel only 87.7 Jiang et al. (2005): feature based 90.50 Pradhan et al. (2005a): feature based 91.0 Table 3: Performance comparison between our method and previous work Method Training Time 4 Sections 19 Sections Ours: grammar- —8.1 hours —7.9 days driven tree kernel Moschitti (2006): —7.9 hours —7.1 days non-grammar-driven tree kernel Table 4: Training time comparison Table 4 reports the training times of the two kernels. We can see that 1) the two kinds of convolution tree kernels have similar computing time. Although computing the grammar-driven one requires exponential time in its worst case, however, in practice, it may only need O( |N1 |• |N2 |) or linear and 2) </context>
</contexts>
<marker>Pradhan, Hacioglu, Krugler, Ward, Martin, Jurafsky, 2005</marker>
<rawString>Sameer Pradhan, Kadri Hacioglu, Valeri Krugler, Wayne Ward, James H. Martin and Daniel Jurafsky. 2005a. Support vector learning for semantic argument classification. Journal of Machine Learning</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Wayne Ward</author>
<author>Kadri Hacioglu</author>
<author>James Martin</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Semantic role labeling using different syntactic views.</title>
<date>2005</date>
<pages>2005</pages>
<contexts>
<context position="7609" citStr="Pradhan et al., 2005" startWordPosition="1134" endWordPosition="1137">cestors): φ(T) = ( ..., # subtreei(T), ...), where 2 Previous Work Feature-based Methods for SRL: most features used in prior SRL research are generally extended from Gildea and Jurafsky (2002), who used a linear interpolation method and extracted basic flat features from a parse tree to identify and classify the constituents in the FrameNet (Baker et al., 1998). Here, the basic features include Phrase Type, Parse Tree Path, and Position. Most of the following work focused on feature engineering (Xue and Palmer, 2004; Jiang et al., 2005) and machine learning models (Nielsen and Pradhan, 2004; Pradhan et al., 2005a). Some other work paid much attention to the robust SRL (Pradhan et al., 2005b) and post inference (Punyakanok et al., 2004). These featurebased methods are considered as the state of the art methods for SRL. However, as we know, the standard flat features are less effective in modeling the 1 Please refer to http://www.cis.upenn.edu/~treebank/ for the detailed definitions of the grammar tags used in the paper. 2 Some rewrite rules in English grammar are generalizations of others: for example, “NP4 DET JJ NN” is a specialized version of “NP4 DET NN”. The same applies to POS. The standard conv</context>
<context position="22810" citStr="Pradhan et al., 2005" startWordPosition="3986" endWordPosition="3989">n done correctly. In this way, we can focus on the classification task and evaluate it more accurately. We evaluate the performance with Accuracy. SVM (Vapnik, 1998) is selected as our classifier and the one vs. others strategy is adopted and the one with the largest margin is selected as the final answer. In our implementation, we use the binary SVMLight (Joachims, 1998) and modify the Tree Kernel Tools (Moschitti, 2004) to a grammardriven one. Kernel Setup: We use the Constituent, Predicate, and Predicate-Constituent related features, which are reported to get the best-reported performance (Pradhan et al., 2005a), as the baseline features. We use Che et al. (2006)’s hybrid convolution tree kernel (the best-reported method for kernel-based SRL) as our baseline kernel. It is defined as Khybrid = BKpath + (1− B)Kcs (0≤ B≤ 1) (for the detailed definitions of Kpath and Kcs , please refer to Che et al. (2006)). Here, we use our grammardriven tree kernel to compute Kpath and Kcs , and we call it grammar-driven hybrid tree kernel while Che et al. (2006)’s is non-grammar-driven hybrid convolution tree kernel. We use a greedy strategy to fine-tune parameters. Evaluation on the development set shows that our k</context>
<context position="27723" citStr="Pradhan et al. (2005" startWordPosition="4765" endWordPosition="4768">2004T14). The training, development and test sets follow the conventional split of Sections 02-21, 00 and 23. Table 3 compares our method with other previously best-reported methods with the same setting as discussed previously. It shows that our method outperforms the previous best-reported one with a relative error rate reduction of 10.8% (0.97/(100-91)). This further verifies the effectiveness of the grammar-driven kernel method for semantic role classification. Method Accuracy (%) Ours (Composite Kernel) 91.97 Moschitti (2006): PAF kernel only 87.7 Jiang et al. (2005): feature based 90.50 Pradhan et al. (2005a): feature based 91.0 Table 3: Performance comparison between our method and previous work Method Training Time 4 Sections 19 Sections Ours: grammar- —8.1 hours —7.9 days driven tree kernel Moschitti (2006): —7.9 hours —7.1 days non-grammar-driven tree kernel Table 4: Training time comparison Table 4 reports the training times of the two kernels. We can see that 1) the two kinds of convolution tree kernels have similar computing time. Although computing the grammar-driven one requires exponential time in its worst case, however, in practice, it may only need O( |N1 |• |N2 |) or linear and 2) </context>
</contexts>
<marker>Pradhan, Ward, Hacioglu, Martin, Jurafsky, 2005</marker>
<rawString>Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James Martin and Daniel Jurafsky. 2005b. Semantic role labeling using different syntactic views. ACL-2005</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasin Punyakanok</author>
<author>Dan Roth</author>
<author>Wen-tau Yih</author>
<author>Dav Zimak</author>
</authors>
<title>Semantic role labeling via integer linear programming inference.</title>
<date>2004</date>
<pages>2004</pages>
<contexts>
<context position="7735" citStr="Punyakanok et al., 2004" startWordPosition="1156" endWordPosition="1159">ior SRL research are generally extended from Gildea and Jurafsky (2002), who used a linear interpolation method and extracted basic flat features from a parse tree to identify and classify the constituents in the FrameNet (Baker et al., 1998). Here, the basic features include Phrase Type, Parse Tree Path, and Position. Most of the following work focused on feature engineering (Xue and Palmer, 2004; Jiang et al., 2005) and machine learning models (Nielsen and Pradhan, 2004; Pradhan et al., 2005a). Some other work paid much attention to the robust SRL (Pradhan et al., 2005b) and post inference (Punyakanok et al., 2004). These featurebased methods are considered as the state of the art methods for SRL. However, as we know, the standard flat features are less effective in modeling the 1 Please refer to http://www.cis.upenn.edu/~treebank/ for the detailed definitions of the grammar tags used in the paper. 2 Some rewrite rules in English grammar are generalizations of others: for example, “NP4 DET JJ NN” is a specialized version of “NP4 DET NN”. The same applies to POS. The standard convolution tree kernel is unable to capture the two cases. 201 # subtreei(T) is the occurrence number of the ith sub-tree type (s</context>
</contexts>
<marker>Punyakanok, Roth, Yih, Zimak, 2004</marker>
<rawString>Vasin Punyakanok, Dan Roth, Wen-tau Yih and Dav Zimak. 2004. Semantic role labeling via integer linear programming inference. COLING-2004</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasin Punyakanok</author>
<author>Dan Roth</author>
<author>Wen Tau Yih</author>
</authors>
<title>The necessity of syntactic parsing for semantic role labeling.</title>
<date>2005</date>
<pages>2005</pages>
<marker>Punyakanok, Roth, Yih, 2005</marker>
<rawString>Vasin Punyakanok, Dan Roth and Wen Tau Yih. 2005. The necessity of syntactic parsing for semantic role labeling. IJCAI-2005</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Anoop Sarkar</author>
<author>A K Joshi</author>
</authors>
<title>Using LTAG based features in parse reranking.</title>
<date>2003</date>
<pages>03</pages>
<marker>Shen, Sarkar, Joshi, 2003</marker>
<rawString>Libin Shen, Anoop Sarkar and A. K. Joshi. 2003. Using LTAG based features in parse reranking. EMNLP-03</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Suzuki</author>
</authors>
<title>Hideki Isozaki and Eisaku Maede.</title>
<date>2004</date>
<pages>2004</pages>
<marker>Suzuki, 2004</marker>
<rawString>Jun Suzuki, Hideki Isozaki and Eisaku Maede. 2004. Convolution kernels with feature selection for Natural Language processing tasks. ACL-2004</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir N Vapnik</author>
</authors>
<title>Statistical Learning Theory.</title>
<date>1998</date>
<publisher>Wiley</publisher>
<contexts>
<context position="22355" citStr="Vapnik, 1998" startWordPosition="3914" endWordPosition="3915">sk, we use sections 02-21 for training, section 24 for development and section 23 for test. There are 35 roles in the data including 7 Core (A0–A5, AA), 14 Adjunct (AM-) and 14 Reference (R-) arguments. Table 1 lists counts of sentences and arguments in the three data sets. Training Development Test Sentences 39,832 1,346 2,416 Arguments 239,858 8,346 14,077 Table 1: Counts on the data set We assume that the semantic role identification has been done correctly. In this way, we can focus on the classification task and evaluate it more accurately. We evaluate the performance with Accuracy. SVM (Vapnik, 1998) is selected as our classifier and the one vs. others strategy is adopted and the one with the largest margin is selected as the final answer. In our implementation, we use the binary SVMLight (Joachims, 1998) and modify the Tree Kernel Tools (Moschitti, 2004) to a grammardriven one. Kernel Setup: We use the Constituent, Predicate, and Predicate-Constituent related features, which are reported to get the best-reported performance (Pradhan et al., 2005a), as the baseline features. We use Che et al. (2006)’s hybrid convolution tree kernel (the best-reported method for kernel-based SRL) as our ba</context>
</contexts>
<marker>Vapnik, 1998</marker>
<rawString>Vladimir N. Vapnik. 1998. Statistical Learning Theory. Wiley</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Martha Palmer</author>
</authors>
<title>Calibrating features for semantic role labeling.</title>
<date>2004</date>
<pages>2004</pages>
<contexts>
<context position="7511" citStr="Xue and Palmer, 2004" startWordPosition="1118" endWordPosition="1121">se tree T is represented by a vector of integer counts of each sub-tree type (regardless of its ancestors): φ(T) = ( ..., # subtreei(T), ...), where 2 Previous Work Feature-based Methods for SRL: most features used in prior SRL research are generally extended from Gildea and Jurafsky (2002), who used a linear interpolation method and extracted basic flat features from a parse tree to identify and classify the constituents in the FrameNet (Baker et al., 1998). Here, the basic features include Phrase Type, Parse Tree Path, and Position. Most of the following work focused on feature engineering (Xue and Palmer, 2004; Jiang et al., 2005) and machine learning models (Nielsen and Pradhan, 2004; Pradhan et al., 2005a). Some other work paid much attention to the robust SRL (Pradhan et al., 2005b) and post inference (Punyakanok et al., 2004). These featurebased methods are considered as the state of the art methods for SRL. However, as we know, the standard flat features are less effective in modeling the 1 Please refer to http://www.cis.upenn.edu/~treebank/ for the detailed definitions of the grammar tags used in the paper. 2 Some rewrite rules in English grammar are generalizations of others: for example, “N</context>
</contexts>
<marker>Xue, Palmer, 2004</marker>
<rawString>Nianwen Xue and Martha Palmer. 2004. Calibrating features for semantic role labeling. EMNLP-2004</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Zelenko</author>
<author>Chinatsu Aone</author>
<author>Anthony Richardella</author>
</authors>
<title>Kernel methods for relation extraction.</title>
<date>2003</date>
<journal>Machine Learning Research,</journal>
<pages>3--1083</pages>
<contexts>
<context position="5429" citStr="Zelenko et al., 2003" startWordPosition="787" endWordPosition="790">ernels have been proposed and applied to research direction. In detail, we propose a gram- the NLP study. In particular, Haussler (1999) promar-driven convolution tree kernel for semantic posed the well-known convolution kernels for a role classification that can carry out more linguisti- discrete structure. In the context of it, more and cally motivated substructure matching. Experimental more kernels for restricted syntaxes or specific doresults show that the proposed method significantly mains (Collins and Duffy, 2001; Lodhi et al., 2002; outperforms the standard convolution tree kernel on Zelenko et al., 2003; Zhang et al., 2006) are prothe data set of the CoNLL-2005 SRL shared task. posed and explored in the NLP domain. The remainder of the paper is organized as fol- Of special interest here, Moschitti (2004) proposed lows: Section 2 reviews the previous work and Sec- Predicate Argument Feature (PAF) kernel for SRL tion 3 discusses our grammar-driven convolution under the framework of convolution tree kernel. He tree kernel. Section 4 shows the experimental re- selected portions of syntactic parse trees as predicatesults. We conclude our work in Section 5. argument feature spaces, which include s</context>
</contexts>
<marker>Zelenko, Aone, Richardella, 2003</marker>
<rawString>Dmitry Zelenko, Chinatsu Aone, and Anthony Richardella. 2003. Kernel methods for relation extraction. Machine Learning Research, 3:1083–1106</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Jie Zhang</author>
<author>Jian Su</author>
<author>Guodong Zhou</author>
</authors>
<title>A Composite Kernel to Extract Relations between Entities with both Flat and Structured Features.</title>
<date>2006</date>
<pages>2006</pages>
<contexts>
<context position="5450" citStr="Zhang et al., 2006" startWordPosition="791" endWordPosition="794">sed and applied to research direction. In detail, we propose a gram- the NLP study. In particular, Haussler (1999) promar-driven convolution tree kernel for semantic posed the well-known convolution kernels for a role classification that can carry out more linguisti- discrete structure. In the context of it, more and cally motivated substructure matching. Experimental more kernels for restricted syntaxes or specific doresults show that the proposed method significantly mains (Collins and Duffy, 2001; Lodhi et al., 2002; outperforms the standard convolution tree kernel on Zelenko et al., 2003; Zhang et al., 2006) are prothe data set of the CoNLL-2005 SRL shared task. posed and explored in the NLP domain. The remainder of the paper is organized as fol- Of special interest here, Moschitti (2004) proposed lows: Section 2 reviews the previous work and Sec- Predicate Argument Feature (PAF) kernel for SRL tion 3 discusses our grammar-driven convolution under the framework of convolution tree kernel. He tree kernel. Section 4 shows the experimental re- selected portions of syntactic parse trees as predicatesults. We conclude our work in Section 5. argument feature spaces, which include salient substructures </context>
</contexts>
<marker>Zhang, Zhang, Su, Zhou, 2006</marker>
<rawString>Min Zhang, Jie Zhang, Jian Su and Guodong Zhou. 2006. A Composite Kernel to Extract Relations between Entities with both Flat and Structured Features. COLING-ACL-2006</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>