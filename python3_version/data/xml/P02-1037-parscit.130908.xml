<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000800">
<note confidence="0.9045065">
Proceedings of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL), Philadelphia, July 2002, pp. 287-294.
</note>
<bodyText confidence="0.9993375">
that share the final n words to re-join. The Markov
assumption is made by necessity, not because those
who make it believe that there is such independence.
In terms of equivalence classes, a Markov lan-
guage model of order n stipulates that two prefix
strings of words belong to the same equivalence
class if their final n words are the same. The effect
of such a model is that the conditioning information
in the chain rule is truncated to include only the pre-
vious n-1 words.
</bodyText>
<equation confidence="0.928171666666667">
NUM P(WO)P (W11WO)
P(Wn-1 lw2) 1-1P(wilwir,i) (2)
i=n
</equation>
<bodyText confidence="0.997149342857143">
Syntactic parsing has traditionally dealt with
given strings, and is performed for the purpose
of generating a parse structure, or many possi-
ble parse structures, for that given string. Pars-
ing algorithms have been generalized to accept lat-
tices of word hypotheses — e.g. Kiefer et al. (2000;
Chappelier et al. (1999) — but these are chart pars-
ing algorithms intended to efficiently yield parse
trees, rather than rescore the arcs in the lattice.
Chelba (2000) applied his parser incrementally to
lattices for the purpose of scoring hypotheses, using
A-star search to find the best hypotheses in the lat-
tice. This approach treated the lattice as a tree of hy-
potheses, without exploiting the re-entrances in the
structure of the lattice. The efficient search meant
that some arcs in the lattice would not be visited.
We propose a novel method for applying a parser
to a lattice of word hypotheses that can take advan-
tage of re-entrances in the lattice, and which allows
the parser to visit and score every arc. It does this
by making a Markov assumption akin to the kind of
Markov assumption made by n-gram models. This
is done, not because we feel that the independence
assumptions are strictly speaking correct, but for the
efficiency gains that they provide.
Before discussing what a Markov assumption
looks like for an incremental parser, let us first
briefly outline how an incremental parser can serve
as a language model.
A PCFG1 defines a probability distribution over
strings of words as follows. Let Ty be the set of all
INVe assume a familiarity with PCFGs. For an introduction
to PCFGs, see, e.g., Manning and Schtitze (1999).
complete trees rooted at the start symbol, with the
string of terminals w&apos;o&apos; as the terminal yield. Then
</bodyText>
<equation confidence="0.9932045">
P(w) = E P(t) (3)
tET„
</equation>
<bodyText confidence="0.998616533333333">
That is, the probability of the string occurring in the
space of all possible strings of the language is the
probability of the set Tw, i.e. the sum of its mem-
bers&apos; probabilities.
A PCFG also defines a marginal probability distri-
bution over string prefixes, and we will present this
in terms of partial derivations. A partial derivation
(or parse) d is defined with respect to a string n as
follows: it is the leftmost derivation2 of the string,
with wi on the right-hand side of the last expansion
in the derivation. Let D be the set of all partial
wo
derivations for a prefix string wg. Then the marginal
probability Pm is the probability of all strings which
begin with the prefix string 40, and is defined as
</bodyText>
<equation confidence="0.99598">
PM( = E P (d) (4)
dED U)0
</equation>
<bodyText confidence="0.999591333333333">
This is the same as summing the probability of all
complete trees which have wg as the first j +1 words
of their terminal yield.
From these marginals, we can calculate the con-
ditional probabilities of the words given the prefix
string. By definition,
</bodyText>
<equation confidence="0.997443333333333">
PM(w1)
P(wi+11wO) =
PM(w)
EdED j+i P (d)
wo
EdED P(d)
</equation>
<bodyText confidence="0.999804">
Note that the numerator at word w3 is the denomi-
nator at word w3+1, so that the product of all of the
word probabilities is the numerator at the final word,
i.e. the marginal string prefix probability.
In short, the conditional probability is calculated
by summing the probability of all of the trees before
the word has been seen, incorporating the word into
the parses to form a new set, summing the proba-
bilities of the new set, and normalizing this by the
</bodyText>
<footnote confidence="0.91139">
2A leftmost derivation is a derivation in which the leftmost
non-terminal is always expanded.
</footnote>
<figure confidence="0.985113785714286">
(5)
sighed
man
uh • man • sighed °
• mans 41
• man 0
hide
sighed
hide
uh • mans •
• the
limn sighed
mans
F
</figure>
<figureCaption confidence="0.856306">
Figure 1: List of hypothesis strings encoded as aigure 2: Tree of hypothesis strings encoded as a
non-deterministic finite-state automaton deterministic finite-state automaton
</figureCaption>
<bodyText confidence="0.9997893">
probability of the original set. The conditional prob-
ability can be thought of as the amount of probabil-
ity mass the parser must expend to incorporate the
next word. As shown in Roark (2001a), this can
be approximated with a top-down, leftmost beam-
search parser by simply performing the sums over
the parses that have not been pruned. In practice,
the pruned probability mass can be relatively low,
yielding a snug lower bound on the joint probability
of each string.
</bodyText>
<sectionHeader confidence="0.970525" genericHeader="abstract">
3 Markov parsing
</sectionHeader>
<bodyText confidence="0.999149263157895">
At this point we can begin thinking about how to as-
sign probabilities in the manner outlined above to
the arcs in a finite-state representation of the hy-
potheses. Figure 1 shows a list of distinct hypoth-
esis strings for a particular utterance, encoded as a
non-deterministic3 finite-state machine. In these fig-
ures, the state in bold is the initial state, the states
with double circles are final states, and each arc is
labeled with a terminal symbol. Each hypothesis
string is represented as a separate path through the
machine from the initial state to the final state.
To score the arcs, the parser accumulates at each
state, from left-to-right through the machine, a
weighted set of candidate parses for the hypothe-
sis to that point. It then incorporates the word la-
beling the arc into the parses, and deposits the new
set of candidate parses at the destination state. The
arc probability is the sum of the probabilities of the
derivations at the end state divided by the sum of
</bodyText>
<footnote confidence="0.5082825">
&apos;The machine is non-deterministic because there are multi-
ple arcs out of the initial state with the same label.
</footnote>
<bodyText confidence="0.999869117647059">
the probabilities of the derivations at the beginning
state. Of course, the probability of the derivation at
the initial state is one.
For example, state 3 in figure 1 will have a set of
partial derivations, all of which have the word &apos;the&apos;
as their terminal yield. There are a variety of ways
in which the word &apos;man&apos;, which labels the transi-
tion from state 3 to state 7, can be incorporated into
these parses. The arc probability, which is the con-
ditional probability of the word &apos;man&apos; given the pre-
vious words, is estimated as the sum of the probabil-
ities of all parses at state 7 divided by the sum of the
probabilities of all parses at state 3.
Figure 2 shows the same set of hypotheses en-
coded as a deterministic finite-state machine. This
is a tree of hypothesis strings, and it encodes ex-
actly the same set of hypotheses as the original ma-
chine. Once again, for the parser, the states are a
set of weighted derivations, and the probability as-
signed to each arc is the normalized probability of
those derivations that traverse the arc.
Figure 3 shows the same set of hypotheses as
the previous machines, in the minimal lattice rep-
resentation. Again, there are four paths from the
initial state to the final state, corresponding to the
four hypotheses. Unlike the trees, the lattice has re-
entrances in the graph, at points where distinct hy-
potheses share the remainder of the string. Finally,
figure 4 shows the machine that results from taking
the minimal lattice and splitting the states so that ev-
ery path into a state has the same last word. Thus,
both paths that go through state 2 have &amp;quot;man&amp;quot; as the
last word of the path to that point. If we interpret
these states as independence assumptions, this lat-
</bodyText>
<figureCaption confidence="0.850268166666667">
Figure 3: Hypothesis strings encoded in the mini-
mal deterministic finite-state automaton, which rep-
resents a Markov assumption of order 0
Figure 4: Hypothesis strings encoded in a determin-
istic finite-state automaton, which has had the states
split to represent a Markov assumption of order l
</figureCaption>
<bodyText confidence="0.987745923076923">
tice is encoding a Markov assumption of order one.
Which brings us to the question of what a Markov
assumption is in the context of incremental parsing.
Perhaps this is easiest to see in terms of equivalence
classes. As mentioned above, a Markov assump-
tion of order n treats all strings with the same n
final words as equivalent. The set of parses asso-
ciated with the prefix string is the set of all parses
in the equivalence class, i.e. all parses for all strings
that have the same n final words. Since we are pri-
marily interested in the probabilities of these sets,
we can limit the sets to all parses with probability
greater than 0. Of course, for a given lattice of word
hypotheses, we do have some more restrictions on
this set; namely, that the only non-zero probability
derivations are for strings which are represented as
paths within the lattice. In other words, we do not
need to consider all possible strings in the equiva-
lence class, only those represented within the lattice
of hypotheses; all others have probability zero.
More formally, for a given Markov order k, let
D33 1k be the set of all partial derivations for prefix
strings cevy&amp;quot; for any a prefix to wi-1 in the
lat-
tice. Then the marginal probability Pm is defined,
analogously to equation 4, as
</bodyText>
<equation confidence="0.954707545454545">
E P(d) (6)
deDwi_i
j-k
As with equation 5, by definition:
PM (Wk)
Pm (w33I1k)
EdeD P (d)
w3
k
EdED )_i P (d)
3-k
</equation>
<bodyText confidence="0.999943">
This conditional probability can also be approxi-
mated with a beam-search parser.
Here is a simple description of how the arcs are
scored: the probability of being in a state is the
sum of the probabilities of all trees at that state;
and the conditional word probability is calculated by
traversing the arc (extending the parses) and normal-
izing the sum of the resulting tree probabilities with
the original state probability. Because of the beam-
search pruning, the amount of work required to tra-
verse a given arc under a Markov assumption will
be much less than traversing the set of outgoing arcs
when the states are split.
A brief note about the pruning. Following
Roark (2001a), our beam threshold is set according
to the highest probability parse that has successfully
integrated the current word. Candidate parses with a
high figure-of-merit are worked on first. When there
is an equivalence class, the set of competing parses
will contain parses with different terminal yields.
The figures-of-merit for parses can be determined
strictly by the parsing model and some look-ahead,
or perhaps the parsing and look-ahead models in
combination with the acoustic scores, which would
boost parses with higher likelihood given the signal.
In any case, parses with a low figure-of-merit will
be pruned by the parser. For the current study, we
only used the parsing and look-ahead models, and
allowed parses within an equivalence class to com-
pete in exactly the same way parses with the same
terminal yield compete, i.e. relative to the best scor-
ing candidate parse from the set.
Note that this formulation allows the parser to
rescore an arbitrary lattice, without placing restric-
tions on the manner in which the conditional prob-
abilities are calculated by the parser for any given
</bodyText>
<equation confidence="0.95203">
P (w k)
3—k
P(W
3 3—k
(7)
</equation>
<bodyText confidence="0.999987875">
parse. Since each parse has a specific terminal yield,
long-distance dependencies may be captured by the
conditional parsing model, even if they extend be-
yond the limit of the Markov assumption. One can-
not guarantee, however, that the lowest cost path is
contained within the parse trees at the final state.
In practical terms, the parser takes an arbitrary lat-
tice for processing. At each state in the lattice, it ac-
cumulates parses. Provided that the parser evaluates
the lattice in a topological ordering, it is ensured that
all incoming arcs are traversed for any given state
before any of its outgoing arcs are traversed. This
means that the entire set of parses is available at each
state when it is reached. Every outgoing arc is tra-
versed by the parser, adding to the set of parses at the
destination state, and allowing the arc to be scored
appropriately. The Markov assumption comes into
play in the structure of the lattice given to the parser.
The basic idea of following the topology of a
given lattice, as it is presented here, can be used
with any incremental derivation strategy, such as
the Structured Language Model (Chelba and Jelinek,
2000). In that model, they normalize their score over
the beam of derivations, rather than summing it, but
the same idea holds. Markov parsing is related to
the ability to calculate marginal string probabilities
from sets of partial derivations. A language model
such as that of Chamiak (2001) can only calculate
string probabilities from the set of complete deriva-
tions, since each derivation can generate the termi-
nals in a different order. Hence this model cannot
take advantage of our approach.
Since the parser operates on arbitrary lattices, we
can give the parser a tree of word hypotheses (i.e.
with no Markov assumption in the structure of the
automaton) or a minimized lattice that has had its
states split according to some desired Markov order.
Empirical results will be presented for n-best search
both with and without a Markov assumption, as well
as for word lattices without n-best extraction.
</bodyText>
<sectionHeader confidence="0.997627" genericHeader="categories and subject descriptors">
4 Empirical evaluation
</sectionHeader>
<bodyText confidence="0.9988552">
The first question that must be asked is what an ef-
fective Markov assumption is. To address this ques-
tion, we can evaluate the performance of the parser
on n-best hypotheses encoded as either a tree (i.e. no
re-entrances) or as a lattice with states that encode a
particular Markov order.
The parser that we used for these trials follows
the basic algorithm from Roark (2001a), as well as
modifications and tree transformations presented in
Roark (200 lb). The beam-search parameters that
are used are also the same as those used in the above
papers. Due to space constraints, we refer the reader
to those publications for parsing algorithm details.
Briefly, we use an incremental beam-search parser
that follows a top-down, leftmost derivation strat-
egy, and conditions rule probabilities on events from
the left-context, including non-terminal labels and
lexical heads. There is one word of look-ahead,
and parsing at each word continues until the beam
threshold is reached, at which point the parser moves
to the next word. Our parser differs from those pre-
sented in the cited papers in that it accepts lattices
and trees of hypotheses as input (encoded as finite-
state automata), and outputs weighted automata and
(optionally) parse trees.
We evaluated recognition performance for a
160,000-word vocabulary North American Busi-
ness News (NAB) task, specifically on the 300
utterance DARPA NAB Eval &apos;95 test set. We
took the lattice output of the multi-pass recognizer
(with acoustic model adaptation) that is presented
in Mohri et al. (to appear). The language model for
the lattices was a Katz backoff trigram (Katz, 1987)
trained on 250 million words. These lattices had an
oracle word error rate (WER) of 2.9 percent, and
a best-path WER of 10.6 percent. We pruned the
lattices using the AT&amp;T FSM tools (Mohri et al.,
2000) using a log cost threshold of 7. This resulted
in lattices with an oracle WER of 3.7 percent. We
then extracted the best paths from each lattice up to
1000 unique hypotheses, which resulted in an aver-
age of 549 per utterance, with 143 out of 300 lattices
containing 1000 unique hypotheses. The language
model score was stripped off, leaving the acoustic
model score ready to be combined with a new lan-
guage model score. The n-best lists were encoded
as deterministic trees, as well as minimized lattices
split to various Markov orders.
We trained the parsing model on sections 2-21 of
the WSJ Penn Treebank, plus the approximately 40
</bodyText>
<footnote confidence="0.9852505">
4This is obtained by using an oracle to find the path that
gives the lowest error rate.
</footnote>
<table confidence="0.999799">
Mixing None 8 7 Markov Assumption 2 1 0
Coeff. A 6 5 4 3
1.0 89.4
0.01 89.8 89.7 89.7 89.8 89.9 89.8 89.9 89.7 89.8 88.6
0.50 90.1 90.0 90.1 90.2 90.1 90.1 90.2 90.0 90.1 89.1
</table>
<tableCaption confidence="0.900106">
Table 1: Word accuracy for n-best re-ranking on the NAB Eval &apos;95 test set. The mixing coefficient A
represents how the trigram and parsing models are mixed together: AP+
</tableCaption>
<bodyText confidence="0.984491628571428">
-,,rigrarn (1-A)Pparser• The parsing
model is always mixed at some epsilon with the trigram, to ensure every arc receives a non-zero score.
million words in the BLLIP 1987-89 WSJ Corpus
Release5, plus our parser&apos;s best guess parses from
another 60 million words of WSJ from 1994-1996.
This total of 100 million words of treebank data
was normalized from text to speech formats (e.g.
numbers converted to how they would be read, etc.)
and used to train the stochastic parsing model that
was used to re-score the lattices produced as out-
lined above. Issues with conversion between tree-
bank and lattice tokenization were dealt with in the
standard way, as discussed in Chelba (2000) and
Roark (2001b).
The parser produces automata weighted with arc
probabilities. These probabilities were interpolated
with the trigram model6 that came with the lattices
according to the parameter A, so that the probabil-
ity of each arc was AP
- trigram + (1-A)Pparser • After
the language model mixing, the scores were con-
verted to negative log probability. The resulting
weighted automaton was intersected with the acous-
tically scored automaton, and the best path extracted
and evaluated.
Table 1 presents the Word Accuracy (100-WER)
for the parsing trials at A = 1.0, 0.01, and 0.5, which
represent trigram accuracy, parser model accuracy,
and mixed accuracy. Surprisingly, we can see that
even with a Markov assumption of order 1, we get
nearly the same performance as with no Markov
assumption. Figure 5 plots the accuracy at vari-
ous Markov orders alongside the non-Markov per-
formance, and figure 6 plots the accuracy versus the
number of arcs that must be traversed to score the
</bodyText>
<footnote confidence="0.999049">
5This corpus consists of the best guess parses from Eugene
Charniak&apos;s very high accuracy statistical parser (LDC Catalog
#LDC2000T43).
°Since the parser can garden path (and hence provide a zero
score to arcs), we always interpolate the parsing model with an
n-gram at some &gt; €.
</footnote>
<figureCaption confidence="0.753745">
Figure 5: Markov order versus word accuracy, for
n-best reranking with and without Markov assump-
tions, n=1000
</figureCaption>
<bodyText confidence="0.9994083125">
automaton, when it is encoded non-deterministically
(i.e. every path separately, as in figure 1), as a de-
terministic tree of hypotheses (as in figure 2) and
as lattices of various Markov orders. This is an
efficiency measure, to the extent that the arcs rep-
resent an equivalent amount of work. That is not
quite true, since the number of derivations at each
state can cause more or less work to traverse the arc,
given the particular beam-search heuristic, so figure
7 shows accuracy versus time per utterance for the
deterministic tree and the lattice. We can see that
it is more-or-less the same graph as figure 6. We
note that a minute per utterance may seem like a lot
of time, but recall that we are speaking of processing
up to a thousand hypothesis strings of business news
per utterance.
</bodyText>
<footnote confidence="0.9550305">
7We didn&apos;t actually parse the individual hypothesis strings,
since this is equivalent to the deterministic tree.
</footnote>
<figure confidence="0.995434880952381">
A.= 1.0 (3-gram alone)
-o- A = .01, Markov parsing
- - A.= .01, no Markov assumption
u = .50, Markov parsing
= .50, no Markov assumption
90
89.5- /
89
88.5
880 1 2 3 4 5
Markov order
91
90.5-
6 7 8
90
89.5Ct
90.5
91
0 Lattices of various orders
0 Deterministic tree of hypotheses
• Individual hypothesis strings
o°da
90.5
91
90
o Lattices of various orders
0 Deterministic tree of hypotheses
89
89
88.5
88.5
880
0.5
880
200
250
1.5 2 2.5
Arcs traversed
3 3.5 4
x 106
50 100 150
Seconds per utterance
</figure>
<figureCaption confidence="0.741048666666667">
Figure 6: Arcs traversed versus word accuracy, for
n-best reranking with and without Markov assump-
tions, n=1000
</figureCaption>
<bodyText confidence="0.99065464">
To summarize, we have shown that a small
Markov assumption results in the same accuracy
gains as parsing with no Markov assumption, with a
large efficiency gain over processing them as a tree
with no Markov assumption, and an even larger gain
than processing each string independently.
However, the principal gain in making a Markov
assumption when processing lattices is that one does
not need to perform n-best extraction in the first
place, but can re-score an arbitrary given lattice. The
number of distinct hypotheses that can be packed
into a relatively small lattice is very large. For ex-
ample, the lattices for this particular test set be-
fore n-best extraction contain, in aggregate, about
23.4 billion unique hypothesis strings, instead of the
300,000 maximum that we place by doing 1000-
best extraction. Evaluating all of these as individual
strings is not feasible, and even encoded as a tree
this is too costly; but we can rescore the lattices by
making a Markov assumption, as outlined above.
Table 2 gives the word accuracy for the parsing
model at A = 0.01 and 0.50 on the lattices, without
n-best extraction, under different Markov assump-
tions8. Recall that we are visiting and scoring ev-
ery arc in the lattices. The first thing to note is
</bodyText>
<footnote confidence="0.815893">
8Three of the lattices in the test set grew to over 1GB in
size when split to a Markov assumption of 6. For the results
presented, we used a Markov assumption of 5 for just those
three utterances in the 6 condition.
</footnote>
<figureCaption confidence="0.932426666666667">
Figure 7: Time in seconds per utterance versus
word accuracy, for n-best reranking with and with-
out Markov assumptions, n=1000
</figureCaption>
<bodyText confidence="0.999863310344827">
that the accuracy goes down relative to the n-best
re-scoring accuracy when the parsing model is pro-
viding most of the language model probability mass
(A = 0.01). This is un-surprising, because the n-best
extraction is done relative to the trigram scores, so
that the parsing model gets some extra information
from this other model, via pruning, about what are
good hypotheses and what are not-so-good. Notice
that when mixed with the trigram model at A = 0.5,
the scores are as good as with the n-best re-scoring.
That is, we are gaining back the points that were lost
by explicitly combining with the other model. It is
also worthwhile pointing out that the rather surpris-
ingly good performance at the smaller Markov or-
ders in the n-best trials also seems to be the result
of the n-best extraction, since we see more degra-
dation of performance as the Markov order drops to
that level without it.
One may wonder why we would want to do lat-
tice re-scoring without n-best extraction, if there is
no ultimate WER improvement as a result. There
are a couple of answers to this. First, the utility of a
weighted lattice is not simply in extracting the best
path from that lattice. Modern multi-pass recogni-
tion techniques involve acoustic and language model
adaptation to individual speakers, and these scored
lattices can serve as input into this adaptation pro-
cess. The better the posterior probabilities within the
lattice, the better the adaptation to the speaker. This
</bodyText>
<table confidence="0.99763525">
Mixing Markov Assumption
Coeff. A 6 5 4 3 2 1
0.01 89.6 89.3 89.4 89.3 89.2 89.1
0.50 90.2 90.2 90.0 90.1 89.7 89.6
</table>
<tableCaption confidence="0.988687">
Table 2: Word accuracy for lattice re-scoring on the
</tableCaption>
<bodyText confidence="0.918715909090909">
NAB Eval &apos;95 test set. The mixing coefficient A
represents how the trigram and parsing models are
mixed together: APtrigram (1-A)-Pparser •
allows the syntactic model to take part in the recog-
nition process earlier, which is likely to help in ways
that simple re-scoring or re-ranking cannot. Second,
these trials were performed on already adapted mod-
els in a domain with relatively good accuracy, so that
the n-best extraction with a large n is effective. In
other domains (e.g. Switchboard), and earlier in the
recognition process, such extraction is less effective.
</bodyText>
<sectionHeader confidence="0.99622" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999996375">
We have presented a generalization of an incremen-
tal statistical parsing algorithm which allows, via a
Markov assumption, the re-scoring of lattices with
the parsing model. We have shown that a surpris-
ingly small Markov order can yield the same word
accuracy as no Markov assumption when re-scoring
a lattice with a statistical parser, with large efficiency
gains. With such an algorithm, we can rescore lat-
tices which encode a very large number of unique
hypotheses. This approach allows the incorporation
of statistical parsers into a recognizer at a much ear-
lier stage of the process.
This paper is not intended to present &amp;quot;compelling&amp;quot;
evidence for an incremental parsing model over non-
incremental. In fact one could argue that, to the ex-
tent that all of these models — n-grams as well as
various statistical parsers — capture different kinds
of dependencies, they should be jointly used to
maximize performance, though perhaps at different
stages in the process.
Future work will include evaluating performance
when scores from the acoustic and/or n-gram mod-
els are incorporated for ranking competing candi-
date parses. Here we imposed specific Markov or-
ders for all states in all lattices; we would like to
investigate more sophisticated methods for deciding
when states should be split. Also, comparisons be-
tween this approach and A-star search for best path
extraction would be of interest. Finally, we will eval-
uate this approach with respect to how it improves
the acoustic model adaptation process, by including
the parsing scores earlier.
</bodyText>
<sectionHeader confidence="0.996122" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.982603">
Thanks to Mehryar Mohri and Michael Riley for
useful discussion on this topic.
</bodyText>
<sectionHeader confidence="0.997443" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999827225">
J. Chappelier, M. Rajman, R. Aragues, and A. Rozen-
knop. 1999. Lattice parsing for speech recognition.
In Proceedings of the Sixth confrence sur le Traitement
Automatique du Langage Naturel (TALN&apos;99), pages
95-104.
Eugene Charniak. 2001. Immediate-head parsing for
language models. In Proceedings of the 39th Annual
Meeting of the Association for Computational Linguis-
tics.
Ciprian Chelba and Frederick Jelinek. 2000. Structured
language modeling. Computer Speech and Language,
14(4):283-332.
Ciprian Chelba. 2000. Exploiting Syntactic Structure for
Natural Language Modeling. Ph.D. thesis, The Johns
Hopkins University.
Slava M. Katz. 1987. Estimation of probabilities from
sparse data for the language model component of a
speech recogniser. IEEE Transactions on Acoustic,
Speech, and Signal Processing, 35(3):400-401.
Bernd Kiefer, Hans-Ulrich Krieger, and Mark-Jan Neder-
hof. 2000. Efficient and robust parsing of word
graphs. In W. Wahlster, editor, Verbmobil: Founda-
tions of Speech-to-Speech Translation, pages 280-295.
Springer, Berlin.
Christopher Manning and Hinrich Schiitze. 1999. Foun-
dations of Statistical Natural Language Processing.
The MIT Press, Cambridge, MA.
Mehryar Mohri, Fernando C. N. Pereira, and Michael
Riley. 2000. The design principles of a weighted
finite-state transducer library. Theoretical Computer
Science, 231:17-32.
Mehryar Mohri, Fernando C. N. Pereira, and Michael Ri-
ley. to appear. Weighted finite-state transducers in
speech recognition. Computer Speech and Language.
Brian Roark. 2001a. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249-276.
Brian Roark. 200 lb. Robust Probabilistic Predictive
Syntactic Processing. Ph.D. thesis, Brown University.
http://arXiv.org/abs/c s/0105019.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.536785">
<note confidence="0.912972">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), Philadelphia, July 2002, pp. 287-294.</note>
<abstract confidence="0.991205511627907">that share the final n words to re-join. The Markov assumption is made by necessity, not because those who make it believe that there is such independence. In terms of equivalence classes, a Markov language model of order n stipulates that two prefix strings of words belong to the same equivalence class if their final n words are the same. The effect of such a model is that the conditioning information in the chain rule is truncated to include only the previous n-1 words. (W11WO) i=n Syntactic parsing has traditionally dealt with given strings, and is performed for the purpose of generating a parse structure, or many possible parse structures, for that given string. Parsing algorithms have been generalized to accept lattices of word hypotheses — e.g. Kiefer et al. (2000; Chappelier et al. (1999) — but these are chart parsing algorithms intended to efficiently yield parse trees, rather than rescore the arcs in the lattice. Chelba (2000) applied his parser incrementally to lattices for the purpose of scoring hypotheses, using A-star search to find the best hypotheses in the lattice. This approach treated the lattice as a tree of hypotheses, without exploiting the re-entrances in the structure of the lattice. The efficient search meant that some arcs in the lattice would not be visited. We propose a novel method for applying a parser to a lattice of word hypotheses that can take advantage of re-entrances in the lattice, and which allows the parser to visit and score every arc. It does this by making a Markov assumption akin to the kind of Markov assumption made by n-gram models. This is done, not because we feel that the independence assumptions are strictly speaking correct, but for the efficiency gains that they provide. Before discussing what a Markov assumption looks like for an incremental parser, let us first briefly outline how an incremental parser can serve as a language model. defines a probability distribution over strings of words as follows. Let Ty be the set of all</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Chappelier</author>
<author>M Rajman</author>
<author>R Aragues</author>
<author>A Rozenknop</author>
</authors>
<title>Lattice parsing for speech recognition.</title>
<date>1999</date>
<booktitle>In Proceedings of the Sixth confrence sur le Traitement Automatique du Langage Naturel (TALN&apos;99),</booktitle>
<pages>95--104</pages>
<contexts>
<context position="981" citStr="Chappelier et al. (1999)" startWordPosition="160" endWordPosition="163">stipulates that two prefix strings of words belong to the same equivalence class if their final n words are the same. The effect of such a model is that the conditioning information in the chain rule is truncated to include only the previous n-1 words. NUM P(WO)P (W11WO) P(Wn-1 lw2) 1-1P(wilwir,i) (2) i=n Syntactic parsing has traditionally dealt with given strings, and is performed for the purpose of generating a parse structure, or many possible parse structures, for that given string. Parsing algorithms have been generalized to accept lattices of word hypotheses — e.g. Kiefer et al. (2000; Chappelier et al. (1999) — but these are chart parsing algorithms intended to efficiently yield parse trees, rather than rescore the arcs in the lattice. Chelba (2000) applied his parser incrementally to lattices for the purpose of scoring hypotheses, using A-star search to find the best hypotheses in the lattice. This approach treated the lattice as a tree of hypotheses, without exploiting the re-entrances in the structure of the lattice. The efficient search meant that some arcs in the lattice would not be visited. We propose a novel method for applying a parser to a lattice of word hypotheses that can take advanta</context>
</contexts>
<marker>Chappelier, Rajman, Aragues, Rozenknop, 1999</marker>
<rawString>J. Chappelier, M. Rajman, R. Aragues, and A. Rozenknop. 1999. Lattice parsing for speech recognition. In Proceedings of the Sixth confrence sur le Traitement Automatique du Langage Naturel (TALN&apos;99), pages 95-104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Immediate-head parsing for language models.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<marker>Charniak, 2001</marker>
<rawString>Eugene Charniak. 2001. Immediate-head parsing for language models. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>Frederick Jelinek</author>
</authors>
<title>Structured language modeling.</title>
<date>2000</date>
<journal>Computer Speech and Language,</journal>
<pages>14--4</pages>
<contexts>
<context position="12249" citStr="Chelba and Jelinek, 2000" startWordPosition="2129" endWordPosition="2132">oming arcs are traversed for any given state before any of its outgoing arcs are traversed. This means that the entire set of parses is available at each state when it is reached. Every outgoing arc is traversed by the parser, adding to the set of parses at the destination state, and allowing the arc to be scored appropriately. The Markov assumption comes into play in the structure of the lattice given to the parser. The basic idea of following the topology of a given lattice, as it is presented here, can be used with any incremental derivation strategy, such as the Structured Language Model (Chelba and Jelinek, 2000). In that model, they normalize their score over the beam of derivations, rather than summing it, but the same idea holds. Markov parsing is related to the ability to calculate marginal string probabilities from sets of partial derivations. A language model such as that of Chamiak (2001) can only calculate string probabilities from the set of complete derivations, since each derivation can generate the terminals in a different order. Hence this model cannot take advantage of our approach. Since the parser operates on arbitrary lattices, we can give the parser a tree of word hypotheses (i.e. wi</context>
</contexts>
<marker>Chelba, Jelinek, 2000</marker>
<rawString>Ciprian Chelba and Frederick Jelinek. 2000. Structured language modeling. Computer Speech and Language, 14(4):283-332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
</authors>
<title>Exploiting Syntactic Structure for Natural Language Modeling.</title>
<date>2000</date>
<tech>Ph.D. thesis,</tech>
<institution>The Johns Hopkins University.</institution>
<contexts>
<context position="1124" citStr="Chelba (2000)" startWordPosition="186" endWordPosition="187">he conditioning information in the chain rule is truncated to include only the previous n-1 words. NUM P(WO)P (W11WO) P(Wn-1 lw2) 1-1P(wilwir,i) (2) i=n Syntactic parsing has traditionally dealt with given strings, and is performed for the purpose of generating a parse structure, or many possible parse structures, for that given string. Parsing algorithms have been generalized to accept lattices of word hypotheses — e.g. Kiefer et al. (2000; Chappelier et al. (1999) — but these are chart parsing algorithms intended to efficiently yield parse trees, rather than rescore the arcs in the lattice. Chelba (2000) applied his parser incrementally to lattices for the purpose of scoring hypotheses, using A-star search to find the best hypotheses in the lattice. This approach treated the lattice as a tree of hypotheses, without exploiting the re-entrances in the structure of the lattice. The efficient search meant that some arcs in the lattice would not be visited. We propose a novel method for applying a parser to a lattice of word hypotheses that can take advantage of re-entrances in the lattice, and which allows the parser to visit and score every arc. It does this by making a Markov assumption akin to</context>
<context position="16752" citStr="Chelba (2000)" startWordPosition="2891" endWordPosition="2892"> epsilon with the trigram, to ensure every arc receives a non-zero score. million words in the BLLIP 1987-89 WSJ Corpus Release5, plus our parser&apos;s best guess parses from another 60 million words of WSJ from 1994-1996. This total of 100 million words of treebank data was normalized from text to speech formats (e.g. numbers converted to how they would be read, etc.) and used to train the stochastic parsing model that was used to re-score the lattices produced as outlined above. Issues with conversion between treebank and lattice tokenization were dealt with in the standard way, as discussed in Chelba (2000) and Roark (2001b). The parser produces automata weighted with arc probabilities. These probabilities were interpolated with the trigram model6 that came with the lattices according to the parameter A, so that the probability of each arc was AP - trigram + (1-A)Pparser • After the language model mixing, the scores were converted to negative log probability. The resulting weighted automaton was intersected with the acoustically scored automaton, and the best path extracted and evaluated. Table 1 presents the Word Accuracy (100-WER) for the parsing trials at A = 1.0, 0.01, and 0.5, which represe</context>
</contexts>
<marker>Chelba, 2000</marker>
<rawString>Ciprian Chelba. 2000. Exploiting Syntactic Structure for Natural Language Modeling. Ph.D. thesis, The Johns Hopkins University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slava M Katz</author>
</authors>
<title>Estimation of probabilities from sparse data for the language model component of a speech recogniser.</title>
<date>1987</date>
<journal>IEEE Transactions on Acoustic, Speech, and Signal Processing,</journal>
<pages>35--3</pages>
<contexts>
<context position="14801" citStr="Katz, 1987" startWordPosition="2546" endWordPosition="2547">e next word. Our parser differs from those presented in the cited papers in that it accepts lattices and trees of hypotheses as input (encoded as finitestate automata), and outputs weighted automata and (optionally) parse trees. We evaluated recognition performance for a 160,000-word vocabulary North American Business News (NAB) task, specifically on the 300 utterance DARPA NAB Eval &apos;95 test set. We took the lattice output of the multi-pass recognizer (with acoustic model adaptation) that is presented in Mohri et al. (to appear). The language model for the lattices was a Katz backoff trigram (Katz, 1987) trained on 250 million words. These lattices had an oracle word error rate (WER) of 2.9 percent, and a best-path WER of 10.6 percent. We pruned the lattices using the AT&amp;T FSM tools (Mohri et al., 2000) using a log cost threshold of 7. This resulted in lattices with an oracle WER of 3.7 percent. We then extracted the best paths from each lattice up to 1000 unique hypotheses, which resulted in an average of 549 per utterance, with 143 out of 300 lattices containing 1000 unique hypotheses. The language model score was stripped off, leaving the acoustic model score ready to be combined with a ne</context>
</contexts>
<marker>Katz, 1987</marker>
<rawString>Slava M. Katz. 1987. Estimation of probabilities from sparse data for the language model component of a speech recogniser. IEEE Transactions on Acoustic, Speech, and Signal Processing, 35(3):400-401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Kiefer</author>
<author>Hans-Ulrich Krieger</author>
<author>Mark-Jan Nederhof</author>
</authors>
<title>Efficient and robust parsing of word graphs.</title>
<date>2000</date>
<booktitle>Verbmobil: Foundations of Speech-to-Speech Translation,</booktitle>
<pages>280--295</pages>
<editor>In W. Wahlster, editor,</editor>
<publisher>Springer,</publisher>
<location>Berlin.</location>
<contexts>
<context position="955" citStr="Kiefer et al. (2000" startWordPosition="156" endWordPosition="159">age model of order n stipulates that two prefix strings of words belong to the same equivalence class if their final n words are the same. The effect of such a model is that the conditioning information in the chain rule is truncated to include only the previous n-1 words. NUM P(WO)P (W11WO) P(Wn-1 lw2) 1-1P(wilwir,i) (2) i=n Syntactic parsing has traditionally dealt with given strings, and is performed for the purpose of generating a parse structure, or many possible parse structures, for that given string. Parsing algorithms have been generalized to accept lattices of word hypotheses — e.g. Kiefer et al. (2000; Chappelier et al. (1999) — but these are chart parsing algorithms intended to efficiently yield parse trees, rather than rescore the arcs in the lattice. Chelba (2000) applied his parser incrementally to lattices for the purpose of scoring hypotheses, using A-star search to find the best hypotheses in the lattice. This approach treated the lattice as a tree of hypotheses, without exploiting the re-entrances in the structure of the lattice. The efficient search meant that some arcs in the lattice would not be visited. We propose a novel method for applying a parser to a lattice of word hypoth</context>
</contexts>
<marker>Kiefer, Krieger, Nederhof, 2000</marker>
<rawString>Bernd Kiefer, Hans-Ulrich Krieger, and Mark-Jan Nederhof. 2000. Efficient and robust parsing of word graphs. In W. Wahlster, editor, Verbmobil: Foundations of Speech-to-Speech Translation, pages 280-295. Springer, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Manning</author>
<author>Hinrich Schiitze</author>
</authors>
<title>Foundations of Statistical Natural Language Processing.</title>
<date>1999</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Manning, Schiitze, 1999</marker>
<rawString>Christopher Manning and Hinrich Schiitze. 1999. Foundations of Statistical Natural Language Processing. The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
<author>Fernando C N Pereira</author>
<author>Michael Riley</author>
</authors>
<title>The design principles of a weighted finite-state transducer library.</title>
<date>2000</date>
<journal>Theoretical Computer Science,</journal>
<pages>231--17</pages>
<contexts>
<context position="15004" citStr="Mohri et al., 2000" startWordPosition="2581" endWordPosition="2584">ata and (optionally) parse trees. We evaluated recognition performance for a 160,000-word vocabulary North American Business News (NAB) task, specifically on the 300 utterance DARPA NAB Eval &apos;95 test set. We took the lattice output of the multi-pass recognizer (with acoustic model adaptation) that is presented in Mohri et al. (to appear). The language model for the lattices was a Katz backoff trigram (Katz, 1987) trained on 250 million words. These lattices had an oracle word error rate (WER) of 2.9 percent, and a best-path WER of 10.6 percent. We pruned the lattices using the AT&amp;T FSM tools (Mohri et al., 2000) using a log cost threshold of 7. This resulted in lattices with an oracle WER of 3.7 percent. We then extracted the best paths from each lattice up to 1000 unique hypotheses, which resulted in an average of 549 per utterance, with 143 out of 300 lattices containing 1000 unique hypotheses. The language model score was stripped off, leaving the acoustic model score ready to be combined with a new language model score. The n-best lists were encoded as deterministic trees, as well as minimized lattices split to various Markov orders. We trained the parsing model on sections 2-21 of the WSJ Penn T</context>
</contexts>
<marker>Mohri, Pereira, Riley, 2000</marker>
<rawString>Mehryar Mohri, Fernando C. N. Pereira, and Michael Riley. 2000. The design principles of a weighted finite-state transducer library. Theoretical Computer Science, 231:17-32.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Mehryar Mohri</author>
<author>Fernando C N Pereira</author>
<author>Michael Riley</author>
</authors>
<title>to appear. Weighted finite-state transducers in speech recognition. Computer Speech and Language.</title>
<marker>Mohri, Pereira, Riley, </marker>
<rawString>Mehryar Mohri, Fernando C. N. Pereira, and Michael Riley. to appear. Weighted finite-state transducers in speech recognition. Computer Speech and Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
</authors>
<title>Probabilistic top-down parsing and language modeling.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<pages>27--2</pages>
<contexts>
<context position="4500" citStr="Roark (2001" startWordPosition="784" endWordPosition="785"> of the new set, and normalizing this by the 2A leftmost derivation is a derivation in which the leftmost non-terminal is always expanded. (5) sighed man uh • man • sighed ° • mans 41 • man 0 hide sighed hide uh • mans • • the limn sighed mans F Figure 1: List of hypothesis strings encoded as aigure 2: Tree of hypothesis strings encoded as a non-deterministic finite-state automaton deterministic finite-state automaton probability of the original set. The conditional probability can be thought of as the amount of probability mass the parser must expend to incorporate the next word. As shown in Roark (2001a), this can be approximated with a top-down, leftmost beamsearch parser by simply performing the sums over the parses that have not been pruned. In practice, the pruned probability mass can be relatively low, yielding a snug lower bound on the joint probability of each string. 3 Markov parsing At this point we can begin thinking about how to assign probabilities in the manner outlined above to the arcs in a finite-state representation of the hypotheses. Figure 1 shows a list of distinct hypothesis strings for a particular utterance, encoded as a non-deterministic3 finite-state machine. In the</context>
<context position="9962" citStr="Roark (2001" startWordPosition="1747" endWordPosition="1748">arch parser. Here is a simple description of how the arcs are scored: the probability of being in a state is the sum of the probabilities of all trees at that state; and the conditional word probability is calculated by traversing the arc (extending the parses) and normalizing the sum of the resulting tree probabilities with the original state probability. Because of the beamsearch pruning, the amount of work required to traverse a given arc under a Markov assumption will be much less than traversing the set of outgoing arcs when the states are split. A brief note about the pruning. Following Roark (2001a), our beam threshold is set according to the highest probability parse that has successfully integrated the current word. Candidate parses with a high figure-of-merit are worked on first. When there is an equivalence class, the set of competing parses will contain parses with different terminal yields. The figures-of-merit for parses can be determined strictly by the parsing model and some look-ahead, or perhaps the parsing and look-ahead models in combination with the acoustic scores, which would boost parses with higher likelihood given the signal. In any case, parses with a low figure-of-</context>
<context position="13551" citStr="Roark (2001" startWordPosition="2348" endWordPosition="2349">had its states split according to some desired Markov order. Empirical results will be presented for n-best search both with and without a Markov assumption, as well as for word lattices without n-best extraction. 4 Empirical evaluation The first question that must be asked is what an effective Markov assumption is. To address this question, we can evaluate the performance of the parser on n-best hypotheses encoded as either a tree (i.e. no re-entrances) or as a lattice with states that encode a particular Markov order. The parser that we used for these trials follows the basic algorithm from Roark (2001a), as well as modifications and tree transformations presented in Roark (200 lb). The beam-search parameters that are used are also the same as those used in the above papers. Due to space constraints, we refer the reader to those publications for parsing algorithm details. Briefly, we use an incremental beam-search parser that follows a top-down, leftmost derivation strategy, and conditions rule probabilities on events from the left-context, including non-terminal labels and lexical heads. There is one word of look-ahead, and parsing at each word continues until the beam threshold is reached</context>
<context position="16768" citStr="Roark (2001" startWordPosition="2894" endWordPosition="2895">trigram, to ensure every arc receives a non-zero score. million words in the BLLIP 1987-89 WSJ Corpus Release5, plus our parser&apos;s best guess parses from another 60 million words of WSJ from 1994-1996. This total of 100 million words of treebank data was normalized from text to speech formats (e.g. numbers converted to how they would be read, etc.) and used to train the stochastic parsing model that was used to re-score the lattices produced as outlined above. Issues with conversion between treebank and lattice tokenization were dealt with in the standard way, as discussed in Chelba (2000) and Roark (2001b). The parser produces automata weighted with arc probabilities. These probabilities were interpolated with the trigram model6 that came with the lattices according to the parameter A, so that the probability of each arc was AP - trigram + (1-A)Pparser • After the language model mixing, the scores were converted to negative log probability. The resulting weighted automaton was intersected with the acoustically scored automaton, and the best path extracted and evaluated. Table 1 presents the Word Accuracy (100-WER) for the parsing trials at A = 1.0, 0.01, and 0.5, which represent trigram accur</context>
</contexts>
<marker>Roark, 2001</marker>
<rawString>Brian Roark. 2001a. Probabilistic top-down parsing and language modeling. Computational Linguistics, 27(2):249-276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
</authors>
<title>lb. Robust Probabilistic Predictive Syntactic Processing.</title>
<date></date>
<tech>Ph.D. thesis,</tech>
<institution>Brown University.</institution>
<note>http://arXiv.org/abs/c s/0105019.</note>
<marker>Roark, </marker>
<rawString>Brian Roark. 200 lb. Robust Probabilistic Predictive Syntactic Processing. Ph.D. thesis, Brown University. http://arXiv.org/abs/c s/0105019.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>