<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.99762">
A Projection Extension Algorithm for Statistical Machine Translation
</title>
<author confidence="0.87585">
Christoph Tillmann
</author>
<affiliation confidence="0.606642">
IBM T.J. Watson Research Center
</affiliation>
<address confidence="0.605742">
Yorktown Heights, NY 10598
</address>
<email confidence="0.99651">
ctill@us.ibm.com
</email>
<sectionHeader confidence="0.995595" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999858157894737">
In this paper, we describe a phrase-based
unigram model for statistical machine
translation that uses a much simpler set
of model parameters than similar phrase-
based models. The units of translation are
blocks – pairs of phrases. During decod-
ing, we use a block unigram model and a
word-based trigram language model. Dur-
ing training, the blocks are learned from
source interval projections using an un-
derlying high-precision word alignment.
The system performance is significantly
increased by applying a novel block exten-
sion algorithm using an additional high-
recall word alignment. The blocks are fur-
ther filtered using unigram-count selection
criteria. The system has been successfully
test on a Chinese-English and an Arabic-
English translation task.
</bodyText>
<sectionHeader confidence="0.999338" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9964715">
Various papers use phrase-based translation systems
(Och et al., 1999; Marcu and Wong, 2002; Ya-
mada and Knight, 2002) that have shown to improve
translation quality over single-word based transla-
tion systems introduced in (Brown et al., 1993). In
this paper, we present a similar system with a much
simpler set of model parameters. Specifically, we
compute the probability of a block sequence . A
block is a pair consisting of a contiguous source
and a contiguous target phrase. The block sequence
</bodyText>
<figureCaption confidence="0.798601">
Figure 1: A block sequence that jointly generates
</figureCaption>
<bodyText confidence="0.975117588235294">
target and source phrases. The example is actual
decoder output and the English translation is slightly
incorrect.
probability is decomposed into conditional
probabilities using the chain rule:
(1)
We try to find the block sequence that maximizes
: .The model pro-
posed is a joint model as in (Marcu and Wong,
2002), since target and source phrases are generated
jointly. The approach is illustrated in Figure 1. The
source phrases are given on the -axis and the target
phrases are given on the -axis. During block decod-
ing a bijection between source and target phrases is
generated. The two types of parameters in Eq 1 are
defined as:
Block unigram model : We compute un-
igram probabilities for the blocks. The blocks
are simpler than the alignment templates (Och
et al., 1999) in that they do not have an internal
structure.
Trigram language model: the probability
between adjacent blocks is com-
puted as the probability of the first target word
in the target clump of given the final two
words of the target clump of .
The exponent is set in informal experiments to be
. No other parameters such as distortion proba-
bilities are used.
To select blocks from training data, we compute
unigram block co-occurrence counts .
cannot be computed for all blocks in the training
data: we would obtain hundreds of millions of
blocks. The blocks are restricted by an underlying
word alignment. In this paper, we present a block
generation algorithm similar to the one in (Och et
al., 1999) in full detail: source intervals are pro-
jected into target intervals under a restriction derived
from a high-precision word alignment. The projec-
tion yields a set of high-precision block links. These
block links are further extended using a high-recall
word alignment. The block extension algorithm is
shown to improve translation performance signifi-
cantly. The system is tested on a Chinese-English
(CE) and an Arabic-English (AE) translation task.
The paper is structured as follows: in Section 2,
we present the baseline block generation algorithm.
The block extension approach is described in Sec-
tion 2.1. Section 3 describes a DP-based decoder
using blocks. Experimental results are presented in
Section 4.
</bodyText>
<sectionHeader confidence="0.70631" genericHeader="method">
2 Block Generation Algorithm
</sectionHeader>
<bodyText confidence="0.979528613636364">
Starting point for the block generation algorithm is
a word alignment obtained from an HMM Viterbi
training (Vogel et al., 1996). The HMM Viterbi
training is carried out twice with English as target
language and Chinese as source language and vice
versa. We obtain two alignment relations:
is an alignment function from source to
target positions and is an alignment func-
tion from target to source positions 1. We compute
the union and the intersection of the two alignment
relations and :
We call the intersection relation , because it rep-
resents a high-precision alignment, and the union
alignment , because it is taken to be a lower pre-
cision higher recall alignment (Och and Ney, 2000).
The intersection is also a (partial) bijection be-
tween the target and source positions: it covers the
same number of target and source positions and
there is a bijection between source and target po-
sitions that are covered. For the CE experiments
reported in Section 4 about % of the target and
source positions are covered by word links in , for
the AE experiments about % are covered. The ex-
tension algorithm presented assumes that ,
which is valid in this case since and are de-
rived from intersection and union. We introduce the
following additional piece of notation:
is the set of all source positions that are cov-
ered by some word links in , where the source po-
sitions are shown along the -axis and the target po-
sitions are shown along the -axis. To derive high-
precision block links from the high-precision word
links, we use the following projection definitions:
and
Here, projects source intervals into target in-
tervals. projects target intervals into source in-
tervals and is defined accordingly. Starting from the
high-precision word alignment , we try to derive a
high-precision block alignment: we project source
intervals , where . We compute
the minimum target index and maximum target
index for the word links that fall into the
1 and denote a source positions. and denote a target
positions.
</bodyText>
<figure confidence="0.99282525">
(2)
and
Target Target
Source
</figure>
<figureCaption confidence="0.994896">
Figure 2: The left picture shows three blocks that Target
</figureCaption>
<bodyText confidence="0.887352">
are learned from projecting three source intervals.
The right picture shows three blocks that cannot be
obtain from source interval projections .
</bodyText>
<tableCaption confidence="0.963427">
Table 1: Block learning algorithm using the inter-
section .
</tableCaption>
<bodyText confidence="0.562856571428571">
input: High-precision alignment
for each interval , where do
Extend block link ’out-
-wards’ using the algorithm in Table 2 and
add extended block link set to
output: Sentence block link set .
interval . This way, we obtain a mapping of
</bodyText>
<listItem confidence="0.5613475">
source intervals into target intervals:
(3)
</listItem>
<bodyText confidence="0.968876666666667">
The approach is illustrated in Figure 2, where in the
left picture, for example, the source interval
is projected into the target interval . The pair
defines a block alignment link . We
use this notation to emphasize that the identity of the
words is not used in the block learning algorithm. To
denote the block consisting of the target and source
words at the link positions, we write
where denote target words and denote source
words. denotes a function that maps intervals
Figure 3: One, two, three, or four word links in
lie on the frontier of a block. Additional word links
may be inside the block.
to the words at these intervals. The algorithm for
generating the high-precision block alignment links
is given in Table 1. The order in which the source
intervals are generated does not change the final link
set.
</bodyText>
<subsectionHeader confidence="0.999458">
2.1 Block Extension Algorithm
</subsectionHeader>
<bodyText confidence="0.9999395">
Empirically, we find that expanding the high-
precision block links significantly improves perfor-
mance. The expansion is parameterised and de-
scribed below. For a block link ,
we compute its frontier by looking at all word
links that lie on one of the four boundary lines of a
block. We make the following observation as shown
in Figure 3: the number of links (filled dots in the
picture) on the frontier is less or equal , since
in every column and row there is at most one link in
, which is a partial bijetion. To learn blocks from a
general word alignment that is not a bijection more
than word links may lie on the frontier of a block,
but to compute all possible blocks, it is sufficient to
look at all possible quadruples of word links. We
extend the links on the frontier by links of the high-
recall alignment , where we use a parameterised
way of locally extending a given word link. We com-
pute an extended link set by extending each word
link on the frontier separately and taking the union
of the resulting links. The way a word link is ex-
tended is illustrated in Figure 4. The filled dot in
the center of the picture is an element of the high-
precision set . Starting from this link, we look for
</bodyText>
<figure confidence="0.96836805">
:
Target
Source
Target
Source Source
Target
Source
Source
−2 −1 0 +1 +2
l3
l2
����l
1
delta=1
delta=2
+2
+1
0
−1
−2
</figure>
<figureCaption confidence="0.9747755">
Figure 4: Point extension scheme. Solid word links
lie in , striped word links lie in .
</figureCaption>
<figure confidence="0.922435">
Target
Source
</figure>
<figureCaption confidence="0.968265">
Figure 5: ’Outward’ extension of a high-precision
block link.
</figureCaption>
<bodyText confidence="0.998547222222222">
extensions in its neighborhood that lie in , where
the neighborhood is defined by a cell width parame-
ter and a distance parameter . For instance, link
in Figure 4 is reached with cell width and
distance , the link is reached with and
, the link is reached with and .
The word link is added to and it is itself extended
using the same scheme. Here, we never make use
of a row or a column covered by other than the
rows and and the columns and . Also, we
do not cross such a row or column using an exten-
sion with : this way only a small fraction of the
word links in is used for extending a single block
link. The extensions are carried out iteratively until
no new alignment links from are added to . The
block extension algorithm in Table 2 uses the exten-
sion set to generate all word link quadruples: the
extended block that is defined by a given quadru-
</bodyText>
<tableCaption confidence="0.973706">
Table 2: Block link extension algorithm. The
</tableCaption>
<bodyText confidence="0.691142">
and function compute the minimum and the
maximum of integer values.
</bodyText>
<table confidence="0.9784302">
input: Block link
Compute extension set from frontier
for each
if (
output: Extended block link set .
</table>
<bodyText confidence="0.99623862745098">
ple is generated and a check is carried out whether
includes the seed block link . The following defi-
nition for block link inclusion is used:
where the block is said to be
included in .
holds iff and . The ’seed’ block link
is extended ’outwardly’: all extended blocks in-
clude the high-precision block . The block link
itself may be included in other high-precision block
links on its part, but holds. An ex-
tended block derived from the block never vio-
lates the projection restriction relative to i.e., we
do not have to re-check the projection restriction for
any generated block, which simplifies and fastens up
the generation algorithm. The approach is illustrated
in Figure 5, where a high-precision block with ele-
ments on its frontier is extended by two blocks con-
taining it.
The block link extension algorithm produces block
links that contain new source and target intervals
and that extend the interval mapping
in Eq. 3. This mapping is no longer a function, but
rather a relation between source and target intervals
i.e., a single source interval is mapped to several tar-
get intervals and vice versa. The extended block set
constitutes a subset of the following set of interval
and
pairs:
The set of high-precision blocks is contained in this
set. We cannot use the entire set of blocks defined
by all pairs in the above relation, the resulting set
of blocks cannot be handled due to memory restric-
tions, which motivates our extension algorithm. We
also tried the following symmetric restriction and
tested the resulting block set:
The modified restriction is implemented in the con-
text of the extension scheme in Table 1 by insert-
ing an if statement before the alignment link is
extended: the alignment link is extended only if the
restriction also holds.
Considering only block links for which the two way
projection in Eq. 4 holds has the following inter-
esting interpretation: assuming a bijection that is
complete i.e., all source and target positions are cov-
ered, an efficient block segmentation algorithm ex-
ists to compute a Viterbi block alignment as in Fig-
ure 1 for a given training sentence pair. The com-
plexity of the algorithm is quadratic in the length
of the source sentence. This dynamic programming
technique is not used in the current block selection
but might be used in future work.
</bodyText>
<subsectionHeader confidence="0.998079">
2.2 Unigram Block Selection
</subsectionHeader>
<bodyText confidence="0.998812142857143">
For selecting blocks from the candidate block links,
we restrict ourselves to block links where target and
source phrases are equal or less than words long.
This way we obtain some tens of millions of blocks
on our training data including blocks that occur only
once. This baseline set is further filtered using the
unigram count
</bodyText>
<footnote confidence="0.944759">
2To apply the restrictions exhaustively, we have imple-
</footnote>
<figureCaption confidence="0.76070325">
mented tree-based data structures to store up to million
blocks with phrases of up to length in less than gigabyte
of RAM.
Figure 6: An example of recursively nested blocks
</figureCaption>
<bodyText confidence="0.997553384615385">
.
as relative frequency over all selected blocks.
An example of blocks obtained from the Chinese-
English training data is shown in Figure 6. ’$DATE’
is a placeholder for a date expression. Block con-
tains the blocks to . All blocks are selected
in training: the unigram decoder prefers even if
, , and are much more frequent. The solid
word links are word links in , the striped word
links are word links in . Using the links in , we
can learn one-to-many block translations, e.g. the
pair ( ,’Xinhua news agency’) is learned from the
training data.
</bodyText>
<sectionHeader confidence="0.998239" genericHeader="method">
3 DP-based Decoder
</sectionHeader>
<bodyText confidence="0.982873">
We use a DP-based beam search procedure similar
to the one presented in (Tillmann and Ney, 2003).
We maximize over all block segmentations for
which the source phrases yield a segmentation of the
input source sentence, generating the target sentence
simultaneously. The decoder processes search states
of the following form:
and are the two predecessor words used for the
trigram language model, is the so-called cover-
age vector to keep track of the already processed
source position, is the last processed source po-
sition. is the source phrase length of the block
and (4)
: denotes the set of blocks
for which . For our Chinese-English
experiments, we use the restriction as our base-
line, and for the Arabic-English experiments the
restriction. Blocks where the target and the source
clump are of length are kept regardless of their
count2. We compute the unigram probability
</bodyText>
<tableCaption confidence="0.998103">
Table 3: Effect of the extension scheme on the
</tableCaption>
<table confidence="0.93802625">
CE translation experiments.
Scheme # blocks # blocks BLEUr4n4
41.88 M 6.53 M 0.148 0.01
14.77 M 2.67 M 0.160 0.01
24.47 M 4.50 M 0.180 0.01
35.23 M 6.18 M 0.183 0.01
37.92 M 6.65 M 0.183 0.01
45.81 M 7.66 M 0.181 0.01
</table>
<tableCaption confidence="0.987195">
Table 4: Effect of the unigram threshold on the
BLEU score. The maximum phrase length is .
</tableCaption>
<table confidence="0.999503625">
Selection # blocks BLEUr4n4
Restriction selected
N2 6.18 M 0.183 0.01
N3 1.69 M 0.185 0.01
N5 0.85 M 0.178 0.01
N10 0.45 M 0.176 0.01
N25 0.26 M 0.166 0.01
N100 0.18 M 0.154 0.01
</table>
<bodyText confidence="0.994785045454545">
currently being matched. is the length of the ini-
tial fragment of the source phrase that has been pro-
cessed so far. is smaller or equal : . Note,
that the partial hypotheses are not distinguished ac-
cording to the identity of the block itself. The de-
coder processes the input sentence ’cardinality syn-
chronously’: all partial hypotheses that are active at
a given point cover the same number of input sen-
tence words. The same beam-search pruning as de-
scribed in (Tillmann and Ney, 2003) is used. The
so-called observation pruning threshold is modified
as follows: for each source interval that is being
matched by a block source phrase at most the best
target phrases according to the joint unigram proba-
bility are hypothesized. The list of blocks that cor-
respond to a matched source interval is stored in a
chart for each input sentence. This way the match-
ing is carried out only once for all partial hypotheses
that try to match the same input sentence interval.
In the current experiments, decoding without block
re-ordering yields the best translation results. The
decoder translates about words per second.
</bodyText>
<sectionHeader confidence="0.999206" genericHeader="method">
4 Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.998718">
4.1 Chinese-English Experiments
</subsectionHeader>
<bodyText confidence="0.997896285714286">
The translation system is tested on a Chinese-to-
English translation task. For testing, we use the
DARPA/NIST MT 2001 dry-run testing data, which
consists of sentences with words ar-
ranged in documents 3. The training data is pro-
vided by the LDC and labeled by NIST as the Large
Data condition for the MT 2002 evaluation. The
</bodyText>
<footnote confidence="0.7171865">
3We removed the first documents that are contained in
the training data.
</footnote>
<bodyText confidence="0.9870424375">
Chinese sentences are segmented into words. The
training data contains million Chinese and
million English words. The block selection algo-
rithm described below runs less than one hour on
a single -Gigahertz linux machine.
Table 3 presents results for various block extension
schemes. The first column describes the extension
scheme used. The second column reports the total
number of blocks in millions collected - including
all the blocks that occurred only once. The third
column reports the number of blocks that occurred
at least twice. These blocks are used to compute the
results in the fourth column: the BLEU score (Pa-
pineni et al., 2002) with reference translation us-
ing -grams along with 95% confidence interval is
reported 4. Line and line of this table show re-
sults where only the source interval projection with-
out any extension is carried out. For the ex-
tension scheme, the high-recall union set itself is
used for projection. The results are worse than for
all other schemes, since a lot of smaller blocks are
discarded due to the projection approach. The
scheme, where just the word links are used is too
restrictive leaving out bigger blocks that are admis-
sible according to . For the Chinese-English test
data, there is only a minor difference between the
different extension schemes, the best results are ob-
tained for the and the extension schemes.
Table 4 shows the effect of the unigram selection
threshold, where the blocks are used. The sec-
ond column shows the number of blocks selected.
The best results are obtained for the and the
</bodyText>
<footnote confidence="0.981364666666667">
4The test data is split into a certain number of subsets. The
BLEU score is computed on each subset. We use the t-test to
compare these scores.
</footnote>
<bodyText confidence="0.998843511111111">
sets. The number of blocks can be reduced dras-
tically where the translation performance declines
only gradually.
Table 5 shows the effect of the maximum phrase
length on the BLEU score for the block set. In-
cluding blocks with longer phrases actually helps to
improve performance, although already a length of
obtains nearly identical results.
We carried out the following control experiments
(using as threshold): we obtained a block
set of million blocks by generating blocks from
all quadruples of word links in 5. This set is a
proper superset of the blocks learned for the
experiment in Table 3. The resulting BLEU score
is . Including additional smaller blocks even
hurts translation performance in this case. Also, for
the extension scheme , we carried out the in-
verse projection as described in Section 2.1 to obtain
a block set of million blocks and a BLEU score
of . This number is smaller than the BLEU
score of for the restriction: for the trans-
lation direction Chinese-to-English, selecting blocks
with longer English phrases seems to be important
for good translation performance. It is interesting
to note, that the unigram translation model is sym-
metric: the translation direction can be switched to
English-to-Chinese without re-training the model -
just a new Chinese language model is needed. Our
experiments, though, show that there is an unbalance
with respect to the projection direction that has a sig-
nificant influence on the translation results. Finally,
we carried out an experiment where we used the
block set as a baseline. The extension algorithm was
applied only to blocks of target and source length
producing one-to-many translations, e.g. the blocks
and in Figure 6. The BLEU score improved
to with a block set of million blocks. It
seems to be important to carry out the block exten-
sion also for larger blocks.
We also ran the N2 system on the June 2002 DARPA
TIDES Large Data evaluation test set. Six re-
search sites and four commercial off-the-shelf sys-
tems were evaluated in Large Data track. A major-
ity of the systems were phrase-based translation sys-
tems. For comparison with other sites, we quote the
</bodyText>
<footnote confidence="0.898025666666667">
5We cannot compute the block set resulting from all word
link quadruples in , which is much bigger, due to CPU and
memory restrictions.
</footnote>
<tableCaption confidence="0.502814">
Table 5: Effect of the maximum phrase length on
the BLEU score. Both target and source phrase are
shorted than the maximum. The unigram threshold
</tableCaption>
<table confidence="0.945380363636364">
is .
maximum # blocks BLEUr4n4
phrase length selected
8 6.18 M 0.183 0.01
7 5.60 M 0.182 0.01
6 4.97 M 0.182 0.01
5 4.25 M 0.179 0.01
4 3.40 M 0.178 0.01
3 2.34 M 0.167 0.01
2 1.07 M 0.150 0.01
1 0.16 M 0.118 0.01
</table>
<tableCaption confidence="0.935553">
Table 6: Effect of the extension scheme on the
</tableCaption>
<table confidence="0.774503375">
AE translation experiments.
Scheme # blocks # blocks BLEUr3n4
79.0 M 6.79 M 0.209 0.03
96.6 M 8.29 M 0.223 0.03
113.16 M 9.87 M 0.232 0.03
NIST score (Doddington, 2002) on this test set: the
N2 system scores 7.56 whereas the official top two
systems scored 7.65 and 7.34 respectively.
</table>
<subsectionHeader confidence="0.939698">
4.2 Arabic-English Experiments
</subsectionHeader>
<bodyText confidence="0.99066708">
We also carried out experiments for the translation
direction Arabic to English using training data from
UN documents. For testing, we use a test set of
sentences with words arranged in docu-
ments The training data contains million Ara-
bic and million English words. The train-
ing data is pre-processed using some morphologi-
cal analysis. For the Arabic experiments, we have
tested the extension schemes ,
is split into several chunks of training sen-
tence pairs each, and the final block set together with
the unigram count is obtained by merging the block
, and
as shown in Table 6. Here, the results for the differ-
ent schemes differ significantly and the scheme
produces the best results. For the AE experiments,
only blocks up to a phrase length of are computed
due to disk memory restrictions. The training data
files for each of the chunks written onto disk mem-
ory. The word-to-word alignment is trained using
iterations of the IBM Model training followed
by iterations of the HMM Viterbi training. This
training procedure takes about a day to execute on a
single machine. Additionally, the overall block se-
lection procedure takes about hours to execute.
</bodyText>
<sectionHeader confidence="0.999065" genericHeader="method">
5 Previous Work
</sectionHeader>
<bodyText confidence="0.999906875">
Block-based translation units are used in several pa-
pers on statistical machine translation. (Och et al.,
1999) describe the alignment template system for
statistical MT: alignment templates correspond to
blocks that do have an internal structure. Marcu
and Wong (2002) use a joint probability model for
blocks where the clumps are contiguous phrases as
in this paper. Yamada and Knight (2002) presents
a decoder for syntax-based MT that uses so-called
phrasal translation units that correspond to blocks.
Block unigram counts are used to filter the blocks.
The phrasal model is included into a syntax-based
model. Projection of phrases has also been used in
(Yarowsky et al., 2001). A word link extension al-
gorithm similar to the one presented in this paper is
given in (Koehn et al., 2003).
</bodyText>
<sectionHeader confidence="0.999578" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999968777777778">
In this paper, we describe a block-based unigram
model for SMT. A novel block learning algorithm is
presented that extends high-precision interval pro-
jections by elements from a high-recall alignment.
The extension method is shown to improve transla-
tion performance significantly. For the Chinese-to-
English task, we obtained a NIST score of on
the June 2002 DARPA TIDES Large Data evalua-
tion test set.
</bodyText>
<sectionHeader confidence="0.997514" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99902575">
This work was partially supported by DARPA and
monitored by SPAWAR under contract No. N66001-
99-2-8916. The paper has greatly profited from dis-
cussion with Fei Xia and Kishore Papineni.
</bodyText>
<sectionHeader confidence="0.999207" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99981162962963">
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The Mathematics
of Statistical Machine Translation: Parameter Estima-
tion. Computational Linguistics, 19(2):263–311.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Proc. of the Second International Confer-
ence ofHuman Language Technology Research, pages
138–145, March.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Proc.
of the HLT-NAACL 2003 conference, pages 127–133,
Edmonton, Alberta, Canada, May.
Daniel Marcu and William Wong. 2002. A Phrased-
Based, Joint Probability Model for Statistical Machine
Translation. In Proc. of the Conf. on Empirical Meth-
ods in Natural Language Processing (EMNLP 02),
pages 133–139, Philadelphia, PA, July.
Franz-Josef Och and Hermann Ney. 2000. Improved Sta-
tistical Alignment Models. In Proc. ofthe 38th Annual
Meeting of the Association of Computational Linguis-
tics (ACL 2000), pages 440–447, Hong-Kong, China,
October.
Franz-Josef Och, Christoph Tillmann, and Hermann Ney.
1999. Improved Alignment Models for Statistical Ma-
chine Translation. In Proc. of the Joint Conf. on Em-
pirical Methods in Natural Language Processing and
Very Large Corpora (EMNLP/VLC 99), pages 20–28,
College Park, MD, June.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of machine translation. In Proc. of the
40th Annual Conf. of the Association for Computa-
tional Linguistics (ACL 02), pages 311–318, Philadel-
phia, PA, July.
Christoph Tillmann and Hermann Ney. 2003. Word Re-
ordering and a DP Beam Search Algorithm for Statis-
tical Machine Translation. Computational Linguistics,
29(1):97–133.
Stefan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM BasedWord Alignment in Statistical Ma-
chine Translation. In Proc. of the 16th Int. Conf.
on Computational Linguistics (COLING 1996), pages
836–841, Copenhagen, Denmark, August.
Kenji Yamada and Kevin Knight. 2002. A Decoder for
Syntax-based Statistical MT. In Proc. of the 40th An-
nual Conf. of the Association for Computational Lin-
guistics (ACL 02), pages 303–310, Philadelphia, PA,
July.
David Yarowsky, Grace Ngai, and Richard Wicentowski.
2001. Inducing Multilingual Text Analysis tools via
Robust Projection across Aligned Corpora. In Proc. of
the HLT 2001 conference, pages 161–168, San Diego,
CA, March.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.734653">
<title confidence="0.999926">A Projection Extension Algorithm for Statistical Machine Translation</title>
<author confidence="0.995711">Christoph</author>
<affiliation confidence="0.8971615">IBM T.J. Watson Research Yorktown Heights, NY</affiliation>
<email confidence="0.999046">ctill@us.ibm.com</email>
<abstract confidence="0.9963654">In this paper, we describe a phrase-based unigram model for statistical machine translation that uses a much simpler set of model parameters than similar phrasebased models. The units of translation are blocks – pairs of phrases. During decoding, we use a block unigram model and a word-based trigram language model. During training, the blocks are learned from source interval projections using an underlying high-precision word alignment. The system performance is significantly increased by applying a novel block extension algorithm using an additional highrecall word alignment. The blocks are further filtered using unigram-count selection criteria. The system has been successfully test on a Chinese-English and an Arabic- English translation task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The Mathematics of Statistical Machine Translation: Parameter Estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1184" citStr="Brown et al., 1993" startWordPosition="173" endWordPosition="176">ing an underlying high-precision word alignment. The system performance is significantly increased by applying a novel block extension algorithm using an additional highrecall word alignment. The blocks are further filtered using unigram-count selection criteria. The system has been successfully test on a Chinese-English and an ArabicEnglish translation task. 1 Introduction Various papers use phrase-based translation systems (Och et al., 1999; Marcu and Wong, 2002; Yamada and Knight, 2002) that have shown to improve translation quality over single-word based translation systems introduced in (Brown et al., 1993). In this paper, we present a similar system with a much simpler set of model parameters. Specifically, we compute the probability of a block sequence . A block is a pair consisting of a contiguous source and a contiguous target phrase. The block sequence Figure 1: A block sequence that jointly generates target and source phrases. The example is actual decoder output and the English translation is slightly incorrect. probability is decomposed into conditional probabilities using the chain rule: (1) We try to find the block sequence that maximizes : .The model proposed is a joint model as in (M</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
</authors>
<title>Automatic evaluation of machine translation quality using n-gram co-occurrence statistics.</title>
<date>2002</date>
<booktitle>In Proc. of the Second International Conference ofHuman Language Technology Research,</booktitle>
<pages>138--145</pages>
<contexts>
<context position="20667" citStr="Doddington, 2002" startWordPosition="3560" endWordPosition="3561">gger, due to CPU and memory restrictions. Table 5: Effect of the maximum phrase length on the BLEU score. Both target and source phrase are shorted than the maximum. The unigram threshold is . maximum # blocks BLEUr4n4 phrase length selected 8 6.18 M 0.183 0.01 7 5.60 M 0.182 0.01 6 4.97 M 0.182 0.01 5 4.25 M 0.179 0.01 4 3.40 M 0.178 0.01 3 2.34 M 0.167 0.01 2 1.07 M 0.150 0.01 1 0.16 M 0.118 0.01 Table 6: Effect of the extension scheme on the AE translation experiments. Scheme # blocks # blocks BLEUr3n4 79.0 M 6.79 M 0.209 0.03 96.6 M 8.29 M 0.223 0.03 113.16 M 9.87 M 0.232 0.03 NIST score (Doddington, 2002) on this test set: the N2 system scores 7.56 whereas the official top two systems scored 7.65 and 7.34 respectively. 4.2 Arabic-English Experiments We also carried out experiments for the translation direction Arabic to English using training data from UN documents. For testing, we use a test set of sentences with words arranged in documents The training data contains million Arabic and million English words. The training data is pre-processed using some morphological analysis. For the Arabic experiments, we have tested the extension schemes , is split into several chunks of training sentence </context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>George Doddington. 2002. Automatic evaluation of machine translation quality using n-gram co-occurrence statistics. In Proc. of the Second International Conference ofHuman Language Technology Research, pages 138–145, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical Phrase-Based Translation.</title>
<date>2003</date>
<booktitle>In Proc. of the HLT-NAACL 2003 conference,</booktitle>
<pages>127--133</pages>
<location>Edmonton, Alberta, Canada,</location>
<contexts>
<context position="22778" citStr="Koehn et al., 2003" startWordPosition="3907" endWordPosition="3910">lignment templates correspond to blocks that do have an internal structure. Marcu and Wong (2002) use a joint probability model for blocks where the clumps are contiguous phrases as in this paper. Yamada and Knight (2002) presents a decoder for syntax-based MT that uses so-called phrasal translation units that correspond to blocks. Block unigram counts are used to filter the blocks. The phrasal model is included into a syntax-based model. Projection of phrases has also been used in (Yarowsky et al., 2001). A word link extension algorithm similar to the one presented in this paper is given in (Koehn et al., 2003). 6 Conclusion In this paper, we describe a block-based unigram model for SMT. A novel block learning algorithm is presented that extends high-precision interval projections by elements from a high-recall alignment. The extension method is shown to improve translation performance significantly. For the Chinese-toEnglish task, we obtained a NIST score of on the June 2002 DARPA TIDES Large Data evaluation test set. Acknowledgements This work was partially supported by DARPA and monitored by SPAWAR under contract No. N66001- 99-2-8916. The paper has greatly profited from discussion with Fei Xia a</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical Phrase-Based Translation. In Proc. of the HLT-NAACL 2003 conference, pages 127–133, Edmonton, Alberta, Canada, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>William Wong</author>
</authors>
<title>A PhrasedBased, Joint Probability Model for Statistical Machine Translation.</title>
<date>2002</date>
<booktitle>In Proc. of the Conf. on Empirical Methods in Natural Language Processing (EMNLP 02),</booktitle>
<pages>133--139</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="1033" citStr="Marcu and Wong, 2002" startWordPosition="149" endWordPosition="152">oding, we use a block unigram model and a word-based trigram language model. During training, the blocks are learned from source interval projections using an underlying high-precision word alignment. The system performance is significantly increased by applying a novel block extension algorithm using an additional highrecall word alignment. The blocks are further filtered using unigram-count selection criteria. The system has been successfully test on a Chinese-English and an ArabicEnglish translation task. 1 Introduction Various papers use phrase-based translation systems (Och et al., 1999; Marcu and Wong, 2002; Yamada and Knight, 2002) that have shown to improve translation quality over single-word based translation systems introduced in (Brown et al., 1993). In this paper, we present a similar system with a much simpler set of model parameters. Specifically, we compute the probability of a block sequence . A block is a pair consisting of a contiguous source and a contiguous target phrase. The block sequence Figure 1: A block sequence that jointly generates target and source phrases. The example is actual decoder output and the English translation is slightly incorrect. probability is decomposed in</context>
<context position="22256" citStr="Marcu and Wong (2002)" startWordPosition="3819" endWordPosition="3822">r each of the chunks written onto disk memory. The word-to-word alignment is trained using iterations of the IBM Model training followed by iterations of the HMM Viterbi training. This training procedure takes about a day to execute on a single machine. Additionally, the overall block selection procedure takes about hours to execute. 5 Previous Work Block-based translation units are used in several papers on statistical machine translation. (Och et al., 1999) describe the alignment template system for statistical MT: alignment templates correspond to blocks that do have an internal structure. Marcu and Wong (2002) use a joint probability model for blocks where the clumps are contiguous phrases as in this paper. Yamada and Knight (2002) presents a decoder for syntax-based MT that uses so-called phrasal translation units that correspond to blocks. Block unigram counts are used to filter the blocks. The phrasal model is included into a syntax-based model. Projection of phrases has also been used in (Yarowsky et al., 2001). A word link extension algorithm similar to the one presented in this paper is given in (Koehn et al., 2003). 6 Conclusion In this paper, we describe a block-based unigram model for SMT.</context>
</contexts>
<marker>Marcu, Wong, 2002</marker>
<rawString>Daniel Marcu and William Wong. 2002. A PhrasedBased, Joint Probability Model for Statistical Machine Translation. In Proc. of the Conf. on Empirical Methods in Natural Language Processing (EMNLP 02), pages 133–139, Philadelphia, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz-Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved Statistical Alignment Models.</title>
<date>2000</date>
<booktitle>In Proc. ofthe 38th Annual Meeting of the Association of Computational Linguistics (ACL</booktitle>
<pages>440--447</pages>
<location>Hong-Kong, China,</location>
<contexts>
<context position="4398" citStr="Och and Ney, 2000" startWordPosition="708" endWordPosition="711">btained from an HMM Viterbi training (Vogel et al., 1996). The HMM Viterbi training is carried out twice with English as target language and Chinese as source language and vice versa. We obtain two alignment relations: is an alignment function from source to target positions and is an alignment function from target to source positions 1. We compute the union and the intersection of the two alignment relations and : We call the intersection relation , because it represents a high-precision alignment, and the union alignment , because it is taken to be a lower precision higher recall alignment (Och and Ney, 2000). The intersection is also a (partial) bijection between the target and source positions: it covers the same number of target and source positions and there is a bijection between source and target positions that are covered. For the CE experiments reported in Section 4 about % of the target and source positions are covered by word links in , for the AE experiments about % are covered. The extension algorithm presented assumes that , which is valid in this case since and are derived from intersection and union. We introduce the following additional piece of notation: is the set of all source p</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz-Josef Och and Hermann Ney. 2000. Improved Statistical Alignment Models. In Proc. ofthe 38th Annual Meeting of the Association of Computational Linguistics (ACL 2000), pages 440–447, Hong-Kong, China, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz-Josef Och</author>
<author>Christoph Tillmann</author>
<author>Hermann Ney</author>
</authors>
<title>Improved Alignment Models for Statistical Machine Translation.</title>
<date>1999</date>
<booktitle>In Proc. of the Joint Conf. on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC 99),</booktitle>
<pages>20--28</pages>
<location>College Park, MD,</location>
<contexts>
<context position="1011" citStr="Och et al., 1999" startWordPosition="145" endWordPosition="148">hrases. During decoding, we use a block unigram model and a word-based trigram language model. During training, the blocks are learned from source interval projections using an underlying high-precision word alignment. The system performance is significantly increased by applying a novel block extension algorithm using an additional highrecall word alignment. The blocks are further filtered using unigram-count selection criteria. The system has been successfully test on a Chinese-English and an ArabicEnglish translation task. 1 Introduction Various papers use phrase-based translation systems (Och et al., 1999; Marcu and Wong, 2002; Yamada and Knight, 2002) that have shown to improve translation quality over single-word based translation systems introduced in (Brown et al., 1993). In this paper, we present a similar system with a much simpler set of model parameters. Specifically, we compute the probability of a block sequence . A block is a pair consisting of a contiguous source and a contiguous target phrase. The block sequence Figure 1: A block sequence that jointly generates target and source phrases. The example is actual decoder output and the English translation is slightly incorrect. probab</context>
<context position="2266" citStr="Och et al., 1999" startWordPosition="357" endWordPosition="360">robabilities using the chain rule: (1) We try to find the block sequence that maximizes : .The model proposed is a joint model as in (Marcu and Wong, 2002), since target and source phrases are generated jointly. The approach is illustrated in Figure 1. The source phrases are given on the -axis and the target phrases are given on the -axis. During block decoding a bijection between source and target phrases is generated. The two types of parameters in Eq 1 are defined as: Block unigram model : We compute unigram probabilities for the blocks. The blocks are simpler than the alignment templates (Och et al., 1999) in that they do not have an internal structure. Trigram language model: the probability between adjacent blocks is computed as the probability of the first target word in the target clump of given the final two words of the target clump of . The exponent is set in informal experiments to be . No other parameters such as distortion probabilities are used. To select blocks from training data, we compute unigram block co-occurrence counts . cannot be computed for all blocks in the training data: we would obtain hundreds of millions of blocks. The blocks are restricted by an underlying word align</context>
<context position="22098" citStr="Och et al., 1999" startWordPosition="3796" endWordPosition="3799">es the best results. For the AE experiments, only blocks up to a phrase length of are computed due to disk memory restrictions. The training data files for each of the chunks written onto disk memory. The word-to-word alignment is trained using iterations of the IBM Model training followed by iterations of the HMM Viterbi training. This training procedure takes about a day to execute on a single machine. Additionally, the overall block selection procedure takes about hours to execute. 5 Previous Work Block-based translation units are used in several papers on statistical machine translation. (Och et al., 1999) describe the alignment template system for statistical MT: alignment templates correspond to blocks that do have an internal structure. Marcu and Wong (2002) use a joint probability model for blocks where the clumps are contiguous phrases as in this paper. Yamada and Knight (2002) presents a decoder for syntax-based MT that uses so-called phrasal translation units that correspond to blocks. Block unigram counts are used to filter the blocks. The phrasal model is included into a syntax-based model. Projection of phrases has also been used in (Yarowsky et al., 2001). A word link extension algor</context>
</contexts>
<marker>Och, Tillmann, Ney, 1999</marker>
<rawString>Franz-Josef Och, Christoph Tillmann, and Hermann Ney. 1999. Improved Alignment Models for Statistical Machine Translation. In Proc. of the Joint Conf. on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC 99), pages 20–28, College Park, MD, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a Method for Automatic Evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of the 40th Annual Conf. of the Association for Computational Linguistics (ACL 02),</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="16762" citStr="Papineni et al., 2002" startWordPosition="2877" endWordPosition="2881">gmented into words. The training data contains million Chinese and million English words. The block selection algorithm described below runs less than one hour on a single -Gigahertz linux machine. Table 3 presents results for various block extension schemes. The first column describes the extension scheme used. The second column reports the total number of blocks in millions collected - including all the blocks that occurred only once. The third column reports the number of blocks that occurred at least twice. These blocks are used to compute the results in the fourth column: the BLEU score (Papineni et al., 2002) with reference translation using -grams along with 95% confidence interval is reported 4. Line and line of this table show results where only the source interval projection without any extension is carried out. For the extension scheme, the high-recall union set itself is used for projection. The results are worse than for all other schemes, since a lot of smaller blocks are discarded due to the projection approach. The scheme, where just the word links are used is too restrictive leaving out bigger blocks that are admissible according to . For the Chinese-English test data, there is only a m</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a Method for Automatic Evaluation of machine translation. In Proc. of the 40th Annual Conf. of the Association for Computational Linguistics (ACL 02), pages 311–318, Philadelphia, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
<author>Hermann Ney</author>
</authors>
<title>Word Reordering and a DP Beam Search Algorithm for Statistical Machine Translation.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="13237" citStr="Tillmann and Ney, 2003" startWordPosition="2275" endWordPosition="2278">d blocks. An example of blocks obtained from the ChineseEnglish training data is shown in Figure 6. ’$DATE’ is a placeholder for a date expression. Block contains the blocks to . All blocks are selected in training: the unigram decoder prefers even if , , and are much more frequent. The solid word links are word links in , the striped word links are word links in . Using the links in , we can learn one-to-many block translations, e.g. the pair ( ,’Xinhua news agency’) is learned from the training data. 3 DP-based Decoder We use a DP-based beam search procedure similar to the one presented in (Tillmann and Ney, 2003). We maximize over all block segmentations for which the source phrases yield a segmentation of the input source sentence, generating the target sentence simultaneously. The decoder processes search states of the following form: and are the two predecessor words used for the trigram language model, is the so-called coverage vector to keep track of the already processed source position, is the last processed source position. is the source phrase length of the block and (4) : denotes the set of blocks for which . For our Chinese-English experiments, we use the restriction as our baseline, and fo</context>
<context position="15038" citStr="Tillmann and Ney, 2003" startWordPosition="2595" endWordPosition="2598">elected N2 6.18 M 0.183 0.01 N3 1.69 M 0.185 0.01 N5 0.85 M 0.178 0.01 N10 0.45 M 0.176 0.01 N25 0.26 M 0.166 0.01 N100 0.18 M 0.154 0.01 currently being matched. is the length of the initial fragment of the source phrase that has been processed so far. is smaller or equal : . Note, that the partial hypotheses are not distinguished according to the identity of the block itself. The decoder processes the input sentence ’cardinality synchronously’: all partial hypotheses that are active at a given point cover the same number of input sentence words. The same beam-search pruning as described in (Tillmann and Ney, 2003) is used. The so-called observation pruning threshold is modified as follows: for each source interval that is being matched by a block source phrase at most the best target phrases according to the joint unigram probability are hypothesized. The list of blocks that correspond to a matched source interval is stored in a chart for each input sentence. This way the matching is carried out only once for all partial hypotheses that try to match the same input sentence interval. In the current experiments, decoding without block re-ordering yields the best translation results. The decoder translate</context>
</contexts>
<marker>Tillmann, Ney, 2003</marker>
<rawString>Christoph Tillmann and Hermann Ney. 2003. Word Reordering and a DP Beam Search Algorithm for Statistical Machine Translation. Computational Linguistics, 29(1):97–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>HMM BasedWord Alignment in Statistical Machine Translation.</title>
<date>1996</date>
<booktitle>In Proc. of the 16th Int. Conf. on Computational Linguistics (COLING</booktitle>
<pages>836--841</pages>
<location>Copenhagen, Denmark,</location>
<contexts>
<context position="3837" citStr="Vogel et al., 1996" startWordPosition="612" endWordPosition="615">ll word alignment. The block extension algorithm is shown to improve translation performance significantly. The system is tested on a Chinese-English (CE) and an Arabic-English (AE) translation task. The paper is structured as follows: in Section 2, we present the baseline block generation algorithm. The block extension approach is described in Section 2.1. Section 3 describes a DP-based decoder using blocks. Experimental results are presented in Section 4. 2 Block Generation Algorithm Starting point for the block generation algorithm is a word alignment obtained from an HMM Viterbi training (Vogel et al., 1996). The HMM Viterbi training is carried out twice with English as target language and Chinese as source language and vice versa. We obtain two alignment relations: is an alignment function from source to target positions and is an alignment function from target to source positions 1. We compute the union and the intersection of the two alignment relations and : We call the intersection relation , because it represents a high-precision alignment, and the union alignment , because it is taken to be a lower precision higher recall alignment (Och and Ney, 2000). The intersection is also a (partial) </context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Stefan Vogel, Hermann Ney, and Christoph Tillmann. 1996. HMM BasedWord Alignment in Statistical Machine Translation. In Proc. of the 16th Int. Conf. on Computational Linguistics (COLING 1996), pages 836–841, Copenhagen, Denmark, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A Decoder for Syntax-based Statistical MT.</title>
<date>2002</date>
<booktitle>In Proc. of the 40th Annual Conf. of the Association for Computational Linguistics (ACL 02),</booktitle>
<pages>303--310</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="1059" citStr="Yamada and Knight, 2002" startWordPosition="153" endWordPosition="157">unigram model and a word-based trigram language model. During training, the blocks are learned from source interval projections using an underlying high-precision word alignment. The system performance is significantly increased by applying a novel block extension algorithm using an additional highrecall word alignment. The blocks are further filtered using unigram-count selection criteria. The system has been successfully test on a Chinese-English and an ArabicEnglish translation task. 1 Introduction Various papers use phrase-based translation systems (Och et al., 1999; Marcu and Wong, 2002; Yamada and Knight, 2002) that have shown to improve translation quality over single-word based translation systems introduced in (Brown et al., 1993). In this paper, we present a similar system with a much simpler set of model parameters. Specifically, we compute the probability of a block sequence . A block is a pair consisting of a contiguous source and a contiguous target phrase. The block sequence Figure 1: A block sequence that jointly generates target and source phrases. The example is actual decoder output and the English translation is slightly incorrect. probability is decomposed into conditional probabiliti</context>
<context position="22380" citStr="Yamada and Knight (2002)" startWordPosition="3840" endWordPosition="3843">aining followed by iterations of the HMM Viterbi training. This training procedure takes about a day to execute on a single machine. Additionally, the overall block selection procedure takes about hours to execute. 5 Previous Work Block-based translation units are used in several papers on statistical machine translation. (Och et al., 1999) describe the alignment template system for statistical MT: alignment templates correspond to blocks that do have an internal structure. Marcu and Wong (2002) use a joint probability model for blocks where the clumps are contiguous phrases as in this paper. Yamada and Knight (2002) presents a decoder for syntax-based MT that uses so-called phrasal translation units that correspond to blocks. Block unigram counts are used to filter the blocks. The phrasal model is included into a syntax-based model. Projection of phrases has also been used in (Yarowsky et al., 2001). A word link extension algorithm similar to the one presented in this paper is given in (Koehn et al., 2003). 6 Conclusion In this paper, we describe a block-based unigram model for SMT. A novel block learning algorithm is presented that extends high-precision interval projections by elements from a high-reca</context>
</contexts>
<marker>Yamada, Knight, 2002</marker>
<rawString>Kenji Yamada and Kevin Knight. 2002. A Decoder for Syntax-based Statistical MT. In Proc. of the 40th Annual Conf. of the Association for Computational Linguistics (ACL 02), pages 303–310, Philadelphia, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
<author>Grace Ngai</author>
<author>Richard Wicentowski</author>
</authors>
<title>Inducing Multilingual Text Analysis tools via Robust Projection across Aligned Corpora.</title>
<date>2001</date>
<booktitle>In Proc. of the HLT 2001 conference,</booktitle>
<pages>161--168</pages>
<location>San Diego, CA,</location>
<contexts>
<context position="22669" citStr="Yarowsky et al., 2001" startWordPosition="3886" endWordPosition="3889">statistical machine translation. (Och et al., 1999) describe the alignment template system for statistical MT: alignment templates correspond to blocks that do have an internal structure. Marcu and Wong (2002) use a joint probability model for blocks where the clumps are contiguous phrases as in this paper. Yamada and Knight (2002) presents a decoder for syntax-based MT that uses so-called phrasal translation units that correspond to blocks. Block unigram counts are used to filter the blocks. The phrasal model is included into a syntax-based model. Projection of phrases has also been used in (Yarowsky et al., 2001). A word link extension algorithm similar to the one presented in this paper is given in (Koehn et al., 2003). 6 Conclusion In this paper, we describe a block-based unigram model for SMT. A novel block learning algorithm is presented that extends high-precision interval projections by elements from a high-recall alignment. The extension method is shown to improve translation performance significantly. For the Chinese-toEnglish task, we obtained a NIST score of on the June 2002 DARPA TIDES Large Data evaluation test set. Acknowledgements This work was partially supported by DARPA and monitored </context>
</contexts>
<marker>Yarowsky, Ngai, Wicentowski, 2001</marker>
<rawString>David Yarowsky, Grace Ngai, and Richard Wicentowski. 2001. Inducing Multilingual Text Analysis tools via Robust Projection across Aligned Corpora. In Proc. of the HLT 2001 conference, pages 161–168, San Diego, CA, March.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>