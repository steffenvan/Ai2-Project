<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.095909">
<title confidence="0.997673">
Fine-Grained Classification of Named Entities
Exploiting Latent Semantic Kernels
</title>
<author confidence="0.988844">
Claudio Giuliano
</author>
<affiliation confidence="0.907229">
FBK-irst
</affiliation>
<address confidence="0.671704">
I-38100, Trento, Italy
</address>
<email confidence="0.997371">
giuliano@fbk.eu
</email>
<sectionHeader confidence="0.995622" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999423923076923">
We present a kernel-based approach for fine-
grained classification of named entities. The
only training data for our algorithm is a few
manually annotated entities for each class. We
defined kernel functions that implicitly map
entities, represented by aggregating all con-
texts in which they occur, into a latent seman-
tic space derived from Wikipedia. Our method
achieves a significant improvement over the
state of the art for the task of populating an
ontology of people, although requiring con-
siderably less training instances than previous
approaches.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999982576923077">
Populating an ontology with relevant entities ex-
tracted from unstructured textual documents is a
crucial step in Semantic Web and knowledge man-
agement systems. As the concepts in an ontology
are generally arranged in deep class/subclass hierar-
chies, the problem of populating ontologies is typi-
cally solved top-down, firstly identifying and classi-
fying entities in the most general concepts, and then
refining the classification process.
Recent advances have made supervised ap-
proaches very successful in entity identification and
classification. However, to achieve satisfactory per-
formance, supervised systems must be supplied with
a sufficiently large amount of training data, usually
consisting of hand tagged texts. As domain specific
ontologies generally contains hundreds of subcate-
gories, such approaches are not directly applicable
for a more fine-grained categorization because the
number of documents required to find sufficient pos-
itive examples for all subclasses becomes too large,
making the manual annotation very expensive.
Consequently, in the literature, supervised ap-
proaches are confined to classify entities into broad
categories, such as persons, locations, and or-
ganizations, while the fine-grained classification
has been approached with minimally supervised
(e.g., Tanev and Magnini (2006) and Giuliano and
Gliozzo (2008)) and unsupervised learning algo-
rithms (e.g., Cimiano and V¨olker (2005) and Giu-
liano and Gliozzo (2007)).
Following this trend, we present a minimally su-
pervised approach to fine-grained categorization of
named entities previously recognized into coarse-
grained categories, e.g., by a named-entity recog-
nizer. The only training data for our algorithm is a
few manually annotated entities for each class. For
example, Niels Bohr, Albert Einstein, and Enrico
Fermi might be used as examples for the class physi-
cists. In some cases, training entities can be acquired
(semi-) automatically from existing ontologies al-
lowing us to automatically derive training entities
for use with our machine learning algorithm. For
instance, we may easily obtain tens of training en-
tities for very specific classes, such as astronomers,
materials scientists, nuclear physicists, by querying
the Yago ontology (Suchanek et al., 2008).
We represent the entities using features extracted
from the textual contexts in which they occur.
Specifically, we use a search engine to collect such
contexts from the Web. Throughout this paper, we
will refer to such a representation as multi-context
representation, in contrast to the single-context rep-
</bodyText>
<page confidence="0.97785">
201
</page>
<note confidence="0.989899">
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 201–209,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.99994776">
resentation in which an entity is categorized using
solely features extracted from the local context sur-
rounding it, usually a window of a few words around
the entity occurrence. Single-context features are
commonly used in named-entity recognition, how-
ever to assign very specific categories the local con-
text might not provide sufficient information. For
example, in the sentence “Prof. Enrico Fermi dis-
covered a way to induce artificial radiation in heavy
elements by shooting neutrons into their atomic nu-
clei,” single-context features such as, the prefix Prof.
and the capital letters, provides enough evidence that
Enrico Fermi is a person and a professor. However,
to discover that he is a physicist we need to analyze
a wider context, or alternatively multiple ones. Re-
cently, Ganti et al. (2008) has shown that exploiting
multi-context information can greatly improve the
fine-grained classification of named entities, when
compared to methods using single context only.
In order to effectively represent entities’ multi-
contexts, we extend the traditional vector space
model (VSM), offering a way to integrate external
semantic information in the classification process by
means of latent semantic kernels (Shawe-Taylor and
Cristianini, 2004). As a result, we obtain a general-
ized similarity function between multi-contexts that
incorporates semantic relations between terms, auto-
matically learned from unlabeled data. In particular,
we use Wikipedia to build the latent semantic space.
The underlying idea is that similar named entities
tend to have a similar description in Wikipedia. As
Wikipedia provides reliable information and it ex-
ceeds all other encyclopedias in coverage, it should
be a valuable resource for the task of populating an
ontology. To validate this hypothesis, we compare
this model with one built from a news corpus.
Our approach achieves a significant improvement
over the state of the art for the task of populating the
People Ontology (Giuliano and Gliozzo, 2008), al-
though requiring considerably less training instances
than previous approaches. The task consists in clas-
sifying person names into a multi-level taxonomy
composed of 21 categories derived from WordNet,
making very fine-grained distinctions (e.g., physi-
cists vs. mathematicians). It provides a more real-
istic and challenging benchmark than the ones pre-
viously available (e.g., Tanev and Magnini (2006)
and Fleischman and Hovy (2002)), that consider a
smaller number of categories arranged in a one-level
taxonomy.
</bodyText>
<sectionHeader confidence="0.963242" genericHeader="method">
2 Entity Representation
</sectionHeader>
<bodyText confidence="0.999950592592592">
The goal of our research is to determine the fine-
grained categories of named entities requiring a min-
imal amount of human supervision.
Our method is based on the common assump-
tion that named entities co-occurring with the same
(domain-specific) terms are highly probable to refer
to the same categories. For example, quantum me-
chanics, atomic physics, and Nobel Prize in physics
are all terms that bound Niels Bohr and Enrico Fermi
to the concept of physics.
To automatically derive features for the training
and testing entities we proceed as follows. We pair
each entity i with a multi-context mi obtained by
querying a search engine with the entity “i” and
merging the first M snippets si,j returned (1 S j S
M). A multi-context is therefore a fictitious doc-
ument obtained by aggregating snippets, i.e., sum-
mary texts of the search engine result. Formally,
mi = UMj=1si,j, where the operator U denotes the
concatenation of strings. For example, Figure 1 (a)
and (b) show some snippets retrieved for “Enrico
Fermi” and “Albert Einstein,” while s1 U s2 U s3 and
s4 U s5 U s6 represent their multi-contexts, respec-
tively.
The following section describes how entities’
multi-contexts are embedded into the feature space
in order to train a kernel-based classifier.
</bodyText>
<sectionHeader confidence="0.832959" genericHeader="method">
3 Kernels for Fine-Grained Classification
of Entities
</sectionHeader>
<bodyText confidence="0.999963538461539">
The strategy adopted by kernel methods (Shawe-
Taylor and Cristianini, 2004; Sch¨olkopf and Smola,
2002) consists of splitting the learning problem in
two parts. They first embed the input data in a suit-
able feature space, and then use a linear algorithm
(e.g., the perceptron) to discover nonlinear pattern in
the input space. Typically, the mapping is performed
implicitly by a so-called kernel function. The ker-
nel function is a similarity measure between the in-
put data that depends exclusively on the specific data
type and domain. A typical similarity function is the
inner product between feature vectors. Characteriz-
ing the similarity of the inputs plays a crucial role in
</bodyText>
<page confidence="0.994452">
202
</page>
<figure confidence="0.967148875">
s1: [Enrico Fermi]PER discovered that many nuclear transformations could be conducted by using neutrons.
s2: [Enrico Fermi]PER led the manhattan project’s effort to create the first man-made and self-sustaining nuclear chain.
s3: [Enrico Fermi]PER was most noted for his work on the development of the first nuclear reactor.
(a)
s4: [Albert Einstein]PER did not directly participate in the invention of the atomic bomb.
s5: [Albert Einstein]PER is one of the most recognized and well-known scientists of the century.
ss: [Albert Einstein]PER was born at Ulm, in W¨urttemberg, Germany, on March 14, 1879.
(b)
</figure>
<figureCaption confidence="0.999989">
Figure 1: Examples of snippets retrieved for Enrico Fermi (a) and Albert Einstein (b).
</figureCaption>
<bodyText confidence="0.998600555555556">
determining the success or failure of the learning al-
gorithm, and it is one of the central questions in the
field of machine learning.
Formally, the kernel is a function k : X xX —* R
that takes as input two data objects (e.g., vectors,
texts, parse trees) and outputs a real number charac-
terizing their similarity, with the property that the
function is symmetric and positive semi-definite.
That is, for all xi, xj E X, it satisfies
</bodyText>
<equation confidence="0.983986">
k(xi, xj) = (φ(xi), φ(xj)) (1)
</equation>
<bodyText confidence="0.999923">
where φ is an explicit mapping from X to an (inner
product) feature space F.
In the next sections, we define and combine differ-
ent kernel functions that calculate the pairwise sim-
ilarly between multi-contexts. They are the only do-
main specific element of our classification system,
while the learning algorithm is a general purpose
component. Many classifiers can be used with ker-
nels. The most popular ones are perceptron, sup-
port vector machines (SVM), and k-nearest neighbor
(KNN).
</bodyText>
<subsectionHeader confidence="0.998381">
3.1 Bag-of-Words Kernel
</subsectionHeader>
<bodyText confidence="0.999959142857143">
The simplest method to estimate the similarity be-
tween two multi-contexts is to compute the inner
product of their vector representations in the VSM.
Formally, we define a space of dimensionality N in
which each dimension is associated with one word
from the dictionary, and the multi-context m is rep-
resented by a row vector
</bodyText>
<equation confidence="0.984406">
φ(m) = (f(t1, m), f(t2, m),... , f(tN, m)), (2)
</equation>
<bodyText confidence="0.99993675">
where the function f(ti, m) records whether a par-
ticular token ti is used in m. Using this representa-
tion we define bag-of-words kernel between multi-
contexts as
</bodyText>
<equation confidence="0.999679">
KsOW(m1,m2) = (φ(m1),φ(m2)) (3)
</equation>
<bodyText confidence="0.999874125">
However, the bag-of-words representation does
not deal well with lexical variability. To significantly
reduce the training set size, we need to map contexts
containing semantically equivalent terms into simi-
lar feature vectors. To this aim, in the next section,
we introduce the class of semantic kernels and show
how to define an effective semantic VSM using (un-
labeled) external knowledge.
</bodyText>
<subsectionHeader confidence="0.999403">
3.2 Semantic Kernels
</subsectionHeader>
<bodyText confidence="0.999958952380952">
It has been shown that semantic information is fun-
damental for improving the accuracy and reducing
the amount of training data in many natural language
tasks, including fine-grained classification of named
entities (Fleischman and Hovy, 2002), question clas-
sification (Li and Roth, 2005), text categorization
(Giozzo and Strapparava, 2005), word sense disam-
biguation (Gliozzo et al., 2005).
In the context of kernel methods, semantic infor-
mation can be integrated considering linear trans-
formations of the type ˜φ(cj) = φ(cj)S, where S
is a N x k matrix (Shawe-Taylor and Cristianini,
2004). The matrix S can be rewritten as S = WP,
where W is a diagonal matrix determining the word
weights, while P is the word proximity matrix cap-
turing the semantic relations between words. The
proximity matrix P can be defined by setting non-
zero entries between those words whose semantic
relation is inferred from an external source of do-
main knowledge. The semantic kernel takes the gen-
eral form
</bodyText>
<equation confidence="0.954349">
˜k(mi, mj) = φ(mi)SS�φ(mj)� = ˜φ(mi)˜φ(mj)�.(4)
</equation>
<bodyText confidence="0.9999442">
It follows directly from the explicit construction that
Equation 4 defines a valid kernel.
WordNet and manually constructed lists of se-
mantically related words typically provide a sim-
ple way to introduce semantic information into the
</bodyText>
<page confidence="0.99347">
203
</page>
<bodyText confidence="0.999951375">
kernel. To define a semantic kernel from such re-
sources, we could explicitly construct the proximity
matrix P by setting its entries to reflect the semantic
proximity between the words i and j in the specific
lexical resource. However, we prefer an approach
that exploits unlabeled data to automatically build
the proximity matrix, defining a language and do-
main independent approach.
</bodyText>
<subsectionHeader confidence="0.981416">
3.2.1 Latent Semantic Kernel
</subsectionHeader>
<bodyText confidence="0.99961075">
To define a proximity matrix, we look at co-
occurrence information in a (large) corpus. Two
words are considered semantically related if they
frequently co-occur in the same texts. We use sin-
gular valued decomposition (SVD) to automatically
derive the proximity matrix II from a corpus, rep-
resented by its term-by-document matrix D, where
the Di,j entry gives the frequency of term ti in doc-
ument dj.1 SVD decomposes the term-by-document
matrix D into three matrixes D = UEV&apos;, where U
and V are orthogonal matrices (i.e., U&apos;U = I and
V&apos;V = I) whose columns are the eigenvectors of
DD&apos; and D&apos;D respectively, and E is the diagonal
matrix containing the singular values of D.
Under this setting, we define the proximity matrix
II as follows:
</bodyText>
<equation confidence="0.971906">
II = UkEk, (5)
</equation>
<bodyText confidence="0.999981166666667">
where Uk is the matrix containing the first k
columns of U and k is the dimensionality of the la-
tent semantic space and can be fixed in advance. By
using a small number of dimensions, we can define a
very compact representation of the proximity matrix
and, consequently, reduce the memory requirements
while preserving most of the information.
The matrix II is used to define a linear transfor-
mation 7r : RN → Rk, that maps the vector 0(mj),
represented in the standard VSM, into the vector
˜0(mj) in the latent semantic space. Formally, 7r is
defined as follows
</bodyText>
<equation confidence="0.999611">
7r(0(mj)) = 0(mj)(WII) = ˜0(mj), (6)
</equation>
<bodyText confidence="0.942233555555556">
where 0(mj) is a row vector, W is a N x N diag-
onal matrix determining the word weights such that
Wi,i = log(idf(wi)), where idf(wi) is the inverse
document frequency of wi.
1SVD has been first applied to perform latent semantic anal-
ysis of terms and latent semantic indexing of documents in large
corpora by Deerwester et al. (1990).
Finally, the latent semantic kernel is explicitly de-
fined as follows
</bodyText>
<equation confidence="0.999795">
KLS(mi,mj) = (7r(0(mi)),7r(0(mj))), (7)
</equation>
<bodyText confidence="0.9996668">
where 0 is the mapping defined in Equation 2 and
7r is the linear transformation defined in Equation 6.
Note that we have used a series of successive map-
pings each of which adds some further improvement
to the multi-context representation.
</bodyText>
<subsectionHeader confidence="0.998238">
3.3 Composite Kernel
</subsectionHeader>
<bodyText confidence="0.999976">
Finally, to combine the two representations of multi-
contexts, we define the composite kernel as follows
</bodyText>
<equation confidence="0.998967">
KsOW(m1, m2) + KLS(m1, m2). (8)
</equation>
<bodyText confidence="0.999799333333333">
It follows directly from the explicit construction of
the feature space and from closure properties of ker-
nels that it is a valid kernel.
</bodyText>
<sectionHeader confidence="0.99969" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999917333333333">
In this section, we compare performance of different
kernel setups and previous approaches on an ontol-
ogy population task.
</bodyText>
<subsectionHeader confidence="0.98644">
4.1 Benchmark
</subsectionHeader>
<bodyText confidence="0.999963318181818">
Experiments were carried out on the People Ontol-
ogy (Giuliano and Gliozzo, 2008). An ontology
extracted from WordNet, containing 1,657 distinct
person instances arranged in a multi-level taxonomy
having 21 fine-grained categories (Figure 2). To pro-
vide a formal distinction between classes and in-
stances, required to assign instances to classes, the
authors followed the directives defined by Gangemi
et al. (2003) for OntoWordNet, in which the infor-
mal WordNet semantics is re-engineered in terms of
a description logic.
In order to have a fair comparison, we reproduced
the same experimental settings used in Giuliano and
Gliozzo (2008). The population task is cast as a cate-
gorization problem, trying to assign person instances
to the most specific category. For each class, the in-
stances were randomly split into two equally sized
subsets. One is used for training and the other for
test, and vice versa. The reported results are the av-
erage performance over these two subsets. When an
instance is assigned to a sub-class it is also implic-
itly assigned to all its super-classes. For instance,
</bodyText>
<page confidence="0.997057">
204
</page>
<figureCaption confidence="0.997867">
Figure 2: The People Ontology defined by Giuliano and Gliozzo (2008). Numbers in brackets are the total numbers
of person instances per category. Concepts with less than 40 instances were removed.
</figureCaption>
<bodyText confidence="0.999691333333333">
classifying Salvador Dali as painter we implicitly
classify him as artist and creator. The evaluation
is performed as proposed by Melamed and Resnik
(2000) for a similar hierarchical categorization task.
For instance, classifying John Lennon as painter, we
obtain a false positive for the spurious classification
painter, a false negative for missing class musician,
and two true positives for the correct assignment to
the super-classes artist and creator.
</bodyText>
<subsectionHeader confidence="0.968255">
4.2 Experimental Settings
</subsectionHeader>
<bodyText confidence="0.999772866666667">
We built two proximity matrices UW and UNYT.
The former is derived from the 200,000 most visited
Wikipedia articles, while the latter from 200,000 ar-
ticles published by the New York Times between
June 1, 1998 and January 01, 2000. After remov-
ing terms that occur less than 5 times, the result-
ing dictionaries contain about 300,000 and 150,000
terms respectively. We used the SVDLIBC pack-
age2 to compute the SVD, truncated to 400 dimen-
sions. To derive the multi-context representation, we
collected 100 english snippets for each person in-
stance by querying GoogleTM. To classify each per-
son instance into one of the fine-grained categories,
we used a KNN classifier (K = 1). No parameter
optimization was performed.
</bodyText>
<footnote confidence="0.927756">
2http://tedlab.mit.edu/˜dr/svdlibc/
</footnote>
<subsectionHeader confidence="0.641807">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.9996567">
Table 1 shows micro- and macro-averaged results
for KBOW, KW, KBOW + KW, KNYT, KBOW +
KNYT, the IBOP method (Giuliano and Gliozzo,
2008), the random baseline, and most frequent base-
line.3 Where KW and KNYT are instances of the
latent semantic kernel, KLS, using the proximity
matrices UW and UNYT, derived from Wikipedia
and the New York Times corpus, respectively. Ta-
ble 2 shows detailed results for each sub- and super-
category for KBOW + KW. Table 3 shows the con-
fusion matrix of KBOW + KW, in which the rows
are ground truth classes and the columns are predic-
tions. The matrix has been calculated for the finer-
grained categories and, then, grouped according to
their super-class. To be compared with the IBOP
method, all experiments were conducted using only
20 training examples per category. Finally, figure
3 shows the learning curves for KBOW + KW ob-
tained varying the number of snippets (12, 25, 50,
and 100) used to derive the multi-contexts.
</bodyText>
<subsectionHeader confidence="0.924578">
4.4 Discussion
</subsectionHeader>
<bodyText confidence="0.9999075">
On the one hand, the results (Table 2) show that
learning the semantic model from Wikipedia gives
no significant improvement. Therefore, we reject the
hypothesis that encyclopedic knowledge can provide
</bodyText>
<footnote confidence="0.993569">
3The most frequent category has been estimated on the train-
ing data.
</footnote>
<page confidence="0.998029">
205
</page>
<figure confidence="0.99992801923077">
12 snippets
2 4 6 8 10 12 14 16 18 20
Number of instances
2 4 6 8 10 12 14 16 18 20
Number of instances
25 snippets
2 4 6 8 10 12 14 16 18 20
Number of instances
100 snippets
2 4 6 8 10 12 14 16 18 20
Number of instances
KBOW
KW
KBOW+KW
50 snippets
KBOW
KW
KBOW+KW
KBOW
KW
KBOW+KW
Micro F1 0.8
0.75
0.7
0.65
0.6
0.55
0.5
Micro F1 0.8
0.75
0.7
0.65
0.6
0.55
0.5
Micro F1 0.8
Micro F1 0.75
0.7
0.65
0.6
0.55
0.5
0.8
0.75
0.7
0.65
0.6
0.55
0.5
KIOW
KW
KIOW+KW
</figure>
<figureCaption confidence="0.9907415">
Figure 3: Learning curves for KBOW + KW obtained varying the number of snippets used to derive the training and
test sets. From top-left to bottom right: 12, 25, 50, and 100.
</figureCaption>
<table confidence="0.999846666666667">
Method Micro-F1 Macro-F1
KBOW 75.6 70.6
KW 78.1 73.1
KBpW + KW 80.0 75.4
KNYT 77.6 72.9
KBOW +KNYT 79.7 75.1
IBOP 70.1 62.3
Random 15.4 15.5
Most Frequent 20.7 3.3
</table>
<tableCaption confidence="0.961157666666667">
Table 1: Comparison among the kernel-based ap-
proaches, the IBOP method (Giuliano and Gliozzo,
2008), the random baseline, and most frequent baseline.
</tableCaption>
<bodyText confidence="0.999962833333334">
more accurate semantic models than general pur-
pose corpora. Moreover, further experiments have
shown that even a larger number of Wikipedia ar-
ticles (600,000) does not help. On the other hand,
the latent semantic kernels outperform all the other
methods, and their composite (KBOW + KW and
KBOW + KNYT) perform the best on every con-
figuration, demonstrating the effectiveness of la-
tent semantic kernels in fine-grained classification
of named entities. As in text categorization and
word sense disambiguation, they have proven effec-
tive tools to overcome the limitation of the VSM by
introducing semantic similarity among words.
An important characteristic of the approach is the
small number of training examples required per cat-
egory. This affects both the prediction accuracy and
the computation time (this is generally a common
property of instance-based algorithms). The learn-
ing curves (Figure 3) show that the composite ker-
nel (KBOW + KLS) obtained the same performance
of the bag-of-word kernel (KBOW) using less than
half of the training examples per category. The
difference is much more pronounced when using
less snippets. The composite kernel KBOW + KW
reaches a plateau around 10 examples, and after 20
examples adding more examples does not signifi-
cantly improve the classification performance.
As most of entities in the People Ontol-
ogy are celebrities, all the snippets retrieved by
GoogleTMgenerally refer to them, alleviating the
problem of ambiguity of proper names. However,
person names are highly ambiguous. In a more real-
istic scenario, the result of a search engine for a per-
son name is usually a mix of contexts about different
entities sharing the same name. In this case, our ap-
proach have to be combined with a system that clus-
ters the search engine result, where each cluster is
assumed to contain all (and only those) contexts that
refer to the same entity. The WePS evaluation cam-
paign on disambiguation of person names (Artiles et
al., 2007; Artiles et al., 2009) has shown that the best
clustering systems achieve a precision of about 90%
</bodyText>
<page confidence="0.996173">
206
</page>
<table confidence="0.999356882352941">
Phy Scientist Soc Performer Fil Creator Communicator Business Health
Mat Che Bio Act Mus Pai Mus Poe Dra Rep man prof
Phy 118 24 10 4 2 0 0 0 0 0 0 0 0 7 2
Mat 2 33 0 0 1 0 0 0 0 0 1 0 0 3 0
Che 13 2 68 9 2 0 0 0 0 0 0 0 0 5 2
Bio 3 0 7 52 0 0 0 0 1 0 0 0 1 6 6
Soc 0 4 1 1 55 0 0 0 0 0 3 1 1 4 2
Act 0 0 0 0 0 98 5 27 0 0 2 14 0 3 0
Mus 0 0 0 0 0 17 67 0 0 32 1 0 1 2 1
Fil 0 0 0 0 0 13 0 45 0 0 1 4 0 2 0
Pai 0 0 0 1 1 2 0 1 100 0 1 0 0 1 0
Mus 0 0 0 0 0 4 29 0 0 139 0 1 0 0 0
Poe 0 2 0 0 0 0 0 0 7 3 98 26 1 2 3
Dra 0 0 0 1 1 9 0 1 0 1 12 61 1 4 1
Rep 0 0 0 0 0 1 1 0 2 0 0 0 197 22 0
Bus 1 0 1 0 1 0 0 1 0 1 0 0 1 36 0
Hea 0 0 0 8 4 0 1 0 0 0 0 1 1 2 31
</table>
<tableCaption confidence="0.9970955">
Table 3: Confusion matrix of KBOW + KW for the more fine-grained categories grouped according to their top-level
concepts of the People Ontology.
</tableCaption>
<table confidence="0.999983541666667">
Category Prec. Recall Fl
Scientist 95.1 90.1 92.6
Physicist 86.1 70.7 77.6
Mathematician 50.8 82.5 62.9
Chemist 78.2 67.3 72.3
Biologist 68.4 68.4 68.4
Social scientist 82.1 76.4 79.1
Performer 75.7 69.3 72.3
Actor 68.1 65.8 66.9
Musician 65.0 55.4 59.8
Creator 78.9 82.6 80.7
Film Maker 60.0 69.2 64.3
Artist 83.6 85.4 84.5
Painter 90.9 93.5 92.2
Musician 79.0 80.3 79.7
Communicator 91.9 86.7 89.2
Representative 96.6 88.3 92.3
Writer 86.8 84.2 85.5
Poet 82.4 69.0 75.1
Dramatist 56.5 66.3 61.0
Business man 36.4 85.7 51.1
Health professional 64.6 64.6 64.6
micro 80.9 79.6 80.2
macro 75.1 76.3 75.7
</table>
<tableCaption confidence="0.996062">
Table 2: Results for each category using KBOW + KW.
</tableCaption>
<bodyText confidence="0.999912071428571">
and a recall of about 70% and that, in the major-
ity of the cases, the number of contexts per entity is
less than 20. This shows that latent semantic kernels
are an effective tool for fine-grained classification of
person names.
Finally, table 3 shows that misclassification er-
rors are largely distributed among categories belong-
ing to the same super-class (i.e., the blocks on the
main diagonal are more densely populated than oth-
ers). As expected, the algorithm is much more accu-
rate for the top-level concepts (i.e., Scientist, Com-
municator, etc.), where the category distinctions are
clearer, while a further fine-grained classification, in
some cases, is even difficult for human annotators.
</bodyText>
<sectionHeader confidence="0.999849" genericHeader="method">
5 Related Work
</sectionHeader>
<bodyText confidence="0.99957796">
Fleischman and Hovy (2002) approach the fine-
grained classification of person instances using su-
pervised learning, where the training set is gener-
ated semi-automatically, bootstrapping from a small
training set. They compare different machine learn-
ing algorithms, providing local features as well as
global semantic information derived from topic sig-
nature and WordNet. Person instances were classi-
fied into one of eight categories.
Cimiano and V¨olker (2005) present an approach
for the fine-grained classification of entities relying
on the Harris’ distributional hypothesis and the vec-
tor space model. They assign a particular instance
to the most similar concept representing both with
lexical-syntactic features extracted from the context
of the instance and the lexicalization of the concept,
respectively. Experiments were performed using a
large ontology with 682 concepts (unfortunately not
yet available).
Tanev and Magnini (2006) proposed a weakly-
supervised method that requires as training data a
list of named entities, without context, for each cat-
egory under consideration. Given a generic syntacti-
cally parsed corpus containing at least each training
entity twice, the algorithm learns, for each category,
</bodyText>
<page confidence="0.993964">
207
</page>
<bodyText confidence="0.99997352238806">
a feature vector describing the contexts where those
entities occur. Then, it compares the new (unknown)
entity with the so obtained feature vectors, assigning
it to the most similar category. Experiments are per-
formed on a benchmark of 5 sub-classes of location
and 5 sub-classes of person.
Giuliano and Gliozzo (2007) propose an unsuper-
vised approach based on lexical entailment, consist-
ing in assigning an entity to the category whose lex-
icalization can be replaced with its occurrences in
a corpus preserving the meaning. Using unsuper-
vised learning, they obtained slightly worst results
than Tanev and Magnini (2006) on the same bench-
mark.
Picca et al. (2007) present an approach for on-
tology learning from open domain text collections,
based on the combination of Super Sense Tagging
and Domain Modeling techniques. The system rec-
ognizes terms pertinent to the domain and assigns
them the correct ontological type.
Giuliano and Gliozzo (2008) present an instance-
based learning algorithm for fine-grained named en-
tity classification based on syntactic features (word-
order, case-marking, agreement, verb tenses, etc.).
Their method can handle much finer distinctions
than previous methods, and it is evaluated on a hi-
erarchical taxonomy of 21 ancestors of people that
was induced from WordNet. One contribution is to
create this richer People Ontology. Another is to
make effective use of the Web 1T 5-gram corpus
(Brants and Franz, 2006) to represent syntactic in-
formation. The main difference between the two ap-
proaches lies primarily in the use of syntactic and
semantic information. Our experiments show that
semantic features do provide richer information than
syntactic ones for a more fine-grained classification
of named entities. In fact, the accuracy improve-
ment achieved by our approach is more evident for
the more specific classes. For example, the improve-
ment in accuracy is about 14% for the class scientist,
while it ranges from 25% to 46% for its sub-classes
(physicist, mathematician, etc.).
Kozareva et al. (2008) propose an approach for
person name categorization based on the domain
distribution. They use the information provided by
WordNet Domains to generated lists of words rele-
vant for a given domain, by mapping and ranking the
words from the WordNet glosses to their WordNet
Domains. A named entity is then classified accord-
ing the similarity between the word-domain lists and
the global context in which the entity appears. How-
ever, the evaluation was performed only on 6 person
names using two categories.
Ganti et al. (2008) present a method that considers
an entity’s context across multiple documents con-
taining it, and exploiting word n-grams and existing
large list of related entities as features. They gener-
ated training and test data using Wikipedia articles
that contain list of instances. They compare their
system with a single-context classifier, showing that
their approach based on aggregate context perform
better.
Finally, Talukdar et al. (2008) propose a graph-
based semi-supervised label propagation algorithm
for acquiring open-domain labeled classes and their
instances from a combination of unstructured and
structured text.
</bodyText>
<sectionHeader confidence="0.999535" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999998421052632">
We presented an approach to automatic fine-grained
categorization of named entities based on kernel
methods. Entities are represented by aggregating all
contexts in which they occur. We employed latent
semantic kernels to extend the bag-of-words repre-
sentation. The latent semantic models were derived
from Wikipedia and a news corpus We evaluated our
approach on the People Ontology, a multi-level on-
tology of people derived from WordNet. Although
this benchmark is still far from being “large”, it al-
lows drawing more valid conclusions than past ones.
We significantly outperformed the previous results
on both coarse- and fine-grained classification, al-
though requiring much less training instances. From
this preliminary analysis, it appears that semantic in-
formation is much more effective that syntactic one
for this task, and deriving the semantic model from
Wikipedia gives no significant improvement, as well
as, using a larger number of Wikipedia articles.
</bodyText>
<sectionHeader confidence="0.997678" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<footnote confidence="0.983015428571429">
Claudio Giuliano is supported by the X-Media project (http:
//www.x-media-project.org), sponsored by the Euro-
pean Commission as part of the Information Society Technolo-
gies (IST) programme under EC grant number IST-FP6-026978
and the ITCH project (http://itch.fbk.eu), sponsored
by the Italian Ministry of University and Research and by the
Autonomous Province of Trento.
</footnote>
<page confidence="0.996163">
208
</page>
<sectionHeader confidence="0.984951" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998817141414142">
Javier Artiles, Julio Gonzalo, and Satoshi Sekine. 2007.
The semeval-2007 weps evaluation: Establishing a
benchmark for the web people search task. In Pro-
ceedings of the Fourth International Workshop on
Semantic Evaluations (SemEval-2007), pages 64–69,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Javier Artiles, Julio Gonzalo, and Satoshi Sekine. 2009.
Weps 2 evaluation campaign: overview of the web
people search clustering task. In 2nd Web Peo-
ple Search Evaluation Workshop (WePS 2009), 18th
WWW Conference, Madrid, Spain, April.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-
gram corpus version 1, Linguistic Data Consortium,
Philadelphia.
Philipp Cimiano and Johanna V¨olker. 2005. Towards
large-scale, open-domain and ontology-based named
entity classification. In Proceedings of RANLP’05,
pages 66– 166–172, Borovets, Bulgaria.
Scott C. Deerwester, Susan T. Dumais, Thoms K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society of Information Science,
41(6):391–407.
Michael Fleischman and Eduard Hovy. 2002. Fine
grained classification of named entities. In Proceed-
ings of the 19th International Conference on Compu-
tational Linguistics, Taipei, Taiwan.
Aldo Gangemi, Roberto Navigli, and Paola Velardi.
2003. Axiomatizing WordNet glosses in the On-
toWordNet project. In Proocedings of the Workshop
on Human Language Technology for the Semantic
Web and Web Services at ISWC 2003, Sanibel Island,
Florida.
Venkatesh Ganti, Arnd C. K¨onig, and Rares Vernica.
2008. Entity categorization over large document col-
lections. In KDD ’08: Proceeding of the 14th ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, pages 274–282, New York,
NY, USA. ACM.
Alfio Giozzo and Carlo Strapparava. 2005. Domain ker-
nels for text categorization. In Ninth Conference on
Computational Natural Language Learning (CoNLL-
2005), pages 56–63, Ann Arbor, Michigan, June.
Claudio Giuliano and Alfio Gliozzo. 2007. Instance
based lexical entailment for ontology population. In
Proceedings of the 2007 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), pages 248–256.
Claudio Giuliano and Alfio Gliozzo. 2008. Instance-
based ontology population exploiting named-entity
substitution. In Proceedings of the 22nd Interna-
tional Conference on Computational Linguistics (Col-
ing 2008), pages 265–272, Manchester, UK, August.
Alfio Massimiliano Gliozzo, Claudio Giuliano, and Carlo
Strapparava. 2005. Domain kernels for word sense
disambiguation. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics (ACL’05), pages 403–410, Ann Arbor, Michigan,
June.
Zornitsa Kozareva, Sonia Vazquez, and Andres Montoyo.
2008. Domain information for fine-grained person
name categorization. In 9th International Conference
on Intelligent Text Processing and Computational Lin-
guistics (CICLing 2008), pages 311–321, Haifa, Israel,
17-23 February.
Xin Li and Dan Roth. 2005. Learning question classi-
fiers: the role of semantic information. Natural Lan-
guage Engineering, 12(3):229–249.
I. Dan Melamed and Philip Resnik. 2000. Tagger eval-
uation given hierarchical tag sets. Computers and the
Humanities, pages 79–84.
Davide Picca, Alfio Gliozzo, and Massimiliano Cia-
ramita. 2007. Semantic domains and supersense tag-
ging for domain-specific ontology learning. In David
Evans, Sadaoki Furui, and Chantal Soul´e-Dupuy, edi-
tors, Recherche d’Information Assist´ee par Ordinateur
(RIAO), Pittsburgh, PA, USA, May.
B. Sch¨olkopf and A. Smola. 2002. Learning with Ker-
nels. MIT Press, Cambridge, Massachusetts.
J. Shawe-Taylor and N. Cristianini. 2004. Kernel Meth-
ods for Pattern Analysis. Cambridge University Press.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2008. YAGO: A large ontology from
wikipedia and wordnet. Elsevier Journal of Web Se-
mantics.
Partha Pratim Talukdar, Joseph Reisinger, Marius Pasca,
Deepak Ravichandran, Rahul Bhagat, and Fernando
Pereira. 2008. Weakly-supervised acquisition of la-
beled class instances using graph random walks. In
Proceedings of the conference on Empirical Methods
in Natural Language Processing (EMNLP), Waikiki,
Honolulu, Hawaii, October 25-27.
Hristo Tanev and Bernardo Magnini. 2006. Weakly su-
pervised approaches for ontology population. In Pro-
ceedings of the Eleventh Conference of the European
Chapter of the Association for Computational Linguis-
tics (EACL-2006), Trento, Italy.
</reference>
<page confidence="0.998958">
209
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.695783">
<title confidence="0.9967745">Fine-Grained Classification of Named Exploiting Latent Semantic Kernels</title>
<author confidence="0.884826">Claudio</author>
<address confidence="0.836685">I-38100, Trento,</address>
<email confidence="0.998958">giuliano@fbk.eu</email>
<abstract confidence="0.996541928571429">We present a kernel-based approach for finegrained classification of named entities. The only training data for our algorithm is a few manually annotated entities for each class. We defined kernel functions that implicitly map entities, represented by aggregating all contexts in which they occur, into a latent semantic space derived from Wikipedia. Our method achieves a significant improvement over the state of the art for the task of populating an ontology of people, although requiring considerably less training instances than previous approaches.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Javier Artiles</author>
<author>Julio Gonzalo</author>
<author>Satoshi Sekine</author>
</authors>
<title>The semeval-2007 weps evaluation: Establishing a benchmark for the web people search task.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>64--69</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="21550" citStr="Artiles et al., 2007" startWordPosition="3466" endWordPosition="3469">are celebrities, all the snippets retrieved by GoogleTMgenerally refer to them, alleviating the problem of ambiguity of proper names. However, person names are highly ambiguous. In a more realistic scenario, the result of a search engine for a person name is usually a mix of contexts about different entities sharing the same name. In this case, our approach have to be combined with a system that clusters the search engine result, where each cluster is assumed to contain all (and only those) contexts that refer to the same entity. The WePS evaluation campaign on disambiguation of person names (Artiles et al., 2007; Artiles et al., 2009) has shown that the best clustering systems achieve a precision of about 90% 206 Phy Scientist Soc Performer Fil Creator Communicator Business Health Mat Che Bio Act Mus Pai Mus Poe Dra Rep man prof Phy 118 24 10 4 2 0 0 0 0 0 0 0 0 7 2 Mat 2 33 0 0 1 0 0 0 0 0 1 0 0 3 0 Che 13 2 68 9 2 0 0 0 0 0 0 0 0 5 2 Bio 3 0 7 52 0 0 0 0 1 0 0 0 1 6 6 Soc 0 4 1 1 55 0 0 0 0 0 3 1 1 4 2 Act 0 0 0 0 0 98 5 27 0 0 2 14 0 3 0 Mus 0 0 0 0 0 17 67 0 0 32 1 0 1 2 1 Fil 0 0 0 0 0 13 0 45 0 0 1 4 0 2 0 Pai 0 0 0 1 1 2 0 1 100 0 1 0 0 1 0 Mus 0 0 0 0 0 4 29 0 0 139 0 1 0 0 0 Poe 0 2 0 0 0 0 </context>
</contexts>
<marker>Artiles, Gonzalo, Sekine, 2007</marker>
<rawString>Javier Artiles, Julio Gonzalo, and Satoshi Sekine. 2007. The semeval-2007 weps evaluation: Establishing a benchmark for the web people search task. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pages 64–69, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Javier Artiles</author>
<author>Julio Gonzalo</author>
<author>Satoshi Sekine</author>
</authors>
<title>Weps 2 evaluation campaign: overview of the web people search clustering task.</title>
<date>2009</date>
<booktitle>In 2nd Web People Search Evaluation Workshop (WePS 2009), 18th WWW Conference,</booktitle>
<location>Madrid, Spain,</location>
<contexts>
<context position="21573" citStr="Artiles et al., 2009" startWordPosition="3470" endWordPosition="3473">he snippets retrieved by GoogleTMgenerally refer to them, alleviating the problem of ambiguity of proper names. However, person names are highly ambiguous. In a more realistic scenario, the result of a search engine for a person name is usually a mix of contexts about different entities sharing the same name. In this case, our approach have to be combined with a system that clusters the search engine result, where each cluster is assumed to contain all (and only those) contexts that refer to the same entity. The WePS evaluation campaign on disambiguation of person names (Artiles et al., 2007; Artiles et al., 2009) has shown that the best clustering systems achieve a precision of about 90% 206 Phy Scientist Soc Performer Fil Creator Communicator Business Health Mat Che Bio Act Mus Pai Mus Poe Dra Rep man prof Phy 118 24 10 4 2 0 0 0 0 0 0 0 0 7 2 Mat 2 33 0 0 1 0 0 0 0 0 1 0 0 3 0 Che 13 2 68 9 2 0 0 0 0 0 0 0 0 5 2 Bio 3 0 7 52 0 0 0 0 1 0 0 0 1 6 6 Soc 0 4 1 1 55 0 0 0 0 0 3 1 1 4 2 Act 0 0 0 0 0 98 5 27 0 0 2 14 0 3 0 Mus 0 0 0 0 0 17 67 0 0 32 1 0 1 2 1 Fil 0 0 0 0 0 13 0 45 0 0 1 4 0 2 0 Pai 0 0 0 1 1 2 0 1 100 0 1 0 0 1 0 Mus 0 0 0 0 0 4 29 0 0 139 0 1 0 0 0 Poe 0 2 0 0 0 0 0 0 7 3 98 26 1 2 3 Dra</context>
</contexts>
<marker>Artiles, Gonzalo, Sekine, 2009</marker>
<rawString>Javier Artiles, Julio Gonzalo, and Satoshi Sekine. 2009. Weps 2 evaluation campaign: overview of the web people search clustering task. In 2nd Web People Search Evaluation Workshop (WePS 2009), 18th WWW Conference, Madrid, Spain, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Alex Franz</author>
</authors>
<title>Web 1T 5-gram corpus version 1, Linguistic Data Consortium,</title>
<date>2006</date>
<location>Philadelphia.</location>
<contexts>
<context position="26491" citStr="Brants and Franz, 2006" startWordPosition="4400" endWordPosition="4403">s. The system recognizes terms pertinent to the domain and assigns them the correct ontological type. Giuliano and Gliozzo (2008) present an instancebased learning algorithm for fine-grained named entity classification based on syntactic features (wordorder, case-marking, agreement, verb tenses, etc.). Their method can handle much finer distinctions than previous methods, and it is evaluated on a hierarchical taxonomy of 21 ancestors of people that was induced from WordNet. One contribution is to create this richer People Ontology. Another is to make effective use of the Web 1T 5-gram corpus (Brants and Franz, 2006) to represent syntactic information. The main difference between the two approaches lies primarily in the use of syntactic and semantic information. Our experiments show that semantic features do provide richer information than syntactic ones for a more fine-grained classification of named entities. In fact, the accuracy improvement achieved by our approach is more evident for the more specific classes. For example, the improvement in accuracy is about 14% for the class scientist, while it ranges from 25% to 46% for its sub-classes (physicist, mathematician, etc.). Kozareva et al. (2008) propo</context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram corpus version 1, Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Cimiano</author>
<author>Johanna V¨olker</author>
</authors>
<title>Towards large-scale, open-domain and ontology-based named entity classification.</title>
<date>2005</date>
<booktitle>In Proceedings of RANLP’05,</booktitle>
<pages>66--166</pages>
<location>Borovets, Bulgaria.</location>
<marker>Cimiano, V¨olker, 2005</marker>
<rawString>Philipp Cimiano and Johanna V¨olker. 2005. Towards large-scale, open-domain and ontology-based named entity classification. In Proceedings of RANLP’05, pages 66– 166–172, Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott C Deerwester</author>
<author>Susan T Dumais</author>
<author>Thoms K Landauer</author>
<author>George W Furnas</author>
<author>Richard A Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society of Information Science,</journal>
<volume>41</volume>
<issue>6</issue>
<contexts>
<context position="14047" citStr="Deerwester et al. (1990)" startWordPosition="2212" endWordPosition="2215">preserving most of the information. The matrix II is used to define a linear transformation 7r : RN → Rk, that maps the vector 0(mj), represented in the standard VSM, into the vector ˜0(mj) in the latent semantic space. Formally, 7r is defined as follows 7r(0(mj)) = 0(mj)(WII) = ˜0(mj), (6) where 0(mj) is a row vector, W is a N x N diagonal matrix determining the word weights such that Wi,i = log(idf(wi)), where idf(wi) is the inverse document frequency of wi. 1SVD has been first applied to perform latent semantic analysis of terms and latent semantic indexing of documents in large corpora by Deerwester et al. (1990). Finally, the latent semantic kernel is explicitly defined as follows KLS(mi,mj) = (7r(0(mi)),7r(0(mj))), (7) where 0 is the mapping defined in Equation 2 and 7r is the linear transformation defined in Equation 6. Note that we have used a series of successive mappings each of which adds some further improvement to the multi-context representation. 3.3 Composite Kernel Finally, to combine the two representations of multicontexts, we define the composite kernel as follows KsOW(m1, m2) + KLS(m1, m2). (8) It follows directly from the explicit construction of the feature space and from closure pro</context>
</contexts>
<marker>Deerwester, Dumais, Landauer, Furnas, Harshman, 1990</marker>
<rawString>Scott C. Deerwester, Susan T. Dumais, Thoms K. Landauer, George W. Furnas, and Richard A. Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society of Information Science, 41(6):391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Fleischman</author>
<author>Eduard Hovy</author>
</authors>
<title>Fine grained classification of named entities.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Computational Linguistics,</booktitle>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="5919" citStr="Fleischman and Hovy (2002)" startWordPosition="869" endWordPosition="872"> model with one built from a news corpus. Our approach achieves a significant improvement over the state of the art for the task of populating the People Ontology (Giuliano and Gliozzo, 2008), although requiring considerably less training instances than previous approaches. The task consists in classifying person names into a multi-level taxonomy composed of 21 categories derived from WordNet, making very fine-grained distinctions (e.g., physicists vs. mathematicians). It provides a more realistic and challenging benchmark than the ones previously available (e.g., Tanev and Magnini (2006) and Fleischman and Hovy (2002)), that consider a smaller number of categories arranged in a one-level taxonomy. 2 Entity Representation The goal of our research is to determine the finegrained categories of named entities requiring a minimal amount of human supervision. Our method is based on the common assumption that named entities co-occurring with the same (domain-specific) terms are highly probable to refer to the same categories. For example, quantum mechanics, atomic physics, and Nobel Prize in physics are all terms that bound Niels Bohr and Enrico Fermi to the concept of physics. To automatically derive features fo</context>
<context position="10918" citStr="Fleischman and Hovy, 2002" startWordPosition="1682" endWordPosition="1685">resentation does not deal well with lexical variability. To significantly reduce the training set size, we need to map contexts containing semantically equivalent terms into similar feature vectors. To this aim, in the next section, we introduce the class of semantic kernels and show how to define an effective semantic VSM using (unlabeled) external knowledge. 3.2 Semantic Kernels It has been shown that semantic information is fundamental for improving the accuracy and reducing the amount of training data in many natural language tasks, including fine-grained classification of named entities (Fleischman and Hovy, 2002), question classification (Li and Roth, 2005), text categorization (Giozzo and Strapparava, 2005), word sense disambiguation (Gliozzo et al., 2005). In the context of kernel methods, semantic information can be integrated considering linear transformations of the type ˜φ(cj) = φ(cj)S, where S is a N x k matrix (Shawe-Taylor and Cristianini, 2004). The matrix S can be rewritten as S = WP, where W is a diagonal matrix determining the word weights, while P is the word proximity matrix capturing the semantic relations between words. The proximity matrix P can be defined by setting nonzero entries </context>
<context position="23850" citStr="Fleischman and Hovy (2002)" startWordPosition="3997" endWordPosition="4000">n 20. This shows that latent semantic kernels are an effective tool for fine-grained classification of person names. Finally, table 3 shows that misclassification errors are largely distributed among categories belonging to the same super-class (i.e., the blocks on the main diagonal are more densely populated than others). As expected, the algorithm is much more accurate for the top-level concepts (i.e., Scientist, Communicator, etc.), where the category distinctions are clearer, while a further fine-grained classification, in some cases, is even difficult for human annotators. 5 Related Work Fleischman and Hovy (2002) approach the finegrained classification of person instances using supervised learning, where the training set is generated semi-automatically, bootstrapping from a small training set. They compare different machine learning algorithms, providing local features as well as global semantic information derived from topic signature and WordNet. Person instances were classified into one of eight categories. Cimiano and V¨olker (2005) present an approach for the fine-grained classification of entities relying on the Harris’ distributional hypothesis and the vector space model. They assign a particul</context>
</contexts>
<marker>Fleischman, Hovy, 2002</marker>
<rawString>Michael Fleischman and Eduard Hovy. 2002. Fine grained classification of named entities. In Proceedings of the 19th International Conference on Computational Linguistics, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aldo Gangemi</author>
<author>Roberto Navigli</author>
<author>Paola Velardi</author>
</authors>
<title>Axiomatizing WordNet glosses in the OntoWordNet project.</title>
<date>2003</date>
<booktitle>In Proocedings of the Workshop on Human Language Technology for the Semantic Web and Web Services at ISWC 2003,</booktitle>
<location>Sanibel Island, Florida.</location>
<contexts>
<context position="15258" citStr="Gangemi et al. (2003)" startWordPosition="2402" endWordPosition="2405">losure properties of kernels that it is a valid kernel. 4 Experiments In this section, we compare performance of different kernel setups and previous approaches on an ontology population task. 4.1 Benchmark Experiments were carried out on the People Ontology (Giuliano and Gliozzo, 2008). An ontology extracted from WordNet, containing 1,657 distinct person instances arranged in a multi-level taxonomy having 21 fine-grained categories (Figure 2). To provide a formal distinction between classes and instances, required to assign instances to classes, the authors followed the directives defined by Gangemi et al. (2003) for OntoWordNet, in which the informal WordNet semantics is re-engineered in terms of a description logic. In order to have a fair comparison, we reproduced the same experimental settings used in Giuliano and Gliozzo (2008). The population task is cast as a categorization problem, trying to assign person instances to the most specific category. For each class, the instances were randomly split into two equally sized subsets. One is used for training and the other for test, and vice versa. The reported results are the average performance over these two subsets. When an instance is assigned to </context>
</contexts>
<marker>Gangemi, Navigli, Velardi, 2003</marker>
<rawString>Aldo Gangemi, Roberto Navigli, and Paola Velardi. 2003. Axiomatizing WordNet glosses in the OntoWordNet project. In Proocedings of the Workshop on Human Language Technology for the Semantic Web and Web Services at ISWC 2003, Sanibel Island, Florida.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Venkatesh Ganti</author>
<author>Arnd C K¨onig</author>
<author>Rares Vernica</author>
</authors>
<title>Entity categorization over large document collections.</title>
<date>2008</date>
<booktitle>In KDD ’08: Proceeding of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>274--282</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>Ganti, K¨onig, Vernica, 2008</marker>
<rawString>Venkatesh Ganti, Arnd C. K¨onig, and Rares Vernica. 2008. Entity categorization over large document collections. In KDD ’08: Proceeding of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 274–282, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alfio Giozzo</author>
<author>Carlo Strapparava</author>
</authors>
<title>Domain kernels for text categorization.</title>
<date>2005</date>
<booktitle>In Ninth Conference on Computational Natural Language Learning (CoNLL2005),</booktitle>
<pages>56--63</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="11015" citStr="Giozzo and Strapparava, 2005" startWordPosition="1695" endWordPosition="1698">set size, we need to map contexts containing semantically equivalent terms into similar feature vectors. To this aim, in the next section, we introduce the class of semantic kernels and show how to define an effective semantic VSM using (unlabeled) external knowledge. 3.2 Semantic Kernels It has been shown that semantic information is fundamental for improving the accuracy and reducing the amount of training data in many natural language tasks, including fine-grained classification of named entities (Fleischman and Hovy, 2002), question classification (Li and Roth, 2005), text categorization (Giozzo and Strapparava, 2005), word sense disambiguation (Gliozzo et al., 2005). In the context of kernel methods, semantic information can be integrated considering linear transformations of the type ˜φ(cj) = φ(cj)S, where S is a N x k matrix (Shawe-Taylor and Cristianini, 2004). The matrix S can be rewritten as S = WP, where W is a diagonal matrix determining the word weights, while P is the word proximity matrix capturing the semantic relations between words. The proximity matrix P can be defined by setting nonzero entries between those words whose semantic relation is inferred from an external source of domain knowled</context>
</contexts>
<marker>Giozzo, Strapparava, 2005</marker>
<rawString>Alfio Giozzo and Carlo Strapparava. 2005. Domain kernels for text categorization. In Ninth Conference on Computational Natural Language Learning (CoNLL2005), pages 56–63, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudio Giuliano</author>
<author>Alfio Gliozzo</author>
</authors>
<title>Instance based lexical entailment for ontology population.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL),</booktitle>
<pages>248--256</pages>
<contexts>
<context position="2176" citStr="Giuliano and Gliozzo (2007)" startWordPosition="307" endWordPosition="311">ectly applicable for a more fine-grained categorization because the number of documents required to find sufficient positive examples for all subclasses becomes too large, making the manual annotation very expensive. Consequently, in the literature, supervised approaches are confined to classify entities into broad categories, such as persons, locations, and organizations, while the fine-grained classification has been approached with minimally supervised (e.g., Tanev and Magnini (2006) and Giuliano and Gliozzo (2008)) and unsupervised learning algorithms (e.g., Cimiano and V¨olker (2005) and Giuliano and Gliozzo (2007)). Following this trend, we present a minimally supervised approach to fine-grained categorization of named entities previously recognized into coarsegrained categories, e.g., by a named-entity recognizer. The only training data for our algorithm is a few manually annotated entities for each class. For example, Niels Bohr, Albert Einstein, and Enrico Fermi might be used as examples for the class physicists. In some cases, training entities can be acquired (semi-) automatically from existing ontologies allowing us to automatically derive training entities for use with our machine learning algor</context>
<context position="25368" citStr="Giuliano and Gliozzo (2007)" startWordPosition="4223" endWordPosition="4226">nev and Magnini (2006) proposed a weaklysupervised method that requires as training data a list of named entities, without context, for each category under consideration. Given a generic syntactically parsed corpus containing at least each training entity twice, the algorithm learns, for each category, 207 a feature vector describing the contexts where those entities occur. Then, it compares the new (unknown) entity with the so obtained feature vectors, assigning it to the most similar category. Experiments are performed on a benchmark of 5 sub-classes of location and 5 sub-classes of person. Giuliano and Gliozzo (2007) propose an unsupervised approach based on lexical entailment, consisting in assigning an entity to the category whose lexicalization can be replaced with its occurrences in a corpus preserving the meaning. Using unsupervised learning, they obtained slightly worst results than Tanev and Magnini (2006) on the same benchmark. Picca et al. (2007) present an approach for ontology learning from open domain text collections, based on the combination of Super Sense Tagging and Domain Modeling techniques. The system recognizes terms pertinent to the domain and assigns them the correct ontological type</context>
</contexts>
<marker>Giuliano, Gliozzo, 2007</marker>
<rawString>Claudio Giuliano and Alfio Gliozzo. 2007. Instance based lexical entailment for ontology population. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL), pages 248–256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudio Giuliano</author>
<author>Alfio Gliozzo</author>
</authors>
<title>Instancebased ontology population exploiting named-entity substitution.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<pages>265--272</pages>
<location>Manchester, UK,</location>
<contexts>
<context position="2072" citStr="Giuliano and Gliozzo (2008)" startWordPosition="292" endWordPosition="295"> As domain specific ontologies generally contains hundreds of subcategories, such approaches are not directly applicable for a more fine-grained categorization because the number of documents required to find sufficient positive examples for all subclasses becomes too large, making the manual annotation very expensive. Consequently, in the literature, supervised approaches are confined to classify entities into broad categories, such as persons, locations, and organizations, while the fine-grained classification has been approached with minimally supervised (e.g., Tanev and Magnini (2006) and Giuliano and Gliozzo (2008)) and unsupervised learning algorithms (e.g., Cimiano and V¨olker (2005) and Giuliano and Gliozzo (2007)). Following this trend, we present a minimally supervised approach to fine-grained categorization of named entities previously recognized into coarsegrained categories, e.g., by a named-entity recognizer. The only training data for our algorithm is a few manually annotated entities for each class. For example, Niels Bohr, Albert Einstein, and Enrico Fermi might be used as examples for the class physicists. In some cases, training entities can be acquired (semi-) automatically from existing </context>
<context position="5484" citStr="Giuliano and Gliozzo, 2008" startWordPosition="806" endWordPosition="809">tween terms, automatically learned from unlabeled data. In particular, we use Wikipedia to build the latent semantic space. The underlying idea is that similar named entities tend to have a similar description in Wikipedia. As Wikipedia provides reliable information and it exceeds all other encyclopedias in coverage, it should be a valuable resource for the task of populating an ontology. To validate this hypothesis, we compare this model with one built from a news corpus. Our approach achieves a significant improvement over the state of the art for the task of populating the People Ontology (Giuliano and Gliozzo, 2008), although requiring considerably less training instances than previous approaches. The task consists in classifying person names into a multi-level taxonomy composed of 21 categories derived from WordNet, making very fine-grained distinctions (e.g., physicists vs. mathematicians). It provides a more realistic and challenging benchmark than the ones previously available (e.g., Tanev and Magnini (2006) and Fleischman and Hovy (2002)), that consider a smaller number of categories arranged in a one-level taxonomy. 2 Entity Representation The goal of our research is to determine the finegrained ca</context>
<context position="14924" citStr="Giuliano and Gliozzo, 2008" startWordPosition="2353" endWordPosition="2356">successive mappings each of which adds some further improvement to the multi-context representation. 3.3 Composite Kernel Finally, to combine the two representations of multicontexts, we define the composite kernel as follows KsOW(m1, m2) + KLS(m1, m2). (8) It follows directly from the explicit construction of the feature space and from closure properties of kernels that it is a valid kernel. 4 Experiments In this section, we compare performance of different kernel setups and previous approaches on an ontology population task. 4.1 Benchmark Experiments were carried out on the People Ontology (Giuliano and Gliozzo, 2008). An ontology extracted from WordNet, containing 1,657 distinct person instances arranged in a multi-level taxonomy having 21 fine-grained categories (Figure 2). To provide a formal distinction between classes and instances, required to assign instances to classes, the authors followed the directives defined by Gangemi et al. (2003) for OntoWordNet, in which the informal WordNet semantics is re-engineered in terms of a description logic. In order to have a fair comparison, we reproduced the same experimental settings used in Giuliano and Gliozzo (2008). The population task is cast as a categor</context>
<context position="17525" citStr="Giuliano and Gliozzo, 2008" startWordPosition="2766" endWordPosition="2769"> 5 times, the resulting dictionaries contain about 300,000 and 150,000 terms respectively. We used the SVDLIBC package2 to compute the SVD, truncated to 400 dimensions. To derive the multi-context representation, we collected 100 english snippets for each person instance by querying GoogleTM. To classify each person instance into one of the fine-grained categories, we used a KNN classifier (K = 1). No parameter optimization was performed. 2http://tedlab.mit.edu/˜dr/svdlibc/ 4.3 Results Table 1 shows micro- and macro-averaged results for KBOW, KW, KBOW + KW, KNYT, KBOW + KNYT, the IBOP method (Giuliano and Gliozzo, 2008), the random baseline, and most frequent baseline.3 Where KW and KNYT are instances of the latent semantic kernel, KLS, using the proximity matrices UW and UNYT, derived from Wikipedia and the New York Times corpus, respectively. Table 2 shows detailed results for each sub- and supercategory for KBOW + KW. Table 3 shows the confusion matrix of KBOW + KW, in which the rows are ground truth classes and the columns are predictions. The matrix has been calculated for the finergrained categories and, then, grouped according to their super-class. To be compared with the IBOP method, all experiments </context>
<context position="19526" citStr="Giuliano and Gliozzo, 2008" startWordPosition="3138" endWordPosition="3141">W KW KBOW+KW Micro F1 0.8 0.75 0.7 0.65 0.6 0.55 0.5 Micro F1 0.8 0.75 0.7 0.65 0.6 0.55 0.5 Micro F1 0.8 Micro F1 0.75 0.7 0.65 0.6 0.55 0.5 0.8 0.75 0.7 0.65 0.6 0.55 0.5 KIOW KW KIOW+KW Figure 3: Learning curves for KBOW + KW obtained varying the number of snippets used to derive the training and test sets. From top-left to bottom right: 12, 25, 50, and 100. Method Micro-F1 Macro-F1 KBOW 75.6 70.6 KW 78.1 73.1 KBpW + KW 80.0 75.4 KNYT 77.6 72.9 KBOW +KNYT 79.7 75.1 IBOP 70.1 62.3 Random 15.4 15.5 Most Frequent 20.7 3.3 Table 1: Comparison among the kernel-based approaches, the IBOP method (Giuliano and Gliozzo, 2008), the random baseline, and most frequent baseline. more accurate semantic models than general purpose corpora. Moreover, further experiments have shown that even a larger number of Wikipedia articles (600,000) does not help. On the other hand, the latent semantic kernels outperform all the other methods, and their composite (KBOW + KW and KBOW + KNYT) perform the best on every configuration, demonstrating the effectiveness of latent semantic kernels in fine-grained classification of named entities. As in text categorization and word sense disambiguation, they have proven effective tools to ove</context>
<context position="25997" citStr="Giuliano and Gliozzo (2008)" startWordPosition="4323" endWordPosition="4326">ropose an unsupervised approach based on lexical entailment, consisting in assigning an entity to the category whose lexicalization can be replaced with its occurrences in a corpus preserving the meaning. Using unsupervised learning, they obtained slightly worst results than Tanev and Magnini (2006) on the same benchmark. Picca et al. (2007) present an approach for ontology learning from open domain text collections, based on the combination of Super Sense Tagging and Domain Modeling techniques. The system recognizes terms pertinent to the domain and assigns them the correct ontological type. Giuliano and Gliozzo (2008) present an instancebased learning algorithm for fine-grained named entity classification based on syntactic features (wordorder, case-marking, agreement, verb tenses, etc.). Their method can handle much finer distinctions than previous methods, and it is evaluated on a hierarchical taxonomy of 21 ancestors of people that was induced from WordNet. One contribution is to create this richer People Ontology. Another is to make effective use of the Web 1T 5-gram corpus (Brants and Franz, 2006) to represent syntactic information. The main difference between the two approaches lies primarily in the </context>
</contexts>
<marker>Giuliano, Gliozzo, 2008</marker>
<rawString>Claudio Giuliano and Alfio Gliozzo. 2008. Instancebased ontology population exploiting named-entity substitution. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 265–272, Manchester, UK, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alfio Massimiliano Gliozzo</author>
<author>Claudio Giuliano</author>
<author>Carlo Strapparava</author>
</authors>
<title>Domain kernels for word sense disambiguation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>403--410</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="11065" citStr="Gliozzo et al., 2005" startWordPosition="1703" endWordPosition="1706">equivalent terms into similar feature vectors. To this aim, in the next section, we introduce the class of semantic kernels and show how to define an effective semantic VSM using (unlabeled) external knowledge. 3.2 Semantic Kernels It has been shown that semantic information is fundamental for improving the accuracy and reducing the amount of training data in many natural language tasks, including fine-grained classification of named entities (Fleischman and Hovy, 2002), question classification (Li and Roth, 2005), text categorization (Giozzo and Strapparava, 2005), word sense disambiguation (Gliozzo et al., 2005). In the context of kernel methods, semantic information can be integrated considering linear transformations of the type ˜φ(cj) = φ(cj)S, where S is a N x k matrix (Shawe-Taylor and Cristianini, 2004). The matrix S can be rewritten as S = WP, where W is a diagonal matrix determining the word weights, while P is the word proximity matrix capturing the semantic relations between words. The proximity matrix P can be defined by setting nonzero entries between those words whose semantic relation is inferred from an external source of domain knowledge. The semantic kernel takes the general form ˜k(</context>
</contexts>
<marker>Gliozzo, Giuliano, Strapparava, 2005</marker>
<rawString>Alfio Massimiliano Gliozzo, Claudio Giuliano, and Carlo Strapparava. 2005. Domain kernels for word sense disambiguation. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 403–410, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zornitsa Kozareva</author>
<author>Sonia Vazquez</author>
<author>Andres Montoyo</author>
</authors>
<title>Domain information for fine-grained person name categorization.</title>
<date>2008</date>
<booktitle>In 9th International Conference on Intelligent Text Processing and Computational Linguistics (CICLing</booktitle>
<pages>311--321</pages>
<location>Haifa,</location>
<contexts>
<context position="27085" citStr="Kozareva et al. (2008)" startWordPosition="4493" endWordPosition="4496">pus (Brants and Franz, 2006) to represent syntactic information. The main difference between the two approaches lies primarily in the use of syntactic and semantic information. Our experiments show that semantic features do provide richer information than syntactic ones for a more fine-grained classification of named entities. In fact, the accuracy improvement achieved by our approach is more evident for the more specific classes. For example, the improvement in accuracy is about 14% for the class scientist, while it ranges from 25% to 46% for its sub-classes (physicist, mathematician, etc.). Kozareva et al. (2008) propose an approach for person name categorization based on the domain distribution. They use the information provided by WordNet Domains to generated lists of words relevant for a given domain, by mapping and ranking the words from the WordNet glosses to their WordNet Domains. A named entity is then classified according the similarity between the word-domain lists and the global context in which the entity appears. However, the evaluation was performed only on 6 person names using two categories. Ganti et al. (2008) present a method that considers an entity’s context across multiple document</context>
</contexts>
<marker>Kozareva, Vazquez, Montoyo, 2008</marker>
<rawString>Zornitsa Kozareva, Sonia Vazquez, and Andres Montoyo. 2008. Domain information for fine-grained person name categorization. In 9th International Conference on Intelligent Text Processing and Computational Linguistics (CICLing 2008), pages 311–321, Haifa, Israel, 17-23 February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xin Li</author>
<author>Dan Roth</author>
</authors>
<title>Learning question classifiers: the role of semantic information.</title>
<date>2005</date>
<journal>Natural Language Engineering,</journal>
<volume>12</volume>
<issue>3</issue>
<contexts>
<context position="10963" citStr="Li and Roth, 2005" startWordPosition="1689" endWordPosition="1692">ty. To significantly reduce the training set size, we need to map contexts containing semantically equivalent terms into similar feature vectors. To this aim, in the next section, we introduce the class of semantic kernels and show how to define an effective semantic VSM using (unlabeled) external knowledge. 3.2 Semantic Kernels It has been shown that semantic information is fundamental for improving the accuracy and reducing the amount of training data in many natural language tasks, including fine-grained classification of named entities (Fleischman and Hovy, 2002), question classification (Li and Roth, 2005), text categorization (Giozzo and Strapparava, 2005), word sense disambiguation (Gliozzo et al., 2005). In the context of kernel methods, semantic information can be integrated considering linear transformations of the type ˜φ(cj) = φ(cj)S, where S is a N x k matrix (Shawe-Taylor and Cristianini, 2004). The matrix S can be rewritten as S = WP, where W is a diagonal matrix determining the word weights, while P is the word proximity matrix capturing the semantic relations between words. The proximity matrix P can be defined by setting nonzero entries between those words whose semantic relation i</context>
</contexts>
<marker>Li, Roth, 2005</marker>
<rawString>Xin Li and Dan Roth. 2005. Learning question classifiers: the role of semantic information. Natural Language Engineering, 12(3):229–249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
<author>Philip Resnik</author>
</authors>
<title>Tagger evaluation given hierarchical tag sets. Computers and the Humanities,</title>
<date>2000</date>
<pages>79--84</pages>
<contexts>
<context position="16297" citStr="Melamed and Resnik (2000)" startWordPosition="2573" endWordPosition="2576"> sized subsets. One is used for training and the other for test, and vice versa. The reported results are the average performance over these two subsets. When an instance is assigned to a sub-class it is also implicitly assigned to all its super-classes. For instance, 204 Figure 2: The People Ontology defined by Giuliano and Gliozzo (2008). Numbers in brackets are the total numbers of person instances per category. Concepts with less than 40 instances were removed. classifying Salvador Dali as painter we implicitly classify him as artist and creator. The evaluation is performed as proposed by Melamed and Resnik (2000) for a similar hierarchical categorization task. For instance, classifying John Lennon as painter, we obtain a false positive for the spurious classification painter, a false negative for missing class musician, and two true positives for the correct assignment to the super-classes artist and creator. 4.2 Experimental Settings We built two proximity matrices UW and UNYT. The former is derived from the 200,000 most visited Wikipedia articles, while the latter from 200,000 articles published by the New York Times between June 1, 1998 and January 01, 2000. After removing terms that occur less tha</context>
</contexts>
<marker>Melamed, Resnik, 2000</marker>
<rawString>I. Dan Melamed and Philip Resnik. 2000. Tagger evaluation given hierarchical tag sets. Computers and the Humanities, pages 79–84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Davide Picca</author>
<author>Alfio Gliozzo</author>
<author>Massimiliano Ciaramita</author>
</authors>
<title>Semantic domains and supersense tagging for domain-specific ontology learning.</title>
<date>2007</date>
<booktitle>In David Evans, Sadaoki Furui, and Chantal Soul´e-Dupuy, editors, Recherche d’Information Assist´ee par Ordinateur (RIAO),</booktitle>
<location>Pittsburgh, PA, USA,</location>
<contexts>
<context position="25713" citStr="Picca et al. (2007)" startWordPosition="4279" endWordPosition="4282">e those entities occur. Then, it compares the new (unknown) entity with the so obtained feature vectors, assigning it to the most similar category. Experiments are performed on a benchmark of 5 sub-classes of location and 5 sub-classes of person. Giuliano and Gliozzo (2007) propose an unsupervised approach based on lexical entailment, consisting in assigning an entity to the category whose lexicalization can be replaced with its occurrences in a corpus preserving the meaning. Using unsupervised learning, they obtained slightly worst results than Tanev and Magnini (2006) on the same benchmark. Picca et al. (2007) present an approach for ontology learning from open domain text collections, based on the combination of Super Sense Tagging and Domain Modeling techniques. The system recognizes terms pertinent to the domain and assigns them the correct ontological type. Giuliano and Gliozzo (2008) present an instancebased learning algorithm for fine-grained named entity classification based on syntactic features (wordorder, case-marking, agreement, verb tenses, etc.). Their method can handle much finer distinctions than previous methods, and it is evaluated on a hierarchical taxonomy of 21 ancestors of peop</context>
</contexts>
<marker>Picca, Gliozzo, Ciaramita, 2007</marker>
<rawString>Davide Picca, Alfio Gliozzo, and Massimiliano Ciaramita. 2007. Semantic domains and supersense tagging for domain-specific ontology learning. In David Evans, Sadaoki Furui, and Chantal Soul´e-Dupuy, editors, Recherche d’Information Assist´ee par Ordinateur (RIAO), Pittsburgh, PA, USA, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Sch¨olkopf</author>
<author>A Smola</author>
</authors>
<title>Learning with Kernels.</title>
<date>2002</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<marker>Sch¨olkopf, Smola, 2002</marker>
<rawString>B. Sch¨olkopf and A. Smola. 2002. Learning with Kernels. MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Shawe-Taylor</author>
<author>N Cristianini</author>
</authors>
<title>Kernel Methods for Pattern Analysis.</title>
<date>2004</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="4736" citStr="Shawe-Taylor and Cristianini, 2004" startWordPosition="689" endWordPosition="692">that Enrico Fermi is a person and a professor. However, to discover that he is a physicist we need to analyze a wider context, or alternatively multiple ones. Recently, Ganti et al. (2008) has shown that exploiting multi-context information can greatly improve the fine-grained classification of named entities, when compared to methods using single context only. In order to effectively represent entities’ multicontexts, we extend the traditional vector space model (VSM), offering a way to integrate external semantic information in the classification process by means of latent semantic kernels (Shawe-Taylor and Cristianini, 2004). As a result, we obtain a generalized similarity function between multi-contexts that incorporates semantic relations between terms, automatically learned from unlabeled data. In particular, we use Wikipedia to build the latent semantic space. The underlying idea is that similar named entities tend to have a similar description in Wikipedia. As Wikipedia provides reliable information and it exceeds all other encyclopedias in coverage, it should be a valuable resource for the task of populating an ontology. To validate this hypothesis, we compare this model with one built from a news corpus. O</context>
<context position="11266" citStr="Shawe-Taylor and Cristianini, 2004" startWordPosition="1737" endWordPosition="1740">ed) external knowledge. 3.2 Semantic Kernels It has been shown that semantic information is fundamental for improving the accuracy and reducing the amount of training data in many natural language tasks, including fine-grained classification of named entities (Fleischman and Hovy, 2002), question classification (Li and Roth, 2005), text categorization (Giozzo and Strapparava, 2005), word sense disambiguation (Gliozzo et al., 2005). In the context of kernel methods, semantic information can be integrated considering linear transformations of the type ˜φ(cj) = φ(cj)S, where S is a N x k matrix (Shawe-Taylor and Cristianini, 2004). The matrix S can be rewritten as S = WP, where W is a diagonal matrix determining the word weights, while P is the word proximity matrix capturing the semantic relations between words. The proximity matrix P can be defined by setting nonzero entries between those words whose semantic relation is inferred from an external source of domain knowledge. The semantic kernel takes the general form ˜k(mi, mj) = φ(mi)SS�φ(mj)� = ˜φ(mi)˜φ(mj)�.(4) It follows directly from the explicit construction that Equation 4 defines a valid kernel. WordNet and manually constructed lists of semantically related wo</context>
</contexts>
<marker>Shawe-Taylor, Cristianini, 2004</marker>
<rawString>J. Shawe-Taylor and N. Cristianini. 2004. Kernel Methods for Pattern Analysis. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian M Suchanek</author>
<author>Gjergji Kasneci</author>
<author>Gerhard Weikum</author>
</authors>
<title>YAGO: A large ontology from wikipedia and wordnet.</title>
<date>2008</date>
<journal>Elsevier Journal of Web Semantics.</journal>
<contexts>
<context position="2986" citStr="Suchanek et al., 2008" startWordPosition="430" endWordPosition="433">ntity recognizer. The only training data for our algorithm is a few manually annotated entities for each class. For example, Niels Bohr, Albert Einstein, and Enrico Fermi might be used as examples for the class physicists. In some cases, training entities can be acquired (semi-) automatically from existing ontologies allowing us to automatically derive training entities for use with our machine learning algorithm. For instance, we may easily obtain tens of training entities for very specific classes, such as astronomers, materials scientists, nuclear physicists, by querying the Yago ontology (Suchanek et al., 2008). We represent the entities using features extracted from the textual contexts in which they occur. Specifically, we use a search engine to collect such contexts from the Web. Throughout this paper, we will refer to such a representation as multi-context representation, in contrast to the single-context rep201 Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 201–209, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics resentation in which an entity is categorized using solely features extracted from the local context </context>
</contexts>
<marker>Suchanek, Kasneci, Weikum, 2008</marker>
<rawString>Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2008. YAGO: A large ontology from wikipedia and wordnet. Elsevier Journal of Web Semantics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Partha Pratim Talukdar</author>
<author>Joseph Reisinger</author>
<author>Marius Pasca</author>
<author>Deepak Ravichandran</author>
<author>Rahul Bhagat</author>
<author>Fernando Pereira</author>
</authors>
<title>Weakly-supervised acquisition of labeled class instances using graph random walks.</title>
<date>2008</date>
<booktitle>In Proceedings of the conference on Empirical Methods in Natural Language Processing (EMNLP), Waikiki,</booktitle>
<pages>25--27</pages>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="28044" citStr="Talukdar et al. (2008)" startWordPosition="4644" endWordPosition="4647">y between the word-domain lists and the global context in which the entity appears. However, the evaluation was performed only on 6 person names using two categories. Ganti et al. (2008) present a method that considers an entity’s context across multiple documents containing it, and exploiting word n-grams and existing large list of related entities as features. They generated training and test data using Wikipedia articles that contain list of instances. They compare their system with a single-context classifier, showing that their approach based on aggregate context perform better. Finally, Talukdar et al. (2008) propose a graphbased semi-supervised label propagation algorithm for acquiring open-domain labeled classes and their instances from a combination of unstructured and structured text. 6 Conclusions We presented an approach to automatic fine-grained categorization of named entities based on kernel methods. Entities are represented by aggregating all contexts in which they occur. We employed latent semantic kernels to extend the bag-of-words representation. The latent semantic models were derived from Wikipedia and a news corpus We evaluated our approach on the People Ontology, a multi-level ont</context>
</contexts>
<marker>Talukdar, Reisinger, Pasca, Ravichandran, Bhagat, Pereira, 2008</marker>
<rawString>Partha Pratim Talukdar, Joseph Reisinger, Marius Pasca, Deepak Ravichandran, Rahul Bhagat, and Fernando Pereira. 2008. Weakly-supervised acquisition of labeled class instances using graph random walks. In Proceedings of the conference on Empirical Methods in Natural Language Processing (EMNLP), Waikiki, Honolulu, Hawaii, October 25-27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hristo Tanev</author>
<author>Bernardo Magnini</author>
</authors>
<title>Weakly supervised approaches for ontology population.</title>
<date>2006</date>
<booktitle>In Proceedings of the Eleventh Conference of the European Chapter of the Association for Computational Linguistics (EACL-2006),</booktitle>
<location>Trento, Italy.</location>
<contexts>
<context position="2040" citStr="Tanev and Magnini (2006)" startWordPosition="287" endWordPosition="290">sisting of hand tagged texts. As domain specific ontologies generally contains hundreds of subcategories, such approaches are not directly applicable for a more fine-grained categorization because the number of documents required to find sufficient positive examples for all subclasses becomes too large, making the manual annotation very expensive. Consequently, in the literature, supervised approaches are confined to classify entities into broad categories, such as persons, locations, and organizations, while the fine-grained classification has been approached with minimally supervised (e.g., Tanev and Magnini (2006) and Giuliano and Gliozzo (2008)) and unsupervised learning algorithms (e.g., Cimiano and V¨olker (2005) and Giuliano and Gliozzo (2007)). Following this trend, we present a minimally supervised approach to fine-grained categorization of named entities previously recognized into coarsegrained categories, e.g., by a named-entity recognizer. The only training data for our algorithm is a few manually annotated entities for each class. For example, Niels Bohr, Albert Einstein, and Enrico Fermi might be used as examples for the class physicists. In some cases, training entities can be acquired (sem</context>
<context position="5888" citStr="Tanev and Magnini (2006)" startWordPosition="864" endWordPosition="867">s hypothesis, we compare this model with one built from a news corpus. Our approach achieves a significant improvement over the state of the art for the task of populating the People Ontology (Giuliano and Gliozzo, 2008), although requiring considerably less training instances than previous approaches. The task consists in classifying person names into a multi-level taxonomy composed of 21 categories derived from WordNet, making very fine-grained distinctions (e.g., physicists vs. mathematicians). It provides a more realistic and challenging benchmark than the ones previously available (e.g., Tanev and Magnini (2006) and Fleischman and Hovy (2002)), that consider a smaller number of categories arranged in a one-level taxonomy. 2 Entity Representation The goal of our research is to determine the finegrained categories of named entities requiring a minimal amount of human supervision. Our method is based on the common assumption that named entities co-occurring with the same (domain-specific) terms are highly probable to refer to the same categories. For example, quantum mechanics, atomic physics, and Nobel Prize in physics are all terms that bound Niels Bohr and Enrico Fermi to the concept of physics. To a</context>
<context position="24763" citStr="Tanev and Magnini (2006)" startWordPosition="4128" endWordPosition="4131">information derived from topic signature and WordNet. Person instances were classified into one of eight categories. Cimiano and V¨olker (2005) present an approach for the fine-grained classification of entities relying on the Harris’ distributional hypothesis and the vector space model. They assign a particular instance to the most similar concept representing both with lexical-syntactic features extracted from the context of the instance and the lexicalization of the concept, respectively. Experiments were performed using a large ontology with 682 concepts (unfortunately not yet available). Tanev and Magnini (2006) proposed a weaklysupervised method that requires as training data a list of named entities, without context, for each category under consideration. Given a generic syntactically parsed corpus containing at least each training entity twice, the algorithm learns, for each category, 207 a feature vector describing the contexts where those entities occur. Then, it compares the new (unknown) entity with the so obtained feature vectors, assigning it to the most similar category. Experiments are performed on a benchmark of 5 sub-classes of location and 5 sub-classes of person. Giuliano and Gliozzo (</context>
</contexts>
<marker>Tanev, Magnini, 2006</marker>
<rawString>Hristo Tanev and Bernardo Magnini. 2006. Weakly supervised approaches for ontology population. In Proceedings of the Eleventh Conference of the European Chapter of the Association for Computational Linguistics (EACL-2006), Trento, Italy.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>