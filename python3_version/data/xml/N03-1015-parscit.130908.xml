<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.959930666666667">
Proceedings of HLT-NAACL 2003
Main Papers , pp. 32-39
Edmonton, May-June 2003
</note>
<title confidence="0.973575">
Word Sense Acquisition from Bilingual Comparable Corpora
</title>
<author confidence="0.998624">
Hiroyuki Kaji
</author>
<affiliation confidence="0.99961">
Central Research Laboratory, Hitachi, Ltd.
</affiliation>
<address confidence="0.964906">
1-280 Higashi-Koigakubo, Kokubunji-shi, Tokyo 185-8601, Japan
</address>
<email confidence="0.998719">
kaji@crl.hitachi.co.jp
</email>
<sectionHeader confidence="0.996656" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999895368421053">
Manually constructing an inventory of word
senses has suffered from problems including
high cost, arbitrary assignment of meaning to
words, and mismatch to domains. To over-
come these problems, we propose a method
to assign word meaning from a bilingual
comparable corpus and a bilingual dictionary.
It clusters second-language translation
equivalents of a first-language target word on
the basis of their translingually aligned dis-
tribution patterns. Thus it produces a hierar-
chy of corpus-relevant meanings of the target
word, each of which is defined with a set of
translation equivalents. The effectiveness of
the method has been demonstrated through an
experiment using a comparable corpus con-
sisting of Wall Street Journal and Nihon Kei-
zai Shimbun corpora together with the EDR
bilingual dictionary.
</bodyText>
<sectionHeader confidence="0.99879" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999068978723405">
Word Sense Disambiguation (WSD) is an important
subtask that is necessary for accomplishing most natu-
ral language processing tasks including machine
translation and information retrieval. A great deal of
research on WSD has been done over the past decade
(Ide and Veronis, 1998). In contrast, word sense acqui-
sition has been a human activity; inventories of word
senses have been constructed by lexicographers based
on their intuition. Manually constructing an inventory
of word senses has suffered from problems such as
high cost, arbitrary division of word senses, and mis-
match to application domains.
We address the problem of word sense acquisition
along the lines of the WSD where word senses are
defined with sets of translation equivalents in another
language. Bilingual corpora or second-language cor-
pora enable unsupervised WSD (Brown, et al., 1991;
Dagan and Itai, 1994). However, the correspondence
between senses of a word and its translations is not
one-to-one, and therefore we need to prepare an in-
ventory of word senses, each of which is defined with
a set of synonymous translation equivalents. Although
conventional bilingual dictionaries usually group
translations according to their senses, the grouping
differs by dictionary. In addition, senses specific to a
domain are often missing while many senses irrelevant
to the domain or rare senses are included. To over-
come these problems, we propose a method for pro-
ducing a hierarchy of clusters of translation equiva-
lents from a bilingual corpus and a bilingual diction-
ary.
To the best of our knowledge, there are two pre-
ceding research papers on word sense acquisition (Fu-
kumoto and Tsujii, 1994; Pantel and Lin, 2002). Both
proposed distributional word clustering algorithms that
are characterized by their capabilities to produce
overlapping clusters. According to their algorithms, a
polysemous word is assigned to multiple clusters, each
of which represents one of its senses. These and our
approach differ in how to define the word sense, i.e., a
set of synonyms in the same language versus a set of
translation equivalents in another language. Schuetze
(1998) proposed a method for dividing occurrences of
a word into classes, each of which consists of contex-
tually similar occurrences. However, it does not pro-
duce definitions of senses such as sets of synonyms
and sets of translation equivalents.
</bodyText>
<sectionHeader confidence="0.993569" genericHeader="method">
2 Basic Idea
</sectionHeader>
<subsectionHeader confidence="0.999597">
2.1 Clustering of translation equivalents
</subsectionHeader>
<bodyText confidence="0.999908826086956">
Most work on automatic extraction of synonyms from
text corpora rests on the idea that synonyms have
similar distribution patterns (Hindle, 1990; Peraira, et
al., 1993; Grefenstette, 1994). This idea is also useful
for our task, i.e., extracting sets of synonymous trans-
lation equivalents, and we adopt the approach to dis-
tributional word clustering.
We need to mention that the singularity of our task
makes the problem easier. First, we do not have to
cluster all words of a language, but we only have to
cluster a small number of translation equivalents for
each target word, whose senses are to be extracted,
separately. As a result, the problem of computational
efficiency becomes less serious. Second, even if a
translation equivalent itself is polysemous, it is not
necessary to consider senses that are irrelevant to the
target word. A translation equivalent usually represents
one and only one sense of the target word, at least in
case the language-pair is those with different origins
like English and Japanese. Therefore, a
non-overlapping clustering algorithm, which is far
simpler than overlapping clustering algorithms, is suf-
ficient.
</bodyText>
<subsectionHeader confidence="0.99151">
2.2 Translingual distributional word clustering
</subsectionHeader>
<bodyText confidence="0.99993065">
In conventional distributional word clustering, a word
is characterized by a vector or weighted set consisting
of words in the same language as that of the word it-
self. In contrast, we propose a translingual distribu-
tional word clustering method, whereby a word is
characterized by a vector or weighted set consisting of
words in another language. It is based on the
sense-vs.-clue correlation matrix calculation method
we originally developed for unsupervised WSD (Kaji
and Morimoto, 2002). That method presupposes that
each sense of a target word x is defined with a syno-
nym set consisting of the target word itself and one or
more translation equivalents which represent the sense.
It calculates correlations between the senses of x and
the words statistically related to x, which act as clues
for determining the sense of x, on the basis of
translingual alignment of pairs of related words. Rows
of the resultant correlation matrix are regarded as
translingual distribution patterns characterizing trans-
lation equivalents.
</bodyText>
<footnote confidence="0.5511165">
Sense-vs.-clue correlation matrix calculation
method *)
1) Alignment of pairs of related words
*) A description of the wild-card pair of related words, which plays
an essential role in recovering alignment failure, has been omitted
for simplicity.
</footnote>
<bodyText confidence="0.933478885714286">
Let X(x) be the set of clues for determining the sen-
se of a first-language target word x. That is,
X(x)={x|(x, x)ERX},
where RX denotes the collection of pairs of related
words extracted from a corpus of the first language.
Henceforth, the j-th clue for determining the sense of x
will be denoted as x(j). Furthermore, let Y(x, x(j)) be
the set consisting of all second-language counterparts
of a first-language pair of related words x and x(j).
That is,
Y(x, x(j)) = {(y, y)  |(y, y)ERY, (x, y)ED,
(x(j), y)ED},
where RY denotes the collection of pairs of related
words extracted from a corpus of the second language,
and D denotes a bilingual dictionary, i.e., a collection
of pairs consisting of a first-language word and a
second-language word that are translations of one an-
other.
Then, for each alignment, i.e., pair of (x, x(j)) and
(y, y) (EY(x, x(j))), a weighted set of common re-
lated words Z((x, x(j)), (y, y )) is constructed as fol-
lows:
Z((x, x(j)), (y, y )) = {x / w(x)  |(x, x)ERX,
(x(j), x)ERX}.
The weight of x, denoted as w(x), is determined as
follows:
- w(x) = 1+α·MI(y, y) when Ely (x, y)ED,
(y, y)ERY, and (y, y)ERY .
- w(x) = 1 otherwise.
This is where MI(y, y) is the mutual information of y
and y. The coefficient α was set to 5 in the experiment
described in Section 4.
2) Calculation of correlation between senses and clues
The correlation between the i-th sense S(x, i) and
the j-th clue x(j) is defined as:
</bodyText>
<equation confidence="0.990801928571429">
CS x i x &apos; j MIx x &apos; j
( ( , ), ( ) ) (
= , ( ) ⋅)
A x x &apos; j y y &apos; S x i
( ( , ( ) , , , ( , )
) ( ) )
,
y S x i
∈ ( , )
1
A x x &apos; j y y &apos; S x k
( ( , ( ) , , , ( , )
) ( ) IQ
J
</equation>
<bodyText confidence="0.999608166666667">
This is where MI(x, x(j)) is the mutual information of
x and x(j), and A((x, x(j)), (y, y), S(x,i)), the plausi-
bility of alignment of (x, x(j)) with (y, y) suggesting
S(x, i), is defined as the weighted sum of the correla-
tions between the sense and the common related words,
i.e.,
</bodyText>
<equation confidence="0.981269617647059">
A x x &apos; j y y &apos; S x i
( ( , ( )), ( , ), ( , ) )=
x&amp;quot; Z x x&apos; j y
∈ (( , ( )), (
The correlations between senses and clues are cal-
( ( , ), .)
S x i x &amp;quot;
)
⋅C
x&amp;quot;
w(
E
(y
max
y &apos;
) ( , ( )),
∈ Y x x&apos; j
max
max
,
k
� I�
Il
) Y ( , ( )),
∈ x x&apos; j
(y
y &apos;
y S
∈
(x, k)
.
,
y&apos;
))
</equation>
<bodyText confidence="0.995424270833334">
culated iteratively with the following initial values:
CO(S(x, i), x(j))=MI(x, x(j)). The number of iterations
was set to 6 in the experiment. Figure 1 shows how the
correlation values converge.
Advantages of using translingually aligned
distribution patterns
Translingual distributional word clustering has advan-
tages over conventional monolingual distributional
word clustering, when they are used to cluster transla-
tion equivalents of a target word. First, it avoids clus-
ters being degraded by polysemous translation equiva-
lents. Let &amp;quot;race&amp;quot; be the target word. One of its
translation equivalents, &amp;quot;L .A&lt;REESU&gt;&amp;quot;, is a poly-
semous word representing &amp;quot;lace&amp;quot; as well as &amp;quot;race&amp;quot;.
According to monolingual distributional word cluster-
ing, &amp;quot;L .A&lt;REESU&gt;&amp;quot; is characterized by a mixture of
the distribution pattern for &amp;quot;L .A&lt;REESU&gt;&amp;quot; repre-
senting &amp;quot;race&amp;quot; and that for &amp;quot;L .A&lt;REESU&gt;&amp;quot; repre-
senting &amp;quot;lace&amp;quot;, which often results in degraded clusters.
In contrast, according to translingual distributional
word clustering, &amp;quot;L .A&lt;REESU&gt;&amp;quot; is characterized by
the distribution pattern for the sense of &amp;quot;race&amp;quot; that
means &amp;quot;competition&amp;quot;.
Second, translingual distributional word clustering
can exclude from the clusters translation equivalents
irrelevant to the corpus. For example, a bilingual dic-
tionary renders &amp;quot;*M&lt;TOKUCHOU&gt;&amp;quot; (&amp;quot;feature&amp;quot;) as a
translation of &amp;quot;race&amp;quot;, but that sense of &amp;quot;race&amp;quot; is used
infrequently. If it is the case in a given domain, &amp;quot;*M
&lt;TOKUCHOU&gt;&amp;quot; has low correlation with most words
related to &amp;quot;race&amp;quot;, and can therefore be excluded from
any clusters.
We should also mention the data-sparseness prob-
lem that hampers distributional word clustering. Gen-
erally speaking, the problem becomes more difficult in
translingual distributional word clustering, since the
sparseness of data in two languages is multiplied.
However, the sense-vs.-clue correlation matrix calcu-
lation method overcomes this difficulty; it calculates
the correlations between senses and clues iteratively to
smooth out the sparse data.
Translingual distributional word clustering can also
be implemented on the basis of word-for-word align-
ment of a parallel corpus. However, availability of
large parallel corpora is extremely limited. In contrast,
the sense-vs.-clue correlation calculation method ac-
cepts comparable corpora which are available in many
domains.
</bodyText>
<subsectionHeader confidence="0.888096">
2.3 Similarity based on subordinate distribu-
tion pattern
</subsectionHeader>
<bodyText confidence="0.957623">
Naive translingual distributional word clustering based
</bodyText>
<figure confidence="0.80998105">
2.5
2.0
1.5
1.0
0.5
0.0
0 1 2 3 4 5 6 7 8 9 10
Iteration
C(S1, brand) C(S2, brand)
C(S3, brand) C(S1, woman)
C(S2, woman) C(S3, woman)
S1={promotion, € C &lt;SENDEN&gt;, 7° u -C- -/ a �
&lt;PUROMOUSHON&gt;, �_-Bi�&lt;URIKOMI&gt;, ...)
(&amp;quot;an activity intended to help sell a product&amp;quot;)
S2={promotion, -4*&lt;SHOUKAKU&gt;, -4&amp;&lt;SHOUSHIN&gt;,
&lt;TOUYOU&gt;, ...)
(&amp;quot;advancement in rank or position&amp;quot;)
S3={promotion, !D &lt;SHOUREI&gt;, WX&lt;SHINKOU&gt;, M
Yk&lt;JOCHOU&gt;,...)
(&amp;quot;action to help something develop or succeed&amp;quot;)
</figure>
<figureCaption confidence="0.997496">
Figure 1. Convergence of correlation between
senses and clues.
</figureCaption>
<bodyText confidence="0.990498">
on the sense-vs.-clue correlation matrix calculation
method is outlined in the following steps:
</bodyText>
<listItem confidence="0.9914895">
1) Define the sense of a target word by using each
translation equivalent.
2) Calculate the sense-vs.-clue correlation matrix for
the set of senses resulting from step 1).
3) Calculate similarities between senses on the basis
of distribution patterns shown by the sense-vs.-clue
correlation matrix.
4) Cluster senses by using a hierarchical agglomera-
tive clustering method, e.g., the group-average
method.
</listItem>
<bodyText confidence="0.995431176470589">
However, this naive method is not effective be-
cause some senses usually have duplicated definitions
in step 1) despite the fact that the sense-vs.-clue corre-
lation matrix calculation algorithm presupposes a set
of senses without duplicated definitions. The algo-
rithm is based on the &amp;quot;one sense per collocation&amp;quot; hy-
pothesis, and it results in each clue having a high cor-
relation with one and only one sense. A clue can never
have high correlations with two or more senses, even
when they are actually the same sense. Consequently,
synonymous translation equivalents do not necessarily
have high similarity.
Figure 2(a) shows parts of distribution patterns for
Correlation
{promotion, 宣伝&lt;SENDEN&gt;}, {promotion, プロモー
ション&lt;PUROMOUSHON&gt;}, and {promotion, 売り込み
&lt;URIKOMI&gt;} all of which define the &amp;quot;sales activity&amp;quot;
sense of &amp;quot;promotion&amp;quot;. We see that most clues for se-
lecting that sense have higher correlation with {pro-
motion, 宣伝&lt;SENDEN&gt;} than with {promotion, プロ
モーション&lt;PUROMOUSHON&gt;} and {promotion, 売り
込み&lt;URIKOMI&gt;}. This is because &amp;quot;宣伝&lt;SENDEN&gt;&amp;quot;
is the most dominant translation equivalent of &amp;quot;promo-
tion&amp;quot; in the corpus.
To resolve the above problem, we calculated the
sense-vs.-clue correlation matrix not only for the full
set of senses but also for the set of senses excluding
one of these senses. Excluding a definition of the sense,
which includes the most dominant translation equiva-
lent, allows most clues for selecting the sense to have
the highest correlations with another definition of the
same sense, which includes the second most dominant
translation equivalent. Figure 2(b) shows parts of dis-
tribution patterns for {promotion, プ ロモ ー シ ョ ン
&lt;PUROMOUSHON&gt;} and {promotion, 売 り 込 み
&lt;URIKOMI&gt;} shown by the sense-vs.-clue correlation
matrix for the set of senses excluding {promotion, 宣
伝&lt;SENDEN&gt;}. We see that most clues for selecting
the &amp;quot;sales activity&amp;quot; sense have higher correlations with
{promotion, プロモーション&lt;PUROMOUSHON&gt;} than
with {promotion, 売り込み&lt;URIKOMI&gt;}. This is be-
cause &amp;quot;プロモーション&lt;PUROMOUSHON&gt;&amp;quot; is the sec-
ond most dominant translation equivalent in the corpus.
We also see that the distribution pattern for {promo-
tion, プロモーション&lt;PUROMOUSHON&gt;} in Fig. 2(b) is
more similar to that for {promotion, 宣伝&lt;SENDEN&gt;}
in Fig. 2(a) than that for {promotion, プロモーション
&lt;PUROMOUSHON&gt;} in Fig. 2(a).
We call the distribution pattern for sense S2, result-
ing from the sense-vs.-clue correlation matrix for the
set of senses excluding sense S1, the distribution pat-
tern for S2 subordinate to S1, while we call the distri-
bution pattern for sense S2, resulting from the
sense-vs.-clue correlation matrix for the full set of
senses, simply the distribution pattern for S2. We de-
fine the similarity of S2 to S1 as the similarity of the
distribution pattern for S2 subordinate to S1 to the dis-
tribution pattern for S1.
Calculating the sense-vs.-clue correlation matrix
for a set of senses excluding one sense is of course
insufficient since three or more translation equivalents
may represent the same sense of the target word. We
should calculate the sense-vs.-clue correlation matrices
both for the full set of senses and for the set of senses
excluding one of these senses again, after merging
similar senses into one. Repeating these procedures
enables corpus-relevant but less dominant translation
equivalents to be drawn up, while corpus-irrelevant
</bodyText>
<figure confidence="0.989909666666667">
7
6
5
4
3
2
1
0
cc
A
Acclaim
Clue
{promotion, 宣伝&lt;SENDEN&gt;}
{promotion, プロモーション&lt;PUROMOUSHON&gt;}
{promotion, 売り込み&lt;URIKOMI&gt;}
(a) Distribution patterns
7
6
5
4
3
2
1
0
Clue
(b) Distribution patterns subordinate to
{promotion, 宣伝&lt;SENDEN&gt;}
</figure>
<figureCaption confidence="0.9992085">
Figure 2. Distribution Patterns for Some Senses
of &amp;quot;promotion&amp;quot;.
</figureCaption>
<bodyText confidence="0.877061333333333">
ones are never drawn up. Thus, a hierarchy of cor-
pus-relevant senses or clusters of corpus-relevant
translation equivalents is produced.
</bodyText>
<sectionHeader confidence="0.992717" genericHeader="method">
3 Proposed Method
</sectionHeader>
<subsectionHeader confidence="0.650975">
3.1 Outline
</subsectionHeader>
<bodyText confidence="0.8533515">
As shown in Fig. 3, our method repeats the following
three steps:
</bodyText>
<listItem confidence="0.96586525">
1) Calculate sense-vs.-clue correlation matrices both
for the full set of senses and for a set of senses ex-
cluding each of these senses.
2) Calculate similarities between senses on the basis
of distribution patterns and subordinate distribution
patterns.
3) Merge each pair of senses with high similarity
into one.
</listItem>
<bodyText confidence="0.999821333333333">
The initial set of senses is given as Σ(x)={{x, y1}, {x,
y2}, ..., {x, yN}} where x is a target word in the first
language, and y1, y2, ..., and yN are translation equiva-
lents of x in the second-language. Translation
equivalents that occur less frequently in the sec-
ond-language corpus can be excluded from the initial
</bodyText>
<figure confidence="0.99751448">
Correlation
ad campaign
affirmative
analyst say
Audi
Batman
brand
Burger King
career
cereal
Coca-Cola
Conrail
Coors Light
Cyrk
discrimination
employee
film
General
Hispanic
industry
job
label
last year
management
Correlation
</figure>
<bodyText confidence="0.984445">
set to shorten the processing time. The details of the
steps are described in the following sections.
</bodyText>
<subsectionHeader confidence="0.987953">
3.2 Calculation of sense-vs.-clue correlation
matrices
</subsectionHeader>
<bodyText confidence="0.9999145">
First, a sense-vs.-clue correlation matrix is calculated
for the full set of senses. The resulting correlation ma-
trix is denoted as C. That is, C(i, j) is the correlation
between the i-th sense S(x,i) of a target word x and its
j-th clue x(j).
Then a set of active senses, ΣA(x), is determined. A
sense is regarded active if and only if the ratio of clues
with which it has the highest correlation exceeds a
predetermined threshold 0 (In the experiment in Sec-
tion 4, 0 was set to 0.05). That is,
</bodyText>
<equation confidence="0.991121">
ΣA(x) ={S(x, i) |R(S(x,i)) &gt;0 } ,
</equation>
<bodyText confidence="0.999951769230769">
where R(S(x, i)) denotes the ratio of clues having the
highest correlation with S(x, i), i.e.,
Thus ΣA(x) consists of senses of the target word x that
are relevant to the corpus.
Finally, a sense-vs.-clue correlation matrix is cal-
culated for the set of senses excluding each of the ac-
tive senses. The correlation matrix calculated for the
set of senses excluding the k-th sense is denoted as C-k.
That is, C-k(i, j) (i≠k) is the correlation between the
i-th sense and the j-th clue that is calculated excluding
the k-th sense. C-k(k, j) (j=1, 2, ...) are set to zero. This
redundant k-th row is included to maintain the same
correspondence between rows and senses as in C.
</bodyText>
<subsectionHeader confidence="0.999924">
3.3 Calculation of sense similarity matrix
</subsectionHeader>
<bodyText confidence="0.99995075">
Similarity of the i-th sense S(x, i) to the j-th sense S(x,
j), Sim(S(x, i), S(x, j)), is defined as the similarity of
the distribution pattern for S(x, i) subordinate to S(x, j)
to the distribution pattern of S(x, j). Note that this
similarity is asymmetric and reflects which sense is
more dominant in the corpus. It is probable that
Sim(S(x, i), S(x, j)) is large but Sim(S(x, j), S(x, i)) is
not when S(x, j) is more dominant than S(x, i).
According to the sense-vs.-clue correlation matrix,
each sense is characterized by a weighted set of clues.
Therefore, we used the weighted Jaccard coefficient as
the similarity measure. That is,
</bodyText>
<equation confidence="0.935562333333333">
{ C i, k C j, k
( ), ( ) }
min k
</equation>
<page confidence="0.3082">
-j
</page>
<figure confidence="0.564784">
Initial set of senses
</figure>
<figureCaption confidence="0.981962">
Figure 3. Flow Diagram of Proposed Method.
</figureCaption>
<bodyText confidence="0.9109346">
Sim(S(x,i),S(x, j)) =0 otherwise.
It should be noted that a sense is characterized by dif-
ferent weighted sets of clues depending on which
sense the similarity is calculated. Note also that inac-
tive senses are neglected because they are not reliable.
</bodyText>
<subsectionHeader confidence="0.997571">
3.4 Merging similar senses
</subsectionHeader>
<bodyText confidence="0.9999285">
The set of senses is updated by merging every pair of
mutually most-similar senses into one. That is,
</bodyText>
<equation confidence="0.979843181818182">
Σ(x) — Σ(x) – {S(x, i), S(x, j)} + {S(x, i)∪S(x, j)}
if Sim S x i S x j =
( ( , ), ( , )) max {max
j&apos;
{ ( ( , ), ( , )), ( ( , &apos; ), ( , ))}}
Sim S x i S x j &apos; Sim S x j S x i ,
Sim S x i S x j =
( ( , ), ( , ))
{ ( ( , ), ( , )), ( ( , ), ( , ))}}
Sim S x i &apos; S x j Sim S x j S x i &apos;
and Sim(S(x,i),S(x, j)) &gt; σ.
</equation>
<bodyText confidence="0.998615461538462">
The a is a predetermined threshold for similarity,
which is introduced to avoid noisy pairs of senses be-
ing merged. In the experiment in Section 4, a was set
to 0.25.
If at least one pair of senses are merged, the whole
procedure, i.e., the calculation of sense-vs.-clue ma-
trices through the merger of similar senses, is repeated
for the updated set of senses. Otherwise, the clustering
procedure terminates.
Agglomerative clustering methods usually suffer
from the problem of when to terminate merging. In our
method described above, the similarity of senses that
are merged into one does not necessarily decrease
</bodyText>
<equation confidence="0.973842125">
k
Sim
=
∑
(S(x,i),S(x,j))
k max
{ C i, k C j, k
( ), ( ) }
</equation>
<page confidence="0.296183">
-j
</page>
<figure confidence="0.917592928571428">
k
when S(x, j)∈ΣA(x).
Sense-vs.-clue correlation matrices
Calculate correlations
between senses and clues
Comparable
corpus
Bilingual
dictionary
Calculate similarities
between distribution patterns
Sense similarity matrix
Merge similar senses
Updated set of senses
</figure>
<equation confidence="0.858156375">
R(S(x,i)) { ( ) ( , ) max ( , ) } { ( )}
= x &apos; j  |C ij = C kj x &apos; j
k
.
∑
max{max
i&apos;
,
</equation>
<bodyText confidence="0.999984857142857">
monotonically, which makes the problem more diffi-
cult. At present, we are forced to output a dendrogram
that represents the history of mergers and leave the
final decision to humans. The dendrogram consists of
translation equivalents that are included in active
senses in the final cycle. Other translation equivalents
are rejected as they are irrelevant to the corpus.
</bodyText>
<sectionHeader confidence="0.985688" genericHeader="method">
4 Experimental Evaluation
</sectionHeader>
<subsectionHeader confidence="0.993497">
4.1 Experimental settings
</subsectionHeader>
<bodyText confidence="0.998415105263158">
Our method was evaluated through an experiment us-
ing a Wall Street Journal corpus (189 Mbytes) and a
Nihon Keizai Shimbun corpus (275 Mbytes).
First, collected pairs of related words, which we
restricted to nouns and unknown words, were obtained
from each corpus by extracting pairs of words
co-occurring in a window, calculating mutual informa-
tion of each pair of words, and selecting pairs with
mutual information larger than the threshold. The size
of the window was 25 words excluding function words,
and the threshold for mutual information was set to
zero. Second, a bilingual dictionary was prepared by
collecting pairs of nouns that were translations of one
another from the Japan Electronic Dictionary Research
Institute (EDR) English-to-Japanese and Japa-
nese-to-English dictionaries. The resulting dictionary
includes 633,000 pairs of 269,000 English nouns and
276,000 Japanese nouns.
Evaluating the performance of word sense acquisi-
tion methods is not a trivial task. First, we do not have
a gold-standard sense inventory. Even if we have one,
we have difficulty mapping acquired senses onto those
in it. Second, there is no way to establish the complete
set of senses appearing in a large corpus. Therefore,
we evaluated our method on a limited number of target
words as follows.
We prepared a standard sense inventory by select-
ing 60 English target words and defining an average of
3.4 senses per target word manually. The senses were
rather coarse-grained; i.e., they nearly corresponded to
groups of translation equivalents within the entries of
everyday English-Japanese dictionaries. We then sam-
pled 100 instances per target word from the Wall
Street Journal corpus, and we sense-tagged them
manually. Thus, we estimated the ratios of the senses
in the training corpus for each target word.
We defined two evaluative measures, recall of
senses and accuracy of sense definitions. The recall of
senses is the proportion of senses with ratios not less
than a threshold that are successfully extracted, and it
varies with change of the threshold. We judged that a
sense was extracted, when it shared at least one trans-
lation equivalent with some active sense in the final
cycle.
To evaluate the accuracy of sense definitions while
avoiding mapping acquired senses onto those in the
standard sense inventory, we regard a set of senses as a
set of pairs of synonymous translation equivalents. Let
TS be a set consisting of pairs of translation equiva-
lents belonging to the same sense in the standard sense
inventory. Likewise, let T(k) be a set consisting of
pairs of translation equivalents belonging to the same
active sense in the k-th cycle. Further, let U be a set of
pairs of translation equivalents that are included in
active senses in the final cycle. Recall and precision of
pairs of synonymous translation equivalents in the k-th
cycle are defined as:
</bodyText>
<equation confidence="0.9982362">
R(k) ❑ TS ❑ T(k)
.
TS ❑ U
P(k) ❑ TS ❑ T(k) .
T(k)
</equation>
<bodyText confidence="0.99918">
Further, F-measure of pairs of synonymous translation
equivalents in the k-th cycle is defined as:
</bodyText>
<equation confidence="0.998674">
F(k) ❑ 2 FR(k) FP(k
R(k) ❑P(k)
</equation>
<bodyText confidence="0.999966">
The F-measure indicates how well the set of active
senses coincides with the set of sense definitions in the
standard senses inventory. Although the current
method cannot determine the optimum cycle, humans
can identify the set of appropriate senses from a hier-
archy of senses at a glance. Therefore, we define the
accuracy of sense definitions as the maximum
F-measure in all cycles.
</bodyText>
<subsectionHeader confidence="0.994941">
4.2 Experimental results
</subsectionHeader>
<bodyText confidence="0.980410425531915">
To simplify the evaluation procedure, we clustered
translation equivalents that were used to define the
senses of each target word in the standard sense in-
ventory, rather than clustering translation equivalents
rendered by the EDR bilingual dictionary. The recall
of senses for totally 201 senses of the 60 target words
was:
96% for senses with ratios not less than 25%,
87% for senses with ratios not less than 5%, and
78% for senses with ratios not less than 1%.
The accuracy of sense definitions, averaged over the
60 target words, was 77%.
The computational efficiency of our method
proved to be acceptable. It took 13 minutes per target
word on a HP9000 C200 workstation (CPU clock: 200
MHz, memory: 32 MB) to produce a hierarchy of
clusters of translation equivalents.
)
.
Some clustering results are shown in Fig. 4. These
demonstrate that our proposed method shows a great
deal of promise. At the same time, evaluating the re-
sults revealed its deficiencies. The first of these lies in
the crucial role of the bilingual dictionary. It is obvious
that a sense is never extracted if the translation equiva-
lents representing it are not included in it. An
exhaustive bilingual dictionary is therefore required.
From this point of view, the EDR bilingual dictionary
is fairly good. The second deficiency lies in the fact
that it performs badly for low-frequency or non-topical
senses. For example, the sense of &amp;quot;bar&amp;quot; as the &amp;quot;legal
profession&amp;quot; was clearly extracted, but its sense as a
&amp;quot;piece of solid material&amp;quot; was not extracted.
We also compared our method with two alterna-
tives: monolingual distributional clustering mentioned
in Section 2.2 and naive translingual clustering men-
tioned in Section 2.3. Figures 5(a), (b), and (c) show
respective examples of clustering obtained by our
method, the monolingual method, and the naive
translingual method. Comparing (a) with (b) reveals
the superiority of the translingual approach to the
monolingual approach, and comparing (a) with (c)
reveals the effectiveness of the subordinate distribu-
tion pattern introduced in Section 2.3. Note that delet-
ing the corpus-irrelevant translation equivalents from
the dendrograms in both (b) and (c) would not result in
appropriate ones.
</bodyText>
<sectionHeader confidence="0.999658" genericHeader="evaluation">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999718409090909">
Our method has several practical advantages. One of
these is that it produces a corpus-dependent inventory
of word senses. That is, the resulting inventory covers
most senses relevant to a domain, while it excludes
senses irrelevant to the domain.
Second, our method unifies word sense acquisition
with word sense disambiguation. The sense-vs.-clue
correlation matrix is originally used for word sense
disambiguation. Therefore, our method guarantees that
acquired senses can be distinguished by machines, and
further it demonstrates the possibility of automatically
optimizing the granularity of word senses.
Some limitations of the present methods are dis-
cussed in the following with possible future extensions.
First, our method produces a hierarchy of clusters but
cannot produce a set of disjoint clusters. It is very im-
portant to terminate merging senses autonomously
during an appropriate cycle. Comparing distribution
patterns (not subordinate ones) may be useful to ter-
minate merging; senses characterized by complemen-
tary distribution patterns should not be merged.
Second, the present method assumes that each
</bodyText>
<table confidence="0.811674666666667">
[Target word] (English equivalent
Resulting dendrogram other than target
word)
[association] (relation)
┌────rA*&lt;KANKEI&gt; (friendship)
┌──┴────�P&apos;&lt;KOUSAI&gt; (cooperation)
┤ ┌─A ,,&lt;TEIKEI&gt; (relation)
│┌────┴─rAz &lt;KANREN&gt; (cooperation)
└┤┌─────#,n&lt;KYOUDOU&gt; (federation)
└┤┌────z q&lt;RENGOU&gt; (society)
└┤┌───yftq&lt;KUMIAI&gt; (society)
└┤┌──S=&lt;KYOUKAI&gt; (society)
└┤┌──-&lt;KAI&gt; (organization)
└┴─QW&lt;DANTAI&gt;
[bar] (shop)
┌────���&lt;URIBA&gt; (counter)
┌─┤┌─,JT 7/13r--&lt;KAUNTAA&gt; (saloon)
│ └┴────,`�--&lt;BAA&gt; (obstacle)
┤ ┌────rA&lt;SHOUHEKI&gt; (lattice)
│┌─┴────��&lt;KOUSHI&gt; (legal profession)
└┤┌─────&amp;1&amp;&lt;HOUSOU&gt; (lawyer)
└┤┌───# f&lt;BENGOSHI&gt; (law court)
└┴────&amp;9&lt;HOUTEI&gt;
[discipline] (training)
┌─M11 &lt;KUNREN&gt; (subject of study)
┌─┴─*f4[&lt;GAKKA&gt; (learning)
┌┤ ┌─*PHI&lt;GAKUMON&gt; (subject of study)
┤└─┴─(f4[&lt;KYOUKA&gt; (order)
│┌─── &apos; 4:&lt;CHITSUJO&gt; (regulation)
└┤ ┌─M $1J&lt;KISEI&gt; (punishment)
│┌┴─�n&lt;CHOUBATSU&gt; (control)
└┤┌─ $1J&lt;TOUSEI&gt; (order)
└┴─M *&lt;KIRITSU&gt;
[measure] (gauge)
┌──X1&lt;SHAKUDO&gt; (quantity)
┌───┤┌──A&lt;RYOU&gt; (index)
│ └┴─M(&lt;SHISUU&gt; (means)
┤ ┌────f.R&lt;SHUDAN&gt; (counter plan)
│┌┴────3�,j&apos; &lt;TAISAKU&gt; (standard)
└┤ ┌───��&lt;KIJUN&gt; (law)
└─┤┌──&amp; r&lt;HOUREI&gt; (bill)
└┤┌─��&lt;GIAN&gt; (bill)
└┴─&amp; &lt;HOUAN&gt;
[promotion] (elevation)
┌─────�M&lt;TOUYOU&gt; (advancement)
┌─┴─────�&amp;&lt;SHOUSHIN&gt; (sale)
┤┌────�_FUM�&lt;URIKOMI&gt; (advertising
└┤┌─���--��� campaign)
││ &lt;PUROMOUSHON&gt; (advertisement)
└┴─────€C&lt;SENDEN&gt;
[traffic] (commerce)
┌──F% &lt;SHOUGYOU&gt; (trade)
┌┤┌─Ta41&lt;TORIHIKI&gt; (bargain)
┤└┴─9W&lt;BAIBAI&gt; (passage)
│┌──3Mn&lt;TSUUKOU&gt; (transport)
└┤┌─�6M&lt;KOUTSUU&gt; (transport)
└┴─&gt;_ J&lt;UNYU&gt;
</table>
<figureCaption confidence="0.997646">
Figure 4. Examples of Clustering.
</figureCaption>
<figure confidence="0.970091419354839">
translation equivalent represents one and only one sense of the target word, but this is not always the case.
[race] (cycle race)
┌───競輪&lt;KEIRIN&gt; (horse race)
┌┤┌──競馬&lt;KEIBA&gt; (competition)
┤└┴─レース&lt;REESU&gt; (nation)
│┌───国民&lt;KOKUMIN&gt; (ethnic)
└┤┌──民族&lt;MINZOKU&gt; (human race)
└┴──人種&lt;JINSHU&gt;
(a) Proposed method
[race]
┌─────────比&lt;HI&gt; (match)
┌────┴────────競争&lt;KYOUSOU&gt; (competition)
┌─┤ ┌─レース&lt;REESU&gt; (competition)
┌─┤ └──────────┴──競馬&lt;KEIBA&gt; (horse race)
│ │ ┌─────────人種&lt;JINSHU&gt; (human race)
│ └─────┤ ┌──────疾走&lt;SHISSOU&gt; (scamper)
│ └──┴────競り合い&lt;SERIAI&gt; (competition)
┤ ┌───────国民&lt;KOKUMIN&gt; (nation)
│ ┌───┴───────品格&lt;HINKAKU&gt; (dignity)
│┌────┴───────────競輪&lt;KEIRIN&gt; (cycle race)
││ ┌───特徴&lt;TOKUCHOU&gt; (feature)
└┤ ┌──────┴───特性&lt;TOKUSEI&gt; (character)
│ ┌─┴──────────品種&lt;HINSHU&gt; (kind)
│ ┌─┤ ┌─────風味&lt;FUUMI&gt; (flavor)
└─┤ └──────┴─────水路&lt;SUIRO&gt; (waterway)
│ ┌────民族&lt;MINZOKU&gt; (ethnic)
└─────────┴────用水&lt;YOUSUI&gt; (water for
irrigation)
(b) Monolingual distributional clustering
[race]
┌──────────────競馬&lt;KEIBA&gt; (horse race)
</figure>
<equation confidence="0.809481705882353">
┌─┴─────────────レース&lt;REESU&gt; (competition)
┌┤ ┌────────────人種&lt;JINSHU&gt; (human race)
│└───┴────────────民族&lt;MINZOKU&gt; (ethnic)
│ ┌─競り合い&lt;SERIAI&gt; (competition)
│ ┌┤┌──疾走&lt;SHISSOU&gt; (scamper)
│ ┌───┤└┴──品格&lt;HINKAKU&gt; (dignity)
│ │ └─────比&lt;HI&gt; (match)
┤ ┌──┤ ┌─────水路&lt;SUIRO&gt; (waterway)
│ │ │┌─┴─────特性&lt;TOKUSEI&gt; (character)
│ ┌─┤ └┤┌──────競争&lt;KYOUSOU&gt; (competition)
│ │ └┴──────特徴&lt;TOKUCHOU&gt; (feature)
│ ┌─┤ └───────────風味&lt;FUUMI&gt; (flavor)
│ │ │ ┌─────────競輪&lt;KEIRIN&gt; (cycle race)
└─┤ │ ┌─┴─────────品種&lt;HINSHU&gt; (kind)
│ └─┴───────────用水&lt;YOUSUI&gt; (water for
│ irrigation)
└───────────────国民&lt;KOKUMIN&gt; (nation)
</equation>
<figureCaption confidence="0.81299">
(c) Naive translingual distributional clustering
Figure 5. Comparison with Alternatives.
</figureCaption>
<bodyText confidence="0.999962666666667">
A Japanese Katakana word resulting from translitera-
tion of an English word sometimes represents multiple
senses of the English word. It is necessary to detect
and split translation equivalents representing more
than one sense of the target word.
Third, not only are acquired senses rather
coarse-grained but also generic senses are difficult to
acquire. One of the reasons for this may be that we
rely on co-occurrence in the window. The fact that
most distributional word clustering methods use syn-
tactic co-occurrence suggests that it is the most effec-
tive tool for extracting pairs of related words.
</bodyText>
<sectionHeader confidence="0.998974" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999414230769231">
We presented a translingual distributional word clus-
tering method enabling word senses, exactly a hierar-
chy of clusters of translation equivalents, to be ac-
quired from a comparable corpus and a bilingual dic-
tionary. Its effectiveness was demonstrated through an
experiment using Wall Street Journal and Nihon Kei-
zai Shimbun corpora and the EDR bilingual dictionary.
The recall of senses was 87% for senses whose ratios
in the corpus were not less than 5%, and the accuracy
of sense definitions was 77%.
Acknowledgments: This research was supported by
the New Energy and Industrial Technology Develop-
ment Organization of Japan.
</bodyText>
<sectionHeader confidence="0.999084" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999872756756757">
Brown, Peter F., Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1991. Word-sense disam-
biguation using statistical methods. In Proceedings of
the 29th Annual Meeting of the Association for Com-
putational Linguistics, pages 264-270.
Dagan, Ido and Alon Itai. 1994. Word sense disambigua-
tion using a second language monolingual corpus.
Computational Linguistics, 20(4): 563-596.
Fukumoto, Fumiyo and Junichi Tsujii. 1994. Automatic
recognition of verbal polysemy. In Proceedings of the
15th International Conference on Computational Lin-
guistics, pages 762-768.
Grefenstette, Gregory. 1994. Explorations in Automatic
Thesaurus Discovery. Kluwer Academic Publishers,
Boston.
Hindle, Donald. 1990. Noun classification from predi-
cate-argument structures. In Proceedings of the 28th
Annual Meeting of the Association for Computational
Linguistics, pages 268-275.
Ide, Nancy and Jean Veronis. 1998. Introduction to the
special issue on word sense disambiguation: The state
of the art. Computational Linguistics, 24(1): 1-40.
Kaji, Hiroyuki and Yasutsugu Morimoto. 2002. Unsuper-
vised word sense disambiguation using bilingual com-
parable corpora. In Proceedings of the 19th Interna-
tional Conference on Computational Linguistics, pages
411-417.
Pantel, Patrick and Dekang Lin. 2002. Discovering word
senses from text. In Proceedings of the 8th ACM
SIGKDD International Conference on Knowledge Dis-
covery and Data Mining, pages 613-619.
Pereira, Fernando, Naftali Tishby, and Lillian Lee. 1993.
Distributional clustering of English words. In Proceed-
ings of the 31st Annual Meeting of the Association for
Computational Linguistics, pages 183-190.
Schuetze, Hinrich. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1): 97-124.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.738146">
<note confidence="0.933048">Proceedings of HLT-NAACL 2003 Main Papers , pp. 32-39 Edmonton, May-June 2003</note>
<title confidence="0.973462">Word Sense Acquisition from Bilingual Comparable Corpora</title>
<author confidence="0.993304">Hiroyuki Kaji</author>
<affiliation confidence="0.998638">Central Research Laboratory, Hitachi, Ltd.</affiliation>
<address confidence="0.996242">1-280 Higashi-Koigakubo, Kokubunji-shi, Tokyo 185-8601, Japan</address>
<email confidence="0.988575">kaji@crl.hitachi.co.jp</email>
<abstract confidence="0.99726745">Manually constructing an inventory of word senses has suffered from problems including high cost, arbitrary assignment of meaning to words, and mismatch to domains. To overcome these problems, we propose a method to assign word meaning from a bilingual comparable corpus and a bilingual dictionary. It clusters second-language translation equivalents of a first-language target word on the basis of their translingually aligned distribution patterns. Thus it produces a hierarchy of corpus-relevant meanings of the target word, each of which is defined with a set of translation equivalents. The effectiveness of the method has been demonstrated through an experiment using a comparable corpus consisting of Wall Street Journal and Nihon Keizai Shimbun corpora together with the EDR bilingual dictionary.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>Word-sense disambiguation using statistical methods.</title>
<date>1991</date>
<booktitle>In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>264--270</pages>
<contexts>
<context position="1963" citStr="Brown, et al., 1991" startWordPosition="288" endWordPosition="291"> the past decade (Ide and Veronis, 1998). In contrast, word sense acquisition has been a human activity; inventories of word senses have been constructed by lexicographers based on their intuition. Manually constructing an inventory of word senses has suffered from problems such as high cost, arbitrary division of word senses, and mismatch to application domains. We address the problem of word sense acquisition along the lines of the WSD where word senses are defined with sets of translation equivalents in another language. Bilingual corpora or second-language corpora enable unsupervised WSD (Brown, et al., 1991; Dagan and Itai, 1994). However, the correspondence between senses of a word and its translations is not one-to-one, and therefore we need to prepare an inventory of word senses, each of which is defined with a set of synonymous translation equivalents. Although conventional bilingual dictionaries usually group translations according to their senses, the grouping differs by dictionary. In addition, senses specific to a domain are often missing while many senses irrelevant to the domain or rare senses are included. To overcome these problems, we propose a method for producing a hierarchy of cl</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1991</marker>
<rawString>Brown, Peter F., Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1991. Word-sense disambiguation using statistical methods. In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics, pages 264-270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Alon Itai</author>
</authors>
<title>Word sense disambiguation using a second language monolingual corpus.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>4</issue>
<pages>563--596</pages>
<contexts>
<context position="1986" citStr="Dagan and Itai, 1994" startWordPosition="292" endWordPosition="295"> and Veronis, 1998). In contrast, word sense acquisition has been a human activity; inventories of word senses have been constructed by lexicographers based on their intuition. Manually constructing an inventory of word senses has suffered from problems such as high cost, arbitrary division of word senses, and mismatch to application domains. We address the problem of word sense acquisition along the lines of the WSD where word senses are defined with sets of translation equivalents in another language. Bilingual corpora or second-language corpora enable unsupervised WSD (Brown, et al., 1991; Dagan and Itai, 1994). However, the correspondence between senses of a word and its translations is not one-to-one, and therefore we need to prepare an inventory of word senses, each of which is defined with a set of synonymous translation equivalents. Although conventional bilingual dictionaries usually group translations according to their senses, the grouping differs by dictionary. In addition, senses specific to a domain are often missing while many senses irrelevant to the domain or rare senses are included. To overcome these problems, we propose a method for producing a hierarchy of clusters of translation e</context>
</contexts>
<marker>Dagan, Itai, 1994</marker>
<rawString>Dagan, Ido and Alon Itai. 1994. Word sense disambiguation using a second language monolingual corpus. Computational Linguistics, 20(4): 563-596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fumiyo Fukumoto</author>
<author>Junichi Tsujii</author>
</authors>
<title>Automatic recognition of verbal polysemy.</title>
<date>1994</date>
<booktitle>In Proceedings of the 15th International Conference on Computational Linguistics,</booktitle>
<pages>762--768</pages>
<contexts>
<context position="2771" citStr="Fukumoto and Tsujii, 1994" startWordPosition="418" endWordPosition="422">h of which is defined with a set of synonymous translation equivalents. Although conventional bilingual dictionaries usually group translations according to their senses, the grouping differs by dictionary. In addition, senses specific to a domain are often missing while many senses irrelevant to the domain or rare senses are included. To overcome these problems, we propose a method for producing a hierarchy of clusters of translation equivalents from a bilingual corpus and a bilingual dictionary. To the best of our knowledge, there are two preceding research papers on word sense acquisition (Fukumoto and Tsujii, 1994; Pantel and Lin, 2002). Both proposed distributional word clustering algorithms that are characterized by their capabilities to produce overlapping clusters. According to their algorithms, a polysemous word is assigned to multiple clusters, each of which represents one of its senses. These and our approach differ in how to define the word sense, i.e., a set of synonyms in the same language versus a set of translation equivalents in another language. Schuetze (1998) proposed a method for dividing occurrences of a word into classes, each of which consists of contextually similar occurrences. Ho</context>
</contexts>
<marker>Fukumoto, Tsujii, 1994</marker>
<rawString>Fukumoto, Fumiyo and Junichi Tsujii. 1994. Automatic recognition of verbal polysemy. In Proceedings of the 15th International Conference on Computational Linguistics, pages 762-768.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>Explorations in Automatic Thesaurus Discovery.</title>
<date>1994</date>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Boston.</location>
<contexts>
<context position="3725" citStr="Grefenstette, 1994" startWordPosition="568" endWordPosition="569">e word sense, i.e., a set of synonyms in the same language versus a set of translation equivalents in another language. Schuetze (1998) proposed a method for dividing occurrences of a word into classes, each of which consists of contextually similar occurrences. However, it does not produce definitions of senses such as sets of synonyms and sets of translation equivalents. 2 Basic Idea 2.1 Clustering of translation equivalents Most work on automatic extraction of synonyms from text corpora rests on the idea that synonyms have similar distribution patterns (Hindle, 1990; Peraira, et al., 1993; Grefenstette, 1994). This idea is also useful for our task, i.e., extracting sets of synonymous translation equivalents, and we adopt the approach to distributional word clustering. We need to mention that the singularity of our task makes the problem easier. First, we do not have to cluster all words of a language, but we only have to cluster a small number of translation equivalents for each target word, whose senses are to be extracted, separately. As a result, the problem of computational efficiency becomes less serious. Second, even if a translation equivalent itself is polysemous, it is not necessary to co</context>
</contexts>
<marker>Grefenstette, 1994</marker>
<rawString>Grefenstette, Gregory. 1994. Explorations in Automatic Thesaurus Discovery. Kluwer Academic Publishers, Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Hindle</author>
</authors>
<title>Noun classification from predicate-argument structures.</title>
<date>1990</date>
<booktitle>In Proceedings of the 28th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>268--275</pages>
<contexts>
<context position="3681" citStr="Hindle, 1990" startWordPosition="562" endWordPosition="563">r approach differ in how to define the word sense, i.e., a set of synonyms in the same language versus a set of translation equivalents in another language. Schuetze (1998) proposed a method for dividing occurrences of a word into classes, each of which consists of contextually similar occurrences. However, it does not produce definitions of senses such as sets of synonyms and sets of translation equivalents. 2 Basic Idea 2.1 Clustering of translation equivalents Most work on automatic extraction of synonyms from text corpora rests on the idea that synonyms have similar distribution patterns (Hindle, 1990; Peraira, et al., 1993; Grefenstette, 1994). This idea is also useful for our task, i.e., extracting sets of synonymous translation equivalents, and we adopt the approach to distributional word clustering. We need to mention that the singularity of our task makes the problem easier. First, we do not have to cluster all words of a language, but we only have to cluster a small number of translation equivalents for each target word, whose senses are to be extracted, separately. As a result, the problem of computational efficiency becomes less serious. Second, even if a translation equivalent its</context>
</contexts>
<marker>Hindle, 1990</marker>
<rawString>Hindle, Donald. 1990. Noun classification from predicate-argument structures. In Proceedings of the 28th Annual Meeting of the Association for Computational Linguistics, pages 268-275.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Ide</author>
<author>Jean Veronis</author>
</authors>
<title>Introduction to the special issue on word sense disambiguation: The state of the art.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<pages>1--40</pages>
<contexts>
<context position="1384" citStr="Ide and Veronis, 1998" startWordPosition="198" endWordPosition="201">chy of corpus-relevant meanings of the target word, each of which is defined with a set of translation equivalents. The effectiveness of the method has been demonstrated through an experiment using a comparable corpus consisting of Wall Street Journal and Nihon Keizai Shimbun corpora together with the EDR bilingual dictionary. 1 Introduction Word Sense Disambiguation (WSD) is an important subtask that is necessary for accomplishing most natural language processing tasks including machine translation and information retrieval. A great deal of research on WSD has been done over the past decade (Ide and Veronis, 1998). In contrast, word sense acquisition has been a human activity; inventories of word senses have been constructed by lexicographers based on their intuition. Manually constructing an inventory of word senses has suffered from problems such as high cost, arbitrary division of word senses, and mismatch to application domains. We address the problem of word sense acquisition along the lines of the WSD where word senses are defined with sets of translation equivalents in another language. Bilingual corpora or second-language corpora enable unsupervised WSD (Brown, et al., 1991; Dagan and Itai, 199</context>
</contexts>
<marker>Ide, Veronis, 1998</marker>
<rawString>Ide, Nancy and Jean Veronis. 1998. Introduction to the special issue on word sense disambiguation: The state of the art. Computational Linguistics, 24(1): 1-40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyuki Kaji</author>
<author>Yasutsugu Morimoto</author>
</authors>
<title>Unsupervised word sense disambiguation using bilingual comparable corpora.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Computational Linguistics,</booktitle>
<pages>411--417</pages>
<contexts>
<context position="5225" citStr="Kaji and Morimoto, 2002" startWordPosition="802" endWordPosition="805">lgorithm, which is far simpler than overlapping clustering algorithms, is sufficient. 2.2 Translingual distributional word clustering In conventional distributional word clustering, a word is characterized by a vector or weighted set consisting of words in the same language as that of the word itself. In contrast, we propose a translingual distributional word clustering method, whereby a word is characterized by a vector or weighted set consisting of words in another language. It is based on the sense-vs.-clue correlation matrix calculation method we originally developed for unsupervised WSD (Kaji and Morimoto, 2002). That method presupposes that each sense of a target word x is defined with a synonym set consisting of the target word itself and one or more translation equivalents which represent the sense. It calculates correlations between the senses of x and the words statistically related to x, which act as clues for determining the sense of x, on the basis of translingual alignment of pairs of related words. Rows of the resultant correlation matrix are regarded as translingual distribution patterns characterizing translation equivalents. Sense-vs.-clue correlation matrix calculation method *) 1) Alig</context>
</contexts>
<marker>Kaji, Morimoto, 2002</marker>
<rawString>Kaji, Hiroyuki and Yasutsugu Morimoto. 2002. Unsupervised word sense disambiguation using bilingual comparable corpora. In Proceedings of the 19th International Conference on Computational Linguistics, pages 411-417.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Dekang Lin</author>
</authors>
<title>Discovering word senses from text.</title>
<date>2002</date>
<booktitle>In Proceedings of the 8th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>613--619</pages>
<contexts>
<context position="2794" citStr="Pantel and Lin, 2002" startWordPosition="423" endWordPosition="426">a set of synonymous translation equivalents. Although conventional bilingual dictionaries usually group translations according to their senses, the grouping differs by dictionary. In addition, senses specific to a domain are often missing while many senses irrelevant to the domain or rare senses are included. To overcome these problems, we propose a method for producing a hierarchy of clusters of translation equivalents from a bilingual corpus and a bilingual dictionary. To the best of our knowledge, there are two preceding research papers on word sense acquisition (Fukumoto and Tsujii, 1994; Pantel and Lin, 2002). Both proposed distributional word clustering algorithms that are characterized by their capabilities to produce overlapping clusters. According to their algorithms, a polysemous word is assigned to multiple clusters, each of which represents one of its senses. These and our approach differ in how to define the word sense, i.e., a set of synonyms in the same language versus a set of translation equivalents in another language. Schuetze (1998) proposed a method for dividing occurrences of a word into classes, each of which consists of contextually similar occurrences. However, it does not prod</context>
</contexts>
<marker>Pantel, Lin, 2002</marker>
<rawString>Pantel, Patrick and Dekang Lin. 2002. Discovering word senses from text. In Proceedings of the 8th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 613-619.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>Naftali Tishby</author>
<author>Lillian Lee</author>
</authors>
<title>Distributional clustering of English words.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>183--190</pages>
<marker>Pereira, Tishby, Lee, 1993</marker>
<rawString>Pereira, Fernando, Naftali Tishby, and Lillian Lee. 1993. Distributional clustering of English words. In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, pages 183-190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Schuetze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<pages>97--124</pages>
<contexts>
<context position="3241" citStr="Schuetze (1998)" startWordPosition="493" endWordPosition="494"> bilingual dictionary. To the best of our knowledge, there are two preceding research papers on word sense acquisition (Fukumoto and Tsujii, 1994; Pantel and Lin, 2002). Both proposed distributional word clustering algorithms that are characterized by their capabilities to produce overlapping clusters. According to their algorithms, a polysemous word is assigned to multiple clusters, each of which represents one of its senses. These and our approach differ in how to define the word sense, i.e., a set of synonyms in the same language versus a set of translation equivalents in another language. Schuetze (1998) proposed a method for dividing occurrences of a word into classes, each of which consists of contextually similar occurrences. However, it does not produce definitions of senses such as sets of synonyms and sets of translation equivalents. 2 Basic Idea 2.1 Clustering of translation equivalents Most work on automatic extraction of synonyms from text corpora rests on the idea that synonyms have similar distribution patterns (Hindle, 1990; Peraira, et al., 1993; Grefenstette, 1994). This idea is also useful for our task, i.e., extracting sets of synonymous translation equivalents, and we adopt t</context>
</contexts>
<marker>Schuetze, 1998</marker>
<rawString>Schuetze, Hinrich. 1998. Automatic word sense discrimination. Computational Linguistics, 24(1): 97-124.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>