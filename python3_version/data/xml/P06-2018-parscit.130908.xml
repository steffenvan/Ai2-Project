<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000007">
<title confidence="0.997289">
Using Machine-Learning to Assign Function Labels to Parser
Output for Spanish
</title>
<author confidence="0.983512">
Grzegorz Chrupała&apos; and Josef van Genabith&apos;,2
</author>
<affiliation confidence="0.960486">
&apos;National Center for Language Technology
Dublin City University
</affiliation>
<address confidence="0.7862995">
Glasnevin, Dublin 9, Ireland
2IBM Dublin Center for Advanced Studies
</address>
<email confidence="0.9554115">
grzegorz.chrupala@computing.dcu.ie
josef@computing.dcu.ie
</email>
<sectionHeader confidence="0.980078" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999969590909091">
Data-driven grammatical function tag as-
signment has been studied for English us-
ing the Penn-II Treebank data. In this pa-
per we address the question of whether
such methods can be applied success-
fully to other languages and treebank re-
sources. In addition to tag assignment ac-
curacy and f-scores we also present re-
sults of a task-based evaluation. We use
three machine-learning methods to assign
Cast3LB function tags to sentences parsed
with Bikel’s parser trained on the Cast3LB
treebank. The best performing method,
SVM, achieves an f-score of 86.87% on
gold-standard trees and 66.67% on parser
output - a statistically significant improve-
ment of 6.74% over the baseline. In a
task-based evaluation we generate LFG
functional-structures from the function-
tag-enriched trees. On this task we achive
an f-score of 75.67%, a statistically signif-
icant 3.4% improvement over the baseline.
</bodyText>
<sectionHeader confidence="0.995172" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99982825">
The research presented in this paper forms
part of an ongoing effort to develop methods
to induce wide-coverage multilingual Lexical-
Functional Grammar (LFG) (Bresnan, 2001) re-
sources from treebanks by means of automatically
associating LFG f-structure information with con-
stituency trees produced by probabilistic parsers
(Cahill et al., 2004). Inducing deep syntactic anal-
yses from treebank data avoids the cost and time
involved in manually creating wide-coverage re-
sources.
Lexical Functional Grammar f-structures pro-
vide a level of syntactic representation based on
the notion of grammatical functions (e.g. Sub-
ject, Object, Oblique, Adjunct etc.). This level
is more abstract and cross-linguistically more uni-
form than constituency trees. F-structures also in-
clude explicit encodings of phenomena such as
control and raising, pro-drop or long distance de-
pendencies. Those characteristics make this level
a suitable representation for many NLP applica-
tions such as transfer-based Machine Translation
or Question Answering.
The f-structure annotation algorithm used for
inducing LFG resources from the Penn-II treebank
for English (Cahill et al., 2004) uses configura-
tional, categorial, function tag and trace informa-
tion. In contrast to English, in many other lan-
guages configurational information is not a good
predictor for LFG grammatical function assign-
ment. For such languages the function tags in-
cluded in many treebanks are a much more impor-
tant source of information for the LFG annotation
algorithm than Penn-II tags are for English.
Cast3LB (Civit and Marti, 2004), the Spanish
treebank used in the current research, contains
comprehensive grammatical function annotation.
In the present paper we use a machine-learning ap-
proach in order to add Cast3LB function tags to
nodes of basic constituent trees output by a prob-
abilistic parser trained on Cast3LB. To our knowl-
edge, this paper is the first to describe applying
a data-driven approach to function-tag assignment
to a language other than English.
Our method statistically significantly outper-
forms the previously used approach which relied
exclusively on the parser to produce trees with
Cast3LB tags (O’Donovan et al., 2005). Addi-
tionally, we perform a task-driven evaluation of
our Cast3LB tag assignment method by using the
tag-enriched trees as input to the Spanish LFG f-
structure annotation algorithm and evaluating the
quality of the resulting f-structures.
Section 2 describes the Spanish Cast3LB tree-
bank. In Section 3 we describe previous research
in LFG induction for English and Spanish as well
</bodyText>
<page confidence="0.458168">
136
</page>
<note confidence="0.916129">
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 136–143,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999520375">
as research on data-driven function tag assign-
ment to parsed text in English. Section 4 provides
the details of our approach to the Cast3LB func-
tion tag assignment task. In Sections 5 and 6 we
present evaluation results for our method. In Sec-
tion 7 we present the error analysis of the results.
Finally, in Section 8 we conclude and discuss ideas
for further research.
</bodyText>
<figure confidence="0.995425958333333">
Tag
ATR
CAG
CC
CD
CD.Q
CI
CPRED
CPRED.CD
CPRED.SUJ
CREG
ET
Meaning
Attribute of copular verb
Agent of passive verb
Compl. of circumstance
Direct object
Direct object of quantity
Indirect object
Predicative complement
Predicative of Direct Object
Predicative of Subject
Prepositional object
Textual element
</figure>
<sectionHeader confidence="0.891525" genericHeader="introduction">
2 The Spanish Treebank
</sectionHeader>
<bodyText confidence="0.999925833333333">
As input to our LFG annotation algorithm we use
the output of Bikel’s parser (Bikel, 2002) trained
on the Cast3LB treebank (Civit and Mart´ı, 2004).
Cast3LB contains around 3,500 constituency trees
(100,000 words) taken from different genres of
European and Latin American Spanish. The POS
tags used in Cast3LB encode morphological infor-
mation in addition to Part-of-Speech information.
Due to the relatively flexible order of main sen-
tence constituents in Spanish, Cast3LB uses a flat,
multiply-branching structure for the S node. There
is no VP node, but rather all complements and ad-
juncts depending on a verb are sisters to the gv
(Verb Group) node containing this verb. An exam-
ple sentence (with the corresponding f-structure)
is shown in Figure 1.
Tree nodes are additionally labelled with gram-
matical function tags. Table 1 provides a list of
function tags with short explanations. Civit (2004)
provides Cast3LB function tag guidelines.
Functional tags carry some of the information
that would be encoded in terms of tree configura-
tions in languages with stricter constituent order
constraints than Spanish.
</bodyText>
<sectionHeader confidence="0.995873" genericHeader="method">
3 Previous Work
</sectionHeader>
<subsectionHeader confidence="0.995284">
3.1 LFG Annotation
</subsectionHeader>
<bodyText confidence="0.998065">
A methodology for automatically obtaining LFG
f-structures from trees output by probabilistic
parsers trained on the Penn-II treebank has been
described by Cahill et al. (2004). It has been
shown that the methods can be ported to other lan-
guages and treebanks (Burke et al., 2004; Cahill et
al., 2003), including Cast3LB (O’Donovan et al.,
2005).
Some properties of Spanish and the encoding
of syntactic information in the Cast3LB treebank
make it non-trivial to apply the method of auto-
matically mapping c-structures to f-structures used
by Cahill et al. (2004), which assigns grammatical
</bodyText>
<figure confidence="0.852612166666667">
Impersonal marker
Verbal modifier
Negation
Passive marker
Subject
Vocative
</figure>
<tableCaption confidence="0.991943">
Table 1: List of function tags in Cast3LB.
</tableCaption>
<bodyText confidence="0.999868424242424">
functions to tree nodes based on their phrasal cat-
egory, the category of the mother node and their
position relative to the local head.
In Spanish, the order of sentence constituents
is flexible and their position relative to the head
is an imperfect predictor of grammatical function.
Also, much of the information that the Penn-II
Treebank encodes in terms of tree configurations
is encoded in Cast3LB in the form of function
tags. As Cast3LB trees lack a VP node, the con-
figurational information normally used in English
to distinguish Subjects (NP which is left sister to
VP) from Direct Objects (NP which is right sister
to V) is not available in Cast3LB-style trees. This
means that assigning correct LFG functional an-
notations to nodes in Cast3LB trees is rather dif-
ficult without use of Cast3LB function tags, and
those tags are typically absent in output generated
by probabilistic parsers.
In order to solve this difficulty, O’Donovan et
al. (2005) train Bikel’s parser to output complex
category-function labels. A complex label such as
sn-SUJ (an NP node tagged with the Subject gram-
matical function) is treated as an atomic category
in the training data, and is output in the trees pro-
duced by the parser. This baseline process is rep-
resented in Figure 2.
This approach can be problematic for two main
reasons. Firstly, by treating complex labels as
atomic categories the number of unique labels in-
creases and parse quality can deteriorate due to
sparse data problems. Secondly, this approach, by
relying on the parser to assign function tags, offers
</bodyText>
<figure confidence="0.999464941176471">
IMPERS
MOD
NEG
PASS
SUJ
VOC
137
S
1
� PRED ‘esperar(SUBJ,OBJ)’
� NEG +
� �TENSE PRES
�
MOOD SUBJUNCTIVE �
SUBJ
rSPEC [SPEC-FORM EL]
the reader 1
L J
SPEC [SPEC-FORM UNO]
PRED ‘definici´on’
PRED `lector&apos;
OBJ
neg-NEG gv
no espere
not expect
sn-SUJ
sn-CD
el lector
c
´o
una defini
i
n
a definition
</figure>
<figureCaption confidence="0.995812">
Figure 1: On the left flat structure of S. Cast3LB function tags are shown in bold. On the right the
corresponding (simplified) LFG f-structure. Translation: Let the reader not expect a definition.
Figure 2: Processing architecture for the baseline.
</figureCaption>
<bodyText confidence="0.891127">
limited control over, or room for improvement in,
this task.
</bodyText>
<subsectionHeader confidence="0.999905">
3.2 Adding Function Tags to Parser Output
</subsectionHeader>
<bodyText confidence="0.999919541666667">
The solution we adopt instead is to add Cast3LB
functional tags to simple constituent trees output
by the parser, as a postprocessing step. For En-
glish, such approaches have been shown to give
good results for the output of parsers trained on
the Penn-II Treebank.
Blaheta and Charniak (2000) use a probabilis-
tic model with feature dependencies encoded by
means of feature trees to add Penn-II Treebank
function tags to Charniak’s parser output. They re-
port an f-score 88.472% on original treebank trees
and 87.277% on the correctly parsed subset of tree
nodes.
Jijkoun and de Rijke (2004) describe a method
of enriching output of a parser with information
that is included in the original Penn-II trees, such
as function tags, empty nodes and coindexations.
They first transform Penn trees to a dependency
format and then use memory-based learning to
perform various graph transformations. One of the
transformations is node relabelling, which adds
function tags to parser output. They report an f-
score of 88.5% for the task of function tagging on
correctly parsed constituents.
</bodyText>
<sectionHeader confidence="0.903535" genericHeader="method">
4 Assigning Cast3LB Function Tags to
</sectionHeader>
<subsectionHeader confidence="0.780763">
Parsed Spanish Text
</subsectionHeader>
<bodyText confidence="0.999994888888889">
The complete processing architecture of our ap-
proach is depicted in Figure 3. We describe it in
detail in this and the following sections.
We divided the Spanish treebank into a training
set of 80%, a development set of 10%, and a test
set of 10% of all trees. We randomly assigned tree-
bank files to these sets to ensure that different tex-
tual genres are about equally represented among
the training, development and test trees.
</bodyText>
<subsectionHeader confidence="0.997667">
4.1 Constituency Parsing
</subsectionHeader>
<bodyText confidence="0.9998415">
For constituency parsing we use Bikel’s (2002)
parser for which we developed a Spanish language
package adapted to the Cast3LB data. Prior to
parsing, we perform one of the tree transforma-
tions described by Cowan and Collins (2005), i.e.
we add a CP and SBAR nodes to subordinate and
relative clauses. This is undone in parser output.
The category labels in the Spanish treebank are
rather fine grained and often contain redundant in-
formation.1 We preprocess the treebank and re-
</bodyText>
<footnote confidence="0.587018">
1For example there are several labels for Nominal Group,
</footnote>
<page confidence="0.399625">
138
</page>
<figureCaption confidence="0.961553">
Figure 3: Processing architecture for the machine-
learning-based method.
</figureCaption>
<bodyText confidence="0.899326038461538">
duce the number of category labels, only retaining
distinctions that we deem useful for our purposes.2
For constituency parsing we also reduce the
number of POS tags by including only selected
morphological features. Table 2 provides the
list of features included for the different parts of
speech. In our experiments we use gold standard
POS tagged development and test-set sentences as
input rather than tagging text automatically.
The results of the evaluation of parsing perfor-
mance on the test set are shown in Table 3. La-
belled bracketing f-score for all sentences is just
below 84% for all sentences, and 84.58% for sen-
tences of length &lt; 70. In comparison, Cowan
and Collins (2005) report an f-score of 85.1%
(&lt; 70) using a version of Collins’ parser adapted
for Cast3LB, and using reranking to boost perfor-
such as grup.nom.ms (masculine singular), grup.nom.fs (fem-
inine singular), grup.nom.mp (masculine plural) etc. This
number and gender information is already encoded in the
POS tags of nouns heading these constituents.
2The labels we retain are the following: INC, S, S.NF,
S.NF.R, S.NF, S.R, conj.subord, coord, data, espec, gerundi,
grup.nom, gv, infinitiu, interjeccio, morf, neg, numero, prep,
relatiu, s.a, sa, sadv, sn, sp, and versions of those suffixed
with .co to indicate coordination).
</bodyText>
<table confidence="0.995358125">
Part of Speech Features included
type, number
type, number
type, number
type, number, person
type, number, mood
Adverb
Conjunction
</table>
<tableCaption confidence="0.980331">
Table 2: Features included in POS tags. Type
</tableCaption>
<bodyText confidence="0.71645025">
refers to subcategories of parts of speech such as
e.g. common and proper for nouns, or main, aux-
iliary and semiauxiliary for verbs. For details see
(Civit, 2000).
</bodyText>
<table confidence="0.999787666666667">
LB Precision LB Recall F-score
All 84.18 83.74 83.96
&lt; 70 84.82 84.35 84.58
</table>
<tableCaption confidence="0.999843">
Table 3: Parser performance.
</tableCaption>
<bodyText confidence="0.999899">
mance. They use a different, more reduced cat-
egory label set as well as a different training-test
split. Both Cowan and Collins and the present pa-
per report scores which ignore punctuation.
</bodyText>
<subsectionHeader confidence="0.980819">
4.2 Cast3LB Function Tagging
</subsectionHeader>
<bodyText confidence="0.99984185">
For the task of Cast3LB function tag assign-
ment we experimented with three generic machine
learning algorithms: a memory-based learner
(Daelemans and van den Bosch, 2005), a maxi-
mum entropy classifier (Berger et al., 1996) and a
Support Vector Machine classifier (Vapnik, 1998).
For each algorithm we use the same set of features
to represent nodes that are to be assigned one of
the Cast3LB function tags. We use a special null
tag for nodes where no Cast3LB tag is present.
In Cast3LB only nodes in certain contexts are
eligible for function tags. For this reason we only
consider a subset of all nodes as candidates for
function tag assignment, namely those which are
sisters of nodes with the category labels gv (Verb
Group), infinitiu (Infinitive) and gerundi (Gerund).
For these candidates we extract the following three
types of features encoding configurational, mor-
phological and lexical information for the target
node and neighboring context nodes:
</bodyText>
<listItem confidence="0.9190134">
• Node features: position relative to head, head
lemma, alternative head lemma (i.e. the head
of NP in PP), head POS, category, definite-
ness, agreement with head verb, yield, hu-
man/nonhuman
</listItem>
<figure confidence="0.89800625">
Determiner
Noun
Adjective
Pronoun
Verb
type
type
139
</figure>
<listItem confidence="0.9010188">
• Local features: head verb, verb person, verb
number, parent category
• Context features: node features (except posi-
tion) of the two previous and two following
sister nodes (if present).
</listItem>
<bodyText confidence="0.964739705882353">
We used cross-validation for refining the set
of features and for tuning the parameters of the
machine-learning algorithms. We did not use any
additional automated feature-selection procedure.
We made use of the following implementations:
TiMBL (Daelemans et al., 2004) for Memory-
Based Learning, the MaxEnt Toolkit (Le, 2004)
for Maximum Entropy and LIBSVM (Changcuand
Lin, 2001) for Support Vector Machines. For
TiMBL we used k nearest neighbors = 7 and the
gain ratio metric for feature weighting. For Max-
Ent, we used the L-BFGS parameter estimation
and 110 iterations, and we regularize the model
using a Gaussian prior with u2 = 1. For SVM we
used the RBF kernel with -y = 2−7 and the cost
parameter C = 32.
076 00 08 8
</bodyText>
<sectionHeader confidence="0.970521" genericHeader="method">
5 Cast3LB Tag Assignment Evaluation
</sectionHeader>
<bodyText confidence="0.992446115384615">
We present evaluation results on the original gold-
standard trees of the test set as well as on the
test-set sentences parsed by Bikel’s parser. For
the evaluation of Cast3LB function tagging per-
formance on gold trees the most straightforward
metric is the accuracy, or the proportion of all can-
didate nodes that were assigned the correct label.
However we cannot use this metric for evalu-
ating results on the parser output. The trees out-
put by the parser are not identical to gold standard
trees due to parsing errors, and the set of candi-
date nodes extracted from parsed trees will not be
the same as for gold trees. For this reason we use
an alternative metric which is independent of tree
configuration and uses only the Cast3LB function
labels and positional indices of tokens in a sen-
tence. For each function-tagged tree we first re-
move the punctuation tokens. Then we extract a
set of tuples of the form (GF, i, j), where GF is
the Cast3LB function tag and i − j is the range
of tokens spanned by the node annotated with this
function. We use the standard measures of preci-
sion, recall and f-score to evaluate the results.
Results for the three algorithms are shown in
Table 4. MBL and MaxEnt show a very sim-
ilar performance, while SVM outperforms both,
</bodyText>
<figure confidence="0.639012">
s
7.0 7.5 8.0 8.5 9.0 9.5
log(n)
</figure>
<figureCaption confidence="0.991175">
Figure 4: Learning curves for TiMBL (t), MaxEnt
(m) and SVM (s).
</figureCaption>
<table confidence="0.99884375">
Acc. Prec. Recall F-score
MBL 87.55 87.00 82.98 84.94
MaxEnt 88.06 87.66 86.87 85.52
SVM 89.34 88.93 84.90 86.87
</table>
<tableCaption confidence="0.823162">
Table 4: Cast3LB function tagging performance
for gold-standard trees
</tableCaption>
<bodyText confidence="0.999501444444444">
scoring 89.34% on accuracy and 86.87% on f-
score. The learning curves for the three algo-
rithms, shown in Figure 4, are also informative,
with SVM outperforming the other two methods
for all training set sizes. In particular, the last sec-
tion of the plot shows SVM performing almost as
well as MBL with half as much learning material.
Neither of the three curves shows signs of hav-
ing reached a maximum, which indicates that in-
</bodyText>
<table confidence="0.999611333333333">
Precision Recall F-score
all corr. all corr. all corr.
Baseline 59.26 72.63 60.61 75.35 59.93 73.96
MBL 64.74 78.09 64.18 78.75 64.46 78.42
MaxEnt 65.48 78.90 64.55 79.44 65.01 79.17
SVM 66.96 80.58 66.38 81.27 66.67 80.92
</table>
<tableCaption confidence="0.599120333333333">
Table 5: Cast3LB function tagging performance
for parser output, for all constituents, and for cor-
rectly parsed constituents only
</tableCaption>
<equation confidence="0.8718645">
s
m
t
ms
t
s
m
t
s
m
t
m t
</equation>
<page confidence="0.48528">
140
</page>
<table confidence="0.9823912">
Methods p-value
Baseline vs SVM 1.169 x 10−9
Baseline vs MBL 2.117 x 10−6
MBL vs MaxEnt 0.0799
MaxEnt vs SVM 0.0005
</table>
<tableCaption confidence="0.8521445">
Table 6: Statistical significance testing results on
for the Cast3LB tag assignment on parser output.
</tableCaption>
<table confidence="0.999894">
Precision Recall F-score
Baseline 73.95 70.67 72.27
SVM 76.90 74.48 75.67
</table>
<tableCaption confidence="0.9552405">
Table 7: LFG F-structure evaluation results for
parser output
</tableCaption>
<bodyText confidence="0.999145734693877">
creasing the size of the training data should result
in further improvements in performance.
Table 5 shows the performance of the three
methods on parser output. The baseline con-
tains the results achieved by treating compound
category-function labels as atomic during parser
training so that they are included in parser output.
For this task we present two sets of results: (i) for
all constituents, and (ii) for correctly parsed con-
stituents only. Again the best algorithm turns out
to be SVM. It outperforms the baseline by a large
margin (6.74% for all constituents).
The difference in performance for gold stan-
dard trees, and the correctly parsed constituents
in parser output is rather larger than what Blaheta
and Charniak report. Further analysis is needed
to identify the source of this difference but we
suspect that one contributing factor is the use of
greater number of context features combined with
a higher parse error rate in comparison to their ex-
periments on the Penn II Treebank. Since any mis-
analysis of constituency structure in the vicinity of
target node can have negative impact, greater re-
liance on context means greater susceptibility to
parse errors. Another factor to consider is the fact
that we trained and adjusted parameters on gold-
standard trees, and the model learned may rely on
features of those trees that the parser is unable to
reproduce.
For the experiments on parser output (all con-
stituents) we performed a series of sign tests in
order to determine to what extent the differences
in performance between the different methods are
statistically significant. For each pair of methods
we calculate the f-score for each sentence in the
test set. For those sentences on which the scores
differ (i.e. the number of trials) we calculate in
how many cases the second method is better than
the first (i.e. the number of successes). We then
perform the test with the null hypothesis that the
probability of success is chance (= 0.5) and the
alternative hypothesis that the probability of suc-
cess is greater than chance (&gt; 0.5). The results
are summarized in Table 6. Given that we perform
4 pairwise comparisons, we apply the Bonferroni
correction and adjust our target αo = 4 . For the
confidence level 95% (αo = 0.0125) all pairs give
statistically significant results, except for MBL vs
MaxEnt.
</bodyText>
<sectionHeader confidence="0.9679015" genericHeader="method">
6 Task-Based LFG Annotation
Evaluation
</sectionHeader>
<bodyText confidence="0.998936735294118">
Finally, we also evaluated the actual f-structures
obtained by running the LFG-annotation algo-
rithm on trees produced by the parser and enriched
with Cast3LB function tags assigned using SVM.
For this task-based evaluation we produced a gold
standard consisting of f-structures corresponding
to all sentences in the test set. The LFG-annotation
algorithm was run on the test set trees (which con-
tained original Cast3LB treebank function tags),
and the resulting f-structures were manually cor-
rected.
Following Crouch et al. (2002), we convert
the f-structures to triples of the form (GF, Pi, Pj),
where Pi is the value of the PRED attribute of the
f-structure, GF is an LFG grammatical function
attribute, and Pj is the value of the PRED attribute
of the f-structure which is the value of the GF
attribute. This is done recursively for each level
of embedding in the f-structure. Attributes with
atomic values are ignored for the purposes of this
evaluation. The results obtained are shown in Ta-
ble 7. We also performed a statistical significance
test for these results, using the same method as for
the Cast3LB tag assigment task. Thep-value given
by the sign test was 2.118x10−5, comfortably be-
low α = 1%.
The higher scores achieved in the LFG f-
structure evaluation in comparison with the pre-
ceding Cast3LB tag assignment evaluation (Table
5) can be attributed to two main factors. Firstly,
the mapping from Cast3LB tags to LFG grammat-
ical functions is not one-to-one. For example three
Cast3LB tags (CC, MOD and ET) are all mapped
to LFG ADJUNCT. Thus mistagging a MOD as
</bodyText>
<page confidence="0.714166">
141
</page>
<table confidence="0.998772375">
ATR CC CD CI CREG MOD SUJ
ATR 136 2 0 0 0 0 5
CC 6 552 12 4 25 18 6
CD 1 19 418 5 3 0 26
CI 0 6 1 50 1 0 0
CREG 0 6 0 2 43 0 0
MOD 0 0 0 0 0 19 0
SUJ 0 8 24 2 0 0 465
</table>
<tableCaption confidence="0.994506">
Table 8: Simplified confusion matrix for SVM
</tableCaption>
<bodyText confidence="0.993228095238095">
on test-set gold-standard trees. The gold-standard
Cast3LB function tags are shown in the first row,
the predicted tags in the first column. So e.g. SUJ
was mistagged as CD in 26 cases. Low frequency
function tags as well as those rarely mispredicted
have been omitted for clarity.
CC does not affect the f-structure score. On the
other hand the Cast3LB CD tag can be mapped
to OBJ, COMP, or XCOMP, and it can be easily
decided which one is appropriate depending on
the category label of the target node. Addition-
ally many nodes which receive no function tag in
Cast3LB, such as noun modifiers, are straightfor-
wardly mapped to LFG ADJUNCT. Similarly, ob-
jects of prepositions receive the LFG OBJ function.
Secondly, the f-structure evaluation metric is
less sensitive to small constituency misconfigura-
tions: it is not necessary to correctly identify the
token range spanned by a target node as long as the
head (which provides the PRED attribute) is cor-
rect.
</bodyText>
<sectionHeader confidence="0.998708" genericHeader="evaluation">
7 Error Analysis
</sectionHeader>
<bodyText confidence="0.9999765">
In order to understand sources of error and de-
termine how much room for further improvement
there is, we examined the most common cases of
Cast3LB function mistagging. A simplified confu-
sion matrix with the most common Cast3LB tags
is shown in Table 8. The most common mistakes
occur between SUJ and CD, in both directions, and
many also CREGs are erroneously tagged as CC.
</bodyText>
<subsectionHeader confidence="0.993327">
7.1 Subject vs Direct Object
</subsectionHeader>
<bodyText confidence="0.9999405">
We noticed that in over 50% of cases when a
Direct Object (CD) was misidentified as Subject
(SUJ), the target node’s mother was a relative
clause. It turns out that in Spanish relative clauses
genuine syntactic ambiguity is not uncommon.
Consider the following Spanish phrase:
</bodyText>
<listItem confidence="0.411009">
(1) Sistemas que usan el 95% de
</listItem>
<bodyText confidence="0.98071275">
Systems which use DET 95% of
los ordenadores.
DET computers
Its translation into English is either Systems that
use 95% of computers or alternatively Systems that
95% of computers use. In Spanish, unlike in En-
glish, preverbal / postverbal position of a con-
stituent is not a good guide to its grammatical
function in this and similar contexts. Human an-
notators can use their world knowledge to decide
on the correct semantic role of a target constituent
and use it in assigning a correct grammatical func-
tion, but such information is obviously not used
in our machine learning methods. Thus such mis-
takes seem likely to remain unresolvable in our
current approach.
</bodyText>
<subsectionHeader confidence="0.974957">
7.2 Prepositional Object vs Adjunct
</subsectionHeader>
<bodyText confidence="0.999983705882353">
The frequent misidentification of Prepositional
Objects (CREG) as Adjuncts (CC) seen in Table 8
can be accounted for by several factors. Firstly,
Prepositional Objects are strongly dependent on
specific verbs and the comparatively small size of
our training data means that there is limited oppor-
tunity for a machine-learning algorithm to learn
low-frequency lexical dependencies. Here the ob-
vious solution is to use a more adequate amount of
training material when it becomes available.
A further problem with the Prepositional Object
- Adjunct distinction is its inherent fuzziness. Be-
cause of this, treebank designers may fail to pro-
vide easy-to-follow, clearcut guidelines and hu-
man annotators necessarily exercise a certain de-
gree of arbitrariness in assigning one or the other
function.
</bodyText>
<sectionHeader confidence="0.98763" genericHeader="conclusions">
8 Conclusions and Future Research
</sectionHeader>
<bodyText confidence="0.999711833333333">
Our research has shown that machine-learning-
based Cast3LB tag assignment as a post-
processing step to raw tree parser output statisti-
cally significantly outperforms a baseline where
the parser itself is trained to learn category
/ Cast3LB-function pairs. In contrast to the
parser-based method, the machine-learning-based
method avoids some sparse data problems and al-
lows for more control over Cast3LB tag assign-
ment. We have found that the SVM algorithm out-
performs the other two machine learning methods
used.
</bodyText>
<page confidence="0.704357">
142
</page>
<bodyText confidence="0.999967529411765">
In addition, we evaluated Cast3LB tag assign-
ment in a task-based setting in the context of au-
tomatically acquiring LFG resources for Spanish
from Cast3LB. Machine-learning-based Cast3LB
tag assignment yields statistically-significantly
improved LFG f-structures compared to parser-
based assignment.
One limitation of our method is the fact that it
treats the classification task separately for each tar-
get node. It thus fails to observe constraints on the
possible sequences of grammatical function tags
in the same local context. Some functions are
unique, such as the Subject, whereas others (Di-
rect and Indirect Object) can only be realized by a
full NP once, although they can be doubled by a
clitic pronoun. Capturing such global constraints
will need further work.
</bodyText>
<sectionHeader confidence="0.992145" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999475666666667">
We gratefully acknowledge support from Science
Foundation Ireland grant 04/IN/I527 for the re-
search reported in this paper.
</bodyText>
<sectionHeader confidence="0.996373" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999475518072289">
A. L. Berger, V. J. Della Pietra, and S. A. Della Pietra.
1996. A maximum entropy approach to natural
language processing. Computational Linguistics,
22(1):39–71, March.
D. Bikel. 2002. Design of a multi-lingual,
parallel-processing statistical parsing engine. In
Human Language Technology Conference (HLT),
San Diego, CA, USA. Software available
at http://www.cis.upenn.edu/-dbikel/
software.html#stat-parser.
D. Blaheta and E. Charniak. 2000. Assigning function
tags to parsed text. In Proceedings of the 1st Con-
ference of the North American Chapter of the ACL,
pages 234–240, Rochester, NY, USA.
J. Bresnan. 2001. Lexical-Functional Syntax. Black-
well Publishers, Oxford.
M. Burke, O. Lam, A. Cahill, R. Chan, R. O’Donovan,
A. Bodomo, J. van Genabith, and A. Way. 2004.
Treebank-based acquisition of a Chinese Lexical-
Functional Grammar. In Proceedings of the 18th
Pacific Asia Conference on Language, Information
and Computation (PACLIC-18).
A. Cahill, M. Forst, M. McCarthy, R. O’Donovan,
and C. Roher. 2003. Treebank-based multilingual
unification-grammar development. In Proceedings
of the 15th Workshop on Ideas and Strategies for
Multilingual Grammar Development, ESSLLI 15,
Vienna, Austria.
A. Cahill, M. Burke, R. O’Donovan, J. van Genabith,
and A. Way. 2004. Long-distance dependency
resolution in automatically acquired wide-coverage
PCFG-based LFG approximations. In Proceed-
ings of the 42nd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 319–326,
Barcelona, Spain.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: a library for support vector machines. Soft-
wareavailableat http://www.csie.ntu.edu.
tw/-cjlin/libsvm.
M. Civit and M. A. Mart´ı. 2004. Building Cast3LB: A
Spanish treebank. Research on Language and Com-
putation, 2(4):549–574, December.
M. Civit. 2000. Gu´ıa para la anotaci´on mor-
fosint´actica del corpus CLiC-TALP, X-TRACT
Working Paper. Technical report. Avail-
able at http://clic.fil.ub.es/personal/
civit/PUBLICA/guia morfol.ps.
M. Civit. 2004. Gu´ıa para la anotaci´on de las funciones
sint´acticas de Cast3LB. Technical report. Avail-
able at http://clic.fil.ub.es/personal/
civit/PUBLICA/funcions.pdf.
B. Cowan and M. Collins. 2005. Morphology and
reranking for the statistical parsing of Spanish. In
Conference on Empirical Methods in Natural Lan-
guage Processing, Vancouver, B.C., Canada.
R. Crouch, R. M. Kaplan, T. H. King, and S. Riezler.
2002. A comparison of evaluation metrics for a
broad-coverage stochastic parser. In Conference on
Language Resources and Evaluation (LREC 02).
W. Daelemans and A. van den Bosch. 2005. Memory-
Based Language Processing. Cambridge University
Press, September.
W. Daelemans, J. Zavrel, K. van der Sloot, and
A. van den Bosch. 2004. TiMBL: Tilburg Memory
Based Learner, version 5.1, Reference Guide. Tech-
nical report. Available from http://ilk.uvt.
nl/downloads/pub/papers/ilk0402.pdf.
V. Jijkoun and M. de Rijke. 2004. Enriching the output
of a parser using memory-based learning. In Pro-
ceedings of the 42nd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 311–318,
Barcelona, Spain.
Zh. Le, 2004. Maximum Entropy Modeling
Toolkit for Python and C++. Available
at http://homepages.inf.ed.ac.uk/
s0450736/software/maxent/manual.pdf.
R. O’Donovan, A. Cahill, J. van Genabith, and A. Way.
2005. Automatic acquisition of Spanish LFG re-
sources from the CAST3LB treebank. In Proceed-
ings of the Tenth International Conference on LFG,
Bergen, Norway.
V. N. Vapnik. 1998. Statistical Learning Theory.
Wiley-Interscience, September.
</reference>
<page confidence="0.966788">
143
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.486542">
<title confidence="0.996927">Using Machine-Learning to Assign Function Labels to Parser Output for Spanish</title>
<author confidence="0.987983">van</author>
<affiliation confidence="0.9973775">Center for Language Technology Dublin City University</affiliation>
<address confidence="0.812108">Glasnevin, Dublin 9, Ireland Dublin Center for Advanced Studies</address>
<email confidence="0.8366315">grzegorz.chrupala@computing.dcu.iejosef@computing.dcu.ie</email>
<abstract confidence="0.999127956521739">Data-driven grammatical function tag assignment has been studied for English using the Penn-II Treebank data. In this paper we address the question of whether such methods can be applied successfully to other languages and treebank resources. In addition to tag assignment accuracy and f-scores we also present results of a task-based evaluation. We use three machine-learning methods to assign Cast3LB function tags to sentences parsed with Bikel’s parser trained on the Cast3LB treebank. The best performing method, achieves an f-score of trees and parser output a statistically significant improveof the baseline. In a task-based evaluation we generate LFG functional-structures from the functiontag-enriched trees. On this task we achive f-score of a statistically signifover the baseline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A L Berger</author>
<author>V J Della Pietra</author>
<author>S A Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="13120" citStr="Berger et al., 1996" startWordPosition="2077" endWordPosition="2080">in, auxiliary and semiauxiliary for verbs. For details see (Civit, 2000). LB Precision LB Recall F-score All 84.18 83.74 83.96 &lt; 70 84.82 84.35 84.58 Table 3: Parser performance. mance. They use a different, more reduced category label set as well as a different training-test split. Both Cowan and Collins and the present paper report scores which ignore punctuation. 4.2 Cast3LB Function Tagging For the task of Cast3LB function tag assignment we experimented with three generic machine learning algorithms: a memory-based learner (Daelemans and van den Bosch, 2005), a maximum entropy classifier (Berger et al., 1996) and a Support Vector Machine classifier (Vapnik, 1998). For each algorithm we use the same set of features to represent nodes that are to be assigned one of the Cast3LB function tags. We use a special null tag for nodes where no Cast3LB tag is present. In Cast3LB only nodes in certain contexts are eligible for function tags. For this reason we only consider a subset of all nodes as candidates for function tag assignment, namely those which are sisters of nodes with the category labels gv (Verb Group), infinitiu (Infinitive) and gerundi (Gerund). For these candidates we extract the following t</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>A. L. Berger, V. J. Della Pietra, and S. A. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–71, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bikel</author>
</authors>
<title>Design of a multi-lingual, parallel-processing statistical parsing engine.</title>
<date>2002</date>
<booktitle>In Human Language Technology Conference (HLT),</booktitle>
<contexts>
<context position="4742" citStr="Bikel, 2002" startWordPosition="717" endWordPosition="718">tions 5 and 6 we present evaluation results for our method. In Section 7 we present the error analysis of the results. Finally, in Section 8 we conclude and discuss ideas for further research. Tag ATR CAG CC CD CD.Q CI CPRED CPRED.CD CPRED.SUJ CREG ET Meaning Attribute of copular verb Agent of passive verb Compl. of circumstance Direct object Direct object of quantity Indirect object Predicative complement Predicative of Direct Object Predicative of Subject Prepositional object Textual element 2 The Spanish Treebank As input to our LFG annotation algorithm we use the output of Bikel’s parser (Bikel, 2002) trained on the Cast3LB treebank (Civit and Mart´ı, 2004). Cast3LB contains around 3,500 constituency trees (100,000 words) taken from different genres of European and Latin American Spanish. The POS tags used in Cast3LB encode morphological information in addition to Part-of-Speech information. Due to the relatively flexible order of main sentence constituents in Spanish, Cast3LB uses a flat, multiply-branching structure for the S node. There is no VP node, but rather all complements and adjuncts depending on a verb are sisters to the gv (Verb Group) node containing this verb. An example sent</context>
</contexts>
<marker>Bikel, 2002</marker>
<rawString>D. Bikel. 2002. Design of a multi-lingual, parallel-processing statistical parsing engine. In Human Language Technology Conference (HLT),</rawString>
</citation>
<citation valid="false">
<authors>
<author>San Diego</author>
<author>USA CA</author>
</authors>
<note>Software available at http://www.cis.upenn.edu/-dbikel/ software.html#stat-parser.</note>
<marker>Diego, CA, </marker>
<rawString>San Diego, CA, USA. Software available at http://www.cis.upenn.edu/-dbikel/ software.html#stat-parser.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Blaheta</author>
<author>E Charniak</author>
</authors>
<title>Assigning function tags to parsed text.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st Conference of the North American Chapter of the ACL,</booktitle>
<pages>234--240</pages>
<location>Rochester, NY, USA.</location>
<contexts>
<context position="9014" citStr="Blaheta and Charniak (2000)" startWordPosition="1412" endWordPosition="1415">eft flat structure of S. Cast3LB function tags are shown in bold. On the right the corresponding (simplified) LFG f-structure. Translation: Let the reader not expect a definition. Figure 2: Processing architecture for the baseline. limited control over, or room for improvement in, this task. 3.2 Adding Function Tags to Parser Output The solution we adopt instead is to add Cast3LB functional tags to simple constituent trees output by the parser, as a postprocessing step. For English, such approaches have been shown to give good results for the output of parsers trained on the Penn-II Treebank. Blaheta and Charniak (2000) use a probabilistic model with feature dependencies encoded by means of feature trees to add Penn-II Treebank function tags to Charniak’s parser output. They report an f-score 88.472% on original treebank trees and 87.277% on the correctly parsed subset of tree nodes. Jijkoun and de Rijke (2004) describe a method of enriching output of a parser with information that is included in the original Penn-II trees, such as function tags, empty nodes and coindexations. They first transform Penn trees to a dependency format and then use memory-based learning to perform various graph transformations. O</context>
</contexts>
<marker>Blaheta, Charniak, 2000</marker>
<rawString>D. Blaheta and E. Charniak. 2000. Assigning function tags to parsed text. In Proceedings of the 1st Conference of the North American Chapter of the ACL, pages 234–240, Rochester, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bresnan</author>
</authors>
<title>Lexical-Functional Syntax.</title>
<date>2001</date>
<publisher>Blackwell Publishers,</publisher>
<location>Oxford.</location>
<contexts>
<context position="1395" citStr="Bresnan, 2001" startWordPosition="203" endWordPosition="204">rained on the Cast3LB treebank. The best performing method, SVM, achieves an f-score of 86.87% on gold-standard trees and 66.67% on parser output - a statistically significant improvement of 6.74% over the baseline. In a task-based evaluation we generate LFG functional-structures from the functiontag-enriched trees. On this task we achive an f-score of 75.67%, a statistically significant 3.4% improvement over the baseline. 1 Introduction The research presented in this paper forms part of an ongoing effort to develop methods to induce wide-coverage multilingual LexicalFunctional Grammar (LFG) (Bresnan, 2001) resources from treebanks by means of automatically associating LFG f-structure information with constituency trees produced by probabilistic parsers (Cahill et al., 2004). Inducing deep syntactic analyses from treebank data avoids the cost and time involved in manually creating wide-coverage resources. Lexical Functional Grammar f-structures provide a level of syntactic representation based on the notion of grammatical functions (e.g. Subject, Object, Oblique, Adjunct etc.). This level is more abstract and cross-linguistically more uniform than constituency trees. F-structures also include ex</context>
</contexts>
<marker>Bresnan, 2001</marker>
<rawString>J. Bresnan. 2001. Lexical-Functional Syntax. Blackwell Publishers, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Burke</author>
<author>O Lam</author>
<author>A Cahill</author>
<author>R Chan</author>
<author>R O’Donovan</author>
<author>A Bodomo</author>
<author>J van Genabith</author>
<author>A Way</author>
</authors>
<title>Treebank-based acquisition of a Chinese LexicalFunctional Grammar.</title>
<date>2004</date>
<booktitle>In Proceedings of the 18th Pacific Asia Conference on Language, Information and Computation (PACLIC-18).</booktitle>
<marker>Burke, Lam, Cahill, Chan, O’Donovan, Bodomo, van Genabith, Way, 2004</marker>
<rawString>M. Burke, O. Lam, A. Cahill, R. Chan, R. O’Donovan, A. Bodomo, J. van Genabith, and A. Way. 2004. Treebank-based acquisition of a Chinese LexicalFunctional Grammar. In Proceedings of the 18th Pacific Asia Conference on Language, Information and Computation (PACLIC-18).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Cahill</author>
<author>M Forst</author>
<author>M McCarthy</author>
<author>R O’Donovan</author>
<author>C Roher</author>
</authors>
<title>Treebank-based multilingual unification-grammar development.</title>
<date>2003</date>
<booktitle>In Proceedings of the 15th Workshop on Ideas and Strategies for Multilingual Grammar Development, ESSLLI 15,</booktitle>
<location>Vienna, Austria.</location>
<marker>Cahill, Forst, McCarthy, O’Donovan, Roher, 2003</marker>
<rawString>A. Cahill, M. Forst, M. McCarthy, R. O’Donovan, and C. Roher. 2003. Treebank-based multilingual unification-grammar development. In Proceedings of the 15th Workshop on Ideas and Strategies for Multilingual Grammar Development, ESSLLI 15, Vienna, Austria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Cahill</author>
<author>M Burke</author>
<author>R O’Donovan</author>
<author>J van Genabith</author>
<author>A Way</author>
</authors>
<title>Long-distance dependency resolution in automatically acquired wide-coverage PCFG-based LFG approximations.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>319--326</pages>
<location>Barcelona,</location>
<marker>Cahill, Burke, O’Donovan, van Genabith, Way, 2004</marker>
<rawString>A. Cahill, M. Burke, R. O’Donovan, J. van Genabith, and A. Way. 2004. Long-distance dependency resolution in automatically acquired wide-coverage PCFG-based LFG approximations. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, pages 319–326, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: a library for support vector machines. Softwareavailableat</title>
<date>2001</date>
<note>http://www.csie.ntu.edu. tw/-cjlin/libsvm.</note>
<marker>Chang, Lin, 2001</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: a library for support vector machines. Softwareavailableat http://www.csie.ntu.edu. tw/-cjlin/libsvm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Civit</author>
<author>M A Mart´ı</author>
</authors>
<title>Building Cast3LB: A Spanish treebank.</title>
<date>2004</date>
<journal>Research on Language and Computation,</journal>
<volume>2</volume>
<issue>4</issue>
<marker>Civit, Mart´ı, 2004</marker>
<rawString>M. Civit and M. A. Mart´ı. 2004. Building Cast3LB: A Spanish treebank. Research on Language and Computation, 2(4):549–574, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Civit</author>
</authors>
<title>Gu´ıa para la anotaci´on morfosint´actica del corpus CLiC-TALP, X-TRACT Working Paper. Technical report. Available at http://clic.fil.ub.es/personal/ civit/PUBLICA/guia morfol.ps.</title>
<date>2000</date>
<contexts>
<context position="12572" citStr="Civit, 2000" startWordPosition="1990" endWordPosition="1991"> labels we retain are the following: INC, S, S.NF, S.NF.R, S.NF, S.R, conj.subord, coord, data, espec, gerundi, grup.nom, gv, infinitiu, interjeccio, morf, neg, numero, prep, relatiu, s.a, sa, sadv, sn, sp, and versions of those suffixed with .co to indicate coordination). Part of Speech Features included type, number type, number type, number type, number, person type, number, mood Adverb Conjunction Table 2: Features included in POS tags. Type refers to subcategories of parts of speech such as e.g. common and proper for nouns, or main, auxiliary and semiauxiliary for verbs. For details see (Civit, 2000). LB Precision LB Recall F-score All 84.18 83.74 83.96 &lt; 70 84.82 84.35 84.58 Table 3: Parser performance. mance. They use a different, more reduced category label set as well as a different training-test split. Both Cowan and Collins and the present paper report scores which ignore punctuation. 4.2 Cast3LB Function Tagging For the task of Cast3LB function tag assignment we experimented with three generic machine learning algorithms: a memory-based learner (Daelemans and van den Bosch, 2005), a maximum entropy classifier (Berger et al., 1996) and a Support Vector Machine classifier (Vapnik, 19</context>
</contexts>
<marker>Civit, 2000</marker>
<rawString>M. Civit. 2000. Gu´ıa para la anotaci´on morfosint´actica del corpus CLiC-TALP, X-TRACT Working Paper. Technical report. Available at http://clic.fil.ub.es/personal/ civit/PUBLICA/guia morfol.ps.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Civit</author>
</authors>
<title>Gu´ıa para la anotaci´on de las funciones sint´acticas de Cast3LB. Technical report. Available at http://clic.fil.ub.es/personal/ civit/PUBLICA/funcions.pdf.</title>
<date>2004</date>
<contexts>
<context position="5553" citStr="Civit (2004)" startWordPosition="845" endWordPosition="846"> tags used in Cast3LB encode morphological information in addition to Part-of-Speech information. Due to the relatively flexible order of main sentence constituents in Spanish, Cast3LB uses a flat, multiply-branching structure for the S node. There is no VP node, but rather all complements and adjuncts depending on a verb are sisters to the gv (Verb Group) node containing this verb. An example sentence (with the corresponding f-structure) is shown in Figure 1. Tree nodes are additionally labelled with grammatical function tags. Table 1 provides a list of function tags with short explanations. Civit (2004) provides Cast3LB function tag guidelines. Functional tags carry some of the information that would be encoded in terms of tree configurations in languages with stricter constituent order constraints than Spanish. 3 Previous Work 3.1 LFG Annotation A methodology for automatically obtaining LFG f-structures from trees output by probabilistic parsers trained on the Penn-II treebank has been described by Cahill et al. (2004). It has been shown that the methods can be ported to other languages and treebanks (Burke et al., 2004; Cahill et al., 2003), including Cast3LB (O’Donovan et al., 2005). Some</context>
</contexts>
<marker>Civit, 2004</marker>
<rawString>M. Civit. 2004. Gu´ıa para la anotaci´on de las funciones sint´acticas de Cast3LB. Technical report. Available at http://clic.fil.ub.es/personal/ civit/PUBLICA/funcions.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Cowan</author>
<author>M Collins</author>
</authors>
<title>Morphology and reranking for the statistical parsing of Spanish.</title>
<date>2005</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Vancouver, B.C.,</location>
<contexts>
<context position="10544" citStr="Cowan and Collins (2005)" startWordPosition="1664" endWordPosition="1667">ted in Figure 3. We describe it in detail in this and the following sections. We divided the Spanish treebank into a training set of 80%, a development set of 10%, and a test set of 10% of all trees. We randomly assigned treebank files to these sets to ensure that different textual genres are about equally represented among the training, development and test trees. 4.1 Constituency Parsing For constituency parsing we use Bikel’s (2002) parser for which we developed a Spanish language package adapted to the Cast3LB data. Prior to parsing, we perform one of the tree transformations described by Cowan and Collins (2005), i.e. we add a CP and SBAR nodes to subordinate and relative clauses. This is undone in parser output. The category labels in the Spanish treebank are rather fine grained and often contain redundant information.1 We preprocess the treebank and re1For example there are several labels for Nominal Group, 138 Figure 3: Processing architecture for the machinelearning-based method. duce the number of category labels, only retaining distinctions that we deem useful for our purposes.2 For constituency parsing we also reduce the number of POS tags by including only selected morphological features. Tab</context>
</contexts>
<marker>Cowan, Collins, 2005</marker>
<rawString>B. Cowan and M. Collins. 2005. Morphology and reranking for the statistical parsing of Spanish. In Conference on Empirical Methods in Natural Language Processing, Vancouver, B.C., Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Crouch</author>
<author>R M Kaplan</author>
<author>T H King</author>
<author>S Riezler</author>
</authors>
<title>A comparison of evaluation metrics for a broad-coverage stochastic parser.</title>
<date>2002</date>
<booktitle>In Conference on Language Resources and Evaluation (LREC 02).</booktitle>
<contexts>
<context position="20652" citStr="Crouch et al. (2002)" startWordPosition="3346" endWordPosition="3349">lly significant results, except for MBL vs MaxEnt. 6 Task-Based LFG Annotation Evaluation Finally, we also evaluated the actual f-structures obtained by running the LFG-annotation algorithm on trees produced by the parser and enriched with Cast3LB function tags assigned using SVM. For this task-based evaluation we produced a gold standard consisting of f-structures corresponding to all sentences in the test set. The LFG-annotation algorithm was run on the test set trees (which contained original Cast3LB treebank function tags), and the resulting f-structures were manually corrected. Following Crouch et al. (2002), we convert the f-structures to triples of the form (GF, Pi, Pj), where Pi is the value of the PRED attribute of the f-structure, GF is an LFG grammatical function attribute, and Pj is the value of the PRED attribute of the f-structure which is the value of the GF attribute. This is done recursively for each level of embedding in the f-structure. Attributes with atomic values are ignored for the purposes of this evaluation. The results obtained are shown in Table 7. We also performed a statistical significance test for these results, using the same method as for the Cast3LB tag assigment task</context>
</contexts>
<marker>Crouch, Kaplan, King, Riezler, 2002</marker>
<rawString>R. Crouch, R. M. Kaplan, T. H. King, and S. Riezler. 2002. A comparison of evaluation metrics for a broad-coverage stochastic parser. In Conference on Language Resources and Evaluation (LREC 02).</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>A van den Bosch</author>
</authors>
<title>MemoryBased Language Processing.</title>
<date>2005</date>
<publisher>Cambridge University Press,</publisher>
<marker>Daelemans, van den Bosch, 2005</marker>
<rawString>W. Daelemans and A. van den Bosch. 2005. MemoryBased Language Processing. Cambridge University Press, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>J Zavrel</author>
<author>K van der Sloot</author>
<author>A van den Bosch</author>
</authors>
<title>TiMBL: Tilburg Memory Based Learner, version 5.1, Reference Guide. Technical report. Available from http://ilk.uvt.</title>
<date>2004</date>
<pages>0402</pages>
<marker>Daelemans, Zavrel, van der Sloot, van den Bosch, 2004</marker>
<rawString>W. Daelemans, J. Zavrel, K. van der Sloot, and A. van den Bosch. 2004. TiMBL: Tilburg Memory Based Learner, version 5.1, Reference Guide. Technical report. Available from http://ilk.uvt. nl/downloads/pub/papers/ilk0402.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Jijkoun</author>
<author>M de Rijke</author>
</authors>
<title>Enriching the output of a parser using memory-based learning.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<location>Barcelona,</location>
<marker>Jijkoun, de Rijke, 2004</marker>
<rawString>V. Jijkoun and M. de Rijke. 2004. Enriching the output of a parser using memory-based learning. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, pages 311–318, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Le</author>
</authors>
<title>Maximum Entropy Modeling Toolkit for Python and C++. Available at http://homepages.inf.ed.ac.uk/ s0450736/software/maxent/manual.pdf.</title>
<date>2004</date>
<contexts>
<context position="14614" citStr="Le, 2004" startWordPosition="2316" endWordPosition="2317">ment with head verb, yield, human/nonhuman Determiner Noun Adjective Pronoun Verb type type 139 • Local features: head verb, verb person, verb number, parent category • Context features: node features (except position) of the two previous and two following sister nodes (if present). We used cross-validation for refining the set of features and for tuning the parameters of the machine-learning algorithms. We did not use any additional automated feature-selection procedure. We made use of the following implementations: TiMBL (Daelemans et al., 2004) for MemoryBased Learning, the MaxEnt Toolkit (Le, 2004) for Maximum Entropy and LIBSVM (Changcuand Lin, 2001) for Support Vector Machines. For TiMBL we used k nearest neighbors = 7 and the gain ratio metric for feature weighting. For MaxEnt, we used the L-BFGS parameter estimation and 110 iterations, and we regularize the model using a Gaussian prior with u2 = 1. For SVM we used the RBF kernel with -y = 2−7 and the cost parameter C = 32. 076 00 08 8 5 Cast3LB Tag Assignment Evaluation We present evaluation results on the original goldstandard trees of the test set as well as on the test-set sentences parsed by Bikel’s parser. For the evaluation of</context>
</contexts>
<marker>Le, 2004</marker>
<rawString>Zh. Le, 2004. Maximum Entropy Modeling Toolkit for Python and C++. Available at http://homepages.inf.ed.ac.uk/ s0450736/software/maxent/manual.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R O’Donovan</author>
<author>A Cahill</author>
<author>J van Genabith</author>
<author>A Way</author>
</authors>
<title>Automatic acquisition of Spanish LFG resources from the CAST3LB treebank.</title>
<date>2005</date>
<booktitle>In Proceedings of the Tenth International Conference on LFG,</booktitle>
<location>Bergen,</location>
<marker>O’Donovan, Cahill, van Genabith, Way, 2005</marker>
<rawString>R. O’Donovan, A. Cahill, J. van Genabith, and A. Way. 2005. Automatic acquisition of Spanish LFG resources from the CAST3LB treebank. In Proceedings of the Tenth International Conference on LFG, Bergen, Norway.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V N Vapnik</author>
</authors>
<title>Statistical Learning Theory.</title>
<date>1998</date>
<location>Wiley-Interscience,</location>
<contexts>
<context position="13175" citStr="Vapnik, 1998" startWordPosition="2087" endWordPosition="2088">vit, 2000). LB Precision LB Recall F-score All 84.18 83.74 83.96 &lt; 70 84.82 84.35 84.58 Table 3: Parser performance. mance. They use a different, more reduced category label set as well as a different training-test split. Both Cowan and Collins and the present paper report scores which ignore punctuation. 4.2 Cast3LB Function Tagging For the task of Cast3LB function tag assignment we experimented with three generic machine learning algorithms: a memory-based learner (Daelemans and van den Bosch, 2005), a maximum entropy classifier (Berger et al., 1996) and a Support Vector Machine classifier (Vapnik, 1998). For each algorithm we use the same set of features to represent nodes that are to be assigned one of the Cast3LB function tags. We use a special null tag for nodes where no Cast3LB tag is present. In Cast3LB only nodes in certain contexts are eligible for function tags. For this reason we only consider a subset of all nodes as candidates for function tag assignment, namely those which are sisters of nodes with the category labels gv (Verb Group), infinitiu (Infinitive) and gerundi (Gerund). For these candidates we extract the following three types of features encoding configurational, morpho</context>
</contexts>
<marker>Vapnik, 1998</marker>
<rawString>V. N. Vapnik. 1998. Statistical Learning Theory. Wiley-Interscience, September.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>