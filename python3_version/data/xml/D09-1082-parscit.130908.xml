<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001279">
<title confidence="0.991019">
Recognizing Textual Relatedness with Predicate-Argument Structures
</title>
<author confidence="0.999103">
Rui Wang
</author>
<affiliation confidence="0.9967885">
Dept of Computational Linguistics
Saarland University
</affiliation>
<address confidence="0.702741">
66123 Saarbr¨ucken, Germany
</address>
<email confidence="0.99517">
rwang@coli.uni-sb.de
</email>
<author confidence="0.997814">
Yi Zhang
</author>
<affiliation confidence="0.886304">
Dept of Computational Linguistics
Saarland University
LT-Lab, DFKI GmbH
</affiliation>
<address confidence="0.77949">
D-66123 Saarbr¨ucken, Germany
</address>
<email confidence="0.997539">
yzhang@coli.uni-sb.de
</email>
<sectionHeader confidence="0.99386" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9974774375">
In this paper, we first compare several
strategies to handle the newly proposed
three-way Recognizing Textual Entailment
(RTE) task. Then we define a new mea-
surement for a pair of texts, called Textual
Relatedness, which is a weaker concept
than semantic similarity or paraphrase. We
show that an alignment model based on the
predicate-argument structures using this
measurement can help an RTE system to
recognize the Unknown cases at the first
stage, and contribute to the improvement
of the overall performance in the RTE task.
In addition, several heterogeneous lexical
resources are tested, and different contri-
butions from them are observed.
</bodyText>
<sectionHeader confidence="0.998943" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997933066666667">
Recognizing Textual Entailment (RTE) (Dagan et
al., 2006) is a task to detect whether one Hypoth-
esis (H) can be inferred (or entailed) by a Text
(T). Being a challenging task, it has been shown
that it is helpful to applications like question an-
swering (Harabagiu and Hickl, 2006). The recent
research on RTE extends the two-way annotation
into three-way1 2, making it even more difficult,
but more linguistic-motivated.
The straightforward strategy is to treat it as a
three-way classification task, but the performance
suffers a significant drop even when using the
same classifier and the same feature model. In
fact, it can also be dealt with as an extension to the
traditional two-way classification, e.g., by identi-
</bodyText>
<footnote confidence="0.994847333333333">
1http://nlp.stanford.edu/RTE3-pilot/
2http://www.nist.gov/tac/tracks/2008/
rte/rte.08.guidelines.html
</footnote>
<bodyText confidence="0.991441060606061">
fying the Entailment (E) cases first and then fur-
ther label the Contradiction (C) and Unknown (U)
T-H pairs. Some other researchers also work on
detecting negative cases, i.e. contradiction, in-
stead of entailment (de Marneffe et al., 2008).
However, according to our best knowledge, the
detailed comparison between these strategies has
not been fully explored, let alone the impact of the
linguistic motivation behind the strategy selection.
This paper will address this issue.
Take the following example from the RTE-4 test
set (Giampiccolo et al., 2009) as an example,
T: At least five people have been killed in
a head-on train collision in north-eastern
France, while others are still trapped in the
wreckage. All the victims are adults.
H: A French train crash killed children.
This is a pair of two contradicting texts, the
mentioning of events (i.e. train crash) in both T
and H are assumed to refer the same event3. In
fact, the only contradicting part lies in the sec-
ond sentence of T against H, that is, whether
there are children among the victims. Therefore,
this pair could also be classified as a Known (K)
pair (=E∪C) against Unknown (U) pairs, instead
of being classified as a Non-entailment (N) case
(=C∪U) against E case in the traditional two-way
annotation.
Furthermore, many state-of-the-art RTE ap-
proaches which are based on overlapping informa-
tion or similarity functions between T and H, in
fact over-cover the E cases, and sometimes, cover
the C cases as well. Therefore, in this paper, we
</bodyText>
<footnote confidence="0.998809666666667">
3See more details about the annotation guideline at
http://www.nist.gov/tac/tracks/2008/rte/
rte.08.guidelines.html
</footnote>
<page confidence="0.911052">
784
</page>
<note confidence="0.9966265">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 784–792,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999917048780488">
would like to test whether applying this style of
approaches to capture the K cases instead of E
cases is more effective. While in lexical seman-
tics, semantic relatedness is a weaker concept than
semantic similarity, there is no counterpart at the
sentence or text level. Therefore, in this paper, we
propose a Recognizing Textual Relatedness (RTR)
task as a subtask or the first step of RTE. By doing
so, we choose predicate-argument structure (PAS)
as the feature representation, which has already
been shown quite useful in the previous RTE chal-
lenges (Wang and Neumann, 2007).
In order to obtain the PAS, we utilize a Semantic
Role Labeling (SRL) system developed by Zhang
et al. (2008). Although SRL has been shown to be
effective for many tasks, e.g. information extrac-
tion, question answering, etc., it has not been suc-
cessfully used for RTE, mainly due to the low cov-
erage of the verb frame or semantic role resources
or the low performance of the automatic SRL sys-
tems. The recent CoNLL shared tasks (Surdeanu
et al., 2008; Hajiˇc et al., 2009) have been focus-
ing on semantic dependency parsing along with
the traditional syntactic dependency parsing. The
PAS from the system output is almost ready for
use to build applications based on it. Therefore,
another focus of this paper will be to apply SRL to
the RTE task. In particular, it can improve the first
stage binary classification (K vs. U), and the final
result improves as well.
The rest of the paper will be organized as fol-
lows: Section 2 will give a brief literature review
on both RTE and SRL; Section 3 describes the se-
mantic parsing system, which includes a syntactic
dependency parser and an SRL system; Section 4
presents an algorithm to align two PASs to recog-
nize textual relatedness between T and H, using
several lexical resources; The experiments will be
described in Section 5, followed by discussions;
and the final section will conclude the paper and
point out directions to work on in the future.
</bodyText>
<sectionHeader confidence="0.999804" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999915542372882">
Although the term of Textual Relatedness has not
been widely used by the community (as far as
we know), many researchers have already incor-
porated modules to tackle it, which are usually
implemented as an alignment module before the
inference/learning module is applied. For exam-
ple, Pado et al. (2009) mentioned two alignment
modules, one is a phrase-based alignment system
called MANLI (MacCartney et al., 2008), and the
other is a stochastic aligner based on dependency
graphs. Siblini and Kosseim (2009) performed the
alignment on top of two ontologies. In this paper,
we would like to follow this line of research but on
another level of representation, i.e. the predicate-
argument structures (PAS), together with different
lexical semantic resources.
As for the whole RTE task, many people di-
rectly do the three-way classification with selec-
tive features (e.g. Agichtein et al. (2009)) or dif-
ferent inference rules to identify entailment and
contradiction simultaneously (e.g. Clark and Har-
rison (2009)); while other researchers also extend
their two-way classification system into three-way
by performing a second-stage classification after-
wards. An interesting task proposed by de Marn-
effe et al. (2008) suggested an alternative way to
deal with the three-way classification, that is, to
split out the contradiction cases first. However,
it has been shown to be more difficult than the
entailment recognition. Based on these previous
works and our experimental observations, we pro-
pose an alternative two-stage binary classification
approach, i.e. to identify the unknown cases from
the known cases (entailment and contradiction)
first. And the results show that due to the nature
of these approaches based on overlapping infor-
mation or similarity between T and H, this way of
splitting is more reasonable.
However, RTE systems using semantic role la-
belers has not shown very promising results, al-
though SRL has been successfully used in many
other NLP tasks, e.g. information extraction,
question answering, etc. According to our anal-
ysis of the data, there are mainly three reasons: a)
the limited coverage of the verb frames or predi-
cates; b) the undetermined relationships between
two frames or predicates; and c) the unsatisfy-
ing performance of an automatic SRL system.
For instance, Burchardt et al. (2007) attempted to
use FrameNet (Baker et al., 1998) for the RTE-3
challenge, but did not show substantial improve-
ment. With the recent CoNLL challenges, more
and more robust and accurate SRL systems are
ready for use, especially for the PAS identifica-
tion. For the lexical semantics, we also discover
that, if we relax the matching criteria (from simi-
larity to relatedness), heterougeous resources can
contribute to the coverage differently and then the
effectiveness of PAS will be shown as well.
</bodyText>
<page confidence="0.99887">
785
</page>
<sectionHeader confidence="0.989348" genericHeader="method">
3 Semantic Parsing
</sectionHeader>
<bodyText confidence="0.999988948717949">
In order to obtain the predicate-argument struc-
tures for the textual entailment corpus, we use the
semantic role labeler described in (Zhang et al.,
2008). The SRL system is trained on the Wall
Street Journal sections of the Penn Treebank us-
ing PropBank and NomBank annotation of ver-
bal and nominal predicates, and relations to their
arguments, and produces as outputs the semantic
dependencies. The head words of the arguments
(including modifiers) are annotated as a direct de-
pendent of the corresponding predicate words, la-
beled with the type of the semantic relation (Arg0,
Arg1 ..., and various ArgMs). Note that for the
application of SRL in RTE task, the PropBank and
NomBank notation appears to be more accessible
and robust than the the FrameNet notation (with
much more detailed roles or frame elements bond
to specific verb frames).
As input, the SRL system requires syntactic
dependency analysis. We use the open source
MST Parser (McDonald et al., 2005), trained also
on the Wall Street Journal Sections of the Penn
Treebank, using a projective decoder with second-
order features. Then the SRL system goes through
a pipeline of 4-stage processing: predicate identifi-
cation (PI) identifies words that evokes a semantic
predicate; argument identification (AI) identifies
the arguments of the predicates; argument classifi-
cation (AC) labels the argument with the semantic
relations (roles); and predicate classification (PC)
further differentiate different use of the predicate
word. All components are built as maximal en-
tropy based classifiers, with their parameters es-
timated by the open source TADM system4, fea-
ture sets selected on the development set. Evalu-
ation results from previous years’ CoNLL shared
tasks show that the system achieves state-of-the-
art performance, especially for its out-domain ap-
plications.
</bodyText>
<sectionHeader confidence="0.996172" genericHeader="method">
4 Textual Relatedness
</sectionHeader>
<bodyText confidence="0.999920142857143">
As we mentioned in the introduction, we break
down the three-way classification into a two-stage
binary classification. Furthermore, we treat the
first stage as a subtask of the main task, which
determines whether H is relevant to T. Similar to
the probabilistic entailment score, we use a relat-
edness score to measure such relationship. Due
</bodyText>
<footnote confidence="0.804133">
4http://tadm.sourceforge.net/
</footnote>
<bodyText confidence="0.99979868">
to the nature of the entailment recognition that
H should be fully entailed by T, we also make
this relatedness relationship asymmetric. Roughly
speaking, this Relatedness function R(T, H) can
be described as whether or how relevant H is to
some part of T. The relevance can be realized as
string similarity, semantic similarity, or being co-
occurred in similar contexts.
Before we define the relatedness function for-
mally, let us look at the representation again. After
semantic parsing described in the previous section,
we obtain a PAS for each sentence. On top of it,
we define a predicate-argument graph (PAG), the
nodes of which are predicates, arguments or some-
times both, and the edges of which are labeled se-
mantic relations. Notice that each predicate can
dominate zero, one, or more arguments, and each
argument have one or more predicates which dom-
inate it. Furthermore, the graph is not necessar-
ily fully connected. Thus, the R(T, H) function
can be defined on the dependency representation
as follows: if the PAG of H is semantically rel-
evant to part of the PAG of T, H is semantically
relevant to T.
In order to compare the two graphs, we further
reduce the alignment complexity by breaking the
graphs into sets of trees. Two types of decomposed
trees are considered: one is to take each predicate
as the root of a tree and arguments as child nodes,
and the other is on the contrary, to take each ar-
gument as root and their governing predicates as
child nodes. We name them as Predicate Trees (P-
Trees) and Argument Trees (A-Trees) respectively.
To obtain the P-Trees, we enumerate each predi-
cate, find all the arguments which it directly dom-
inates, and then construct a P-Tree. The algorithm
to obtain A-Trees works in the similar way. Fi-
nally, we will have a set of P-Trees and a set of A-
Trees for each PAG, both of which are simple trees
with depth of one. Figure 1 shows an example of
such procedures. Notice that we do not consider
cross-sentential inference, instead, we simply take
the union of tree sets from all the sentences. Figure
2 illustrates the PAG for both T and H after seman-
tic parsing, and the resulting P-Trees and A-Trees
after applying the decomposition algorithm.
Formally, we define the relatedness function for
a T-H pair as the maximum value of the related-
ness scores of all pairs of trees in T and H (P-trees
and A-trees).
</bodyText>
<page confidence="0.996658">
786
</page>
<figureCaption confidence="0.765293">
Figure 1: Decomposition of predicate-argument graphs (left) into P-Trees (right top) and A-Trees (right
bottom)
</figureCaption>
<figure confidence="0.991452684210526">
d e f
a
b c e
d e f
e
g
a a b a
e
d e
f
g
b c
P−Tree(s)
a
A−Tree(s)
g
�R(TreeTi, TreeHj)�
R(T, H) = max
1&lt;i&lt;r,1&lt;j&lt;s
</figure>
<bodyText confidence="0.9988263">
In order to compare two P-Trees or A-Trees,
we further define each predicate-argument pair
contained in a tree as a semantic dependency
triple. Each semantic dependency triple con-
tains a predicate, an argument, and the seman-
tic dependency label in between, in the form
of (Predicate, Dependency, Argument). Then
we define the relatedness function between two
trees as the minimum value of the relatedness
scores of all the triple pairs from the two trees.
</bodyText>
<equation confidence="0.438085333333333">
R(TreeT, TreeH) = min
1&lt;i&lt;n,1&lt;j&lt;m
IR((PT, DTi, ATi), (PH, DHj, AHj))I
</equation>
<bodyText confidence="0.976137625">
For the relatedness function between two se-
mantic dependency triples, we define the follow-
ing two settings: the FULL match and the NOT-
FULL match. Either match requires that the pred-
icates are related at the first place. The former
means both the dependencies and the arguments
are related; while the latter only requires the de-
pendencies to be related.
</bodyText>
<construct confidence="0.650658">
R((PT, DT, AT), (PH, DH, AH)) =
Full R(PT,PH)=R(DT,DH)=R(AT,AH)=1
NotFull R(PT,PH)=R(DT,DH)=1
Other Otherwise
</construct>
<bodyText confidence="0.9999025">
Now, the only missing components in our defi-
nition is the relatedness functions between pred-
icates, arguments, and semantic dependencies.
Fortunately, many people have done research on
semantic relatedness in lexical semantics that we
could use. Therefore, these functions can be
realized by different string matching algorithms
and/or lexical resources. Since the meaning of rel-
evance is rather wide, apart from the string match-
ing of the lemmas, we also incorporate various
resources, from distributionally collected ones to
hand-crafted ontologies. We choose VerbOcean
(Chklovski and Pantel, 2004) to obtain the relat-
edness between predicates (after using WordNet
(Fellbaum, 1998) to change all the nominal pred-
icates into verbs) and use WordNet for the argu-
ment alignment. For the verb relations in Ver-
bOcean, we consider all of them as related; and
for WordNet, we not only use the synonyms, hy-
ponyms, and hypernyms, but antonyms as well.
Consequently, we simplify these basic relatedness
functions into a binary decision. If the correspond-
ing strings are matched or the relations mentioned
above exist, the two predicates, arguments, or de-
pendencies are related; otherwise, not.
In addition, the Normalized Google Distance
(NGD) (Cilibrasi and Vitanyi, 2007) is applied to
both cases5. As for the comparison between de-
pendencies, we simply apply the string matching,
except for modifier labels, which we treat them as
the same6. In all, the main idea here is to incorpo-
rate both distributional semantics and ontological
semantics in order to see whether their contribu-
tions are overlapping or complementary. In prac-
tice, we use empirical value 0.5 as the threshold.
Below the threshold means they are related, oth-
</bodyText>
<footnote confidence="0.995382">
5You may find the NGD values of all the con-
tent word pairs in RTE-3 and RTE-4 datasets at
http://www.coli.uni-sb.de/˜rwang/
resources/RTE3_RTE4_NGD.txt.
6This is mainly because it is more difficult for the SRL
system to differentiate modifier labels than the complements.
</footnote>
<figure confidence="0.73360975">
⎧
⎨
⎩
787
</figure>
<figureCaption confidence="0.949365">
Figure 2: Predicate-argument graphs and corresponding P-Trees and A-trees of the T-H pair
</figureCaption>
<figure confidence="0.997591113207547">
A1
A1
A1
A1
AM−ADV
AM−ADV
A−Trees
train
crash children
people while ...... train still ......
trapped
killed
AM−ADV
A0 A1
killed
collision
T H
crash
A1
AM−ADV
A1
people
while
...... train
still
children
A1
... ...
PAG
train
killed
A1
AM−ADV
collision
A1
trapped
AM−ADV
killed
A0 A1
people
while
...... train
still
... ...
crash
children
train
killed killed killed collision trapped trapped
killed killed crash
A0
crash
A1
P−Trees
</figure>
<bodyText confidence="0.9985285">
erwise not. In order to achieve a better coverage,
we use the OR operator to connect all the related-
ness functions above, which means, if any of them
holds, the two items are related.
Notice that, although we define only the relat-
edness between T and H, in principle, the graph
representation can also be used for the entailment
relationship. However, since it needs more fine-
grained analysis and resources, we will leave it as
the future work.
</bodyText>
<sectionHeader confidence="0.999464" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999990666666667">
In order to evaluate our method, we setup several
experiments. The baseline system here is a simple
Naive Bayes classifier with a feature set contain-
ing the Bag-of-Words (BoW) overlapping ratio be-
tween T and H, and also the syntactic dependency
overlapping ratio. The feature model combines
two baseline systems proposed by previous work,
which gives out quite competitive performance.
Since the main goal of this paper is to show the
impact of the PAS-based alignment module, we
will not compare our results with other RTE sys-
tems (In fact, the baseline system already outper-
forms the average accuracy score of the RTE-4
challenge).
The main data set used for testing here is the
RTE-4 data set with three-way annotations (500
entailment T-H pairs (E), 150 contradiction pairs
(C), and 350 unknown pairs (U)). The results on
RTE-3 data set (combination of the development
set and test set, in all, 822 E pairs, 161 C pairs,
and 617 U pairs) is also shown, although the origi-
nal annotation is two-way and the three-way anno-
tation was done by different researchers after the
challenge7.
We will first show the performance of the base-
line systems, followed by the results of our PAS-
based alignment module and its impact on the
whole task. After that, we will also give more de-
tailed analysis of our alignment module, according
to different lexical relatedness measurements.
</bodyText>
<footnote confidence="0.5644124">
7The annotation of the development set was done by stu-
dents at Stanford, and the annotation of the test set was done
as double annotation by NIST assessors, followed by adjudi-
cation of disagreements. Answers were kept consistent with
the two-way decisions in the main task gold answer file.
</footnote>
<page confidence="0.992568">
788
</page>
<subsectionHeader confidence="0.97347">
5.1 Baselines
</subsectionHeader>
<bodyText confidence="0.999974951219512">
The baseline systems used here are based on over-
lapping ratio of words and syntactic dependencies
between T and H. For the word overlapping ratio,
we calculate the number of overlapping tokens be-
tween T and H and normalize it by dividing it by
the number of tokens in H. The syntactic depen-
dency overlapping ratio works similarly: we cal-
culate the number of overlapping syntactic depen-
dencies and divide it by the number of syntactic
dependencies in H, i.e. the same as the number
of tokens. Enlightened by the relatedness func-
tion, we also allow either FULL match (meaning
both the dependencies and the parent tokens are
matched), and NOTFULL match (meaning only the
dependencies are matched). Here we only use
string match between lemmas and syntactic de-
pendencies. Table 1 presents the performance of
the baseline system.
The results show that, even with the same clas-
sifier and the same feature model, with a proper
two-stage strategy, it can already achieve better
results than the three-way classification. Note
that, the first strategy is not so successful, and
that is the traditional two-way annotation of the
RTE task. Our explanation here is that the BoW
method (even with syntactic dependency features)
is based on overlapping information shared by T
and H, which essentially means the more informa-
tion they share, the more relevant they are, instead
of being more similar or the same. Therefore, for
the “ECU → E/CU” setting, methods based on
overlapping information are not the best choice,
while for “ECU → U/EC”, they are more ap-
propriate.
In addition, the upper bound numbers show the
accuracy when the first-stage classification is per-
fect, which give us an indication of how far we
could go. The lower upper bound for the second
strategy is mainly due to the low proportion of the
C cases (15%) in the data set; while the other two
both show large space for improvement.
</bodyText>
<subsectionHeader confidence="0.999213">
5.2 The PAS-based Alignment Module
</subsectionHeader>
<bodyText confidence="0.996362266666667">
In this subsection, we present a separate evalua-
tion of our PAS-based alignment module. As we
mentioned before (cf. Section 4), there are sev-
eral parameters to be tuned in our alignment algo-
rithm: a) whether the relatedness function between
P-Trees asks for the FULL match; b) whether the
function for A-Trees asks for the FULL match; and
c) whether both P-Trees and A-Trees being related
are required or either of them holds is enough.
Since they are all binary values, we use the 3-digit
code to represent each setting, e.g. [FFO]8 means
either P-Trees are FULL matched or A-Trees are
FULL matched. The performances of different set-
tings of the module are shown in the following
Precision-Recall figure 3,
</bodyText>
<figureCaption confidence="0.8890595">
Figure 3: Precision and recall of different align-
ment settings
</figureCaption>
<bodyText confidence="0.999977166666667">
Since we will combine this module with the
baseline system and it will be integrated as the
first-stage classification, the F1 scores are not in-
dicative for selecting the best setting. Intuitively,
we may prefer higher precision than recall.
One limitation of our method we need to point
out here is that, if some important predicates or ar-
guments in H are not (correctly) identified by the
SRL system, fewer P-Trees and A-Trees are re-
quired to be related to some part of T, thus, the
relatedness of the whole pair could easily be satis-
fied, leading to false positive cases.
</bodyText>
<subsectionHeader confidence="0.998926">
5.3 Impact on the Final Results
</subsectionHeader>
<bodyText confidence="0.999876454545454">
The best settings for RTE-3 data set is [NNA] and
for RTE-4 data set is [NFO], which are both in the
middle of the setting range shown in the previous
figure 3.
As for the integration of the PAS-based align-
ment model with our BoW-based baseline, we
only consider the third two-stage classification
strategy in Table 1. Other strategies would also be
interesting to try, however, the proposed alignment
algorithm exploits relatedness between T and H,
which might not be fine-grained enough to detect
</bodyText>
<footnote confidence="0.8585435">
8F stands for FULL, and O stands for OR. Other letters
are, N stands for NOTFULL, and A stands for AND.
</footnote>
<page confidence="0.990223">
789
</page>
<table confidence="0.9988155">
Strategies Three-Way Two-Stage
E/C/U E/CU → E/C/U C/EU → C/E/U U/EC → U/E/C
Accuray 53.20% 50.00% 53.50% 54.20%
Upper Bound / 82.80% 68.70% 84.90%
</table>
<tableCaption confidence="0.999897">
Table 1: Performances of the Baselines
</tableCaption>
<bodyText confidence="0.99995121875">
entailment or contradiction. New alignment algo-
rithm has to be designed to explore other strate-
gies. Thus, in this work, we believe that the align-
ment algorithm based on PAS (and other methods
based on overlapping information between T and
H) is suitable for the U/EC → U/E/C classifi-
cation strategy.
Table 2 shows the final results.
The first observation is that the improvement of
accuracy on the first stage of the classification can
be preserved to the final results. And our PAS-
based alignment module can help, though there
is still large space for improvement. Compared
with the significantly improved results on RTE-4,
the improvement on RTE-3 is less obvious, mainly
due to the relatively lower precision (70.33% vs.
79.67%) of the alignment module itself.
Also, we have to say that the improvement is not
as big as we expected. There are several reasons
for this. Besides the limitation of our approach
mentioned in the previous section, the predicates
and arguments themselves might be too sparse to
convey all the information we need for the en-
tailment detection. In addition, in some sense,
the baseline is quite strong for this comparison,
since the PAS-based alignment module relies on
the overlapping words at the first place, there are
quite a few pairs solved by both the main approach
and the baseline. Then, it would be interesting
to take a closer look at the lexical resources used
in the main system, which is another additional
knowledge it has, comparing with the baseline.
</bodyText>
<subsectionHeader confidence="0.991919">
5.4 Impact of the Lexical Resources
</subsectionHeader>
<bodyText confidence="0.99997536">
We did an ablation test of the lexical resources
used in our alignment module. Recall that we
have applied three lexical resources, VerbOcean
for the predicate relatedness function, WordNet
for the argument relatedness function, and Nor-
malized Google Distance for both. Table 3 shows
the performances of the system without each of the
resources,
The results clearly show that each lexical re-
source does contribute some improvement to the
final performance of the system and it confirms
the idea of combining lexical resources being ac-
quired in different ways. For instance, at the
beginning, we expected that the relationship be-
tween “people” and “children” could be captured
by WordNet, but in fact not. Fortunately, the NGD
has a quite low value of this pair of words (0.21),
which suggests that they occur together quite of-
ten, or in other words, they are relevant.
One interesting future work on this aspect is to
substitute the OR connector between these lexical
resources with an AND operator. Thus, instead of
using them to achieve a higher coverage, whether
they could be filters for each other to increase the
precision will also be interesting to know.
</bodyText>
<sectionHeader confidence="0.99749" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.9997928">
In this paper, we address the motivation and issues
of casting the three-way RTE problem into a two-
stage binary classification task. We apply an SRL
system to derive the predicate-argument structure
of the input sentences, and propose ways of cal-
culating semantic relatedness between the shallow
semantic structures of T and H. The experiments
show improvements in the first-stage classifica-
tion, which accordingly contribute to the final re-
sults of the RTE task.
For future work, we would like to see whether
the PAS can help the second-stage classification
as well, e.g. the semantic dependency of negation
(AM-NEG) could be helpful for the contraction
recognition. Furthermore, since the PAS is usu-
ally a bag of unconnected graphs, we could find
a way to joint them together, in order to consider
both inter- and intra- sentential inferences based
on it.
In addition, this approach has the potential to
be integrated with other RTE modules. For in-
stance, for the predicate alignment, we may con-
sider to use DIRT rules (Lin and Pantel, 2001)
or other paraphrase resources (Callison-Burch,
2008), and for the argument alignment, exter-
nal named-entity recognizer and anaphora resolver
would be very helpful. Even more, we also plan to
compare/combine it with other methods which are
not based on overlapping information between T
and H.
</bodyText>
<page confidence="0.986909">
790
</page>
<table confidence="0.9998325">
Systems Baseline1 Baseline2 SRL+Baseline2 The First Stage
Data Sets Three-Way Two-Stage Two-Stage Baseline2 SRL+Baseline2 SRL
RTE-3 [NNA] 52.19% 52.50% 53.69%(2.87%1) 59.50% 60.56%(1.78%1) 70.33%
RTE-4 [NFO] 53.20% 54.20% 56.60%(6.39%1) 67.10% 70.20%(4.62%1) 79.67%
</table>
<tableCaption confidence="0.990597">
Table 2: Results on the Whole Datasets
</tableCaption>
<table confidence="0.999330666666667">
Data Sets SRL+Baseline SRL+Baseline - VO SRL+Baseline - NGD SRL+Baseline - WN
RTE-3 [NNA] 53.69% 53.19%(0.93%1) 53.50%(0.35%1) 52.88%(1.51%1)
RTE-4 [NFO] 56.60% 56.00%(1.06%1) 56.10%(0.88%1) 55.70%(1.59%1)
</table>
<tableCaption confidence="0.999558">
Table 3: Impact of the Lexical Resources
</tableCaption>
<sectionHeader confidence="0.997094" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999830333333333">
The first author is funded by the PIRE PhD
scholarship program sponsored by the German
Research Foundation (DFG). The second author
thanks the German Excellence Cluster of Multi-
modal Computing and Interaction for the support
of the work.
</bodyText>
<sectionHeader confidence="0.998932" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999857605263158">
Eugene Agichtein, Walt Askew, and Yandong Liu.
2009. Combining Lexical, Syntactic, and Semantic
Evidence for Textual Entailment Classification. In
Proceedings of the First Text Analysis Conference
(TAC 2008).
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceed-
ings of the 36th Annual Meeting of the Associa-
tion for Computational Linguistics and 17th Inter-
national Conference on Computational Linguistics,
Volume 1, pages 86–90, Montreal, Canada.
Aljoscha Burchardt, Nils Reiter, Stefan Thater, and
Anette Frank. 2007. A semantic approach to textual
entailment: System evaluation and task analysis. In
Proceedings of the ACL-PASCAL Workshop on Tex-
tual Entailment and Paraphrasing, Prague, Czech
Republic.
Chris Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of EMNLP.
Timothy Chklovski and Patrick Pantel. 2004. VerbO-
cean: Mining the Web for Fine-Grained Semantic
Verb Relations. In Proceedings of Conference on
Empirical Methods in Natural Language Processing
(EMNLP-04), Barcelona, Spain.
Rudi Cilibrasi and Paul M. B. Vitanyi. 2007. The
Google Similarity Distance. IEEE/ACM Trans-
actions on Knowledge and Data Engineering,
19(3):370–383.
Peter Clark and Phil Harrison. 2009. Recognizing
Textual Entailment with Logical Inference. In Pro-
ceedings of the First Text Analysis Conference (TAC
2008).
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL Recognising Textual Entail-
ment Challenge. In Machine Learning Challenges,
volume 3944 of Lecture Notes in Computer Science,
pages 177–190. Springer.
Marie-Catherine de Marneffe, Anna N. Rafferty, and
Christopher D. Manning. 2008. Finding contradic-
tions in text. In Proceedings of ACL-08.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Danilo Giampiccolo, Hoa Trang Dang, Bernardog
Magnini, Ido Dagan, Elena Cabrio, and Bill Dolan.
2009. The Fourth PASCAL Recognizing Textual
Entailment Challenge. In Proceedings of the First
Text Analysis Conference (TAC 2008).
Jan Hajiˇc, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Martf, Llufs
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad´o, Jan ˇStˇep´anek, Pavel Straˇn´ak, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic depen-
dencies in multiple languages. In Proceedings of
the 13th Conference on Computational Natural Lan-
guage Learning, Boulder, CO, USA.
Sanda Harabagiu and Andrew Hickl. 2006. Meth-
ods for Using Textual Entailment in Open-Domain
Question Answering. In Proceedings of COLING-
ACL 2006, pages 905–912, Sydney, Australia.
Dekang Lin and Patrick Pantel. 2001. DIRT - Dis-
covery of Inference Rules from Text. In In Proceed-
ings of the ACM SIGKDD Conference on Knowledge
Discovery and Data Mining.
Bill MacCartney, Michel Galley, and Christopher D.
Manning. 2008. A phrase-based alignment model
for natural language inference. In Proceedings of
EMNLP 2008.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-Projective Dependency Pars-
ing using Spanning Tree Algorithms. In Proceed-
ings of hlt-emnlp 2005, pages 523–530, Vancouver,
Canada.
Sebastian Pado, Marie-Catherine de Marneffe, Bill
MacCartney, Anna N. Rafferty, Eric Yeh, and
</reference>
<page confidence="0.971595">
791
</page>
<reference confidence="0.99960904">
Christopher D. Manning. 2009. Deciding en-
tailment and contradiction with stochastic and edit
distance-based alignment. In Proceedings of the
First Text Analysis Conference (TAC 2008).
Reda Siblini and Leila Kosseim. 2009. Using Ontol-
ogy Alignment for the TAC RTE Challenge. In Pro-
ceedings of the First Text Analysis Conference (TAC
2008).
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu´ıs M`arquez, and Joakim Nivre. 2008. The
CoNLL-2008 shared task on joint parsing of syntac-
tic and semantic dependencies. In Proceedings of
the 12th conference on computational natural lan-
guage learning (CoNLL-2008), Manchester, UK.
Rui Wang and G¨unter Neumann. 2007. Recog-
nizing textual entailment using a subsequence ker-
nel method. In Proceedings of the Twenty-Second
AAAI Conference on Artificial Intelligence (AAAI-
07), pages 937–942, Vancouver, Canada.
Yi Zhang, Rui Wang, and Hans Uszkoreit. 2008. Hy-
brid Learning of Dependency Structures from Het-
erogeneous Linguistic Resources. In Proceedings of
the Twelfth Conference on Computational Natural
Language Learning (CoNLL 2008), pages 198–202,
Manchester, UK.
</reference>
<page confidence="0.997388">
792
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.144801">
<title confidence="0.999969">Recognizing Textual Relatedness with Predicate-Argument Structures</title>
<author confidence="0.995965">Rui</author>
<affiliation confidence="0.7692085">Dept of Computational Saarland</affiliation>
<address confidence="0.938129">66123 Saarbr¨ucken,</address>
<email confidence="0.993602">rwang@coli.uni-sb.de</email>
<author confidence="0.98294">Yi</author>
<affiliation confidence="0.8432885">Dept of Computational Saarland</affiliation>
<address confidence="0.558719">LT-Lab, DFKI D-66123 Saarbr¨ucken,</address>
<email confidence="0.991899">yzhang@coli.uni-sb.de</email>
<abstract confidence="0.997721705882353">In this paper, we first compare several strategies to handle the newly proposed Textual Entailment (RTE) task. Then we define a new meafor a pair of texts, called which is a weaker concept than semantic similarity or paraphrase. We show that an alignment model based on the predicate-argument structures using this measurement can help an RTE system to the at the first stage, and contribute to the improvement of the overall performance in the RTE task. In addition, several heterogeneous lexical resources are tested, and different contributions from them are observed.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Agichtein</author>
<author>Walt Askew</author>
<author>Yandong Liu</author>
</authors>
<title>Combining Lexical, Syntactic, and Semantic Evidence for Textual Entailment Classification.</title>
<date>2009</date>
<booktitle>In Proceedings of the First Text Analysis Conference (TAC</booktitle>
<contexts>
<context position="6479" citStr="Agichtein et al. (2009)" startWordPosition="1025" endWordPosition="1028">ied. For example, Pado et al. (2009) mentioned two alignment modules, one is a phrase-based alignment system called MANLI (MacCartney et al., 2008), and the other is a stochastic aligner based on dependency graphs. Siblini and Kosseim (2009) performed the alignment on top of two ontologies. In this paper, we would like to follow this line of research but on another level of representation, i.e. the predicateargument structures (PAS), together with different lexical semantic resources. As for the whole RTE task, many people directly do the three-way classification with selective features (e.g. Agichtein et al. (2009)) or different inference rules to identify entailment and contradiction simultaneously (e.g. Clark and Harrison (2009)); while other researchers also extend their two-way classification system into three-way by performing a second-stage classification afterwards. An interesting task proposed by de Marneffe et al. (2008) suggested an alternative way to deal with the three-way classification, that is, to split out the contradiction cases first. However, it has been shown to be more difficult than the entailment recognition. Based on these previous works and our experimental observations, we prop</context>
</contexts>
<marker>Agichtein, Askew, Liu, 2009</marker>
<rawString>Eugene Agichtein, Walt Askew, and Yandong Liu. 2009. Combining Lexical, Syntactic, and Semantic Evidence for Textual Entailment Classification. In Proceedings of the First Text Analysis Conference (TAC 2008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The berkeley framenet project.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics,</booktitle>
<volume>1</volume>
<pages>86--90</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="7956" citStr="Baker et al., 1998" startWordPosition="1255" endWordPosition="1258">ilarity between T and H, this way of splitting is more reasonable. However, RTE systems using semantic role labelers has not shown very promising results, although SRL has been successfully used in many other NLP tasks, e.g. information extraction, question answering, etc. According to our analysis of the data, there are mainly three reasons: a) the limited coverage of the verb frames or predicates; b) the undetermined relationships between two frames or predicates; and c) the unsatisfying performance of an automatic SRL system. For instance, Burchardt et al. (2007) attempted to use FrameNet (Baker et al., 1998) for the RTE-3 challenge, but did not show substantial improvement. With the recent CoNLL challenges, more and more robust and accurate SRL systems are ready for use, especially for the PAS identification. For the lexical semantics, we also discover that, if we relax the matching criteria (from similarity to relatedness), heterougeous resources can contribute to the coverage differently and then the effectiveness of PAS will be shown as well. 785 3 Semantic Parsing In order to obtain the predicate-argument structures for the textual entailment corpus, we use the semantic role labeler described</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The berkeley framenet project. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 1, pages 86–90, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aljoscha Burchardt</author>
<author>Nils Reiter</author>
<author>Stefan Thater</author>
<author>Anette Frank</author>
</authors>
<title>A semantic approach to textual entailment: System evaluation and task analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="7909" citStr="Burchardt et al. (2007)" startWordPosition="1247" endWordPosition="1250"> approaches based on overlapping information or similarity between T and H, this way of splitting is more reasonable. However, RTE systems using semantic role labelers has not shown very promising results, although SRL has been successfully used in many other NLP tasks, e.g. information extraction, question answering, etc. According to our analysis of the data, there are mainly three reasons: a) the limited coverage of the verb frames or predicates; b) the undetermined relationships between two frames or predicates; and c) the unsatisfying performance of an automatic SRL system. For instance, Burchardt et al. (2007) attempted to use FrameNet (Baker et al., 1998) for the RTE-3 challenge, but did not show substantial improvement. With the recent CoNLL challenges, more and more robust and accurate SRL systems are ready for use, especially for the PAS identification. For the lexical semantics, we also discover that, if we relax the matching criteria (from similarity to relatedness), heterougeous resources can contribute to the coverage differently and then the effectiveness of PAS will be shown as well. 785 3 Semantic Parsing In order to obtain the predicate-argument structures for the textual entailment cor</context>
</contexts>
<marker>Burchardt, Reiter, Thater, Frank, 2007</marker>
<rawString>Aljoscha Burchardt, Nils Reiter, Stefan Thater, and Anette Frank. 2007. A semantic approach to textual entailment: System evaluation and task analysis. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
</authors>
<title>Syntactic constraints on paraphrases extracted from parallel corpora.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="26813" citStr="Callison-Burch, 2008" startWordPosition="4396" endWordPosition="4397">work, we would like to see whether the PAS can help the second-stage classification as well, e.g. the semantic dependency of negation (AM-NEG) could be helpful for the contraction recognition. Furthermore, since the PAS is usually a bag of unconnected graphs, we could find a way to joint them together, in order to consider both inter- and intra- sentential inferences based on it. In addition, this approach has the potential to be integrated with other RTE modules. For instance, for the predicate alignment, we may consider to use DIRT rules (Lin and Pantel, 2001) or other paraphrase resources (Callison-Burch, 2008), and for the argument alignment, external named-entity recognizer and anaphora resolver would be very helpful. Even more, we also plan to compare/combine it with other methods which are not based on overlapping information between T and H. 790 Systems Baseline1 Baseline2 SRL+Baseline2 The First Stage Data Sets Three-Way Two-Stage Two-Stage Baseline2 SRL+Baseline2 SRL RTE-3 [NNA] 52.19% 52.50% 53.69%(2.87%1) 59.50% 60.56%(1.78%1) 70.33% RTE-4 [NFO] 53.20% 54.20% 56.60%(6.39%1) 67.10% 70.20%(4.62%1) 79.67% Table 2: Results on the Whole Datasets Data Sets SRL+Baseline SRL+Baseline - VO SRL+Basel</context>
</contexts>
<marker>Callison-Burch, 2008</marker>
<rawString>Chris Callison-Burch. 2008. Syntactic constraints on paraphrases extracted from parallel corpora. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Chklovski</author>
<author>Patrick Pantel</author>
</authors>
<title>VerbOcean: Mining the Web for Fine-Grained Semantic Verb Relations.</title>
<date>2004</date>
<booktitle>In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP-04),</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="14834" citStr="Chklovski and Pantel, 2004" startWordPosition="2386" endWordPosition="2389">R(DT,DH)=1 Other Otherwise Now, the only missing components in our definition is the relatedness functions between predicates, arguments, and semantic dependencies. Fortunately, many people have done research on semantic relatedness in lexical semantics that we could use. Therefore, these functions can be realized by different string matching algorithms and/or lexical resources. Since the meaning of relevance is rather wide, apart from the string matching of the lemmas, we also incorporate various resources, from distributionally collected ones to hand-crafted ontologies. We choose VerbOcean (Chklovski and Pantel, 2004) to obtain the relatedness between predicates (after using WordNet (Fellbaum, 1998) to change all the nominal predicates into verbs) and use WordNet for the argument alignment. For the verb relations in VerbOcean, we consider all of them as related; and for WordNet, we not only use the synonyms, hyponyms, and hypernyms, but antonyms as well. Consequently, we simplify these basic relatedness functions into a binary decision. If the corresponding strings are matched or the relations mentioned above exist, the two predicates, arguments, or dependencies are related; otherwise, not. In addition, th</context>
</contexts>
<marker>Chklovski, Pantel, 2004</marker>
<rawString>Timothy Chklovski and Patrick Pantel. 2004. VerbOcean: Mining the Web for Fine-Grained Semantic Verb Relations. In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP-04), Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rudi Cilibrasi</author>
<author>Paul M B Vitanyi</author>
</authors>
<title>The Google Similarity Distance.</title>
<date>2007</date>
<journal>IEEE/ACM Transactions on Knowledge and Data Engineering,</journal>
<volume>19</volume>
<issue>3</issue>
<contexts>
<context position="15498" citStr="Cilibrasi and Vitanyi, 2007" startWordPosition="2492" endWordPosition="2495">redicates (after using WordNet (Fellbaum, 1998) to change all the nominal predicates into verbs) and use WordNet for the argument alignment. For the verb relations in VerbOcean, we consider all of them as related; and for WordNet, we not only use the synonyms, hyponyms, and hypernyms, but antonyms as well. Consequently, we simplify these basic relatedness functions into a binary decision. If the corresponding strings are matched or the relations mentioned above exist, the two predicates, arguments, or dependencies are related; otherwise, not. In addition, the Normalized Google Distance (NGD) (Cilibrasi and Vitanyi, 2007) is applied to both cases5. As for the comparison between dependencies, we simply apply the string matching, except for modifier labels, which we treat them as the same6. In all, the main idea here is to incorporate both distributional semantics and ontological semantics in order to see whether their contributions are overlapping or complementary. In practice, we use empirical value 0.5 as the threshold. Below the threshold means they are related, oth5You may find the NGD values of all the content word pairs in RTE-3 and RTE-4 datasets at http://www.coli.uni-sb.de/˜rwang/ resources/RTE3_RTE4_N</context>
</contexts>
<marker>Cilibrasi, Vitanyi, 2007</marker>
<rawString>Rudi Cilibrasi and Paul M. B. Vitanyi. 2007. The Google Similarity Distance. IEEE/ACM Transactions on Knowledge and Data Engineering, 19(3):370–383.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Clark</author>
<author>Phil Harrison</author>
</authors>
<title>Recognizing Textual Entailment with Logical Inference.</title>
<date>2009</date>
<booktitle>In Proceedings of the First Text Analysis Conference (TAC</booktitle>
<contexts>
<context position="6597" citStr="Clark and Harrison (2009)" startWordPosition="1041" endWordPosition="1045">MANLI (MacCartney et al., 2008), and the other is a stochastic aligner based on dependency graphs. Siblini and Kosseim (2009) performed the alignment on top of two ontologies. In this paper, we would like to follow this line of research but on another level of representation, i.e. the predicateargument structures (PAS), together with different lexical semantic resources. As for the whole RTE task, many people directly do the three-way classification with selective features (e.g. Agichtein et al. (2009)) or different inference rules to identify entailment and contradiction simultaneously (e.g. Clark and Harrison (2009)); while other researchers also extend their two-way classification system into three-way by performing a second-stage classification afterwards. An interesting task proposed by de Marneffe et al. (2008) suggested an alternative way to deal with the three-way classification, that is, to split out the contradiction cases first. However, it has been shown to be more difficult than the entailment recognition. Based on these previous works and our experimental observations, we propose an alternative two-stage binary classification approach, i.e. to identify the unknown cases from the known cases (</context>
</contexts>
<marker>Clark, Harrison, 2009</marker>
<rawString>Peter Clark and Phil Harrison. 2009. Recognizing Textual Entailment with Logical Inference. In Proceedings of the First Text Analysis Conference (TAC 2008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The PASCAL Recognising Textual Entailment Challenge.</title>
<date>2006</date>
<booktitle>In Machine Learning Challenges,</booktitle>
<volume>3944</volume>
<pages>177--190</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="1041" citStr="Dagan et al., 2006" startWordPosition="141" endWordPosition="144"> Textual Entailment (RTE) task. Then we define a new measurement for a pair of texts, called Textual Relatedness, which is a weaker concept than semantic similarity or paraphrase. We show that an alignment model based on the predicate-argument structures using this measurement can help an RTE system to recognize the Unknown cases at the first stage, and contribute to the improvement of the overall performance in the RTE task. In addition, several heterogeneous lexical resources are tested, and different contributions from them are observed. 1 Introduction Recognizing Textual Entailment (RTE) (Dagan et al., 2006) is a task to detect whether one Hypothesis (H) can be inferred (or entailed) by a Text (T). Being a challenging task, it has been shown that it is helpful to applications like question answering (Harabagiu and Hickl, 2006). The recent research on RTE extends the two-way annotation into three-way1 2, making it even more difficult, but more linguistic-motivated. The straightforward strategy is to treat it as a three-way classification task, but the performance suffers a significant drop even when using the same classifier and the same feature model. In fact, it can also be dealt with as an exte</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2006</marker>
<rawString>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The PASCAL Recognising Textual Entailment Challenge. In Machine Learning Challenges, volume 3944 of Lecture Notes in Computer Science, pages 177–190. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Anna N Rafferty</author>
<author>Christopher D Manning</author>
</authors>
<title>Finding contradictions in text.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08.</booktitle>
<marker>de Marneffe, Rafferty, Manning, 2008</marker>
<rawString>Marie-Catherine de Marneffe, Anna N. Rafferty, and Christopher D. Manning. 2008. Finding contradictions in text. In Proceedings of ACL-08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="14917" citStr="Fellbaum, 1998" startWordPosition="2400" endWordPosition="2401">s functions between predicates, arguments, and semantic dependencies. Fortunately, many people have done research on semantic relatedness in lexical semantics that we could use. Therefore, these functions can be realized by different string matching algorithms and/or lexical resources. Since the meaning of relevance is rather wide, apart from the string matching of the lemmas, we also incorporate various resources, from distributionally collected ones to hand-crafted ontologies. We choose VerbOcean (Chklovski and Pantel, 2004) to obtain the relatedness between predicates (after using WordNet (Fellbaum, 1998) to change all the nominal predicates into verbs) and use WordNet for the argument alignment. For the verb relations in VerbOcean, we consider all of them as related; and for WordNet, we not only use the synonyms, hyponyms, and hypernyms, but antonyms as well. Consequently, we simplify these basic relatedness functions into a binary decision. If the corresponding strings are matched or the relations mentioned above exist, the two predicates, arguments, or dependencies are related; otherwise, not. In addition, the Normalized Google Distance (NGD) (Cilibrasi and Vitanyi, 2007) is applied to both</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danilo Giampiccolo</author>
<author>Hoa Trang Dang</author>
<author>Bernardog Magnini</author>
<author>Ido Dagan</author>
<author>Elena Cabrio</author>
<author>Bill Dolan</author>
</authors>
<title>The Fourth PASCAL Recognizing Textual Entailment Challenge.</title>
<date>2009</date>
<booktitle>In Proceedings of the First Text Analysis Conference (TAC</booktitle>
<contexts>
<context position="2362" citStr="Giampiccolo et al., 2009" startWordPosition="341" endWordPosition="344">lot/ 2http://www.nist.gov/tac/tracks/2008/ rte/rte.08.guidelines.html fying the Entailment (E) cases first and then further label the Contradiction (C) and Unknown (U) T-H pairs. Some other researchers also work on detecting negative cases, i.e. contradiction, instead of entailment (de Marneffe et al., 2008). However, according to our best knowledge, the detailed comparison between these strategies has not been fully explored, let alone the impact of the linguistic motivation behind the strategy selection. This paper will address this issue. Take the following example from the RTE-4 test set (Giampiccolo et al., 2009) as an example, T: At least five people have been killed in a head-on train collision in north-eastern France, while others are still trapped in the wreckage. All the victims are adults. H: A French train crash killed children. This is a pair of two contradicting texts, the mentioning of events (i.e. train crash) in both T and H are assumed to refer the same event3. In fact, the only contradicting part lies in the second sentence of T against H, that is, whether there are children among the victims. Therefore, this pair could also be classified as a Known (K) pair (=E∪C) against Unknown (U) pa</context>
</contexts>
<marker>Giampiccolo, Dang, Magnini, Dagan, Cabrio, Dolan, 2009</marker>
<rawString>Danilo Giampiccolo, Hoa Trang Dang, Bernardog Magnini, Ido Dagan, Elena Cabrio, and Bill Dolan. 2009. The Fourth PASCAL Recognizing Textual Entailment Challenge. In Proceedings of the First Text Analysis Conference (TAC 2008).</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jan Hajiˇc</author>
<author>Massimiliano Ciaramita</author>
<author>Richard Johansson</author>
<author>Daisuke Kawahara</author>
<author>Maria Ant`onia Martf</author>
<author>Llufs M`arquez</author>
<author>Adam Meyers</author>
<author>Joakim Nivre</author>
<author>Sebastian Pad´o</author>
<author>Jan ˇStˇep´anek</author>
<author>Pavel Straˇn´ak</author>
<author>Mihai Surdeanu</author>
<author>Nianwen Xue</author>
<author>Yi Zhang</author>
</authors>
<title>The CoNLL2009 shared task: Syntactic and semantic dependencies in multiple languages.</title>
<date>2009</date>
<booktitle>In Proceedings of the 13th Conference on Computational Natural Language Learning,</booktitle>
<location>Boulder, CO, USA.</location>
<marker>Hajiˇc, Ciaramita, Johansson, Kawahara, Martf, M`arquez, Meyers, Nivre, Pad´o, ˇStˇep´anek, Straˇn´ak, Surdeanu, Xue, Zhang, 2009</marker>
<rawString>Jan Hajiˇc, Massimiliano Ciaramita, Richard Johansson, Daisuke Kawahara, Maria Ant`onia Martf, Llufs M`arquez, Adam Meyers, Joakim Nivre, Sebastian Pad´o, Jan ˇStˇep´anek, Pavel Straˇn´ak, Mihai Surdeanu, Nianwen Xue, and Yi Zhang. 2009. The CoNLL2009 shared task: Syntactic and semantic dependencies in multiple languages. In Proceedings of the 13th Conference on Computational Natural Language Learning, Boulder, CO, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda Harabagiu</author>
<author>Andrew Hickl</author>
</authors>
<title>Methods for Using Textual Entailment in Open-Domain Question Answering.</title>
<date>2006</date>
<booktitle>In Proceedings of COLINGACL</booktitle>
<pages>905--912</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="1264" citStr="Harabagiu and Hickl, 2006" startWordPosition="182" endWordPosition="185">d on the predicate-argument structures using this measurement can help an RTE system to recognize the Unknown cases at the first stage, and contribute to the improvement of the overall performance in the RTE task. In addition, several heterogeneous lexical resources are tested, and different contributions from them are observed. 1 Introduction Recognizing Textual Entailment (RTE) (Dagan et al., 2006) is a task to detect whether one Hypothesis (H) can be inferred (or entailed) by a Text (T). Being a challenging task, it has been shown that it is helpful to applications like question answering (Harabagiu and Hickl, 2006). The recent research on RTE extends the two-way annotation into three-way1 2, making it even more difficult, but more linguistic-motivated. The straightforward strategy is to treat it as a three-way classification task, but the performance suffers a significant drop even when using the same classifier and the same feature model. In fact, it can also be dealt with as an extension to the traditional two-way classification, e.g., by identi1http://nlp.stanford.edu/RTE3-pilot/ 2http://www.nist.gov/tac/tracks/2008/ rte/rte.08.guidelines.html fying the Entailment (E) cases first and then further lab</context>
</contexts>
<marker>Harabagiu, Hickl, 2006</marker>
<rawString>Sanda Harabagiu and Andrew Hickl. 2006. Methods for Using Textual Entailment in Open-Domain Question Answering. In Proceedings of COLINGACL 2006, pages 905–912, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>DIRT - Discovery of Inference Rules from Text. In</title>
<date>2001</date>
<booktitle>In Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining.</booktitle>
<contexts>
<context position="26760" citStr="Lin and Pantel, 2001" startWordPosition="4388" endWordPosition="4391">ute to the final results of the RTE task. For future work, we would like to see whether the PAS can help the second-stage classification as well, e.g. the semantic dependency of negation (AM-NEG) could be helpful for the contraction recognition. Furthermore, since the PAS is usually a bag of unconnected graphs, we could find a way to joint them together, in order to consider both inter- and intra- sentential inferences based on it. In addition, this approach has the potential to be integrated with other RTE modules. For instance, for the predicate alignment, we may consider to use DIRT rules (Lin and Pantel, 2001) or other paraphrase resources (Callison-Burch, 2008), and for the argument alignment, external named-entity recognizer and anaphora resolver would be very helpful. Even more, we also plan to compare/combine it with other methods which are not based on overlapping information between T and H. 790 Systems Baseline1 Baseline2 SRL+Baseline2 The First Stage Data Sets Three-Way Two-Stage Two-Stage Baseline2 SRL+Baseline2 SRL RTE-3 [NNA] 52.19% 52.50% 53.69%(2.87%1) 59.50% 60.56%(1.78%1) 70.33% RTE-4 [NFO] 53.20% 54.20% 56.60%(6.39%1) 67.10% 70.20%(4.62%1) 79.67% Table 2: Results on the Whole Datase</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. DIRT - Discovery of Inference Rules from Text. In In Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill MacCartney</author>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>A phrase-based alignment model for natural language inference.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<contexts>
<context position="6003" citStr="MacCartney et al., 2008" startWordPosition="949" endWordPosition="952">everal lexical resources; The experiments will be described in Section 5, followed by discussions; and the final section will conclude the paper and point out directions to work on in the future. 2 Related Work Although the term of Textual Relatedness has not been widely used by the community (as far as we know), many researchers have already incorporated modules to tackle it, which are usually implemented as an alignment module before the inference/learning module is applied. For example, Pado et al. (2009) mentioned two alignment modules, one is a phrase-based alignment system called MANLI (MacCartney et al., 2008), and the other is a stochastic aligner based on dependency graphs. Siblini and Kosseim (2009) performed the alignment on top of two ontologies. In this paper, we would like to follow this line of research but on another level of representation, i.e. the predicateargument structures (PAS), together with different lexical semantic resources. As for the whole RTE task, many people directly do the three-way classification with selective features (e.g. Agichtein et al. (2009)) or different inference rules to identify entailment and contradiction simultaneously (e.g. Clark and Harrison (2009)); whi</context>
</contexts>
<marker>MacCartney, Galley, Manning, 2008</marker>
<rawString>Bill MacCartney, Michel Galley, and Christopher D. Manning. 2008. A phrase-based alignment model for natural language inference. In Proceedings of EMNLP 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
<author>Kiril Ribarov</author>
<author>Jan Hajic</author>
</authors>
<title>Non-Projective Dependency Parsing using Spanning Tree Algorithms.</title>
<date>2005</date>
<booktitle>In Proceedings of hlt-emnlp</booktitle>
<pages>523--530</pages>
<location>Vancouver, Canada.</location>
<contexts>
<context position="9392" citStr="McDonald et al., 2005" startWordPosition="1490" endWordPosition="1493"> and produces as outputs the semantic dependencies. The head words of the arguments (including modifiers) are annotated as a direct dependent of the corresponding predicate words, labeled with the type of the semantic relation (Arg0, Arg1 ..., and various ArgMs). Note that for the application of SRL in RTE task, the PropBank and NomBank notation appears to be more accessible and robust than the the FrameNet notation (with much more detailed roles or frame elements bond to specific verb frames). As input, the SRL system requires syntactic dependency analysis. We use the open source MST Parser (McDonald et al., 2005), trained also on the Wall Street Journal Sections of the Penn Treebank, using a projective decoder with secondorder features. Then the SRL system goes through a pipeline of 4-stage processing: predicate identification (PI) identifies words that evokes a semantic predicate; argument identification (AI) identifies the arguments of the predicates; argument classification (AC) labels the argument with the semantic relations (roles); and predicate classification (PC) further differentiate different use of the predicate word. All components are built as maximal entropy based classifiers, with their</context>
</contexts>
<marker>McDonald, Pereira, Ribarov, Hajic, 2005</marker>
<rawString>Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Hajic. 2005. Non-Projective Dependency Parsing using Spanning Tree Algorithms. In Proceedings of hlt-emnlp 2005, pages 523–530, Vancouver, Canada.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Sebastian Pado</author>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Anna N Rafferty</author>
</authors>
<title>Eric Yeh,</title>
<location>and</location>
<marker>Pado, de Marneffe, MacCartney, Rafferty, </marker>
<rawString>Sebastian Pado, Marie-Catherine de Marneffe, Bill MacCartney, Anna N. Rafferty, Eric Yeh, and</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
</authors>
<title>Deciding entailment and contradiction with stochastic and edit distance-based alignment.</title>
<date>2009</date>
<booktitle>In Proceedings of the First Text Analysis Conference (TAC</booktitle>
<marker>Manning, 2009</marker>
<rawString>Christopher D. Manning. 2009. Deciding entailment and contradiction with stochastic and edit distance-based alignment. In Proceedings of the First Text Analysis Conference (TAC 2008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reda Siblini</author>
<author>Leila Kosseim</author>
</authors>
<title>Using Ontology Alignment for the TAC RTE Challenge.</title>
<date>2009</date>
<booktitle>In Proceedings of the First Text Analysis Conference (TAC</booktitle>
<contexts>
<context position="6097" citStr="Siblini and Kosseim (2009)" startWordPosition="964" endWordPosition="967">sions; and the final section will conclude the paper and point out directions to work on in the future. 2 Related Work Although the term of Textual Relatedness has not been widely used by the community (as far as we know), many researchers have already incorporated modules to tackle it, which are usually implemented as an alignment module before the inference/learning module is applied. For example, Pado et al. (2009) mentioned two alignment modules, one is a phrase-based alignment system called MANLI (MacCartney et al., 2008), and the other is a stochastic aligner based on dependency graphs. Siblini and Kosseim (2009) performed the alignment on top of two ontologies. In this paper, we would like to follow this line of research but on another level of representation, i.e. the predicateargument structures (PAS), together with different lexical semantic resources. As for the whole RTE task, many people directly do the three-way classification with selective features (e.g. Agichtein et al. (2009)) or different inference rules to identify entailment and contradiction simultaneously (e.g. Clark and Harrison (2009)); while other researchers also extend their two-way classification system into three-way by perform</context>
</contexts>
<marker>Siblini, Kosseim, 2009</marker>
<rawString>Reda Siblini and Leila Kosseim. 2009. Using Ontology Alignment for the TAC RTE Challenge. In Proceedings of the First Text Analysis Conference (TAC 2008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Richard Johansson</author>
<author>Adam Meyers</author>
<author>Llu´ıs M`arquez</author>
<author>Joakim Nivre</author>
</authors>
<title>The CoNLL-2008 shared task on joint parsing of syntactic and semantic dependencies.</title>
<date>2008</date>
<booktitle>In Proceedings of the 12th conference on computational natural language learning (CoNLL-2008),</booktitle>
<location>Manchester, UK.</location>
<marker>Surdeanu, Johansson, Meyers, M`arquez, Nivre, 2008</marker>
<rawString>Mihai Surdeanu, Richard Johansson, Adam Meyers, Llu´ıs M`arquez, and Joakim Nivre. 2008. The CoNLL-2008 shared task on joint parsing of syntactic and semantic dependencies. In Proceedings of the 12th conference on computational natural language learning (CoNLL-2008), Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rui Wang</author>
<author>G¨unter Neumann</author>
</authors>
<title>Recognizing textual entailment using a subsequence kernel method.</title>
<date>2007</date>
<booktitle>In Proceedings of the Twenty-Second AAAI Conference on Artificial Intelligence (AAAI07),</booktitle>
<pages>937--942</pages>
<location>Vancouver, Canada.</location>
<contexts>
<context position="4173" citStr="Wang and Neumann, 2007" startWordPosition="635" endWordPosition="638">pore, 6-7 August 2009. c�2009 ACL and AFNLP would like to test whether applying this style of approaches to capture the K cases instead of E cases is more effective. While in lexical semantics, semantic relatedness is a weaker concept than semantic similarity, there is no counterpart at the sentence or text level. Therefore, in this paper, we propose a Recognizing Textual Relatedness (RTR) task as a subtask or the first step of RTE. By doing so, we choose predicate-argument structure (PAS) as the feature representation, which has already been shown quite useful in the previous RTE challenges (Wang and Neumann, 2007). In order to obtain the PAS, we utilize a Semantic Role Labeling (SRL) system developed by Zhang et al. (2008). Although SRL has been shown to be effective for many tasks, e.g. information extraction, question answering, etc., it has not been successfully used for RTE, mainly due to the low coverage of the verb frame or semantic role resources or the low performance of the automatic SRL systems. The recent CoNLL shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009) have been focusing on semantic dependency parsing along with the traditional syntactic dependency parsing. The PAS from the s</context>
</contexts>
<marker>Wang, Neumann, 2007</marker>
<rawString>Rui Wang and G¨unter Neumann. 2007. Recognizing textual entailment using a subsequence kernel method. In Proceedings of the Twenty-Second AAAI Conference on Artificial Intelligence (AAAI07), pages 937–942, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi Zhang</author>
<author>Rui Wang</author>
<author>Hans Uszkoreit</author>
</authors>
<title>Hybrid Learning of Dependency Structures from Heterogeneous Linguistic Resources.</title>
<date>2008</date>
<booktitle>In Proceedings of the Twelfth Conference on Computational Natural Language Learning (CoNLL</booktitle>
<pages>198--202</pages>
<location>Manchester, UK.</location>
<contexts>
<context position="4284" citStr="Zhang et al. (2008)" startWordPosition="655" endWordPosition="658"> the K cases instead of E cases is more effective. While in lexical semantics, semantic relatedness is a weaker concept than semantic similarity, there is no counterpart at the sentence or text level. Therefore, in this paper, we propose a Recognizing Textual Relatedness (RTR) task as a subtask or the first step of RTE. By doing so, we choose predicate-argument structure (PAS) as the feature representation, which has already been shown quite useful in the previous RTE challenges (Wang and Neumann, 2007). In order to obtain the PAS, we utilize a Semantic Role Labeling (SRL) system developed by Zhang et al. (2008). Although SRL has been shown to be effective for many tasks, e.g. information extraction, question answering, etc., it has not been successfully used for RTE, mainly due to the low coverage of the verb frame or semantic role resources or the low performance of the automatic SRL systems. The recent CoNLL shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009) have been focusing on semantic dependency parsing along with the traditional syntactic dependency parsing. The PAS from the system output is almost ready for use to build applications based on it. Therefore, another focus of this paper </context>
<context position="8580" citStr="Zhang et al., 2008" startWordPosition="1356" endWordPosition="1359">the RTE-3 challenge, but did not show substantial improvement. With the recent CoNLL challenges, more and more robust and accurate SRL systems are ready for use, especially for the PAS identification. For the lexical semantics, we also discover that, if we relax the matching criteria (from similarity to relatedness), heterougeous resources can contribute to the coverage differently and then the effectiveness of PAS will be shown as well. 785 3 Semantic Parsing In order to obtain the predicate-argument structures for the textual entailment corpus, we use the semantic role labeler described in (Zhang et al., 2008). The SRL system is trained on the Wall Street Journal sections of the Penn Treebank using PropBank and NomBank annotation of verbal and nominal predicates, and relations to their arguments, and produces as outputs the semantic dependencies. The head words of the arguments (including modifiers) are annotated as a direct dependent of the corresponding predicate words, labeled with the type of the semantic relation (Arg0, Arg1 ..., and various ArgMs). Note that for the application of SRL in RTE task, the PropBank and NomBank notation appears to be more accessible and robust than the the FrameNet</context>
</contexts>
<marker>Zhang, Wang, Uszkoreit, 2008</marker>
<rawString>Yi Zhang, Rui Wang, and Hans Uszkoreit. 2008. Hybrid Learning of Dependency Structures from Heterogeneous Linguistic Resources. In Proceedings of the Twelfth Conference on Computational Natural Language Learning (CoNLL 2008), pages 198–202, Manchester, UK.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>