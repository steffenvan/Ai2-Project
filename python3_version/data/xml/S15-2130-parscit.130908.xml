<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.055219">
<title confidence="0.986391">
Sentiue: Target and Aspect based Sentiment Analysis
in SemEval-2015 Task 12
</title>
<author confidence="0.641954">
Jos´e Saias
</author>
<address confidence="0.72989">
DI - ECT - Universidade de ´Evora
Rua Rom˜ao Ramalho, 59
7000-671 ´Evora, Portugal
</address>
<email confidence="0.994929">
jsaias@uevora.pt
</email>
<sectionHeader confidence="0.995573" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999972875">
This paper describes our participation in
SemEval-2015 Task 12, and the opinion min-
ing system sentiue. The general idea is
that systems must determine the polarity of the
sentiment expressed about a certain aspect of
a target entity. For slot 1, entity and attribute
category detection, our system applies a super-
vised machine learning classifier, for each la-
bel, followed by a selection based on the prob-
ability of the entity/attribute pair, on that do-
main. The target expression detection, for slot
2, is achieved by using a catalog of known
targets for each entity type, complemented
with named entity recognition. In the opin-
ion sentiment slot, we used a 3 class polarity
classifier, having BoW, lemmas, bigrams after
verbs, presence of polarized terms, and punc-
tuation based features. Working in uncon-
strained mode, our results for slot 1 were as-
sessed with precision between 57% and 63%,
and recall varying between 42% and 47%.
In sentiment polarity, sentiue’s result ac-
curacy was approximately 79%, reaching the
best score in 2 of the 3 domains.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99855956097561">
Social networks and other online platforms are an
important communication mechanism in current
lifestyle. These platforms aggregate user-generated
content, such as opinions that people write and
publish freely on the Web, and are now valued
for market research and trend analysis. Natural
language processing (NLP) helps to automatically
extract information from these written opinions.
This paper describes a participation in SemEval-
2015 Task 121, Aspect Based Sentiment Analysis
(Pontiki et al., 2015), with the sentiue system,
from Universidade de ´Evora. In previous editions
of SemEval, we participated in Sentiment Analysis
(SA) tasks, but in terms of overall polarity, over
Twitter messages (Rosenthal et al., 2014), not being
aspect oriented. The general idea for this challenge,
is that, for a text, the system must determine the
polarity of the sentiment expressed about a certain
aspect of a particular target entity. Our sentiue
system is an evolution from our previous work
(Saias and Fernandes, 2013; Saias, 2014), for target
oriented SA.
Task 12 was run in two phases. In phase A systems
are tested for aspect detection with one slot to aspect
category, and a second slot for the opinion target
expression on the text. Test data includes review
texts for two domains: restaurants and laptops. In
phase B, aspect category is provided, and systems
must assign a polarity (positive, negative, or neutral)
for each opinion. In this phase, systems received
also texts from a third domain, hotels, for which no
sentiment training data was given.
We used a supervised machine learning classifier
combined with a probability based selection pro-
cess, for entity and attribute category detection, on
slot 1. Target expression detection was performed
with an entity catalog, filled with known targets
for each entity type, and named entity recognition
(NER). For the sentiment polarity slot, we used a a
supervised machine learning classifier, having bag-
of-words (BoW), lemmas, bigrams after verbs, and
</bodyText>
<footnote confidence="0.993138">
1http://alt.qcri.org/semeval2015/task12/
</footnote>
<page confidence="0.908636">
767
</page>
<bodyText confidence="0.7509548">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 767–771,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
punctuation based features, along with sentiment
lexicon based features. The detailed procedure is
explained in section 3.
</bodyText>
<sectionHeader confidence="0.99934" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999871225806451">
Many SA related publications, originating both in
industry and in academia, have appeared, and it is
notorious the growing interest by companies. Pop-
ular scientific forums and events include activities
and workshops on this area, such as RepLab (Amig´o
et al., 2014) at CLEF2, for online reputation, or
ABSA and Twitter SA tasks in SemEval.
In last year’s edition of this SemEval task (Pontiki et
al., 2014), there were 26 systems participating in the
polarity subtask. The two systems with better polar-
ity classification accuracy were from NRC-Canada
and DCU teams. NRC-Canada system (Kiritchenko
et al., 2014) was trained with the data provided in
the task, and complemented with lexicons generated
from other corpora of customer reviews, to help
feature extraction in machine learning. Stanford
CoreNLP was used to tokenize, POS tagging, and
dependency parse trees. They address polarity
classification with a linear SVM classifier, with
features for: the target, and its surrounding words;
POS based features; dependency tree based features;
unigrams and bigrams; lexicon based features. The
DCU system (Wagner et al., 2014) also uses SVM
for aspect and for polarity classification, combining
bag-of-n-gram features with rule-based features.
N-grams (with size from 1 to 5) in a window around
the aspect term, are used as features, as well as
features derived from a sentiment lexicon. The
rule-based approach to predict the polarity of an
aspect term, generated features considering all
words score and their distance to the aspect term.
</bodyText>
<sectionHeader confidence="0.986434" genericHeader="method">
3 Method
</sectionHeader>
<bodyText confidence="0.999908">
Our participation involved the adaptation of our pre-
vious real-time system, for text overall sentiment
classification, into a target oriented SA system. The
next subsections explain how the system works, for
each part of Task 12 challenge.
</bodyText>
<footnote confidence="0.89912">
2http://clef2014.clef-initiative.eu/
</footnote>
<subsectionHeader confidence="0.999908">
3.1 Aspect Entity and Attribute
</subsectionHeader>
<bodyText confidence="0.999949622222222">
The first annotation task focuses on aspect category.
This category is an entity and attribute pair, each
chosen from an inventory with possible values, in
each domain, for entity types and attributes. Since
the possible category types are known and limited,
we decided to use a classifier for each entity type
(e.g. food, laptop) and for each attribute label (e.g.
price, quality). Our approach comprises two stages.
The first processes each review sentence assigning
to it zero, one, or more entity types and attribute la-
bels. The second stage chooses and combines iden-
tified entities and attributes, forming the aspect an-
notation. Analyzing the training data, we found that
in the same sentence, there may be opinions on var-
ious types of entity (e.g. CPU, battery) or attributes.
Thus, we have chosen to train a classifier for each
entity type, and a classifier for each attribute la-
bel. We set a supervised machine learning text clas-
sifier, using MALLET (McCallum, 2002), a Java-
based tool for NLP, with machine learning applica-
tions to text. For the purpose of this stage, it was
necessary to prepare the training data for each bi-
nary classifier, that would determine whether a sen-
tence contains an opinion on its tag (entity type or at-
tribute label). The train process was the same for all
tags, entity type or attribute label, of each domain.
We created a dataset where each instance is a sen-
tence text, and its class is tag, if the sentence had at
least one opinion with that tag, or no tag otherwise.
Text preprocessing includes tokenization, POS tag-
ging and lemmatization, all performed with Stanford
CoreNLP (Toutanova et al., 2003; Manning et al.,
2014) tool. The classifier algorithm was Maximum
Entropy3, and the classifier model features were text
words and lemmas.
Second stage starts with each sentence annotated
with a set and tags, some for entity type and some
for attribute label. When a sentence has no annota-
tions, the system assumes that there is no opinion. In
case of 1 tag on entity type and 1 tag in the attribute
label, then it is the trivial case where the junction
of the two results in the aspect annotation. For sen-
tences with 1 tag on entity type and 0 tags for the
attribute, our system searches for the most frequent
aspect annotation, within the sentence domain, that
</bodyText>
<footnote confidence="0.847818">
3MALLET class: cc.mallet.classify.MaxEnt
</footnote>
<page confidence="0.989936">
768
</page>
<bodyText confidence="0.999946083333333">
includes that entity type. The equivalent is applied
in the case of 0 tags to entity and 1 tag for the at-
tribute label. If both sides have one or more tags,
the system applies a cycle, where each loop iteration
forms the more frequent pair (entity,attribute) in that
domain, and removes these two tags from the sen-
tence tag set. This is repeated until the first, entity
or attribute side, exhausts the tags provided by the
previous stage classifier. And if some tags are left,
on the opposite side of the pair, the system applies,
for each, the same process already explained for case
0-1 or 1-0.
</bodyText>
<subsectionHeader confidence="0.999019">
3.2 Opinion Target Expression
</subsectionHeader>
<bodyText confidence="0.999981222222222">
At this point, sentences are already marked as hav-
ing (or not) opinions on certain aspect category.
For each opinion on restaurants domain, the sys-
tem needed to identify the entity mention on the
sentence text, referred to as the opinion target ex-
pression (OTE). We collected the opinion targets for
each entity type, from the training data, forming a
catalog. If any of the targets already known (e.g.
restaurant name, or meal) appears in the sentence
text, next to a verb or adjective, it is chosen as the
OTE. If this does not lead to any OTE candidate, our
system applies named entity recognition, looking for
references to organization and location entities, us-
ing Stanford NER tool (Finkel et al., 2005; Manning
et al., 2014). Having found one OTE, through the
catalog or by NER, its text and position are marked
in slot 2. If no mention is found, OTE slot is filled
with the NULL value.
</bodyText>
<subsectionHeader confidence="0.999713">
3.3 Sentiment Polarity
</subsectionHeader>
<bodyText confidence="0.99971459375">
Phase B was held in a subsequent period, and the
input given to the systems is a little different, hav-
ing the correct annotations on the aspect category,
in restaurants, laptops and hotels domains. For each
opinion, the participating systems must assign a sen-
timent polarity (positive, negative or neutral), con-
sidering the opinion aspect.
For training, there were 1654 opinions on restau-
rants domain, and 1974 more opinions about lap-
tops, all annotated for polarity. No sentiment train-
ing data was given for hotels domain. Considering
the available data, and the objective of this phase, we
used a supervised machine learning classifier to pre-
dict each opinion polarity. Instead of multiple classi-
fiers, such as implemented for slot 1, we prepared a
single classifier, thought, as before, for text but tuned
with a different model, so that it can choose between
positive, negative or neutral polarity.
Sentences without opinion are not considered in the
training, because here the polarity is associated with
opinions. Further, a single sentence may have sev-
eral opinions about different aspects, and each may
have a different and independent polarity. To train
the classifier, for each opinion we created a polarity
data instance, containing the sentence text, its do-
main, its aspect entity and attribute, OTE (if avail-
able, in restaurants), and the opinion polarity to be
learned. As before, MALLET was used with a Max-
imum Entropy classifier. The sentence text prepro-
cessing was the same we did for aspect category
classification. The features to represent each in-
stance were:
</bodyText>
<listItem confidence="0.9880983125">
• BoW with a feature for each token text;
• lemmas for verbs and adjectives;
• bigram after verb (lemmatized);
• presence of negation terms;
• bigram after negation term;
• presence of exclamation/question mark;
• presence of polarized terms (positive or nega-
tive), according to each sentiment lexicon;
• whether there are polarized terms before excla-
mation mark and question mark;
• bigram before, and after, any polarized term;
• polarity inversion, by negation detection before
some polarized term;
• presence of polarized terms in the last 5 tokens;
• a feature for the domain, and two features for
the entity type and the attribute label.
</listItem>
<bodyText confidence="0.9891628">
To see whether a term is polarized, each token text
is verified in each sentiment lexicon. These polarity
support resources are AFINN lexicon (Nielsen,
2011), Bing Liu’s opinion lexicon (Liu et al., 2005)
and MPQA subjectivity clues (Wiebe et al., 2005).
</bodyText>
<page confidence="0.995921">
769
</page>
<table confidence="0.999598">
Domain Precision Recall F-measure
restaurants 0,633 0,472 0,541
laptops 0,577 0,441 0,500
</table>
<tableCaption confidence="0.9999215">
Table 1: sentiue’s evaluation on aspect category.
Table 3: sentiue accuracy on sentiment polarity.
</tableCaption>
<table confidence="0.9905205">
Domain Accuracy
restaurants 0,787
laptops 0,793
hotels 0,788
Domain Precision Recall F-measure
restaurants 0,488 0,336 0,398
</table>
<tableCaption confidence="0.999735">
Table 2: sentiue’s evaluation on target detection.
</tableCaption>
<bodyText confidence="0.999431416666667">
After some experimentation, we decided to use a
single full train, joining the instances of restaurants
and laptops as a whole training set. The resulting
model was used to classify the opinion polarity for
the three domains.
Because we used sentiment lexicons, our system
operates in unconstrained mode. These additional
resources served as support for features extraction.
No supplementary training texts were used. In our
development testing, we obtained an 80% accuracy
for polarity. After this, the result is written in XML
format for submission.
</bodyText>
<sectionHeader confidence="0.999864" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.995191409090909">
The phase A test data had 685 sentences on restau-
rants domain and 761 on laptops domain. With the
method described above, the sentiue system ex-
tracted 596 opinion categories for restaurants do-
main and 751 other for laptops domain.
Table 1 shows the evaluation for slot 1. Among the
15 submissions evaluated in the first domain, the
best system F-score value was 0,627, while our re-
sult F-score was 0,541. For laptops aspect category,
sentiue’s scores were lower, but improving in the
comparison with other systems, achieving the sec-
ond best F-measure, out of 9 evaluated submissions.
The evaluation of our result in opinion target expres-
sion in given in Table 2. This was a poor result,
when compared with the 0,524 average F-measure
of the 21 submissions for this slot.
In phase B systems had to fill the slot 3, with senti-
ment polarity to 845 opinions on restaurants domain,
949 opinions on laptops domain, and 339 opinions
on hotels domain.
On Table 3 we find our system’s result accuracy, in
the two trained domains plus the untrained hotels do-
</bodyText>
<table confidence="0.9998886">
Domain,Polarity Precision Recall F-measure
restaurants, positive 0,767 0,914 0,834
restaurants, negative 0,825 0,708 0,762
restaurants, neutral 0,714 0,111 0,192
laptops, positive 0,831 0,891 0,860
laptops, negative 0,766 0,787 0,777
laptops, neutral 0,387 0,152 0,218
hotels, positive 0,887 0,840 0,863
hotels, negative 0,608 0,738 0,667
hotels, neutral 0,143 0,083 0,105
</table>
<tableCaption confidence="0.999907">
Table 4: sentiue’s SA evaluation per domain.
</tableCaption>
<bodyText confidence="0.999092833333333">
main. In this slot we got the most satisfactory re-
sult, with the best accuracy in restaurants and lap-
tops, and an above average score, in the hotels do-
main. The detailed evaluation is shown in Table 4,
with values for precision, recall and f-measure, per
domain and polarity class.
</bodyText>
<sectionHeader confidence="0.996564" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999848857142857">
By participating in this SemEval edition, we sought
to develop our previous work, in order to achieve
SA results focused on the opinion targets.
Our results were poor for OTE detection, but we
think it will be easy to correct the implementation
problems for that part. As example, while checking
if a sentence contained a known target, from the
catalog, the system did not require whole words to
be matched, and this led to some misidentification
of word substrings as target.
Our result was more satisfactory for slot 1, with a
F-measure slightly above average between the 15
evaluated submissions for restaurants domain, and
4.5% better than submissions average for laptops
domain. The distribution of opinions for each aspect
category is not uniform. For example, for attribute
label classification, we already know that QUALITY
and GENERAL have much more instances than other
labels. This analysis inspired our approach in the
second stage, explained in section 3.1. To improve
this part, we think to introduce a cascade classifier.
</bodyText>
<page confidence="0.982213">
770
</page>
<bodyText confidence="0.999987916666667">
After the classification obtained in the current first
stage, other machine learning classifier will decide
how to pair entity+attribute, based on the wording
of the sentence. Another future work idea is to use
more corpora for training the aspect classifiers, as
other systems (Kiritchenko et al., 2014) have tried.
In phase B sentiue achieved good results. This,
perhaps, is justified by our previous experience in
overall SA. Many of the polarity classifier features
are inherited from our former system. SemEval
challenge is always a motivation to test our system
and an opportunity to learn from other participants.
</bodyText>
<sectionHeader confidence="0.99856" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99857125">
We would like to thank to LabInterop project, for
providing the infrastructure for the developed sys-
tem. LabInterop is funded by Programa Opera-
cional Regional do Alentejo (INALENTEJO).
</bodyText>
<sectionHeader confidence="0.998122" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9998746625">
Enrique Amig´o, Jorge Carrillo-de-Albornoz, Irina
Chugur, Adolfo Corujo, Julio Gonzalo, Edgar Meij,
Maarten de Rijke, Damiano Spina. 2014. Overview
of RepLab 2014: Author Profiling and Reputation Di-
mensions for Online Reputation Management. In In-
formation Access Evaluation. Multilinguality, Multi-
modality, and Interaction. Lecture Notes in Computer
Science, Volume 8685, 2014, pp 307-322.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating Non-local Information
into Information Extraction Systems by Gibbs Sam-
pling. Proceedings of the 43nd Annual Meeting of
the Association for Computational Linguistics (ACL
2005), pp. 363-370.
Svetlana Kiritchenko, Xiaodan Zhu, Colin Cherry, and
Saif M. Mohammad. 2014. NRC-Canada-2014: De-
tecting Aspects and Sentiment in Customer Reviews.
Proceedings of the 8th International Workshop on
Semantic Evaluation (SemEval 2014). p. 437–442.
Dublin, Ireland, August 2014.
Bing Liu, Minqing Hu and Junsheng Cheng. 2005. Opin-
ion Observer: Analyzing and Comparing Opinions
on the Web. In Proceedings of the 14th International
World Wide Web conference (WWW-2005). Chiba,
Japan.
Manning, Christopher D., Surdeanu, Mihai, Bauer, John,
Finkel, Jenny, Bethard, Steven J., and McClosky,
David. 2014. The Stanford CoreNLP Natural Lan-
guage Processing Toolkit. In Proceedings of 52nd
Annual Meeting of the Association for Computational
Linguistics: System Demonstrations, pp. 55-60.
Andrew Kachites McCallum. 2002. MALLET: A Ma-
chine Learning for Language Toolkit.
Finn ˚Arup Nielsen. 2011. A New ANEW: Evaluation of
a Word List for Sentiment Analysis in Microblogs.
In Proceedings, 1st Workshop on Making Sense of
Microposts (#MSM2011): Big things come in small
packages. pp: 93-98. Greece.
Maria Pontiki, D. Galanis, J. Pavlopoulos, H. Papageor-
giou, I. Androutsopoulos, S. Manandhar. SemEval-
2014 Task 4: Aspect Based Sentiment Analysis. Pro-
ceedings of the 8th SemEval, 2014. Dublin, Ireland.
Maria Pontiki, Dimitrios Galanis, Haris Papageogiou,
Suresh Manandhar, and Ion Androutsopoulos. 2015.
SemEval-2015 Task 12: Aspect Based Sentiment
Analysis. In Proceedings of the 9th International
Workshop on Semantic Evaluation (SemEval 2015),
Denver, Colorado.
Sara Rosenthal, Alan Ritter, Veselin Stoyanov, and
Preslav Nakov. 2014. SemEval-2014 Task 9: Senti-
ment Analysis in Twitter. In Proceedings of the Eighth
International Workshop on Semantic Evaluation (Se-
mEval’14). August 23-24, 2014, Dublin, Ireland.
Jos´e Saias and Hil´ario Fernandes. 2013. Senti.ue-en:
An approach for informally written short texts in
SemEval-2013 Sentiment Analysis task. In Second
Joint Conference on Lexical and Computational Se-
mantics (*SEM), Volume 2: Proceedings of the Sev-
enth International Workshop on Semantic Evaluation
(SemEval 2013), pages 508–512, Atlanta, Georgia,
USA.
Jos´e Saias. Senti.ue: Tweet overall sentiment classifica-
tion approach for SemEval-2014 task 9. In Proceed-
ings of the 8th International Workshop on Semantic
Evaluation (SemEval 2014), pages 546–550, Dublin,
Ireland, August 2014. ISBN 978-1-941643-24-2.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-Rich Part-of-Speech
Tagging with a Cyclic Dependency Network. In Pro-
ceedings of HLT-NAACL 2003, pp. 252-259.
Joachim Wagner, Piyush Arora, Santiago Cortes, Utsab
Barman, Dasha Bogdanova, Jennifer Foster and Lamia
Tounsi. 2014. DCU: Aspect-based Polarity Classifica-
tion for SemEval Task 4. Proceedings of the 8th Inter-
national Workshop on Semantic Evaluation (SemEval
2014). p. 223–229. Dublin, Ireland, August 2014.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions
in language. Language Resources and Evaluation,
39:165–210.
</reference>
<page confidence="0.997948">
771
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.503433">
<title confidence="0.9490345">Sentiue: Target and Aspect based Sentiment in SemEval-2015 Task 12</title>
<author confidence="0.790179">Jos´e Saias DI_- ECT</author>
<address confidence="0.994204">Ramalho, 59 7000-671 ´Evora, Portugal</address>
<email confidence="0.994641">jsaias@uevora.pt</email>
<abstract confidence="0.99885124">This paper describes our participation in SemEval-2015 Task 12, and the opinion minsystem The general idea is that systems must determine the polarity of the sentiment expressed about a certain aspect of a target entity. For slot 1, entity and attribute category detection, our system applies a supervised machine learning classifier, for each label, followed by a selection based on the probability of the entity/attribute pair, on that domain. The target expression detection, for slot 2, is achieved by using a catalog of known targets for each entity type, complemented with named entity recognition. In the opinion sentiment slot, we used a 3 class polarity classifier, having BoW, lemmas, bigrams after verbs, presence of polarized terms, and punctuation based features. Working in unconstrained mode, our results for slot 1 were assessed with precision between 57% and 63%, and recall varying between 42% and 47%. sentiment polarity, result accuracy was approximately 79%, reaching the best score in 2 of the 3 domains.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Enrique Amig´o</author>
<author>Jorge Carrillo-de-Albornoz</author>
<author>Irina Chugur</author>
<author>Adolfo Corujo</author>
<author>Julio Gonzalo</author>
<author>Edgar Meij</author>
<author>Maarten de Rijke</author>
<author>Damiano Spina</author>
</authors>
<title>Overview of RepLab 2014: Author Profiling and Reputation Dimensions for Online Reputation Management.</title>
<date>2014</date>
<booktitle>In Information Access Evaluation. Multilinguality, Multimodality, and Interaction. Lecture Notes in Computer Science,</booktitle>
<volume>8685</volume>
<pages>307--322</pages>
<marker>Amig´o, Carrillo-de-Albornoz, Chugur, Corujo, Gonzalo, Meij, de Rijke, Spina, 2014</marker>
<rawString>Enrique Amig´o, Jorge Carrillo-de-Albornoz, Irina Chugur, Adolfo Corujo, Julio Gonzalo, Edgar Meij, Maarten de Rijke, Damiano Spina. 2014. Overview of RepLab 2014: Author Profiling and Reputation Dimensions for Online Reputation Management. In Information Access Evaluation. Multilinguality, Multimodality, and Interaction. Lecture Notes in Computer Science, Volume 8685, 2014, pp 307-322.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling.</title>
<date>2005</date>
<booktitle>Proceedings of the 43nd Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>363--370</pages>
<contexts>
<context position="9183" citStr="Finkel et al., 2005" startWordPosition="1479" endWordPosition="1482"> category. For each opinion on restaurants domain, the system needed to identify the entity mention on the sentence text, referred to as the opinion target expression (OTE). We collected the opinion targets for each entity type, from the training data, forming a catalog. If any of the targets already known (e.g. restaurant name, or meal) appears in the sentence text, next to a verb or adjective, it is chosen as the OTE. If this does not lead to any OTE candidate, our system applies named entity recognition, looking for references to organization and location entities, using Stanford NER tool (Finkel et al., 2005; Manning et al., 2014). Having found one OTE, through the catalog or by NER, its text and position are marked in slot 2. If no mention is found, OTE slot is filled with the NULL value. 3.3 Sentiment Polarity Phase B was held in a subsequent period, and the input given to the systems is a little different, having the correct annotations on the aspect category, in restaurants, laptops and hotels domains. For each opinion, the participating systems must assign a sentiment polarity (positive, negative or neutral), considering the opinion aspect. For training, there were 1654 opinions on restauran</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling. Proceedings of the 43nd Annual Meeting of the Association for Computational Linguistics (ACL 2005), pp. 363-370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Svetlana Kiritchenko</author>
<author>Xiaodan Zhu</author>
<author>Colin Cherry</author>
<author>Saif M Mohammad</author>
</authors>
<title>NRC-Canada-2014: Detecting Aspects and Sentiment in Customer Reviews.</title>
<date>2014</date>
<booktitle>Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>437--442</pages>
<location>Dublin, Ireland,</location>
<contexts>
<context position="4249" citStr="Kiritchenko et al., 2014" startWordPosition="656" endWordPosition="659"> Related Work Many SA related publications, originating both in industry and in academia, have appeared, and it is notorious the growing interest by companies. Popular scientific forums and events include activities and workshops on this area, such as RepLab (Amig´o et al., 2014) at CLEF2, for online reputation, or ABSA and Twitter SA tasks in SemEval. In last year’s edition of this SemEval task (Pontiki et al., 2014), there were 26 systems participating in the polarity subtask. The two systems with better polarity classification accuracy were from NRC-Canada and DCU teams. NRC-Canada system (Kiritchenko et al., 2014) was trained with the data provided in the task, and complemented with lexicons generated from other corpora of customer reviews, to help feature extraction in machine learning. Stanford CoreNLP was used to tokenize, POS tagging, and dependency parse trees. They address polarity classification with a linear SVM classifier, with features for: the target, and its surrounding words; POS based features; dependency tree based features; unigrams and bigrams; lexicon based features. The DCU system (Wagner et al., 2014) also uses SVM for aspect and for polarity classification, combining bag-of-n-gram </context>
</contexts>
<marker>Kiritchenko, Zhu, Cherry, Mohammad, 2014</marker>
<rawString>Svetlana Kiritchenko, Xiaodan Zhu, Colin Cherry, and Saif M. Mohammad. 2014. NRC-Canada-2014: Detecting Aspects and Sentiment in Customer Reviews. Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014). p. 437–442. Dublin, Ireland, August 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
<author>Minqing Hu</author>
<author>Junsheng Cheng</author>
</authors>
<title>Opinion Observer: Analyzing and Comparing Opinions on the Web.</title>
<date>2005</date>
<booktitle>In Proceedings of the 14th International World Wide Web conference (WWW-2005).</booktitle>
<location>Chiba, Japan.</location>
<contexts>
<context position="11820" citStr="Liu et al., 2005" startWordPosition="1914" endWordPosition="1917">ized terms (positive or negative), according to each sentiment lexicon; • whether there are polarized terms before exclamation mark and question mark; • bigram before, and after, any polarized term; • polarity inversion, by negation detection before some polarized term; • presence of polarized terms in the last 5 tokens; • a feature for the domain, and two features for the entity type and the attribute label. To see whether a term is polarized, each token text is verified in each sentiment lexicon. These polarity support resources are AFINN lexicon (Nielsen, 2011), Bing Liu’s opinion lexicon (Liu et al., 2005) and MPQA subjectivity clues (Wiebe et al., 2005). 769 Domain Precision Recall F-measure restaurants 0,633 0,472 0,541 laptops 0,577 0,441 0,500 Table 1: sentiue’s evaluation on aspect category. Table 3: sentiue accuracy on sentiment polarity. Domain Accuracy restaurants 0,787 laptops 0,793 hotels 0,788 Domain Precision Recall F-measure restaurants 0,488 0,336 0,398 Table 2: sentiue’s evaluation on target detection. After some experimentation, we decided to use a single full train, joining the instances of restaurants and laptops as a whole training set. The resulting model was used to classif</context>
</contexts>
<marker>Liu, Hu, Cheng, 2005</marker>
<rawString>Bing Liu, Minqing Hu and Junsheng Cheng. 2005. Opinion Observer: Analyzing and Comparing Opinions on the Web. In Proceedings of the 14th International World Wide Web conference (WWW-2005). Chiba, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Mihai Surdeanu</author>
<author>John Bauer</author>
<author>Jenny Finkel</author>
<author>Steven J Bethard</author>
<author>David McClosky</author>
</authors>
<title>The Stanford CoreNLP Natural Language Processing Toolkit.</title>
<date>2014</date>
<booktitle>In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,</booktitle>
<pages>55--60</pages>
<contexts>
<context position="7163" citStr="Manning et al., 2014" startWordPosition="1127" endWordPosition="1130">t. For the purpose of this stage, it was necessary to prepare the training data for each binary classifier, that would determine whether a sentence contains an opinion on its tag (entity type or attribute label). The train process was the same for all tags, entity type or attribute label, of each domain. We created a dataset where each instance is a sentence text, and its class is tag, if the sentence had at least one opinion with that tag, or no tag otherwise. Text preprocessing includes tokenization, POS tagging and lemmatization, all performed with Stanford CoreNLP (Toutanova et al., 2003; Manning et al., 2014) tool. The classifier algorithm was Maximum Entropy3, and the classifier model features were text words and lemmas. Second stage starts with each sentence annotated with a set and tags, some for entity type and some for attribute label. When a sentence has no annotations, the system assumes that there is no opinion. In case of 1 tag on entity type and 1 tag in the attribute label, then it is the trivial case where the junction of the two results in the aspect annotation. For sentences with 1 tag on entity type and 0 tags for the attribute, our system searches for the most frequent aspect annot</context>
<context position="9206" citStr="Manning et al., 2014" startWordPosition="1483" endWordPosition="1486">pinion on restaurants domain, the system needed to identify the entity mention on the sentence text, referred to as the opinion target expression (OTE). We collected the opinion targets for each entity type, from the training data, forming a catalog. If any of the targets already known (e.g. restaurant name, or meal) appears in the sentence text, next to a verb or adjective, it is chosen as the OTE. If this does not lead to any OTE candidate, our system applies named entity recognition, looking for references to organization and location entities, using Stanford NER tool (Finkel et al., 2005; Manning et al., 2014). Having found one OTE, through the catalog or by NER, its text and position are marked in slot 2. If no mention is found, OTE slot is filled with the NULL value. 3.3 Sentiment Polarity Phase B was held in a subsequent period, and the input given to the systems is a little different, having the correct annotations on the aspect category, in restaurants, laptops and hotels domains. For each opinion, the participating systems must assign a sentiment polarity (positive, negative or neutral), considering the opinion aspect. For training, there were 1654 opinions on restaurants domain, and 1974 mor</context>
</contexts>
<marker>Manning, Surdeanu, Bauer, Finkel, Bethard, McClosky, 2014</marker>
<rawString>Manning, Christopher D., Surdeanu, Mihai, Bauer, John, Finkel, Jenny, Bethard, Steven J., and McClosky, David. 2014. The Stanford CoreNLP Natural Language Processing Toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pp. 55-60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kachites McCallum</author>
</authors>
<title>MALLET: A Machine Learning for Language Toolkit.</title>
<date>2002</date>
<contexts>
<context position="6473" citStr="McCallum, 2002" startWordPosition="1007" endWordPosition="1008"> (e.g. price, quality). Our approach comprises two stages. The first processes each review sentence assigning to it zero, one, or more entity types and attribute labels. The second stage chooses and combines identified entities and attributes, forming the aspect annotation. Analyzing the training data, we found that in the same sentence, there may be opinions on various types of entity (e.g. CPU, battery) or attributes. Thus, we have chosen to train a classifier for each entity type, and a classifier for each attribute label. We set a supervised machine learning text classifier, using MALLET (McCallum, 2002), a Javabased tool for NLP, with machine learning applications to text. For the purpose of this stage, it was necessary to prepare the training data for each binary classifier, that would determine whether a sentence contains an opinion on its tag (entity type or attribute label). The train process was the same for all tags, entity type or attribute label, of each domain. We created a dataset where each instance is a sentence text, and its class is tag, if the sentence had at least one opinion with that tag, or no tag otherwise. Text preprocessing includes tokenization, POS tagging and lemmati</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew Kachites McCallum. 2002. MALLET: A Machine Learning for Language Toolkit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Finn ˚Arup Nielsen</author>
</authors>
<title>A New ANEW: Evaluation of a Word List for Sentiment Analysis in Microblogs.</title>
<date>2011</date>
<booktitle>In Proceedings, 1st Workshop on Making Sense of Microposts (#MSM2011): Big things come in small packages.</booktitle>
<pages>93--98</pages>
<contexts>
<context position="11773" citStr="Nielsen, 2011" startWordPosition="1908" endWordPosition="1909">clamation/question mark; • presence of polarized terms (positive or negative), according to each sentiment lexicon; • whether there are polarized terms before exclamation mark and question mark; • bigram before, and after, any polarized term; • polarity inversion, by negation detection before some polarized term; • presence of polarized terms in the last 5 tokens; • a feature for the domain, and two features for the entity type and the attribute label. To see whether a term is polarized, each token text is verified in each sentiment lexicon. These polarity support resources are AFINN lexicon (Nielsen, 2011), Bing Liu’s opinion lexicon (Liu et al., 2005) and MPQA subjectivity clues (Wiebe et al., 2005). 769 Domain Precision Recall F-measure restaurants 0,633 0,472 0,541 laptops 0,577 0,441 0,500 Table 1: sentiue’s evaluation on aspect category. Table 3: sentiue accuracy on sentiment polarity. Domain Accuracy restaurants 0,787 laptops 0,793 hotels 0,788 Domain Precision Recall F-measure restaurants 0,488 0,336 0,398 Table 2: sentiue’s evaluation on target detection. After some experimentation, we decided to use a single full train, joining the instances of restaurants and laptops as a whole traini</context>
</contexts>
<marker>Nielsen, 2011</marker>
<rawString>Finn ˚Arup Nielsen. 2011. A New ANEW: Evaluation of a Word List for Sentiment Analysis in Microblogs. In Proceedings, 1st Workshop on Making Sense of Microposts (#MSM2011): Big things come in small packages. pp: 93-98. Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Pontiki</author>
<author>D Galanis</author>
<author>J Pavlopoulos</author>
<author>H Papageorgiou</author>
<author>I Androutsopoulos</author>
<author>S Manandhar</author>
</authors>
<title>SemEval2014 Task 4: Aspect Based Sentiment Analysis.</title>
<date>2014</date>
<booktitle>Proceedings of the 8th SemEval,</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="4045" citStr="Pontiki et al., 2014" startWordPosition="626" endWordPosition="629"> Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics punctuation based features, along with sentiment lexicon based features. The detailed procedure is explained in section 3. 2 Related Work Many SA related publications, originating both in industry and in academia, have appeared, and it is notorious the growing interest by companies. Popular scientific forums and events include activities and workshops on this area, such as RepLab (Amig´o et al., 2014) at CLEF2, for online reputation, or ABSA and Twitter SA tasks in SemEval. In last year’s edition of this SemEval task (Pontiki et al., 2014), there were 26 systems participating in the polarity subtask. The two systems with better polarity classification accuracy were from NRC-Canada and DCU teams. NRC-Canada system (Kiritchenko et al., 2014) was trained with the data provided in the task, and complemented with lexicons generated from other corpora of customer reviews, to help feature extraction in machine learning. Stanford CoreNLP was used to tokenize, POS tagging, and dependency parse trees. They address polarity classification with a linear SVM classifier, with features for: the target, and its surrounding words; POS based fea</context>
</contexts>
<marker>Pontiki, Galanis, Pavlopoulos, Papageorgiou, Androutsopoulos, Manandhar, 2014</marker>
<rawString>Maria Pontiki, D. Galanis, J. Pavlopoulos, H. Papageorgiou, I. Androutsopoulos, S. Manandhar. SemEval2014 Task 4: Aspect Based Sentiment Analysis. Proceedings of the 8th SemEval, 2014. Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Pontiki</author>
<author>Dimitrios Galanis</author>
<author>Haris Papageogiou</author>
<author>Suresh Manandhar</author>
<author>Ion Androutsopoulos</author>
</authors>
<title>SemEval-2015 Task 12: Aspect Based Sentiment Analysis.</title>
<date>2015</date>
<booktitle>In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015),</booktitle>
<location>Denver, Colorado.</location>
<contexts>
<context position="1768" citStr="Pontiki et al., 2015" startWordPosition="274" endWordPosition="277">arity, sentiue’s result accuracy was approximately 79%, reaching the best score in 2 of the 3 domains. 1 Introduction Social networks and other online platforms are an important communication mechanism in current lifestyle. These platforms aggregate user-generated content, such as opinions that people write and publish freely on the Web, and are now valued for market research and trend analysis. Natural language processing (NLP) helps to automatically extract information from these written opinions. This paper describes a participation in SemEval2015 Task 121, Aspect Based Sentiment Analysis (Pontiki et al., 2015), with the sentiue system, from Universidade de ´Evora. In previous editions of SemEval, we participated in Sentiment Analysis (SA) tasks, but in terms of overall polarity, over Twitter messages (Rosenthal et al., 2014), not being aspect oriented. The general idea for this challenge, is that, for a text, the system must determine the polarity of the sentiment expressed about a certain aspect of a particular target entity. Our sentiue system is an evolution from our previous work (Saias and Fernandes, 2013; Saias, 2014), for target oriented SA. Task 12 was run in two phases. In phase A systems </context>
</contexts>
<marker>Pontiki, Galanis, Papageogiou, Manandhar, Androutsopoulos, 2015</marker>
<rawString>Maria Pontiki, Dimitrios Galanis, Haris Papageogiou, Suresh Manandhar, and Ion Androutsopoulos. 2015. SemEval-2015 Task 12: Aspect Based Sentiment Analysis. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), Denver, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Rosenthal</author>
<author>Alan Ritter</author>
<author>Veselin Stoyanov</author>
<author>Preslav Nakov</author>
</authors>
<title>SemEval-2014 Task 9: Sentiment Analysis in Twitter.</title>
<date>2014</date>
<booktitle>In Proceedings of the Eighth International Workshop on Semantic Evaluation (SemEval’14).</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="1987" citStr="Rosenthal et al., 2014" startWordPosition="307" endWordPosition="310">style. These platforms aggregate user-generated content, such as opinions that people write and publish freely on the Web, and are now valued for market research and trend analysis. Natural language processing (NLP) helps to automatically extract information from these written opinions. This paper describes a participation in SemEval2015 Task 121, Aspect Based Sentiment Analysis (Pontiki et al., 2015), with the sentiue system, from Universidade de ´Evora. In previous editions of SemEval, we participated in Sentiment Analysis (SA) tasks, but in terms of overall polarity, over Twitter messages (Rosenthal et al., 2014), not being aspect oriented. The general idea for this challenge, is that, for a text, the system must determine the polarity of the sentiment expressed about a certain aspect of a particular target entity. Our sentiue system is an evolution from our previous work (Saias and Fernandes, 2013; Saias, 2014), for target oriented SA. Task 12 was run in two phases. In phase A systems are tested for aspect detection with one slot to aspect category, and a second slot for the opinion target expression on the text. Test data includes review texts for two domains: restaurants and laptops. In phase B, as</context>
</contexts>
<marker>Rosenthal, Ritter, Stoyanov, Nakov, 2014</marker>
<rawString>Sara Rosenthal, Alan Ritter, Veselin Stoyanov, and Preslav Nakov. 2014. SemEval-2014 Task 9: Sentiment Analysis in Twitter. In Proceedings of the Eighth International Workshop on Semantic Evaluation (SemEval’14). August 23-24, 2014, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jos´e Saias</author>
<author>Hil´ario Fernandes</author>
</authors>
<title>Senti.ue-en: An approach for informally written short texts in SemEval-2013 Sentiment Analysis task.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>508--512</pages>
<location>Atlanta, Georgia, USA.</location>
<contexts>
<context position="2278" citStr="Saias and Fernandes, 2013" startWordPosition="355" endWordPosition="358">his paper describes a participation in SemEval2015 Task 121, Aspect Based Sentiment Analysis (Pontiki et al., 2015), with the sentiue system, from Universidade de ´Evora. In previous editions of SemEval, we participated in Sentiment Analysis (SA) tasks, but in terms of overall polarity, over Twitter messages (Rosenthal et al., 2014), not being aspect oriented. The general idea for this challenge, is that, for a text, the system must determine the polarity of the sentiment expressed about a certain aspect of a particular target entity. Our sentiue system is an evolution from our previous work (Saias and Fernandes, 2013; Saias, 2014), for target oriented SA. Task 12 was run in two phases. In phase A systems are tested for aspect detection with one slot to aspect category, and a second slot for the opinion target expression on the text. Test data includes review texts for two domains: restaurants and laptops. In phase B, aspect category is provided, and systems must assign a polarity (positive, negative, or neutral) for each opinion. In this phase, systems received also texts from a third domain, hotels, for which no sentiment training data was given. We used a supervised machine learning classifier combined </context>
</contexts>
<marker>Saias, Fernandes, 2013</marker>
<rawString>Jos´e Saias and Hil´ario Fernandes. 2013. Senti.ue-en: An approach for informally written short texts in SemEval-2013 Sentiment Analysis task. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 508–512, Atlanta, Georgia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jos´e Saias</author>
</authors>
<title>Senti.ue: Tweet overall sentiment classification approach for SemEval-2014 task 9.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>546--550</pages>
<location>Dublin, Ireland,</location>
<contexts>
<context position="2292" citStr="Saias, 2014" startWordPosition="359" endWordPosition="360">cipation in SemEval2015 Task 121, Aspect Based Sentiment Analysis (Pontiki et al., 2015), with the sentiue system, from Universidade de ´Evora. In previous editions of SemEval, we participated in Sentiment Analysis (SA) tasks, but in terms of overall polarity, over Twitter messages (Rosenthal et al., 2014), not being aspect oriented. The general idea for this challenge, is that, for a text, the system must determine the polarity of the sentiment expressed about a certain aspect of a particular target entity. Our sentiue system is an evolution from our previous work (Saias and Fernandes, 2013; Saias, 2014), for target oriented SA. Task 12 was run in two phases. In phase A systems are tested for aspect detection with one slot to aspect category, and a second slot for the opinion target expression on the text. Test data includes review texts for two domains: restaurants and laptops. In phase B, aspect category is provided, and systems must assign a polarity (positive, negative, or neutral) for each opinion. In this phase, systems received also texts from a third domain, hotels, for which no sentiment training data was given. We used a supervised machine learning classifier combined with a probabi</context>
</contexts>
<marker>Saias, 2014</marker>
<rawString>Jos´e Saias. Senti.ue: Tweet overall sentiment classification approach for SemEval-2014 task 9. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 546–550, Dublin, Ireland, August 2014. ISBN 978-1-941643-24-2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL</booktitle>
<pages>252--259</pages>
<contexts>
<context position="7140" citStr="Toutanova et al., 2003" startWordPosition="1123" endWordPosition="1126">ning applications to text. For the purpose of this stage, it was necessary to prepare the training data for each binary classifier, that would determine whether a sentence contains an opinion on its tag (entity type or attribute label). The train process was the same for all tags, entity type or attribute label, of each domain. We created a dataset where each instance is a sentence text, and its class is tag, if the sentence had at least one opinion with that tag, or no tag otherwise. Text preprocessing includes tokenization, POS tagging and lemmatization, all performed with Stanford CoreNLP (Toutanova et al., 2003; Manning et al., 2014) tool. The classifier algorithm was Maximum Entropy3, and the classifier model features were text words and lemmas. Second stage starts with each sentence annotated with a set and tags, some for entity type and some for attribute label. When a sentence has no annotations, the system assumes that there is no opinion. In case of 1 tag on entity type and 1 tag in the attribute label, then it is the trivial case where the junction of the two results in the aspect annotation. For sentences with 1 tag on entity type and 0 tags for the attribute, our system searches for the mos</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher Manning, and Yoram Singer. 2003. Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network. In Proceedings of HLT-NAACL 2003, pp. 252-259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joachim Wagner</author>
<author>Piyush Arora</author>
<author>Santiago Cortes</author>
<author>Utsab Barman</author>
<author>Dasha Bogdanova</author>
<author>Jennifer Foster</author>
<author>Lamia Tounsi</author>
</authors>
<title>DCU: Aspect-based Polarity Classification for SemEval Task 4.</title>
<date>2014</date>
<booktitle>Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>223--229</pages>
<location>Dublin, Ireland,</location>
<contexts>
<context position="4766" citStr="Wagner et al., 2014" startWordPosition="733" endWordPosition="736">lassification accuracy were from NRC-Canada and DCU teams. NRC-Canada system (Kiritchenko et al., 2014) was trained with the data provided in the task, and complemented with lexicons generated from other corpora of customer reviews, to help feature extraction in machine learning. Stanford CoreNLP was used to tokenize, POS tagging, and dependency parse trees. They address polarity classification with a linear SVM classifier, with features for: the target, and its surrounding words; POS based features; dependency tree based features; unigrams and bigrams; lexicon based features. The DCU system (Wagner et al., 2014) also uses SVM for aspect and for polarity classification, combining bag-of-n-gram features with rule-based features. N-grams (with size from 1 to 5) in a window around the aspect term, are used as features, as well as features derived from a sentiment lexicon. The rule-based approach to predict the polarity of an aspect term, generated features considering all words score and their distance to the aspect term. 3 Method Our participation involved the adaptation of our previous real-time system, for text overall sentiment classification, into a target oriented SA system. The next subsections ex</context>
</contexts>
<marker>Wagner, Arora, Cortes, Barman, Bogdanova, Foster, Tounsi, 2014</marker>
<rawString>Joachim Wagner, Piyush Arora, Santiago Cortes, Utsab Barman, Dasha Bogdanova, Jennifer Foster and Lamia Tounsi. 2014. DCU: Aspect-based Polarity Classification for SemEval Task 4. Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014). p. 223–229. Dublin, Ireland, August 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Claire Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language. Language Resources and Evaluation,</title>
<date>2005</date>
<pages>39--165</pages>
<contexts>
<context position="11869" citStr="Wiebe et al., 2005" startWordPosition="1922" endWordPosition="1925"> each sentiment lexicon; • whether there are polarized terms before exclamation mark and question mark; • bigram before, and after, any polarized term; • polarity inversion, by negation detection before some polarized term; • presence of polarized terms in the last 5 tokens; • a feature for the domain, and two features for the entity type and the attribute label. To see whether a term is polarized, each token text is verified in each sentiment lexicon. These polarity support resources are AFINN lexicon (Nielsen, 2011), Bing Liu’s opinion lexicon (Liu et al., 2005) and MPQA subjectivity clues (Wiebe et al., 2005). 769 Domain Precision Recall F-measure restaurants 0,633 0,472 0,541 laptops 0,577 0,441 0,500 Table 1: sentiue’s evaluation on aspect category. Table 3: sentiue accuracy on sentiment polarity. Domain Accuracy restaurants 0,787 laptops 0,793 hotels 0,788 Domain Precision Recall F-measure restaurants 0,488 0,336 0,398 Table 2: sentiue’s evaluation on target detection. After some experimentation, we decided to use a single full train, joining the instances of restaurants and laptops as a whole training set. The resulting model was used to classify the opinion polarity for the three domains. Bec</context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emotions in language. Language Resources and Evaluation, 39:165–210.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>