<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000344">
<title confidence="0.997849">
A Data Mining Approach to Learn Reorder Rules for SMT
</title>
<author confidence="0.919812">
Avinesh PVS
</author>
<affiliation confidence="0.9568625">
IIIT Hyderabad
Language Technologies Research Centre
</affiliation>
<email confidence="0.989446">
avinesh@research.iiit.ac.in
</email>
<sectionHeader confidence="0.998556" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999794882352941">
In this paper, we describe a syntax based
source side reordering method for phrase-
based statistical machine translation (SMT)
systems. The source side training corpus is
first parsed, then reordering rules are auto-
matically learnt from source-side phrases and
word alignments. Later the source side train-
ing and test corpus are reordered and given
to the SMT system. Reordering is a common
problem observed in language pairs of distant
language origins. This paper describes an au-
tomated approach for learning reorder rules
from a word-aligned parallel corpus using as-
sociation rule mining. Reordered and gener-
alized rules are the most significant in our ap-
proach. Our experiments were conducted on
an English-Hindi EILMT corpus.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.993294288888889">
In recent years SMT systems (Brown et al.,
1990), (Yamada and Knight, 2001), (Chiang,
2005), (Charniak et al., 2003) have been in focus. It
is easy to develop a MT system for a new pair of lan-
guages using an existing SMT system and a parallel
corpora. It isn’t a surprise to see SMT being attrac-
tive in terms of less human labour as compared to
traditional rule-based systems. However to achieve
good scores SMT requires large amounts of sentence
aligned parallel text. Such resources are available
only for few languages, whereas for many languages
the online resources are low. So we propose an ap-
proach for a pair of resource rich and resource poor
languages.
52
Some of the previous approaches include (Collins
et al., 2005), (Xia and McCord, 2004). Former
describes an approach for reordering the source
sentence in German-English MT system. Their
approach involves six transformations on the parsed
source sentence. Later propose an approach which
automatically extracts rewrite patterns by parsing
the source and target sides of the training corpus
for French-English pair. These rewritten patterns
are applied to the source sentence so that the source
and target word orders are similar. (Costa-juss`a
and Fonollosa, 2006) consider Part-Of-Speech
(POS) based source reordering as a translation
task. These approaches modify the source language
word order before decoding in order to produce a
word order similar to the target language. Later
the reordered sentence is given as an input to the
standard phrase-based decoder to be translated
without the reordering condition.
We propose an approach along the same lines
those described above. Here we follow a data
mining approach to learn the reordering/rewrite
rules applied on an English-Hindi MT system. The
rest of the paper is organized as follows. In Section
2 we briefly describe our approach. In Section 3 we
present a rule learning framework using Association
Rule Mining (Agrawal et al., 1993). Section 4
consists of experimental setup and sample rules
learnt. We present some discussion in Section 5 and
finally detail proposed future work in Section 6.
</bodyText>
<footnote confidence="0.378018333333333">
Proceedings of the NAACL HLT 2010 Student Research Workshop, pages 52–57,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
* Reodered Nodes
</footnote>
<figureCaption confidence="0.999272">
Figure 1: English-Hindi Example
</figureCaption>
<figure confidence="0.999089023809524">
most
2 things 3
been 14
RBS NNS
never 12
risk−free10
S
PP
ADVP
,
.
VP
NP
,
. 15
4
,
, 6
IN NP
RB
11
Like
1
ADJP
ADVP
VBZ
VP
VBN
however 5
NP NP
is 8 not
has 13
9
RB
JJ
NN
VBZ
business 7 RB
VP CC VP
and
Alignments: Source Position : Target Position
1:6 2:3 3:4 4:7 5:1 6:2 7:8 8:18 9:17 10:13 11:16 12:10 13:14 14:14 15:19
</figure>
<sectionHeader confidence="0.980792" genericHeader="introduction">
2 Approach
</sectionHeader>
<bodyText confidence="0.9999905">
Our approach is inspired by Association rule min-
ing, a popular concept in data mining for discovering
interesting relations between items in large transac-
tion records. For example, the rule {milk, bread} ==&gt;.
{butter} found in the customer database would indi-
cate if a customer buys milk and bread together, he
or she is also likely to buy butter. Similar notions
can be projected to the learning of reorder rules. For
example, {NNP, VB, NNP} ==&gt;. {1,3,2} would indi-
cate if NNP,VB and NNP occur together in source
text, then its ordering on the target side would be
{1,3,2}. The original problem of association rule
mining doesn’t consider the order of items in the
rule, whereas in our problem order is important as
well.
In this approach we start with extracting the most
frequent patterns from the English language model.
The English language model consists of both POS
and chunk tag n-gram model built using SRILM
toolkit 1. Then to learn the reordering rules for these
patterns we used a word-aligned English-Hindi par-
allel corpus, where the alignments are generated us-
ing GIZA++ (Och and Ney, 2003). These align-
ments are used to learn the rewrite rules by calculat-
ing the target positions of the source nodes. Fig 1
shows an English phrase structure tree (PS) 2 and its
</bodyText>
<footnote confidence="0.997951">
1http://www-speech.sri.com/projects/srilm/
2Stanford Parser: http://nlp.stanford.edu/software/lex-
</footnote>
<bodyText confidence="0.916992">
alignments corresponding to the target sentence.
</bodyText>
<subsectionHeader confidence="0.998748">
2.1 Calculation of target position:
</subsectionHeader>
<bodyText confidence="0.975569777777778">
Target position of a node is equal to the target
position of the head among the children (Aho and
Ullman, 1972). For example the head node of a NP
is the right most NN, NNP, NNS (or) NNX. Rules
developed by Collins are used to calculate the head
node (Collins, 2003).
Psn(T,Node)=Psn(T,Head(Node))
In Fig 1, Position of VP in target side is 18.
Psn(T,VP)=Psn(T,Head(VP))=Psn(T,VBZ)=18
</bodyText>
<sectionHeader confidence="0.964859" genericHeader="method">
3 Association rule mining
</sectionHeader>
<bodyText confidence="0.979761714285714">
We modified the original definition by Rakesh Agar-
wal to suit our needs (Agrawal et al., 1993; Srikant
and Agrawal, 1995) . The problem here is defined
as: Let E=P:{e1,e2,e3,...en } be a sequence of N
children of a node P. Let A={a1,a2,a3,...an } be the
alignment set of the corresponding set E.
Let D=P:{ S1,S2,S3,...Sm } be set consisting of all
possible ordered sequence of children of the node P,
Ex: S1=S:{NP,VP,NP}, where S is the parent node
and NP, VP and NP are its children. Each set in D
has a unique ID, which represents the occurrence of
the source order of the children. A rule is defined
as an implication of the form X==&gt;.Y where XCE and
parser.shtml
</bodyText>
<page confidence="0.994499">
53
</page>
<bodyText confidence="0.998332482758621">
YCTarget Positions(E,A). The sets of items X and
Y are called LHS and RHS of the rule. To illus-
trate the concepts, we use a simple example from
the English-Hindi parallel corpus.
Consider the set of items I={Set of POS
tags} U {Set of Chunk tags}. For Example,
I={NN,VBZ,NNS,NP,VP} and an example rule
could be {NN,VBZ,NNS} =�- {1,3,2}, which means
that when NN, VBZ and NNS occur in a continuous
pattern they are reordered to 1,3 and 2 positions
respectively on the target side. The above example
is a naive example. If we consider the training
corpus with the alignments we could use constraints
on various measures of significance. We use the
best-known constraints, namely minimum threshold
support and confidence. The support supp(X) of an
itemset X is defined as the proportion of sentences
which contain the itemset. The confidence of a rule
is defined as
conf(X==&gt;-Y)=supp(XUY)/supp(X).
Association rules require language specific mini-
mum support and minimum confidence at the same
time. To achieve this, association rule learning is
done in two steps. Firstly, minimum support is ap-
plied to find all frequent itemsets in the source lan-
guage model. In the second step, these frequent
itemsets and the minimum confidence constraints
are used to generate rules from the word-aligned par-
allel corpus.
</bodyText>
<subsectionHeader confidence="0.997279">
3.1 Frequent Pattern mining
</subsectionHeader>
<bodyText confidence="0.999617076923077">
For the first task of collecting the most frequent
itemsets we used Fpgrowth algorithm 3 (Borgelt,
2005) implemented by Christian Borgelt. We used
a POS and a chunk tag English language model. In
a given parse tree the pattern model based on the or-
der of pre-terminals is called POS language model
and the pattern model based on the Non-terminals is
called the Chunk language model. The below algo-
rithm is run on every Non-terminal and pre-terminal
node of a parse tree. In the modified version of min-
ing frequent itemsets we also include generalization
of the frequent sets, similar to the work done by
(Chiang, 2005).
</bodyText>
<footnote confidence="0.699465">
3http://www.borgelt.net/fpgrowth.html
</footnote>
<bodyText confidence="0.7959634">
Steps for extracting frequent LHSs: Consider
X1,X2,X3,X4,...Xx are all possible children of a
node S. The transaction here is the sequence of chil-
dren of the node S. The sample example is shown in
Fig 2.
</bodyText>
<listItem confidence="0.972430375">
1. Collect all occurrences of the children of a node
and their frequencies from the transactions and
name the set L1.
2. Calculate L2=L1 * L1 which is the frequency
set of two elements.
3. Similarly calculate L,,,, till n = maximum pos-
sible children of parent S.
4. Once the maximum possible set is calculated,
</listItem>
<bodyText confidence="0.882516666666667">
K-best frequent sets are collected and then el-
ements which occur above a threshold(0) are
combined to form a single element.
Ex, most common patterns occurring as a chil-
dren of NP are {JJ,NN,NN},{JJ,NN} etc.
5. The threshold was calculated based on various
experiments, and then set to 0=20% less than
the frequency of least frequent itemset between
the elements of the two L’s.
For example,
L3={JJ,NN}*{NN}={JJ,NN,NNP}.
If freq{JJ,NN}=10, and freq{NNP}=20 and
{JJ,NN,NNP}=9, 0=10-(20% of 10)=8.
So {JJ,NN} =�- X1.
This way the generalized rules are learnt
for all the tables (L,,,, L,,,_1..L3). Using these
generalized rules, the initial transactions are
modified.
</bodyText>
<listItem confidence="0.992209666666667">
6. Recalculate L1,L2,..L,,, based on the rules learnt
above. Continue the process until no new rules
are extracted at the end of the iteration.
</listItem>
<subsectionHeader confidence="0.997314">
3.2 Generate rules
</subsectionHeader>
<bodyText confidence="0.999978666666667">
The second problem is to generate association rules
for these large itemsets with the constraints of min-
imal confidence. Suppose one of the large itemsets
of a parent node S is Lk, Lk = P:{e1,e2,,ek }, as-
sociation rules with these itemsets are generated in
the following way: Firstly a set P:{ e1,e2,..ek } is
</bodyText>
<page confidence="0.971276">
54
</page>
<figure confidence="0.6428646">
S
X1 X2 X3 X4 X5
Transactions
(patterns in the)
corpus
</figure>
<equation confidence="0.873989846153846">
X1 X2 X3
X1 X2 X3
X1 X2 X4
X0 X3 X5
Update Val
L 1 1* 1
L 2 =L L
X1 15 X3 X4 10 X1 X2 X3 9 X1 X2 K−best
X2 12 X2 X3 9 = Frequent
X12 Itemsets
X3 20 X3 X5 5 Threshold
X4 25 X1 X2 10
X5 10 X0 X3 3
</equation>
<figureCaption confidence="0.995976">
Figure 2: N-stage Generalization
</figureCaption>
<bodyText confidence="0.999432">
matched with the source sequences of parent P and
then their corresponding alignment information is
used to generate the target sequence. The numbers
on the rhs represent the position of the elements in
the target sentence. Then by checking the constraint
confidence this rule can be determined as interesting
or not. Constraint confidence used here is the prob-
ability of occurrence of the non-monotone rule.
If c1,c2,c3,c4...cx are the children of a Node X.
LHS is the original order of the children. RHS
is the sorted order of the children on the basis of
Psn(T,Psn(S,cz)), where 1GiGx.
From Fig 1, let us consider the top node and find
the rule based on the head based method.
Suppose that given from the above frequency
rule
</bodyText>
<equation confidence="0.951942727272727">
Lk = S:�’PP’ ’,’ ’ADVP’ ’,’ ’NP’ ’VP’}
Children(S) = ’PP’ ’,’ ’ADVP’ ’,’ ’NP’ ’VP’ ’.’
The target positions are calculated as shown in
Table 1: Target Positions of Children(S)
Psn(T,’PP’) = Psn(T,1) =6
Psn(T,’,’) = Psn(T,4) =7
Psn(T,’ADVP’) = Psn(T,5) =1
Psn(T,’,’) = Psn(T,6) =2
Psn(T,’NP’) =Psn(T,7) =8
Psn(T,’VP’) =Psn(T,8) =18
Psn(T,’.’) = Psn(T,15) =19
</equation>
<bodyText confidence="0.9872845">
the Table 1. RHS is calculated based on the target
positions.
</bodyText>
<equation confidence="0.818497">
LHS = PP , ADVP , NP VP .
RHS = 3 4 1 2 5 6 7
</equation>
<subsectionHeader confidence="0.746882">
3.2.1 Use of Generalization:
</subsectionHeader>
<bodyText confidence="0.999929833333333">
The above rule generated is the most commonly
occurring phenomenon in English to Hindi machine
translation. It is observed that adverbial phrase
generally occurs at the beginning of the sentence
on the Hindi side. The rule generated above will
be captured less frequently because the exact
pattern in LHS is rarely matched. Using the above
generalization in frequent itemset mining we can
merge all the most frequent occurring patterns into
a common pattern.
The above example pattern is modified to the below
using the generalization technique.
</bodyText>
<subsectionHeader confidence="0.5101445">
Rule: X1 ADVP , X2 � 2 3 1 4
3.2.2 Rules and their Application
</subsectionHeader>
<bodyText confidence="0.999903615384615">
These generated rules are taken to calculate the
probability of the non-monotone rules with respect
to monotone rules. If the probability of the non-
monotone rule was &gt;0.5 then the rule was appended
to the final list. The final list included all the gener-
alized and non-generalized rules of different parent
nodes.
The final list of rules is applied on both training
and test corpus based on the longest possible se-
quence match. If the rule matches, then the source
structures are reordered as per the rule. Specific
rules are given more priority over the generalized
rules.
</bodyText>
<page confidence="0.998731">
55
</page>
<sectionHeader confidence="0.998275" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9647335">
Table 2, Table 3 show some of the high frequency
and generalized rules. The total number of rules
learnt were 727 for a 11k training corpus. Number
of generalizations learnt were 54.
</bodyText>
<tableCaption confidence="0.953599">
Table 2: Most Frequent Rules
</tableCaption>
<table confidence="0.9991">
Rule LHS RHS
1 IN NP 2 1
2 NP VP NP 1 3 2
3 NP PP 2 1
4 VBG PP 2 1
5 VBZ ADVP NP 2 3 1
</table>
<tableCaption confidence="0.971096">
Table 3: Generalized Rules
</tableCaption>
<table confidence="0.973887166666667">
Rule LHS RHS
1 X1 ADVP , X2 2 3 1 4
2 X3 VBZIIVBG X4 1 3 2
3 ADVP X5 . 2 1 3
4 MD RB X6 3 1 2
5 VB X7 NP-TMP 2 3 1
</table>
<bodyText confidence="0.991406">
Once the training and test sentences are reordered
using the above rules, they are fed to the Moses sys-
tem. It is clear that without reordering the perfor-
mace of the system is worst. Training and test data
consisted of 11,300 and 500 sentences respectively.
</bodyText>
<tableCaption confidence="0.998944">
Table 4: Evaluation on Moses
</tableCaption>
<table confidence="0.83606575">
Config Blue Score NIST
Moses Without Reorder 0.2123 5.5315
Moses + Our Reorder 0.2329 5.6605
Moses With Reorder 0.2475 5.7069
</table>
<sectionHeader confidence="0.999382" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999974733333333">
Our method showed a drop in terms of blue score
as compared to Moses reordering; this is proba-
bly due to the reordering based on lexicalized rules
in Moses. The above generalization works effec-
tively in case of the Stanford parser as it stitches
the nodes at top level. English-Hindi tourism corpus
distributed as a part of ICON 2008 shared task. Our
learning based on phrase structure doesn’t handle
the movement of children across nodes. Whereas,
dependency structure based rule learning would help
in handling more constructs in terms of word-level
reordering patterns. Some of the least frequent pat-
terns are actually interesting patterns in terms of re-
ordering. Learning these kinds of patterns would be
a challenging task.
</bodyText>
<sectionHeader confidence="0.999802" genericHeader="discussions">
6 Future Work
</sectionHeader>
<bodyText confidence="0.999943666666667">
Work has to be done in terms of prioritization of the
rules, for example first priority should be given to
more specific rules (the one with constraints) then to
the general rules. More constraints with respect to
morphological features would also help in improv-
ing the diversity of the rules. We will also look
into the linguistic clause based reordering features
which would help in reordering of distant pair of lan-
guages. Manual evaluation of the output will throw
some light on the effectiveness of this system. To
further evaluate the approach we would also try the
approach on someother distant language pairs.
</bodyText>
<sectionHeader confidence="0.998933" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99966248">
Rakesh Agrawal, Tomasz Imieli´nski, and Arun Swami.
1993. Mining association rules between sets of items
in large databases. In SIGMOD ’93: Proceedings of
the 1993 ACM SIGMOD international conference on
Management of data, pages 207–216, New York, NY,
USA. ACM.
Alfred V. Aho and Jeffrey D. Ullman. 1972. The The-
ory of Parsing, Translation and Compiling, volume 1.
Prentice-Hall, Englewood Cliffs, NJ.
Christian Borgelt. 2005. An implementation of the fp-
growth algorithm. In OSDM ’05: Proceedings of the
1st international workshop on open source data min-
ing, pages 1–5, New York, NY, USA. ACM.
Peter F. Brown, John Cocke, Stephen A. Della Pietra,
Vincent J. Della Pietra, Fredrick Jelinek, John D. Laf-
ferty, Robert L. Mercer, and Paul S. Roossin. 1990. A
statistical approach to machine translation. COMPU-
TATIONAL LINGUISTICS, 16(2):79–85.
Eugene Charniak, Kevin Knight, and Kenji Yamada.
2003. Syntax-based language models for statistical
machine translation. In MT Summit IX. Intl. Assoc.
for Machine Translation.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In In ACL, pages
263–270.
</reference>
<page confidence="0.964781">
56
</page>
<reference confidence="0.999187064516129">
Michael Collins, Philipp Koehn, and Ivona Kuˇcerov´a.
2005. Clause restructuring for statistical machine
translation. In ACL ’05: Proceedings of the 43rd An-
nual Meeting on Association for Computational Lin-
guistics, pages 531–540, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Technical report.
Marta R. Costa-juss`a and Jos´e A. R. Fonollosa. 2006.
Statistical machine reordering. In EMNLP ’06: Pro-
ceedings of the 2006 Conference on Empirical Meth-
ods in Natural Language Processing, pages 70–76,
Morristown, NJ, USA. Association for Computational
Linguistics.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19–51.
Ramakrishnan Srikant and Rakesh Agrawal. 1995. Min-
ing generalized association rules. In Research Report
RJ 9963, IBM Almaden Research.
Fei Xia and Michael McCord. 2004. Improving a sta-
tistical mt system with automatically learned rewrite
patterns. In COLING ’04: Proceedings of the 20th
international conference on Computational Linguis-
tics, page 508, Morristown, NJ, USA. Association for
Computational Linguistics.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In ACL ’01: Proceedings
of the 39th Annual Meeting on Association for Compu-
tational Linguistics, pages 523–530, Morristown, NJ,
USA. Association for Computational Linguistics.
</reference>
<page confidence="0.999143">
57
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.418376">
<title confidence="0.999083">A Data Mining Approach to Learn Reorder Rules for SMT</title>
<author confidence="0.613602">Avinesh</author>
<affiliation confidence="0.693171">IIIT Language Technologies Research</affiliation>
<email confidence="0.693168">avinesh@research.iiit.ac.in</email>
<abstract confidence="0.998600777777778">In this paper, we describe a syntax based source side reordering method for phrasebased statistical machine translation (SMT) systems. The source side training corpus is first parsed, then reordering rules are automatically learnt from source-side phrases and word alignments. Later the source side training and test corpus are reordered and given to the SMT system. Reordering is a common problem observed in language pairs of distant language origins. This paper describes an automated approach for learning reorder rules from a word-aligned parallel corpus using association rule mining. Reordered and generalized rules are the most significant in our approach. Our experiments were conducted on an English-Hindi EILMT corpus.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rakesh Agrawal</author>
<author>Tomasz Imieli´nski</author>
<author>Arun Swami</author>
</authors>
<title>Mining association rules between sets of items in large databases.</title>
<date>1993</date>
<booktitle>In SIGMOD ’93: Proceedings of the 1993 ACM SIGMOD international conference on Management of data,</booktitle>
<pages>207--216</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>Agrawal, Imieli´nski, Swami, 1993</marker>
<rawString>Rakesh Agrawal, Tomasz Imieli´nski, and Arun Swami. 1993. Mining association rules between sets of items in large databases. In SIGMOD ’93: Proceedings of the 1993 ACM SIGMOD international conference on Management of data, pages 207–216, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alfred V Aho</author>
<author>Jeffrey D Ullman</author>
</authors>
<title>The Theory of Parsing,</title>
<date>1972</date>
<journal>Translation and Compiling,</journal>
<volume>1</volume>
<publisher>Prentice-Hall,</publisher>
<location>Englewood Cliffs, NJ.</location>
<contexts>
<context position="5133" citStr="Aho and Ullman, 1972" startWordPosition="837" endWordPosition="840">eordering rules for these patterns we used a word-aligned English-Hindi parallel corpus, where the alignments are generated using GIZA++ (Och and Ney, 2003). These alignments are used to learn the rewrite rules by calculating the target positions of the source nodes. Fig 1 shows an English phrase structure tree (PS) 2 and its 1http://www-speech.sri.com/projects/srilm/ 2Stanford Parser: http://nlp.stanford.edu/software/lexalignments corresponding to the target sentence. 2.1 Calculation of target position: Target position of a node is equal to the target position of the head among the children (Aho and Ullman, 1972). For example the head node of a NP is the right most NN, NNP, NNS (or) NNX. Rules developed by Collins are used to calculate the head node (Collins, 2003). Psn(T,Node)=Psn(T,Head(Node)) In Fig 1, Position of VP in target side is 18. Psn(T,VP)=Psn(T,Head(VP))=Psn(T,VBZ)=18 3 Association rule mining We modified the original definition by Rakesh Agarwal to suit our needs (Agrawal et al., 1993; Srikant and Agrawal, 1995) . The problem here is defined as: Let E=P:{e1,e2,e3,...en } be a sequence of N children of a node P. Let A={a1,a2,a3,...an } be the alignment set of the corresponding set E. Let </context>
</contexts>
<marker>Aho, Ullman, 1972</marker>
<rawString>Alfred V. Aho and Jeffrey D. Ullman. 1972. The Theory of Parsing, Translation and Compiling, volume 1. Prentice-Hall, Englewood Cliffs, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Borgelt</author>
</authors>
<title>An implementation of the fpgrowth algorithm.</title>
<date>2005</date>
<booktitle>In OSDM ’05: Proceedings of the 1st international workshop on open source data mining,</booktitle>
<pages>1--5</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="7541" citStr="Borgelt, 2005" startWordPosition="1242" endWordPosition="1243">The confidence of a rule is defined as conf(X==&gt;-Y)=supp(XUY)/supp(X). Association rules require language specific minimum support and minimum confidence at the same time. To achieve this, association rule learning is done in two steps. Firstly, minimum support is applied to find all frequent itemsets in the source language model. In the second step, these frequent itemsets and the minimum confidence constraints are used to generate rules from the word-aligned parallel corpus. 3.1 Frequent Pattern mining For the first task of collecting the most frequent itemsets we used Fpgrowth algorithm 3 (Borgelt, 2005) implemented by Christian Borgelt. We used a POS and a chunk tag English language model. In a given parse tree the pattern model based on the order of pre-terminals is called POS language model and the pattern model based on the Non-terminals is called the Chunk language model. The below algorithm is run on every Non-terminal and pre-terminal node of a parse tree. In the modified version of mining frequent itemsets we also include generalization of the frequent sets, similar to the work done by (Chiang, 2005). 3http://www.borgelt.net/fpgrowth.html Steps for extracting frequent LHSs: Consider X</context>
</contexts>
<marker>Borgelt, 2005</marker>
<rawString>Christian Borgelt. 2005. An implementation of the fpgrowth algorithm. In OSDM ’05: Proceedings of the 1st international workshop on open source data mining, pages 1–5, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>John Cocke</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Fredrick Jelinek</author>
<author>John D Lafferty</author>
<author>Robert L Mercer</author>
<author>Paul S Roossin</author>
</authors>
<title>A statistical approach to machine translation.</title>
<date>1990</date>
<journal>COMPUTATIONAL LINGUISTICS,</journal>
<volume>16</volume>
<issue>2</issue>
<contexts>
<context position="949" citStr="Brown et al., 1990" startWordPosition="141" endWordPosition="144">t parsed, then reordering rules are automatically learnt from source-side phrases and word alignments. Later the source side training and test corpus are reordered and given to the SMT system. Reordering is a common problem observed in language pairs of distant language origins. This paper describes an automated approach for learning reorder rules from a word-aligned parallel corpus using association rule mining. Reordered and generalized rules are the most significant in our approach. Our experiments were conducted on an English-Hindi EILMT corpus. 1 Introduction In recent years SMT systems (Brown et al., 1990), (Yamada and Knight, 2001), (Chiang, 2005), (Charniak et al., 2003) have been in focus. It is easy to develop a MT system for a new pair of languages using an existing SMT system and a parallel corpora. It isn’t a surprise to see SMT being attractive in terms of less human labour as compared to traditional rule-based systems. However to achieve good scores SMT requires large amounts of sentence aligned parallel text. Such resources are available only for few languages, whereas for many languages the online resources are low. So we propose an approach for a pair of resource rich and resource p</context>
</contexts>
<marker>Brown, Cocke, Pietra, Pietra, Jelinek, Lafferty, Mercer, Roossin, 1990</marker>
<rawString>Peter F. Brown, John Cocke, Stephen A. Della Pietra, Vincent J. Della Pietra, Fredrick Jelinek, John D. Lafferty, Robert L. Mercer, and Paul S. Roossin. 1990. A statistical approach to machine translation. COMPUTATIONAL LINGUISTICS, 16(2):79–85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Kevin Knight</author>
<author>Kenji Yamada</author>
</authors>
<title>Syntax-based language models for statistical machine translation.</title>
<date>2003</date>
<booktitle>In MT Summit IX. Intl. Assoc. for Machine Translation.</booktitle>
<contexts>
<context position="1017" citStr="Charniak et al., 2003" startWordPosition="151" endWordPosition="154">rce-side phrases and word alignments. Later the source side training and test corpus are reordered and given to the SMT system. Reordering is a common problem observed in language pairs of distant language origins. This paper describes an automated approach for learning reorder rules from a word-aligned parallel corpus using association rule mining. Reordered and generalized rules are the most significant in our approach. Our experiments were conducted on an English-Hindi EILMT corpus. 1 Introduction In recent years SMT systems (Brown et al., 1990), (Yamada and Knight, 2001), (Chiang, 2005), (Charniak et al., 2003) have been in focus. It is easy to develop a MT system for a new pair of languages using an existing SMT system and a parallel corpora. It isn’t a surprise to see SMT being attractive in terms of less human labour as compared to traditional rule-based systems. However to achieve good scores SMT requires large amounts of sentence aligned parallel text. Such resources are available only for few languages, whereas for many languages the online resources are low. So we propose an approach for a pair of resource rich and resource poor languages. 52 Some of the previous approaches include (Collins e</context>
</contexts>
<marker>Charniak, Knight, Yamada, 2003</marker>
<rawString>Eugene Charniak, Kevin Knight, and Kenji Yamada. 2003. Syntax-based language models for statistical machine translation. In MT Summit IX. Intl. Assoc. for Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation. In</title>
<date>2005</date>
<booktitle>In ACL,</booktitle>
<pages>263--270</pages>
<contexts>
<context position="992" citStr="Chiang, 2005" startWordPosition="149" endWordPosition="150"> learnt from source-side phrases and word alignments. Later the source side training and test corpus are reordered and given to the SMT system. Reordering is a common problem observed in language pairs of distant language origins. This paper describes an automated approach for learning reorder rules from a word-aligned parallel corpus using association rule mining. Reordered and generalized rules are the most significant in our approach. Our experiments were conducted on an English-Hindi EILMT corpus. 1 Introduction In recent years SMT systems (Brown et al., 1990), (Yamada and Knight, 2001), (Chiang, 2005), (Charniak et al., 2003) have been in focus. It is easy to develop a MT system for a new pair of languages using an existing SMT system and a parallel corpora. It isn’t a surprise to see SMT being attractive in terms of less human labour as compared to traditional rule-based systems. However to achieve good scores SMT requires large amounts of sentence aligned parallel text. Such resources are available only for few languages, whereas for many languages the online resources are low. So we propose an approach for a pair of resource rich and resource poor languages. 52 Some of the previous appr</context>
<context position="8055" citStr="Chiang, 2005" startWordPosition="1332" endWordPosition="1333">he first task of collecting the most frequent itemsets we used Fpgrowth algorithm 3 (Borgelt, 2005) implemented by Christian Borgelt. We used a POS and a chunk tag English language model. In a given parse tree the pattern model based on the order of pre-terminals is called POS language model and the pattern model based on the Non-terminals is called the Chunk language model. The below algorithm is run on every Non-terminal and pre-terminal node of a parse tree. In the modified version of mining frequent itemsets we also include generalization of the frequent sets, similar to the work done by (Chiang, 2005). 3http://www.borgelt.net/fpgrowth.html Steps for extracting frequent LHSs: Consider X1,X2,X3,X4,...Xx are all possible children of a node S. The transaction here is the sequence of children of the node S. The sample example is shown in Fig 2. 1. Collect all occurrences of the children of a node and their frequencies from the transactions and name the set L1. 2. Calculate L2=L1 * L1 which is the frequency set of two elements. 3. Similarly calculate L,,,, till n = maximum possible children of parent S. 4. Once the maximum possible set is calculated, K-best frequent sets are collected and then e</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In In ACL, pages 263–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
<author>Ivona Kuˇcerov´a</author>
</authors>
<title>Clause restructuring for statistical machine translation.</title>
<date>2005</date>
<booktitle>In ACL ’05: Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>531--540</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<marker>Collins, Koehn, Kuˇcerov´a, 2005</marker>
<rawString>Michael Collins, Philipp Koehn, and Ivona Kuˇcerov´a. 2005. Clause restructuring for statistical machine translation. In ACL ’05: Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 531–540, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>2003</date>
<tech>Technical report.</tech>
<contexts>
<context position="5288" citStr="Collins, 2003" startWordPosition="869" endWordPosition="870">e alignments are used to learn the rewrite rules by calculating the target positions of the source nodes. Fig 1 shows an English phrase structure tree (PS) 2 and its 1http://www-speech.sri.com/projects/srilm/ 2Stanford Parser: http://nlp.stanford.edu/software/lexalignments corresponding to the target sentence. 2.1 Calculation of target position: Target position of a node is equal to the target position of the head among the children (Aho and Ullman, 1972). For example the head node of a NP is the right most NN, NNP, NNS (or) NNX. Rules developed by Collins are used to calculate the head node (Collins, 2003). Psn(T,Node)=Psn(T,Head(Node)) In Fig 1, Position of VP in target side is 18. Psn(T,VP)=Psn(T,Head(VP))=Psn(T,VBZ)=18 3 Association rule mining We modified the original definition by Rakesh Agarwal to suit our needs (Agrawal et al., 1993; Srikant and Agrawal, 1995) . The problem here is defined as: Let E=P:{e1,e2,e3,...en } be a sequence of N children of a node P. Let A={a1,a2,a3,...an } be the alignment set of the corresponding set E. Let D=P:{ S1,S2,S3,...Sm } be set consisting of all possible ordered sequence of children of the node P, Ex: S1=S:{NP,VP,NP}, where S is the parent node and NP</context>
</contexts>
<marker>Collins, 2003</marker>
<rawString>Michael Collins. 2003. Head-driven statistical models for natural language parsing. Technical report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta R Costa-juss`a</author>
<author>Jos´e A R Fonollosa</author>
</authors>
<title>Statistical machine reordering.</title>
<date>2006</date>
<booktitle>In EMNLP ’06: Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>70--76</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<marker>Costa-juss`a, Fonollosa, 2006</marker>
<rawString>Marta R. Costa-juss`a and Jos´e A. R. Fonollosa. 2006. Statistical machine reordering. In EMNLP ’06: Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 70–76, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="4668" citStr="Och and Ney, 2003" startWordPosition="769" endWordPosition="772">NP occur together in source text, then its ordering on the target side would be {1,3,2}. The original problem of association rule mining doesn’t consider the order of items in the rule, whereas in our problem order is important as well. In this approach we start with extracting the most frequent patterns from the English language model. The English language model consists of both POS and chunk tag n-gram model built using SRILM toolkit 1. Then to learn the reordering rules for these patterns we used a word-aligned English-Hindi parallel corpus, where the alignments are generated using GIZA++ (Och and Ney, 2003). These alignments are used to learn the rewrite rules by calculating the target positions of the source nodes. Fig 1 shows an English phrase structure tree (PS) 2 and its 1http://www-speech.sri.com/projects/srilm/ 2Stanford Parser: http://nlp.stanford.edu/software/lexalignments corresponding to the target sentence. 2.1 Calculation of target position: Target position of a node is equal to the target position of the head among the children (Aho and Ullman, 1972). For example the head node of a NP is the right most NN, NNP, NNS (or) NNX. Rules developed by Collins are used to calculate the head </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ramakrishnan Srikant</author>
<author>Rakesh Agrawal</author>
</authors>
<title>Mining generalized association rules.</title>
<date>1995</date>
<booktitle>In Research Report RJ 9963, IBM Almaden Research.</booktitle>
<contexts>
<context position="5554" citStr="Srikant and Agrawal, 1995" startWordPosition="905" endWordPosition="908">tware/lexalignments corresponding to the target sentence. 2.1 Calculation of target position: Target position of a node is equal to the target position of the head among the children (Aho and Ullman, 1972). For example the head node of a NP is the right most NN, NNP, NNS (or) NNX. Rules developed by Collins are used to calculate the head node (Collins, 2003). Psn(T,Node)=Psn(T,Head(Node)) In Fig 1, Position of VP in target side is 18. Psn(T,VP)=Psn(T,Head(VP))=Psn(T,VBZ)=18 3 Association rule mining We modified the original definition by Rakesh Agarwal to suit our needs (Agrawal et al., 1993; Srikant and Agrawal, 1995) . The problem here is defined as: Let E=P:{e1,e2,e3,...en } be a sequence of N children of a node P. Let A={a1,a2,a3,...an } be the alignment set of the corresponding set E. Let D=P:{ S1,S2,S3,...Sm } be set consisting of all possible ordered sequence of children of the node P, Ex: S1=S:{NP,VP,NP}, where S is the parent node and NP, VP and NP are its children. Each set in D has a unique ID, which represents the occurrence of the source order of the children. A rule is defined as an implication of the form X==&gt;.Y where XCE and parser.shtml 53 YCTarget Positions(E,A). The sets of items X and Y </context>
</contexts>
<marker>Srikant, Agrawal, 1995</marker>
<rawString>Ramakrishnan Srikant and Rakesh Agrawal. 1995. Mining generalized association rules. In Research Report RJ 9963, IBM Almaden Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
<author>Michael McCord</author>
</authors>
<title>Improving a statistical mt system with automatically learned rewrite patterns.</title>
<date>2004</date>
<booktitle>In COLING ’04: Proceedings of the 20th international conference on Computational Linguistics,</booktitle>
<pages>508</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1653" citStr="Xia and McCord, 2004" startWordPosition="263" endWordPosition="266">ocus. It is easy to develop a MT system for a new pair of languages using an existing SMT system and a parallel corpora. It isn’t a surprise to see SMT being attractive in terms of less human labour as compared to traditional rule-based systems. However to achieve good scores SMT requires large amounts of sentence aligned parallel text. Such resources are available only for few languages, whereas for many languages the online resources are low. So we propose an approach for a pair of resource rich and resource poor languages. 52 Some of the previous approaches include (Collins et al., 2005), (Xia and McCord, 2004). Former describes an approach for reordering the source sentence in German-English MT system. Their approach involves six transformations on the parsed source sentence. Later propose an approach which automatically extracts rewrite patterns by parsing the source and target sides of the training corpus for French-English pair. These rewritten patterns are applied to the source sentence so that the source and target word orders are similar. (Costa-juss`a and Fonollosa, 2006) consider Part-Of-Speech (POS) based source reordering as a translation task. These approaches modify the source language </context>
</contexts>
<marker>Xia, McCord, 2004</marker>
<rawString>Fei Xia and Michael McCord. 2004. Improving a statistical mt system with automatically learned rewrite patterns. In COLING ’04: Proceedings of the 20th international conference on Computational Linguistics, page 508, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A syntax-based statistical translation model.</title>
<date>2001</date>
<booktitle>In ACL ’01: Proceedings of the 39th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>523--530</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="976" citStr="Yamada and Knight, 2001" startWordPosition="145" endWordPosition="148">ing rules are automatically learnt from source-side phrases and word alignments. Later the source side training and test corpus are reordered and given to the SMT system. Reordering is a common problem observed in language pairs of distant language origins. This paper describes an automated approach for learning reorder rules from a word-aligned parallel corpus using association rule mining. Reordered and generalized rules are the most significant in our approach. Our experiments were conducted on an English-Hindi EILMT corpus. 1 Introduction In recent years SMT systems (Brown et al., 1990), (Yamada and Knight, 2001), (Chiang, 2005), (Charniak et al., 2003) have been in focus. It is easy to develop a MT system for a new pair of languages using an existing SMT system and a parallel corpora. It isn’t a surprise to see SMT being attractive in terms of less human labour as compared to traditional rule-based systems. However to achieve good scores SMT requires large amounts of sentence aligned parallel text. Such resources are available only for few languages, whereas for many languages the online resources are low. So we propose an approach for a pair of resource rich and resource poor languages. 52 Some of t</context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>Kenji Yamada and Kevin Knight. 2001. A syntax-based statistical translation model. In ACL ’01: Proceedings of the 39th Annual Meeting on Association for Computational Linguistics, pages 523–530, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>