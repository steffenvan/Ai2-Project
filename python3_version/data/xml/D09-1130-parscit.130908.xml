<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.645499">
Semi-supervised Speech Act Recognition in Emails and Forums
</title>
<author confidence="0.725018">
Minwoo Jeong†* Chin-Yew Lin$ Gary Geunbae Lee†
</author>
<affiliation confidence="0.994931">
†Pohang University of Science &amp; Technology, Pohang, Korea
$Microsoft Research Asia, Beijing, China
</affiliation>
<email confidence="0.990306">
†{stardust,gblee}@postech.ac.kr $cyl@microsoft.com
</email>
<sectionHeader confidence="0.994854" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99998">
In this paper, we present a semi-supervised
method for automatic speech act recogni-
tion in email and forums. The major chal-
lenge of this task is due to lack of labeled
data in these two genres. Our method
leverages labeled data in the Switchboard-
DAMSL and the Meeting Recorder Dia-
log Act database and applies simple do-
main adaptation techniques over a large
amount of unlabeled email and forum data
to address this problem. Our method uses
automatically extracted features such as
phrases and dependency trees, called sub-
tree features, for semi-supervised learn-
ing. Empirical results demonstrate that
our model is effective in email and forum
speech act recognition.
</bodyText>
<sectionHeader confidence="0.998112" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999797833333333">
Email and online forums are important social me-
dia. For example, thousands of emails and posts
are created daily in online communities, e.g.,
Usenet newsgroups or the TripAdvisor travel fo-
rum1, in which users interact with each other us-
ing emails/posts in complicated ways in discus-
sion threads. To uncover the rich interactions in
these email exchanges and forum discussions, we
propose to apply speech act recognition to email
and forum threads.
Despite extensive studies of speech act recogni-
tion in many areas, developing speech act recogni-
tion for online forms of conversation is very chal-
lenging. A major challenge is that emails and
forums usually have no labeled data for training
statistical speech act recognizers. Fortunately, la-
beled speech act data are available in other do-
mains (i.e., telephone and meeting conversations
</bodyText>
<note confidence="0.848227">
*This work was conducted during the author’s internship
at Microsoft Research Asia.
</note>
<footnote confidence="0.790673">
1http://tripadvisor.com/
</footnote>
<bodyText confidence="0.980730666666667">
in this paper) and large unlabeled data sets can be
collected from the Web. Thus, we focus on the
problem of how to accurately recognize speech
acts in emails and forums by making maximum
use of data from existing resources.
Recently, there are increasing interests in
speech act recognition of online text-based con-
versations. Analysis of speech acts for online
chat and instant messages and have been studied
in computer-mediated communication (CMC) and
distance learning (Twitchell et al., 2004; Nastri et
al., 2006; Ros´e et al., 2008). In natural language
processing, Cohen et al. (2004) and Feng et al.
(2006) used speech acts to capture the intentional
focus of emails and discussion boards. However,
they assume that enough labeled data are available
for developing speech act recognition models.
A main contribution of this paper is that we ad-
dress the problem of learning speech act recog-
nition in a semi-supervised way. To our knowl-
edge, this is the first use of semi-supervised speech
act recognition in emails and online forums. To
do this, we make use of labeled data from spo-
ken conversations (Jurafsky et al., 1997; Dhillon
et al., 2004). A second contribution is that our
model learns subtree features that constitute dis-
criminative patterns: for example, variable length
n-grams and partial dependency structures. There-
fore, our model can capture both local features
such as n-grams and non-local dependencies. In
this paper, we extend subtree pattern mining to the
semi-supervised learning problem.
This paper is structured as follows. Section 2
reviews prior work on speech act recognition and
Section 3 presents the problem statement and our
data sets. Section 4 describes a supervised method
of learning subtree features that shows the effec-
tiveness of subtree features on labeled data sets.
Section 5 proposes semi-supervised learning tech-
niques for speech act recognition and Section 6
demonstrates our method applied to email and on-
1250
</bodyText>
<note confidence="0.999121">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1250–1259,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.823688">
line forum thread data. Section 7 concludes this
paper with future work.
</bodyText>
<sectionHeader confidence="0.999656" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999799846153846">
Speech act theory is fundamental to many stud-
ies in discourse analysis and pragmatics (Austin,
1962; Searle, 1969). A speech act is an illo-
cutionary act of conversation and reflects shal-
low discourse structures of language. Recent re-
search on spoken dialog processing has investi-
gated computational speech act models of human-
human and human-computer conversations (Stol-
cke et al., 2000) and applications of these mod-
els to CMC and distance learning (Twitchell et al.,
2004; Nastri et al., 2006; Ros´e et al., 2008).
Our work in this paper is closely related to prior
work on email and forum speech act recognition.
Cohen et al. (2004) proposed the notion of ‘email
speech act’ for classifying the intent of an email
sender. They defined verb and noun categories
for email speech acts and used supervised learn-
ing to recognize them. Feng et al. (2006) pre-
sented a method of detecting conversation focus
based on the speech acts of messages in discus-
sion boards. Extending Feng et al. (2006)’s work,
Ravi and Kim (2007) applied speech act classifi-
cation to detect unanswered questions. However,
none of these studies have focused on the semi-
supervised speech act recognition problem and ex-
amined their methods across different genres.
The speech processing community frequently
employs two large-scale corpora for speech act
annotation: Switchboard-DAMSL (SWBD) and
Meeting Recorder Dialog Act (MRDA). SWBD is
an annotation scheme and collection of labeled di-
alog act2 data for telephone conversations (Juraf-
sky et al., 1997). The main purpose of SWBD is
to acquire stochastic discourse grammars for train-
ing better language models for automatic speech
recognition. More recently, an MRDA corpus has
been adapted from SWBD but its tag set for la-
beling meetings has been modified to better reflect
the types of interaction in multi-party face-to-face
meetings (Dhillon et al., 2004). These two corpora
have been extensively studied, e.g., (Stolcke et al.,
2000; Ang et al., 2005; Galley et al., 2004). We
also use these for our experiments.
2A dialog act is the meaning of an utterance at the level
of illocutionary force (Austin, 1962), and broadly covers the
speech act and adjacency pair (Stolcke et al., 2000). In this
paper, we use only the term ‘speech act’ for clarity.
This paper focuses on the problem of semi-
supervised speech act recognition. The goal of
semi-supervised learning techniques is to use aux-
iliary data to improve a model’s capability to rec-
ognize speech acts. The approach in Tur et al.
(2005) presented semi-supervised learning to em-
ploy auxiliary unlabeled data in call classification,
and is closely related to our work. However, our
approach uses the most discriminative subtree fea-
tures, which is particularly attractive for reducing
the model’s size. Our problem setting is closely re-
lated to the domain adaptation problem (Ando and
Zhang, 2005), i.e., we seek to obtain a model that
analyzes target domains (emails and forums) by
adapting a method that analyzes source domains
(SWBD and MRDA). Recently, this type of do-
main adaptation has become an important topic in
natural language processing.
</bodyText>
<sectionHeader confidence="0.980262" genericHeader="method">
3 Problem Definition
</sectionHeader>
<subsectionHeader confidence="0.999707">
3.1 Problem Statement
</subsectionHeader>
<bodyText confidence="0.999973633333333">
We define speech act recognition to be the task
that, given a sentence, maps it to one of the speech
act types. Figure 1 shows two examples of our
email and forum speech act recognition. E1-6 are
all sentences in an email message. F1-3, F4-5,
and F6 are three posts in a forum thread. A sen-
tence interacts alone or with others, for example,
F6 agrees with the previous post (F4-5). To gain
insight into our work, it is useful to consider that
E2, 3 and F1, 4, 6 are summaries of two dis-
courses. In particular, F1 denotes a question and
F4 and F6 are corresponding answers. More re-
cently, using speech acts has become an appealing
approach in summarizing the discussions (Galley
et al., 2004; McKeown et al., 2007).
Next, we define speech act category based on
MRDA. Dhillon et al. (2004) included definitions
of speech acts for colloquial style interactions
(e.g., backchannel, disruption, and floorgrabber),
but these are not applicable in emails and forums.
After removing these categories, we define 12 tags
(Table 1). Dhillon et al. (2004) provides detailed
descriptions of each tag. We note that our tag set
definition is different from (Cohen et al., 2004;
Feng et al., 2006; Ravi and Kim, 2007) for two
reasons. First, prior work primarily interested in
the domain-specific speech acts, but our work use
domain-independent speech act tags. Second, we
focus on speech act recognition on the sentence-
level.
</bodyText>
<page confidence="0.301276">
1251
</page>
<listItem confidence="0.881103916666667">
E1: I am planning my schedule at CHI 2003 (http://www.chi2003.org/) S
E2: - will there be anything happening at the conference related to this W3C User interest group? QY
E3: I do not see anything on the program yet, but I suspect we could at least have an informal SIG S
E4: - a chance to meet others and bring someone like me up to speed on what is happening. S
E5: There will be many competing activities, so the sooner we can set this up the more likely I can attend. S
E6: Keith S
F1: If given a choice, should I choose Huangpu area, or should I choose Pudong area? QR
F2: Both location are separated by a Huangpu river, not sure which area is more convenient for sight seeing? QW
F3: Thanks in advance for reply! P
F4: Stay on the Puxi side of the Huangpu river and visit the Pudong side by the incredible tourist tunnel. AC
F5: If you stay on the Pudong side add half an hour to visit the majority of the tourist attractions. S
F6: I definitely agree with previous post. AA
</listItem>
<figureCaption confidence="0.988847">
Figure 1: Examples of speech act recognition in emails and online forums. Tags are defined in Table 1.
</figureCaption>
<tableCaption confidence="0.9817595">
Table 1: Tags used to describe components of
speech acts
</tableCaption>
<table confidence="0.852464692307693">
Tag Description
A Accept response
AA Acknowledge and appreciate
AC Action motivator
P Polite mechanism
QH Rhetorical question
QO Open-ended question
QR Or/or-clause question
QW Wh-question
QY Yes-no question
R Reject response
S Statement
U Uncertain response
</table>
<bodyText confidence="0.999477357142857">
The goal of semi-supervised speech act recogni-
tion is to learn a classifier using both labeled and
unlabeled data. We formally define our problem
as follows. Let x = {xj} be a forest, i.e., a set of
trees that represents a natural language structure,
for example, a sequence of words and a depen-
dency parse tree. We will describe this in more
detail in Section 4. Let y be a speech act. Then,
we define DL = {xi7 yi}Z1 as the set of labeled
training data, and DU = {xi}&apos;L+1 as the set of
unlabeled training data where l = n + m and m is
the number of unlabeled data instances. Our goal
is to find a learning method to minimize the clas-
sification errors in DL and DU.
</bodyText>
<subsectionHeader confidence="0.999397">
3.2 Data Preparation
</subsectionHeader>
<bodyText confidence="0.999900666666667">
In this paper, we separate labeled (DL) and un-
labeled data (DU). First we use SWBD3 and
MRDA4 as our labeled data. We automatically
</bodyText>
<equation confidence="0.643898">
3LDC Catalog No. LDC97S62
4http://www.icsi.berkeley.edu/—ees/dadb/
</equation>
<bodyText confidence="0.999978457142857">
map original annotations in SWBD and MRDA to
one of the 12 speech acts.5 Inter-annotator agree-
ment K in both data sets is — 0.8 (Jurafsky et al.,
1997; Dhillon et al., 2004). For evaluation pur-
poses, we divide labeled data into three sets: train-
ing, development, and evaluation sets (Table 2).
Of the 1,155 available conversations in the SWBD
corpus, we use 855 for training, 100 for devel-
opment, and 200 for evaluation. Among the 75
available meetings in the MRDA corpus, we ex-
clude two meetings of different natures (btr001
and btr002). Of the remaining meetings, we use
59 for training, 6 for development, and 8 for eval-
uation. Then we merge multi-segments utterances
that belong to the same speaker and then divide all
data sets into sentences.
As stated earlier, our unlabeled data consists
of email (EMAIL) and online forum (FORUM)
data. For the EMAIL set, we selected 22,391
emails from Enron data6 (discussion threads,
all documents, and calendar folders). For the FO-
RUM set, we crawled 11,602 threads and 55,743
posts from the TripAdvisor travel forum site (Bei-
jing, Shanghai, and Hongkong forums). As our
evaluation sets, we used 40 email threads of the
BC3 corpus7 for EMAIL and 100 threads selected
from the same travel forum site for FORUM. Ev-
ery sentences was automatically segmented by the
MSRA sentence boundary detector (Table 2). An-
notation was performed by two human annotators,
and inter-annotator agreements were K = 0.79 for
EMAIL and K = 0.73 for FORUM.
Overall performance of automatic evaluation
measures usually depends on the distribution of
tags. In both labeled and unlabeled sets, the most
</bodyText>
<footnote confidence="0.99911">
5Our mapping tables are available at
http://home.postech.ac.kr/—stardust/acl09/.
6http://www.cs.cmu.edu/—enron/
7http://www.cs.ubc.ca/nest/lci/bc3.html
</footnote>
<page confidence="0.810742">
1252
</page>
<tableCaption confidence="0.994318">
Table 2: Number of sentences in labeled and unlabeled data
</tableCaption>
<table confidence="0.997429142857143">
Set SWBD MRDA
Set EMAIL FORUM
Training 96,553 50,865
Development 12,299 8,366
Evaluation 24,264 10,492
Unlabeled 122,125 297,017
Evaluation 2,267 3,711
</table>
<figureCaption confidence="0.997831">
Figure 2: Distribution of speech acts in the evaluation sets. Tags are defined in Table 1.
</figureCaption>
<bodyText confidence="0.999641666666667">
frequent tag is the statement (S) tag (Figure 2).
Distributions of tags are similar in training and de-
velopment sets of SWBD and MRDA.
</bodyText>
<sectionHeader confidence="0.969291" genericHeader="method">
4 Speech Act Recognition
</sectionHeader>
<bodyText confidence="0.999930888888889">
Previous work in speech act recognition used a
large set of lexical features, e.g., bag-of-words,
bigrams and trigrams (Stolcke et al., 2000; Co-
hen et al., 2004; Ang et al., 2005; Ravi and Kim,
2007). However, these methods create a large
number of lexical features that might not be nec-
essary for speech act identification. For example,
a Wh-question “What site should we use to book a
Beijing-Chonqing flight?” can be predicted by two
discriminative features, “(&lt;s&gt;, WRB) —* QW”
and “(?, &lt;/s&gt;) —* QW” where &lt;s&gt; and &lt;/s&gt;
are sentence start and end symbols, and WRB is
a part-of-speech tag that denotes a Wh-adverb.
In addition, useful features could be of various
lengths, i.e. not fixed length n-grams, and non-
adjacent. One key idea of this paper is a novel use
of subtree features to model these for speech act
recognition.
</bodyText>
<subsectionHeader confidence="0.993448">
4.1 Exploiting Subtree Features
</subsectionHeader>
<bodyText confidence="0.99744975">
To exploit subtree features in our model, we use
a subtree pattern mining method proposed by
Kudo and Matsumoto (2004). We briefly intro-
duce this algorithm here. In Section 3.1, we de-
fined x = {xj} as the forest that is a set of trees.
More precisely, xj is a labeled ordered tree where
each node has its own label and is ordered left-
to-right. Several types of labeled ordered trees
</bodyText>
<figureCaption confidence="0.95008825">
Figure 3: Representations of tree: (a) bag-of-
words, (b) n-gram, (c) word pair, and (d) depen-
dency tree. A node denotes a word and a directed
edge indicates a parent-and-child relationship.
</figureCaption>
<bodyText confidence="0.987900181818182">
are possible (Figure 3). Note that S-expression
can be used instead for computation, for example
(a(b(c(d)))) for the n-gram (Figure 3(b)).
Moreover, we employ a combination of multiple
trees as the input of the subtree pattern mining al-
gorithm.
We extract subtree features from the forest set
{xi}. A subtree t is a tree if t C x. For exam-
ple, (a), (a(b)), and (b(c(d))) are subtrees
of Figure 3(b). We define the subtree feature as a
weak learner:
</bodyText>
<equation confidence="0.983649">
�
+y t C x,
f(y, t, x) � (1)
−y otherwise,
</equation>
<bodyText confidence="0.972118133333333">
where we assume a binary case y E Y =
{+1, −1} for simplicity. Even though the ap-
proach in Kudo and Matsumoto (2004) and ours
are simiar, there are two clear distinctions. First,
our method employs multiple tree structures, and
uses different constraints to generate subtree can-
didates. In this paper, we only restrict generating
1253
the dependency subtrees which should have 3 or
more nodes. Second, our method is of interest
for semi-supervised learning problems. To learn
subtree features, Kudo and Matsumoto (2004) as-
sumed supervised data {(xi, yi)1. Here, we de-
scribe the supervised learning method and will de-
scribe our semi-supervised method in Section 5.
</bodyText>
<subsectionHeader confidence="0.994421">
4.2 Supervised Boosting Learning
</subsectionHeader>
<bodyText confidence="0.999971875">
Given training examples, we construct a ensem-
ble learner F(x) = Pk λkf(yk, tk, x), where λk
is a coefficient for linear combination. A final
classifier h(x) can be derived from the ensemble
learner, i.e., h(x) °= sgn (F(x)). As an optimiza-
tion framework (Mason et al., 2000), the objective
of boosting learning is to find F such that the cost
of functional
</bodyText>
<equation confidence="0.9959105">
C(F) = X αiC[yiF(xi)] (2)
iED
</equation>
<bodyText confidence="0.998627230769231">
is minimized for some non-negative and monoton-
ically decreasing cost function C : R -* R and
the weight αi E R+. In this paper, we use the
AdaBoost algorithm (Schapire and Singer, 1999);
thus the cost function is defined as C(z) = e−z.
Constructing an ensemble learner requires that
the user choose a base learner, f(y, t, x), to
maximize the inner product −(VC(F), f) (Ma-
son et al., 2000). Finding f(y, t, x) to maxi-
mize −(VC(F), f) is equivalent to searching for
f(y, t, x) to minimize 2 Pi:f(y,t,xi)�=yi wi − 1,
where wi for i E DL, is the empirical data dis-
tribution w(k)
</bodyText>
<equation confidence="0.970266666666667">
i at step k. It is defined as:
(k)
wi = αi e−yiF (xi). (3)
</equation>
<bodyText confidence="0.756732">
From Eq. 3, a proper base learner (i.e., subtree)
can be found by maximizing weighted gain, where
</bodyText>
<equation confidence="0.997577">
gain(t, y) = X yiwif(y, t, xi). (4)
iEDL
</equation>
<bodyText confidence="0.6020565">
Thus, subtree mining is formulated as the prob-
lem of finding (t, y) = arg max gain(t, y). We
</bodyText>
<equation confidence="0.909366">
(t,y)EX xY
</equation>
<bodyText confidence="0.991686333333333">
need to search with respect to a non-monotonic
score function (Eq. 4), thus we use the monotonic
bound, gain(t, y) &lt; µ(t), where
</bodyText>
<equation confidence="0.999303833333333">
µ(t) = max ⎝ 2 wi − yif(y, t, xi),
⎛
{i|yi=+1,tCxi} i=1
X X
n
⎞yif(y, t, xi) ⎠ . (5)
</equation>
<tableCaption confidence="0.805165333333333">
Table 3: Result of supervised learning experiment;
columns are micro-averaged F1 score with macro-
averaged F1 score in parentheses. MAXENT:
maximum entropy model; BOW: bag-of-words
model; NGRAM: n-gram model; +POSTAG,
+DEPTREE, +SPEAKER indicate that the com-
ponents were added individually onto NGRAM.
‘*’ indicates results significantly better than the
NGRAM model (p &lt; 0.001).
</tableCaption>
<table confidence="0.9931105">
Model SWBD MRDA
MAXENT 92.76 (63.54) 82.48 (57.19)
BOW 91.32 (54.47) 82.17 (55.42)
NGRAM 92.60 (58.43) 83.30 (57.53)
+POSTAG 92.69 (60.07) 83.60 (58.46)
+DEPTREE 92.67 (61.75) *83.57 (57.45)
+SPEAKER *92.86 (63.13) 83.40 (58.20)
ALL *92.87 (63.77) 83.49 (59.04)
</table>
<bodyText confidence="0.999556428571428">
The subtree set is efficiently enumerated using a
branch-and-bound procedure based on µ(t) (Kudo
and Matsumoto, 2004).
After finding an optimal base leaner, f(y, t, x),
we need to set the coefficient λk to form a new en-
semble, F(xi) — F(xi) + λkf(�t, y, xi). In Ad-
aBoost, we choose
</bodyText>
<equation confidence="0.999842">
1 1 + gain(t, y)1
λk = 2 log µ1 − gain(t, y) / (6)
</equation>
<bodyText confidence="0.998094">
After K iterations, the boosting algorithm returns
the ensemble learner F(x) which consists of a set
of appropriate base learners f(y, t, x).
</bodyText>
<subsectionHeader confidence="0.996133">
4.3 Evaluation on Labeled Data
</subsectionHeader>
<bodyText confidence="0.998621625">
We verified the effectiveness of using subtree fea-
tures on the SWBD and MRDA data sets. For
boosting learning, one typically assumes αi = 1.
In addition, the number of iterations, which relates
to the number of patterns, was determined by a
development set. We also used a one-vs.-all strat-
egy for the multi-class problem. Precision and re-
call were computed and combined into micro- and
macro-averaged F1 scores. The significance of our
results was evaluated using the McNemar paired
test (Gillick and Cox, 1989), which is based on in-
dividual labeling decisions to compare the correct-
ness of two models. All experiments were imple-
mented in C++ and executed in Windows XP on a
PC with a Dual 2.1 GHz Intel Core2 processor and
2.0 Gbyte of main memory.
</bodyText>
<equation confidence="0.724501">
n
X2 wi +X
{i|yi=−1,tCxi} i=1
1254
</equation>
<figureCaption confidence="0.999054">
Figure 4: Comparison of different trees (SWBD)
</figureCaption>
<bodyText confidence="0.999993264705882">
We show that use of subtree features is ef-
fective to solve the supervised speech act recog-
nition problem. We also compared our model
with the state-of-the-art maximum entropy classi-
fier (MAXENT). We used bag-of-words, bigram
and trigram features for MAXENT, which mod-
eled702k (SWBD) and 460k (MRDA) parameters
(i.e., patterns), and produced micro-averaged F1
scores of 92.76 (macro-averaged F1 = 63.54) for
SWBD and 82.48 (macro-averaged F1 = 57.19)
for MRDA. In contrast, our method generated ap-
proximately 4k to 5k patterns on average with sim-
ilar or greater F1 scores (Table 3); hence, com-
pared to MAXENT, our model requires fewer cal-
culations and is just as accurate.
The n-gram model (NGRAM) performed signif-
icantly better than the bag-of-words model (Mc-
Nemar test; p &lt; 0.001) (Table 3). Unlike MAX-
ENT, NGRAM automatically selects a relevant set
of variable length n-gram features (i.e., phrase
features). To this set, we separately added two
syntax type features, part-of-speech tag n-gram
(POSTAG) and dependency parse tree (DEPTREE)
automatically parsed by Minipar8, and one dis-
course type feature, speaker n-gram (SPEAKER).
Although some micro-averaged F1 are not statisti-
cally significant between the original NGRAM and
the models that include POSTAG, DEPTREE or
SPEAKER, macro-averaged F1 values indicate that
minor classes can take advantage of other struc-
tures. For example, in the result of SWBD (Fig-
ure 4), DEPTREE and SPEAKER models help to
predict uncertain responses (U), whereas NGRAM
and POSTAG cannot do this.
</bodyText>
<sectionHeader confidence="0.996725" genericHeader="method">
5 Semi-supervised Learning
</sectionHeader>
<bodyText confidence="0.9997095">
Our goal is to eventually make maximum use
of existing resources in SWBD and MRDA for
</bodyText>
<footnote confidence="0.316073">
8http://www.cs.ualberta.ca/—lindek/minipar.htm
</footnote>
<bodyText confidence="0.999897733333333">
email/forum speech act recognition. We call the
model trained on the mixed data of these two cor-
pora BASELINE. We use ALL features in con-
structing the BASELINE for the semi-supervised
experiments. While this model gave promising re-
sults using SWBD and MRDA, language used in
emails and forums differs from that used in spo-
ken conversation. For example, ‘thanx’ is an ex-
pression commonly used as a polite mechanism
in online communications. To adapt our model to
understand this type of difference between spoken
and online text-based conversations, we should in-
duce new patterns from unlabeled email and fo-
rum data. We describe here two methods of semi-
supervised learning.
</bodyText>
<subsectionHeader confidence="0.9993">
5.1 Method 1: Bootstrapping
</subsectionHeader>
<bodyText confidence="0.999994285714286">
First, we bootstrap the BASELINE model using au-
tomatically predicted unlabeled examples. How-
ever, using all of the unlabeled data results in noisy
models; therefore filtering or selecting data is very
important in practice. To this end, we only select
similar examples by criterion, d(xi, xj) &lt; r or k
nearest neighbors where xi E DL and xj E DU.
In practice, r or k are fixed. In our method, exam-
ples are represented by trees; hence we use a “tree
edit distance” for calculating d(xi, xj) (Shasha
and Zhang, 1990). Selected examples are evalu-
ated using BASELINE, and using subtree pattern
mining runs on the augmented data (i.e. unla-
beled). We call this method BOOTSTRAP.
</bodyText>
<subsectionHeader confidence="0.998475">
5.2 Method 2: Semi-supervised Boosting
</subsectionHeader>
<bodyText confidence="0.999343">
Our second method is based on a principle of
semi-supervised boosting learning (Bennett et al.,
2002). Because we have no supervised guidance
for DU, our objective functional to find F is de-
fined as:
</bodyText>
<equation confidence="0.999261666666667">
C(F) = � αiC[yiF(xi)] + � 0iC[|F(xi)|]
iEDL iEDU
(7)
</equation>
<bodyText confidence="0.918963833333333">
This cost functional is non-differentiable. To
solve it, we introduce pseudo-labels y� where y� =
sgn(F(x)) and |F(x) |= yF(x). Using the same
derivation in Section 4.2, we obtain the following
1255
gain function and update rules:
</bodyText>
<equation confidence="0.99573475">
gain(t, y) = � yiwif(y, t, xi)
iEDL
+ � �yiwif(y, t, xi), (8)
iEDU
�
αi · e_yiF(xi) i ∈ DL,
wi = (9)
βi · e_yiF(xi) i ∈ DU.
</equation>
<bodyText confidence="0.99995325">
Intuitively, an unlabeled example that has a
high-confidence |F(x) |at the current step, will
probably receive more weight at the next step.
That is, similar instances become more impor-
tant when learning and mining subtrees. This
semi-supervised boosting learning iteratively gen-
erates pseudo-labels for unlabeled data and finds
the value of F that minimizes training errors (Ben-
nett et al., 2002). Also, the algorithm infers new
features from unlabeled data, and these features
are iteratively re-evaluated by the current ensem-
ble learner. We call this method SEMIBOOST.
</bodyText>
<sectionHeader confidence="0.993281" genericHeader="evaluation">
6 Experiment
</sectionHeader>
<subsectionHeader confidence="0.999556">
6.1 Setting
</subsectionHeader>
<bodyText confidence="0.999773925925926">
We describe specific settings used in our exper-
iment. Because we have no development set,
we set the maximum number of iterations K at
10,000. At most K patterns can be extracted, but
this seldom happens because duplicated patterns
are merged. Typical settings for semi-supervised
boosting are αi = 1 and βi = 0.5, that is, we
penalize the weights for unlabeled data.
For efficiency, BASELINE model used 10% of
the SWBD and MRDA data, selected at random.
We observed that this data set does not degrade the
results of semi-supervised speech act recognition.
For BOOTSTRAP and SEMIBOOST, we selected
k = 100 nearest neighbors of unlabeled exam-
ples for each labeled example using tree edit dis-
tance, and then used 24,625 (SWBD) and 54,961
(MRDA) sentences for the semi-supervised set-
ting.
All trees were combined as described in Section
4.3 (ALL model). In EMAIL and FORUM data we
added different types of discourse features: mes-
sage type (e.g., initial or reply posts), authorship
(e.g., an identification of 2nd or 3rd posts written
by the same author), and relative position of a sen-
tence. In Figure 1, for example, F1∼3 is an initial
post, and F4∼5 and F6 are reply posts. Moreover,
F1, F4, and F6 are the first sentence in each post.
</bodyText>
<tableCaption confidence="0.928911">
Table 4: Results of speech act recognition on on-
</tableCaption>
<table confidence="0.795715666666667">
line conversations; columns are micro-averaged
F1 score with macro-averaged scores in parenthe-
ses. ‘*’ indicates that the result is significantly bet-
ter than BASELINE (p &lt; 0.001).
Model EMAIL FORUM
BASELINE 78.87 (37.44) 78.93 (35.57)
BOOTSTRAP *83.11 (44.90) 79.09 (44.38)
SEMIBOOST *82.80 (44.64) *81.76 (44.21)
SUPERVISED 90.95 (75.71) 83.67 (40.68)
</table>
<bodyText confidence="0.986781333333333">
These features do not occur in SWBD or MRDA
because these are utterance-by-utterance conver-
sations.
</bodyText>
<subsectionHeader confidence="0.999196">
6.2 Result and Discussion
</subsectionHeader>
<bodyText confidence="0.999902696969697">
First, we show that our method of semi-supervised
learning can improve modeling of the speech
act of emails and forums. As our baseline,
BASELINE achieved a micro-averaged F1 score
of ∼ 79 for both data sets. This implies that
SWBD and MRDA data are useful for our prob-
lem. Using unlabeled data, semi-supervised meth-
ods BOOTSTRAP and SEMIBOOST perform bet-
ter than BASELINE (Table 4; Figure 5). To verify
our claim, we evaluated the supervised speech act
recognition on EMAIL and FORUM evaluation
sets with 5-fold cross validation (SUPERVISED in
Table 4). In particular, our semi-supervised speech
act recognition is competitive with the supervised
model in FORUM data.
The difference in performance between super-
vised results in EMAIL and FORUM seems to
indicate that the latter is a more difficult data
set. However, our SEMIBOOST method were able
to come close to the supervised FORUM results
(81.76 vs. 83.67). This is also close to the range of
supervised MRDA data set (F1 = 83.49 for ALL,
Table 3). Moreover, we analyzed a main reason of
why transfer results were competitive in the FO-
RUM but not in the EMAIL. This might be due
to the mismatch in the unlabeled data, that is, we
used different email collections, the BC3 corpus
(email communication of W3C on w3.org sites),
for evaluation while used Enron data for adaption.
We also conjecture that the discrepancy between
EMAIL and FORUM is probably due to the more
heterogeneous nature of the FORUM data where
anyone can post and reply while EMAIL (Enron or
</bodyText>
<figure confidence="0.924325">
1256
(a) EMAIL (b) FORUM
</figure>
<figureCaption confidence="0.999936">
Figure 5: Result of the semi-supervised learning method
</figureCaption>
<bodyText confidence="0.999355847826087">
BC3) might have a more fix set of participants.
The improvement of less frequent tags is promi-
nent, for example 25% for action motivator (AC),
40% for polite mechanism (P), and 15% for rhetor-
ical question (QR) error rate reductions were
achieved in FORUM data (Figure 5(b)). There-
fore, the semi-supervised learning method is more
effective with small amounts of labeled data (i.e.,
less frequent annotations). We believe that despite
their relative rarity, these speech acts are more im-
portant than the statement (S) in some applica-
tions, e.g., summarization.
Next, we give a qualitative analysis for better
interpretation of our problem and results. Due to
limited space, we focus on FORUM data, which
can potentially be applied to many applications.
Of the top ranked patterns extracted by SEMI-
BOOST (Figure 6(a)), subtree patterns of n-gram,
part-of-speech, dependency parse trees are most
discriminative. The patterns from unlabeled data
have relatively lower ranks, but this is not surpris-
ing. This indicates that BASELINE model provides
the base knowledge for semi-supervised speech
act recognition. Also, unlabeled data for EMAIL
and FORUM help to induce new patterns or ad-
just the model’s parameters. As a result, the semi-
supervised method is better than the BASELINE
when an identical number of patterns is modeled
(Figure 6(b)). For this result, we conclude that our
method successfully transfers knowledge from a
source domain (i.e., SWBD and MRDA) to a tar-
get domain (i.e., EMAIL and FORUM); hence it
can be a solution to the domain adaption problem.
Finally, we determine the main reasons for error
(in SEMIBOOST), to gain insights that may allow
development of better models in future work (Fig-
ure 6(c)). We sorted speech act tags by their se-
mantics and partitioned the confusion matrix into
question type (Q*) and statement, which are two
high-level speech acts. Most errors occur in the
similar categories, that is, language usage in ques-
tion discourse is definitely distinct from that in
statement discourse. From this analysis, we be-
lieve that more advanced techniques (e.g. two-
stage classification and learning with hierarchy-
augmented loss) can improve our model.
</bodyText>
<sectionHeader confidence="0.998439" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999759214285714">
Despite the increasing interest in online text-based
conversations, no study to date has investigated
semi-supervised speech act recognition in email
and forum threads. This paper has addressed the
problem of learning to recognize speech acts us-
ing labeled and unlabeled data. We have also con-
tributed to the development of a novel applica-
tion of boosting subtree mining. Empirical results
have demonstrated that semi-supervised learning
of speech act recognition with subtree features im-
proves the performance in email and forum data
sets. An attractive future direction is to exploit
prior knowledge for semi-supervised speech act
recognition. Druck et al. (2008) described gen-
eralized expectation criteria in which a discrimi-
native model can employ the labeled features and
unlabeled instances. Using prior knowledge, we
expect that our model will effectively learn useful
patterns from unlabeled data.
As work progresses on analyzing online text-
based conversations such as emails, forums, and
online chats, the importance of developing models
for discourse without annotating much new data
will become more important. In the future, we
plan to explore other related problems such as ad-
jacency pairs (Levinson, 1983) and discourse pars-
ing (Soricut and Marcu, 2003) for large-scale on-
line forum data.
</bodyText>
<figure confidence="0.988553">
1257
(b) Learning behavior (c) Confusion matrix
0 2000 4000 6000
BASELINE
BOOTSTRAP
SEMIBOOST
Number of base leaners
(a) Example patterns
</figure>
<figureCaption confidence="0.99938">
Figure 6: Analysis on FORUM data
</figureCaption>
<sectionHeader confidence="0.951979" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.99997525">
We would like to thank to anonymous reviewers
for their valuable comments, and Yunbo Cao, Wei
Lai, Xinying Song, Jingtian Jing, and Wei Wu for
their help in preparing our data.
</bodyText>
<sectionHeader confidence="0.99847" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998809060606061">
R. Ando and T. Zhang. 2005. A framework for learn-
ing predictive structures from multiple tasks and
unlabeled data. Journal of Machine Learning Re-
search, 6:1817–1853.
J. Ang, Y. Liu, and E. Shriberg. 2005. Automatic dia-
log act segmentation and classification in multiparty
meetings. In Proceedings of ICASSP, pages 1061–
106.
J. Austin. 1962. How to Do Things With Words. Har-
vard Univ. Press, Cambridge, MA.
K.P. Bennett, A. Demiriz, and R. Maclin. 2002. Ex-
ploiting unlabeled data in ensemble methods. In
Proceedings of ACM SIGKDD, pages 289–296.
W.W. Cohen, V.R. Carvalho, and T. Mitchell. 2004.
Learning to classify email into “speech acts”. In
Proceedings of EMNLP, pages 309–316.
R. Dhillon, S. Bhagat, H. Carvey, and E. Shriberg.
2004. Meeting recorder project: Dialog act label-
ing guide. Technical report, International Computer
Science Institute.
G. Druck, G. Mann, and A. McCallum. 2008. Learn-
ing from labeled features using generalized expecta-
tion criteria. In Proceedings of ACM SIGIR, pages
595–602.
D. Feng, E. Shaw, J. Kim, and E. H. Hovy. 2006.
Learning to detect conversation focus of threaded
discussions. In Proceedings of HLT-NAACL, pages
208–215.
M. Galley, K. McKeown, J. Hirschberg, and
E. Shriberg. 2004. Identifying agreement and dis-
agreement in conversational speech: use of bayesian
networks to model pragmatic dependencies. In Pro-
ceedings of ACL.
L. Gillick and S. Cox. 1989. Some statistical issues in
the comparison of speech recognition algorithms. In
Proceedings of ICASSP, pages 532–535.
D. Jurafsky, E. Shriberg, and D. Biasca. 1997. Switch-
board SWBD-DAMSL labeling project coder’s
manual, draft 13. Technical report, Univ. of Col-
orado Institute of Cognitive Science.
T. Kudo and Y. Matsumoto. 2004. A boosting algo-
rithm for classification of semi-structured text. In
Proceedings of EMNLP, pages 301–308.
S. Levinson. 1983. Pragmatics. Cambridge Univ.
Press, Cambridge.
L. Mason, P. Bartlett, J. Baxter, and M. Frean. 2000.
Functional gradient techniques for combining hy-
potheses. In A.J. Smola, P.L. Bartlett, B. Sch¨olkopf,
and D. Schuurmans, editors, Advances in Large
Margin Classifiers, pages 221–246. MIT Press,
Cambridge, MA.
K. McKeown, L. Shrestha, and O. Rambow. 2007. Us-
ing question-answer pairs in extractive summariza-
tion of email conversations. In Proceedings of CI-
CLing, volume 4394 of Lecture Notes in Computer
Science, pages 542–550.
J. Nastri, J. Pe na, and J. T. Hancock. 2006. The
construction of away messages: A speech act anal-
ysis. Journal of Computer-Mediated Communica-
tion, 11(4):article 7.
S. Ravi and J. Kim. 2007. Profiling student interac-
tions in threaded discussions with speech act classi-
fiers. In Proceedings of the AI in Education Confer-
ence.
1258
C. Ros´e, Y. Wang, Y. Cui, J. Arguello, K. Stegmann,
A. Weinberger, and F. Fischer. 2008. Analyzing
collaborative learning processes automatically: Ex-
ploiting the advances of computational linguistics in
computer-supported collaborative learning. Interna-
tional Journal of Computer-Supported Collabora-
tive Learning, 3(3):237–271.
R.E. Schapire and Y. Singer. 1999. Improved boosting
algorithms using confidence-rated predictions. Ma-
chine Learning, 37(3):297–336.
J. Searle. 1969. Speech Acts. Cambridge Univ. Press,
Cambridge.
D. Shasha and K. Zhang. 1990. Fast algorithms for the
unit cost editing distance between trees. Journal of
Algorithms, 11(4):581–621.
R. Soricut and D. Marcu. 2003. Sentence level dis-
course parsing using syntactic and lexical informa-
tion. In Proceedings of NAACL-HLT, pages 149–
156.
A. Stolcke, K. Ries, N. Coccaro, E. Shriberg, R. Bates,
D. Jurafsky, P. Taylor, R. Martin, C. Van Ess-
Dykema, and M. Meteer. 2000. Dialogue act
modeling for automatic tagging and recognition of
conversational speech. Computational Linguistics,
26(3):339–373.
G. Tur, D. Hakkani-T¨ur, and R. E. Schapire. 2005.
Combining active and semi-supervised learning for
spoken language understanding. Speech Communi-
cation, 45(2):171–186.
D. P. Twitchell, J. F. Nunamaker, and J. K. Burgoon.
2004. Using speech act profiling for deception de-
tection. In Second Symposium on Intelligence and
Security Informatics, volume 3073 of Lecture Notes
in Computer Science, pages 403–410.
</reference>
<page confidence="0.82799">
1259
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.929900">
<title confidence="0.998236">Semi-supervised Speech Act Recognition in Emails and Forums</title>
<author confidence="0.95966">Chin-Yew Geunbae</author>
<affiliation confidence="0.999766">University of Science &amp; Technology, Pohang,</affiliation>
<address confidence="0.993511">Research Asia, Beijing, China</address>
<abstract confidence="0.998635333333333">In this paper, we present a semi-supervised method for automatic speech act recognition in email and forums. The major challenge of this task is due to lack of labeled data in these two genres. Our method leverages labeled data in the Switchboard- DAMSL and the Meeting Recorder Dialog Act database and applies simple domain adaptation techniques over a large amount of unlabeled email and forum data to address this problem. Our method uses automatically extracted features such as phrases and dependency trees, called subtree features, for semi-supervised learning. Empirical results demonstrate that our model is effective in email and forum speech act recognition.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Ando</author>
<author>T Zhang</author>
</authors>
<title>A framework for learning predictive structures from multiple tasks and unlabeled data.</title>
<date>2005</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>6--1817</pages>
<contexts>
<context position="6969" citStr="Ando and Zhang, 2005" startWordPosition="1102" endWordPosition="1105">eech act’ for clarity. This paper focuses on the problem of semisupervised speech act recognition. The goal of semi-supervised learning techniques is to use auxiliary data to improve a model’s capability to recognize speech acts. The approach in Tur et al. (2005) presented semi-supervised learning to employ auxiliary unlabeled data in call classification, and is closely related to our work. However, our approach uses the most discriminative subtree features, which is particularly attractive for reducing the model’s size. Our problem setting is closely related to the domain adaptation problem (Ando and Zhang, 2005), i.e., we seek to obtain a model that analyzes target domains (emails and forums) by adapting a method that analyzes source domains (SWBD and MRDA). Recently, this type of domain adaptation has become an important topic in natural language processing. 3 Problem Definition 3.1 Problem Statement We define speech act recognition to be the task that, given a sentence, maps it to one of the speech act types. Figure 1 shows two examples of our email and forum speech act recognition. E1-6 are all sentences in an email message. F1-3, F4-5, and F6 are three posts in a forum thread. A sentence interact</context>
</contexts>
<marker>Ando, Zhang, 2005</marker>
<rawString>R. Ando and T. Zhang. 2005. A framework for learning predictive structures from multiple tasks and unlabeled data. Journal of Machine Learning Research, 6:1817–1853.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ang</author>
<author>Y Liu</author>
<author>E Shriberg</author>
</authors>
<title>Automatic dialog act segmentation and classification in multiparty meetings.</title>
<date>2005</date>
<booktitle>In Proceedings of ICASSP,</booktitle>
<pages>1061--106</pages>
<contexts>
<context position="6072" citStr="Ang et al., 2005" startWordPosition="954" endWordPosition="957"> and Meeting Recorder Dialog Act (MRDA). SWBD is an annotation scheme and collection of labeled dialog act2 data for telephone conversations (Jurafsky et al., 1997). The main purpose of SWBD is to acquire stochastic discourse grammars for training better language models for automatic speech recognition. More recently, an MRDA corpus has been adapted from SWBD but its tag set for labeling meetings has been modified to better reflect the types of interaction in multi-party face-to-face meetings (Dhillon et al., 2004). These two corpora have been extensively studied, e.g., (Stolcke et al., 2000; Ang et al., 2005; Galley et al., 2004). We also use these for our experiments. 2A dialog act is the meaning of an utterance at the level of illocutionary force (Austin, 1962), and broadly covers the speech act and adjacency pair (Stolcke et al., 2000). In this paper, we use only the term ‘speech act’ for clarity. This paper focuses on the problem of semisupervised speech act recognition. The goal of semi-supervised learning techniques is to use auxiliary data to improve a model’s capability to recognize speech acts. The approach in Tur et al. (2005) presented semi-supervised learning to employ auxiliary unlab</context>
<context position="13386" citStr="Ang et al., 2005" startWordPosition="2196" endWordPosition="2199">nces in labeled and unlabeled data Set SWBD MRDA Set EMAIL FORUM Training 96,553 50,865 Development 12,299 8,366 Evaluation 24,264 10,492 Unlabeled 122,125 297,017 Evaluation 2,267 3,711 Figure 2: Distribution of speech acts in the evaluation sets. Tags are defined in Table 1. frequent tag is the statement (S) tag (Figure 2). Distributions of tags are similar in training and development sets of SWBD and MRDA. 4 Speech Act Recognition Previous work in speech act recognition used a large set of lexical features, e.g., bag-of-words, bigrams and trigrams (Stolcke et al., 2000; Cohen et al., 2004; Ang et al., 2005; Ravi and Kim, 2007). However, these methods create a large number of lexical features that might not be necessary for speech act identification. For example, a Wh-question “What site should we use to book a Beijing-Chonqing flight?” can be predicted by two discriminative features, “(&lt;s&gt;, WRB) —* QW” and “(?, &lt;/s&gt;) —* QW” where &lt;s&gt; and &lt;/s&gt; are sentence start and end symbols, and WRB is a part-of-speech tag that denotes a Wh-adverb. In addition, useful features could be of various lengths, i.e. not fixed length n-grams, and nonadjacent. One key idea of this paper is a novel use of subtree fea</context>
</contexts>
<marker>Ang, Liu, Shriberg, 2005</marker>
<rawString>J. Ang, Y. Liu, and E. Shriberg. 2005. Automatic dialog act segmentation and classification in multiparty meetings. In Proceedings of ICASSP, pages 1061– 106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Austin</author>
</authors>
<title>How to Do Things With Words.</title>
<date>1962</date>
<publisher>Harvard Univ. Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="4196" citStr="Austin, 1962" startWordPosition="653" endWordPosition="654">es a supervised method of learning subtree features that shows the effectiveness of subtree features on labeled data sets. Section 5 proposes semi-supervised learning techniques for speech act recognition and Section 6 demonstrates our method applied to email and on1250 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1250–1259, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP line forum thread data. Section 7 concludes this paper with future work. 2 Related Work Speech act theory is fundamental to many studies in discourse analysis and pragmatics (Austin, 1962; Searle, 1969). A speech act is an illocutionary act of conversation and reflects shallow discourse structures of language. Recent research on spoken dialog processing has investigated computational speech act models of humanhuman and human-computer conversations (Stolcke et al., 2000) and applications of these models to CMC and distance learning (Twitchell et al., 2004; Nastri et al., 2006; Ros´e et al., 2008). Our work in this paper is closely related to prior work on email and forum speech act recognition. Cohen et al. (2004) proposed the notion of ‘email speech act’ for classifying the in</context>
<context position="6230" citStr="Austin, 1962" startWordPosition="984" endWordPosition="985">97). The main purpose of SWBD is to acquire stochastic discourse grammars for training better language models for automatic speech recognition. More recently, an MRDA corpus has been adapted from SWBD but its tag set for labeling meetings has been modified to better reflect the types of interaction in multi-party face-to-face meetings (Dhillon et al., 2004). These two corpora have been extensively studied, e.g., (Stolcke et al., 2000; Ang et al., 2005; Galley et al., 2004). We also use these for our experiments. 2A dialog act is the meaning of an utterance at the level of illocutionary force (Austin, 1962), and broadly covers the speech act and adjacency pair (Stolcke et al., 2000). In this paper, we use only the term ‘speech act’ for clarity. This paper focuses on the problem of semisupervised speech act recognition. The goal of semi-supervised learning techniques is to use auxiliary data to improve a model’s capability to recognize speech acts. The approach in Tur et al. (2005) presented semi-supervised learning to employ auxiliary unlabeled data in call classification, and is closely related to our work. However, our approach uses the most discriminative subtree features, which is particular</context>
</contexts>
<marker>Austin, 1962</marker>
<rawString>J. Austin. 1962. How to Do Things With Words. Harvard Univ. Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K P Bennett</author>
<author>A Demiriz</author>
<author>R Maclin</author>
</authors>
<title>Exploiting unlabeled data in ensemble methods.</title>
<date>2002</date>
<booktitle>In Proceedings of ACM SIGKDD,</booktitle>
<pages>289--296</pages>
<contexts>
<context position="22492" citStr="Bennett et al., 2002" startWordPosition="3745" endWordPosition="3748"> is very important in practice. To this end, we only select similar examples by criterion, d(xi, xj) &lt; r or k nearest neighbors where xi E DL and xj E DU. In practice, r or k are fixed. In our method, examples are represented by trees; hence we use a “tree edit distance” for calculating d(xi, xj) (Shasha and Zhang, 1990). Selected examples are evaluated using BASELINE, and using subtree pattern mining runs on the augmented data (i.e. unlabeled). We call this method BOOTSTRAP. 5.2 Method 2: Semi-supervised Boosting Our second method is based on a principle of semi-supervised boosting learning (Bennett et al., 2002). Because we have no supervised guidance for DU, our objective functional to find F is defined as: C(F) = � αiC[yiF(xi)] + � 0iC[|F(xi)|] iEDL iEDU (7) This cost functional is non-differentiable. To solve it, we introduce pseudo-labels y� where y� = sgn(F(x)) and |F(x) |= yF(x). Using the same derivation in Section 4.2, we obtain the following 1255 gain function and update rules: gain(t, y) = � yiwif(y, t, xi) iEDL + � �yiwif(y, t, xi), (8) iEDU � αi · e_yiF(xi) i ∈ DL, wi = (9) βi · e_yiF(xi) i ∈ DU. Intuitively, an unlabeled example that has a high-confidence |F(x) |at the current step, will</context>
</contexts>
<marker>Bennett, Demiriz, Maclin, 2002</marker>
<rawString>K.P. Bennett, A. Demiriz, and R. Maclin. 2002. Exploiting unlabeled data in ensemble methods. In Proceedings of ACM SIGKDD, pages 289–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W W Cohen</author>
<author>V R Carvalho</author>
<author>T Mitchell</author>
</authors>
<title>Learning to classify email into “speech acts”.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>309--316</pages>
<contexts>
<context position="2485" citStr="Cohen et al. (2004)" startWordPosition="379" endWordPosition="382">. 1http://tripadvisor.com/ in this paper) and large unlabeled data sets can be collected from the Web. Thus, we focus on the problem of how to accurately recognize speech acts in emails and forums by making maximum use of data from existing resources. Recently, there are increasing interests in speech act recognition of online text-based conversations. Analysis of speech acts for online chat and instant messages and have been studied in computer-mediated communication (CMC) and distance learning (Twitchell et al., 2004; Nastri et al., 2006; Ros´e et al., 2008). In natural language processing, Cohen et al. (2004) and Feng et al. (2006) used speech acts to capture the intentional focus of emails and discussion boards. However, they assume that enough labeled data are available for developing speech act recognition models. A main contribution of this paper is that we address the problem of learning speech act recognition in a semi-supervised way. To our knowledge, this is the first use of semi-supervised speech act recognition in emails and online forums. To do this, we make use of labeled data from spoken conversations (Jurafsky et al., 1997; Dhillon et al., 2004). A second contribution is that our mod</context>
<context position="4731" citStr="Cohen et al. (2004)" startWordPosition="741" endWordPosition="744">ry is fundamental to many studies in discourse analysis and pragmatics (Austin, 1962; Searle, 1969). A speech act is an illocutionary act of conversation and reflects shallow discourse structures of language. Recent research on spoken dialog processing has investigated computational speech act models of humanhuman and human-computer conversations (Stolcke et al., 2000) and applications of these models to CMC and distance learning (Twitchell et al., 2004; Nastri et al., 2006; Ros´e et al., 2008). Our work in this paper is closely related to prior work on email and forum speech act recognition. Cohen et al. (2004) proposed the notion of ‘email speech act’ for classifying the intent of an email sender. They defined verb and noun categories for email speech acts and used supervised learning to recognize them. Feng et al. (2006) presented a method of detecting conversation focus based on the speech acts of messages in discussion boards. Extending Feng et al. (2006)’s work, Ravi and Kim (2007) applied speech act classification to detect unanswered questions. However, none of these studies have focused on the semisupervised speech act recognition problem and examined their methods across different genres. T</context>
<context position="8426" citStr="Cohen et al., 2004" startWordPosition="1350" endWordPosition="1353">F6 are corresponding answers. More recently, using speech acts has become an appealing approach in summarizing the discussions (Galley et al., 2004; McKeown et al., 2007). Next, we define speech act category based on MRDA. Dhillon et al. (2004) included definitions of speech acts for colloquial style interactions (e.g., backchannel, disruption, and floorgrabber), but these are not applicable in emails and forums. After removing these categories, we define 12 tags (Table 1). Dhillon et al. (2004) provides detailed descriptions of each tag. We note that our tag set definition is different from (Cohen et al., 2004; Feng et al., 2006; Ravi and Kim, 2007) for two reasons. First, prior work primarily interested in the domain-specific speech acts, but our work use domain-independent speech act tags. Second, we focus on speech act recognition on the sentencelevel. 1251 E1: I am planning my schedule at CHI 2003 (http://www.chi2003.org/) S E2: - will there be anything happening at the conference related to this W3C User interest group? QY E3: I do not see anything on the program yet, but I suspect we could at least have an informal SIG S E4: - a chance to meet others and bring someone like me up to speed on w</context>
<context position="13368" citStr="Cohen et al., 2004" startWordPosition="2191" endWordPosition="2195">e 2: Number of sentences in labeled and unlabeled data Set SWBD MRDA Set EMAIL FORUM Training 96,553 50,865 Development 12,299 8,366 Evaluation 24,264 10,492 Unlabeled 122,125 297,017 Evaluation 2,267 3,711 Figure 2: Distribution of speech acts in the evaluation sets. Tags are defined in Table 1. frequent tag is the statement (S) tag (Figure 2). Distributions of tags are similar in training and development sets of SWBD and MRDA. 4 Speech Act Recognition Previous work in speech act recognition used a large set of lexical features, e.g., bag-of-words, bigrams and trigrams (Stolcke et al., 2000; Cohen et al., 2004; Ang et al., 2005; Ravi and Kim, 2007). However, these methods create a large number of lexical features that might not be necessary for speech act identification. For example, a Wh-question “What site should we use to book a Beijing-Chonqing flight?” can be predicted by two discriminative features, “(&lt;s&gt;, WRB) —* QW” and “(?, &lt;/s&gt;) —* QW” where &lt;s&gt; and &lt;/s&gt; are sentence start and end symbols, and WRB is a part-of-speech tag that denotes a Wh-adverb. In addition, useful features could be of various lengths, i.e. not fixed length n-grams, and nonadjacent. One key idea of this paper is a novel </context>
</contexts>
<marker>Cohen, Carvalho, Mitchell, 2004</marker>
<rawString>W.W. Cohen, V.R. Carvalho, and T. Mitchell. 2004. Learning to classify email into “speech acts”. In Proceedings of EMNLP, pages 309–316.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Dhillon</author>
<author>S Bhagat</author>
<author>H Carvey</author>
<author>E Shriberg</author>
</authors>
<title>Meeting recorder project: Dialog act labeling guide.</title>
<date>2004</date>
<tech>Technical report,</tech>
<institution>International Computer Science Institute.</institution>
<contexts>
<context position="3046" citStr="Dhillon et al., 2004" startWordPosition="475" endWordPosition="478">, 2008). In natural language processing, Cohen et al. (2004) and Feng et al. (2006) used speech acts to capture the intentional focus of emails and discussion boards. However, they assume that enough labeled data are available for developing speech act recognition models. A main contribution of this paper is that we address the problem of learning speech act recognition in a semi-supervised way. To our knowledge, this is the first use of semi-supervised speech act recognition in emails and online forums. To do this, we make use of labeled data from spoken conversations (Jurafsky et al., 1997; Dhillon et al., 2004). A second contribution is that our model learns subtree features that constitute discriminative patterns: for example, variable length n-grams and partial dependency structures. Therefore, our model can capture both local features such as n-grams and non-local dependencies. In this paper, we extend subtree pattern mining to the semi-supervised learning problem. This paper is structured as follows. Section 2 reviews prior work on speech act recognition and Section 3 presents the problem statement and our data sets. Section 4 describes a supervised method of learning subtree features that shows</context>
<context position="5976" citStr="Dhillon et al., 2004" startWordPosition="938" endWordPosition="941">munity frequently employs two large-scale corpora for speech act annotation: Switchboard-DAMSL (SWBD) and Meeting Recorder Dialog Act (MRDA). SWBD is an annotation scheme and collection of labeled dialog act2 data for telephone conversations (Jurafsky et al., 1997). The main purpose of SWBD is to acquire stochastic discourse grammars for training better language models for automatic speech recognition. More recently, an MRDA corpus has been adapted from SWBD but its tag set for labeling meetings has been modified to better reflect the types of interaction in multi-party face-to-face meetings (Dhillon et al., 2004). These two corpora have been extensively studied, e.g., (Stolcke et al., 2000; Ang et al., 2005; Galley et al., 2004). We also use these for our experiments. 2A dialog act is the meaning of an utterance at the level of illocutionary force (Austin, 1962), and broadly covers the speech act and adjacency pair (Stolcke et al., 2000). In this paper, we use only the term ‘speech act’ for clarity. This paper focuses on the problem of semisupervised speech act recognition. The goal of semi-supervised learning techniques is to use auxiliary data to improve a model’s capability to recognize speech acts</context>
<context position="8052" citStr="Dhillon et al. (2004)" startWordPosition="1293" endWordPosition="1296">ech act recognition. E1-6 are all sentences in an email message. F1-3, F4-5, and F6 are three posts in a forum thread. A sentence interacts alone or with others, for example, F6 agrees with the previous post (F4-5). To gain insight into our work, it is useful to consider that E2, 3 and F1, 4, 6 are summaries of two discourses. In particular, F1 denotes a question and F4 and F6 are corresponding answers. More recently, using speech acts has become an appealing approach in summarizing the discussions (Galley et al., 2004; McKeown et al., 2007). Next, we define speech act category based on MRDA. Dhillon et al. (2004) included definitions of speech acts for colloquial style interactions (e.g., backchannel, disruption, and floorgrabber), but these are not applicable in emails and forums. After removing these categories, we define 12 tags (Table 1). Dhillon et al. (2004) provides detailed descriptions of each tag. We note that our tag set definition is different from (Cohen et al., 2004; Feng et al., 2006; Ravi and Kim, 2007) for two reasons. First, prior work primarily interested in the domain-specific speech acts, but our work use domain-independent speech act tags. Second, we focus on speech act recogniti</context>
<context position="11142" citStr="Dhillon et al., 2004" startWordPosition="1841" endWordPosition="1844">d training data, and DU = {xi}&apos;L+1 as the set of unlabeled training data where l = n + m and m is the number of unlabeled data instances. Our goal is to find a learning method to minimize the classification errors in DL and DU. 3.2 Data Preparation In this paper, we separate labeled (DL) and unlabeled data (DU). First we use SWBD3 and MRDA4 as our labeled data. We automatically 3LDC Catalog No. LDC97S62 4http://www.icsi.berkeley.edu/—ees/dadb/ map original annotations in SWBD and MRDA to one of the 12 speech acts.5 Inter-annotator agreement K in both data sets is — 0.8 (Jurafsky et al., 1997; Dhillon et al., 2004). For evaluation purposes, we divide labeled data into three sets: training, development, and evaluation sets (Table 2). Of the 1,155 available conversations in the SWBD corpus, we use 855 for training, 100 for development, and 200 for evaluation. Among the 75 available meetings in the MRDA corpus, we exclude two meetings of different natures (btr001 and btr002). Of the remaining meetings, we use 59 for training, 6 for development, and 8 for evaluation. Then we merge multi-segments utterances that belong to the same speaker and then divide all data sets into sentences. As stated earlier, our u</context>
</contexts>
<marker>Dhillon, Bhagat, Carvey, Shriberg, 2004</marker>
<rawString>R. Dhillon, S. Bhagat, H. Carvey, and E. Shriberg. 2004. Meeting recorder project: Dialog act labeling guide. Technical report, International Computer Science Institute.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Druck</author>
<author>G Mann</author>
<author>A McCallum</author>
</authors>
<title>Learning from labeled features using generalized expectation criteria.</title>
<date>2008</date>
<booktitle>In Proceedings of ACM SIGIR,</booktitle>
<pages>595--602</pages>
<contexts>
<context position="29819" citStr="Druck et al. (2008)" startWordPosition="4945" endWordPosition="4948">-based conversations, no study to date has investigated semi-supervised speech act recognition in email and forum threads. This paper has addressed the problem of learning to recognize speech acts using labeled and unlabeled data. We have also contributed to the development of a novel application of boosting subtree mining. Empirical results have demonstrated that semi-supervised learning of speech act recognition with subtree features improves the performance in email and forum data sets. An attractive future direction is to exploit prior knowledge for semi-supervised speech act recognition. Druck et al. (2008) described generalized expectation criteria in which a discriminative model can employ the labeled features and unlabeled instances. Using prior knowledge, we expect that our model will effectively learn useful patterns from unlabeled data. As work progresses on analyzing online textbased conversations such as emails, forums, and online chats, the importance of developing models for discourse without annotating much new data will become more important. In the future, we plan to explore other related problems such as adjacency pairs (Levinson, 1983) and discourse parsing (Soricut and Marcu, 200</context>
</contexts>
<marker>Druck, Mann, McCallum, 2008</marker>
<rawString>G. Druck, G. Mann, and A. McCallum. 2008. Learning from labeled features using generalized expectation criteria. In Proceedings of ACM SIGIR, pages 595–602.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Feng</author>
<author>E Shaw</author>
<author>J Kim</author>
<author>E H Hovy</author>
</authors>
<title>Learning to detect conversation focus of threaded discussions.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>208--215</pages>
<contexts>
<context position="2508" citStr="Feng et al. (2006)" startWordPosition="384" endWordPosition="387">m/ in this paper) and large unlabeled data sets can be collected from the Web. Thus, we focus on the problem of how to accurately recognize speech acts in emails and forums by making maximum use of data from existing resources. Recently, there are increasing interests in speech act recognition of online text-based conversations. Analysis of speech acts for online chat and instant messages and have been studied in computer-mediated communication (CMC) and distance learning (Twitchell et al., 2004; Nastri et al., 2006; Ros´e et al., 2008). In natural language processing, Cohen et al. (2004) and Feng et al. (2006) used speech acts to capture the intentional focus of emails and discussion boards. However, they assume that enough labeled data are available for developing speech act recognition models. A main contribution of this paper is that we address the problem of learning speech act recognition in a semi-supervised way. To our knowledge, this is the first use of semi-supervised speech act recognition in emails and online forums. To do this, we make use of labeled data from spoken conversations (Jurafsky et al., 1997; Dhillon et al., 2004). A second contribution is that our model learns subtree featu</context>
<context position="4947" citStr="Feng et al. (2006)" startWordPosition="778" endWordPosition="781">esearch on spoken dialog processing has investigated computational speech act models of humanhuman and human-computer conversations (Stolcke et al., 2000) and applications of these models to CMC and distance learning (Twitchell et al., 2004; Nastri et al., 2006; Ros´e et al., 2008). Our work in this paper is closely related to prior work on email and forum speech act recognition. Cohen et al. (2004) proposed the notion of ‘email speech act’ for classifying the intent of an email sender. They defined verb and noun categories for email speech acts and used supervised learning to recognize them. Feng et al. (2006) presented a method of detecting conversation focus based on the speech acts of messages in discussion boards. Extending Feng et al. (2006)’s work, Ravi and Kim (2007) applied speech act classification to detect unanswered questions. However, none of these studies have focused on the semisupervised speech act recognition problem and examined their methods across different genres. The speech processing community frequently employs two large-scale corpora for speech act annotation: Switchboard-DAMSL (SWBD) and Meeting Recorder Dialog Act (MRDA). SWBD is an annotation scheme and collection of lab</context>
<context position="8445" citStr="Feng et al., 2006" startWordPosition="1354" endWordPosition="1357"> answers. More recently, using speech acts has become an appealing approach in summarizing the discussions (Galley et al., 2004; McKeown et al., 2007). Next, we define speech act category based on MRDA. Dhillon et al. (2004) included definitions of speech acts for colloquial style interactions (e.g., backchannel, disruption, and floorgrabber), but these are not applicable in emails and forums. After removing these categories, we define 12 tags (Table 1). Dhillon et al. (2004) provides detailed descriptions of each tag. We note that our tag set definition is different from (Cohen et al., 2004; Feng et al., 2006; Ravi and Kim, 2007) for two reasons. First, prior work primarily interested in the domain-specific speech acts, but our work use domain-independent speech act tags. Second, we focus on speech act recognition on the sentencelevel. 1251 E1: I am planning my schedule at CHI 2003 (http://www.chi2003.org/) S E2: - will there be anything happening at the conference related to this W3C User interest group? QY E3: I do not see anything on the program yet, but I suspect we could at least have an informal SIG S E4: - a chance to meet others and bring someone like me up to speed on what is happening. S</context>
</contexts>
<marker>Feng, Shaw, Kim, Hovy, 2006</marker>
<rawString>D. Feng, E. Shaw, J. Kim, and E. H. Hovy. 2006. Learning to detect conversation focus of threaded discussions. In Proceedings of HLT-NAACL, pages 208–215.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>K McKeown</author>
<author>J Hirschberg</author>
<author>E Shriberg</author>
</authors>
<title>Identifying agreement and disagreement in conversational speech: use of bayesian networks to model pragmatic dependencies.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="6094" citStr="Galley et al., 2004" startWordPosition="958" endWordPosition="961">der Dialog Act (MRDA). SWBD is an annotation scheme and collection of labeled dialog act2 data for telephone conversations (Jurafsky et al., 1997). The main purpose of SWBD is to acquire stochastic discourse grammars for training better language models for automatic speech recognition. More recently, an MRDA corpus has been adapted from SWBD but its tag set for labeling meetings has been modified to better reflect the types of interaction in multi-party face-to-face meetings (Dhillon et al., 2004). These two corpora have been extensively studied, e.g., (Stolcke et al., 2000; Ang et al., 2005; Galley et al., 2004). We also use these for our experiments. 2A dialog act is the meaning of an utterance at the level of illocutionary force (Austin, 1962), and broadly covers the speech act and adjacency pair (Stolcke et al., 2000). In this paper, we use only the term ‘speech act’ for clarity. This paper focuses on the problem of semisupervised speech act recognition. The goal of semi-supervised learning techniques is to use auxiliary data to improve a model’s capability to recognize speech acts. The approach in Tur et al. (2005) presented semi-supervised learning to employ auxiliary unlabeled data in call clas</context>
<context position="7955" citStr="Galley et al., 2004" startWordPosition="1276" endWordPosition="1279"> maps it to one of the speech act types. Figure 1 shows two examples of our email and forum speech act recognition. E1-6 are all sentences in an email message. F1-3, F4-5, and F6 are three posts in a forum thread. A sentence interacts alone or with others, for example, F6 agrees with the previous post (F4-5). To gain insight into our work, it is useful to consider that E2, 3 and F1, 4, 6 are summaries of two discourses. In particular, F1 denotes a question and F4 and F6 are corresponding answers. More recently, using speech acts has become an appealing approach in summarizing the discussions (Galley et al., 2004; McKeown et al., 2007). Next, we define speech act category based on MRDA. Dhillon et al. (2004) included definitions of speech acts for colloquial style interactions (e.g., backchannel, disruption, and floorgrabber), but these are not applicable in emails and forums. After removing these categories, we define 12 tags (Table 1). Dhillon et al. (2004) provides detailed descriptions of each tag. We note that our tag set definition is different from (Cohen et al., 2004; Feng et al., 2006; Ravi and Kim, 2007) for two reasons. First, prior work primarily interested in the domain-specific speech ac</context>
</contexts>
<marker>Galley, McKeown, Hirschberg, Shriberg, 2004</marker>
<rawString>M. Galley, K. McKeown, J. Hirschberg, and E. Shriberg. 2004. Identifying agreement and disagreement in conversational speech: use of bayesian networks to model pragmatic dependencies. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Gillick</author>
<author>S Cox</author>
</authors>
<title>Some statistical issues in the comparison of speech recognition algorithms.</title>
<date>1989</date>
<booktitle>In Proceedings of ICASSP,</booktitle>
<pages>532--535</pages>
<contexts>
<context position="18966" citStr="Gillick and Cox, 1989" startWordPosition="3172" endWordPosition="3175"> learner F(x) which consists of a set of appropriate base learners f(y, t, x). 4.3 Evaluation on Labeled Data We verified the effectiveness of using subtree features on the SWBD and MRDA data sets. For boosting learning, one typically assumes αi = 1. In addition, the number of iterations, which relates to the number of patterns, was determined by a development set. We also used a one-vs.-all strategy for the multi-class problem. Precision and recall were computed and combined into micro- and macro-averaged F1 scores. The significance of our results was evaluated using the McNemar paired test (Gillick and Cox, 1989), which is based on individual labeling decisions to compare the correctness of two models. All experiments were implemented in C++ and executed in Windows XP on a PC with a Dual 2.1 GHz Intel Core2 processor and 2.0 Gbyte of main memory. n X2 wi +X {i|yi=−1,tCxi} i=1 1254 Figure 4: Comparison of different trees (SWBD) We show that use of subtree features is effective to solve the supervised speech act recognition problem. We also compared our model with the state-of-the-art maximum entropy classifier (MAXENT). We used bag-of-words, bigram and trigram features for MAXENT, which modeled702k (SW</context>
</contexts>
<marker>Gillick, Cox, 1989</marker>
<rawString>L. Gillick and S. Cox. 1989. Some statistical issues in the comparison of speech recognition algorithms. In Proceedings of ICASSP, pages 532–535.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Jurafsky</author>
<author>E Shriberg</author>
<author>D Biasca</author>
</authors>
<title>Switchboard SWBD-DAMSL labeling project coder’s manual, draft 13.</title>
<date>1997</date>
<tech>Technical report,</tech>
<institution>Univ. of Colorado Institute of Cognitive Science.</institution>
<contexts>
<context position="3023" citStr="Jurafsky et al., 1997" startWordPosition="471" endWordPosition="474">al., 2006; Ros´e et al., 2008). In natural language processing, Cohen et al. (2004) and Feng et al. (2006) used speech acts to capture the intentional focus of emails and discussion boards. However, they assume that enough labeled data are available for developing speech act recognition models. A main contribution of this paper is that we address the problem of learning speech act recognition in a semi-supervised way. To our knowledge, this is the first use of semi-supervised speech act recognition in emails and online forums. To do this, we make use of labeled data from spoken conversations (Jurafsky et al., 1997; Dhillon et al., 2004). A second contribution is that our model learns subtree features that constitute discriminative patterns: for example, variable length n-grams and partial dependency structures. Therefore, our model can capture both local features such as n-grams and non-local dependencies. In this paper, we extend subtree pattern mining to the semi-supervised learning problem. This paper is structured as follows. Section 2 reviews prior work on speech act recognition and Section 3 presents the problem statement and our data sets. Section 4 describes a supervised method of learning subt</context>
<context position="5620" citStr="Jurafsky et al., 1997" startWordPosition="880" endWordPosition="884"> based on the speech acts of messages in discussion boards. Extending Feng et al. (2006)’s work, Ravi and Kim (2007) applied speech act classification to detect unanswered questions. However, none of these studies have focused on the semisupervised speech act recognition problem and examined their methods across different genres. The speech processing community frequently employs two large-scale corpora for speech act annotation: Switchboard-DAMSL (SWBD) and Meeting Recorder Dialog Act (MRDA). SWBD is an annotation scheme and collection of labeled dialog act2 data for telephone conversations (Jurafsky et al., 1997). The main purpose of SWBD is to acquire stochastic discourse grammars for training better language models for automatic speech recognition. More recently, an MRDA corpus has been adapted from SWBD but its tag set for labeling meetings has been modified to better reflect the types of interaction in multi-party face-to-face meetings (Dhillon et al., 2004). These two corpora have been extensively studied, e.g., (Stolcke et al., 2000; Ang et al., 2005; Galley et al., 2004). We also use these for our experiments. 2A dialog act is the meaning of an utterance at the level of illocutionary force (Aus</context>
<context position="11119" citStr="Jurafsky et al., 1997" startWordPosition="1837" endWordPosition="1840">Z1 as the set of labeled training data, and DU = {xi}&apos;L+1 as the set of unlabeled training data where l = n + m and m is the number of unlabeled data instances. Our goal is to find a learning method to minimize the classification errors in DL and DU. 3.2 Data Preparation In this paper, we separate labeled (DL) and unlabeled data (DU). First we use SWBD3 and MRDA4 as our labeled data. We automatically 3LDC Catalog No. LDC97S62 4http://www.icsi.berkeley.edu/—ees/dadb/ map original annotations in SWBD and MRDA to one of the 12 speech acts.5 Inter-annotator agreement K in both data sets is — 0.8 (Jurafsky et al., 1997; Dhillon et al., 2004). For evaluation purposes, we divide labeled data into three sets: training, development, and evaluation sets (Table 2). Of the 1,155 available conversations in the SWBD corpus, we use 855 for training, 100 for development, and 200 for evaluation. Among the 75 available meetings in the MRDA corpus, we exclude two meetings of different natures (btr001 and btr002). Of the remaining meetings, we use 59 for training, 6 for development, and 8 for evaluation. Then we merge multi-segments utterances that belong to the same speaker and then divide all data sets into sentences. A</context>
</contexts>
<marker>Jurafsky, Shriberg, Biasca, 1997</marker>
<rawString>D. Jurafsky, E. Shriberg, and D. Biasca. 1997. Switchboard SWBD-DAMSL labeling project coder’s manual, draft 13. Technical report, Univ. of Colorado Institute of Cognitive Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kudo</author>
<author>Y Matsumoto</author>
</authors>
<title>A boosting algorithm for classification of semi-structured text.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>301--308</pages>
<contexts>
<context position="14185" citStr="Kudo and Matsumoto (2004)" startWordPosition="2331" endWordPosition="2334">“What site should we use to book a Beijing-Chonqing flight?” can be predicted by two discriminative features, “(&lt;s&gt;, WRB) —* QW” and “(?, &lt;/s&gt;) —* QW” where &lt;s&gt; and &lt;/s&gt; are sentence start and end symbols, and WRB is a part-of-speech tag that denotes a Wh-adverb. In addition, useful features could be of various lengths, i.e. not fixed length n-grams, and nonadjacent. One key idea of this paper is a novel use of subtree features to model these for speech act recognition. 4.1 Exploiting Subtree Features To exploit subtree features in our model, we use a subtree pattern mining method proposed by Kudo and Matsumoto (2004). We briefly introduce this algorithm here. In Section 3.1, we defined x = {xj} as the forest that is a set of trees. More precisely, xj is a labeled ordered tree where each node has its own label and is ordered leftto-right. Several types of labeled ordered trees Figure 3: Representations of tree: (a) bag-ofwords, (b) n-gram, (c) word pair, and (d) dependency tree. A node denotes a word and a directed edge indicates a parent-and-child relationship. are possible (Figure 3). Note that S-expression can be used instead for computation, for example (a(b(c(d)))) for the n-gram (Figure 3(b)). Moreov</context>
<context position="15651" citStr="Kudo and Matsumoto (2004)" startWordPosition="2589" endWordPosition="2592"> Figure 3(b). We define the subtree feature as a weak learner: � +y t C x, f(y, t, x) � (1) −y otherwise, where we assume a binary case y E Y = {+1, −1} for simplicity. Even though the approach in Kudo and Matsumoto (2004) and ours are simiar, there are two clear distinctions. First, our method employs multiple tree structures, and uses different constraints to generate subtree candidates. In this paper, we only restrict generating 1253 the dependency subtrees which should have 3 or more nodes. Second, our method is of interest for semi-supervised learning problems. To learn subtree features, Kudo and Matsumoto (2004) assumed supervised data {(xi, yi)1. Here, we describe the supervised learning method and will describe our semi-supervised method in Section 5. 4.2 Supervised Boosting Learning Given training examples, we construct a ensemble learner F(x) = Pk λkf(yk, tk, x), where λk is a coefficient for linear combination. A final classifier h(x) can be derived from the ensemble learner, i.e., h(x) °= sgn (F(x)). As an optimization framework (Mason et al., 2000), the objective of boosting learning is to find F such that the cost of functional C(F) = X αiC[yiF(xi)] (2) iED is minimized for some non-negative </context>
<context position="18065" citStr="Kudo and Matsumoto, 2004" startWordPosition="3010" endWordPosition="3013">AXENT: maximum entropy model; BOW: bag-of-words model; NGRAM: n-gram model; +POSTAG, +DEPTREE, +SPEAKER indicate that the components were added individually onto NGRAM. ‘*’ indicates results significantly better than the NGRAM model (p &lt; 0.001). Model SWBD MRDA MAXENT 92.76 (63.54) 82.48 (57.19) BOW 91.32 (54.47) 82.17 (55.42) NGRAM 92.60 (58.43) 83.30 (57.53) +POSTAG 92.69 (60.07) 83.60 (58.46) +DEPTREE 92.67 (61.75) *83.57 (57.45) +SPEAKER *92.86 (63.13) 83.40 (58.20) ALL *92.87 (63.77) 83.49 (59.04) The subtree set is efficiently enumerated using a branch-and-bound procedure based on µ(t) (Kudo and Matsumoto, 2004). After finding an optimal base leaner, f(y, t, x), we need to set the coefficient λk to form a new ensemble, F(xi) — F(xi) + λkf(�t, y, xi). In AdaBoost, we choose 1 1 + gain(t, y)1 λk = 2 log µ1 − gain(t, y) / (6) After K iterations, the boosting algorithm returns the ensemble learner F(x) which consists of a set of appropriate base learners f(y, t, x). 4.3 Evaluation on Labeled Data We verified the effectiveness of using subtree features on the SWBD and MRDA data sets. For boosting learning, one typically assumes αi = 1. In addition, the number of iterations, which relates to the number of </context>
</contexts>
<marker>Kudo, Matsumoto, 2004</marker>
<rawString>T. Kudo and Y. Matsumoto. 2004. A boosting algorithm for classification of semi-structured text. In Proceedings of EMNLP, pages 301–308.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Levinson</author>
</authors>
<date>1983</date>
<publisher>Pragmatics. Cambridge Univ. Press,</publisher>
<location>Cambridge.</location>
<marker>Levinson, 1983</marker>
<rawString>S. Levinson. 1983. Pragmatics. Cambridge Univ. Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Mason</author>
<author>P Bartlett</author>
<author>J Baxter</author>
<author>M Frean</author>
</authors>
<title>Functional gradient techniques for combining hypotheses.</title>
<date>2000</date>
<booktitle>Advances in Large Margin Classifiers,</booktitle>
<pages>221--246</pages>
<editor>In A.J. Smola, P.L. Bartlett, B. Sch¨olkopf, and D. Schuurmans, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="16103" citStr="Mason et al., 2000" startWordPosition="2665" endWordPosition="2668">btrees which should have 3 or more nodes. Second, our method is of interest for semi-supervised learning problems. To learn subtree features, Kudo and Matsumoto (2004) assumed supervised data {(xi, yi)1. Here, we describe the supervised learning method and will describe our semi-supervised method in Section 5. 4.2 Supervised Boosting Learning Given training examples, we construct a ensemble learner F(x) = Pk λkf(yk, tk, x), where λk is a coefficient for linear combination. A final classifier h(x) can be derived from the ensemble learner, i.e., h(x) °= sgn (F(x)). As an optimization framework (Mason et al., 2000), the objective of boosting learning is to find F such that the cost of functional C(F) = X αiC[yiF(xi)] (2) iED is minimized for some non-negative and monotonically decreasing cost function C : R -* R and the weight αi E R+. In this paper, we use the AdaBoost algorithm (Schapire and Singer, 1999); thus the cost function is defined as C(z) = e−z. Constructing an ensemble learner requires that the user choose a base learner, f(y, t, x), to maximize the inner product −(VC(F), f) (Mason et al., 2000). Finding f(y, t, x) to maximize −(VC(F), f) is equivalent to searching for f(y, t, x) to minimize</context>
</contexts>
<marker>Mason, Bartlett, Baxter, Frean, 2000</marker>
<rawString>L. Mason, P. Bartlett, J. Baxter, and M. Frean. 2000. Functional gradient techniques for combining hypotheses. In A.J. Smola, P.L. Bartlett, B. Sch¨olkopf, and D. Schuurmans, editors, Advances in Large Margin Classifiers, pages 221–246. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K McKeown</author>
<author>L Shrestha</author>
<author>O Rambow</author>
</authors>
<title>Using question-answer pairs in extractive summarization of email conversations.</title>
<date>2007</date>
<booktitle>In Proceedings of CICLing,</booktitle>
<volume>4394</volume>
<pages>542--550</pages>
<contexts>
<context position="7978" citStr="McKeown et al., 2007" startWordPosition="1280" endWordPosition="1283">e speech act types. Figure 1 shows two examples of our email and forum speech act recognition. E1-6 are all sentences in an email message. F1-3, F4-5, and F6 are three posts in a forum thread. A sentence interacts alone or with others, for example, F6 agrees with the previous post (F4-5). To gain insight into our work, it is useful to consider that E2, 3 and F1, 4, 6 are summaries of two discourses. In particular, F1 denotes a question and F4 and F6 are corresponding answers. More recently, using speech acts has become an appealing approach in summarizing the discussions (Galley et al., 2004; McKeown et al., 2007). Next, we define speech act category based on MRDA. Dhillon et al. (2004) included definitions of speech acts for colloquial style interactions (e.g., backchannel, disruption, and floorgrabber), but these are not applicable in emails and forums. After removing these categories, we define 12 tags (Table 1). Dhillon et al. (2004) provides detailed descriptions of each tag. We note that our tag set definition is different from (Cohen et al., 2004; Feng et al., 2006; Ravi and Kim, 2007) for two reasons. First, prior work primarily interested in the domain-specific speech acts, but our work use do</context>
</contexts>
<marker>McKeown, Shrestha, Rambow, 2007</marker>
<rawString>K. McKeown, L. Shrestha, and O. Rambow. 2007. Using question-answer pairs in extractive summarization of email conversations. In Proceedings of CICLing, volume 4394 of Lecture Notes in Computer Science, pages 542–550.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nastri</author>
<author>J Pe na</author>
<author>J T Hancock</author>
</authors>
<title>The construction of away messages: A speech act analysis.</title>
<date>2006</date>
<journal>Journal of Computer-Mediated Communication,</journal>
<volume>11</volume>
<issue>4</issue>
<pages>7</pages>
<contexts>
<context position="2411" citStr="Nastri et al., 2006" startWordPosition="367" endWordPosition="370">rk was conducted during the author’s internship at Microsoft Research Asia. 1http://tripadvisor.com/ in this paper) and large unlabeled data sets can be collected from the Web. Thus, we focus on the problem of how to accurately recognize speech acts in emails and forums by making maximum use of data from existing resources. Recently, there are increasing interests in speech act recognition of online text-based conversations. Analysis of speech acts for online chat and instant messages and have been studied in computer-mediated communication (CMC) and distance learning (Twitchell et al., 2004; Nastri et al., 2006; Ros´e et al., 2008). In natural language processing, Cohen et al. (2004) and Feng et al. (2006) used speech acts to capture the intentional focus of emails and discussion boards. However, they assume that enough labeled data are available for developing speech act recognition models. A main contribution of this paper is that we address the problem of learning speech act recognition in a semi-supervised way. To our knowledge, this is the first use of semi-supervised speech act recognition in emails and online forums. To do this, we make use of labeled data from spoken conversations (Jurafsky </context>
<context position="4590" citStr="Nastri et al., 2006" startWordPosition="715" endWordPosition="718">6-7 August 2009. c�2009 ACL and AFNLP line forum thread data. Section 7 concludes this paper with future work. 2 Related Work Speech act theory is fundamental to many studies in discourse analysis and pragmatics (Austin, 1962; Searle, 1969). A speech act is an illocutionary act of conversation and reflects shallow discourse structures of language. Recent research on spoken dialog processing has investigated computational speech act models of humanhuman and human-computer conversations (Stolcke et al., 2000) and applications of these models to CMC and distance learning (Twitchell et al., 2004; Nastri et al., 2006; Ros´e et al., 2008). Our work in this paper is closely related to prior work on email and forum speech act recognition. Cohen et al. (2004) proposed the notion of ‘email speech act’ for classifying the intent of an email sender. They defined verb and noun categories for email speech acts and used supervised learning to recognize them. Feng et al. (2006) presented a method of detecting conversation focus based on the speech acts of messages in discussion boards. Extending Feng et al. (2006)’s work, Ravi and Kim (2007) applied speech act classification to detect unanswered questions. However, </context>
</contexts>
<marker>Nastri, na, Hancock, 2006</marker>
<rawString>J. Nastri, J. Pe na, and J. T. Hancock. 2006. The construction of away messages: A speech act analysis. Journal of Computer-Mediated Communication, 11(4):article 7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ravi</author>
<author>J Kim</author>
</authors>
<title>Profiling student interactions in threaded discussions with speech act classifiers.</title>
<date>2007</date>
<booktitle>In Proceedings of the AI in Education Conference.</booktitle>
<contexts>
<context position="5114" citStr="Ravi and Kim (2007)" startWordPosition="807" endWordPosition="810">tions of these models to CMC and distance learning (Twitchell et al., 2004; Nastri et al., 2006; Ros´e et al., 2008). Our work in this paper is closely related to prior work on email and forum speech act recognition. Cohen et al. (2004) proposed the notion of ‘email speech act’ for classifying the intent of an email sender. They defined verb and noun categories for email speech acts and used supervised learning to recognize them. Feng et al. (2006) presented a method of detecting conversation focus based on the speech acts of messages in discussion boards. Extending Feng et al. (2006)’s work, Ravi and Kim (2007) applied speech act classification to detect unanswered questions. However, none of these studies have focused on the semisupervised speech act recognition problem and examined their methods across different genres. The speech processing community frequently employs two large-scale corpora for speech act annotation: Switchboard-DAMSL (SWBD) and Meeting Recorder Dialog Act (MRDA). SWBD is an annotation scheme and collection of labeled dialog act2 data for telephone conversations (Jurafsky et al., 1997). The main purpose of SWBD is to acquire stochastic discourse grammars for training better lan</context>
<context position="8466" citStr="Ravi and Kim, 2007" startWordPosition="1358" endWordPosition="1361">ntly, using speech acts has become an appealing approach in summarizing the discussions (Galley et al., 2004; McKeown et al., 2007). Next, we define speech act category based on MRDA. Dhillon et al. (2004) included definitions of speech acts for colloquial style interactions (e.g., backchannel, disruption, and floorgrabber), but these are not applicable in emails and forums. After removing these categories, we define 12 tags (Table 1). Dhillon et al. (2004) provides detailed descriptions of each tag. We note that our tag set definition is different from (Cohen et al., 2004; Feng et al., 2006; Ravi and Kim, 2007) for two reasons. First, prior work primarily interested in the domain-specific speech acts, but our work use domain-independent speech act tags. Second, we focus on speech act recognition on the sentencelevel. 1251 E1: I am planning my schedule at CHI 2003 (http://www.chi2003.org/) S E2: - will there be anything happening at the conference related to this W3C User interest group? QY E3: I do not see anything on the program yet, but I suspect we could at least have an informal SIG S E4: - a chance to meet others and bring someone like me up to speed on what is happening. S E5: There will be ma</context>
<context position="13407" citStr="Ravi and Kim, 2007" startWordPosition="2200" endWordPosition="2203">d unlabeled data Set SWBD MRDA Set EMAIL FORUM Training 96,553 50,865 Development 12,299 8,366 Evaluation 24,264 10,492 Unlabeled 122,125 297,017 Evaluation 2,267 3,711 Figure 2: Distribution of speech acts in the evaluation sets. Tags are defined in Table 1. frequent tag is the statement (S) tag (Figure 2). Distributions of tags are similar in training and development sets of SWBD and MRDA. 4 Speech Act Recognition Previous work in speech act recognition used a large set of lexical features, e.g., bag-of-words, bigrams and trigrams (Stolcke et al., 2000; Cohen et al., 2004; Ang et al., 2005; Ravi and Kim, 2007). However, these methods create a large number of lexical features that might not be necessary for speech act identification. For example, a Wh-question “What site should we use to book a Beijing-Chonqing flight?” can be predicted by two discriminative features, “(&lt;s&gt;, WRB) —* QW” and “(?, &lt;/s&gt;) —* QW” where &lt;s&gt; and &lt;/s&gt; are sentence start and end symbols, and WRB is a part-of-speech tag that denotes a Wh-adverb. In addition, useful features could be of various lengths, i.e. not fixed length n-grams, and nonadjacent. One key idea of this paper is a novel use of subtree features to model these </context>
</contexts>
<marker>Ravi, Kim, 2007</marker>
<rawString>S. Ravi and J. Kim. 2007. Profiling student interactions in threaded discussions with speech act classifiers. In Proceedings of the AI in Education Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Ros´e</author>
<author>Y Wang</author>
<author>Y Cui</author>
<author>J Arguello</author>
<author>K Stegmann</author>
<author>A Weinberger</author>
<author>F Fischer</author>
</authors>
<title>Analyzing collaborative learning processes automatically: Exploiting the advances of computational linguistics in computer-supported collaborative learning.</title>
<date>2008</date>
<journal>International Journal of Computer-Supported Collaborative Learning,</journal>
<volume>3</volume>
<issue>3</issue>
<marker>Ros´e, Wang, Cui, Arguello, Stegmann, Weinberger, Fischer, 2008</marker>
<rawString>C. Ros´e, Y. Wang, Y. Cui, J. Arguello, K. Stegmann, A. Weinberger, and F. Fischer. 2008. Analyzing collaborative learning processes automatically: Exploiting the advances of computational linguistics in computer-supported collaborative learning. International Journal of Computer-Supported Collaborative Learning, 3(3):237–271.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R E Schapire</author>
<author>Y Singer</author>
</authors>
<title>Improved boosting algorithms using confidence-rated predictions.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>37</volume>
<issue>3</issue>
<contexts>
<context position="16401" citStr="Schapire and Singer, 1999" startWordPosition="2720" endWordPosition="2723">method in Section 5. 4.2 Supervised Boosting Learning Given training examples, we construct a ensemble learner F(x) = Pk λkf(yk, tk, x), where λk is a coefficient for linear combination. A final classifier h(x) can be derived from the ensemble learner, i.e., h(x) °= sgn (F(x)). As an optimization framework (Mason et al., 2000), the objective of boosting learning is to find F such that the cost of functional C(F) = X αiC[yiF(xi)] (2) iED is minimized for some non-negative and monotonically decreasing cost function C : R -* R and the weight αi E R+. In this paper, we use the AdaBoost algorithm (Schapire and Singer, 1999); thus the cost function is defined as C(z) = e−z. Constructing an ensemble learner requires that the user choose a base learner, f(y, t, x), to maximize the inner product −(VC(F), f) (Mason et al., 2000). Finding f(y, t, x) to maximize −(VC(F), f) is equivalent to searching for f(y, t, x) to minimize 2 Pi:f(y,t,xi)�=yi wi − 1, where wi for i E DL, is the empirical data distribution w(k) i at step k. It is defined as: (k) wi = αi e−yiF (xi). (3) From Eq. 3, a proper base learner (i.e., subtree) can be found by maximizing weighted gain, where gain(t, y) = X yiwif(y, t, xi). (4) iEDL Thus, subtr</context>
</contexts>
<marker>Schapire, Singer, 1999</marker>
<rawString>R.E. Schapire and Y. Singer. 1999. Improved boosting algorithms using confidence-rated predictions. Machine Learning, 37(3):297–336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Searle</author>
</authors>
<title>Speech Acts.</title>
<date>1969</date>
<publisher>Cambridge Univ. Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="4211" citStr="Searle, 1969" startWordPosition="655" endWordPosition="656">d method of learning subtree features that shows the effectiveness of subtree features on labeled data sets. Section 5 proposes semi-supervised learning techniques for speech act recognition and Section 6 demonstrates our method applied to email and on1250 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1250–1259, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP line forum thread data. Section 7 concludes this paper with future work. 2 Related Work Speech act theory is fundamental to many studies in discourse analysis and pragmatics (Austin, 1962; Searle, 1969). A speech act is an illocutionary act of conversation and reflects shallow discourse structures of language. Recent research on spoken dialog processing has investigated computational speech act models of humanhuman and human-computer conversations (Stolcke et al., 2000) and applications of these models to CMC and distance learning (Twitchell et al., 2004; Nastri et al., 2006; Ros´e et al., 2008). Our work in this paper is closely related to prior work on email and forum speech act recognition. Cohen et al. (2004) proposed the notion of ‘email speech act’ for classifying the intent of an emai</context>
</contexts>
<marker>Searle, 1969</marker>
<rawString>J. Searle. 1969. Speech Acts. Cambridge Univ. Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Shasha</author>
<author>K Zhang</author>
</authors>
<title>Fast algorithms for the unit cost editing distance between trees.</title>
<date>1990</date>
<journal>Journal of Algorithms,</journal>
<volume>11</volume>
<issue>4</issue>
<contexts>
<context position="22193" citStr="Shasha and Zhang, 1990" startWordPosition="3699" endWordPosition="3702">d email and forum data. We describe here two methods of semisupervised learning. 5.1 Method 1: Bootstrapping First, we bootstrap the BASELINE model using automatically predicted unlabeled examples. However, using all of the unlabeled data results in noisy models; therefore filtering or selecting data is very important in practice. To this end, we only select similar examples by criterion, d(xi, xj) &lt; r or k nearest neighbors where xi E DL and xj E DU. In practice, r or k are fixed. In our method, examples are represented by trees; hence we use a “tree edit distance” for calculating d(xi, xj) (Shasha and Zhang, 1990). Selected examples are evaluated using BASELINE, and using subtree pattern mining runs on the augmented data (i.e. unlabeled). We call this method BOOTSTRAP. 5.2 Method 2: Semi-supervised Boosting Our second method is based on a principle of semi-supervised boosting learning (Bennett et al., 2002). Because we have no supervised guidance for DU, our objective functional to find F is defined as: C(F) = � αiC[yiF(xi)] + � 0iC[|F(xi)|] iEDL iEDU (7) This cost functional is non-differentiable. To solve it, we introduce pseudo-labels y� where y� = sgn(F(x)) and |F(x) |= yF(x). Using the same deriva</context>
</contexts>
<marker>Shasha, Zhang, 1990</marker>
<rawString>D. Shasha and K. Zhang. 1990. Fast algorithms for the unit cost editing distance between trees. Journal of Algorithms, 11(4):581–621.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Soricut</author>
<author>D Marcu</author>
</authors>
<title>Sentence level discourse parsing using syntactic and lexical information.</title>
<date>2003</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>149--156</pages>
<marker>Soricut, Marcu, 2003</marker>
<rawString>R. Soricut and D. Marcu. 2003. Sentence level discourse parsing using syntactic and lexical information. In Proceedings of NAACL-HLT, pages 149– 156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
<author>K Ries</author>
<author>N Coccaro</author>
<author>E Shriberg</author>
<author>R Bates</author>
<author>D Jurafsky</author>
<author>P Taylor</author>
<author>R Martin</author>
<author>C Van EssDykema</author>
<author>M Meteer</author>
</authors>
<title>Dialogue act modeling for automatic tagging and recognition of conversational speech.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>3</issue>
<marker>Stolcke, Ries, Coccaro, Shriberg, Bates, Jurafsky, Taylor, Martin, Van EssDykema, Meteer, 2000</marker>
<rawString>A. Stolcke, K. Ries, N. Coccaro, E. Shriberg, R. Bates, D. Jurafsky, P. Taylor, R. Martin, C. Van EssDykema, and M. Meteer. 2000. Dialogue act modeling for automatic tagging and recognition of conversational speech. Computational Linguistics, 26(3):339–373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Tur</author>
<author>D Hakkani-T¨ur</author>
<author>R E Schapire</author>
</authors>
<title>Combining active and semi-supervised learning for spoken language understanding.</title>
<date>2005</date>
<journal>Speech Communication,</journal>
<volume>45</volume>
<issue>2</issue>
<marker>Tur, Hakkani-T¨ur, Schapire, 2005</marker>
<rawString>G. Tur, D. Hakkani-T¨ur, and R. E. Schapire. 2005. Combining active and semi-supervised learning for spoken language understanding. Speech Communication, 45(2):171–186.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D P Twitchell</author>
<author>J F Nunamaker</author>
<author>J K Burgoon</author>
</authors>
<title>Using speech act profiling for deception detection.</title>
<date>2004</date>
<booktitle>In Second Symposium on Intelligence and Security Informatics,</booktitle>
<volume>3073</volume>
<pages>403--410</pages>
<contexts>
<context position="2390" citStr="Twitchell et al., 2004" startWordPosition="363" endWordPosition="366">g conversations *This work was conducted during the author’s internship at Microsoft Research Asia. 1http://tripadvisor.com/ in this paper) and large unlabeled data sets can be collected from the Web. Thus, we focus on the problem of how to accurately recognize speech acts in emails and forums by making maximum use of data from existing resources. Recently, there are increasing interests in speech act recognition of online text-based conversations. Analysis of speech acts for online chat and instant messages and have been studied in computer-mediated communication (CMC) and distance learning (Twitchell et al., 2004; Nastri et al., 2006; Ros´e et al., 2008). In natural language processing, Cohen et al. (2004) and Feng et al. (2006) used speech acts to capture the intentional focus of emails and discussion boards. However, they assume that enough labeled data are available for developing speech act recognition models. A main contribution of this paper is that we address the problem of learning speech act recognition in a semi-supervised way. To our knowledge, this is the first use of semi-supervised speech act recognition in emails and online forums. To do this, we make use of labeled data from spoken con</context>
<context position="4569" citStr="Twitchell et al., 2004" startWordPosition="711" endWordPosition="714">s 1250–1259, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP line forum thread data. Section 7 concludes this paper with future work. 2 Related Work Speech act theory is fundamental to many studies in discourse analysis and pragmatics (Austin, 1962; Searle, 1969). A speech act is an illocutionary act of conversation and reflects shallow discourse structures of language. Recent research on spoken dialog processing has investigated computational speech act models of humanhuman and human-computer conversations (Stolcke et al., 2000) and applications of these models to CMC and distance learning (Twitchell et al., 2004; Nastri et al., 2006; Ros´e et al., 2008). Our work in this paper is closely related to prior work on email and forum speech act recognition. Cohen et al. (2004) proposed the notion of ‘email speech act’ for classifying the intent of an email sender. They defined verb and noun categories for email speech acts and used supervised learning to recognize them. Feng et al. (2006) presented a method of detecting conversation focus based on the speech acts of messages in discussion boards. Extending Feng et al. (2006)’s work, Ravi and Kim (2007) applied speech act classification to detect unanswered</context>
</contexts>
<marker>Twitchell, Nunamaker, Burgoon, 2004</marker>
<rawString>D. P. Twitchell, J. F. Nunamaker, and J. K. Burgoon. 2004. Using speech act profiling for deception detection. In Second Symposium on Intelligence and Security Informatics, volume 3073 of Lecture Notes in Computer Science, pages 403–410.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>