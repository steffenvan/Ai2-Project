<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005050">
<title confidence="0.919549">
SVM Model Tampering and Anchored Learning: A Case Study in Hebrew
NP Chunking
</title>
<author confidence="0.995043">
Yoav Goldberg and Michael Elhadad
</author>
<affiliation confidence="0.9747805">
Computer Science Department
Ben Gurion University of the Negev
</affiliation>
<address confidence="0.778663">
P.O.B 653 Be’er Sheva 84105, Israel
</address>
<email confidence="0.996819">
yoavg,elhadad@cs.bgu.ac.il
</email>
<sectionHeader confidence="0.998587" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999305">
We study the issue of porting a known NLP
method to a language with little existing NLP
resources, specifically Hebrew SVM-based
chunking. We introduce two SVM-based
methods – Model Tampering and Anchored
Learning. These allow fine grained analysis
of the learned SVM models, which provides
guidance to identify errors in the training cor-
pus, distinguish the role and interaction of
lexical features and eventually construct a
model with —10% error reduction. The re-
sulting chunker is shown to be robust in the
presence of noise in the training corpus, relies
on less lexical features than was previously
understood and achieves an F-measure perfor-
mance of 92.2 on automatically PoS-tagged
text. The SVM analysis methods also provide
general insight on SVM-based chunking.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999948047619048">
While high-quality NLP corpora and tools are avail-
able in English, such resources are difficult to obtain
in most other languages. Three challenges must be
met when adapting results established in English to
another language: (1) acquiring high quality anno-
tated data; (2) adapting the English task definition
to the nature of a different language, and (3) adapt-
ing the algorithm to the new language. This paper
presents a case study in the adaptation of a well
known task to a language with few NLP resources
available. Specifically, we deal with SVM based He-
brew NP chunking. In (Goldberg et al., 2006), we
established that the task is not trivially transferable
to Hebrew, but reported that SVM based chunking
(Kudo and Matsumoto, 2000) performs well. We
extend that work and study the problem from 3 an-
gles: (1) how to deal with a corpus that is smaller
and with a higher level of noise than is available in
English; we propose techniques that help identify
‘suspicious’ data points in the corpus, and identify
how robust the model is in the presence of noise;
(2) we compare the task definition in English and in
Hebrew through quantitative evaluation of the differ-
ences between the two languages by analyzing the
relative importance of features in the learned SVM
models; and (3) we analyze the structure of learned
SVM models to better understand the characteristics
of the chunking problem in Hebrew.
While most work on chunking with machine
learning techniques tend to treat the classification
engine as a black-box, we try to investigate the re-
sulting classification model in order to understand
its inner working, strengths and weaknesses. We in-
troduce two SVM-based methods – Model Tamper-
ing and Anchored Learning – and demonstrate how
a fine-grained analysis of SVM models provides in-
sights on all three accounts. The understanding of
the relative contribution of each feature in the model
helps us construct a better model, which achieves
—10% error reduction in Hebrew chunking, as well
as identify corpus errors. The methods also provide
general insight on SVM-based chunking.
</bodyText>
<sectionHeader confidence="0.993463" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.6543735">
NP chunking is the task of marking the bound-
aries of simple noun-phrases in text. It is a well
studied problem in English, and was the focus of
CoNLL2000’s Shared Task (Sang and Buchholz,
</bodyText>
<page confidence="0.965258">
224
</page>
<note confidence="0.9260425">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 224–231,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.997012397058823">
2000). Early attempts at NP Chunking were rule consistencies and tagging errors. In addition, the
learning systems, such as the Error Driven Prun- identification of SimpleNPs from the tree bank also
ing method of Pierce and Cardie (1998). Follow- introduces some errors. Finally, we want to investi-
ing Ramshaw and Marcus (1995), the current dom- gate chunking in a scenario where PoS tags are as-
inant approach is formulating chunking as a clas- signed automatically and chunks are then computed.
sification task, in which each word is classified as The Hebrew PoS tagger we use introduces about 8%
the (B)eginning, (I)nside or (O)outside of a chunk. errors (compared with about 4% in English). We
Features for this classification usually involve local are, therefore, interested in identifying errors in the
context features. Kudo and Matsumoto (2000) used chunking corpus, and investigating how the chunker
SVM as a classification engine and achieved an F- operates in the presence of noise in the PoS tag se-
Score of 93.79 on the shared task NPs. Since SVM quence.
is a binary classifier, to use it for the 3-class classi- 3 Model Tampering
fication of the chunking task, 3 different classifiers 3.1 Notation and Technical Review
{B/I, B/O, I/O} were trained and their majority vote This section presents notation as well as a technical
was taken. review of SVM chunking details relevant to the cur-
NP chunks in the shared task data are BaseNPs, rent study. Further details can be found in Kudo and
which are non-recursive NPs, a definition first pro- Matsumoto (2000; 2003).
posed by Ramshaw and Marcus (1995). This defini- SVM (Vapnik, 1995) is a supervised binary clas-
tion yields good NP chunks for English. In (Gold- sifier. The input to the learner is a set of l train-
berg et al., 2006) we argued that it is not applica- ing samples (x1, y1), ... , (xl, yl), x ∈ Rn, y ∈
ble to Hebrew, mainly because of the prevalence {+1, −1}. xi is an n dimensional feature vec-
of the Hebrew’s construct state (smixut). 5mixut tor representing the ith sample, and yi is the la-
is similar to a noun-compound construct, but one bel for that sample. The result of the learning pro-
that can join a noun (with a special morphologi- cess is the set 5V of Support Vectors, the asso-
cal marking) with a full NP. It appears in about ciated weights αi, and a constant b. The Support
40% of Hebrew NPs. We proposed an alterna- Vectors are a subset of the training vectors, and to-
tive definition (termed SimpleNP) for Hebrew NP gether with the weights and b they define a hyper-
chunks. A SimpleNP cannot contain embedded rel- plane that optimally separates the training samples.
atives, prepositions, VPs and NP-conjunctions (ex- The basic SVM formulation is of a linear classifier,
cept when they are licensed by smixut). It can but by introducing a kernel function K that non-
contain smixut, possessives (even when they are linearly transforms the data from Rn into a higher
attached by the ‘לש/of’ preposition) and partitives dimensional space, SVM can be used to perform
(and, therefore, allows for a limited amount of re- non-linear classification. SVM’s decision function
cursion). We applied this definition to the Hebrew (E )
Tree Bank (Sima’an et al., 2001), and constructed is: y(x) = sgn j�SV yjαjK(xj, x) + b where
a moderate size corpus (about 5,000 sentences) for x is an n dimensional feature vector to be classi-
Hebrew SimpleNP chunking. SimpleNPs are differ- fied. In the linear case, K is a dot product oper-
ent than English BaseNPs, and indeed some meth- ation and the sum w = E yjαjxj is an n dimen-
ods that work well for English performed poorly sional weight vector assigning weight for each of
on Hebrew data. However, we found that chunk- the n features. The other kernel function we con-
ing with SVM provides good result for Hebrew Sim- sider in this paper is a polynomial kernel of degree
pleNPs. We analyzed that this success comes from 2: K(xi, xj) = (xi · xj + 1)2. When using binary
SVM’s ability to use lexical features, as well as two valued features, this kernel function essentially im-
Hebrew morphological features, namely “number” plies that the classifier considers not only the explic-
and “construct-state”. itly specified features, but also all available pairs of
One of the main issues when dealing with Hebrew features. In order to cope with inseparable data, the
chunking is that the available tree bank is rather learning process of SVM allows for some misclas-
small, and since it is quite new, and has not been sification, the amount of which is determined by a
used intensively, it contains a certain amount of in-
225
parameter C, which can be thought of as a penalty
for each misclassified training sample.
In SVM based chunking, each word and its con-
text is considered a learning sample. We refer to
the word being classified as w0, and to its part-of-
speech (PoS) tag, morphology, and B/I/O tag as p0,
m0 and t0 respectively. The information consid-
ered for classification is w−cw . . . wcw, p−cp . . . pcp,
m−c,,, . . . mc,,, and t−ct ... t−1. The feature vector
F is an indexed list of all the features present in
the corpus. A feature fi of the form w+1 = dog
means that the word following the one being clas-
sified is ‘dog’. Every learning sample is repre-
sented by an n = IF I dimensional binary vector x.
xi = 1 iff the feature fi is active in the given sample,
and 0 otherwise. This encoding leads to extremely
high dimensional vectors, due to the lexical features
w−cw ... wcw.
</bodyText>
<subsectionHeader confidence="0.956665">
3.2 Introducing Model Tampering
</subsectionHeader>
<bodyText confidence="0.999978388888889">
An important observation about SVM classifiers is
that features which are not active in any of the Sup-
port Vectors have no effect on the classifier deci-
sion. We introduce Model Tampering, a procedure
in which we change the Support Vectors in a model
by forcing some values in the vectors to 0.
The result of this procedure is a new Model in
which the deleted features never take part in the clas-
sification.
Model tampering is different than feature selec-
tion: on the one hand, it is a method that helps us
identify irrelevant features in a model after training;
on the other hand, and this is the key insight, re-
moving features after training is not the same as re-
moving them before training. The presence of the
low-relevance features during training has an impact
on the generalization performed by the learner as
shown below.
</bodyText>
<subsectionHeader confidence="0.999398">
3.3 The Role of Lexical Features
</subsectionHeader>
<bodyText confidence="0.999712333333333">
In Goldberg et al. (2006), we have established that
using lexical features increases the chunking F-
measure from 78 to over 92 on the Hebrew Tree-
bank. We refine this observation by using Model
Tampering, in order to assess the importance of lex-
ical features in NP Chunking. We are interested in
identifying which specific lexical items and contexts
impact the chunking decision, and quantifying their
effect. Our method is to train a chunking model
on a given training corpus, tamper with the result-
ing model in various ways and measure the perfor-
mance1 of the tampered models on a test corpus.
</bodyText>
<subsectionHeader confidence="0.982781">
3.4 Experimental Setting
</subsectionHeader>
<bodyText confidence="0.99999596">
We conducted experiments both for English and He-
brew chunking. For the Hebrew experiments, we use
the corpora of (Goldberg et al., 2006). The first one
is derived from the original Treebank by projecting
the full syntactic tree, constructed manually, onto a
set of NP chunks according to the SimpleNP rules.
We refer to the resulting corpus as HEBGold since
PoS tags are fully reliable. The HEBErr version
of the corpus is obtained by projecting the chunk
boundaries on the sequence of PoS and morphology
tags obtained by the automatic PoS tagger of Adler
&amp; Elhadad (2006). This corpus includes an error
rate of about 8% on PoS tags. The first 500 sen-
tences are used for testing, and the rest for training.
The corpus contains 27K NP chunks. For the En-
glish experiments, we use the now-standard training
and test sets that were introduced in (Marcus and
Ramshaw, 1995)2. Training was done using Kudo’s
YAMCHA toolkit3. Both Hebrew and English mod-
els were trained using a polynomial kernel of de-
gree 2, with C = 1. For English, the features used
were: w−2 ... w2, p−2 . . . p2, t−2 ... t−1. The same
features were used for Hebrew, with the addition of
m−2 ... m2. These are the same settings as in (Kudo
and Matsumoto, 2000; Goldberg et al., 2006).
</bodyText>
<subsectionHeader confidence="0.870047">
3.5 Tamperings
</subsectionHeader>
<bodyText confidence="0.998689307692308">
We experimented with the following tamperings:
TopN – We define model feature count to be the
number of Support Vectors in which a feature is ac-
tive in a given classifier. This tampering leaves in the
model only the top N lexical features in each classi-
fier, according to their count.
NoPOS – all the lexical features corresponding to
a given part-of-speech are removed from the model.
For example, in a NoJJ tampering, all the features of
the form wi = X are removed from all the support
vectors in which pi = JJ is active.
Loc74i – all the lexical features with index i are
removed from the model e.g., in a Loci+2 tamper-
</bodyText>
<footnote confidence="0.9998354">
1The performance metric we use is the standard Preci-
sion/Recall/F measures, as computed by the conlleval program:
http://www.cnts.ua.ac.be/conll2000/chunking/conlleval.txt
2ftp://ftp.cis.upenn.edu/pub/chunker
3http://chasen.org/∼taku/software/yamcha/
</footnote>
<page confidence="0.997982">
226
</page>
<bodyText confidence="0.999070666666667">
ing, features of the form w+2 = X are removed).
Loc=i – all the lexical features with an index other
than i are removed from the model.
</bodyText>
<subsectionHeader confidence="0.863425">
3.6 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.9983275">
Highlights of the results are presented in Tables (1-
3). The numbers reported are F measures.
</bodyText>
<table confidence="0.998031375">
TopN HEBGold HEBE, ENG
ALL 93.58 92.48 93.79
N=0 78.32 76.27 90.10
N=10 90.21 88.68 90.24
N=50 91.78 90.85 91.22
N=100 92.25 91.62 91.72
N=500 93.60 92.23 93.12
N=1000 93.56 92.41 93.30
</table>
<tableCaption confidence="0.999917">
Table 1: Results of TopN Tampering.
</tableCaption>
<bodyText confidence="0.999830241379311">
The results of the TopN tamperings show that for
both languages, most of the lexical features are irrel-
evant for the classification – the numbers achieved
by using all the lexical features (about 30,000 in He-
brew and 75,000 in English) are very close to those
obtained using only a few lexical features. This
finding is very encouraging, and suggests that SVM
based chunking is robust to corpus variations.
Another conclusion is that lexical features help
balance the fact that PoS tags can be noisy: we
know both HEBE, and ENG include PoS tag-
ging errors (about 8% in Hebrew and 4% in En-
glish). While in the case of “perfect” PoS tagging
(HEBGold), a very small amount of lexical features
is sufficient to reach the best F-result (500 out of
30,264), in the presence of PoS errors, more than
the top 1000 lexical features are needed to reach the
result obtained with all lexical features.
More striking is the fact that in Hebrew, the
top 10 lexical features are responsible for an im-
provement of 12.4 in F-score. The words cov-
ered by these 10 features are the following: Start
of Sentence marker and comma, quote,
‘of/לש’, ‘and/ו’, ‘the/ה’ and ‘in/ב’.
This finding suggests that the Hebrew PoS tagset
might not be informative enough for the chunking
task, especially where punctuation 4 and preposi-
tions are concerned. The results in Table 2 give fur-
ther support for this claim.
</bodyText>
<footnote confidence="0.989653">
4Unlike the WSJ PoS tagset in which most punctuations get
unique tags, our tagset treat punctuation marks as one group.
</footnote>
<table confidence="0.99946225">
NoPOS HEBG HEBE NoPOS HEBG HEBE
Prep 85.25 84.40 Pronoun 92.97 92.14
Punct 88.90 87.66 Conjunction 92.31 91.67
Adverb 92.02 90.72 Determiner 92.55 91.39
</table>
<tableCaption confidence="0.995573">
Table 2: Results of Hebrew NoPOS Tampering.
Other scores are &gt; 93.3(HEBG), &gt; 92.2(HEBE).
</tableCaption>
<bodyText confidence="0.999423976190476">
When removing lexical features of a specific
PoS, the most dramatic loss of F-score is reached
for Prepositions and Punctuation marks, followed
by Adverbs, and Conjunctions. Strikingly, lexi-
cal information for most open-class PoS (including
Proper Names and Nouns) has very little impact on
Hebrew chunking performance.
From this observation, one could conclude that
enriching a model based only on PoS with lexical
features for only a few closed-class PoS (prepo-
sitions and punctuation) could provide appropri-
ate results even with a simpler learning method,
one that cannot deal with a large number of fea-
tures. We tested this hypothesis by training the
Error-Driven Pruning (EDP) method of (Cardie and
Pierce, 1998) with an extended set of features. EDP
with PoS features only produced an F-result of 76.3
on HEBGold. By adding lexical features only for
prepositions {מ ב ה כ לש}, one conjunction {ו} and
punctuation, the F-score on HEBGold indeed jumps
to 85.4. However, when applied on HEBE, EDP
falls down again to 59.4. This striking disparity, by
comparison, lets us appreciate the resilience of the
SVM model to PoS tagging errors, and its gener-
alization capability even with a reduced number of
lexical features.
Another implication of this data is that commas
and quotation marks play a major role in deter-
mining NP boundaries in Hebrew. In Goldberg
et al. (2006), we noted the Hebrew Treebank is not
consistent in its treatment of punctuation, and thus
we evaluated the chunker only after performing nor-
malization of chunk boundaries for punctuations.
We now hypothesize that, since commas and quo-
tation marks play such an important role in the clas-
sification, performing such normalization before the
training stage might be beneficial. Indeed results on
the normalized corpus show improvement of about
1.0 in F score on both HEBE,, and HEBGold. A
10-fold cross validation experiment on punctuation
normalized HEBE,, resulted in an F-Score of 92.2,
improving the results reported by (Goldberg et al.,
</bodyText>
<page confidence="0.987061">
227
</page>
<table confidence="0.924830285714286">
2006) on the same setting (91.4).
Loc=I HEBE ENG Loc54I HEBE ENG
-2 78.26 89.79 -2 91.62 93.87
-1 76.96 90.90 -1 91.86 93.03
0 90.33 92.37 0 79.44 91.16
1 76.90 90.47 1 92.33 93.30
2 76.55 90.06 2 92.18 93.65
</table>
<tableCaption confidence="0.999933">
Table 3: Results of Loc Tamperings.
</tableCaption>
<bodyText confidence="0.99994">
We now turn to analyzing the importance of con-
text positions (Table 3). For both languages, the
most important lexical feature (by far) is at position
0, that is, the word currently being classified. For
English, it is followed by positions 1 and -1, and
then positions 2 and -2. For Hebrew, back context
seems to have more effect than front context. In
Hebrew, all the positions positively contribute to the
decision, while in English removing w2/_2 slightly
improves the results (note also that including only
feature w2/_2 performs worse than with no lexical
information in English).
</bodyText>
<subsectionHeader confidence="0.97334">
3.7 The Real Role of Lexical Features
</subsectionHeader>
<bodyText confidence="0.999980836363636">
Model tampering (i.e., removing features after the
learning stage) is not the same as learning without
these features. This claim is verified empirically:
training on the English corpus without the lexical
features at position –2 yields worse results than with
them (93.73 vs. 93.79) – while removing the w_2
features via tampering on a model trained with w_2
yields better results (93.87). Similarly, for all cor-
pora, training using only the top 1,000 features (as
defined in the Top1000 tampering) results in loss of
about 2 in F-Score (ENG 92.02, HEBE, 90.30,
HEBGold 91.67), while tampering Top1000 yields
a result very close to the best obtained (93.56, 92.41
or 93.3F).
This observation leads us to an interesting conclu-
sion about the real role of lexical features in SVM
based chunking: rare events (features) are used to
memorize hard examples. Intuitively, by giving a
heavy weight to rare events, the classifier learns spe-
cific rules such as “if the word at position -2 is X and
the PoS at position 2 is Y, then the current word is
Inside a noun-phrase”. Most of these rules are acci-
dental – there is no real relation between the partic-
ular word-pos combination and the class of the cur-
rent word, it just happens to be this way in the train-
ing samples. Marking the rare occurrences helps the
learner achieve better generalization on the other,
more common cases, which are similar to the outlier
on most features, except the “irrelevant ones”. As
the events are rare, such rules usually have no effect
on chunking accuracy: they simply never occur in
the test data. This observation refines the common
conception that SVM chunking does not suffer from
irrelevant features: in chunking, SVM indeed gener-
alizes well for the common cases but also over-fits
the model on outliers.
Model tampering helps us design a model in two
ways: (1) it is a way to “open the black box” ob-
tained when training an SVM and to analyze the re-
spective importance of features. In our case, this
analysis allowed us to identify the importance of
punctuation and prepositions and improve the model
by defining more focused features (improving over-
all result by ∼1.0 F-point). (2) The analysis also led
us to the conclusion that “feature selection” is com-
plex in the case of SVM – irrelevant features help
prevent over-generalization by forcing over-fitting
on outliers.
We have also confirmed that the model learned re-
mains robust in the presence of noise in the PoS tags
and relies on only few lexical features. This veri-
fication is critical in the context of languages with
few computational resources, as we expect the size
of corpora and the quality of taggers to keep lagging
behind that achieved in English.
</bodyText>
<sectionHeader confidence="0.992861" genericHeader="method">
4 Anchored Learning
</sectionHeader>
<bodyText confidence="0.999933466666667">
We pursue the observation of how SVM deals
with outliers by developing the Anchored Learning
method. The idea behind Anchored Learning is to
add a unique feature az (an anchor) to each training
sample (we add as many new features to the model
as there are training samples). These new features
make our data linearly separable. The SVM learner
can then use these anchors (which will never occur
on the test data) to memorize the hard cases, de-
creasing this burden from “real” features.
We present two uses for Anchored Learning. The
first is the identification of hard cases and corpus er-
rors, and the second is a preliminary feature selec-
tion approach for SVM to improve chunking accu-
racy.
</bodyText>
<subsectionHeader confidence="0.92732">
4.1 Mining for Errors and Hard Cases
</subsectionHeader>
<bodyText confidence="0.9926815">
Following the intuition that SVM gives more weight
to anchor features of hard-to-classify cases, we can
</bodyText>
<page confidence="0.995489">
228
</page>
<bodyText confidence="0.998944936170213">
actively look for such cases by training an SVM
chunker on anchored data (as the anchored data is
guaranteed to be linearly separable, we can set a very
high value to the C parameter, preventing any mis-
classification), and then investigating either the an-
chors whose weights5 are above some threshold t or
the top N heaviest anchors, and their corresponding
corpus locations. These locations are those that
the learner considers hard to classify. They can
be either corpus errors, or genuinely hard cases.
This method is similar to the corpus error detec-
tion method presented by Nakagawa and Matsumoto
(2002). They constructed an SVM model for PoS
tagging, and considered Support Vectors with high
α values to be indicative of suspicious corpus loca-
tions. These locations can be either outliers, or cor-
rectly labeled locations similar to an outlier. They
then looked for similar corpus locations with a dif-
ferent label, to point out right-wrong pairs with high
precision.
Using anchors improves their method in three as-
pects: (1) without anchors, similar examples are of-
ten indistinguishable to the SVM learner, and in case
they have conflicting labels both examples will be
given high weights. That is, both the regular case
and the hard case will be considered as hard exam-
ples. Moreover, similar corpus errors might result
in only one support vector that cover all the group of
similar errors. Anchors mitigate these effects, result-
ing in better precision and recall. (2) The more er-
rors there are in the corpus, the less linearly separa-
ble it is. Un-anchored learning on erroneous corpus
can take unreasonable amount of time. (3) Anchors
allow learning while removing some of the impor-
tant features but still allow the process to converge
in reasonable time. This lets us analyze which cases
become hard to learn if we don’t use certain features,
or in other words: what problematic cases are solved
by specific features.
The hard cases analysis achieved by anchored
learning is different from the usual error analysis
carried out on observed classification errors. The
traditional methods give us intuitions about where
the classifier fails to generalize, while the method
we present here gives us intuition about what the
classifier considers hard to learn, based on the
training examples alone.
</bodyText>
<footnote confidence="0.8712645">
5As each anchor appear in only one support vector, we can
treat the vector’s α value as the anchor weight
</footnote>
<bodyText confidence="0.999885684210526">
The intuition that “hard to learn” examples are
suspect corpus errors is not new, and appears also
in Abney et al. (1999) , who consider the “heaviest”
samples in the final distribution of the AdaBoost al-
gorithm to be the hardest to classify and thus likely
corpus errors. While AdaBoost models are easy to
interpret, this is not the case with SVM. Anchored
learning allows us to extract the hard to learn cases
from an SVM model. Interestingly, while both Ad-
aBoost and SVM are ‘large margin’ based classi-
fiers, there is less than 50% overlap in the hard cases
for the two methods (in terms of mistakes on the test
data, there were 234 mistakes shared by AdaBoost
and SVM, 69 errors unique to SVM and 126 errors
unique to AdaBoost)6. Analyzing the difference in
what the two classifiers consider hard is interesting,
and we will address it in future work. In the current
work, we note that for finding corpus errors the two
methods are complementary.
</bodyText>
<subsectionHeader confidence="0.5561">
Experiment 1 – Locating Hard Cases
</subsectionHeader>
<bodyText confidence="0.999659423076923">
A linear SVM model (Mf,,ll) was trained on
the training subset of the anchored, punctuation-
normalized, HEBGold corpus, with the same fea-
tures as in the previous experiments, and a C value
of 9,999. Corpus locations corresponding to anchors
with weights &gt;1 were inspected. There were about
120 such locations out of 4,500 sentences used in the
training set. Decreasing the threshold t would result
in more cases. We analyzed these locations into 3
categories: corpus errors, cases that challenge the
SimpleNP definition, and cases where the chunking
decision is genuinely difficult to make in the absence
of global syntactic context or world knowledge.
Corpus Errors: The analysis revealed the fol-
lowing corpus errors: we identified 29 hard cases
related to conjunction and apposition (is the comma,
colon or slash inside an NP or separating two distinct
NPs). 14 of these hard cases were indeed mistakes
in the corpus. This was anticipated, as we distin-
guished appositions and conjunctive commas using
heuristics, since the Treebank marking of conjunc-
tions is somewhat inconsistent.
In order to build the Chunk NP corpus, the syn-
tactic trees of the Treebank were processed to derive
chunks according to the SimpleNP definition. The
hard cases analysis identified 18 instances where this
</bodyText>
<footnote confidence="0.9936275">
6These numbers are for pairwise Linear SVM and AdaBoost
classifiers trained on the same features.
</footnote>
<page confidence="0.998277">
229
</page>
<bodyText confidence="0.9997901">
transformation results in erroneous chunks. For ex-
ample, null elements result in improper chunks, such
as chunks containing only adverbs or only adjec-
tives.
We also found 3 invalid sentences, 6 inconsisten-
cies in the tagging of interrogatives with respect to
chunk boundaries, as well as 34 other specific mis-
takes. Overall, more than half of the locations iden-
tified by the anchors were corpus errors. Looking for
cases similar to the errors identified by anchors, we
found 99 more locations, 77 of which were errors.
Refining the SimpleNP Definition: The hard
cases analysis identified examples that challenge
the SimpleNP definition proposed in Goldberg
et al. (2006). The most notable cases are:
The ‘et’ marker : ‘et’ is a syntactic marker of defi-
nite direct objects in Hebrew. It was regarded as a
part of SimpleNPs in their definition. In some cases,
this forces the resulting SimpleNP to be too inclu-
sive:
</bodyText>
<equation confidence="0.5331765">
[תרושקתהו טפשמה תיב תסנכה ,הלשממה תא]
[‘et’ (the government, the parliament and the media)]
</equation>
<bodyText confidence="0.980199406779661">
Because in the Treebank the conjunction depends on
‘et’ as a single constituent, it is fully embedded in
the chunk. Such a conjunction should not be consid-
ered simple.
The לש preposition (‘of’) marks generalized posses-
sion and was considered unambiguous and included
in SimpleNPs. We found cases where ‘לש’ causes
PP attachment ambiguity:
[הרטשמה] לש [תעמשמ] ל [ןידה תיב אישנ]
[president-cons house-cons the-law] for [discipline] of [the
police] / The Police Disciplinary Court President
Because 2 prepositions are involved in this NP, ‘לש’
(of) and ‘ל’ (for), the ‘לש’ part cannot be attached
unambiguously to its head (‘court’). It is unclear
whether the ‘ל’ preposition should be given special
treatment to allow it to enter simple NPs in certain
contexts, or whether the inconsistent handling of
the ‘לש’ that results from the ‘ל’ inter-position is
preferable.
Complex determiners and quanti�ers: In many
cases, complex determiners in Hebrew are multi-
word expressions that include nouns. The inclusion
of such determiners inside the SimpleNPs is not
consistent.
Genuinely hard cases were also identified.
These include prepositions, conjunctions and multi-
word idioms (most of them are adjectives and prepo-
sitions which are made up of nouns and determin-
ers, e.g., as the word unanimously is expressed in
Hebrew as the multi-word expression ‘one mouth’).
Also, some adverbials and adjectives are impossible
to distinguish using only local context.
The anchors analysis helped us improve the
chunking method on two accounts: (1) it identified
corpus errors with high precision; (2) it made us fo-
cus on hard cases that challenge the linguistic defi-
nition of chunks we have adopted. Following these
findings, we intend to refine the Hebrew SimpleNP
definition, and create a new version of the Hebrew
chunking corpus.
Experiment 2 – determining the role of
contextual lexical features
The intent of this experiment is to understand the
role of the contextual lexical features (wi, i 7� 0).
This is done by training 2 additional anchored lin-
ear SVM models, Mno−cont and Mnear. These are
the same as Mfull except for the lexical features
used during training. Mno−cont uses only w0, while
Mnear uses w0,w−1,w+1.
Anchors are again used to locate the hard exam-
ples for each classifier, and the differences are ex-
amined. The examples that are hard for Mnear but
not for Mfull are those solved by w−2,w+2. Sim-
ilarly, the examples that are hard for Mno−cont but
not for Mnear are those solved by w−1,w+1. Table 4
indicates the number of hard cases identified by the
anchor method for each model. One way to inter-
pret these figures, is that the introduction of features
w−1,+1 solves 5 times more hard cases than w−2,+2.
</bodyText>
<table confidence="0.978146">
Model Number of hard Hard cases for
cases (t = 1) classifier B-I
Mfvll 120 2
Mnear 320 (+ 200) 12
Mna_cant 1360 (+ 1040) 164
</table>
<tableCaption confidence="0.999561">
Table 4: Number of hard cases per model type.
</tableCaption>
<bodyText confidence="0.999944363636364">
Qualitative analysis of the hard cases solved by
the contextual lexical features shows that they con-
tribute mostly to the identification of chunk bound-
aries in cases of conjunction, apposition, attachment
of adverbs and adjectives, and some multi-word ex-
pressions.
The number of hard cases specific to the B-I clas-
sifier indicates how the features contribute to the de-
cision of splitting or continuing back-to-back NPs.
Back-to-back NPs amount to 6% of the NPs in
HEBGold and 8% of the NPs in ENG. However,
</bodyText>
<page confidence="0.977858">
230
</page>
<bodyText confidence="0.999959407407408">
while in English most of these cases are easily re-
solved, Hebrew phenomena such as null-equatives
and free word order make them harder. To quantify
the difference: 79% of the first words of the second
NP in English belong to one of the closed classes
POS, DT, WDT, PRP, WP – categories which mostly
cannot appear in the middle of base NPs. In con-
trast, in Hebrew, 59% are Nouns, Numbers or Proper
Names. Moreover, in English the ratio of unique first
words to number of adjacent NPs is 0.068, while in
Hebrew it is 0.47. That is, in Hebrew, almost every
second such NP starts with a different word.
These figures explain why surrounding lexical in-
formation is needed by the learner in order to clas-
sify such cases. They also suggest that this learning
is mostly superficial, that is, the learner just mem-
orizes some examples, but these will not generalize
well on test data. Indeed, the most common class of
errors reported in Goldberg et al. , 2006 are of the
split/merge type. These are followed by conjunction
related errors, which suffer from the same problem.
Morphological features of smixut and agreement can
help to some extent, but this is still a limited solu-
tion. It seems that deciding the [NP][NP] case is
beyond the capabilities of chunking with local con-
text features alone, and more global features should
be sought.
</bodyText>
<subsectionHeader confidence="0.995753">
4.2 Facilitating Better Learning
</subsectionHeader>
<bodyText confidence="0.999993875">
This section presents preliminary results using An-
chored Learning for better NP chunking. We present
a setting (English Base NP chunking) in which
selected features coupled together with anchored
learning show an improvement over previous results.
Section 3.6 hinted that SVM based chunking
might be hurt by using too many lexical features.
Specifically, the features w_2,w+2 were shown to
cause the chunker to overfit in English chunking.
Learning without these features, however, yields
lower results. This can be overcome by introduc-
ing anchors as a substitute. Anchors play the same
role as rare features when learning, while lowering
the chance of misleading the classifier on test data.
The results of the experiment using 5-fold cross
validation on ENG indicate that the F-score im-
proves on average from 93.95 to 94.10 when using
anchors instead of wf2 (+0.15), while just ignoring
the wf2 features drops the F-score by 0.10. The im-
provement is minor but consistent. Its implication
is that anchors can substitute for “irrelevant” lexical
features for better learning results. In future work,
we will experiment with better informed sets of lex-
ical features mixed with anchors.
</bodyText>
<sectionHeader confidence="0.99936" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999936272727273">
We have introduced two novel methods to under-
stand the inner structure of SVM-learned models.
We have applied these techniques to Hebrew NP
chunking, and demonstrated that the learned model
is robust in the presence of noise in the PoS tags, and
relies on only a few lexical features. We have iden-
tified corpus errors, better understood the nature of
the task in Hebrew – and compared it quantitatively
to the task in English.
The methods provide general insight in the way
SVM classification works for chunking.
</bodyText>
<sectionHeader confidence="0.999679" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999859724137931">
S. Abney, R. Schapire, and Y. Singer. 1999. Boosting
applied to tagging and PP attachment. EMNLP-1999.
M. Adler and M. Elhadad. 2006. An unsupervised
morpheme-based hmm for hebrew morphological dis-
ambiguation. In COLING/ACL2006.
C. Cardie and D. Pierce. 1998. Error-driven pruning of
treebank grammars for base noun phrase identification.
In ACL-1998.
Y. Goldberg, M. Adler, and M. Elhadad. 2006. Noun
phrase chunking in hebrew: Influence of lexical and
morphological features. In COLING/ACL2006.
T. Kudo and Y. Matsumoto. 2000. Use of support vector
learning for chunk identification. In CoNLL-2000.
T. Kudo and Y. Matsumoto. 2003. Fast methods for
kernel-based text analysis. In ACL-2003.
M. Marcus and L. Ramshaw. 1995. Text Chunking Us-
ing Transformation-Based Learning. In Proc. of the
3rd ACL Workshop on Very Large Corpora.
T. Nakagawa and Y. Matsumoto. 2002. Detecting er-
rors in corpora using support vector machines. In
COLING-2002.
Erik F. Tjong Kim Sang and S. Buchholz. 2000. Intro-
duction to the conll-2000 shared task: chunking. In
CoNLL-2000.
K. Sima’an, A. Itai, Y. Winter, A. Altman, and N. Nativ.
2001. Building a tree-bank of modern hebrew text.
Traitement Automatique des Langues, 42(2).
V. Vapnik. 1995. The nature of statistical learning the-
ory. Springer-Verlag New York, Inc.
</reference>
<page confidence="0.997933">
231
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.530933">
<title confidence="0.995238">SVM Model Tampering and Anchored Learning: A Case Study in Hebrew NP Chunking</title>
<author confidence="0.998789">Yoav Goldberg</author>
<author confidence="0.998789">Michael Elhadad</author>
<affiliation confidence="0.7809215">Computer Science Department Ben Gurion University of the Negev</affiliation>
<address confidence="0.965991">P.O.B 653 Be’er Sheva 84105, Israel</address>
<email confidence="0.99855">yoavg,elhadad@cs.bgu.ac.il</email>
<abstract confidence="0.99701">We study the issue of porting a known NLP method to a language with little existing NLP resources, specifically Hebrew SVM-based chunking. We introduce two SVM-based methods – Model Tampering and Anchored Learning. These allow fine grained analysis of the learned SVM models, which provides guidance to identify errors in the training corpus, distinguish the role and interaction of lexical features and eventually construct a with error reduction. The resulting chunker is shown to be robust in the presence of noise in the training corpus, relies on less lexical features than was previously understood and achieves an F-measure performance of 92.2 on automatically PoS-tagged text. The SVM analysis methods also provide general insight on SVM-based chunking.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Abney</author>
<author>R Schapire</author>
<author>Y Singer</author>
</authors>
<date>1999</date>
<note>Boosting applied to tagging and PP attachment. EMNLP-1999.</note>
<contexts>
<context position="23865" citStr="Abney et al. (1999)" startWordPosition="4040" endWordPosition="4043">cific features. The hard cases analysis achieved by anchored learning is different from the usual error analysis carried out on observed classification errors. The traditional methods give us intuitions about where the classifier fails to generalize, while the method we present here gives us intuition about what the classifier considers hard to learn, based on the training examples alone. 5As each anchor appear in only one support vector, we can treat the vector’s α value as the anchor weight The intuition that “hard to learn” examples are suspect corpus errors is not new, and appears also in Abney et al. (1999) , who consider the “heaviest” samples in the final distribution of the AdaBoost algorithm to be the hardest to classify and thus likely corpus errors. While AdaBoost models are easy to interpret, this is not the case with SVM. Anchored learning allows us to extract the hard to learn cases from an SVM model. Interestingly, while both AdaBoost and SVM are ‘large margin’ based classifiers, there is less than 50% overlap in the hard cases for the two methods (in terms of mistakes on the test data, there were 234 mistakes shared by AdaBoost and SVM, 69 errors unique to SVM and 126 errors unique to</context>
</contexts>
<marker>Abney, Schapire, Singer, 1999</marker>
<rawString>S. Abney, R. Schapire, and Y. Singer. 1999. Boosting applied to tagging and PP attachment. EMNLP-1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Adler</author>
<author>M Elhadad</author>
</authors>
<title>An unsupervised morpheme-based hmm for hebrew morphological disambiguation.</title>
<date>2006</date>
<booktitle>In COLING/ACL2006.</booktitle>
<contexts>
<context position="11103" citStr="Adler &amp; Elhadad (2006)" startWordPosition="1888" endWordPosition="1891">models on a test corpus. 3.4 Experimental Setting We conducted experiments both for English and Hebrew chunking. For the Hebrew experiments, we use the corpora of (Goldberg et al., 2006). The first one is derived from the original Treebank by projecting the full syntactic tree, constructed manually, onto a set of NP chunks according to the SimpleNP rules. We refer to the resulting corpus as HEBGold since PoS tags are fully reliable. The HEBErr version of the corpus is obtained by projecting the chunk boundaries on the sequence of PoS and morphology tags obtained by the automatic PoS tagger of Adler &amp; Elhadad (2006). This corpus includes an error rate of about 8% on PoS tags. The first 500 sentences are used for testing, and the rest for training. The corpus contains 27K NP chunks. For the English experiments, we use the now-standard training and test sets that were introduced in (Marcus and Ramshaw, 1995)2. Training was done using Kudo’s YAMCHA toolkit3. Both Hebrew and English models were trained using a polynomial kernel of degree 2, with C = 1. For English, the features used were: w−2 ... w2, p−2 . . . p2, t−2 ... t−1. The same features were used for Hebrew, with the addition of m−2 ... m2. These are</context>
</contexts>
<marker>Adler, Elhadad, 2006</marker>
<rawString>M. Adler and M. Elhadad. 2006. An unsupervised morpheme-based hmm for hebrew morphological disambiguation. In COLING/ACL2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cardie</author>
<author>D Pierce</author>
</authors>
<title>Error-driven pruning of treebank grammars for base noun phrase identification.</title>
<date>1998</date>
<booktitle>In ACL-1998.</booktitle>
<contexts>
<context position="15611" citStr="Cardie and Pierce, 1998" startWordPosition="2647" endWordPosition="2650">d for Prepositions and Punctuation marks, followed by Adverbs, and Conjunctions. Strikingly, lexical information for most open-class PoS (including Proper Names and Nouns) has very little impact on Hebrew chunking performance. From this observation, one could conclude that enriching a model based only on PoS with lexical features for only a few closed-class PoS (prepositions and punctuation) could provide appropriate results even with a simpler learning method, one that cannot deal with a large number of features. We tested this hypothesis by training the Error-Driven Pruning (EDP) method of (Cardie and Pierce, 1998) with an extended set of features. EDP with PoS features only produced an F-result of 76.3 on HEBGold. By adding lexical features only for prepositions {מ ב ה כ לש}, one conjunction {ו} and punctuation, the F-score on HEBGold indeed jumps to 85.4. However, when applied on HEBE, EDP falls down again to 59.4. This striking disparity, by comparison, lets us appreciate the resilience of the SVM model to PoS tagging errors, and its generalization capability even with a reduced number of lexical features. Another implication of this data is that commas and quotation marks play a major role in determ</context>
</contexts>
<marker>Cardie, Pierce, 1998</marker>
<rawString>C. Cardie and D. Pierce. 1998. Error-driven pruning of treebank grammars for base noun phrase identification. In ACL-1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Goldberg</author>
<author>M Adler</author>
<author>M Elhadad</author>
</authors>
<title>Noun phrase chunking in hebrew: Influence of lexical and morphological features.</title>
<date>2006</date>
<booktitle>In COLING/ACL2006.</booktitle>
<contexts>
<context position="1639" citStr="Goldberg et al., 2006" startWordPosition="256" endWordPosition="259">g. 1 Introduction While high-quality NLP corpora and tools are available in English, such resources are difficult to obtain in most other languages. Three challenges must be met when adapting results established in English to another language: (1) acquiring high quality annotated data; (2) adapting the English task definition to the nature of a different language, and (3) adapting the algorithm to the new language. This paper presents a case study in the adaptation of a well known task to a language with few NLP resources available. Specifically, we deal with SVM based Hebrew NP chunking. In (Goldberg et al., 2006), we established that the task is not trivially transferable to Hebrew, but reported that SVM based chunking (Kudo and Matsumoto, 2000) performs well. We extend that work and study the problem from 3 angles: (1) how to deal with a corpus that is smaller and with a higher level of noise than is available in English; we propose techniques that help identify ‘suspicious’ data points in the corpus, and identify how robust the model is in the presence of noise; (2) we compare the task definition in English and in Hebrew through quantitative evaluation of the differences between the two languages by</context>
<context position="9937" citStr="Goldberg et al. (2006)" startWordPosition="1691" endWordPosition="1694"> values in the vectors to 0. The result of this procedure is a new Model in which the deleted features never take part in the classification. Model tampering is different than feature selection: on the one hand, it is a method that helps us identify irrelevant features in a model after training; on the other hand, and this is the key insight, removing features after training is not the same as removing them before training. The presence of the low-relevance features during training has an impact on the generalization performed by the learner as shown below. 3.3 The Role of Lexical Features In Goldberg et al. (2006), we have established that using lexical features increases the chunking Fmeasure from 78 to over 92 on the Hebrew Treebank. We refine this observation by using Model Tampering, in order to assess the importance of lexical features in NP Chunking. We are interested in identifying which specific lexical items and contexts impact the chunking decision, and quantifying their effect. Our method is to train a chunking model on a given training corpus, tamper with the resulting model in various ways and measure the performance1 of the tampered models on a test corpus. 3.4 Experimental Setting We con</context>
<context position="11777" citStr="Goldberg et al., 2006" startWordPosition="2014" endWordPosition="2017">PoS tags. The first 500 sentences are used for testing, and the rest for training. The corpus contains 27K NP chunks. For the English experiments, we use the now-standard training and test sets that were introduced in (Marcus and Ramshaw, 1995)2. Training was done using Kudo’s YAMCHA toolkit3. Both Hebrew and English models were trained using a polynomial kernel of degree 2, with C = 1. For English, the features used were: w−2 ... w2, p−2 . . . p2, t−2 ... t−1. The same features were used for Hebrew, with the addition of m−2 ... m2. These are the same settings as in (Kudo and Matsumoto, 2000; Goldberg et al., 2006). 3.5 Tamperings We experimented with the following tamperings: TopN – We define model feature count to be the number of Support Vectors in which a feature is active in a given classifier. This tampering leaves in the model only the top N lexical features in each classifier, according to their count. NoPOS – all the lexical features corresponding to a given part-of-speech are removed from the model. For example, in a NoJJ tampering, all the features of the form wi = X are removed from all the support vectors in which pi = JJ is active. Loc74i – all the lexical features with index i are removed</context>
<context position="16267" citStr="Goldberg et al. (2006)" startWordPosition="2759" endWordPosition="2762"> EDP with PoS features only produced an F-result of 76.3 on HEBGold. By adding lexical features only for prepositions {מ ב ה כ לש}, one conjunction {ו} and punctuation, the F-score on HEBGold indeed jumps to 85.4. However, when applied on HEBE, EDP falls down again to 59.4. This striking disparity, by comparison, lets us appreciate the resilience of the SVM model to PoS tagging errors, and its generalization capability even with a reduced number of lexical features. Another implication of this data is that commas and quotation marks play a major role in determining NP boundaries in Hebrew. In Goldberg et al. (2006), we noted the Hebrew Treebank is not consistent in its treatment of punctuation, and thus we evaluated the chunker only after performing normalization of chunk boundaries for punctuations. We now hypothesize that, since commas and quotation marks play such an important role in the classification, performing such normalization before the training stage might be beneficial. Indeed results on the normalized corpus show improvement of about 1.0 in F score on both HEBE,, and HEBGold. A 10-fold cross validation experiment on punctuation normalized HEBE,, resulted in an F-Score of 92.2, improving th</context>
<context position="26789" citStr="Goldberg et al. (2006)" startWordPosition="4521" endWordPosition="4524">example, null elements result in improper chunks, such as chunks containing only adverbs or only adjectives. We also found 3 invalid sentences, 6 inconsistencies in the tagging of interrogatives with respect to chunk boundaries, as well as 34 other specific mistakes. Overall, more than half of the locations identified by the anchors were corpus errors. Looking for cases similar to the errors identified by anchors, we found 99 more locations, 77 of which were errors. Refining the SimpleNP Definition: The hard cases analysis identified examples that challenge the SimpleNP definition proposed in Goldberg et al. (2006). The most notable cases are: The ‘et’ marker : ‘et’ is a syntactic marker of definite direct objects in Hebrew. It was regarded as a part of SimpleNPs in their definition. In some cases, this forces the resulting SimpleNP to be too inclusive: [תרושקתהו טפשמה תיב תסנכה ,הלשממה תא] [‘et’ (the government, the parliament and the media)] Because in the Treebank the conjunction depends on ‘et’ as a single constituent, it is fully embedded in the chunk. Such a conjunction should not be considered simple. The לש preposition (‘of’) marks generalized possession and was considered unambiguous and includ</context>
</contexts>
<marker>Goldberg, Adler, Elhadad, 2006</marker>
<rawString>Y. Goldberg, M. Adler, and M. Elhadad. 2006. Noun phrase chunking in hebrew: Influence of lexical and morphological features. In COLING/ACL2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kudo</author>
<author>Y Matsumoto</author>
</authors>
<title>Use of support vector learning for chunk identification.</title>
<date>2000</date>
<booktitle>In CoNLL-2000.</booktitle>
<contexts>
<context position="1774" citStr="Kudo and Matsumoto, 2000" startWordPosition="277" endWordPosition="280">other languages. Three challenges must be met when adapting results established in English to another language: (1) acquiring high quality annotated data; (2) adapting the English task definition to the nature of a different language, and (3) adapting the algorithm to the new language. This paper presents a case study in the adaptation of a well known task to a language with few NLP resources available. Specifically, we deal with SVM based Hebrew NP chunking. In (Goldberg et al., 2006), we established that the task is not trivially transferable to Hebrew, but reported that SVM based chunking (Kudo and Matsumoto, 2000) performs well. We extend that work and study the problem from 3 angles: (1) how to deal with a corpus that is smaller and with a higher level of noise than is available in English; we propose techniques that help identify ‘suspicious’ data points in the corpus, and identify how robust the model is in the presence of noise; (2) we compare the task definition in English and in Hebrew through quantitative evaluation of the differences between the two languages by analyzing the relative importance of features in the learned SVM models; and (3) we analyze the structure of learned SVM models to bet</context>
<context position="4374" citStr="Kudo and Matsumoto (2000)" startWordPosition="697" endWordPosition="700">rdie (1998). Follow- introduces some errors. Finally, we want to investiing Ramshaw and Marcus (1995), the current dom- gate chunking in a scenario where PoS tags are asinant approach is formulating chunking as a clas- signed automatically and chunks are then computed. sification task, in which each word is classified as The Hebrew PoS tagger we use introduces about 8% the (B)eginning, (I)nside or (O)outside of a chunk. errors (compared with about 4% in English). We Features for this classification usually involve local are, therefore, interested in identifying errors in the context features. Kudo and Matsumoto (2000) used chunking corpus, and investigating how the chunker SVM as a classification engine and achieved an F- operates in the presence of noise in the PoS tag seScore of 93.79 on the shared task NPs. Since SVM quence. is a binary classifier, to use it for the 3-class classi- 3 Model Tampering fication of the chunking task, 3 different classifiers 3.1 Notation and Technical Review {B/I, B/O, I/O} were trained and their majority vote This section presents notation as well as a technical was taken. review of SVM chunking details relevant to the curNP chunks in the shared task data are BaseNPs, rent </context>
<context position="11753" citStr="Kudo and Matsumoto, 2000" startWordPosition="2010" endWordPosition="2013">error rate of about 8% on PoS tags. The first 500 sentences are used for testing, and the rest for training. The corpus contains 27K NP chunks. For the English experiments, we use the now-standard training and test sets that were introduced in (Marcus and Ramshaw, 1995)2. Training was done using Kudo’s YAMCHA toolkit3. Both Hebrew and English models were trained using a polynomial kernel of degree 2, with C = 1. For English, the features used were: w−2 ... w2, p−2 . . . p2, t−2 ... t−1. The same features were used for Hebrew, with the addition of m−2 ... m2. These are the same settings as in (Kudo and Matsumoto, 2000; Goldberg et al., 2006). 3.5 Tamperings We experimented with the following tamperings: TopN – We define model feature count to be the number of Support Vectors in which a feature is active in a given classifier. This tampering leaves in the model only the top N lexical features in each classifier, according to their count. NoPOS – all the lexical features corresponding to a given part-of-speech are removed from the model. For example, in a NoJJ tampering, all the features of the form wi = X are removed from all the support vectors in which pi = JJ is active. Loc74i – all the lexical features </context>
</contexts>
<marker>Kudo, Matsumoto, 2000</marker>
<rawString>T. Kudo and Y. Matsumoto. 2000. Use of support vector learning for chunk identification. In CoNLL-2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kudo</author>
<author>Y Matsumoto</author>
</authors>
<title>Fast methods for kernel-based text analysis.</title>
<date>2003</date>
<booktitle>In ACL-2003.</booktitle>
<marker>Kudo, Matsumoto, 2003</marker>
<rawString>T. Kudo and Y. Matsumoto. 2003. Fast methods for kernel-based text analysis. In ACL-2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>L Ramshaw</author>
</authors>
<title>Text Chunking Using Transformation-Based Learning.</title>
<date>1995</date>
<booktitle>In Proc. of the 3rd ACL Workshop on Very Large Corpora.</booktitle>
<contexts>
<context position="11399" citStr="Marcus and Ramshaw, 1995" startWordPosition="1941" endWordPosition="1944">nually, onto a set of NP chunks according to the SimpleNP rules. We refer to the resulting corpus as HEBGold since PoS tags are fully reliable. The HEBErr version of the corpus is obtained by projecting the chunk boundaries on the sequence of PoS and morphology tags obtained by the automatic PoS tagger of Adler &amp; Elhadad (2006). This corpus includes an error rate of about 8% on PoS tags. The first 500 sentences are used for testing, and the rest for training. The corpus contains 27K NP chunks. For the English experiments, we use the now-standard training and test sets that were introduced in (Marcus and Ramshaw, 1995)2. Training was done using Kudo’s YAMCHA toolkit3. Both Hebrew and English models were trained using a polynomial kernel of degree 2, with C = 1. For English, the features used were: w−2 ... w2, p−2 . . . p2, t−2 ... t−1. The same features were used for Hebrew, with the addition of m−2 ... m2. These are the same settings as in (Kudo and Matsumoto, 2000; Goldberg et al., 2006). 3.5 Tamperings We experimented with the following tamperings: TopN – We define model feature count to be the number of Support Vectors in which a feature is active in a given classifier. This tampering leaves in the mode</context>
</contexts>
<marker>Marcus, Ramshaw, 1995</marker>
<rawString>M. Marcus and L. Ramshaw. 1995. Text Chunking Using Transformation-Based Learning. In Proc. of the 3rd ACL Workshop on Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Nakagawa</author>
<author>Y Matsumoto</author>
</authors>
<title>Detecting errors in corpora using support vector machines.</title>
<date>2002</date>
<booktitle>In COLING-2002.</booktitle>
<contexts>
<context position="21944" citStr="Nakagawa and Matsumoto (2002)" startWordPosition="3721" endWordPosition="3724">fy cases, we can 228 actively look for such cases by training an SVM chunker on anchored data (as the anchored data is guaranteed to be linearly separable, we can set a very high value to the C parameter, preventing any misclassification), and then investigating either the anchors whose weights5 are above some threshold t or the top N heaviest anchors, and their corresponding corpus locations. These locations are those that the learner considers hard to classify. They can be either corpus errors, or genuinely hard cases. This method is similar to the corpus error detection method presented by Nakagawa and Matsumoto (2002). They constructed an SVM model for PoS tagging, and considered Support Vectors with high α values to be indicative of suspicious corpus locations. These locations can be either outliers, or correctly labeled locations similar to an outlier. They then looked for similar corpus locations with a different label, to point out right-wrong pairs with high precision. Using anchors improves their method in three aspects: (1) without anchors, similar examples are often indistinguishable to the SVM learner, and in case they have conflicting labels both examples will be given high weights. That is, both</context>
</contexts>
<marker>Nakagawa, Matsumoto, 2002</marker>
<rawString>T. Nakagawa and Y. Matsumoto. 2002. Detecting errors in corpora using support vector machines. In COLING-2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F Tjong Kim Sang</author>
<author>S Buchholz</author>
</authors>
<title>Introduction to the conll-2000 shared task: chunking.</title>
<date>2000</date>
<booktitle>In CoNLL-2000.</booktitle>
<marker>Sang, Buchholz, 2000</marker>
<rawString>Erik F. Tjong Kim Sang and S. Buchholz. 2000. Introduction to the conll-2000 shared task: chunking. In CoNLL-2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sima’an</author>
<author>A Itai</author>
<author>Y Winter</author>
<author>A Altman</author>
<author>N Nativ</author>
</authors>
<title>Building a tree-bank of modern hebrew text.</title>
<date>2001</date>
<booktitle>Traitement Automatique des Langues,</booktitle>
<volume>42</volume>
<issue>2</issue>
<marker>Sima’an, Itai, Winter, Altman, Nativ, 2001</marker>
<rawString>K. Sima’an, A. Itai, Y. Winter, A. Altman, and N. Nativ. 2001. Building a tree-bank of modern hebrew text. Traitement Automatique des Langues, 42(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Vapnik</author>
</authors>
<title>The nature of statistical learning theory.</title>
<date>1995</date>
<publisher>Springer-Verlag</publisher>
<location>New York, Inc.</location>
<contexts>
<context position="5166" citStr="Vapnik, 1995" startWordPosition="835" endWordPosition="836"> shared task NPs. Since SVM quence. is a binary classifier, to use it for the 3-class classi- 3 Model Tampering fication of the chunking task, 3 different classifiers 3.1 Notation and Technical Review {B/I, B/O, I/O} were trained and their majority vote This section presents notation as well as a technical was taken. review of SVM chunking details relevant to the curNP chunks in the shared task data are BaseNPs, rent study. Further details can be found in Kudo and which are non-recursive NPs, a definition first pro- Matsumoto (2000; 2003). posed by Ramshaw and Marcus (1995). This defini- SVM (Vapnik, 1995) is a supervised binary clastion yields good NP chunks for English. In (Gold- sifier. The input to the learner is a set of l trainberg et al., 2006) we argued that it is not applica- ing samples (x1, y1), ... , (xl, yl), x ∈ Rn, y ∈ ble to Hebrew, mainly because of the prevalence {+1, −1}. xi is an n dimensional feature vecof the Hebrew’s construct state (smixut). 5mixut tor representing the ith sample, and yi is the lais similar to a noun-compound construct, but one bel for that sample. The result of the learning prothat can join a noun (with a special morphologi- cess is the set 5V of Suppor</context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>V. Vapnik. 1995. The nature of statistical learning theory. Springer-Verlag New York, Inc.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>