<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000060">
<note confidence="0.9520175">
Proceedings of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL), Philadelphia, July 2002, pp. 176-182.
</note>
<title confidence="0.9960695">
Improving Language Model Size Reduction using Better Pruning
Criteria
</title>
<author confidence="0.988001">
Jianfeng Gao Min Zhang1
</author>
<affiliation confidence="0.848943666666667">
Microsoft Research, Asia State Key Lab of Intelligent Tech &amp; Sys.
Beijing, 100080, China Computer Science &amp; Technology Dept.
jfgao@microsoft.com Tsinghua University, China
</affiliation>
<sectionHeader confidence="0.981283" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999954857142857">
Reducing language model (LM) size is a
critical issue when applying a LM to
realistic applications which have memory
constraints. In this paper, three measures
are studied for the purpose of LM
pruning. They are probability, rank, and
entropy. We evaluated the performance of
the three pruning criteria in a real
application of Chinese text input in terms
of character error rate (CER). We first
present an empirical comparison, showing
that rank performs the best in most cases.
We also show that the high-performance
of rank lies in its strong correlation with
error rate. We then present a novel
method of combining two criteria in
model pruning. Experimental results
show that the combined criterion
consistently leads to smaller models than
the models pruned using either of the
criteria separately, at the same CER.
</bodyText>
<sectionHeader confidence="0.999136" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999799024390244">
Backoff n-gram models for applications such as
large vocabulary speech recognition are typically
trained on very large text corpora. An
uncompressed LM is usually too large for practical
use since all realistic applications have memory
constraints. Therefore, LM pruning techniques are
used to produce the smallest model while keeping
the performance loss as small as possible.
Research on backoff n-gram model pruning has
been focused on the development of the pruning
criterion, which is used to estimate the performance
loss of the pruned model. The traditional count
cutoff method (Jelinek, 1990) used a pruning
criterion based on absolute frequency while recent
research has shown that better pruning criteria can
be developed based on more sophisticated measures
such as perplexity.
In this paper, we study three measures for
pruning backoff n-gram models. They are
probability, rank and entropy. We evaluated the
performance of the three pruning criteria in a real
application of Chinese text input (Gao et al., 2002)
through CER. We first present an empirical
comparison, showing that rank performs the best in
most cases. We also show that the high-performance
of rank lies in its strong correlation with error rate.
We then present a novel method of combining two
pruning criteria in model pruning. Our results show
that the combined criterion consistently leads to
smaller models than the models pruned using either
of the criteria separately. In particular, the
combination of rank and entropy achieves the
smallest models at a given CER.
The rest of the paper is structured as follows:
Section 2 discusses briefly the related work on
backoff n-gram pruning. Section 3 describes in
detail several pruning criteria. Section 4 presents an
empirical comparison of pruning criteria using a
Chinese text input system. Section 5 proposes our
method of combining two criteria in model pruning.
Section 6 presents conclusions and our future work.
</bodyText>
<sectionHeader confidence="0.999833" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999823888888889">
N-gram models predict the next word given the
previous n-1 words by estimating the conditional
probability P(wn|w1...wn_1). In practice, n is usually
set to 2 (bigram), or 3 (trigram). For simplicity, we
restrict our discussion to bigrams P(wn |wn_1), but our
approaches can be extended to any n-gram.
The bigram probabilities are estimated from the
training data by maximum likelihood estimation
(MLE). However, the intrinsic problem of MLE is
</bodyText>
<footnote confidence="0.654472">
1 This work was done while Zhang was working at Microsoft Research Asia as a visiting student.
</footnote>
<bodyText confidence="0.999410958333333">
that of data sparseness: MLE leads to zero-value
probabilities for unseen bigrams. To deal with this
problem, Katz (1987) proposed a backoff scheme.
He estimates the probability of an unseen bigram by
utilizing unigram estimates as follows
where c(wi-1wi) is the frequency of word pair (wi-1wi)
in the training data, Pd represents the Good-Turing
discounted estimate for seen word pairs, and α(wi-1)
is a normalization factor.
Due to the memory limitation in realistic
applications, only a finite set of word pairs have
conditional probability P(wi|wi-1) explicitly
represented in the model. The remaining word pairs
are assigned a probability by backoff (i.e. unigram
estimates). The goal of bigram pruning is to remove
uncommon explicit bigram estimates P(wi|wi-1) from
the model to reduce the number of parameters while
minimizing the performance loss.
The research on backoff n-gram model pruning
can be formulated as the definition of the pruning
criterion, which is used to estimate the performance
loss of the pruned model. Given the pruning
criterion, a simple thresholding algorithm for
pruning bigram models can be described as follows:
</bodyText>
<listItem confidence="0.999494857142857">
1. Select a threshold 0.
2. Compute the performance loss due to
pruning each bigram individually using the
pruning criterion.
3. Remove all bigrams with performance loss
less than 0.
4. Re-compute backoff weights.
</listItem>
<figureCaption confidence="0.9685955">
Figure 1: Thresholding algorithm for bigram
pruning
</figureCaption>
<bodyText confidence="0.999976823529412">
The algorithm in Figure 1 together with several
pruning criteria has been studied previously
(Seymore and Rosenfeld, 1996; Stolcke, 1998; Gao
and Lee, 2000; etc). A comparative study of these
techniques is presented in (Goodman and Gao,
2000).
In this paper, three pruning criteria will be
studied: probability, rank, and entropy. Probability
serves as the baseline pruning criterion. It is derived
from perplexity which has been widely used as a LM
evaluation measure. Rank and entropy have been
previously used as a metric for LM evaluation in
(Clarkson and Robinson, 2001). In the current paper,
these two measures will be studied for the purpose of
backoff n-gram model pruning. In the next section,
we will describe how pruning criteria are developed
using these two measures.
</bodyText>
<sectionHeader confidence="0.946825" genericHeader="method">
3 Pruning Criteria
</sectionHeader>
<bodyText confidence="0.999964428571429">
In this section, we describe the three pruning criteria
we evaluated. They are derived from LM evaluation
measures including perplexity, rank, and entropy.
The goal of the pruning criterion is to estimate the
performance loss due to pruning each bigram
individually. Therefore, we represent the pruning
criterion as a loss function, denoted by LF below.
</bodyText>
<subsectionHeader confidence="0.998972">
3.1 Probability
</subsectionHeader>
<bodyText confidence="0.999917">
The probability pruning criterion is derived from
perplexity. The perplexity is defined as
</bodyText>
<equation confidence="0.998838333333333">
N
− log (  |1)
P w i w i − (2)
= =
N i 1
PP 2
</equation>
<bodyText confidence="0.999834071428571">
where N is the size of the test data. The perplexity
can be roughly interpreted as the expected branching
factor of the test document when presented to the
LM. It is expected that lower perplexities are
correlated with lower error rates.
The method of pruning bigram models using
probability can be described as follows: all bigrams
that change perplexity by less than a threshold are
removed from the model. In this study, we assume
that the change in model perplexity of the LM can be
expressed in terms of a weighted difference of the
log probability estimate before and after pruning a
bigram. The loss function of probability LFprobability,
is then defined as
</bodyText>
<equation confidence="0.983581">
−P(wi−1wi)[logP&apos;(wi |wi−1)−logP(wi |wi−1)], (3)
</equation>
<bodyText confidence="0.999994285714286">
where P(.|.) denotes the conditional probabilities
assigned by the original model, P’(.|.) denotes the
probabilities in the pruned model, and P(wi-1 wi) is a
smoothed probability estimate in the original model.
We notice that LFprobability of Equation (3) is very
similar to that proposed by Seymore and Rosenfeld
(1996), where the loss function is
</bodyText>
<equation confidence="0.697485">
−N(wi−1wi )[logP&apos; (wi  |wi−1) − logP(wi  |wi−1)]
</equation>
<bodyText confidence="0.991805">
Here N(wi-1wi) is the discounted frequency that
bigram wi-1wi was observed in training. N(wi-1wi) is
conceptually identical to P(wi-1 wi) in Equation (3).
From Equations (2) and (3), we can see that lower
LFprobability is strongly correlated with lower
perplexity. However, we found that LFprobability is
suboptimal as a pruning criterion, evaluated on CER
in our experiments. We assume that it is largely due
to the deficiency of perplexity as a LM performance
measure.
Although perplexity is widely used due to its
simplicity and efficiency, recent researches show
that its correlation with error rate is not as strong as
once thought. Clarkson and Robinson (2001)
</bodyText>
<figure confidence="0.591223538461539">
P w w
(  |)=
i i−1
−1 ie
α
(w )P(w) otherwis
, (1)
l Pd (wi  |wi−1) Oi−1 , wO&gt;
�
�
0
1
Y_
</figure>
<bodyText confidence="0.999725105263158">
analyzed the reason behind it and concluded that the
calculation of perplexity is based solely on the
probabilities of words contained within the test text,
so it disregards the probabilities of alternative
words, which will be competing with the correct
word (referred to as target word below) within the
decoder (e.g. in a speech recognition system).
Therefore, they used other measures such as rank
and entropy for LM evaluation. These measures are
based on the probability distribution over the whole
vocabulary. That is, if the test text is w1n, then
perplexity is based on the values of P(wi |wi-1), and
the new measures will be based on the values of
P(w|wi-1) for all w in the vocabulary. Since these
measures take into account the probability
distribution over all competing words (including the
target word) within the decoder, they are, hopefully,
better correlated with error rate, and expected to
evaluate LMs more precisely than perplexity.
</bodyText>
<subsectionHeader confidence="0.998677">
3.2 Rank
</subsectionHeader>
<bodyText confidence="0.999986727272727">
The rank of the target word w is defined as the
word’s position in an ordered list of the bigram
probabilities P(w|wi-1) where w∈ V, and V is the
vocabulary. Thus the most likely word (within the
decoder at a certain time point) has the rank of one,
and the least likely has rank |V|, where |V |is the
vocabulary size.
We propose to use rank for pruning as follows: all
bigrams that change rank by less than a threshold
after pruning are removed from the model. The
corresponding loss function LFrank is defined as
</bodyText>
<equation confidence="0.9925095">
p(wi−1wi){log[R′(wi |wi−1)+k] −log R(wi |wi−1)} (4)
1
</equation>
<bodyText confidence="0.998803714285714">
where R(.|.) denotes the rank of the observed bigram
P(wi|wi-1) in the list of bigram probabilities P(w|wi-1)
where w∈V, before pruning, R’(.|.) is the new rank
of it after pruning, and the summation is over all
word pairs (wi-1wi). k is a constant to assure that
log[R′(wi  |wi−1)+k]− log R(wi  |wi−1) ≠ 0 . k is set to
0.1 in our experiments.
</bodyText>
<subsectionHeader confidence="0.988821">
3.3 Entropy
</subsectionHeader>
<bodyText confidence="0.90931775">
Given a bigram model, the entropy H of the
probability distribution over the vocabulary V is
generally given by
H(w ) = −∑ V= 1 P(w  |w ) log P(w  |w ) .
</bodyText>
<equation confidence="0.798053">
i j j i j i
</equation>
<bodyText confidence="0.999944">
We propose to use entropy for pruning as follows:
all bigrams that change entropy by less than a
threshold after pruning are removed from the model.
The corresponding loss function LFentropy is defined
as
</bodyText>
<equation confidence="0.996435">
1
− ∑= ′
N −
N 1( ( 1 ) ( 1))
H wi H wi
i − − (5)
</equation>
<bodyText confidence="0.997770181818182">
where H is the entropy before pruning given history
wi-1, H’ is the new entropy after pruning, and N is the
size of the test data.
The entropy-based pruning is conceptually
similar to the pruning method proposed in (Stolcke,
1998). Stolcke used the Kullback-Leibler divergence
between the pruned and un-pruned model
probability distribution in a given context over the
entire vocabulary. In particular, the increase in
relative entropy from pruning a bigram is computed
by
</bodyText>
<equation confidence="0.917471">
P(wi−1wi)[logP&apos;(wi  |wi−1)− log P(wi  |wi−1)]
</equation>
<bodyText confidence="0.998393">
where the summation is over all word pairs (wi-1wi).
</bodyText>
<sectionHeader confidence="0.998346" genericHeader="method">
4 Empirical Comparison
</sectionHeader>
<bodyText confidence="0.999982117647059">
We evaluated the pruning criteria introduced in the
previous section on a realistic application, Chinese
text input. In this application, a string of Pinyin
(phonetic alphabet) is converted into Chinese
characters, which is the standard way of inputting
text on Chinese computers. This is a similar problem
to speech recognition except that it does not include
acoustic ambiguity. We measure performance in
terms of character error rate (CER), which is the
number of characters wrongly converted from the
Pinyin string divided by the number of characters in
the correct transcript. The role of the language
model is, for all possible word strings that match the
typed Pinyin string, to select the word string with the
highest language model probability.
The training data we used is a balanced corpus of
approximately 26 million characters from various
domains of text such as newspapers, novels,
manuals, etc. The test data consists of half a million
characters that have been proofread and balanced
among domain, style and time.
The back-off bigram models we generated in this
study are character-based models. That is, the
training and test corpora are not word-segmented.
As a result, the lexicon we used contains 7871 single
Chinese characters only. While word-based n-gram
models are widely applied, we used character-based
models for two reasons. First, pilot experiments
show that the results of word-based and
character-based models are qualitatively very
similar. More importantly, because we need to build
a very large number of models in our experiments as
shown below, character-based models are much
more efficient, both for training and for decoding.
</bodyText>
<equation confidence="0.969694">
∑
wiwi
−
∑
wi wi
−1
</equation>
<bodyText confidence="0.999866107142857">
We used the absolute discount smoothing method
for model training.
None of the pruning techniques we consider are
loss-less. Therefore, whenever we compare pruning
criteria, we do so by comparing the size reduction of
the pruning criteria at the same CER.
Figure 2 shows how the CER varies with the
bigram numbers in the models. For comparison, we
also include in Figure 2 the results using count cutoff
pruning. We can see that CER decreases as we keep
more and more bigrams in the model. A steeper
curve indicates a better pruning criterion.
The main result to notice here is that the
rank-based pruning achieves consistently the best
performance among all of them over a wide range of
CER values, producing models that are at 55-85% of
the size of the probability-based pruned models with
the same CER. An example of the detailed
comparison results is shown in Table 1, where the
CER is 13.8% and the value of cutoff is 1. The last
column of Table 1 shows the relative model sizes
with respect to the probability-based pruned model
with the CER 13.8%.
Another interesting result is the good
performance of count cutoff, which is almost
overlapping with probability-based pruning at larger
model sizes 2 . The entropy-based pruning
unfortunately, achieved the worst performance.
</bodyText>
<figureCaption confidence="0.82242">
Figure 2: Comparison of pruning criteria
Table 1: LM size comparison at CER 13.8%
</figureCaption>
<bodyText confidence="0.934310833333334">
criterion # of bigram size (MB) % of prob
probability 774483 6.1 100.0%
cutoff (=1) 707088 5.6 91.8%
entropy 1167699 9.3 152.5%
rank 512339 4.1 67.2%
2 The result is consistent with that reported in (Goodman
and Gao, 2000), where an explanation was offered.
We assume that the superior performance of
rank-based pruning lies in the fact that rank (acting
as a LM evaluation measure) has better correlation
with CER. Clarkson and Robinson (2001) estimated
the correlation between LM evaluation measures
and word error rate in a speech recognition system.
The related part of their results to our study are
shown in Table 2, where r is the Pearson
product-moment correlation coefficient, rs is the
Spearman rank-order correlation coefficient, and T
is the Kendall rank-order correlation coefficient.
</bodyText>
<tableCaption confidence="0.918549">
Table 2: Correlation of LM evaluation measures
with word error rates (Clarkson and Robinson,
2001)
</tableCaption>
<table confidence="0.9980895">
r rs T
Mean log rank 0.967 0.957 0.846
Perplexity 0.955 0.955 0.840
Mean entropy -0.799 -0.792 -0.602
</table>
<tableCaption confidence="0.712955">
Table 2 indicates that the mean log rank (i.e.
</tableCaption>
<bodyText confidence="0.998329111111111">
related to the pruning criterion of rank we used) has
the best correlation with word error rate, followed by
the perplexity (i.e. related to the pruning criterion of
probability we used) and the mean entropy (i.e.
related to the pruning criterion of entropy we used),
which support our test results. We can conclude that
the LM evaluation measures which are better
correlated with error rate lead to better pruning
criteria.
</bodyText>
<sectionHeader confidence="0.964583" genericHeader="method">
5 Combining Two Criteria
</sectionHeader>
<bodyText confidence="0.999986076923077">
We now investigate methods of combining pruning
criteria described above. We begin by examining the
overlap of the bigrams pruned by two different
criteria to investigate which might usefully be
combined. Then the thresholding pruning algorithm
described in Figure 1 is modified so as to make use
of two pruning criteria simultaneously. The problem
here is how to find the optimal settings of the
pruning threshold pair (each for one pruning
criterion) for different model sizes. We show how an
optimal function which defines the optimal settings
of the threshold pairs is efficiently established using
our techniques.
</bodyText>
<subsectionHeader confidence="0.93399">
5.1 Overlap
</subsectionHeader>
<bodyText confidence="0.999940333333333">
From the abovementioned three pruning criteria, we
investigated the overlap of the bigrams pruned by a
pair of criteria. There are three criteria pairs. The
overlap results are shown in Figure 3.
We can see that the percentage of the number of
bigrams pruned by both criteria seems to increase as
</bodyText>
<figure confidence="0.973263461538461">
average error rate
14.0
13.9
13.8
13.7
13.6
14.1
3.E+05 4.E+05 5.E+05 6.E+05 7.E+05 8.E+05 9.E+05
# of bigrams in the model
entropy
count cutoff
rank
prob
</figure>
<bodyText confidence="0.998082545454545">
the model size decreases, but all criterion-pairs have
overlaps much lower than 100%. In particular, we
find that the average overlap between probability
and entropy is approximately 71%, which is the
biggest among the three pairs. The pruning method
based on the criteria of rank and entropy has the
smallest average overlap of 63.6%. The results
suggest that we might be able to obtain
improvements by combining these two criteria for
bigram pruning since the information provided by
these criteria is, in some sense, complementary.
</bodyText>
<figureCaption confidence="0.9593095">
Figure 3: Overlap of selected bigrams between
criterion pairs
</figureCaption>
<subsectionHeader confidence="0.999799">
5.2 Pruning by two criteria
</subsectionHeader>
<bodyText confidence="0.9981415">
In order to prune a bigram model based on two
criteria simultaneously, we modified the
thresholding pruning algorithm described in Figure
1. Let lfi be the value of the performance loss
estimated by the loss function LFi, θi be the
threshold defined by the pruning criterion Ci. The
modified thresholding pruning algorithm can be
described as follows:
</bodyText>
<listItem confidence="0.999281125">
1. Select a setting of threshold pair (θ1θ2)
2. Compute the values of performance loss lf1
and lf2 due to pruning each bigram
individually using the two pruning criteria
C1 and C2, respectively.
3. Remove all bigrams with performance loss
lf1 less than θ1, and lf2 less than θ2.
4. Re-compute backoff weights.
</listItem>
<figureCaption confidence="0.9853425">
Figure 4: Modified thresholding algorithm for
bigram pruning
</figureCaption>
<bodyText confidence="0.999980041666667">
Now, the remaining problem is how to find the
optimal settings of the pruning threshold pair for
different model sizes. This seems to be a very
tedious task since for each model size, a large
number of settings (θ1θ2) have to be tried for finding
the optimal ones. Therefore, we convert the problem
to the following one: How to find an optimal
function θ2=f(θ1) by which the optimal threshold θ2
is defined for each threshold θ1. The function can be
learned by pilot experiments described below. Given
two thresholds θ1 and θ2 of pruning criteria C1 and
C2, we try a large number of values of θ1, θ2, and
build a large number of models pruned using the
algorithm described in Figure 4. For each model
size, we find an optimal setting of the threshold
setting (θ1θ2) which results in a pruned model with
the lowest CER. Finally, all these optimal threshold
settings serve as the sample data, from which the
optimal function can be learned. We found that in
pilot experiments, a relatively small set of sample
settings is enough to generate the function which is
close enough to the optimal one. This allows us to
relatively quickly search through what would
otherwise be an overwhelmingly large search space.
</bodyText>
<subsectionHeader confidence="0.885088">
5.3 Results
</subsectionHeader>
<bodyText confidence="0.996039533333333">
We used the same training data described in Section
4 for bigram model training. We divided the test set
described in Section 4 into two non-overlapped
subsets. We performed testing on one subset
containing 80% of the test set. We performed
optimal function learning using the remaining 20%
of the test set (referred to as held-out data below).
Take the combination of rank and entropy as an
example. An uncompressed bigram model was first
built using all training data. We then built a very
large number of pruned bigram models using
different threshold setting (θ rank θentropy), where the
values θ rank, θentropy ∈ [3E-12, 3E-6]. By evaluating
pruned models on the held-out data, optimal settings
can be found. Some sample settings are shown in
</bodyText>
<tableCaption confidence="0.993004">
Table 3.
Table 3: Sample optimal parameter settings for
combination of criteria based on rank and entropy
</tableCaption>
<table confidence="0.985574">
# bigrams θ rank θentropy
137987 8.00E-07 8.00E-09
196809 3.00E-07 8.00E-09
200294 3.00E-07 5.00E-09
274434 3.00E-07 5.00E-10
304619 8.00E-08 8.00E-09
394300 5.00E-08 3.00E-10
443695 3.00E-08 3.00E-10
570907 8.00E-09 3.00E-09
669051 5.00E-09 5.00E-10
890664 5.00E-11 3.00E-10
# of overlaped bigrams
</table>
<figure confidence="0.955480285714286">
8.E+05
6.E+05
4.E+05
2.E+05
0.E+00
1.E+06
0.E+00 2.E+05 4.E+05 6.E+05 8.E+05 1.E+06
# of pruned bigrams
prob+rank
prob+entropy
rank+entropy
100% overlap
892214 5.00E-12 3.00E-10
892257 3.00E-12 3.00E-10
</figure>
<bodyText confidence="0.999723">
In experiments, we found that a linear regression
model of Equation (6) is powerful enough to learn a
function which is close enough to the optimal one.
</bodyText>
<equation confidence="0.999951">
log(0entropy) = α1 x log(0rank)+α2 (6)
</equation>
<bodyText confidence="0.99603">
Here α1 and α2 are coefficients estimated from the
sample settings. Optimal functions of the other two
threshold-pair settings (0 rank0probability) and (0
probability0entropy) are obtained similarly. They are
shown in Table 4.
</bodyText>
<tableCaption confidence="0.675568">
Table 4. Optimal functions
</tableCaption>
<equation confidence="0.999920333333333">
log(0entropy) = 0.3 x log(0rank) + 6.5
log(0 probability) = 6.2 , for any 0rank
log(0entropy) = 0.7 x log(0probability) + 3.5
</equation>
<bodyText confidence="0.99993575">
In Figure 5, we present the results using models
pruned with all three threshold-pairs defined by the
functions in Table 4. As we expected, in all three
cases, using a combination of two pruning criteria
achieves consistently better performance than using
either of the criteria separately. In particular, using
the combination of rank and entropy, we obtained
the best models over a wide large of CER values. It
corresponds to a significant size reduction of
15-54% over the probability-based LM pruning at
the same CER. An example of the detailed
comparison results is shown in Table 5.
</bodyText>
<figureCaption confidence="0.883711">
Figure 5: Comparison of combined pruning
criterion performance
</figureCaption>
<tableCaption confidence="0.987493">
Table 5: LM size comparison at CER 13.8%
</tableCaption>
<table confidence="0.997507857142857">
Criterion # of bigram size (MB) % of prob
Prob 1036627 8.2 100.0%
Entropy 1291000 10.2 124.4%
Rank 643411 5.1 62.2%
Prob + entropy 542124 4.28 52.2%
Prob + rank 579115 4.57 55.7%
rank + entropy 538252 4.25 51.9%
</table>
<bodyText confidence="0.9999295">
There are two reasons for the superior
performance of the combination of rank and entropy.
First, the rank-based pruning achieves very good
performance as described in Section 4. Second, as
shown in Section 5.1, there is a relatively small
overlap between the bigrams chosen by these two
pruning criteria, thus big improvement can be
achieved through the combination.
</bodyText>
<sectionHeader confidence="0.999232" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999943633333333">
The research on backoff n-gram pruning has been
focused on the development of the pruning criterion,
which is used to estimate the performance loss of the
pruned model.
This paper explores several pruning criteria for
backoff n-gram model size reduction. Besides the
widely used probability, two new pruning criteria
have been developed based on rank and entropy. We
have performed an empirical comparison of these
pruning criteria. We also presented a thresholding
algorithm for model pruning, in which two pruning
criteria can be used simultaneously. Finally, we
described our techniques of finding the optimal
setting of the threshold pair given a specific model
size.
We have shown several interesting results. They
include the confirmation of the estimation that the
measures which are better correlated with CER for
LM evaluation leads to better pruning criteria. Our
experiments show that rank, which has the best
correlation with CER, achieves the best performance
when there is only one criterion used in bigram
model pruning. We then show empirically that the
overlap of the bigrams pruned by different criteria is
relatively low. This indicates that we might obtain
improvements through a combination of two criteria
for bigram pruning since the information provided
by these criteria is complementary. This hypothesis
is confirmed by our experiments. Results show that
using two pruning criteria simultaneously achieves
</bodyText>
<figure confidence="0.801589692307692">
rank
prob
entropy
rank+prob
rank+entropy
prob+entropy
14.2
14.1
average error rate
14.0
13.9
13.8
13.
</figure>
<page confidence="0.6435895">
7
13.6
</page>
<bodyText confidence="0.923023727272728">
3.E+05 5.E+05 7.E+05 9.E+05 1.E+06
# of bigrams in the model
better bigram models than using either of the criteria
separately. In particular, the combination of rank
and entropy achieves the smallest bigram models at
the same CER.
For our future work, more experiments will be
performed on other language models such as
word-based bigram and trigram for Chinese and
English. More pruning criteria and their
combinations will be investigated as well.
</bodyText>
<sectionHeader confidence="0.997962" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999135">
The authors wish to thank Ashley Chang, Joshua
Goodman, Chang-Ning Huang, Hang Li, Hisami
Suzuki and Ming Zhou for suggestions and
comments on a preliminary draft of this paper.
Thanks also to three anonymous reviews for
valuable and insightful comments.
</bodyText>
<sectionHeader confidence="0.999204" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.989321725">
Clarkson, P. and Robinson, T. (2001), Improved
language modeling through better language
model evaluation measures, Computer Speech
and Language, 15:39-53, 2001.
Gao, J. and Lee K.F (2000). Distribution-based
pruning of backoff language models, 38th Annual
meetings of the Association for Computational
Linguistics (ACL’00), HongKong, 2000.
Gao, J., Goodman, J., Li, M., and Lee, K. F. (2002).
Toward a unified approach to statistical language
modeling for Chinese. ACM Transactions on
Asian Language Information Processing, Vol. 1,
No. 1, pp 3-33. Draft available from
http://www.research.microsoft.com/~jfgao
Goodman, J. and Gao, J. (2000) Language model
size reduction by pruning and clustering,
ICSLP-2000, International Conference on
Spoken Language Processing, Beijing, October
16-20, 2000.
Jelinek, F. (1990). Self-organized language
modeling for speech recognition. In Readings in
Speech Recognition, A. Waibel and K. F. Lee,
eds., Morgan-Kaufmann, San Mateo, CA, pp.
450-506.
Katz, S. M., (1987). Estimation of probabilities from
sparse data for other language component of a
speech recognizer. IEEE transactions on
Acoustics, Speech and Signal Processing,
35(3):400-401, 1987.
Rosenfeld, R. (1996). A maximum entropy approach
to adaptive statistical language modeling.
Computer, Speech and Language, vol. 10, pp.
187-- 228, 1996.
Seymore, K., and Rosenfeld, R. (1996). Scalable
backoff language models. Proc. ICSLP, Vol. 1.,
pp.232-235, Philadelphia, 1996
Stolcke, A. (1998). Entropy-based Pruning of
Backoff Language Models. Proc. DARPA News
Transcription and Understanding Workshop,
1998, pp. 270-274, Lansdowne, VA.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.998123">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), Philadelphia, July 2002, pp. 176-182.</note>
<title confidence="0.859731">Improving Language Model Size Reduction using Better Pruning Criteria</title>
<author confidence="0.888965">Gao Min</author>
<affiliation confidence="0.963305">Microsoft Research, Asia State Key Lab of Intelligent Tech &amp; Sys.</affiliation>
<address confidence="0.714444">Beijing, 100080, China Computer Science &amp; Technology Dept.</address>
<email confidence="0.340835">jfgao@microsoft.comTsinghuaUniversity,China</email>
<abstract confidence="0.99890491576674">Reducing language model (LM) size is a critical issue when applying a LM to realistic applications which have memory constraints. In this paper, three measures are studied for the purpose of LM pruning. They are probability, rank, and entropy. We evaluated the performance of the three pruning criteria in a real application of Chinese text input in terms of character error rate (CER). We first present an empirical comparison, showing that rank performs the best in most cases. We also show that the high-performance of rank lies in its strong correlation with error rate. We then present a novel method of combining two criteria in model pruning. Experimental results show that the combined criterion consistently leads to smaller models than the models pruned using either of the criteria separately, at the same CER. models for applications such as large vocabulary speech recognition are typically trained on very large text corpora. An uncompressed LM is usually too large for practical use since all realistic applications have memory constraints. Therefore, LM pruning techniques are used to produce the smallest model while keeping the performance loss as small as possible. on backoff model pruning has been focused on the development of the pruning criterion, which is used to estimate the performance loss of the pruned model. The traditional count cutoff method (Jelinek, 1990) used a pruning criterion based on absolute frequency while recent research has shown that better pruning criteria can be developed based on more sophisticated measures such as perplexity. In this paper, we study three measures for backoff models. They are We evaluated the performance of the three pruning criteria in a real of Chinese text input (Gao 2002) through CER. We first present an empirical comparison, showing that rank performs the best in most cases. We also show that the high-performance of rank lies in its strong correlation with error rate. We then present a novel method of combining two pruning criteria in model pruning. Our results show that the combined criterion consistently leads to smaller models than the models pruned using either of the criteria separately. In particular, the combination of rank and entropy achieves the smallest models at a given CER. The rest of the paper is structured as follows: Section 2 discusses briefly the related work on pruning. Section 3 describes in detail several pruning criteria. Section 4 presents an empirical comparison of pruning criteria using a Chinese text input system. Section 5 proposes our method of combining two criteria in model pruning. Section 6 presents conclusions and our future work. Work N-gram models predict the next word given the words by estimating the conditional In practice, usually set to 2 (bigram), or 3 (trigram). For simplicity, we our discussion to bigrams but our can be extended to any The bigram probabilities are estimated from the training data by maximum likelihood estimation (MLE). However, the intrinsic problem of MLE is work was done while Zhang was working at Microsoft Research Asia as a visiting student. that of data sparseness: MLE leads to zero-value probabilities for unseen bigrams. To deal with this problem, Katz (1987) proposed a backoff scheme. He estimates the probability of an unseen bigram by utilizing unigram estimates as follows is the frequency of word pair the training data, the Good-Turing estimate for seen word pairs, and is a normalization factor. Due to the memory limitation in realistic applications, only a finite set of word pairs have probability explicitly represented in the model. The remaining word pairs are assigned a probability by backoff (i.e. unigram estimates). The goal of bigram pruning is to remove explicit bigram estimates from the model to reduce the number of parameters while minimizing the performance loss. research on backoff model pruning can be formulated as the definition of the pruning criterion, which is used to estimate the performance loss of the pruned model. Given the pruning criterion, a simple thresholding algorithm for pruning bigram models can be described as follows: Select a threshold 2. Compute the performance loss due to pruning each bigram individually using the pruning criterion. 3. Remove all bigrams with performance loss than 4. Re-compute backoff weights. Thresholding algorithm for bigram pruning The algorithm in Figure 1 together with several pruning criteria has been studied previously (Seymore and Rosenfeld, 1996; Stolcke, 1998; Gao and Lee, 2000; etc). A comparative study of these techniques is presented in (Goodman and Gao, 2000). In this paper, three pruning criteria will be studied: probability, rank, and entropy. Probability serves as the baseline pruning criterion. It is derived from perplexity which has been widely used as a LM evaluation measure. Rank and entropy have been previously used as a metric for LM evaluation in (Clarkson and Robinson, 2001). In the current paper, these two measures will be studied for the purpose of model pruning. In the next section, we will describe how pruning criteria are developed using these two measures. Criteria In this section, we describe the three pruning criteria we evaluated. They are derived from LM evaluation measures including perplexity, rank, and entropy. The goal of the pruning criterion is to estimate the performance loss due to pruning each bigram individually. Therefore, we represent the pruning as a loss function, denoted by 3.1 Probability The probability pruning criterion is derived from perplexity. The perplexity is defined as N ( | w iw the size of the test data. The perplexity can be roughly interpreted as the expected branching factor of the test document when presented to the LM. It is expected that lower perplexities are correlated with lower error rates. The method of pruning bigram models using probability can be described as follows: all bigrams that change perplexity by less than a threshold are removed from the model. In this study, we assume that the change in model perplexity of the LM can be expressed in terms of a weighted difference of the log probability estimate before and after pruning a The loss function of probability is then defined as (3) the conditional probabilities by the original model, the in the pruned model, and is a smoothed probability estimate in the original model. notice that of Equation (3) is very similar to that proposed by Seymore and Rosenfeld (1996), where the loss function is  || is the discounted frequency that was observed in training. is identical to in Equation (3). From Equations (2) and (3), we can see that lower strongly correlated with lower However, we found that is suboptimal as a pruning criterion, evaluated on CER in our experiments. We assume that it is largely due to the deficiency of perplexity as a LM performance measure. Although perplexity is widely used due to its simplicity and efficiency, recent researches show that its correlation with error rate is not as strong as once thought. Clarkson and Robinson (2001) P w w | α , (1) | � � 0 1 Y_ analyzed the reason behind it and concluded that the calculation of perplexity is based solely on the probabilities of words contained within the test text, so it disregards the probabilities of alternative words, which will be competing with the correct (referred to as word within the decoder (e.g. in a speech recognition system). Therefore, they used other measures such as rank and entropy for LM evaluation. These measures are based on the probability distribution over the whole That is, if the test text is then is based on the values of and the new measures will be based on the values of for all the vocabulary. Since these measures take into account the probability distribution over all competing words (including the target word) within the decoder, they are, hopefully, better correlated with error rate, and expected to evaluate LMs more precisely than perplexity. 3.2 Rank rank of the target word defined as the word’s position in an ordered list of the bigram where and the vocabulary. Thus the most likely word (within the decoder at a certain time point) has the rank of one, the least likely has rank where the vocabulary size. We propose to use rank for pruning as follows: all bigrams that change rank by less than a threshold after pruning are removed from the model. The loss function is defined as 1 the rank of the observed bigram the list of bigram probabilities before pruning, the new rank of it after pruning, and the summation is over all pairs k a constant to assure that   ||set to 0.1 in our experiments. 3.3 Entropy a bigram model, the entropy the distribution over the vocabulary generally given by −∑ log i j j i j i We propose to use entropy for pruning as follows: all bigrams that change entropy by less than a threshold after pruning are removed from the model. corresponding loss function is defined as 1 ( ( H − (5) the entropy before pruning given history the new entropy after pruning, and the size of the test data. The entropy-based pruning is conceptually similar to the pruning method proposed in (Stolcke, Stolcke used the between the pruned and un-pruned model probability distribution in a given context over the entire vocabulary. In particular, the increase in relative entropy from pruning a bigram is computed by  || the summation is over all word pairs Comparison We evaluated the pruning criteria introduced in the previous section on a realistic application, Chinese text input. In this application, a string of Pinyin (phonetic alphabet) is converted into Chinese characters, which is the standard way of inputting text on Chinese computers. This is a similar problem to speech recognition except that it does not include acoustic ambiguity. We measure performance in terms of character error rate (CER), which is the number of characters wrongly converted from the Pinyin string divided by the number of characters in the correct transcript. The role of the language model is, for all possible word strings that match the typed Pinyin string, to select the word string with the highest language model probability. The training data we used is a balanced corpus of approximately 26 million characters from various domains of text such as newspapers, novels, manuals, etc. The test data consists of half a million characters that have been proofread and balanced among domain, style and time. The back-off bigram models we generated in this study are character-based models. That is, the training and test corpora are not word-segmented. As a result, the lexicon we used contains 7871 single characters only. While word-based models are widely applied, we used character-based models for two reasons. First, pilot experiments show that the results of word-based and character-based models are qualitatively very similar. More importantly, because we need to build a very large number of models in our experiments as shown below, character-based models are much more efficient, both for training and for decoding. ∑ − ∑ We used the absolute discount smoothing method for model training. None of the pruning techniques we consider are loss-less. Therefore, whenever we compare pruning criteria, we do so by comparing the size reduction of the pruning criteria at the same CER. Figure 2 shows how the CER varies with the bigram numbers in the models. For comparison, we also include in Figure 2 the results using count cutoff pruning. We can see that CER decreases as we keep more and more bigrams in the model. A steeper curve indicates a better pruning criterion. The main result to notice here is that the rank-based pruning achieves consistently the best performance among all of them over a wide range of CER values, producing models that are at 55-85% of the size of the probability-based pruned models with the same CER. An example of the detailed comparison results is shown in Table 1, where the CER is 13.8% and the value of cutoff is 1. The last column of Table 1 shows the relative model sizes with respect to the probability-based pruned model with the CER 13.8%. Another interesting result is the good performance of count cutoff, which is almost overlapping with probability-based pruning at larger sizes 2. The entropy-based pruning unfortunately, achieved the worst performance. Comparison of pruning criteria LM size comparison at CER 13.8% criterion # of bigram size (MB) % of prob probability 774483 6.1 100.0% cutoff (=1) 707088 5.6 91.8% entropy 1167699 9.3 152.5% rank 512339 4.1 67.2% result is consistent with that reported in (Goodman and Gao, 2000), where an explanation was offered. We assume that the superior performance of rank-based pruning lies in the fact that rank (acting as a LM evaluation measure) has better correlation with CER. Clarkson and Robinson (2001) estimated the correlation between LM evaluation measures and word error rate in a speech recognition system. The related part of their results to our study are in Table 2, where the Pearson correlation coefficient, is the rank-order correlation coefficient, and is the Kendall rank-order correlation coefficient. 2: of LM evaluation measures with word error rates (Clarkson and Robinson, 2001) r T Mean log rank 0.967 0.957 0.846 Perplexity 0.955 0.955 0.840 Mean entropy -0.799 -0.792 -0.602 Table 2 indicates that the mean log rank (i.e. related to the pruning criterion of rank we used) has the best correlation with word error rate, followed by the perplexity (i.e. related to the pruning criterion of probability we used) and the mean entropy (i.e. related to the pruning criterion of entropy we used), which support our test results. We can conclude that the LM evaluation measures which are better correlated with error rate lead to better pruning criteria. Two Criteria We now investigate methods of combining pruning criteria described above. We begin by examining the overlap of the bigrams pruned by two different criteria to investigate which might usefully be combined. Then the thresholding pruning algorithm described in Figure 1 is modified so as to make use of two pruning criteria simultaneously. The problem here is how to find the optimal settings of the pruning threshold pair (each for one pruning criterion) for different model sizes. We show how an optimal function which defines the optimal settings of the threshold pairs is efficiently established using our techniques. 5.1 Overlap From the abovementioned three pruning criteria, we investigated the overlap of the bigrams pruned by a pair of criteria. There are three criteria pairs. The overlap results are shown in Figure 3. We can see that the percentage of the number of bigrams pruned by both criteria seems to increase as average error rate 14.0 13.9 13.8 13.7 13.6 14.1 3.E+05 4.E+05 5.E+05 6.E+05 7.E+05 8.E+05 9.E+05 entropy count cutoff rank prob the model size decreases, but all criterion-pairs have overlaps much lower than 100%. In particular, we find that the average overlap between probability and entropy is approximately 71%, which is the biggest among the three pairs. The pruning method based on the criteria of rank and entropy has the smallest average overlap of 63.6%. The results suggest that we might be able to obtain improvements by combining these two criteria for bigram pruning since the information provided by these criteria is, in some sense, complementary. Overlap of selected bigrams between criterion pairs 5.2 Pruning by two criteria In order to prune a bigram model based on two criteria simultaneously, we modified the thresholding pruning algorithm described in Figure Let the value of the performance loss by the loss function the defined by the pruning criterion The modified thresholding pruning algorithm can be described as follows: Select a setting of threshold pair Compute the values of performance loss to pruning each bigram individually using the two pruning criteria respectively. 3. Remove all bigrams with performance loss than and than 4. backoff weights. Modified thresholding algorithm for bigram pruning Now, the remaining problem is how to find the optimal settings of the pruning threshold pair for different model sizes. This seems to be a very tedious task since for each model size, a large of settings have to be tried for finding the optimal ones. Therefore, we convert the problem to the following one: How to find an optimal by which the optimal threshold defined for each threshold The function can be learned by pilot experiments described below. Given thresholds pruning criteria we try a large number of values of and build a large number of models pruned using the algorithm described in Figure 4. For each model size, we find an optimal setting of the threshold which results in a pruned model with the lowest CER. Finally, all these optimal threshold settings serve as the sample data, from which the optimal function can be learned. We found that in pilot experiments, a relatively small set of sample settings is enough to generate the function which is close enough to the optimal one. This allows us to relatively quickly search through what would otherwise be an overwhelmingly large search space. 5.3 Results We used the same training data described in Section 4 for bigram model training. We divided the test set described in Section 4 into two non-overlapped subsets. We performed testing on one subset containing 80% of the test set. We performed optimal function learning using the remaining 20% of the test set (referred to as held-out data below). Take the combination of rank and entropy as an example. An uncompressed bigram model was first built using all training data. We then built a very large number of pruned bigram models using threshold setting where the 3E-6]. By evaluating pruned models on the held-out data, optimal settings can be found. Some sample settings are shown in Table 3. Sample optimal parameter settings for combination of criteria based on rank and entropy</abstract>
<address confidence="0.644087444444444">137987 8.00E-07 8.00E-09 196809 3.00E-07 8.00E-09 200294 3.00E-07 5.00E-09 274434 3.00E-07 5.00E-10 304619 8.00E-08 8.00E-09 394300 5.00E-08 3.00E-10 443695 3.00E-08 3.00E-10 570907 8.00E-09 3.00E-09 669051 5.00E-09 5.00E-10</address>
<phone confidence="0.440195">890664 5.00E-11 3.00E-10</phone>
<note confidence="0.778300333333333">8.E+05 6.E+05 4.E+05 2.E+05 0.E+00 1.E+06</note>
<abstract confidence="0.964029306306307">0.E+00 2.E+05 4.E+05 6.E+05 8.E+05 1.E+06 prob+rank prob+entropy rank+entropy 100% overlap 892214 5.00E-12 3.00E-10 892257 3.00E-12 3.00E-10 In experiments, we found that a linear regression model of Equation (6) is powerful enough to learn a function which is close enough to the optimal one. x coefficients estimated from the sample settings. Optimal functions of the other two settings and are obtained similarly. They are shown in Table 4. Optimal functions for any + In Figure 5, we present the results using models pruned with all three threshold-pairs defined by the functions in Table 4. As we expected, in all three cases, using a combination of two pruning criteria achieves consistently better performance than using either of the criteria separately. In particular, using the combination of rank and entropy, we obtained the best models over a wide large of CER values. It corresponds to a significant size reduction of 15-54% over the probability-based LM pruning at the same CER. An example of the detailed comparison results is shown in Table 5. Comparison of combined pruning criterion performance LM size comparison at CER 13.8% Criterion bigram size (MB) % of prob Prob 1036627 8.2 100.0% Entropy 1291000 10.2 124.4% Rank 643411 5.1 62.2% Prob + entropy 542124 4.28 52.2% Prob + rank 579115 4.57 55.7% rank + entropy 538252 4.25 51.9% There are two reasons for the superior performance of the combination of rank and entropy. First, the rank-based pruning achieves very good performance as described in Section 4. Second, as shown in Section 5.1, there is a relatively small overlap between the bigrams chosen by these two pruning criteria, thus big improvement can be achieved through the combination. research on backoff pruning has been focused on the development of the pruning criterion, which is used to estimate the performance loss of the pruned model. This paper explores several pruning criteria for model size reduction. Besides the widely used probability, two new pruning criteria have been developed based on rank and entropy. We have performed an empirical comparison of these pruning criteria. We also presented a thresholding algorithm for model pruning, in which two pruning criteria can be used simultaneously. Finally, we described our techniques of finding the optimal setting of the threshold pair given a specific model size. We have shown several interesting results. They include the confirmation of the estimation that the measures which are better correlated with CER for LM evaluation leads to better pruning criteria. Our experiments show that rank, which has the best correlation with CER, achieves the best performance when there is only one criterion used in bigram model pruning. We then show empirically that the overlap of the bigrams pruned by different criteria is relatively low. This indicates that we might obtain improvements through a combination of two criteria for bigram pruning since the information provided by these criteria is complementary. This hypothesis is confirmed by our experiments. Results show that using two pruning criteria simultaneously achieves rank prob entropy rank+prob rank+entropy prob+entropy 14.2 14.1 average error rate 14.0 13.9 13.8 13. 7 13.6 3.E+05 5.E+05 7.E+05 9.E+05 1.E+06 better bigram models than using either of the criteria separately. In particular, the combination of rank and entropy achieves the smallest bigram models at the same CER. For our future work, more experiments will be performed on other language models such as word-based bigram and trigram for Chinese and English. More pruning criteria and their combinations will be investigated as well. Acknowledgements The authors wish to thank Ashley Chang, Joshua Goodman, Chang-Ning Huang, Hang Li, Hisami Suzuki and Ming Zhou for suggestions and comments on a preliminary draft of this paper. Thanks also to three anonymous reviews for valuable and insightful comments.</abstract>
<note confidence="0.680061071428571">References Clarkson, P. and Robinson, T. (2001), Improved language modeling through better language evaluation measures, Speech 15:39-53, 2001. Gao, J. and Lee K.F (2000). Distribution-based of backoff language models, Annual meetings of the Association for Computational HongKong, 2000. Gao, J., Goodman, J., Li, M., and Lee, K. F. (2002). Toward a unified approach to statistical language for Chinese. Transactions on Language Information Vol. 1, 1, pp 3-33. available from</note>
<web confidence="0.860374">http://www.research.microsoft.com/~jfgao</web>
<note confidence="0.827719666666667">Goodman, J. and Gao, J. (2000) Language model size reduction by pruning and clustering, ICSLP-2000, International Conference on Language Processing, October 16-20, 2000. Jelinek, F. (1990). Self-organized language</note>
<title confidence="0.677502">for speech recognition. In in</title>
<author confidence="0.840335">A Waibel</author>
<author confidence="0.840335">K F Lee</author>
<email confidence="0.728082">eds.,Morgan-Kaufmann,SanMateo,CA,pp.</email>
<abstract confidence="0.795119125">450-506. Katz, S. M., (1987). Estimation of probabilities from sparse data for other language component of a recognizer. transactions on Speech and Signal 35(3):400-401, 1987. Rosenfeld, R. (1996). A maximum entropy approach to adaptive statistical language modeling.</abstract>
<note confidence="0.9510376">Speech and Language, 10, pp. 187-- 228, 1996. Seymore, K., and Rosenfeld, R. (1996). Scalable language models. Vol. 1., pp.232-235, Philadelphia, 1996</note>
<title confidence="0.710194">Stolcke, A. (1998). Entropy-based Pruning of Language Models. DARPA News Transcription and Understanding Workshop,</title>
<address confidence="0.959858">1998, pp. 270-274, Lansdowne, VA.</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P Clarkson</author>
<author>T Robinson</author>
</authors>
<title>Improved language modeling through better language model evaluation measures,</title>
<date>2001</date>
<journal>Computer Speech and Language,</journal>
<pages>15--39</pages>
<contexts>
<context position="5739" citStr="Clarkson and Robinson, 2001" startWordPosition="889" endWordPosition="892"> Figure 1: Thresholding algorithm for bigram pruning The algorithm in Figure 1 together with several pruning criteria has been studied previously (Seymore and Rosenfeld, 1996; Stolcke, 1998; Gao and Lee, 2000; etc). A comparative study of these techniques is presented in (Goodman and Gao, 2000). In this paper, three pruning criteria will be studied: probability, rank, and entropy. Probability serves as the baseline pruning criterion. It is derived from perplexity which has been widely used as a LM evaluation measure. Rank and entropy have been previously used as a metric for LM evaluation in (Clarkson and Robinson, 2001). In the current paper, these two measures will be studied for the purpose of backoff n-gram model pruning. In the next section, we will describe how pruning criteria are developed using these two measures. 3 Pruning Criteria In this section, we describe the three pruning criteria we evaluated. They are derived from LM evaluation measures including perplexity, rank, and entropy. The goal of the pruning criterion is to estimate the performance loss due to pruning each bigram individually. Therefore, we represent the pruning criterion as a loss function, denoted by LF below. 3.1 Probability The </context>
<context position="8246" citStr="Clarkson and Robinson (2001)" startWordPosition="1298" endWordPosition="1301">ency that bigram wi-1wi was observed in training. N(wi-1wi) is conceptually identical to P(wi-1 wi) in Equation (3). From Equations (2) and (3), we can see that lower LFprobability is strongly correlated with lower perplexity. However, we found that LFprobability is suboptimal as a pruning criterion, evaluated on CER in our experiments. We assume that it is largely due to the deficiency of perplexity as a LM performance measure. Although perplexity is widely used due to its simplicity and efficiency, recent researches show that its correlation with error rate is not as strong as once thought. Clarkson and Robinson (2001) P w w ( |)= i i−1 −1 ie α (w )P(w) otherwis , (1) l Pd (wi |wi−1) Oi−1 , wO&gt; � � 0 1 Y_ analyzed the reason behind it and concluded that the calculation of perplexity is based solely on the probabilities of words contained within the test text, so it disregards the probabilities of alternative words, which will be competing with the correct word (referred to as target word below) within the decoder (e.g. in a speech recognition system). Therefore, they used other measures such as rank and entropy for LM evaluation. These measures are based on the probability distribution over the whole vocabu</context>
<context position="14728" citStr="Clarkson and Robinson (2001)" startWordPosition="2403" endWordPosition="2406">ng at larger model sizes 2 . The entropy-based pruning unfortunately, achieved the worst performance. Figure 2: Comparison of pruning criteria Table 1: LM size comparison at CER 13.8% criterion # of bigram size (MB) % of prob probability 774483 6.1 100.0% cutoff (=1) 707088 5.6 91.8% entropy 1167699 9.3 152.5% rank 512339 4.1 67.2% 2 The result is consistent with that reported in (Goodman and Gao, 2000), where an explanation was offered. We assume that the superior performance of rank-based pruning lies in the fact that rank (acting as a LM evaluation measure) has better correlation with CER. Clarkson and Robinson (2001) estimated the correlation between LM evaluation measures and word error rate in a speech recognition system. The related part of their results to our study are shown in Table 2, where r is the Pearson product-moment correlation coefficient, rs is the Spearman rank-order correlation coefficient, and T is the Kendall rank-order correlation coefficient. Table 2: Correlation of LM evaluation measures with word error rates (Clarkson and Robinson, 2001) r rs T Mean log rank 0.967 0.957 0.846 Perplexity 0.955 0.955 0.840 Mean entropy -0.799 -0.792 -0.602 Table 2 indicates that the mean log rank (i.e</context>
</contexts>
<marker>Clarkson, Robinson, 2001</marker>
<rawString>Clarkson, P. and Robinson, T. (2001), Improved language modeling through better language model evaluation measures, Computer Speech and Language, 15:39-53, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gao</author>
<author>K F Lee</author>
</authors>
<title>Distribution-based pruning of backoff language models,</title>
<date>2000</date>
<booktitle>38th Annual meetings of the Association for Computational Linguistics (ACL’00), HongKong,</booktitle>
<contexts>
<context position="5319" citStr="Gao and Lee, 2000" startWordPosition="822" endWordPosition="825"> criterion, which is used to estimate the performance loss of the pruned model. Given the pruning criterion, a simple thresholding algorithm for pruning bigram models can be described as follows: 1. Select a threshold 0. 2. Compute the performance loss due to pruning each bigram individually using the pruning criterion. 3. Remove all bigrams with performance loss less than 0. 4. Re-compute backoff weights. Figure 1: Thresholding algorithm for bigram pruning The algorithm in Figure 1 together with several pruning criteria has been studied previously (Seymore and Rosenfeld, 1996; Stolcke, 1998; Gao and Lee, 2000; etc). A comparative study of these techniques is presented in (Goodman and Gao, 2000). In this paper, three pruning criteria will be studied: probability, rank, and entropy. Probability serves as the baseline pruning criterion. It is derived from perplexity which has been widely used as a LM evaluation measure. Rank and entropy have been previously used as a metric for LM evaluation in (Clarkson and Robinson, 2001). In the current paper, these two measures will be studied for the purpose of backoff n-gram model pruning. In the next section, we will describe how pruning criteria are developed</context>
</contexts>
<marker>Gao, Lee, 2000</marker>
<rawString>Gao, J. and Lee K.F (2000). Distribution-based pruning of backoff language models, 38th Annual meetings of the Association for Computational Linguistics (ACL’00), HongKong, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gao</author>
<author>J Goodman</author>
<author>M Li</author>
<author>K F Lee</author>
</authors>
<title>Toward a unified approach to statistical language modeling for Chinese.</title>
<date>2002</date>
<journal>ACM Transactions on Asian Language Information Processing,</journal>
<volume>1</volume>
<pages>3--33</pages>
<note>Draft available from http://www.research.microsoft.com/~jfgao</note>
<contexts>
<context position="2269" citStr="Gao et al., 2002" startWordPosition="344" endWordPosition="347">del pruning has been focused on the development of the pruning criterion, which is used to estimate the performance loss of the pruned model. The traditional count cutoff method (Jelinek, 1990) used a pruning criterion based on absolute frequency while recent research has shown that better pruning criteria can be developed based on more sophisticated measures such as perplexity. In this paper, we study three measures for pruning backoff n-gram models. They are probability, rank and entropy. We evaluated the performance of the three pruning criteria in a real application of Chinese text input (Gao et al., 2002) through CER. We first present an empirical comparison, showing that rank performs the best in most cases. We also show that the high-performance of rank lies in its strong correlation with error rate. We then present a novel method of combining two pruning criteria in model pruning. Our results show that the combined criterion consistently leads to smaller models than the models pruned using either of the criteria separately. In particular, the combination of rank and entropy achieves the smallest models at a given CER. The rest of the paper is structured as follows: Section 2 discusses brief</context>
</contexts>
<marker>Gao, Goodman, Li, Lee, 2002</marker>
<rawString>Gao, J., Goodman, J., Li, M., and Lee, K. F. (2002). Toward a unified approach to statistical language modeling for Chinese. ACM Transactions on Asian Language Information Processing, Vol. 1, No. 1, pp 3-33. Draft available from http://www.research.microsoft.com/~jfgao</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goodman</author>
<author>J Gao</author>
</authors>
<title>Language model size reduction by pruning and clustering,</title>
<date>2000</date>
<booktitle>ICSLP-2000, International Conference on Spoken Language Processing,</booktitle>
<location>Beijing,</location>
<contexts>
<context position="5406" citStr="Goodman and Gao, 2000" startWordPosition="836" endWordPosition="839">ven the pruning criterion, a simple thresholding algorithm for pruning bigram models can be described as follows: 1. Select a threshold 0. 2. Compute the performance loss due to pruning each bigram individually using the pruning criterion. 3. Remove all bigrams with performance loss less than 0. 4. Re-compute backoff weights. Figure 1: Thresholding algorithm for bigram pruning The algorithm in Figure 1 together with several pruning criteria has been studied previously (Seymore and Rosenfeld, 1996; Stolcke, 1998; Gao and Lee, 2000; etc). A comparative study of these techniques is presented in (Goodman and Gao, 2000). In this paper, three pruning criteria will be studied: probability, rank, and entropy. Probability serves as the baseline pruning criterion. It is derived from perplexity which has been widely used as a LM evaluation measure. Rank and entropy have been previously used as a metric for LM evaluation in (Clarkson and Robinson, 2001). In the current paper, these two measures will be studied for the purpose of backoff n-gram model pruning. In the next section, we will describe how pruning criteria are developed using these two measures. 3 Pruning Criteria In this section, we describe the three pr</context>
<context position="14506" citStr="Goodman and Gao, 2000" startWordPosition="2368" endWordPosition="2371">relative model sizes with respect to the probability-based pruned model with the CER 13.8%. Another interesting result is the good performance of count cutoff, which is almost overlapping with probability-based pruning at larger model sizes 2 . The entropy-based pruning unfortunately, achieved the worst performance. Figure 2: Comparison of pruning criteria Table 1: LM size comparison at CER 13.8% criterion # of bigram size (MB) % of prob probability 774483 6.1 100.0% cutoff (=1) 707088 5.6 91.8% entropy 1167699 9.3 152.5% rank 512339 4.1 67.2% 2 The result is consistent with that reported in (Goodman and Gao, 2000), where an explanation was offered. We assume that the superior performance of rank-based pruning lies in the fact that rank (acting as a LM evaluation measure) has better correlation with CER. Clarkson and Robinson (2001) estimated the correlation between LM evaluation measures and word error rate in a speech recognition system. The related part of their results to our study are shown in Table 2, where r is the Pearson product-moment correlation coefficient, rs is the Spearman rank-order correlation coefficient, and T is the Kendall rank-order correlation coefficient. Table 2: Correlation of </context>
</contexts>
<marker>Goodman, Gao, 2000</marker>
<rawString>Goodman, J. and Gao, J. (2000) Language model size reduction by pruning and clustering, ICSLP-2000, International Conference on Spoken Language Processing, Beijing, October 16-20, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
</authors>
<title>Self-organized language modeling for speech recognition.</title>
<date>1990</date>
<booktitle>In Readings in Speech Recognition,</booktitle>
<pages>450--506</pages>
<editor>A. Waibel and K. F. Lee, eds., Morgan-Kaufmann,</editor>
<location>San Mateo, CA,</location>
<contexts>
<context position="1845" citStr="Jelinek, 1990" startWordPosition="279" endWordPosition="280"> Introduction Backoff n-gram models for applications such as large vocabulary speech recognition are typically trained on very large text corpora. An uncompressed LM is usually too large for practical use since all realistic applications have memory constraints. Therefore, LM pruning techniques are used to produce the smallest model while keeping the performance loss as small as possible. Research on backoff n-gram model pruning has been focused on the development of the pruning criterion, which is used to estimate the performance loss of the pruned model. The traditional count cutoff method (Jelinek, 1990) used a pruning criterion based on absolute frequency while recent research has shown that better pruning criteria can be developed based on more sophisticated measures such as perplexity. In this paper, we study three measures for pruning backoff n-gram models. They are probability, rank and entropy. We evaluated the performance of the three pruning criteria in a real application of Chinese text input (Gao et al., 2002) through CER. We first present an empirical comparison, showing that rank performs the best in most cases. We also show that the high-performance of rank lies in its strong cor</context>
</contexts>
<marker>Jelinek, 1990</marker>
<rawString>Jelinek, F. (1990). Self-organized language modeling for speech recognition. In Readings in Speech Recognition, A. Waibel and K. F. Lee, eds., Morgan-Kaufmann, San Mateo, CA, pp. 450-506.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Katz</author>
</authors>
<title>Estimation of probabilities from sparse data for other language component of a speech recognizer.</title>
<date>1987</date>
<journal>IEEE transactions on Acoustics, Speech and Signal Processing,</journal>
<pages>35--3</pages>
<contexts>
<context position="3872" citStr="Katz (1987)" startWordPosition="602" endWordPosition="603">ious n-1 words by estimating the conditional probability P(wn|w1...wn_1). In practice, n is usually set to 2 (bigram), or 3 (trigram). For simplicity, we restrict our discussion to bigrams P(wn |wn_1), but our approaches can be extended to any n-gram. The bigram probabilities are estimated from the training data by maximum likelihood estimation (MLE). However, the intrinsic problem of MLE is 1 This work was done while Zhang was working at Microsoft Research Asia as a visiting student. that of data sparseness: MLE leads to zero-value probabilities for unseen bigrams. To deal with this problem, Katz (1987) proposed a backoff scheme. He estimates the probability of an unseen bigram by utilizing unigram estimates as follows where c(wi-1wi) is the frequency of word pair (wi-1wi) in the training data, Pd represents the Good-Turing discounted estimate for seen word pairs, and α(wi-1) is a normalization factor. Due to the memory limitation in realistic applications, only a finite set of word pairs have conditional probability P(wi|wi-1) explicitly represented in the model. The remaining word pairs are assigned a probability by backoff (i.e. unigram estimates). The goal of bigram pruning is to remove </context>
</contexts>
<marker>Katz, 1987</marker>
<rawString>Katz, S. M., (1987). Estimation of probabilities from sparse data for other language component of a speech recognizer. IEEE transactions on Acoustics, Speech and Signal Processing, 35(3):400-401, 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rosenfeld</author>
</authors>
<title>A maximum entropy approach to adaptive statistical language modeling.</title>
<date>1996</date>
<journal>Computer, Speech and Language,</journal>
<volume>10</volume>
<pages>187--228</pages>
<contexts>
<context position="5285" citStr="Rosenfeld, 1996" startWordPosition="818" endWordPosition="819">as the definition of the pruning criterion, which is used to estimate the performance loss of the pruned model. Given the pruning criterion, a simple thresholding algorithm for pruning bigram models can be described as follows: 1. Select a threshold 0. 2. Compute the performance loss due to pruning each bigram individually using the pruning criterion. 3. Remove all bigrams with performance loss less than 0. 4. Re-compute backoff weights. Figure 1: Thresholding algorithm for bigram pruning The algorithm in Figure 1 together with several pruning criteria has been studied previously (Seymore and Rosenfeld, 1996; Stolcke, 1998; Gao and Lee, 2000; etc). A comparative study of these techniques is presented in (Goodman and Gao, 2000). In this paper, three pruning criteria will be studied: probability, rank, and entropy. Probability serves as the baseline pruning criterion. It is derived from perplexity which has been widely used as a LM evaluation measure. Rank and entropy have been previously used as a metric for LM evaluation in (Clarkson and Robinson, 2001). In the current paper, these two measures will be studied for the purpose of backoff n-gram model pruning. In the next section, we will describe </context>
<context position="7504" citStr="Rosenfeld (1996)" startWordPosition="1183" endWordPosition="1184">study, we assume that the change in model perplexity of the LM can be expressed in terms of a weighted difference of the log probability estimate before and after pruning a bigram. The loss function of probability LFprobability, is then defined as −P(wi−1wi)[logP&apos;(wi |wi−1)−logP(wi |wi−1)], (3) where P(.|.) denotes the conditional probabilities assigned by the original model, P’(.|.) denotes the probabilities in the pruned model, and P(wi-1 wi) is a smoothed probability estimate in the original model. We notice that LFprobability of Equation (3) is very similar to that proposed by Seymore and Rosenfeld (1996), where the loss function is −N(wi−1wi )[logP&apos; (wi |wi−1) − logP(wi |wi−1)] Here N(wi-1wi) is the discounted frequency that bigram wi-1wi was observed in training. N(wi-1wi) is conceptually identical to P(wi-1 wi) in Equation (3). From Equations (2) and (3), we can see that lower LFprobability is strongly correlated with lower perplexity. However, we found that LFprobability is suboptimal as a pruning criterion, evaluated on CER in our experiments. We assume that it is largely due to the deficiency of perplexity as a LM performance measure. Although perplexity is widely used due to its simplic</context>
</contexts>
<marker>Rosenfeld, 1996</marker>
<rawString>Rosenfeld, R. (1996). A maximum entropy approach to adaptive statistical language modeling. Computer, Speech and Language, vol. 10, pp. 187-- 228, 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Seymore</author>
<author>R Rosenfeld</author>
</authors>
<title>Scalable backoff language models.</title>
<date>1996</date>
<booktitle>Proc. ICSLP,</booktitle>
<volume>1</volume>
<pages>232--235</pages>
<location>Philadelphia,</location>
<contexts>
<context position="5285" citStr="Seymore and Rosenfeld, 1996" startWordPosition="816" endWordPosition="819"> formulated as the definition of the pruning criterion, which is used to estimate the performance loss of the pruned model. Given the pruning criterion, a simple thresholding algorithm for pruning bigram models can be described as follows: 1. Select a threshold 0. 2. Compute the performance loss due to pruning each bigram individually using the pruning criterion. 3. Remove all bigrams with performance loss less than 0. 4. Re-compute backoff weights. Figure 1: Thresholding algorithm for bigram pruning The algorithm in Figure 1 together with several pruning criteria has been studied previously (Seymore and Rosenfeld, 1996; Stolcke, 1998; Gao and Lee, 2000; etc). A comparative study of these techniques is presented in (Goodman and Gao, 2000). In this paper, three pruning criteria will be studied: probability, rank, and entropy. Probability serves as the baseline pruning criterion. It is derived from perplexity which has been widely used as a LM evaluation measure. Rank and entropy have been previously used as a metric for LM evaluation in (Clarkson and Robinson, 2001). In the current paper, these two measures will be studied for the purpose of backoff n-gram model pruning. In the next section, we will describe </context>
<context position="7504" citStr="Seymore and Rosenfeld (1996)" startWordPosition="1181" endWordPosition="1184">el. In this study, we assume that the change in model perplexity of the LM can be expressed in terms of a weighted difference of the log probability estimate before and after pruning a bigram. The loss function of probability LFprobability, is then defined as −P(wi−1wi)[logP&apos;(wi |wi−1)−logP(wi |wi−1)], (3) where P(.|.) denotes the conditional probabilities assigned by the original model, P’(.|.) denotes the probabilities in the pruned model, and P(wi-1 wi) is a smoothed probability estimate in the original model. We notice that LFprobability of Equation (3) is very similar to that proposed by Seymore and Rosenfeld (1996), where the loss function is −N(wi−1wi )[logP&apos; (wi |wi−1) − logP(wi |wi−1)] Here N(wi-1wi) is the discounted frequency that bigram wi-1wi was observed in training. N(wi-1wi) is conceptually identical to P(wi-1 wi) in Equation (3). From Equations (2) and (3), we can see that lower LFprobability is strongly correlated with lower perplexity. However, we found that LFprobability is suboptimal as a pruning criterion, evaluated on CER in our experiments. We assume that it is largely due to the deficiency of perplexity as a LM performance measure. Although perplexity is widely used due to its simplic</context>
</contexts>
<marker>Seymore, Rosenfeld, 1996</marker>
<rawString>Seymore, K., and Rosenfeld, R. (1996). Scalable backoff language models. Proc. ICSLP, Vol. 1., pp.232-235, Philadelphia, 1996</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>Entropy-based Pruning of Backoff Language Models.</title>
<date>1998</date>
<booktitle>Proc. DARPA News Transcription and Understanding Workshop,</booktitle>
<pages>270--274</pages>
<location>Lansdowne, VA.</location>
<contexts>
<context position="5300" citStr="Stolcke, 1998" startWordPosition="820" endWordPosition="821"> of the pruning criterion, which is used to estimate the performance loss of the pruned model. Given the pruning criterion, a simple thresholding algorithm for pruning bigram models can be described as follows: 1. Select a threshold 0. 2. Compute the performance loss due to pruning each bigram individually using the pruning criterion. 3. Remove all bigrams with performance loss less than 0. 4. Re-compute backoff weights. Figure 1: Thresholding algorithm for bigram pruning The algorithm in Figure 1 together with several pruning criteria has been studied previously (Seymore and Rosenfeld, 1996; Stolcke, 1998; Gao and Lee, 2000; etc). A comparative study of these techniques is presented in (Goodman and Gao, 2000). In this paper, three pruning criteria will be studied: probability, rank, and entropy. Probability serves as the baseline pruning criterion. It is derived from perplexity which has been widely used as a LM evaluation measure. Rank and entropy have been previously used as a metric for LM evaluation in (Clarkson and Robinson, 2001). In the current paper, these two measures will be studied for the purpose of backoff n-gram model pruning. In the next section, we will describe how pruning cri</context>
<context position="10872" citStr="Stolcke, 1998" startWordPosition="1783" endWordPosition="1784"> of the probability distribution over the vocabulary V is generally given by H(w ) = −∑ V= 1 P(w |w ) log P(w |w ) . i j j i j i We propose to use entropy for pruning as follows: all bigrams that change entropy by less than a threshold after pruning are removed from the model. The corresponding loss function LFentropy is defined as 1 − ∑= ′ N − N 1( ( 1 ) ( 1)) H wi H wi i − − (5) where H is the entropy before pruning given history wi-1, H’ is the new entropy after pruning, and N is the size of the test data. The entropy-based pruning is conceptually similar to the pruning method proposed in (Stolcke, 1998). Stolcke used the Kullback-Leibler divergence between the pruned and un-pruned model probability distribution in a given context over the entire vocabulary. In particular, the increase in relative entropy from pruning a bigram is computed by P(wi−1wi)[logP&apos;(wi |wi−1)− log P(wi |wi−1)] where the summation is over all word pairs (wi-1wi). 4 Empirical Comparison We evaluated the pruning criteria introduced in the previous section on a realistic application, Chinese text input. In this application, a string of Pinyin (phonetic alphabet) is converted into Chinese characters, which is the standard </context>
</contexts>
<marker>Stolcke, 1998</marker>
<rawString>Stolcke, A. (1998). Entropy-based Pruning of Backoff Language Models. Proc. DARPA News Transcription and Understanding Workshop, 1998, pp. 270-274, Lansdowne, VA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>