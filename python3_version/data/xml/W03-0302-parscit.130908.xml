<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000337">
<title confidence="0.99009">
ProAlign: Shared Task System Description
</title>
<author confidence="0.997417">
Dekang Lin and Colin Cherry
</author>
<affiliation confidence="0.998922">
Department of Computing Science
University of Alberta
</affiliation>
<address confidence="0.595056">
Edmonton, Alberta, Canada, T6G 2E8
</address>
<email confidence="0.999064">
{lindek,colinc}@cs.ualberta.ca
</email>
<sectionHeader confidence="0.998601" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999802222222222">
ProAlign combines several different ap-
proaches in order to produce high quality word
word alignments. Like competitive linking,
ProAlign uses a constrained search to find high
scoring alignments. Like EM-based methods,
a probability model is used to rank possible
alignments. The goal of this paper is to give
a bird’s eye view of the ProAlign system to
encourage discussion and comparison.
</bodyText>
<sectionHeader confidence="0.940516" genericHeader="keywords">
1 Alignment Algorithm at a Glance
</sectionHeader>
<bodyText confidence="0.99705156">
We have submitted the ProAlign alignment system to
the WPT’03 shared task. It received a 5.71% AER on
the English-French task and 29.36% on the Romanian-
English task. These results are with the no-null data; our
output was not formatted to work with explicit nulls.
ProAlign works by iteratively improving an align-
ment. The algorithm creates an initial alignment us-
ing search, constraints, and summed φ2 correlation-based
scores (Gale and Church, 1991). This is similar to the
competitive linking process (Melamed, 2000). It then
learns a probability model from the current alignment,
and conducts a constrained search again, this time scor-
ing alignments according to the probability model. The
process continues until results on a validation set begin to
indicate over-fitting.
For the purposes of our algorithm, we view an align-
ment as a set of links between the words in a sen-
tence pair. Before describing the algorithm, we will de-
fine the following notation. Let E be an English sen-
tence e1, e2, ... , em and let F be a French sentence
f1, f2, ... , fn. We define a link l(ei, fj) to exist if ei and
fj are a translation (or part of a translation) of one an-
other. We define the null link l(ei, f0) to exist if ei does
not correspond to a translation for any French word in F.
The null link l(e0, fj) is defined similarly. An alignment
A for two sentences E and F is a set of links such that ev-
ery word in E and F participates in at least one link, and
a word linked to e0 or f0 participates in no other links. If
e occurs in E x times and f occurs in F y times, we say
that e and f co-occur xy times in this sentence pair.
ProAlign conducts a best-first search (with constant
beam and agenda size) to search a constrained space of
possible alignments. A state in this space is a partial
alignment, and a transition is defined as the addition of
a single link to the current state. Any link which would
create a state that does not violate any constraint is con-
sidered to be a valid transition. Our start state is the empty
alignment, where all words in E and F are implicitly
linked to null. A terminal state is a state in which no more
links can be added without violating a constraint. Our
goal is to find the terminal state with the highest proba-
bility.
To complete this algorithm, one requires a set of con-
straints and a method for determining which alignment is
most likely. These are presented in the next two sections.
The algorithm takes as input a set of English-French sen-
tence pairs, along with dependency trees for the English
sentences. The presence of the English dependency tree
allows us to incorporate linguistic features into our model
and linguistic intuitions into our constraints.
</bodyText>
<sectionHeader confidence="0.995341" genericHeader="introduction">
2 Constraints
</sectionHeader>
<bodyText confidence="0.971415459459459">
The model used for scoring alignments has no mecha-
nism to prevent certain types of undesirable alignments,
such as having all French words align to the same En-
glish word. To guide the search to correct alignments, we
employ two constraints to limit our search for the most
probable alignment. The first constraint is the one-to-one
constraint (Melamed, 2000): every word (except the null
words e0 and f0) participates in exactly one link.
The second constraint, known as the cohesion con-
straint (Fox, 2002), uses the dependency tree (Mel’ˇcuk,
1987) of the English sentence to restrict possible link
combinations. Given the dependency tree TE and a (par-
tial) alignment A, the cohesion constraint requires that
phrasal cohesion is maintained in the French sentence. If
two phrases are disjoint in the English sentence, the align-
ment must not map them to overlapping intervals in the
French sentence. This notion of phrasal constraints on
alignments need not be restricted to phrases determined
from a dependency structure. However, the experiments
conducted in (Fox, 2002) indicate that dependency trees
demonstrate a higher degree of phrasal cohesion during
translation than other structures.
Consider the partial alignment in Figure 1. The most
probable lexical match for the English word to is the
French word `a. When the system attempts to link to and
`a, the distinct English phrases [the reboot] and [the host
to discover all the devices] will be mapped to intervals
in the French sentence, creating the induced phrasal in-
tervals [`a ... [r´einitialisation] ... p´eriph´eriques]. Re-
gardless of what these French phrases will be after the
alignment is completed, we know now that their intervals
will overlap. Therefore, this link will not be added to the
partial alignment.
mod obj
pre
det subj det subjaux det
The reboot causes the host to discover all the devices
</bodyText>
<figure confidence="0.70346625">
1 2 3 4 5 6 7 8 9 10
1 2 3 4 5 6 7 8 9 10 11
Suite à la réinitialisation , l&apos; hôte repère tous les périphériques
after to the reboot the host locate all the peripherals
</figure>
<figureCaption confidence="0.99973">
Figure 1: An Example of Cohesion Constraint
</figureCaption>
<bodyText confidence="0.9094238">
To define this notion more formally, let TE(ei) be
the subtree of TE rooted at ei. The phrase span of
ei, spanP(ei, TE, A), is the image of the English phrase
headed by ei in F given a (partial) alignment A. More
precisely, spanP(ei, TE, A) = [k1, k2], where
</bodyText>
<equation confidence="0.865335166666667">
k1 = min{j|l(u, j) ∈ A, e., ∈ TE(ei)}
k2 = max{j|l(u, j) ∈ A, e., ∈ TE(ei)}
The head span is the image of ei itself. We define
spanH(ei, TE, A) = [k1, k2], where
k1 = min{j|l(i, j) ∈ A}
k2 = max{j|l(i, j) ∈ A}
</equation>
<bodyText confidence="0.93820225">
In Figure 1, for the node reboot, the phrase span is
[4,4] and the head span is also [4,4]; for the node discover
(with the link between to and a` in place), the phrase span
is [2,11] and the head span is the empty set ∅.
With these definitions of phrase and head spans, we de-
fine two notions of overlap, originally introduced in (Fox,
2002) as crossings. Given a head node eh and its modi-
fier em, a head-modifier overlap occurs when:
spanH(eh, TE, A) ∩ spanP(em, TE, A) =6 ∅
Given two nodes em1 and em2 which both modify the
same head node, a modifier-modifier overlap occurs
when:
</bodyText>
<equation confidence="0.231923">
spanP(em1, TE, A) ∩ spanP(em2, TE, A) =6 ∅
</equation>
<bodyText confidence="0.999618307692308">
Following (Fox, 2002), we say an alignment is co-
hesive with respect to TE if it does not introduce
any head-modifier or modifier-modifier overlaps. For
example, the alignment A in Figure 1 is not cohe-
sive because spanP(reboot, TE, A) = [4, 4] intersects
spanP(discover, TE, A) = [2,11]. Since both reboot
and discover modify causes, this creates a modifier-
modifier overlap. One can check for constraint viola-
tions inexpensively by incrementally updating the vari-
ous spans as new links are added to the partial alignment,
and checking for overlap after each modification. More
details on the cohesion constraint can be found in (Lin
and Cherry, 2003).
</bodyText>
<sectionHeader confidence="0.999245" genericHeader="method">
3 Probability Model
</sectionHeader>
<bodyText confidence="0.996897214285714">
We define the word alignment problem as finding the
alignment A that maximizes P(A|E, F). ProAlign mod-
els P(A|E, F) directly, using a different decomposition
of terms than the model used by IBM (Brown et al.,
1993). In the IBM models of translation, alignments ex-
ist as artifacts of a stochastic process, where the words
in the English sentence generate the words in the French
sentence. Our model does not assume that one sentence
generates the other. Instead it takes both sentences as
given, and uses the sentences to determine an alignment.
An alignment A consists of t links {l1, l2, ... , lt}, where
each lk = l(eik, fjk) for some ik and jk. We will refer to
consecutive subsets of A as lji = {li, li+1, ... , lj}. Given
this notation, P(A|E, F) can be decomposed as follows:
</bodyText>
<equation confidence="0.771857">
P(A|E, F) = P(lt1|E, F) =
</equation>
<bodyText confidence="0.683992">
At this point, we factor P(lk|E, F, lk−1
1 ) to make com-
putation feasible. Let Ck = {E, F,lk−1
1 } represent the
context of lk. Note that both the context Ck and the link
lk imply the occurrence of eik and fjk. We can rewrite
</bodyText>
<equation confidence="0.993917">
P(lk|Ck) as:
P(lk|E, F, lk−1
1 )
t
H
k=1
P(lk|Ck) = P(Ck) =P(Ck, eik, fjk)
P(Ck|lk)
= P(lk|eik, fjk) ×
P(Ck|eik, fjk)
P(lk,Ck) P(Ck|lk)P(lk)
</equation>
<bodyText confidence="0.987993076923077">
Here P(lk|eik, fjk) is link probability given a co-
occurrence of the two words, which is similar in spirit to
Melamed’s explicit noise model (Melamed, 2000). This
term depends only on the words involved directly in the
link. The ratio P(Ck|lk) modifies the link probability,
P(Ck |eik ,fjk
providing context-sensitive information.
Ck remains too broad to deal with in practical sys-
tems. We will consider only a subset FTk of relevant
features of Ck. We will make the Naive Bayes-style as-
sumption that these features ft ∈ FTk are conditionally
independent given either lk or (eik, fjk). This produces a
tractable formulation for P(A|E, F):
</bodyText>
<figure confidence="0.9752787">
pre�
subj�
det❑ det[]
obj❑
theF]host❑ discovers❑all®he[devices❑
111 211 311 411 511 611
111211 311 411 511
6❑
l&apos; hôteC-tepère❑tousdesloériphériques❑
thelhost❑ locate❑ all❑ the[] peripherals[]
</figure>
<figureCaption confidence="0.99667">
Figure 2: Feature Extraction Example
</figureCaption>
<equation confidence="0.978742">
⎛ ⎞
P(lk|eik,fjk) × Y P(/P(ft|lk)
ftEFTk (ft |eik, fjk )
</equation>
<bodyText confidence="0.993315">
More details on the probability model used by ProAlign
are available in (Cherry and Lin, 2003).
</bodyText>
<subsectionHeader confidence="0.998906">
3.1 Features used in the shared task
</subsectionHeader>
<bodyText confidence="0.999878675675676">
For the purposes of the shared task, we use two feature
types. Each type could have any number of instantiations
for any number of contexts. Note that each feature type
is described in terms of the context surrounding a word
pair.
The first feature type fta concerns surrounding links.
It has been observed that words close to each other in
the source language tend to remain close to each other in
the translation (S. Vogel and Tillmann, 1996). To capture
this notion, for any word pair (ei, fj), if a link l(ei0, fj0)
exists within a window of two words (where i − 2 ≤ i&apos; ≤
i+2 and j −2 ≤ j&apos; ≤ j +2), then we say that the feature
fta(i − i&apos;, j − j&apos;, ei0) is active for this context. We refer
to these as adjacency features.
The second feature type ftd uses the English parse tree
to capture regularities among grammatical relations be-
tween languages. For example, when dealing with French
and English, the location of the determiner with respect
to its governor is never swapped during translation, while
the location of adjectives is swapped frequently. For any
word pair (ei, fj), let ei0 be the governor of ei, and let
rel be the relationship between them. If a link l(ei0, fj0)
exists, then we say that the feature ftd(j − j&apos;, rel) is ac-
tive for this context. We refer to these as dependency
features.
Take for example Figure 2 which shows a partial align-
ment with all links completed except for those involving
the. Given this sentence pair and English parse tree, we
can extract features of both types to assist in the align-
ment of the1. The word pair (the1, l&apos;) will have an active
adjacency feature fta(+1, +1, host) as well as a depen-
dency feature ftd(−1, det). These two features will work
together to increase the probability of this correct link.
In contrast, the incorrect link (the1, les) will have only
ftd(+3, det), which will work to lower the link probabil-
ity, since most determiners are located before their gov-
ernors.
</bodyText>
<subsectionHeader confidence="0.997006">
3.2 Training the model
</subsectionHeader>
<bodyText confidence="0.999980052631579">
Since we always work from a current alignment, training
the model is a simple matter of counting events in the
current alignment. Link probability is the number of time
two words are linked, divided by the number of times
they co-occur. The various feature probabilities can be
calculated by also counting the number of times a feature
occurs in the context of a linked pair of words, and the
number of times the feature is active for co-occurrences
of the same word pair.
Considering only a single, potentially noisy alignment
for a given sentence pair can result in reinforcing errors
present in the current alignment during training. To avoid
this problem, we sample from a space of probable align-
ments, as is done in IBM models 3 and above (Brown
et al., 1993), and weight counts based on the likelihood
of each alignment sampled under the current probability
model. To further reduce the impact of rare, and poten-
tially incorrect events, we also smooth our probabilities
using m-estimate smoothing (Mitchell, 1997).
</bodyText>
<sectionHeader confidence="0.998029" genericHeader="method">
4 Multiple Alignments
</sectionHeader>
<bodyText confidence="0.999956">
The result of the constrained alignment search is a high-
precision, word-to-word alignment. We then relax the
word-to-word constraint, and use statistics regarding col-
locations with unaligned words in order to make many-to-
one alignments. We also employ a further relaxed link-
ing process to catch some cases where the cohesion con-
straint ruled out otherwise good alignments. These auxil-
iary methods are currently not integrated into our search
or our probability model, although that is certainly a di-
rection for future work.
</bodyText>
<equation confidence="0.937169">
t
Y
k=1
</equation>
<sectionHeader confidence="0.998334" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999996">
We have presented a brief overview of the major ideas
behind our entry to the WPT’03 Shared Task. Primary
among these ideas are the use of a cohesion constraint in
search, and our novel probability model.
</bodyText>
<sectionHeader confidence="0.998643" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999856">
This project is funded by and jointly undertaken with Sun
Microsystems, Inc. We wish to thank Finola Brady, Bob
Kuhns and Michael McHugh for their help. We also wish
to thank the WPT’03 reviewers for their helpful com-
ments.
</bodyText>
<sectionHeader confidence="0.999678" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999742">
P. F. Brown, V. S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263–312.
Colin Cherry and Dekang Lin. 2003. A probability
model to improve word alignment. Submitted.
Heidi J. Fox. 2002. Phrasal cohesion and statistical
machine translation. In 2002 Conference on Empiri-
cal Methods in Natural Language Processing (EMNLP
2002), pages 304–311.
W.A. Gale and K.W. Church. 1991. Identifying word
correspondences in parallel texts. In 4th Speech
and Natural Language Workshop, pages 152–157.
DARPA, Morgan Kaufmann.
Dekang Lin and Colin Cherry. 2003. Word alignment
with cohesion constraint. Submitted.
I. Dan Melamed. 2000. Models of translational equiv-
alence among words. Computational Linguistics,
26(2):221–249, June.
Igor A. Mel’ˇcuk. 1987. Dependency syntax: theory and
practice. State University of New York Press, Albany.
Tom Mitchell. 1997. Machine Learning. McGraw Hill.
H. Ney S. Vogel and C. Tillmann. 1996. HMM-based
word alignment in statistical translation. In 16th In-
ternational Conference on Computational Linguistics,
pages 836–841, Copenhagen, Denmark, August.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.044087">
<title confidence="0.999932">ProAlign: Shared Task System Description</title>
<author confidence="0.999105">Lin</author>
<affiliation confidence="0.9990335">Department of Computing University of</affiliation>
<address confidence="0.958279">Edmonton, Alberta, Canada, T6G</address>
<abstract confidence="0.998487066666667">ProAlign combines several different approaches in order to produce high quality word word alignments. Like competitive linking, ProAlign uses a constrained search to find high scoring alignments. Like EM-based methods, a probability model is used to rank possible alignments. The goal of this paper is to give a bird’s eye view of the ProAlign system to encourage discussion and comparison. 1 Alignment Algorithm at a Glance We have submitted the ProAlign alignment system to the WPT’03 shared task. It received a 5.71% AER on the English-French task and 29.36% on the Romanian- English task. These results are with the no-null data; our output was not formatted to work with explicit nulls. ProAlign works by iteratively improving an alignment. The algorithm creates an initial alignment ussearch, constraints, and summed correlation-based scores (Gale and Church, 1991). This is similar to the competitive linking process (Melamed, 2000). It then learns a probability model from the current alignment, and conducts a constrained search again, this time scoring alignments according to the probability model. The process continues until results on a validation set begin to indicate over-fitting. For the purposes of our algorithm, we view an alignment as a set of links between the words in a sentence pair. Before describing the algorithm, we will dethe following notation. Let an English sen- ... , and let a French sentence ... , We define a exist if a translation (or part of a translation) of one an- We define the link exist if correspond to a translation for any French word in null link defined similarly. An two sentences a set of links such that evword in in at least one link, and word linked to in no other links. If in x and in y we say in this sentence pair. ProAlign conducts a best-first search (with constant beam and agenda size) to search a constrained space of possible alignments. A state in this space is a partial alignment, and a transition is defined as the addition of a single link to the current state. Any link which would create a state that does not violate any constraint is considered to be a valid transition. Our start state is the empty where all words in implicitly linked to null. A terminal state is a state in which no more links can be added without violating a constraint. Our goal is to find the terminal state with the highest probability. To complete this algorithm, one requires a set of constraints and a method for determining which alignment is most likely. These are presented in the next two sections. The algorithm takes as input a set of English-French sentence pairs, along with dependency trees for the English sentences. The presence of the English dependency tree allows us to incorporate linguistic features into our model and linguistic intuitions into our constraints. 2 Constraints The model used for scoring alignments has no mechanism to prevent certain types of undesirable alignments, such as having all French words align to the same English word. To guide the search to correct alignments, we employ two constraints to limit our search for the most alignment. The first constraint is the 2000): every word (except the null participates in exactly one link. second constraint, known as the con- 2002), uses the dependency tree (Mel’ˇcuk, 1987) of the English sentence to restrict possible link Given the dependency tree a (paralignment the cohesion constraint requires that phrasal cohesion is maintained in the French sentence. If two phrases are disjoint in the English sentence, the alignment must not map them to overlapping intervals in the French sentence. This notion of phrasal constraints on alignments need not be restricted to phrases determined from a dependency structure. However, the experiments conducted in (Fox, 2002) indicate that dependency trees demonstrate a higher degree of phrasal cohesion during translation than other structures. Consider the partial alignment in Figure 1. The most lexical match for the English word the word When the system attempts to link the distinct English phrases and host discover all the will be mapped to intervals in the French sentence, creating the induced phrasal in- Regardless of what these French phrases will be after the alignment is completed, we know now that their intervals will overlap. Therefore, this link will not be added to the partial alignment. mod obj pre det subj det subjaux det The reboot causes the host to discover all the devices 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10 11 Suite à la réinitialisation , l&apos; hôte repère tous les périphériques after to the reboot the host locate all the peripherals Figure 1: An Example of Cohesion Constraint define this notion more formally, let subtree of at The span is the image of the English phrase by a (partial) alignment More = where span the image of We define = where Figure 1, for the node the phrase span is and the head span is also [4,4]; for the node the link between place), the phrase span [2,11] and the head span is the empty set With these definitions of phrase and head spans, we define two notions of overlap, originally introduced in (Fox, as Given a head node its modia overlap when: ∅ two nodes both modify the head node, a overlap when: ∅ Following (Fox, 2002), we say an alignment is cowith respect to it does not introduce any head-modifier or modifier-modifier overlaps. For the alignment Figure 1 is not cohebecause = = Since both this creates a modifiermodifier overlap. One can check for constraint violations inexpensively by incrementally updating the various spans as new links are added to the partial alignment, and checking for overlap after each modification. More details on the cohesion constraint can be found in (Lin and Cherry, 2003). 3 Probability Model We define the word alignment problem as finding the maximizes ProAlign modusing a different decomposition of terms than the model used by IBM (Brown et al., 1993). In the IBM models of translation, alignments exist as artifacts of a stochastic process, where the words in the English sentence generate the words in the French sentence. Our model does not assume that one sentence generates the other. Instead it takes both sentences as given, and uses the sentences to determine an alignment. alignment of ... , where some We will refer to subsets of , Given notation, be decomposed as follows: = = this point, we factor F, make comfeasible. Let the of Note that both the context the link the occurrence of We can rewrite F, t H = link probability given a cooccurrence of the two words, which is similar in spirit to Melamed’s explicit noise model (Melamed, 2000). This term depends only on the words involved directly in the The ratio the link probability, providing context-sensitive information. too broad to deal with in practical sys- We will consider only a subset relevant of We will make the Naive Bayes-style asthat these features conditionally given either This produces a formulation for pre� subj� det❑ det[] obj❑ theF]host❑ discovers❑all®he[devices❑</abstract>
<phone confidence="0.655325">111 211 311 411 511 611 111211 311 411 511</phone>
<abstract confidence="0.967766559633028">6❑ l&apos; hôteC-tepère❑tousdesloériphériques❑ thelhost❑ locate❑ all❑ the[] peripherals[] Figure 2: Feature Extraction Example ⎛ ⎞ Y More details on the probability model used by ProAlign are available in (Cherry and Lin, 2003). 3.1 Features used in the shared task For the purposes of the shared task, we use two feature types. Each type could have any number of instantiations for any number of contexts. Note that each feature type is described in terms of the context surrounding a word pair. first feature type concerns surrounding links. It has been observed that words close to each other in the source language tend to remain close to each other in the translation (S. Vogel and Tillmann, 1996). To capture notion, for any word pair if a link within a window of two words (where ≤ ≤ then we say that the feature j active for this context. We refer these as second feature type uses the English parse tree to capture regularities among grammatical relations between languages. For example, when dealing with French and English, the location of the determiner with respect to its governor is never swapped during translation, while the location of adjectives is swapped frequently. For any pair let the governor of and let the relationship between them. If a link then we say that the feature acfor this context. We refer to these as Take for example Figure 2 which shows a partial alignment with all links completed except for those involving Given this sentence pair and English parse tree, we can extract features of both types to assist in the alignof The word pair have an active feature well as a depenfeature These two features will work together to increase the probability of this correct link. contrast, the incorrect link have only which will work to lower the link probability, since most determiners are located before their governors. 3.2 Training the model Since we always work from a current alignment, training the model is a simple matter of counting events in the current alignment. Link probability is the number of time two words are linked, divided by the number of times they co-occur. The various feature probabilities can be calculated by also counting the number of times a feature occurs in the context of a linked pair of words, and the number of times the feature is active for co-occurrences of the same word pair. Considering only a single, potentially noisy alignment for a given sentence pair can result in reinforcing errors present in the current alignment during training. To avoid this problem, we sample from a space of probable alignments, as is done in IBM models 3 and above (Brown et al., 1993), and weight counts based on the likelihood of each alignment sampled under the current probability model. To further reduce the impact of rare, and potentially incorrect events, we also smooth our probabilities smoothing (Mitchell, 1997). 4 Multiple Alignments The result of the constrained alignment search is a highprecision, word-to-word alignment. We then relax the word-to-word constraint, and use statistics regarding collocations with unaligned words in order to make many-toone alignments. We also employ a further relaxed linking process to catch some cases where the cohesion constraint ruled out otherwise good alignments. These auxiliary methods are currently not integrated into our search or our probability model, although that is certainly a direction for future work. t Y 5 Conclusions We have presented a brief overview of the major ideas behind our entry to the WPT’03 Shared Task. Primary among these ideas are the use of a cohesion constraint in search, and our novel probability model. Acknowledgments This project is funded by and jointly undertaken with Sun Microsystems, Inc. We wish to thank Finola Brady, Bob Kuhns and Michael McHugh for their help. We also wish to thank the WPT’03 reviewers for their helpful comments. References P. F. Brown, V. S. A. Della Pietra, V. J. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical translation: Parameter estimation. Computa- 19(2):263–312. Colin Cherry and Dekang Lin. 2003. A probability model to improve word alignment. Submitted. Heidi J. Fox. 2002. Phrasal cohesion and statistical translation. In Conference on Empirical Methods in Natural Language Processing (EMNLP pages 304–311. W.A. Gale and K.W. Church. 1991. Identifying word in parallel texts. In Speech Natural Language pages 152–157. DARPA, Morgan Kaufmann. Dekang Lin and Colin Cherry. 2003. Word alignment with cohesion constraint. Submitted. I. Dan Melamed. 2000. Models of translational equivamong words. 26(2):221–249, June.</abstract>
<note confidence="0.890982285714286">A. Mel’ˇcuk. 1987. syntax: theory and State University of New York Press, Albany. Mitchell. 1997. McGraw Hill. H. Ney S. Vogel and C. Tillmann. 1996. HMM-based alignment in statistical translation. In In- Conference on Computational pages 836–841, Copenhagen, Denmark, August.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>V S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="7407" citStr="Brown et al., 1993" startWordPosition="1295" endWordPosition="1298">er, TE, A) = [2,11]. Since both reboot and discover modify causes, this creates a modifiermodifier overlap. One can check for constraint violations inexpensively by incrementally updating the various spans as new links are added to the partial alignment, and checking for overlap after each modification. More details on the cohesion constraint can be found in (Lin and Cherry, 2003). 3 Probability Model We define the word alignment problem as finding the alignment A that maximizes P(A|E, F). ProAlign models P(A|E, F) directly, using a different decomposition of terms than the model used by IBM (Brown et al., 1993). In the IBM models of translation, alignments exist as artifacts of a stochastic process, where the words in the English sentence generate the words in the French sentence. Our model does not assume that one sentence generates the other. Instead it takes both sentences as given, and uses the sentences to determine an alignment. An alignment A consists of t links {l1, l2, ... , lt}, where each lk = l(eik, fjk) for some ik and jk. We will refer to consecutive subsets of A as lji = {li, li+1, ... , lj}. Given this notation, P(A|E, F) can be decomposed as follows: P(A|E, F) = P(lt1|E, F) = At thi</context>
<context position="12152" citStr="Brown et al., 1993" startWordPosition="2134" endWordPosition="2137">y is the number of time two words are linked, divided by the number of times they co-occur. The various feature probabilities can be calculated by also counting the number of times a feature occurs in the context of a linked pair of words, and the number of times the feature is active for co-occurrences of the same word pair. Considering only a single, potentially noisy alignment for a given sentence pair can result in reinforcing errors present in the current alignment during training. To avoid this problem, we sample from a space of probable alignments, as is done in IBM models 3 and above (Brown et al., 1993), and weight counts based on the likelihood of each alignment sampled under the current probability model. To further reduce the impact of rare, and potentially incorrect events, we also smooth our probabilities using m-estimate smoothing (Mitchell, 1997). 4 Multiple Alignments The result of the constrained alignment search is a highprecision, word-to-word alignment. We then relax the word-to-word constraint, and use statistics regarding collocations with unaligned words in order to make many-toone alignments. We also employ a further relaxed linking process to catch some cases where the cohes</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. F. Brown, V. S. A. Della Pietra, V. J. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>Dekang Lin</author>
</authors>
<title>A probability model to improve word alignment.</title>
<date>2003</date>
<note>Submitted.</note>
<contexts>
<context position="9386" citStr="Cherry and Lin, 2003" startWordPosition="1639" endWordPosition="1642">sider only a subset FTk of relevant features of Ck. We will make the Naive Bayes-style assumption that these features ft ∈ FTk are conditionally independent given either lk or (eik, fjk). This produces a tractable formulation for P(A|E, F): pre� subj� det❑ det[] obj❑ theF]host❑ discovers❑all®he[devices❑ 111 211 311 411 511 611 111211 311 411 511 6❑ l&apos; hôteC-tepère❑tousdesloériphériques❑ thelhost❑ locate❑ all❑ the[] peripherals[] Figure 2: Feature Extraction Example ⎛ ⎞ P(lk|eik,fjk) × Y P(/P(ft|lk) ftEFTk (ft |eik, fjk ) More details on the probability model used by ProAlign are available in (Cherry and Lin, 2003). 3.1 Features used in the shared task For the purposes of the shared task, we use two feature types. Each type could have any number of instantiations for any number of contexts. Note that each feature type is described in terms of the context surrounding a word pair. The first feature type fta concerns surrounding links. It has been observed that words close to each other in the source language tend to remain close to each other in the translation (S. Vogel and Tillmann, 1996). To capture this notion, for any word pair (ei, fj), if a link l(ei0, fj0) exists within a window of two words (wher</context>
</contexts>
<marker>Cherry, Lin, 2003</marker>
<rawString>Colin Cherry and Dekang Lin. 2003. A probability model to improve word alignment. Submitted.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heidi J Fox</author>
</authors>
<title>Phrasal cohesion and statistical machine translation.</title>
<date>2002</date>
<booktitle>In 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<pages>304--311</pages>
<contexts>
<context position="3848" citStr="Fox, 2002" startWordPosition="665" endWordPosition="666">porate linguistic features into our model and linguistic intuitions into our constraints. 2 Constraints The model used for scoring alignments has no mechanism to prevent certain types of undesirable alignments, such as having all French words align to the same English word. To guide the search to correct alignments, we employ two constraints to limit our search for the most probable alignment. The first constraint is the one-to-one constraint (Melamed, 2000): every word (except the null words e0 and f0) participates in exactly one link. The second constraint, known as the cohesion constraint (Fox, 2002), uses the dependency tree (Mel’ˇcuk, 1987) of the English sentence to restrict possible link combinations. Given the dependency tree TE and a (partial) alignment A, the cohesion constraint requires that phrasal cohesion is maintained in the French sentence. If two phrases are disjoint in the English sentence, the alignment must not map them to overlapping intervals in the French sentence. This notion of phrasal constraints on alignments need not be restricted to phrases determined from a dependency structure. However, the experiments conducted in (Fox, 2002) indicate that dependency trees dem</context>
<context position="6237" citStr="Fox, 2002" startWordPosition="1097" endWordPosition="1098">ment A. More precisely, spanP(ei, TE, A) = [k1, k2], where k1 = min{j|l(u, j) ∈ A, e., ∈ TE(ei)} k2 = max{j|l(u, j) ∈ A, e., ∈ TE(ei)} The head span is the image of ei itself. We define spanH(ei, TE, A) = [k1, k2], where k1 = min{j|l(i, j) ∈ A} k2 = max{j|l(i, j) ∈ A} In Figure 1, for the node reboot, the phrase span is [4,4] and the head span is also [4,4]; for the node discover (with the link between to and a` in place), the phrase span is [2,11] and the head span is the empty set ∅. With these definitions of phrase and head spans, we define two notions of overlap, originally introduced in (Fox, 2002) as crossings. Given a head node eh and its modifier em, a head-modifier overlap occurs when: spanH(eh, TE, A) ∩ spanP(em, TE, A) =6 ∅ Given two nodes em1 and em2 which both modify the same head node, a modifier-modifier overlap occurs when: spanP(em1, TE, A) ∩ spanP(em2, TE, A) =6 ∅ Following (Fox, 2002), we say an alignment is cohesive with respect to TE if it does not introduce any head-modifier or modifier-modifier overlaps. For example, the alignment A in Figure 1 is not cohesive because spanP(reboot, TE, A) = [4, 4] intersects spanP(discover, TE, A) = [2,11]. Since both reboot and discov</context>
</contexts>
<marker>Fox, 2002</marker>
<rawString>Heidi J. Fox. 2002. Phrasal cohesion and statistical machine translation. In 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP 2002), pages 304–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Gale</author>
<author>K W Church</author>
</authors>
<title>Identifying word correspondences in parallel texts.</title>
<date>1991</date>
<booktitle>In 4th Speech and Natural Language Workshop,</booktitle>
<pages>152--157</pages>
<publisher>DARPA, Morgan Kaufmann.</publisher>
<contexts>
<context position="1074" citStr="Gale and Church, 1991" startWordPosition="159" endWordPosition="162"> possible alignments. The goal of this paper is to give a bird’s eye view of the ProAlign system to encourage discussion and comparison. 1 Alignment Algorithm at a Glance We have submitted the ProAlign alignment system to the WPT’03 shared task. It received a 5.71% AER on the English-French task and 29.36% on the RomanianEnglish task. These results are with the no-null data; our output was not formatted to work with explicit nulls. ProAlign works by iteratively improving an alignment. The algorithm creates an initial alignment using search, constraints, and summed φ2 correlation-based scores (Gale and Church, 1991). This is similar to the competitive linking process (Melamed, 2000). It then learns a probability model from the current alignment, and conducts a constrained search again, this time scoring alignments according to the probability model. The process continues until results on a validation set begin to indicate over-fitting. For the purposes of our algorithm, we view an alignment as a set of links between the words in a sentence pair. Before describing the algorithm, we will define the following notation. Let E be an English sentence e1, e2, ... , em and let F be a French sentence f1, f2, ... </context>
</contexts>
<marker>Gale, Church, 1991</marker>
<rawString>W.A. Gale and K.W. Church. 1991. Identifying word correspondences in parallel texts. In 4th Speech and Natural Language Workshop, pages 152–157. DARPA, Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Colin Cherry</author>
</authors>
<title>Word alignment with cohesion constraint.</title>
<date>2003</date>
<note>Submitted.</note>
<contexts>
<context position="7171" citStr="Lin and Cherry, 2003" startWordPosition="1255" endWordPosition="1258">y an alignment is cohesive with respect to TE if it does not introduce any head-modifier or modifier-modifier overlaps. For example, the alignment A in Figure 1 is not cohesive because spanP(reboot, TE, A) = [4, 4] intersects spanP(discover, TE, A) = [2,11]. Since both reboot and discover modify causes, this creates a modifiermodifier overlap. One can check for constraint violations inexpensively by incrementally updating the various spans as new links are added to the partial alignment, and checking for overlap after each modification. More details on the cohesion constraint can be found in (Lin and Cherry, 2003). 3 Probability Model We define the word alignment problem as finding the alignment A that maximizes P(A|E, F). ProAlign models P(A|E, F) directly, using a different decomposition of terms than the model used by IBM (Brown et al., 1993). In the IBM models of translation, alignments exist as artifacts of a stochastic process, where the words in the English sentence generate the words in the French sentence. Our model does not assume that one sentence generates the other. Instead it takes both sentences as given, and uses the sentences to determine an alignment. An alignment A consists of t link</context>
</contexts>
<marker>Lin, Cherry, 2003</marker>
<rawString>Dekang Lin and Colin Cherry. 2003. Word alignment with cohesion constraint. Submitted.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
</authors>
<title>Models of translational equivalence among words.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>2</issue>
<contexts>
<context position="1142" citStr="Melamed, 2000" startWordPosition="171" endWordPosition="172"> the ProAlign system to encourage discussion and comparison. 1 Alignment Algorithm at a Glance We have submitted the ProAlign alignment system to the WPT’03 shared task. It received a 5.71% AER on the English-French task and 29.36% on the RomanianEnglish task. These results are with the no-null data; our output was not formatted to work with explicit nulls. ProAlign works by iteratively improving an alignment. The algorithm creates an initial alignment using search, constraints, and summed φ2 correlation-based scores (Gale and Church, 1991). This is similar to the competitive linking process (Melamed, 2000). It then learns a probability model from the current alignment, and conducts a constrained search again, this time scoring alignments according to the probability model. The process continues until results on a validation set begin to indicate over-fitting. For the purposes of our algorithm, we view an alignment as a set of links between the words in a sentence pair. Before describing the algorithm, we will define the following notation. Let E be an English sentence e1, e2, ... , em and let F be a French sentence f1, f2, ... , fn. We define a link l(ei, fj) to exist if ei and fj are a transla</context>
<context position="3700" citStr="Melamed, 2000" startWordPosition="640" endWordPosition="641"> of English-French sentence pairs, along with dependency trees for the English sentences. The presence of the English dependency tree allows us to incorporate linguistic features into our model and linguistic intuitions into our constraints. 2 Constraints The model used for scoring alignments has no mechanism to prevent certain types of undesirable alignments, such as having all French words align to the same English word. To guide the search to correct alignments, we employ two constraints to limit our search for the most probable alignment. The first constraint is the one-to-one constraint (Melamed, 2000): every word (except the null words e0 and f0) participates in exactly one link. The second constraint, known as the cohesion constraint (Fox, 2002), uses the dependency tree (Mel’ˇcuk, 1987) of the English sentence to restrict possible link combinations. Given the dependency tree TE and a (partial) alignment A, the cohesion constraint requires that phrasal cohesion is maintained in the French sentence. If two phrases are disjoint in the English sentence, the alignment must not map them to overlapping intervals in the French sentence. This notion of phrasal constraints on alignments need not b</context>
<context position="8523" citStr="Melamed, 2000" startWordPosition="1505" endWordPosition="1506">lj}. Given this notation, P(A|E, F) can be decomposed as follows: P(A|E, F) = P(lt1|E, F) = At this point, we factor P(lk|E, F, lk−1 1 ) to make computation feasible. Let Ck = {E, F,lk−1 1 } represent the context of lk. Note that both the context Ck and the link lk imply the occurrence of eik and fjk. We can rewrite P(lk|Ck) as: P(lk|E, F, lk−1 1 ) t H k=1 P(lk|Ck) = P(Ck) =P(Ck, eik, fjk) P(Ck|lk) = P(lk|eik, fjk) × P(Ck|eik, fjk) P(lk,Ck) P(Ck|lk)P(lk) Here P(lk|eik, fjk) is link probability given a cooccurrence of the two words, which is similar in spirit to Melamed’s explicit noise model (Melamed, 2000). This term depends only on the words involved directly in the link. The ratio P(Ck|lk) modifies the link probability, P(Ck |eik ,fjk providing context-sensitive information. Ck remains too broad to deal with in practical systems. We will consider only a subset FTk of relevant features of Ck. We will make the Naive Bayes-style assumption that these features ft ∈ FTk are conditionally independent given either lk or (eik, fjk). This produces a tractable formulation for P(A|E, F): pre� subj� det❑ det[] obj❑ theF]host❑ discovers❑all®he[devices❑ 111 211 311 411 511 611 111211 311 411 511 6❑ l&apos; hôte</context>
</contexts>
<marker>Melamed, 2000</marker>
<rawString>I. Dan Melamed. 2000. Models of translational equivalence among words. Computational Linguistics, 26(2):221–249, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor A Mel’ˇcuk</author>
</authors>
<title>Dependency syntax: theory and practice.</title>
<date>1987</date>
<publisher>Press,</publisher>
<institution>State University of New York</institution>
<location>Albany. Tom Mitchell.</location>
<marker>Mel’ˇcuk, 1987</marker>
<rawString>Igor A. Mel’ˇcuk. 1987. Dependency syntax: theory and practice. State University of New York Press, Albany. Tom Mitchell. 1997. Machine Learning. McGraw Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ney S Vogel</author>
<author>C Tillmann</author>
</authors>
<title>HMM-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In 16th International Conference on Computational Linguistics,</booktitle>
<pages>836--841</pages>
<location>Copenhagen, Denmark,</location>
<contexts>
<context position="9869" citStr="Vogel and Tillmann, 1996" startWordPosition="1724" endWordPosition="1727">eik,fjk) × Y P(/P(ft|lk) ftEFTk (ft |eik, fjk ) More details on the probability model used by ProAlign are available in (Cherry and Lin, 2003). 3.1 Features used in the shared task For the purposes of the shared task, we use two feature types. Each type could have any number of instantiations for any number of contexts. Note that each feature type is described in terms of the context surrounding a word pair. The first feature type fta concerns surrounding links. It has been observed that words close to each other in the source language tend to remain close to each other in the translation (S. Vogel and Tillmann, 1996). To capture this notion, for any word pair (ei, fj), if a link l(ei0, fj0) exists within a window of two words (where i − 2 ≤ i&apos; ≤ i+2 and j −2 ≤ j&apos; ≤ j +2), then we say that the feature fta(i − i&apos;, j − j&apos;, ei0) is active for this context. We refer to these as adjacency features. The second feature type ftd uses the English parse tree to capture regularities among grammatical relations between languages. For example, when dealing with French and English, the location of the determiner with respect to its governor is never swapped during translation, while the location of adjectives is swapped</context>
</contexts>
<marker>Vogel, Tillmann, 1996</marker>
<rawString>H. Ney S. Vogel and C. Tillmann. 1996. HMM-based word alignment in statistical translation. In 16th International Conference on Computational Linguistics, pages 836–841, Copenhagen, Denmark, August.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>