<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006390">
<title confidence="0.98564">
Importance of linguistic constraints in statistical dependency parsing
</title>
<author confidence="0.95524">
Bharat Ram Ambati
</author>
<affiliation confidence="0.8209095">
Language Technologies Research Centre, IIIT-Hyderabad,
Gachibowli, Hyderabad, India – 500032.
</affiliation>
<email confidence="0.989662">
ambati@research.iiit.ac.in
</email>
<sectionHeader confidence="0.993703" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998656210526316">
Statistical systems with high accuracy are very
useful in real-world applications. If these sys-
tems can capture basic linguistic information,
then the usefulness of these statistical systems
improve a lot. This paper is an attempt at in-
corporating linguistic constraints in statistical
dependency parsing. We consider a simple
linguistic constraint that a verb should not
have multiple subjects/objects as its children
in the dependency tree. We first describe the
importance of this constraint considering Ma-
chine Translation systems which use depen-
dency parser output, as an example applica-
tion. We then show how the current state-of-
the-art dependency parsers violate this con-
straint. We present two new methods to handle
this constraint. We evaluate our methods on
the state-of-the-art dependency parsers for
Hindi and Czech.
</bodyText>
<sectionHeader confidence="0.998988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999977424242425">
Parsing is one of the major tasks which helps in
understanding the natural language. It is useful in
several natural language applications. Machine
translation, anaphora resolution, word sense dis-
ambiguation, question answering, summarization
are few of them. This led to the development of
grammar-driven, data-driven and hybrid parsers.
Due to the availability of annotated corpora in
recent years, data driven parsing has achieved
considerable success. The availability of phrase
structure treebank for English (Marcus et al.,
1993) has seen the development of many effi-
cient parsers. Using the dependency analysis, a
similar large scale annotation effort for Czech,
has been the Prague Dependency Treebank (Ha-
jicova, 1998). Unlike English, Czech is a free-
word-order language and is also morphologically
very rich. It has been suggested that free-word-
order languages can be handled better using the
dependency based framework than the constitu-
ency based one (Hudson, 1984; Shieber, 1985;
Mel‟čuk, 1988, Bharati et al., 1995). The basic
difference between a constituent based represen-
tation and a dependency representation is the
lack of nonterminal nodes in the latter. It has also
been noted that use of appropriate edge labels
gives a level of semantics. It is perhaps due to
these reasons that the recent past has seen a surge
in the development of dependency based tree-
banks.
Due to the availability of dependency tree-
banks, there are several recent attempts at build-
ing dependency parsers. Two CoNLL shared
tasks (Buchholz and Marsi, 2006; Nivre et al.,
2007a) were held aiming at building state-of-the-
art dependency parsers for different languages.
Recently in NLP Tools Contest in ICON-2009
(Husain, 2009 and references therein), rule-
based, constraint based, statistical and hybrid
approaches were explored towards building de-
pendency parsers for three Indian languages
namely, Telugu, Hindi and Bangla. In all these
efforts, state-of-the-art accuracies are obtained
by two data-driven parsers, namely, Malt (Nivre
et al., 2007b) and MST (McDonald et al., 2006).
The major limitation of both these parsers is that
they won&apos;t take linguistic constraints into account
explicitly. But, in real-world applications of the
parsers, some basic linguistic constraints are very
useful. If we can make these parsers handle lin-
guistic constraints also, then they become very
useful in real-world applications.
This paper is an effort towards incorporating
linguistic constraints in statistical dependency
parser. We consider a simple constraint that a
verb should not have multiple subjects/objects as
its children. In section 2, we take machine trans-
lation using dependency parser as an example
and explain the need of this linguistic constraint.
In section 3, we propose two approaches to han-
dle this case. We evaluate our approaches on the
state-of-the-art dependency parsers for Hindi and
Czech and analyze the results in section 4. Gen-
eral discussion and future directions of the work
are presented in section 5. We conclude our pa-
per in section 6.
</bodyText>
<page confidence="0.991351">
103
</page>
<note confidence="0.804152">
Proceedings of the ACL 2010 Student Research Workshop, pages 103–108,
Uppsala, Sweden, 13 July 2010. c�2010 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.977322" genericHeader="introduction">
2 Motivation
</sectionHeader>
<bodyText confidence="0.999730857142857">
In this section we take Machine Translation
(MT) systems that use dependency parser output
as an example and explain the need of linguistic
constraints. We take a simple constraint that a
verb should not have multiple subjects/objects as
its children in the dependency tree. Indian Lan-
guage to Indian Language Machine Transtion
System1 is one such MT system which uses de-
pendency parser output. In this system the gener-
al framework has three major components. a)
dependency analysis of the source sentence. b)
transfer from source dependency tree to target
dependency tree, and c) sentence generation
from the target dependency tree. In the transfer
part several rules are framed based on the source
language dependency tree. For instance, for Te-
lugu to Hindi MT system, based on the depen-
dency labels of the Telugu sentence post-
positions markers that need to be added to the
words are decided. Consider the following ex-
ample,
</bodyText>
<equation confidence="0.89807">
(1)
Telugu: raamu oka pamdu tinnaadu
‘Ramu’ ‘one’ ‘fruit’ ‘ate’
Hindi: raamu ne eka phala khaayaa
‘Ramu’ ‘ERG’ ‘one’ ‘fruit’ ‘ate’
English: “Ramu ate a fruit”.
</equation>
<bodyText confidence="0.985953">
In the above Telugu sentence, `raamu&apos; is the
subject of the verb `tinnaadu&apos;. While translating
this sentence to Hindi, the post-position marker
`ne&apos; is added to the subject. If the dependency
parser marks two subjects, both the words will
have `ne&apos; marker. This affects the comprehensi-
bility. If we can avoid such instances, then the
output of the MT system will be improved.
This problem is not due to morphological
richness or free-word-order nature of the target
language. Consider an example of free-word-
order language to fixed-word-order language MT
system like Hindi to English MT system. The
dependency labels help in identifying the posi-
tion of the word in the target sentence. Consider
the example sentences given below.
(2a) raama seba khaatha hai
`Ram&apos; `apple&apos; `eats&apos; `is&apos;
`Ram eats an apple&apos;
</bodyText>
<figure confidence="0.50188275">
1 http://sampark.iiit.ac.in/
(2b) seba raama khaatha hai
`apple&apos; `Ram&apos; `eats&apos; `is&apos;
‘Ram eats an apple’
</figure>
<bodyText confidence="0.99983256">
Though the source sentence is different, the
target sentence is same. Even though the source
sentences are different, the dependency tree is
same for both the sentences. In both the cases,
`raama’ is the subject and `seba&apos; is the object of
the verb `khaatha&apos;. This information helps in
getting the correct translation. If the parser for
the source sentence assigns the label `subject&apos; to
both `raama’ and `seba&apos;, the MT system can not
give the correct output.
There were some attempts at handling these
kind of linguistic constraints using integer pro-
gramming approaches (Riedel et al., 2006; Bha-
rati et al., 2008). In these approaches dependency
parsing is formulated as solving an integer pro-
gram as McDonald et al. (2006) has formulated
dependency parsing as MST problem. All the
linguistic constraints are encoded as constraints
while solving the integer program. In other
words, all the parses that violate these constraints
are removed from the solution list. The parse
with satisfies all the constraints is considered as
the dependency tree for the sentence. In the fol-
lowing section, we describe two new approaches
to avoid multiple subjects/objects for a verb.
</bodyText>
<sectionHeader confidence="0.99732" genericHeader="method">
3 Approaches
</sectionHeader>
<bodyText confidence="0.99988275">
In this section, we describe the two different ap-
proaches for avoiding the cases of a verb having
multiple subjects/objects as its children in the
dependency tree.
</bodyText>
<subsectionHeader confidence="0.998666">
3.1 Naive Approach (NA)
</subsectionHeader>
<bodyText confidence="0.999900428571428">
In this approach we first run a parser on the input
sentence. Instead of first best dependency label,
we extract the k-best labels for each token in the
sentence. For each verb in the sentence, we
check if there are multiple children with the de-
pendency label `subject&apos;. If there are any such
cases, we extract the list of all the children with
label `subject&apos;. we find the node in this list which
appears left most in the sentence with respect to
other nodes. We assign `subject&apos; to this node. For
the rest of the nodes in this list we assign the
second best label and remove the first best label
from their respective k-best list of labels. We
check recursively, till all such instances are
</bodyText>
<page confidence="0.99382">
104
</page>
<bodyText confidence="0.99412805">
avoided. We repeat the same procedure for `ob-
ject&apos;.
Main criterion to avoid multiple sub-
jects/objects in this approach is position of the
node in the sentence. Consider the following ex-
ample,
Eg. 3: raama seba khaatha hai
`Ram&apos; `apple&apos; `eats&apos; `is&apos;
`Ram eats an apple&apos;
Suppose the parser assigns the label `subject&apos;
to both the nouns, `raama&apos; and `seba&apos;. Then
naive approach assigns the label subject to `raa-
ma&apos; and second best label to `seba&apos; as `raama&apos;
precedes `seba&apos;.
In this manner we can avoid a verb having
multiple children with dependency labels sub-
ject/object.
Limitation to this approach is word-order. The
algorithm described here works well for fixed
word order languages. For example, consider a
language with fixed word order like English.
English is a SVO (Subject, Verb, Object) lan-
guage. Subject always occurs before the object.
So, if a verb has multiple subjects, based on posi-
tion we can say that the node that occurs first
will be the subject. But if we consider a free-
word order language like Hindi, this approach
wouldn&apos;t work always.
Consider (2a) and (2b). In both these exam-
ples, `raama&apos; is the subject of the verb `khaatha&apos;
and `seba&apos; is the object of the verb `khaatha&apos;.
The only difference in these two sentences is the
order of the word. In (2a), subject precedes ob-
ject. Whereas in (2b), object precedes subject.
Suppose the parser identifies both `raama&apos; and
`seba&apos; as subjects. NA can correctly identify
`raama&apos; as the subject in case of (2a). But in case
of (2b), `seba&apos; is identified as the subject. To
handle these kind of instances, we use a proba-
bilistic approach.
</bodyText>
<subsectionHeader confidence="0.99925">
3.2 Probabilistic Approach (PA)
</subsectionHeader>
<bodyText confidence="0.998881592592593">
The probabilistic approach is similar to naive
approach except that the main criterion to avoid
multiple subjects/objects in this approach is
probability of the node having a particular label.
Whereas in naive approach, position of the node
is the main criterion to avoid multiple sub-
jects/objects. In this approach, for each node in
the sentence, we extract the k-best labels along
with their probabilities. Similar to NA, we first
check for each verb if there are multiple children
with the dependency label `subject&apos;. If there are
any such cases, we extract the list of all the
children with label `subject&apos;. We find the node in
this list which has the highest probability value.
We assign `subject&apos; to this node. For the rest of
the nodes in this list we assign the second best
label and remove the first best label from their
respective k-best list of labels. We check recur-
sively, till all such instances are avoided. We
repeat the same procedure for `object&apos;.
Consider (2a) and (2b). Suppose the parser
identifies both `raama&apos; and `seba&apos; as subjects.
Probability of `raama&apos; being a subject will be
more than `seba&apos; being a subject. So, the proba-
bilistic approach correctly marks `raama&apos; as sub-
ject in both (2a) and (2b). But, NA couldn&apos;t iden-
tify `raama&apos; as subject in (2b).
</bodyText>
<sectionHeader confidence="0.999774" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9999755">
We evaluate our approaches on the state-of-the-
art parsers for two languages namely, Hindi and
Czech. First we calculate the instances of mul-
tiple subjects/objects in the output of the state-of-
the-art parsers for these two languages. Then we
apply our approaches and analyze the results.
</bodyText>
<subsectionHeader confidence="0.962481">
4.1 Hindi
</subsectionHeader>
<bodyText confidence="0.99937352">
Recently in NLP Tools Contest in ICON-2009
(Husain, 2009 and references herein), rule-based,
constraint based, statistical and hybrid approach-
es were explored for parsing Hindi. All these
attempts were at finding the inter-chunk depen-
dency relations, given gold-standard POS and
chunk tags. The state-of-the-art accuracy of
74.48% LAS (Labeled Attachment Score) is
achieved by Ambati et al. (2009) for Hindi.
They used two well-known data-driven parsers,
Malt2 (Nivre et al., 2007b), and MST3 (McDo-
nald et al., 2006) for their experiments. As the
accuracy of the labeler of MST parser is very
low, they used maximum entropy classification
algorithm, MAXENT4 for labeling.
For Hindi, dependency annotation is done us-
ing paninian framework (Begum et al., 2008;
Bharati et al., 1995). So, in Hindi, the equivalent
labels for subject and object are `karta (k1)&apos; and
`karma (k2)&apos;. `karta&apos; and `karma&apos; are syntactico-
semantic labels which have some properties of
both grammatical roles and thematic roles. k1
behaves similar to subject and agent. k2 behaves
similar to object and patient (Bharati et al., 1995;
Bharati et al., 2009). Here, by object we mean
</bodyText>
<footnote confidence="0.934314">
2 Malt Version 1.3.1
3 MST Version 0.4b
4http://homepages.inf.ed.ac.uk/lzhang10/maxent_toolkit.htm
l
</footnote>
<page confidence="0.99909">
105
</page>
<bodyText confidence="0.999178923076923">
only direct object. Thus we consider only k1 and
k2 labels which are equivalent of subject and di-
rect object. Annotation scheme is such that there
wouldn&apos;t be multiple subjects/objects for a verb
in any case (Bharati et al., 2009). For example,
even in case of coordination, coordinating con-
junction is the head and conjuncts are children of
the coordinating conjunction. The coordinating
conjunction is attached to the verb with k1/k2
label and the conjuncts get attached to the coor-
dinating conjunction with a dependency label
`ccof&apos;.
We replicated the experiments of Ambati et al.
(2009) on test set (150 sentences) of Hindi and
analyzed the outputs of Malt and MST+MaxEnt.
We consider this as the baseline. In the output of
Malt, there are 39 instances of multiple sub-
jects/objects. There are 51 such instances in the
output of MST+MAXENT.
Malt is good at short distance labeling and
MST is good at long distance labeling (McDo-
nald and Nivre, 2007). As `k1&apos; and `k2&apos; are short
distance labels, Malt could able predict these la-
bels more accurately than MST. Because of this
output of MST has higher number of instances of
multiple subjects/objects than Malt.
</bodyText>
<table confidence="0.957632">
Total Instances
Malt 39
MST + MAXENT 51
</table>
<tableCaption confidence="0.987108">
Table 1: Number of instances of multiple subjects or
objects in the output of the state-of-the-art parsers for
</tableCaption>
<bodyText confidence="0.962851529411765">
Hindi
Both the parsers output first best label for each
node in the sentence. In case of Malt, we mod-
ified the implementation to extract all the possi-
ble dependency labels with their scores. As Malt
uses libsvm for learning, we couldn&apos;t able to get
the probabilities. Though interpreting the scores
provided by libsvm as probabilities is not the
correct way, that is the only option currently
available with Malt. In case of MST+MAXENT,
labeling is performed by MAXENT. We used a
java version of MAXENT5 to extract all possible
tags with their scores. We applied both the naive
and probabilistic approaches to avoid multiple
subjects/objects. We evaluated our experiments
based on unlabeled attachment score (UAS), la-
beled attachment score (LAS) and labeled score
</bodyText>
<footnote confidence="0.643938">
5 http://maxent.sourceforge.net/
</footnote>
<bodyText confidence="0.998408181818182">
(LS) (Nivre et al., 2007a). Results are presented
in Table 2.
As expected, PA performs better than NA.
With PA we got an improvement of 0.26% in
LAS over the previous best results for Malt. In
case of MST+MAXENT we got an improvement
of 0.61% in LAS over the previous best results.
Note that in case of MST+MAXENT, the slight
difference between state-of-the-art results of
Ambati et al. (2009) and our baseline accuracy is
due different MAXENT package used.
</bodyText>
<table confidence="0.994661">
Malt MST+MAXENT
UAS LAS LS UAS LAS LS
Baseline 90.14 74.48 76.38 91.26 72.75 75.26
NA 90.14 74.57 76.38 91.26 72.84 75.26
PA 90.14 74.74 76.56 91.26 73.36 75.87
</table>
<tableCaption confidence="0.994045">
Table 2: Comparison of NA and PA with previous
best results for Hindi
</tableCaption>
<bodyText confidence="0.999656777777778">
Improvement in case of MST+MAXENT is
greater than that of Malt. One reason is because
of more number of instances of multiple sub-
jects/objects in case of MST+MAXENT. Other
reason is use of probabilities in case
MST+MAXENT. Whereas in case of Malt, we
interpreted the scores as probabilities which is
not a good way to do. But, in case of Malt, that is
the only option available.
</bodyText>
<subsectionHeader confidence="0.992513">
4.2 Czech
</subsectionHeader>
<bodyText confidence="0.997021954545454">
In case of Czech, we replicated the experiments
of Hall et al. (2007) using latest version of Malt
(version 1.3.1) and analyzed the output. We con-
sider this as the baseline. The minor variation of
the baseline results from the results of CoNLL-
2007 shared task is due to different version Malt
parser being used. Due to practical reasons we
couldn&apos;t use the older version. In the output of
Malt, there are 39 instances of multiple sub-
jects/objects out of 286 sentences in the testing
data. In case of Czech, the equivalent labels for
subject and object are `agent&apos; and `theme&apos;.
Czech is a free-word-order language similar to
Hindi. So as expected, PA performed better than
NA. Interestingly, accuracy of PA is lower than
the baseline. Main reason for this is scores of
libsvm of Malt. We explain the reason for this
using the following example, consider a verb `V&apos;
has two children `C1&apos; and `C2&apos; with dependency
label subject. Assume that the label for `C1&apos; is
subject and the label of `C2&apos; is object in the gold-
data. As the parser marked `C1&apos; with subject, this
</bodyText>
<page confidence="0.996454">
106
</page>
<bodyText confidence="0.998686923076923">
adds to the accuracy of the parser. While avoid-
ing multiple subjects, if „C1&apos; is marked as sub-
ject, then the accuracy doesn&apos;t drop. If „C2&apos; is
marked as object then the accuracy increases.
But, if „C2&apos; is marked as subject and „C1&apos; is
marked as object then the accuracy drops. This
could happen if probability of „C1&apos; having sub-
ject as label is lower than „C1&apos; having subject as
the label. This is because of two reasons, (a)
parser itself wrongly predicted the probabilities,
and (b) parser predicted correctly, but due to the
limitation of libsvm, we couldn&apos;t get the scores
correctly.
</bodyText>
<table confidence="0.99410625">
UAS LAS LS
Baseline 82.92 76.32 83.69
NA 82.92 75.92 83.35
PA 82.92 75.97 83.40
</table>
<tableCaption confidence="0.9906055">
Table 3: Comparison of NA and PA with previous
best results for Czech
</tableCaption>
<sectionHeader confidence="0.995088" genericHeader="evaluation">
5 Discussion and Future Work
</sectionHeader>
<bodyText confidence="0.99990224137931">
Results show that the probabilistic approach per-
forms consistently better than the naive ap-
proach. For Hindi, we could able to achieve an
improvement 0.26% and 0.61% in LAS over the
previous best results using Malt and MST re-
spectively. We couldn&apos;t able to achieve any im-
provement in case of Czech due to the limitation
of libsvm learner used in Malt.
We plan to evaluate our approaches on all the
data-sets of CoNLL-X and CoNLL-2007 shared
tasks using Malt. Settings of MST parser are
available only for CoNLL-X shared task data
sets. So, we plan to evaluate our approaches on
CoNLL-X shared task data using MST also. Malt
has the limitation for extracting probabilities due
to libsvm learner. Latest version of Malt (version
1.3.1) provides option for liblinear learner also.
Liblinear provides option for extracting probabil-
ities. So we can also use liblinear learning algo-
rithm for Malt and explore the usefulness of our
approaches. Currently, we are handling only two
labels, subject and object. Apart from subject and
object there can be other labels for which mul-
tiple instances for a single verb is not valid. We
can extend our approaches to handle such labels
also. We tried to incorporate one simple linguis-
tic constraint in the statistical dependency pars-
ers. We can also explore the ways of incorporat-
ing other useful linguistic constraints.
</bodyText>
<sectionHeader confidence="0.998618" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999964769230769">
Statistical systems with high accuracy are very
useful in practical applications. If these systems
can capture basic linguistic information, then the
usefulness of the statistical system improves a
lot. In this paper, we presented a new method of
incorporating linguistic constraints into the sta-
tistical dependency parsers. We took a simple
constraint that a verb should not have multiple
subjects/objects as its children. We proposed two
approaches, one based on position and the other
based on probabilities to handle this. We eva-
luated our approaches on state-of-the-art depen-
dency parsers for Hindi and Czech.
</bodyText>
<sectionHeader confidence="0.998379" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.988658">
I would like to express my gratitude to Prof. Joa-
kim Nivre and Prof. Rajeev Sangal for their
guidance and support. I would also like to thank
Mr. Samar Husain for his valuable suggestions.
</bodyText>
<sectionHeader confidence="0.998741" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998054677419355">
B. R. Ambati, P. Gadde and K. Jindal. 2009. Experi-
ments in Indian Language Dependency Parsing. In
Proceedings of the ICON09 NLP Tools Contest:
Indian Language Dependency Parsing, pp 32-37.
R. Begum, S. Husain, A. Dhwaj, D. Sharma, L. Bai,
and R. Sangal. 2008. Dependency annotation
scheme for Indian languages. In Proceedings of
IJCNLP-2008.
A. Bharati, V. Chaitanya and R. Sangal. 1995. Natu-
ral Language Processing: A Paninian Perspective,
Prentice-Hall of India, New Delhi, pp. 65-106.
A. Bharati, S. Husain, D. M. Sharma, and R. Sangal.
2008. A Two-Stage Constraint Based Dependency
Parser for Free Word Order Languages. In Pro-
ceedings of the COLIPS International Conference
on Asian Language Processing 2008 (IALP).
Chiang Mai, Thailand.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proc.
of the Tenth Conf. on Computational Natural Lan-
guage Learning (CoNLL).
E. Hajicova. 1998. Prague Dependency Treebank:
From Analytic to Tectogrammatical Annotation. In
Proc. TSD’98.
J. Hall, J. Nilsson, J. Nivre, G. Eryigit, B. Megyesi,
M. Nilsson and M. Saers. 2007. Single Malt or
Blended? A Study in Multilingual Parser Optimiza-
tion. In Proceedings of the CoNLL Shared Task
Session of EMNLP-CoNLL.
R. Hudson. 1984. Word Grammar, Basil Blackwell,
108 Cowley Rd, Oxford, OX4 1JF, England.
</reference>
<page confidence="0.983801">
107
</page>
<reference confidence="0.999476314285714">
S. Husain. 2009. Dependency Parsers for Indian Lan-
guages. In Proceedings of ICON09 NLP Tools
Contest: Indian Language Dependency Parsing.
Hyderabad, India.
M. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1993. Building a large annotated corpus of English:
The Penn Treebank, Computational Linguistics
1993.
I. A. Mel&apos;čuk. 1988. Dependency Syntax: Theory and
Practice, State University, Press of New York.
R. McDonald, K. Lerman, and F. Pereira. 2006. Mul-
tilingual dependency analysis with a two-stage dis-
criminative parser. In Proceedings of the Tenth
Conference on Computational Natural Language
Learning (CoNLL-X), pp. 216–220.
R. McDonald and J. Nivre. 2007. Characterizing the
errors of data-driven dependency parsing models.
In Proc. of EMNLP-CoNLL.
J. Nivre, J. Hall, S. Kubler, R. McDonald, J. Nilsson,
S. Riedel and D. Yuret. 2007a. The CoNLL 2007
Shared Task on Dependency Parsing. In Proceed-
ings of EMNLP/CoNLL-2007.
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit, S.
Kübler, S. Marinov and E Marsi. 2007b. MaltPars-
er: A language-independent system for data-driven
dependency parsing. Natural Language Engineer-
ing, 13(2), 95-135.
S. Riedel, Ruket Çakıcı and Ivan Meza-Ruiz. 2006.
Multi-lingual Dependency Parsing with Incremen-
tal Integer Linear Programming. In Proceedings of
the Tenth Conference on Computational Natural
Language Learning (CoNLL-X).
S. M. Shieber. 1985. Evidence against the context-
freeness of natural language. In Linguistics and
Philosophy, p. 8, 334–343.
</reference>
<page confidence="0.998364">
108
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.637382">
<title confidence="0.999332">Importance of linguistic constraints in statistical dependency parsing</title>
<author confidence="0.999537">Bharat Ram Ambati</author>
<affiliation confidence="0.987504">Language Technologies Research Centre, IIIT-Hyderabad,</affiliation>
<address confidence="0.966316">Hyderabad, India</address>
<email confidence="0.982658">ambati@research.iiit.ac.in</email>
<abstract confidence="0.98151955">Statistical systems with high accuracy are very useful in real-world applications. If these systems can capture basic linguistic information, then the usefulness of these statistical systems improve a lot. This paper is an attempt at incorporating linguistic constraints in statistical dependency parsing. We consider a simple linguistic constraint that a verb should not have multiple subjects/objects as its children in the dependency tree. We first describe the importance of this constraint considering Machine Translation systems which use dependency parser output, as an example application. We then show how the current state-ofthe-art dependency parsers violate this constraint. We present two new methods to handle this constraint. We evaluate our methods on the state-of-the-art dependency parsers for Hindi and Czech.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>B R Ambati</author>
<author>P Gadde</author>
<author>K Jindal</author>
</authors>
<title>Experiments in Indian Language Dependency Parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of the ICON09 NLP Tools Contest: Indian Language Dependency Parsing,</booktitle>
<pages>32--37</pages>
<contexts>
<context position="11970" citStr="Ambati et al. (2009)" startWordPosition="1920" endWordPosition="1923">s namely, Hindi and Czech. First we calculate the instances of multiple subjects/objects in the output of the state-ofthe-art parsers for these two languages. Then we apply our approaches and analyze the results. 4.1 Hindi Recently in NLP Tools Contest in ICON-2009 (Husain, 2009 and references herein), rule-based, constraint based, statistical and hybrid approaches were explored for parsing Hindi. All these attempts were at finding the inter-chunk dependency relations, given gold-standard POS and chunk tags. The state-of-the-art accuracy of 74.48% LAS (Labeled Attachment Score) is achieved by Ambati et al. (2009) for Hindi. They used two well-known data-driven parsers, Malt2 (Nivre et al., 2007b), and MST3 (McDonald et al., 2006) for their experiments. As the accuracy of the labeler of MST parser is very low, they used maximum entropy classification algorithm, MAXENT4 for labeling. For Hindi, dependency annotation is done using paninian framework (Begum et al., 2008; Bharati et al., 1995). So, in Hindi, the equivalent labels for subject and object are `karta (k1)&apos; and `karma (k2)&apos;. `karta&apos; and `karma&apos; are syntacticosemantic labels which have some properties of both grammatical roles and thematic roles</context>
<context position="13419" citStr="Ambati et al. (2009)" startWordPosition="2150" endWordPosition="2153">ng10/maxent_toolkit.htm l 105 only direct object. Thus we consider only k1 and k2 labels which are equivalent of subject and direct object. Annotation scheme is such that there wouldn&apos;t be multiple subjects/objects for a verb in any case (Bharati et al., 2009). For example, even in case of coordination, coordinating conjunction is the head and conjuncts are children of the coordinating conjunction. The coordinating conjunction is attached to the verb with k1/k2 label and the conjuncts get attached to the coordinating conjunction with a dependency label `ccof&apos;. We replicated the experiments of Ambati et al. (2009) on test set (150 sentences) of Hindi and analyzed the outputs of Malt and MST+MaxEnt. We consider this as the baseline. In the output of Malt, there are 39 instances of multiple subjects/objects. There are 51 such instances in the output of MST+MAXENT. Malt is good at short distance labeling and MST is good at long distance labeling (McDonald and Nivre, 2007). As `k1&apos; and `k2&apos; are short distance labels, Malt could able predict these labels more accurately than MST. Because of this output of MST has higher number of instances of multiple subjects/objects than Malt. Total Instances Malt 39 MST </context>
<context position="15332" citStr="Ambati et al. (2009)" startWordPosition="2470" endWordPosition="2473">e naive and probabilistic approaches to avoid multiple subjects/objects. We evaluated our experiments based on unlabeled attachment score (UAS), labeled attachment score (LAS) and labeled score 5 http://maxent.sourceforge.net/ (LS) (Nivre et al., 2007a). Results are presented in Table 2. As expected, PA performs better than NA. With PA we got an improvement of 0.26% in LAS over the previous best results for Malt. In case of MST+MAXENT we got an improvement of 0.61% in LAS over the previous best results. Note that in case of MST+MAXENT, the slight difference between state-of-the-art results of Ambati et al. (2009) and our baseline accuracy is due different MAXENT package used. Malt MST+MAXENT UAS LAS LS UAS LAS LS Baseline 90.14 74.48 76.38 91.26 72.75 75.26 NA 90.14 74.57 76.38 91.26 72.84 75.26 PA 90.14 74.74 76.56 91.26 73.36 75.87 Table 2: Comparison of NA and PA with previous best results for Hindi Improvement in case of MST+MAXENT is greater than that of Malt. One reason is because of more number of instances of multiple subjects/objects in case of MST+MAXENT. Other reason is use of probabilities in case MST+MAXENT. Whereas in case of Malt, we interpreted the scores as probabilities which is not </context>
</contexts>
<marker>Ambati, Gadde, Jindal, 2009</marker>
<rawString>B. R. Ambati, P. Gadde and K. Jindal. 2009. Experiments in Indian Language Dependency Parsing. In Proceedings of the ICON09 NLP Tools Contest: Indian Language Dependency Parsing, pp 32-37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Begum</author>
<author>S Husain</author>
<author>A Dhwaj</author>
<author>D Sharma</author>
<author>L Bai</author>
<author>R Sangal</author>
</authors>
<title>Dependency annotation scheme for Indian languages.</title>
<date>2008</date>
<booktitle>In Proceedings of IJCNLP-2008.</booktitle>
<contexts>
<context position="12330" citStr="Begum et al., 2008" startWordPosition="1978" endWordPosition="1981">aches were explored for parsing Hindi. All these attempts were at finding the inter-chunk dependency relations, given gold-standard POS and chunk tags. The state-of-the-art accuracy of 74.48% LAS (Labeled Attachment Score) is achieved by Ambati et al. (2009) for Hindi. They used two well-known data-driven parsers, Malt2 (Nivre et al., 2007b), and MST3 (McDonald et al., 2006) for their experiments. As the accuracy of the labeler of MST parser is very low, they used maximum entropy classification algorithm, MAXENT4 for labeling. For Hindi, dependency annotation is done using paninian framework (Begum et al., 2008; Bharati et al., 1995). So, in Hindi, the equivalent labels for subject and object are `karta (k1)&apos; and `karma (k2)&apos;. `karta&apos; and `karma&apos; are syntacticosemantic labels which have some properties of both grammatical roles and thematic roles. k1 behaves similar to subject and agent. k2 behaves similar to object and patient (Bharati et al., 1995; Bharati et al., 2009). Here, by object we mean 2 Malt Version 1.3.1 3 MST Version 0.4b 4http://homepages.inf.ed.ac.uk/lzhang10/maxent_toolkit.htm l 105 only direct object. Thus we consider only k1 and k2 labels which are equivalent of subject and direct</context>
</contexts>
<marker>Begum, Husain, Dhwaj, Sharma, Bai, Sangal, 2008</marker>
<rawString>R. Begum, S. Husain, A. Dhwaj, D. Sharma, L. Bai, and R. Sangal. 2008. Dependency annotation scheme for Indian languages. In Proceedings of IJCNLP-2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bharati</author>
<author>V Chaitanya</author>
<author>R Sangal</author>
</authors>
<title>Natural Language Processing: A Paninian Perspective,</title>
<date>1995</date>
<booktitle>Prentice-Hall of India,</booktitle>
<pages>65--106</pages>
<location>New Delhi,</location>
<contexts>
<context position="2090" citStr="Bharati et al., 1995" startWordPosition="300" endWordPosition="303">driven parsing has achieved considerable success. The availability of phrase structure treebank for English (Marcus et al., 1993) has seen the development of many efficient parsers. Using the dependency analysis, a similar large scale annotation effort for Czech, has been the Prague Dependency Treebank (Hajicova, 1998). Unlike English, Czech is a freeword-order language and is also morphologically very rich. It has been suggested that free-wordorder languages can be handled better using the dependency based framework than the constituency based one (Hudson, 1984; Shieber, 1985; Mel‟čuk, 1988, Bharati et al., 1995). The basic difference between a constituent based representation and a dependency representation is the lack of nonterminal nodes in the latter. It has also been noted that use of appropriate edge labels gives a level of semantics. It is perhaps due to these reasons that the recent past has seen a surge in the development of dependency based treebanks. Due to the availability of dependency treebanks, there are several recent attempts at building dependency parsers. Two CoNLL shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007a) were held aiming at building state-of-theart dependency pa</context>
<context position="12353" citStr="Bharati et al., 1995" startWordPosition="1982" endWordPosition="1985">for parsing Hindi. All these attempts were at finding the inter-chunk dependency relations, given gold-standard POS and chunk tags. The state-of-the-art accuracy of 74.48% LAS (Labeled Attachment Score) is achieved by Ambati et al. (2009) for Hindi. They used two well-known data-driven parsers, Malt2 (Nivre et al., 2007b), and MST3 (McDonald et al., 2006) for their experiments. As the accuracy of the labeler of MST parser is very low, they used maximum entropy classification algorithm, MAXENT4 for labeling. For Hindi, dependency annotation is done using paninian framework (Begum et al., 2008; Bharati et al., 1995). So, in Hindi, the equivalent labels for subject and object are `karta (k1)&apos; and `karma (k2)&apos;. `karta&apos; and `karma&apos; are syntacticosemantic labels which have some properties of both grammatical roles and thematic roles. k1 behaves similar to subject and agent. k2 behaves similar to object and patient (Bharati et al., 1995; Bharati et al., 2009). Here, by object we mean 2 Malt Version 1.3.1 3 MST Version 0.4b 4http://homepages.inf.ed.ac.uk/lzhang10/maxent_toolkit.htm l 105 only direct object. Thus we consider only k1 and k2 labels which are equivalent of subject and direct object. Annotation sch</context>
</contexts>
<marker>Bharati, Chaitanya, Sangal, 1995</marker>
<rawString>A. Bharati, V. Chaitanya and R. Sangal. 1995. Natural Language Processing: A Paninian Perspective, Prentice-Hall of India, New Delhi, pp. 65-106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bharati</author>
<author>S Husain</author>
<author>D M Sharma</author>
<author>R Sangal</author>
</authors>
<title>A Two-Stage Constraint Based Dependency Parser for Free Word Order Languages.</title>
<date>2008</date>
<booktitle>In Proceedings of the COLIPS International Conference on Asian Language Processing</booktitle>
<contexts>
<context position="6891" citStr="Bharati et al., 2008" startWordPosition="1067" endWordPosition="1071">apple’ Though the source sentence is different, the target sentence is same. Even though the source sentences are different, the dependency tree is same for both the sentences. In both the cases, `raama’ is the subject and `seba&apos; is the object of the verb `khaatha&apos;. This information helps in getting the correct translation. If the parser for the source sentence assigns the label `subject&apos; to both `raama’ and `seba&apos;, the MT system can not give the correct output. There were some attempts at handling these kind of linguistic constraints using integer programming approaches (Riedel et al., 2006; Bharati et al., 2008). In these approaches dependency parsing is formulated as solving an integer program as McDonald et al. (2006) has formulated dependency parsing as MST problem. All the linguistic constraints are encoded as constraints while solving the integer program. In other words, all the parses that violate these constraints are removed from the solution list. The parse with satisfies all the constraints is considered as the dependency tree for the sentence. In the following section, we describe two new approaches to avoid multiple subjects/objects for a verb. 3 Approaches In this section, we describe th</context>
</contexts>
<marker>Bharati, Husain, Sharma, Sangal, 2008</marker>
<rawString>A. Bharati, S. Husain, D. M. Sharma, and R. Sangal. 2008. A Two-Stage Constraint Based Dependency Parser for Free Word Order Languages. In Proceedings of the COLIPS International Conference on Asian Language Processing 2008 (IALP). Chiang Mai, Thailand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Buchholz</author>
<author>E Marsi</author>
</authors>
<title>CoNLL-X shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proc. of the Tenth Conf. on Computational Natural Language Learning (CoNLL).</booktitle>
<contexts>
<context position="2609" citStr="Buchholz and Marsi, 2006" startWordPosition="387" endWordPosition="390">mework than the constituency based one (Hudson, 1984; Shieber, 1985; Mel‟čuk, 1988, Bharati et al., 1995). The basic difference between a constituent based representation and a dependency representation is the lack of nonterminal nodes in the latter. It has also been noted that use of appropriate edge labels gives a level of semantics. It is perhaps due to these reasons that the recent past has seen a surge in the development of dependency based treebanks. Due to the availability of dependency treebanks, there are several recent attempts at building dependency parsers. Two CoNLL shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007a) were held aiming at building state-of-theart dependency parsers for different languages. Recently in NLP Tools Contest in ICON-2009 (Husain, 2009 and references therein), rulebased, constraint based, statistical and hybrid approaches were explored towards building dependency parsers for three Indian languages namely, Telugu, Hindi and Bangla. In all these efforts, state-of-the-art accuracies are obtained by two data-driven parsers, namely, Malt (Nivre et al., 2007b) and MST (McDonald et al., 2006). The major limitation of both these parsers is that they won&apos;t take lingui</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>S. Buchholz and E. Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In Proc. of the Tenth Conf. on Computational Natural Language Learning (CoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hajicova</author>
</authors>
<title>Prague Dependency Treebank: From Analytic to Tectogrammatical Annotation. In</title>
<date>1998</date>
<booktitle>Proc. TSD’98.</booktitle>
<contexts>
<context position="1789" citStr="Hajicova, 1998" startWordPosition="254" endWordPosition="256">tural language applications. Machine translation, anaphora resolution, word sense disambiguation, question answering, summarization are few of them. This led to the development of grammar-driven, data-driven and hybrid parsers. Due to the availability of annotated corpora in recent years, data driven parsing has achieved considerable success. The availability of phrase structure treebank for English (Marcus et al., 1993) has seen the development of many efficient parsers. Using the dependency analysis, a similar large scale annotation effort for Czech, has been the Prague Dependency Treebank (Hajicova, 1998). Unlike English, Czech is a freeword-order language and is also morphologically very rich. It has been suggested that free-wordorder languages can be handled better using the dependency based framework than the constituency based one (Hudson, 1984; Shieber, 1985; Mel‟čuk, 1988, Bharati et al., 1995). The basic difference between a constituent based representation and a dependency representation is the lack of nonterminal nodes in the latter. It has also been noted that use of appropriate edge labels gives a level of semantics. It is perhaps due to these reasons that the recent past has seen a</context>
</contexts>
<marker>Hajicova, 1998</marker>
<rawString>E. Hajicova. 1998. Prague Dependency Treebank: From Analytic to Tectogrammatical Annotation. In Proc. TSD’98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hall</author>
<author>J Nilsson</author>
<author>J Nivre</author>
<author>G Eryigit</author>
<author>B Megyesi</author>
<author>M Nilsson</author>
<author>M Saers</author>
</authors>
<title>Single Malt or Blended? A Study in Multilingual Parser Optimization.</title>
<date>2007</date>
<booktitle>In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="16086" citStr="Hall et al. (2007)" startWordPosition="2604" endWordPosition="2607">72.75 75.26 NA 90.14 74.57 76.38 91.26 72.84 75.26 PA 90.14 74.74 76.56 91.26 73.36 75.87 Table 2: Comparison of NA and PA with previous best results for Hindi Improvement in case of MST+MAXENT is greater than that of Malt. One reason is because of more number of instances of multiple subjects/objects in case of MST+MAXENT. Other reason is use of probabilities in case MST+MAXENT. Whereas in case of Malt, we interpreted the scores as probabilities which is not a good way to do. But, in case of Malt, that is the only option available. 4.2 Czech In case of Czech, we replicated the experiments of Hall et al. (2007) using latest version of Malt (version 1.3.1) and analyzed the output. We consider this as the baseline. The minor variation of the baseline results from the results of CoNLL2007 shared task is due to different version Malt parser being used. Due to practical reasons we couldn&apos;t use the older version. In the output of Malt, there are 39 instances of multiple subjects/objects out of 286 sentences in the testing data. In case of Czech, the equivalent labels for subject and object are `agent&apos; and `theme&apos;. Czech is a free-word-order language similar to Hindi. So as expected, PA performed better th</context>
</contexts>
<marker>Hall, Nilsson, Nivre, Eryigit, Megyesi, Nilsson, Saers, 2007</marker>
<rawString>J. Hall, J. Nilsson, J. Nivre, G. Eryigit, B. Megyesi, M. Nilsson and M. Saers. 2007. Single Malt or Blended? A Study in Multilingual Parser Optimization. In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Hudson</author>
</authors>
<title>Word Grammar, Basil Blackwell,</title>
<date>1984</date>
<booktitle>108 Cowley Rd, Oxford, OX4 1JF,</booktitle>
<location>England.</location>
<contexts>
<context position="2037" citStr="Hudson, 1984" startWordPosition="294" endWordPosition="295"> of annotated corpora in recent years, data driven parsing has achieved considerable success. The availability of phrase structure treebank for English (Marcus et al., 1993) has seen the development of many efficient parsers. Using the dependency analysis, a similar large scale annotation effort for Czech, has been the Prague Dependency Treebank (Hajicova, 1998). Unlike English, Czech is a freeword-order language and is also morphologically very rich. It has been suggested that free-wordorder languages can be handled better using the dependency based framework than the constituency based one (Hudson, 1984; Shieber, 1985; Mel‟čuk, 1988, Bharati et al., 1995). The basic difference between a constituent based representation and a dependency representation is the lack of nonterminal nodes in the latter. It has also been noted that use of appropriate edge labels gives a level of semantics. It is perhaps due to these reasons that the recent past has seen a surge in the development of dependency based treebanks. Due to the availability of dependency treebanks, there are several recent attempts at building dependency parsers. Two CoNLL shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007a) were </context>
</contexts>
<marker>Hudson, 1984</marker>
<rawString>R. Hudson. 1984. Word Grammar, Basil Blackwell, 108 Cowley Rd, Oxford, OX4 1JF, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Husain</author>
</authors>
<title>Dependency Parsers for Indian Languages.</title>
<date>2009</date>
<booktitle>In Proceedings of ICON09 NLP Tools Contest: Indian Language Dependency Parsing.</booktitle>
<location>Hyderabad, India.</location>
<contexts>
<context position="2777" citStr="Husain, 2009" startWordPosition="414" endWordPosition="415">endency representation is the lack of nonterminal nodes in the latter. It has also been noted that use of appropriate edge labels gives a level of semantics. It is perhaps due to these reasons that the recent past has seen a surge in the development of dependency based treebanks. Due to the availability of dependency treebanks, there are several recent attempts at building dependency parsers. Two CoNLL shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007a) were held aiming at building state-of-theart dependency parsers for different languages. Recently in NLP Tools Contest in ICON-2009 (Husain, 2009 and references therein), rulebased, constraint based, statistical and hybrid approaches were explored towards building dependency parsers for three Indian languages namely, Telugu, Hindi and Bangla. In all these efforts, state-of-the-art accuracies are obtained by two data-driven parsers, namely, Malt (Nivre et al., 2007b) and MST (McDonald et al., 2006). The major limitation of both these parsers is that they won&apos;t take linguistic constraints into account explicitly. But, in real-world applications of the parsers, some basic linguistic constraints are very useful. If we can make these parser</context>
<context position="11629" citStr="Husain, 2009" startWordPosition="1873" endWordPosition="1874"> `seba&apos; as subjects. Probability of `raama&apos; being a subject will be more than `seba&apos; being a subject. So, the probabilistic approach correctly marks `raama&apos; as subject in both (2a) and (2b). But, NA couldn&apos;t identify `raama&apos; as subject in (2b). 4 Experiments We evaluate our approaches on the state-of-theart parsers for two languages namely, Hindi and Czech. First we calculate the instances of multiple subjects/objects in the output of the state-ofthe-art parsers for these two languages. Then we apply our approaches and analyze the results. 4.1 Hindi Recently in NLP Tools Contest in ICON-2009 (Husain, 2009 and references herein), rule-based, constraint based, statistical and hybrid approaches were explored for parsing Hindi. All these attempts were at finding the inter-chunk dependency relations, given gold-standard POS and chunk tags. The state-of-the-art accuracy of 74.48% LAS (Labeled Attachment Score) is achieved by Ambati et al. (2009) for Hindi. They used two well-known data-driven parsers, Malt2 (Nivre et al., 2007b), and MST3 (McDonald et al., 2006) for their experiments. As the accuracy of the labeler of MST parser is very low, they used maximum entropy classification algorithm, MAXENT</context>
</contexts>
<marker>Husain, 2009</marker>
<rawString>S. Husain. 2009. Dependency Parsers for Indian Languages. In Proceedings of ICON09 NLP Tools Contest: Indian Language Dependency Parsing. Hyderabad, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank, Computational Linguistics</title>
<date>1993</date>
<contexts>
<context position="1598" citStr="Marcus et al., 1993" startWordPosition="223" endWordPosition="226">thods on the state-of-the-art dependency parsers for Hindi and Czech. 1 Introduction Parsing is one of the major tasks which helps in understanding the natural language. It is useful in several natural language applications. Machine translation, anaphora resolution, word sense disambiguation, question answering, summarization are few of them. This led to the development of grammar-driven, data-driven and hybrid parsers. Due to the availability of annotated corpora in recent years, data driven parsing has achieved considerable success. The availability of phrase structure treebank for English (Marcus et al., 1993) has seen the development of many efficient parsers. Using the dependency analysis, a similar large scale annotation effort for Czech, has been the Prague Dependency Treebank (Hajicova, 1998). Unlike English, Czech is a freeword-order language and is also morphologically very rich. It has been suggested that free-wordorder languages can be handled better using the dependency based framework than the constituency based one (Hudson, 1984; Shieber, 1985; Mel‟čuk, 1988, Bharati et al., 1995). The basic difference between a constituent based representation and a dependency representation is the lac</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. Marcus, B. Santorini, and M.A. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank, Computational Linguistics 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I A Mel&apos;čuk</author>
</authors>
<title>Dependency Syntax: Theory and Practice,</title>
<date>1988</date>
<publisher>State University, Press of</publisher>
<location>New York.</location>
<marker>Mel&apos;čuk, 1988</marker>
<rawString>I. A. Mel&apos;čuk. 1988. Dependency Syntax: Theory and Practice, State University, Press of New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Lerman</author>
<author>F Pereira</author>
</authors>
<title>Multilingual dependency analysis with a two-stage discriminative parser.</title>
<date>2006</date>
<booktitle>In Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL-X),</booktitle>
<pages>216--220</pages>
<contexts>
<context position="3134" citStr="McDonald et al., 2006" startWordPosition="463" endWordPosition="466"> recent attempts at building dependency parsers. Two CoNLL shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007a) were held aiming at building state-of-theart dependency parsers for different languages. Recently in NLP Tools Contest in ICON-2009 (Husain, 2009 and references therein), rulebased, constraint based, statistical and hybrid approaches were explored towards building dependency parsers for three Indian languages namely, Telugu, Hindi and Bangla. In all these efforts, state-of-the-art accuracies are obtained by two data-driven parsers, namely, Malt (Nivre et al., 2007b) and MST (McDonald et al., 2006). The major limitation of both these parsers is that they won&apos;t take linguistic constraints into account explicitly. But, in real-world applications of the parsers, some basic linguistic constraints are very useful. If we can make these parsers handle linguistic constraints also, then they become very useful in real-world applications. This paper is an effort towards incorporating linguistic constraints in statistical dependency parser. We consider a simple constraint that a verb should not have multiple subjects/objects as its children. In section 2, we take machine translation using dependen</context>
<context position="7001" citStr="McDonald et al. (2006)" startWordPosition="1086" endWordPosition="1089"> are different, the dependency tree is same for both the sentences. In both the cases, `raama’ is the subject and `seba&apos; is the object of the verb `khaatha&apos;. This information helps in getting the correct translation. If the parser for the source sentence assigns the label `subject&apos; to both `raama’ and `seba&apos;, the MT system can not give the correct output. There were some attempts at handling these kind of linguistic constraints using integer programming approaches (Riedel et al., 2006; Bharati et al., 2008). In these approaches dependency parsing is formulated as solving an integer program as McDonald et al. (2006) has formulated dependency parsing as MST problem. All the linguistic constraints are encoded as constraints while solving the integer program. In other words, all the parses that violate these constraints are removed from the solution list. The parse with satisfies all the constraints is considered as the dependency tree for the sentence. In the following section, we describe two new approaches to avoid multiple subjects/objects for a verb. 3 Approaches In this section, we describe the two different approaches for avoiding the cases of a verb having multiple subjects/objects as its children i</context>
<context position="12089" citStr="McDonald et al., 2006" startWordPosition="1939" endWordPosition="1943">fthe-art parsers for these two languages. Then we apply our approaches and analyze the results. 4.1 Hindi Recently in NLP Tools Contest in ICON-2009 (Husain, 2009 and references herein), rule-based, constraint based, statistical and hybrid approaches were explored for parsing Hindi. All these attempts were at finding the inter-chunk dependency relations, given gold-standard POS and chunk tags. The state-of-the-art accuracy of 74.48% LAS (Labeled Attachment Score) is achieved by Ambati et al. (2009) for Hindi. They used two well-known data-driven parsers, Malt2 (Nivre et al., 2007b), and MST3 (McDonald et al., 2006) for their experiments. As the accuracy of the labeler of MST parser is very low, they used maximum entropy classification algorithm, MAXENT4 for labeling. For Hindi, dependency annotation is done using paninian framework (Begum et al., 2008; Bharati et al., 1995). So, in Hindi, the equivalent labels for subject and object are `karta (k1)&apos; and `karma (k2)&apos;. `karta&apos; and `karma&apos; are syntacticosemantic labels which have some properties of both grammatical roles and thematic roles. k1 behaves similar to subject and agent. k2 behaves similar to object and patient (Bharati et al., 1995; Bharati et a</context>
</contexts>
<marker>McDonald, Lerman, Pereira, 2006</marker>
<rawString>R. McDonald, K. Lerman, and F. Pereira. 2006. Multilingual dependency analysis with a two-stage discriminative parser. In Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL-X), pp. 216–220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>J Nivre</author>
</authors>
<title>Characterizing the errors of data-driven dependency parsing models.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="13781" citStr="McDonald and Nivre, 2007" startWordPosition="2213" endWordPosition="2217">re children of the coordinating conjunction. The coordinating conjunction is attached to the verb with k1/k2 label and the conjuncts get attached to the coordinating conjunction with a dependency label `ccof&apos;. We replicated the experiments of Ambati et al. (2009) on test set (150 sentences) of Hindi and analyzed the outputs of Malt and MST+MaxEnt. We consider this as the baseline. In the output of Malt, there are 39 instances of multiple subjects/objects. There are 51 such instances in the output of MST+MAXENT. Malt is good at short distance labeling and MST is good at long distance labeling (McDonald and Nivre, 2007). As `k1&apos; and `k2&apos; are short distance labels, Malt could able predict these labels more accurately than MST. Because of this output of MST has higher number of instances of multiple subjects/objects than Malt. Total Instances Malt 39 MST + MAXENT 51 Table 1: Number of instances of multiple subjects or objects in the output of the state-of-the-art parsers for Hindi Both the parsers output first best label for each node in the sentence. In case of Malt, we modified the implementation to extract all the possible dependency labels with their scores. As Malt uses libsvm for learning, we couldn&apos;t ab</context>
</contexts>
<marker>McDonald, Nivre, 2007</marker>
<rawString>R. McDonald and J. Nivre. 2007. Characterizing the errors of data-driven dependency parsing models. In Proc. of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>S Kubler</author>
<author>R McDonald</author>
<author>J Nilsson</author>
<author>S Riedel</author>
<author>D Yuret</author>
</authors>
<title>Shared Task on Dependency Parsing.</title>
<date>2007</date>
<journal>The CoNLL</journal>
<booktitle>In Proceedings of EMNLP/CoNLL-2007.</booktitle>
<contexts>
<context position="2629" citStr="Nivre et al., 2007" startWordPosition="391" endWordPosition="394">cy based one (Hudson, 1984; Shieber, 1985; Mel‟čuk, 1988, Bharati et al., 1995). The basic difference between a constituent based representation and a dependency representation is the lack of nonterminal nodes in the latter. It has also been noted that use of appropriate edge labels gives a level of semantics. It is perhaps due to these reasons that the recent past has seen a surge in the development of dependency based treebanks. Due to the availability of dependency treebanks, there are several recent attempts at building dependency parsers. Two CoNLL shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007a) were held aiming at building state-of-theart dependency parsers for different languages. Recently in NLP Tools Contest in ICON-2009 (Husain, 2009 and references therein), rulebased, constraint based, statistical and hybrid approaches were explored towards building dependency parsers for three Indian languages namely, Telugu, Hindi and Bangla. In all these efforts, state-of-the-art accuracies are obtained by two data-driven parsers, namely, Malt (Nivre et al., 2007b) and MST (McDonald et al., 2006). The major limitation of both these parsers is that they won&apos;t take linguistic constraints int</context>
<context position="12053" citStr="Nivre et al., 2007" startWordPosition="1933" endWordPosition="1936">cts in the output of the state-ofthe-art parsers for these two languages. Then we apply our approaches and analyze the results. 4.1 Hindi Recently in NLP Tools Contest in ICON-2009 (Husain, 2009 and references herein), rule-based, constraint based, statistical and hybrid approaches were explored for parsing Hindi. All these attempts were at finding the inter-chunk dependency relations, given gold-standard POS and chunk tags. The state-of-the-art accuracy of 74.48% LAS (Labeled Attachment Score) is achieved by Ambati et al. (2009) for Hindi. They used two well-known data-driven parsers, Malt2 (Nivre et al., 2007b), and MST3 (McDonald et al., 2006) for their experiments. As the accuracy of the labeler of MST parser is very low, they used maximum entropy classification algorithm, MAXENT4 for labeling. For Hindi, dependency annotation is done using paninian framework (Begum et al., 2008; Bharati et al., 1995). So, in Hindi, the equivalent labels for subject and object are `karta (k1)&apos; and `karma (k2)&apos;. `karta&apos; and `karma&apos; are syntacticosemantic labels which have some properties of both grammatical roles and thematic roles. k1 behaves similar to subject and agent. k2 behaves similar to object and patient</context>
<context position="14963" citStr="Nivre et al., 2007" startWordPosition="2406" endWordPosition="2409"> libsvm for learning, we couldn&apos;t able to get the probabilities. Though interpreting the scores provided by libsvm as probabilities is not the correct way, that is the only option currently available with Malt. In case of MST+MAXENT, labeling is performed by MAXENT. We used a java version of MAXENT5 to extract all possible tags with their scores. We applied both the naive and probabilistic approaches to avoid multiple subjects/objects. We evaluated our experiments based on unlabeled attachment score (UAS), labeled attachment score (LAS) and labeled score 5 http://maxent.sourceforge.net/ (LS) (Nivre et al., 2007a). Results are presented in Table 2. As expected, PA performs better than NA. With PA we got an improvement of 0.26% in LAS over the previous best results for Malt. In case of MST+MAXENT we got an improvement of 0.61% in LAS over the previous best results. Note that in case of MST+MAXENT, the slight difference between state-of-the-art results of Ambati et al. (2009) and our baseline accuracy is due different MAXENT package used. Malt MST+MAXENT UAS LAS LS UAS LAS LS Baseline 90.14 74.48 76.38 91.26 72.75 75.26 NA 90.14 74.57 76.38 91.26 72.84 75.26 PA 90.14 74.74 76.56 91.26 73.36 75.87 Table</context>
</contexts>
<marker>Nivre, Hall, Kubler, McDonald, Nilsson, Riedel, Yuret, 2007</marker>
<rawString>J. Nivre, J. Hall, S. Kubler, R. McDonald, J. Nilsson, S. Riedel and D. Yuret. 2007a. The CoNLL 2007 Shared Task on Dependency Parsing. In Proceedings of EMNLP/CoNLL-2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>J Nilsson</author>
<author>A Chanev</author>
<author>G Eryigit</author>
<author>S Kübler</author>
<author>S Marinov</author>
<author>E Marsi</author>
</authors>
<title>MaltParser: A language-independent system for data-driven dependency parsing.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<volume>13</volume>
<issue>2</issue>
<pages>95--135</pages>
<contexts>
<context position="2629" citStr="Nivre et al., 2007" startWordPosition="391" endWordPosition="394">cy based one (Hudson, 1984; Shieber, 1985; Mel‟čuk, 1988, Bharati et al., 1995). The basic difference between a constituent based representation and a dependency representation is the lack of nonterminal nodes in the latter. It has also been noted that use of appropriate edge labels gives a level of semantics. It is perhaps due to these reasons that the recent past has seen a surge in the development of dependency based treebanks. Due to the availability of dependency treebanks, there are several recent attempts at building dependency parsers. Two CoNLL shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007a) were held aiming at building state-of-theart dependency parsers for different languages. Recently in NLP Tools Contest in ICON-2009 (Husain, 2009 and references therein), rulebased, constraint based, statistical and hybrid approaches were explored towards building dependency parsers for three Indian languages namely, Telugu, Hindi and Bangla. In all these efforts, state-of-the-art accuracies are obtained by two data-driven parsers, namely, Malt (Nivre et al., 2007b) and MST (McDonald et al., 2006). The major limitation of both these parsers is that they won&apos;t take linguistic constraints int</context>
<context position="12053" citStr="Nivre et al., 2007" startWordPosition="1933" endWordPosition="1936">cts in the output of the state-ofthe-art parsers for these two languages. Then we apply our approaches and analyze the results. 4.1 Hindi Recently in NLP Tools Contest in ICON-2009 (Husain, 2009 and references herein), rule-based, constraint based, statistical and hybrid approaches were explored for parsing Hindi. All these attempts were at finding the inter-chunk dependency relations, given gold-standard POS and chunk tags. The state-of-the-art accuracy of 74.48% LAS (Labeled Attachment Score) is achieved by Ambati et al. (2009) for Hindi. They used two well-known data-driven parsers, Malt2 (Nivre et al., 2007b), and MST3 (McDonald et al., 2006) for their experiments. As the accuracy of the labeler of MST parser is very low, they used maximum entropy classification algorithm, MAXENT4 for labeling. For Hindi, dependency annotation is done using paninian framework (Begum et al., 2008; Bharati et al., 1995). So, in Hindi, the equivalent labels for subject and object are `karta (k1)&apos; and `karma (k2)&apos;. `karta&apos; and `karma&apos; are syntacticosemantic labels which have some properties of both grammatical roles and thematic roles. k1 behaves similar to subject and agent. k2 behaves similar to object and patient</context>
<context position="14963" citStr="Nivre et al., 2007" startWordPosition="2406" endWordPosition="2409"> libsvm for learning, we couldn&apos;t able to get the probabilities. Though interpreting the scores provided by libsvm as probabilities is not the correct way, that is the only option currently available with Malt. In case of MST+MAXENT, labeling is performed by MAXENT. We used a java version of MAXENT5 to extract all possible tags with their scores. We applied both the naive and probabilistic approaches to avoid multiple subjects/objects. We evaluated our experiments based on unlabeled attachment score (UAS), labeled attachment score (LAS) and labeled score 5 http://maxent.sourceforge.net/ (LS) (Nivre et al., 2007a). Results are presented in Table 2. As expected, PA performs better than NA. With PA we got an improvement of 0.26% in LAS over the previous best results for Malt. In case of MST+MAXENT we got an improvement of 0.61% in LAS over the previous best results. Note that in case of MST+MAXENT, the slight difference between state-of-the-art results of Ambati et al. (2009) and our baseline accuracy is due different MAXENT package used. Malt MST+MAXENT UAS LAS LS UAS LAS LS Baseline 90.14 74.48 76.38 91.26 72.75 75.26 NA 90.14 74.57 76.38 91.26 72.84 75.26 PA 90.14 74.74 76.56 91.26 73.36 75.87 Table</context>
</contexts>
<marker>Nivre, Hall, Nilsson, Chanev, Eryigit, Kübler, Marinov, Marsi, 2007</marker>
<rawString>J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit, S. Kübler, S. Marinov and E Marsi. 2007b. MaltParser: A language-independent system for data-driven dependency parsing. Natural Language Engineering, 13(2), 95-135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Riedel</author>
<author>Ruket Çakıcı</author>
<author>Ivan Meza-Ruiz</author>
</authors>
<title>Multi-lingual Dependency Parsing with Incremental Integer Linear Programming.</title>
<date>2006</date>
<booktitle>In Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL-X).</booktitle>
<contexts>
<context position="6868" citStr="Riedel et al., 2006" startWordPosition="1063" endWordPosition="1066">s&apos; `is&apos; ‘Ram eats an apple’ Though the source sentence is different, the target sentence is same. Even though the source sentences are different, the dependency tree is same for both the sentences. In both the cases, `raama’ is the subject and `seba&apos; is the object of the verb `khaatha&apos;. This information helps in getting the correct translation. If the parser for the source sentence assigns the label `subject&apos; to both `raama’ and `seba&apos;, the MT system can not give the correct output. There were some attempts at handling these kind of linguistic constraints using integer programming approaches (Riedel et al., 2006; Bharati et al., 2008). In these approaches dependency parsing is formulated as solving an integer program as McDonald et al. (2006) has formulated dependency parsing as MST problem. All the linguistic constraints are encoded as constraints while solving the integer program. In other words, all the parses that violate these constraints are removed from the solution list. The parse with satisfies all the constraints is considered as the dependency tree for the sentence. In the following section, we describe two new approaches to avoid multiple subjects/objects for a verb. 3 Approaches In this </context>
</contexts>
<marker>Riedel, Çakıcı, Meza-Ruiz, 2006</marker>
<rawString>S. Riedel, Ruket Çakıcı and Ivan Meza-Ruiz. 2006. Multi-lingual Dependency Parsing with Incremental Integer Linear Programming. In Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL-X).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Shieber</author>
</authors>
<title>Evidence against the contextfreeness of natural language.</title>
<date>1985</date>
<booktitle>In Linguistics and Philosophy,</booktitle>
<pages>8--334</pages>
<contexts>
<context position="2052" citStr="Shieber, 1985" startWordPosition="296" endWordPosition="297">corpora in recent years, data driven parsing has achieved considerable success. The availability of phrase structure treebank for English (Marcus et al., 1993) has seen the development of many efficient parsers. Using the dependency analysis, a similar large scale annotation effort for Czech, has been the Prague Dependency Treebank (Hajicova, 1998). Unlike English, Czech is a freeword-order language and is also morphologically very rich. It has been suggested that free-wordorder languages can be handled better using the dependency based framework than the constituency based one (Hudson, 1984; Shieber, 1985; Mel‟čuk, 1988, Bharati et al., 1995). The basic difference between a constituent based representation and a dependency representation is the lack of nonterminal nodes in the latter. It has also been noted that use of appropriate edge labels gives a level of semantics. It is perhaps due to these reasons that the recent past has seen a surge in the development of dependency based treebanks. Due to the availability of dependency treebanks, there are several recent attempts at building dependency parsers. Two CoNLL shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007a) were held aiming at </context>
</contexts>
<marker>Shieber, 1985</marker>
<rawString>S. M. Shieber. 1985. Evidence against the contextfreeness of natural language. In Linguistics and Philosophy, p. 8, 334–343.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>