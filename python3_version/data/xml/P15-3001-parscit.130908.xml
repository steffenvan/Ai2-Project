<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001182">
<title confidence="0.998747">
Unsupervised Learning and Modeling of Knowledge and Intent
for Spoken Dialogue Systems
</title>
<author confidence="0.997942">
Yun-Nung Chen
</author>
<affiliation confidence="0.999412">
School of Computer Science, Carnegie Mellon University
</affiliation>
<address confidence="0.511073">
5000 Forbes Avenue, Pittsburgh, PA 15213-3891, USA
</address>
<email confidence="0.999032">
yvchen@cs.cmu.edu
</email>
<sectionHeader confidence="0.993904" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999960333333333">
Spoken dialogue systems (SDS) are
rapidly appearing in various smart devices
(smartphone, smart-TV, in-car navigating
system, etc). The key role in a success-
ful SDS is a spoken language understand-
ing (SLU) component, which parses user
utterances into semantic concepts in order
to understand users’ intentions. However,
such semantic concepts and their struc-
ture are manually created by experts, and
the annotation process results in extremely
high cost and poor scalability in system
development. Therefore, the dissertation
focuses on improving SDS generalization
and scalability by automatically inferring
domain knowledge and learning structures
from unlabeled conversations through a
matrix factorization (MF) technique. With
the automatically acquired semantic con-
cepts and structures, we further investigate
whether such information can be utilized
to effectively understand user utterances
and then show the feasibility of reducing
human effort during SDS development.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999973319148936">
Various smart devices (e.g. smartphone, smart-
TV, in-car navigating system) are incorporating
spoken language interfaces, a.k.a. spoken dia-
logue systems (SDS), in order to help users finish
tasks more efficiently. The key role in a successful
SDS is a spoken language understanding (SLU)
component; in order to capture the language vari-
ation from dialogue participants, the SLU compo-
nent must create a mapping between the natural
language inputs and semantic representations that
correspond to users’ intentions.
The semantic representation must include “con-
cepts’ and a “structure”: concepts are the domain-
specific topics, and the structure describes the re-
lations between concepts and conveys intentions.
However, most prior work focused on learning
the mapping between utterances and semantic rep-
resentations, where such knowledge still remains
predefined. The need of annotations results in
extremely high cost and poor scalability in sys-
tem development. Therefore, current technology
usually limits conversational interactions to a few
narrow predefined domains/topics. With the in-
creasing conversational interactions, this disserta-
tion focuses on improving generalization and scal-
ability of building SDSs with little human effort.
In order to achieve the goal, two questions need
to be addressed: 1) Given unlabelled conversa-
tions, how can a system automatically induce and
organize the domain-specific concepts? 2) With
the automatically acquired knowledge, how can a
system understand user utterances and intents? To
tackle the above problems, we propose to acquire
the domain knowledge that captures human’s se-
mantics, intents, and behaviors. Then based on the
acquired knowledge, we build an SLU component
to understand users and to offer better interactions
in dialogues.
The dissertation shows the feasibility of build-
ing a dialogue learning system that is able to un-
derstand how particular domains work based on
unlabeled conversations. As a result, an initial
SDS can be automatically built according to the
learned knowledge, and its performance can be
quickly improved by interacting with users for
practical usage, presenting the potential of reduc-
ing human effort for SDS development.
</bodyText>
<sectionHeader confidence="0.999851" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999751">
Unsupervised SLU Tur et al. (2011; 2012) were
among the first to consider unsupervised ap-
proaches for SLU, where they exploited query logs
for slot-filling. In a subsequent study, Heck and
Hakkani-T¨ur (2012) studied the Semantic Web for
</bodyText>
<page confidence="0.822323">
1
</page>
<note confidence="0.9613585">
Proceedings of the ACL-IJCNLP 2015 Student Research Workshop, pages 1–7,
Beijing, China, July 28, 2015. c�2015 Association for Computational Linguistics
</note>
<figure confidence="0.999535612244898">
Unlabeled
Collection
Knowledge Acquisition
Semantic Concept Induction
Concept Relation Model
Feature Relation Model
Feature Observation Semantic Concept (Slot / Behavior)
SLU Modeling by MF
Feature Model
Ff
Fs
Relation
Propagation Model
.
Rf
Rs
“can I have a cheap restaurant”
Semantic
Representation
SLU
Model
food cheap restaurant food expensiveness
target
Train
Test
... ... ...
Utterance 1
i would like a cheap restaurant
Utterance 2
find a restaurant for chinese food
Feature Relation Model Semantic Concept Induction Concept Relation Model
.90
1 1
1
.05
1
1 1
Reasoning with Matrix Factorization
.93
1
.85
.98
1 1
.97
.05
1
.95
.92
1
</figure>
<figureCaption confidence="0.999914">
Figure 1: (a): The proposed framework. (b): Our MF method completes a partially-missing matrix
</figureCaption>
<bodyText confidence="0.989695391304348">
for semantic decoding/behavior prediction. Dark circles are observed facts, shaded circles are inferred
facts. The ontology induction maps observed feature patterns to semantic concepts. The feature rela-
tion model constructs correlations between observed feature patterns. The concept relation model learns
the high-level semantic correlations for inferring hidden semantic slots or predicting subsequent behav-
iors. Reasoning with matrix factorization incorporates these models jointly, and produces a coherent and
domain-specific SLU model.
the intent detection problem in SLU, showing that
results obtained from the unsupervised training
process align well with the performance of tradi-
tional supervised learning. Following their suc-
cess of unsupervised SLU, recent studies have also
obtained interesting results on the tasks of rela-
tion detection (Hakkani-T¨ur et al., 2013; Chen et
al., 2014a), entity extraction (Wang et al., 2014),
and extending domain coverage (El-Kahky et al.,
2014; Chen and Rudnicky, 2014). However, most
studies above do not explicitly learn latent factor
representations from the data—while we hypothe-
size that the better robustness can be achieved by
explicitly modeling the measurement errors (usu-
ally produced by automatic speech recognizers
(ASR)) using latent variable models and taking ad-
ditional local and global semantic constraints into
account.
Latent Variable Modeling in SLU Early stud-
ies on latent variable modeling in speech included
the classic hidden Markov model for statistical
speech recognition (Jelinek, 1997). Recently, Ce-
likyilmaz et al. (2011) were the first to study the
intent detection problem using query logs and a
discrete Bayesian latent variable model. In the
field of dialogue modeling, the partially observ-
able Markov decision process (POMDP) (Young
et al., 2013) model is a popular technique for di-
alogue management, reducing the cost of hand-
crafted dialogue managers while producing ro-
bustness against speech recognition errors. More
recently, Tur et al. (2013) used a semi-supervised
LDA model to show improvement on the slot fill-
ing task. Also, Zhai and Williams (2014) proposed
an unsupervised model for connecting words with
latent states in HMMs using topic models, obtain-
ing interesting qualitative and quantitative results.
However, for unsupervised SLU, it is not obvi-
ous how to incorporate additional information in
the HMMs. With increasing works about learn-
</bodyText>
<page confidence="0.962552">
2
</page>
<bodyText confidence="0.999949818181818">
ing the feature matrices for language representa-
tions (Mikolov et al., 2013), matrix factorization
(MF) has become very popular for both implicit
and explicit feedback (Rendle et al., 2009; Chen
et al., 2015a).
This thesis proposal is the first to propose a
framework about unsupervised SLU modeling,
which is able to simultaneously consider various
local and global knowledge automatically learned
from unlabelled data using a matrix factorization
(MF) technique.
</bodyText>
<sectionHeader confidence="0.9919" genericHeader="method">
3 The Proposed Work
</sectionHeader>
<bodyText confidence="0.999870736842105">
The proposed framework is shown in Figure 1(a),
where there are two main parts, one is knowledge
acquisition and another is SLU modeling by MF.
The first part is to acquire the domain knowledge
that is useful for building the domain-specific dia-
logue systems, which addresses the question about
how to induce and organize the semantic concepts
(the first problem). Here we propose ontology in-
duction and structure learning procedures. The on-
tology induction refers to the semantic concept in-
duction (yellow block) and the structure learning
refers to relation models (blue and pink blocks) in
Figure 1(a). The details are described in Section 4.
The second part is to self-train an SLU compo-
nent using the acquired knowledge for the domain-
specific SDS, and this part answers to the ques-
tion about how to utilize the obtained information
in SDSs to understand user utterances and intents.
There are two aspects regarding to SLU modeling,
semantic decoding and behavior prediction. The
semantic decoding is to parse the input utterances
into semantic forms for better understanding, and
the behavior prediction is to predict the subsequent
user behaviors for providing better system interac-
tions. This dissertation plans to apply MF tech-
niques to unsupervised SLU modeling, including
both semantic decoding and behavior prediction.
In the proposed model, we first build a fea-
ture matrix to represent training utterances, where
each row refers to an utterance and each column
refers to an observed feature pattern or a learned
semantic concept (either a slot or a behavior). Fig-
ure 1(b) illustrates an example of the matrix. Then
given a testing utterance, we can convert it into
a vector based on the observed patterns, and fill
in the missing values of the semantic concepts.
In the first example utterance of the figure, al-
though semantic slot food is not observed, the ut-
</bodyText>
<figure confidence="0.7781496">
can i have a cheap restaurant
Frame: expensiveness
FT LU: cheap
Frame: locale_by_use
FT/FE LU: restaurant
</figure>
<figureCaption confidence="0.717351333333333">
Figure 2: An example of probabilistic frame-
semantic parsing on ASR output. FT: frame target.
FE: frame element. LU: lexical unit.
</figureCaption>
<bodyText confidence="0.999544428571428">
terance implies the meaning facet food. The MF
approach is able to learn the latent feature vec-
tors for utterances and semantic concepts, infer-
ring implicit semantics to improve the decoding
process—namely, by filling the matrix with prob-
abilities (lower part of the matrix in Figure 1(b)).
The feature model is built on the observed fea-
ture patterns and the learned concepts, where the
concepts are obtained from the knowledge acqui-
sition process (Chen et al., 2013; Chen et al.,
2015b). Section 5.1 explains the detail of the
feature model. In order to consider the addi-
tional structure information, we propose a rela-
tion propagation model based on the learned struc-
ture, which includes a feature relation model (blue
block) and a concept relation model (pink block)
described in Section 5.2.
Finally we train an SLU model by learn-
ing latent feature vectors for utterances and
slots/behaviors through MF techniques. Combin-
ing with a relation propagation model, the trained
SLU model is able to estimate the probability that
each concept occurs in the testing utterance, and
how likely each concept is domain-specific simul-
taneously. In other words, the SLU model is
able to transform testing utterances into domain-
specific semantic representations or predicted be-
haviors without human involvement.
</bodyText>
<sectionHeader confidence="0.995914" genericHeader="method">
4 Knowledge Acquisition
</sectionHeader>
<bodyText confidence="0.999867166666667">
Given unlabeled conversations and available
knowledge resources, we plan to extract organized
knowledge that can be used for domain-specific
SDSs. The ontology induction and structure learn-
ing are proposed to automate an ontology building
process.
</bodyText>
<subsectionHeader confidence="0.94047">
4.1 Ontology Induction
</subsectionHeader>
<bodyText confidence="0.924455666666667">
Chen et al. (2013; 2014b) proposed to automat-
ically induce semantic slots for SDSs by frame-
semantic parsing, where all ASR-decoded utter-
</bodyText>
<note confidence="0.9695315">
Frame: capability
FT LU: can FE Filler: i
</note>
<page confidence="0.98783">
3
</page>
<figureCaption confidence="0.9978095">
Figure 3: A simplified example of the automati-
cally derived knowledge graph.
</figureCaption>
<bodyText confidence="0.91605456">
ances are parsed using SEMAFOR1, a state-of-
the-art frame-semantic parser (Das et al., 2010;
Das et al., 2013), and then all frames from parsed
results are extracted as slot candidates (Dinarelli
et al., 2009). For example, Figure 2 shows an ex-
ample of an ASR-decoded text output parsed by
SEMAFOR. There are three frames (capability,
expensiveness, and locale by use) in the utter-
ance, which we consider as slot candidates.
Since SEMAFOR was trained on FrameNet
annotation, which has a more generic frame-
semantic context, not all the frames from the pars-
ing results can be used as the actual slots in the
domain-specific dialogue systems. For instance, in
Figure 2, “expensiveness” and “locale by use”
frames are essentially the key slots for the pur-
pose of understanding in the restaurant query do-
main, whereas the “capability” frame does not
convey particularly valuable information for the
domain-specific SDS. In order to fix this is-
sue, Chen et al. (2014b) proved that integrating
continuous-valued word embeddings with a prob-
abilistic frame-semantic parser is able to identify
key semantic slots in an unsupervised fashion, re-
ducing the cost of designing task-oriented SDSs.
</bodyText>
<subsectionHeader confidence="0.996565">
4.2 Structure Learning
</subsectionHeader>
<bodyText confidence="0.999700090909091">
A key challenge of designing a coherent seman-
tic ontology for SLU is to consider the struc-
ture and relations between semantic concepts. In
practice, however, it is difficult for domain ex-
perts and professional annotators to define a co-
herent slot set, while considering various lexical,
syntactic, and semantic dependencies. The pre-
vious work exploited the typed syntactic depen-
dency theory for unsupervised induction and or-
ganization of semantic slots in SDSs (Chen et
al., 2015b). More specifically, two knowledge
</bodyText>
<footnote confidence="0.945632">
1http://www.ark.cs.cmu.edu/SEMAFOR/
</footnote>
<bodyText confidence="0.998935">
graphs, a slot-based semantic knowledge graph
and a word-based lexical knowledge graph, are au-
tomatically constructed. To jointly consider the
word-to-word, word-to-slot, and slot-to-slot rela-
tions, we use a random walk inference algorithm
to combine these two knowledge graphs, guided
by dependency grammars. Figure 3 is a simpli-
fied example of the automatically built semantic
knowledge graph corresponding to the restaurant
domain. The experiments showed that considering
inter-slot relations is crucial for generating a more
coherent and compete slot set, resulting in a better
SLU model, while enhancing the interpretability
of semantic slots.
</bodyText>
<sectionHeader confidence="0.960525" genericHeader="method">
5 SLU Modeling by Matrix Factorization
</sectionHeader>
<bodyText confidence="0.999984076923077">
For two aspects of SLU modeling: semantic de-
coding and behavior prediction, we plan to apply
MF to both tasks by treating learned concepts as
semantic slots and human behaviors respectively.
Considering the benefits brought by MF tech-
niques, including 1) modeling the noisy data, 2)
modeling hidden information, and 3) modeling
the dependency between observations, the disser-
tation applies an MF approach to SLU modeling
for SDSs. In our model, we use U to denote the
set of input utterances, F as the set of observed
feature patterns, and 5 as the set of semantic con-
cepts we would like to predict (slots or human be-
haviors). The pair of an utterance u E U and a
feature/concept x E {F +5}, (u, x), is afact. The
input to our model is a set of observed facts O, and
the observed facts for a given utterance is denoted
by {(u, x) E O1. The goal of our model is to es-
timate, for a given utterance u and a given feature
pattern/concept x, the probability, p(Mu,x = 1),
where Mu,x is a binary random variable that is
true if and only if x is the feature pattern/domain-
specific concept in the utterance u. We introduce a
series of exponential family models that estimate
the probability using a natural parameter 0u,x and
the logistic sigmoid function:
</bodyText>
<equation confidence="0.999580666666667">
p(Mu,x = 1 1 0u,x) = σ(0u,x) (1)
1
1 + exp (_0u,x).
</equation>
<bodyText confidence="0.999517">
We construct a matrix M|U|×(|F|+|S|) as observed
facts for MF by integrating a feature model and a
relation propagation model below.
</bodyText>
<figure confidence="0.9966578">
AMOD
food expensiveness
AMOD
NN
locale_by_use
relational_quantity
AMOD
PREP_FOR
PREP_FOR
seeking
</figure>
<page confidence="0.944354">
4
</page>
<subsectionHeader confidence="0.584813">
5.1 Feature Model
</subsectionHeader>
<bodyText confidence="0.9999826">
First, we build a binary feature pattern matrix Ff
based on the observations, where each row refers
to an utterance and each column refers to a feature
pattern (a word or a phrase). In other words, Ff
carries the basic word/phrase vector for each utter-
ance, which is illustrated as the left part of the ma-
trix in Figure 1(b). Then we build a binary matrix
Fs based on the induced semantic concepts from
Section 4.1, which also denotes the slot/behavior
features for all utterances (right part of the matrix
in Figure 1(b)).
For building the feature model MF, we concate-
nate two matrices and obtain MF = [ Ff Fs ],
which refers to the upper part of the matrix in Fig-
ure 1(b) for training utterances.
</bodyText>
<subsectionHeader confidence="0.913257">
5.2 Relation Propagation Model
</subsectionHeader>
<bodyText confidence="0.999978444444445">
It is shown that the structure of semantic concepts
helps decide domain-specific slots and further im-
proves the SLU performance (Chen et al., 2015b).
With the learned structure from Section 4.2, we
can model the relations between semantic con-
cepts, such as inter-slot and inter-behavior rela-
tions. Also, the relations between feature patterns
can be modeled in the similar way. We construct
two knowledge graphs to model the structure:
</bodyText>
<listItem confidence="0.990826">
• Feature knowledge graph is built as Gf =
(Vf, Eff), where Vf = {fi E F} and Eff =
{eij  |fi, fj E Vf}.
• Semantic concept knowledge graph is built
as Gs = (Vs, Ess), where Vs = {si E S}
and Ess = {eij  |si, sj E Vs}.
</listItem>
<bodyText confidence="0.998229">
The structured graph can model the relation
between the connected node pair (xi, xj) as
r(xi, xj). Here we compute two matrices Rs =
[r(si, sj)]jSjxjSj and Rf = [r(fi, fj)]jFjxjFj to
represent concept relations and feature relations
respectively. With the built relation models, we
combine them as a relation propagation matrix
</bodyText>
<equation confidence="0.985799">
MR2 :
MR = [ �f 0s]. (2)
</equation>
<bodyText confidence="0.998017">
The goal of this matrix is to propagate scores be-
tween nodes according to different types of rela-
tions in the constructed knowledge graphs (Chen
and Metze, 2012).
</bodyText>
<footnote confidence="0.8779875">
2The values in the diagonal of MR are 0 to model the
propagation from other entries.
</footnote>
<subsectionHeader confidence="0.479481">
5.3 Integrated Model
</subsectionHeader>
<bodyText confidence="0.996887">
With a feature model MF and a relation propaga-
tion model MR, we integrate them into a single
matrix.
</bodyText>
<equation confidence="0.997071666666667">
M = MF · (MR + I) (3)
[ FfRf+Ff 0 1
0 FsRs + Fs J&apos;
</equation>
<bodyText confidence="0.99999092">
where M is final matrix and I is the identity ma-
trix in order to remain the original values. The
matrix M is similar to MF, but some weights are
enhanced through the relation propagation model.
The feature relations are built by FfRf, which is
the matrix with internal weight propagation on the
feature knowledge graph (the blue arrow in Fig-
ure 1(b)). Similarly, FsRs models the semantic
concept correlations, and can be treated as the ma-
trix with internal weight propagation on the se-
mantic concept knowledge graph (the pink arrow
in Figure 1(b)). The propagation model can be
treated as running a random walk algorithm on the
graphs.
By integrating with the relation propagation
model, the relations can be propagated via the
knowledge graphs, and the hidden information
may be modeled based on the assumption that mu-
tual relations usually help inference (Chen et al.,
2015b). Hence, the structure information can be
automatically involved in the matrix. In conclu-
sion, for each utterance, the integrated model not
only predicts the probabilities that semantic con-
cepts occur but also considers whether they are
domain-specific.
</bodyText>
<subsectionHeader confidence="0.998972">
5.4 Model Learning
</subsectionHeader>
<bodyText confidence="0.99962275">
The proposed model is parameterized through
weights and latent component vectors, where the
parameters are estimated by maximizing the log
likelihood of observed data (Collins et al., 2001).
</bodyText>
<equation confidence="0.924409666666667">
p(θ  |Mu) (4)
p(Mu  |θ)p(θ)
ln p(Mu  |θ) − λθ,
</equation>
<bodyText confidence="0.9997114">
where Mu is the vector corresponding to the utter-
ance u from Mu,x in (1), because we assume that
each utterance is independent of others.
To avoid treating unobserved facts as designed
negative facts, we consider our positive-only data
</bodyText>
<figure confidence="0.950757166666667">
θ~ = arg max
θ
ri
uEU
arg max
θ
ri
uEU
�
= arg max
θ
uEU
</figure>
<page confidence="0.962802">
5
</page>
<bodyText confidence="0.99987845">
as implicit feedback. Bayesian Personalized Rank-
ing (BPR) is an optimization criterion that learns
from implicit feedback for MF, which uses a vari-
ant of the ranking: giving observed true facts
higher scores than unobserved (true or false)
facts (Rendle et al., 2009). Riedel et al. (2013)
also showed that BPR learns the implicit relations
and improves a relation extraction task.
To estimate the parameters in (4), we create a
dataset of ranked pairs from M in (3): for each
utterance u and each observed fact f+ = (u, x+),
where Mu,x &gt; 6, we choose each semantic con-
cept x− such that f− = (u, x−), where Mu,x &lt;
6, which refers to the semantic concept we have
not observed in utterance u. That is, we con-
struct the observed data O from M. Then for
each pair of facts f+ and f−, we want to model
p(f+) &gt; p(f−) and hence Bf+ &gt; Bf− accord-
ing to (1). BPR maximizes the summation of each
ranked pair, where the objective is
</bodyText>
<equation confidence="0.9220935">
� ln p(Mu  |B) = � � lnσ(Bf+ − Bf−).
uEU f+EO f−0O
</equation>
<bodyText confidence="0.999975857142857">
The BPR objective is an approximation to the
per utterance AUC (area under the ROC curve),
which directly correlates to what we want to
achieve – well-ranked semantic concepts per ut-
terance, which denotes the better estimation of se-
mantic slots or human behaviors.
To maximize the objective in (5), we employ a
stochastic gradient descent (SGD) algorithm (Ren-
dle et al., 2009). For each randomly sampled ob-
served fact (u, x+), we sample an unobserved fact
(u, x−), which results in |O |fact pairs (f−, f+).
For each pair, we perform an SGD update using
the gradient of the corresponding objective func-
tion for matrix factorization (Gantner et al., 2011).
</bodyText>
<sectionHeader confidence="0.997012" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.9996146">
This thesis proposal proposes an unsupervised
SLU approach by automating the dialogue learn-
ing process on speech conversations. The prelim-
inary results show that for the automatic speech
recognition (ASR) transcripts (word error rate is
about 37%), the acquired knowledge can be suc-
cessfully applied to SLU modeling through MF
techniques, guiding the direction of the method-
ology.
The main planed tasks include:
</bodyText>
<listItem confidence="0.999809333333333">
• Semantic concept identification
• Semantic concept annotation
• SLU modeling by matrix factorization
</listItem>
<bodyText confidence="0.999968875">
In this thesis proposal, ongoing work and future
plans have been presented towards an automati-
cally built domain-specific SDS. With increasing
semantic resources, such as Google’s Knowledge
Graph and Microsoft Satori, the dissertation shows
the feasibility that utilizing available knowledge
improves the generalization and the scalability of
dialogue system development for practical usage.
</bodyText>
<sectionHeader confidence="0.994941" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.977523">
I thank my committee members, Prof. Alexander
I. Rudnicky, Prof. Anatole Gershman, Prof. Alan
W Black, and Dr. Dilek Hakkani-T¨ur for their ad-
vising and anonymous reviewers for their useful
comments. I am also grateful to Prof. Mei Ling
Meng for her helpful mentoring.
</bodyText>
<sectionHeader confidence="0.998762" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.976604441176471">
(5)Asli Celikyilmaz, Dilek Hakkani-T¨ur, and Gokhan T¨ur.
2011. Leveraging web query logs to learn user in-
tent via bayesian discrete latent variable model. In
Proceedings of ICML.
Yun-Nung Chen and Florian Metze. 2012. Two-
layer mutually reinforced random walk for improved
multi-party meeting summarization. In Proceedings
of The 4th IEEE Workshop on Spoken Language
Tachnology, pages 461–466.
Yun-Nung Chen and Alexander I. Rudnicky. 2014.
Dynamically supporting unexplored domains in
conversational interactions by enriching semantics
with neural word embeddings. In Proceedings of
2014 IEEE Spoken Language Technology Workshop
(SLT), pages 590–595. IEEE.
Yun-Nung Chen, William Yang Wang, and Alexander I
Rudnicky. 2013. Unsupervised induction and filling
of semantic slots for spoken dialogue systems us-
ing frame-semantic parsing. In Proceedings of 2013
IEEE Workshop on Automatic Speech Recognition
and Understanding (ASRU), pages 120–125. IEEE.
Yun-Nung Chen, Dilek Hakkani-T¨ur, and Gokan Tur.
2014a. Deriving local relational surface forms from
dependency-based entity embeddings for unsuper-
vised spoken language understanding. In Proceed-
ings of 2014 IEEE Spoken Language Technology
Workshop (SLT), pages 242–247. IEEE.
Yun-Nung Chen, William Yang Wang, and Alexan-
der I. Rudnicky. 2014b. Leveraging frame se-
mantics and distributional semantics for unsuper-
vised semantic slot induction in spoken dialogue
systems. In Proceedings of 2014 IEEE Spoken Lan-
guage Technology Workshop (SLT), pages 584–589.
IEEE.
</reference>
<page confidence="0.99393">
6
</page>
<reference confidence="0.996590105769231">
Yun-Nung Chen, William Yang Wang, Anatole Ger-
shman, and Alexander I. Rudnicky. 2015a. Ma-
trix factorization with knowledge graph propagation
for unsupervised spoken language understanding.
In Proceedings of The 53rd Annual Meeting of the
Association for Computational Linguistics and The
7th International Joint Conference on Natural Lan-
guage Processing of the Asian Federation of Natural
Language Processing (ACL-IJCNLP 2015). ACL.
Yun-Nung Chen, William Yang Wang, and Alexan-
der I. Rudnicky. 2015b. Jointly modeling inter-
slot relations by random walk on knowledge graphs
for unsupervised spoken language understanding. In
Proceedings of the 2015 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics - Human Language Technologies.
ACL.
Michael Collins, Sanjoy Dasgupta, and Robert E
Schapire. 2001. A generalization of principal com-
ponents analysis to the exponential family. In Pro-
ceedings of Advances in Neural Information Pro-
cessing Systems, pages 617–624.
Dipanjan Das, Nathan Schneider, Desai Chen, and
Noah A Smith. 2010. Probabilistic frame-semantic
parsing. In Proceedings of The Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 948–956.
Dipanjan Das, Desai Chen, Andr´e F. T. Martins,
Nathan Schneider, and Noah A. Smith. 2013.
Frame-semantic parsing. Computational Linguis-
tics.
Marco Dinarelli, Silvia Quarteroni, Sara Tonelli,
Alessandro Moschitti, and Giuseppe Riccardi.
2009. Annotating spoken dialogs: from speech seg-
ments to dialog acts and frame semantics. In Pro-
ceedings of the 2nd Workshop on Semantic Repre-
sentation of Spoken Language, pages 34–41. ACL.
Ali El-Kahky, Derek Liu, Ruhi Sarikaya, G¨okhan T¨ur,
Dilek Hakkani-T¨ur, and Larry Heck. 2014. Ex-
tending domain coverage of language understanding
systems via intent transfer between domains using
knowledge graphs and search query click logs. In
Proceedings of ICASSP.
Zeno Gantner, Steffen Rendle, Christoph Freuden-
thaler, and Lars Schmidt-Thieme. 2011. Mymedi-
alite: A free recommender system library. In Pro-
ceedings of the fifth ACM conference on Recom-
mender systems, pages 305–308. ACM.
Dilek Hakkani-T¨ur, Larry Heck, and Gokhan Tur.
2013. Using a knowledge graph and query click logs
for unsupervised learning of relation detection. In
Proceedings of ICASSP, pages 8327–8331.
Larry Heck and Dilek Hakkani-T¨ur. 2012. Exploiting
the semantic web for unsupervised spoken language
understanding. In Proceedings of SLT, pages 228–
233.
Frederick Jelinek. 1997. Statistical methods for speech
recognition. MIT press.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Proceedings of Advances in Neural Informa-
tion Processing Systems, pages 3111–3119.
Steffen Rendle, Christoph Freudenthaler, Zeno Gant-
ner, and Lars Schmidt-Thieme. 2009. BPR:
Bayesian personalized ranking from implicit feed-
back. In Proceedings of the Twenty-Fifth Confer-
ence on Uncertainty in Artificial Intelligence, pages
452–461. AUAI Press.
Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M Marlin. 2013. Relation extraction with
matrix factorization and universal schemas. In Pro-
ceedings of NAACL-HLT, pages 74–84.
Gokhan Tur, Dilek Z Hakkani-T¨ur, Dustin Hillard, and
Asli Celikyilmaz. 2011. Towards unsupervised
spoken language understanding: Exploiting query
click logs for slot filling. In Proceedings of INTER-
SPEECH, pages 1293–1296.
Gokhan Tur, Minwoo Jeong, Ye-Yi Wang, Dilek
Hakkani-T¨ur, and Larry P Heck. 2012. Exploit-
ing the semantic web for unsupervised natural lan-
guage semantic parsing. In Proceedings of INTER-
SPEECH.
Gokhan Tur, Asli Celikyilmaz, and Dilek Hakkani-
Tur. 2013. Latent semantic modeling for slot fill-
ing in conversational understanding. In Proceedings
of 2013 IEEE International Conference on Acous-
tics, Speech and Signal Processing (ICASSP), pages
8307–8311. IEEE.
Lu Wang, Dilek Hakkani-T¨ur, and Larry Heck. 2014.
Leveraging semantic web search and browse ses-
sions for multi-turn spoken dialog systems. In Pro-
ceedings of 2014 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP),
pages 4082–4086. IEEE.
Steve Young, Milica Gasic, Blaise Thomson, and Ja-
son D Williams. 2013. POMDP-based statistical
spoken dialog systems: A review. Proceedings of
the IEEE, 101(5):1160–1179.
Ke Zhai and Jason D Williams. 2014. Discovering
latent structure in task-oriented dialogues. In Pro-
ceedings of the Association for Computational Lin-
guistics.
</reference>
<page confidence="0.999505">
7
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.875908">
<title confidence="0.997557">Unsupervised Learning and Modeling of Knowledge and for Spoken Dialogue Systems</title>
<author confidence="0.921451">Yun-Nung</author>
<affiliation confidence="0.998375">School of Computer Science, Carnegie Mellon</affiliation>
<address confidence="0.999521">5000 Forbes Avenue, Pittsburgh, PA 15213-3891,</address>
<email confidence="0.999555">yvchen@cs.cmu.edu</email>
<abstract confidence="0.99824848">Spoken dialogue systems (SDS) are rapidly appearing in various smart devices (smartphone, smart-TV, in-car navigating system, etc). The key role in a successful SDS is a spoken language understanding (SLU) component, which parses user utterances into semantic concepts in order to understand users’ intentions. However, such semantic concepts and their structure are manually created by experts, and the annotation process results in extremely high cost and poor scalability in system development. Therefore, the dissertation focuses on improving SDS generalization and scalability by automatically inferring domain knowledge and learning structures from unlabeled conversations through a matrix factorization (MF) technique. With the automatically acquired semantic concepts and structures, we further investigate whether such information can be utilized to effectively understand user utterances and then show the feasibility of reducing human effort during SDS development.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Asli Celikyilmaz</author>
<author>Dilek Hakkani-T¨ur</author>
<author>Gokhan T¨ur</author>
</authors>
<title>Leveraging web query logs to learn user intent via bayesian discrete latent variable model.</title>
<date>2011</date>
<booktitle>In Proceedings of ICML.</booktitle>
<marker>Celikyilmaz, Hakkani-T¨ur, T¨ur, 2011</marker>
<rawString>(5)Asli Celikyilmaz, Dilek Hakkani-T¨ur, and Gokhan T¨ur. 2011. Leveraging web query logs to learn user intent via bayesian discrete latent variable model. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yun-Nung Chen</author>
<author>Florian Metze</author>
</authors>
<title>Twolayer mutually reinforced random walk for improved multi-party meeting summarization.</title>
<date>2012</date>
<booktitle>In Proceedings of The 4th IEEE Workshop on Spoken Language Tachnology,</booktitle>
<pages>461--466</pages>
<contexts>
<context position="17343" citStr="Chen and Metze, 2012" startWordPosition="2728" endWordPosition="2731">ntic concept knowledge graph is built as Gs = (Vs, Ess), where Vs = {si E S} and Ess = {eij |si, sj E Vs}. The structured graph can model the relation between the connected node pair (xi, xj) as r(xi, xj). Here we compute two matrices Rs = [r(si, sj)]jSjxjSj and Rf = [r(fi, fj)]jFjxjFj to represent concept relations and feature relations respectively. With the built relation models, we combine them as a relation propagation matrix MR2 : MR = [ �f 0s]. (2) The goal of this matrix is to propagate scores between nodes according to different types of relations in the constructed knowledge graphs (Chen and Metze, 2012). 2The values in the diagonal of MR are 0 to model the propagation from other entries. 5.3 Integrated Model With a feature model MF and a relation propagation model MR, we integrate them into a single matrix. M = MF · (MR + I) (3) [ FfRf+Ff 0 1 0 FsRs + Fs J&apos; where M is final matrix and I is the identity matrix in order to remain the original values. The matrix M is similar to MF, but some weights are enhanced through the relation propagation model. The feature relations are built by FfRf, which is the matrix with internal weight propagation on the feature knowledge graph (the blue arrow in Fi</context>
</contexts>
<marker>Chen, Metze, 2012</marker>
<rawString>Yun-Nung Chen and Florian Metze. 2012. Twolayer mutually reinforced random walk for improved multi-party meeting summarization. In Proceedings of The 4th IEEE Workshop on Spoken Language Tachnology, pages 461–466.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yun-Nung Chen</author>
<author>Alexander I Rudnicky</author>
</authors>
<title>Dynamically supporting unexplored domains in conversational interactions by enriching semantics with neural word embeddings.</title>
<date>2014</date>
<booktitle>In Proceedings of 2014 IEEE Spoken Language Technology Workshop (SLT),</booktitle>
<pages>590--595</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="5587" citStr="Chen and Rudnicky, 2014" startWordPosition="813" endWordPosition="816">g subsequent behaviors. Reasoning with matrix factorization incorporates these models jointly, and produces a coherent and domain-specific SLU model. the intent detection problem in SLU, showing that results obtained from the unsupervised training process align well with the performance of traditional supervised learning. Following their success of unsupervised SLU, recent studies have also obtained interesting results on the tasks of relation detection (Hakkani-T¨ur et al., 2013; Chen et al., 2014a), entity extraction (Wang et al., 2014), and extending domain coverage (El-Kahky et al., 2014; Chen and Rudnicky, 2014). However, most studies above do not explicitly learn latent factor representations from the data—while we hypothesize that the better robustness can be achieved by explicitly modeling the measurement errors (usually produced by automatic speech recognizers (ASR)) using latent variable models and taking additional local and global semantic constraints into account. Latent Variable Modeling in SLU Early studies on latent variable modeling in speech included the classic hidden Markov model for statistical speech recognition (Jelinek, 1997). Recently, Celikyilmaz et al. (2011) were the first to s</context>
</contexts>
<marker>Chen, Rudnicky, 2014</marker>
<rawString>Yun-Nung Chen and Alexander I. Rudnicky. 2014. Dynamically supporting unexplored domains in conversational interactions by enriching semantics with neural word embeddings. In Proceedings of 2014 IEEE Spoken Language Technology Workshop (SLT), pages 590–595. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yun-Nung Chen</author>
<author>William Yang Wang</author>
<author>Alexander I Rudnicky</author>
</authors>
<title>Unsupervised induction and filling of semantic slots for spoken dialogue systems using frame-semantic parsing.</title>
<date>2013</date>
<booktitle>In Proceedings of 2013 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU),</booktitle>
<pages>120--125</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="10065" citStr="Chen et al., 2013" startWordPosition="1524" endWordPosition="1527">FT/FE LU: restaurant Figure 2: An example of probabilistic framesemantic parsing on ASR output. FT: frame target. FE: frame element. LU: lexical unit. terance implies the meaning facet food. The MF approach is able to learn the latent feature vectors for utterances and semantic concepts, inferring implicit semantics to improve the decoding process—namely, by filling the matrix with probabilities (lower part of the matrix in Figure 1(b)). The feature model is built on the observed feature patterns and the learned concepts, where the concepts are obtained from the knowledge acquisition process (Chen et al., 2013; Chen et al., 2015b). Section 5.1 explains the detail of the feature model. In order to consider the additional structure information, we propose a relation propagation model based on the learned structure, which includes a feature relation model (blue block) and a concept relation model (pink block) described in Section 5.2. Finally we train an SLU model by learning latent feature vectors for utterances and slots/behaviors through MF techniques. Combining with a relation propagation model, the trained SLU model is able to estimate the probability that each concept occurs in the testing utter</context>
</contexts>
<marker>Chen, Wang, Rudnicky, 2013</marker>
<rawString>Yun-Nung Chen, William Yang Wang, and Alexander I Rudnicky. 2013. Unsupervised induction and filling of semantic slots for spoken dialogue systems using frame-semantic parsing. In Proceedings of 2013 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), pages 120–125. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yun-Nung Chen</author>
<author>Dilek Hakkani-T¨ur</author>
<author>Gokan Tur</author>
</authors>
<title>Deriving local relational surface forms from dependency-based entity embeddings for unsupervised spoken language understanding.</title>
<date>2014</date>
<booktitle>In Proceedings of 2014 IEEE Spoken Language Technology Workshop (SLT),</booktitle>
<pages>242--247</pages>
<publisher>IEEE.</publisher>
<marker>Chen, Hakkani-T¨ur, Tur, 2014</marker>
<rawString>Yun-Nung Chen, Dilek Hakkani-T¨ur, and Gokan Tur. 2014a. Deriving local relational surface forms from dependency-based entity embeddings for unsupervised spoken language understanding. In Proceedings of 2014 IEEE Spoken Language Technology Workshop (SLT), pages 242–247. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yun-Nung Chen</author>
<author>William Yang Wang</author>
<author>Alexander I Rudnicky</author>
</authors>
<title>Leveraging frame semantics and distributional semantics for unsupervised semantic slot induction in spoken dialogue systems.</title>
<date>2014</date>
<booktitle>In Proceedings of 2014 IEEE Spoken Language Technology Workshop (SLT),</booktitle>
<pages>584--589</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="5466" citStr="Chen et al., 2014" startWordPosition="795" endWordPosition="798">oncept relation model learns the high-level semantic correlations for inferring hidden semantic slots or predicting subsequent behaviors. Reasoning with matrix factorization incorporates these models jointly, and produces a coherent and domain-specific SLU model. the intent detection problem in SLU, showing that results obtained from the unsupervised training process align well with the performance of traditional supervised learning. Following their success of unsupervised SLU, recent studies have also obtained interesting results on the tasks of relation detection (Hakkani-T¨ur et al., 2013; Chen et al., 2014a), entity extraction (Wang et al., 2014), and extending domain coverage (El-Kahky et al., 2014; Chen and Rudnicky, 2014). However, most studies above do not explicitly learn latent factor representations from the data—while we hypothesize that the better robustness can be achieved by explicitly modeling the measurement errors (usually produced by automatic speech recognizers (ASR)) using latent variable models and taking additional local and global semantic constraints into account. Latent Variable Modeling in SLU Early studies on latent variable modeling in speech included the classic hidden</context>
<context position="12410" citStr="Chen et al. (2014" startWordPosition="1898" endWordPosition="1901">eness, and locale by use) in the utterance, which we consider as slot candidates. Since SEMAFOR was trained on FrameNet annotation, which has a more generic framesemantic context, not all the frames from the parsing results can be used as the actual slots in the domain-specific dialogue systems. For instance, in Figure 2, “expensiveness” and “locale by use” frames are essentially the key slots for the purpose of understanding in the restaurant query domain, whereas the “capability” frame does not convey particularly valuable information for the domain-specific SDS. In order to fix this issue, Chen et al. (2014b) proved that integrating continuous-valued word embeddings with a probabilistic frame-semantic parser is able to identify key semantic slots in an unsupervised fashion, reducing the cost of designing task-oriented SDSs. 4.2 Structure Learning A key challenge of designing a coherent semantic ontology for SLU is to consider the structure and relations between semantic concepts. In practice, however, it is difficult for domain experts and professional annotators to define a coherent slot set, while considering various lexical, syntactic, and semantic dependencies. The previous work exploited th</context>
</contexts>
<marker>Chen, Wang, Rudnicky, 2014</marker>
<rawString>Yun-Nung Chen, William Yang Wang, and Alexander I. Rudnicky. 2014b. Leveraging frame semantics and distributional semantics for unsupervised semantic slot induction in spoken dialogue systems. In Proceedings of 2014 IEEE Spoken Language Technology Workshop (SLT), pages 584–589. IEEE.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Yun-Nung Chen</author>
<author>William Yang Wang</author>
<author>Anatole Gershman</author>
<author>Alexander I Rudnicky</author>
</authors>
<title>Matrix factorization with knowledge graph propagation for unsupervised spoken language understanding.</title>
<date>2015</date>
<booktitle>In Proceedings of The 53rd Annual Meeting of the Association for Computational Linguistics and The 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing (ACL-IJCNLP</booktitle>
<publisher>ACL.</publisher>
<contexts>
<context position="7213" citStr="Chen et al., 2015" startWordPosition="1063" endWordPosition="1066">) used a semi-supervised LDA model to show improvement on the slot filling task. Also, Zhai and Williams (2014) proposed an unsupervised model for connecting words with latent states in HMMs using topic models, obtaining interesting qualitative and quantitative results. However, for unsupervised SLU, it is not obvious how to incorporate additional information in the HMMs. With increasing works about learn2 ing the feature matrices for language representations (Mikolov et al., 2013), matrix factorization (MF) has become very popular for both implicit and explicit feedback (Rendle et al., 2009; Chen et al., 2015a). This thesis proposal is the first to propose a framework about unsupervised SLU modeling, which is able to simultaneously consider various local and global knowledge automatically learned from unlabelled data using a matrix factorization (MF) technique. 3 The Proposed Work The proposed framework is shown in Figure 1(a), where there are two main parts, one is knowledge acquisition and another is SLU modeling by MF. The first part is to acquire the domain knowledge that is useful for building the domain-specific dialogue systems, which addresses the question about how to induce and organize </context>
<context position="10084" citStr="Chen et al., 2015" startWordPosition="1528" endWordPosition="1531">t Figure 2: An example of probabilistic framesemantic parsing on ASR output. FT: frame target. FE: frame element. LU: lexical unit. terance implies the meaning facet food. The MF approach is able to learn the latent feature vectors for utterances and semantic concepts, inferring implicit semantics to improve the decoding process—namely, by filling the matrix with probabilities (lower part of the matrix in Figure 1(b)). The feature model is built on the observed feature patterns and the learned concepts, where the concepts are obtained from the knowledge acquisition process (Chen et al., 2013; Chen et al., 2015b). Section 5.1 explains the detail of the feature model. In order to consider the additional structure information, we propose a relation propagation model based on the learned structure, which includes a feature relation model (blue block) and a concept relation model (pink block) described in Section 5.2. Finally we train an SLU model by learning latent feature vectors for utterances and slots/behaviors through MF techniques. Combining with a relation propagation model, the trained SLU model is able to estimate the probability that each concept occurs in the testing utterance, and how likel</context>
<context position="13134" citStr="Chen et al., 2015" startWordPosition="2009" endWordPosition="2012">able to identify key semantic slots in an unsupervised fashion, reducing the cost of designing task-oriented SDSs. 4.2 Structure Learning A key challenge of designing a coherent semantic ontology for SLU is to consider the structure and relations between semantic concepts. In practice, however, it is difficult for domain experts and professional annotators to define a coherent slot set, while considering various lexical, syntactic, and semantic dependencies. The previous work exploited the typed syntactic dependency theory for unsupervised induction and organization of semantic slots in SDSs (Chen et al., 2015b). More specifically, two knowledge 1http://www.ark.cs.cmu.edu/SEMAFOR/ graphs, a slot-based semantic knowledge graph and a word-based lexical knowledge graph, are automatically constructed. To jointly consider the word-to-word, word-to-slot, and slot-to-slot relations, we use a random walk inference algorithm to combine these two knowledge graphs, guided by dependency grammars. Figure 3 is a simplified example of the automatically built semantic knowledge graph corresponding to the restaurant domain. The experiments showed that considering inter-slot relations is crucial for generating a mor</context>
<context position="16322" citStr="Chen et al., 2015" startWordPosition="2544" endWordPosition="2547">hich is illustrated as the left part of the matrix in Figure 1(b). Then we build a binary matrix Fs based on the induced semantic concepts from Section 4.1, which also denotes the slot/behavior features for all utterances (right part of the matrix in Figure 1(b)). For building the feature model MF, we concatenate two matrices and obtain MF = [ Ff Fs ], which refers to the upper part of the matrix in Figure 1(b) for training utterances. 5.2 Relation Propagation Model It is shown that the structure of semantic concepts helps decide domain-specific slots and further improves the SLU performance (Chen et al., 2015b). With the learned structure from Section 4.2, we can model the relations between semantic concepts, such as inter-slot and inter-behavior relations. Also, the relations between feature patterns can be modeled in the similar way. We construct two knowledge graphs to model the structure: • Feature knowledge graph is built as Gf = (Vf, Eff), where Vf = {fi E F} and Eff = {eij |fi, fj E Vf}. • Semantic concept knowledge graph is built as Gs = (Vs, Ess), where Vs = {si E S} and Ess = {eij |si, sj E Vs}. The structured graph can model the relation between the connected node pair (xi, xj) as r(xi,</context>
<context position="18478" citStr="Chen et al., 2015" startWordPosition="2931" endWordPosition="2934">h internal weight propagation on the feature knowledge graph (the blue arrow in Figure 1(b)). Similarly, FsRs models the semantic concept correlations, and can be treated as the matrix with internal weight propagation on the semantic concept knowledge graph (the pink arrow in Figure 1(b)). The propagation model can be treated as running a random walk algorithm on the graphs. By integrating with the relation propagation model, the relations can be propagated via the knowledge graphs, and the hidden information may be modeled based on the assumption that mutual relations usually help inference (Chen et al., 2015b). Hence, the structure information can be automatically involved in the matrix. In conclusion, for each utterance, the integrated model not only predicts the probabilities that semantic concepts occur but also considers whether they are domain-specific. 5.4 Model Learning The proposed model is parameterized through weights and latent component vectors, where the parameters are estimated by maximizing the log likelihood of observed data (Collins et al., 2001). p(θ |Mu) (4) p(Mu |θ)p(θ) ln p(Mu |θ) − λθ, where Mu is the vector corresponding to the utterance u from Mu,x in (1), because we assum</context>
</contexts>
<marker>Chen, Wang, Gershman, Rudnicky, 2015</marker>
<rawString>Yun-Nung Chen, William Yang Wang, Anatole Gershman, and Alexander I. Rudnicky. 2015a. Matrix factorization with knowledge graph propagation for unsupervised spoken language understanding. In Proceedings of The 53rd Annual Meeting of the Association for Computational Linguistics and The 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing (ACL-IJCNLP 2015). ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yun-Nung Chen</author>
<author>William Yang Wang</author>
<author>Alexander I Rudnicky</author>
</authors>
<title>Jointly modeling interslot relations by random walk on knowledge graphs for unsupervised spoken language understanding.</title>
<date>2015</date>
<booktitle>In Proceedings of the 2015 Conference of the North American Chapter of</booktitle>
<publisher>ACL.</publisher>
<contexts>
<context position="7213" citStr="Chen et al., 2015" startWordPosition="1063" endWordPosition="1066">) used a semi-supervised LDA model to show improvement on the slot filling task. Also, Zhai and Williams (2014) proposed an unsupervised model for connecting words with latent states in HMMs using topic models, obtaining interesting qualitative and quantitative results. However, for unsupervised SLU, it is not obvious how to incorporate additional information in the HMMs. With increasing works about learn2 ing the feature matrices for language representations (Mikolov et al., 2013), matrix factorization (MF) has become very popular for both implicit and explicit feedback (Rendle et al., 2009; Chen et al., 2015a). This thesis proposal is the first to propose a framework about unsupervised SLU modeling, which is able to simultaneously consider various local and global knowledge automatically learned from unlabelled data using a matrix factorization (MF) technique. 3 The Proposed Work The proposed framework is shown in Figure 1(a), where there are two main parts, one is knowledge acquisition and another is SLU modeling by MF. The first part is to acquire the domain knowledge that is useful for building the domain-specific dialogue systems, which addresses the question about how to induce and organize </context>
<context position="10084" citStr="Chen et al., 2015" startWordPosition="1528" endWordPosition="1531">t Figure 2: An example of probabilistic framesemantic parsing on ASR output. FT: frame target. FE: frame element. LU: lexical unit. terance implies the meaning facet food. The MF approach is able to learn the latent feature vectors for utterances and semantic concepts, inferring implicit semantics to improve the decoding process—namely, by filling the matrix with probabilities (lower part of the matrix in Figure 1(b)). The feature model is built on the observed feature patterns and the learned concepts, where the concepts are obtained from the knowledge acquisition process (Chen et al., 2013; Chen et al., 2015b). Section 5.1 explains the detail of the feature model. In order to consider the additional structure information, we propose a relation propagation model based on the learned structure, which includes a feature relation model (blue block) and a concept relation model (pink block) described in Section 5.2. Finally we train an SLU model by learning latent feature vectors for utterances and slots/behaviors through MF techniques. Combining with a relation propagation model, the trained SLU model is able to estimate the probability that each concept occurs in the testing utterance, and how likel</context>
<context position="13134" citStr="Chen et al., 2015" startWordPosition="2009" endWordPosition="2012">able to identify key semantic slots in an unsupervised fashion, reducing the cost of designing task-oriented SDSs. 4.2 Structure Learning A key challenge of designing a coherent semantic ontology for SLU is to consider the structure and relations between semantic concepts. In practice, however, it is difficult for domain experts and professional annotators to define a coherent slot set, while considering various lexical, syntactic, and semantic dependencies. The previous work exploited the typed syntactic dependency theory for unsupervised induction and organization of semantic slots in SDSs (Chen et al., 2015b). More specifically, two knowledge 1http://www.ark.cs.cmu.edu/SEMAFOR/ graphs, a slot-based semantic knowledge graph and a word-based lexical knowledge graph, are automatically constructed. To jointly consider the word-to-word, word-to-slot, and slot-to-slot relations, we use a random walk inference algorithm to combine these two knowledge graphs, guided by dependency grammars. Figure 3 is a simplified example of the automatically built semantic knowledge graph corresponding to the restaurant domain. The experiments showed that considering inter-slot relations is crucial for generating a mor</context>
<context position="16322" citStr="Chen et al., 2015" startWordPosition="2544" endWordPosition="2547">hich is illustrated as the left part of the matrix in Figure 1(b). Then we build a binary matrix Fs based on the induced semantic concepts from Section 4.1, which also denotes the slot/behavior features for all utterances (right part of the matrix in Figure 1(b)). For building the feature model MF, we concatenate two matrices and obtain MF = [ Ff Fs ], which refers to the upper part of the matrix in Figure 1(b) for training utterances. 5.2 Relation Propagation Model It is shown that the structure of semantic concepts helps decide domain-specific slots and further improves the SLU performance (Chen et al., 2015b). With the learned structure from Section 4.2, we can model the relations between semantic concepts, such as inter-slot and inter-behavior relations. Also, the relations between feature patterns can be modeled in the similar way. We construct two knowledge graphs to model the structure: • Feature knowledge graph is built as Gf = (Vf, Eff), where Vf = {fi E F} and Eff = {eij |fi, fj E Vf}. • Semantic concept knowledge graph is built as Gs = (Vs, Ess), where Vs = {si E S} and Ess = {eij |si, sj E Vs}. The structured graph can model the relation between the connected node pair (xi, xj) as r(xi,</context>
<context position="18478" citStr="Chen et al., 2015" startWordPosition="2931" endWordPosition="2934">h internal weight propagation on the feature knowledge graph (the blue arrow in Figure 1(b)). Similarly, FsRs models the semantic concept correlations, and can be treated as the matrix with internal weight propagation on the semantic concept knowledge graph (the pink arrow in Figure 1(b)). The propagation model can be treated as running a random walk algorithm on the graphs. By integrating with the relation propagation model, the relations can be propagated via the knowledge graphs, and the hidden information may be modeled based on the assumption that mutual relations usually help inference (Chen et al., 2015b). Hence, the structure information can be automatically involved in the matrix. In conclusion, for each utterance, the integrated model not only predicts the probabilities that semantic concepts occur but also considers whether they are domain-specific. 5.4 Model Learning The proposed model is parameterized through weights and latent component vectors, where the parameters are estimated by maximizing the log likelihood of observed data (Collins et al., 2001). p(θ |Mu) (4) p(Mu |θ)p(θ) ln p(Mu |θ) − λθ, where Mu is the vector corresponding to the utterance u from Mu,x in (1), because we assum</context>
</contexts>
<marker>Chen, Wang, Rudnicky, 2015</marker>
<rawString>Yun-Nung Chen, William Yang Wang, and Alexander I. Rudnicky. 2015b. Jointly modeling interslot relations by random walk on knowledge graphs for unsupervised spoken language understanding. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Sanjoy Dasgupta</author>
<author>Robert E Schapire</author>
</authors>
<title>A generalization of principal components analysis to the exponential family.</title>
<date>2001</date>
<booktitle>In Proceedings of Advances in Neural Information Processing Systems,</booktitle>
<pages>617--624</pages>
<contexts>
<context position="18942" citStr="Collins et al., 2001" startWordPosition="2999" endWordPosition="3002">d via the knowledge graphs, and the hidden information may be modeled based on the assumption that mutual relations usually help inference (Chen et al., 2015b). Hence, the structure information can be automatically involved in the matrix. In conclusion, for each utterance, the integrated model not only predicts the probabilities that semantic concepts occur but also considers whether they are domain-specific. 5.4 Model Learning The proposed model is parameterized through weights and latent component vectors, where the parameters are estimated by maximizing the log likelihood of observed data (Collins et al., 2001). p(θ |Mu) (4) p(Mu |θ)p(θ) ln p(Mu |θ) − λθ, where Mu is the vector corresponding to the utterance u from Mu,x in (1), because we assume that each utterance is independent of others. To avoid treating unobserved facts as designed negative facts, we consider our positive-only data θ~ = arg max θ ri uEU arg max θ ri uEU � = arg max θ uEU 5 as implicit feedback. Bayesian Personalized Ranking (BPR) is an optimization criterion that learns from implicit feedback for MF, which uses a variant of the ranking: giving observed true facts higher scores than unobserved (true or false) facts (Rendle et al</context>
</contexts>
<marker>Collins, Dasgupta, Schapire, 2001</marker>
<rawString>Michael Collins, Sanjoy Dasgupta, and Robert E Schapire. 2001. A generalization of principal components analysis to the exponential family. In Proceedings of Advances in Neural Information Processing Systems, pages 617–624.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Nathan Schneider</author>
<author>Desai Chen</author>
<author>Noah A Smith</author>
</authors>
<title>Probabilistic frame-semantic parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of The Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>948--956</pages>
<contexts>
<context position="11541" citStr="Das et al., 2010" startWordPosition="1753" endWordPosition="1756">ven unlabeled conversations and available knowledge resources, we plan to extract organized knowledge that can be used for domain-specific SDSs. The ontology induction and structure learning are proposed to automate an ontology building process. 4.1 Ontology Induction Chen et al. (2013; 2014b) proposed to automatically induce semantic slots for SDSs by framesemantic parsing, where all ASR-decoded utterFrame: capability FT LU: can FE Filler: i 3 Figure 3: A simplified example of the automatically derived knowledge graph. ances are parsed using SEMAFOR1, a state-ofthe-art frame-semantic parser (Das et al., 2010; Das et al., 2013), and then all frames from parsed results are extracted as slot candidates (Dinarelli et al., 2009). For example, Figure 2 shows an example of an ASR-decoded text output parsed by SEMAFOR. There are three frames (capability, expensiveness, and locale by use) in the utterance, which we consider as slot candidates. Since SEMAFOR was trained on FrameNet annotation, which has a more generic framesemantic context, not all the frames from the parsing results can be used as the actual slots in the domain-specific dialogue systems. For instance, in Figure 2, “expensiveness” and “loc</context>
</contexts>
<marker>Das, Schneider, Chen, Smith, 2010</marker>
<rawString>Dipanjan Das, Nathan Schneider, Desai Chen, and Noah A Smith. 2010. Probabilistic frame-semantic parsing. In Proceedings of The Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 948–956.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Desai Chen</author>
<author>Andr´e F T Martins</author>
<author>Nathan Schneider</author>
<author>Noah A Smith</author>
</authors>
<title>Frame-semantic parsing. Computational Linguistics.</title>
<date>2013</date>
<contexts>
<context position="11560" citStr="Das et al., 2013" startWordPosition="1757" endWordPosition="1760">ersations and available knowledge resources, we plan to extract organized knowledge that can be used for domain-specific SDSs. The ontology induction and structure learning are proposed to automate an ontology building process. 4.1 Ontology Induction Chen et al. (2013; 2014b) proposed to automatically induce semantic slots for SDSs by framesemantic parsing, where all ASR-decoded utterFrame: capability FT LU: can FE Filler: i 3 Figure 3: A simplified example of the automatically derived knowledge graph. ances are parsed using SEMAFOR1, a state-ofthe-art frame-semantic parser (Das et al., 2010; Das et al., 2013), and then all frames from parsed results are extracted as slot candidates (Dinarelli et al., 2009). For example, Figure 2 shows an example of an ASR-decoded text output parsed by SEMAFOR. There are three frames (capability, expensiveness, and locale by use) in the utterance, which we consider as slot candidates. Since SEMAFOR was trained on FrameNet annotation, which has a more generic framesemantic context, not all the frames from the parsing results can be used as the actual slots in the domain-specific dialogue systems. For instance, in Figure 2, “expensiveness” and “locale by use” frames </context>
</contexts>
<marker>Das, Chen, Martins, Schneider, Smith, 2013</marker>
<rawString>Dipanjan Das, Desai Chen, Andr´e F. T. Martins, Nathan Schneider, and Noah A. Smith. 2013. Frame-semantic parsing. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Dinarelli</author>
<author>Silvia Quarteroni</author>
<author>Sara Tonelli</author>
<author>Alessandro Moschitti</author>
<author>Giuseppe Riccardi</author>
</authors>
<title>Annotating spoken dialogs: from speech segments to dialog acts and frame semantics.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2nd Workshop on Semantic Representation of Spoken Language,</booktitle>
<pages>34--41</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="11659" citStr="Dinarelli et al., 2009" startWordPosition="1773" endWordPosition="1776">be used for domain-specific SDSs. The ontology induction and structure learning are proposed to automate an ontology building process. 4.1 Ontology Induction Chen et al. (2013; 2014b) proposed to automatically induce semantic slots for SDSs by framesemantic parsing, where all ASR-decoded utterFrame: capability FT LU: can FE Filler: i 3 Figure 3: A simplified example of the automatically derived knowledge graph. ances are parsed using SEMAFOR1, a state-ofthe-art frame-semantic parser (Das et al., 2010; Das et al., 2013), and then all frames from parsed results are extracted as slot candidates (Dinarelli et al., 2009). For example, Figure 2 shows an example of an ASR-decoded text output parsed by SEMAFOR. There are three frames (capability, expensiveness, and locale by use) in the utterance, which we consider as slot candidates. Since SEMAFOR was trained on FrameNet annotation, which has a more generic framesemantic context, not all the frames from the parsing results can be used as the actual slots in the domain-specific dialogue systems. For instance, in Figure 2, “expensiveness” and “locale by use” frames are essentially the key slots for the purpose of understanding in the restaurant query domain, wher</context>
</contexts>
<marker>Dinarelli, Quarteroni, Tonelli, Moschitti, Riccardi, 2009</marker>
<rawString>Marco Dinarelli, Silvia Quarteroni, Sara Tonelli, Alessandro Moschitti, and Giuseppe Riccardi. 2009. Annotating spoken dialogs: from speech segments to dialog acts and frame semantics. In Proceedings of the 2nd Workshop on Semantic Representation of Spoken Language, pages 34–41. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ali El-Kahky</author>
<author>Derek Liu</author>
<author>Ruhi Sarikaya</author>
<author>G¨okhan T¨ur</author>
<author>Dilek Hakkani-T¨ur</author>
<author>Larry Heck</author>
</authors>
<title>Extending domain coverage of language understanding systems via intent transfer between domains using knowledge graphs and search query click logs.</title>
<date>2014</date>
<booktitle>In Proceedings of ICASSP.</booktitle>
<marker>El-Kahky, Liu, Sarikaya, T¨ur, Hakkani-T¨ur, Heck, 2014</marker>
<rawString>Ali El-Kahky, Derek Liu, Ruhi Sarikaya, G¨okhan T¨ur, Dilek Hakkani-T¨ur, and Larry Heck. 2014. Extending domain coverage of language understanding systems via intent transfer between domains using knowledge graphs and search query click logs. In Proceedings of ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zeno Gantner</author>
<author>Steffen Rendle</author>
<author>Christoph Freudenthaler</author>
<author>Lars Schmidt-Thieme</author>
</authors>
<title>Mymedialite: A free recommender system library.</title>
<date>2011</date>
<booktitle>In Proceedings of the fifth ACM conference on Recommender systems,</booktitle>
<pages>305--308</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="20907" citStr="Gantner et al., 2011" startWordPosition="3363" endWordPosition="3366">ation to the per utterance AUC (area under the ROC curve), which directly correlates to what we want to achieve – well-ranked semantic concepts per utterance, which denotes the better estimation of semantic slots or human behaviors. To maximize the objective in (5), we employ a stochastic gradient descent (SGD) algorithm (Rendle et al., 2009). For each randomly sampled observed fact (u, x+), we sample an unobserved fact (u, x−), which results in |O |fact pairs (f−, f+). For each pair, we perform an SGD update using the gradient of the corresponding objective function for matrix factorization (Gantner et al., 2011). 6 Conclusion and Future Work This thesis proposal proposes an unsupervised SLU approach by automating the dialogue learning process on speech conversations. The preliminary results show that for the automatic speech recognition (ASR) transcripts (word error rate is about 37%), the acquired knowledge can be successfully applied to SLU modeling through MF techniques, guiding the direction of the methodology. The main planed tasks include: • Semantic concept identification • Semantic concept annotation • SLU modeling by matrix factorization In this thesis proposal, ongoing work and future plans</context>
</contexts>
<marker>Gantner, Rendle, Freudenthaler, Schmidt-Thieme, 2011</marker>
<rawString>Zeno Gantner, Steffen Rendle, Christoph Freudenthaler, and Lars Schmidt-Thieme. 2011. Mymedialite: A free recommender system library. In Proceedings of the fifth ACM conference on Recommender systems, pages 305–308. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dilek Hakkani-T¨ur</author>
<author>Larry Heck</author>
<author>Gokhan Tur</author>
</authors>
<title>Using a knowledge graph and query click logs for unsupervised learning of relation detection.</title>
<date>2013</date>
<booktitle>In Proceedings of ICASSP,</booktitle>
<pages>8327--8331</pages>
<marker>Hakkani-T¨ur, Heck, Tur, 2013</marker>
<rawString>Dilek Hakkani-T¨ur, Larry Heck, and Gokhan Tur. 2013. Using a knowledge graph and query click logs for unsupervised learning of relation detection. In Proceedings of ICASSP, pages 8327–8331.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Larry Heck</author>
<author>Dilek Hakkani-T¨ur</author>
</authors>
<title>Exploiting the semantic web for unsupervised spoken language understanding.</title>
<date>2012</date>
<booktitle>In Proceedings of SLT,</booktitle>
<pages>228--233</pages>
<marker>Heck, Hakkani-T¨ur, 2012</marker>
<rawString>Larry Heck and Dilek Hakkani-T¨ur. 2012. Exploiting the semantic web for unsupervised spoken language understanding. In Proceedings of SLT, pages 228– 233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
</authors>
<title>Statistical methods for speech recognition.</title>
<date>1997</date>
<publisher>MIT press.</publisher>
<contexts>
<context position="6130" citStr="Jelinek, 1997" startWordPosition="894" endWordPosition="895">ending domain coverage (El-Kahky et al., 2014; Chen and Rudnicky, 2014). However, most studies above do not explicitly learn latent factor representations from the data—while we hypothesize that the better robustness can be achieved by explicitly modeling the measurement errors (usually produced by automatic speech recognizers (ASR)) using latent variable models and taking additional local and global semantic constraints into account. Latent Variable Modeling in SLU Early studies on latent variable modeling in speech included the classic hidden Markov model for statistical speech recognition (Jelinek, 1997). Recently, Celikyilmaz et al. (2011) were the first to study the intent detection problem using query logs and a discrete Bayesian latent variable model. In the field of dialogue modeling, the partially observable Markov decision process (POMDP) (Young et al., 2013) model is a popular technique for dialogue management, reducing the cost of handcrafted dialogue managers while producing robustness against speech recognition errors. More recently, Tur et al. (2013) used a semi-supervised LDA model to show improvement on the slot filling task. Also, Zhai and Williams (2014) proposed an unsupervis</context>
</contexts>
<marker>Jelinek, 1997</marker>
<rawString>Frederick Jelinek. 1997. Statistical methods for speech recognition. MIT press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Proceedings of Advances in Neural Information Processing Systems,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="7082" citStr="Mikolov et al., 2013" startWordPosition="1042" endWordPosition="1045">the cost of handcrafted dialogue managers while producing robustness against speech recognition errors. More recently, Tur et al. (2013) used a semi-supervised LDA model to show improvement on the slot filling task. Also, Zhai and Williams (2014) proposed an unsupervised model for connecting words with latent states in HMMs using topic models, obtaining interesting qualitative and quantitative results. However, for unsupervised SLU, it is not obvious how to incorporate additional information in the HMMs. With increasing works about learn2 ing the feature matrices for language representations (Mikolov et al., 2013), matrix factorization (MF) has become very popular for both implicit and explicit feedback (Rendle et al., 2009; Chen et al., 2015a). This thesis proposal is the first to propose a framework about unsupervised SLU modeling, which is able to simultaneously consider various local and global knowledge automatically learned from unlabelled data using a matrix factorization (MF) technique. 3 The Proposed Work The proposed framework is shown in Figure 1(a), where there are two main parts, one is knowledge acquisition and another is SLU modeling by MF. The first part is to acquire the domain knowled</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Proceedings of Advances in Neural Information Processing Systems, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steffen Rendle</author>
<author>Christoph Freudenthaler</author>
<author>Zeno Gantner</author>
<author>Lars Schmidt-Thieme</author>
</authors>
<title>BPR: Bayesian personalized ranking from implicit feedback.</title>
<date>2009</date>
<booktitle>In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence,</booktitle>
<pages>452--461</pages>
<publisher>AUAI Press.</publisher>
<contexts>
<context position="7194" citStr="Rendle et al., 2009" startWordPosition="1059" endWordPosition="1062">tly, Tur et al. (2013) used a semi-supervised LDA model to show improvement on the slot filling task. Also, Zhai and Williams (2014) proposed an unsupervised model for connecting words with latent states in HMMs using topic models, obtaining interesting qualitative and quantitative results. However, for unsupervised SLU, it is not obvious how to incorporate additional information in the HMMs. With increasing works about learn2 ing the feature matrices for language representations (Mikolov et al., 2013), matrix factorization (MF) has become very popular for both implicit and explicit feedback (Rendle et al., 2009; Chen et al., 2015a). This thesis proposal is the first to propose a framework about unsupervised SLU modeling, which is able to simultaneously consider various local and global knowledge automatically learned from unlabelled data using a matrix factorization (MF) technique. 3 The Proposed Work The proposed framework is shown in Figure 1(a), where there are two main parts, one is knowledge acquisition and another is SLU modeling by MF. The first part is to acquire the domain knowledge that is useful for building the domain-specific dialogue systems, which addresses the question about how to i</context>
<context position="19550" citStr="Rendle et al., 2009" startWordPosition="3110" endWordPosition="3113">t al., 2001). p(θ |Mu) (4) p(Mu |θ)p(θ) ln p(Mu |θ) − λθ, where Mu is the vector corresponding to the utterance u from Mu,x in (1), because we assume that each utterance is independent of others. To avoid treating unobserved facts as designed negative facts, we consider our positive-only data θ~ = arg max θ ri uEU arg max θ ri uEU � = arg max θ uEU 5 as implicit feedback. Bayesian Personalized Ranking (BPR) is an optimization criterion that learns from implicit feedback for MF, which uses a variant of the ranking: giving observed true facts higher scores than unobserved (true or false) facts (Rendle et al., 2009). Riedel et al. (2013) also showed that BPR learns the implicit relations and improves a relation extraction task. To estimate the parameters in (4), we create a dataset of ranked pairs from M in (3): for each utterance u and each observed fact f+ = (u, x+), where Mu,x &gt; 6, we choose each semantic concept x− such that f− = (u, x−), where Mu,x &lt; 6, which refers to the semantic concept we have not observed in utterance u. That is, we construct the observed data O from M. Then for each pair of facts f+ and f−, we want to model p(f+) &gt; p(f−) and hence Bf+ &gt; Bf− according to (1). BPR maximizes the </context>
</contexts>
<marker>Rendle, Freudenthaler, Gantner, Schmidt-Thieme, 2009</marker>
<rawString>Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. 2009. BPR: Bayesian personalized ranking from implicit feedback. In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, pages 452–461. AUAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Limin Yao</author>
<author>Andrew McCallum</author>
<author>Benjamin M Marlin</author>
</authors>
<title>Relation extraction with matrix factorization and universal schemas.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>74--84</pages>
<contexts>
<context position="19572" citStr="Riedel et al. (2013)" startWordPosition="3114" endWordPosition="3117"> (4) p(Mu |θ)p(θ) ln p(Mu |θ) − λθ, where Mu is the vector corresponding to the utterance u from Mu,x in (1), because we assume that each utterance is independent of others. To avoid treating unobserved facts as designed negative facts, we consider our positive-only data θ~ = arg max θ ri uEU arg max θ ri uEU � = arg max θ uEU 5 as implicit feedback. Bayesian Personalized Ranking (BPR) is an optimization criterion that learns from implicit feedback for MF, which uses a variant of the ranking: giving observed true facts higher scores than unobserved (true or false) facts (Rendle et al., 2009). Riedel et al. (2013) also showed that BPR learns the implicit relations and improves a relation extraction task. To estimate the parameters in (4), we create a dataset of ranked pairs from M in (3): for each utterance u and each observed fact f+ = (u, x+), where Mu,x &gt; 6, we choose each semantic concept x− such that f− = (u, x−), where Mu,x &lt; 6, which refers to the semantic concept we have not observed in utterance u. That is, we construct the observed data O from M. Then for each pair of facts f+ and f−, we want to model p(f+) &gt; p(f−) and hence Bf+ &gt; Bf− according to (1). BPR maximizes the summation of each rank</context>
</contexts>
<marker>Riedel, Yao, McCallum, Marlin, 2013</marker>
<rawString>Sebastian Riedel, Limin Yao, Andrew McCallum, and Benjamin M Marlin. 2013. Relation extraction with matrix factorization and universal schemas. In Proceedings of NAACL-HLT, pages 74–84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gokhan Tur</author>
<author>Dilek Z Hakkani-T¨ur</author>
<author>Dustin Hillard</author>
<author>Asli Celikyilmaz</author>
</authors>
<title>Towards unsupervised spoken language understanding: Exploiting query click logs for slot filling.</title>
<date>2011</date>
<booktitle>In Proceedings of INTERSPEECH,</booktitle>
<pages>1293--1296</pages>
<marker>Tur, Hakkani-T¨ur, Hillard, Celikyilmaz, 2011</marker>
<rawString>Gokhan Tur, Dilek Z Hakkani-T¨ur, Dustin Hillard, and Asli Celikyilmaz. 2011. Towards unsupervised spoken language understanding: Exploiting query click logs for slot filling. In Proceedings of INTERSPEECH, pages 1293–1296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gokhan Tur</author>
<author>Minwoo Jeong</author>
<author>Ye-Yi Wang</author>
<author>Dilek Hakkani-T¨ur</author>
<author>Larry P Heck</author>
</authors>
<title>Exploiting the semantic web for unsupervised natural language semantic parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of INTERSPEECH.</booktitle>
<marker>Tur, Jeong, Wang, Hakkani-T¨ur, Heck, 2012</marker>
<rawString>Gokhan Tur, Minwoo Jeong, Ye-Yi Wang, Dilek Hakkani-T¨ur, and Larry P Heck. 2012. Exploiting the semantic web for unsupervised natural language semantic parsing. In Proceedings of INTERSPEECH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gokhan Tur</author>
<author>Asli Celikyilmaz</author>
<author>Dilek HakkaniTur</author>
</authors>
<title>Latent semantic modeling for slot filling in conversational understanding.</title>
<date>2013</date>
<booktitle>In Proceedings of 2013 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),</booktitle>
<pages>8307--8311</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="6597" citStr="Tur et al. (2013)" startWordPosition="966" endWordPosition="969">in SLU Early studies on latent variable modeling in speech included the classic hidden Markov model for statistical speech recognition (Jelinek, 1997). Recently, Celikyilmaz et al. (2011) were the first to study the intent detection problem using query logs and a discrete Bayesian latent variable model. In the field of dialogue modeling, the partially observable Markov decision process (POMDP) (Young et al., 2013) model is a popular technique for dialogue management, reducing the cost of handcrafted dialogue managers while producing robustness against speech recognition errors. More recently, Tur et al. (2013) used a semi-supervised LDA model to show improvement on the slot filling task. Also, Zhai and Williams (2014) proposed an unsupervised model for connecting words with latent states in HMMs using topic models, obtaining interesting qualitative and quantitative results. However, for unsupervised SLU, it is not obvious how to incorporate additional information in the HMMs. With increasing works about learn2 ing the feature matrices for language representations (Mikolov et al., 2013), matrix factorization (MF) has become very popular for both implicit and explicit feedback (Rendle et al., 2009; C</context>
</contexts>
<marker>Tur, Celikyilmaz, HakkaniTur, 2013</marker>
<rawString>Gokhan Tur, Asli Celikyilmaz, and Dilek HakkaniTur. 2013. Latent semantic modeling for slot filling in conversational understanding. In Proceedings of 2013 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 8307–8311. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lu Wang</author>
<author>Dilek Hakkani-T¨ur</author>
<author>Larry Heck</author>
</authors>
<title>Leveraging semantic web search and browse sessions for multi-turn spoken dialog systems.</title>
<date>2014</date>
<booktitle>In Proceedings of 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),</booktitle>
<pages>4082--4086</pages>
<publisher>IEEE.</publisher>
<marker>Wang, Hakkani-T¨ur, Heck, 2014</marker>
<rawString>Lu Wang, Dilek Hakkani-T¨ur, and Larry Heck. 2014. Leveraging semantic web search and browse sessions for multi-turn spoken dialog systems. In Proceedings of 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 4082–4086. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve Young</author>
<author>Milica Gasic</author>
<author>Blaise Thomson</author>
<author>Jason D Williams</author>
</authors>
<title>POMDP-based statistical spoken dialog systems: A review.</title>
<date>2013</date>
<booktitle>Proceedings of the IEEE,</booktitle>
<volume>101</volume>
<issue>5</issue>
<contexts>
<context position="6397" citStr="Young et al., 2013" startWordPosition="935" endWordPosition="938">asurement errors (usually produced by automatic speech recognizers (ASR)) using latent variable models and taking additional local and global semantic constraints into account. Latent Variable Modeling in SLU Early studies on latent variable modeling in speech included the classic hidden Markov model for statistical speech recognition (Jelinek, 1997). Recently, Celikyilmaz et al. (2011) were the first to study the intent detection problem using query logs and a discrete Bayesian latent variable model. In the field of dialogue modeling, the partially observable Markov decision process (POMDP) (Young et al., 2013) model is a popular technique for dialogue management, reducing the cost of handcrafted dialogue managers while producing robustness against speech recognition errors. More recently, Tur et al. (2013) used a semi-supervised LDA model to show improvement on the slot filling task. Also, Zhai and Williams (2014) proposed an unsupervised model for connecting words with latent states in HMMs using topic models, obtaining interesting qualitative and quantitative results. However, for unsupervised SLU, it is not obvious how to incorporate additional information in the HMMs. With increasing works abou</context>
</contexts>
<marker>Young, Gasic, Thomson, Williams, 2013</marker>
<rawString>Steve Young, Milica Gasic, Blaise Thomson, and Jason D Williams. 2013. POMDP-based statistical spoken dialog systems: A review. Proceedings of the IEEE, 101(5):1160–1179.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ke Zhai</author>
<author>Jason D Williams</author>
</authors>
<title>Discovering latent structure in task-oriented dialogues.</title>
<date>2014</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="6707" citStr="Zhai and Williams (2014)" startWordPosition="985" endWordPosition="988"> statistical speech recognition (Jelinek, 1997). Recently, Celikyilmaz et al. (2011) were the first to study the intent detection problem using query logs and a discrete Bayesian latent variable model. In the field of dialogue modeling, the partially observable Markov decision process (POMDP) (Young et al., 2013) model is a popular technique for dialogue management, reducing the cost of handcrafted dialogue managers while producing robustness against speech recognition errors. More recently, Tur et al. (2013) used a semi-supervised LDA model to show improvement on the slot filling task. Also, Zhai and Williams (2014) proposed an unsupervised model for connecting words with latent states in HMMs using topic models, obtaining interesting qualitative and quantitative results. However, for unsupervised SLU, it is not obvious how to incorporate additional information in the HMMs. With increasing works about learn2 ing the feature matrices for language representations (Mikolov et al., 2013), matrix factorization (MF) has become very popular for both implicit and explicit feedback (Rendle et al., 2009; Chen et al., 2015a). This thesis proposal is the first to propose a framework about unsupervised SLU modeling, </context>
</contexts>
<marker>Zhai, Williams, 2014</marker>
<rawString>Ke Zhai and Jason D Williams. 2014. Discovering latent structure in task-oriented dialogues. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>