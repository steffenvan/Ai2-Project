<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.096639">
<title confidence="0.918489">
Learning about Voice Search for Spoken Dialogue Systems
</title>
<author confidence="0.936064">
Rebecca J. Passonneau1, Susan L. Epstein2,3, Tiziana Ligorio2,
Joshua B. Gordon4, Pravin Bhutada4
</author>
<affiliation confidence="0.99973725">
1Center for Computational Learning Systems, Columbia University
2Department of Computer Science, Hunter College of The City University of New York
3Department of Computer Science, The Graduate Center of The City University of New York
4Department of Computer Science, Columbia University
</affiliation>
<email confidence="0.947782">
becky@cs.columbia.edu, susan.epstein@hunter.cuny.edu, tligorio@gc.cuny.edu,
joshua@cs.columbia.edu, pravin.bhutada@gmail.com
</email>
<sectionHeader confidence="0.995506" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999856684210526">
In a Wizard-of-Oz experiment with multiple
wizard subjects, each wizard viewed automated
speech recognition (ASR) results for utterances
whose interpretation is critical to task success:
requests for books by title from a library data-
base. To avoid non-understandings, the wizard
directly queried the application database with
the ASR hypothesis (voice search). To learn
how to avoid misunderstandings, we investi-
gated how wizards dealt with uncertainty in
voice search results. Wizards were quite suc-
cessful at selecting the correct title from query
results that included a match. The most suc-
cessful wizard could also tell when the query
results did not contain the requested title. Our
learned models of the best wizard’s behavior
combine features available to wizards with
some that are not, such as recognition confi-
dence and acoustic model scores.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999976465116279">
Wizard-of-Oz (WOz) studies have long been used
for spoken dialogue system design. In a relatively
new variant, a subject (the wizard) is presented
with real or simulated automated speech recogni-
tion (ASR) to observe how people deal with incor-
rect speech recognition output (Rieser, Kruijff-
Korbayová, &amp; Lemon, 2005; Skantze, 2003;
Stuttle, Williams, &amp; Young, 2004; Williams &amp;
Young, 2003, 2004; Zollo, 1999). In these experi-
ments, when a wizard could not interpret the ASR
output (non-understanding), she rarely asked users
to repeat themselves. Instead, the wizard found
other ways to continue the task.
This paper describes an experiment that pre-
sented wizards with ASR results for utterances
whose interpretation is critical to task success: re-
quests for books from a library database, identified
by title. To avoid non-understandings, wizards
used voice search (Wang et al., 2008): they direct-
ly queried the application database with ASR out-
put. To investigate how to avoid errors in
understanding (misunderstandings), we examined
how wizards dealt with uncertainty in voice search
results. When the voice search results included the
requested title, all seven of our wizards were likely
to identify it. One wizard, however, recognized far
better than the others when the voice search results
did not contain the requested title. The experiment
employed a novel design that made it possible to
include system features in models of wizard beha-
vior. The principal result is that our learned models
of the best wizard’s behavior combine features that
are available to wizards with some that are not,
such as recognition confidence and acoustic model
scores.
The next section of the paper motivates our ex-
periment. Subsequent sections describe related
work, the dialogue system and embedded wizard
infrastructure, experimental design, learning me-
thods, and results. We then discuss how to general-
ize from the results of our study for spoken
dialogue system design. We conclude with a sum-
mary of results and their implications.
</bodyText>
<sectionHeader confidence="0.993223" genericHeader="introduction">
2 Motivation
</sectionHeader>
<bodyText confidence="0.992078333333333">
Rather than investigate full dialogues, we ad-
dressed a single type of turn exchange or adjacency
pair (Sacks et al., 1974): a request for a book by its
</bodyText>
<page confidence="0.951105">
840
</page>
<note confidence="0.751357">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 840–848,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.998582230769231">
title. This allowed us to collect data exclusively
about an utterance type critical for task success in
our application domain. We hypothesized that low-
level features from speech recognition, such as
acoustic model fit, could independently affect
voice search confidence. We therefore applied a
novel approach, embedded WOz, in which a wizard
and the system together interpret noisy ASR.
To address how to avoid misunderstandings, we
investigated how wizards dealt with uncertainty in
voice search returns. To illustrate what we mean
by uncertainty, if we query our book title database
with the ASR hypothesis:
</bodyText>
<sectionHeader confidence="0.916299" genericHeader="method">
ROLL DWELL
</sectionHeader>
<bodyText confidence="0.683332">
our voice search procedure returns, in this order:
</bodyText>
<sectionHeader confidence="0.967977333333333" genericHeader="method">
CROMWELL
ROBERT LOWELL
ROAD TO WEALTH
</sectionHeader>
<bodyText confidence="0.99996529113924">
The correct title appears last because of the score it
is assigned by the string similarity metric we use.
Three factors motivated our use of voice search
to interpret book title requests: noisy ASR, un-
usually long query targets, and high overlap of the
vocabulary across different query types (e.g., au-
thor and title) as well as with non-query words in
caller utterances (e.g., “Could you look up . . .”).
First, accurate speech recognition for a real-
world telephone application can be difficult to
achieve, given unpredictable background noise and
transmission quality. For example, the 68% word
error rate (WER) for the fielded version of Let’s
Go Public! (Raux et al., 2005) far exceeded its
17% WER under controlled conditions. Our appli-
cation handles library requests by telephone, and
would benefit from robustness to noisy ASR.
Second, the book title field in our database dif-
fers from the typical case for spoken dialogue sys-
tems that access a relational database. Such
systems include travel booking (Levin et al., 2000),
bus route information (Raux et al., 2006), restau-
rant guides (Johnston et al., 2002; Komatani et al.,
2005), weather (Zue et al., 2000) and directory
services (Georgila et al., 2003). In general for these
systems, a few words are sufficient to retrieve the
desired attribute value, such as a neighborhood, a
street, or a surname. Mean utterance length in a
sample of 40,000 Let’s Go Public! utterances, for
example, is 2.4 words. The average book title
length in our database is 5.4 words.
Finally, our dialogue system, CheckItOut, al-
lows users to choose whether to request books by
title, author, or catalogue number. The database
represents 5028 active patrons (with real borrow-
ing histories and preferences but fictitious personal
information), 71,166 book titles and 28,031 au-
thors. Though much smaller than a database for a
directory service application (Georgila et al.,
2003), this is much larger than that of many current
research systems. For example, Let’s Go Public!
accesses a database with 70 bus routes and 1300
place names. Titles and author names contribute
50,394 words to the vocabulary, of which 57.4%
occur only in titles, 32.1% only in author names,
and 10.5% in both. Many book titles (e.g., You See
I Haven’t Forgotten, You Never Know) have a high
potential for confusability with non-title phrases in
users’ book requests. Given the longer database
field and the confusability of the book title lan-
guage, integrating voice search is likely to have a
relatively larger impact in CheckItOut.
We seek to minimize non-understandings and
misunderstandings for several reasons. First, user
corrections in both situations have been shown to
be more poorly recognized than non-correction ut-
terances (Litman et al., 2006). Non-understandings
typically result in re-prompting the user for the
same information. This often leads to hyper-
articulation and concomitant degradation in recog-
nition performance. Second, users seem to prefer
systems that minimize non-understandings and mi-
sunderstandings, even at the expense of dialogue
efficiency. Users of the TOOT train information
spoken dialogue system preferred system-initiative
to mixed- or user-initiative, and preferred explicit
confirmation to implicit or no confirmation
(Litman &amp; Pan, 1999). This was true despite the
fact that a mixed-initiative, implicit confirmation
strategy led to fewer turns for the same task. Most
of the more recent work on spoken dialogue sys-
tems focuses on mixed-initiative systems in labora-
tory settings. Still, recent work suggests that while
mixed- or user-initiative is rated highly in usability
studies, under real usage it “fails to provide [a] ro-
bust enough interface” (Turunen et al., 2006). In-
corporating accurate voice search into spoken
dialogue systems could lead to fewer non-
understandings and fewer misunderstandings.
</bodyText>
<sectionHeader confidence="0.99993" genericHeader="method">
3 Related Work
</sectionHeader>
<bodyText confidence="0.966207666666667">
Our approach to noisy ASR contrasts with many
other information-seeking and transaction-based
dialogue systems. Those systems typically perform
</bodyText>
<page confidence="0.995717">
841
</page>
<bodyText confidence="0.999964625">
natural language understanding on ASR output be-
fore database query with techniques that try to im-
prove or expand ASR output. None that we know
of use voice search. For one directory service ap-
plication, users spell the first three letters of sur-
names, and then ASR results are expanded using
frequently confused phones (Georgila et al., 2003).
A two-pass recognition architecture added to Let’s
Go Public! improved concept recognition in post-
confirmation user utterances (Stoyanchev &amp; Stent,
2009). In (Komatani et al., 2005), a shallow se-
mantic interpretation phase was followed by deci-
sion trees to classify utterances as relevant either to
query type or to specific query slots, to narrow the
set of possible interpretations. CheckItOut is most
similar in spirit to the latter approach, but relies on
the database earlier, and only for semantic interpre-
tation, not to also guide the dialogue strategy.
Our approach to noisy ASR is inspired by pre-
vious WOz studies with real (Skantze, 2003; Zollo,
1999) or simulated ASR (Kruijff-Korbayová et al.,
2005; Rieser et al., 2005; Williams &amp; Young,
2004). Simulation makes it possible to collect di-
alogues without building a speech recognizer, and
to control for WER. In the studies that involved
task-oriented dialogues, wizards typically focused
more on the task and less on resolving ASR errors
(Williams &amp; Young, 2004; Skantze, 2003; Zollo,
1999). In studies more like the information-seeking
dialogues addressed here, an entirely different pat-
tern is observed (Kruijff-Korbayová et al., 2005;
Rieser et al., 2005).
Zollo collected seven dialogues with different
human-wizard pairs to develop an evacuation plan.
The overall WER was 30%. Of the 227 cases of
incorrect ASR, wizard utterances indicated a fail-
ure to understand for only 35% of them. Wizards
ignored words not salient in the domain and hy-
pothesized words based on phonetic similarity. In
(Skantze, 2003), both users and wizards knew
there was no dialogue system; 44 direction-finding
dialogues were collected with 16 subjects. Despite
a WER of 43%, the wizard operators signaled mis-
understanding only 5% of the time, in part because
they often ignored ASR errors and continued the
dialogue. For the 20% of non-understandings, op-
erators continued a route description, asked a task-
related question, or requested a clarification.
Williams and Young collected 144 dialogues
simulating tourist requests for directions and other
negotiations. WER was constrained to be high,
medium, or low. Under medium WER, a task-
related question in response to a non-understanding
or misunderstanding led to full understanding more
often than explicit repairs. Under high WER, how-
ever, the reverse was true. Misunderstandings sig-
nificantly increased when wizards followed non-
understandings or misunderstandings with a task-
related question instead of a repair.
In (Rieser et al., 2005), wizards simulated a
multimodal MP3 player application with access to
a database of 150K music albums. Responses
could be presented verbally or graphically. In the
noisy transcription condition, wizards made clarifi-
cation requests about twice as often as that found
in similar human-human dialogue.
In a system like CheckItOut, user utterances that
request database information must be understood.
We seek an approach that would reduce the rate of
misunderstandings observed for high WER in
(Williams &amp; Young, 2004) and the rate of clarifi-
cation requests observed in (Rieser et al., 2005).
</bodyText>
<sectionHeader confidence="0.962866" genericHeader="method">
4 CheckItOut and Embedded Wizards
</sectionHeader>
<bodyText confidence="0.99948816">
CheckItOut is modeled on library transactions at
the Andrew Heiskell Braille and Talking Book Li-
brary, a branch of the New York Public Library
and part of the National Library of Congress. Bor-
rowing requests are handled by telephone. Books,
mainly in a proprietary audio format, travel by
mail. In a dialogue with CheckItOut, a user identi-
fies herself, requests books, and is told which are
available for immediate shipment or will go on re-
serve. The user can request a book by catalogue
number, title, or author.
CheckItOut builds on the Olympus/RavenClaw
framework (Bohus &amp; Rudnicky, 2009) that has
been the basis for about a dozen dialogue systems
in different domains, including Let’s Go Public!
(Raux et al., 2005). Speech recognition relies on
PocketSphinx. Phoenix, a robust context-free
grammar (CFG) semantic parser, handles natural
language understanding (Ward &amp; Issar, 1994). The
Apollo interaction manager (Raux &amp; Eskenazi,
2007) detects utterance boundaries using informa-
tion from speech recognition, semantic parsing,
and Helios, an utterance-level confidence annotator
(Bohus &amp; Rudnicky, 2002). The dialogue manager
is implemented in RavenClaw.
</bodyText>
<page confidence="0.98315">
842
</page>
<bodyText confidence="0.99992784">
To design CheckItOut’s dialogue manager, we
recorded 175 calls (4.5 hours) from patrons to li-
brarians. We identified 82 book request calls, tran-
scribed them, aligned the utterances with the
speech signal, and annotated the transcripts for di-
alogue acts. Because active patrons receive
monthly newsletters listing new titles in the desired
formats, patrons request specific items with ad-
vance knowledge of the author, title, or catalogue
number. Most book title requests accurately repro-
duce the exact title, the title less an initial deter-
miner (“the,” “a”), or a subtitle.
We exploited the Galaxy message passing archi-
tecture of Olympus/RavenClaw to insert a wizard
server into CheckItOut. The hub passes messages
between the system and a wizard’s graphical user
interface (GUI), allowing us to collect runtime in-
formation that can be included in models of wi-
zards’ actions.
For speech recognition, CheckItOut relies on
PocketSphinx 0.5, a Hidden Markov Model-based
recognizer. Speech recognition for this experiment,
relied on the freely available Wall Street Journal
“read speech” acoustic models. We did not adapt
the models to our population or to spontaneous
speech, thus insuring that wizards would receive
relatively noisy recognition output.
We built trigram language models from the
book titles using the CMU Statistical Language
Modeling Toolkit. Pilot tests with one male and
one female native speaker indicated that a lan-
guage model based on 7500 titles would yield
WER in the desired range. (Average WER for the
book title requests in our experiment was 71%.) To
model one aspect of the real world useful for an ac-
tual system, titles with below average circulation
were eliminated. An offline pilot study had demon-
strated that one-word titles were easy for wizards,
so we eliminated those as well. A random sample
of 7,500 was chosen from the remaining 19,708
titles to build the trigram language model.
We used Ratcliff/Obersherhelp (R/O) to meas-
ure the similarity of an ASR string to book titles in
the database (Ratcliff &amp; Metzener, 1988). R/O cal-
culates the ratio r of the number of matching cha-
racters to the total length of both strings, but
requires O(r2) time on average and O(r3) time in
the worst case. We therefore computed an upper
bound on the similarity of a title/ASR pair prior to
full R/O to speed processing.
</bodyText>
<sectionHeader confidence="0.996839" genericHeader="method">
5 Experimental Design
</sectionHeader>
<bodyText confidence="0.999935466666667">
In this experiment, a user and a wizard sat in sepa-
rate rooms where they could not overhear one
another. Each had a headset with microphone and a
GUI. Audio input on the wizard’s headset was dis-
abled. When the user requested a title, the ASR
hypothesis for the title appeared on the wizard’s
GUI. The wizard then selected the ASR hypothesis
to execute a voice search against the database.
Given the ASR and the query return, the wi-
zard’s task was to guess which candidate in the
query return, if any, matched the ASR hypothesis.
Voice search accessed the full backend of 71,166
titles. The custom query designed for the experi-
ment produced four types of return, in real time,
based on R/O scores:
</bodyText>
<listItem confidence="0.996796666666667">
· Singleton: a single best candidate (R/O ≥ 0.85)
· AmbiguousList: two to five moderately good
candidates (0.85 &gt; R/O ≥ 0.55)
· NoisyList: six to ten poor but non-random can-
didates (0.55 &gt; R/O ≥ 0.40)
· Empty: No candidate titles (max R/O &lt; 0.40)
</listItem>
<bodyText confidence="0.999767">
In pilot tests, 5%-10% of returns were empty ver-
sus none in the experiment. The distribution of
other returns was: 46.7% Singleton, 50.5% Ambi-
guousList, and 2.8% NoisyList.
Seven undergraduate computer science majors
at Hunter College participated. Two were non-
native speakers of English (one Spanish, one Ro-
manian). Each of the possible 21 pairs of students
met for five trials. During each trial, one student
served as wizard and the other as user for a session
of 20 title cycles. They immediately reversed roles
for a second session, as discussed further below.
The experiment yielded 4172 title cycles rather
than the full 4200, because users were permitted to
end sessions early. All titles were selected from the
7500 used to construct the language model.
Each user received a printed list of 20 titles and
a brief synopsis of each book. The acoustic quality
of titles read individually from a list is unlikely to
approximate that of a patron asking for a specific
title. Therefore, immediately before each session,
the user was asked to read a synopsis of each book,
and to reorder the titles to reflect some logical
grouping, such as genre or topic. Users requested
titles in this new order that they had created.
Participants were encouraged to maximize a ses-
sion score, with a reward for the experiment win-
ner. Scoring was designed to foster cooperative
</bodyText>
<page confidence="0.993095">
843
</page>
<bodyText confidence="0.9999656">
strategies. The wizard scored +1 for a correctly
identified title, +0.5 for a thoughtful question, and
-1 for an incorrect title. The user scored +0.5 for a
successfully recognized title. User and wizard
traded roles for the second session, to discourage
participants from sabotaging the others’ scores.
The wizard’s GUI presented a real-time live
feed of ASR hypotheses, weighted by grayscale to
reflect acoustic confidence. Words in each candi-
date title that matched a word in the ASR appeared
darker: dark black for Singleton or AmbiguousList,
and medium black for NoisyList. All other words
were in grayscale in proportion to the degree of
character overlap. The wizard queried the database
with a recognition hypothesis for one utterance at a
time, but could concatenate successive utterances,
possibly with some limited editing.
After a query, the wizard’s GUI displayed can-
didate matches in descending order of R/O score.
The wizard had four options: make a firm choice of
a candidate, make a tentative choice, ask a ques-
tion, or give up to end the title cycle. Questions
were recorded. The wizard’s GUI showed the suc-
cess or failure of each title cycle before the next
one began. The user’s GUI posted the 20 titles to
be read during the session. On the GUI, the user
rated the wizard’s title choices as correct or incor-
rect. Titles were highlighted green if the user
judged a wizard’s offered title correct, red if incor-
rect, yellow if in progress, and not highlighted if
still pending. The user also rated the wizard’s
questions. Average elapsed time for each 20-title
session was 15.5 minutes.
A questionnaire similar to the type used in
PARADISE evaluations (Walker et al., 1998) was
administered to wizards and users for each pair of
sessions. On a 5-point Likert scale, the average re-
sponse to the question “I found the system easy to
use this time” was 4 (sd=0; 4=Agree), indicating
that participants were comfortable with the task.
All other questions received an average score of
Neutral (3) or Disagree (2). For example, partici-
pants were neutral (3) regarding confidence in
guessing the correct title, and disagreed (2) that
they became more confident as time went on.
</bodyText>
<sectionHeader confidence="0.909454" genericHeader="method">
6 Learning Method and Goals
</sectionHeader>
<bodyText confidence="0.999986962962963">
To model wizard actions, we assembled 60 fea-
tures that would be available at run time. Part of
our task was to detect their relative independence,
meaningfulness, and predictive ability. Features
described the wizard’s GUI, the current title ses-
sion, similarity between ASR and candidates, ASR
relevance to the database, and recognition and con-
fidence measures. Because the number of voice
search returns varied from one title to the next, fea-
tures pertaining to candidates were averaged.
We used three machine-learning techniques to
predict wizards’ actions: decision trees, linear re-
gression, and logistic regression. All models were
produced with the Weka data mining package, us-
ing 10-fold cross-validation (Witten &amp; Frank,
2005). A decision tree is a predictive model that
maps feature values to a target value. One applies a
decision tree by tracing a path from the root (the
top node) to a leaf, which provides the target value.
Here the leaves are the wizard actions: firm choice,
tentative choice, question, or give up. The algo-
rithm used is a version of C4.5 (Quinlan, 1993),
where gain ratio is the splitting criterion.
To confirm the learnability and quality of the
decision tree models, we also trained logistic re-
gression and linear regression models on the same
data, normalized in [0, 1]. The logistic regression
model predicts the probability of wizards’ actions
by fitting the data to a logistic curve. It generalizes
the linear model to the prediction of categorical da-
ta; here, categories correspond to wizards’ actions.
The linear regression models represent wizards’
actions numerically, in decreasing value: firm
choice, tentative choice, question, give up.
Although analysis of individual wizards has not
been systematic in other work, we consider the
variation in human performance significant. Be-
cause we seek excellent, not average, teachers for
CheckItOut, our focus is on understanding good
wizardry. Therefore, we learned two kinds of mod-
els with each of the three methods: the overall
model using data from all of our wizards, and indi-
vidual wizard models.
Preliminary cross-correlation confirmed that
many of the 60 features were heavily interdepen-
dent. Through an initial manual curation phase, we
isolated groups of features with R2 &gt; 0.5. When
these groups referenced semantically similar fea-
tures, we selected a single representative from the
group and retained only that one. For example, the
features that described similarity between hypo-
theses and candidates were highly correlated, so
we chose the most comprehensive one: the number
of exact word matches. We also grouped together
</bodyText>
<page confidence="0.999403">
844
</page>
<tableCaption confidence="0.9713395">
Table 1. Raw session score, accuracy, proportion of offered titles that were listed first in the query return, and
frequency of correct non-offers for seven participants.
</tableCaption>
<table confidence="0.999325125">
Participant Cycles Session Score Accuracy Offered Return 1 Correct Non-Offers
W4 600 0.7585 0.8550 0.70 0.64
W5 600 0.7584 0.8133 0.76 0.43
W7 599 0.6971 0.7346 0.76 0.14
W1 593 0.6936 0.7319 0.79 0.16
W2 599 0.6703 0.7212 0.74 0.10
W3 581 0.6648 0.6954 0.81 0.20
W6 600 0.6103 0.6950 0.86 0.03
</table>
<bodyText confidence="0.99996835">
and represented by a single feature: three features
that described the gaps between exact word
matches, three that described the data presented to
the wizard, nine that described various system con-
fidence scores, and three that described the user’s
speaking rate. This left 28 features.
Next we ran CfsSubsetEval, a supervised
attribute selection algorithm for each model
(Witten &amp; Frank, 2005). This greedy, hill-climbing
algorithm with backtracking evaluates a subset of
attributes by the predictive ability of each feature
and the degree of redundancy among them. This
process further reduced the 28 features to 8-12 fea-
tures per model. Finally, to reduce overfitting for
decision trees, we used pruning and subtree rising.
For linear regression we used the M5 method, re-
peatedly removing the attribute with the smallest
standardized coefficient until there was no further
improvement in the error estimate given by the
Akaike information criterion.
</bodyText>
<sectionHeader confidence="0.9999" genericHeader="method">
7 Results
</sectionHeader>
<bodyText confidence="0.999947068181818">
Table 1 shows the number of title cycles per wi-
zard, the raw session score according to the formu-
la given to the wizards, and accuracy. Accuracy is
the proportion of title cycles where the wizard
found the correct title, or correctly guessed that the
correct title was not present (asked a question or
gave up). Note that score and accuracy are highly
correlated (R=0.91, p=0.0041), indicating that the
instructions to participants elicited behavior con-
sistent with what we wanted to measure.
Wizards clearly differed in performance, large-
ly due to their response when the candidate list did
not include the correct title. Analysis of variance
with wizard as predictor and accuracy as the de-
pendent variable is highly significant (p=0.0006);
significance is somewhat greater (p=0.0001) where
session score is the dependent variable. Table 2
shows the distribution of correct actions: to offer a
candidate at a given position in the query return
(Returns 1 through 9), or to ask a question or give
up. As reflected in Table 2, a baseline accuracy of
about 65% could be achieved by offering the first
return. The fifth column of Table 1 shows how of-
ten wizards did that (Offered Return 1), and clearly
illustrates that those who did so most often (W3
and W6) had accuracy results closest to the base-
line. The wizard who did so least often (W4) had
the highest accuracy, primarily because she more
often correctly offered no title, as shown in the last
column of Table 1. We conclude that a spoken di-
alogue system would do well to emulate W4.
Overall, our results in modeling wizards’ actions
were uniform across the three learning methods,
gauged by accuracy and F measure. For the com-
bined wizard data, logistic regression had an accu-
racy of 75.2%, and F measures of 0.83 for firm
choices and 0.72 for tentative choices; the decision
tree accuracy was 82.2%, and the F measures for
firm versus tentative choices were respectively
0.82 and 0.71. The decision tree had a root mean
squared error of 0.306, linear regression 0.483. Ta-
ble 3 shows the accuracy and F measures on firm
choices for the decision trees by individual wizard,
along with the numbers of attributes and nodes per
</bodyText>
<tableCaption confidence="0.997723">
Table 2. Distribution of correct actions
</tableCaption>
<table confidence="0.999705545454546">
Correct Action N %
Return 1 2722 65.2445
Return 2 126 3.0201
Return 3 56 1.3423
Return 4 46 1.1026
Return 5 26 0.6232
Return 7 7 0.1678
Return 8 1 0.0002
Return 9 2 0.0005
Question or Giveup 1186 28.4276
Total 4172 1.0000
</table>
<page confidence="0.986407">
845
</page>
<tableCaption confidence="0.99926">
Table 3. Learning results for wizards
</tableCaption>
<table confidence="0.99848725">
Tree Rank Nodes Attributes Accuracy F firm
W4 1 55 12 75.67 0.85
W5 2 21 10 76.17 0.85
W1 3 7 8 80.44 0.87
W7 4 45 11 73.62 0.83
W3 5 33 10 77.42 0.84
W2 6 35 10 78.49 0.85
W6 7 23 10 85.19 0.80
</table>
<bodyText confidence="0.963748">
tree. Although relatively few attributes appeared in
any one tree, most attributes appeared in multiple
nodes. W1 was the exception, with a very small
pruned tree of 7 nodes.
Accuracy of the decision trees does not correlate
with wizard rank. In general, the decision trees
could consistently predict a confident choice (0.80
&lt; F &lt; 0.87), but were less consistent on a tentative
choice (0.60 &lt; F &lt; 0.89), and could predict a ques-
tion only for W4, the wizard with the highest accu-
racy and greatest success at detecting when the
correct title was not in the candidates.
What wizards saw on the GUI, their recent suc-
cess, and recognizer confidence scores were key
attributes in the decision trees. The five features
that appeared most often in the root and top-level
nodes of all tree models reported in Table 3 were:
</bodyText>
<listItem confidence="0.916655363636364">
· DisplayType of the return (Singleton, Ambi-
guous List, NoisyList)
· RecentSuccess, how often the wizard chose the
correct title within the last three title cycles
· ContiguousWordMatch, the maximum number
of contiguous exact word matches between a
candidate and the ASR hypothesis (averaged
across candidates)
· NumberOfCandidates, how many titles were re-
turned by the voice search
· Confidence, the Helios confidence score
</listItem>
<bodyText confidence="0.997385068181818">
DisplayType, NumberOfCandidates and Conti-
guousWordMatch pertain to what the wizard could
see on her GUI. (Recall that DisplayType is distin-
guished by font darkness, as well as by number of
candidates.) The impact of RecentSuccess might
result not just from the wizard’s confidence in her
current strategy, but also from consistency in the
user’s speech characteristics. The Helios confi-
dence annotation uses a learned model based on
features from the recognizer, the parser, and the di-
alogue state. Here confidence primarily reflects
recognition confidence; due to the simplicity of our
grammar, parse results only indicate whether there
is a parse. In addition to these five features, every
tree relied on at least one measure of similarity be-
tween the hypothesis and the candidates.
W4 achieved superior accuracy: she knew when
to offer a title and when not to. In the learned tree
for W4, if the DisplayType was NoisyList, W4
asked a question; if DisplayType was Ambiguous-
List, the features used to predict W4’s action in-
cluded the five listed above, along with the acous-
tic model score, word length of the ASR, number
of times the wizard had asked the user to repeat,
and the maximum size of the gap between words in
the candidates that matched the ASR hypothesis.
To focus on W4’s questioning behavior, we
trained an additional decision tree to learn how W4
chose between two actions: offering a title versus
asking a question. This 37-node, 8-attribute tree
was based on 600 data points, with F=0.91 for
making an offer and F=0.68 for asking a question.
The tree is distinctive in that it splits at the root on
the number of frames in the ASR. If the ASR is
short (as measured both by the number of recogni-
tion frames and the words), W4 asks a question
when DisplayType = AmbiguousList or NoisyList,
either RecentSuccess &lt; 1 or ContiguousWord-
Match = 0, and the acoustic model score is low.
Note that shorter titles are more confusable. If the
ASR is long, W4 asks a question when Conti-
guousWordMatch &lt; 1, RecentSuccess &lt; 2, and ei-
ther CandidateDisplay = NoisyList, or Confidence
is low, and there is a choice of titles.
</bodyText>
<sectionHeader confidence="0.996107" genericHeader="method">
8 Discussion
</sectionHeader>
<bodyText confidence="0.999919266666667">
Our experiment addressed whether voice search
can compensate for incorrect ASR hypotheses and
permit identification of a user’s desired book, giv-
en a request by title. The results show that with
high WER, a baseline dialogue strategy that always
offers the highest-ranked database return can nev-
ertheless achieve moderate accuracy. This is true
even with the relatively simplistic measure of simi-
larity between the ASR hypothesis and candidate
titles used here. As a result, we have integrated
voice search into CheckItOut, along with a linguis-
tically motivated grammar for book titles. Our cur-
rent Phoenix grammar relies on CFG rules
automatically generated from dependency parses
of the book titles, using the MICA parser
</bodyText>
<page confidence="0.993826">
846
</page>
<bodyText confidence="0.999882592592593">
(Bangalore et al., 2009). As described in (Gordon
&amp; Passonneau, 2010), a book title parse can con-
tain multiple title slots that consume discontinuous
sequences of words from the ASR hypothesis, thus
accommodating noisy ASR. For the voice search
phase, we now concatenate the words consumed by
a sequence of title slots. We are also experimenting
with a statistical machine learning approach that
will replace or complement the semantic parsing.
Computers clearly do some tasks faster and
more accurately than people, including database
search. To benefit from such strengths, a dialogue
system should also accommodate human prefe-
rences in dialogue strategy. Previous work has
shown that user satisfaction depends in part on task
success, but also on minimizing behaviors that can
increase task success but require the user to correct
the system (Litman et al., 2006).
The decision tree that models W4 has lower ac-
curacy than other models’ (see Table 3), in part be-
cause her decisions had finer granularity. A spoken
dialogue system could potentially do as well as or
better than the best human at detecting when the
title is not present, given the proper training data.
To support this, a dataset could be created that was
biased toward a larger proportion of cases where
not offering a candidate is the correct action.
</bodyText>
<sectionHeader confidence="0.9842" genericHeader="conclusions">
9 Conclusion and Current Work
</sectionHeader>
<bodyText confidence="0.999985641025641">
This paper presents a novel methodology that em-
beds wizards in a spoken dialogue system, and col-
lects data for a single turn exchange. Our results
illustrate the merits of ranking wizards, and learn-
ing from the best. Our wizards were uniformly
good at choosing the correct title when it was
present, but most were overly eager to identify a
title when it was not among the candidates. In this
respect, the best wizard (W4) achieved the highest
accuracy because she demonstrated a much greater
ability to know when not to offer a title. We have
shown that it is feasible to replicate this ability in a
model learned from features that include the pres-
entation of the search results (length of the candi-
date list, amount of word overlap of candidates
with the ASR hypothesis), recent success at select-
ing the correct candidate, and measures pertaining
to recognition results (confidence, acoustic model
score, speaker rate). If replicated in a spoken di-
alogue system, such a model could support integra-
tion of voice search in a way that avoids
misunderstandings. We conclude that learning
from embedded wizards can exploit a wider range
of relevant features, that dialogue managers can
profit from access to more fine-grained representa-
tions of user utterances, and that machine learners
should be selective about which people to model.
That wizard actions can be modeled using sys-
tem features bodes well for future work. Our next
experiment will collect full dialogues with embed-
ded wizards whose actions will again be restricted
through an interface. This time, NLU will integrate
voice search with the linguistically motivated CFG
rules for book titles described earlier, and a larger
language model and grammar for database entities.
We will select wizards who perform well during
pilot tests. Again, the goal will be to model the
most successful wizards, based upon data from
recognition results, NLU, and voice search results.
</bodyText>
<sectionHeader confidence="0.996935" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999778666666667">
This research was supported by the National
Science Foundation under IIS-0745369, IIS-
084966, and IIS-0744904. We thank the anonym-
ous reviewers, the Heiskell Library, our CMU col-
laborators, our statistical wizard Liana Epstein, and
our enthusiastic undergraduate research assistants.
</bodyText>
<sectionHeader confidence="0.998595" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997850695652174">
Bangalore, Srinivas; Bouillier, Pierre; Nasr, Alexis;
Rambow, Owen; Sagot, Benoit (2009). MICA: a
probabilistic dependency parser based on tree
insertion grammars. Application Note. Human
Language Technology and North American Chapter
of the Association for Computational Linguistics,
pp. 185-188.
Bohus, D.; Rudnicky, A.I. (2009). The RavenClaw
dialog management framework: Architecture and
systems. Computer Speech and Language, 23(3),
332-361.
Bohus, Daniel; Rudnicky, Alex (2002). Integrating
multiple knowledge sources for utterance-level
confidence annotation in the CMU Communicator
spoken dialog system (Technical Report No. CS-
190): Carnegie Mellon University.
Georgila, Kallirroi; Sgarbas, Kyrakos; Tsopanoglou,
Anastasios; Fakotakis, Nikos; Kokkinakis, George
(2003). A speech-based human-computer interaction
system for automating directory assistance services.
International Journal of Speech Technology, Special
Issue on Speech and Human-Computer Interaction,
6(2), 145-59.
</reference>
<page confidence="0.991627">
847
</page>
<reference confidence="0.999575495495495">
Gordon, Joshua, B.; Passonneau, Rebecca J. (2010). An
evaluation framework for natural language
understanding in spoken dialogue systems. Seventh
International Conference on Language Resources
and Evaluation (LREC).
Johnston, Michael; Bangalore, Srinivas; Vasireddy,
Gunaranjan; Stent, Amanda; Ehlen, Patrick; Walker,
Marilyn A., et al. (2002). MATCH--An architecture
for multimodal dialogue systems. Proceedings of the
40th Annual Meeting of the Association for
Computational Linguistics, pp. 376-83.
Komatani, Kazunori; Kanda, Naoyuki; Ogata, Tetsuya;
Okuno, Hiroshi G. (2005). Contextual constraints
based on dialogue models in database search task
for spoken dialogue systems. The Ninth European
Conference on Speech Communication and
Technology (Eurospeech), pp. 877-880.
Kruijff-Korbayová, Ivana; Blaylock, Nate;
Gerstenberger, Ciprian; Rieser, Verena; Becker,
Tilman; Kaisser, Michael, et al. (2005). An
experiment setup for collecting data for adaptive
output planning in a multimodal dialogue system.
10th European Workshop on Natural Language
Generation (ENLG), pp. 191-196.
Levin, Esther; Narayanan, Shrikanth; Pieraccini,
Roberto; Biatov, Konstantin; Bocchieri, E.; De
Fabbrizio, Giuseppe, et al. (2000). The AT&amp;T-
DARPA Communicator Mixed-Initiative Spoken
Dialog System. Sixth International Conference on
Spoken Dialogue Processing (ICLSP), pp. 122-125.
Litman, Diane; Hirschberg, Julia; Swerts, Marc (2006).
Characterizing and predicting corrections in spoken
dialogue systems. Computational Linguistics, 32(3),
417-438.
Litman, Diane; Pan, Shimei (1999). Empirically
evaluating an adaptable spoken dialogue system. 7th
International Conference on User Modeling (UM),
pp. 55-46.
Quinlan, J. Ross (1993). C4.5: Programs for Machine
Learning. San Mateo, CA: Morgan Kaufmann.
Ratcliff, John W.; Metzener, David (1988). Pattern
Matching: The Gestalt Approach. Dr. Dobb&apos;s
Journal, 46
Raux, Antoine; Bohus, Dan; Langner, Brian; Black,
Alan W.; Eskenazi, Maxine (2006). Doing research
on a deployed spoken dialogue system: one year of
Let&apos;s Go! experience. Ninth International
Conference on Spoken Language Processing
(Interspeech/ICSLP).
Raux, Antoine; Eskenazi, Maxine (2007). A Multi-layer
architecture for semi-synchronous event-driven
dialogue management.IEEE Workshop on
Automatic Speech Recognition and Understanding
(ASRU 2007), Kyoto, Japan.
Raux, Antoine; Langner, Brian; Black, Alan W.;
Eskenazi, Maxine (2005). Let&apos;s Go Public! Taking a
spoken dialog system to the real world.Interspeech
2005 (Eurospeech), Lisbon, Portugal.
Rieser, Verena; Kruijff-Korbayová, Ivana; Lemon,
Oliver (2005). A corpus collection and annotation
framework for learning multimodal clarification
strategies. Sixth SIGdial Workshop on Discourse
and Dialogue, pp. 97-106.
Sacks, Harvey; Schegloff, Emanuel A.; Jefferson, Gail
(1974). A simplest systematics for the organization
of turn-taking for conversation. Language, 50(4),
696-735.
Skantze, Gabriel (2003). Exploring human error
handling strategies: Implications for Spoken
Dialogue Systems. Proceedings of ISCA Tutorial
and Research Workshp on Error Handling in Spoken
Dialogue Systems, pp. 71-76.
Stoyanchev, Svetlana; Stent, Amanda (2009).
Predicting concept types in user corrections in
dialog. Proceedings of the EACL Workshop SRSL
2009, the Second Workshop on Semantic
Representation of Spoken Language, pp. 42-49.
Turunen, Markku; Hakulinen, Jaakko; Kainulainen,
Anssi (2006). Evaluation of a spoken dialogue
system with usability tests and long-term pilot
studies. Ninth International Conference on Spoken
Language Processing (Interspeech 2006 - ICSLP).
Walker, M A.; Litman, D, J.; Kamm, C. A.; Abella, A.
(1998). Evaluating Spoken Dialogue Agents with
PARADISE: Two Case Studies. Computer Speech
and Language, 12, 317-348.
Wang, Ye-Yi; Yu, Dong; Ju, Yun-Cheng; Acero, Alex
(2008). An introduction to voice search. IEEE
Signal Process. Magazine, 25(3).
Ward, Wayne; Issar, Sunil (1994). Recent improvements
in the CMU spoken language understanding
system.ARPA Human Language Technology
Workshop, Plainsboro, NJ.
Williams, Jason D.; Young, Steve (2004).
Characterising Task-oriented Dialog using a
Simulated ASR Channel. Eight International
Conference on Spoken Language Processing
(ICSLP/Interspeech), pp. 185-188.
Witten, Ian H.; Frank, Eibe (2005). Data Mining:
Practical Machine Learning Tools and Techniques
(2nd ed.). San Francisco: Morgan Kaufmann.
Zollo, Teresa (1999). A study of human dialogue
strategies in the presence of speech recognition
errors. Proceedings of AAAI Fall Symposium on
Psychological Models of Communication in
Collaborative Systems, pp. 132-139.
Zue, Victor; Seneff, Stephanie; Glass, James; Polifroni,
Joseph; Pao, Christine; Hazen, Timothy J., et al.
(2000). A Telephone-based conversational interface
for weather information. IEEE Transactions on
Speech and Audio Processing, 8, 85-96.
</reference>
<page confidence="0.997183">
848
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.225490">
<title confidence="0.999992">Learning about Voice Search for Spoken Dialogue Systems</title>
<author confidence="0.7679545">J Susan L Tiziana B Pravin</author>
<affiliation confidence="0.48977375">for Computational Learning Systems, Columbia of Computer Science, Hunter College of The City University of New of Computer Science, The Graduate Center of The City University of New of Computer Science, Columbia</affiliation>
<email confidence="0.9771915">becky@cs.columbia.edu,susan.epstein@hunter.cuny.edu,joshua@cs.columbia.edu,pravin.bhutada@gmail.com</email>
<abstract confidence="0.9982921">In a Wizard-of-Oz experiment with multiple wizard subjects, each wizard viewed automated recognition results for utterances whose interpretation is critical to task success: requests for books by title from a library database. To avoid non-understandings, the wizard directly queried the application database with ASR hypothesis To learn how to avoid misunderstandings, we investigated how wizards dealt with uncertainty in voice search results. Wizards were quite successful at selecting the correct title from query results that included a match. The most successful wizard could also tell when the query results did not contain the requested title. Our learned models of the best wizard’s behavior combine features available to wizards with some that are not, such as recognition confidence and acoustic model scores.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Pierre Bouillier</author>
<author>Alexis Nasr</author>
<author>Owen Rambow</author>
<author>Benoit Sagot</author>
</authors>
<title>MICA: a probabilistic dependency parser based on tree insertion grammars. Application Note.</title>
<date>2009</date>
<booktitle>Human Language Technology and North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>185--188</pages>
<contexts>
<context position="30998" citStr="Bangalore et al., 2009" startWordPosition="5002" endWordPosition="5005">esired book, given a request by title. The results show that with high WER, a baseline dialogue strategy that always offers the highest-ranked database return can nevertheless achieve moderate accuracy. This is true even with the relatively simplistic measure of similarity between the ASR hypothesis and candidate titles used here. As a result, we have integrated voice search into CheckItOut, along with a linguistically motivated grammar for book titles. Our current Phoenix grammar relies on CFG rules automatically generated from dependency parses of the book titles, using the MICA parser 846 (Bangalore et al., 2009). As described in (Gordon &amp; Passonneau, 2010), a book title parse can contain multiple title slots that consume discontinuous sequences of words from the ASR hypothesis, thus accommodating noisy ASR. For the voice search phase, we now concatenate the words consumed by a sequence of title slots. We are also experimenting with a statistical machine learning approach that will replace or complement the semantic parsing. Computers clearly do some tasks faster and more accurately than people, including database search. To benefit from such strengths, a dialogue system should also accommodate human </context>
</contexts>
<marker>Bangalore, Bouillier, Nasr, Rambow, Sagot, 2009</marker>
<rawString>Bangalore, Srinivas; Bouillier, Pierre; Nasr, Alexis; Rambow, Owen; Sagot, Benoit (2009). MICA: a probabilistic dependency parser based on tree insertion grammars. Application Note. Human Language Technology and North American Chapter of the Association for Computational Linguistics, pp. 185-188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bohus</author>
<author>A I Rudnicky</author>
</authors>
<title>The RavenClaw dialog management framework: Architecture and systems.</title>
<date>2009</date>
<journal>Computer Speech and Language,</journal>
<volume>23</volume>
<issue>3</issue>
<pages>332--361</pages>
<contexts>
<context position="12666" citStr="Bohus &amp; Rudnicky, 2009" startWordPosition="1958" endWordPosition="1961">CheckItOut and Embedded Wizards CheckItOut is modeled on library transactions at the Andrew Heiskell Braille and Talking Book Library, a branch of the New York Public Library and part of the National Library of Congress. Borrowing requests are handled by telephone. Books, mainly in a proprietary audio format, travel by mail. In a dialogue with CheckItOut, a user identifies herself, requests books, and is told which are available for immediate shipment or will go on reserve. The user can request a book by catalogue number, title, or author. CheckItOut builds on the Olympus/RavenClaw framework (Bohus &amp; Rudnicky, 2009) that has been the basis for about a dozen dialogue systems in different domains, including Let’s Go Public! (Raux et al., 2005). Speech recognition relies on PocketSphinx. Phoenix, a robust context-free grammar (CFG) semantic parser, handles natural language understanding (Ward &amp; Issar, 1994). The Apollo interaction manager (Raux &amp; Eskenazi, 2007) detects utterance boundaries using information from speech recognition, semantic parsing, and Helios, an utterance-level confidence annotator (Bohus &amp; Rudnicky, 2002). The dialogue manager is implemented in RavenClaw. 842 To design CheckItOut’s dial</context>
</contexts>
<marker>Bohus, Rudnicky, 2009</marker>
<rawString>Bohus, D.; Rudnicky, A.I. (2009). The RavenClaw dialog management framework: Architecture and systems. Computer Speech and Language, 23(3), 332-361.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Bohus</author>
<author>Rudnicky</author>
</authors>
<title>Integrating multiple knowledge sources for utterance-level confidence annotation in the CMU Communicator spoken dialog system</title>
<date>2002</date>
<tech>(Technical Report No. CS190):</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="13183" citStr="Bohus &amp; Rudnicky, 2002" startWordPosition="2030" endWordPosition="2033">ue number, title, or author. CheckItOut builds on the Olympus/RavenClaw framework (Bohus &amp; Rudnicky, 2009) that has been the basis for about a dozen dialogue systems in different domains, including Let’s Go Public! (Raux et al., 2005). Speech recognition relies on PocketSphinx. Phoenix, a robust context-free grammar (CFG) semantic parser, handles natural language understanding (Ward &amp; Issar, 1994). The Apollo interaction manager (Raux &amp; Eskenazi, 2007) detects utterance boundaries using information from speech recognition, semantic parsing, and Helios, an utterance-level confidence annotator (Bohus &amp; Rudnicky, 2002). The dialogue manager is implemented in RavenClaw. 842 To design CheckItOut’s dialogue manager, we recorded 175 calls (4.5 hours) from patrons to librarians. We identified 82 book request calls, transcribed them, aligned the utterances with the speech signal, and annotated the transcripts for dialogue acts. Because active patrons receive monthly newsletters listing new titles in the desired formats, patrons request specific items with advance knowledge of the author, title, or catalogue number. Most book title requests accurately reproduce the exact title, the title less an initial determiner</context>
</contexts>
<marker>Bohus, Rudnicky, 2002</marker>
<rawString>Bohus, Daniel; Rudnicky, Alex (2002). Integrating multiple knowledge sources for utterance-level confidence annotation in the CMU Communicator spoken dialog system (Technical Report No. CS190): Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kallirroi Georgila</author>
</authors>
<title>Sgarbas, Kyrakos; Tsopanoglou, Anastasios; Fakotakis, Nikos; Kokkinakis,</title>
<date>2003</date>
<journal>International Journal of Speech Technology, Special Issue on Speech and Human-Computer Interaction,</journal>
<volume>6</volume>
<issue>2</issue>
<pages>145--59</pages>
<location>George</location>
<marker>Georgila, 2003</marker>
<rawString>Georgila, Kallirroi; Sgarbas, Kyrakos; Tsopanoglou, Anastasios; Fakotakis, Nikos; Kokkinakis, George (2003). A speech-based human-computer interaction system for automating directory assistance services. International Journal of Speech Technology, Special Issue on Speech and Human-Computer Interaction, 6(2), 145-59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Gordon</author>
<author>B Passonneau</author>
<author>J Rebecca</author>
</authors>
<title>An evaluation framework for natural language understanding in spoken dialogue systems.</title>
<date>2010</date>
<booktitle>Seventh International Conference on Language Resources and Evaluation (LREC).</booktitle>
<marker>Gordon, Passonneau, Rebecca, 2010</marker>
<rawString>Gordon, Joshua, B.; Passonneau, Rebecca J. (2010). An evaluation framework for natural language understanding in spoken dialogue systems. Seventh International Conference on Language Resources and Evaluation (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Johnston</author>
<author>Srinivas Bangalore</author>
<author>Gunaranjan Vasireddy</author>
<author>Amanda Stent</author>
<author>Patrick Ehlen</author>
<author>Marilyn A Walker</author>
</authors>
<title>MATCH--An architecture for multimodal dialogue systems.</title>
<date>2002</date>
<booktitle>Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>376--83</pages>
<contexts>
<context position="5664" citStr="Johnston et al., 2002" startWordPosition="866" endWordPosition="869">lt to achieve, given unpredictable background noise and transmission quality. For example, the 68% word error rate (WER) for the fielded version of Let’s Go Public! (Raux et al., 2005) far exceeded its 17% WER under controlled conditions. Our application handles library requests by telephone, and would benefit from robustness to noisy ASR. Second, the book title field in our database differs from the typical case for spoken dialogue systems that access a relational database. Such systems include travel booking (Levin et al., 2000), bus route information (Raux et al., 2006), restaurant guides (Johnston et al., 2002; Komatani et al., 2005), weather (Zue et al., 2000) and directory services (Georgila et al., 2003). In general for these systems, a few words are sufficient to retrieve the desired attribute value, such as a neighborhood, a street, or a surname. Mean utterance length in a sample of 40,000 Let’s Go Public! utterances, for example, is 2.4 words. The average book title length in our database is 5.4 words. Finally, our dialogue system, CheckItOut, allows users to choose whether to request books by title, author, or catalogue number. The database represents 5028 active patrons (with real borrowing</context>
</contexts>
<marker>Johnston, Bangalore, Vasireddy, Stent, Ehlen, Walker, 2002</marker>
<rawString>Johnston, Michael; Bangalore, Srinivas; Vasireddy, Gunaranjan; Stent, Amanda; Ehlen, Patrick; Walker, Marilyn A., et al. (2002). MATCH--An architecture for multimodal dialogue systems. Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pp. 376-83.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazunori Komatani</author>
<author>Naoyuki Kanda</author>
<author>Tetsuya Ogata</author>
<author>Hiroshi G Okuno</author>
</authors>
<title>Contextual constraints based on dialogue models in database search task for spoken dialogue systems.</title>
<date>2005</date>
<booktitle>The Ninth European Conference on Speech Communication and Technology (Eurospeech),</booktitle>
<pages>877--880</pages>
<contexts>
<context position="5688" citStr="Komatani et al., 2005" startWordPosition="870" endWordPosition="873">predictable background noise and transmission quality. For example, the 68% word error rate (WER) for the fielded version of Let’s Go Public! (Raux et al., 2005) far exceeded its 17% WER under controlled conditions. Our application handles library requests by telephone, and would benefit from robustness to noisy ASR. Second, the book title field in our database differs from the typical case for spoken dialogue systems that access a relational database. Such systems include travel booking (Levin et al., 2000), bus route information (Raux et al., 2006), restaurant guides (Johnston et al., 2002; Komatani et al., 2005), weather (Zue et al., 2000) and directory services (Georgila et al., 2003). In general for these systems, a few words are sufficient to retrieve the desired attribute value, such as a neighborhood, a street, or a surname. Mean utterance length in a sample of 40,000 Let’s Go Public! utterances, for example, is 2.4 words. The average book title length in our database is 5.4 words. Finally, our dialogue system, CheckItOut, allows users to choose whether to request books by title, author, or catalogue number. The database represents 5028 active patrons (with real borrowing histories and preferenc</context>
<context position="9098" citStr="Komatani et al., 2005" startWordPosition="1396" endWordPosition="1399"> information-seeking and transaction-based dialogue systems. Those systems typically perform 841 natural language understanding on ASR output before database query with techniques that try to improve or expand ASR output. None that we know of use voice search. For one directory service application, users spell the first three letters of surnames, and then ASR results are expanded using frequently confused phones (Georgila et al., 2003). A two-pass recognition architecture added to Let’s Go Public! improved concept recognition in postconfirmation user utterances (Stoyanchev &amp; Stent, 2009). In (Komatani et al., 2005), a shallow semantic interpretation phase was followed by decision trees to classify utterances as relevant either to query type or to specific query slots, to narrow the set of possible interpretations. CheckItOut is most similar in spirit to the latter approach, but relies on the database earlier, and only for semantic interpretation, not to also guide the dialogue strategy. Our approach to noisy ASR is inspired by previous WOz studies with real (Skantze, 2003; Zollo, 1999) or simulated ASR (Kruijff-Korbayová et al., 2005; Rieser et al., 2005; Williams &amp; Young, 2004). Simulation makes it pos</context>
</contexts>
<marker>Komatani, Kanda, Ogata, Okuno, 2005</marker>
<rawString>Komatani, Kazunori; Kanda, Naoyuki; Ogata, Tetsuya; Okuno, Hiroshi G. (2005). Contextual constraints based on dialogue models in database search task for spoken dialogue systems. The Ninth European Conference on Speech Communication and Technology (Eurospeech), pp. 877-880.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivana Kruijff-Korbayová</author>
<author>Nate Blaylock</author>
<author>Ciprian Gerstenberger</author>
<author>Verena Rieser</author>
<author>Tilman Becker</author>
<author>Michael Kaisser</author>
</authors>
<title>An experiment setup for collecting data for adaptive output planning in a multimodal dialogue system.</title>
<date>2005</date>
<booktitle>10th European Workshop on Natural Language Generation (ENLG),</booktitle>
<pages>191--196</pages>
<contexts>
<context position="9627" citStr="Kruijff-Korbayová et al., 2005" startWordPosition="1483" endWordPosition="1486">ecognition in postconfirmation user utterances (Stoyanchev &amp; Stent, 2009). In (Komatani et al., 2005), a shallow semantic interpretation phase was followed by decision trees to classify utterances as relevant either to query type or to specific query slots, to narrow the set of possible interpretations. CheckItOut is most similar in spirit to the latter approach, but relies on the database earlier, and only for semantic interpretation, not to also guide the dialogue strategy. Our approach to noisy ASR is inspired by previous WOz studies with real (Skantze, 2003; Zollo, 1999) or simulated ASR (Kruijff-Korbayová et al., 2005; Rieser et al., 2005; Williams &amp; Young, 2004). Simulation makes it possible to collect dialogues without building a speech recognizer, and to control for WER. In the studies that involved task-oriented dialogues, wizards typically focused more on the task and less on resolving ASR errors (Williams &amp; Young, 2004; Skantze, 2003; Zollo, 1999). In studies more like the information-seeking dialogues addressed here, an entirely different pattern is observed (Kruijff-Korbayová et al., 2005; Rieser et al., 2005). Zollo collected seven dialogues with different human-wizard pairs to develop an evacuati</context>
</contexts>
<marker>Kruijff-Korbayová, Blaylock, Gerstenberger, Rieser, Becker, Kaisser, 2005</marker>
<rawString>Kruijff-Korbayová, Ivana; Blaylock, Nate; Gerstenberger, Ciprian; Rieser, Verena; Becker, Tilman; Kaisser, Michael, et al. (2005). An experiment setup for collecting data for adaptive output planning in a multimodal dialogue system. 10th European Workshop on Natural Language Generation (ENLG), pp. 191-196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Esther Levin</author>
<author>Shrikanth Narayanan</author>
<author>Roberto Pieraccini</author>
<author>Konstantin Biatov</author>
<author>E Bocchieri</author>
<author>Giuseppe De Fabbrizio</author>
</authors>
<date>2000</date>
<booktitle>The AT&amp;TDARPA Communicator Mixed-Initiative Spoken Dialog System. Sixth International Conference on Spoken Dialogue Processing (ICLSP),</booktitle>
<pages>122--125</pages>
<marker>Levin, Narayanan, Pieraccini, Biatov, Bocchieri, De Fabbrizio, 2000</marker>
<rawString>Levin, Esther; Narayanan, Shrikanth; Pieraccini, Roberto; Biatov, Konstantin; Bocchieri, E.; De Fabbrizio, Giuseppe, et al. (2000). The AT&amp;TDARPA Communicator Mixed-Initiative Spoken Dialog System. Sixth International Conference on Spoken Dialogue Processing (ICLSP), pp. 122-125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diane Litman</author>
<author>Julia Hirschberg</author>
<author>Swerts</author>
</authors>
<title>Characterizing and predicting corrections in spoken dialogue systems.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>3</issue>
<pages>417--438</pages>
<contexts>
<context position="7317" citStr="Litman et al., 2006" startWordPosition="1131" endWordPosition="1134">7.4% occur only in titles, 32.1% only in author names, and 10.5% in both. Many book titles (e.g., You See I Haven’t Forgotten, You Never Know) have a high potential for confusability with non-title phrases in users’ book requests. Given the longer database field and the confusability of the book title language, integrating voice search is likely to have a relatively larger impact in CheckItOut. We seek to minimize non-understandings and misunderstandings for several reasons. First, user corrections in both situations have been shown to be more poorly recognized than non-correction utterances (Litman et al., 2006). Non-understandings typically result in re-prompting the user for the same information. This often leads to hyperarticulation and concomitant degradation in recognition performance. Second, users seem to prefer systems that minimize non-understandings and misunderstandings, even at the expense of dialogue efficiency. Users of the TOOT train information spoken dialogue system preferred system-initiative to mixed- or user-initiative, and preferred explicit confirmation to implicit or no confirmation (Litman &amp; Pan, 1999). This was true despite the fact that a mixed-initiative, implicit confirmat</context>
<context position="31840" citStr="Litman et al., 2006" startWordPosition="5134" endWordPosition="5137">hase, we now concatenate the words consumed by a sequence of title slots. We are also experimenting with a statistical machine learning approach that will replace or complement the semantic parsing. Computers clearly do some tasks faster and more accurately than people, including database search. To benefit from such strengths, a dialogue system should also accommodate human preferences in dialogue strategy. Previous work has shown that user satisfaction depends in part on task success, but also on minimizing behaviors that can increase task success but require the user to correct the system (Litman et al., 2006). The decision tree that models W4 has lower accuracy than other models’ (see Table 3), in part because her decisions had finer granularity. A spoken dialogue system could potentially do as well as or better than the best human at detecting when the title is not present, given the proper training data. To support this, a dataset could be created that was biased toward a larger proportion of cases where not offering a candidate is the correct action. 9 Conclusion and Current Work This paper presents a novel methodology that embeds wizards in a spoken dialogue system, and collects data for a sin</context>
</contexts>
<marker>Litman, Hirschberg, Swerts, 2006</marker>
<rawString>Litman, Diane; Hirschberg, Julia; Swerts, Marc (2006). Characterizing and predicting corrections in spoken dialogue systems. Computational Linguistics, 32(3), 417-438.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diane Litman</author>
<author>Shimei Pan</author>
</authors>
<title>Empirically evaluating an adaptable spoken dialogue system.</title>
<date>1999</date>
<booktitle>7th International Conference on User Modeling (UM),</booktitle>
<pages>55--46</pages>
<contexts>
<context position="7841" citStr="Litman &amp; Pan, 1999" startWordPosition="1202" endWordPosition="1205">ve been shown to be more poorly recognized than non-correction utterances (Litman et al., 2006). Non-understandings typically result in re-prompting the user for the same information. This often leads to hyperarticulation and concomitant degradation in recognition performance. Second, users seem to prefer systems that minimize non-understandings and misunderstandings, even at the expense of dialogue efficiency. Users of the TOOT train information spoken dialogue system preferred system-initiative to mixed- or user-initiative, and preferred explicit confirmation to implicit or no confirmation (Litman &amp; Pan, 1999). This was true despite the fact that a mixed-initiative, implicit confirmation strategy led to fewer turns for the same task. Most of the more recent work on spoken dialogue systems focuses on mixed-initiative systems in laboratory settings. Still, recent work suggests that while mixed- or user-initiative is rated highly in usability studies, under real usage it “fails to provide [a] robust enough interface” (Turunen et al., 2006). Incorporating accurate voice search into spoken dialogue systems could lead to fewer nonunderstandings and fewer misunderstandings. 3 Related Work Our approach to </context>
</contexts>
<marker>Litman, Pan, 1999</marker>
<rawString>Litman, Diane; Pan, Shimei (1999). Empirically evaluating an adaptable spoken dialogue system. 7th International Conference on User Modeling (UM), pp. 55-46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ross Quinlan</author>
</authors>
<title>C4.5: Programs for Machine Learning.</title>
<date>1993</date>
<publisher>Morgan Kaufmann.</publisher>
<location>San Mateo, CA:</location>
<contexts>
<context position="21215" citStr="Quinlan, 1993" startWordPosition="3370" endWordPosition="3371">tes were averaged. We used three machine-learning techniques to predict wizards’ actions: decision trees, linear regression, and logistic regression. All models were produced with the Weka data mining package, using 10-fold cross-validation (Witten &amp; Frank, 2005). A decision tree is a predictive model that maps feature values to a target value. One applies a decision tree by tracing a path from the root (the top node) to a leaf, which provides the target value. Here the leaves are the wizard actions: firm choice, tentative choice, question, or give up. The algorithm used is a version of C4.5 (Quinlan, 1993), where gain ratio is the splitting criterion. To confirm the learnability and quality of the decision tree models, we also trained logistic regression and linear regression models on the same data, normalized in [0, 1]. The logistic regression model predicts the probability of wizards’ actions by fitting the data to a logistic curve. It generalizes the linear model to the prediction of categorical data; here, categories correspond to wizards’ actions. The linear regression models represent wizards’ actions numerically, in decreasing value: firm choice, tentative choice, question, give up. Alt</context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>Quinlan, J. Ross (1993). C4.5: Programs for Machine Learning. San Mateo, CA: Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John W Ratcliff</author>
<author>David Metzener</author>
</authors>
<date>1988</date>
<journal>Pattern Matching: The Gestalt Approach. Dr. Dobb&apos;s Journal,</journal>
<volume>46</volume>
<contexts>
<context position="15294" citStr="Ratcliff &amp; Metzener, 1988" startWordPosition="2366" endWordPosition="2369">a language model based on 7500 titles would yield WER in the desired range. (Average WER for the book title requests in our experiment was 71%.) To model one aspect of the real world useful for an actual system, titles with below average circulation were eliminated. An offline pilot study had demonstrated that one-word titles were easy for wizards, so we eliminated those as well. A random sample of 7,500 was chosen from the remaining 19,708 titles to build the trigram language model. We used Ratcliff/Obersherhelp (R/O) to measure the similarity of an ASR string to book titles in the database (Ratcliff &amp; Metzener, 1988). R/O calculates the ratio r of the number of matching characters to the total length of both strings, but requires O(r2) time on average and O(r3) time in the worst case. We therefore computed an upper bound on the similarity of a title/ASR pair prior to full R/O to speed processing. 5 Experimental Design In this experiment, a user and a wizard sat in separate rooms where they could not overhear one another. Each had a headset with microphone and a GUI. Audio input on the wizard’s headset was disabled. When the user requested a title, the ASR hypothesis for the title appeared on the wizard’s </context>
</contexts>
<marker>Ratcliff, Metzener, 1988</marker>
<rawString>Ratcliff, John W.; Metzener, David (1988). Pattern Matching: The Gestalt Approach. Dr. Dobb&apos;s Journal, 46</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antoine Raux</author>
<author>Dan Bohus</author>
<author>Brian Langner</author>
<author>Alan W Black</author>
<author>Eskenazi</author>
</authors>
<title>Doing research on a deployed spoken dialogue system: one year of Let&apos;s Go!</title>
<date>2006</date>
<booktitle>experience. Ninth International Conference on Spoken Language Processing (Interspeech/ICSLP).</booktitle>
<contexts>
<context position="5622" citStr="Raux et al., 2006" startWordPosition="859" endWordPosition="862">ld telephone application can be difficult to achieve, given unpredictable background noise and transmission quality. For example, the 68% word error rate (WER) for the fielded version of Let’s Go Public! (Raux et al., 2005) far exceeded its 17% WER under controlled conditions. Our application handles library requests by telephone, and would benefit from robustness to noisy ASR. Second, the book title field in our database differs from the typical case for spoken dialogue systems that access a relational database. Such systems include travel booking (Levin et al., 2000), bus route information (Raux et al., 2006), restaurant guides (Johnston et al., 2002; Komatani et al., 2005), weather (Zue et al., 2000) and directory services (Georgila et al., 2003). In general for these systems, a few words are sufficient to retrieve the desired attribute value, such as a neighborhood, a street, or a surname. Mean utterance length in a sample of 40,000 Let’s Go Public! utterances, for example, is 2.4 words. The average book title length in our database is 5.4 words. Finally, our dialogue system, CheckItOut, allows users to choose whether to request books by title, author, or catalogue number. The database represent</context>
</contexts>
<marker>Raux, Bohus, Langner, Black, Eskenazi, 2006</marker>
<rawString>Raux, Antoine; Bohus, Dan; Langner, Brian; Black, Alan W.; Eskenazi, Maxine (2006). Doing research on a deployed spoken dialogue system: one year of Let&apos;s Go! experience. Ninth International Conference on Spoken Language Processing (Interspeech/ICSLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antoine Raux</author>
<author>Eskenazi</author>
</authors>
<title>A Multi-layer architecture for semi-synchronous event-driven dialogue management.IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU</title>
<date>2007</date>
<location>Kyoto, Japan.</location>
<contexts>
<context position="13016" citStr="Raux &amp; Eskenazi, 2007" startWordPosition="2009" endWordPosition="2012">ItOut, a user identifies herself, requests books, and is told which are available for immediate shipment or will go on reserve. The user can request a book by catalogue number, title, or author. CheckItOut builds on the Olympus/RavenClaw framework (Bohus &amp; Rudnicky, 2009) that has been the basis for about a dozen dialogue systems in different domains, including Let’s Go Public! (Raux et al., 2005). Speech recognition relies on PocketSphinx. Phoenix, a robust context-free grammar (CFG) semantic parser, handles natural language understanding (Ward &amp; Issar, 1994). The Apollo interaction manager (Raux &amp; Eskenazi, 2007) detects utterance boundaries using information from speech recognition, semantic parsing, and Helios, an utterance-level confidence annotator (Bohus &amp; Rudnicky, 2002). The dialogue manager is implemented in RavenClaw. 842 To design CheckItOut’s dialogue manager, we recorded 175 calls (4.5 hours) from patrons to librarians. We identified 82 book request calls, transcribed them, aligned the utterances with the speech signal, and annotated the transcripts for dialogue acts. Because active patrons receive monthly newsletters listing new titles in the desired formats, patrons request specific item</context>
</contexts>
<marker>Raux, Eskenazi, 2007</marker>
<rawString>Raux, Antoine; Eskenazi, Maxine (2007). A Multi-layer architecture for semi-synchronous event-driven dialogue management.IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU 2007), Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antoine Raux</author>
<author>Brian Langner</author>
<author>Alan W Black</author>
<author>Eskenazi</author>
</authors>
<title>Let&apos;s Go Public! Taking a spoken dialog system to the real world.Interspeech</title>
<date>2005</date>
<location>(Eurospeech), Lisbon, Portugal.</location>
<contexts>
<context position="5227" citStr="Raux et al., 2005" startWordPosition="795" endWordPosition="798">signed by the string similarity metric we use. Three factors motivated our use of voice search to interpret book title requests: noisy ASR, unusually long query targets, and high overlap of the vocabulary across different query types (e.g., author and title) as well as with non-query words in caller utterances (e.g., “Could you look up . . .”). First, accurate speech recognition for a realworld telephone application can be difficult to achieve, given unpredictable background noise and transmission quality. For example, the 68% word error rate (WER) for the fielded version of Let’s Go Public! (Raux et al., 2005) far exceeded its 17% WER under controlled conditions. Our application handles library requests by telephone, and would benefit from robustness to noisy ASR. Second, the book title field in our database differs from the typical case for spoken dialogue systems that access a relational database. Such systems include travel booking (Levin et al., 2000), bus route information (Raux et al., 2006), restaurant guides (Johnston et al., 2002; Komatani et al., 2005), weather (Zue et al., 2000) and directory services (Georgila et al., 2003). In general for these systems, a few words are sufficient to re</context>
<context position="12794" citStr="Raux et al., 2005" startWordPosition="1980" endWordPosition="1983">, a branch of the New York Public Library and part of the National Library of Congress. Borrowing requests are handled by telephone. Books, mainly in a proprietary audio format, travel by mail. In a dialogue with CheckItOut, a user identifies herself, requests books, and is told which are available for immediate shipment or will go on reserve. The user can request a book by catalogue number, title, or author. CheckItOut builds on the Olympus/RavenClaw framework (Bohus &amp; Rudnicky, 2009) that has been the basis for about a dozen dialogue systems in different domains, including Let’s Go Public! (Raux et al., 2005). Speech recognition relies on PocketSphinx. Phoenix, a robust context-free grammar (CFG) semantic parser, handles natural language understanding (Ward &amp; Issar, 1994). The Apollo interaction manager (Raux &amp; Eskenazi, 2007) detects utterance boundaries using information from speech recognition, semantic parsing, and Helios, an utterance-level confidence annotator (Bohus &amp; Rudnicky, 2002). The dialogue manager is implemented in RavenClaw. 842 To design CheckItOut’s dialogue manager, we recorded 175 calls (4.5 hours) from patrons to librarians. We identified 82 book request calls, transcribed the</context>
</contexts>
<marker>Raux, Langner, Black, Eskenazi, 2005</marker>
<rawString>Raux, Antoine; Langner, Brian; Black, Alan W.; Eskenazi, Maxine (2005). Let&apos;s Go Public! Taking a spoken dialog system to the real world.Interspeech 2005 (Eurospeech), Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Verena Rieser</author>
<author>Ivana Kruijff-Korbayová</author>
<author>Oliver Lemon</author>
</authors>
<title>A corpus collection and annotation framework for learning multimodal clarification strategies.</title>
<date>2005</date>
<booktitle>Sixth SIGdial Workshop on Discourse and Dialogue,</booktitle>
<pages>97--106</pages>
<contexts>
<context position="9648" citStr="Rieser et al., 2005" startWordPosition="1487" endWordPosition="1490">ser utterances (Stoyanchev &amp; Stent, 2009). In (Komatani et al., 2005), a shallow semantic interpretation phase was followed by decision trees to classify utterances as relevant either to query type or to specific query slots, to narrow the set of possible interpretations. CheckItOut is most similar in spirit to the latter approach, but relies on the database earlier, and only for semantic interpretation, not to also guide the dialogue strategy. Our approach to noisy ASR is inspired by previous WOz studies with real (Skantze, 2003; Zollo, 1999) or simulated ASR (Kruijff-Korbayová et al., 2005; Rieser et al., 2005; Williams &amp; Young, 2004). Simulation makes it possible to collect dialogues without building a speech recognizer, and to control for WER. In the studies that involved task-oriented dialogues, wizards typically focused more on the task and less on resolving ASR errors (Williams &amp; Young, 2004; Skantze, 2003; Zollo, 1999). In studies more like the information-seeking dialogues addressed here, an entirely different pattern is observed (Kruijff-Korbayová et al., 2005; Rieser et al., 2005). Zollo collected seven dialogues with different human-wizard pairs to develop an evacuation plan. The overall </context>
<context position="11450" citStr="Rieser et al., 2005" startWordPosition="1764" endWordPosition="1767">d a route description, asked a taskrelated question, or requested a clarification. Williams and Young collected 144 dialogues simulating tourist requests for directions and other negotiations. WER was constrained to be high, medium, or low. Under medium WER, a taskrelated question in response to a non-understanding or misunderstanding led to full understanding more often than explicit repairs. Under high WER, however, the reverse was true. Misunderstandings significantly increased when wizards followed nonunderstandings or misunderstandings with a taskrelated question instead of a repair. In (Rieser et al., 2005), wizards simulated a multimodal MP3 player application with access to a database of 150K music albums. Responses could be presented verbally or graphically. In the noisy transcription condition, wizards made clarification requests about twice as often as that found in similar human-human dialogue. In a system like CheckItOut, user utterances that request database information must be understood. We seek an approach that would reduce the rate of misunderstandings observed for high WER in (Williams &amp; Young, 2004) and the rate of clarification requests observed in (Rieser et al., 2005). 4 CheckIt</context>
</contexts>
<marker>Rieser, Kruijff-Korbayová, Lemon, 2005</marker>
<rawString>Rieser, Verena; Kruijff-Korbayová, Ivana; Lemon, Oliver (2005). A corpus collection and annotation framework for learning multimodal clarification strategies. Sixth SIGdial Workshop on Discourse and Dialogue, pp. 97-106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harvey Sacks</author>
<author>Emanuel A Schegloff</author>
<author>Jefferson</author>
</authors>
<title>A simplest systematics for the organization of turn-taking for conversation.</title>
<date>1974</date>
<journal>Language,</journal>
<volume>50</volume>
<issue>4</issue>
<pages>696--735</pages>
<contexts>
<context position="3606" citStr="Sacks et al., 1974" startWordPosition="537" endWordPosition="540">res that are available to wizards with some that are not, such as recognition confidence and acoustic model scores. The next section of the paper motivates our experiment. Subsequent sections describe related work, the dialogue system and embedded wizard infrastructure, experimental design, learning methods, and results. We then discuss how to generalize from the results of our study for spoken dialogue system design. We conclude with a summary of results and their implications. 2 Motivation Rather than investigate full dialogues, we addressed a single type of turn exchange or adjacency pair (Sacks et al., 1974): a request for a book by its 840 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 840–848, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics title. This allowed us to collect data exclusively about an utterance type critical for task success in our application domain. We hypothesized that lowlevel features from speech recognition, such as acoustic model fit, could independently affect voice search confidence. We therefore applied a novel approach, embedded WOz, in which a wizard and the system together i</context>
</contexts>
<marker>Sacks, Schegloff, Jefferson, 1974</marker>
<rawString>Sacks, Harvey; Schegloff, Emanuel A.; Jefferson, Gail (1974). A simplest systematics for the organization of turn-taking for conversation. Language, 50(4), 696-735.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriel Skantze</author>
</authors>
<title>Exploring human error handling strategies: Implications for Spoken Dialogue Systems.</title>
<date>2003</date>
<booktitle>Proceedings of ISCA Tutorial and Research Workshp on Error Handling in Spoken Dialogue Systems,</booktitle>
<pages>71--76</pages>
<contexts>
<context position="1773" citStr="Skantze, 2003" startWordPosition="249" endWordPosition="250">e most successful wizard could also tell when the query results did not contain the requested title. Our learned models of the best wizard’s behavior combine features available to wizards with some that are not, such as recognition confidence and acoustic model scores. 1 Introduction Wizard-of-Oz (WOz) studies have long been used for spoken dialogue system design. In a relatively new variant, a subject (the wizard) is presented with real or simulated automated speech recognition (ASR) to observe how people deal with incorrect speech recognition output (Rieser, KruijffKorbayová, &amp; Lemon, 2005; Skantze, 2003; Stuttle, Williams, &amp; Young, 2004; Williams &amp; Young, 2003, 2004; Zollo, 1999). In these experiments, when a wizard could not interpret the ASR output (non-understanding), she rarely asked users to repeat themselves. Instead, the wizard found other ways to continue the task. This paper describes an experiment that presented wizards with ASR results for utterances whose interpretation is critical to task success: requests for books from a library database, identified by title. To avoid non-understandings, wizards used voice search (Wang et al., 2008): they directly queried the application datab</context>
<context position="9564" citStr="Skantze, 2003" startWordPosition="1476" endWordPosition="1477">e added to Let’s Go Public! improved concept recognition in postconfirmation user utterances (Stoyanchev &amp; Stent, 2009). In (Komatani et al., 2005), a shallow semantic interpretation phase was followed by decision trees to classify utterances as relevant either to query type or to specific query slots, to narrow the set of possible interpretations. CheckItOut is most similar in spirit to the latter approach, but relies on the database earlier, and only for semantic interpretation, not to also guide the dialogue strategy. Our approach to noisy ASR is inspired by previous WOz studies with real (Skantze, 2003; Zollo, 1999) or simulated ASR (Kruijff-Korbayová et al., 2005; Rieser et al., 2005; Williams &amp; Young, 2004). Simulation makes it possible to collect dialogues without building a speech recognizer, and to control for WER. In the studies that involved task-oriented dialogues, wizards typically focused more on the task and less on resolving ASR errors (Williams &amp; Young, 2004; Skantze, 2003; Zollo, 1999). In studies more like the information-seeking dialogues addressed here, an entirely different pattern is observed (Kruijff-Korbayová et al., 2005; Rieser et al., 2005). Zollo collected seven dia</context>
</contexts>
<marker>Skantze, 2003</marker>
<rawString>Skantze, Gabriel (2003). Exploring human error handling strategies: Implications for Spoken Dialogue Systems. Proceedings of ISCA Tutorial and Research Workshp on Error Handling in Spoken Dialogue Systems, pp. 71-76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Svetlana Stoyanchev</author>
<author>Stent</author>
</authors>
<title>Predicting concept types in user corrections in dialog.</title>
<date>2009</date>
<booktitle>Proceedings of the EACL Workshop SRSL 2009, the Second Workshop on Semantic Representation of Spoken Language,</booktitle>
<pages>42--49</pages>
<contexts>
<context position="9070" citStr="Stoyanchev &amp; Stent, 2009" startWordPosition="1391" endWordPosition="1394">y ASR contrasts with many other information-seeking and transaction-based dialogue systems. Those systems typically perform 841 natural language understanding on ASR output before database query with techniques that try to improve or expand ASR output. None that we know of use voice search. For one directory service application, users spell the first three letters of surnames, and then ASR results are expanded using frequently confused phones (Georgila et al., 2003). A two-pass recognition architecture added to Let’s Go Public! improved concept recognition in postconfirmation user utterances (Stoyanchev &amp; Stent, 2009). In (Komatani et al., 2005), a shallow semantic interpretation phase was followed by decision trees to classify utterances as relevant either to query type or to specific query slots, to narrow the set of possible interpretations. CheckItOut is most similar in spirit to the latter approach, but relies on the database earlier, and only for semantic interpretation, not to also guide the dialogue strategy. Our approach to noisy ASR is inspired by previous WOz studies with real (Skantze, 2003; Zollo, 1999) or simulated ASR (Kruijff-Korbayová et al., 2005; Rieser et al., 2005; Williams &amp; Young, 20</context>
</contexts>
<marker>Stoyanchev, Stent, 2009</marker>
<rawString>Stoyanchev, Svetlana; Stent, Amanda (2009). Predicting concept types in user corrections in dialog. Proceedings of the EACL Workshop SRSL 2009, the Second Workshop on Semantic Representation of Spoken Language, pp. 42-49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markku Turunen</author>
</authors>
<title>Hakulinen, Jaakko; Kainulainen, Anssi</title>
<date>2006</date>
<booktitle>Ninth International Conference on Spoken Language Processing (Interspeech</booktitle>
<marker>Turunen, 2006</marker>
<rawString>Turunen, Markku; Hakulinen, Jaakko; Kainulainen, Anssi (2006). Evaluation of a spoken dialogue system with usability tests and long-term pilot studies. Ninth International Conference on Spoken Language Processing (Interspeech 2006 - ICSLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Walker</author>
<author>D Litman</author>
<author>J Kamm</author>
<author>C A</author>
<author>A Abella</author>
</authors>
<title>Evaluating Spoken Dialogue Agents with PARADISE: Two Case Studies.</title>
<date>1998</date>
<journal>Computer Speech and Language,</journal>
<volume>12</volume>
<pages>317--348</pages>
<contexts>
<context position="19606" citStr="Walker et al., 1998" startWordPosition="3104" endWordPosition="3107">estions were recorded. The wizard’s GUI showed the success or failure of each title cycle before the next one began. The user’s GUI posted the 20 titles to be read during the session. On the GUI, the user rated the wizard’s title choices as correct or incorrect. Titles were highlighted green if the user judged a wizard’s offered title correct, red if incorrect, yellow if in progress, and not highlighted if still pending. The user also rated the wizard’s questions. Average elapsed time for each 20-title session was 15.5 minutes. A questionnaire similar to the type used in PARADISE evaluations (Walker et al., 1998) was administered to wizards and users for each pair of sessions. On a 5-point Likert scale, the average response to the question “I found the system easy to use this time” was 4 (sd=0; 4=Agree), indicating that participants were comfortable with the task. All other questions received an average score of Neutral (3) or Disagree (2). For example, participants were neutral (3) regarding confidence in guessing the correct title, and disagreed (2) that they became more confident as time went on. 6 Learning Method and Goals To model wizard actions, we assembled 60 features that would be available a</context>
</contexts>
<marker>Walker, Litman, Kamm, A, Abella, 1998</marker>
<rawString>Walker, M A.; Litman, D, J.; Kamm, C. A.; Abella, A. (1998). Evaluating Spoken Dialogue Agents with PARADISE: Two Case Studies. Computer Speech and Language, 12, 317-348.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ye-Yi Wang</author>
<author>Dong Yu</author>
<author>Ju</author>
</authors>
<title>An introduction to voice search.</title>
<date>2008</date>
<journal>IEEE Signal Process. Magazine,</journal>
<volume>25</volume>
<issue>3</issue>
<location>Yun-Cheng; Acero, Alex</location>
<contexts>
<context position="2328" citStr="Wang et al., 2008" startWordPosition="333" endWordPosition="336"> output (Rieser, KruijffKorbayová, &amp; Lemon, 2005; Skantze, 2003; Stuttle, Williams, &amp; Young, 2004; Williams &amp; Young, 2003, 2004; Zollo, 1999). In these experiments, when a wizard could not interpret the ASR output (non-understanding), she rarely asked users to repeat themselves. Instead, the wizard found other ways to continue the task. This paper describes an experiment that presented wizards with ASR results for utterances whose interpretation is critical to task success: requests for books from a library database, identified by title. To avoid non-understandings, wizards used voice search (Wang et al., 2008): they directly queried the application database with ASR output. To investigate how to avoid errors in understanding (misunderstandings), we examined how wizards dealt with uncertainty in voice search results. When the voice search results included the requested title, all seven of our wizards were likely to identify it. One wizard, however, recognized far better than the others when the voice search results did not contain the requested title. The experiment employed a novel design that made it possible to include system features in models of wizard behavior. The principal result is that our</context>
</contexts>
<marker>Wang, Yu, Ju, 2008</marker>
<rawString>Wang, Ye-Yi; Yu, Dong; Ju, Yun-Cheng; Acero, Alex (2008). An introduction to voice search. IEEE Signal Process. Magazine, 25(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wayne Ward</author>
<author>Issar</author>
</authors>
<title>Recent improvements in the CMU spoken language understanding system.ARPA Human Language Technology Workshop,</title>
<date>1994</date>
<location>Plainsboro, NJ.</location>
<contexts>
<context position="12960" citStr="Ward &amp; Issar, 1994" startWordPosition="2001" endWordPosition="2004">udio format, travel by mail. In a dialogue with CheckItOut, a user identifies herself, requests books, and is told which are available for immediate shipment or will go on reserve. The user can request a book by catalogue number, title, or author. CheckItOut builds on the Olympus/RavenClaw framework (Bohus &amp; Rudnicky, 2009) that has been the basis for about a dozen dialogue systems in different domains, including Let’s Go Public! (Raux et al., 2005). Speech recognition relies on PocketSphinx. Phoenix, a robust context-free grammar (CFG) semantic parser, handles natural language understanding (Ward &amp; Issar, 1994). The Apollo interaction manager (Raux &amp; Eskenazi, 2007) detects utterance boundaries using information from speech recognition, semantic parsing, and Helios, an utterance-level confidence annotator (Bohus &amp; Rudnicky, 2002). The dialogue manager is implemented in RavenClaw. 842 To design CheckItOut’s dialogue manager, we recorded 175 calls (4.5 hours) from patrons to librarians. We identified 82 book request calls, transcribed them, aligned the utterances with the speech signal, and annotated the transcripts for dialogue acts. Because active patrons receive monthly newsletters listing new titl</context>
</contexts>
<marker>Ward, Issar, 1994</marker>
<rawString>Ward, Wayne; Issar, Sunil (1994). Recent improvements in the CMU spoken language understanding system.ARPA Human Language Technology Workshop, Plainsboro, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason D Williams</author>
<author>Steve Young</author>
</authors>
<title>Characterising Task-oriented Dialog using a Simulated ASR Channel.</title>
<date>2004</date>
<booktitle>Eight International Conference on Spoken Language Processing (ICSLP/Interspeech),</booktitle>
<pages>185--188</pages>
<contexts>
<context position="9673" citStr="Williams &amp; Young, 2004" startWordPosition="1491" endWordPosition="1494">nchev &amp; Stent, 2009). In (Komatani et al., 2005), a shallow semantic interpretation phase was followed by decision trees to classify utterances as relevant either to query type or to specific query slots, to narrow the set of possible interpretations. CheckItOut is most similar in spirit to the latter approach, but relies on the database earlier, and only for semantic interpretation, not to also guide the dialogue strategy. Our approach to noisy ASR is inspired by previous WOz studies with real (Skantze, 2003; Zollo, 1999) or simulated ASR (Kruijff-Korbayová et al., 2005; Rieser et al., 2005; Williams &amp; Young, 2004). Simulation makes it possible to collect dialogues without building a speech recognizer, and to control for WER. In the studies that involved task-oriented dialogues, wizards typically focused more on the task and less on resolving ASR errors (Williams &amp; Young, 2004; Skantze, 2003; Zollo, 1999). In studies more like the information-seeking dialogues addressed here, an entirely different pattern is observed (Kruijff-Korbayová et al., 2005; Rieser et al., 2005). Zollo collected seven dialogues with different human-wizard pairs to develop an evacuation plan. The overall WER was 30%. Of the 227 c</context>
<context position="11966" citStr="Williams &amp; Young, 2004" startWordPosition="1842" endWordPosition="1845">derstandings or misunderstandings with a taskrelated question instead of a repair. In (Rieser et al., 2005), wizards simulated a multimodal MP3 player application with access to a database of 150K music albums. Responses could be presented verbally or graphically. In the noisy transcription condition, wizards made clarification requests about twice as often as that found in similar human-human dialogue. In a system like CheckItOut, user utterances that request database information must be understood. We seek an approach that would reduce the rate of misunderstandings observed for high WER in (Williams &amp; Young, 2004) and the rate of clarification requests observed in (Rieser et al., 2005). 4 CheckItOut and Embedded Wizards CheckItOut is modeled on library transactions at the Andrew Heiskell Braille and Talking Book Library, a branch of the New York Public Library and part of the National Library of Congress. Borrowing requests are handled by telephone. Books, mainly in a proprietary audio format, travel by mail. In a dialogue with CheckItOut, a user identifies herself, requests books, and is told which are available for immediate shipment or will go on reserve. The user can request a book by catalogue num</context>
</contexts>
<marker>Williams, Young, 2004</marker>
<rawString>Williams, Jason D.; Young, Steve (2004). Characterising Task-oriented Dialog using a Simulated ASR Channel. Eight International Conference on Spoken Language Processing (ICSLP/Interspeech), pp. 185-188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Frank</author>
</authors>
<title>Eibe</title>
<date>2005</date>
<booktitle>Data Mining: Practical Machine Learning Tools and Techniques (2nd ed.).</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<location>San Francisco:</location>
<contexts>
<context position="20864" citStr="Witten &amp; Frank, 2005" startWordPosition="3304" endWordPosition="3307">etect their relative independence, meaningfulness, and predictive ability. Features described the wizard’s GUI, the current title session, similarity between ASR and candidates, ASR relevance to the database, and recognition and confidence measures. Because the number of voice search returns varied from one title to the next, features pertaining to candidates were averaged. We used three machine-learning techniques to predict wizards’ actions: decision trees, linear regression, and logistic regression. All models were produced with the Weka data mining package, using 10-fold cross-validation (Witten &amp; Frank, 2005). A decision tree is a predictive model that maps feature values to a target value. One applies a decision tree by tracing a path from the root (the top node) to a leaf, which provides the target value. Here the leaves are the wizard actions: firm choice, tentative choice, question, or give up. The algorithm used is a version of C4.5 (Quinlan, 1993), where gain ratio is the splitting criterion. To confirm the learnability and quality of the decision tree models, we also trained logistic regression and linear regression models on the same data, normalized in [0, 1]. The logistic regression mode</context>
<context position="23617" citStr="Witten &amp; Frank, 2005" startWordPosition="3745" endWordPosition="3748">on-Offers W4 600 0.7585 0.8550 0.70 0.64 W5 600 0.7584 0.8133 0.76 0.43 W7 599 0.6971 0.7346 0.76 0.14 W1 593 0.6936 0.7319 0.79 0.16 W2 599 0.6703 0.7212 0.74 0.10 W3 581 0.6648 0.6954 0.81 0.20 W6 600 0.6103 0.6950 0.86 0.03 and represented by a single feature: three features that described the gaps between exact word matches, three that described the data presented to the wizard, nine that described various system confidence scores, and three that described the user’s speaking rate. This left 28 features. Next we ran CfsSubsetEval, a supervised attribute selection algorithm for each model (Witten &amp; Frank, 2005). This greedy, hill-climbing algorithm with backtracking evaluates a subset of attributes by the predictive ability of each feature and the degree of redundancy among them. This process further reduced the 28 features to 8-12 features per model. Finally, to reduce overfitting for decision trees, we used pruning and subtree rising. For linear regression we used the M5 method, repeatedly removing the attribute with the smallest standardized coefficient until there was no further improvement in the error estimate given by the Akaike information criterion. 7 Results Table 1 shows the number of tit</context>
</contexts>
<marker>Witten, Frank, 2005</marker>
<rawString>Witten, Ian H.; Frank, Eibe (2005). Data Mining: Practical Machine Learning Tools and Techniques (2nd ed.). San Francisco: Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Teresa Zollo</author>
</authors>
<title>A study of human dialogue strategies in the presence of speech recognition errors.</title>
<date>1999</date>
<booktitle>Proceedings of AAAI Fall Symposium on Psychological Models of Communication in Collaborative Systems,</booktitle>
<pages>132--139</pages>
<contexts>
<context position="1851" citStr="Zollo, 1999" startWordPosition="261" endWordPosition="262"> the requested title. Our learned models of the best wizard’s behavior combine features available to wizards with some that are not, such as recognition confidence and acoustic model scores. 1 Introduction Wizard-of-Oz (WOz) studies have long been used for spoken dialogue system design. In a relatively new variant, a subject (the wizard) is presented with real or simulated automated speech recognition (ASR) to observe how people deal with incorrect speech recognition output (Rieser, KruijffKorbayová, &amp; Lemon, 2005; Skantze, 2003; Stuttle, Williams, &amp; Young, 2004; Williams &amp; Young, 2003, 2004; Zollo, 1999). In these experiments, when a wizard could not interpret the ASR output (non-understanding), she rarely asked users to repeat themselves. Instead, the wizard found other ways to continue the task. This paper describes an experiment that presented wizards with ASR results for utterances whose interpretation is critical to task success: requests for books from a library database, identified by title. To avoid non-understandings, wizards used voice search (Wang et al., 2008): they directly queried the application database with ASR output. To investigate how to avoid errors in understanding (misu</context>
<context position="9578" citStr="Zollo, 1999" startWordPosition="1478" endWordPosition="1479">s Go Public! improved concept recognition in postconfirmation user utterances (Stoyanchev &amp; Stent, 2009). In (Komatani et al., 2005), a shallow semantic interpretation phase was followed by decision trees to classify utterances as relevant either to query type or to specific query slots, to narrow the set of possible interpretations. CheckItOut is most similar in spirit to the latter approach, but relies on the database earlier, and only for semantic interpretation, not to also guide the dialogue strategy. Our approach to noisy ASR is inspired by previous WOz studies with real (Skantze, 2003; Zollo, 1999) or simulated ASR (Kruijff-Korbayová et al., 2005; Rieser et al., 2005; Williams &amp; Young, 2004). Simulation makes it possible to collect dialogues without building a speech recognizer, and to control for WER. In the studies that involved task-oriented dialogues, wizards typically focused more on the task and less on resolving ASR errors (Williams &amp; Young, 2004; Skantze, 2003; Zollo, 1999). In studies more like the information-seeking dialogues addressed here, an entirely different pattern is observed (Kruijff-Korbayová et al., 2005; Rieser et al., 2005). Zollo collected seven dialogues with di</context>
</contexts>
<marker>Zollo, 1999</marker>
<rawString>Zollo, Teresa (1999). A study of human dialogue strategies in the presence of speech recognition errors. Proceedings of AAAI Fall Symposium on Psychological Models of Communication in Collaborative Systems, pp. 132-139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor Zue</author>
<author>Stephanie Seneff</author>
<author>James Glass</author>
<author>Joseph Polifroni</author>
<author>Christine Pao</author>
<author>Timothy J Hazen</author>
</authors>
<title>A Telephone-based conversational interface for weather information.</title>
<date>2000</date>
<journal>IEEE Transactions on Speech and Audio Processing,</journal>
<volume>8</volume>
<pages>85--96</pages>
<contexts>
<context position="5716" citStr="Zue et al., 2000" startWordPosition="875" endWordPosition="878">transmission quality. For example, the 68% word error rate (WER) for the fielded version of Let’s Go Public! (Raux et al., 2005) far exceeded its 17% WER under controlled conditions. Our application handles library requests by telephone, and would benefit from robustness to noisy ASR. Second, the book title field in our database differs from the typical case for spoken dialogue systems that access a relational database. Such systems include travel booking (Levin et al., 2000), bus route information (Raux et al., 2006), restaurant guides (Johnston et al., 2002; Komatani et al., 2005), weather (Zue et al., 2000) and directory services (Georgila et al., 2003). In general for these systems, a few words are sufficient to retrieve the desired attribute value, such as a neighborhood, a street, or a surname. Mean utterance length in a sample of 40,000 Let’s Go Public! utterances, for example, is 2.4 words. The average book title length in our database is 5.4 words. Finally, our dialogue system, CheckItOut, allows users to choose whether to request books by title, author, or catalogue number. The database represents 5028 active patrons (with real borrowing histories and preferences but fictitious personal i</context>
</contexts>
<marker>Zue, Seneff, Glass, Polifroni, Pao, Hazen, 2000</marker>
<rawString>Zue, Victor; Seneff, Stephanie; Glass, James; Polifroni, Joseph; Pao, Christine; Hazen, Timothy J., et al. (2000). A Telephone-based conversational interface for weather information. IEEE Transactions on Speech and Audio Processing, 8, 85-96.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>