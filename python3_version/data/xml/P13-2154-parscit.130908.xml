<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.061918">
<title confidence="0.998281">
A New Set of Norms for Semantic Relatedness Measures
</title>
<author confidence="0.998738">
Sean Szumlanski
</author>
<affiliation confidence="0.9984555">
Department of EECS
University of Central Florida
</affiliation>
<email confidence="0.991696">
seansz@cs.ucf.edu
</email>
<author confidence="0.996329">
Fernando Gomez
</author>
<affiliation confidence="0.998325">
Department of EECS
University of Central Florida
</affiliation>
<email confidence="0.991135">
gomez@eecs.ucf.edu
</email>
<author confidence="0.989358">
Valerie K. Sims
</author>
<affiliation confidence="0.998435">
Department of Psychology
University of Central Florida
</affiliation>
<email confidence="0.990359">
Valerie.Sims@ucf.edu
</email>
<sectionHeader confidence="0.993669" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99997568">
We have elicited human quantitative judg-
ments of semantic relatedness for 122
pairs of nouns and compiled them into a
new set of relatedness norms that we call
Rel-122. Judgments from individual sub-
jects in our study exhibit high average cor-
relation to the resulting relatedness means
(r = 0.77, a = 0.09, N = 73), although not
as high as Resnik’s (1995) upper bound
for expected average human correlation to
similarity means (r = 0.90). This suggests
that human perceptions of relatedness are
less strictly constrained than perceptions
of similarity and establishes a clearer ex-
pectation for what constitutes human-like
performance by a computational measure
of semantic relatedness.
We compare the results of several
WordNet-based similarity and relatedness
measures to our Rel-122 norms and
demonstrate the limitations of WordNet
for discovering general indications of
semantic relatedness. We also offer a cri-
tique of the field’s reliance upon similarity
norms to evaluate relatedness measures.
</bodyText>
<sectionHeader confidence="0.99913" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999871625">
Despite the well-established technical distinc-
tion between semantic similarity and relatedness
(Agirre et al., 2009; Budanitsky and Hirst, 2006;
Resnik, 1995), comparison to established similar-
ity norms from psychology remains part of the
standard evaluative procedure for assessing com-
putational measures of semantic relatedness. Be-
cause similarity is only one particular type of re-
latedness, comparison to similarity norms fails to
give a complete view of a relatedness measure’s
efficacy.
In keeping with Budanitsky and Hirst’s (2006)
observation that “comparison with human judg-
ments is the ideal way to evaluate a measure of
similarity or relatedness,” we have undertaken the
creation of a new set of relatedness norms.
</bodyText>
<sectionHeader confidence="0.987652" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999964863636364">
The similarity norms of Rubenstein and Goode-
nough (1965; henceforth R&amp;G) and Miller and
Charles (1991; henceforth M&amp;C) have seen ubiq-
uitous use in evaluation of computational mea-
sures of semantic similarity and relatedness.
R&amp;G established their similarity norms by pre-
senting subjects with 65 slips of paper, each of
which contained a pair of nouns. Subjects were
directed to read through all 65 noun pairs, then
sort the pairs “according to amount of ‘similarity
of meaning.’” Subjects then assigned similarity
scores to each pair on a scale of 0.0 (completely
dissimilar) to 4.0 (strongly synonymous).
The R&amp;G results have proven to be highly repli-
cable. M&amp;C repeated R&amp;G’s study using a subset
of 30 of the original word pairs, and their resulting
similarity norms correlated to the R&amp;G norms at
r = 0.97. Resnik’s (1995) subsequent replication
of M&amp;C’s study similarly yielded a correlation of
r = 0.96. The M&amp;C pairs were also included in a
similarity study by Finkelstein et al. (2002), which
yielded correlation of r = 0.95 to the M&amp;C norms.
</bodyText>
<subsectionHeader confidence="0.998351">
2.1 WordSim353
</subsectionHeader>
<bodyText confidence="0.997571444444444">
WordSim353 (Finkelstein et al., 2002) has re-
cently emerged as a potential surrogate dataset for
evaluating relatedness measures. Several studies
have reported correlation to WordSim353 norms
as part of their evaluation procedures, with some
studies explicitly referring to it as a collection of
human-assigned relatedness scores (Gabrilovich
and Markovitch, 2007; Hughes and Ramage,
2007; Milne and Witten, 2008).
</bodyText>
<page confidence="0.959544">
890
</page>
<bodyText confidence="0.953663666666667">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 890–895,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
Yet, the instructions presented to Finkelstein et
al.’s subjects give us pause to reconsider Word-
Sim353’s classification as a set of relatedness
norms. They repeatedly framed the task as one in
which subjects were expected to assign word simi-
larity scores, although participants were instructed
to extend their definition of similarity to include
antonymy, which perhaps explains why the au-
thors later referred to their data as “relatedness”
norms rather than merely “similarity” norms.
Jarmasz and Szpakowicz (2003) have raised fur-
ther methodological concerns about the construc-
tion of WordSim353, including: (a) similarity was
rated on a scale of 0.0 to 10.0, which is intrin-
sically more difficult for humans to manage than
the scale of 0.0 to 4.0 used by R&amp;G and M&amp;C,
and (b) the inclusion of proper nouns introduced
an element of cultural bias into the dataset (e.g.,
the evaluation of the pair Arafat–terror).
Cognizant of the problematic conflation of sim-
ilarity and relatedness in WordSim353, Agirre et
al. (2009) partitioned the data into two sets:
one containing noun pairs exhibiting similarity,
and one containing pairs of related but dissimilar
nouns. However, pairs in the latter set were not
assessed for scoring distribution validity to ensure
that strongly related word pairs were not penalized
by human subjects for being dissimilar.1
</bodyText>
<sectionHeader confidence="0.998488" genericHeader="method">
3 Methodology
</sectionHeader>
<bodyText confidence="0.999948944444444">
In our experiments, we elicited human ratings of
semantic relatedness for 122 noun pairs. In doing
so, we followed the methodology of Rubenstein
and Goodenough (1965) as closely as possible:
participants were instructed to read through a set
of noun pairs, sort them by how strongly related
they were, and then assign each pair a relatedness
score on a scale of 0.0 (“completely unrelated”) to
4.0 (“very strongly related”).
We made two notable modifications to the ex-
perimental procedure of Rubenstein and Goode-
nough. First, instead of asking participants to
judge “amount of ‘similarity of meaning,’” we
asked them to judge “how closely related in mean-
ing” each pair of nouns was. Second, we used a
Web interface to collect data in our study; instead
of reordering a deck of cards, participants were
presented with a grid of cards that they were able
</bodyText>
<footnote confidence="0.980528666666667">
1Perhaps not surprisingly, the highest scores in Word-
Sim353 (all ratings from 9.0 to 10.0) were assigned to pairs
that Agirre et al. placed in their similarity partition.
</footnote>
<bodyText confidence="0.985715">
to rearrange interactively with the use of a mouse
or any touch-enabled device, such as a tablet PC.2
</bodyText>
<subsectionHeader confidence="0.996378">
3.1 Experimental Conditions
</subsectionHeader>
<bodyText confidence="0.999974175">
Each participant in our study was randomly as-
signed to one of four conditions. Each condition
contained 32 noun pairs for evaluation.
Of those pairs, 10 were randomly selected
from from WordNet++ (Ponzetto and Navigli,
2010) and 10 from SGN (Szumlanski and Gomez,
2010)—two semantic networks that categori-
cally indicate strong relatedness between Word-
Net noun senses. 10 additional pairs were gen-
erated by randomly pairing words from a list of
all nouns occurring in Wikipedia. The nouns
in the pairs we used from each of these three
sources were matched for frequency of occurrence
in Wikipedia.
We manually selected two additional pairs that
appeared across all four conditions: leaves–rake
and lion–cage. These control pairs were included
to ensure that each condition contained examples
of strong semantic relatedness, and potentially to
help identify and eliminate data from participants
who assigned random relatedness scores. Within
each condition, the 32 word pairs were presented
to all subjects in the same random order. Across
conditions, the two control pairs were always pre-
sented in the same positions in the word pair grid.
Each word pair was subjected to additional
scrutiny before being included in our dataset. We
eliminated any pairs falling into one or more
of the following categories: (a) pairs containing
proper nouns, (b) pairs in which one or both nouns
might easily be mistaken for adjectives or verbs,
(c) pairs with advanced vocabulary or words that
might require domain-specific knowledge in or-
der to be properly evaluated, and (d) pairs with
shared stems or common head nouns (e.g., first
cousin–second cousin and sinner–sinning). The
latter were eliminated to prevent subjects from
latching onto superficial lexical commonalities as
indicators of strong semantic relatedness without
reflecting upon meaning.
</bodyText>
<subsectionHeader confidence="0.997088">
3.2 Participants
</subsectionHeader>
<bodyText confidence="0.9985295">
Participants in our study were recruited from in-
troductory undergraduate courses in psychology
and computer science at the University of Cen-
tral Florida. Students from the psychology courses
</bodyText>
<footnote confidence="0.993512">
2Online demo: http://www.cs.ucf.edu/∼seansz/rel-122
</footnote>
<page confidence="0.996168">
891
</page>
<bodyText confidence="0.998873035714286">
participated for course credit and accounted for
89% of respondents.
92 participants provided data for our study. Of
these, we identified 19 as outliers, and their data
were excluded from our norms to prevent interfer-
ence from individuals who appeared to be assign-
ing random scores to noun pairs. We considered
an outlier to be any individual whose numeric rat-
ings fell outside two standard deviations from the
means for more than 10% of the word pairs they
evaluated (i.e., at least four word pairs, since each
condition contained 32 word pairs).
For outlier detection, means and standard de-
viations were computed using leave-one-out sam-
pling. That is, data from individual J were not in-
corporated into means or standard deviations when
considering whether to eliminate J as an outlier.3
Of the 73 participants remaining after outlier
elimination, there was a near-even split between
males (37) and females (35), with one individual
declining to provide any demographic data. The
average age of participants was 20.32 (Q = 4.08,
N = 72). Most students were freshmen (49), fol-
lowed in frequency by sophomores (16), seniors
(4), and juniors (3). Participants earned an average
score of 42% on a standardized test of advanced
vocabulary (Q = 16%, N = 72) (Test I – V-4 from
Ekstrom et al. (1976)).
</bodyText>
<sectionHeader confidence="0.999963" genericHeader="method">
4 Results
</sectionHeader>
<bodyText confidence="0.999120214285714">
Each word pair in Rel-122 was evaluated by at
least 20 human subjects. After outlier removal
(described above), each word pair retained eval-
uations from 14 to 22 individuals. The resulting
relatedness means are available online.4
An excerpt of the Rel-122 norms is shown in
Table 1. We note that the highest rated pairs in our
dataset are not strictly similar entities; exactly half
of the 10 most strongly related nouns in Table 1 are
dissimilar (e.g., digital camera–photographer).
Judgments from individual subjects in our study
exhibited high average correlation to the elicited
relatedness means (r = 0.769, Q = 0.09, N =
73). Resnik (1995), in his replication of the
</bodyText>
<footnote confidence="0.99570625">
3We used this sampling method to prevent extreme out-
liers from masking their own aberration during outlier de-
tection, which is potentially problematic when dealing with
small populations. Without leave-one-out-sampling, we
would have identified fewer outliers (14 instead of 19), but
the resulting means would still have correlated strongly to
our final relatedness norms (r = 0.991, p &lt; 0.01).
4http://www.cs.ucf.edu/∼seansz/rel-122
</footnote>
<table confidence="0.999888846153846">
# Word Pair µ
underwear lingerie 3.94
digital camera photographer 3.85
tuition fee 3.85
leaves rake 3.82
symptom fever 3.79
fertility ovary 3.78
beef slaughterhouse 3.78
broadcast commentator 3.75
apparel jewellery 3.72
arrest detention 3.69
. . .
122. gladiator plastic bag 0.13
</table>
<tableCaption confidence="0.999838">
Table 1: Excerpt of Rel-122 norms.
</tableCaption>
<bodyText confidence="0.995772181818182">
M&amp;C study, reported average individual correla-
tion of r = 0.90 (Q = 0.07, N = 10) to similar-
ity means elicited from a population of 10 gradu-
ate students and postdoctoral researchers. Presum-
ably Resnik’s subjects had advanced knowledge of
what constitutes semantic similarity, as he estab-
lished r = 0.90 as an upper bound for expected
human correlation on that task.
The fact that average human correlation in our
study is weaker than in previous studies suggests
that human perceptions of relatedness are less
strictly constrained than perceptions of similarity,
and that a reasonable computational measure of re-
latedness might only approach a correlation of r =
0.769 to relatedness norms.
In Table 2, we present the performance of a va-
riety of relatedness and similarity measures on our
new set of relatedness means.5 Coefficients of cor-
relation are given for Pearson’s product-moment
correlation (r), as well as Spearman’s rank corre-
lation (p). For comparison, we include results for
the correlation of these measures to the M&amp;C and
R&amp;G similarity means.
The generally weak performance of the
WordNet-based measures on this task is not
surprising, given WordNet’s strong disposition
toward codifying semantic similarity, which
makes it an impoverished resource for discovering
general semantic relatedness. We note that the
three WordNet-based measures from Table 2
that are regarded in the literature as relatedness
measures (Banerjee and Pedersen, 2003; Hirst and
St-Onge, 1998; Patwardhan and Pedersen, 2006)
</bodyText>
<footnote confidence="0.996903">
5Results based on standard implementations in the Word-
Net::Similarity Perl module of Pedersen et al. (2004) (v2.05).
</footnote>
<page confidence="0.990715">
892
</page>
<table confidence="0.973796666666667">
Rel-122 M&amp;C R&amp;G
Measure r p r p r p
* Szumlanski and Gomez (2010) 0.654 0.534 0.852 0.859 0.824 0.841
* Patwardhan and Pedersen (2006) 0.341 0.364 0.865 0.906 0.793 0.795
Path Length 0.225 0.183 0.755 0.715 0.784 0.783
* Banerjee and Pedersen (2003) 0.210 0.258 0.356 0.804 0.340 0.718
Resnik (1995) 0.203 0.182 0.806 0.741 0.822 0.757
Jiang and Conrath (1997) 0.188 0.133 0.473 0.663 0.575 0.592
Leacock and Chodorow (1998) 0.173 0.167 0.779 0.715 0.839 0.783
Wu and Palmer (1994) 0.187 0.180 0.764 0.732 0.797 0.768
Lin (1998) 0.145 0.148 0.739 0.687 0.726 0.636
* Hirst and St-Onge (1998) 0.141 0.160 0.667 0.782 0.726 0.797
</table>
<tableCaption confidence="0.735458333333333">
Table 2: Correlation of similarity and relatedness measures to Rel-122, M&amp;C, and R&amp;G. Starred rows
(*) are considered relatedness measures. All measures are WordNet-based, except for the scoring metric
of Szumlanski and Gomez (2010), which is based on lexical co-occurrence frequency in Wikipedia.
</tableCaption>
<table confidence="0.9889953125">
# Noun Pair Sim. Rel. # Noun Pair Sim. Rel.
1. car automobile 3.92 4.00 16. lad brother 1.66 2.68
2. gem jewel 3.84 3.98 17. journey car 1.16 3.00
3. journey voyage 3.84 3.97 18. monk oracle 1.10 2.54
4. boy lad 3.76 3.97 19. cemetery woodland 0.95 1.69
5. coast shore 3.70 3.97 20. food rooster 0.89 2.59
6. asylum madhouse 3.61 3.91 21. coast hill 0.87 1.59
7. magician wizard 3.50 3.58 22. forest graveyard 0.84 2.01
8. midday noon 3.42 4.00 23. shore woodland 0.63 1.63
9. furnace stove 3.11 3.67 24. monk slave 0.55 1.31
10. food fruit 3.08 3.91 25. coast forest 0.42 1.89
11. bird cock 3.05 3.71 26. lad wizard 0.42 2.12
12. bird crane 2.97 3.96 27. chord smile 0.13 0.68
13. tool implement 2.95 2.86 28. glass magician 0.11 1.30
14. brother monk 2.82 2.89 29. rooster voyage 0.08 0.63
15. crane implement 1.68 0.90 30. noon string 0.08 0.14
</table>
<tableCaption confidence="0.99994">
Table 3: Comparison of relatedness means to M&amp;C similarity means. Correlation is r = 0.91.
</tableCaption>
<bodyText confidence="0.998654">
have been hampered by their reliance upon Word-
Net. The disparity between their performance on
Rel-122 and the M&amp;C and R&amp;G norms suggests
the shortcomings of using similarity norms for
evaluating measures of relatedness.
</bodyText>
<sectionHeader confidence="0.792216" genericHeader="method">
5 (Re-)Evaluating Similarity Norms
</sectionHeader>
<bodyText confidence="0.999525857142857">
After establishing our relatedness norms, we cre-
ated two additional experimental conditions in
which subjects evaluated the relatedness of noun
pairs from the M&amp;C study. Each condition again
had 32 noun pairs: 15 from M&amp;C and 17 from
Rel-122. Pairs from M&amp;C and Rel-122 were uni-
formly distributed between these two new condi-
tions based on matched normative similarity or re-
latedness scores from their respective datasets.
Results from this second phase of our study are
shown in Table 3. The correlation of our relat-
edness means on this set to the similarity means
of M&amp;C was strong (r = 0.91), but not as strong
as in replications of the study that asked subjects
to evaluate similarity (e.g. r = 0.96 in Resnik’s
(1995) replication and r = 0.95 in Finkelstein et
al.’s (2002) M&amp;C subset).
That the synonymous M&amp;C pairs garner high
relatedness ratings in our study is not surprising;
strong similarity is, after all, one type of strong
relatedness. The more interesting result from
</bodyText>
<page confidence="0.997793">
893
</page>
<bodyText confidence="0.999945571428571">
our study, shown in Table 3, is that relatedness
norms for pairs that are related but dissimilar (e.g.,
journey–car and forest–graveyard) deviate signif-
icantly from established similarity norms. This in-
dicates that asking subjects to evaluate “similar-
ity” instead of “relatedness” can significantly im-
pact the norms established in such studies.
</bodyText>
<sectionHeader confidence="0.996301" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999983392857143">
We have established a new set of relatedness
norms, Rel-122, that is offered as a supplementary
evaluative standard for assessing semantic related-
ness measures.
We have also demonstrated the shortcomings
of using similarity norms to evaluate such mea-
sures. Namely, since similarity is only one type of
relatedness, comparison to similarity norms fails
to provide a complete view of a measure’s abil-
ity to capture more general types of relatedness.
This is particularly problematic when evaluating
WordNet-based measures, which naturally excel at
capturing similarity, given the nature of the Word-
Net ontology.
Furthermore, we have found that asking judges
to evaluate “relatedness” of terms, rather than
“similarity,” has a substantive impact on resulting
norms, particularly with respect to the M&amp;C sim-
ilarity dataset. Correlation of individual judges’
ratings to resulting means was also significantly
lower on average in our study than in previous
studies that focused on similarity (e.g., Resnik,
1995). These results suggest that human percep-
tions of relatedness are less strictly constrained
than perceptions of similarity and validate the
need for new relatedness norms to supplement ex-
isting gold standard similarity norms in the evalu-
ation of relatedness measures.
</bodyText>
<sectionHeader confidence="0.998423" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999263815384615">
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas¸ca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and WordNet-based approaches. In Proceed-
ings of the North American Chapter of the Associa-
tion for Computational Linguistics (NAACL), pages
19–27.
Satanjeev Banerjee and Ted Pedersen. 2003. Ex-
tended gloss overlaps as a measure of semantic re-
latedness. In Proceedings of the 18th International
Joint Conference on Artificial Intelligence (IJCAI),
pages 805–810.
Alexander Budanitsky and Graeme Hirst. 2006. Eval-
uating WordNet-based measures of lexical semantic
relatedness. Computational Linguistics, 32(1):13–
47.
Ruth B. Ekstrom, John W. French, Harry H. Harman,
and Diran Dermen. 1976. Manual for Kit of Factor-
Referenced Cognitive Tests. Educational Testing
Service, Princeton, NJ.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2002. Placing search in context: The
concept revisited. ACM Transactions on Informa-
tion Systems (TOIS), 20(1):116–131.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using Wikipedia-
based explicit semantic analysis. In Proceedings of
the 20th International Joint Conference on Artificial
Intelligence, pages 1606–1611.
Graeme Hirst and David St-Onge. 1998. Lexical
chains as representations of context for the detec-
tion and correction of malapropisms. In Christiane
Fellbaum, editor, WordNet: An Electronic Lexical
Database, pages 305–332. MIT Press.
Thad Hughes and Daniel Ramage. 2007. Lexi-
cal semantic relatedness with random graph walks.
In Proceedings of the 2007 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 581–589, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Mario Jarmasz and Stan Szpakowicz. 2003. Roget’s
thesaurus and semantic similarity. In Proceedings of
the International Conference on Recent Advances in
Natural Language Processing (RANLP), pages 212–
219.
Jay J. Jiang and David W. Conrath. 1997. Semantic
similarity based on corpus statistics and lexical tax-
onomy. In Proceedings of the International Confer-
ence on Research in Computational Linguistics (RO-
CLING), pages 19–33.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and WordNet similarity for
word sense identification. In Christiane Fellbaum,
editor, WordNet: An Electronic Lexical Database,
pages 265–283. MIT Press.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the 15th Inter-
national Conference on Machine Learning (ICML),
pages 296–304.
George A. Miller and Walter G. Charles. 1991. Con-
textual correlates of semantic similarity. Language
and Cognitive Processes, 6(1):1–28.
</reference>
<page confidence="0.988566">
894
</page>
<reference confidence="0.999403684210526">
David Milne and Ian H. Witten. 2008. An effective,
low-cost measure of semantic relatedness obtained
from Wikipedia links. In Proceedings of the First
AAAI Workshop on Wikipedia and Artificial Intelli-
gence (WIKIAI), pages 25–30.
Siddharth Patwardhan and Ted Pedersen. 2006. Us-
ing WordNet-based context vectors to estimate the
semantic relatedness of concepts. In Proceedings
of the 11th Conference of the European Chapter of
the Association for Computational Linguistics Work-
shop on Making Sense of Sense, pages 1–8.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. WordNet::Similarity – Measuring
the relatedness of concepts. In Proceedings of the
5th Annual Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL), pages 38–11.
Simone Paolo Ponzetto and Roberto Navigli. 2010.
Knowledge-rich word sense disambiguation rivaling
supervised systems. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 1522–1531.
Philip Resnik. 1995. Using information content to
evaluate semantic similarity in a taxonomy. In Pro-
ceedings of the 14th International Joint Conference
on Artificial Intelligence (IJCAI), pages 448–453.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Communica-
tions of the ACM, 8(10):627–633.
Sean Szumlanski and Fernando Gomez. 2010. Au-
tomatically acquiring a semantic network of related
concepts. In Proceedings of the 19th ACM Inter-
national Conference on Information and Knowledge
Management (CIKM), pages 19–28.
Zhibiao Wu and Martha Palmer. 1994. Verb seman-
tics and lexical selection. In Proceedings of the
32nd Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 133–139.
</reference>
<page confidence="0.998998">
895
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.942636">
<title confidence="0.999969">A New Set of Norms for Semantic Relatedness Measures</title>
<author confidence="0.999523">Sean</author>
<affiliation confidence="0.999627">Department of University of Central</affiliation>
<email confidence="0.999552">seansz@cs.ucf.edu</email>
<author confidence="0.991825">Fernando</author>
<affiliation confidence="0.9997455">Department of University of Central</affiliation>
<email confidence="0.998993">gomez@eecs.ucf.edu</email>
<author confidence="0.996942">K Valerie</author>
<affiliation confidence="0.9998585">Department of University of Central</affiliation>
<email confidence="0.994026">Valerie.Sims@ucf.edu</email>
<abstract confidence="0.998531076923077">We have elicited human quantitative judgments of semantic relatedness for 122 pairs of nouns and compiled them into a new set of relatedness norms that we call Rel-122. Judgments from individual subjects in our study exhibit high average correlation to the resulting relatedness means 0.77, 0.09, 73), although not as high as Resnik’s (1995) upper bound for expected average human correlation to means 0.90). This suggests that human perceptions of relatedness are less strictly constrained than perceptions of similarity and establishes a clearer expectation for what constitutes human-like performance by a computational measure of semantic relatedness. We compare the results of several WordNet-based similarity and relatedness measures to our Rel-122 norms and demonstrate the limitations of WordNet for discovering general indications of semantic relatedness. We also offer a critique of the field’s reliance upon similarity norms to evaluate relatedness measures.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Enrique Alfonseca</author>
<author>Keith Hall</author>
<author>Jana Kravalova</author>
<author>Marius Pas¸ca</author>
<author>Aitor Soroa</author>
</authors>
<title>A study on similarity and relatedness using distributional and WordNet-based approaches.</title>
<date>2009</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL),</booktitle>
<pages>pages</pages>
<marker>Agirre, Alfonseca, Hall, Kravalova, Pas¸ca, Soroa, 2009</marker>
<rawString>Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kravalova, Marius Pas¸ca, and Aitor Soroa. 2009. A study on similarity and relatedness using distributional and WordNet-based approaches. In Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL), pages 19–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>Extended gloss overlaps as a measure of semantic relatedness.</title>
<date>2003</date>
<booktitle>In Proceedings of the 18th International Joint Conference on Artificial Intelligence (IJCAI),</booktitle>
<pages>805--810</pages>
<contexts>
<context position="12550" citStr="Banerjee and Pedersen, 2003" startWordPosition="1958" endWordPosition="1961">ts of correlation are given for Pearson’s product-moment correlation (r), as well as Spearman’s rank correlation (p). For comparison, we include results for the correlation of these measures to the M&amp;C and R&amp;G similarity means. The generally weak performance of the WordNet-based measures on this task is not surprising, given WordNet’s strong disposition toward codifying semantic similarity, which makes it an impoverished resource for discovering general semantic relatedness. We note that the three WordNet-based measures from Table 2 that are regarded in the literature as relatedness measures (Banerjee and Pedersen, 2003; Hirst and St-Onge, 1998; Patwardhan and Pedersen, 2006) 5Results based on standard implementations in the WordNet::Similarity Perl module of Pedersen et al. (2004) (v2.05). 892 Rel-122 M&amp;C R&amp;G Measure r p r p r p * Szumlanski and Gomez (2010) 0.654 0.534 0.852 0.859 0.824 0.841 * Patwardhan and Pedersen (2006) 0.341 0.364 0.865 0.906 0.793 0.795 Path Length 0.225 0.183 0.755 0.715 0.784 0.783 * Banerjee and Pedersen (2003) 0.210 0.258 0.356 0.804 0.340 0.718 Resnik (1995) 0.203 0.182 0.806 0.741 0.822 0.757 Jiang and Conrath (1997) 0.188 0.133 0.473 0.663 0.575 0.592 Leacock and Chodorow (19</context>
</contexts>
<marker>Banerjee, Pedersen, 2003</marker>
<rawString>Satanjeev Banerjee and Ted Pedersen. 2003. Extended gloss overlaps as a measure of semantic relatedness. In Proceedings of the 18th International Joint Conference on Artificial Intelligence (IJCAI), pages 805–810.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Budanitsky</author>
<author>Graeme Hirst</author>
</authors>
<title>Evaluating WordNet-based measures of lexical semantic relatedness.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>1</issue>
<pages>47</pages>
<contexts>
<context position="1477" citStr="Budanitsky and Hirst, 2006" startWordPosition="212" endWordPosition="215">tions of similarity and establishes a clearer expectation for what constitutes human-like performance by a computational measure of semantic relatedness. We compare the results of several WordNet-based similarity and relatedness measures to our Rel-122 norms and demonstrate the limitations of WordNet for discovering general indications of semantic relatedness. We also offer a critique of the field’s reliance upon similarity norms to evaluate relatedness measures. 1 Introduction Despite the well-established technical distinction between semantic similarity and relatedness (Agirre et al., 2009; Budanitsky and Hirst, 2006; Resnik, 1995), comparison to established similarity norms from psychology remains part of the standard evaluative procedure for assessing computational measures of semantic relatedness. Because similarity is only one particular type of relatedness, comparison to similarity norms fails to give a complete view of a relatedness measure’s efficacy. In keeping with Budanitsky and Hirst’s (2006) observation that “comparison with human judgments is the ideal way to evaluate a measure of similarity or relatedness,” we have undertaken the creation of a new set of relatedness norms. 2 Background The s</context>
</contexts>
<marker>Budanitsky, Hirst, 2006</marker>
<rawString>Alexander Budanitsky and Graeme Hirst. 2006. Evaluating WordNet-based measures of lexical semantic relatedness. Computational Linguistics, 32(1):13– 47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruth B Ekstrom</author>
<author>John W French</author>
<author>Harry H Harman</author>
<author>Diran Dermen</author>
</authors>
<date>1976</date>
<institution>Manual for Kit of FactorReferenced Cognitive Tests. Educational Testing Service,</institution>
<location>Princeton, NJ.</location>
<contexts>
<context position="9658" citStr="Ekstrom et al. (1976)" startWordPosition="1506" endWordPosition="1509">al J were not incorporated into means or standard deviations when considering whether to eliminate J as an outlier.3 Of the 73 participants remaining after outlier elimination, there was a near-even split between males (37) and females (35), with one individual declining to provide any demographic data. The average age of participants was 20.32 (Q = 4.08, N = 72). Most students were freshmen (49), followed in frequency by sophomores (16), seniors (4), and juniors (3). Participants earned an average score of 42% on a standardized test of advanced vocabulary (Q = 16%, N = 72) (Test I – V-4 from Ekstrom et al. (1976)). 4 Results Each word pair in Rel-122 was evaluated by at least 20 human subjects. After outlier removal (described above), each word pair retained evaluations from 14 to 22 individuals. The resulting relatedness means are available online.4 An excerpt of the Rel-122 norms is shown in Table 1. We note that the highest rated pairs in our dataset are not strictly similar entities; exactly half of the 10 most strongly related nouns in Table 1 are dissimilar (e.g., digital camera–photographer). Judgments from individual subjects in our study exhibited high average correlation to the elicited rela</context>
</contexts>
<marker>Ekstrom, French, Harman, Dermen, 1976</marker>
<rawString>Ruth B. Ekstrom, John W. French, Harry H. Harman, and Diran Dermen. 1976. Manual for Kit of FactorReferenced Cognitive Tests. Educational Testing Service, Princeton, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Finkelstein</author>
<author>Evgeniy Gabrilovich</author>
<author>Yossi Matias</author>
<author>Ehud Rivlin</author>
<author>Zach Solan</author>
<author>Gadi Wolfman</author>
<author>Eytan Ruppin</author>
</authors>
<title>Placing search in context: The concept revisited.</title>
<date>2002</date>
<journal>ACM Transactions on Information Systems (TOIS),</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="3064" citStr="Finkelstein et al. (2002)" startWordPosition="467" endWordPosition="470">rected to read through all 65 noun pairs, then sort the pairs “according to amount of ‘similarity of meaning.’” Subjects then assigned similarity scores to each pair on a scale of 0.0 (completely dissimilar) to 4.0 (strongly synonymous). The R&amp;G results have proven to be highly replicable. M&amp;C repeated R&amp;G’s study using a subset of 30 of the original word pairs, and their resulting similarity norms correlated to the R&amp;G norms at r = 0.97. Resnik’s (1995) subsequent replication of M&amp;C’s study similarly yielded a correlation of r = 0.96. The M&amp;C pairs were also included in a similarity study by Finkelstein et al. (2002), which yielded correlation of r = 0.95 to the M&amp;C norms. 2.1 WordSim353 WordSim353 (Finkelstein et al., 2002) has recently emerged as a potential surrogate dataset for evaluating relatedness measures. Several studies have reported correlation to WordSim353 norms as part of their evaluation procedures, with some studies explicitly referring to it as a collection of human-assigned relatedness scores (Gabrilovich and Markovitch, 2007; Hughes and Ramage, 2007; Milne and Witten, 2008). 890 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 890–895, Sofia</context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, Ruppin, 2002</marker>
<rawString>Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. 2002. Placing search in context: The concept revisited. ACM Transactions on Information Systems (TOIS), 20(1):116–131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeniy Gabrilovich</author>
<author>Shaul Markovitch</author>
</authors>
<title>Computing semantic relatedness using Wikipediabased explicit semantic analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of the 20th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>1606--1611</pages>
<contexts>
<context position="3499" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="530" endWordPosition="533">s at r = 0.97. Resnik’s (1995) subsequent replication of M&amp;C’s study similarly yielded a correlation of r = 0.96. The M&amp;C pairs were also included in a similarity study by Finkelstein et al. (2002), which yielded correlation of r = 0.95 to the M&amp;C norms. 2.1 WordSim353 WordSim353 (Finkelstein et al., 2002) has recently emerged as a potential surrogate dataset for evaluating relatedness measures. Several studies have reported correlation to WordSim353 norms as part of their evaluation procedures, with some studies explicitly referring to it as a collection of human-assigned relatedness scores (Gabrilovich and Markovitch, 2007; Hughes and Ramage, 2007; Milne and Witten, 2008). 890 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 890–895, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Yet, the instructions presented to Finkelstein et al.’s subjects give us pause to reconsider WordSim353’s classification as a set of relatedness norms. They repeatedly framed the task as one in which subjects were expected to assign word similarity scores, although participants were instructed to extend their definition of similarity to include antonymy, </context>
</contexts>
<marker>Gabrilovich, Markovitch, 2007</marker>
<rawString>Evgeniy Gabrilovich and Shaul Markovitch. 2007. Computing semantic relatedness using Wikipediabased explicit semantic analysis. In Proceedings of the 20th International Joint Conference on Artificial Intelligence, pages 1606–1611.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graeme Hirst</author>
<author>David St-Onge</author>
</authors>
<title>Lexical chains as representations of context for the detection and correction of malapropisms.</title>
<date>1998</date>
<booktitle>In Christiane Fellbaum, editor, WordNet: An Electronic Lexical Database,</booktitle>
<pages>305--332</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="12575" citStr="Hirst and St-Onge, 1998" startWordPosition="1962" endWordPosition="1965">or Pearson’s product-moment correlation (r), as well as Spearman’s rank correlation (p). For comparison, we include results for the correlation of these measures to the M&amp;C and R&amp;G similarity means. The generally weak performance of the WordNet-based measures on this task is not surprising, given WordNet’s strong disposition toward codifying semantic similarity, which makes it an impoverished resource for discovering general semantic relatedness. We note that the three WordNet-based measures from Table 2 that are regarded in the literature as relatedness measures (Banerjee and Pedersen, 2003; Hirst and St-Onge, 1998; Patwardhan and Pedersen, 2006) 5Results based on standard implementations in the WordNet::Similarity Perl module of Pedersen et al. (2004) (v2.05). 892 Rel-122 M&amp;C R&amp;G Measure r p r p r p * Szumlanski and Gomez (2010) 0.654 0.534 0.852 0.859 0.824 0.841 * Patwardhan and Pedersen (2006) 0.341 0.364 0.865 0.906 0.793 0.795 Path Length 0.225 0.183 0.755 0.715 0.784 0.783 * Banerjee and Pedersen (2003) 0.210 0.258 0.356 0.804 0.340 0.718 Resnik (1995) 0.203 0.182 0.806 0.741 0.822 0.757 Jiang and Conrath (1997) 0.188 0.133 0.473 0.663 0.575 0.592 Leacock and Chodorow (1998) 0.173 0.167 0.779 0.7</context>
</contexts>
<marker>Hirst, St-Onge, 1998</marker>
<rawString>Graeme Hirst and David St-Onge. 1998. Lexical chains as representations of context for the detection and correction of malapropisms. In Christiane Fellbaum, editor, WordNet: An Electronic Lexical Database, pages 305–332. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thad Hughes</author>
<author>Daniel Ramage</author>
</authors>
<title>Lexical semantic relatedness with random graph walks.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>581--589</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="3524" citStr="Hughes and Ramage, 2007" startWordPosition="534" endWordPosition="537">sequent replication of M&amp;C’s study similarly yielded a correlation of r = 0.96. The M&amp;C pairs were also included in a similarity study by Finkelstein et al. (2002), which yielded correlation of r = 0.95 to the M&amp;C norms. 2.1 WordSim353 WordSim353 (Finkelstein et al., 2002) has recently emerged as a potential surrogate dataset for evaluating relatedness measures. Several studies have reported correlation to WordSim353 norms as part of their evaluation procedures, with some studies explicitly referring to it as a collection of human-assigned relatedness scores (Gabrilovich and Markovitch, 2007; Hughes and Ramage, 2007; Milne and Witten, 2008). 890 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 890–895, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Yet, the instructions presented to Finkelstein et al.’s subjects give us pause to reconsider WordSim353’s classification as a set of relatedness norms. They repeatedly framed the task as one in which subjects were expected to assign word similarity scores, although participants were instructed to extend their definition of similarity to include antonymy, which perhaps explains wh</context>
</contexts>
<marker>Hughes, Ramage, 2007</marker>
<rawString>Thad Hughes and Daniel Ramage. 2007. Lexical semantic relatedness with random graph walks. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 581–589, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mario Jarmasz</author>
<author>Stan Szpakowicz</author>
</authors>
<title>Roget’s thesaurus and semantic similarity.</title>
<date>2003</date>
<booktitle>In Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP),</booktitle>
<pages>212--219</pages>
<contexts>
<context position="4258" citStr="Jarmasz and Szpakowicz (2003)" startWordPosition="641" endWordPosition="644">l Linguistics, pages 890–895, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Yet, the instructions presented to Finkelstein et al.’s subjects give us pause to reconsider WordSim353’s classification as a set of relatedness norms. They repeatedly framed the task as one in which subjects were expected to assign word similarity scores, although participants were instructed to extend their definition of similarity to include antonymy, which perhaps explains why the authors later referred to their data as “relatedness” norms rather than merely “similarity” norms. Jarmasz and Szpakowicz (2003) have raised further methodological concerns about the construction of WordSim353, including: (a) similarity was rated on a scale of 0.0 to 10.0, which is intrinsically more difficult for humans to manage than the scale of 0.0 to 4.0 used by R&amp;G and M&amp;C, and (b) the inclusion of proper nouns introduced an element of cultural bias into the dataset (e.g., the evaluation of the pair Arafat–terror). Cognizant of the problematic conflation of similarity and relatedness in WordSim353, Agirre et al. (2009) partitioned the data into two sets: one containing noun pairs exhibiting similarity, and one co</context>
</contexts>
<marker>Jarmasz, Szpakowicz, 2003</marker>
<rawString>Mario Jarmasz and Stan Szpakowicz. 2003. Roget’s thesaurus and semantic similarity. In Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP), pages 212– 219.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay J Jiang</author>
<author>David W Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy.</title>
<date>1997</date>
<booktitle>In Proceedings of the International Conference on Research in Computational Linguistics (ROCLING),</booktitle>
<pages>pages</pages>
<contexts>
<context position="13089" citStr="Jiang and Conrath (1997)" startWordPosition="2047" endWordPosition="2050">t are regarded in the literature as relatedness measures (Banerjee and Pedersen, 2003; Hirst and St-Onge, 1998; Patwardhan and Pedersen, 2006) 5Results based on standard implementations in the WordNet::Similarity Perl module of Pedersen et al. (2004) (v2.05). 892 Rel-122 M&amp;C R&amp;G Measure r p r p r p * Szumlanski and Gomez (2010) 0.654 0.534 0.852 0.859 0.824 0.841 * Patwardhan and Pedersen (2006) 0.341 0.364 0.865 0.906 0.793 0.795 Path Length 0.225 0.183 0.755 0.715 0.784 0.783 * Banerjee and Pedersen (2003) 0.210 0.258 0.356 0.804 0.340 0.718 Resnik (1995) 0.203 0.182 0.806 0.741 0.822 0.757 Jiang and Conrath (1997) 0.188 0.133 0.473 0.663 0.575 0.592 Leacock and Chodorow (1998) 0.173 0.167 0.779 0.715 0.839 0.783 Wu and Palmer (1994) 0.187 0.180 0.764 0.732 0.797 0.768 Lin (1998) 0.145 0.148 0.739 0.687 0.726 0.636 * Hirst and St-Onge (1998) 0.141 0.160 0.667 0.782 0.726 0.797 Table 2: Correlation of similarity and relatedness measures to Rel-122, M&amp;C, and R&amp;G. Starred rows (*) are considered relatedness measures. All measures are WordNet-based, except for the scoring metric of Szumlanski and Gomez (2010), which is based on lexical co-occurrence frequency in Wikipedia. # Noun Pair Sim. Rel. # Noun Pair </context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>Jay J. Jiang and David W. Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. In Proceedings of the International Conference on Research in Computational Linguistics (ROCLING), pages 19–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Leacock</author>
<author>Martin Chodorow</author>
</authors>
<title>Combining local context and WordNet similarity for word sense identification.</title>
<date>1998</date>
<booktitle>In Christiane Fellbaum, editor, WordNet: An Electronic Lexical Database,</booktitle>
<pages>265--283</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="13153" citStr="Leacock and Chodorow (1998)" startWordPosition="2057" endWordPosition="2060">erjee and Pedersen, 2003; Hirst and St-Onge, 1998; Patwardhan and Pedersen, 2006) 5Results based on standard implementations in the WordNet::Similarity Perl module of Pedersen et al. (2004) (v2.05). 892 Rel-122 M&amp;C R&amp;G Measure r p r p r p * Szumlanski and Gomez (2010) 0.654 0.534 0.852 0.859 0.824 0.841 * Patwardhan and Pedersen (2006) 0.341 0.364 0.865 0.906 0.793 0.795 Path Length 0.225 0.183 0.755 0.715 0.784 0.783 * Banerjee and Pedersen (2003) 0.210 0.258 0.356 0.804 0.340 0.718 Resnik (1995) 0.203 0.182 0.806 0.741 0.822 0.757 Jiang and Conrath (1997) 0.188 0.133 0.473 0.663 0.575 0.592 Leacock and Chodorow (1998) 0.173 0.167 0.779 0.715 0.839 0.783 Wu and Palmer (1994) 0.187 0.180 0.764 0.732 0.797 0.768 Lin (1998) 0.145 0.148 0.739 0.687 0.726 0.636 * Hirst and St-Onge (1998) 0.141 0.160 0.667 0.782 0.726 0.797 Table 2: Correlation of similarity and relatedness measures to Rel-122, M&amp;C, and R&amp;G. Starred rows (*) are considered relatedness measures. All measures are WordNet-based, except for the scoring metric of Szumlanski and Gomez (2010), which is based on lexical co-occurrence frequency in Wikipedia. # Noun Pair Sim. Rel. # Noun Pair Sim. Rel. 1. car automobile 3.92 4.00 16. lad brother 1.66 2.68 </context>
</contexts>
<marker>Leacock, Chodorow, 1998</marker>
<rawString>Claudia Leacock and Martin Chodorow. 1998. Combining local context and WordNet similarity for word sense identification. In Christiane Fellbaum, editor, WordNet: An Electronic Lexical Database, pages 265–283. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>An information-theoretic definition of similarity.</title>
<date>1998</date>
<booktitle>In Proceedings of the 15th International Conference on Machine Learning (ICML),</booktitle>
<pages>296--304</pages>
<contexts>
<context position="13257" citStr="Lin (1998)" startWordPosition="2077" endWordPosition="2078">ons in the WordNet::Similarity Perl module of Pedersen et al. (2004) (v2.05). 892 Rel-122 M&amp;C R&amp;G Measure r p r p r p * Szumlanski and Gomez (2010) 0.654 0.534 0.852 0.859 0.824 0.841 * Patwardhan and Pedersen (2006) 0.341 0.364 0.865 0.906 0.793 0.795 Path Length 0.225 0.183 0.755 0.715 0.784 0.783 * Banerjee and Pedersen (2003) 0.210 0.258 0.356 0.804 0.340 0.718 Resnik (1995) 0.203 0.182 0.806 0.741 0.822 0.757 Jiang and Conrath (1997) 0.188 0.133 0.473 0.663 0.575 0.592 Leacock and Chodorow (1998) 0.173 0.167 0.779 0.715 0.839 0.783 Wu and Palmer (1994) 0.187 0.180 0.764 0.732 0.797 0.768 Lin (1998) 0.145 0.148 0.739 0.687 0.726 0.636 * Hirst and St-Onge (1998) 0.141 0.160 0.667 0.782 0.726 0.797 Table 2: Correlation of similarity and relatedness measures to Rel-122, M&amp;C, and R&amp;G. Starred rows (*) are considered relatedness measures. All measures are WordNet-based, except for the scoring metric of Szumlanski and Gomez (2010), which is based on lexical co-occurrence frequency in Wikipedia. # Noun Pair Sim. Rel. # Noun Pair Sim. Rel. 1. car automobile 3.92 4.00 16. lad brother 1.66 2.68 2. gem jewel 3.84 3.98 17. journey car 1.16 3.00 3. journey voyage 3.84 3.97 18. monk oracle 1.10 2.54 4</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. An information-theoretic definition of similarity. In Proceedings of the 15th International Conference on Machine Learning (ICML), pages 296–304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Walter G Charles</author>
</authors>
<title>Contextual correlates of semantic similarity.</title>
<date>1991</date>
<booktitle>Language and Cognitive Processes,</booktitle>
<pages>6--1</pages>
<contexts>
<context position="2173" citStr="Miller and Charles (1991" startWordPosition="319" endWordPosition="322">ogy remains part of the standard evaluative procedure for assessing computational measures of semantic relatedness. Because similarity is only one particular type of relatedness, comparison to similarity norms fails to give a complete view of a relatedness measure’s efficacy. In keeping with Budanitsky and Hirst’s (2006) observation that “comparison with human judgments is the ideal way to evaluate a measure of similarity or relatedness,” we have undertaken the creation of a new set of relatedness norms. 2 Background The similarity norms of Rubenstein and Goodenough (1965; henceforth R&amp;G) and Miller and Charles (1991; henceforth M&amp;C) have seen ubiquitous use in evaluation of computational measures of semantic similarity and relatedness. R&amp;G established their similarity norms by presenting subjects with 65 slips of paper, each of which contained a pair of nouns. Subjects were directed to read through all 65 noun pairs, then sort the pairs “according to amount of ‘similarity of meaning.’” Subjects then assigned similarity scores to each pair on a scale of 0.0 (completely dissimilar) to 4.0 (strongly synonymous). The R&amp;G results have proven to be highly replicable. M&amp;C repeated R&amp;G’s study using a subset of </context>
</contexts>
<marker>Miller, Charles, 1991</marker>
<rawString>George A. Miller and Walter G. Charles. 1991. Contextual correlates of semantic similarity. Language and Cognitive Processes, 6(1):1–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Milne</author>
<author>Ian H Witten</author>
</authors>
<title>An effective, low-cost measure of semantic relatedness obtained from Wikipedia links.</title>
<date>2008</date>
<booktitle>In Proceedings of the First AAAI Workshop on Wikipedia and Artificial Intelligence (WIKIAI),</booktitle>
<pages>25--30</pages>
<contexts>
<context position="3549" citStr="Milne and Witten, 2008" startWordPosition="538" endWordPosition="541">C’s study similarly yielded a correlation of r = 0.96. The M&amp;C pairs were also included in a similarity study by Finkelstein et al. (2002), which yielded correlation of r = 0.95 to the M&amp;C norms. 2.1 WordSim353 WordSim353 (Finkelstein et al., 2002) has recently emerged as a potential surrogate dataset for evaluating relatedness measures. Several studies have reported correlation to WordSim353 norms as part of their evaluation procedures, with some studies explicitly referring to it as a collection of human-assigned relatedness scores (Gabrilovich and Markovitch, 2007; Hughes and Ramage, 2007; Milne and Witten, 2008). 890 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 890–895, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Yet, the instructions presented to Finkelstein et al.’s subjects give us pause to reconsider WordSim353’s classification as a set of relatedness norms. They repeatedly framed the task as one in which subjects were expected to assign word similarity scores, although participants were instructed to extend their definition of similarity to include antonymy, which perhaps explains why the authors later refer</context>
</contexts>
<marker>Milne, Witten, 2008</marker>
<rawString>David Milne and Ian H. Witten. 2008. An effective, low-cost measure of semantic relatedness obtained from Wikipedia links. In Proceedings of the First AAAI Workshop on Wikipedia and Artificial Intelligence (WIKIAI), pages 25–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siddharth Patwardhan</author>
<author>Ted Pedersen</author>
</authors>
<title>Using WordNet-based context vectors to estimate the semantic relatedness of concepts.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics Workshop on Making Sense of Sense,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="12607" citStr="Patwardhan and Pedersen, 2006" startWordPosition="1966" endWordPosition="1969">nt correlation (r), as well as Spearman’s rank correlation (p). For comparison, we include results for the correlation of these measures to the M&amp;C and R&amp;G similarity means. The generally weak performance of the WordNet-based measures on this task is not surprising, given WordNet’s strong disposition toward codifying semantic similarity, which makes it an impoverished resource for discovering general semantic relatedness. We note that the three WordNet-based measures from Table 2 that are regarded in the literature as relatedness measures (Banerjee and Pedersen, 2003; Hirst and St-Onge, 1998; Patwardhan and Pedersen, 2006) 5Results based on standard implementations in the WordNet::Similarity Perl module of Pedersen et al. (2004) (v2.05). 892 Rel-122 M&amp;C R&amp;G Measure r p r p r p * Szumlanski and Gomez (2010) 0.654 0.534 0.852 0.859 0.824 0.841 * Patwardhan and Pedersen (2006) 0.341 0.364 0.865 0.906 0.793 0.795 Path Length 0.225 0.183 0.755 0.715 0.784 0.783 * Banerjee and Pedersen (2003) 0.210 0.258 0.356 0.804 0.340 0.718 Resnik (1995) 0.203 0.182 0.806 0.741 0.822 0.757 Jiang and Conrath (1997) 0.188 0.133 0.473 0.663 0.575 0.592 Leacock and Chodorow (1998) 0.173 0.167 0.779 0.715 0.839 0.783 Wu and Palmer (19</context>
</contexts>
<marker>Patwardhan, Pedersen, 2006</marker>
<rawString>Siddharth Patwardhan and Ted Pedersen. 2006. Using WordNet-based context vectors to estimate the semantic relatedness of concepts. In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics Workshop on Making Sense of Sense, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Siddharth Patwardhan</author>
<author>Jason Michelizzi</author>
</authors>
<title>WordNet::Similarity – Measuring the relatedness of concepts.</title>
<date>2004</date>
<booktitle>In Proceedings of the 5th Annual Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL),</booktitle>
<pages>38--11</pages>
<contexts>
<context position="12715" citStr="Pedersen et al. (2004)" startWordPosition="1982" endWordPosition="1985">on of these measures to the M&amp;C and R&amp;G similarity means. The generally weak performance of the WordNet-based measures on this task is not surprising, given WordNet’s strong disposition toward codifying semantic similarity, which makes it an impoverished resource for discovering general semantic relatedness. We note that the three WordNet-based measures from Table 2 that are regarded in the literature as relatedness measures (Banerjee and Pedersen, 2003; Hirst and St-Onge, 1998; Patwardhan and Pedersen, 2006) 5Results based on standard implementations in the WordNet::Similarity Perl module of Pedersen et al. (2004) (v2.05). 892 Rel-122 M&amp;C R&amp;G Measure r p r p r p * Szumlanski and Gomez (2010) 0.654 0.534 0.852 0.859 0.824 0.841 * Patwardhan and Pedersen (2006) 0.341 0.364 0.865 0.906 0.793 0.795 Path Length 0.225 0.183 0.755 0.715 0.784 0.783 * Banerjee and Pedersen (2003) 0.210 0.258 0.356 0.804 0.340 0.718 Resnik (1995) 0.203 0.182 0.806 0.741 0.822 0.757 Jiang and Conrath (1997) 0.188 0.133 0.473 0.663 0.575 0.592 Leacock and Chodorow (1998) 0.173 0.167 0.779 0.715 0.839 0.783 Wu and Palmer (1994) 0.187 0.180 0.764 0.732 0.797 0.768 Lin (1998) 0.145 0.148 0.739 0.687 0.726 0.636 * Hirst and St-Onge (</context>
</contexts>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi. 2004. WordNet::Similarity – Measuring the relatedness of concepts. In Proceedings of the 5th Annual Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL), pages 38–11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Paolo Ponzetto</author>
<author>Roberto Navigli</author>
</authors>
<title>Knowledge-rich word sense disambiguation rivaling supervised systems.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>1522--1531</pages>
<contexts>
<context position="6486" citStr="Ponzetto and Navigli, 2010" startWordPosition="1006" endWordPosition="1009">d of reordering a deck of cards, participants were presented with a grid of cards that they were able 1Perhaps not surprisingly, the highest scores in WordSim353 (all ratings from 9.0 to 10.0) were assigned to pairs that Agirre et al. placed in their similarity partition. to rearrange interactively with the use of a mouse or any touch-enabled device, such as a tablet PC.2 3.1 Experimental Conditions Each participant in our study was randomly assigned to one of four conditions. Each condition contained 32 noun pairs for evaluation. Of those pairs, 10 were randomly selected from from WordNet++ (Ponzetto and Navigli, 2010) and 10 from SGN (Szumlanski and Gomez, 2010)—two semantic networks that categorically indicate strong relatedness between WordNet noun senses. 10 additional pairs were generated by randomly pairing words from a list of all nouns occurring in Wikipedia. The nouns in the pairs we used from each of these three sources were matched for frequency of occurrence in Wikipedia. We manually selected two additional pairs that appeared across all four conditions: leaves–rake and lion–cage. These control pairs were included to ensure that each condition contained examples of strong semantic relatedness, a</context>
</contexts>
<marker>Ponzetto, Navigli, 2010</marker>
<rawString>Simone Paolo Ponzetto and Roberto Navigli. 2010. Knowledge-rich word sense disambiguation rivaling supervised systems. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL), pages 1522–1531.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Using information content to evaluate semantic similarity in a taxonomy.</title>
<date>1995</date>
<booktitle>In Proceedings of the 14th International Joint Conference on Artificial Intelligence (IJCAI),</booktitle>
<pages>448--453</pages>
<contexts>
<context position="1492" citStr="Resnik, 1995" startWordPosition="216" endWordPosition="217">blishes a clearer expectation for what constitutes human-like performance by a computational measure of semantic relatedness. We compare the results of several WordNet-based similarity and relatedness measures to our Rel-122 norms and demonstrate the limitations of WordNet for discovering general indications of semantic relatedness. We also offer a critique of the field’s reliance upon similarity norms to evaluate relatedness measures. 1 Introduction Despite the well-established technical distinction between semantic similarity and relatedness (Agirre et al., 2009; Budanitsky and Hirst, 2006; Resnik, 1995), comparison to established similarity norms from psychology remains part of the standard evaluative procedure for assessing computational measures of semantic relatedness. Because similarity is only one particular type of relatedness, comparison to similarity norms fails to give a complete view of a relatedness measure’s efficacy. In keeping with Budanitsky and Hirst’s (2006) observation that “comparison with human judgments is the ideal way to evaluate a measure of similarity or relatedness,” we have undertaken the creation of a new set of relatedness norms. 2 Background The similarity norms</context>
<context position="10316" citStr="Resnik (1995)" startWordPosition="1616" endWordPosition="1617">aluated by at least 20 human subjects. After outlier removal (described above), each word pair retained evaluations from 14 to 22 individuals. The resulting relatedness means are available online.4 An excerpt of the Rel-122 norms is shown in Table 1. We note that the highest rated pairs in our dataset are not strictly similar entities; exactly half of the 10 most strongly related nouns in Table 1 are dissimilar (e.g., digital camera–photographer). Judgments from individual subjects in our study exhibited high average correlation to the elicited relatedness means (r = 0.769, Q = 0.09, N = 73). Resnik (1995), in his replication of the 3We used this sampling method to prevent extreme outliers from masking their own aberration during outlier detection, which is potentially problematic when dealing with small populations. Without leave-one-out-sampling, we would have identified fewer outliers (14 instead of 19), but the resulting means would still have correlated strongly to our final relatedness norms (r = 0.991, p &lt; 0.01). 4http://www.cs.ucf.edu/∼seansz/rel-122 # Word Pair µ underwear lingerie 3.94 digital camera photographer 3.85 tuition fee 3.85 leaves rake 3.82 symptom fever 3.79 fertility ovar</context>
<context position="13028" citStr="Resnik (1995)" startWordPosition="2039" endWordPosition="2040"> the three WordNet-based measures from Table 2 that are regarded in the literature as relatedness measures (Banerjee and Pedersen, 2003; Hirst and St-Onge, 1998; Patwardhan and Pedersen, 2006) 5Results based on standard implementations in the WordNet::Similarity Perl module of Pedersen et al. (2004) (v2.05). 892 Rel-122 M&amp;C R&amp;G Measure r p r p r p * Szumlanski and Gomez (2010) 0.654 0.534 0.852 0.859 0.824 0.841 * Patwardhan and Pedersen (2006) 0.341 0.364 0.865 0.906 0.793 0.795 Path Length 0.225 0.183 0.755 0.715 0.784 0.783 * Banerjee and Pedersen (2003) 0.210 0.258 0.356 0.804 0.340 0.718 Resnik (1995) 0.203 0.182 0.806 0.741 0.822 0.757 Jiang and Conrath (1997) 0.188 0.133 0.473 0.663 0.575 0.592 Leacock and Chodorow (1998) 0.173 0.167 0.779 0.715 0.839 0.783 Wu and Palmer (1994) 0.187 0.180 0.764 0.732 0.797 0.768 Lin (1998) 0.145 0.148 0.739 0.687 0.726 0.636 * Hirst and St-Onge (1998) 0.141 0.160 0.667 0.782 0.726 0.797 Table 2: Correlation of similarity and relatedness measures to Rel-122, M&amp;C, and R&amp;G. Starred rows (*) are considered relatedness measures. All measures are WordNet-based, except for the scoring metric of Szumlanski and Gomez (2010), which is based on lexical co-occurren</context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>Philip Resnik. 1995. Using information content to evaluate semantic similarity in a taxonomy. In Proceedings of the 14th International Joint Conference on Artificial Intelligence (IJCAI), pages 448–453.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert Rubenstein</author>
<author>John B Goodenough</author>
</authors>
<title>Contextual correlates of synonymy.</title>
<date>1965</date>
<journal>Communications of the ACM,</journal>
<volume>8</volume>
<issue>10</issue>
<contexts>
<context position="2127" citStr="Rubenstein and Goodenough (1965" startWordPosition="311" endWordPosition="315">mparison to established similarity norms from psychology remains part of the standard evaluative procedure for assessing computational measures of semantic relatedness. Because similarity is only one particular type of relatedness, comparison to similarity norms fails to give a complete view of a relatedness measure’s efficacy. In keeping with Budanitsky and Hirst’s (2006) observation that “comparison with human judgments is the ideal way to evaluate a measure of similarity or relatedness,” we have undertaken the creation of a new set of relatedness norms. 2 Background The similarity norms of Rubenstein and Goodenough (1965; henceforth R&amp;G) and Miller and Charles (1991; henceforth M&amp;C) have seen ubiquitous use in evaluation of computational measures of semantic similarity and relatedness. R&amp;G established their similarity norms by presenting subjects with 65 slips of paper, each of which contained a pair of nouns. Subjects were directed to read through all 65 noun pairs, then sort the pairs “according to amount of ‘similarity of meaning.’” Subjects then assigned similarity scores to each pair on a scale of 0.0 (completely dissimilar) to 4.0 (strongly synonymous). The R&amp;G results have proven to be highly replicabl</context>
<context position="5274" citStr="Rubenstein and Goodenough (1965)" startWordPosition="804" endWordPosition="807">rafat–terror). Cognizant of the problematic conflation of similarity and relatedness in WordSim353, Agirre et al. (2009) partitioned the data into two sets: one containing noun pairs exhibiting similarity, and one containing pairs of related but dissimilar nouns. However, pairs in the latter set were not assessed for scoring distribution validity to ensure that strongly related word pairs were not penalized by human subjects for being dissimilar.1 3 Methodology In our experiments, we elicited human ratings of semantic relatedness for 122 noun pairs. In doing so, we followed the methodology of Rubenstein and Goodenough (1965) as closely as possible: participants were instructed to read through a set of noun pairs, sort them by how strongly related they were, and then assign each pair a relatedness score on a scale of 0.0 (“completely unrelated”) to 4.0 (“very strongly related”). We made two notable modifications to the experimental procedure of Rubenstein and Goodenough. First, instead of asking participants to judge “amount of ‘similarity of meaning,’” we asked them to judge “how closely related in meaning” each pair of nouns was. Second, we used a Web interface to collect data in our study; instead of reordering</context>
</contexts>
<marker>Rubenstein, Goodenough, 1965</marker>
<rawString>Herbert Rubenstein and John B. Goodenough. 1965. Contextual correlates of synonymy. Communications of the ACM, 8(10):627–633.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sean Szumlanski</author>
<author>Fernando Gomez</author>
</authors>
<title>Automatically acquiring a semantic network of related concepts.</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th ACM International Conference on Information and Knowledge Management (CIKM),</booktitle>
<pages>pages</pages>
<contexts>
<context position="6531" citStr="Szumlanski and Gomez, 2010" startWordPosition="1014" endWordPosition="1017"> were presented with a grid of cards that they were able 1Perhaps not surprisingly, the highest scores in WordSim353 (all ratings from 9.0 to 10.0) were assigned to pairs that Agirre et al. placed in their similarity partition. to rearrange interactively with the use of a mouse or any touch-enabled device, such as a tablet PC.2 3.1 Experimental Conditions Each participant in our study was randomly assigned to one of four conditions. Each condition contained 32 noun pairs for evaluation. Of those pairs, 10 were randomly selected from from WordNet++ (Ponzetto and Navigli, 2010) and 10 from SGN (Szumlanski and Gomez, 2010)—two semantic networks that categorically indicate strong relatedness between WordNet noun senses. 10 additional pairs were generated by randomly pairing words from a list of all nouns occurring in Wikipedia. The nouns in the pairs we used from each of these three sources were matched for frequency of occurrence in Wikipedia. We manually selected two additional pairs that appeared across all four conditions: leaves–rake and lion–cage. These control pairs were included to ensure that each condition contained examples of strong semantic relatedness, and potentially to help identify and eliminate</context>
<context position="12794" citStr="Szumlanski and Gomez (2010)" startWordPosition="1999" endWordPosition="2002">ak performance of the WordNet-based measures on this task is not surprising, given WordNet’s strong disposition toward codifying semantic similarity, which makes it an impoverished resource for discovering general semantic relatedness. We note that the three WordNet-based measures from Table 2 that are regarded in the literature as relatedness measures (Banerjee and Pedersen, 2003; Hirst and St-Onge, 1998; Patwardhan and Pedersen, 2006) 5Results based on standard implementations in the WordNet::Similarity Perl module of Pedersen et al. (2004) (v2.05). 892 Rel-122 M&amp;C R&amp;G Measure r p r p r p * Szumlanski and Gomez (2010) 0.654 0.534 0.852 0.859 0.824 0.841 * Patwardhan and Pedersen (2006) 0.341 0.364 0.865 0.906 0.793 0.795 Path Length 0.225 0.183 0.755 0.715 0.784 0.783 * Banerjee and Pedersen (2003) 0.210 0.258 0.356 0.804 0.340 0.718 Resnik (1995) 0.203 0.182 0.806 0.741 0.822 0.757 Jiang and Conrath (1997) 0.188 0.133 0.473 0.663 0.575 0.592 Leacock and Chodorow (1998) 0.173 0.167 0.779 0.715 0.839 0.783 Wu and Palmer (1994) 0.187 0.180 0.764 0.732 0.797 0.768 Lin (1998) 0.145 0.148 0.739 0.687 0.726 0.636 * Hirst and St-Onge (1998) 0.141 0.160 0.667 0.782 0.726 0.797 Table 2: Correlation of similarity an</context>
</contexts>
<marker>Szumlanski, Gomez, 2010</marker>
<rawString>Sean Szumlanski and Fernando Gomez. 2010. Automatically acquiring a semantic network of related concepts. In Proceedings of the 19th ACM International Conference on Information and Knowledge Management (CIKM), pages 19–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhibiao Wu</author>
<author>Martha Palmer</author>
</authors>
<title>Verb semantics and lexical selection.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>133--139</pages>
<contexts>
<context position="13210" citStr="Wu and Palmer (1994)" startWordPosition="2067" endWordPosition="2070">d Pedersen, 2006) 5Results based on standard implementations in the WordNet::Similarity Perl module of Pedersen et al. (2004) (v2.05). 892 Rel-122 M&amp;C R&amp;G Measure r p r p r p * Szumlanski and Gomez (2010) 0.654 0.534 0.852 0.859 0.824 0.841 * Patwardhan and Pedersen (2006) 0.341 0.364 0.865 0.906 0.793 0.795 Path Length 0.225 0.183 0.755 0.715 0.784 0.783 * Banerjee and Pedersen (2003) 0.210 0.258 0.356 0.804 0.340 0.718 Resnik (1995) 0.203 0.182 0.806 0.741 0.822 0.757 Jiang and Conrath (1997) 0.188 0.133 0.473 0.663 0.575 0.592 Leacock and Chodorow (1998) 0.173 0.167 0.779 0.715 0.839 0.783 Wu and Palmer (1994) 0.187 0.180 0.764 0.732 0.797 0.768 Lin (1998) 0.145 0.148 0.739 0.687 0.726 0.636 * Hirst and St-Onge (1998) 0.141 0.160 0.667 0.782 0.726 0.797 Table 2: Correlation of similarity and relatedness measures to Rel-122, M&amp;C, and R&amp;G. Starred rows (*) are considered relatedness measures. All measures are WordNet-based, except for the scoring metric of Szumlanski and Gomez (2010), which is based on lexical co-occurrence frequency in Wikipedia. # Noun Pair Sim. Rel. # Noun Pair Sim. Rel. 1. car automobile 3.92 4.00 16. lad brother 1.66 2.68 2. gem jewel 3.84 3.98 17. journey car 1.16 3.00 3. journ</context>
</contexts>
<marker>Wu, Palmer, 1994</marker>
<rawString>Zhibiao Wu and Martha Palmer. 1994. Verb semantics and lexical selection. In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics (ACL), pages 133–139.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>