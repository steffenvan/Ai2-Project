<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000142">
<title confidence="0.998812">
Parsing the Internal Structure of Words:
A New Paradigm for Chinese Word Segmentation
</title>
<author confidence="0.970513">
Zhongguo Li
</author>
<affiliation confidence="0.9307675">
State Key Laboratory on Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology
Department of Computer Science and Technology
Tsinghua University, Beijing 100084, China
</affiliation>
<email confidence="0.998123">
eemath@gmail.com
</email>
<sectionHeader confidence="0.996658" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998868411764706">
Lots of Chinese characters are very produc-
tive in that they can form many structured
words either as prefixes or as suffixes. Pre-
vious research in Chinese word segmentation
mainly focused on identifying only the word
boundaries without considering the rich inter-
nal structures of many words. In this paper we
argue that this is unsatisfying in many ways,
both practically and theoretically. Instead, we
propose that word structures should be recov-
ered in morphological analysis. An elegant
approach for doing this is given and the result
is shown to be promising enough for encour-
aging further effort in this direction. Our prob-
ability model is trained with the Penn Chinese
Treebank and actually is able to parse both
word and phrase structures in a unified way.
</bodyText>
<sectionHeader confidence="0.896577" genericHeader="categories and subject descriptors">
1 Why Parse Word Structures?
</sectionHeader>
<bodyText confidence="0.999771533333333">
Research in Chinese word segmentation has pro-
gressed tremendously in recent years, with state of
the art performing at around 97% in precision and
recall (Xue, 2003; Gao et al., 2005; Zhang and
Clark, 2007; Li and Sun, 2009). However, virtually
all these systems focus exclusively on recognizing
the word boundaries, giving no consideration to the
internal structures of many words. Though it has
been the standard practice for many years, we argue
that this paradigm is inadequate both in theory and
in practice, for at least the following four reasons.
The first reason is that if we confine our defi-
nition of word segmentation to the identification of
word boundaries, then people tend to have divergent
opinions as to whether a linguistic unit is a word or
not (Sproat et al., 1996). This has led to many dif-
ferent annotation standards for Chinese word seg-
mentation. Even worse, this could cause inconsis-
tency in the same corpus. For instance, 䉂t奒
‘vice president’ is considered to be one word in the
Penn Chinese Treebank (Xue et al., 2005), but is
split into two words by the Peking University cor-
pus in the SIGHAN Bakeoffs (Sproat and Emerson,
2003). Meanwhile, 䉂䀓惼 ‘vice director’ and 䉂
䚲䡮 ‘deputy manager’ are both segmented into two
words in the same Penn Chinese Treebank. In fact,
all these words are composed of the prefix 䉂 ‘vice’
and a root word. Thus the structure of 䉂t奒 ‘vice
president’ can be represented with the tree in Fig-
ure 1. Without a doubt, there is complete agree-
</bodyText>
<equation confidence="0.779625">
NN
� �
� �
JJf NNf
䉂 擌奒
</equation>
<figureCaption confidence="0.999813">
Figure 1: Example of a word with internal structure.
</figureCaption>
<bodyText confidence="0.875847909090909">
ment on the correctness of this structure among na-
tive Chinese speakers. So if instead of annotating
only word boundaries, we annotate the structures of
every word, 1 then the annotation tends to be more
1Here it is necessary to add a note on terminology used in
this paper. Since there is no universally accepted definition
of the “word” concept in linguistics and especially in Chinese,
whenever we use the term “word” we might mean a linguistic
unit such as 䉂t奒 ‘vice president’ whose structure is shown
as the tree in Figure 1, or we might mean a smaller unit such as
t奒 ‘president’ which is a substructure of that tree. Hopefully,
</bodyText>
<page confidence="0.919337">
1405
</page>
<note confidence="0.9792555">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1405–1414,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999867">
consistent and there could be less duplication of ef-
forts in developing the expensive annotated corpus.
The second reason is applications have different
requirements for granularity of words. Take the per-
sonal name 撱嗤A ‘Zhou Shuren’ as an example.
It’s considered to be one word in the Penn Chinese
Treebank, but is segmented into a surname and a
given name in the Peking University corpus. For
some applications such as information extraction,
the former segmentation is adequate, while for oth-
ers like machine translation, the later finer-grained
output is more preferable. If the analyzer can pro-
duce a structure as shown in Figure 4(a), then ev-
ery application can extract what it needs from this
tree. A solution with tree output like this is more el-
egant than approaches which try to meet the needs
of different applications in post-processing (Gao et
al., 2004).
The third reason is that traditional word segmen-
tation has problems in handling many phenomena
in Chinese. For example, the telescopic compound
㦌撥怂惆 ‘universities, middle schools and primary
schools’ is in fact composed of three coordinating el-
ements 㦌惆 ‘university’, 撥惆 ‘middle school’ and
怂惆 ‘primary school’. Regarding it as one flat word
loses this important information. Another example
is separable words like 扩扙 ‘swim’. With a lin-
ear segmentation, the meaning of ‘swimming’ as in
扩堑扙 ‘after swimming’ cannot be properly rep-
resented, since 扩扙 ‘swim’ will be segmented into
discontinuous units. These language usages lie at the
boundary between syntax and morphology, and are
not uncommon in Chinese. They can be adequately
represented with trees (Figure 2).
</bodyText>
<figureCaption confidence="0.9906905">
Figure 2: Example of telescopic compound (a) and sepa-
rable word (b).
</figureCaption>
<bodyText confidence="0.9975394375">
The last reason why we should care about word
the context will always make it clear what is being referred to
with the term word”.
structures is related to head driven statistical parsers
(Collins, 2003). To illustrate this, note that in the
Penn Chinese Treebank, the word 戽䊂䠽A ‘En-
glish People’ does not occur at all. Hence con-
stituents headed by such words could cause some
difficulty for head driven models in which out-of-
vocabulary words need to be treated specially both
when they are generated and when they are condi-
tioned upon. But this word is in turn headed by its
suffix A ‘people’, and there are 2,233 such words
in Penn Chinese Treebank. If we annotate the struc-
ture of every compound containing this suffix (e.g.
Figure 3), such data sparsity simply goes away.
</bodyText>
<equation confidence="0.9127676">
NN
❜❜
✧ ✧
NRf NNf
戽䊂䠽 吼
</equation>
<figureCaption confidence="0.985013">
Figure 3: Structure of the out-of-vocabulary word 戽䊂
䠽A ‘English People’.
</figureCaption>
<bodyText confidence="0.998913294117647">
Had there been only a few words with inter-
nal structures, current Chinese word segmentation
paradigm would be sufficient. We could simply re-
cover word structures in post-processing. But this is
far from the truth. In Chinese there is a large number
of such words. We just name a few classes of these
words and give one example for each class (a dot is
used to separate roots from affixes):
personal name: 㡿増·揽 ‘Nagao Makoto’
location name: 凝挕·撲 ‘New York State’
noun with a suffix: ft䡡·勬 ‘classifier’
noun with a prefix: 敏·䧥䧥 ‘mother-to-be’
verb with a suffix: 敧䃄·䑺 ‘automatize’
verb with a prefix: 䆓·噙 ‘waterproof’
adjective with a suffix: A䏜·怮 ‘composite’
adjective with a prefix: 䆚·搔喪 ‘informal’
pronoun with a prefix: 䊈·eft ‘everybody’
</bodyText>
<listItem confidence="0.566366666666667">
time expression: 憘䛊䛊H·兣 ‘the year 1995’
ordinal number: 䀱·喛憘 ‘eleventh’
retroflex suffixation: 䑳䃹·䄎 ‘flower’
</listItem>
<bodyText confidence="0.994672833333333">
This list is not meant to be complete, but we can get
a feel of how extensive the words with non-trivial
structures can be. With so many productive suf-
fixes and prefixes, analyzing word structures in post-
processing is difficult, because a character may or
may not act as an affix depending on the context.
</bodyText>
<equation confidence="0.9833705">
(a) NN
✟✟ ✟ ❍❍❍
JJ NNf
✟✟ ✟ ❍❍
(b) VV
✟ ✟ ❍❍
VV NNf
✚ ✚ ❩❩
JJf JJf JJf 惆 VVf VVf 扙
㦌 撥 怂 扩 堑
</equation>
<page confidence="0.949544">
1406
</page>
<bodyText confidence="0.99945925">
For example, the character A ‘people’ in 撇嗤A
‘the one who plants’ is a suffix, but in the personal
name 撱嗤A ‘Zhou Shuren’ it isn’t. The structures
of these two words are shown in Figure 4.
</bodyText>
<figure confidence="0.95140875">
(a) NR (b) NN
✚✚❩❩ ✚✚ ❩❩
NFf NGf VVf NNf
撱 嗤吼 撇嗤 吼
</figure>
<figureCaption confidence="0.825459363636364">
Figure 4: Two words that differ only in one character,
but have different internal structures. The character A
‘people’ is part of a personal name in tree (a), but is a
suffix in (b).
A second reason why generally we cannot re-
cover word structures in post-processing is that some
words have very complex structures. For example,
the tree of 壃搕䈿擌懂揶 ‘anarchist’ is shown in
Figure 5. Parsing this structure correctly without a
principled method is difficult and messy, if not im-
possible.
</figureCaption>
<equation confidence="0.951045625">
✦ NN ❛❛❛
✦ ✦
NN NNf
✟✟ ✟ ❍❍❍
VV NNf 揶
✚✚ ❩❩
VVf NNf 擌懂
壃 搕䈿
</equation>
<figureCaption confidence="0.98931">
Figure 5: An example word which has very complex
structures.
</figureCaption>
<bodyText confidence="0.999951">
Finally, it must be mentioned that we cannot store
all word structures in a dictionary, as the word for-
mation process is very dynamic and productive in
nature. Take 䌬 ‘hall’ as an example. Standard Chi-
nese dictionaries usually contain 埣嗖䌬 ‘library’,
but not many other words such as 䎰愒䌬 ‘aquar-
ium’ generated by this same character. This is un-
derstandable since the character 䌬 ‘hall’ is so pro-
ductive that it is impossible for a dictionary to list
every word with this character as a suffix. The same
thing happens for natural language processing sys-
tems. Thus it is necessary to have a dynamic mech-
anism for parsing word structures.
In this paper, we propose a new paradigm for
Chinese word segmentation in which not only word
boundaries are identified but the internal structures
of words are recovered (Section 3). To achieve this,
we design a joint morphological and syntactic pars-
ing model of Chinese (Section 4). Our generative
story describes the complete process from sentence
and word structures to the surface string of char-
acters in a top-down fashion. With this probabil-
ity model, we give an algorithm to find the parse
tree of a raw sentence with the highest probabil-
ity (Section 5). The output of our parser incorpo-
rates word structures naturally. Evaluation shows
that the model can learn much of the regularity of
word structures, and also achieves reasonable ac-
curacy in parsing higher level constituent structures
(Section 6).
</bodyText>
<sectionHeader confidence="0.999785" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999993142857143">
The necessity of parsing word structures has been
noticed by Zhao (2009), who presented a character-
level dependency scheme as an alternative to the lin-
ear representation of words. Although our work is
based on the same notion, there are two key dif-
ferences. The first one is that part-of-speech tags
and constituent labels are fundamental for our pars-
ing model, while Zhao focused on unlabeled depen-
dencies between characters in a word, and part-of-
speech information was not utilized. Secondly, we
distinguish explicitly the generation of flat words
such as 䑵喏䃮 ‘Washington’ and words with inter-
nal structures. Our parsing algorithm also has to be
adapted accordingly. Such distinction was not made
in Zhao’s parsing model and algorithm.
Many researchers have also noticed the awkward-
ness and insufficiency of current boundary-only Chi-
nese word segmentation paradigm, so they tried to
customize the output to meet the requirements of
various applications (Wu, 2003; Gao et al., 2004).
In a related research, Jiang et al. (2009) presented a
strategy to transfer annotated corpora between dif-
ferent segmentation standards in the hope of saving
some expensive human labor. We believe the best
solution to the problem of divergent standards and
requirements is to annotate and analyze word struc-
tures. Then applications can make use of these struc-
tures according to their own convenience.
</bodyText>
<page confidence="0.966461">
1407
</page>
<bodyText confidence="0.999898764705882">
Since the distinction between morphology and
syntax in Chinese is somewhat blurred, our model
for word structure parsing is integrated with con-
stituent parsing. There has been many efforts to in-
tegrate Chinese word segmentation, part-of-speech
tagging and parsing (Wu and Zixin, 1998; Zhou and
Su, 2003; Luo, 2003; Fung et al., 2004). However,
in these research all words were considered to be
flat, and thus word structures were not parsed. This
is a crucial difference with our work. Specifically,
consider the word 0碜扨 ‘olive oil’. Our parser
output tree Figure 6(a), while Luo (2003) output tree
(b), giving no hint to the structure of this word since
the result is the same with a real flat word 䧢哫膝
‘Los Angeles’(c).
of head-driven generative models (Charniak, 1997;
Bikel and Chiang, 2000) .
</bodyText>
<sectionHeader confidence="0.964113" genericHeader="method">
3 The New Paradigm
</sectionHeader>
<bodyText confidence="0.999532142857143">
Given a raw Chinese sentence like 䤕撓 䏓 A敯
䋳 㢧 喓, a traditional word segmentation system
would output some result like 䤕撓䏓 A 敯䋳㢧
喓(‘Lin Zhihao’, ‘is’, ‘chief engineer’). In our new
paradigm, the output should at least be a linear se-
quence of trees representing the structures of each
word as in Figure 7.
</bodyText>
<figure confidence="0.998046133333333">
VV
VVf
JJ
NGf
NFf
NR
✑◗◗
✑
NN
✟✟ ✟ ❍❍❍
NN
❩❩
✚✚
䤕 撓䏓 A JJf NNf NNf
,1 䋳㢧 9
</figure>
<figureCaption confidence="0.993859">
Figure 7: Proposed output for the new Chinese word seg-
mentation paradigm.
</figureCaption>
<figure confidence="0.999165">
(a) NN
✚✚❩❩
NNf NNf
ta 扨
(b) NN
NNf
ta扨
(c) NR
NRf
䧢哫膝
</figure>
<figureCaption confidence="0.95959275">
Figure 6: Difference between our output (a) of parsing
the word 0碜扨 ‘olive oil’ and the output (b) of Luo
(2003). In (c) we have a true flat word, namely the loca-
tion name 䧢哫膝 ‘Los Angeles’.
</figureCaption>
<bodyText confidence="0.99997865625">
The benefits of joint modeling has been noticed
by many. For example, Li et al. (2010) reported that
a joint syntactic and semantic model improved the
accuracy of both tasks, while Ng and Low (2004)
showed it’s beneficial to integrate word segmenta-
tion and part-of-speech tagging into one model. The
later result is confirmed by many others (Zhang and
Clark, 2008; Jiang et al., 2008; Kruengkrai et al.,
2009). Goldberg and Tsarfaty (2008) showed that
a single model for morphological segmentation and
syntactic parsing of Hebrew yielded an error reduc-
tion of 12% over the best pipelined models. This is
because an integrated approach can effectively take
into account more information from different levels
of analysis.
Parsing of Chinese word structures can be re-
duced to the usual constituent parsing, for which
there has been great progress in the past several
years. Our generative model for unified word and
phrase structure parsing is a direct adaptation of the
model presented by Collins (2003). Many other ap-
proaches of constituent parsing also use this kind
Note that in the proposed output, all words are an-
notated with their part-of-speech tags. This is nec-
essary since part-of-speech plays an important role
in the generation of compound words. For example,
揶 ‘person’ usually combines with a verb to form a
compound noun such as 唗䕏揶 ‘designer’.
In this paper, we will actually design an integrated
morphological and syntactical parser trained with
a treebank. Therefore, the real output of our sys-
tem looks like Figure 8. It’s clear that besides all
</bodyText>
<equation confidence="0.785857">
S
✏✏ ✏ ✏ PPPP
VP
✦ ❛❛❛
✦ ✦
VV NN
✟ ✟ ❍❍
</equation>
<figure confidence="0.81799625">
VVf JJ NN
✚✚❩❩
A JJf NNf NNf
,1 䋳㢧 9
</figure>
<figureCaption confidence="0.795607666666667">
Figure 8: The actual output of our parser trained with a
fully annotated treebank.
the information of the proposed output for the new
</figureCaption>
<figure confidence="0.675935833333333">
NP
NR
✚✚❩❩
NFf
䤕 撓䏓
NGf
</figure>
<page confidence="0.95492">
1408
</page>
<bodyText confidence="0.9987165">
paradigm, our model’s output also includes higher-
level syntactic parsing results.
</bodyText>
<subsectionHeader confidence="0.9385">
3.1 Training Data
</subsectionHeader>
<figure confidence="0.9245165">
(a) NN
NNf
憞䠮䞎
(b) NN
� �
� �
JJf NNf
卣 敯埚
</figure>
<bodyText confidence="0.999807142857143">
We employ a statistical model to parse phrase and
word structures as illustrated in Figure 8. The cur-
rently available treebank for us is the Penn Chinese
Treebank (CTB) 5.0 (Xue et al., 2005). Because our
model belongs to the family of head-driven statisti-
cal parsing models (Collins, 2003), we use the head-
finding rules described by Sun and Jurafsky (2004).
Unfortunately, this treebank or any other tree-
banks for that matter, does not contain annotations
of word structures. Therefore, we must annotate
these structures by ourselves. The good news is that
the annotation is not too complicated. First, we ex-
tract all words in the treebank and check each of
them manually. Words with non-trivial structures
are thus annotated. Finally, we install these small
trees of words into the original treebank. Whether a
word has structures or not is mostly context indepen-
dent, so we only have to annotate each word once.
There are two noteworthy issues in this process.
Firstly, as we’ll see in Section 4, flat words and
non-flat words will be modeled differently, thus it’s
important to adapt the part-of-speech tags to facili-
tate this modeling strategy. For example, the tag for
nouns is NN as in 憞䠮䞎 ‘Iraq’ and 卣敯-It ‘for-
mer president’. After annotation, the former is flat,
but the later has a structure (Figure 9). So we change
the POS tag for flat nouns to NNf, then during bot-
tom up parsing, whenever a new constituent ending
with ‘f’ is found, we can assign it a probability in a
way different from a structured word or phrase.
Secondly, we should record the head position of
each word tree in accordance with the requirements
of head driven parsing models. As an example, the
right tree in Figure 9 has the context free rule “NN
→ JJf NNf”, the head of which should be the right-
most NNf. Therefore, in 卣敯-It ‘former president’
the head is 敯-It ‘president’.
In passing, the readers should note the fact that
in Figure 9, we have to add a parent labeled NN to
the flat word 憞䠮䞎 ‘Iraq’ so as not to change the
context-free rules contained inherently in the origi-
nal treebank.
</bodyText>
<figureCaption confidence="0.8249635">
Figure 9: Example word structure annotation. We add an
‘f’ to the POS tags of words with no further structures.
</figureCaption>
<sectionHeader confidence="0.997455" genericHeader="method">
4 The Model
</sectionHeader>
<bodyText confidence="0.99994675">
Given an observed raw sentences S, our generative
model tells a story about how this surface sequence
of Chinese characters is generated with a linguisti-
cally plausible morphological and syntactical pro-
cess, thereby defining a joint probability Pr(T, S)
where T is a parse tree carrying word structures as
well as phrase structures. With this model, the pars-
ing problem is to search for the tree T* such that
</bodyText>
<equation confidence="0.9991275">
T* = arg max Pr(T, S) (1)
T
</equation>
<bodyText confidence="0.999937875">
The generation of S is defined in a top down fash-
ion, which can be roughly summarized as follows.
First, the lexicalized constituent structures are gen-
erated, then the lexicalized structure of each word
is generated. Finally, flat words with no structures
are generated. As soon as this is done, we get a tree
whose leaves are Chinese characters and can be con-
catenated to get the surface character sequence S.
</bodyText>
<subsectionHeader confidence="0.998845">
4.1 Generation of Constituent Structures
</subsectionHeader>
<bodyText confidence="0.9982005">
Each node in the constituent tree corresponds to a
lexicalized context free rule
</bodyText>
<equation confidence="0.984102">
P → Ln Ln−1 ··· L1HR1 R2 ··· R. (2)
</equation>
<bodyText confidence="0.999919777777778">
where P, LZ, RZ and H are lexicalized nonterminals
and H is the head. To generate this constituent, first
P is generated, then the head child H is generated
conditioned on P, and finally each LZ and Rj are
generated conditioned on P and H and a distance
metric. This breakdown of lexicalized PCFG rules
is essentially the Model 2 defined by Collins (1999).
We refer the readers to Collins’ thesis for further de-
tails.
</bodyText>
<page confidence="0.98037">
1409
</page>
<subsectionHeader confidence="0.888563">
4.2 Generation of Words with Internal
Structures
</subsectionHeader>
<bodyText confidence="0.878194">
Words with rich internal structures can be described
using a context-free grammar formalism as
</bodyText>
<table confidence="0.882830333333333">
word → root
word → word suffix
word → prefix word
</table>
<bodyText confidence="0.99971685">
Here the root is any word without interesting internal
structures, and the prefixes and suffixes are not lim-
ited to single characters. For example, 1懂 ‘ism’ as
in 她㦓1懂 ‘modernism’ is a well known and very
productive suffix. Also, we can see that rules (4) and
(5) are recursive and hence can handle words with
very complex structures.
By (3)–(5), the generation of word structures is
exactly the same as that of ordinary phrase struc-
tures. Hence the probabilities of these words can be
defined in the same way as higher level constituents
in (2). Note that in our case, each word with struc-
tures is naturally lexicalized, since in the annotation
process we have been careful to record the head po-
sition of each complex word.
As an example, consider a word w = R(r) 5(s)
where R is the root part-of-speech headed by the
word r, and 5 is the suffix part-of-speech headed
by s. If the head of this word is its suffix, then we
can define the probability of w by
</bodyText>
<equation confidence="0.997907">
Pr(w) = Pr(5, s) · Pr(R, r|5, s) (6)
</equation>
<bodyText confidence="0.999958571428571">
This is equivalent to saying that to generate w, we
first generate its head 5(s), then conditioned on this
head, other components of this word are generated.
In actual parsing, because a word always occurs in
some contexts, the above probability should also be
conditioned on these contexts, such as its parent and
the parent’s head word.
</bodyText>
<subsectionHeader confidence="0.999479">
4.3 Generation of Flat Words
</subsectionHeader>
<bodyText confidence="0.999987444444445">
We say a word is flat if it contains only one mor-
pheme such as 憞䠮䞎 ‘Iraq’, or if it is a compound
like 䝭䅵 ‘develop’ which does not have a produc-
tive component we are currently interested in. De-
pending on whether a flat word is known or not,
their generative probabilities are computed also dif-
ferently. Generation of flat words seen in training is
trivial and deterministic since every phrase and word
structure rules are lexicalized.
However, the generation of unknown flat words
is a different story. During training, words that oc-
cur less than 6 times are substituted with the symbol
UNKNOWN. In testing, unknown words are gener-
ated after the generation of symbol UNKNOWN, and
we define their probability by a first-order Markov
model. That is, given a flat word w = c1c2 · · · cn
not seen in training, we define its probability condi-
tioned with the part-of-speech p as
</bodyText>
<equation confidence="0.996992">
n+1
Pr(w|p) = Pr(ci|ci−1,p) (7)
i=1
</equation>
<bodyText confidence="0.99965475">
where c0 is taken to be a START symbol indicating
the left boundary of a word and cn+1 is the STOP
symbol to indicate the right boundary. Note that the
generation of w is only conditioned on its part-of-
speech p, ignoring the larger constituent or word in
which w occurs.
We use a back-off strategy to smooth the proba-
bilities in (7):
</bodyText>
<equation confidence="0.999975333333333">
�Pr(ci|ci−1,p) = A1 · Pr(ci|ci−1,p)
+ A2 · Pr(ci|ci−1)
+A3 · Pr(ci) (8)
</equation>
<bodyText confidence="0.984381">
where A1 + A2 + A3 = 1 to ensure the conditional
probability is well formed. These As will be esti-
mated with held-out data. The probabilities on the
right side of (8) can be estimated with simple counts:
</bodyText>
<equation confidence="0.998561">
COUNT (ci−1ci, p)
pr(ci  |ci−1, p) = (9)
COUNT (ci−1, p)
</equation>
<bodyText confidence="0.996639">
The other probabilities can be estimated in the same
way.
</bodyText>
<subsectionHeader confidence="0.99816">
4.4 Summary of the Generative Story
</subsectionHeader>
<bodyText confidence="0.999947285714286">
We make a brief summary of our generative story for
the integrated morphological and syntactic parsing
model. For a sentence 5 and its parse tree T, if we
denote the set of lexicalized phrase structures in T
by C, the set of lexicalized word structures by W,
and the set of unknown flat words by F, then the
joint probability Pr(T, 5) according to our model is
</bodyText>
<equation confidence="0.960999">
Pr(T, 5) = fl Pr(c) fl Pr(w) fl Pr(f) (10)
cEC wEW fEF
</equation>
<page confidence="0.857367">
1410
</page>
<bodyText confidence="0.9990425">
In practice, the logarithm of this probability can be
calculated instead to avoid numerical difficulties.
</bodyText>
<sectionHeader confidence="0.952158" genericHeader="method">
5 The Parsing Algorithm
</sectionHeader>
<bodyText confidence="0.999931272727273">
To find the parse tree with highest probability we
use a chart parser adapted from Collins (1999). Two
key changes must be made to the search process,
though. Firstly, because we are proposing a new
paradigm for Chinese word segmentation, the input
to the parser must be raw sentences by definition.
Hence to use the bottom-up parser, we need a lex-
icon of all characters together with what roles they
can play in a flat word. We can get this lexicon from
the treebank. For example, from the word +愊/NNf
‘center’, we can extract a role bNNf for character +
‘middle’ and a role eNNf for character 愊 ‘center’.
The role bNNf means the beginning of the flat la-
bel NNf, while eNNf stands for the end of the label
NNf. This scheme was first proposed by Luo (2003)
in his character-based Chinese parser, and we find it
quite adequate for our purpose here.
Secondly, in the bottom-up parser for head driven
models, whenever a new edge is found, we must as-
sign it a probability and a head word. If the newly
discovered constituent is a flat word (its label ends
with ‘f’), then we set its head word to be the con-
catenation of all its child characters, i.e. the word
itself. If it is an unknown word, we use (7) to assign
the probability, otherwise its probability is set to be
1. On the other hand, if the new edge is a phrase or
word with internal structures, the probability is set
according to (2), while the head word is found with
the appropriate head rules. In this bottom-up way,
the probability for a complete parse tree is known
as soon as it is completed. This probability includes
both word generation probabilities and constituent
probabilities.
</bodyText>
<sectionHeader confidence="0.996643" genericHeader="evaluation">
6 Evaluation
</sectionHeader>
<bodyText confidence="0.999959377358491">
For several reasons, it is a little tricky to evaluate the
accuracy of our model for integrated morphological
and syntactic parsing. First and foremost, we cur-
rently know of no other same effort in parsing the
structures of Chinese words, and we have to anno-
tate word structures by ourselves. Hence there is no
baseline performance to compare with. Secondly,
simply reporting the accuracy of labeled precision
and recall is not very informative because our parser
takes raw sentences as input, and its output includes
a lot of easy cases like word segmentation and part-
of-speech tagging results.
Despite these difficulties, we note that higher-
level constituent parsing results are still somewhat
comparable with previous performance in parsing
Penn Chinese Treebank, because constituent parsing
does not involve word structures directly. Having
said that, it must be pointed out that the comparison
is meaningful only in a limited sense, as in previous
literatures on Chinese parsing, the input is always
word segmented or even part-of-speech tagged. That
is, the bracketing in our case is around characters
instead of words. Another observation is we can
still evaluate Chinese word segmentation and part-
of-speech tagging accuracy, by reading off the cor-
responding result from parse trees. Again because
we split the words with internal structures into their
components, comparison with other systems should
be viewed with that in mind.
Based on these discussions, we divide the labels
of all constituents into three categories:
Phrase labels are the labels in Peen Chinese Tree-
bank for nonterminal phrase structures, includ-
ing NP, VP, PP, etc.
POS labels represent part-of-speech tags such as
NN, VV, DEG, etc.
Flat labels are generated in our annotation for
words with no interesting structures. Recall
that they always end with an ‘f’ such as NNf,
VVf and DEGf, etc.
With this classification, we report our parser’s ac-
curacy for phrase labels, which is approximately
the accuracy of constituent parsing of Penn Chinese
Treebank. We report our parser’s word segmenta-
tion accuracy based on the flat labels. This accu-
racy is in fact the joint accuracy of segmentation
and part-of-speech tagging. Most importantly, we
can report our parser’s accuracy in recovering word
structures based on POS labels and flat labels, since
word structures may contain only these two kinds of
labels.
With the standard split of CTB 5.0 data into train-
ing, development and test sets (Zhang and Clark,
</bodyText>
<page confidence="0.980691">
1411
</page>
<bodyText confidence="0.999163166666667">
2009), the result are summarized in Table 1. For all
label categories, the PARSEEVAL measures (Abney
et al., 1991) are used in computing the labeled pre-
cision and recall.
correctly analyzed it into a root 㦌斊 ‘masterpiece’
and a suffix 䕛 ‘expert’, as in Figure 10(b). This
</bodyText>
<figure confidence="0.872530761904762">
(b) NN
❩❩
✚✚
(a) NP
✱ ❧
✱ ❧
Types LP LR F1
Phrase 79.3 80.1 79.7
Flat 93.2 93.8 93.5
Flat* 97.1 97.6 97.3
POS &amp; Flat 92.7 93.2 92.9
JJ
NNf
NNf
NN
䕛
NNf
JJf
㦌斊
㦌
斊䕛
</figure>
<tableCaption confidence="0.772424">
Table 1: Labeled precision and recall for the three types
of labels. The line labeled ‘Flat*’ is for unlabeled met-
rics of flat words, which is effectively the ordinary word
segmentation accuracy.
</tableCaption>
<bodyText confidence="0.99998278125">
Though not directly comparable, we can make
some remarks to the accuracy of our model. For
constituent parsing, the best result on CTB 5.0 is
reported to be 78% F1 measure for unlimited sen-
tences with automatically assigned POS tags (Zhang
and Clark, 2009). Our result for phrase labels is
close to this accuracy. Besides, the result for flat
labels compares favorably with the state of the art
accuracy of about 93% F1 for joint word segmen-
tation and part-of-speech tagging (Jiang et al., 2008;
Kruengkrai et al., 2009). For ordinary word segmen-
tation, the best result is reported to be around 97%
F1 on CTB 5.0 (Kruengkrai et al., 2009), while our
parser performs at 97.3%, though we should remem-
ber that the result concerns flat words only. Finally,
we see the performance of word structure recovery
is almost as good as the recognition of flat words.
This means that parsing word structures accurately
is possible with a generative model.
It is interesting to see how well the parser does
in recognizing the structure of words that were not
seen during training. For this, we sampled 100
such words including those with prefixes or suffixes
and personal names. We found that for 82 of these
words, our parser can correctly recognize their struc-
tures. This means our model has learnt something
that generalizes well to unseen words.
In error analysis, we found that the parser tends
to over generalize for prefix and suffix characters.
For example, 㦌斊䕛 ‘great writer’ is a noun phrase
consisting of an adjective 㦌 ‘great’ and a noun 斊䕛
‘writer’, as shown in Figure 10(a), but our parser in-
</bodyText>
<figureCaption confidence="0.9689485">
Figure 10: Example of parser error. Tree (a) is correct,
and (b) is the wrong result by our parser.
</figureCaption>
<bodyText confidence="0.9998355">
is because the character 䕛 ‘expert’ is a very pro-
ductive suffix, as in 䑺惆䕛 ‘chemist’ and 堉䘂䕛
‘diplomat’. This observation is illuminating because
most errors of our parser follow this pattern. Cur-
rently we don’t have any non-ad hoc way of prevent-
ing such kind of over generalization.
</bodyText>
<sectionHeader confidence="0.987259" genericHeader="conclusions">
7 Conclusion and Discussion
</sectionHeader>
<bodyText confidence="0.999720148148148">
In this paper we proposed a new paradigm for Chi-
nese word segmentation in which not only flat words
were identified but words with structures were also
parsed. We gave good reasons why this should be
done, and we presented an effective method show-
ing how this could be done. With the progress in
statistical parsing technology and the development
of large scale treebanks, the time has now come for
this paradigm shift to happen. We believe such a
new paradigm for word segmentation is linguisti-
cally justified and pragmatically beneficial to real
world applications. We showed that word struc-
tures can be recovered with high precision, though
there’s still much room for improvement, especially
for higher level constituent parsing.
Our model is generative, but discriminative mod-
els such as maximum entropy technique (Berger
et al., 1996) can be used in parsing word struc-
tures too. Many parsers using these techniques
have been proved to be quite successful (Luo, 2003;
Fung et al., 2004; Wang et al., 2006). Another
possible direction is to combine generative models
with discriminative reranking to enhance the accu-
racy (Collins and Koo, 2005; Charniak and Johnson,
2005).
Finally, we must note that the use of flat labels
such as “NNf” is less than ideal. The most impor-
</bodyText>
<page confidence="0.984094">
1412
</page>
<bodyText confidence="0.999963727272727">
tant reason these labels are used is we want to com-
pare the performance of our parser with previous re-
sults in constituent parsing, part-of-speech tagging
and word segmentation, as we did in Section 6. The
problem with this approach is that word structures
and phrase structures are then not treated in a truly
unified way, and besides the 33 part-of-speech tags
originally contained in Penn Chinese Treebank, an-
other 33 tags ending with ‘f’ are introduced. We
leave this problem open for now and plan to address
it in future work.
</bodyText>
<sectionHeader confidence="0.996882" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999957857142857">
I would like to thank Professor Maosong Sun for
many helpful discussions on topics of Chinese mor-
phological and syntactic analysis. The author is sup-
ported by NSFC under Grant No. 60873174. Heart-
felt thanks also go to the reviewers for many per-
tinent comments which have greatly improved the
presentation of this paper.
</bodyText>
<sectionHeader confidence="0.997114" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9995679375">
S. Abney, S. Flickenger, C. Gdaniec, C. Grishman,
P. Harrison, D. Hindle, R. Ingria, F. Jelinek, J. Kla-
vans, M. Liberman, M. Marcus, S. Roukos, B. San-
torini, and T. Strzalkowski. 1991. Procedure for quan-
titatively comparing the syntactic coverage of English
grammars. In E. Black, editor, Proceedings of the
workshop on Speech and Natural Language, HLT ’91,
pages 306–311, Morristown, NJ, USA. Association for
Computational Linguistics.
Adam L. Berger, Vincent J. Della Pietra, and Stephen A.
Della Pietra. 1996. A maximum entropy approach to
natural language processing. Computational Linguis-
tics, 22(1):39–71.
Daniel M. Bikel and David Chiang. 2000. Two statis-
tical parsing models applied to the Chinese treebank.
In Second Chinese Language Processing Workshop,
pages 1–6, Hong Kong, China, October. Association
for Computational Linguistics.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics, ACL ’05,
pages 173–180, Morristown, NJ, USA. Association for
Computational Linguistics.
Eugene Charniak. 1997. Statistical parsing with a
context-free grammar and word statistics. In Proceed-
ings of the fourteenth national conference on artificial
intelligence and ninth conference on Innovative ap-
plications of artificial intelligence, AAAI’97/IAAI’97,
pages 598–603. AAAI Press.
Michael Collins and Terry Koo. 2005. Discrimina-
tive reranking for natural language parsing. Compu-
tational Linguistics, 31:25–70, March.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Linguis-
tics, 29(4):589–637.
Pascale Fung, Grace Ngai, Yongsheng Yang, and Ben-
feng Chen. 2004. A maximum-entropy Chinese
parser augmented by transformation-based learning.
ACM Transactions on Asian Language Information
Processing, 3:159–168, June.
Jianfeng Gao, Andi Wu, Cheng-Ning Huang, Hong qiao
Li, Xinsong Xia, and Hauwei Qin. 2004. Adaptive
Chinese word segmentation. In Proceedings of the
42nd Meeting of the Association for Computational
Linguistics (ACL’04), Main Volume, pages 462–469,
Barcelona, Spain, July.
Jianfeng Gao, Mu Li, Andi Wu, and Chang-Ning Huang.
2005. Chinese word segmentation and named entity
recognition: A pragmatic approach. Computational
Linguistics, 31(4):531–574.
Yoav Goldberg and Reut Tsarfaty. 2008. A single gener-
ative model for joint morphological segmentation and
syntactic parsing. In Proceedings of ACL-08: HLT,
pages 371–379, Columbus, Ohio, June. Association
for Computational Linguistics.
Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan L¨u.
2008. A cascaded linear model for joint Chinese word
segmentation and part-of-speech tagging. In Proceed-
ings of ACL-08: HLT, pages 897–904, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Au-
tomatic adaptation of annotation standards: Chinese
word segmentation and POS tagging – a case study. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP, pages 522–530, Suntec, Singapore, Au-
gust. Association for Computational Linguistics.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun’ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hybrid
model for joint Chinese word segmentation and POS
tagging. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
</reference>
<page confidence="0.594457">
1413
</page>
<reference confidence="0.999852134831461">
ing of the AFNLP, pages 513–521, Suntec, Singapore,
August. Association for Computational Linguistics.
Zhongguo Li and Maosong Sun. 2009. Punctuation as
implicit annotations for Chinese word segmentation.
Computational Linguistics, 35:505–512, December.
Junhui Li, Guodong Zhou, and Hwee Tou Ng. 2010.
Joint syntactic and semantic parsing of Chinese. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, pages 1108–
1117, Uppsala, Sweden, July. Association for Compu-
tational Linguistics.
Xiaoqiang Luo. 2003. A maximum entropy Chinese
character-based parser. In Michael Collins and Mark
Steedman, editors, Proceedings of the 2003 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 192–199.
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-of-
speech tagging: One-at-a-time or all-at-once? word-
based or character-based? In Dekang Lin and Dekai
Wu, editors, Proceedings of EMNLP 2004, pages 277–
284, Barcelona, Spain, July. Association for Computa-
tional Linguistics.
Richard Sproat and Thomas Emerson. 2003. The first
international Chinese word segmentation bakeoff. In
Proceedings of the Second SIGHAN Workshop on Chi-
nese Language Processing, pages 133–143, Sapporo,
Japan, July. Association for Computational Linguis-
tics.
Richard Sproat, William Gale, Chilin Shih, and Nancy
Chang. 1996. A stochastic finite-state word-
segmentation algorithm for Chinese. Computational
Linguistics, 22(3):377–404.
Honglin Sun and Daniel Jurafsky. 2004. Shallow se-
mantc parsing of Chinese. In Daniel Marcu Su-
san Dumais and Salim Roukos, editors, HLT-NAACL
2004: Main Proceedings, pages 249–256, Boston,
Massachusetts, USA, May 2 - May 7. Association for
Computational Linguistics.
Mengqiu Wang, Kenji Sagae, and Teruko Mitamura.
2006. A fast, accurate deterministic parser for chinese.
In Proceedings of the 21st International Conference
on Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics,
pages 425–432, Sydney, Australia, July. Association
for Computational Linguistics.
Andi Wu and Jiang Zixin. 1998. Word segmentation in
sentence analysis. In Proceedings of the 1998Interna-
tional Conference on Chinese information processing,
Beijing, China.
Andi Wu. 2003. Customizable segmentation of morpho-
logically derived words in Chinese. Computational
Linguistics and Chinese language processing, 8(1):1–
28.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese Treebank: phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2):207–238.
Nianwen Xue. 2003. Chinese word segmentation as
character tagging. Computational Linguistics and
Chinese Language Processing, 8(1):29–48.
Yue Zhang and Stephen Clark. 2007. Chinese segmenta-
tion with a word-based perceptron algorithm. In Pro-
ceedings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 840–847, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Yue Zhang and Stephen Clark. 2008. Joint word segmen-
tation and POS tagging using a single perceptron. In
Proceedings ofACL-08: HLT, pages 888–896, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Yue Zhang and Stephen Clark. 2009. Transition-based
parsing of the Chinese treebank using a global dis-
criminative model. In Proceedings of the 11th Inter-
national Conference on Parsing Technologies, IWPT
’09, pages 162–171, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Hai Zhao. 2009. Character-level dependencies in Chi-
nese: Usefulness and learning. In Proceedings of the
12th Conference of the European Chapter of the ACL
(EACL 2009), pages 879–887, Athens, Greece, March.
Association for Computational Linguistics.
Guodong Zhou and Jian Su. 2003. A Chinese effi-
cient analyser integrating word segmentation, part-of-
speech tagging, partial parsing and full parsing. In
Proceedings of the Second SIGHAN Workshop on Chi-
nese Language Processing, pages 78–83, Sapporo,
Japan, July. Association for Computational Linguis-
tics.
</reference>
<page confidence="0.994687">
1414
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9998675">Parsing the Internal Structure of Words: A New Paradigm for Chinese Word Segmentation</title>
<author confidence="0.74953">Zhongguo</author>
<affiliation confidence="0.859512">State Key Laboratory on Intelligent Technology and Tsinghua National Laboratory for Information Science and Department of Computer Science and Tsinghua University, Beijing 100084,</affiliation>
<email confidence="0.999436">eemath@gmail.com</email>
<abstract confidence="0.993533521031209">Lots of Chinese characters are very productive in that they can form many structured words either as prefixes or as suffixes. Previous research in Chinese word segmentation mainly focused on identifying only the word boundaries without considering the rich internal structures of many words. In this paper we argue that this is unsatisfying in many ways, both practically and theoretically. Instead, we propose that word structures should be recovered in morphological analysis. An elegant approach for doing this is given and the result is shown to be promising enough for encouraging further effort in this direction. Our probability model is trained with the Penn Chinese Treebank and actually is able to parse both word and phrase structures in a unified way. 1 Why Parse Word Structures? Research in Chinese word segmentation has progressed tremendously in recent years, with state of the art performing at around 97% in precision and recall (Xue, 2003; Gao et al., 2005; Zhang and Clark, 2007; Li and Sun, 2009). However, virtually all these systems focus exclusively on recognizing the word boundaries, giving no consideration to the internal structures of many words. Though it has been the standard practice for many years, we argue that this paradigm is inadequate both in theory and in practice, for at least the following four reasons. first reason that if we confine our definition of word segmentation to the identification of word boundaries, then people tend to have divergent opinions as to whether a linguistic unit is a word or not (Sproat et al., 1996). This has led to many different annotation standards for Chinese word segmentation. Even worse, this could cause inconsisin the same corpus. For instance, ‘vice president’ is considered to be one word in the Penn Chinese Treebank (Xue et al., 2005), but is split into two words by the Peking University corpus in the SIGHAN Bakeoffs (Sproat and Emerson, Meanwhile, director’ and manager’ are both segmented into two words in the same Penn Chinese Treebank. In fact, these words are composed of the prefix a root word. Thus the structure of president’ can be represented with the tree in Fig- 1. Without a doubt, there is complete agree- NN � � � � JJf NNf 䉂 擌奒 Figure 1: Example of a word with internal structure. ment on the correctness of this structure among native Chinese speakers. So if instead of annotating only word boundaries, we annotate the structures of word, 1then the annotation tends to be more it is necessary to add a note on terminology used in this paper. Since there is no universally accepted definition of the “word” concept in linguistics and especially in Chinese, whenever we use the term “word” we might mean a linguistic such as president’ whose structure is shown as the tree in Figure 1, or we might mean a smaller unit such as which is a substructure of that tree. Hopefully, 1405 of the 49th Annual Meeting of the Association for Computational pages 1405–1414, Oregon, June 19-24, 2011. Association for Computational Linguistics consistent and there could be less duplication of efforts in developing the expensive annotated corpus. second reason applications have different requirements for granularity of words. Take the pername Shuren’ as an example. It’s considered to be one word in the Penn Chinese Treebank, but is segmented into a surname and a given name in the Peking University corpus. For some applications such as information extraction, the former segmentation is adequate, while for others like machine translation, the later finer-grained output is more preferable. If the analyzer can produce a structure as shown in Figure 4(a), then every application can extract what it needs from this tree. A solution with tree output like this is more elegant than approaches which try to meet the needs of different applications in post-processing (Gao et al., 2004). third reason that traditional word segmentation has problems in handling many phenomena in Chinese. For example, the telescopic compound middle schools and primary schools’ is in fact composed of three coordinating elschool’ and school’. Regarding it as one flat word loses this important information. Another example separable words like With a linear segmentation, the meaning of ‘swimming’ as in swimming’ cannot be properly repsince will be segmented into discontinuous units. These language usages lie at the boundary between syntax and morphology, and are not uncommon in Chinese. They can be adequately represented with trees (Figure 2). Figure 2: Example of telescopic compound (a) and separable word (b). last reason we should care about word the context will always make it clear what is being referred to with the term word”. structures is related to head driven statistical parsers (Collins, 2003). To illustrate this, note that in the Chinese Treebank, the word ‘English People’ does not occur at all. Hence constituents headed by such words could cause some difficulty for head driven models in which out-ofvocabulary words need to be treated specially both when they are generated and when they are conditioned upon. But this word is in turn headed by its and there are 2,233 such words in Penn Chinese Treebank. If we annotate the structure of every compound containing this suffix (e.g. Figure 3), such data sparsity simply goes away. NN ❜❜ ✧ ✧ NRf NNf 戽䊂䠽 吼 3: Structure of the out-of-vocabulary word People’. Had there been only a few words with internal structures, current Chinese word segmentation paradigm would be sufficient. We could simply recover word structures in post-processing. But this is far from the truth. In Chinese there is a large number of such words. We just name a few classes of these words and give one example for each class (a dot is used to separate roots from affixes): name: Makoto’ name: York State’ with a suffix: with a prefix: with a suffix: with a prefix: with a suffix: with a prefix: with a prefix: expression: year 1995’ number: suffixation: This list is not meant to be complete, but we can get a feel of how extensive the words with non-trivial structures can be. With so many productive suffixes and prefixes, analyzing word structures in postprocessing is difficult, because a character may or may not act as an affix depending on the context. (a) NN ✟✟ ✟ ❍❍❍ JJ NNf ✟ (b) VV VV NNf ✚ ✚ ❩❩ JJf JJf VVf 㦌 撥 怂 扩 堑 1406 example, the character in ‘the one who plants’ is a suffix, but in the personal Shuren’ it isn’t. The structures of these two words are shown in Figure 4. (a) NR (b) NN ✚✚❩❩ ✚✚ ❩❩ NFf NGf VVf NNf 撱 嗤吼 撇嗤 吼 Figure 4: Two words that differ only in one character, have different internal structures. The character ‘people’ is part of a personal name in tree (a), but is a suffix in (b). A second reason why generally we cannot recover word structures in post-processing is that some words have very complex structures. For example, tree of is shown in Figure 5. Parsing this structure correctly without a principled method is difficult and messy, if not impossible. ❛❛❛ ✦ ✦ NN NNf ✟✟ ✟ ❍❍❍ NNf ✚✚ ❩❩ NNf 壃 搕䈿 Figure 5: An example word which has very complex structures. Finally, it must be mentioned that we cannot store all word structures in a dictionary, as the word formation process is very dynamic and productive in Take as an example. Standard Chidictionaries usually contain not many other words such as ‘aquarium’ generated by this same character. This is unsince the character is so productive that it is impossible for a dictionary to list every word with this character as a suffix. The same thing happens for natural language processing systems. Thus it is necessary to have a dynamic mechanism for parsing word structures. In this paper, we propose a new paradigm for Chinese word segmentation in which not only word boundaries are identified but the internal structures of words are recovered (Section 3). To achieve this, we design a joint morphological and syntactic parsing model of Chinese (Section 4). Our generative story describes the complete process from sentence and word structures to the surface string of characters in a top-down fashion. With this probability model, we give an algorithm to find the parse tree of a raw sentence with the highest probability (Section 5). The output of our parser incorporates word structures naturally. Evaluation shows that the model can learn much of the regularity of word structures, and also achieves reasonable accuracy in parsing higher level constituent structures (Section 6). 2 Related Work The necessity of parsing word structures has been noticed by Zhao (2009), who presented a characterlevel dependency scheme as an alternative to the linear representation of words. Although our work is based on the same notion, there are two key differences. The first one is that part-of-speech tags and constituent labels are fundamental for our parsing model, while Zhao focused on unlabeled dependencies between characters in a word, and part-ofspeech information was not utilized. Secondly, we distinguish explicitly the generation of flat words as and words with internal structures. Our parsing algorithm also has to be adapted accordingly. Such distinction was not made in Zhao’s parsing model and algorithm. Many researchers have also noticed the awkwardness and insufficiency of current boundary-only Chinese word segmentation paradigm, so they tried to customize the output to meet the requirements of various applications (Wu, 2003; Gao et al., 2004). In a related research, Jiang et al. (2009) presented a strategy to transfer annotated corpora between different segmentation standards in the hope of saving some expensive human labor. We believe the best solution to the problem of divergent standards and requirements is to annotate and analyze word structures. Then applications can make use of these structures according to their own convenience. 1407 Since the distinction between morphology and syntax in Chinese is somewhat blurred, our model for word structure parsing is integrated with constituent parsing. There has been many efforts to integrate Chinese word segmentation, part-of-speech tagging and parsing (Wu and Zixin, 1998; Zhou and Su, 2003; Luo, 2003; Fung et al., 2004). However, in these research all words were considered to be flat, and thus word structures were not parsed. This is a crucial difference with our work. Specifically, the word oil’. Our parser output tree Figure 6(a), while Luo (2003) output tree (b), giving no hint to the structure of this word since result is the same with a real flat word ‘Los Angeles’(c). of head-driven generative models (Charniak, 1997; Bikel and Chiang, 2000) . 3 The New Paradigm a raw Chinese sentence like 䏓 A敯 㢧 a traditional word segmentation system output some result like A 敯䋳㢧 Zhihao’, ‘is’, ‘chief engineer’). In our new paradigm, the output should at least be a linear sequence of trees representing the structures of each word as in Figure 7. VV VVf JJ NGf NFf NR ✑◗◗ ✑ NN ✟✟ ✟ ❍❍❍ NN ❩❩ ✚✚ 撓䏓 A NNf NNf ,1 䋳㢧 9 Figure 7: Proposed output for the new Chinese word segmentation paradigm. (a) NN ✚✚❩❩ NNf NNf ta 扨 (b) NN NNf ta扨 (c) NR NRf 䧢哫膝 Figure 6: Difference between our output (a) of parsing word oil’ and the output (b) of Luo (2003). In (c) we have a true flat word, namely the locaname Angeles’. The benefits of joint modeling has been noticed by many. For example, Li et al. (2010) reported that a joint syntactic and semantic model improved the accuracy of both tasks, while Ng and Low (2004) showed it’s beneficial to integrate word segmentation and part-of-speech tagging into one model. The later result is confirmed by many others (Zhang and Clark, 2008; Jiang et al., 2008; Kruengkrai et al., 2009). Goldberg and Tsarfaty (2008) showed that a single model for morphological segmentation and syntactic parsing of Hebrew yielded an error reduction of 12% over the best pipelined models. This is because an integrated approach can effectively take into account more information from different levels of analysis. Parsing of Chinese word structures can be reduced to the usual constituent parsing, for which there has been great progress in the past several years. Our generative model for unified word and phrase structure parsing is a direct adaptation of the model presented by Collins (2003). Many other approaches of constituent parsing also use this kind Note that in the proposed output, all words are annotated with their part-of-speech tags. This is necessary since part-of-speech plays an important role in the generation of compound words. For example, usually combines with a verb to form a noun such as In this paper, we will actually design an integrated morphological and syntactical parser trained with a treebank. Therefore, the real output of our system looks like Figure 8. It’s clear that besides all S ✏ ✏ VP ✦ ❛❛❛ ✦ ✦ VV NN VVf JJ NN ✚✚❩❩ NNf NNf ,1 䋳㢧 9 Figure 8: The actual output of our parser trained with a fully annotated treebank. the information of the proposed output for the new NR ✚✚❩❩ NFf 䤕 撓䏓 NGf 1408 paradigm, our model’s output also includes higherlevel syntactic parsing results. 3.1 Training Data (a) NN NNf 憞䠮䞎 (b) NN � � � � JJf NNf 卣 敯埚 We employ a statistical model to parse phrase and word structures as illustrated in Figure 8. The currently available treebank for us is the Penn Chinese Treebank (CTB) 5.0 (Xue et al., 2005). Because our model belongs to the family of head-driven statistical parsing models (Collins, 2003), we use the headfinding rules described by Sun and Jurafsky (2004). Unfortunately, this treebank or any other treebanks for that matter, does not contain annotations of word structures. Therefore, we must annotate these structures by ourselves. The good news is that the annotation is not too complicated. First, we extract all words in the treebank and check each of them manually. Words with non-trivial structures are thus annotated. Finally, we install these small trees of words into the original treebank. Whether a word has structures or not is mostly context independent, so we only have to annotate each word once. There are two noteworthy issues in this process. Firstly, as we’ll see in Section 4, flat words and non-flat words will be modeled differently, thus it’s important to adapt the part-of-speech tags to facilitate this modeling strategy. For example, the tag for is NN as in and ‘former president’. After annotation, the former is flat, but the later has a structure (Figure 9). So we change the POS tag for flat nouns to NNf, then during bottom up parsing, whenever a new constituent ending with ‘f’ is found, we can assign it a probability in a way different from a structured word or phrase. Secondly, we should record the head position of each word tree in accordance with the requirements of head driven parsing models. As an example, the right tree in Figure 9 has the context free rule “NN NNf”, the head of which should be the right- NNf. Therefore, in president’ head is In passing, the readers should note the fact that in Figure 9, we have to add a parent labeled NN to flat word so as not to change the context-free rules contained inherently in the original treebank. Figure 9: Example word structure annotation. We add an ‘f’ to the POS tags of words with no further structures. 4 The Model an observed raw sentences our generative model tells a story about how this surface sequence of Chinese characters is generated with a linguistically plausible morphological and syntactical prothereby defining a joint probability a parse tree carrying word structures as well as phrase structures. With this model, the parsproblem is to search for the tree such that arg max T generation of defined in a top down fashion, which can be roughly summarized as follows. First, the lexicalized constituent structures are generated, then the lexicalized structure of each word is generated. Finally, flat words with no structures are generated. As soon as this is done, we get a tree whose leaves are Chinese characters and can be conto get the surface character sequence 4.1 Generation of Constituent Structures Each node in the constituent tree corresponds to a lexicalized context free rule lexicalized nonterminals the head. To generate this constituent, first generated, then the head child generated on and finally each conditioned on a distance metric. This breakdown of lexicalized PCFG rules is essentially the Model 2 defined by Collins (1999). We refer the readers to Collins’ thesis for further details. 1409 4.2 Generation of Words with Internal Structures Words with rich internal structures can be described using a context-free grammar formalism as word → root word → word suffix word → prefix word Here the root is any word without interesting internal structures, and the prefixes and suffixes are not limto single characters. For example, as is a well known and very productive suffix. Also, we can see that rules (4) and (5) are recursive and hence can handle words with very complex structures. By (3)–(5), the generation of word structures is exactly the same as that of ordinary phrase structures. Hence the probabilities of these words can be defined in the same way as higher level constituents in (2). Note that in our case, each word with structures is naturally lexicalized, since in the annotation process we have been careful to record the head position of each complex word. an example, consider a word the root part-of-speech headed by the and the suffix part-of-speech headed If the head of this word is its suffix, then we define the probability of = is equivalent to saying that to generate we generate its head then conditioned on this head, other components of this word are generated. In actual parsing, because a word always occurs in some contexts, the above probability should also be conditioned on these contexts, such as its parent and the parent’s head word. 4.3 Generation of Flat Words We say a word is flat if it contains only one morsuch as or if it is a compound which does not have a productive component we are currently interested in. Depending on whether a flat word is known or not, their generative probabilities are computed also differently. Generation of flat words seen in training is trivial and deterministic since every phrase and word structure rules are lexicalized. However, the generation of unknown flat words is a different story. During training, words that occur less than 6 times are substituted with the symbol In testing, unknown words are generafter the generation of symbol and we define their probability by a first-order Markov That is, given a flat word · · not seen in training, we define its probability condiwith the part-of-speech = taken to be a indicating left boundary of a word and is the symbol to indicate the right boundary. Note that the of only conditioned on its part-ofignoring the larger constituent or word in We use a back-off strategy to smooth the probabilities in (7): = 1 ensure the conditional is well formed. These will be estimated with held-out data. The probabilities on the right side of (8) can be estimated with simple counts: = The other probabilities can be estimated in the same way. 4.4 Summary of the Generative Story We make a brief summary of our generative story for the integrated morphological and syntactic parsing For a sentence its parse tree if we the set of lexicalized phrase structures in the set of lexicalized word structures by the set of unknown flat words by then the probability to our model is = flflfl 1410 In practice, the logarithm of this probability can be calculated instead to avoid numerical difficulties. 5 The Parsing Algorithm To find the parse tree with highest probability we use a chart parser adapted from Collins (1999). Two key changes must be made to the search process, though. Firstly, because we are proposing a new paradigm for Chinese word segmentation, the input to the parser must be raw sentences by definition. Hence to use the bottom-up parser, we need a lexicon of all characters together with what roles they can play in a flat word. We can get this lexicon from treebank. For example, from the word we can extract a role bNNf for character and a role eNNf for character The role bNNf means the beginning of the flat label NNf, while eNNf stands for the end of the label NNf. This scheme was first proposed by Luo (2003) in his character-based Chinese parser, and we find it quite adequate for our purpose here. Secondly, in the bottom-up parser for head driven models, whenever a new edge is found, we must assign it a probability and a head word. If the newly discovered constituent is a flat word (its label ends with ‘f’), then we set its head word to be the concatenation of all its child characters, i.e. the word itself. If it is an unknown word, we use (7) to assign the probability, otherwise its probability is set to be 1. On the other hand, if the new edge is a phrase or word with internal structures, the probability is set according to (2), while the head word is found with the appropriate head rules. In this bottom-up way, the probability for a complete parse tree is known as soon as it is completed. This probability includes both word generation probabilities and constituent probabilities. 6 Evaluation For several reasons, it is a little tricky to evaluate the accuracy of our model for integrated morphological and syntactic parsing. First and foremost, we currently know of no other same effort in parsing the structures of Chinese words, and we have to annotate word structures by ourselves. Hence there is no baseline performance to compare with. Secondly, simply reporting the accuracy of labeled precision and recall is not very informative because our parser takes raw sentences as input, and its output includes a lot of easy cases like word segmentation and partof-speech tagging results. Despite these difficulties, we note that higherlevel constituent parsing results are still somewhat comparable with previous performance in parsing Penn Chinese Treebank, because constituent parsing does not involve word structures directly. Having said that, it must be pointed out that the comparison is meaningful only in a limited sense, as in previous literatures on Chinese parsing, the input is always word segmented or even part-of-speech tagged. That is, the bracketing in our case is around characters instead of words. Another observation is we can still evaluate Chinese word segmentation and partof-speech tagging accuracy, by reading off the corresponding result from parse trees. Again because we split the words with internal structures into their components, comparison with other systems should be viewed with that in mind. Based on these discussions, we divide the labels of all constituents into three categories: labels the labels in Peen Chinese Treebank for nonterminal phrase structures, including NP, VP, PP, etc. labels part-of-speech tags such as NN, VV, DEG, etc. labels generated in our annotation for words with no interesting structures. Recall that they always end with an ‘f’ such as NNf, VVf and DEGf, etc. With this classification, we report our parser’s accuracy for phrase labels, which is approximately the accuracy of constituent parsing of Penn Chinese Treebank. We report our parser’s word segmentation accuracy based on the flat labels. This accuracy is in fact the joint accuracy of segmentation and part-of-speech tagging. Most importantly, we can report our parser’s accuracy in recovering word structures based on POS labels and flat labels, since word structures may contain only these two kinds of labels. With the standard split of CTB 5.0 data into training, development and test sets (Zhang and Clark, 1411 2009), the result are summarized in Table 1. For all label categories, the PARSEEVAL measures (Abney et al., 1991) are used in computing the labeled precision and recall. analyzed it into a root a suffix as in Figure 10(b). This (b) NN ❩❩ ✚✚ (a) NP ❧ ✱ ❧ Types LP LR Phrase 79.3 80.1 79.7 Flat 93.2 93.8 93.5 Flat* 97.1 97.6 97.3 POS &amp; Flat 92.7 93.2 92.9 JJ NNf NNf NN 䕛 NNf JJf 㦌斊 㦌 斊䕛 Table 1: Labeled precision and recall for the three types of labels. The line labeled ‘Flat*’ is for unlabeled metrics of flat words, which is effectively the ordinary word segmentation accuracy. Though not directly comparable, we can make some remarks to the accuracy of our model. For constituent parsing, the best result on CTB 5.0 is to be 78% for unlimited sentences with automatically assigned POS tags (Zhang and Clark, 2009). Our result for phrase labels is close to this accuracy. Besides, the result for flat labels compares favorably with the state of the art of about 93% joint word segmentation and part-of-speech tagging (Jiang et al., 2008; Kruengkrai et al., 2009). For ordinary word segmentation, the best result is reported to be around 97% CTB 5.0 (Kruengkrai et al., 2009), while our parser performs at 97.3%, though we should remember that the result concerns flat words only. Finally, we see the performance of word structure recovery is almost as good as the recognition of flat words. This means that parsing word structures accurately is possible with a generative model. It is interesting to see how well the parser does in recognizing the structure of words that were not seen during training. For this, we sampled 100 such words including those with prefixes or suffixes and personal names. We found that for 82 of these words, our parser can correctly recognize their structures. This means our model has learnt something that generalizes well to unseen words. In error analysis, we found that the parser tends to over generalize for prefix and suffix characters. example, writer’ is a noun phrase of an adjective and a noun as shown in Figure 10(a), but our parser in- Figure 10: Example of parser error. Tree (a) is correct, and (b) is the wrong result by our parser. because the character is a very prosuffix, as in and ‘diplomat’. This observation is illuminating because most errors of our parser follow this pattern. Currently we don’t have any non-ad hoc way of preventing such kind of over generalization. 7 Conclusion and Discussion In this paper we proposed a new paradigm for Chinese word segmentation in which not only flat words were identified but words with structures were also parsed. We gave good reasons why this should be done, and we presented an effective method showing how this could be done. With the progress in statistical parsing technology and the development of large scale treebanks, the time has now come for this paradigm shift to happen. We believe such a new paradigm for word segmentation is linguistically justified and pragmatically beneficial to real world applications. We showed that word structures can be recovered with high precision, though there’s still much room for improvement, especially for higher level constituent parsing. Our model is generative, but discriminative models such as maximum entropy technique (Berger et al., 1996) can be used in parsing word structures too. Many parsers using these techniques have been proved to be quite successful (Luo, 2003; Fung et al., 2004; Wang et al., 2006). Another possible direction is to combine generative models with discriminative reranking to enhance the accuracy (Collins and Koo, 2005; Charniak and Johnson, 2005). Finally, we must note that the use of flat labels as “NNf” is less than ideal. The most impor- 1412 tant reason these labels are used is we want to compare the performance of our parser with previous results in constituent parsing, part-of-speech tagging and word segmentation, as we did in Section 6. The problem with this approach is that word structures and phrase structures are then not treated in a truly unified way, and besides the 33 part-of-speech tags originally contained in Penn Chinese Treebank, another 33 tags ending with ‘f’ are introduced. We leave this problem open for now and plan to address it in future work. Acknowledgments I would like to thank Professor Maosong Sun for many helpful discussions on topics of Chinese morphological and syntactic analysis. The author is supported by NSFC under Grant No. 60873174. Heartfelt thanks also go to the reviewers for many pertinent comments which have greatly improved the presentation of this paper.</abstract>
<title confidence="0.923987">References</title>
<author confidence="0.935878666666667">S Abney</author>
<author confidence="0.935878666666667">S Flickenger</author>
<author confidence="0.935878666666667">C Gdaniec</author>
<author confidence="0.935878666666667">C Grishman</author>
<author confidence="0.935878666666667">P Harrison</author>
<author confidence="0.935878666666667">D Hindle</author>
<author confidence="0.935878666666667">R Ingria</author>
<author confidence="0.935878666666667">F Jelinek</author>
<author confidence="0.935878666666667">J Klavans</author>
<author confidence="0.935878666666667">M Liberman</author>
<author confidence="0.935878666666667">M Marcus</author>
<author confidence="0.935878666666667">S Roukos</author>
<author confidence="0.935878666666667">B San-</author>
<abstract confidence="0.912876148148148">torini, and T. Strzalkowski. 1991. Procedure for quantitatively comparing the syntactic coverage of English In E. Black, editor, of the on Speech and Natural HLT ’91, pages 306–311, Morristown, NJ, USA. Association for Computational Linguistics. Adam L. Berger, Vincent J. Della Pietra, and Stephen A. Della Pietra. 1996. A maximum entropy approach to language processing. Linguis- 22(1):39–71. Daniel M. Bikel and David Chiang. 2000. Two statistical parsing models applied to the Chinese treebank. Chinese Language Processing pages 1–6, Hong Kong, China, October. Association for Computational Linguistics. Eugene Charniak and Mark Johnson. 2005. Coarse-tofine n-best parsing and maxent discriminative rerank- In of the 43rd Annual Meeting on for Computational ACL ’05, pages 173–180, Morristown, NJ, USA. Association for Computational Linguistics. Eugene Charniak. 1997. Statistical parsing with a grammar and word statistics. In Proceedings of the fourteenth national conference on artificial intelligence and ninth conference on Innovative apof artificial AAAI’97/IAAI’97, pages 598–603. AAAI Press.</abstract>
<author confidence="0.571153">Discrimina-</author>
<affiliation confidence="0.758412">reranking for natural language parsing. Compu-</affiliation>
<address confidence="0.828676">31:25–70, March.</address>
<note confidence="0.830859183333333">Collins. 1999. Statistical Models Natural Language Parsing. thesis, University of Pennsylvania. Michael Collins. 2003. Head-driven statistical models natural language parsing. Linguis- 29(4):589–637. Pascale Fung, Grace Ngai, Yongsheng Yang, and Benfeng Chen. 2004. A maximum-entropy Chinese parser augmented by transformation-based learning. ACM Transactions on Asian Language Information 3:159–168, June. Jianfeng Gao, Andi Wu, Cheng-Ning Huang, Hong qiao Li, Xinsong Xia, and Hauwei Qin. 2004. Adaptive word segmentation. In of the 42nd Meeting of the Association for Computational (ACL’04), Main pages 462–469, Barcelona, Spain, July. Jianfeng Gao, Mu Li, Andi Wu, and Chang-Ning Huang. 2005. Chinese word segmentation and named entity A pragmatic approach. 31(4):531–574. Yoav Goldberg and Reut Tsarfaty. 2008. A single generative model for joint morphological segmentation and parsing. In of ACL-08: pages 371–379, Columbus, Ohio, June. Association for Computational Linguistics. Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan L¨u. 2008. A cascaded linear model for joint Chinese word and part-of-speech tagging. In Proceedof ACL-08: pages 897–904, Columbus, Ohio, June. Association for Computational Linguistics. Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Automatic adaptation of annotation standards: Chinese word segmentation and POS tagging – a case study. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of pages 522–530, Suntec, Singapore, August. Association for Computational Linguistics. Canasai Kruengkrai, Kiyotaka Uchimoto, Jun’ichi Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi Isahara. 2009. An error-driven word-character hybrid model for joint Chinese word segmentation and POS In of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th Interna- Joint Conference on Natural Language Process- 1413 of the pages 513–521, Suntec, Singapore, August. Association for Computational Linguistics. Zhongguo Li and Maosong Sun. 2009. Punctuation as implicit annotations for Chinese word segmentation. 35:505–512, December. Junhui Li, Guodong Zhou, and Hwee Tou Ng. 2010. Joint syntactic and semantic parsing of Chinese. In Proceedings of the 48th Annual Meeting of the Asfor Computational pages 1108– 1117, Uppsala, Sweden, July. Association for Computational Linguistics. Xiaoqiang Luo. 2003. A maximum entropy Chinese</note>
<abstract confidence="0.6335921">character-based parser. In Michael Collins and Mark editors, of the 2003 Conference on Empirical Methods in Natural Language Propages 192–199. Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-ofspeech tagging: One-at-a-time or all-at-once? wordbased or character-based? In Dekang Lin and Dekai editors, of EMNLP pages 277– 284, Barcelona, Spain, July. Association for Computational Linguistics.</abstract>
<author confidence="0.872074">The first</author>
<affiliation confidence="0.7889585">international Chinese word segmentation bakeoff. In Proceedings of the Second SIGHAN Workshop on Chi-</affiliation>
<address confidence="0.7488205">Language pages 133–143, Sapporo, Japan, July. Association for Computational Linguis-</address>
<email confidence="0.737769">tics.</email>
<author confidence="0.649291">Richard Sproat</author>
<author confidence="0.649291">William Gale</author>
<author confidence="0.649291">Chilin Shih</author>
<author confidence="0.649291">Nancy</author>
<abstract confidence="0.7485522">1996. A stochastic finite-state algorithm for Chinese. 22(3):377–404. Honglin Sun and Daniel Jurafsky. 2004. Shallow semantc parsing of Chinese. In Daniel Marcu Su-</abstract>
<note confidence="0.943161631578947">Dumais and Salim Roukos, editors, Main pages 249–256, Boston, Massachusetts, USA, May 2 - May 7. Association for Computational Linguistics. Mengqiu Wang, Kenji Sagae, and Teruko Mitamura. 2006. A fast, accurate deterministic parser for chinese. of the 21st International Conference on Computational Linguistics and 44th Annual Meetof the Association for Computational pages 425–432, Sydney, Australia, July. Association for Computational Linguistics. Andi Wu and Jiang Zixin. 1998. Word segmentation in analysis. In of the 1998Interna- Conference on Chinese information Beijing, China. Andi Wu. 2003. Customizable segmentation of morphoderived words in Chinese. and Chinese language 8(1):1– 28.</note>
<title confidence="0.510506">Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha</title>
<author confidence="0.441352">The Penn Chinese Treebank phrase</author>
<abstract confidence="0.724942571428571">annotation of a large corpus. Lan- 11(2):207–238. Nianwen Xue. 2003. Chinese word segmentation as tagging. Linguistics and Language 8(1):29–48. Yue Zhang and Stephen Clark. 2007. Chinese segmentawith a word-based perceptron algorithm. In Pro-</abstract>
<note confidence="0.96992484">ceedings of the 45th Annual Meeting of the Association Computational pages 840–847, Prague, Czech Republic, June. Association for Computational Linguistics. Yue Zhang and Stephen Clark. 2008. Joint word segmentation and POS tagging using a single perceptron. In ofACL-08: pages 888–896, Columbus, Ohio, June. Association for Computational Linguistics. Yue Zhang and Stephen Clark. 2009. Transition-based parsing of the Chinese treebank using a global dismodel. In of the 11th Inter- Conference on Parsing IWPT ’09, pages 162–171, Morristown, NJ, USA. Association for Computational Linguistics. Hai Zhao. 2009. Character-level dependencies in Chi- Usefulness and learning. In of the 12th Conference of the European Chapter of the ACL pages 879–887, Athens, Greece, March. Association for Computational Linguistics. Guodong Zhou and Jian Su. 2003. A Chinese efficient analyser integrating word segmentation, part-ofspeech tagging, partial parsing and full parsing. In Proceedings of the Second SIGHAN Workshop on Chi- Language pages 78–83, Sapporo,</note>
<keyword confidence="0.3371175">Japan, July. Association for Computational Linguistics.</keyword>
<intro confidence="0.49373">1414</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>S Abney</author>
<author>S Flickenger</author>
<author>C Gdaniec</author>
<author>C Grishman</author>
<author>P Harrison</author>
<author>D Hindle</author>
<author>R Ingria</author>
<author>F Jelinek</author>
</authors>
<title>Procedure for quantitatively comparing the syntactic coverage of English grammars.</title>
<date>1991</date>
<booktitle>Proceedings of the workshop on Speech and Natural Language, HLT ’91,</booktitle>
<pages>306--311</pages>
<editor>J. Klavans, M. Liberman, M. Marcus, S. Roukos, B. Santorini, and</editor>
<publisher>Association for Computational Linguistics.</publisher>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="26192" citStr="Abney et al., 1991" startWordPosition="4535" endWordPosition="4538"> of constituent parsing of Penn Chinese Treebank. We report our parser’s word segmentation accuracy based on the flat labels. This accuracy is in fact the joint accuracy of segmentation and part-of-speech tagging. Most importantly, we can report our parser’s accuracy in recovering word structures based on POS labels and flat labels, since word structures may contain only these two kinds of labels. With the standard split of CTB 5.0 data into training, development and test sets (Zhang and Clark, 1411 2009), the result are summarized in Table 1. For all label categories, the PARSEEVAL measures (Abney et al., 1991) are used in computing the labeled precision and recall. correctly analyzed it into a root 㦌斊 ‘masterpiece’ and a suffix 䕛 ‘expert’, as in Figure 10(b). This (b) NN ❩❩ ✚✚ (a) NP ✱ ❧ ✱ ❧ Types LP LR F1 Phrase 79.3 80.1 79.7 Flat 93.2 93.8 93.5 Flat* 97.1 97.6 97.3 POS &amp; Flat 92.7 93.2 92.9 JJ NNf NNf NN 䕛 NNf JJf 㦌斊 㦌 斊䕛 Table 1: Labeled precision and recall for the three types of labels. The line labeled ‘Flat*’ is for unlabeled metrics of flat words, which is effectively the ordinary word segmentation accuracy. Though not directly comparable, we can make some remarks to the accuracy of our mo</context>
</contexts>
<marker>Abney, Flickenger, Gdaniec, Grishman, Harrison, Hindle, Ingria, Jelinek, 1991</marker>
<rawString>S. Abney, S. Flickenger, C. Gdaniec, C. Grishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek, J. Klavans, M. Liberman, M. Marcus, S. Roukos, B. Santorini, and T. Strzalkowski. 1991. Procedure for quantitatively comparing the syntactic coverage of English grammars. In E. Black, editor, Proceedings of the workshop on Speech and Natural Language, HLT ’91, pages 306–311, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="29554" citStr="Berger et al., 1996" startWordPosition="5120" endWordPosition="5123">ed an effective method showing how this could be done. With the progress in statistical parsing technology and the development of large scale treebanks, the time has now come for this paradigm shift to happen. We believe such a new paradigm for word segmentation is linguistically justified and pragmatically beneficial to real world applications. We showed that word structures can be recovered with high precision, though there’s still much room for improvement, especially for higher level constituent parsing. Our model is generative, but discriminative models such as maximum entropy technique (Berger et al., 1996) can be used in parsing word structures too. Many parsers using these techniques have been proved to be quite successful (Luo, 2003; Fung et al., 2004; Wang et al., 2006). Another possible direction is to combine generative models with discriminative reranking to enhance the accuracy (Collins and Koo, 2005; Charniak and Johnson, 2005). Finally, we must note that the use of flat labels such as “NNf” is less than ideal. The most impor1412 tant reason these labels are used is we want to compare the performance of our parser with previous results in constituent parsing, part-of-speech tagging and </context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam L. Berger, Vincent J. Della Pietra, and Stephen A. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
<author>David Chiang</author>
</authors>
<title>Two statistical parsing models applied to the Chinese treebank.</title>
<date>2000</date>
<booktitle>In Second Chinese Language Processing Workshop,</booktitle>
<pages>1--6</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Hong Kong, China,</location>
<contexts>
<context position="11855" citStr="Bikel and Chiang, 2000" startWordPosition="2005" endWordPosition="2008">en many efforts to integrate Chinese word segmentation, part-of-speech tagging and parsing (Wu and Zixin, 1998; Zhou and Su, 2003; Luo, 2003; Fung et al., 2004). However, in these research all words were considered to be flat, and thus word structures were not parsed. This is a crucial difference with our work. Specifically, consider the word 0碜扨 ‘olive oil’. Our parser output tree Figure 6(a), while Luo (2003) output tree (b), giving no hint to the structure of this word since the result is the same with a real flat word 䧢哫膝 ‘Los Angeles’(c). of head-driven generative models (Charniak, 1997; Bikel and Chiang, 2000) . 3 The New Paradigm Given a raw Chinese sentence like 䤕撓 䏓 A敯 䋳 㢧 喓, a traditional word segmentation system would output some result like 䤕撓䏓 A 敯䋳㢧 喓(‘Lin Zhihao’, ‘is’, ‘chief engineer’). In our new paradigm, the output should at least be a linear sequence of trees representing the structures of each word as in Figure 7. VV VVf JJ NGf NFf NR ✑◗◗ ✑ NN ✟✟ ✟ ❍❍❍ NN ❩❩ ✚✚ 䤕 撓䏓 A JJf NNf NNf ,1 䋳㢧 9 Figure 7: Proposed output for the new Chinese word segmentation paradigm. (a) NN ✚✚❩❩ NNf NNf ta 扨 (b) NN NNf ta扨 (c) NR NRf 䧢哫膝 Figure 6: Difference between our output (a) of parsing the word 0碜扨 ‘o</context>
</contexts>
<marker>Bikel, Chiang, 2000</marker>
<rawString>Daniel M. Bikel and David Chiang. 2000. Two statistical parsing models applied to the Chinese treebank. In Second Chinese Language Processing Workshop, pages 1–6, Hong Kong, China, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-tofine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05,</booktitle>
<pages>173--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="29890" citStr="Charniak and Johnson, 2005" startWordPosition="5175" endWordPosition="5178">orld applications. We showed that word structures can be recovered with high precision, though there’s still much room for improvement, especially for higher level constituent parsing. Our model is generative, but discriminative models such as maximum entropy technique (Berger et al., 1996) can be used in parsing word structures too. Many parsers using these techniques have been proved to be quite successful (Luo, 2003; Fung et al., 2004; Wang et al., 2006). Another possible direction is to combine generative models with discriminative reranking to enhance the accuracy (Collins and Koo, 2005; Charniak and Johnson, 2005). Finally, we must note that the use of flat labels such as “NNf” is less than ideal. The most impor1412 tant reason these labels are used is we want to compare the performance of our parser with previous results in constituent parsing, part-of-speech tagging and word segmentation, as we did in Section 6. The problem with this approach is that word structures and phrase structures are then not treated in a truly unified way, and besides the 33 part-of-speech tags originally contained in Penn Chinese Treebank, another 33 tags ending with ‘f’ are introduced. We leave this problem open for now an</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarse-tofine n-best parsing and maxent discriminative reranking. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05, pages 173–180, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Statistical parsing with a context-free grammar and word statistics.</title>
<date>1997</date>
<booktitle>In Proceedings of the fourteenth national conference on artificial intelligence and ninth conference on Innovative applications of artificial intelligence, AAAI’97/IAAI’97,</booktitle>
<pages>598--603</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="11830" citStr="Charniak, 1997" startWordPosition="2003" endWordPosition="2004">ng. There has been many efforts to integrate Chinese word segmentation, part-of-speech tagging and parsing (Wu and Zixin, 1998; Zhou and Su, 2003; Luo, 2003; Fung et al., 2004). However, in these research all words were considered to be flat, and thus word structures were not parsed. This is a crucial difference with our work. Specifically, consider the word 0碜扨 ‘olive oil’. Our parser output tree Figure 6(a), while Luo (2003) output tree (b), giving no hint to the structure of this word since the result is the same with a real flat word 䧢哫膝 ‘Los Angeles’(c). of head-driven generative models (Charniak, 1997; Bikel and Chiang, 2000) . 3 The New Paradigm Given a raw Chinese sentence like 䤕撓 䏓 A敯 䋳 㢧 喓, a traditional word segmentation system would output some result like 䤕撓䏓 A 敯䋳㢧 喓(‘Lin Zhihao’, ‘is’, ‘chief engineer’). In our new paradigm, the output should at least be a linear sequence of trees representing the structures of each word as in Figure 7. VV VVf JJ NGf NFf NR ✑◗◗ ✑ NN ✟✟ ✟ ❍❍❍ NN ❩❩ ✚✚ 䤕 撓䏓 A JJf NNf NNf ,1 䋳㢧 9 Figure 7: Proposed output for the new Chinese word segmentation paradigm. (a) NN ✚✚❩❩ NNf NNf ta 扨 (b) NN NNf ta扨 (c) NR NRf 䧢哫膝 Figure 6: Difference between our output (a) o</context>
</contexts>
<marker>Charniak, 1997</marker>
<rawString>Eugene Charniak. 1997. Statistical parsing with a context-free grammar and word statistics. In Proceedings of the fourteenth national conference on artificial intelligence and ninth conference on Innovative applications of artificial intelligence, AAAI’97/IAAI’97, pages 598–603. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Terry Koo</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<pages>31--25</pages>
<contexts>
<context position="29861" citStr="Collins and Koo, 2005" startWordPosition="5171" endWordPosition="5174">ly beneficial to real world applications. We showed that word structures can be recovered with high precision, though there’s still much room for improvement, especially for higher level constituent parsing. Our model is generative, but discriminative models such as maximum entropy technique (Berger et al., 1996) can be used in parsing word structures too. Many parsers using these techniques have been proved to be quite successful (Luo, 2003; Fung et al., 2004; Wang et al., 2006). Another possible direction is to combine generative models with discriminative reranking to enhance the accuracy (Collins and Koo, 2005; Charniak and Johnson, 2005). Finally, we must note that the use of flat labels such as “NNf” is less than ideal. The most impor1412 tant reason these labels are used is we want to compare the performance of our parser with previous results in constituent parsing, part-of-speech tagging and word segmentation, as we did in Section 6. The problem with this approach is that word structures and phrase structures are then not treated in a truly unified way, and besides the 33 part-of-speech tags originally contained in Penn Chinese Treebank, another 33 tags ending with ‘f’ are introduced. We leave</context>
</contexts>
<marker>Collins, Koo, 2005</marker>
<rawString>Michael Collins and Terry Koo. 2005. Discriminative reranking for natural language parsing. Computational Linguistics, 31:25–70, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="18084" citStr="Collins (1999)" startWordPosition="3125" endWordPosition="3126">whose leaves are Chinese characters and can be concatenated to get the surface character sequence S. 4.1 Generation of Constituent Structures Each node in the constituent tree corresponds to a lexicalized context free rule P → Ln Ln−1 ··· L1HR1 R2 ··· R. (2) where P, LZ, RZ and H are lexicalized nonterminals and H is the head. To generate this constituent, first P is generated, then the head child H is generated conditioned on P, and finally each LZ and Rj are generated conditioned on P and H and a distance metric. This breakdown of lexicalized PCFG rules is essentially the Model 2 defined by Collins (1999). We refer the readers to Collins’ thesis for further details. 1409 4.2 Generation of Words with Internal Structures Words with rich internal structures can be described using a context-free grammar formalism as word → root word → word suffix word → prefix word Here the root is any word without interesting internal structures, and the prefixes and suffixes are not limited to single characters. For example, 1懂 ‘ism’ as in 她㦓1懂 ‘modernism’ is a well known and very productive suffix. Also, we can see that rules (4) and (5) are recursive and hence can handle words with very complex structures. By </context>
<context position="22026" citStr="Collins (1999)" startWordPosition="3836" endWordPosition="3837">erative story for the integrated morphological and syntactic parsing model. For a sentence 5 and its parse tree T, if we denote the set of lexicalized phrase structures in T by C, the set of lexicalized word structures by W, and the set of unknown flat words by F, then the joint probability Pr(T, 5) according to our model is Pr(T, 5) = fl Pr(c) fl Pr(w) fl Pr(f) (10) cEC wEW fEF 1410 In practice, the logarithm of this probability can be calculated instead to avoid numerical difficulties. 5 The Parsing Algorithm To find the parse tree with highest probability we use a chart parser adapted from Collins (1999). Two key changes must be made to the search process, though. Firstly, because we are proposing a new paradigm for Chinese word segmentation, the input to the parser must be raw sentences by definition. Hence to use the bottom-up parser, we need a lexicon of all characters together with what roles they can play in a flat word. We can get this lexicon from the treebank. For example, from the word +愊/NNf ‘center’, we can extract a role bNNf for character + ‘middle’ and a role eNNf for character 愊 ‘center’. The role bNNf means the beginning of the flat label NNf, while eNNf stands for the end of </context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>4</issue>
<contexts>
<context position="5439" citStr="Collins, 2003" startWordPosition="898" endWordPosition="899">m’. With a linear segmentation, the meaning of ‘swimming’ as in 扩堑扙 ‘after swimming’ cannot be properly represented, since 扩扙 ‘swim’ will be segmented into discontinuous units. These language usages lie at the boundary between syntax and morphology, and are not uncommon in Chinese. They can be adequately represented with trees (Figure 2). Figure 2: Example of telescopic compound (a) and separable word (b). The last reason why we should care about word the context will always make it clear what is being referred to with the term word”. structures is related to head driven statistical parsers (Collins, 2003). To illustrate this, note that in the Penn Chinese Treebank, the word 戽䊂䠽A ‘English People’ does not occur at all. Hence constituents headed by such words could cause some difficulty for head driven models in which out-ofvocabulary words need to be treated specially both when they are generated and when they are conditioned upon. But this word is in turn headed by its suffix A ‘people’, and there are 2,233 such words in Penn Chinese Treebank. If we annotate the structure of every compound containing this suffix (e.g. Figure 3), such data sparsity simply goes away. NN ❜❜ ✧ ✧ NRf NNf 戽䊂䠽 吼 Figu</context>
<context position="13578" citStr="Collins (2003)" startWordPosition="2319" endWordPosition="2320">engkrai et al., 2009). Goldberg and Tsarfaty (2008) showed that a single model for morphological segmentation and syntactic parsing of Hebrew yielded an error reduction of 12% over the best pipelined models. This is because an integrated approach can effectively take into account more information from different levels of analysis. Parsing of Chinese word structures can be reduced to the usual constituent parsing, for which there has been great progress in the past several years. Our generative model for unified word and phrase structure parsing is a direct adaptation of the model presented by Collins (2003). Many other approaches of constituent parsing also use this kind Note that in the proposed output, all words are annotated with their part-of-speech tags. This is necessary since part-of-speech plays an important role in the generation of compound words. For example, 揶 ‘person’ usually combines with a verb to form a compound noun such as 唗䕏揶 ‘designer’. In this paper, we will actually design an integrated morphological and syntactical parser trained with a treebank. Therefore, the real output of our system looks like Figure 8. It’s clear that besides all S ✏✏ ✏ ✏ PPPP VP ✦ ❛❛❛ ✦ ✦ VV NN ✟ ✟ ❍</context>
<context position="14813" citStr="Collins, 2003" startWordPosition="2547" endWordPosition="2548">f NNf NNf ,1 䋳㢧 9 Figure 8: The actual output of our parser trained with a fully annotated treebank. the information of the proposed output for the new NP NR ✚✚❩❩ NFf 䤕 撓䏓 NGf 1408 paradigm, our model’s output also includes higherlevel syntactic parsing results. 3.1 Training Data (a) NN NNf 憞䠮䞎 (b) NN � � � � JJf NNf 卣 敯埚 We employ a statistical model to parse phrase and word structures as illustrated in Figure 8. The currently available treebank for us is the Penn Chinese Treebank (CTB) 5.0 (Xue et al., 2005). Because our model belongs to the family of head-driven statistical parsing models (Collins, 2003), we use the headfinding rules described by Sun and Jurafsky (2004). Unfortunately, this treebank or any other treebanks for that matter, does not contain annotations of word structures. Therefore, we must annotate these structures by ourselves. The good news is that the annotation is not too complicated. First, we extract all words in the treebank and check each of them manually. Words with non-trivial structures are thus annotated. Finally, we install these small trees of words into the original treebank. Whether a word has structures or not is mostly context independent, so we only have to </context>
</contexts>
<marker>Collins, 2003</marker>
<rawString>Michael Collins. 2003. Head-driven statistical models for natural language parsing. Computational Linguistics, 29(4):589–637.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
<author>Grace Ngai</author>
<author>Yongsheng Yang</author>
<author>Benfeng Chen</author>
</authors>
<title>A maximum-entropy Chinese parser augmented by transformation-based learning.</title>
<date>2004</date>
<journal>ACM Transactions on Asian Language Information Processing,</journal>
<volume>3</volume>
<contexts>
<context position="11392" citStr="Fung et al., 2004" startWordPosition="1927" endWordPosition="1930">tion standards in the hope of saving some expensive human labor. We believe the best solution to the problem of divergent standards and requirements is to annotate and analyze word structures. Then applications can make use of these structures according to their own convenience. 1407 Since the distinction between morphology and syntax in Chinese is somewhat blurred, our model for word structure parsing is integrated with constituent parsing. There has been many efforts to integrate Chinese word segmentation, part-of-speech tagging and parsing (Wu and Zixin, 1998; Zhou and Su, 2003; Luo, 2003; Fung et al., 2004). However, in these research all words were considered to be flat, and thus word structures were not parsed. This is a crucial difference with our work. Specifically, consider the word 0碜扨 ‘olive oil’. Our parser output tree Figure 6(a), while Luo (2003) output tree (b), giving no hint to the structure of this word since the result is the same with a real flat word 䧢哫膝 ‘Los Angeles’(c). of head-driven generative models (Charniak, 1997; Bikel and Chiang, 2000) . 3 The New Paradigm Given a raw Chinese sentence like 䤕撓 䏓 A敯 䋳 㢧 喓, a traditional word segmentation system would output some result li</context>
<context position="29704" citStr="Fung et al., 2004" startWordPosition="5147" endWordPosition="5150">the time has now come for this paradigm shift to happen. We believe such a new paradigm for word segmentation is linguistically justified and pragmatically beneficial to real world applications. We showed that word structures can be recovered with high precision, though there’s still much room for improvement, especially for higher level constituent parsing. Our model is generative, but discriminative models such as maximum entropy technique (Berger et al., 1996) can be used in parsing word structures too. Many parsers using these techniques have been proved to be quite successful (Luo, 2003; Fung et al., 2004; Wang et al., 2006). Another possible direction is to combine generative models with discriminative reranking to enhance the accuracy (Collins and Koo, 2005; Charniak and Johnson, 2005). Finally, we must note that the use of flat labels such as “NNf” is less than ideal. The most impor1412 tant reason these labels are used is we want to compare the performance of our parser with previous results in constituent parsing, part-of-speech tagging and word segmentation, as we did in Section 6. The problem with this approach is that word structures and phrase structures are then not treated in a trul</context>
</contexts>
<marker>Fung, Ngai, Yang, Chen, 2004</marker>
<rawString>Pascale Fung, Grace Ngai, Yongsheng Yang, and Benfeng Chen. 2004. A maximum-entropy Chinese parser augmented by transformation-based learning. ACM Transactions on Asian Language Information Processing, 3:159–168, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Andi Wu</author>
<author>Cheng-Ning Huang</author>
<author>Hong qiao Li</author>
<author>Xinsong Xia</author>
<author>Hauwei Qin</author>
</authors>
<title>Adaptive Chinese word segmentation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume,</booktitle>
<pages>462--469</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="4400" citStr="Gao et al., 2004" startWordPosition="730" endWordPosition="733">’s considered to be one word in the Penn Chinese Treebank, but is segmented into a surname and a given name in the Peking University corpus. For some applications such as information extraction, the former segmentation is adequate, while for others like machine translation, the later finer-grained output is more preferable. If the analyzer can produce a structure as shown in Figure 4(a), then every application can extract what it needs from this tree. A solution with tree output like this is more elegant than approaches which try to meet the needs of different applications in post-processing (Gao et al., 2004). The third reason is that traditional word segmentation has problems in handling many phenomena in Chinese. For example, the telescopic compound 㦌撥怂惆 ‘universities, middle schools and primary schools’ is in fact composed of three coordinating elements 㦌惆 ‘university’, 撥惆 ‘middle school’ and 怂惆 ‘primary school’. Regarding it as one flat word loses this important information. Another example is separable words like 扩扙 ‘swim’. With a linear segmentation, the meaning of ‘swimming’ as in 扩堑扙 ‘after swimming’ cannot be properly represented, since 扩扙 ‘swim’ will be segmented into discontinuous units</context>
<context position="10652" citStr="Gao et al., 2004" startWordPosition="1809" endWordPosition="1812">e Zhao focused on unlabeled dependencies between characters in a word, and part-ofspeech information was not utilized. Secondly, we distinguish explicitly the generation of flat words such as 䑵喏䃮 ‘Washington’ and words with internal structures. Our parsing algorithm also has to be adapted accordingly. Such distinction was not made in Zhao’s parsing model and algorithm. Many researchers have also noticed the awkwardness and insufficiency of current boundary-only Chinese word segmentation paradigm, so they tried to customize the output to meet the requirements of various applications (Wu, 2003; Gao et al., 2004). In a related research, Jiang et al. (2009) presented a strategy to transfer annotated corpora between different segmentation standards in the hope of saving some expensive human labor. We believe the best solution to the problem of divergent standards and requirements is to annotate and analyze word structures. Then applications can make use of these structures according to their own convenience. 1407 Since the distinction between morphology and syntax in Chinese is somewhat blurred, our model for word structure parsing is integrated with constituent parsing. There has been many efforts to i</context>
</contexts>
<marker>Gao, Wu, Huang, Li, Xia, Qin, 2004</marker>
<rawString>Jianfeng Gao, Andi Wu, Cheng-Ning Huang, Hong qiao Li, Xinsong Xia, and Hauwei Qin. 2004. Adaptive Chinese word segmentation. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume, pages 462–469, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Mu Li</author>
<author>Andi Wu</author>
<author>Chang-Ning Huang</author>
</authors>
<title>Chinese word segmentation and named entity recognition: A pragmatic approach.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>4</issue>
<contexts>
<context position="1315" citStr="Gao et al., 2005" startWordPosition="203" endWordPosition="206"> practically and theoretically. Instead, we propose that word structures should be recovered in morphological analysis. An elegant approach for doing this is given and the result is shown to be promising enough for encouraging further effort in this direction. Our probability model is trained with the Penn Chinese Treebank and actually is able to parse both word and phrase structures in a unified way. 1 Why Parse Word Structures? Research in Chinese word segmentation has progressed tremendously in recent years, with state of the art performing at around 97% in precision and recall (Xue, 2003; Gao et al., 2005; Zhang and Clark, 2007; Li and Sun, 2009). However, virtually all these systems focus exclusively on recognizing the word boundaries, giving no consideration to the internal structures of many words. Though it has been the standard practice for many years, we argue that this paradigm is inadequate both in theory and in practice, for at least the following four reasons. The first reason is that if we confine our definition of word segmentation to the identification of word boundaries, then people tend to have divergent opinions as to whether a linguistic unit is a word or not (Sproat et al., 1</context>
</contexts>
<marker>Gao, Li, Wu, Huang, 2005</marker>
<rawString>Jianfeng Gao, Mu Li, Andi Wu, and Chang-Ning Huang. 2005. Chinese word segmentation and named entity recognition: A pragmatic approach. Computational Linguistics, 31(4):531–574.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Reut Tsarfaty</author>
</authors>
<title>A single generative model for joint morphological segmentation and syntactic parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>371--379</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="13015" citStr="Goldberg and Tsarfaty (2008)" startWordPosition="2227" endWordPosition="2230">gure 6: Difference between our output (a) of parsing the word 0碜扨 ‘olive oil’ and the output (b) of Luo (2003). In (c) we have a true flat word, namely the location name 䧢哫膝 ‘Los Angeles’. The benefits of joint modeling has been noticed by many. For example, Li et al. (2010) reported that a joint syntactic and semantic model improved the accuracy of both tasks, while Ng and Low (2004) showed it’s beneficial to integrate word segmentation and part-of-speech tagging into one model. The later result is confirmed by many others (Zhang and Clark, 2008; Jiang et al., 2008; Kruengkrai et al., 2009). Goldberg and Tsarfaty (2008) showed that a single model for morphological segmentation and syntactic parsing of Hebrew yielded an error reduction of 12% over the best pipelined models. This is because an integrated approach can effectively take into account more information from different levels of analysis. Parsing of Chinese word structures can be reduced to the usual constituent parsing, for which there has been great progress in the past several years. Our generative model for unified word and phrase structure parsing is a direct adaptation of the model presented by Collins (2003). Many other approaches of constituen</context>
</contexts>
<marker>Goldberg, Tsarfaty, 2008</marker>
<rawString>Yoav Goldberg and Reut Tsarfaty. 2008. A single generative model for joint morphological segmentation and syntactic parsing. In Proceedings of ACL-08: HLT, pages 371–379, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbin Jiang</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
<author>Yajuan L¨u</author>
</authors>
<title>A cascaded linear model for joint Chinese word segmentation and part-of-speech tagging.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>897--904</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<marker>Jiang, Huang, Liu, L¨u, 2008</marker>
<rawString>Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan L¨u. 2008. A cascaded linear model for joint Chinese word segmentation and part-of-speech tagging. In Proceedings of ACL-08: HLT, pages 897–904, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbin Jiang</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
</authors>
<title>Automatic adaptation of annotation standards: Chinese word segmentation and POS tagging – a case study.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>522--530</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="10696" citStr="Jiang et al. (2009)" startWordPosition="1817" endWordPosition="1820">etween characters in a word, and part-ofspeech information was not utilized. Secondly, we distinguish explicitly the generation of flat words such as 䑵喏䃮 ‘Washington’ and words with internal structures. Our parsing algorithm also has to be adapted accordingly. Such distinction was not made in Zhao’s parsing model and algorithm. Many researchers have also noticed the awkwardness and insufficiency of current boundary-only Chinese word segmentation paradigm, so they tried to customize the output to meet the requirements of various applications (Wu, 2003; Gao et al., 2004). In a related research, Jiang et al. (2009) presented a strategy to transfer annotated corpora between different segmentation standards in the hope of saving some expensive human labor. We believe the best solution to the problem of divergent standards and requirements is to annotate and analyze word structures. Then applications can make use of these structures according to their own convenience. 1407 Since the distinction between morphology and syntax in Chinese is somewhat blurred, our model for word structure parsing is integrated with constituent parsing. There has been many efforts to integrate Chinese word segmentation, part-of-</context>
</contexts>
<marker>Jiang, Huang, Liu, 2009</marker>
<rawString>Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Automatic adaptation of annotation standards: Chinese word segmentation and POS tagging – a case study. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 522–530, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Canasai Kruengkrai</author>
<author>Kiyotaka Uchimoto</author>
<author>Jun’ichi Kazama</author>
<author>Yiou Wang</author>
<author>Kentaro Torisawa</author>
<author>Hitoshi Isahara</author>
</authors>
<title>An error-driven word-character hybrid model for joint Chinese word segmentation and POS tagging.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>513--521</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="12985" citStr="Kruengkrai et al., 2009" startWordPosition="2223" endWordPosition="2226"> NNf ta扨 (c) NR NRf 䧢哫膝 Figure 6: Difference between our output (a) of parsing the word 0碜扨 ‘olive oil’ and the output (b) of Luo (2003). In (c) we have a true flat word, namely the location name 䧢哫膝 ‘Los Angeles’. The benefits of joint modeling has been noticed by many. For example, Li et al. (2010) reported that a joint syntactic and semantic model improved the accuracy of both tasks, while Ng and Low (2004) showed it’s beneficial to integrate word segmentation and part-of-speech tagging into one model. The later result is confirmed by many others (Zhang and Clark, 2008; Jiang et al., 2008; Kruengkrai et al., 2009). Goldberg and Tsarfaty (2008) showed that a single model for morphological segmentation and syntactic parsing of Hebrew yielded an error reduction of 12% over the best pipelined models. This is because an integrated approach can effectively take into account more information from different levels of analysis. Parsing of Chinese word structures can be reduced to the usual constituent parsing, for which there has been great progress in the past several years. Our generative model for unified word and phrase structure parsing is a direct adaptation of the model presented by Collins (2003). Many </context>
<context position="27230" citStr="Kruengkrai et al., 2009" startWordPosition="4726" endWordPosition="4729">’ is for unlabeled metrics of flat words, which is effectively the ordinary word segmentation accuracy. Though not directly comparable, we can make some remarks to the accuracy of our model. For constituent parsing, the best result on CTB 5.0 is reported to be 78% F1 measure for unlimited sentences with automatically assigned POS tags (Zhang and Clark, 2009). Our result for phrase labels is close to this accuracy. Besides, the result for flat labels compares favorably with the state of the art accuracy of about 93% F1 for joint word segmentation and part-of-speech tagging (Jiang et al., 2008; Kruengkrai et al., 2009). For ordinary word segmentation, the best result is reported to be around 97% F1 on CTB 5.0 (Kruengkrai et al., 2009), while our parser performs at 97.3%, though we should remember that the result concerns flat words only. Finally, we see the performance of word structure recovery is almost as good as the recognition of flat words. This means that parsing word structures accurately is possible with a generative model. It is interesting to see how well the parser does in recognizing the structure of words that were not seen during training. For this, we sampled 100 such words including those w</context>
</contexts>
<marker>Kruengkrai, Uchimoto, Kazama, Wang, Torisawa, Isahara, 2009</marker>
<rawString>Canasai Kruengkrai, Kiyotaka Uchimoto, Jun’ichi Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi Isahara. 2009. An error-driven word-character hybrid model for joint Chinese word segmentation and POS tagging. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 513–521, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongguo Li</author>
<author>Maosong Sun</author>
</authors>
<title>Punctuation as implicit annotations for Chinese word segmentation.</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<pages>35--505</pages>
<contexts>
<context position="1357" citStr="Li and Sun, 2009" startWordPosition="211" endWordPosition="214">we propose that word structures should be recovered in morphological analysis. An elegant approach for doing this is given and the result is shown to be promising enough for encouraging further effort in this direction. Our probability model is trained with the Penn Chinese Treebank and actually is able to parse both word and phrase structures in a unified way. 1 Why Parse Word Structures? Research in Chinese word segmentation has progressed tremendously in recent years, with state of the art performing at around 97% in precision and recall (Xue, 2003; Gao et al., 2005; Zhang and Clark, 2007; Li and Sun, 2009). However, virtually all these systems focus exclusively on recognizing the word boundaries, giving no consideration to the internal structures of many words. Though it has been the standard practice for many years, we argue that this paradigm is inadequate both in theory and in practice, for at least the following four reasons. The first reason is that if we confine our definition of word segmentation to the identification of word boundaries, then people tend to have divergent opinions as to whether a linguistic unit is a word or not (Sproat et al., 1996). This has led to many different annot</context>
</contexts>
<marker>Li, Sun, 2009</marker>
<rawString>Zhongguo Li and Maosong Sun. 2009. Punctuation as implicit annotations for Chinese word segmentation. Computational Linguistics, 35:505–512, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Junhui Li</author>
<author>Guodong Zhou</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Joint syntactic and semantic parsing of Chinese.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1108--1117</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="12662" citStr="Li et al. (2010)" startWordPosition="2170" endWordPosition="2173">In our new paradigm, the output should at least be a linear sequence of trees representing the structures of each word as in Figure 7. VV VVf JJ NGf NFf NR ✑◗◗ ✑ NN ✟✟ ✟ ❍❍❍ NN ❩❩ ✚✚ 䤕 撓䏓 A JJf NNf NNf ,1 䋳㢧 9 Figure 7: Proposed output for the new Chinese word segmentation paradigm. (a) NN ✚✚❩❩ NNf NNf ta 扨 (b) NN NNf ta扨 (c) NR NRf 䧢哫膝 Figure 6: Difference between our output (a) of parsing the word 0碜扨 ‘olive oil’ and the output (b) of Luo (2003). In (c) we have a true flat word, namely the location name 䧢哫膝 ‘Los Angeles’. The benefits of joint modeling has been noticed by many. For example, Li et al. (2010) reported that a joint syntactic and semantic model improved the accuracy of both tasks, while Ng and Low (2004) showed it’s beneficial to integrate word segmentation and part-of-speech tagging into one model. The later result is confirmed by many others (Zhang and Clark, 2008; Jiang et al., 2008; Kruengkrai et al., 2009). Goldberg and Tsarfaty (2008) showed that a single model for morphological segmentation and syntactic parsing of Hebrew yielded an error reduction of 12% over the best pipelined models. This is because an integrated approach can effectively take into account more information </context>
</contexts>
<marker>Li, Zhou, Ng, 2010</marker>
<rawString>Junhui Li, Guodong Zhou, and Hwee Tou Ng. 2010. Joint syntactic and semantic parsing of Chinese. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1108– 1117, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Luo</author>
</authors>
<title>A maximum entropy Chinese character-based parser.</title>
<date>2003</date>
<booktitle>Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>192--199</pages>
<editor>In Michael Collins and Mark Steedman, editors,</editor>
<contexts>
<context position="11372" citStr="Luo, 2003" startWordPosition="1925" endWordPosition="1926">nt segmentation standards in the hope of saving some expensive human labor. We believe the best solution to the problem of divergent standards and requirements is to annotate and analyze word structures. Then applications can make use of these structures according to their own convenience. 1407 Since the distinction between morphology and syntax in Chinese is somewhat blurred, our model for word structure parsing is integrated with constituent parsing. There has been many efforts to integrate Chinese word segmentation, part-of-speech tagging and parsing (Wu and Zixin, 1998; Zhou and Su, 2003; Luo, 2003; Fung et al., 2004). However, in these research all words were considered to be flat, and thus word structures were not parsed. This is a crucial difference with our work. Specifically, consider the word 0碜扨 ‘olive oil’. Our parser output tree Figure 6(a), while Luo (2003) output tree (b), giving no hint to the structure of this word since the result is the same with a real flat word 䧢哫膝 ‘Los Angeles’(c). of head-driven generative models (Charniak, 1997; Bikel and Chiang, 2000) . 3 The New Paradigm Given a raw Chinese sentence like 䤕撓 䏓 A敯 䋳 㢧 喓, a traditional word segmentation system would o</context>
<context position="22685" citStr="Luo (2003)" startWordPosition="3958" endWordPosition="3959">ess, though. Firstly, because we are proposing a new paradigm for Chinese word segmentation, the input to the parser must be raw sentences by definition. Hence to use the bottom-up parser, we need a lexicon of all characters together with what roles they can play in a flat word. We can get this lexicon from the treebank. For example, from the word +愊/NNf ‘center’, we can extract a role bNNf for character + ‘middle’ and a role eNNf for character 愊 ‘center’. The role bNNf means the beginning of the flat label NNf, while eNNf stands for the end of the label NNf. This scheme was first proposed by Luo (2003) in his character-based Chinese parser, and we find it quite adequate for our purpose here. Secondly, in the bottom-up parser for head driven models, whenever a new edge is found, we must assign it a probability and a head word. If the newly discovered constituent is a flat word (its label ends with ‘f’), then we set its head word to be the concatenation of all its child characters, i.e. the word itself. If it is an unknown word, we use (7) to assign the probability, otherwise its probability is set to be 1. On the other hand, if the new edge is a phrase or word with internal structures, the p</context>
<context position="29685" citStr="Luo, 2003" startWordPosition="5145" endWordPosition="5146">treebanks, the time has now come for this paradigm shift to happen. We believe such a new paradigm for word segmentation is linguistically justified and pragmatically beneficial to real world applications. We showed that word structures can be recovered with high precision, though there’s still much room for improvement, especially for higher level constituent parsing. Our model is generative, but discriminative models such as maximum entropy technique (Berger et al., 1996) can be used in parsing word structures too. Many parsers using these techniques have been proved to be quite successful (Luo, 2003; Fung et al., 2004; Wang et al., 2006). Another possible direction is to combine generative models with discriminative reranking to enhance the accuracy (Collins and Koo, 2005; Charniak and Johnson, 2005). Finally, we must note that the use of flat labels such as “NNf” is less than ideal. The most impor1412 tant reason these labels are used is we want to compare the performance of our parser with previous results in constituent parsing, part-of-speech tagging and word segmentation, as we did in Section 6. The problem with this approach is that word structures and phrase structures are then no</context>
</contexts>
<marker>Luo, 2003</marker>
<rawString>Xiaoqiang Luo. 2003. A maximum entropy Chinese character-based parser. In Michael Collins and Mark Steedman, editors, Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing, pages 192–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Jin Kiat Low</author>
</authors>
<title>Chinese part-ofspeech tagging: One-at-a-time or all-at-once? wordbased or character-based?</title>
<date>2004</date>
<booktitle>In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004,</booktitle>
<pages>277--284</pages>
<publisher>Association for Computational Linguistics.</publisher>
<location>Barcelona, Spain,</location>
<contexts>
<context position="12774" citStr="Ng and Low (2004)" startWordPosition="2189" endWordPosition="2192">ch word as in Figure 7. VV VVf JJ NGf NFf NR ✑◗◗ ✑ NN ✟✟ ✟ ❍❍❍ NN ❩❩ ✚✚ 䤕 撓䏓 A JJf NNf NNf ,1 䋳㢧 9 Figure 7: Proposed output for the new Chinese word segmentation paradigm. (a) NN ✚✚❩❩ NNf NNf ta 扨 (b) NN NNf ta扨 (c) NR NRf 䧢哫膝 Figure 6: Difference between our output (a) of parsing the word 0碜扨 ‘olive oil’ and the output (b) of Luo (2003). In (c) we have a true flat word, namely the location name 䧢哫膝 ‘Los Angeles’. The benefits of joint modeling has been noticed by many. For example, Li et al. (2010) reported that a joint syntactic and semantic model improved the accuracy of both tasks, while Ng and Low (2004) showed it’s beneficial to integrate word segmentation and part-of-speech tagging into one model. The later result is confirmed by many others (Zhang and Clark, 2008; Jiang et al., 2008; Kruengkrai et al., 2009). Goldberg and Tsarfaty (2008) showed that a single model for morphological segmentation and syntactic parsing of Hebrew yielded an error reduction of 12% over the best pipelined models. This is because an integrated approach can effectively take into account more information from different levels of analysis. Parsing of Chinese word structures can be reduced to the usual constituent pa</context>
</contexts>
<marker>Ng, Low, 2004</marker>
<rawString>Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-ofspeech tagging: One-at-a-time or all-at-once? wordbased or character-based? In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004, pages 277– 284, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sproat</author>
<author>Thomas Emerson</author>
</authors>
<title>The first international Chinese word segmentation bakeoff.</title>
<date>2003</date>
<booktitle>In Proceedings of the Second SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>133--143</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sapporo, Japan,</location>
<contexts>
<context position="2289" citStr="Sproat and Emerson, 2003" startWordPosition="370" endWordPosition="373">following four reasons. The first reason is that if we confine our definition of word segmentation to the identification of word boundaries, then people tend to have divergent opinions as to whether a linguistic unit is a word or not (Sproat et al., 1996). This has led to many different annotation standards for Chinese word segmentation. Even worse, this could cause inconsistency in the same corpus. For instance, 䉂t奒 ‘vice president’ is considered to be one word in the Penn Chinese Treebank (Xue et al., 2005), but is split into two words by the Peking University corpus in the SIGHAN Bakeoffs (Sproat and Emerson, 2003). Meanwhile, 䉂䀓惼 ‘vice director’ and 䉂 䚲䡮 ‘deputy manager’ are both segmented into two words in the same Penn Chinese Treebank. In fact, all these words are composed of the prefix 䉂 ‘vice’ and a root word. Thus the structure of 䉂t奒 ‘vice president’ can be represented with the tree in Figure 1. Without a doubt, there is complete agreeNN � � � � JJf NNf 䉂 擌奒 Figure 1: Example of a word with internal structure. ment on the correctness of this structure among native Chinese speakers. So if instead of annotating only word boundaries, we annotate the structures of every word, 1 then the annotation t</context>
</contexts>
<marker>Sproat, Emerson, 2003</marker>
<rawString>Richard Sproat and Thomas Emerson. 2003. The first international Chinese word segmentation bakeoff. In Proceedings of the Second SIGHAN Workshop on Chinese Language Processing, pages 133–143, Sapporo, Japan, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sproat</author>
<author>William Gale</author>
<author>Chilin Shih</author>
<author>Nancy Chang</author>
</authors>
<title>A stochastic finite-state wordsegmentation algorithm for Chinese.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>3</issue>
<contexts>
<context position="1919" citStr="Sproat et al., 1996" startWordPosition="305" endWordPosition="308">Gao et al., 2005; Zhang and Clark, 2007; Li and Sun, 2009). However, virtually all these systems focus exclusively on recognizing the word boundaries, giving no consideration to the internal structures of many words. Though it has been the standard practice for many years, we argue that this paradigm is inadequate both in theory and in practice, for at least the following four reasons. The first reason is that if we confine our definition of word segmentation to the identification of word boundaries, then people tend to have divergent opinions as to whether a linguistic unit is a word or not (Sproat et al., 1996). This has led to many different annotation standards for Chinese word segmentation. Even worse, this could cause inconsistency in the same corpus. For instance, 䉂t奒 ‘vice president’ is considered to be one word in the Penn Chinese Treebank (Xue et al., 2005), but is split into two words by the Peking University corpus in the SIGHAN Bakeoffs (Sproat and Emerson, 2003). Meanwhile, 䉂䀓惼 ‘vice director’ and 䉂 䚲䡮 ‘deputy manager’ are both segmented into two words in the same Penn Chinese Treebank. In fact, all these words are composed of the prefix 䉂 ‘vice’ and a root word. Thus the structure of 䉂t</context>
</contexts>
<marker>Sproat, Gale, Shih, Chang, 1996</marker>
<rawString>Richard Sproat, William Gale, Chilin Shih, and Nancy Chang. 1996. A stochastic finite-state wordsegmentation algorithm for Chinese. Computational Linguistics, 22(3):377–404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Honglin Sun</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Shallow semantc parsing of Chinese.</title>
<date>2004</date>
<booktitle>HLT-NAACL 2004: Main Proceedings,</booktitle>
<volume>2</volume>
<pages>249--256</pages>
<editor>In Daniel Marcu Susan Dumais and Salim Roukos, editors,</editor>
<location>Boston, Massachusetts, USA,</location>
<contexts>
<context position="14880" citStr="Sun and Jurafsky (2004)" startWordPosition="2557" endWordPosition="2560">r trained with a fully annotated treebank. the information of the proposed output for the new NP NR ✚✚❩❩ NFf 䤕 撓䏓 NGf 1408 paradigm, our model’s output also includes higherlevel syntactic parsing results. 3.1 Training Data (a) NN NNf 憞䠮䞎 (b) NN � � � � JJf NNf 卣 敯埚 We employ a statistical model to parse phrase and word structures as illustrated in Figure 8. The currently available treebank for us is the Penn Chinese Treebank (CTB) 5.0 (Xue et al., 2005). Because our model belongs to the family of head-driven statistical parsing models (Collins, 2003), we use the headfinding rules described by Sun and Jurafsky (2004). Unfortunately, this treebank or any other treebanks for that matter, does not contain annotations of word structures. Therefore, we must annotate these structures by ourselves. The good news is that the annotation is not too complicated. First, we extract all words in the treebank and check each of them manually. Words with non-trivial structures are thus annotated. Finally, we install these small trees of words into the original treebank. Whether a word has structures or not is mostly context independent, so we only have to annotate each word once. There are two noteworthy issues in this pr</context>
</contexts>
<marker>Sun, Jurafsky, 2004</marker>
<rawString>Honglin Sun and Daniel Jurafsky. 2004. Shallow semantc parsing of Chinese. In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLT-NAACL 2004: Main Proceedings, pages 249–256, Boston, Massachusetts, USA, May 2 - May 7. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mengqiu Wang</author>
<author>Kenji Sagae</author>
<author>Teruko Mitamura</author>
</authors>
<title>A fast, accurate deterministic parser for chinese.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>425--432</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="29724" citStr="Wang et al., 2006" startWordPosition="5151" endWordPosition="5154">me for this paradigm shift to happen. We believe such a new paradigm for word segmentation is linguistically justified and pragmatically beneficial to real world applications. We showed that word structures can be recovered with high precision, though there’s still much room for improvement, especially for higher level constituent parsing. Our model is generative, but discriminative models such as maximum entropy technique (Berger et al., 1996) can be used in parsing word structures too. Many parsers using these techniques have been proved to be quite successful (Luo, 2003; Fung et al., 2004; Wang et al., 2006). Another possible direction is to combine generative models with discriminative reranking to enhance the accuracy (Collins and Koo, 2005; Charniak and Johnson, 2005). Finally, we must note that the use of flat labels such as “NNf” is less than ideal. The most impor1412 tant reason these labels are used is we want to compare the performance of our parser with previous results in constituent parsing, part-of-speech tagging and word segmentation, as we did in Section 6. The problem with this approach is that word structures and phrase structures are then not treated in a truly unified way, and b</context>
</contexts>
<marker>Wang, Sagae, Mitamura, 2006</marker>
<rawString>Mengqiu Wang, Kenji Sagae, and Teruko Mitamura. 2006. A fast, accurate deterministic parser for chinese. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 425–432, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andi Wu</author>
<author>Jiang Zixin</author>
</authors>
<title>Word segmentation in sentence analysis.</title>
<date>1998</date>
<booktitle>In Proceedings of the 1998International Conference on Chinese information processing,</booktitle>
<location>Beijing, China.</location>
<contexts>
<context position="11342" citStr="Wu and Zixin, 1998" startWordPosition="1917" endWordPosition="1920">nsfer annotated corpora between different segmentation standards in the hope of saving some expensive human labor. We believe the best solution to the problem of divergent standards and requirements is to annotate and analyze word structures. Then applications can make use of these structures according to their own convenience. 1407 Since the distinction between morphology and syntax in Chinese is somewhat blurred, our model for word structure parsing is integrated with constituent parsing. There has been many efforts to integrate Chinese word segmentation, part-of-speech tagging and parsing (Wu and Zixin, 1998; Zhou and Su, 2003; Luo, 2003; Fung et al., 2004). However, in these research all words were considered to be flat, and thus word structures were not parsed. This is a crucial difference with our work. Specifically, consider the word 0碜扨 ‘olive oil’. Our parser output tree Figure 6(a), while Luo (2003) output tree (b), giving no hint to the structure of this word since the result is the same with a real flat word 䧢哫膝 ‘Los Angeles’(c). of head-driven generative models (Charniak, 1997; Bikel and Chiang, 2000) . 3 The New Paradigm Given a raw Chinese sentence like 䤕撓 䏓 A敯 䋳 㢧 喓, a traditional wo</context>
</contexts>
<marker>Wu, Zixin, 1998</marker>
<rawString>Andi Wu and Jiang Zixin. 1998. Word segmentation in sentence analysis. In Proceedings of the 1998International Conference on Chinese information processing, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andi Wu</author>
</authors>
<title>Customizable segmentation of morphologically derived words in Chinese. Computational Linguistics and Chinese language processing,</title>
<date>2003</date>
<volume>8</volume>
<issue>1</issue>
<pages>28</pages>
<contexts>
<context position="10633" citStr="Wu, 2003" startWordPosition="1807" endWordPosition="1808">odel, while Zhao focused on unlabeled dependencies between characters in a word, and part-ofspeech information was not utilized. Secondly, we distinguish explicitly the generation of flat words such as 䑵喏䃮 ‘Washington’ and words with internal structures. Our parsing algorithm also has to be adapted accordingly. Such distinction was not made in Zhao’s parsing model and algorithm. Many researchers have also noticed the awkwardness and insufficiency of current boundary-only Chinese word segmentation paradigm, so they tried to customize the output to meet the requirements of various applications (Wu, 2003; Gao et al., 2004). In a related research, Jiang et al. (2009) presented a strategy to transfer annotated corpora between different segmentation standards in the hope of saving some expensive human labor. We believe the best solution to the problem of divergent standards and requirements is to annotate and analyze word structures. Then applications can make use of these structures according to their own convenience. 1407 Since the distinction between morphology and syntax in Chinese is somewhat blurred, our model for word structure parsing is integrated with constituent parsing. There has bee</context>
</contexts>
<marker>Wu, 2003</marker>
<rawString>Andi Wu. 2003. Customizable segmentation of morphologically derived words in Chinese. Computational Linguistics and Chinese language processing, 8(1):1– 28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Fei Xia</author>
<author>Fu-Dong Chiou</author>
<author>Martha Palmer</author>
</authors>
<title>The Penn Chinese Treebank: phrase structure annotation of a large corpus.</title>
<date>2005</date>
<journal>Natural Language Engineering,</journal>
<volume>11</volume>
<issue>2</issue>
<contexts>
<context position="2178" citStr="Xue et al., 2005" startWordPosition="350" endWordPosition="353">many years, we argue that this paradigm is inadequate both in theory and in practice, for at least the following four reasons. The first reason is that if we confine our definition of word segmentation to the identification of word boundaries, then people tend to have divergent opinions as to whether a linguistic unit is a word or not (Sproat et al., 1996). This has led to many different annotation standards for Chinese word segmentation. Even worse, this could cause inconsistency in the same corpus. For instance, 䉂t奒 ‘vice president’ is considered to be one word in the Penn Chinese Treebank (Xue et al., 2005), but is split into two words by the Peking University corpus in the SIGHAN Bakeoffs (Sproat and Emerson, 2003). Meanwhile, 䉂䀓惼 ‘vice director’ and 䉂 䚲䡮 ‘deputy manager’ are both segmented into two words in the same Penn Chinese Treebank. In fact, all these words are composed of the prefix 䉂 ‘vice’ and a root word. Thus the structure of 䉂t奒 ‘vice president’ can be represented with the tree in Figure 1. Without a doubt, there is complete agreeNN � � � � JJf NNf 䉂 擌奒 Figure 1: Example of a word with internal structure. ment on the correctness of this structure among native Chinese speakers. So i</context>
<context position="14714" citStr="Xue et al., 2005" startWordPosition="2530" endWordPosition="2533">like Figure 8. It’s clear that besides all S ✏✏ ✏ ✏ PPPP VP ✦ ❛❛❛ ✦ ✦ VV NN ✟ ✟ ❍❍ VVf JJ NN ✚✚❩❩ A JJf NNf NNf ,1 䋳㢧 9 Figure 8: The actual output of our parser trained with a fully annotated treebank. the information of the proposed output for the new NP NR ✚✚❩❩ NFf 䤕 撓䏓 NGf 1408 paradigm, our model’s output also includes higherlevel syntactic parsing results. 3.1 Training Data (a) NN NNf 憞䠮䞎 (b) NN � � � � JJf NNf 卣 敯埚 We employ a statistical model to parse phrase and word structures as illustrated in Figure 8. The currently available treebank for us is the Penn Chinese Treebank (CTB) 5.0 (Xue et al., 2005). Because our model belongs to the family of head-driven statistical parsing models (Collins, 2003), we use the headfinding rules described by Sun and Jurafsky (2004). Unfortunately, this treebank or any other treebanks for that matter, does not contain annotations of word structures. Therefore, we must annotate these structures by ourselves. The good news is that the annotation is not too complicated. First, we extract all words in the treebank and check each of them manually. Words with non-trivial structures are thus annotated. Finally, we install these small trees of words into the origina</context>
</contexts>
<marker>Xue, Xia, Chiou, Palmer, 2005</marker>
<rawString>Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha Palmer. 2005. The Penn Chinese Treebank: phrase structure annotation of a large corpus. Natural Language Engineering, 11(2):207–238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
</authors>
<title>Chinese word segmentation as character tagging.</title>
<date>2003</date>
<booktitle>Computational Linguistics and Chinese Language Processing,</booktitle>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="1297" citStr="Xue, 2003" startWordPosition="201" endWordPosition="202"> ways, both practically and theoretically. Instead, we propose that word structures should be recovered in morphological analysis. An elegant approach for doing this is given and the result is shown to be promising enough for encouraging further effort in this direction. Our probability model is trained with the Penn Chinese Treebank and actually is able to parse both word and phrase structures in a unified way. 1 Why Parse Word Structures? Research in Chinese word segmentation has progressed tremendously in recent years, with state of the art performing at around 97% in precision and recall (Xue, 2003; Gao et al., 2005; Zhang and Clark, 2007; Li and Sun, 2009). However, virtually all these systems focus exclusively on recognizing the word boundaries, giving no consideration to the internal structures of many words. Though it has been the standard practice for many years, we argue that this paradigm is inadequate both in theory and in practice, for at least the following four reasons. The first reason is that if we confine our definition of word segmentation to the identification of word boundaries, then people tend to have divergent opinions as to whether a linguistic unit is a word or not</context>
</contexts>
<marker>Xue, 2003</marker>
<rawString>Nianwen Xue. 2003. Chinese word segmentation as character tagging. Computational Linguistics and Chinese Language Processing, 8(1):29–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Chinese segmentation with a word-based perceptron algorithm.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>840--847</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="1338" citStr="Zhang and Clark, 2007" startWordPosition="207" endWordPosition="210">heoretically. Instead, we propose that word structures should be recovered in morphological analysis. An elegant approach for doing this is given and the result is shown to be promising enough for encouraging further effort in this direction. Our probability model is trained with the Penn Chinese Treebank and actually is able to parse both word and phrase structures in a unified way. 1 Why Parse Word Structures? Research in Chinese word segmentation has progressed tremendously in recent years, with state of the art performing at around 97% in precision and recall (Xue, 2003; Gao et al., 2005; Zhang and Clark, 2007; Li and Sun, 2009). However, virtually all these systems focus exclusively on recognizing the word boundaries, giving no consideration to the internal structures of many words. Though it has been the standard practice for many years, we argue that this paradigm is inadequate both in theory and in practice, for at least the following four reasons. The first reason is that if we confine our definition of word segmentation to the identification of word boundaries, then people tend to have divergent opinions as to whether a linguistic unit is a word or not (Sproat et al., 1996). This has led to m</context>
</contexts>
<marker>Zhang, Clark, 2007</marker>
<rawString>Yue Zhang and Stephen Clark. 2007. Chinese segmentation with a word-based perceptron algorithm. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 840–847, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Joint word segmentation and POS tagging using a single perceptron.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-08: HLT,</booktitle>
<pages>888--896</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="12939" citStr="Zhang and Clark, 2008" startWordPosition="2215" endWordPosition="2218">n paradigm. (a) NN ✚✚❩❩ NNf NNf ta 扨 (b) NN NNf ta扨 (c) NR NRf 䧢哫膝 Figure 6: Difference between our output (a) of parsing the word 0碜扨 ‘olive oil’ and the output (b) of Luo (2003). In (c) we have a true flat word, namely the location name 䧢哫膝 ‘Los Angeles’. The benefits of joint modeling has been noticed by many. For example, Li et al. (2010) reported that a joint syntactic and semantic model improved the accuracy of both tasks, while Ng and Low (2004) showed it’s beneficial to integrate word segmentation and part-of-speech tagging into one model. The later result is confirmed by many others (Zhang and Clark, 2008; Jiang et al., 2008; Kruengkrai et al., 2009). Goldberg and Tsarfaty (2008) showed that a single model for morphological segmentation and syntactic parsing of Hebrew yielded an error reduction of 12% over the best pipelined models. This is because an integrated approach can effectively take into account more information from different levels of analysis. Parsing of Chinese word structures can be reduced to the usual constituent parsing, for which there has been great progress in the past several years. Our generative model for unified word and phrase structure parsing is a direct adaptation o</context>
</contexts>
<marker>Zhang, Clark, 2008</marker>
<rawString>Yue Zhang and Stephen Clark. 2008. Joint word segmentation and POS tagging using a single perceptron. In Proceedings ofACL-08: HLT, pages 888–896, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Transition-based parsing of the Chinese treebank using a global discriminative model.</title>
<date>2009</date>
<booktitle>In Proceedings of the 11th International Conference on Parsing Technologies, IWPT ’09,</booktitle>
<pages>162--171</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="26966" citStr="Zhang and Clark, 2009" startWordPosition="4681" endWordPosition="4684"> This (b) NN ❩❩ ✚✚ (a) NP ✱ ❧ ✱ ❧ Types LP LR F1 Phrase 79.3 80.1 79.7 Flat 93.2 93.8 93.5 Flat* 97.1 97.6 97.3 POS &amp; Flat 92.7 93.2 92.9 JJ NNf NNf NN 䕛 NNf JJf 㦌斊 㦌 斊䕛 Table 1: Labeled precision and recall for the three types of labels. The line labeled ‘Flat*’ is for unlabeled metrics of flat words, which is effectively the ordinary word segmentation accuracy. Though not directly comparable, we can make some remarks to the accuracy of our model. For constituent parsing, the best result on CTB 5.0 is reported to be 78% F1 measure for unlimited sentences with automatically assigned POS tags (Zhang and Clark, 2009). Our result for phrase labels is close to this accuracy. Besides, the result for flat labels compares favorably with the state of the art accuracy of about 93% F1 for joint word segmentation and part-of-speech tagging (Jiang et al., 2008; Kruengkrai et al., 2009). For ordinary word segmentation, the best result is reported to be around 97% F1 on CTB 5.0 (Kruengkrai et al., 2009), while our parser performs at 97.3%, though we should remember that the result concerns flat words only. Finally, we see the performance of word structure recovery is almost as good as the recognition of flat words. T</context>
</contexts>
<marker>Zhang, Clark, 2009</marker>
<rawString>Yue Zhang and Stephen Clark. 2009. Transition-based parsing of the Chinese treebank using a global discriminative model. In Proceedings of the 11th International Conference on Parsing Technologies, IWPT ’09, pages 162–171, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
</authors>
<title>Character-level dependencies in Chinese: Usefulness and learning.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL</booktitle>
<pages>879--887</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Athens, Greece,</location>
<contexts>
<context position="9741" citStr="Zhao (2009)" startWordPosition="1666" endWordPosition="1667">r generative story describes the complete process from sentence and word structures to the surface string of characters in a top-down fashion. With this probability model, we give an algorithm to find the parse tree of a raw sentence with the highest probability (Section 5). The output of our parser incorporates word structures naturally. Evaluation shows that the model can learn much of the regularity of word structures, and also achieves reasonable accuracy in parsing higher level constituent structures (Section 6). 2 Related Work The necessity of parsing word structures has been noticed by Zhao (2009), who presented a characterlevel dependency scheme as an alternative to the linear representation of words. Although our work is based on the same notion, there are two key differences. The first one is that part-of-speech tags and constituent labels are fundamental for our parsing model, while Zhao focused on unlabeled dependencies between characters in a word, and part-ofspeech information was not utilized. Secondly, we distinguish explicitly the generation of flat words such as 䑵喏䃮 ‘Washington’ and words with internal structures. Our parsing algorithm also has to be adapted accordingly. Suc</context>
</contexts>
<marker>Zhao, 2009</marker>
<rawString>Hai Zhao. 2009. Character-level dependencies in Chinese: Usefulness and learning. In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009), pages 879–887, Athens, Greece, March. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guodong Zhou</author>
<author>Jian Su</author>
</authors>
<title>A Chinese efficient analyser integrating word segmentation, part-ofspeech tagging, partial parsing and full parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the Second SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>78--83</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sapporo, Japan,</location>
<contexts>
<context position="11361" citStr="Zhou and Su, 2003" startWordPosition="1921" endWordPosition="1924">ora between different segmentation standards in the hope of saving some expensive human labor. We believe the best solution to the problem of divergent standards and requirements is to annotate and analyze word structures. Then applications can make use of these structures according to their own convenience. 1407 Since the distinction between morphology and syntax in Chinese is somewhat blurred, our model for word structure parsing is integrated with constituent parsing. There has been many efforts to integrate Chinese word segmentation, part-of-speech tagging and parsing (Wu and Zixin, 1998; Zhou and Su, 2003; Luo, 2003; Fung et al., 2004). However, in these research all words were considered to be flat, and thus word structures were not parsed. This is a crucial difference with our work. Specifically, consider the word 0碜扨 ‘olive oil’. Our parser output tree Figure 6(a), while Luo (2003) output tree (b), giving no hint to the structure of this word since the result is the same with a real flat word 䧢哫膝 ‘Los Angeles’(c). of head-driven generative models (Charniak, 1997; Bikel and Chiang, 2000) . 3 The New Paradigm Given a raw Chinese sentence like 䤕撓 䏓 A敯 䋳 㢧 喓, a traditional word segmentation sys</context>
</contexts>
<marker>Zhou, Su, 2003</marker>
<rawString>Guodong Zhou and Jian Su. 2003. A Chinese efficient analyser integrating word segmentation, part-ofspeech tagging, partial parsing and full parsing. In Proceedings of the Second SIGHAN Workshop on Chinese Language Processing, pages 78–83, Sapporo, Japan, July. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>