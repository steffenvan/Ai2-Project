<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.750571">
The FINITE STRING Newsletter Abstracts of Current Literature
</note>
<bodyText confidence="0.825049666666667">
The Sixth European Meeting on Cybernetics and
Systems Research , organized by the Austrian Society
for Cybernetic Studies, will be held April 13-16, 1982,
at the University of Vienna, Austria. [See AJCL 7:2,
pg. 123.1 For a Preliminary Program contact the
Chairman:
</bodyText>
<affiliation confidence="0.880670666666667">
Professor Robert Trappl
Department of Medical Cybernetics
University of Vienna
</affiliation>
<address confidence="0.724583">
Freyung 6/2
A-1010 Vienna, AUSTRIA
</address>
<bodyText confidence="0.748288815789474">
A Symposium on Artificial Intelligence, sponsored
by the Austrian Society for Artificial Intelligence and
the Austrian Society for Cybernetic Studies, will be
held at the Sixth European Meeting on Cybernetics
and Systems Research, April 13-16, 1982, in Vienna.
[See AJCL 7:2, pg. 123.] Extended abstracts must be
submitted by December 1, 1981, to:
Organizing Committee of the 6th EMCSR 82
Austrian Society for Cybernetic Studies
Schottengasse 3
A-1010 Vienna, AUSTRIA
The 1982 National Computer Conference (NCC),
sponsored by AFIPS, will be held June 7-10, 1982, in
the Houston Astroarena. [See AJCL 7:2, pg. 122.]
Papers must be submitted by October 31, 1981, to:
Dr. Howard L. Morgan
NCC&apos;82 Technical Program Chairman
Department of Decision Sciences
The Wharton School/CC
University of Pennsylvania
Philadelphia, Pennsylvania 19104
The Fifth European Conference on Electrotechnics
will take place in Copenhagen at The Technical Uni-
versity of Denmark, on June 14-18, 1982. [See AJCL
7:2, pg. 122.] Further details may be obtained from:
DIEU, Danish Engineers&apos; Post Graduate Inst.
The Technical Univ. of Denmark, Bldg. 208
DK-2800 Lyngby, DENMARK
The Ninth International Conference on Computation-
al Linguistics (COLING 82) will be held July 5-10,
1982, in Prague, Czechoslovakia. It will be sponsored
by the International Committee on Computational
Linguistics, in association with the Linguistic Institute
of L. Star, Slovak Academy of Science, Bratislava, and
the Faculty of Mathematics and Physics, Charles Uni-
versity, Prague. [See AJCL 7:2, pg. 122.] Four cop-
ies of a 3-4 page, double-spaced summary must be
submitted by December 1, 1981, to:
</bodyText>
<footnote confidence="0.7487255">
COLING 82
MFF UK, Linguistics
Malostranske n. 25
118 00 Prague 1, CZECHOSLOVAKIA
</footnote>
<title confidence="0.814688">
Abstracts of Current Literature
</title>
<note confidence="0.9314085">
KNET Extensions
Michael W. Freeman
</note>
<sectionHeader confidence="0.479729" genericHeader="abstract">
ADO/FSSG
</sectionHeader>
<subsectionHeader confidence="0.659527">
Burroughs Corporation
</subsectionHeader>
<title confidence="0.478538">
P.O. Box 517
Paoli, Pennsylvania 19301
</title>
<author confidence="0.948614">
Henry H. Leitner
</author>
<affiliation confidence="0.843249">
Center for Research in Computing Technology
Aiken Computation Laboratory
Harvard University
Cambridge, Massachusetts 02138
</affiliation>
<subsubsectionHeader confidence="0.53351">
Manuscript, March 1980.
</subsubsectionHeader>
<bodyText confidence="0.9999382">
This paper describes a number of extensions to
Brachman&apos;s Structured Inheritance Network (SI-Net)
formalism, with a view to creating an interactive sys-
tem which will enable domain experts to develop and
enhance knowledge networks (KNETs) representative
of their particular application domains. The three
principal extensions introduced are the QUA and VIZ
links, and augmented event transition networks. The
former permit us to partition the attribute space of
conceptual entities in accordance with the defining
roles played by these entities in certain kinds of socio-
legal interactions. We use an event transition network
to represent the value-class of so-called &amp;quot;dynamic
attributes&amp;quot; in order to define all (and only all) the
possible event sequences which the associated entity
can participate in relative to the given attribute and
still remain itself. The basic concept of simple succes-
sion of events in the transition network is then aug-
mented by conditions on dependency relations which
may exist across events among various roles (including
bounds on the amount of time that can elapse between
successive events). Since such event transition nets
are themselves concepts within the KNET formalism,
they can be placed along a specialization/generaliza-
tion hierarchy. We show how these KNET extensions
provide an extremely flexible basis for maintaining
data base integrity and consistency, determining roll-
back dependencies during error recovery or historical
perspectives, signalling possible irregularities in the
attempted recording of new events which violate the
legal event sequences defined by the net, as well as for
generating derivative sets and mutually non-exclusive
stative attribute values and associated interpretations
for the wide range of ways this opens up for referenc-
ing such concepts in natural language.
</bodyText>
<note confidence="0.957269333333333">
American Journal of Computational Linguistics, Volume 7, Number 3, July-September 1981 185
The FINITE STRING Newsletter Abstracts of Current Literature
Representing Knowledge in an
Interactive Planner
Ann E. Robinson and David E. Wilkins
Artificial Intelligence Center
SRI International
333 Ravenswood Avenue
Menlo Park, California 94025
</note>
<author confidence="0.252131">
Proc. 1980 AAAI Conf., August 1980, 148-150.
</author>
<bodyText confidence="0.999502">
This note discusses the representation for actions
and plans being developed as part of the current plan-
ning research at SRI. Described is a method for uni-
formly representing actions that can take place both in
the domain and during planning. The representation
accommodates descriptions of abstract (hypothetical)
objects.
</bodyText>
<sectionHeader confidence="0.4773625" genericHeader="method">
A Computer Model of Child Language Learning
Mallory Selfridge
</sectionHeader>
<subsectionHeader confidence="0.283098666666667">
Electrical Engineering and Computer Science Dept.
The University of Connecticut
Storrs, Connecticut 06288
</subsectionHeader>
<subsubsectionHeader confidence="0.229778">
Proc. 1980 AAAI Conf., August 1980, 224-227.
</subsubsectionHeader>
<bodyText confidence="0.999822444444444">
A computer program modelling a child between the
ages of 1 and 2 years is described. This program is
based on observations of the knowledge this child had
at age 1, the comprehension abilities he had at age 2,
and the language experiences he had between these
ages. The computer program described begins at the
age 1 level, is given similar language experiences, and
uses inference and learning rules to acquire compre-
hension at the age 2 level.
</bodyText>
<subsectionHeader confidence="0.566102">
An Approach to Acquiring and Applying
Knowledge
</subsectionHeader>
<bodyText confidence="0.250952">
Norman Haas and Gary G. Hendrix
</bodyText>
<sectionHeader confidence="0.679642" genericHeader="method">
Artificial Intelligence Center
SRI International
</sectionHeader>
<subsectionHeader confidence="0.735226">
333 Ravenswood Avenue
Menlo Park, California 94025
</subsectionHeader>
<subsubsectionHeader confidence="0.462118">
Proc. 1980 AAAI Conf., August 1980, 235-239.
</subsubsectionHeader>
<bodyText confidence="0.999958210526316">
The problem addressed in this paper is how to ena-
ble a computer system to acquire facts about new do-
mains from tutors who are experts in their respective
fields, but who have little or no training in computer
science. The information to be acquired is that needed
to support question-answering activities. The basic
acquisition approach is &amp;quot;learning by being told.&amp;quot; We
have been especially interested in exploring the notion
of simultaneously learning not only new concepts, but
also the linguistic constructions used to express those
concepts. As a research vehicle we have developed a
system that is preprogrammed with deductive algor-
ithms and a fixed set of syntactic/semantic rules cov-
ering a small subset of English. It has been endowed
with sufficient seed concepts and seed vocabulary to
support effective tutorial interaction. Furthermore,
the system is capable of learning new concepts and
vocabulary, and can apply its acquired knowledge in a
prescribed range of problem-solving situations.
</bodyText>
<subsectionHeader confidence="0.675629666666667">
Project EPISTLE: A System for the Automatic
Analysis of Business Correspondence
Lance A. Miller
</subsectionHeader>
<note confidence="0.8789715">
IBM Thomas J. Watson Research Center
P.O. Box 218
</note>
<author confidence="0.48577">
Yorktown Heights, New York 10598
Proc. 1980 AAAI Conf., August 1980, 280-282.
</author>
<bodyText confidence="0.999939090909091">
The system described here is intended to provide
the business executive with useful applications for the
computer processing of correspondence in the office
environment. Applications will include the synopsis
and abstraction of incoming mail and a variety of criti-
ques of newly-generated letters, all based upon the
capability of understanding the natural language text
at least to a level corresponding to customary business
communication. Successive sections of the paper de-
scribe the background and prior work, the planned
system output, and the implementation.
</bodyText>
<subsectionHeader confidence="0.818949">
When Expectation Fails: Towards a
Self-Correcting Inference System
</subsectionHeader>
<author confidence="0.933392">
Richard H. Granger, Jr.
</author>
<affiliation confidence="0.87768075">
Artificial Intelligence Project
Department of Information and Computer Science
University of California
Irvine, California 92717
</affiliation>
<subsubsectionHeader confidence="0.332455">
Proc. 1980 AAA/ Conf., August 1980, 301-305.
</subsubsectionHeader>
<bodyText confidence="0.999892285714286">
Contextual understanding depends on a reader&apos;s
ability to correctly infer a context within which to
interpret the events in a story. This &amp;quot;context-selection
problem&amp;quot; has traditionally been expressed in terms of
heuristics for making the correct initial selection of a
story context. This paper presents a view of context
selection as an ongoing process spread throughout the
understanding process. This view requires that the
understander be capable of recognizing and correcting
erroneous initial context inferences. A computer pro-
gram called ARTHUR is described, which selects the
correct context for a story by dynamically re-
evaluating its own initial inferences in light of subse-
quent information in a story.
</bodyText>
<subsectionHeader confidence="0.904278666666667">
Generating Relevant Explanations:
Natural Language Responses to Questions
about Database Structure
</subsectionHeader>
<author confidence="0.950168">
Kathleen R. McKeown
</author>
<affiliation confidence="0.96144525">
Computer and Information Science
Moore School of Electrical Engineering D2
University of Pennsylvania
Philadelphia, Pennsylvania 19104
</affiliation>
<note confidence="0.674410666666667">
Proc. 1980 AAA/ Conf., August 1980, 306-309.
186 American Journal of Computational Linguistics, Volume 7, Number 3, July-September 1981
The FINITE STRING Newsletter Abstracts of Current Literature
</note>
<bodyText confidence="0.9998958">
The research described here is aimed at unresolved
problems in both natural language generation and nat-
ural language interfaces to database systems. How
relevant information is selected and then organized for
the generation of responses to questions about data-
base structure is examined. Due to limited space, this
paper reports on only one method of explanation,
called &amp;quot;compare and contrast.&amp;quot; In particular, it de-
scribes a specific constraint on relevancy and organi-
zation that can be used for this response type.
</bodyText>
<subsectionHeader confidence="0.886045">
The Semantic Interpretation of
Nominal Compounds
</subsectionHeader>
<author confidence="0.513893">
Timothy Wilking Finin
</author>
<affiliation confidence="0.947897">
Computer and Information Science
Moore School of Electrical Engrg. D2
University of Pennsylvania
Philadelphia, Pennsylvania 19104
</affiliation>
<subsubsectionHeader confidence="0.419933">
Proc. 1980 AAAI Conf., Aug, 1980, 310-312.
</subsubsectionHeader>
<bodyText confidence="0.999861428571428">
This paper briefly introduces an approach to the
problem of building semantic interpretations of nomi-
nal compounds, i.e. sequences of two or more nouns
related through modification. Examples of the kinds
of nominal compounds dealt with are: &amp;quot;engine re-
pairs,&amp;quot; &amp;quot;aircraft flight arrival,&amp;quot; &amp;quot;aluminum water
pump,&amp;quot; and &amp;quot;noun noun modification.&amp;quot;
</bodyText>
<note confidence="0.64398">
Towards an Al Model of Argumentation
Lawrence Birnbaum, Margot Flowers, and Rod
McGuire
</note>
<subsectionHeader confidence="0.4237065">
Department of Computer Science
Box 2158 Yale Station
Yale University
New Haven, Connecticut 06520
</subsectionHeader>
<subsubsectionHeader confidence="0.280403">
Proc. 1980 AAAI Conf., August 1980, 313-315.
</subsubsectionHeader>
<bodyText confidence="0.999965714285714">
This paper describes a process model of human
argumentation, and provides examples of its operation
as implemented in a computer program. Our main
concerns include such issues as the rules and structures
underlying argumentation, how these relate to conver-
sational rules, how reasoning is used in arguments, and
how arguing and reasoning interact.
</bodyText>
<subsectionHeader confidence="0.606935">
Knowledge Representation for
Syntactic/Semantic Processing
</subsectionHeader>
<author confidence="0.269144">
Robert J. Bobrow
</author>
<affiliation confidence="0.6758955">
Bolt Beranek and Newman Inc.
50 Moulton Street
Cambridge, Massachusetts 02138
Bonnie Lynn Webber
Computer and Information Science
Moore School of Electrical Engrg. 02
University of Pennsylvania
Philadelphia, Pennsylvania 19104
</affiliation>
<subsubsectionHeader confidence="0.385384">
Proc. 1980 AAAI Conf., August 1980, 316-323.
</subsubsectionHeader>
<bodyText confidence="0.999954166666667">
This paper describes the RUS framework for natural
language processing, in which a parser incorporating a
substantial ATN grammar for English interacts with a
semantic interpreter to simultaneously parse and inter-
pret input. The structure of that interaction is dis-
cussed, including the roles played by syntactic and
semantic knowledge. Several implementations of the
RUS framework are currently in use, sharing the same
grammar, but differing in the form of their semantic
component. One of these, the PSI-KLONE system, is
based on a general object-centered knowledge repre-
sentation system, called KL-ONE. The operation of
PSI-KLONE is described, including its use of KL-
ONE to support a general inference process called
&amp;quot;incremental description refinement.&amp;quot; The last section
of the paper discusses several important criteria for
knowledge representation systems to be used in syn-
tactic and semantic processing.
</bodyText>
<sectionHeader confidence="0.534293" genericHeader="method">
Language and Memory:
</sectionHeader>
<subsectionHeader confidence="0.610133833333333">
Generalization as a Part of Understanding
Michael Lebowitz
Computer Science Department
Columbia University
406 Mudd Building
New York, New York 10027
</subsectionHeader>
<subsubsectionHeader confidence="0.204715">
Proc. 1980 AAA/ Conf., August 1980, 324-326.
</subsubsectionHeader>
<bodyText confidence="0.999865833333333">
This paper presents the Integrated Partial Parser
(IPP), a computer model that combines text under-
standing and memory of events. An extended example
of the program&apos;s ability to understand newspaper sto-
ries and make generalizations that are useful for mem-
ory organization is presented.
</bodyText>
<subsectionHeader confidence="0.635449">
Failures in Natural Language Systems:
Applications to Data Base Query Systems
Eric Mays
</subsectionHeader>
<affiliation confidence="0.85593425">
Computer and Information Science
Moore School of Electrical Engineering D2
University of Pennsylvania
Philadelphia, Pennsylvania 19104
</affiliation>
<subsubsectionHeader confidence="0.372754">
Proc. 1980 AAAI Conf., August 1980, 327-330.
</subsubsectionHeader>
<bodyText confidence="0.95466845">
A significant class of failures in interactions with
data base query systems is attributable to misconcep-
tions or incomplete knowledge regarding the domain
of discourse on the part of the user. This paper de-
scribes several types of user failures, namely, inten-
sional failures of presumptions. These failures are
distinguished from extensional failures of presumptions
since they are dependent on the structure rather than
the contents of the data base. A knowledge represent-
ation has been developed for the recognition of inten-
sional failures that are due to the assumption of non-
existent relationships between entities. Several other
intensional failures which depend on more sophisticat-
ed knowledge representations are also discussed. Ap-
American Journal of Computational Linguistics, Volume 7, Number 3, July-September 1981 187
The FINITE STRING Newsletter Abstracts of Current Literature
propriate forms of corrective behavior are outlined
which would enable the user to formulate queries di-
rected to the solution of his/her particular task and
compatible with the knowledge organization.
</bodyText>
<subsectionHeader confidence="0.981749">
Organizing Memory and Keeping It Organized
Janet L. Kolodner
Information and Computer Science Department
Georgia Institute of Technology
Atlanta, Georgia 30332
</subsectionHeader>
<subsubsectionHeader confidence="0.558067">
Proc. 1980 AAAI Conf., August 1980, 331-333.
</subsubsectionHeader>
<bodyText confidence="0.9983682">
Maintaining good memory organization is important
in large memory systems. This paper presents a
scheme for automatically reorganizing event informa-
tion in memory. The processes are implemented in a
computer program called CYRUS.
</bodyText>
<subsectionHeader confidence="0.942605833333333">
Narrative Text Summarization
Wendy Lehnert
Department of Computer Science
Box 2158 Yale Station
Yale University
New Haven, Connecticut 06520
</subsectionHeader>
<subsubsectionHeader confidence="0.341697">
Proc. 1980 AAAI Conf., August 1980, 337-339.
</subsubsectionHeader>
<bodyText confidence="0.999900307692308">
In order to summarize a story it is necessary to
access a high level analysis that highlights the story&apos;s
central concepts. A technique of memory representa-
tion based on affect units appears to provide the nec-
essary foundation for such an analysis. Affect units
are conceptual structures that overlap with each other
when a narrative is cohesive. When overlapping inter-
sections are interpreted as arcs in a graph of affect
units, the resulting graph encodes the plot of the story.
Structural features of the graph then reveal which
concepts are central to the story. Affect unit analysis
is currently being investigated as a processing strategy
for narrative summarization.
</bodyText>
<subsectionHeader confidence="0.974375428571428">
Meta-Planning
Robert Wilensky
Electronics Research Laboratory
Computer Science Division
Department of EECS
University of California, Berkeley
Berkeley, California 94720
</subsectionHeader>
<subsubsectionHeader confidence="0.551896">
Proc. 1980 AAAI Conf., August 1980, 334-336.
</subsubsectionHeader>
<bodyText confidence="0.999870533333333">
This paper is concerned with the problems of plan-
ning and understanding. These problems are related
because a natural language understander must apply
knowledge about people&apos;s goals and plans in order to
make the inferences necessary to explain the behavior
of a character in a story. Thus while a story under-
stander is not a planner, it must embody a theory of
planning knowledge. I have developed such a theory
in the construction of PAM (Plan Applier Mecha-
nism), a story understanding program. This paper is
concerned not with the understanding mechanism it-
self, but that part of its planning knowledge which is
independent of whether that knowledge is used to
explain someone&apos;s behavior or to generate a plan for
one&apos;s own use.
</bodyText>
<subsectionHeader confidence="0.994814">
Issues in the Development of
Natural Language Front-Ends
</subsectionHeader>
<author confidence="0.829459">
J. Hendler, T.P. Kehler, P.R. Michaelis,
B. Phillips, K.M. Ross, and H.R. Tennant
</author>
<subsectionHeader confidence="0.62302">
Texas Instruments
P.O. Box 225936
MS 371
Dallas, Texas 75023
</subsectionHeader>
<subsubsectionHeader confidence="0.688343">
AFIPS Conf. Proc. 50, 1981 NCC, May 1981, 643-648.
</subsubsectionHeader>
<bodyText confidence="0.999948125">
This paper will discuss some issues we believe to be
important to the design of a natural language front-
end. These are divided into three categories: concep-
tual coverage, linguistic coverage, and implementation
issues. The section on conceptual coverage discusses
the use of a domain expert, which understands what
the user is saying even though the system to which the
front-end is interfaced might not be able to properly
do what the user wants. The section on linguistic
coverage discusses attempts to allow a natural lan-
guage interface to handle natural, interactive human
communication. Two solutions are explored: first, the
design of a robust natural language understanding
system composed of many experts that know about
some aspect of the organization of language is consid-
ered; second, because the design of a robust system is
a large task, the intermediate goal of limiting the vo-
cabulary and constructions that can be used while
retaining all the user-oriented benefits of natural lan-
guage is considered. The implementation issues con-
sidered are the design of a system in which the gram-
mar and the domain of discourse can be easily extend-
ed and which can be used for more than one domain
without extensive rewriting.
</bodyText>
<subsectionHeader confidence="0.900030833333333">
Text-Critiquing with the EPISTLE System:
An Author&apos;s Aid to Better Syntax
Lance A. Miller, George E. Heidorn, and Karen Jensen
IBM Thomas J. Watson Research Center
P.O. Box 218
Yorktown Heights, New York 10598
</subsectionHeader>
<subsubsectionHeader confidence="0.597191">
AFIPS Conf. Proc. 50, 1981 NCC, May 1981, 649-655.
</subsubsectionHeader>
<bodyText confidence="0.999804222222222">
The experimental EPISTLE system is ultimately
intended to provide office workers with intelligent
applications for the processing of natural language
text, particularly business correspondence. A variety
of possible critiques of textual material are identified
in this paper, but the discussion focuses on the
system&apos;s capability to detect several classes of gram-
matical errors, such as disagreement in number be-
tween the subject and the verb. The system&apos;s error-
</bodyText>
<page confidence="0.7715">
188 American Journal of Computational Linguistics, Volume 7, Number 3, July-September 1981
</page>
<note confidence="0.399922">
The FINITE STRING Newsletter Abstracts of Current Literature
</note>
<bodyText confidence="0.999507333333333">
detection performance relies critically on its parsing
component which determines the syntactic structure of
each sentence and the grammatical functions fulfilled
by various phrases. Details of the system&apos;s operations
are provided, and some of the future critiquing objec-
tives are outlined.
</bodyText>
<subsectionHeader confidence="0.9428682">
Shifting to a Higher Gear in
a Natural Language System
Bozena Henisz Thompson and Frederick B. Thompson
California Institute of Technology
Pasadena, California 91125
</subsectionHeader>
<subsubsectionHeader confidence="0.644402">
AFIPS Conf. Proc. 50, 1981 NCC, May 1981, 657-662.
</subsubsectionHeader>
<bodyText confidence="0.999970117647059">
We have completed the development of the REL
System, a system for communicating with the comput-
er in natural language concerning a relational data-
base. We have been using that system in a series of
experiments on how people actually do communicate
in solving an intellectual task. These experiments,
together with our general experience with REL and
related work elsewhere, have led us to the specifica-
tion and development of a new system, the POL
(Problem Oriented Language) System. POL is an
evolutionary extension of REL, preserving what has
worked, and extending and adding new capabilities to
meet observed needs. These improvements include
more responsive diagnostics, handling of sentence
fragments, inter-knowledge-base communications, and
new facilities for building and extending the knowl-
edge bases of users. This paper introduces POL.
</bodyText>
<subsectionHeader confidence="0.9548145">
A Practical Comparison of Parsing Strategies
Jonathan Slocum
Linguistics Research Center
P.O. Box 7247
University of Texas
Austin, Texas 78712
</subsectionHeader>
<subsubsectionHeader confidence="0.627419">
Proc. 19th Annual ACL Conf., June 1981, 1-6.
</subsubsectionHeader>
<bodyText confidence="0.999978361111111">
Although the literature dealing with formal and
natural languages abounds with theoretical arguments
of worst-case performance by various parsing strate-
gies, there is little discussion of comparative perform-
ance based on actual practice in understanding natural
language. Yet important practical considerations do
arise when writing programs to understand one aspect
or another of natural language utterances. Where, for
example, a theorist will characterize a parsing strategy
according to its space and/or time requirements in
attempting to analyze the worst possible input accord-
ing to an arbitrary grammar strictly limited in expres-
sive power, the researcher studying natural language
processing can be justified in concerning himself more
with issues of practical performance in parsing sen-
tences encountered in language as humans actually use
it, using a grammar expressed in a form convenient to
the human linguist who is writing it.
This paper has two purposes. One is to report an
evaluation of the performance of several parsing stra-
tegies in a real-world setting, pointing out practical
problems in making the attempt, indicating which of
the strategies is superior to the others in which situa-
tions, and most of all determining the reasons why the
best strategy outclasses its competition in order to
stimulate and direct the design of improvements. The
other, more important purpose is to assist in establish-
ing such evaluation as a meaningful and valuable en-
terprise that contributes to the evolution of natural
language processing from an art form into an empirical
science. That is, our concern for parsing efficiency
transcends the issue of mere practicality.
In this paper we detail our experimental setting and
approach, present the results, discuss the implications
of those results, and conclude with some remarks on
what has been learned.
</bodyText>
<subsectionHeader confidence="0.975392571428571">
Computational Complexity and
Lexical Functional Grammar
Robert C. Berwick
Al Laboratory
Massachusetts Institute of Technology
545 Technology Square
Cambridge, Massachusetts 02139
</subsectionHeader>
<subsubsectionHeader confidence="0.565625">
Proc. 19th Annual ACL Conf., June 1981, 7-12.
</subsubsectionHeader>
<bodyText confidence="0.989912114285714">
Recently, a new theory of grammar has been ad-
vanced with the explicitly stated aim of meeting the
dual demands of learnability and parsability — the
Lexical Functional Grammars (LFGs) of Bresnan.
The theory of Lexical Functional Grammars is claimed
to have all the descriptive merits of transformational
grammar, but none of its computational unruliness. In
LFG, there are no transformations (as classically de-
scribed); the work formerly ascribed to transforma-
tions such as &amp;quot;passive&amp;quot; is shouldered by information
stored in lexical entries associated with lexical items.
The elimination of transformational power naturally
gives rise to the hope that a lexically-based system
would be computationally simpler than a transforma-
tional one.
The main result of this paper is to show that cer-
tain Lexical Functional Grammars can generate lan-
guages whose recognition time is very likely computa-
tionally intractable, at least according to our current
understanding of what is or is not rapidly solvable.
Briefly, the demonstration proceeds by showing how a
problem that is widely conjectured to be computation-
ally difficult — namely, whether there exists an as-
signment of l&apos;s and O&apos;s (or &amp;quot;T&amp;quot;s and &amp;quot;F&apos;s) to the
literals of a Boolean formula in conjunctive normal
form that makes the formula evaluate to &amp;quot;1&amp;quot; (or
&amp;quot;true&amp;quot;) — can be re-expressed as the problem of rec-
ognizing whether a particular string is or is not a mem-
American Journal of Computational Linguistics, Volume 7, Number 3, July-September 1981 189
The FINITE STRING Newsletter Abstracts of Current Literature
ber of the language generated by a certain lexical
functional grammar.
This paper also discusses the relevance of this tech-
nical result for more down-to-earth computational
linguistics.
</bodyText>
<subsectionHeader confidence="0.926634375">
Corepresentational Grammar and
Parsing English Comparatives
Karen Ryan
Linguistics Department
142 Klaeber Court
University of Minnesota
Minneapolis, Minnesota 55455
Proc. 19th Annual ACL Conf., June 1981, 13-18.
</subsectionHeader>
<bodyText confidence="0.999847045454546">
Marcus (1980) notes that the syntax of English
comparative constructions is highly complex, and
claims that both syntactic and semantic information
must be available for them to be parsed. This paper
argues that comparatives can be structurally analyzed
on the basis of syntactic information alone via a strict-
ly surface-based grammar. Such a grammar is given in
Ryan (1981), based on the corepresentational model
of Kac (1978). While the grammar does not define a
parsing algorithm per se, it nonetheless expresses regu-
larities of surface organization and its relationship to
semantic interpretation that an adequate parser would
be expected to incorporate. The central problem in
parsing comparatives involves identifying the argu-
ments of comparative predicates, and the relations
borne by these arguments to such predicates. A core-
presentational grammar is explicitly designed to assign
predicate-argument structure to sentences on the basis
of their surface syntactic organization. This paper
discusses four problem areas in the description of com-
paratives and outlines the sections of the grammar of
Ryan (1981) that apply to them.
</bodyText>
<subsectionHeader confidence="0.949419181818182">
Performance Comparison of Component
Algorithms for the Phonemicization of
Orthography
Jared Bernstein
Telesensory Speech Systems
3408 Hillview Avenue
Palo Alto, California 94304
Larry Nessly
University of North Carolina
Chapel Hill, North Carolina 27514
Proc. 19th Annual ACL Conf., June 1981, 19-22.
</subsectionHeader>
<bodyText confidence="0.999919892857143">
A system for converting English text into synthetic
speech can be divided into two processes that operate
in series: a text-to-phoneme converter, and a
phonemic-input speech synthesizer. The conversion of
orthographic text into a phonemic form may itself
comprise several processes in series, such as formatting
text to expand abbreviations and non-alphabetic ex-
pressions, parsing and word class determination, seg-
mental phonemicization of words, word and clause
level stress assignment, word internal and word bound-
ary allophonic adjustments, and duration and funda-
mental frequency settings for phonological units.
Comparing the accuracy of different algorithms for
text-to-phoneme conversion is often difficult because
authors measure and report system performance in
incommensurable ways. Furthermore, comparison of
the output speech from two complete systems may not
always provide a good test of the performance of the
corresponding component algorithms in the two sys-
tems, because radical performance differences in other
components can obscure small differences in the com-
ponents of interest. The only reported direct compari-
son of two complete text-to-speech systems (MITALK
and TSI&apos;s TTS-X) was conducted by Bernstein and
Pisoni. This paper reports one study that compared
two algorithms for automatic segmental phonemiciza-
tion of words, and a second study that compared two
algorithms for automatic assignment of lexical stress.
</bodyText>
<subsectionHeader confidence="0.904096857142857">
PHONY: A Heuristic Phonological Analyzer
Lee A. Becker
Department of Computer Science
Indiana University
101 Lindley Hall
Bloomington, Indiana 47401
Proc. 19th Annual ACL Conf., June 1981, 23-27.
</subsectionHeader>
<bodyText confidence="0.958386695652174">
PHONY is a program to do phonological analysis.
Within the generative model of grammar the function
of the phonological component is to assign a phonetic
representation to an utterance by modifying the
&amp;quot;underlying representations&amp;quot; (URs) of its constituent
morphemes. URs are abstract entities which contain
the idiosyncratic information about pronunciations of
morphemes.
The input to PHONY is pronunciations of words
and phrases upon which a preliminary morphological
analysis has been completed. They have been divided
into morphemes, and different instances of the same
morpheme have been associated. These are represent-
ed as strings of phonetic symbols including morpheme-
and word-boundaries. Indices are used to associate
various instances of the same morpheme. The output
of PHONY is a set of phonological rules or regularities
in the data, as well as a set of underlying representa-
tions for the morphemes. The phonological rules gen-
erate the various pronunciations of the morphemes
from their underlying representations.
190 American Journal of Computational Linguistics, Volume 7, Number 3, July-September 1981
The FINITE STRING Newsletter Abstracts of Current Literature
</bodyText>
<subsectionHeader confidence="0.7589716">
Two Discourse Generators
William C. Mann
USC/Information Sciences Institute
4676 Admiralty Way
Marina del Rey, California 90291
</subsectionHeader>
<subsubsectionHeader confidence="0.78969">
Proc. 19th Annual ACL Conf., June 1981, 43-47.
</subsubsectionHeader>
<bodyText confidence="0.999989761904762">
The task of discourse generation is to produce mul-
tisentential text in natural language which (when heard
or read) produces effects (informing, motivation, etc.)
and impressions (conciseness, correctness, ease of
reading, etc.) which are appropriate to a need or goal
held by the creator of the text. In comparing two AI
discourse generators here we can do no more than
suggest opportunities and attractive options for future
exploration. We describe them only in terms of a few
of the techniques which they employ, partly because
these techniques seem more valuable than the system
designs in which they happen to have been used. The
systems which we describe are PROTEUS, by Antho-
ny Davey at Edinburgh and KDS by Mann and Moore
at ISI, both of which are severely limited and idiosync-
ratic in scope and technique. First we identify partic-
ular techniques in each system which contribute
strongly to the quality of the resulting text. Then we
compare the two systems discussing their common
failings and the possibilities for creating a system hav-
ing the best of both.
</bodyText>
<subsectionHeader confidence="0.8035535">
A Grammar and a Lexicon for a
Text-Production System
Christian Matthiessen
USC/Information Sciences Institute
4676 Admiralty Way
Marina del Rey, California 90291
</subsectionHeader>
<subsubsectionHeader confidence="0.810567">
Proc. 19th Annual ACL Conf., June 1981, 49-55.
</subsubsectionHeader>
<bodyText confidence="0.999943833333333">
In a text-production system high and special de-
mands are placed on the grammar and the lexicon.
This paper views these components in such a system.
First, the subcomponents dealing with semantic infor-
mation and with syntactic information are presented
separately. The problems of relating these two types
of information are then identified. Finally, strategies
designed to meet the problems are proposed and dis-
cussed. One of the issues that is illustrated is what
happens when a systemic linguistic approach is com-
bined with a KL-ONE like knowledge representation
— a novel and hitherto unexplored combination.
</bodyText>
<subsectionHeader confidence="0.786845">
Language Production:
The Source of the Dictionary
</subsectionHeader>
<affiliation confidence="0.79184925">
David D. McDonald
Computer and Information Science
University of Massachusetts
Amherst, Massachusetts 01002
</affiliation>
<subsubsectionHeader confidence="0.892274">
Proc. 19th Annual ACL Conf., June 1981, 57-62.
</subsubsectionHeader>
<bodyText confidence="0.999970277777778">
Ultimately in any natural language production sys-
tem the largest amount of human effort will go into
the construction of the dictionary: the data base that
associates objects and relations in the program&apos;s do-
main with the words and phrases that could be used to
describe them. This paper describes a technique for
basing the dictionary directly on the semantic abstrac-
tion network used for the domain knowledge itself,
taking advantage of the inheritance and specialization
mechanisms of a network formalism such as KL-ONE.
The technique creates considerable economies of scale,
and makes possible the automatic description of indi-
vidual objects according to their position in the seman-
tic net. Furthermore, because the process of deciding
what properties to use in an object&apos;s description is
now given over to a common procedure, we can write
general-purpose rules to, for example, avoid redundan-
cy or grammatically awkward constructions.
</bodyText>
<subsectionHeader confidence="0.9435684">
Analogies in Spontaneous Discourse
Rachel Reichman
Bolt Beranek and Newman Inc.
50 Moulton Street
Cambridge, Massachusetts 02138
</subsectionHeader>
<subsubsectionHeader confidence="0.722571">
Proc. 19th Annual ACL Conf., June 1981, 63-69.
</subsubsectionHeader>
<bodyText confidence="0.9999909375">
This paper presents an analysis of analogies based
on observations of natural conversations. People&apos;s
spontaneous use of analogies provides insight into
their implicit evaluation procedures for analogies. The
treatment here, therefore, reveals aspects of analogical
processing that are somewhat more difficult to see in
an experimental context. The work involves explicit
treatment of the discourse context in which analogy
occurs. A major focus here is the formalization of the
effects of analogy on discourse development. There is
much rule-like behavior in this process, both in under-
lying thematic development of the discourse and in the
surface linguistic forms used in this development.
Both these forms of regular behavior are discussed in
terms of a hierarchical structuring of a discourse into
distinct, but related and linked, context spaces.
</bodyText>
<subsectionHeader confidence="0.93201">
Investigation of Processing Strategies for the
Structural Analysis of Arguments
Robin Cohen
Department of Computer Science
University of Toronto
</subsectionHeader>
<bodyText confidence="0.516584">
Toronto, Ontario CANADA M5S 1A7
</bodyText>
<subsubsectionHeader confidence="0.825045">
Proc. 19th Annual ACL Conf., June 1981, 71-75.
</subsubsectionHeader>
<bodyText confidence="0.991939055555556">
This paper outlines research on processing strategies
being developed for a language understanding system,
designed to interpret the structure of arguments. For
the system, arguments are viewed as trees, with claims
as fathers to their evidence. Then understanding be-
comes a problem of developing a representative argu-
ment tree, by locating each proposition of the argu-
American Journal of Computational Linguistics, Volume 7, Number 3, July-September 1981 191
The FINITE STRING Newsletter Abstracts of Current Literature
ment at its appropriate place. The processing strate-
gies we develop for the hearer are based on expecta-
tions that the speaker will use particular coherent
transmission strategies and are designed to be fairly
efficient (work in linear time). We also comment on
the use by the speaker of linguistic clues to indicate
structure, illustrating how the hearer can interpret the
clues to limit his processing search and thus improve
the complexity of the understanding process.
</bodyText>
<subsectionHeader confidence="0.891062714285714">
What&apos;s Necessary to Hide?:
Modelling Action Verbs
James F. Allen
Department of Computer Science
University of Rochester
Mathematical Sciences Building
Rochester, New York 14627
</subsectionHeader>
<subsubsectionHeader confidence="0.70098">
Proc. 19th Annual ACL Conf., June 1981, 77-83.
</subsubsectionHeader>
<bodyText confidence="0.999973">
This paper considers what types of knowledge one
must possess in order to reason about actions. Rather
than concentrating on how actions are performed, as is
done in the problem-solving literature, it examines the
set of conditions under which an action can be said to
have occurred. In other words, if one is told that ac-
tion A occurred, what can be inferred about the state
of the world? In particular, if the representation can
define such conditions, it must have good models of
time, belief, and intention. This paper discusses these
issues and suggests a formalism in which general ac-
tions and events can be defined. Throughout, the
action of hiding a book from someone is used as a
motivating example.
</bodyText>
<sectionHeader confidence="0.600486" genericHeader="method">
A Rule-based Conversation Participant
Robert E. Frederking
</sectionHeader>
<subsectionHeader confidence="0.872717">
Department of Computer Science
Carnegie-Mellon University
Pittsburgh, Pennsylvania 15213
Proc. 19th Annual ACL Conf., June 1981, 83-87.
</subsectionHeader>
<bodyText confidence="0.999788583333333">
The problem of modelling human understanding and
generation of a coherent dialog is investigated by sim-
ulating a conversation participant. The rule-based
system currently under development attempts to cap-
ture the intuitive concept of &amp;quot;topic&amp;quot; using data struc-
tures consisting of declarative representations of the
subjects under discussion linked to the utterances and
rules that generated them. Scripts, goal trees, and a
semantic network are brought to bear by general,
domain-independent conversational rules to under-
stand and generate coherent topic transitions and spe-
cific output utterances.
</bodyText>
<note confidence="0.598945">
Search and Inference Strategies in Pronoun
Resolution: An Experimental Study
</note>
<subsectionHeader confidence="0.8519275">
Kate Ehrlich
Department of Psychology
University of Massachusetts
Amherst, Massachusetts 01003
</subsectionHeader>
<subsubsectionHeader confidence="0.493043">
Proc. 19th Annual ACL Conf., June 1981, 89-93.
</subsubsectionHeader>
<bodyText confidence="0.999992411764706">
The process of assigning a referent to a pronoun
can be viewed as utilizing two kinds of strategies.
One strategy is concerned with selecting the best re-
ferent from among the candidates available. The other
strategy is concerned with searching through memory
for the candidates. These two types of strategies,
which will be referred to mnemonically as inference
and search strategies, have different kinds of charac-
teristics. A search strategy dictates the order in which
candidates are evaluated, but has no machinery for
carrying out the evaluation. The inference strategy
helps to set up the representation of the information in
the text against which candidates can be evaluated,
but has no way of finding the candidates. In this pa-
per, the way these strategies might interact is explored
and the results of two studies that bear on the issues
are reported on.
</bodyText>
<subsectionHeader confidence="0.943225166666667">
Presupposition and lmplicature in
Model-Theoretic Pragmatics
Douglas B. Moran
Department of Computer Science
Oregon State University
Corvallis, Oregon 97331
</subsectionHeader>
<subsubsectionHeader confidence="0.580603">
Proc. 19th Annual ACL Conf., June 1981, 107-108.
</subsubsectionHeader>
<bodyText confidence="0.999933">
Model-theoretic pragmatics is an attempt to provide
a formal description of the pragmatics of natural lan-
guage as effects arising from using model-theoretic
semantics in a dynamic environment. The pragmatic
phenomena considered here have been variously la-
beled presupposition and conventional implicature.
The models used in traditional model-theoretic
semantics provide a complete and static representation
of knowledge about the world. However, this is not
the environment in which language is used. Language
is used in a dynamic environment — the participants
have incomplete knowledge of the world and the un-
derstanding of a sentence can add to the knowledge of
the listener. A formalism which allows models to con-
tain incomplete knowledge and to which knowledge
can be added has been developed.
</bodyText>
<subsectionHeader confidence="0.937952833333333">
Some Computational Aspects of
Situation Semantics
Jon Barwise
Department of Philosophy
Stanford University
Stanford, California 94305
</subsectionHeader>
<subsubsectionHeader confidence="0.617291">
Proc. 19th Annual ACL Conf., June 1981, 109-111.
</subsubsectionHeader>
<bodyText confidence="0.970727379310345">
192 American Journal of Computational Linguistics, Volume 7, Number 3, July-September 1981
The FINITE STRING Newsletter Abstracts of Current Literature
Can a realist model theory of natural language be
computationally plausible? Or, to put it another way,
is the view of linguistic meaning as a relation between
expressions of natural language and things (objects,
properties, etc.) in the world, as opposed to a relation
between expressions and procedures in the head, con-
sistent with a computational approach to understand-
ing natural language? The model theorist must either
claim that the answer is yes, or be willing to admit
that humans transcend the computationally feasible in
their use of language.
Until recently the only model theory of natural
language that was at all well developed was Montague
Grammar. Unfortunately, it was based on the primi-
tive notion of &amp;quot;possible world&amp;quot; and so was not a real-
ist theory, unless you are prepared to grant that all
possible worlds are real. Montague Grammar is also
computationally intractable, for reasons discussed in
the paper.
John Perry and I have developed a somewhat dif-
ferent approach to the model theory of natural lan-
guage, a theory we call &amp;quot;Situation Semantics&amp;quot;. Since
one of my own motivations in the early days of this
project was to use the insights of generalized recursion
theory to find a computationally plausible alternative
to Montague Grammar, it seems fitting to give a prog-
ress report here.
</bodyText>
<subsectionHeader confidence="0.9669782">
A Situation Semantics Approach to the
Analysis of Speech Acts
David Andreoff Evans
Stanford University
Stanford, California 94305
</subsectionHeader>
<subsubsectionHeader confidence="0.800186">
Proc. 19th Annual ACL Conf., June 1981, 113-116.
</subsubsectionHeader>
<bodyText confidence="0.999974529411765">
During the past two decades, much work in linguis-
tics has focused on sentences as minimal units of com-
munication, and the project of rigorously characteriz-
ing the structure of sentences in natural language has
met with some success. Not surprisingly, however,
sentence grammars have contributed little to the analy-
sis of discourse. Human discourse consists not just of
words in sequences directed by a speaker to an addres-
see, used to represent situations and to reveal intentions.
Only when the addressee has apprehended both these
aspects of the message communicated can the message
be interpreted. The analysis of discourse that emerges
from Austin&apos;s work, grounded in a theory of action,
takes this view as central, and the concept of the
speech act follows naturally. This paper describes a
situation semantics approach to the analysis of speech
acts.
</bodyText>
<subsectionHeader confidence="0.7692895">
Problems in Logical Form
Robert C. Moore
Artificial Intelligence Center
SRI International
333 Ravenswood Avenue
Menlo Park, California 94025
</subsectionHeader>
<subsubsectionHeader confidence="0.802903">
Proc. 19th Annual ACL Conf., June 1981, 117-124.
</subsubsectionHeader>
<bodyText confidence="0.999686866666667">
This paper surveys what we at SRI view as some of
the key problems encountered in defining a system of
representation for the logical forms of English sen-
tences, and suggests possible approaches to their solu-
tion. We first look at some general issues related to
the notion of logical form, and then discuss a number
of problems associated with the way information in-
volving certain key concepts is expressed in English.
Although our main concern here is with theoretical
issues rather than with system performance, this paper
is not merely speculative. The DIALOGIC system
currently under development in the SRI Artificial In-
telligence Center parses English sentences and trans-
lates them into logical forms embodying many of the
ideas presented here.
</bodyText>
<subsectionHeader confidence="0.782069166666667">
A Case for Rule-Driven Semantic Processing
Martha Palmer
Computer and Information Science
Moore School of Electrical Engineering D2
University of Pennsylvania
Philadelphia, Pennsylvania 19104
</subsectionHeader>
<subsubsectionHeader confidence="0.829752">
Proc. 19th Annual ACL Conf., June 1981, 125-131.
</subsubsectionHeader>
<bodyText confidence="0.999746571428571">
The primary task of semantic processing is to pro-
vide an appropriate mapping between the syntactic
constituents of a parsed sentence and the arguments of
the semantic predicates implied by the verb. This is
known as the Alignment Problem. Section One of this
paper gives an overview of a generally accepted ap-
proach to semantic processing that goes through sever-
al levels of representation to achieve this mapping.
Although somewhat inflexible and cumbersome, the
different levels succeed in preserving the context sen-
sitive information provided by verb semantics. Section
Two presents the author&apos;s rule-driven approach which
is more uniform and flexible yet still accommodates
context sensitive constraints. This approach is based
on general underlying principles for syntactic methods
of introducing semantic arguments and has interesting
implications for linguistic theories about case. These
implications are discussed in Section Three. A system
that implements this approach has been designed for
and tested on pulley problem statements gathered from
several physics text books.
</bodyText>
<table confidence="0.89331775">
American Journal of Computational Linguistics, Volume 7, Number 3, July-September 1981 193
The FINITE STRING Newsletter Abstracts of Current Literature
A Taxonomy for English Nouns and Verbs
Robert A. Amsler
Artificial Intelligence Center
SRI International
333 Ravenswood Avenue
Menlo Park, California 94025
</table>
<subsubsectionHeader confidence="0.613025">
Proc. 19th Annual ACL Conf., June 1981, 133-138.
</subsubsectionHeader>
<bodyText confidence="0.999953705882353">
The definition texts of a machine-readable pocket
dictionary were analyzed to determine the disambigu-
ated word sense of the kernel terms of each word
sense being defined. The resultant sets of word pairs
of defined and defining words were then computation-
ally connected into two taxonomic semi-lattices
(&amp;quot;tangled hierarchies&amp;quot;) representing some 24,000
noun nodes and 11,000 verb nodes. The study of the
nature of the &amp;quot;topmost&amp;quot; nodes in these hierarchies and
the structure of the trees reveal information about the
nature of the dictionary&apos;s organization of the language,
the concept of semantic primitives and other aspects
of lexical semantics. The data proves that the diction-
ary offers a fundamentally consistent description of
word meaning and may provide the basis for future
research and applications in computational linguistic
systems.
</bodyText>
<subsectionHeader confidence="0.928695333333333">
Interpreting Natural Language
Database Updates
S. Jerrold Kaplan and Jim Davidson
Department of Computer Science
Stanford University
Stanford, California 94305
</subsectionHeader>
<subsubsectionHeader confidence="0.848562">
Proc. 19th Annual ACL Conf., June 1981, 139-141.
</subsubsectionHeader>
<bodyText confidence="0.999500384615385">
Although the problem of querying a database in
natural language has been studied extensively, there
has been relatively little work on processing database
updates expressed in natural language. To interpret
update requests, several linguistic issues must be ad-
dressed that do not typically pose difficulties when
dealing exclusively with queries. The primary difficul-
ty with interpreting natural language updates is that
there may be several ways in which a particular update
can be performed in the underlying database. Many
of these options, while literally correct and semantical-
ly meaningful, may correspond to bizarre interpreta-
tions of the request. While human speakers would
intuitively reject these unusual readings, a computer
program may be unable to distinguish them from more
appropriate ones. If carried out, they often have un-
desirable side effects on the database. Our approach
to this problem is to generate a limited set of
&amp;quot;candidate&amp;quot; updates, rank them according to a set of
domain-independent heuristics that reflect general
properties of &amp;quot;reasonable&amp;quot; updates, and either perform
the update or present the highest ranked options to the
user for selection. This paper briefly examines some
of the linguistic problems encountered, and describes
an implemented system that performs simple natural
language database updates.
</bodyText>
<subsectionHeader confidence="0.779997833333333">
Dynamic Strategy Selection in Flexible Parsing
Jaime G. Carbonell and Philip J. Hayes
Department of Computer Science
Carnegie-Mellon University
Schenley Park
Pittsburgh, Pennsylvania 15213
</subsectionHeader>
<subsubsectionHeader confidence="0.831341">
Proc. 19th Annual ACL Conf., June 1981, 143-147.
</subsubsectionHeader>
<bodyText confidence="0.999692947368421">
Robust natural language interpretation requires
strong semantic domain models, &amp;quot;fail-soft&amp;quot; recovery
heuristics, and very flexible control structures. Al-
though single-strategy parsers have met with a meas-
ure of success, a multi-strategy approach is shown to
provide a much higher degree of flexibility, redundan-
cy, and ability to bring task-specific domain knowl-
edge (in addition to general linguistic knowledge) to
bear on both grammatical and ungrammatical input. A
parsing algorithm is presented that integrates several
different parsing strategies, with case-frame instantia-
tion dominating. Each of these parsing strategies ex-
ploits different types of knowledge; and their combi-
nation provides a strong framework in which to proc-
ess conjunctions, fragmentary input, and ungrammati-
cal structures, as well as less exotic, grammatically
correct input. Several specific heuristics for handling
ungrammatical input are presented within this multi-
strategy framework.
</bodyText>
<table confidence="0.483154857142857">
A Construction-Specific Approach to Focused
Interaction in Flexible Parsing
Philip J. Hayes
Department of Computer Science
Carnegie-Mellon University
Schenley Park
Pittsburgh, Pennsylvania 15213
</table>
<subsubsectionHeader confidence="0.785949">
Proc. 19th Annual ACL Conf., June 1981, 149-152.
</subsubsectionHeader>
<bodyText confidence="0.999919444444444">
A flexible parser can deal with input that deviates
from its grammar, in addition to input that conforms
to it. Ideally, such a parser will correct the deviant
input; sometimes, it will be unable to correct it at all;
at other times, correction will be possible, but only to
within a range of ambiguous possibilities. This paper
is concerned with such ambiguous situations, and with
making it as easy as possible for the ambiguity to be
resolved through consultation with the user. We show
the importance of asking the user for clarification in as
focused a way as possible. Focused interaction of this
kind is facilitated by a construction-specific approach
to flexible parsing, with specialized parsing techniques
for each type of construction, and specialized ambigui-
ty representations for each type of ambiguity that a
particular construction can give rise to. A construc-
tion-specific approach also aids in task-specific lan-
guage development by allowing a language definition
</bodyText>
<page confidence="0.568969">
194 American Journal of Computational Linguistics, Volume 7, Number 3, July-September 1981
</page>
<note confidence="0.563173">
The FINITE STRING Newsletter Abstracts of Current Literature
</note>
<bodyText confidence="0.998753">
that is natural in terms of the task domain to be inter-
preted directly without compilation into a uniform
grammar formalism, thus greatly speeding the testing
of changes to the language definition.
</bodyText>
<table confidence="0.9427786">
Controlled Transformational Sentence
Generation
Madeleine Bates
Bolt Beranek and Newman Inc.
50 Moulton Street
Cambridge, Massachusetts, 02138
Robert Ingria
Department of Linguistics
Massachusetts Institute of Technology
Cambridge, Massachusetts 02139
</table>
<subsubsectionHeader confidence="0.75329">
Proc. 19th Annual ACL Conf., June 1981, 153-158.
</subsubsectionHeader>
<bodyText confidence="0.999882136363636">
This paper describes a transformational sentence
generator that was built primarily to focus on syntactic
form and syntactic relationships. Our main goal was
to produce a tutorial system for the English language;
the intended users of the system are people with lan-
guage delaying handicaps such as deafness, and people
learning English as a foreign language. For these pop-
ulations, extensive exposure to standard English con-
structions (negatives, questions, relativization, etc.)
and their interactions is necessary. The purpose of the
generator was to serve as a powerful resource for tuto-
rial programs that need examples of particular con-
structions and/or related sentences to embed in exer-
cises or examples for the student. The focus of the
generator is thus not so much on what to express as on
how to express it in acceptable English. The generator
is composed of three major parts: a base component
that produces base trees, a transformer that applies
transformational rules to the trees to derive a surface
tree, and a set of mechanisms to control the operation
of the first two components. We discuss each of these
components separately.
</bodyText>
<table confidence="0.652840142857143">
Transportable Natural-Language Interfaces
to Databases
Gary G. Hendrix and William H. Lewis
Artificial Intelligence Center
SRI International
333 Ravenswood Avenue
Menlo Park, California 94025
</table>
<subsubsectionHeader confidence="0.745819">
Proc. 19th Annual ACL Conf., June 1981, 159-162.
</subsubsectionHeader>
<bodyText confidence="0.999987225806452">
Over the last few years a number of application
systems have been constructed that allow users to
access databases by posing questions in natural lan-
guages, such as English. When used in the restricted
domains for which they have been especially designed,
these systems have achieved reasonably high levels of
performance. Such systems as LADDER, PLANES,
ROBOT, and REL require the encoding of knowledge
about the domain of application in such constructs as
database schemata, lexicons, pragmatic grammars, and
the like. The creation of these data structures typical-
ly requires considerable effort on the part of a com-
puter professional who has had special training in
computational linguistics and the use of databases.
Thus, the utility of these systems is severely limited by
the high cost involved in developing an interface to
any particular database.
This paper describes initial work on a methodology
for creating natural-language processing capabilities
for new domains without the need for intervention by
specially trained experts. Our approach is to acquire
logical schemata and lexical information through sim-
ple interactive dialogues with someone who is familiar
with the form and content of the database, but unfa-
miliar with the technology of natural-language inter-
faces. To test our approach in an actual computer
environment, we have developed a prototype system
called TED (Transportable English Datamanager). As
a result of our experience with TED, the NL group at
SRI is now undertaking the development of a much
more ambitious system based on the same philosophy.
</bodyText>
<subsectionHeader confidence="0.924014333333333">
Chart Parsing and Rule Schemata in PSG
Henry Thompson
Department of Artificial Intelligence
University of Edinburgh
Hope Park Square
Meadow Lane, Edinburgh EH8 9NW
</subsectionHeader>
<subsubsectionHeader confidence="0.817372">
Proc. 19th Annual ACL Conf., June 1981, 167-172.
</subsubsectionHeader>
<bodyText confidence="0.999474727272727">
MCHART is a flexible, modular chart parsing
framework I have been developing (in LISP) at Edin-
burgh, whose initial design characteristics were largely
determined by pedagogical needs. PSG is a grammati-
cal theory developed by Gerald Gazdar at Sussex, in
collaboration with others in both the U.S. and Britain,
most notably Ivan Sag, Geoff Pullum, and Ewan Klein.
It is a notationally rich context free phrase structure
grammar, incorporating meta-rules and rule schemata
to capture generalizations.
In this paper I describe how I have used MCHART
in beginning to construct a parser for grammars ex-
pressed in PSG, and how aspects of the chart parsing
approach in general and MCHART in particular have
made it easy to accommodate two significant aspects
of PSG: rule schemata involving variables over cate-
gories, and compound category symbols (&amp;quot;slash&amp;quot; cate-
gories). To do this I briefly introduce the basic ideas
of chart parsing, describe the salient aspects of
MCHART, give an overview of PSG, and finally pres-
ent the interesting aspects of the parser I am building
for PSG using MCHART.
</bodyText>
<table confidence="0.9374189">
American Journal of Computational Linguistics, Volume 7, Number 3, July-September 1981 195
The FINITE STRING Newsletter Abstracts of Current Literature
Generating Descriptions and Explanations:
Applications to Questions about Database
Structure
Kathleen R. McKeown
Computer and Information Science
Moore School of Electrical Engineering D2
University of Pennsylvania
Philadelphia, Pennsylvania 19104
</table>
<subsubsectionHeader confidence="0.512824">
Technical Report MS-CIS-80-9, 1980.
</subsubsectionHeader>
<bodyText confidence="0.990949260869566">
The research being proposed is within the area of
natural language generation. The generation process
may be regarded as consisting of two major phases of
computation; the first will determine what is to be said
and how it is to be said, and the second will translate
that message from an internal representation to Eng-
lish. Emphasis will be placed on the problems in-
volved in the first component of generation.
The application for the generation of natural lan-
guage is within a natural language interface to a data-
base system. To date, the kinds of answers which can
be generated by such systems are restricted to lists or
tables of objects in the database. I am proposing gen-
erating responses to questions about the structure of
the database. The kinds of responses that will be gen-
erated include descriptions of classes of objects in the
database, differences between the classes, relations
that hold between classes, information available in the
database, and definitions of classes of objects, among
others.
A Technique for Managing the Lexicon
in a Natural Language Interface
to a Changing Data Base
</bodyText>
<subsectionHeader confidence="0.490499333333333">
S. Jerrold Kaplan, Eric Mays, and Aravind K. Joshi
Computer and Information Science
Moore School of Electrical Engineering D2
University of Pennsylvania
Philadelphia, Pennsylvania 19104
Technical Report MS-CIS-80-10, August 1979.
</subsectionHeader>
<bodyText confidence="0.999672862068966">
A difficulty in designing a Natural Language (NL)
interface to a Data Base (DB) management system is
insuring that DB updates do not obsolete the NL com-
ponents. Of particular concern is the lexicon (the list
of words that the system can process), mainly because
NL queries can contain terms that appear as values in
the DB, and hence are subject to change as the con-
tents of the DB changes. For example, to process the
question &amp;quot;Does John Jones still work for the compa-
ny?&amp;quot;, it is necessary to identify the string &amp;quot;John
Jones&amp;quot; as a (potential) value in the &amp;quot;EMPLOYEE-
NAME&amp;quot; field (assuming a suitable DB). If such
names must appear in the lexicon for the system to
process the query, then the lexicon will go out of date
as the company&apos;s personnel shifts. Using the DB itself
as an extension of the lexicon is equally problematic:
the system will be unable to parse and provide a nega-
tive answer to such a question if the name does not
appear at all in the DB.
The approach suggested here is to infer a plausible
field from the context of the query and semantic infor-
mation about the domain that is implicitly encoded in
the structure of the DB. This technique has been im-
plemented in CO-OP, a NL DB query system that
provides cooperative responses and operates with a
typical CODASYL DB system. CO-OP treats the
problem of selecting a plausible field as a special case
of resolving semantic ambiguities. Examples drawn
from the implementation are presented.
</bodyText>
<table confidence="0.681361666666667">
Centered Logic: The Role of Entity Centered
Sentence Representation in Natural Language
Inferencing.
Aravind K. Joshi and Steve Kuhn
Computer and Information Science
Moore School of Electrical Engineering D2
University of Pennsylvania
Philadelphia, Pennsylvania 19104
Technical Report MS-CIS-80-11, August 1979.
</table>
<bodyText confidence="0.999856142857143">
We will briefly describe the role of entity centered
structure (ECS) of sentences in natural language infer-
encing. The basic structure in discourse generally
singles out an entity, to be called center, among all
those which are the arguments of the main predicate.
ECS makes n-ary predicates look like monadic ones by
temporarily masking their structure, thereby affecting
the relative ease with which certain inferences are
made and information is retrieved. This short paper
deals with a preliminary formulation of a system de-
signed to capture these ideas and contains several ex-
amples of how some natural language inferences can
be represented in the system. Formal properties of the
system are under investigation.
</bodyText>
<subsectionHeader confidence="0.771166">
Paraphrasing Using Given and New Information
in a Question-Answer System
Kathleen R. McKeown
Computer and Information Science
Moore School of Electrical Engineering D2
University of Pennsylvania
</subsectionHeader>
<bodyText confidence="0.602131">
Philadelphia, Pennsylvania 19104
</bodyText>
<subsubsectionHeader confidence="0.701565">
Technical Report MS-CIS-80-13, 1980.
</subsubsectionHeader>
<bodyText confidence="0.99958875">
The design and implementation of a paraphrase
component for a natural language question-answer
system (CO-OP) is presented. A major point made is
the role of given and new information in formulating a
paraphrase that differs in a meaningful way from the
user&apos;s question. A description is also given of the
transformational grammar used by the paraphraser to
generate questions.
</bodyText>
<page confidence="0.842219">
196 American Journal of Computational Linguistics, Volume 7, Number 3, July-September 1981
</page>
<table confidence="0.386838125">
The FINITE STRING Newsletter Abstracts of Current Literature
Natural Language Interaction with Dynamic
Knowledge Bases: Monitoring as Response
E. Mays, S. Lanka, A.K. Joshi, and B.L. Webber
Computer and Information Science
Moore School of Electrical Engineering D2
University of Pennsylvania
Philadelphia, Pennsylvania 19104
</table>
<subsubsectionHeader confidence="0.665748">
Technical Report MS-CIS-80-46, 1980.
</subsubsectionHeader>
<bodyText confidence="0.999853428571429">
In this communication, we discuss an interesting
aspect of natural language interaction with dynamically
changing knowledge bases - the ability to monitor for
relevant future changes in that knowledge. We also
indicate the status of our current work in this area and
the overall goals of our research on question-
answering and monitoring dynamic knowledge bases.
</bodyText>
<subsectionHeader confidence="0.822351666666667">
Control of Inference: Role of Some Aspects
of Discourse Structure-Centering
Aravind K. Joshi and Scott Weinstein
Computer and Information Science
Moore School of Electrical Engineering D2
University of Pennsylvania
</subsectionHeader>
<bodyText confidence="0.655164">
Philadelphia, Pennsylvania 19104
</bodyText>
<subsubsectionHeader confidence="0.583554">
Technical Report MS-CIS-80-47, 1980.
</subsubsectionHeader>
<bodyText confidence="0.9999701">
The purpose of this communication is to examine
one particular aspect of discourse structure, namely, a
discourse construct called center of a sentence
(utterance) in discourse and its relation to the larger
issue of control of inference. We have described very
briefly the notion of center(s) of a sentence in dis-
course and discussed how the centering phenomenon
might be incorporated in a formal model of inference
and its relation to the intrinsic complexity of certain
inferences.
</bodyText>
<subsectionHeader confidence="0.907649666666667">
Varieties of Cooperative Responses
in Question-Answer Systems
Aravind K. Joshi
Computer and Information Science
Moore School of Electrical Engineering D2
University of Pennsylvania
</subsectionHeader>
<bodyText confidence="0.553189">
Philadelphia, Pennsylvania 19104
</bodyText>
<subsubsectionHeader confidence="0.687046">
Technical Report MS-CIS-80-48, 1980.
</subsubsectionHeader>
<bodyText confidence="0.999986833333333">
In this paper, certain types of cooperative responses
desirable in question-answer systems in a data base
environment have been briefly reviewed. In particular,
cooperative responses have been considered dealing
with the situation when the user&apos;s view and the
system&apos;s view of the structure and/or content of the
data base are disparate. Responses explaining the
structure of the data base as well as responses which
are implicit requests for monitoring states of dynamic
data bases have also been discussed. Finally, a general
formulation of a particular type of cooperative behav-
ior has been briefly described.
</bodyText>
<note confidence="0.6653285">
Phrase Structure Trees Bear More Fruit
Than You Would Have Thought
Aravind K. Joshi
Computer and Information Science
</note>
<subsectionHeader confidence="0.8720225">
Moore School of Electrical Engineering D2
University of Pennsylvania
</subsectionHeader>
<bodyText confidence="0.510439">
Philadelphia, Pennsylvania 19104
</bodyText>
<subsubsectionHeader confidence="0.706804">
Technical Report MS-CIS-80-49, 1980.
</subsubsectionHeader>
<bodyText confidence="0.999956285714286">
Several results concerning phrase structure trees
have been presented. These results show that phrase
structure trees when viewed in certain ways have much
more descriptive power than one would have thought.
A brief account of local constraints on structural de-
scriptions and an intuitive proof have been presented.
The local constraints approach has been compared to
some aspects of Gazdar&apos;s framework and that of Pe-
ters and Karttunen. Some results on skeletons (phrase
structure trees without labels) have been presented
also. It can be shown that phrase structure trees even
when deprived of the labels retain in a certain sense
all the structural information. This result has implica-
tions for grammatical inference procedures.
</bodyText>
<subsectionHeader confidence="0.665071333333333">
Parasession on Topics in Interactive Discourse:
Influence of the Problem Text
Aravind K. Joshi
Computer and Information Science
Moore School of Electrical Engineering D2
University of Pennsylvania
</subsectionHeader>
<bodyText confidence="0.731241">
Philadelphia, Pennsylvania 19104
</bodyText>
<subsubsectionHeader confidence="0.705713">
Technical Report MS-CIS-80-50, 1980.
</subsubsectionHeader>
<bodyText confidence="0.999986083333333">
This paper was presented in the parasession on
topics in interactive discourse, which was a special
session organized in conjunction with the 18th Annual
Meeting of the Association for Computational Linguis-
tics (June, 1980, Philadelphia). The paper consists of
comments in response to several issues raised by Bar-
bara Grosz, the panel chairperson. All the comments
pertain to the primary issue of how the purpose of the
interaction of the problem context affects what is said
and how it is interpreted. Wherever possible, the is-
sues are discussed more in the context of &amp;quot;information
seeking&amp;quot; interaction and the data base domain.
</bodyText>
<subsectionHeader confidence="0.8012866">
Mutual Beliefs in Question-Answer Systems
Aravind K. Joshi
Computer and Information Science
Moore School of Electrical Engineering D2
University of Pennsylvania
</subsectionHeader>
<bodyText confidence="0.612001">
Philadelphia, Pennsylvania 19104
</bodyText>
<subsubsectionHeader confidence="0.701125">
Technical Report MS-CIS-80-51, 1980.
</subsubsectionHeader>
<bodyText confidence="0.977938666666667">
In this paper, we will briefly discuss the role of
mutual beliefs with respect to some specific aspects of
man-machine interactions. Cooperative or helpful
behavior can be defined in various ways; however in
this paper we will deliberately limit ourselves to some
American Journal of Computational Linguistics, Volume 7, Number 3, July-September 1981 197
The FINITE STRING Newsletter Abstracts of Current Literature
very particular aspects of cooperation and relate them
to the more technical definitions of mutual beliefs.
We will be particularly concerned with the case where
cooperativeness involves both giving a truthful and
informative response and &amp;quot;squaring away&amp;quot; the relevant
mutual beliefs. In this context, we will suggest that
the &amp;quot;squaring away&amp;quot; contribution of the interaction
can be explained by a suitable modification of one of
the maxims of cooperative conversation.
We also discuss a related topic dealing with excess
(or surplus) information and how and when the system
should assimilate this information, i.e., implicitly up-
date itself without being explicitly told to do so.
These considerations require that mutual beliefs should
not be regarded just as a set of propositions but rather
with some structure over them where the structuring
of information itself is a mutual belief.
</bodyText>
<subsectionHeader confidence="0.938019">
Correcting Misconceptions About
Data Base Structure
Eric Mays
Computer and Information Science
Moore School of Electrical Engineering D2
University of Pennsylvania
</subsectionHeader>
<bodyText confidence="0.981421692307692">
Philadelphia, Pennsylvania 19104
Technical Report MS-CIS-80-52, 1980.
This paper presents a method for computation of
intensional failures of presumptions in queries to a
natural language interface to a data base system.
These failures are distinguished from extensional fail-
ures since they are dependent on the structure rather
than the content of the data base. A knowledge rep-
resentation has been investigated that can be used to
recognize intensional failures. When intensional fail-
ures are detected, a form of corrective behavior is
proposed to inform the user about possibly relevant
data base structure that is related to the failure.
</bodyText>
<subsectionHeader confidence="0.7741955">
BORIS -- An Experiment in In-Depth
Understanding of Narratives
</subsectionHeader>
<note confidence="0.411511">
Wendy Lehnert, Michael G. Dyer, Peter N. Johnson,
C.J. Yang, and Steve Harley
</note>
<subsectionHeader confidence="0.91770925">
Department of Computer Science
Box 2158 Yale Station
Yale University
New Haven, Connecticut 06520
</subsectionHeader>
<subsubsectionHeader confidence="0.621456">
Research Report 188, January 1981, 74 pages.
</subsubsectionHeader>
<bodyText confidence="0.999424">
BORIS is a story understanding and question an-
swering system which involves the specification and
interaction of many sources of knowledge. Unlike
skimmers, which simply extract the &amp;quot;gist&amp;quot; of a story in
a top-down manner and ignore everything else, BORIS
attempts to understand everything that it reads to as
great a depth as possible. This report focuses on how
the BORIS program handles a complex story involving
a divorce.
</bodyText>
<table confidence="0.665822142857143">
Conceptual Information Retrieval
Roger Schank, Janet Kolodner, and Gerald DeJong
Department of Computer Science
Box 2158 Yale Station
Yale University
New Haven, Connecticut 06520
Research Report 190, December 1980, 45 pages.
</table>
<bodyText confidence="0.999880161290322">
If we want to build intelligent information retrieval
systems, we will have to give them the capabilities of
understanding natural language, automatically organiz-
ing and reorganizing their memories, and using intelli-
gent heuristics for searching their memories. These
systems will have to analyze and understand both new
text and natural language queries. In answering ques-
tions, they will have to direct memory search to rea-
sonable places. This requires good organization of
both the conceptual content of texts and knowledge
necessary for understanding those texts and accessing
memory.
The CYRUS and FRUMP systems (Kolodner
(1978), Schank and Kolodner (1979), DeJong (1979))
comprise an information retrieval system called CyFr.
Together, they have the analysis and retrieval capabili-
ties mentioned above. Frump analyzes news stories
from the UPI wire for their conceptual content, and
produces summaries of those stories. It sends sum-
maries of stories about important people to CYRUS.
CYRUS automatically adds those stories to its memo-
ry, and can then retrieve that information to answer
questions posed to it in natural language.
This paper describes the problems involved in
building such an intelligent system. It proposes solu-
tions to some of those problems based on recent re-
search in artificial intelligence and natural language
processing, and describes the CyFr system, which im-
plements those solutions. The solutions we propose
and implement are based on a model of human under-
standing and memory retrieval.
</bodyText>
<subsectionHeader confidence="0.894749857142857">
Finding Objects With Given Spatial Properties
Drew McDermott
Department of Computer Science
Box 2158 Yale Station
Yale University
New Haven, Connecticut 06520
Research Report 195, March 1981, 55 pages.
</subsectionHeader>
<bodyText confidence="0.9998998">
An important class of queries for a spatial-retrieval
system is the retrieval of an object given some desired
properties, such as that it be located in a given region,
or point in a given direction. Handling this kind of
query requires using a discrimination tree, which
breaks the space of possibilities down repeatedly, ulti-
mately into manageable buckets. In a spatial system, a
useful kind of retrieval is &amp;quot;spiral search,&amp;quot; in which the
system searches the tree by starting from the &amp;quot;best&amp;quot;
cell and gradually enlarging its attack. A good way of
</bodyText>
<page confidence="0.749562">
198 American Journal of Computational Linguistics, Volume 7, Number 3, July-September 1981
</page>
<note confidence="0.563803">
The FINITE STRING Newsletter Abstracts of Current Literature
</note>
<bodyText confidence="0.999731333333333">
implementing such a module is to create a domain-
independent system, data-driven by specialized user
functions. In this scheme, the user functions can de-
cide how to index an object, how to search a node of
the tree, and how to reorganize a node when it is not
discriminated properly. Special problems arise when
this program is applied to the domain of simple shaped
objects; a shaped object can fall into more than one
bucket, and a shaped object can have different lengths
in different directions. The user must tell the system
what frames of reference to use for computing object
coordinates.
</bodyText>
<table confidence="0.2476945">
A Temporal Logic for Reasoning About
Processes and Plans
Drew McDermott
Department of Computer Science
Box 2158 Yale Station
Yale University
New Haven, Connecticut 06520
Research Report 196, March 1981, 79 pages.
</table>
<bodyText confidence="0.999489692307692">
Much previous work in artificial intelligence has
neglected representing time in all its complexity. In
particular, it has neglected continuous change and the
indeterminacy of the future. To rectify this, I have
developed a first-order temporal logic, in which it is
possible to name and prove things about facts, events,
plans, and world histories. In particular, the logic
provides analyses of causality, continuous change in
quantities, the persistence of facts (the frame prob-
lem), and the relationship between tasks and actions.
It may be possible to implement a temporal-inference
machine based on this logic, which keeps track of se-
veral &amp;quot;maps&amp;quot; of a time line, one per possible history.
</bodyText>
<subsectionHeader confidence="0.870237571428571">
What&apos;s the Point?
Roger C. Schank, Gregg C. Collins, Ernest Davis,
Peter N. Johnson, Steve Lytinen, and Brian J. Reiser
Department of Computer Science
Box 2158 Yale Station
Yale University
New Haven, Connecticut 06520
</subsectionHeader>
<subsubsectionHeader confidence="0.483895">
Research Report 205, May 1981, 57 pages.
</subsubsectionHeader>
<bodyText confidence="0.999460833333333">
Understanding dialogue usually requires determin-
ing the intent, or point, of the utterance. Finding the
point serves to constrain further processing. We pres-
ent a categorization of points, and we propose algor-
ithms and heuristics for deriving the point of a given
utterance.
</bodyText>
<note confidence="0.515345">
MAGPIE: A Goal-Based Model of Conversation
</note>
<table confidence="0.685356666666667">
Peter N. Johnson and Scott P. Robertson
Department of Computer Science
Box 2158 Yale Station
Yale University
New Haven, Connecticut 06520
Research Report 206, May 1981, 105 pages.
</table>
<bodyText confidence="0.999834782608696">
The importance of intention in conversation has
been considered by many researchers in artificial intel-
ligence and psychology. However, most models of
conversation have been limited to pursuing the transfer
of knowledge between the system and a user. We
propose that conversational goals can address commu-
nication at a number of other levels, such as the
conversants&apos; emotions, their relationship, and their
attitudes. MAGPIE (Multiple Active Goal Processor
in Interactive Exchanges) is a computer model of a
conversant that acquires and pursues conversational
goals at a number of levels, including the goal of seek-
ing dominance in its relationship with the other con-
versant. At the heart of the program is a set of track-
ing procedures, each of which monitors a specific level
of communication flow in a conversation. These pro-
cedures are coupled with a conversational goal planner
which generates responses that simultaneously pursue
a number of goals. Currently, MAGPIE is able to
model a wife during a short marital dispute with her
husband. Normative data from human subjects is
presented which supports the conversational goals
proposed in our analysis.
</bodyText>
<subsectionHeader confidence="0.947443666666667">
Knowledge Organization and Distribution
for Medical Diagnosis
Fernando Gomez and B. Chandraseka ran
Department of Computer and Information Science
The Ohio State University
Columbus, Ohio 43210
</subsectionHeader>
<subsubsectionHeader confidence="0.882747">
IEEE Trans. Sys. Man Cyb. 11, 1 (Jan. 1981), 34-42.
</subsubsectionHeader>
<bodyText confidence="0.999489722222222">
A diagnostician, when he arrives at a diagnosis or
diagnoses, has invoked some concepts. They can be
diseases, causes of them, or other notions that are
relevant to the diagnosis. These concepts form a hier-
archical structure similar to a botanical or zoological
classification. The diagnostician&apos;s knowledge is dis-
tributed through this hierarchy. The concepts in the
hierarchy provide the criteria to organize under them
small pieces of knowledge represented in the form of
production rules. Thus concepts may be viewed as
clusters of production rules. They extend the capabili-
ties of production rules to more complex problem solv-
ing situations. The rules under each concept are fur-
ther organized into three subgroups: exclusionary,
confirmatory, and recommendation rules. During the
problem solving process, the concepts are taken as
specialists. They interact and communicate among
themselves by means of a blackboard.
</bodyText>
<note confidence="0.566357">
American Journal of Computational Linguistics, Volume 7, Number 3, July-September 1981 199
The FINITE STRING Newsletter Abstracts of Current Literature
</note>
<subsectionHeader confidence="0.719230857142857">
Toward a Theory of Distributed Word Expert
Natural Language Parsing
Chuck Rieger and Steve Small
Department of Computer Science
University of Maryland
College Park, Maryland 20742
IEEE Trans. Sys. Man Cyb. 11, 1 (Jan. 1981), 43-51.
</subsectionHeader>
<bodyText confidence="0.999889625">
An approach to natural language meaning-based
parsing in which the unit of linguistic knowledge is the
word rather than the rewrite rule is described. In the
word expert parser, knowledge about language is dis-
tributed across a population of procedural experts,
each representing a word of the language, and each an
expert at diagnosing that word&apos;s intended usage in
context. The parser is structured around a coroutine
control environment in which the generator-like word
experts ask questions and exchange information in
coming to collective agreement on sentence meaning.
The word expert theory is advanced as a better cogni-
tive model of human language expertise than the tradi-
tional rule-based approach. The technical discussion is
organized around examples taken from the prototype
LISP system which implements parts of the theory.
</bodyText>
<subsectionHeader confidence="0.870882833333333">
Integrating Knowledge Sources for
Computer &amp;quot;Understanding&amp;quot; Tasks
Richard E. Cullingford
Department of Electrical Engineering and Computer Science
University of Connecticut
Storrs, Connecticut 06268
</subsectionHeader>
<subsubsectionHeader confidence="0.522168">
IEEE Trans. Sys. Man Cyb. 11, 1 (Jan. 1981), 52-60.
</subsubsectionHeader>
<bodyText confidence="0.999911884615385">
Recent research in artificial intelligence has identi-
fied a number of knowledge sources which appear to
be needed for effective automatic &amp;quot;understanding&amp;quot; of
connected natural language speech and text. These
include word- and phrase-level semantics, models for
actors and objects, and inference techniques for using
script- or goal-oriented knowledge structures. A uni-
fied model of the understanding process can be de-
fined using the distributed-computing viewpoint, pro-
vided some way can be found to integrate and control
a collection of &amp;quot;experts,&amp;quot; each one associated with a
certain kind of knowledge source. A technique is de-
scribed, called hierarchical task management, for con-
structing computer language-processing systems com-
prising an arbitrary number of distinct, potentially
distributed, processes. The technique is based upon
the repeated activation and expansion of data struc-
tures called tasks, which define important components
of the understanding process in terms of a controlled
interaction among the experts. The tasks are main-
tained on several &amp;quot;agendas,&amp;quot; and are manipulated by a
uniform monitor called the task manager. The process
of task management is illustrated in a multiprocess
story understander called a distributable script applier
mechanism (DSAM), which reads and summarizes
newspaper stories about plane crashes.
</bodyText>
<note confidence="0.679892">
A Re-Evaluation of Story Grammars
Alan M. Frisch and Donald Perlis
</note>
<subsectionHeader confidence="0.96416175">
Department of Computer Science
Mathematical Sciences Building
The University of Rochester
Rochester, New York 14627
</subsectionHeader>
<subsubsectionHeader confidence="0.436965">
Cognitive Science 5, 1 (Jan.-March 1981), 79-86.
</subsubsectionHeader>
<bodyText confidence="0.999892105263158">
Black and Wilensky (1979) have made serious
methodological errors in analyzing story grammars,
and in the process they have committed additional
errors in applying formal language theory. Our argu-
ments involve clarifying certain aspects of knowledge
representation crucial to a proper treatment of story
understanding.
Particular criticisms focus on the following short-
comings of their presentation: (1) an erroneous state-
ment from formal language theory, (2) misapplication
of formal language theory to story grammars, (3) un-
substantiated and doubtful analogies with English
grammar, (4) various non sequiturs concerning the
generation of non-stories, (5) a false claim based on
the artificial distinction between syntax and semantics,
and (6) misinterpretation of the role of story gram-
mars in story understanding. We conclude by suggest-
ing appropriate criteria for the evaluation of story
grammars.
</bodyText>
<sectionHeader confidence="0.396252" genericHeader="method">
A Model for Planning in Complex Situations
</sectionHeader>
<subsectionHeader confidence="0.975043571428571">
Robert Wilensky
Electronics Research Laboratory
Computer Science Division
Department of EECS
University of California, Berkeley
Berkeley, California 94720
Memorandum UCB1ERL M81149, June 1981, 34 pages.
</subsectionHeader>
<bodyText confidence="0.968884459459459">
A model of planning applicable to complex, com-
monplace activities is being developed. This model
differs from previous approaches in that it is based on
the following assumptions: (1) a planning agent must
be able to infer its own goals in addition to being able
to generate plans for these goals; (2) everyday plan-
ning is primarily concerned with reasoning about the
interactions between plans and goals; (3) meta-
planning (formulating knowledge about how to plan
abstract plans and goals, and having the planner use
this knowledge to solve its own planning problems) is
used as a driving principle; (4) projection (simulating
hypothetical futures based on current plans and world
knowledge) is used to infer goals and debug plans; and
(5) planning knowledge should be equally available for
understanding as well as for planning.
Coupled together, the mechanisms for implement-
ing these features give rise to a system of considerable
200 American Journal of Computational Linguistics, Volume 7, Number 3, July-September 1981
The FINITE STRING Newsletter Abstracts of Current Literature
power. For example, the ability to infer one&apos;s own
goals is needed in an autonomous planning agent since
it must deal with unexpected situations. However,
together with meta-planning knowledge and the ability
to project hypothetical futures, this feature enables the
planner to detect and reason about complicated goal
interactions, anticipate problems with proposed plans,
and make choices in the face of competing alterna-
tives.
This model has been developed in detail for the
detection and resolution of goal conflicts. In doing so,
we postulate several goal conflict resolution strategies,
or meta-plans, called RE-PLAN, CHANGE-
CIRCUMSTANCE, and SIMULATE-AND-SELECT.
The structure and application of these meta-plans is
explored in the context of both decision making and
understanding the actions of other planners.
</bodyText>
<table confidence="0.480580785714286">
It&apos;s for Your Own Good:
A Note on Inaccurate Reference
C. Raymond Perrault
Department of Computer Science
University of Toronto
Toronto, Ontario CANADA M5S 1A7
Philip R. Cohen
Department of Computer Science
Oregon State University
Corvallis, Oregon 97331
BBN Report 4723, 1981.
(Also appears in Elements of Discourse Understanding,
Joshi, Webber, and Sag (Eds.), Cambridge Univ. Press,
1981.)
</table>
<bodyText confidence="0.999961166666667">
In his book Speech Acts, Searle suggests that his
analysis of illocutionary acts can be extended to ac-
count for the &amp;quot;propositional act&amp;quot; of reference. He
wants to give necessary conditions for a speaker suc-
cessfully referring to some entity by using a certain
referring expression in an utterance to a certain hear-
er. We point out here some inadequacies in Searle&apos;s
conditions, in particular how they fail to account for
cases of successful reference through expressions
which speaker (and hearer) may believe are not true
of their intended referent. More specific conditions
based on the notion of mutual belief are proposed.
</bodyText>
<subsectionHeader confidence="0.890035666666667">
Stories within Stories
Bertram Bruce
Bolt Beranek and Newman Inc.
50 Moulton Street
Cambridge, Massachusetts 02138
BBN Report 29, August 1981, 17 pages.
</subsectionHeader>
<bodyText confidence="0.999597571428571">
What appears to be a single story is often a complex
set of stories within stories, each with its distinct au-
thor and reader. Examples of such stories within sto-
ries are presented. Results of analyses of basal read-
ers and trade books in terms of embedded stories are
also discussed. These suggest that a greater variety of
stories could and should be made available to children.
</bodyText>
<subsectionHeader confidence="0.9702105">
Higher-Level Features in Children&apos;s Stories:
Rhetorical Structure and Conflict
Cindy Steinberg and Bertram Bruce
Bolt Beranek and Newman Inc.
50 Moulton Street
Cambridge, Massachusetts 02138
</subsectionHeader>
<bodyText confidence="0.966853333333333">
BBN Report 18, October 1980, 25 pages.
Traditional surveys of children&apos;s literature have
examined features such as text structure and topic, but
have failed to take into account rhetorical elements
such as author-reader distance, commentary, point of
view, and insight into characters&apos; minds. Similarly,
they have glossed over aspects of character-to-
character interaction such as responses to interperson-
al conflict. These &amp;quot;higher-level features&amp;quot; of stories
may be what makes stories interesting to read. They
are also principal contributors to story complexity, and
hence, to difficulty for beginning readers. With regard
to both interestingness and complexity, it is important
to come to a better understanding of these features.
To concretize our discussion, we first present two
examples showing the importance of higher-level text
features. Second, we sketch a theory of higher-level
story features. Then, we briefly describe how we are
applying our analysis to a selection of children&apos;s sto-
ries. Finally, we discuss some implications of this
work.
</bodyText>
<subsectionHeader confidence="0.937358142857143">
Strategies for Controlling Hypothesis
Formation in Reading
Bertram Bruce and Andee Rubin
Bolt Beranek and Newman Inc.
50 Moulton Street
Cambridge, Massachusetts 02138
BBN Report 22, June 1981, 40 pages.
</subsectionHeader>
<bodyText confidence="0.999883846153846">
Reading is a process of forming and evaluating hy-
potheses to account for the data in a text. Because of
its complexity, the task of reading requires strategies
for controlling the proliferation of hypotheses. Four
of these strategies, (a) jumping to conclusions, (b)
maintaining inertia, (c) relying on background knowl-
edge, and (d) working backwards from the goal, are
generally effective, but they occasionally create read-
ing problems, rather than alleviating them. Examples
from protocols of readers reading a reading test pas-
sage are presented. These examples show both the
effective use of the strategies and some problems that
may arise from their use.
</bodyText>
<table confidence="0.350313125">
American Journal of Computational Linguistics, Volume 7, Number 3, July-September 1981 201
The FINITE STRING Newsletter Abstracts of Current Literature
A New Point of View on Children&apos;s Stories
Bertram Bruce
Bolt Beranek and Newman Inc.
50 Moulton Street
Cambridge, Massachusetts 02138
BBN Report 25, July 1981, 47 pages.
</table>
<bodyText confidence="0.999114428571429">
Recent work on text analysis at the Center for the
Study of Reading and elsewhere has produced surpris-
ing results regarding the texts that children read in
school. These results support the hypothesis that part
of the difficulty children encounter in making the
transition from beginning to skilled readings lies in an
abrupt shift in text characteristics between lower and
upper elementary school. Moreover, a comparison
between school texts and popular trade books shows
that the school texts may provide inadequate prepara-
tion for the texts that skilled readers need to master.
Thus, characteristics of the texts that children are ex-
pected to read may hinder rather than help in the at-
tainment of educational goals.
</bodyText>
<subsectionHeader confidence="0.9480035">
Processing Models for Children&apos;s
Story Comprehension
Robert de Beaugrande and Genevieve W. Miller
English Department
University of Florida
Gainesville, Florida 32611
</subsectionHeader>
<bodyText confidence="0.9718679375">
Poetics 9 (1980), 181-201.
It is argued that in abandoning trace-abstraction
models of story comprehension in favor of schema-
based ones, we have left some issues unresolved in
regard to the treatment of bottom-up input. Using a
real children&apos;s story employed in empiric tests, we
review evidence that schemas undergo specification
and modification via bottom-up input as the story is
progressively read: a phenomenon describable as pro-
cedural attachment. There seems to be some trace-
abstraction during comprehension but in a peripheral
role and with unreliable or inconsistent results across a
test population. We conclude that accuracy of recall is
a less crucial question than the identification of strate-
gies active during recall of both abstractive and con-
structive nature.
</bodyText>
<subsectionHeader confidence="0.94621">
The Pragmatics of Discourse Planning
Robert-Alain de Beaugrande
English Department
University of Florida
Gainesville, Florida 32611
</subsectionHeader>
<subsubsectionHeader confidence="0.760408">
Journal of Pragmatics 4 (1980), 15-42.
</subsubsectionHeader>
<bodyText confidence="0.99995936">
It is argued that the basic notions of natural lan-
guage pragmatics cannot be the same as those of syn-
tax and semantics as developed so far. Instead, prag-
matics must be an empirically oriented theory of ac-
tion and interaction. The role of sentences and predi-
cations is secondary.
The most promising approaches for such a pragmat-
ics are: (1) conceptual dependency theory, in which
language is a form of actions specified by goal-
directed plans (e.g. Schank); (2) plan theory, in
which the analysis of tasks and resources leads to the
specification of a planned sequence of steps (e.g. Sa-
cerdoti); (3) problem-solving theory, in which points
or states in a problem space have to be connected by a
successful pathway (e.g. Newell and Simon); and (4)
procedural theory of discourse, in which language ele-
ments and systems are investigated with respect to
how people utilize them in communication and proc-
essing.
The paper offers the framework of a natural lan-
guage pragmatics along these lines and applies the
resulting theory to a study of a scene from a stage
play by Sidney Howard. It is shown that the actions
and discourse actions of the scene are indeed generat-
ed by the characters&apos; plans and goals.
</bodyText>
<subsectionHeader confidence="0.914125333333333">
Linguistic Theory and Metatheory
for a Science of Texts
Robert De Beaugrande
English Department
University of Florida
Gainesville, Florida 32611
</subsectionHeader>
<bodyText confidence="0.9759265">
Text 1, 2 (1981), 113-161.
This article explores the typical reactions which
occur when an established science confronts a new
object of inquiry, as we find when linguistic theory
encounters the text. The usual discussions are not
productive as long as the old &amp;quot;paradigm&amp;quot; is still ac-
cepted as the framework for achievement. The issues
are therefore re-examined in terms of the metatheory
of science (e.g. Sneed, Stegmtiller, Lakatos, Feyera-
bend, Hempel), and some general solutions are ex-
pounded for the problems of validating theories on the
basis of empirical content. A paradigmatic example is
then presented in order to show a possible role for
logical linguistics in future theories: a computer gram-
mar that parses text sentences into a progressive net-
work and back again via theorem-proving, with further
capacities for applying schemas, answering questions,
and generating summaries. This example serves as an
application of general design values and criteria for
preferring and comparing alternative theories.
</bodyText>
<subsectionHeader confidence="0.725476">
Theoretical Foundations of the Automatic
Production and Processing of Technical Reports
Robert de Beaugrande
English Department
University of Florida
</subsectionHeader>
<bodyText confidence="0.386949">
Gainesville, Florida 32611
</bodyText>
<note confidence="0.634644333333333">
J. Technical Writing and Communication 9, 3 (1979).
202 American Journal of Computational Linguistics, Volume 7, Number 3, July-September 1981
The FINITE STRING Newsletter Abstracts of Current Literature
</note>
<bodyText confidence="0.999925636363637">
The following treatise surveys the issues and ap-
proaches for designing a computer system capable of
reading, understanding, and writing technical reports.
Recent progress in computer science and artificial
intelligence research is used to specify the nature of
the modules in the system. The processing of a sam-
ple text is observed during the phases of reading and
writing a report on the origin of sunspots. The author
advances some proposals for correlating syntax and
semantics of English from a procedural standpoint.
The discussion is illustrated with structural diagrams.
</bodyText>
<subsectionHeader confidence="0.919843571428572">
Checking for Spelling and Typographical Errors
in Computer-Based Text
Thomas N. Turba
Sperry Univac
Language Systems
Roseville Development Center
Roseville, Minnesota 55113
</subsectionHeader>
<bodyText confidence="0.9855868">
Sigplan Notices 16, 6 (June 1981), 51-60.
This paper addresses the problems and techniques
of checking for spelling and typographical errors in
computer-based text. To some extent, the paper is a
combination of a report of work done by the author
and a survey of other work which, although not all
used by the author, is of equal value and interest.
Some of the material presented is related to other as-
pects of text processing such as data compaction and
the efficient searching of very large dictionaries.
</bodyText>
<sectionHeader confidence="0.698209" genericHeader="method">
Computer Aids for Writers
</sectionHeader>
<subsectionHeader confidence="0.98114125">
Lorinde Cherry
Bell Telephone Laboratories
600 Mountain Avenue
Murray Hill, New Jersey 07974
</subsectionHeader>
<bodyText confidence="0.9896595">
Sigplan Notices 16, 6 (June 1981), 61-67.
For many people, writing is painful and editing
one&apos;s own prose is difficult, tedious, and error-prone.
It is often hard to see which parts of a document are
difficult to read or how to transform a wordy sentence
into a more concise one. It is even harder to discover
that one overuses a particular linguistic construct. The
system of programs described here helps writers to
evaluate documents and to produce better written and
more readable prose. The system consists of programs
to measure surface features of text that are important
to good writing style as well as programs to do some
of the tedious jobs of a copy editor. Some of the sur-
face features measured are readability, sentence and
word length, sentence type, word usage, and sentence
openers. The copy editing programs find spelling er-
rors, wordy phrases, bad diction, some punctuation
errors, double words, and split infinitives.
</bodyText>
<note confidence="0.662706666666667">
Data-Base and Query Systems:
New and Simple Ways to Gain Multiple Views
of the Patterns in Text
Linda D. Misek-Falkoff
IBM Thomas J. Watson Research Center
P.O. Box 218
</note>
<author confidence="0.413181">
Yorktown Heights, New York 10598
</author>
<subsubsectionHeader confidence="0.76776">
Research Report RC 8769, March 1981, 52 pages.
</subsubsectionHeader>
<bodyText confidence="0.999985944444444">
The goal of this paper is to suggest a spontaneous,
&amp;quot;problem solving&amp;quot; approach to textual analysis, in a
user-friendly milieu made possible by modern data-
base facilities. The study of language is ubiquitous,
with interactive systems increasingly supporting studies
which previously might have been run in batch mode:
the language scholar today is less reliant on mediating
technical support. He can enter and query his data
directly. He can, but need not, be a &amp;quot;programmer,&amp;quot;
and can personally control the degree of complexity
entailed by his studies, according to his own felt needs
and goals. This freedom of inquiry should greatly aid
interpretation and inference, since the scholar can
more easily alter his methods and critical hypotheses.
Interesting connections between language studies at
large and the practice of data-base administration may
be gleaned, and the growing concord between human-
ism and technology further enhanced.
</bodyText>
<subsectionHeader confidence="0.596649666666667">
A Knowledge Engineering Approach to
Natural Language Understanding
Jeannette G. Neal
Department of Computer Science
State University of New York at Buffalo
4226 Ridge Lea Road
</subsectionHeader>
<bodyText confidence="0.385834">
Amherst, New York 14226
</bodyText>
<subsectionHeader confidence="0.760757">
Technical Report 179, June 1981.
</subsectionHeader>
<bodyText confidence="0.973995785714286">
This paper presents the results of preliminary study
of a knowledge engineering approach to natural lan-
guage understanding. The KE system used is the
SNePS semantic network processing system (Shapiro,
1979a). As a part of the study, a SNePS front-end
system, called the NL-system, was implemented to
enable the NLU expert to enter linguistic knowledge
into the network in natural language and experiment
with this base of knowledge.
This paper discusses the capabilities of the SNePS-
based NL-system and illustrates these capabilities by
example. The NL-system enables the NLU user-
expert to: (a) input his rules about NLU and his lexi-
con into the semantic network knowledge base in nat-
ural language; (b) trace the inference processes, which
utilize his rules, in natural language; and (c) employ
an experimental rule-based generator to express
knowledge built into the network via his NLU rules.
American Journal of Computational Linguistics, Volume 7, Number 3, July-September 1981 203
The FINITE STRING Newsletter Abstracts of Current Literature
An example is presented to illustrate: the repre-
sentation of both surface strings and semantic knowl-
edge in the network; the input and representation of
rules; how rules build conceptual structures from sur-
face strings; natural language tracing of an NLU proc-
ess; the use of knowledge for disambiguating an earli-
er, partially understood sentence; and rule-based gen-
eration.
</bodyText>
<subsectionHeader confidence="0.729939666666667">
Combining Path-Based and Node-Based
Inference In SNEPS
Rohini K. Srihari
Department of Computer Science
State University of New York at Buffalo
4226 Ridge Lea Road
</subsectionHeader>
<bodyText confidence="0.686688">
Amherst, New York 14226
</bodyText>
<subsubsectionHeader confidence="0.462979">
Technical Report 183, June 1981, 52 pages.
</subsubsectionHeader>
<bodyText confidence="0.999995">
This paper describes a recent enhancement made to
SNePS to include path-based inference. The previous
version of SNePS used only node-based inference and
was thus limited in its capabilities. Combining path-
based inference and node-based inference in SNePS
has greatly increased the scope of the system. The
paper first motivates the need for such a change and
then proceeds to outline the theory as well as the im-
plementation details. Finally, a series of examples is
provided which supports the claim that the system is
indeed more powerful and efficient now.
</bodyText>
<subsectionHeader confidence="0.932901666666667">
Formal Semantics for Time in Databases
James Clifford and David S. Warren
Department of Computer Science
State University of New York at Stony Brook
Stony Brook, New York 11794
Technical Report 811025, June 1981.
</subsectionHeader>
<bodyText confidence="0.99998775">
The concept of an historical database is introduced
as a tool for modelling the dynamic nature of some
part of the real world. Just as first-order logic has
been shown to be a useful formalism for understanding
the underlying semantics of the relational database
model, intensional logic is presented as an analogous
formalism for understanding the temporal semantics
involved in an historical database. The various com-
ponents of the relational model, as extended to include
historical relations, are discussed in terms of the model
theory for the logic IL formulated by Richard Mon-
tague. The concepts of intensional and extensional
data constraints and queries are introduced and con-
trasted. Finally, the potential application of these
ideas to the problem of natural language database
querying is discussed.
</bodyText>
<subsectionHeader confidence="0.849636857142857">
More Notes on an Intensional Logic for English:
Reversing Extensional Forms
Keith Brian Gallagher
Department of Computer and Communication Sciences
221 Angell Hall
The University of Michigan
Ann Arbor, Michigan 48109
</subsectionHeader>
<subsubsectionHeader confidence="0.795752">
Computer Studies in Formal Ling. N-25, May 1981.
</subsubsectionHeader>
<bodyText confidence="0.993337444444444">
In a series of papers, Joyce Friedman and David
Warren gave sufficient conditions for obtaining the
extensional forms of words in Richard Montague&apos;s The
Proper Treatment of Quantification in Ordinary English
(PTQ). Given these extensionalized forms, one desires
to return to the logically equivalent lambda normal
form, as a prelude to obtaining the sentence that yield-
ed this form. A procedure for doing this with a proof
of its correctness is given.
</bodyText>
<subsectionHeader confidence="0.8792078">
Sentence Analysis Programs Based on
Montague Grammar
Bipin Indurkhya
Philips International Institute of Technological Studies
Eindhoven NETHERLANDS
</subsectionHeader>
<subsubsectionHeader confidence="0.693127">
Masters Thesis, 1981.
</subsubsectionHeader>
<bodyText confidence="0.999904222222222">
The paper investigates computational aspects of the
work in theoretical linguistics which is being done in
the framework of &amp;quot;Montague Grammar.&amp;quot; Grammars in
this framework give precise descriptions of the relation
between the surface forms of English sentences, their
syntactic structures, and their meanings as represented
by logical formulas. On the basis of such grammars
effective analysis programs for English sentences may
be constructed.
</bodyText>
<subsectionHeader confidence="0.612057333333333">
Focalizers, the Scoping Problem, and Semantic
Interpretation Rules in Logic Grammars
Michael C. McCord
Computer Science Department
University of Kentucky
Lexington, Kentucky 40506
</subsectionHeader>
<bodyText confidence="0.748129090909091">
Technical Report 81-81, August 1981.
(To appear in Proceedings of the International
Workshop on Logic Programming for Intelligent
Systems, August 1981, Logicon, Woodland Hills, Calif.)
This paper deals with a system for semantic inter-
pretation of natural language within the framework of
logic programming. Of special interest are a class of
grammatical items called focalizers, and the problem of
determining their scopes in logical form. Focalizers
include quantificational determiners, certain adverbs,
and abstract items relating to discourse structure.
</bodyText>
<page confidence="0.745358">
204 American Journal of Computational Linguistics, Volume 7, Number 3, July-September 1981
</page>
<table confidence="0.925240714285714">
The FINITE STRING Newsletter Abstracts of Current Literature
An Efficient Easily Adaptable System For
Interpreting Natural Language Queries
David H.D. Warren
Department of Artificial Intelligence
University of Edinburgh
Forrest Hill
Edinburgh EH1 2QL SCOTLAND
Fernando C.N. Pereira
CAAD Studies, Department of Architecture
University of Edinburgh
Forrest Hill
Edinburgh EH1 2QL SCOTLAND
DAI Research Paper 155, February 1981, 18 pages.
</table>
<bodyText confidence="0.9999224375">
This paper gives an overall account of a prototype
natural language question answering system, call Chat-
80. Chat-80 has been designed to be both efficient
and easily adaptable to a variety of applications. The
system is implemented entirely in PROLOG, a pro-
gramming language based on logic. With the aid of a
logic-based grammar formalism called extraposition
grammar, Chat-80 translates English questions into the
PROLOG subset of logic. The resulting logical ex-
pression is then transformed by a planning algorithm
into efficient PROLOG, cf. &amp;quot;query optimization&amp;quot; in a
relational database. Finally the PROLOG form is
executed to yield the answer. On a domain of world
geography, most questions within the English subset
are answered in well under one second, including rela-
tively complex queries.
</bodyText>
<subsectionHeader confidence="0.715293">
Some Problems In Early Noun Phrase
Interpretation
C.S. Mellish
Department of Artificial Intelligence
University of Edinburgh
Forrest Hill
Edinburgh EH1 2QL SCOTLAND
DAI Research Paper No. 147, 1980, 5 pages.
</subsectionHeader>
<bodyText confidence="0.999833846153846">
How does a piece of text provide the information
necessary for generating a symbolic &amp;quot;meaning&amp;quot; and
how can a computer program be organized to pick up
that information? The work described here aims to
investigate some of the constraints on the timing of
semantic interpretation. In particular, we are interest-
ed in seeing to what extent the meaning can be built
up in an incremental way as the analysis proceeds
from left to right. We look at some problems of noun
phrase interpretation in such a scheme and indicate
some representational ideas that help to overcome
them. This paper is a brief summary of a forthcoming
Ph.D thesis (Mellish 80).
</bodyText>
<subsectionHeader confidence="0.9279735">
Semantic Long Term Memory and the
Understanding of Language
</subsectionHeader>
<figure confidence="0.590252666666667">
M. Wettler
ISSCO
Universite de Geneve
17 Rue de CandoIle
CH-1205 Geneve SWITZERLAND
Working Paper 37, 1978 (in German).
</figure>
<bodyText confidence="0.986474142857143">
This is a study of the formal representation of con-
ceptual knowledge in relation to the understanding and
production of utterances. After the initial historical
introduction, the final sections deal with the structure
of schemata; inference and consistency of schemata;
topic shift in dialogues; and the generation of descrip-
tions of surface structure.
</bodyText>
<figure confidence="0.62163025">
Six Lectures from Recursive Function Theory
to Artificial Intelligence
G. Trautteur
ISSCO
Universite de Geneve
17 Rue de CandoIle
CH-1205 Geneve SWITZERLAND
Working Paper 38, 1979.
</figure>
<bodyText confidence="0.999391733333333">
This paper is based on a series of lectures given at
ISSCO by Trautteur 1977/78. There are 3 sections.
The first introduces the theory of effective procedures,
leading up to a statement of Church&apos;s Thesis. The
second investigates the relationship between language
and metalanguage, bringing in the notions of a deci-
sion problem and incompleteness. The last section
discusses various attempts to formalize the idea of
complexity, and this is followed by the presentation of
inductive inference, language identification, and dia-
logue systems from the point of view of recursive
function theory. Finally, a discussion of the role of
analog machinery in the theory of effective procedures
is used to define a position for Al on various philo-
sophical problems.
</bodyText>
<subsectionHeader confidence="0.930469">
Some Consideration in Mapping Natural
Language Queries on to Database Systems
</subsectionHeader>
<sectionHeader confidence="0.564708" genericHeader="method">
L. Mazlack
ISSCO
Universite de Geneve
17 Rue de CandoIle
CH-1205 Geneve SWITZERLAND
</sectionHeader>
<subsectionHeader confidence="0.325582">
Working Paper 39, 1979.
</subsectionHeader>
<bodyText confidence="0.8721763">
The aim of this paper is to outline the design of an
NL interface for existing database systems. Several
examples are discussed where the same database query
is expressed by different NL queries. A two-stage
design is proposed which incorporates the notion of an
intermediate database-independent query language
which is then mapped into queries for the particular
database under consideration.
American Journal of Computational Linguistics, Volume 7, Number 3, July-September 1981 205
The FINITE STRING Newsletter Abstracts of Current Literature
</bodyText>
<reference confidence="0.860399714285714">
Case in Linguistics and Cognitive Science
M. Rosner and H. Somers
ISSCO
Universite de Geneve
17 Rue de CandoIle
CH-1205 Geneve SWITZERLAND
Working Paper 40, 1980.
</reference>
<bodyText confidence="0.9995159">
The meaning of &amp;quot;case&amp;quot; as applied to sentences,
verbs, events, and event types is quite different. Ex-
amples of the use of case in each of these four cate-
gories are exemplified and contrasted in detail. An
attempt is made to show that comparisons of systems
that use case should be avoided when in reality they
only have trivial surface characteristics in common. It
is concluded that there is no one Case Grammar: in
reality the use of case is always relativized to a specif-
ic application.
</bodyText>
<figure confidence="0.643527125">
ISSCO*PTOSYS: Brief Description
and User Manual
H. Somers
ISSCO
Universite de Geneve
17 Rue de CandoIle
CH-1205 Geneve SWITZERLAND
Working Paper 41, 1980.
</figure>
<bodyText confidence="0.999613333333333">
This paper is a description of PTOSYS (ptotic sys-
tem), a computer program which builds case frame
representations of input English sentences. A central
feature of the system is the incremental assembly of a
dictionary structure which is built up through an inter-
active discourse with the user. The system invokes
this process whenever it discovers a word it does not
recognize, using its linguistic knowledge to guess the
most pertinent questions.
</bodyText>
<subsectionHeader confidence="0.9354255">
The Use of Verb Features in Arriving at a
&amp;quot;Meaning Representation&amp;quot;
</subsectionHeader>
<bodyText confidence="0.351148">
H. Somers
</bodyText>
<sectionHeader confidence="0.639465" genericHeader="method">
ISSCO
</sectionHeader>
<reference confidence="0.47561975">
Universite de Geneve
17 Rue de CandoIle
CH-1205 Geneve SWITZERLAND
Working Paper 42, 1981.
</reference>
<bodyText confidence="0.9757536">
A critical examination of the linguistic theory un-
derlying the notion that in trying to map English sen-
tences on to a case representation of deep structure, it
is possible to use semantic features attached to the
main verb to infer the correct type of case frame.
</bodyText>
<reference confidence="0.611451444444444">
A Preliminary Study on Linguistic Implications
of Resource Control in Natural Language
Understanding
J.S. Bien
ISSCO
Universite de Geneve
17 Rue de CandoIle
CH-1205 Geneve SWITZERLAND
Working Paper 44, 1980.
</reference>
<bodyText confidence="0.999793153846154">
The paper presents some hypotheses concerning the
organization of language processing by human and
computer, which allow us to view a variety of appar-
ently unrelated linguistic phenomena in terms of the
sophisticated interactions between a few basic compo-
nents. In particular, the fact that English articles are
rendered in Slavonic languages mainly by word order
and vice versa, which has up till now remained com-
pletely mysterious, is seen assuming different ways of
controlling the depth of nominal phrase processing.
Although not yet sustained satisfactorily, the hypothe-
sis offers a general, intuitively appealing approach to
natural language research.
</bodyText>
<sectionHeader confidence="0.788635" genericHeader="method">
Erfahrungen Mit Zwei Natiirlich-Sprachlichen
Abfragesystemen
</sectionHeader>
<reference confidence="0.863012625">
Wilfrid Kettler and Arno Schmidt
Technical University of Berlin
Berlin, WEST GERMANY
Magdalena Zoeppritz
IBM Scientific Center
Tiergartenstrasse 15
0-6900 Heidelberg, WEST GERMANY
Technical Report 81.01.001, January 1981, 18 pages.
</reference>
<bodyText confidence="0.999884454545455">
This paper reports on experiences and results of a
joint study between the Technical University of Berlin
and the Heidelberg Scientific Center. The purpose of
the study was to implement the application &amp;quot;Rooms
and Office Space Allocation&amp;quot; of the University Cen-
tral Administration with the User Specialty Languages
System (USL) developed at the Heidelberg Scientific
Center and with the Berlin Semantic Oriented Transla-
tion System (BEAST) developed at the Technical Uni-
versity of Berlin to provide an on-line natural language
facility for the administration, to evaluate the USL-
System (not the BEAST-System because the DBMS to
which it interfaces was not complete) in an actual
work environment, and to study the effects of the
different approaches to natural language system design
taken in the two systems. The paper outlines the ap-
plication and describes the BEAST and USL systems.
Then follows an account of the experience gained
from implementing the application with each of the
two systems and results and observations during the
implementation and exploration phases with the USL-
System.
</bodyText>
<page confidence="0.789923">
206 American Journal of Computational Linguistics, Volume 7, Number 3, July-September 1981
</page>
<note confidence="0.767029">
The FINITE STRING Newsletter Abstracts of Current Literature
</note>
<reference confidence="0.986813571428572">
Proceedings of the Workshop on Data
Abstraction, Databases and Conceptual
Modelling
Michael L. Brodie
Computer Science Department
University of Maryland
College Park, Maryland 20742
Stephen N. Zilles
IBM Research Laboratory
5600 Cottle Road
San Jose, California 95193
SIGART Newsletter 74 (Jan. 1981).
SIGMOD Record 11, 2 (Feb. 1981).
SIGPLAN Notices 16, 1 (Jan. 1981).
</reference>
<bodyText confidence="0.999374434782609">
A workshop on data abstraction, databases and
conceptual modelling was held June 23-26, 1980, in
Pingree Park in the Colorado Rockies. The meeting
was intended as a forum in which artificial intelligence,
database and programming language researchers could
exchange ideas on conceptual modelling of systems of
complex data. This proceedings consists of tutorials,
edited transcripts of the workshop sessions and posi-
tion papers prepared by the participants. Tutorial
papers on artificial intelligence, databases and pro-
gramming languages present current research prob-
lems, results, goals and terminology. Summaries of the
tutorial presentations including questions and answers
are also included as they contain additional material
and illustrate some of the confusion which arises in
discussions among the three research communities.
The subject of the workshop is treated in seven topical
sessions and a concluding summary session. The tran-
scriptions of these sessions follow the general order of
the presentations made at the workshop. The position
papers are based on papers submitted for selecting
workshop participants, which have been revised to
reflect the participants&apos; experience at the workshop.
</bodyText>
<sectionHeader confidence="0.651073833333333" genericHeader="method">
Computational Strategies for Analyzing the
Organization and Use of Information
Donald E. Walker
Artificial Intelligence Center
SRI International
333 Ravenswood Ave.
</sectionHeader>
<subsectionHeader confidence="0.57932">
Menlo Park, California 94025
</subsectionHeader>
<bodyText confidence="0.970854769230769">
Technical Note 253, July 1981.
This chapter describes new developments in
computer-based procedures that can improve our un-
derstanding of how people organize and use informa-
tion. Relevant recent research in information science,
computational linguistics, and artificial intelligence is
reviewed. A program of research is presented that is
producing systems that make it possible to study the
organization and use of information and, at the same
time, provide more effective support for people en-
gaged in those activities. Finally, several current pro-
jects that are part of this longer-term program are
discussed.
</bodyText>
<reference confidence="0.950196875">
The Command Language Grammar:
A Representation for the User Interface
of Interactive Computer Systems
Thomas P. Moran
Xerox Palo Alto Research Center
3333 Coyote Hill Road
Palo Alto, California 94304
Int. J. of Man-Machine Studies 15, 1 (July 1981).
</reference>
<bodyText confidence="0.999835363636363">
This article introduces and discusses a specific
grammatical structure — the Command Language
Grammar (CLG) — as a representational framework
for describing the user interface aspects of interactive
computer systems. CLG partitions a system into a
Conceptual Component (tasks and abstract concepts),
a Communication Component (command language),
and a Physical Component (display, keyboard, etc.).
The components are further stratified into distinct
Levels — a Task Level, a Semantic Level, a Syntactic
Level, and an Interaction Level — each Level being a
complete description of the system at its level of ab-
straction. Each Level&apos;s description contains proce-
dures for accomplishing the tasks addressed by the
system in terms of the actions available at that Level.
That is, the system is described by progressive refine-
ment. An extensive example, a small message-
processing system, is described at all Levels in the
CLG notation.
CLG is discussed from three points of view: The
Linguistic View sees CLG as elaborating the structure
of the system&apos;s user interface and of the communica-
tion between the user and the system. The principal
goal of CLG in this view is to lay out the space of
command language systems. The Psychological View
sees CLG as describing the user&apos;s mental model of the
system. The main concern in this view is with the psy-
chological validity of the CLG description. The De-
sign View sees CLG as a series of representations for
specifying the design of a system. CLG proposes a
topdown design process in which the conceptual model
of the system is first specified and then a command
language is created to communicate with it.
</bodyText>
<reference confidence="0.804595875">
A Experiment in English-Spanish Automated
Translation of Medical Language Data
Isabel Garcia-Hidalgo and George S. Dunham
Laboratory of Statistical and Mathematical Methodology
Building 12A, Room 3041
National Institutes of Health
Bethesda, Maryland 20205
Meth. of Inform. in Med. 20, 1 (Jan. 1981), 38-46.
</reference>
<bodyText confidence="0.989816047619048">
An English-to-Spanish translation procedure and its
associated dictionaries were developed and implement-
ed for the 1,426 terms of the morphology section of
American Journal of Computational Linguistics, Volume 7, Number 3, July-September 1981 207
The FINITE STRING Newsletter Abstracts of Current Literature
the International Classification of Diseases for Oncol-
ogy. Morphological substitutions and respelling rules
permit translation of most of the ICD-0 vocabulary
composed of Latin and Greek terms, which are cog-
nate in the source and target languages, without the
construction of a large word lexicon.
A fairly simple classification of words, which could
be implemented by recognition of terminal mor-
phemes, and which classifies them both syntactically
and semantically, served as an adequate basis for
translation, and sheds some light on the linguistic
structure of this type of complex noun phrase seen
universally in medical writings and communications. A
set of 17 reduction-transformation rules based on the
word classification provide syntactic control of the
translation process.
</bodyText>
<reference confidence="0.799014">
A Method for Rapidly Applying Context
Sensitive Phonological Rules
Robert L. Mercer and Paul S. Cohen
IBM Thomas J. Watson Research Center
P.O. Box 218
Yorktown Heights, New York 10598
IBM Research Report RC 8889, June 1981, 25 pages.
</reference>
<bodyText confidence="0.9997633125">
The application of phonological rules to phonemic
strings to create phonetic graphs is a time-consuming
process. Since many such graphs must be constructed
during the decoding phase of automatic speech recog-
nition, it is valuable to be able to rapidly construct
phonetic graphs for strings of words from phonetic
graphs for the individual words in the string. Howev-
er, because many phonological rules operate across
word boundaries or require interword context, it is not
possible to determine a unique phonetic graph for a
word independent of the context in which it occurs.
This report describes a method for determining the
phonetic graph for a word in isolation together with
auxiliary information to allow phonetic graphs for
different words to be rapidly interconnected to form a
phonetic graph for a string of words.
</bodyText>
<reference confidence="0.713150833333333">
A Model for Automated Phonemicization
M. Boot, E. Maat, and J. Renkers
Instituut Voor Toegepaste Taalkunde
en Computerlinguistiek
Wilhelminapark 11
3581 NC Utrecht
</reference>
<subsubsectionHeader confidence="0.679994">
Research Report ISBN 90-6244-512-8, 1980, 91 pages.
</subsubsectionHeader>
<bodyText confidence="0.999960333333334">
In this report a computer program is designed that
should be able to perform a transcription of written
text into the phonematic format according to the prin-
ciples of phonematic transcription. This report focuses
on the design of the program and answers questions
concerning the relation between the technical part
(implementation) and the linguistic considerations
behind this computer program. It is argued that in the
past a computer program performing this linguistic
task was principally designed from the implementation
point of view. This has led to a computer program
which has a strong ad hoc kind of problem solving part
of it. Therefore, this computer program turns out to
be not adaptable to new situations and unforeseen
mistakes. In this paper it is argued that the implemen-
tation of linguistic problem solving algorithms itself is
a secondary problem and it is also argued that the
definition of the task as well as the kind of system
being created should be founded in intelligent linguis-
tic considerations. &amp;quot;Definitions&amp;quot; and &amp;quot;system&amp;quot; are
summarized with the notion &amp;quot;design&amp;quot; in this report.
Thus, in the report, it is argued that the design of
computer programs for linguistic operations should be
principally based on intelligent linguistic considera-
tions. This is the fundamental difference between
other computer programs proposed in the past for the
phonemicization of Dutch. It is argued that such a
design is more prosperous as far as the results of the
phonemicization program are concerned. The princi-
pals of this type of design are applied to the concrete
problem of transcription of written Dutch in a phone-
matic format. First results of the program FONGRAF
are documented and discussed in this report.
</bodyText>
<sectionHeader confidence="0.671521" genericHeader="method">
FONGRAF: A Computer Program for
Automated Phonemicization
</sectionHeader>
<reference confidence="0.6907404">
J. Renkers and M. Boot
Instituut Voor Toegepaste Taalkunde
en Computerlinguistiek
Wilhelminapark 11
3581 NC Utrecht
</reference>
<subsubsectionHeader confidence="0.698193">
Research Report ISBN 90-6244-513-6, 1980, 59 pages.
</subsubsectionHeader>
<bodyText confidence="0.998256666666667">
This report provides a listing of the PL/I program
for automated phonemicization described in the ab-
stract above.
</bodyText>
<page confidence="0.862915">
208 American Journal of Computational Linguistics, Volume 7, Number 3, July-September 1981
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.997854">The FINITE STRING Newsletter Abstracts of Current Literature</title>
<author confidence="0.721863">European Meeting on Cybernetics</author>
<note confidence="0.8994572">Research , by the Austrian Society for Cybernetic Studies, will be held April 13-16, 1982, the University of Vienna, Austria. [See 7:2, pg. 123.1 For a Preliminary Program contact the Chairman:</note>
<author confidence="0.940006">Professor Robert Trappl</author>
<affiliation confidence="0.9996795">Department of Medical Cybernetics University of Vienna</affiliation>
<address confidence="0.9884185">Freyung 6/2 A-1010 Vienna, AUSTRIA</address>
<note confidence="0.734779857142857">on Artificial Intelligence, by the Austrian Society for Artificial Intelligence and the Austrian Society for Cybernetic Studies, will be held at the Sixth European Meeting on Cybernetics and Systems Research, April 13-16, 1982, in Vienna. 7:2, 123.] Extended abstracts must be submitted by December 1, 1981, to:</note>
<title confidence="0.650279">Organizing Committee of the 6th EMCSR 82 Austrian Society for Cybernetic Studies</title>
<address confidence="0.9348875">Schottengasse 3 A-1010 Vienna, AUSTRIA</address>
<affiliation confidence="0.928141">National Computer Conference (NCC),</affiliation>
<note confidence="0.9378236">sponsored by AFIPS, will be held June 7-10, 1982, in Houston Astroarena. [See 7:2, 122.] Papers must be submitted by October 31, 1981, to: Dr. Howard L. Morgan NCC&apos;82 Technical Program Chairman</note>
<affiliation confidence="0.979802666666667">Department of Decision Sciences The Wharton School/CC University of Pennsylvania</affiliation>
<address confidence="0.998745">Philadelphia, Pennsylvania 19104</address>
<note confidence="0.961463190476191">European Conference on Electrotechnics will take place in Copenhagen at The Technical Uniof Denmark, on June 14-18, 1982. [See 122.] Further details may be obtained from: DIEU, Danish Engineers&apos; Post Graduate Inst. The Technical Univ. of Denmark, Bldg. 208 DK-2800 Lyngby, DENMARK International Conference on Computation- Linguistics (COLING 82) be held July 5-10, 1982, in Prague, Czechoslovakia. It will be sponsored by the International Committee on Computational Linguistics, in association with the Linguistic Institute of L. Star, Slovak Academy of Science, Bratislava, and the Faculty of Mathematics and Physics, Charles Uni- Prague. [See 7:2, 122.] Four copies of a 3-4 page, double-spaced summary must be submitted by December 1, 1981, to: COLING 82 MFF UK, Linguistics Malostranske n. 25 118 00 Prague 1, CZECHOSLOVAKIA</note>
<title confidence="0.886446">Abstracts of Current Literature</title>
<author confidence="0.999975">Michael W Freeman</author>
<affiliation confidence="0.9433705">ADO/FSSG Burroughs Corporation</affiliation>
<address confidence="0.958137">P.O. Box 517 Pennsylvania</address>
<author confidence="0.999797">Henry H Leitner</author>
<affiliation confidence="0.999935666666667">Center for Research in Computing Technology Aiken Computation Laboratory Harvard University</affiliation>
<address confidence="0.99998">Cambridge, Massachusetts 02138</address>
<abstract confidence="0.993918722222222">1980. This paper describes a number of extensions to Brachman&apos;s Structured Inheritance Network (SI-Net) formalism, with a view to creating an interactive system which will enable domain experts to develop and enhance knowledge networks (KNETs) representative of their particular application domains. The three principal extensions introduced are the QUA and VIZ links, and augmented event transition networks. The former permit us to partition the attribute space of conceptual entities in accordance with the defining roles played by these entities in certain kinds of sociolegal interactions. We use an event transition network to represent the value-class of so-called &amp;quot;dynamic attributes&amp;quot; in order to define all (and only all) the possible event sequences which the associated entity can participate in relative to the given attribute and still remain itself. The basic concept of simple succession of events in the transition network is then augmented by conditions on dependency relations which may exist across events among various roles (including bounds on the amount of time that can elapse between successive events). Since such event transition nets are themselves concepts within the KNET formalism, they can be placed along a specialization/generalization hierarchy. We show how these KNET extensions provide an extremely flexible basis for maintaining data base integrity and consistency, determining rollback dependencies during error recovery or historical perspectives, signalling possible irregularities in the attempted recording of new events which violate the legal event sequences defined by the net, as well as for generating derivative sets and mutually non-exclusive stative attribute values and associated interpretations for the wide range of ways this opens up for referencing such concepts in natural language.</abstract>
<note confidence="0.916118">Journal of Computational Linguistics, Volume 7, Number 3, July-September 1981</note>
<title confidence="0.996356">The FINITE STRING Newsletter Abstracts of Current Literature Representing Knowledge in an Interactive Planner</title>
<author confidence="0.999857">Ann E Robinson</author>
<author confidence="0.999857">David E Wilkins</author>
<affiliation confidence="0.999778">Artificial Intelligence Center SRI International</affiliation>
<address confidence="0.9979095">333 Ravenswood Avenue Menlo Park, California 94025</address>
<abstract confidence="0.91522">Proc. 1980 AAAI Conf., August 1980, 148-150. This note discusses the representation for actions and plans being developed as part of the current planning research at SRI. Described is a method for uniformly representing actions that can take place both in the domain and during planning. The representation accommodates descriptions of abstract (hypothetical) objects.</abstract>
<title confidence="0.992951">A Computer Model of Child Language Learning</title>
<author confidence="0.999859">Mallory Selfridge</author>
<affiliation confidence="0.999599">Electrical Engineering and Computer Science Dept. The University of Connecticut</affiliation>
<address confidence="0.997865">Storrs, Connecticut 06288</address>
<abstract confidence="0.9227955">Proc. 1980 AAAI Conf., August 1980, 224-227. A computer program modelling a child between the ages of 1 and 2 years is described. This program is based on observations of the knowledge this child had at age 1, the comprehension abilities he had at age 2, and the language experiences he had between these ages. The computer program described begins at the age 1 level, is given similar language experiences, and uses inference and learning rules to acquire comprehension at the age 2 level.</abstract>
<title confidence="0.9810735">An Approach to Acquiring and Applying Knowledge</title>
<author confidence="0.999989">Norman Haas</author>
<author confidence="0.999989">Gary G Hendrix</author>
<affiliation confidence="0.999834">Artificial Intelligence Center SRI International</affiliation>
<address confidence="0.9980315">333 Ravenswood Avenue Menlo Park, California 94025</address>
<abstract confidence="0.980272">Proc. 1980 AAAI Conf., August 1980, 235-239. The problem addressed in this paper is how to enable a computer system to acquire facts about new domains from tutors who are experts in their respective fields, but who have little or no training in computer science. The information to be acquired is that needed to support question-answering activities. The basic acquisition approach is &amp;quot;learning by being told.&amp;quot; We have been especially interested in exploring the notion of simultaneously learning not only new concepts, but also the linguistic constructions used to express those concepts. As a research vehicle we have developed a system that is preprogrammed with deductive algorithms and a fixed set of syntactic/semantic rules covering a small subset of English. It has been endowed with sufficient seed concepts and seed vocabulary to support effective tutorial interaction. Furthermore, the system is capable of learning new concepts and vocabulary, and can apply its acquired knowledge in a prescribed range of problem-solving situations.</abstract>
<title confidence="0.720524">Project EPISTLE: A System for the Automatic Analysis of Business Correspondence</title>
<author confidence="0.997158">Lance A Miller</author>
<affiliation confidence="0.976899">J. Watson Research Center</affiliation>
<address confidence="0.9582715">P.O. Box 218 Yorktown Heights, New York 10598</address>
<abstract confidence="0.977391166666667">Proc. 1980 AAAI Conf., August 1980, 280-282. The system described here is intended to provide the business executive with useful applications for the computer processing of correspondence in the office environment. Applications will include the synopsis and abstraction of incoming mail and a variety of critiques of newly-generated letters, all based upon the capability of understanding the natural language text at least to a level corresponding to customary business communication. Successive sections of the paper describe the background and prior work, the planned system output, and the implementation.</abstract>
<title confidence="0.9044685">When Expectation Fails: Towards a Self-Correcting Inference System</title>
<author confidence="0.999968">Richard H Granger</author>
<affiliation confidence="0.998561">Artificial Intelligence Project Department of Information and Computer Science University of California</affiliation>
<address confidence="0.999648">Irvine, California 92717</address>
<abstract confidence="0.9518966">Proc. 1980 AAA/ Conf., August 1980, 301-305. Contextual understanding depends on a reader&apos;s ability to correctly infer a context within which to interpret the events in a story. This &amp;quot;context-selection problem&amp;quot; has traditionally been expressed in terms of for making the correct of a story context. This paper presents a view of context as an ongoing process spread understanding process. This view requires that the understander be capable of recognizing and correcting erroneous initial context inferences. A computer program called ARTHUR is described, which selects the correct context for a story by dynamically reevaluating its own initial inferences in light of subsequent information in a story.</abstract>
<title confidence="0.987636">Generating Relevant Explanations: Natural Language Responses to Questions about Database Structure</title>
<author confidence="0.999994">Kathleen R McKeown</author>
<affiliation confidence="0.832912333333333">Computer and Information Science Moore School of Electrical Engineering D2 University of Pennsylvania</affiliation>
<address confidence="0.998533">Philadelphia, Pennsylvania 19104</address>
<note confidence="0.993517">Proc. 1980 AAA/ Conf., August 1980, 306-309. Journal of Computational Linguistics, Volume 7, Number 3, July-September 1981</note>
<title confidence="0.889398">The FINITE STRING Newsletter Abstracts of Current Literature</title>
<abstract confidence="0.9826046">The research described here is aimed at unresolved problems in both natural language generation and natural language interfaces to database systems. How relevant information is selected and then organized for the generation of responses to questions about database structure is examined. Due to limited space, this paper reports on only one method of explanation, called &amp;quot;compare and contrast.&amp;quot; In particular, it describes a specific constraint on relevancy and organization that can be used for this response type.</abstract>
<title confidence="0.9848905">The Semantic Interpretation of Nominal Compounds</title>
<author confidence="0.938452">Timothy Wilking Finin</author>
<affiliation confidence="0.886976">Computer and Information Science</affiliation>
<address confidence="0.440776">Moore School of Electrical Engrg. D2</address>
<affiliation confidence="0.999665">University of Pennsylvania</affiliation>
<address confidence="0.999241">Philadelphia, Pennsylvania 19104</address>
<abstract confidence="0.894602125">Proc. 1980 AAAI Conf., Aug, 1980, 310-312. This paper briefly introduces an approach to the of building semantic interpretations of nomisequences of two or more nouns related through modification. Examples of the kinds of nominal compounds dealt with are: &amp;quot;engine repairs,&amp;quot; &amp;quot;aircraft flight arrival,&amp;quot; &amp;quot;aluminum water pump,&amp;quot; and &amp;quot;noun noun modification.&amp;quot;</abstract>
<title confidence="0.71261">Towards an Al Model of Argumentation</title>
<author confidence="0.89544">Lawrence Birnbaum</author>
<author confidence="0.89544">Margot Flowers</author>
<author confidence="0.89544">Rod McGuire</author>
<affiliation confidence="0.999833">Department of Computer Science</affiliation>
<address confidence="0.979755">Box 2158 Yale Station</address>
<affiliation confidence="0.874168">Yale University</affiliation>
<address confidence="0.992156">New Haven, Connecticut 06520</address>
<abstract confidence="0.88696125">Proc. 1980 AAAI Conf., August 1980, 313-315. This paper describes a process model of human argumentation, and provides examples of its operation as implemented in a computer program. Our main concerns include such issues as the rules and structures underlying argumentation, how these relate to conversational rules, how reasoning is used in arguments, and how arguing and reasoning interact.</abstract>
<title confidence="0.9884995">Knowledge Representation for Syntactic/Semantic Processing</title>
<author confidence="0.999999">Robert J Bobrow</author>
<affiliation confidence="0.99402">Bolt Beranek and Newman Inc.</affiliation>
<address confidence="0.797754">50 Moulton Street Massachusetts</address>
<author confidence="0.999493">Bonnie Lynn Webber</author>
<affiliation confidence="0.977769">Computer and Information Science</affiliation>
<address confidence="0.672801">Moore School of Electrical Engrg. 02</address>
<affiliation confidence="0.999237">University of Pennsylvania</affiliation>
<address confidence="0.999446">Philadelphia, Pennsylvania 19104</address>
<abstract confidence="0.979199315789474">Proc. 1980 AAAI Conf., August 1980, 316-323. This paper describes the RUS framework for natural language processing, in which a parser incorporating a substantial ATN grammar for English interacts with a semantic interpreter to simultaneously parse and interpret input. The structure of that interaction is discussed, including the roles played by syntactic and semantic knowledge. Several implementations of the RUS framework are currently in use, sharing the same grammar, but differing in the form of their semantic component. One of these, the PSI-KLONE system, is based on a general object-centered knowledge representation system, called KL-ONE. The operation of PSI-KLONE is described, including its use of KL- ONE to support a general inference process called &amp;quot;incremental description refinement.&amp;quot; The last section of the paper discusses several important criteria for knowledge representation systems to be used in syntactic and semantic processing.</abstract>
<title confidence="0.9914485">Language and Memory: Generalization as a Part of Understanding</title>
<author confidence="0.999995">Michael Lebowitz</author>
<affiliation confidence="0.999981">Computer Science Department Columbia University</affiliation>
<address confidence="0.9521775">406 Mudd Building New York, New York 10027</address>
<note confidence="0.6478385">Proc. 1980 AAA/ Conf., August 1980, 324-326. This paper presents the Integrated Partial Parser</note>
<abstract confidence="0.997101">computer model that combines text understanding and memory of events. An extended example of the program&apos;s ability to understand newspaper stories and make generalizations that are useful for memory organization is presented.</abstract>
<title confidence="0.9961855">Failures in Natural Language Systems: Applications to Data Base Query Systems</title>
<author confidence="0.999226">Eric Mays</author>
<affiliation confidence="0.832468333333333">Computer and Information Science Moore School of Electrical Engineering D2 University of Pennsylvania</affiliation>
<address confidence="0.999551">Philadelphia, Pennsylvania 19104</address>
<abstract confidence="0.956541047619048">Proc. 1980 AAAI Conf., August 1980, 327-330. A significant class of failures in interactions with data base query systems is attributable to misconceptions or incomplete knowledge regarding the domain of discourse on the part of the user. This paper describes several types of user failures, namely, intensional failures of presumptions. These failures are distinguished from extensional failures of presumptions since they are dependent on the structure rather than the contents of the data base. A knowledge representation has been developed for the recognition of intensional failures that are due to the assumption of nonexistent relationships between entities. Several other intensional failures which depend on more sophisticatknowledge representations are also discussed. Ap- Journal of Computational Linguistics, Volume 7, Number 3, July-September 1981 The FINITE STRING Newsletter Abstracts of Current Literature propriate forms of corrective behavior are outlined which would enable the user to formulate queries directed to the solution of his/her particular task and compatible with the knowledge organization.</abstract>
<title confidence="0.933992">Organizing Memory and Keeping It Organized</title>
<author confidence="0.594293">Kolodner</author>
<affiliation confidence="0.97225">Information and Computer Science Department Georgia Institute of Technology</affiliation>
<address confidence="0.99965">Atlanta, Georgia 30332</address>
<abstract confidence="0.950753166666667">Proc. 1980 AAAI Conf., August 1980, 331-333. Maintaining good memory organization is important in large memory systems. This paper presents a scheme for automatically reorganizing event information in memory. The processes are implemented in a computer program called CYRUS.</abstract>
<title confidence="0.987223">Narrative Text Summarization</title>
<author confidence="0.999911">Wendy Lehnert</author>
<affiliation confidence="0.999959">Department of Computer Science</affiliation>
<address confidence="0.979823">Box 2158 Yale Station</address>
<affiliation confidence="0.875061">Yale University</affiliation>
<address confidence="0.99576">New Haven, Connecticut 06520</address>
<abstract confidence="0.972280142857143">Proc. 1980 AAAI Conf., August 1980, 337-339. In order to summarize a story it is necessary to access a high level analysis that highlights the story&apos;s central concepts. A technique of memory representation based on affect units appears to provide the necessary foundation for such an analysis. Affect units are conceptual structures that overlap with each other when a narrative is cohesive. When overlapping intersections are interpreted as arcs in a graph of affect units, the resulting graph encodes the plot of the story. Structural features of the graph then reveal which concepts are central to the story. Affect unit analysis is currently being investigated as a processing strategy for narrative summarization.</abstract>
<title confidence="0.970752">Meta-Planning</title>
<author confidence="0.999951">Robert Wilensky</author>
<affiliation confidence="0.99844325">Electronics Research Laboratory Computer Science Division Department of EECS University of California, Berkeley</affiliation>
<address confidence="0.999815">Berkeley, California 94720</address>
<abstract confidence="0.9644555">Proc. 1980 AAAI Conf., August 1980, 334-336. This paper is concerned with the problems of planning and understanding. These problems are related because a natural language understander must apply knowledge about people&apos;s goals and plans in order to make the inferences necessary to explain the behavior of a character in a story. Thus while a story understander is not a planner, it must embody a theory of planning knowledge. I have developed such a theory in the construction of PAM (Plan Applier Mechanism), a story understanding program. This paper is concerned not with the understanding mechanism itself, but that part of its planning knowledge which is independent of whether that knowledge is used to explain someone&apos;s behavior or to generate a plan for one&apos;s own use.</abstract>
<title confidence="0.9230455">Issues in the Development of Natural Language Front-Ends</title>
<author confidence="0.8992665">J Hendler</author>
<author confidence="0.8992665">T P Kehler</author>
<author confidence="0.8992665">P R Michaelis</author>
<author confidence="0.8992665">B Phillips</author>
<author confidence="0.8992665">K M Ross</author>
<author confidence="0.8992665">H R Tennant</author>
<affiliation confidence="0.904385">Texas Instruments</affiliation>
<address confidence="0.991784">P.O. Box 225936 MS 371 Dallas, Texas 75023</address>
<abstract confidence="0.97022532">AFIPS Conf. Proc. 50, 1981 NCC, May 1981, 643-648. This paper will discuss some issues we believe to be important to the design of a natural language frontend. These are divided into three categories: conceptual coverage, linguistic coverage, and implementation issues. The section on conceptual coverage discusses the use of a domain expert, which understands what the user is saying even though the system to which the front-end is interfaced might not be able to properly do what the user wants. The section on linguistic coverage discusses attempts to allow a natural language interface to handle natural, interactive human communication. Two solutions are explored: first, the design of a robust natural language understanding system composed of many experts that know about some aspect of the organization of language is considered; second, because the design of a robust system is a large task, the intermediate goal of limiting the vocabulary and constructions that can be used while retaining all the user-oriented benefits of natural language is considered. The implementation issues considered are the design of a system in which the grammar and the domain of discourse can be easily extended and which can be used for more than one domain without extensive rewriting.</abstract>
<title confidence="0.9847955">Text-Critiquing with the EPISTLE System: An Author&apos;s Aid to Better Syntax</title>
<author confidence="0.999193">A Miller</author>
<author confidence="0.999193">George Heidorn</author>
<author confidence="0.999193">Karen Jensen</author>
<affiliation confidence="0.969297">J. Watson Research Center</affiliation>
<address confidence="0.955462">P.O. Box 218 Yorktown Heights, New York 10598</address>
<abstract confidence="0.954385888888889">AFIPS Conf. Proc. 50, 1981 NCC, May 1981, 649-655. The experimental EPISTLE system is ultimately intended to provide office workers with intelligent applications for the processing of natural language text, particularly business correspondence. A variety of possible critiques of textual material are identified in this paper, but the discussion focuses on the system&apos;s capability to detect several classes of grammatical errors, such as disagreement in number bethe subject and the verb. The system&apos;s error- Journal of Computational Linguistics, Volume 7, Number 3, July-September 1981 The FINITE STRING Newsletter Abstracts of Current Literature detection performance relies critically on its parsing component which determines the syntactic structure of each sentence and the grammatical functions fulfilled by various phrases. Details of the system&apos;s operations are provided, and some of the future critiquing objectives are outlined.</abstract>
<title confidence="0.929999">Shifting to a Higher Gear in a Natural Language System</title>
<author confidence="0.995387">Bozena Henisz Thompson</author>
<author confidence="0.995387">Frederick B Thompson</author>
<affiliation confidence="0.999993">California Institute of Technology</affiliation>
<address confidence="0.999859">Pasadena, California 91125</address>
<abstract confidence="0.988142833333333">AFIPS Conf. Proc. 50, 1981 NCC, May 1981, 657-662. We have completed the development of the REL System, a system for communicating with the computer in natural language concerning a relational database. We have been using that system in a series of experiments on how people actually do communicate in solving an intellectual task. These experiments, together with our general experience with REL and related work elsewhere, have led us to the specification and development of a new system, the POL (Problem Oriented Language) System. POL is an evolutionary extension of REL, preserving what has worked, and extending and adding new capabilities to meet observed needs. These improvements include more responsive diagnostics, handling of sentence fragments, inter-knowledge-base communications, and new facilities for building and extending the knowledge bases of users. This paper introduces POL.</abstract>
<title confidence="0.99835">A Practical Comparison of Parsing Strategies</title>
<author confidence="0.999983">Jonathan Slocum</author>
<affiliation confidence="0.999622">Linguistics Research Center</affiliation>
<address confidence="0.998764">P.O. Box 7247</address>
<affiliation confidence="0.998298">University of Texas</affiliation>
<address confidence="0.99761">Austin, Texas 78712</address>
<abstract confidence="0.983037945945946">Proc. 19th Annual ACL Conf., June 1981, 1-6. Although the literature dealing with formal and natural languages abounds with theoretical arguments of worst-case performance by various parsing strategies, there is little discussion of comparative performance based on actual practice in understanding natural language. Yet important practical considerations do arise when writing programs to understand one aspect or another of natural language utterances. Where, for example, a theorist will characterize a parsing strategy according to its space and/or time requirements in attempting to analyze the worst possible input according to an arbitrary grammar strictly limited in expressive power, the researcher studying natural language processing can be justified in concerning himself more practical performance in parsing sentences encountered in language as humans actually use it, using a grammar expressed in a form convenient to the human linguist who is writing it. This paper has two purposes. One is to report an evaluation of the performance of several parsing strategies in a real-world setting, pointing out practical problems in making the attempt, indicating which of the strategies is superior to the others in which situations, and most of all determining the reasons why the best strategy outclasses its competition in order to stimulate and direct the design of improvements. The other, more important purpose is to assist in establishing such evaluation as a meaningful and valuable enterprise that contributes to the evolution of natural language processing from an art form into an empirical science. That is, our concern for parsing efficiency transcends the issue of mere practicality. In this paper we detail our experimental setting and approach, present the results, discuss the implications of those results, and conclude with some remarks on what has been learned.</abstract>
<affiliation confidence="0.714925">Computational Complexity and</affiliation>
<title confidence="0.957556">Lexical Functional Grammar</title>
<author confidence="0.991753">Robert C Berwick</author>
<affiliation confidence="0.999966">Massachusetts Institute of Technology</affiliation>
<address confidence="0.9993465">545 Technology Square Cambridge, Massachusetts 02139</address>
<abstract confidence="0.949971305555556">Proc. 19th Annual ACL Conf., June 1981, 7-12. Recently, a new theory of grammar has been advanced with the explicitly stated aim of meeting the dual demands of learnability and parsability — the Lexical Functional Grammars (LFGs) of Bresnan. The theory of Lexical Functional Grammars is claimed to have all the descriptive merits of transformational grammar, but none of its computational unruliness. In LFG, there are no transformations (as classically described); the work formerly ascribed to transformations such as &amp;quot;passive&amp;quot; is shouldered by information stored in lexical entries associated with lexical items. The elimination of transformational power naturally gives rise to the hope that a lexically-based system would be computationally simpler than a transformational one. The main result of this paper is to show that certain Lexical Functional Grammars can generate lanwhose recognition time likely computationally intractable, at least according to our current understanding of what is or is not rapidly solvable. Briefly, the demonstration proceeds by showing how a problem that is widely conjectured to be computationally difficult — namely, whether there exists an assignment of l&apos;s and O&apos;s (or &amp;quot;T&amp;quot;s and &amp;quot;F&apos;s) to the literals of a Boolean formula in conjunctive normal form that makes the formula evaluate to &amp;quot;1&amp;quot; (or &amp;quot;true&amp;quot;) — can be re-expressed as the problem of recwhether a particular string is or is not a mem- Journal of Computational Linguistics, Volume 7, Number 3, July-September 1981 The FINITE STRING Newsletter Abstracts of Current Literature ber of the language generated by a certain lexical functional grammar. This paper also discusses the relevance of this technical result for more down-to-earth computational linguistics.</abstract>
<title confidence="0.5322405">Corepresentational Grammar and English</title>
<author confidence="0.999337">Karen Ryan</author>
<affiliation confidence="0.850009">Linguistics Department 142 Klaeber Court University of Minnesota</affiliation>
<address confidence="0.999714">Minneapolis, Minnesota 55455</address>
<note confidence="0.5059775">Proc. 19th Annual ACL Conf., June 1981, 13-18. Marcus (1980) notes that the syntax of English</note>
<abstract confidence="0.996190857142857">comparative constructions is highly complex, and claims that both syntactic and semantic information must be available for them to be parsed. This paper argues that comparatives can be structurally analyzed on the basis of syntactic information alone via a strictly surface-based grammar. Such a grammar is given in Ryan (1981), based on the corepresentational model of Kac (1978). While the grammar does not define a parsing algorithm per se, it nonetheless expresses regularities of surface organization and its relationship to semantic interpretation that an adequate parser would be expected to incorporate. The central problem in parsing comparatives involves identifying the arguments of comparative predicates, and the relations borne by these arguments to such predicates. A corepresentational grammar is explicitly designed to assign predicate-argument structure to sentences on the basis of their surface syntactic organization. This paper discusses four problem areas in the description of comparatives and outlines the sections of the grammar of Ryan (1981) that apply to them.</abstract>
<title confidence="0.974457666666667">Performance Comparison of Component Algorithms for the Phonemicization of Orthography</title>
<author confidence="0.999916">Jared Bernstein</author>
<affiliation confidence="0.963668">Telesensory Speech Systems</affiliation>
<address confidence="0.9959385">3408 Hillview Avenue Alto, California</address>
<author confidence="0.996992">Larry Nessly</author>
<affiliation confidence="0.997217">University of North Carolina</affiliation>
<address confidence="0.958129">Chapel Hill, North Carolina 27514</address>
<abstract confidence="0.979510586206897">Proc. 19th Annual ACL Conf., June 1981, 19-22. A system for converting English text into synthetic speech can be divided into two processes that operate in series: a text-to-phoneme converter, and phonemic-input speech synthesizer. The conversion of orthographic text into a phonemic form may itself comprise several processes in series, such as formatting text to expand abbreviations and non-alphabetic expressions, parsing and word class determination, segmental phonemicization of words, word and clause level stress assignment, word internal and word boundary allophonic adjustments, and duration and fundamental frequency settings for phonological units. Comparing the accuracy of different algorithms for text-to-phoneme conversion is often difficult because authors measure and report system performance in incommensurable ways. Furthermore, comparison of the output speech from two complete systems may not always provide a good test of the performance of the corresponding component algorithms in the two systems, because radical performance differences in other components can obscure small differences in the components of interest. The only reported direct comparison of two complete text-to-speech systems (MITALK and TSI&apos;s TTS-X) was conducted by Bernstein and Pisoni. This paper reports one study that compared two algorithms for automatic segmental phonemicization of words, and a second study that compared two algorithms for automatic assignment of lexical stress.</abstract>
<title confidence="0.933577">PHONY: A Heuristic Phonological Analyzer</title>
<author confidence="0.999899">Lee A Becker</author>
<affiliation confidence="0.999967">Department of Computer Science Indiana University</affiliation>
<address confidence="0.9993725">101 Lindley Hall Bloomington, Indiana 47401</address>
<abstract confidence="0.982588090909091">Proc. 19th Annual ACL Conf., June 1981, 23-27. PHONY is a program to do phonological analysis. Within the generative model of grammar the function of the phonological component is to assign a phonetic representation to an utterance by modifying the &amp;quot;underlying representations&amp;quot; (URs) of its constituent morphemes. URs are abstract entities which contain the idiosyncratic information about pronunciations of morphemes. The input to PHONY is pronunciations of words and phrases upon which a preliminary morphological analysis has been completed. They have been divided into morphemes, and different instances of the same morpheme have been associated. These are represented as strings of phonetic symbols including morphemeand word-boundaries. Indices are used to associate various instances of the same morpheme. The output of PHONY is a set of phonological rules or regularities in the data, as well as a set of underlying representations for the morphemes. The phonological rules generate the various pronunciations of the morphemes from their underlying representations.</abstract>
<note confidence="0.953474">Journal of Computational Linguistics, Volume 7, Number 3, July-September 1981</note>
<title confidence="0.9957195">The FINITE STRING Newsletter Abstracts of Current Literature Two Discourse Generators</title>
<author confidence="0.999967">William C Mann</author>
<affiliation confidence="0.999929">USC/Information Sciences Institute</affiliation>
<address confidence="0.9964305">4676 Admiralty Way Marina del Rey, California 90291</address>
<abstract confidence="0.971687909090909">Proc. 19th Annual ACL Conf., June 1981, 43-47. The task of discourse generation is to produce multisentential text in natural language which (when heard or read) produces effects (informing, motivation, etc.) and impressions (conciseness, correctness, ease of reading, etc.) which are appropriate to a need or goal held by the creator of the text. In comparing two AI discourse generators here we can do no more than suggest opportunities and attractive options for future exploration. We describe them only in terms of a few of the techniques which they employ, partly because these techniques seem more valuable than the system designs in which they happen to have been used. The systems which we describe are PROTEUS, by Anthony Davey at Edinburgh and KDS by Mann and Moore at ISI, both of which are severely limited and idiosyncratic in scope and technique. First we identify particular techniques in each system which contribute strongly to the quality of the resulting text. Then we compare the two systems discussing their common failings and the possibilities for creating a system having the best of both.</abstract>
<title confidence="0.9698465">A Grammar and a Lexicon for a Text-Production System</title>
<author confidence="0.999995">Christian Matthiessen</author>
<affiliation confidence="0.999851">USC/Information Sciences Institute</affiliation>
<address confidence="0.9969145">4676 Admiralty Way Marina del Rey, California 90291</address>
<abstract confidence="0.979570846153846">Proc. 19th Annual ACL Conf., June 1981, 49-55. In a text-production system high and special demands are placed on the grammar and the lexicon. This paper views these components in such a system. First, the subcomponents dealing with semantic information and with syntactic information are presented separately. The problems of relating these two types of information are then identified. Finally, strategies designed to meet the problems are proposed and discussed. One of the issues that is illustrated is what happens when a systemic linguistic approach is combined with a KL-ONE like knowledge representation — a novel and hitherto unexplored combination.</abstract>
<title confidence="0.9401445">Language Production: The Source of the Dictionary</title>
<author confidence="0.999985">David D McDonald</author>
<affiliation confidence="0.9996155">Computer and Information Science University of Massachusetts</affiliation>
<address confidence="0.999457">Amherst, Massachusetts 01002</address>
<abstract confidence="0.978871368421053">Proc. 19th Annual ACL Conf., June 1981, 57-62. Ultimately in any natural language production system the largest amount of human effort will go into construction of the data base that associates objects and relations in the program&apos;s domain with the words and phrases that could be used to describe them. This paper describes a technique for basing the dictionary directly on the semantic abstraction network used for the domain knowledge itself, taking advantage of the inheritance and specialization mechanisms of a network formalism such as KL-ONE. The technique creates considerable economies of scale, and makes possible the automatic description of individual objects according to their position in the semantic net. Furthermore, because the process of deciding what properties to use in an object&apos;s description is now given over to a common procedure, we can write general-purpose rules to, for example, avoid redundancy or grammatically awkward constructions.</abstract>
<title confidence="0.915266">Analogies in Spontaneous Discourse</title>
<author confidence="0.999911">Rachel Reichman</author>
<affiliation confidence="0.996504">Bolt Beranek and Newman Inc.</affiliation>
<address confidence="0.9996625">50 Moulton Street Cambridge, Massachusetts 02138</address>
<abstract confidence="0.969929294117647">Proc. 19th Annual ACL Conf., June 1981, 63-69. This paper presents an analysis of analogies based on observations of natural conversations. People&apos;s spontaneous use of analogies provides insight into their implicit evaluation procedures for analogies. The treatment here, therefore, reveals aspects of analogical processing that are somewhat more difficult to see in an experimental context. The work involves explicit treatment of the discourse context in which analogy occurs. A major focus here is the formalization of the effects of analogy on discourse development. There is much rule-like behavior in this process, both in underlying thematic development of the discourse and in the surface linguistic forms used in this development. Both these forms of regular behavior are discussed in terms of a hierarchical structuring of a discourse into distinct, but related and linked, context spaces.</abstract>
<title confidence="0.71991">Investigation of Processing Strategies for the Analysis of</title>
<author confidence="0.99993">Robin Cohen</author>
<affiliation confidence="0.9999755">Department of Computer Science University of Toronto</affiliation>
<address confidence="0.998561">Toronto, Ontario CANADA M5S 1A7</address>
<abstract confidence="0.961024">Proc. 19th Annual ACL Conf., June 1981, 71-75. This paper outlines research on processing strategies being developed for a language understanding system, designed to interpret the structure of arguments. For the system, arguments are viewed as trees, with claims as fathers to their evidence. Then understanding becomes a problem of developing a representative argutree, by locating each proposition of the argu- Journal of Computational Linguistics, Volume 7, Number 3, July-September 1981 The FINITE STRING Newsletter Abstracts of Current Literature ment at its appropriate place. The processing strategies we develop for the hearer are based on expectations that the speaker will use particular coherent transmission strategies and are designed to be fairly efficient (work in linear time). We also comment on the use by the speaker of linguistic clues to indicate structure, illustrating how the hearer can interpret the clues to limit his processing search and thus improve the complexity of the understanding process.</abstract>
<title confidence="0.9464375">What&apos;s Necessary to Hide?: Modelling Action Verbs</title>
<author confidence="0.999967">James F Allen</author>
<affiliation confidence="0.978011333333333">Department of Computer Science University of Rochester Mathematical Sciences Building</affiliation>
<address confidence="0.999178">Rochester, New York 14627</address>
<abstract confidence="0.969194266666667">Proc. 19th Annual ACL Conf., June 1981, 77-83. This paper considers what types of knowledge one must possess in order to reason about actions. Rather than concentrating on how actions are performed, as is done in the problem-solving literature, it examines the set of conditions under which an action can be said to have occurred. In other words, if one is told that action A occurred, what can be inferred about the state of the world? In particular, if the representation can define such conditions, it must have good models of time, belief, and intention. This paper discusses these issues and suggests a formalism in which general actions and events can be defined. Throughout, the action of hiding a book from someone is used as a motivating example.</abstract>
<title confidence="0.987882">A Rule-based Conversation Participant</title>
<author confidence="0.999948">Robert E Frederking</author>
<affiliation confidence="0.9997595">Department of Computer Science Carnegie-Mellon University</affiliation>
<address confidence="0.999673">Pittsburgh, Pennsylvania 15213</address>
<note confidence="0.565864">Proc. 19th Annual ACL Conf., June 1981, 83-87.</note>
<abstract confidence="0.992994333333333">The problem of modelling human understanding and generation of a coherent dialog is investigated by simulating a conversation participant. The rule-based system currently under development attempts to capture the intuitive concept of &amp;quot;topic&amp;quot; using data structures consisting of declarative representations of the subjects under discussion linked to the utterances and rules that generated them. Scripts, goal trees, and a semantic network are brought to bear by general, domain-independent conversational rules to understand and generate coherent topic transitions and specific output utterances.</abstract>
<title confidence="0.9582605">Search and Inference Strategies in Pronoun Resolution: An Experimental Study</title>
<author confidence="0.999587">Kate Ehrlich</author>
<affiliation confidence="0.9999505">Department of Psychology University of Massachusetts</affiliation>
<address confidence="0.999381">Amherst, Massachusetts 01003</address>
<abstract confidence="0.982173944444444">Proc. 19th Annual ACL Conf., June 1981, 89-93. The process of assigning a referent to a pronoun can be viewed as utilizing two kinds of strategies. One strategy is concerned with selecting the best referent from among the candidates available. The other strategy is concerned with searching through memory for the candidates. These two types of strategies, which will be referred to mnemonically as inference and search strategies, have different kinds of characteristics. A search strategy dictates the order in which candidates are evaluated, but has no machinery for carrying out the evaluation. The inference strategy helps to set up the representation of the information in the text against which candidates can be evaluated, but has no way of finding the candidates. In this paper, the way these strategies might interact is explored and the results of two studies that bear on the issues are reported on.</abstract>
<title confidence="0.9593965">Presupposition and lmplicature in Model-Theoretic Pragmatics</title>
<author confidence="0.999928">Douglas B Moran</author>
<affiliation confidence="0.999953">Department of Computer Science Oregon State University</affiliation>
<address confidence="0.999284">Corvallis, Oregon 97331</address>
<abstract confidence="0.969591058823529">Proc. 19th Annual ACL Conf., June 1981, 107-108. Model-theoretic pragmatics is an attempt to provide a formal description of the pragmatics of natural language as effects arising from using model-theoretic semantics in a dynamic environment. The pragmatic phenomena considered here have been variously laimplicature. The models used in traditional model-theoretic semantics provide a complete and static representation of knowledge about the world. However, this is not the environment in which language is used. Language is used in a dynamic environment — the participants have incomplete knowledge of the world and the understanding of a sentence can add to the knowledge of the listener. A formalism which allows models to contain incomplete knowledge and to which knowledge can be added has been developed.</abstract>
<title confidence="0.51307">Some Computational Aspects of Situation Semantics</title>
<affiliation confidence="0.935185">Department of Philosophy Stanford University</affiliation>
<address confidence="0.999329">Stanford, California 94305</address>
<note confidence="0.887634">19th Annual ACL Conf., June 1981, Journal of Computational Linguistics, Volume 7, Number 3, July-September 1981</note>
<title confidence="0.976358">The FINITE STRING Newsletter Abstracts of Current Literature</title>
<abstract confidence="0.994755962962963">Can a realist model theory of natural language be computationally plausible? Or, to put it another way, is the view of linguistic meaning as a relation between expressions of natural language and things (objects, properties, etc.) in the world, as opposed to a relation between expressions and procedures in the head, consistent with a computational approach to understanding natural language? The model theorist must either claim that the answer is yes, or be willing to admit that humans transcend the computationally feasible in their use of language. Until recently the only model theory of natural language that was at all well developed was Montague Grammar. Unfortunately, it was based on the primitive notion of &amp;quot;possible world&amp;quot; and so was not a realist theory, unless you are prepared to grant that all possible worlds are real. Montague Grammar is also computationally intractable, for reasons discussed in the paper. John Perry and I have developed a somewhat different approach to the model theory of natural language, a theory we call &amp;quot;Situation Semantics&amp;quot;. Since one of my own motivations in the early days of this project was to use the insights of generalized recursion theory to find a computationally plausible alternative to Montague Grammar, it seems fitting to give a progress report here.</abstract>
<title confidence="0.893251">A Situation Semantics Approach to the Analysis of Speech Acts</title>
<author confidence="0.939146">David Andreoff Evans</author>
<affiliation confidence="0.999974">Stanford University</affiliation>
<address confidence="0.999792">Stanford, California 94305</address>
<abstract confidence="0.971274888888889">Proc. 19th Annual ACL Conf., June 1981, 113-116. During the past two decades, much work in linguistics has focused on sentences as minimal units of communication, and the project of rigorously characterizing the structure of sentences in natural language has met with some success. Not surprisingly, however, sentence grammars have contributed little to the analysis of discourse. Human discourse consists not just of words in sequences directed by a speaker to an addresused to situations to intentions. Only when the addressee has apprehended both these aspects of the message communicated can the message be interpreted. The analysis of discourse that emerges from Austin&apos;s work, grounded in a theory of action, takes this view as central, and the concept of the speech act follows naturally. This paper describes a situation semantics approach to the analysis of speech acts.</abstract>
<title confidence="0.955029">Problems in Logical Form</title>
<author confidence="0.999667">Robert C Moore</author>
<affiliation confidence="0.9996175">Artificial Intelligence Center SRI International</affiliation>
<address confidence="0.9977915">333 Ravenswood Avenue Menlo Park, California 94025</address>
<abstract confidence="0.953847625">Proc. 19th Annual ACL Conf., June 1981, 117-124. This paper surveys what we at SRI view as some of the key problems encountered in defining a system of representation for the logical forms of English sentences, and suggests possible approaches to their solution. We first look at some general issues related to the notion of logical form, and then discuss a number of problems associated with the way information involving certain key concepts is expressed in English. Although our main concern here is with theoretical issues rather than with system performance, this paper is not merely speculative. The DIALOGIC system currently under development in the SRI Artificial Intelligence Center parses English sentences and translates them into logical forms embodying many of the ideas presented here.</abstract>
<title confidence="0.989316">A Case for Rule-Driven Semantic Processing</title>
<author confidence="0.999789">Martha Palmer</author>
<affiliation confidence="0.829420333333333">Computer and Information Science Moore School of Electrical Engineering D2 University of Pennsylvania</affiliation>
<address confidence="0.999217">Philadelphia, Pennsylvania 19104</address>
<abstract confidence="0.972774318181818">Proc. 19th Annual ACL Conf., June 1981, 125-131. The primary task of semantic processing is to provide an appropriate mapping between the syntactic constituents of a parsed sentence and the arguments of the semantic predicates implied by the verb. This is known as the Alignment Problem. Section One of this paper gives an overview of a generally accepted approach to semantic processing that goes through several levels of representation to achieve this mapping. Although somewhat inflexible and cumbersome, the different levels succeed in preserving the context sensitive information provided by verb semantics. Section Two presents the author&apos;s rule-driven approach which is more uniform and flexible yet still accommodates context sensitive constraints. This approach is based on general underlying principles for syntactic methods of introducing semantic arguments and has interesting implications for linguistic theories about case. These implications are discussed in Section Three. A system that implements this approach has been designed for and tested on pulley problem statements gathered from several physics text books.</abstract>
<note confidence="0.950167">Journal of Computational Linguistics, Volume 7, Number 3, July-September 1981</note>
<title confidence="0.9989505">The FINITE STRING Newsletter Abstracts of Current Literature A Taxonomy for English Nouns and Verbs</title>
<author confidence="0.999983">Robert A Amsler</author>
<affiliation confidence="0.999835">Artificial Intelligence Center SRI International</affiliation>
<address confidence="0.997787">333 Ravenswood Avenue Menlo Park, California 94025</address>
<abstract confidence="0.968799111111111">Proc. 19th Annual ACL Conf., June 1981, 133-138. The definition texts of a machine-readable pocket dictionary were analyzed to determine the disambiguated word sense of the kernel terms of each word sense being defined. The resultant sets of word pairs of defined and defining words were then computationally connected into two taxonomic semi-lattices (&amp;quot;tangled hierarchies&amp;quot;) representing some 24,000 noun nodes and 11,000 verb nodes. The study of the nature of the &amp;quot;topmost&amp;quot; nodes in these hierarchies and the structure of the trees reveal information about the nature of the dictionary&apos;s organization of the language, the concept of semantic primitives and other aspects of lexical semantics. The data proves that the dictionary offers a fundamentally consistent description of word meaning and may provide the basis for future research and applications in computational linguistic systems.</abstract>
<title confidence="0.995577">Interpreting Natural Language Database Updates</title>
<author confidence="0.999896">S Jerrold Kaplan</author>
<author confidence="0.999896">Jim Davidson</author>
<affiliation confidence="0.999909">Department of Computer Science Stanford University</affiliation>
<address confidence="0.999806">Stanford, California 94305</address>
<abstract confidence="0.986211037037037">Proc. 19th Annual ACL Conf., June 1981, 139-141. the problem of database in natural language has been studied extensively, there has been relatively little work on processing database in natural language. To interpret update requests, several linguistic issues must be addressed that do not typically pose difficulties when dealing exclusively with queries. The primary difficulty with interpreting natural language updates is that there may be several ways in which a particular update can be performed in the underlying database. Many of these options, while literally correct and semantically meaningful, may correspond to bizarre interpretations of the request. While human speakers would intuitively reject these unusual readings, a computer program may be unable to distinguish them from more appropriate ones. If carried out, they often have undesirable side effects on the database. Our approach to this problem is to generate a limited set of &amp;quot;candidate&amp;quot; updates, rank them according to a set of domain-independent heuristics that reflect general properties of &amp;quot;reasonable&amp;quot; updates, and either perform the update or present the highest ranked options to the user for selection. This paper briefly examines some of the linguistic problems encountered, and describes an implemented system that performs simple natural language database updates.</abstract>
<title confidence="0.993801">Dynamic Strategy Selection in Flexible Parsing</title>
<author confidence="0.997683">Carbonell</author>
<author confidence="0.997683">Philip J Hayes</author>
<affiliation confidence="0.9997875">Department of Computer Science Carnegie-Mellon University</affiliation>
<address confidence="0.8897545">Schenley Park Pittsburgh, Pennsylvania 15213</address>
<note confidence="0.757434">Proc. 19th Annual ACL Conf., June 1981, 143-147.</note>
<abstract confidence="0.989862315789474">Robust natural language interpretation requires strong semantic domain models, &amp;quot;fail-soft&amp;quot; recovery heuristics, and very flexible control structures. Although single-strategy parsers have met with a measure of success, a multi-strategy approach is shown to provide a much higher degree of flexibility, redundancy, and ability to bring task-specific domain knowledge (in addition to general linguistic knowledge) to bear on both grammatical and ungrammatical input. A parsing algorithm is presented that integrates several different parsing strategies, with case-frame instantiation dominating. Each of these parsing strategies exploits different types of knowledge; and their combination provides a strong framework in which to process conjunctions, fragmentary input, and ungrammatical structures, as well as less exotic, grammatically correct input. Several specific heuristics for handling ungrammatical input are presented within this multistrategy framework.</abstract>
<title confidence="0.9884045">A Construction-Specific Approach to Focused Interaction in Flexible Parsing</title>
<author confidence="0.999993">Philip J Hayes</author>
<affiliation confidence="0.999791">Department of Computer Science Carnegie-Mellon University</affiliation>
<address confidence="0.8889925">Schenley Park Pittsburgh, Pennsylvania 15213</address>
<note confidence="0.681503">Proc. 19th Annual ACL Conf., June 1981, 149-152. A flexible parser can deal with input that deviates</note>
<abstract confidence="0.971246913043478">from its grammar, in addition to input that conforms to it. Ideally, such a parser will correct the deviant input; sometimes, it will be unable to correct it at all; at other times, correction will be possible, but only to within a range of ambiguous possibilities. This paper is concerned with such ambiguous situations, and with making it as easy as possible for the ambiguity to be resolved through consultation with the user. We show the importance of asking the user for clarification in as focused a way as possible. Focused interaction of this kind is facilitated by a construction-specific approach to flexible parsing, with specialized parsing techniques for each type of construction, and specialized ambiguity representations for each type of ambiguity that a particular construction can give rise to. A construction-specific approach also aids in task-specific language development by allowing a language definition Journal of Computational Linguistics, Volume 7, Number 3, July-September 1981 The FINITE STRING Newsletter Abstracts of Current Literature that is natural in terms of the task domain to be interpreted directly without compilation into a uniform grammar formalism, thus greatly speeding the testing of changes to the language definition.</abstract>
<title confidence="0.7663655">Controlled Transformational Sentence Generation</title>
<author confidence="0.993919">Madeleine Bates</author>
<affiliation confidence="0.992773">Bolt Beranek and Newman Inc.</affiliation>
<address confidence="0.937894">50 Moulton Street Massachusetts,</address>
<author confidence="0.995628">Robert Ingria</author>
<affiliation confidence="0.999958">Department of Linguistics Massachusetts Institute of Technology</affiliation>
<address confidence="0.999909">Cambridge, Massachusetts 02139</address>
<abstract confidence="0.955882217391304">Proc. 19th Annual ACL Conf., June 1981, 153-158. This paper describes a transformational sentence generator that was built primarily to focus on syntactic form and syntactic relationships. Our main goal was to produce a tutorial system for the English language; the intended users of the system are people with language delaying handicaps such as deafness, and people learning English as a foreign language. For these populations, extensive exposure to standard English constructions (negatives, questions, relativization, etc.) and their interactions is necessary. The purpose of the generator was to serve as a powerful resource for tutorial programs that need examples of particular constructions and/or related sentences to embed in exercises or examples for the student. The focus of the generator is thus not so much on what to express as on how to express it in acceptable English. The generator is composed of three major parts: a base component that produces base trees, a transformer that applies transformational rules to the trees to derive a surface tree, and a set of mechanisms to control the operation of the first two components. We discuss each of these components separately.</abstract>
<title confidence="0.9811585">Transportable Natural-Language Interfaces to Databases</title>
<author confidence="0.994245">Gary G Hendrix</author>
<author confidence="0.994245">William H Lewis</author>
<affiliation confidence="0.999858">Artificial Intelligence Center SRI International</affiliation>
<address confidence="0.997821">333 Ravenswood Avenue Menlo Park, California 94025</address>
<abstract confidence="0.9813726875">Proc. 19th Annual ACL Conf., June 1981, 159-162. Over the last few years a number of application systems have been constructed that allow users to access databases by posing questions in natural languages, such as English. When used in the restricted domains for which they have been especially designed, these systems have achieved reasonably high levels of performance. Such systems as LADDER, PLANES, ROBOT, and REL require the encoding of knowledge about the domain of application in such constructs as database schemata, lexicons, pragmatic grammars, and the like. The creation of these data structures typically requires considerable effort on the part of a computer professional who has had special training in computational linguistics and the use of databases. Thus, the utility of these systems is severely limited by the high cost involved in developing an interface to any particular database. This paper describes initial work on a methodology for creating natural-language processing capabilities for new domains without the need for intervention by specially trained experts. Our approach is to acquire logical schemata and lexical information through simple interactive dialogues with someone who is familiar with the form and content of the database, but unfamiliar with the technology of natural-language interfaces. To test our approach in an actual computer environment, we have developed a prototype system called TED (Transportable English Datamanager). As a result of our experience with TED, the NL group at SRI is now undertaking the development of a much more ambitious system based on the same philosophy.</abstract>
<author confidence="0.689651">Chart Parsing</author>
<author confidence="0.689651">Rule Schemata in PSG</author>
<affiliation confidence="0.998828">Department of Artificial Intelligence University of Edinburgh</affiliation>
<author confidence="0.803142">Hope Park Square</author>
<address confidence="0.400009">Meadow Lane, Edinburgh EH8 9NW</address>
<abstract confidence="0.972457130434782">Proc. 19th Annual ACL Conf., June 1981, 167-172. MCHART is a flexible, modular chart parsing framework I have been developing (in LISP) at Edinburgh, whose initial design characteristics were largely determined by pedagogical needs. PSG is a grammatical theory developed by Gerald Gazdar at Sussex, in collaboration with others in both the U.S. and Britain, most notably Ivan Sag, Geoff Pullum, and Ewan Klein. It is a notationally rich context free phrase structure grammar, incorporating meta-rules and rule schemata to capture generalizations. In this paper I describe how I have used MCHART in beginning to construct a parser for grammars expressed in PSG, and how aspects of the chart parsing approach in general and MCHART in particular have made it easy to accommodate two significant aspects of PSG: rule schemata involving variables over categories, and compound category symbols (&amp;quot;slash&amp;quot; categories). To do this I briefly introduce the basic ideas of chart parsing, describe the salient aspects of MCHART, give an overview of PSG, and finally present the interesting aspects of the parser I am building for PSG using MCHART.</abstract>
<note confidence="0.945761">Journal of Computational Linguistics, Volume 7, Number 3, July-September 1981</note>
<title confidence="0.99484525">The FINITE STRING Newsletter Abstracts of Current Literature Generating Descriptions and Explanations: Applications to Questions about Database Structure</title>
<author confidence="0.999995">Kathleen R McKeown</author>
<affiliation confidence="0.832923">Computer and Information Science Moore School of Electrical Engineering D2 University of Pennsylvania</affiliation>
<address confidence="0.999026">Philadelphia, Pennsylvania 19104</address>
<note confidence="0.4853985">Technical Report MS-CIS-80-9, 1980. The research being proposed is within the area of</note>
<abstract confidence="0.999264894736842">natural language generation. The generation process may be regarded as consisting of two major phases of computation; the first will determine what is to be said and how it is to be said, and the second will translate that message from an internal representation to English. Emphasis will be placed on the problems involved in the first component of generation. The application for the generation of natural language is within a natural language interface to a database system. To date, the kinds of answers which can be generated by such systems are restricted to lists or tables of objects in the database. I am proposing generating responses to questions about the structure of the database. The kinds of responses that will be generated include descriptions of classes of objects in the database, differences between the classes, relations that hold between classes, information available in the database, and definitions of classes of objects, among others.</abstract>
<title confidence="0.884715666666667">A Technique for Managing the Lexicon in a Natural Language Interface to a Changing Data Base</title>
<author confidence="0.999883">S Jerrold Kaplan</author>
<author confidence="0.999883">Eric Mays</author>
<author confidence="0.999883">Aravind K Joshi</author>
<affiliation confidence="0.834188666666667">Computer and Information Science Moore School of Electrical Engineering D2 University of Pennsylvania</affiliation>
<address confidence="0.999067">Philadelphia, Pennsylvania 19104</address>
<pubnum confidence="0.314829">Technical Report MS-CIS-80-10, August 1979.</pubnum>
<abstract confidence="0.973106655172414">A difficulty in designing a Natural Language (NL) interface to a Data Base (DB) management system is insuring that DB updates do not obsolete the NL com- Of particular concern is the list of words that the system can process), mainly because NL queries can contain terms that appear as values in the DB, and hence are subject to change as the contents of the DB changes. For example, to process the question &amp;quot;Does John Jones still work for the company?&amp;quot;, it is necessary to identify the string &amp;quot;John Jones&amp;quot; as a (potential) value in the &amp;quot;EMPLOYEE- NAME&amp;quot; field (assuming a suitable DB). If such names must appear in the lexicon for the system to process the query, then the lexicon will go out of date as the company&apos;s personnel shifts. Using the DB itself as an extension of the lexicon is equally problematic: the system will be unable to parse and provide a negative answer to such a question if the name does not appear at all in the DB. The approach suggested here is to infer a plausible field from the context of the query and semantic information about the domain that is implicitly encoded in the structure of the DB. This technique has been implemented in CO-OP, a NL DB query system that provides cooperative responses and operates with a typical CODASYL DB system. CO-OP treats the problem of selecting a plausible field as a special case resolving ambiguities. drawn from the implementation are presented.</abstract>
<title confidence="0.795819666666667">Centered Logic: The Role of Entity Centered Sentence Representation in Natural Language Inferencing.</title>
<author confidence="0.997178">Aravind K Joshi</author>
<author confidence="0.997178">Steve Kuhn</author>
<affiliation confidence="0.833877333333333">Computer and Information Science Moore School of Electrical Engineering D2 University of Pennsylvania</affiliation>
<address confidence="0.999119">Philadelphia, Pennsylvania 19104</address>
<abstract confidence="0.9561746">Technical Report MS-CIS-80-11, August 1979. We will briefly describe the role of entity centered structure (ECS) of sentences in natural language inferencing. The basic structure in discourse generally singles out an entity, to be called center, among all those which are the arguments of the main predicate. ECS makes n-ary predicates look like monadic ones by temporarily masking their structure, thereby affecting the relative ease with which certain inferences are made and information is retrieved. This short paper deals with a preliminary formulation of a system designed to capture these ideas and contains several examples of how some natural language inferences can be represented in the system. Formal properties of the system are under investigation.</abstract>
<title confidence="0.9970385">Paraphrasing Using Given and New Information in a Question-Answer System</title>
<author confidence="0.999996">Kathleen R McKeown</author>
<affiliation confidence="0.832924666666667">Computer and Information Science Moore School of Electrical Engineering D2 University of Pennsylvania</affiliation>
<address confidence="0.999089">Philadelphia, Pennsylvania 19104</address>
<abstract confidence="0.952946333333333">Technical Report MS-CIS-80-13, 1980. The design and implementation of a paraphrase component for a natural language question-answer system (CO-OP) is presented. A major point made is the role of given and new information in formulating a paraphrase that differs in a meaningful way from the user&apos;s question. A description is also given of the transformational grammar used by the paraphraser to generate questions.</abstract>
<note confidence="0.952781">Journal of Computational Linguistics, Volume 7, Number 3, July-September 1981</note>
<title confidence="0.995349333333333">The FINITE STRING Newsletter Abstracts of Current Literature Natural Language Interaction with Dynamic Knowledge Bases: Monitoring as Response</title>
<author confidence="0.999179">E Mays</author>
<author confidence="0.999179">S Lanka</author>
<author confidence="0.999179">A K Joshi</author>
<author confidence="0.999179">B L Webber</author>
<affiliation confidence="0.831516333333333">Computer and Information Science Moore School of Electrical Engineering D2 University of Pennsylvania</affiliation>
<address confidence="0.999105">Philadelphia, Pennsylvania 19104</address>
<abstract confidence="0.97571975">Technical Report MS-CIS-80-46, 1980. In this communication, we discuss an interesting aspect of natural language interaction with dynamically changing knowledge bases the ability to monitor for relevant future changes in that knowledge. We also indicate the status of our current work in this area and the overall goals of our research on questionanswering and monitoring dynamic knowledge bases.</abstract>
<title confidence="0.861042">Control of Inference: Role of Some Aspects of Discourse Structure-Centering</title>
<author confidence="0.999417">Aravind K Joshi</author>
<author confidence="0.999417">Scott Weinstein</author>
<affiliation confidence="0.831561666666667">Computer and Information Science Moore School of Electrical Engineering D2 University of Pennsylvania</affiliation>
<address confidence="0.999096">Philadelphia, Pennsylvania 19104</address>
<abstract confidence="0.970269545454546">Technical Report MS-CIS-80-47, 1980. The purpose of this communication is to examine one particular aspect of discourse structure, namely, a construct called a sentence its relation to the larger issue of control of inference. We have described very briefly the notion of center(s) of a sentence in discourse and discussed how the centering phenomenon might be incorporated in a formal model of inference and its relation to the intrinsic complexity of certain inferences.</abstract>
<title confidence="0.959418">Varieties of Cooperative Responses in Question-Answer Systems</title>
<author confidence="0.99995">Aravind K Joshi</author>
<affiliation confidence="0.834196666666667">Computer and Information Science Moore School of Electrical Engineering D2 University of Pennsylvania</affiliation>
<address confidence="0.999101">Philadelphia, Pennsylvania 19104</address>
<abstract confidence="0.983170076923077">Technical Report MS-CIS-80-48, 1980. In this paper, certain types of cooperative responses desirable in question-answer systems in a data base environment have been briefly reviewed. In particular, cooperative responses have been considered dealing with the situation when the user&apos;s view and the system&apos;s view of the structure and/or content of the data base are disparate. Responses explaining the structure of the data base as well as responses which are implicit requests for monitoring states of dynamic data bases have also been discussed. Finally, a general formulation of a particular type of cooperative behavior has been briefly described.</abstract>
<title confidence="0.8410885">Phrase Structure Trees Bear More Fruit Than You Would Have Thought</title>
<author confidence="0.99989">Aravind K Joshi</author>
<affiliation confidence="0.834198666666667">Computer and Information Science Moore School of Electrical Engineering D2 University of Pennsylvania</affiliation>
<address confidence="0.999159">Philadelphia, Pennsylvania 19104</address>
<abstract confidence="0.968265533333333">Technical Report MS-CIS-80-49, 1980. Several results concerning phrase structure trees have been presented. These results show that phrase structure trees when viewed in certain ways have much more descriptive power than one would have thought. A brief account of local constraints on structural descriptions and an intuitive proof have been presented. The local constraints approach has been compared to some aspects of Gazdar&apos;s framework and that of Peters and Karttunen. Some results on skeletons (phrase structure trees without labels) have been presented also. It can be shown that phrase structure trees even when deprived of the labels retain in a certain sense all the structural information. This result has implications for grammatical inference procedures.</abstract>
<title confidence="0.9969665">Parasession on Topics in Interactive Discourse: Influence of the Problem Text</title>
<author confidence="0.999952">Aravind K Joshi</author>
<affiliation confidence="0.834197666666667">Computer and Information Science Moore School of Electrical Engineering D2 University of Pennsylvania</affiliation>
<address confidence="0.999099">Philadelphia, Pennsylvania 19104</address>
<abstract confidence="0.966064692307693">Technical Report MS-CIS-80-50, 1980. This paper was presented in the parasession on topics in interactive discourse, which was a special session organized in conjunction with the 18th Annual Meeting of the Association for Computational Linguistics (June, 1980, Philadelphia). The paper consists of comments in response to several issues raised by Barbara Grosz, the panel chairperson. All the comments pertain to the primary issue of how the purpose of the interaction of the problem context affects what is said and how it is interpreted. Wherever possible, the issues are discussed more in the context of &amp;quot;information seeking&amp;quot; interaction and the data base domain.</abstract>
<title confidence="0.990589">Mutual Beliefs in Question-Answer Systems</title>
<author confidence="0.999928">Aravind K Joshi</author>
<affiliation confidence="0.834196">Computer and Information Science Moore School of Electrical Engineering D2 University of Pennsylvania</affiliation>
<address confidence="0.99909">Philadelphia, Pennsylvania 19104</address>
<abstract confidence="0.97728032">Technical Report MS-CIS-80-51, 1980. In this paper, we will briefly discuss the role of mutual beliefs with respect to some specific aspects of man-machine interactions. Cooperative or helpful behavior can be defined in various ways; however in this paper we will deliberately limit ourselves to some Journal of Computational Linguistics, Volume 7, Number 3, July-September 1981 The FINITE STRING Newsletter Abstracts of Current Literature very particular aspects of cooperation and relate them to the more technical definitions of mutual beliefs. We will be particularly concerned with the case where cooperativeness involves both giving a truthful and informative response and &amp;quot;squaring away&amp;quot; the relevant mutual beliefs. In this context, we will suggest that the &amp;quot;squaring away&amp;quot; contribution of the interaction can be explained by a suitable modification of one of the maxims of cooperative conversation. We also discuss a related topic dealing with excess (or surplus) information and how and when the system should assimilate this information, i.e., implicitly update itself without being explicitly told to do so. These considerations require that mutual beliefs should not be regarded just as a set of propositions but rather with some structure over them where the structuring of information itself is a mutual belief.</abstract>
<title confidence="0.884008">Correcting Misconceptions About Data Base Structure</title>
<author confidence="0.999344">Eric Mays</author>
<affiliation confidence="0.832455666666667">Computer and Information Science Moore School of Electrical Engineering D2 University of Pennsylvania</affiliation>
<address confidence="0.968156">Pennsylvania</address>
<abstract confidence="0.973948333333333">Technical Report MS-CIS-80-52, 1980. This paper presents a method for computation of intensional failures of presumptions in queries to a natural language interface to a data base system. These failures are distinguished from extensional failures since they are dependent on the structure rather than the content of the data base. A knowledge representation has been investigated that can be used to recognize intensional failures. When intensional failures are detected, a form of corrective behavior is proposed to inform the user about possibly relevant data base structure that is related to the failure.</abstract>
<title confidence="0.682756">BORIS -- An Experiment in In-Depth Understanding of Narratives</title>
<author confidence="0.9958505">Wendy Lehnert</author>
<author confidence="0.9958505">Michael G Dyer</author>
<author confidence="0.9958505">Peter N Johnson</author>
<author confidence="0.9958505">C J Yang</author>
<author confidence="0.9958505">Steve Harley</author>
<affiliation confidence="0.999778">Department of Computer Science</affiliation>
<address confidence="0.979771">Box 2158 Yale Station</address>
<affiliation confidence="0.875706">Yale University</affiliation>
<address confidence="0.997696">New Haven, Connecticut 06520</address>
<abstract confidence="0.9881175">Research Report 188, January 1981, 74 pages. BORIS is a story understanding and question answering system which involves the specification and interaction of many sources of knowledge. Unlike skimmers, which simply extract the &amp;quot;gist&amp;quot; of a story in a top-down manner and ignore everything else, BORIS attempts to understand everything that it reads to as great a depth as possible. This report focuses on how the BORIS program handles a complex story involving a divorce.</abstract>
<title confidence="0.942964">Conceptual Information Retrieval</title>
<author confidence="0.998444">Roger Schank</author>
<author confidence="0.998444">Janet Kolodner</author>
<author confidence="0.998444">Gerald DeJong</author>
<affiliation confidence="0.999957">Department of Computer Science</affiliation>
<address confidence="0.979818">Box 2158 Yale Station</address>
<affiliation confidence="0.875759">Yale University</affiliation>
<address confidence="0.997922">New Haven, Connecticut 06520</address>
<abstract confidence="0.99517159375">Research Report 190, December 1980, 45 pages. If we want to build intelligent information retrieval systems, we will have to give them the capabilities of understanding natural language, automatically organizing and reorganizing their memories, and using intelligent heuristics for searching their memories. These systems will have to analyze and understand both new text and natural language queries. In answering questions, they will have to direct memory search to reasonable places. This requires good organization of both the conceptual content of texts and knowledge necessary for understanding those texts and accessing memory. The CYRUS and FRUMP systems (Kolodner (1978), Schank and Kolodner (1979), DeJong (1979)) comprise an information retrieval system called CyFr. Together, they have the analysis and retrieval capabilities mentioned above. Frump analyzes news stories from the UPI wire for their conceptual content, and produces summaries of those stories. It sends summaries of stories about important people to CYRUS. CYRUS automatically adds those stories to its memory, and can then retrieve that information to answer questions posed to it in natural language. This paper describes the problems involved in building such an intelligent system. It proposes soluto some of those problems based on recent research in artificial intelligence and natural language processing, and describes the CyFr system, which implements those solutions. The solutions we propose and implement are based on a model of human understanding and memory retrieval.</abstract>
<title confidence="0.997657">Finding Objects With Given Spatial Properties</title>
<author confidence="0.99988">Drew McDermott</author>
<affiliation confidence="0.999937">Department of Computer Science</affiliation>
<address confidence="0.979813">Box 2158 Yale Station</address>
<affiliation confidence="0.875669">Yale University</affiliation>
<address confidence="0.99757">New Haven, Connecticut 06520</address>
<abstract confidence="0.9846888">Research Report 195, March 1981, 55 pages. An important class of queries for a spatial-retrieval system is the retrieval of an object given some desired properties, such as that it be located in a given region, or point in a given direction. Handling this kind of query requires using a discrimination tree, which breaks the space of possibilities down repeatedly, ultimately into manageable buckets. In a spatial system, a useful kind of retrieval is &amp;quot;spiral search,&amp;quot; in which the system searches the tree by starting from the &amp;quot;best&amp;quot; cell and gradually enlarging its attack. A good way of Journal of Computational Linguistics, Volume 7, Number 3, July-September 1981 The FINITE STRING Newsletter Abstracts of Current Literature implementing such a module is to create a domainindependent system, data-driven by specialized user functions. In this scheme, the user functions can decide how to index an object, how to search a node of the tree, and how to reorganize a node when it is not discriminated properly. Special problems arise when this program is applied to the domain of simple shaped objects; a shaped object can fall into more than one bucket, and a shaped object can have different lengths in different directions. The user must tell the system what frames of reference to use for computing object coordinates.</abstract>
<title confidence="0.9889265">A Temporal Logic for Reasoning About Processes and Plans</title>
<author confidence="0.9999">Drew McDermott</author>
<affiliation confidence="0.999937">Department of Computer Science</affiliation>
<address confidence="0.979813">Box 2158 Yale Station</address>
<affiliation confidence="0.87556">Yale University</affiliation>
<address confidence="0.997136">New Haven, Connecticut 06520</address>
<abstract confidence="0.9841115">Research Report 196, March 1981, 79 pages. Much previous work in artificial intelligence has neglected representing time in all its complexity. In particular, it has neglected continuous change and the indeterminacy of the future. To rectify this, I have developed a first-order temporal logic, in which it is possible to name and prove things about facts, events, plans, and world histories. In particular, the logic provides analyses of causality, continuous change in quantities, the persistence of facts (the frame problem), and the relationship between tasks and actions. It may be possible to implement a temporal-inference machine based on this logic, which keeps track of several &amp;quot;maps&amp;quot; of a time line, one per possible history.</abstract>
<title confidence="0.750952">What&apos;s the Point?</title>
<author confidence="0.992898">Roger C Schank</author>
<author confidence="0.992898">Gregg C Collins</author>
<author confidence="0.992898">Ernest Davis</author>
<author confidence="0.992898">Peter N Johnson</author>
<author confidence="0.992898">Steve Lytinen</author>
<author confidence="0.992898">Brian J Reiser</author>
<affiliation confidence="0.999919">Department of Computer Science</affiliation>
<address confidence="0.979806">Box 2158 Yale Station</address>
<affiliation confidence="0.875733">Yale University</affiliation>
<address confidence="0.997814">New Haven, Connecticut 06520</address>
<abstract confidence="0.985484285714286">Research Report 205, May 1981, 57 pages. Understanding dialogue usually requires determining the intent, or point, of the utterance. Finding the point serves to constrain further processing. We present a categorization of points, and we propose algorithms and heuristics for deriving the point of a given utterance.</abstract>
<title confidence="0.987633">MAGPIE: A Goal-Based Model of Conversation</title>
<author confidence="0.999882">Peter N Johnson</author>
<author confidence="0.999882">Scott P Robertson</author>
<affiliation confidence="0.999887">Department of Computer Science</affiliation>
<address confidence="0.979797">Box 2158 Yale Station</address>
<affiliation confidence="0.875739">Yale University</affiliation>
<address confidence="0.997837">New Haven, Connecticut 06520</address>
<abstract confidence="0.995527625">Research Report 206, May 1981, 105 pages. The importance of intention in conversation has been considered by many researchers in artificial intelligence and psychology. However, most models of conversation have been limited to pursuing the transfer of knowledge between the system and a user. We propose that conversational goals can address communication at a number of other levels, such as the conversants&apos; emotions, their relationship, and their attitudes. MAGPIE (Multiple Active Goal Processor in Interactive Exchanges) is a computer model of a conversant that acquires and pursues conversational goals at a number of levels, including the goal of seeking dominance in its relationship with the other conversant. At the heart of the program is a set of tracking procedures, each of which monitors a specific level of communication flow in a conversation. These procedures are coupled with a conversational goal planner which generates responses that simultaneously pursue a number of goals. Currently, MAGPIE is able to model a wife during a short marital dispute with her husband. Normative data from human subjects is presented which supports the conversational goals proposed in our analysis.</abstract>
<title confidence="0.993796">Knowledge Organization and Distribution for Medical Diagnosis</title>
<author confidence="0.999871">Fernando Gomez</author>
<author confidence="0.999871">B Chandraseka ran</author>
<affiliation confidence="0.9998825">Department of Computer and Information Science The Ohio State University</affiliation>
<address confidence="0.999928">Columbus, Ohio 43210</address>
<note confidence="0.4190215">Sys. Man Cyb. 11, 1 (Jan. 1981), 34-42. A diagnostician, when he arrives at a diagnosis or</note>
<abstract confidence="0.995885647058823">diagnoses, has invoked some concepts. They can be diseases, causes of them, or other notions that are relevant to the diagnosis. These concepts form a hierarchical structure similar to a botanical or zoological classification. The diagnostician&apos;s knowledge is distributed through this hierarchy. The concepts in the hierarchy provide the criteria to organize under them small pieces of knowledge represented in the form of production rules. Thus concepts may be viewed as clusters of production rules. They extend the capabilities of production rules to more complex problem solving situations. The rules under each concept are further organized into three subgroups: exclusionary, confirmatory, and recommendation rules. During the problem solving process, the concepts are taken as interact and communicate among themselves by means of a blackboard.</abstract>
<note confidence="0.936471">Journal of Computational Linguistics, Volume 7, Number 3, July-September 1981</note>
<title confidence="0.999222">The FINITE STRING Newsletter Abstracts of Current Literature Toward a Theory of Distributed Word Expert Natural Language Parsing</title>
<author confidence="0.999909">Chuck Rieger</author>
<author confidence="0.999909">Steve Small</author>
<affiliation confidence="0.9999845">Department of Computer Science University of Maryland</affiliation>
<address confidence="0.991049">College Park, Maryland 20742</address>
<note confidence="0.8685835">IEEE Trans. Sys. Man Cyb. 11, 1 (Jan. 1981), 43-51. An approach to natural language meaning-based</note>
<abstract confidence="0.984919466666667">parsing in which the unit of linguistic knowledge is the word rather than the rewrite rule is described. In the word expert parser, knowledge about language is distributed across a population of procedural experts, each representing a word of the language, and each an expert at diagnosing that word&apos;s intended usage in context. The parser is structured around a coroutine control environment in which the generator-like word experts ask questions and exchange information in coming to collective agreement on sentence meaning. The word expert theory is advanced as a better cognitive model of human language expertise than the traditional rule-based approach. The technical discussion is organized around examples taken from the prototype LISP system which implements parts of the theory.</abstract>
<title confidence="0.987119">Integrating Knowledge Sources for Computer &amp;quot;Understanding&amp;quot; Tasks</title>
<author confidence="0.99999">Richard E Cullingford</author>
<affiliation confidence="0.9999515">Department of Electrical Engineering and Computer Science University of Connecticut</affiliation>
<address confidence="0.995287">Storrs, Connecticut 06268</address>
<note confidence="0.9568945">IEEE Trans. Sys. Man Cyb. 11, 1 (Jan. 1981), 52-60. Recent research in artificial intelligence has identi-</note>
<abstract confidence="0.9976134">fied a number of knowledge sources which appear to be needed for effective automatic &amp;quot;understanding&amp;quot; of connected natural language speech and text. These include wordand phrase-level semantics, models for actors and objects, and inference techniques for using scriptor goal-oriented knowledge structures. A unified model of the understanding process can be defined using the distributed-computing viewpoint, provided some way can be found to integrate and control a collection of &amp;quot;experts,&amp;quot; each one associated with a certain kind of knowledge source. A technique is described, called hierarchical task management, for constructing computer language-processing systems comprising an arbitrary number of distinct, potentially distributed, processes. The technique is based upon the repeated activation and expansion of data structures called tasks, which define important components of the understanding process in terms of a controlled interaction among the experts. The tasks are maintained on several &amp;quot;agendas,&amp;quot; and are manipulated by a uniform monitor called the task manager. The process of task management is illustrated in a multiprocess story understander called a distributable script applier mechanism (DSAM), which reads and summarizes newspaper stories about plane crashes.</abstract>
<title confidence="0.992551">of Story Grammars</title>
<author confidence="0.943452">M</author>
<author confidence="0.943452">Donald Perlis</author>
<affiliation confidence="0.995157">Department of Computer Science Mathematical Sciences Building The University of Rochester</affiliation>
<address confidence="0.998469">Rochester, New York 14627</address>
<abstract confidence="0.9620597">Cognitive Science 5, 1 (Jan.-March 1981), 79-86. Black and Wilensky (1979) have made serious methodological errors in analyzing story grammars, and in the process they have committed additional errors in applying formal language theory. Our arguments involve clarifying certain aspects of knowledge representation crucial to a proper treatment of story understanding. Particular criticisms focus on the following shortcomings of their presentation: (1) an erroneous statement from formal language theory, (2) misapplication of formal language theory to story grammars, (3) unsubstantiated and doubtful analogies with English grammar, (4) various non sequiturs concerning the generation of non-stories, (5) a false claim based on the artificial distinction between syntax and semantics, and (6) misinterpretation of the role of story grammars in story understanding. We conclude by suggesting appropriate criteria for the evaluation of story grammars.</abstract>
<title confidence="0.997088">A Model for Planning in Complex Situations</title>
<author confidence="0.99998">Robert Wilensky</author>
<affiliation confidence="0.9984435">Electronics Research Laboratory Computer Science Division Department of EECS University of California, Berkeley</affiliation>
<address confidence="0.999841">Berkeley, California 94720</address>
<abstract confidence="0.977583605263158">Memorandum UCB1ERL M81149, June 1981, 34 pages. A model of planning applicable to complex, commonplace activities is being developed. This model differs from previous approaches in that it is based on the following assumptions: (1) a planning agent must be able to infer its own goals in addition to being able to generate plans for these goals; (2) everyday planning is primarily concerned with reasoning about the between plans and goals; (3) metaknowledge about how to plan abstract plans and goals, and having the planner use this knowledge to solve its own planning problems) is as a driving principle; (4) hypothetical futures based on current plans and world knowledge) is used to infer goals and debug plans; and (5) planning knowledge should be equally available for understanding as well as for planning. Coupled together, the mechanisms for implementing these features give rise to a system of considerable Journal of Computational Linguistics, Volume 7, Number 3, July-September 1981 The FINITE STRING Newsletter Abstracts of Current Literature power. For example, the ability to infer one&apos;s own goals is needed in an autonomous planning agent since it must deal with unexpected situations. However, together with meta-planning knowledge and the ability to project hypothetical futures, this feature enables the planner to detect and reason about complicated goal interactions, anticipate problems with proposed plans, and make choices in the face of competing alternatives. This model has been developed in detail for the detection and resolution of goal conflicts. In doing so, we postulate several goal conflict resolution strategies, or meta-plans, called RE-PLAN, CHANGE- CIRCUMSTANCE, and SIMULATE-AND-SELECT. The structure and application of these meta-plans is explored in the context of both decision making and understanding the actions of other planners.</abstract>
<title confidence="0.96524">It&apos;s for Your Own Good: A Note on Inaccurate Reference</title>
<author confidence="0.999712">C Raymond Perrault</author>
<affiliation confidence="0.99998">Department of Computer Science University of Toronto</affiliation>
<address confidence="0.99949">Toronto, Ontario CANADA M5S 1A7</address>
<author confidence="0.999959">Philip R Cohen</author>
<affiliation confidence="0.9999555">Department of Computer Science Oregon State University</affiliation>
<address confidence="0.998919">Corvallis, Oregon 97331</address>
<note confidence="0.907917666666667">BBN Report 4723, 1981. (Also appears in Elements of Discourse Understanding, Joshi, Webber, and Sag (Eds.), Cambridge Univ. Press,</note>
<date confidence="0.718934">1981.</date>
<abstract confidence="0.998752083333333">his book suggests that his analysis of illocutionary acts can be extended to account for the &amp;quot;propositional act&amp;quot; of reference. He wants to give necessary conditions for a speaker successfully referring to some entity by using a certain referring expression in an utterance to a certain hearer. We point out here some inadequacies in Searle&apos;s conditions, in particular how they fail to account for cases of successful reference through expressions which speaker (and hearer) may believe are not true of their intended referent. More specific conditions based on the notion of mutual belief are proposed.</abstract>
<title confidence="0.985135">Stories within Stories</title>
<author confidence="0.999876">Bertram Bruce</author>
<affiliation confidence="0.995053">Bolt Beranek and Newman Inc.</affiliation>
<address confidence="0.9998195">50 Moulton Street Cambridge, Massachusetts 02138</address>
<abstract confidence="0.99511">BBN Report 29, August 1981, 17 pages. What appears to be a single story is often a complex set of stories within stories, each with its distinct author and reader. Examples of such stories within stories are presented. Results of analyses of basal readers and trade books in terms of embedded stories are also discussed. These suggest that a greater variety of stories could and should be made available to children.</abstract>
<title confidence="0.994261">Higher-Level Features in Children&apos;s Stories: Rhetorical Structure and Conflict</title>
<author confidence="0.999884">Cindy Steinberg</author>
<author confidence="0.999884">Bertram Bruce</author>
<affiliation confidence="0.995048">Bolt Beranek and Newman Inc.</affiliation>
<address confidence="0.9998255">50 Moulton Street Cambridge, Massachusetts 02138</address>
<abstract confidence="0.999127333333333">BBN Report 18, October 1980, 25 pages. Traditional surveys of children&apos;s literature have examined features such as text structure and topic, but have failed to take into account rhetorical elements such as author-reader distance, commentary, point of view, and insight into characters&apos; minds. Similarly, they have glossed over aspects of character-tocharacter interaction such as responses to interpersonal conflict. These &amp;quot;higher-level features&amp;quot; of stories may be what makes stories interesting to read. They are also principal contributors to story complexity, and hence, to difficulty for beginning readers. With regard to both interestingness and complexity, it is important to come to a better understanding of these features. To concretize our discussion, we first present two examples showing the importance of higher-level text features. Second, we sketch a theory of higher-level story features. Then, we briefly describe how we are applying our analysis to a selection of children&apos;s stories. Finally, we discuss some implications of this work.</abstract>
<title confidence="0.9926345">Strategies for Controlling Hypothesis Formation in Reading</title>
<author confidence="0.999203">Bertram Bruce</author>
<author confidence="0.999203">Andee Rubin</author>
<affiliation confidence="0.996947">Bolt Beranek and Newman Inc.</affiliation>
<address confidence="0.9996885">50 Moulton Street Cambridge, Massachusetts 02138</address>
<abstract confidence="0.928671571428571">Report 22, June 1981, 40 Reading is a process of forming and evaluating hypotheses to account for the data in a text. Because of its complexity, the task of reading requires strategies for controlling the proliferation of hypotheses. Four of these strategies, (a) jumping to conclusions, (b) maintaining inertia, (c) relying on background knowledge, and (d) working backwards from the goal, are generally effective, but they occasionally create reading problems, rather than alleviating them. Examples from protocols of readers reading a reading test passage are presented. These examples show both the effective use of the strategies and some problems that may arise from their use.</abstract>
<note confidence="0.937556">Journal of Computational Linguistics, Volume 7, Number 3, July-September 1981</note>
<title confidence="0.999291">The FINITE STRING Newsletter Abstracts of Current Literature A New Point of View on Children&apos;s Stories</title>
<author confidence="0.999916">Bertram Bruce</author>
<affiliation confidence="0.995053">Bolt Beranek and Newman Inc.</affiliation>
<address confidence="0.9997975">50 Moulton Street Cambridge, Massachusetts 02138</address>
<abstract confidence="0.984841133333333">BBN Report 25, July 1981, 47 pages. Recent work on text analysis at the Center for the Study of Reading and elsewhere has produced surprising results regarding the texts that children read in school. These results support the hypothesis that part of the difficulty children encounter in making the transition from beginning to skilled readings lies in an abrupt shift in text characteristics between lower and upper elementary school. Moreover, a comparison between school texts and popular trade books shows that the school texts may provide inadequate preparation for the texts that skilled readers need to master. Thus, characteristics of the texts that children are expected to read may hinder rather than help in the attainment of educational goals.</abstract>
<title confidence="0.995839">Processing Models for Children&apos;s Story Comprehension</title>
<author confidence="0.999593">Robert de_Beaugrande</author>
<author confidence="0.999593">Genevieve W Miller</author>
<affiliation confidence="0.9687785">English Department University of Florida</affiliation>
<address confidence="0.997853">Gainesville, Florida 32611</address>
<abstract confidence="0.98245825">Poetics 9 (1980), 181-201. It is argued that in abandoning trace-abstraction models of story comprehension in favor of schemabased ones, we have left some issues unresolved in regard to the treatment of bottom-up input. Using a real children&apos;s story employed in empiric tests, we review evidence that schemas undergo specification and modification via bottom-up input as the story is read: a phenomenon describable as proattachment. seems to be some traceabstraction during comprehension but in a peripheral role and with unreliable or inconsistent results across a test population. We conclude that accuracy of recall is a less crucial question than the identification of strategies active during recall of both abstractive and constructive nature.</abstract>
<title confidence="0.738273">The Pragmatics of Discourse Planning Robert-Alain de Beaugrande</title>
<author confidence="0.620602">English Department</author>
<affiliation confidence="0.999964">University of Florida</affiliation>
<address confidence="0.993182">Gainesville, Florida 32611</address>
<note confidence="0.675936">Journal of Pragmatics 4 (1980), 15-42.</note>
<abstract confidence="0.98573428">It is argued that the basic notions of natural language pragmatics cannot be the same as those of syntax and semantics as developed so far. Instead, pragmatics must be an empirically oriented theory of action and interaction. The role of sentences and predications is secondary. The most promising approaches for such a pragmatics are: (1) conceptual dependency theory, in which language is a form of actions specified by goaldirected plans (e.g. Schank); (2) plan theory, in which the analysis of tasks and resources leads to the specification of a planned sequence of steps (e.g. Sacerdoti); (3) problem-solving theory, in which points or states in a problem space have to be connected by a successful pathway (e.g. Newell and Simon); and (4) procedural theory of discourse, in which language elements and systems are investigated with respect to how people utilize them in communication and processing. The paper offers the framework of a natural language pragmatics along these lines and applies the resulting theory to a study of a scene from a stage play by Sidney Howard. It is shown that the actions and discourse actions of the scene are indeed generated by the characters&apos; plans and goals.</abstract>
<title confidence="0.9567375">Linguistic Theory and Metatheory for a Science of Texts</title>
<author confidence="0.985337">Robert De_Beaugrande</author>
<affiliation confidence="0.835207">English Department University of Florida</affiliation>
<address confidence="0.998789">Gainesville, Florida 32611</address>
<abstract confidence="0.98708675">Text 1, 2 (1981), 113-161. This article explores the typical reactions which occur when an established science confronts a new object of inquiry, as we find when linguistic theory encounters the text. The usual discussions are not productive as long as the old &amp;quot;paradigm&amp;quot; is still accepted as the framework for achievement. The issues are therefore re-examined in terms of the metatheory of science (e.g. Sneed, Stegmtiller, Lakatos, Feyerabend, Hempel), and some general solutions are expounded for the problems of validating theories on the basis of empirical content. A paradigmatic example is then presented in order to show a possible role for logical linguistics in future theories: a computer grammar that parses text sentences into a progressive network and back again via theorem-proving, with further capacities for applying schemas, answering questions, and generating summaries. This example serves as an application of general design values and criteria for preferring and comparing alternative theories.</abstract>
<title confidence="0.905661">Theoretical Foundations of the Automatic Production and Processing of Technical Reports</title>
<author confidence="0.974851">Robert de_Beaugrande</author>
<affiliation confidence="0.834357">English Department University of Florida</affiliation>
<address confidence="0.997338">Gainesville, Florida 32611</address>
<note confidence="0.737094">J. Technical Writing and Communication 9, 3 (1979). 202 American Journal of Computational Linguistics, Volume 7, Number 3, July-September 1981</note>
<title confidence="0.745134">The FINITE STRING Newsletter Abstracts of Current Literature</title>
<abstract confidence="0.972767818181818">The following treatise surveys the issues and approaches for designing a computer system capable of reading, understanding, and writing technical reports. Recent progress in computer science and artificial intelligence research is used to specify the nature of the modules in the system. The processing of a sample text is observed during the phases of reading and writing a report on the origin of sunspots. The author advances some proposals for correlating syntax and semantics of English from a procedural standpoint. The discussion is illustrated with structural diagrams.</abstract>
<title confidence="0.9874585">Checking for Spelling and Typographical Errors in Computer-Based Text</title>
<author confidence="0.999774">Thomas N Turba</author>
<affiliation confidence="0.889013333333333">Sperry Univac Language Systems Roseville Development Center</affiliation>
<address confidence="0.999545">Roseville, Minnesota 55113</address>
<abstract confidence="0.9780556">Sigplan Notices 16, 6 (June 1981), 51-60. This paper addresses the problems and techniques of checking for spelling and typographical errors in computer-based text. To some extent, the paper is a combination of a report of work done by the author and a survey of other work which, although not all used by the author, is of equal value and interest. Some of the material presented is related to other aspects of text processing such as data compaction and the efficient searching of very large dictionaries.</abstract>
<title confidence="0.696615">Computer Aids for Writers</title>
<author confidence="0.612132">Lorinde Cherry</author>
<affiliation confidence="0.991882">Bell Telephone Laboratories</affiliation>
<address confidence="0.9989665">600 Mountain Avenue Murray Hill, New Jersey 07974</address>
<abstract confidence="0.963833">Sigplan Notices 16, 6 (June 1981), 61-67. For many people, writing is painful and editing one&apos;s own prose is difficult, tedious, and error-prone. It is often hard to see which parts of a document are difficult to read or how to transform a wordy sentence into a more concise one. It is even harder to discover that one overuses a particular linguistic construct. The system of programs described here helps writers to evaluate documents and to produce better written and more readable prose. The system consists of programs to measure surface features of text that are important to good writing style as well as programs to do some of the tedious jobs of a copy editor. Some of the surface features measured are readability, sentence and word length, sentence type, word usage, and sentence openers. The copy editing programs find spelling errors, wordy phrases, bad diction, some punctuation errors, double words, and split infinitives.</abstract>
<title confidence="0.996783">Data-Base and Query Systems: New and Simple Ways to Gain Multiple Views of the Patterns in Text</title>
<author confidence="0.999939">Linda D Misek-Falkoff</author>
<affiliation confidence="0.999572">IBM Thomas J. Watson Research Center</affiliation>
<address confidence="0.905485">P.O. Box 218 Yorktown Heights, New York 10598</address>
<abstract confidence="0.995606684210526">Research Report RC 8769, March 1981, 52 pages. The goal of this paper is to suggest a spontaneous, &amp;quot;problem solving&amp;quot; approach to textual analysis, in a user-friendly milieu made possible by modern database facilities. The study of language is ubiquitous, with interactive systems increasingly supporting studies which previously might have been run in batch mode: the language scholar today is less reliant on mediating technical support. He can enter and query his data directly. He can, but need not, be a &amp;quot;programmer,&amp;quot; and can personally control the degree of complexity entailed by his studies, according to his own felt needs and goals. This freedom of inquiry should greatly aid interpretation and inference, since the scholar can more easily alter his methods and critical hypotheses. Interesting connections between language studies at large and the practice of data-base administration may be gleaned, and the growing concord between humanism and technology further enhanced.</abstract>
<title confidence="0.9766605">A Knowledge Engineering Approach to Natural Language Understanding</title>
<author confidence="0.999949">Jeannette G Neal</author>
<affiliation confidence="0.999682">Department of Computer Science State University of New York at Buffalo</affiliation>
<address confidence="0.998705">4226 Ridge Lea Road Amherst, New York 14226</address>
<abstract confidence="0.960328842105263">Technical Report 179, June 1981. This paper presents the results of preliminary study of a knowledge engineering approach to natural language understanding. The KE system used is the SNePS semantic network processing system (Shapiro, 1979a). As a part of the study, a SNePS front-end system, called the NL-system, was implemented to enable the NLU expert to enter linguistic knowledge into the network in natural language and experiment with this base of knowledge. This paper discusses the capabilities of the SNePSbased NL-system and illustrates these capabilities by example. The NL-system enables the NLU userexpert to: (a) input his rules about NLU and his lexicon into the semantic network knowledge base in natural language; (b) trace the inference processes, which utilize his rules, in natural language; and (c) employ an experimental rule-based generator to express knowledge built into the network via his NLU rules.</abstract>
<note confidence="0.870943">Journal of Computational Linguistics, Volume 7, Number 3, July-September 203</note>
<title confidence="0.871228">The FINITE STRING Newsletter Abstracts of Current Literature</title>
<abstract confidence="0.984246875">An example is presented to illustrate: the representation of both surface strings and semantic knowledge in the network; the input and representation of rules; how rules build conceptual structures from surface strings; natural language tracing of an NLU process; the use of knowledge for disambiguating an earlier, partially understood sentence; and rule-based generation.</abstract>
<title confidence="0.9859875">Combining Path-Based and Node-Based Inference In SNEPS</title>
<author confidence="0.999989">Rohini K Srihari</author>
<affiliation confidence="0.9996885">Department of Computer Science State University of New York at Buffalo</affiliation>
<address confidence="0.9992095">4226 Ridge Lea Road Amherst, New York 14226</address>
<abstract confidence="0.998395666666667">Technical Report 183, June 1981, 52 pages. This paper describes a recent enhancement made to SNePS to include path-based inference. The previous version of SNePS used only node-based inference and was thus limited in its capabilities. Combining pathbased inference and node-based inference in SNePS has greatly increased the scope of the system. The paper first motivates the need for such a change and then proceeds to outline the theory as well as the implementation details. Finally, a series of examples is provided which supports the claim that the system is indeed more powerful and efficient now.</abstract>
<title confidence="0.990226">Formal Semantics for Time in Databases</title>
<author confidence="0.999957">James Clifford</author>
<author confidence="0.999957">David S Warren</author>
<affiliation confidence="0.998149">Department of Computer Science State University of New York at Stony Brook</affiliation>
<address confidence="0.919526">Stony Brook, New York 11794</address>
<note confidence="0.452651">Technical Report 811025, June 1981.</note>
<abstract confidence="0.99806125">concept of an database introduced as a tool for modelling the dynamic nature of some part of the real world. Just as first-order logic has been shown to be a useful formalism for understanding the underlying semantics of the relational database model, intensional logic is presented as an analogous formalism for understanding the temporal semantics involved in an historical database. The various components of the relational model, as extended to include historical relations, are discussed in terms of the model theory for the logic IL formulated by Richard Mon- The concepts of data constraints and queries are introduced and contrasted. Finally, the potential application of these ideas to the problem of natural language database querying is discussed.</abstract>
<title confidence="0.9895575">More Notes on an Intensional Logic for English: Reversing Extensional Forms</title>
<author confidence="0.999984">Keith Brian Gallagher</author>
<affiliation confidence="0.999972">Department of Computer and Communication Sciences</affiliation>
<address confidence="0.999142">221 Angell Hall</address>
<affiliation confidence="0.998189">The University of Michigan</affiliation>
<address confidence="0.989088">Ann Arbor, Michigan 48109</address>
<note confidence="0.813443333333333">Computer Studies in Formal Ling. N-25, May 1981. In a series of papers, Joyce Friedman and David Warren gave sufficient conditions for obtaining the</note>
<title confidence="0.4186745">forms of words in Richard Montague&apos;s Proper Treatment of Quantification in Ordinary English</title>
<abstract confidence="0.9735782">(PTQ). Given these extensionalized forms, one desires to return to the logically equivalent lambda normal form, as a prelude to obtaining the sentence that yielded this form. A procedure for doing this with a proof of its correctness is given.</abstract>
<title confidence="0.863327">Sentence Analysis Programs Based on</title>
<author confidence="0.878328">Montague Grammar</author>
<affiliation confidence="0.864280666666667">Bipin Indurkhya Philips International Institute of Technological Studies Eindhoven NETHERLANDS</affiliation>
<abstract confidence="0.9559868">Masters Thesis, 1981. The paper investigates computational aspects of the work in theoretical linguistics which is being done in the framework of &amp;quot;Montague Grammar.&amp;quot; Grammars in this framework give precise descriptions of the relation between the surface forms of English sentences, their syntactic structures, and their meanings as represented by logical formulas. On the basis of such grammars effective analysis programs for English sentences may be constructed.</abstract>
<title confidence="0.970901">Focalizers, the Scoping Problem, and Semantic Interpretation Rules in Logic Grammars</title>
<author confidence="0.999986">Michael C McCord</author>
<affiliation confidence="0.999969">Computer Science Department University of Kentucky</affiliation>
<address confidence="0.98087">Lexington, Kentucky 40506</address>
<note confidence="0.8947502">Technical Report 81-81, August 1981. (To appear in Proceedings of the International Workshop on Logic Programming for Intelligent Systems, August 1981, Logicon, Woodland Hills, Calif.) This paper deals with a system for semantic inter-</note>
<abstract confidence="0.977577">pretation of natural language within the framework of logic programming. Of special interest are a class of items called the problem of determining their scopes in logical form. Focalizers include quantificational determiners, certain adverbs, and abstract items relating to discourse structure.</abstract>
<note confidence="0.928605">Journal of Computational Linguistics, Volume 7, Number 3, July-September 1981</note>
<title confidence="0.999186">The FINITE STRING Newsletter Abstracts of Current Literature An Efficient Easily Adaptable System For Interpreting Natural Language Queries</title>
<author confidence="0.999976">David H D Warren</author>
<affiliation confidence="0.999691">Department of Artificial Intelligence University of Edinburgh</affiliation>
<address confidence="0.767431">Forrest Hill Edinburgh EH1 2QL SCOTLAND</address>
<author confidence="0.989594">Fernando C N Pereira</author>
<affiliation confidence="0.9968605">CAAD Studies, Department of Architecture University of Edinburgh</affiliation>
<address confidence="0.7655565">Forrest Hill Edinburgh EH1 2QL SCOTLAND</address>
<abstract confidence="0.995003176470588">DAI Research Paper 155, February 1981, 18 pages. This paper gives an overall account of a prototype natural language question answering system, call Chat- 80. Chat-80 has been designed to be both efficient and easily adaptable to a variety of applications. The system is implemented entirely in PROLOG, a programming language based on logic. With the aid of a logic-based grammar formalism called extraposition grammar, Chat-80 translates English questions into the PROLOG subset of logic. The resulting logical expression is then transformed by a planning algorithm into efficient PROLOG, cf. &amp;quot;query optimization&amp;quot; in a relational database. Finally the PROLOG form is executed to yield the answer. On a domain of world geography, most questions within the English subset are answered in well under one second, including relatively complex queries.</abstract>
<title confidence="0.9853">Some Problems In Early Noun Phrase Interpretation</title>
<author confidence="0.999554">C S Mellish</author>
<affiliation confidence="0.999778">Department of Artificial Intelligence University of Edinburgh</affiliation>
<address confidence="0.765058">Forrest Hill Edinburgh EH1 2QL SCOTLAND</address>
<abstract confidence="0.977698785714286">DAI Research Paper No. 147, 1980, 5 pages. How does a piece of text provide the information necessary for generating a symbolic &amp;quot;meaning&amp;quot; and how can a computer program be organized to pick up that information? The work described here aims to investigate some of the constraints on the timing of semantic interpretation. In particular, we are interested in seeing to what extent the meaning can be built up in an incremental way as the analysis proceeds from left to right. We look at some problems of noun phrase interpretation in such a scheme and indicate some representational ideas that help to overcome them. This paper is a brief summary of a forthcoming Ph.D thesis (Mellish 80).</abstract>
<title confidence="0.731154">Semantic Long Term Memory and the Understanding of Language</title>
<author confidence="0.996126">M Wettler</author>
<affiliation confidence="0.9800235">ISSCO Universite de Geneve</affiliation>
<address confidence="0.5547635">17 Rue de CandoIle CH-1205 Geneve SWITZERLAND</address>
<note confidence="0.6267595">Working Paper 37, 1978 (in German). This is a study of the formal representation of conceptual knowledge in relation to the understanding and production of utterances. After the initial historical</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Case in Linguistics</author>
<author>Cognitive Science M Rosner</author>
<author>H Somers ISSCO</author>
</authors>
<marker>Linguistics, Rosner, ISSCO, </marker>
<rawString>Case in Linguistics and Cognitive Science M. Rosner and H. Somers ISSCO</rawString>
</citation>
<citation valid="true">
<date>1980</date>
<booktitle>Universite de Geneve 17 Rue de CandoIle CH-1205 Geneve SWITZERLAND Working Paper 40,</booktitle>
<contexts>
<context position="24110" citStr="(1980)" startWordPosition="3624" endWordPosition="3624">ther a particular string is or is not a memAmerican Journal of Computational Linguistics, Volume 7, Number 3, July-September 1981 189 The FINITE STRING Newsletter Abstracts of Current Literature ber of the language generated by a certain lexical functional grammar. This paper also discusses the relevance of this technical result for more down-to-earth computational linguistics. Corepresentational Grammar and Parsing English Comparatives Karen Ryan Linguistics Department 142 Klaeber Court University of Minnesota Minneapolis, Minnesota 55455 Proc. 19th Annual ACL Conf., June 1981, 13-18. Marcus (1980) notes that the syntax of English comparative constructions is highly complex, and claims that both syntactic and semantic information must be available for them to be parsed. This paper argues that comparatives can be structurally analyzed on the basis of syntactic information alone via a strictly surface-based grammar. Such a grammar is given in Ryan (1981), based on the corepresentational model of Kac (1978). While the grammar does not define a parsing algorithm per se, it nonetheless expresses regularities of surface organization and its relationship to semantic interpretation that an adeq</context>
<context position="83161" citStr="(1980)" startWordPosition="12599" endWordPosition="12599">led readings lies in an abrupt shift in text characteristics between lower and upper elementary school. Moreover, a comparison between school texts and popular trade books shows that the school texts may provide inadequate preparation for the texts that skilled readers need to master. Thus, characteristics of the texts that children are expected to read may hinder rather than help in the attainment of educational goals. Processing Models for Children&apos;s Story Comprehension Robert de Beaugrande and Genevieve W. Miller English Department University of Florida Gainesville, Florida 32611 Poetics 9 (1980), 181-201. It is argued that in abandoning trace-abstraction models of story comprehension in favor of schemabased ones, we have left some issues unresolved in regard to the treatment of bottom-up input. Using a real children&apos;s story employed in empiric tests, we review evidence that schemas undergo specification and modification via bottom-up input as the story is progressively read: a phenomenon describable as procedural attachment. There seems to be some traceabstraction during comprehension but in a peripheral role and with unreliable or inconsistent results across a test population. We co</context>
</contexts>
<marker>1980</marker>
<rawString>Universite de Geneve 17 Rue de CandoIle CH-1205 Geneve SWITZERLAND Working Paper 40, 1980.</rawString>
</citation>
<citation valid="true">
<date>1981</date>
<booktitle>Universite de Geneve 17 Rue de CandoIle CH-1205 Geneve SWITZERLAND Working Paper 42,</booktitle>
<contexts>
<context position="24471" citStr="(1981)" startWordPosition="3681" endWordPosition="3681">tional linguistics. Corepresentational Grammar and Parsing English Comparatives Karen Ryan Linguistics Department 142 Klaeber Court University of Minnesota Minneapolis, Minnesota 55455 Proc. 19th Annual ACL Conf., June 1981, 13-18. Marcus (1980) notes that the syntax of English comparative constructions is highly complex, and claims that both syntactic and semantic information must be available for them to be parsed. This paper argues that comparatives can be structurally analyzed on the basis of syntactic information alone via a strictly surface-based grammar. Such a grammar is given in Ryan (1981), based on the corepresentational model of Kac (1978). While the grammar does not define a parsing algorithm per se, it nonetheless expresses regularities of surface organization and its relationship to semantic interpretation that an adequate parser would be expected to incorporate. The central problem in parsing comparatives involves identifying the arguments of comparative predicates, and the relations borne by these arguments to such predicates. A corepresentational grammar is explicitly designed to assign predicate-argument structure to sentences on the basis of their surface syntactic or</context>
<context position="85455" citStr="(1981)" startWordPosition="12970" endWordPosition="12970"> theory of discourse, in which language elements and systems are investigated with respect to how people utilize them in communication and processing. The paper offers the framework of a natural language pragmatics along these lines and applies the resulting theory to a study of a scene from a stage play by Sidney Howard. It is shown that the actions and discourse actions of the scene are indeed generated by the characters&apos; plans and goals. Linguistic Theory and Metatheory for a Science of Texts Robert De Beaugrande English Department University of Florida Gainesville, Florida 32611 Text 1, 2 (1981), 113-161. This article explores the typical reactions which occur when an established science confronts a new object of inquiry, as we find when linguistic theory encounters the text. The usual discussions are not productive as long as the old &amp;quot;paradigm&amp;quot; is still accepted as the framework for achievement. The issues are therefore re-examined in terms of the metatheory of science (e.g. Sneed, Stegmtiller, Lakatos, Feyerabend, Hempel), and some general solutions are expounded for the problems of validating theories on the basis of empirical content. A paradigmatic example is then presented in o</context>
</contexts>
<marker>1981</marker>
<rawString>Universite de Geneve 17 Rue de CandoIle CH-1205 Geneve SWITZERLAND Working Paper 42, 1981.</rawString>
</citation>
<citation valid="false">
<title>A Preliminary Study on Linguistic Implications of Resource Control in Natural Language Understanding</title>
<marker></marker>
<rawString>A Preliminary Study on Linguistic Implications of Resource Control in Natural Language Understanding</rawString>
</citation>
<citation valid="true">
<authors>
<author>J S Bien</author>
</authors>
<date>1980</date>
<booktitle>ISSCO Universite de Geneve 17 Rue de CandoIle CH-1205 Geneve SWITZERLAND Working Paper 44,</booktitle>
<marker>Bien, 1980</marker>
<rawString>J.S. Bien ISSCO Universite de Geneve 17 Rue de CandoIle CH-1205 Geneve SWITZERLAND Working Paper 44, 1980.</rawString>
</citation>
<citation valid="false">
<date></date>
<institution>Wilfrid Kettler and Arno Schmidt Technical University of Berlin</institution>
<location>Berlin, WEST</location>
<marker></marker>
<rawString>Wilfrid Kettler and Arno Schmidt Technical University of Berlin Berlin, WEST GERMANY</rawString>
</citation>
<citation valid="false">
<date></date>
<volume>15</volume>
<pages>0--6900</pages>
<institution>Magdalena Zoeppritz IBM Scientific Center Tiergartenstrasse</institution>
<location>Heidelberg, WEST</location>
<marker></marker>
<rawString>Magdalena Zoeppritz IBM Scientific Center Tiergartenstrasse 15 0-6900 Heidelberg, WEST GERMANY</rawString>
</citation>
<citation valid="true">
<date>1981</date>
<booktitle>Proceedings of the Workshop on Data Abstraction, Databases and Conceptual Modelling</booktitle>
<tech>Technical Report 81.01.001,</tech>
<pages>pages.</pages>
<contexts>
<context position="24471" citStr="(1981)" startWordPosition="3681" endWordPosition="3681">tional linguistics. Corepresentational Grammar and Parsing English Comparatives Karen Ryan Linguistics Department 142 Klaeber Court University of Minnesota Minneapolis, Minnesota 55455 Proc. 19th Annual ACL Conf., June 1981, 13-18. Marcus (1980) notes that the syntax of English comparative constructions is highly complex, and claims that both syntactic and semantic information must be available for them to be parsed. This paper argues that comparatives can be structurally analyzed on the basis of syntactic information alone via a strictly surface-based grammar. Such a grammar is given in Ryan (1981), based on the corepresentational model of Kac (1978). While the grammar does not define a parsing algorithm per se, it nonetheless expresses regularities of surface organization and its relationship to semantic interpretation that an adequate parser would be expected to incorporate. The central problem in parsing comparatives involves identifying the arguments of comparative predicates, and the relations borne by these arguments to such predicates. A corepresentational grammar is explicitly designed to assign predicate-argument structure to sentences on the basis of their surface syntactic or</context>
<context position="85455" citStr="(1981)" startWordPosition="12970" endWordPosition="12970"> theory of discourse, in which language elements and systems are investigated with respect to how people utilize them in communication and processing. The paper offers the framework of a natural language pragmatics along these lines and applies the resulting theory to a study of a scene from a stage play by Sidney Howard. It is shown that the actions and discourse actions of the scene are indeed generated by the characters&apos; plans and goals. Linguistic Theory and Metatheory for a Science of Texts Robert De Beaugrande English Department University of Florida Gainesville, Florida 32611 Text 1, 2 (1981), 113-161. This article explores the typical reactions which occur when an established science confronts a new object of inquiry, as we find when linguistic theory encounters the text. The usual discussions are not productive as long as the old &amp;quot;paradigm&amp;quot; is still accepted as the framework for achievement. The issues are therefore re-examined in terms of the metatheory of science (e.g. Sneed, Stegmtiller, Lakatos, Feyerabend, Hempel), and some general solutions are expounded for the problems of validating theories on the basis of empirical content. A paradigmatic example is then presented in o</context>
</contexts>
<marker>1981</marker>
<rawString>Technical Report 81.01.001, January 1981, 18 pages. Proceedings of the Workshop on Data Abstraction, Databases and Conceptual Modelling</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Michael</author>
</authors>
<date></date>
<institution>Brodie Computer Science Department University of Maryland College Park,</institution>
<location>Maryland</location>
<marker>Michael, </marker>
<rawString>Michael L. Brodie Computer Science Department University of Maryland College Park, Maryland 20742</rawString>
</citation>
<citation valid="false">
<authors>
<author>N Stephen</author>
</authors>
<journal>Zilles IBM Research Laboratory</journal>
<volume>5600</volume>
<note>Cottle Road</note>
<marker>Stephen, </marker>
<rawString>Stephen N. Zilles IBM Research Laboratory 5600 Cottle Road</rawString>
</citation>
<citation valid="true">
<authors>
<author>San Jose</author>
</authors>
<date>1981</date>
<journal>California 95193 SIGART Newsletter</journal>
<volume>74</volume>
<marker>Jose, 1981</marker>
<rawString>San Jose, California 95193 SIGART Newsletter 74 (Jan. 1981).</rawString>
</citation>
<citation valid="true">
<date>1981</date>
<journal>SIGMOD Record</journal>
<volume>11</volume>
<contexts>
<context position="24471" citStr="(1981)" startWordPosition="3681" endWordPosition="3681">tional linguistics. Corepresentational Grammar and Parsing English Comparatives Karen Ryan Linguistics Department 142 Klaeber Court University of Minnesota Minneapolis, Minnesota 55455 Proc. 19th Annual ACL Conf., June 1981, 13-18. Marcus (1980) notes that the syntax of English comparative constructions is highly complex, and claims that both syntactic and semantic information must be available for them to be parsed. This paper argues that comparatives can be structurally analyzed on the basis of syntactic information alone via a strictly surface-based grammar. Such a grammar is given in Ryan (1981), based on the corepresentational model of Kac (1978). While the grammar does not define a parsing algorithm per se, it nonetheless expresses regularities of surface organization and its relationship to semantic interpretation that an adequate parser would be expected to incorporate. The central problem in parsing comparatives involves identifying the arguments of comparative predicates, and the relations borne by these arguments to such predicates. A corepresentational grammar is explicitly designed to assign predicate-argument structure to sentences on the basis of their surface syntactic or</context>
<context position="85455" citStr="(1981)" startWordPosition="12970" endWordPosition="12970"> theory of discourse, in which language elements and systems are investigated with respect to how people utilize them in communication and processing. The paper offers the framework of a natural language pragmatics along these lines and applies the resulting theory to a study of a scene from a stage play by Sidney Howard. It is shown that the actions and discourse actions of the scene are indeed generated by the characters&apos; plans and goals. Linguistic Theory and Metatheory for a Science of Texts Robert De Beaugrande English Department University of Florida Gainesville, Florida 32611 Text 1, 2 (1981), 113-161. This article explores the typical reactions which occur when an established science confronts a new object of inquiry, as we find when linguistic theory encounters the text. The usual discussions are not productive as long as the old &amp;quot;paradigm&amp;quot; is still accepted as the framework for achievement. The issues are therefore re-examined in terms of the metatheory of science (e.g. Sneed, Stegmtiller, Lakatos, Feyerabend, Hempel), and some general solutions are expounded for the problems of validating theories on the basis of empirical content. A paradigmatic example is then presented in o</context>
</contexts>
<marker>1981</marker>
<rawString>SIGMOD Record 11, 2 (Feb. 1981). SIGPLAN Notices 16, 1 (Jan. 1981).</rawString>
</citation>
<citation valid="false">
<title>The Command Language Grammar: A Representation for the User Interface of Interactive Computer Systems Thomas P.</title>
<location>Moran</location>
<marker></marker>
<rawString>The Command Language Grammar: A Representation for the User Interface of Interactive Computer Systems Thomas P. Moran</rawString>
</citation>
<citation valid="false">
<date></date>
<institution>Xerox Palo Alto Research Center 3333 Coyote Hill Road</institution>
<location>Palo Alto, California</location>
<marker></marker>
<rawString>Xerox Palo Alto Research Center 3333 Coyote Hill Road Palo Alto, California 94304</rawString>
</citation>
<citation valid="true">
<authors>
<author>J</author>
</authors>
<title>A Experiment in English-Spanish Automated Translation of Medical Language Data Isabel Garcia-Hidalgo and</title>
<date>1981</date>
<journal>of Man-Machine Studies</journal>
<booktitle>Dunham Laboratory of Statistical and Mathematical Methodology Building 12A, Room 3041 National Institutes of Health</booktitle>
<volume>15</volume>
<location>Bethesda, Maryland</location>
<marker>J, 1981</marker>
<rawString>Int. J. of Man-Machine Studies 15, 1 (July 1981). A Experiment in English-Spanish Automated Translation of Medical Language Data Isabel Garcia-Hidalgo and George S. Dunham Laboratory of Statistical and Mathematical Methodology Building 12A, Room 3041 National Institutes of Health Bethesda, Maryland 20205</rawString>
</citation>
<citation valid="true">
<authors>
<author>of Inform in Med</author>
</authors>
<title>A Method for Rapidly Applying Context Sensitive Phonological Rules</title>
<date>1981</date>
<journal>Box</journal>
<volume>20</volume>
<pages>38--46</pages>
<marker>Med, 1981</marker>
<rawString>Meth. of Inform. in Med. 20, 1 (Jan. 1981), 38-46. A Method for Rapidly Applying Context Sensitive Phonological Rules Robert L. Mercer and Paul S. Cohen IBM Thomas J. Watson Research Center P.O. Box 218</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yorktown Heights</author>
</authors>
<date>1981</date>
<tech>10598 IBM Research Report RC 8889,</tech>
<pages>25</pages>
<location>New York</location>
<marker>Heights, 1981</marker>
<rawString>Yorktown Heights, New York 10598 IBM Research Report RC 8889, June 1981, 25 pages.</rawString>
</citation>
<citation valid="false">
<authors>
<author>M Boot</author>
<author>E Maat</author>
<author>J</author>
</authors>
<title>A Model for Automated Phonemicization</title>
<journal>Computerlinguistiek Wilhelminapark</journal>
<volume>11</volume>
<pages>3581</pages>
<publisher>NC Utrecht</publisher>
<marker>Boot, Maat, J, </marker>
<rawString>A Model for Automated Phonemicization M. Boot, E. Maat, and J. Renkers Instituut Voor Toegepaste Taalkunde en Computerlinguistiek Wilhelminapark 11 3581 NC Utrecht J. Renkers and M. Boot Instituut Voor Toegepaste Taalkunde en Computerlinguistiek Wilhelminapark 11 3581 NC Utrecht</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>