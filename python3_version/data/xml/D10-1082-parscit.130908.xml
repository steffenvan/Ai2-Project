<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.012846">
<title confidence="0.999127">
A Fast Decoder for Joint Word Segmentation and POS-Tagging Using a
Single Discriminative Model
</title>
<author confidence="0.999459">
Yue Zhang and Stephen Clark
</author>
<affiliation confidence="0.7706085">
University of Cambridge Computer Laboratory
William Gates Building,
</affiliation>
<address confidence="0.9969105">
15 JJ Thomson Avenue,
Cambridge CB3 0FD, UK
</address>
<email confidence="0.998853">
{yue.zhang, stephen.clark}@cl.cam.ac.uk
</email>
<sectionHeader confidence="0.995638" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999922125">
We show that the standard beam-search al-
gorithm can be used as an efficient decoder
for the global linear model of Zhang and
Clark (2008) for joint word segmentation and
POS-tagging, achieving a significant speed im-
provement. Such decoding is enabled by:
(1) separating full word features from par-
tial word features so that feature templates
can be instantiated incrementally, according to
whether the current character is separated or
appended; (2) deciding the POS-tag of a poten-
tial word when its first character is processed.
Early-update is used with perceptron training
so that the linear model gives a high score to a
correct partial candidate as well as a full out-
put. Effective scoring of partial structures al-
lows the decoder to give high accuracy with a
small beam-size of 16. In our 10-fold cross-
validation experiments with the Chinese Tree-
bank, our system performed over 10 times as
fast as Zhang and Clark (2008) with little ac-
curacy loss. The accuracy of our system on
the standard CTB 5 test was competitive with
the best in the literature.
</bodyText>
<sectionHeader confidence="0.990809" genericHeader="categories and subject descriptors">
1 Introduction and Motivation
</sectionHeader>
<bodyText confidence="0.999926976744186">
Several approaches have been proposed to solve
word segmentation and POS-tagging jointly, includ-
ing the reranking approach (Shi and Wang, 2007;
Jiang et al., 2008b), the hybrid approach (Nakagawa
and Uchimoto, 2007; Jiang et al., 2008a), and the
single-model approach (Ng and Low, 2004; Zhang
and Clark, 2008; Kruengkrai et al., 2009). These
methods led to accuracy improvements over the tra-
ditional, pipelined segmentation and POS-tagging
baseline by avoiding segmentation error propagation
and making use of part-of-speech information to im-
prove segmentation.
The single-model approach to joint segmentation
and POS-tagging offers consistent training of all in-
formation, concerning words, characters and parts-
of-speech. However, exact inference with dynamic
programming can be infeasible if features are de-
fined over a large enough range of the output, such
as over a two-word history. In our previous work
(Zhang and Clark, 2008), which we refer to as
Z&amp;C08 from now on, we used an approximate de-
coding algorithm that keeps track of a set of partially
built structures for each character, which can be seen
as a dynamic programming chart which is greatly re-
duced by pruning.
In this paper we follow the line of single-model
research, in particular the global linear model of
Z&amp;C08. We show that effective decoding can be
achieved with standard beam-search, which gives
significant speed improvements compared to the de-
coding algorithm of Z&amp;C08, and achieves accura-
cies that are competitive with the state-of-the-art.
Our research is also in line with recent research on
improving the speed of NLP systems with little or
no accuracy loss (Charniak et al., 2006; Roark and
Hollingshead, 2008).
Our speed improvement is achieved by the use
of a single-beam decoder. Given an input sentence,
candidate outputs are built incrementally, one char-
acter at a time. When each character is processed,
it is combined with existing candidates in all possi-
ble ways to generate new candidates, and an agenda
is used to keep the N-best candidate outputs from
</bodyText>
<page confidence="0.984639">
843
</page>
<note confidence="0.817753">
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 843–852,
MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.99902325">
the begining of the sentence to the current character.
Compared to the multiple-beam search algorithm of
Z&amp;C08, the use of a single beam can lead to an order
of magnitude faster decoding speed.
</bodyText>
<subsectionHeader confidence="0.994621">
1.1 The processing of partial words
</subsectionHeader>
<bodyText confidence="0.999985522222223">
An important problem that we solve in this paper
is the handling of partial words with a single beam
decoder for the global model. As we pointed out
in Z&amp;C08, it is very difficult to score partial words
properly when they are compared with full words,
although such comparison is necessary for incre-
mental decoding with a single-beam. To allow com-
parisons with full words, partial words can either be
treated as full words, or handled differently.
We showed in Z&amp;C08 that a naive single-beam
decoder which treats partial words in the same way
as full words failed to give a competitive accu-
racy. An important reason for the low accuracy is
over-segmentation during beam-search. Consider
the three characters “A来*k (tap water)”. The first
two characters do not make sense when put together
as a single word. Rather, when treated as two single-
character words, they can make sense in a sentence
such as “请 (please) A (self) 来 (come) 取 (take)”.
Therefore, when using single-beam search to pro-
cess “A来*k (tap water)”, the two-character word
candidate “A来” is likely to have been thrown off
the agenda before the third character “*k” is con-
sidered, leading to an unrecoverable segmentation
error.
This problem is even more severe for a joint seg-
mentor and POS-tagger than for a pure word seg-
mentor, since the POS-tags and POS-tag bigram of
“A” and “来” further supports them being separated
when ”来” is considered. The multiple-beam search
decoder we proposed in Z&amp;C08 can be seen as a
means to ensure that the three characters “A来*k”
always have a chance to be considered as a single
word. It explores candidate segmentations from the
beginning of the sentence until each character, and
avoids the problem of processing partial words by
considering only full words. However, since it ex-
plores a larger part of the search space than a single-
beam decoder, its time complexity is correspond-
ingly higher.
In this paper, we treat partial words differently
from full words, so that in the previous example,
the decoder can take the first two characters in “A
来 *k (tap water)” as a partial word, and keep it
in the beam before the third character is processed.
One challenge is the representation of POS-tags for
partial words. The POS of a partial word is unde-
fined without the corresponding full word informa-
tion. Though a partial word can make sense with
a particular POS-tag when it is treated as a com-
plete word, this POS-tag is not necessarily the POS of
the full word which contains the partial word. Take
the three-character sequence “T雨天” as an exam-
ple. The first character “T” represents a single-
character word “below”, for which the POS can be
LC or VV. The first two characters “T雨” repre-
sent a two-character word “rain”, for which the POS
can be VV. Moreover, all three characters when put
together make the word “rainy day”, for which the
POS is NN. As discussed above, assigning POS tags
to partial words as if they were full words leads to
low accuracy.
An obvious solution to the above problem is not to
assign a POS to a partial word until it becomes a full
word. However, lack of POS information for partial
words makes them less competitive compared to full
words in the beam, since the scores of full words are
futher supported by POS and POS ngram informa-
tion. Therefore, not assigning POS to partial words
potentially leads to over segmentation. In our exper-
iments, this method did not give comparable accura-
cies to our Z&amp;C08 system.
In this paper, we take a different approach, and
assign a POS-tag to a partial word when its first char-
acter is separated from the final character of the pre-
vious word. When more characters are appended to
a partial word, the POS is not changed. The idea is
to use the POS of a partial word as the predicted POS
of the full word it will become. Possible predictions
are made with the first character of the word, and the
likely ones will be kept in the beam for the next pro-
cessing steps. For example, with the three characters
“T雨天”, we try to keep two partial words (besides
full words) in the beam when the first word “T” is
processed, with the POS being VV and NN, respec-
tively. The first POS predicts the two-character word
“T雨”,and the second the three-character word
“T雨天”. Now when the second character is pro-
cessed, we still need to maintain the possible POS
NN in the agenda, which predicts the three-character
</bodyText>
<page confidence="0.99558">
844
</page>
<bodyText confidence="0.996681019230769">
word “下雨天”.
As a main contribution of this paper, we show that
the mechanism of predicting the POS at the first char-
acter gives competitive accuracy. This mechanism
can be justified theoretically. Unlike alphabetical
languages, each Chinese character represents some
specific meanings. Given a character, it is natural for
a human speaker to know immediately what types
of words it can start. The allows the knowledge of
possible POS-tags of words that a character can start,
using information about the character from the train-
ing data. Moreover, the POS of the previous words to
the current word are also useful in deciding possible
POS for the word.1
The mechanism of first-character decision of POS
also boosts the efficiency, since the enumeration of
POS is unecessary when a character is appended to
the end of an existing word. As a result, the com-
plexity of each processing step is reduce by half
compared to a method without POS prediction.
Finally, an intuitive way to represent the status of
a partial word is using a flag explicitly, which means
an early decision of the segmentation of the next in-
coming character. We take a simpler alternative ap-
proach, and treat every word as a partial word un-
til the next incoming character is separated from the
last character of this word. Before a word is con-
firmed as a full word, we only apply to it features
that represent its current partial status, such as char-
acter bigrams, its starting character and its part-of-
speech, etc. Full word features, including the first
and last characters of a word, are applied immedi-
ately after a word is confirmed as complete.
An important component for our proposed system
is the training process, which needs to ensure that
the model scores a partial word with predicted POS
properly. We use the averaged perceptron (Collins,
2002) for training, together with the “early update”
mechanism of Collins and Roark (2004). Rather
than updating the parameters after decoding is com-
plete, the modified algorithm updates parameters at
any processing step if the correct partial candidate
falls out of the beam.
In our experiments using the Chinese Treebank
1The next incoming characters are also a useful source
of information for predicting the POS. However, our system
achieved competitive accuracy with Z&amp;C08 without such char-
acter lookahead features.
data, our system ran an order of magnitude faster
than our Z&amp;C08 system with little loss of accuracy.
The accuracy of our system was competitive with
other recent models.
</bodyText>
<sectionHeader confidence="0.616025" genericHeader="keywords">
2 Model and Feature Templates
</sectionHeader>
<bodyText confidence="0.999368666666667">
We use a linear model to score both partial and full
candidate outputs. Given an input x, the score of a
candidate output y is computed as:
</bodyText>
<equation confidence="0.966372">
Score(y) _ -b(y) · w,
</equation>
<bodyText confidence="0.99984845945946">
where 4b(y) is the global feature vector extracted
from y, and w� is the parameter vector of the model.
Figure 1 shows the feature templates for the
model, where templates 1 – 14 contain only seg-
mentation information and templates 15 – 29 contain
both segmentation and POS information. Each tem-
plate is instantiated according to the current charac-
ter in the decoding process. Row “For” shows the
conditions for template instantiation, where “s” in-
dicates that the corresponding template is instanti-
ated when the current character starts a new word,
and “a” indicates that the corresponding template is
instantiated when the current character does not start
a new word. In the row for feature templates, w, t
and c are used to represent a word, a POS-tag and
a character, respectively. The subscripts are based
on the current character, where w−1 represents the
first word to the left of the current character, and
p−2 represents the POS-tag on the second word to
the left of the current character, and so on. As an
example, feature template 1 is instantiated when the
current character starts a new word, and the resulting
feature value is the word to the left of this charac-
ter. start(w), end(w) and len(w) represent the first
character, the last character and the length of word
w, respectively. The length of a word is normalized
to 16 if it is larger than 16. cat(c) represents the POS
category of character c, which is the set of POS-tags
seen on character c, as we used in Z&amp;C08.
Given a partial or complete candidate y, its global
feature vector 4b(y) is computed by instantiating all
applicable feature templates from Table 1 for each
character in y, according to whether or not the char-
acter is separated from the previous character.
The feature templates are mostly taken from, or
inspired by, the feature templates of Z&amp;C08. Tem-
plates 1, 2, 3, 4, 5, 8, 10, 12, 13, 14, 15, 19, 20,
</bodyText>
<page confidence="0.984746">
845
</page>
<figure confidence="0.998867857142857">
Feature template
For
s
s
s
s
s
s
a
s
s
s
s
s
s
s
1
2
3
4
5
6
7
8
9
10
11
12
13
14
function DECODE(sent, agenda):
CLEAR(agenda)
ADDITEM(agenda, “”)
for index in [0..LEN(sent)]:
for cand in agenda:
new +— APPEND(cand, sent[index])
ADDITEM(agenda, new)
for pos in TAGSET():
new +— SEP(cand, sent[index], pos)
ADDITEM(agenda, new)
agenda +— N-BEST(agenda)
return BEST(agenda)
</figure>
<figureCaption confidence="0.994317">
Figure 1: The incremental beam-search decoder.
</figureCaption>
<figure confidence="0.997050794117647">
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
s
s
s
s
s
s
s
s
s
s
s, a
a
s
s
s
s
a
</figure>
<tableCaption confidence="0.9643">
Table 1: Feature templates.
</tableCaption>
<bodyText confidence="0.999871166666666">
24, 27 and 29 concern complete word information,
and they are used in the model to differentiate cor-
rect and incorrect output structures in the same way
as our Z&amp;C08 model. Templates 6, 7, 9, 16, 17,
18, 21, 22, 23, 25, 26 and 28 concern partial word
information, whose role in the model is to indicate
the likelihood that the partial word including the cur-
rent character will become a correct full word. They
act as guidance for the action to take for the cur-
rent character according to the context, and are the
crucial reason for the effectiveness of the algorithm
with a small beam-size.
</bodyText>
<subsectionHeader confidence="0.922522">
2.1 Decoding
</subsectionHeader>
<bodyText confidence="0.995946807692308">
The decoding algorithm builds an output candidate
incrementally, one character at a time. Each char-
acter can either be attached to the current word or
separated as the start a new word. When the current
character starts a new word, a POS-tag is assigned to
the new word. An agenda is used by the decoder to
keep the N-best candidates during the incremental
process. Before decoding starts, the agenda is ini-
tialized with an empty sentence. When a character is
processed, existing candidates are removed from the
agenda and extended with the current character in all
possible ways, and the N-best newly generated can-
didates are put back onto the agenda. After all input
characters have been processed, the highest-scored
candidate from the agenda is taken as the output.
Pseudo code for the decoder is shown in Figure
1. CLEAR removes all items from the agenda, AD-
DITEM adds a new item onto the agenda, N-BEST
returns the N highest-scored items from the agenda,
and BEST returns the highest-scored item from the
agenda. LEN returns the number of characters in a
sentence, and sent[i] returns the ith character from
the sentence. APPEND appends a character to the
last word in a candidate, and SEP joins a character
as the start of a new word in a candidate, assigning
a POS-tag to the new word.
</bodyText>
<figure confidence="0.959916911764706">
w−1
w−1w−2
w−1, where len(w−1) = 1
start(w−1)len(w−1)
end(w−1)len(w−1)
end(w−1)c0
c−1c0
begin(w−1)end(w−1)
w−1c0
end(w−2)w−1
start(w−1)c0
end(w−2)end(w−1)
w−2len(w−1)
len(w−2)w−1
w−1t−1
t−1t0
t−2t−1t0
w−1t0
t−2w−1
w−1t−1end(w−2)
w−1t−1c0
c−2c−1c0t−1,
where len(w−1) = 1
start(w0)t0
t−1start(w−1)
t0c0
c0t0start(w0)
ct−1end(w−1),
where c E w−1 and c =� end(w−1)
c0t0cat(start(w0))
ct−1cat(end(w−1)),
where c E w−1 and c =� end(w−1)
c0t0c−1t−1
c0t0c−1
</figure>
<bodyText confidence="0.999791533333333">
Both our decoding algorithm and the decoding al-
gorithm of Z&amp;C08 run in linear time. However, in
order to generate possible candidates for each char-
acter, Z&amp;C08 uses an extra loop to search for pos-
sible words that end with the current character. A
restriction to the maximum word length is applied
to limit the number of iterations in this loop, with-
out which the algorithm would have quadratic time
complexity. In contrast, our decoder does not search
backword for the possible starting character of any
word. Segmentation ambiguities are resolved by bi-
nary choices between the actions append or sepa-
rate for each character, and no POS enumeration is
required when the character is appended. This im-
proves the speed by a significant factor.
</bodyText>
<subsectionHeader confidence="0.990051">
2.2 Training
</subsectionHeader>
<bodyText confidence="0.993712617647059">
The learning algorithm is based on the generalized
perceptron (Collins, 2002), but parameter adjust-
ments can be performed at any character during the
decoding process, using the “early update” mecha-
nism of Collins and Roark (2004).
The parameter vector of the model is initialized as
all zeros before training, and used to decode training
examples. Each training example is turned into the
raw input format, and processed in the same way as
decoding. After each character is processed, partial
candidates in the agenda are compared to the cor-
responding gold-standard output for the same char-
acters. If none of the candidates in the agenda are
correct, the decoding is stopped and the parameter
vector is updated by adding the global feature vector
of the gold-standard partial output and subtracting
the global feature vector of the highest-scored par-
tial candidate in the agenda. The training process
then moves on to the next example. However, if any
item in the agenda is the same as the correspond-
ing gold-standard, the decoding process moves to
the next character, without any change to the pa-
rameter values. After all characters are processed,
the decoder prediction is compared with the training
example. If the prediction is correct, the parame-
ter vector is not changed; otherwise it is updated by
adding the global feature vector of the training ex-
ample and subtracting the global feature vector of
the decoder prediction, just as the perceptron algo-
rithm does. The same training examples can be used
to train the model for multiple iterations. We use
the averaged parameter vector (Collins, 2002) as the
final model.
Pseudocode for the training algorithm is shown in
</bodyText>
<figureCaption confidence="0.7783935">
Figure 2. It is based on the decoding algorithm in
Figure 1, and the main differences are: (1) the train-
</figureCaption>
<bodyText confidence="0.985692823529412">
ing algorithm takes the gold-standard output and the
parameter vector as two additional arguments; (2)
the training algorithm does not return a prediction,
but modifies the parameter vector when necessary;
(3) lines 11 to 20 are additional lines of code for pa-
rameter updates.
Without lines 11 to 16, the training algorithm is
exactly the same as the generalized perceptron al-
gorithm. These lines are added to ensure that the
agenda contains highly probable candidates during
the whole beam-search process, and they are crucial
to the high accuracy of the system. As stated earlier,
the decoder relies on proper scoring of partial words
to maintain a set of high quality candidates in the
agenda. Updating the value of the parameter vector
for partial outputs can be seen as a means to ensure
correct scoring of partial candidates at any character.
</bodyText>
<subsectionHeader confidence="0.99758">
2.3 Pruning
</subsectionHeader>
<bodyText confidence="0.999961047619048">
We follow Z&amp;C08 and use several pruning methods,
most of which serve to to improve the accuracy by
removing irrelevant candidates from the beam. First,
the system records the maximum number of charac-
ters that a word with a particular POS-tag can have.
For example, from the Chinese Treebank that we
used for our experiments, most POS are associated
with only with one- or two-character words. The
only POS-tags that are seen with words over ten char-
acters long are NN (noun), NR (proper noun) and
CD (numbers). The maximum word length informa-
tion is initialized as all ones, and updated according
to each training example before it is processed.
Second, a tag dictionary is used to record POS-
tags associated with each word. During decoding,
frequent words and words with “closed set” tags2
are only allowed POS-tags according to the tag dic-
tionary, while other words are allowed every POS-tag
to make candidate outputs. Whether a word is a fre-
quent word is decided by the number of times it has
been seen in the training process. Denoting the num-
</bodyText>
<footnote confidence="0.773171666666667">
2“Closed set” tags are the set of POS-tags which are only
associated with a fixed set of words, according to the Penn Chi-
nese Treebank specifications (Xia, 2000).
</footnote>
<page confidence="0.996698">
847
</page>
<bodyText confidence="0.950495">
function TRAIN(sent, agenda, gold-standard, w—):
</bodyText>
<listItem confidence="0.986589476190476">
1: CLEAR(agenda)
2: ADDITEM(agenda, “”)
3: for index in [0..LEN(sent)]:
4: for cand in agenda:
5: new ← APPEND(cand, sent[index])
6: ADDITEM(agenda, new)
7: for pos in TAGSET():
8: new ← SEP(cand, sent[index], pos)
9: ADDITEM(agenda, new)
10: agenda ← N-BEST(agenda)
11: for cand in agenda:
12: if cand = gold-standard[0:index]:
13: CONTINUE
14: w� ← w� + (gold-standard[0:index])
15: w� ← w� - (BEST(agenda))
16: return
17: if BEST(agenda) 74 gold-standard:
18: w� ← w� + (gold-standard)
19: w� ← w� - (BEST(agenda))
20: return
21: return
</listItem>
<figureCaption confidence="0.999368">
Figure 2: The incremental learning function.
</figureCaption>
<bodyText confidence="0.999875315789474">
ber of times the most frequent word has been seen
with M, a word is a frequent word if it has been
seen more than M/5000 + 5 times. The threshold
value is taken from Z&amp;C08, and we did not adjust
it during development. Word frequencies are initial-
ized as zeros and updated according to each training
example before it is processed; the tag dictionary is
initialized as empty and updated according to each
training example before it is processed.
Third, we make an additional record of the initial
characters for words with “closed set” tags. During
decoding, when the current character is added as the
start of a new word, “closed set” tags are only as-
signed to the word if it is consistent with the record.
This type of pruning is used in addition to the tag
dictionary to prune invalid partial words, while the
tag dictionary is used to prune complete words. The
record for initial character and POS is initially empty,
and udpated according to each training example be-
fore it is processed.
Finally, at any decoding step, we group partial
candidates that are generated by separating the cur-
rent character as the start of a new word by the sig-
nature p0p−1w−1, and keep only the best among
those having the same p0p−1w−1. The signature
p0p−1w−1 is decided by the feature templates we
use: it can be shown that if two candidates cand1
and cand2 generated at the same step have the same
signature, and the score of cand1 is higher than the
score of cand2, then at any future step, the highest
scored candidate generated from cand1 will always
have a higher score than the highest scored candidate
generated from cand2.
From the above pruning methods, only the third
was not used by Z&amp;C08. It can be seen as an extra
mechanism to help keep likely partial words in the
agenda and improve the accuracy, but which does
not give our system a speed advantage over Z&amp;C08.
</bodyText>
<sectionHeader confidence="0.999236" genericHeader="introduction">
3 Experiments
</sectionHeader>
<bodyText confidence="0.9992465">
We used the Chinese Treebank (CTB) data to per-
form one set of development tests and two sets of fi-
</bodyText>
<page confidence="0.98101">
848
</page>
<figure confidence="0.997728666666667">
F-measure
0.9
0.8
0.7
0.6
0.5
0.4
5 10 15 20 25 30
Training iteration
</figure>
<figureCaption confidence="0.9980705">
Figure 3: The influence of beam-sizes, and the conver-
gence of the perceptron.
</figureCaption>
<bodyText confidence="0.999916857142857">
nal tests. The CTB 4 was split into two parts, with the
CTB 3 being used for a 10-fold cross validation test
to compare speed and accuracies with Z&amp;C08, and
the rest being used for development. The CTB 5 was
used to perform the additional set of experiments to
compare accuracies with other recent work.
We use the standard F-measure to evaluate output
accuracies. For word segmentation, precision is de-
fined as the number of correctly segmented words
divided by the total number of words in the output,
and recall is defined as the number of correctly seg-
mented words divided by the total number of words
in the gold-standard output. For joint segmentation
and POS-tagging, precision is defined as the num-
ber of correctly segmented and POS-tagged words
divided by the total number of words from the out-
put, and recall is defined as the correctly segmented
and POS-tagged words divided by the total number
of words in the gold-standard output.
All our experiments were performed on a Linux
platform, and a single 2.66GHz Intel Core 2 CPU.
</bodyText>
<subsectionHeader confidence="0.999418">
3.1 Development tests
</subsectionHeader>
<bodyText confidence="0.999110325">
Our development data consists of 150K words in
4798 sentences. 80% of the data were randomly
chosen as the development training data, while the
rest were used as the development test data. Our de-
velopment tests were mainly used to decide the size
of the beam, the number of training iterations, the ef-
fect of partial features in beam-search decoding, and
the effect of incremental learning (i.e. early update).
Figure 3 shows the accuracy curves for joint seg-
mentation and POS-tagging by the number of train-
ing iterations, using different beam sizes. With the
size of the beam increasing from 1 to 32, the accura-
cies generally increase, while the amount of increase
becomes small when the size of the beam becomes
16. After the 10th iteration, a beam size of 32 does
not always give better accuracies than a beam size
of 16. We therefore chose 16 as the size of the beam
for our system.
The testing times for each beam size between 1
and 32 are 7.16s, 11.90s, 18.42s, 27.82s, 46.77s
and 89.21s, respectively. The corresponding speeds
in the number of sentences per second are 111.45,
67.06, 43.32, 28.68, 17.06 and 8.95, respectively.
Figure 3 also shows that the accuracy increases
with an increased number of training iterations, but
the amount of increase becomes small after the 25th
iteration. We chose 29 as the number of iterations to
train our system.
The effect of incremental training: We compare
the accuracies by incremental training using early
update and normal perceptron training. In the nor-
mal perceptron training case, lines 11 to 16 are taken
out of the training algorithm in Figure 2. The algo-
rithm reached the best performance at the 22nd iter-
ation, with the segmentation F-score being 90.58%
and joint F-score being 83.38%. In the incremental
training case, the algorithm reached the best accu-
racy at the 30th training iteration, obtaining a seg-
mentation F-score of 91.14% and a joint F-score of
84.06%.
</bodyText>
<subsectionHeader confidence="0.995592">
3.2 Final tests using CTB 3
</subsectionHeader>
<bodyText confidence="0.999946666666667">
CTB 3 consists of 150K words in 10364 sentences.
We follow Z&amp;C08 and split it into 10 equal-sized
parts. In each test, one part is taken as the test
data and the other nine are combined together as
the training data. We compare the speed and accu-
racy with the joint segmentor and tagger of Z&amp;C08,
which is publicly available as the ZPar system, ver-
sion 0.23.
The results are shown in Table 2, where each row
shows one cross validation test. The column head-
ings “sf”, “jf”, “time” and “speed” refer to segmen-
tation F-measure, joint F-measure, testing time (in
</bodyText>
<footnote confidence="0.690438">
3http://www.sourceforge.net/projects/zpar
</footnote>
<figure confidence="0.9965425">
beam=1
beam=2
beam=4
beam=8
beam=16
beam=32
</figure>
<page confidence="0.981811">
849
</page>
<table confidence="0.991139538461538">
Z&amp;C08 this paper
# sf jf time speed sf jf time speed
1 97.18 93.27 557.97 1.86 97.25 93.51 44.20 23.44
2 97.65 93.81 521.63 1.99 97.66 93.97 42.07 24.26
3 96.08 91.04 444.69 2.33 95.55 90.65 39.23 26.41
4 96.31 91.93 431.04 2.40 96.37 92.15 39.54 26.20
5 96.35 91.94 508.39 2.04 95.84 91.51 43.30 23.93
6 94.48 88.63 482.78 2.15 94.25 88.53 43.77 23.67
7 95.27 90.52 361.95 2.86 95.10 90.42 41.76 24.81
8 94.98 90.01 418.54 2.47 94.87 90.30 39.81 26.02
9 95.23 90.84 471.3 2.20 95.21 90.55 42.03 26.65
10 96.49 92.11 500.72 2.08 96.33 92.12 43.12 24.03
average 96.00 91.41 469.90 2.24 95.84 91.37 41.88 24.94
</table>
<tableCaption confidence="0.998953">
Table 2: Speed and acccuracy comparisons with Z&amp;C08 by 10-fold cross validation.
</tableCaption>
<bodyText confidence="0.941094">
seconds) and testing speed (in the number of sen-
tences per second), respectively.
Our system gave a joint segmentation and POS-
tagging F-score of 91.37%, which is only 0.04%
lower than that of ZPar 0.2. The speed of our system
was over 10 times as fast as ZPar 0.2.
</bodyText>
<subsectionHeader confidence="0.99038">
3.3 Final tests using CTB 5
</subsectionHeader>
<bodyText confidence="0.999958347826087">
We follow Kruengkrai et al. (2009) and split the CTB
5 into training, development testing and testing sets,
as shown in Table 3. We ignored the development
test data since our system had been developed in pre-
vious experiments.
Kruengkrai et al. (2009) made use of character
type knowledge for spaces, numerals, symbols, al-
phabets, Chinese and other characters. In the previ-
ous experiments, our system did not use any knowl-
edge beyond the training data. To make the compar-
ison fairer, we included knowledge of English let-
ters and Arabic numbers in this experiment. During
both training and decoding, English letters and Ara-
bic numbers are segmented using simple rules, treat-
ing consecutive English letters or Arabic numbers as
a single word.
The results are shown in Table 4, where row
“N07” refers to the model of Nakagawa and Uchi-
moto (2007), rows “J08a” and “b” refer to the mod-
els of Jiang et al. (2008a) and Jiang et al. (2008b),
and row “K09” refers to the models of Kruengkrai et
al. (2009). Columns “sf” and “jf” refer to segmen-
tation and joint accuracies, respectively. Our system
</bodyText>
<table confidence="0.9910995">
Sections Sentences Words
Training 1–270 18,085 493,892
400–931
1001–1151
Dev 301–325 350 6,821
Test 271–300 348 8,008
</table>
<tableCaption confidence="0.994786">
Table 3: Training, development and test data on CTB 5.
</tableCaption>
<table confidence="0.999419571428571">
sf jf
K09 (error-driven) 97.87 93.67
our system 97.78 93.67
K09 (baseline) 97.79 93.60
J08a 97.85 93.41
J08b 97.74 93.37
N07 97.83 93.32
</table>
<tableCaption confidence="0.996895">
Table 4: Accuracy comparisons with recent studies on
CTB 5.
</tableCaption>
<bodyText confidence="0.999963">
gave comparable accuracies to these recent works,
obtaining the best (same as the error-driven version
of K09) joint F-score.
</bodyText>
<sectionHeader confidence="0.999983" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999966">
The effectiveness of our beam-search decoder
showed that the joint segmentation and tagging
problem may be less complex than previously per-
ceived (Zhang and Clark, 2008; Jiang et al., 2008a).
At the very least, the single model approach with a
simple decoder achieved competitive accuracies to
what has been achieved so far by the reranking (Shi
</bodyText>
<page confidence="0.988295">
850
</page>
<bodyText confidence="0.999952382352941">
and Wang, 2007; Jiang et al., 2008b) models and
an ensemble model using machine-translation tech-
niques (Jiang et al., 2008a). This may shed new light
on joint segmentation and POS-tagging methods.
Kruengkrai et al. (2009) and Zhang and Clark
(2008) are the most similar to our system among
related work. Both systems use a discriminatively
trained linear model to score candidate outputs. The
work of Kruengkrai et al. (2009) is based on Nak-
agawa and Uchimoto (2007), which separates the
processing of known words and unknown words,
and uses a set of segmentation tags to represent the
segmentation of characters. In contrast, our model
is conceptually simpler, and does not differentiate
known words and unknown words. Moreover, our
model is based on our previous work, in line with
Zhang and Clark (2007), which does not treat word
segmentation as character sequence labeling.
Our learning and decoding algorithms are also
different from Kruengkrai et al. (2009). While Kru-
engkrai et al. (2009) perform dynamic programming
and MIRA learning, we use beam-search to perform
incremental decoding, and the early-update version
of the perceptron algorithm to train the model. Dy-
namic programming is exact inference, for which
the time complexity is decided by the locality of
feature templates. In contrast, beam-search is ap-
proximate and can run in linear time. The param-
eter updating for our algorithm is conceptually and
computationally simpler than MIRA, though its per-
formance can be slightly lower. However, the early-
update mechanism we use is consistent with our in-
cremental approach, and improves the learning of
the beam-search process.
</bodyText>
<sectionHeader confidence="0.999271" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.998136363636364">
We showed that a simple beam-search decoding al-
gorithm can be effectively applied to the decoding
problem for a global linear model for joint word
segmentation and POS-tagging. By guiding search
with partial word information and performing learn-
ing for partial candidates, our system achieved sig-
nificantly faster speed with little accuracy loss com-
pared to the system of Z&amp;C08.
The source code of our joint segmentor and POS-
tagger can be found at:
www.sourceforge.net/projects/zpar, version 0.4.
</bodyText>
<sectionHeader confidence="0.992224" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.998048166666667">
We thank Canasai Kruengkrai for discussion on effi-
ciency issues, and the anonymous reviewers for their
suggestions. Yue Zhang and Stephen Clark are sup-
ported by the European Union Seventh Framework
Programme (FP7-ICT-2009-4) under grant agree-
ment no. 247762.
</bodyText>
<sectionHeader confidence="0.999006" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999857071428571">
Eugene Charniak, Mark Johnson, Micha Elsner, Joseph
Austerweil, David Ellis, Isaac Haxton, Catherine Hill,
R. Shrivaths, Jeremy Moore, Michael Pozar, and
Theresa Vu. 2006. Multilevel coarse-to-fine PCFG
parsing. In Proceedings of HLT/NAACL, pages 168–
175, New York City, USA, June. Association for Com-
putational Linguistics.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceedings
ofACL, pages 111–118, Barcelona, Spain, July.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP, pages 1–8, Philadelphia, USA, July.
Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan L¨u.
2008a. A cascaded linear model for joint Chinese
word segmentation and part-of-speech tagging. In
Proceedings ofACL/HLT, pages 897–904, Columbus,
Ohio, June.
Wenbin Jiang, Haitao Mi, and Qun Liu. 2008b. Word
lattice reranking for Chinese word segmentation and
part-of-speech tagging. In Proceedings of COLING,
pages 385–392, Manchester, UK, August.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun’ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hybrid
model for joint Chinese word segmentation and POS
tagging. In Proceedings of ACL/AFNLP, pages 513–
521, Suntec, Singapore, August.
Tetsuji Nakagawa and Kiyotaka Uchimoto. 2007. A
hybrid approach to word segmentation and POS tag-
ging. In Proceedings of ACL Demo and Poster Ses-
sion, Prague, Czech Republic, June.
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-of-
speech tagging: One-at-a-time or all-at-once? word-
based or character-based? In Proceedings of EMNLP,
Barcelona, Spain.
Brian Roark and Kristy Hollingshead. 2008. Classify-
ing chart cells for quadratic complexity context-free
inference. In Proceedings of COLING, pages 745–
752, Manchester, UK, August. Coling 2008 Organiz-
ing Committee.
</reference>
<page confidence="0.982996">
851
</page>
<reference confidence="0.991896142857143">
Yanxin Shi and Mengqiu Wang. 2007. A dual-layer CRF
based joint decoding method for cascade segmentation
and labelling tasks. In Proceedings of IJCAI, Hyder-
abad, India.
Fei Xia, 2000. The part-of-speech tagging guidelines for
the Chinese Treebank (3.0).
Yue Zhang and Stephen Clark. 2007. Chinese segmenta-
tion with a word-based perceptron algorithm. In Pro-
ceedings of ACL, pages 840–847, Prague, Czech Re-
public, June.
Yue Zhang and Stephen Clark. 2008. Joint word segmen-
tation and POS tagging using a single perceptron. In
Proceedings ofACL/HLT, pages 888–896, Columbus,
Ohio, June.
</reference>
<page confidence="0.998354">
852
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.575734">
<title confidence="0.9221675">Fast Decoder for Joint Word Segmentation and Using Single Discriminative Model</title>
<author confidence="0.962948">Zhang</author>
<affiliation confidence="0.9251605">University of Cambridge Computer William Gates</affiliation>
<address confidence="0.892868">15 JJ Thomson Cambridge CB3 0FD,</address>
<abstract confidence="0.99937372">We show that the standard beam-search algorithm can be used as an efficient decoder for the global linear model of Zhang and Clark (2008) for joint word segmentation and achieving a significant speed improvement. Such decoding is enabled by: (1) separating full word features from partial word features so that feature templates can be instantiated incrementally, according to whether the current character is separated or (2) deciding the of a potential word when its first character is processed. Early-update is used with perceptron training so that the linear model gives a high score to a correct partial candidate as well as a full output. Effective scoring of partial structures allows the decoder to give high accuracy with a small beam-size of 16. In our 10-fold crossvalidation experiments with the Chinese Treebank, our system performed over 10 times as fast as Zhang and Clark (2008) with little accuracy loss. The accuracy of our system on standard test was competitive with the best in the literature.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
<author>Micha Elsner</author>
<author>Joseph Austerweil</author>
<author>David Ellis</author>
<author>Isaac Haxton</author>
<author>Catherine Hill</author>
<author>R Shrivaths</author>
<author>Jeremy Moore</author>
<author>Michael Pozar</author>
<author>Theresa Vu</author>
</authors>
<title>Multilevel coarse-to-fine PCFG parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT/NAACL,</booktitle>
<pages>168--175</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City, USA,</location>
<contexts>
<context position="3033" citStr="Charniak et al., 2006" startWordPosition="479" endWordPosition="482"> of partially built structures for each character, which can be seen as a dynamic programming chart which is greatly reduced by pruning. In this paper we follow the line of single-model research, in particular the global linear model of Z&amp;C08. We show that effective decoding can be achieved with standard beam-search, which gives significant speed improvements compared to the decoding algorithm of Z&amp;C08, and achieves accuracies that are competitive with the state-of-the-art. Our research is also in line with recent research on improving the speed of NLP systems with little or no accuracy loss (Charniak et al., 2006; Roark and Hollingshead, 2008). Our speed improvement is achieved by the use of a single-beam decoder. Given an input sentence, candidate outputs are built incrementally, one character at a time. When each character is processed, it is combined with existing candidates in all possible ways to generate new candidates, and an agenda is used to keep the N-best candidate outputs from 843 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 843–852, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics the begining of </context>
</contexts>
<marker>Charniak, Johnson, Elsner, Austerweil, Ellis, Haxton, Hill, Shrivaths, Moore, Pozar, Vu, 2006</marker>
<rawString>Eugene Charniak, Mark Johnson, Micha Elsner, Joseph Austerweil, David Ellis, Isaac Haxton, Catherine Hill, R. Shrivaths, Jeremy Moore, Michael Pozar, and Theresa Vu. 2006. Multilevel coarse-to-fine PCFG parsing. In Proceedings of HLT/NAACL, pages 168– 175, New York City, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Brian Roark</author>
</authors>
<title>Incremental parsing with the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>111--118</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="10127" citStr="Collins and Roark (2004)" startWordPosition="1700" endWordPosition="1703">word. Before a word is confirmed as a full word, we only apply to it features that represent its current partial status, such as character bigrams, its starting character and its part-ofspeech, etc. Full word features, including the first and last characters of a word, are applied immediately after a word is confirmed as complete. An important component for our proposed system is the training process, which needs to ensure that the model scores a partial word with predicted POS properly. We use the averaged perceptron (Collins, 2002) for training, together with the “early update” mechanism of Collins and Roark (2004). Rather than updating the parameters after decoding is complete, the modified algorithm updates parameters at any processing step if the correct partial candidate falls out of the beam. In our experiments using the Chinese Treebank 1The next incoming characters are also a useful source of information for predicting the POS. However, our system achieved competitive accuracy with Z&amp;C08 without such character lookahead features. data, our system ran an order of magnitude faster than our Z&amp;C08 system with little loss of accuracy. The accuracy of our system was competitive with other recent models</context>
<context position="16684" citStr="Collins and Roark (2004)" startWordPosition="2826" endWordPosition="2829">he algorithm would have quadratic time complexity. In contrast, our decoder does not search backword for the possible starting character of any word. Segmentation ambiguities are resolved by binary choices between the actions append or separate for each character, and no POS enumeration is required when the character is appended. This improves the speed by a significant factor. 2.2 Training The learning algorithm is based on the generalized perceptron (Collins, 2002), but parameter adjustments can be performed at any character during the decoding process, using the “early update” mechanism of Collins and Roark (2004). The parameter vector of the model is initialized as all zeros before training, and used to decode training examples. Each training example is turned into the raw input format, and processed in the same way as decoding. After each character is processed, partial candidates in the agenda are compared to the corresponding gold-standard output for the same characters. If none of the candidates in the agenda are correct, the decoding is stopped and the parameter vector is updated by adding the global feature vector of the gold-standard partial output and subtracting the global feature vector of t</context>
</contexts>
<marker>Collins, Roark, 2004</marker>
<rawString>Michael Collins and Brian Roark. 2004. Incremental parsing with the perceptron algorithm. In Proceedings ofACL, pages 111–118, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1--8</pages>
<location>Philadelphia, USA,</location>
<contexts>
<context position="10042" citStr="Collins, 2002" startWordPosition="1689" endWordPosition="1690">l the next incoming character is separated from the last character of this word. Before a word is confirmed as a full word, we only apply to it features that represent its current partial status, such as character bigrams, its starting character and its part-ofspeech, etc. Full word features, including the first and last characters of a word, are applied immediately after a word is confirmed as complete. An important component for our proposed system is the training process, which needs to ensure that the model scores a partial word with predicted POS properly. We use the averaged perceptron (Collins, 2002) for training, together with the “early update” mechanism of Collins and Roark (2004). Rather than updating the parameters after decoding is complete, the modified algorithm updates parameters at any processing step if the correct partial candidate falls out of the beam. In our experiments using the Chinese Treebank 1The next incoming characters are also a useful source of information for predicting the POS. However, our system achieved competitive accuracy with Z&amp;C08 without such character lookahead features. data, our system ran an order of magnitude faster than our Z&amp;C08 system with little </context>
<context position="16531" citStr="Collins, 2002" startWordPosition="2803" endWordPosition="2804">with the current character. A restriction to the maximum word length is applied to limit the number of iterations in this loop, without which the algorithm would have quadratic time complexity. In contrast, our decoder does not search backword for the possible starting character of any word. Segmentation ambiguities are resolved by binary choices between the actions append or separate for each character, and no POS enumeration is required when the character is appended. This improves the speed by a significant factor. 2.2 Training The learning algorithm is based on the generalized perceptron (Collins, 2002), but parameter adjustments can be performed at any character during the decoding process, using the “early update” mechanism of Collins and Roark (2004). The parameter vector of the model is initialized as all zeros before training, and used to decode training examples. Each training example is turned into the raw input format, and processed in the same way as decoding. After each character is processed, partial candidates in the agenda are compared to the corresponding gold-standard output for the same characters. If none of the candidates in the agenda are correct, the decoding is stopped a</context>
<context position="18060" citStr="Collins, 2002" startWordPosition="3055" endWordPosition="3056">orresponding gold-standard, the decoding process moves to the next character, without any change to the parameter values. After all characters are processed, the decoder prediction is compared with the training example. If the prediction is correct, the parameter vector is not changed; otherwise it is updated by adding the global feature vector of the training example and subtracting the global feature vector of the decoder prediction, just as the perceptron algorithm does. The same training examples can be used to train the model for multiple iterations. We use the averaged parameter vector (Collins, 2002) as the final model. Pseudocode for the training algorithm is shown in Figure 2. It is based on the decoding algorithm in Figure 1, and the main differences are: (1) the training algorithm takes the gold-standard output and the parameter vector as two additional arguments; (2) the training algorithm does not return a prediction, but modifies the parameter vector when necessary; (3) lines 11 to 20 are additional lines of code for parameter updates. Without lines 11 to 16, the training algorithm is exactly the same as the generalized perceptron algorithm. These lines are added to ensure that the</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms. In Proceedings of EMNLP, pages 1–8, Philadelphia, USA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbin Jiang</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
<author>Yajuan L¨u</author>
</authors>
<title>A cascaded linear model for joint Chinese word segmentation and part-of-speech tagging.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL/HLT,</booktitle>
<pages>897--904</pages>
<location>Columbus, Ohio,</location>
<marker>Jiang, Huang, Liu, L¨u, 2008</marker>
<rawString>Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan L¨u. 2008a. A cascaded linear model for joint Chinese word segmentation and part-of-speech tagging. In Proceedings ofACL/HLT, pages 897–904, Columbus, Ohio, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbin Jiang</author>
<author>Haitao Mi</author>
<author>Qun Liu</author>
</authors>
<title>Word lattice reranking for Chinese word segmentation and part-of-speech tagging.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>385--392</pages>
<location>Manchester, UK,</location>
<contexts>
<context position="1533" citStr="Jiang et al., 2008" startWordPosition="242" endWordPosition="245">t partial candidate as well as a full output. Effective scoring of partial structures allows the decoder to give high accuracy with a small beam-size of 16. In our 10-fold crossvalidation experiments with the Chinese Treebank, our system performed over 10 times as fast as Zhang and Clark (2008) with little accuracy loss. The accuracy of our system on the standard CTB 5 test was competitive with the best in the literature. 1 Introduction and Motivation Several approaches have been proposed to solve word segmentation and POS-tagging jointly, including the reranking approach (Shi and Wang, 2007; Jiang et al., 2008b), the hybrid approach (Nakagawa and Uchimoto, 2007; Jiang et al., 2008a), and the single-model approach (Ng and Low, 2004; Zhang and Clark, 2008; Kruengkrai et al., 2009). These methods led to accuracy improvements over the traditional, pipelined segmentation and POS-tagging baseline by avoiding segmentation error propagation and making use of part-of-speech information to improve segmentation. The single-model approach to joint segmentation and POS-tagging offers consistent training of all information, concerning words, characters and partsof-speech. However, exact inference with dynamic pr</context>
<context position="28621" citStr="Jiang et al. (2008" startWordPosition="4879" endWordPosition="4882">e for spaces, numerals, symbols, alphabets, Chinese and other characters. In the previous experiments, our system did not use any knowledge beyond the training data. To make the comparison fairer, we included knowledge of English letters and Arabic numbers in this experiment. During both training and decoding, English letters and Arabic numbers are segmented using simple rules, treating consecutive English letters or Arabic numbers as a single word. The results are shown in Table 4, where row “N07” refers to the model of Nakagawa and Uchimoto (2007), rows “J08a” and “b” refer to the models of Jiang et al. (2008a) and Jiang et al. (2008b), and row “K09” refers to the models of Kruengkrai et al. (2009). Columns “sf” and “jf” refer to segmentation and joint accuracies, respectively. Our system Sections Sentences Words Training 1–270 18,085 493,892 400–931 1001–1151 Dev 301–325 350 6,821 Test 271–300 348 8,008 Table 3: Training, development and test data on CTB 5. sf jf K09 (error-driven) 97.87 93.67 our system 97.78 93.67 K09 (baseline) 97.79 93.60 J08a 97.85 93.41 J08b 97.74 93.37 N07 97.83 93.32 Table 4: Accuracy comparisons with recent studies on CTB 5. gave comparable accuracies to these recent wor</context>
</contexts>
<marker>Jiang, Mi, Liu, 2008</marker>
<rawString>Wenbin Jiang, Haitao Mi, and Qun Liu. 2008b. Word lattice reranking for Chinese word segmentation and part-of-speech tagging. In Proceedings of COLING, pages 385–392, Manchester, UK, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Canasai Kruengkrai</author>
<author>Kiyotaka Uchimoto</author>
<author>Jun’ichi Kazama</author>
<author>Yiou Wang</author>
<author>Kentaro Torisawa</author>
<author>Hitoshi Isahara</author>
</authors>
<title>An error-driven word-character hybrid model for joint Chinese word segmentation and POS tagging.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL/AFNLP,</booktitle>
<pages>513--521</pages>
<location>Suntec, Singapore,</location>
<contexts>
<context position="1705" citStr="Kruengkrai et al., 2009" startWordPosition="269" endWordPosition="272">-fold crossvalidation experiments with the Chinese Treebank, our system performed over 10 times as fast as Zhang and Clark (2008) with little accuracy loss. The accuracy of our system on the standard CTB 5 test was competitive with the best in the literature. 1 Introduction and Motivation Several approaches have been proposed to solve word segmentation and POS-tagging jointly, including the reranking approach (Shi and Wang, 2007; Jiang et al., 2008b), the hybrid approach (Nakagawa and Uchimoto, 2007; Jiang et al., 2008a), and the single-model approach (Ng and Low, 2004; Zhang and Clark, 2008; Kruengkrai et al., 2009). These methods led to accuracy improvements over the traditional, pipelined segmentation and POS-tagging baseline by avoiding segmentation error propagation and making use of part-of-speech information to improve segmentation. The single-model approach to joint segmentation and POS-tagging offers consistent training of all information, concerning words, characters and partsof-speech. However, exact inference with dynamic programming can be infeasible if features are defined over a large enough range of the output, such as over a two-word history. In our previous work (Zhang and Clark, 2008), </context>
<context position="27750" citStr="Kruengkrai et al. (2009)" startWordPosition="4728" endWordPosition="4731">.76 24.81 8 94.98 90.01 418.54 2.47 94.87 90.30 39.81 26.02 9 95.23 90.84 471.3 2.20 95.21 90.55 42.03 26.65 10 96.49 92.11 500.72 2.08 96.33 92.12 43.12 24.03 average 96.00 91.41 469.90 2.24 95.84 91.37 41.88 24.94 Table 2: Speed and acccuracy comparisons with Z&amp;C08 by 10-fold cross validation. seconds) and testing speed (in the number of sentences per second), respectively. Our system gave a joint segmentation and POStagging F-score of 91.37%, which is only 0.04% lower than that of ZPar 0.2. The speed of our system was over 10 times as fast as ZPar 0.2. 3.3 Final tests using CTB 5 We follow Kruengkrai et al. (2009) and split the CTB 5 into training, development testing and testing sets, as shown in Table 3. We ignored the development test data since our system had been developed in previous experiments. Kruengkrai et al. (2009) made use of character type knowledge for spaces, numerals, symbols, alphabets, Chinese and other characters. In the previous experiments, our system did not use any knowledge beyond the training data. To make the comparison fairer, we included knowledge of English letters and Arabic numbers in this experiment. During both training and decoding, English letters and Arabic numbers </context>
<context position="29887" citStr="Kruengkrai et al. (2009)" startWordPosition="5084" endWordPosition="5087">driven version of K09) joint F-score. 4 Related Work The effectiveness of our beam-search decoder showed that the joint segmentation and tagging problem may be less complex than previously perceived (Zhang and Clark, 2008; Jiang et al., 2008a). At the very least, the single model approach with a simple decoder achieved competitive accuracies to what has been achieved so far by the reranking (Shi 850 and Wang, 2007; Jiang et al., 2008b) models and an ensemble model using machine-translation techniques (Jiang et al., 2008a). This may shed new light on joint segmentation and POS-tagging methods. Kruengkrai et al. (2009) and Zhang and Clark (2008) are the most similar to our system among related work. Both systems use a discriminatively trained linear model to score candidate outputs. The work of Kruengkrai et al. (2009) is based on Nakagawa and Uchimoto (2007), which separates the processing of known words and unknown words, and uses a set of segmentation tags to represent the segmentation of characters. In contrast, our model is conceptually simpler, and does not differentiate known words and unknown words. Moreover, our model is based on our previous work, in line with Zhang and Clark (2007), which does no</context>
</contexts>
<marker>Kruengkrai, Uchimoto, Kazama, Wang, Torisawa, Isahara, 2009</marker>
<rawString>Canasai Kruengkrai, Kiyotaka Uchimoto, Jun’ichi Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi Isahara. 2009. An error-driven word-character hybrid model for joint Chinese word segmentation and POS tagging. In Proceedings of ACL/AFNLP, pages 513– 521, Suntec, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuji Nakagawa</author>
<author>Kiyotaka Uchimoto</author>
</authors>
<title>A hybrid approach to word segmentation and POS tagging.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL Demo and Poster Session,</booktitle>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="1585" citStr="Nakagawa and Uchimoto, 2007" startWordPosition="249" endWordPosition="252">t. Effective scoring of partial structures allows the decoder to give high accuracy with a small beam-size of 16. In our 10-fold crossvalidation experiments with the Chinese Treebank, our system performed over 10 times as fast as Zhang and Clark (2008) with little accuracy loss. The accuracy of our system on the standard CTB 5 test was competitive with the best in the literature. 1 Introduction and Motivation Several approaches have been proposed to solve word segmentation and POS-tagging jointly, including the reranking approach (Shi and Wang, 2007; Jiang et al., 2008b), the hybrid approach (Nakagawa and Uchimoto, 2007; Jiang et al., 2008a), and the single-model approach (Ng and Low, 2004; Zhang and Clark, 2008; Kruengkrai et al., 2009). These methods led to accuracy improvements over the traditional, pipelined segmentation and POS-tagging baseline by avoiding segmentation error propagation and making use of part-of-speech information to improve segmentation. The single-model approach to joint segmentation and POS-tagging offers consistent training of all information, concerning words, characters and partsof-speech. However, exact inference with dynamic programming can be infeasible if features are defined </context>
<context position="28558" citStr="Nakagawa and Uchimoto (2007)" startWordPosition="4864" endWordPosition="4868">experiments. Kruengkrai et al. (2009) made use of character type knowledge for spaces, numerals, symbols, alphabets, Chinese and other characters. In the previous experiments, our system did not use any knowledge beyond the training data. To make the comparison fairer, we included knowledge of English letters and Arabic numbers in this experiment. During both training and decoding, English letters and Arabic numbers are segmented using simple rules, treating consecutive English letters or Arabic numbers as a single word. The results are shown in Table 4, where row “N07” refers to the model of Nakagawa and Uchimoto (2007), rows “J08a” and “b” refer to the models of Jiang et al. (2008a) and Jiang et al. (2008b), and row “K09” refers to the models of Kruengkrai et al. (2009). Columns “sf” and “jf” refer to segmentation and joint accuracies, respectively. Our system Sections Sentences Words Training 1–270 18,085 493,892 400–931 1001–1151 Dev 301–325 350 6,821 Test 271–300 348 8,008 Table 3: Training, development and test data on CTB 5. sf jf K09 (error-driven) 97.87 93.67 our system 97.78 93.67 K09 (baseline) 97.79 93.60 J08a 97.85 93.41 J08b 97.74 93.37 N07 97.83 93.32 Table 4: Accuracy comparisons with recent s</context>
<context position="30132" citStr="Nakagawa and Uchimoto (2007)" startWordPosition="5125" endWordPosition="5129">a). At the very least, the single model approach with a simple decoder achieved competitive accuracies to what has been achieved so far by the reranking (Shi 850 and Wang, 2007; Jiang et al., 2008b) models and an ensemble model using machine-translation techniques (Jiang et al., 2008a). This may shed new light on joint segmentation and POS-tagging methods. Kruengkrai et al. (2009) and Zhang and Clark (2008) are the most similar to our system among related work. Both systems use a discriminatively trained linear model to score candidate outputs. The work of Kruengkrai et al. (2009) is based on Nakagawa and Uchimoto (2007), which separates the processing of known words and unknown words, and uses a set of segmentation tags to represent the segmentation of characters. In contrast, our model is conceptually simpler, and does not differentiate known words and unknown words. Moreover, our model is based on our previous work, in line with Zhang and Clark (2007), which does not treat word segmentation as character sequence labeling. Our learning and decoding algorithms are also different from Kruengkrai et al. (2009). While Kruengkrai et al. (2009) perform dynamic programming and MIRA learning, we use beam-search to </context>
</contexts>
<marker>Nakagawa, Uchimoto, 2007</marker>
<rawString>Tetsuji Nakagawa and Kiyotaka Uchimoto. 2007. A hybrid approach to word segmentation and POS tagging. In Proceedings of ACL Demo and Poster Session, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Jin Kiat Low</author>
</authors>
<title>Chinese part-ofspeech tagging: One-at-a-time or all-at-once? wordbased or character-based?</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="1656" citStr="Ng and Low, 2004" startWordPosition="261" endWordPosition="264">y with a small beam-size of 16. In our 10-fold crossvalidation experiments with the Chinese Treebank, our system performed over 10 times as fast as Zhang and Clark (2008) with little accuracy loss. The accuracy of our system on the standard CTB 5 test was competitive with the best in the literature. 1 Introduction and Motivation Several approaches have been proposed to solve word segmentation and POS-tagging jointly, including the reranking approach (Shi and Wang, 2007; Jiang et al., 2008b), the hybrid approach (Nakagawa and Uchimoto, 2007; Jiang et al., 2008a), and the single-model approach (Ng and Low, 2004; Zhang and Clark, 2008; Kruengkrai et al., 2009). These methods led to accuracy improvements over the traditional, pipelined segmentation and POS-tagging baseline by avoiding segmentation error propagation and making use of part-of-speech information to improve segmentation. The single-model approach to joint segmentation and POS-tagging offers consistent training of all information, concerning words, characters and partsof-speech. However, exact inference with dynamic programming can be infeasible if features are defined over a large enough range of the output, such as over a two-word histor</context>
</contexts>
<marker>Ng, Low, 2004</marker>
<rawString>Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-ofspeech tagging: One-at-a-time or all-at-once? wordbased or character-based? In Proceedings of EMNLP, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Kristy Hollingshead</author>
</authors>
<title>Classifying chart cells for quadratic complexity context-free inference.</title>
<date>2008</date>
<journal>Organizing Committee.</journal>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>745--752</pages>
<location>Manchester, UK,</location>
<contexts>
<context position="3064" citStr="Roark and Hollingshead, 2008" startWordPosition="483" endWordPosition="486">uctures for each character, which can be seen as a dynamic programming chart which is greatly reduced by pruning. In this paper we follow the line of single-model research, in particular the global linear model of Z&amp;C08. We show that effective decoding can be achieved with standard beam-search, which gives significant speed improvements compared to the decoding algorithm of Z&amp;C08, and achieves accuracies that are competitive with the state-of-the-art. Our research is also in line with recent research on improving the speed of NLP systems with little or no accuracy loss (Charniak et al., 2006; Roark and Hollingshead, 2008). Our speed improvement is achieved by the use of a single-beam decoder. Given an input sentence, candidate outputs are built incrementally, one character at a time. When each character is processed, it is combined with existing candidates in all possible ways to generate new candidates, and an agenda is used to keep the N-best candidate outputs from 843 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 843–852, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics the begining of the sentence to the current cha</context>
</contexts>
<marker>Roark, Hollingshead, 2008</marker>
<rawString>Brian Roark and Kristy Hollingshead. 2008. Classifying chart cells for quadratic complexity context-free inference. In Proceedings of COLING, pages 745– 752, Manchester, UK, August. Coling 2008 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yanxin Shi</author>
<author>Mengqiu Wang</author>
</authors>
<title>A dual-layer CRF based joint decoding method for cascade segmentation and labelling tasks.</title>
<date>2007</date>
<booktitle>In Proceedings of IJCAI,</booktitle>
<location>Hyderabad, India.</location>
<contexts>
<context position="1513" citStr="Shi and Wang, 2007" startWordPosition="238" endWordPosition="241">gh score to a correct partial candidate as well as a full output. Effective scoring of partial structures allows the decoder to give high accuracy with a small beam-size of 16. In our 10-fold crossvalidation experiments with the Chinese Treebank, our system performed over 10 times as fast as Zhang and Clark (2008) with little accuracy loss. The accuracy of our system on the standard CTB 5 test was competitive with the best in the literature. 1 Introduction and Motivation Several approaches have been proposed to solve word segmentation and POS-tagging jointly, including the reranking approach (Shi and Wang, 2007; Jiang et al., 2008b), the hybrid approach (Nakagawa and Uchimoto, 2007; Jiang et al., 2008a), and the single-model approach (Ng and Low, 2004; Zhang and Clark, 2008; Kruengkrai et al., 2009). These methods led to accuracy improvements over the traditional, pipelined segmentation and POS-tagging baseline by avoiding segmentation error propagation and making use of part-of-speech information to improve segmentation. The single-model approach to joint segmentation and POS-tagging offers consistent training of all information, concerning words, characters and partsof-speech. However, exact infer</context>
</contexts>
<marker>Shi, Wang, 2007</marker>
<rawString>Yanxin Shi and Mengqiu Wang. 2007. A dual-layer CRF based joint decoding method for cascade segmentation and labelling tasks. In Proceedings of IJCAI, Hyderabad, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
</authors>
<title>The part-of-speech tagging guidelines for the Chinese Treebank (3.0).</title>
<date>2000</date>
<contexts>
<context position="20303" citStr="Xia, 2000" startWordPosition="3438" endWordPosition="3439">to each training example before it is processed. Second, a tag dictionary is used to record POStags associated with each word. During decoding, frequent words and words with “closed set” tags2 are only allowed POS-tags according to the tag dictionary, while other words are allowed every POS-tag to make candidate outputs. Whether a word is a frequent word is decided by the number of times it has been seen in the training process. Denoting the num2“Closed set” tags are the set of POS-tags which are only associated with a fixed set of words, according to the Penn Chinese Treebank specifications (Xia, 2000). 847 function TRAIN(sent, agenda, gold-standard, w—): 1: CLEAR(agenda) 2: ADDITEM(agenda, “”) 3: for index in [0..LEN(sent)]: 4: for cand in agenda: 5: new ← APPEND(cand, sent[index]) 6: ADDITEM(agenda, new) 7: for pos in TAGSET(): 8: new ← SEP(cand, sent[index], pos) 9: ADDITEM(agenda, new) 10: agenda ← N-BEST(agenda) 11: for cand in agenda: 12: if cand = gold-standard[0:index]: 13: CONTINUE 14: w� ← w� + (gold-standard[0:index]) 15: w� ← w� - (BEST(agenda)) 16: return 17: if BEST(agenda) 74 gold-standard: 18: w� ← w� + (gold-standard) 19: w� ← w� - (BEST(agenda)) 20: return 21: return Figur</context>
</contexts>
<marker>Xia, 2000</marker>
<rawString>Fei Xia, 2000. The part-of-speech tagging guidelines for the Chinese Treebank (3.0).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Chinese segmentation with a word-based perceptron algorithm.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>840--847</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="30472" citStr="Zhang and Clark (2007)" startWordPosition="5181" endWordPosition="5184">ing methods. Kruengkrai et al. (2009) and Zhang and Clark (2008) are the most similar to our system among related work. Both systems use a discriminatively trained linear model to score candidate outputs. The work of Kruengkrai et al. (2009) is based on Nakagawa and Uchimoto (2007), which separates the processing of known words and unknown words, and uses a set of segmentation tags to represent the segmentation of characters. In contrast, our model is conceptually simpler, and does not differentiate known words and unknown words. Moreover, our model is based on our previous work, in line with Zhang and Clark (2007), which does not treat word segmentation as character sequence labeling. Our learning and decoding algorithms are also different from Kruengkrai et al. (2009). While Kruengkrai et al. (2009) perform dynamic programming and MIRA learning, we use beam-search to perform incremental decoding, and the early-update version of the perceptron algorithm to train the model. Dynamic programming is exact inference, for which the time complexity is decided by the locality of feature templates. In contrast, beam-search is approximate and can run in linear time. The parameter updating for our algorithm is co</context>
</contexts>
<marker>Zhang, Clark, 2007</marker>
<rawString>Yue Zhang and Stephen Clark. 2007. Chinese segmentation with a word-based perceptron algorithm. In Proceedings of ACL, pages 840–847, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Joint word segmentation and POS tagging using a single perceptron.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL/HLT,</booktitle>
<pages>888--896</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="1210" citStr="Zhang and Clark (2008)" startWordPosition="189" endWordPosition="192">rd features so that feature templates can be instantiated incrementally, according to whether the current character is separated or appended; (2) deciding the POS-tag of a potential word when its first character is processed. Early-update is used with perceptron training so that the linear model gives a high score to a correct partial candidate as well as a full output. Effective scoring of partial structures allows the decoder to give high accuracy with a small beam-size of 16. In our 10-fold crossvalidation experiments with the Chinese Treebank, our system performed over 10 times as fast as Zhang and Clark (2008) with little accuracy loss. The accuracy of our system on the standard CTB 5 test was competitive with the best in the literature. 1 Introduction and Motivation Several approaches have been proposed to solve word segmentation and POS-tagging jointly, including the reranking approach (Shi and Wang, 2007; Jiang et al., 2008b), the hybrid approach (Nakagawa and Uchimoto, 2007; Jiang et al., 2008a), and the single-model approach (Ng and Low, 2004; Zhang and Clark, 2008; Kruengkrai et al., 2009). These methods led to accuracy improvements over the traditional, pipelined segmentation and POS-tagging</context>
<context position="29484" citStr="Zhang and Clark, 2008" startWordPosition="5018" endWordPosition="5021">–931 1001–1151 Dev 301–325 350 6,821 Test 271–300 348 8,008 Table 3: Training, development and test data on CTB 5. sf jf K09 (error-driven) 97.87 93.67 our system 97.78 93.67 K09 (baseline) 97.79 93.60 J08a 97.85 93.41 J08b 97.74 93.37 N07 97.83 93.32 Table 4: Accuracy comparisons with recent studies on CTB 5. gave comparable accuracies to these recent works, obtaining the best (same as the error-driven version of K09) joint F-score. 4 Related Work The effectiveness of our beam-search decoder showed that the joint segmentation and tagging problem may be less complex than previously perceived (Zhang and Clark, 2008; Jiang et al., 2008a). At the very least, the single model approach with a simple decoder achieved competitive accuracies to what has been achieved so far by the reranking (Shi 850 and Wang, 2007; Jiang et al., 2008b) models and an ensemble model using machine-translation techniques (Jiang et al., 2008a). This may shed new light on joint segmentation and POS-tagging methods. Kruengkrai et al. (2009) and Zhang and Clark (2008) are the most similar to our system among related work. Both systems use a discriminatively trained linear model to score candidate outputs. The work of Kruengkrai et al.</context>
</contexts>
<marker>Zhang, Clark, 2008</marker>
<rawString>Yue Zhang and Stephen Clark. 2008. Joint word segmentation and POS tagging using a single perceptron. In Proceedings ofACL/HLT, pages 888–896, Columbus, Ohio, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>