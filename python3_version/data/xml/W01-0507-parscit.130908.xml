<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007555">
<title confidence="0.995927">
Feature Space Restructuring for SVMs
with Application to Text Categorization
</title>
<author confidence="0.998312">
Hiroya Takamura and Yuji Matsumoto
</author>
<affiliation confidence="0.999786">
Department of Information Technology
Nara Institute of Science and Technology
</affiliation>
<address confidence="0.958983">
8516-9, Takayama, Ikoma, 630-0101 Japan
</address>
<email confidence="0.999539">
{hiroya-t,matsu}@is.aist-nara.ac.jp
</email>
<sectionHeader confidence="0.98019" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999671583333333">
In this paper, we propose a new method of text
categorization based on feature space restruc-
turing for SVMs. In our method, independent
components of document vectors are extracted
using ICA and concatenated with the original
vectors. This restructuring makes it possible
for SVMs to focus on the latent semantic space
without losing information given by the original
feature space. Using this method, we achieved
high performance in text categorization both
with small number and large numbers of labeled
data.
</bodyText>
<sectionHeader confidence="0.996275" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999753103448276">
The task of text categorization has been exten-
sively studied in Natural Language Processing.
Most successful works rely on a large number
of classified data. However, it is hard to collect
classified data, so considering real applications,
text categorization must be realized even with a
small number of labeled data. Several methods
to realize it have been proposed so far (Nigam et
al, 2000), but they need to be further developed.
For that purpose, we have to take advantage of
invaluable information offered by the property
of unlabeled data. In this paper, we propose
a new categorization method based on Sup-
port Vector Machines (SVMs) (Vapnik, 1995)
and Independent Component Analysis (ICA)
(Herault and Jutten, 1986; Bell and Sejnowski,
1995). SVM is gaining popularity as a classi-
fier with high performance, and ICA is one of
the most prospective algorithms in the field of
signal processing, which extracts independent
components from mixed signals.
SVM has been applied in many applications
such as Image Processing and Natural Language
Processing. The idea to apply SVM for text cat-
egorization was first introduced in (Joachims,
1998). However, when the number of labeled
data are small, SVM often fails to produce a
good result, although several efforts against this
problem have been made. There are two strate-
gies for improving performance in the case of
a limited number of data. One is to modify
the learning algorithm itself (Joachims, 1999a;
Glenn and Mangasarian, 2001), and the other
is to process training data (Weston et al, 2000),
including the selection of features. In this pa-
per, we focus on the latter, especially on fea-
ture space restructuring. For processing train-
ing data, Principal Component Analysis (PCA)
is often adopted in classifiers such as k-Nearest
Neighbor method (Mitchell, 1997). But the con-
ventional dimension-reduction methods fail for
SVM as shown by experiments in Section 6. Un-
like the conventional ones, our approach uses
the components obtained with ICA to augment
the dimension of the feature space.
ICA is built on the assumptions that the
sources are independent of each other and that
the signals observed at multiple-points are lin-
ear mixtures of the sources. While the theoret-
ical aspects of ICA are being studied, its pos-
sibility to applications is often pointed out as
in (Bell and Sejnowski, 1997). The idea of us-
ing ICA for text clustering is adopted in sev-
eral works such as in (Isbell and Viola, 1998).
In those works, vector representation model is
adopted (i.e. each text is represented as a vector
with the word-frequencies as the elements). It
is reported however that the independent com-
ponents do not always correspond to the desired
classes, but represent some kind of characteris-
tics of texts (Kolenda et al, 2000). In (Kaban
and Girolami, 2000), they showed that the num-
ber of potential components were larger than
that of human-annotated classes. These facts
imply that it is not easy to apply ICA directly
for text classification.
Taking these observations into consideration,
we take the following strategy: first we perform
ICA on input document vectors, and second,
create the restructured information by concate-
nating the reduced vectors (i.e. the values of
the independent components) and the original
feature vectors.
PCA is an alternative restructuring method.
So we conducted experiments using SVM with
various input vectors: original feature vectors,
reduced feature vectors and restructured fea-
ture vectors (reduction and restructuring are
performed by PCA and ICA). For comparison,
we conducted experiments using Transductive
SVM (TSVM) (Joachims, 1999a) as well, which
is designed for the case of a small number of
labeled data.
Using the proposed method (SVM with ICA),
we obtain better results than ordinary SVM and
TSVM, with both small and large numbers of
labeled data.
</bodyText>
<sectionHeader confidence="0.935558" genericHeader="introduction">
2 Support Vector Machines
</sectionHeader>
<subsectionHeader confidence="0.9948945">
2.1 Brief Overview of Support Vector
Machines
</subsectionHeader>
<bodyText confidence="0.998955333333333">
Support Vector Machine (SVM) is one of the
large-margin classifiers (Smola et al, 2000).
Given a set of pairs,
</bodyText>
<figureCaption confidence="0.99908">
Figure 1: Support Vector Machine
</figureCaption>
<bodyText confidence="0.99431575">
(the solid line corresponds to the optimal hy-
perplane).
where ai&apos;s are Lagrange multipliers. Using the
ai&apos;s that maximize (4), w is expressed as
</bodyText>
<equation confidence="0.9358695">
w=� aiyixi. (5)
i
Substituting (5) into (2), we obtain
Margin
Positive example
Negative example
(x1, y1), (x2, y2), ... ,(xn, yn) (1) f(x) = Y, aiyixi • x + b. (6)
8i, xi 2 Rd, yi 2 f-1, 1g
</equation>
<bodyText confidence="0.99975675">
of a feature vector and a label, SVM constructs
a separating hyperplane with the largest margin
(the distance between the hyperplane and the
vectors, see Figure 1):
</bodyText>
<equation confidence="0.990421">
f(x) = w • x + b. (2)
</equation>
<bodyText confidence="0.991388">
Finding the largest margin is equivalent to min-
imizing the norm kwk, which is expressed as:
</bodyText>
<equation confidence="0.9758525">
min . 2 kwk2, 1(3)
s.t. 8i, yi(xi • w + b) — 1 &gt; 0.
</equation>
<bodyText confidence="0.762947">
This is realized by solving the quadratic pro-
gram (dual problem of (3)):
</bodyText>
<equation confidence="0.982068">
max. Ei ai — 2 Ei,j ai aj yi yj xi - xj (4)
s.t. Ei aiyi = 0,
8i, ai &gt; 0,
</equation>
<bodyText confidence="0.972056">
Unlabeled data are classified according to the
signs of (6).
</bodyText>
<subsectionHeader confidence="0.997625">
2.2 Kernel Method
</subsectionHeader>
<bodyText confidence="0.999521227272727">
SVM is a linear classifier and its separating abil-
ity is limited. To compensate this limitation,
Kernel Method is usually combined with SVM
(Vapnik, 1995).
In Kernel Method, the dot-product in (4) and
(6) is replaced by a more general inner-product
K(xi, x), called the kernel function. Polynomial
kernel (xi • xj + 1)d (d 2 N+) and RBF ker-
nel expf—kxi — xjk2/2Q2g are often used. Us-
ing kernel method means that feature vectors
are mapped into a (higher dimensional) Hilbert
space and linearly separated there. This map-
ping structure makes non-linear separation pos-
sible, although SVM is basically a linear classi-
fier.
Another advantage of kernel method is that
although it deals with a high dimensional (pos-
sibly infinite) Hilbert space, there is no need
to compute high dimensional vectors explicitly.
Only the general inner-products of two vectors
are needed. This leads to a relatively small com-
putational overhead.
</bodyText>
<subsectionHeader confidence="0.996697">
2.3 Transductive SVMs
</subsectionHeader>
<bodyText confidence="0.998218166666667">
The Transductive Support Vector Machine
(TSVM) is introduced in (Joachims, 1999a),
which is one realization of transductive learning
in (Vapnik, 1995). It is designed for the classifi-
cation with a small number of labeled data. Its
algorithm is approximately as follows:
</bodyText>
<listItem confidence="0.911002818181818">
1. construct a hyperplane using labeled data
in the same way as the ordinary SVMs.
2. classify the unlabeled (test) data according
to the current hyperplane.
3. select the pair of a positively classified sam-
ple and a negatively classified sample that
are nearest to the hyperplane.
4. exchange the labels of those samples, if the
margin gets larger by exchanging them.
5. terminate if a stopping-criterion is satisfied.
Otherwise, go back to step 2.
</listItem>
<bodyText confidence="0.9999615">
This is one way to search for the largest mar-
gin, permitting the relabeling of test data that
have already been labeled by the classifier in the
previous iteration.
</bodyText>
<sectionHeader confidence="0.991243" genericHeader="method">
3 Independent Component Analysis
</sectionHeader>
<bodyText confidence="0.987314285714286">
Independent Component Analysis (ICA) is a
method by which source signals are extracted
from mixed signals. It is based on the assump-
tions that the sources s E R&apos; are statisti-
cally independent of each other and that the
observed signals x E Rn are linear mixtures of
the sources:
</bodyText>
<equation confidence="0.973878">
x = As. (7)
</equation>
<bodyText confidence="0.999925">
Here the matrix A is called a mixing matrix. We
observe x as a time series and estimate both A
and s = (s1, • • • , s,,). So our purpose here is to
find a demixing matrix W such that sl, • • • , s.,
are as independent of each other as possible:
The computation proceeds by way of descent
learning with an objective function indicating
independence. There are several criteria of
independence and their learning rules, among
which we take here Infomax approach (Bell
and Sejnowski, 1995), but with natural gradi-
ent (Amari, 1998). Its learning rule is
</bodyText>
<equation confidence="0.9891">
ÆW = (I + (I — 2g(Wx))(Wx)t)W, (9)
</equation>
<bodyText confidence="0.954658">
where, g(u) = 1/(1 + exp(—u)).
</bodyText>
<sectionHeader confidence="0.878094" genericHeader="method">
4 Text Categorization Enhanced
with Feature Space Restructuring
</sectionHeader>
<bodyText confidence="0.998399333333333">
As in most previous works, we adopt Vector
Space Model (Salton and McGill, 1983) for
representing documents. In this framework,
each document d is represented as a vector
(fl, • • • , fd) with word-frequencies as its ele-
ments.
</bodyText>
<subsectionHeader confidence="0.983773">
4.1 Feature Space Restructuring
</subsectionHeader>
<bodyText confidence="0.999938571428571">
First we reduce the dimension of document vec-
tors using PCA or ICA. As for PCA, we fol-
low the previous work described in , e.g., (Deer-
wester et al, 1990). In (Isbell and Viola, 1998),
they use ICA for dimension reduction and ob-
tain a good result in Information Retrieval. At
the first step of our method, where the reduced
vectors are obtained, we follow their method.
In this framework, each document d is consid-
ered as a linear mixture of sources s representing
topics. Each wordplays a role of &amp;quot;microphone&amp;quot;
and receives a word-frequency in the document
as a mixed signal at each time unit. This for-
mulation is represented by the equation:
</bodyText>
<equation confidence="0.738963">
d = As, (10)
</equation>
<bodyText confidence="0.999842666666667">
where A is a mixing matrix. Although both A
and s are unknown, they can be obtained using
the independence assumption. The source sig-
nals s are considered as a reduced expression of
this document. In the case of PCA, the restruc-
turing is processed in the same way. The only
difference is that independent components cor-
respond to principal components for the PCA
case.
After computing a reduced vector s with PCA
or ICA, we concatenate the original vector d
and the reduced vector s:
</bodyText>
<equation confidence="0.8135265">
s = Wx. (8) d�= �d (11)
s
</equation>
<bodyText confidence="0.99813625">
This transformation means that we do not rely
only on the reduced information, but make use
of both the reduced and the original informa-
tion, that is, the restructured information.
</bodyText>
<subsectionHeader confidence="0.943494">
4.2 Text Categorization
</subsectionHeader>
<bodyText confidence="0.999841">
Regarding d as the input feature vector of a
document, we use SVM for categorization.
Since SVMs are binary classifiers themselves,
so we take here the one-versus-rest method to
apply them for multi-class classification tasks.
</bodyText>
<sectionHeader confidence="0.996257" genericHeader="method">
5 Theoretical Perspective
</sectionHeader>
<subsectionHeader confidence="0.995387">
5.1 Validation as a Kernel Function
</subsectionHeader>
<bodyText confidence="0.999917666666667">
The proposed feature restructuring method can
be considered as the use of a certain kernel for
the pre-restructured feature space. We give an
explanation for the linear case. Given two vec-
tors, d1 and d2, the kernel function K in the
restructured space is expressed as,
</bodyText>
<equation confidence="0.99591725">
dt
1 d2
= dt1d2 + st1s2
= dt1d2 + d&apos;AtAd2. (12)
</equation>
<bodyText confidence="0.9992906">
Considering the fact that each of two terms
above is a kernel and that the sum of two kernels
is also a kernel (Vapnik, 1995), the proposed re-
structuring is equivalent to using a certain ker-
nel in the pre-restructured space.
</bodyText>
<subsectionHeader confidence="0.888164">
5.2 Interpretation of Feature Space
Restructuring
</subsectionHeader>
<bodyText confidence="0.999993545454546">
The expression (12) shows that weights are put
on the latent semantic indices determined by
ICA and PCA respectively. The criterion of
meaningfulness depends on which of ICA and
PCA is used. Note that weighting is differ-
ent from reducing. In the dimension-reduction
methods, only the latent semantic space is con-
sidered, but in our method, the original feature
space still directly influences the classification
result.
This property of our method makes it pos-
sible to focus on the information given by the
latent semantic space, without losing informa-
tion given by the original feature space.
In text categorization, classes to be predicted
are sometimes characterized by local informa-
tion such as the occurrence of a certain word,
but sometimes dominated by global information
such as the total frequency of a certain group of
words. Considering this situation and the above
property of our method, it is not surprising that
out method gives a good result.
</bodyText>
<sectionHeader confidence="0.996053" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999977571428572">
To evaluate the proposed method, we conducted
several experiments.
The data used here is the Reuters-21578
dataset. The most frequent 6 categories are ex-
tracted from the training-set of the corpus. This
leaves 4872 documents (see Table 1). Some part
of them is used as training data and the rest is
used as test data. Only the words occurring
more than twice are used. Both stemming and
stop-word removal are performed. For compu-
tation, we used SVM-light (Joachims, 1999b).
We conducted two kinds of experiments. The
first one focuses on evaluating the performance
of the proposed method for each category, with
a fixed number of labeled data (Section 6.1).
The second one is conducted to show that the
proposed method gives a good result also when
the number of labeled data increases (Section
6.2).
The results are evaluated by F-measures.
To evaluate the performance across categories,
we computed Micro-average and Macro-average
(Yang, 1999) of F-measures. Micro-average is
obtained by first computing precision and re-
call for all categories and then using them to
compute the F-measure. Macro-average is com-
puted by first calculating F-measures for each
category and then averaging them. Micro-
average tends to be dominated by large-sized
categories, and Macro-average by small-sized
ones.
The kernel function used here is a linear ker-
nel. The number of independent or principal
components extracted by ICA or PCA is set to
50.
</bodyText>
<subsectionHeader confidence="0.995395">
6.1 Performance with a Fixed Number
of Data
</subsectionHeader>
<bodyText confidence="0.999985625">
In this experiment, we treated 100, 500, 1000
and 2000 samples as labeled respectively and
kept the other 4772, 4372, 3872 and 2872 sam-
ples unlabeled. The experiment was conducted
10 times for each sample-size repeatedly with
randomly selected labeled samples and their av-
erage values are computed. The result is shown
in Tables 2, 3, 4 and 5. In the row of &amp;quot;Method&amp;quot;,
</bodyText>
<equation confidence="0.950389">
d1,
d2) =
K(
</equation>
<tableCaption confidence="0.998105">
Table 1: Documents used in Experiments
</tableCaption>
<table confidence="0.976234714285714">
category number of documents
earn 2673
acq 1435
trade 225
crude 223
money-fx 176
interest 140
</table>
<bodyText confidence="0.996740176470588">
combinations of restructuring methods are writ-
ten. &amp;quot;Original&amp;quot; means the data of original docu-
ment vectors. &amp;quot;PCA&amp;quot; and &amp;quot;ICA&amp;quot; mean the data
of only reduced vectors, respectively. &amp;quot;Orig-
inal+PCA&amp;quot; and &amp;quot;Original+ICA&amp;quot; are the re-
structured data explained in Section 4.
The proposed method yields a high F-
measure in all the categories for 1000 and 2000
labeled data and in most categories for 100 and
500 labeled data. The last two rows of Tables
2, 3, 4 and 5 show that both Micro-average
and Macro-average are the highest for the pro-
posed method. This means that the proposed
method performs well both for large-sized cat-
egories (e.g., earn) and small-sized categories
(e.g., interest), regardless with the number of
labeled data.
</bodyText>
<subsectionHeader confidence="0.9684755">
6.2 Performance for the Increase of the
Labeled Data
</subsectionHeader>
<bodyText confidence="0.9999946">
To investigate how each method behaves when
the number of labeled data increases, we con-
ducted this experiment. The number of labeled
data ranges from 100 to 2000. The results are
shown in Figure 2 and Figure 3. &amp;quot;PCA&amp;quot; gives a
good score only with a small number of data and
&amp;quot;Original&amp;quot; gives a good score only with a large
number of data. In contrast to them, the pro-
posed method produces high performance both
with small and large numbers of data.
</bodyText>
<sectionHeader confidence="0.997115" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999148777777778">
We proposed a new method of feature space re-
structuring for SVM. In our method, indepen-
dent components are extracted using ICA and
concatenated with the original vectors. Using
this new vectors in the restructured space, we
achieved high performance both with small and
large numbers of labeled data.
The proposed method can be applied also
to other machine learning algorithms provided
</bodyText>
<figureCaption confidence="0.978951">
Figure 2: Micro-average
</figureCaption>
<figure confidence="0.994081">
0 200 400 600 800 1000 1200 1400 1600 1800 2000
Number of Labeled Data
</figure>
<figureCaption confidence="0.999802">
Figure 3: Macro-average
</figureCaption>
<bodyText confidence="0.9998389">
that they are robust against noise and can han-
dle a high-dimensional feature space. From this
point of view, it is expected that the proposed
method is useful for kernel-based methods, to
which SVM belongs.
As a future work, we need to find a way to de-
cide the number of independent components to
be extracted. In this paper, we set the number
to 50 in an ad-hoc way. However, the appropri-
ate number must be predicted based on a theo-
</bodyText>
<figure confidence="0.985359129032258">
96
94
92
90
88
86
84
82
80
78
Original+ICA
PCA
Original
Original(TEU)l
0 200 400 600 800 1000 1200 1400 1600 1800 2000
Number of Labeled Data
Micro-average of F-measures
Macro-average of F-measures
95
90
85
80
75
70
65
60
55
Original+ICA
PCA
Original
Original(TEM)
</figure>
<tableCaption confidence="0.93313">
Table 2: F-Measures (100 Labeled Data)
</tableCaption>
<table confidence="0.999768444444444">
Method Original Original(TSVM) PCA ICA Original+PCA Original+ICA
earn 92.96 84.00 91.13 86.60 92.97 92.88
acq 85.88 81.42 85.67 80.86 85.91 87.48
trade 36.52 65.59 72.41 72.28 36.68 70.73
crude 65.69 70.90 79.75 80.67 65.93 82.87
money-fx 32.46 45.01 52.69 54.37 32.47 48.62
interest 51.30 52.69 64.44 63.48 51.30 64.84
microaverage 83.63 79.48 85.98 82.14 83.66 87.40
macroaverage 60.80 66.60 74.34 73.04 60.87 74.56
</table>
<tableCaption confidence="0.913786">
Table 3: F-Measures (500 Labeled Data)
</tableCaption>
<table confidence="0.999435111111111">
Method Original Original(TSVM) PCA ICA Original+PCA Original+ICA
earn 96.49 93.97 94.38 93.45 96.49 96.70
acq 93.23 91.57 89.18 87.45 93.22 93.41
trade 86.31 80.81 87.42 86.58 86.37 91.70
crude 83.33 79.78 81.36 78.28 83.43 87.12
money-fx 62.94 64.88 72.83 73.45 63.17 73.99
interest 59.31 52.02 73.37 72.18 59.31 70.41
microaverage 92.17 89.75 90.54 89.33 92.19 93.48
macroaverage 80.26 77.17 83.09 81.89 80.34 85.55
</table>
<tableCaption confidence="0.925083">
Table 4: F-Measures (1000 Labeled Data)
</tableCaption>
<table confidence="0.998194777777778">
Method Original Original(TSVM) PCA ICA Original+PCA Original+ICA
earn 97.15 95.52 96.07 95.53 97.15 97.26
acq 94.60 93.77 92.18 91.44 94.60 94.84
trade 91.19 86.11 87.13 86.87 91.23 93.25
crude 87.99 80.03 80.93 78.75 87.99 89.41
money-fx 73.68 68.85 72.96 72.68 69.96 80.99
interest 75.34 57.26 72.83 68.25 75.34 79.27
microaverage 94.23 91.79 92.31 91.54 94.09 94.90
macroaverage 86.65 80.25 83.68 82.25 86.04 89.17
</table>
<tableCaption confidence="0.867529">
Table 5: F-Measures (2000 Labeled Data)
</tableCaption>
<table confidence="0.622894555555556">
Method Original Original(TSVM) PCA ICA Original+PCA Original+ICA
earn 97.48 95.92 97.18 97.12 97.48 97.55
acq 95.39 94.39 94.78 94.80 95.39 95.65
trade 93.81 86.33 88.61 85.28 93.81 95.90
crude 89.88 80.35 82.63 78.56 89.88 90.25
money-fx 77.44 70.60 74.84 70.69 77.49 81.56
interest 82.71 62.15 73.99 68.46 82.76 83.02
microaverage 95.19 92.43 93.93 93.26 95.20 95.58
macroaverage 89.45 81.62 85.33 82.48 89.47 90.65
</table>
<bodyText confidence="0.998204384615384">
retical reason. Toward this problem, theories of
model selection such as Minimum Description
Length (Rissanen, 1987) or Akaike Information
Criterion (Akaike, 1974) could be a good theo-
retical basis.
As explained in Section 4, two terms d&apos;1d2
and d1A&apos;Ad2 are simply concatenated in our
method. But either of these terms can be mul-
tiplied with a certain constant. This means that
either of the original space and the Latent Se-
mantic Space can be weighted. Searching for
the best weighting scheme is one of the future
works.
</bodyText>
<sectionHeader confidence="0.96925" genericHeader="acknowledgments">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.999583">
We would like to thank Thomas Kolenda
(Technical University of Denmark) for helping
us with the code.
</bodyText>
<sectionHeader confidence="0.997773" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999780311688312">
Akaike, H. 1974. A New Look at the Statis-
tical Model Identification. IEEE Trans. Au-
tom. Control, vol. AC-19, pp. 716{723.
Amari, S. 1998. Natural Gradient Works Effi-
ciently in Learning. Neural Computation, vol.
10-2, pp. 251{276.
Bell, A. J. and Sejnowski, T. J. 1995. An In-
formation Maximization Approach to Blind
Separation and Blind Deconvolution. Neural
Computation, 7, 1129{1159.
Bell, A. J. and Sejnowski, T. J. 1997. The
&apos;Independent Components&apos; of Natural Scenes
are Edge Filters. Vision Research, 37(23), pp.
3327{3338.
Deerwester, S., Dumais, T., Landauer, T., Fur-
nas, W. and Harshman, A. 1990. Indexing by
Latent Semantic Analysis. Journal of the So-
ciety for Information Science, 41(6), pp. 391{
497.
Glenn, F. and Mangasarian, O. 2001. Semi-
Supervised Support Vector Machines for Un-
labeled Data Classification. Optimization
Methods and Software, pp. 114.
Herault, J. and Jutten, J. 1986. Space or Time
Adaptive Signal Processing by Neural Net-
work Models. Neural networks for computing:
AIP conference proceedings 151, pp. 206{211.
Isbell, C. and Viola. P. 1998. Restructuring
Sparse High Dimensional Data for Effective
Retrieval. Advances in Neural Information
Processing Systems, volume 11.
Joachims, T. 1998. Text Categorization with
Support Vector Machines: Learning with
Many Relevant Features. Proceedings of the
European Conference on Machine Learning,
pp. 137{142.
Joachims, T. 1999a. Transductive Inference for
Text Classification using Support Vector Ma-
chines. Machine Learning - Proc. 16th Int&apos;l
Conf. (ICML &apos;99), pp. 200{209.
Joachims, T. 1999b. Making large-Scale SVM
Learning Practical. Advances in Kernel
Methods - Support Vector Learning, pp. 169{
184.
Kaban, A. and Girolami, M. 2000. Unsuper-
vised Topic Separation and Keyword Identifi-
cation in Document Collections: A Projection
Approach Technical Report.
Kolenda, T, Hansen, L., K. and Sigurdsson, S.
2000. Indepedent Components in Text . Ad-
vances in Independent Component Analysis,
Springer-Verlag, pp. 235{256.
Mitchell, T. 1997. Machine Learning, McGraw
Hill.
Nigam, K., McCallum, A., Thrun, S. and
Mitchell, T. 2000. Text Classification from
Labeled and Unlabeled Documents using EM.
Machine Learning, 39(23). pp. 103{134.
Rissanen, J. 1987. Stochastic Complexity.
Journal of Royal Statistical Society, Series B,
49(3), pp. 223{239.
Salton, G. and McGill, M. J. 1983. Introduction
to Modern Information Retrieval. McGraw-
Hill Book Company, New York.
Smola, A., Bartlett, P., Scholkopf, B. and Schu-
urmans, D. 2000. Advances in Large Margin
Classifiers. MIT Press
Vapnik, V. 1995. The Nature of Statistical
Learning Theory. Springer.
Weston, J., Mukherjee, S., Chapelle, O., Pon-
til, M., Poggio, T. and Vapnik, V. 2000. Fea-
ture Selection for SVMs. In Advances in Neu-
ral Information Processing Systems, volume
13.
Yang, Y. An Evaluation of Statistical Ap-
proaches to Text Categorization. Information
Retrieval, volume 1, 1-2, pp. 69{90.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.871106">
<title confidence="0.999903">Feature Space Restructuring for with Application to Text Categorization</title>
<author confidence="0.968499">Hiroya Takamura</author>
<author confidence="0.968499">Yuji</author>
<affiliation confidence="0.999916">Department of Information Nara Institute of Science and</affiliation>
<address confidence="0.992074">8516-9, Takayama, Ikoma, 630-0101</address>
<email confidence="0.968184">hiroya-t@is.aist-nara.ac.jp</email>
<email confidence="0.968184">matsu@is.aist-nara.ac.jp</email>
<abstract confidence="0.995081615384615">In this paper, we propose a new method of text categorization based on feature space restructuring for SVMs. In our method, independent components of document vectors are extracted using ICA and concatenated with the original vectors. This restructuring makes it possible for SVMs to focus on the latent semantic space without losing information given by the original feature space. Using this method, we achieved high performance in text categorization both with small number and large numbers of labeled data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H Akaike</author>
</authors>
<title>A New Look at the Statistical Model Identification.</title>
<date>1974</date>
<journal>IEEE Trans. Autom. Control,</journal>
<volume>19</volume>
<pages>716--723</pages>
<marker>Akaike, 1974</marker>
<rawString>Akaike, H. 1974. A New Look at the Statistical Model Identification. IEEE Trans. Autom. Control, vol. AC-19, pp. 716{723.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Amari</author>
</authors>
<title>Natural Gradient Works Efficiently in Learning.</title>
<date>1998</date>
<journal>Neural Computation,</journal>
<volume>vol.</volume>
<pages>10--2</pages>
<contexts>
<context position="8448" citStr="Amari, 1998" startWordPosition="1404" endWordPosition="1405">and that the observed signals x E Rn are linear mixtures of the sources: x = As. (7) Here the matrix A is called a mixing matrix. We observe x as a time series and estimate both A and s = (s1, • • • , s,,). So our purpose here is to find a demixing matrix W such that sl, • • • , s., are as independent of each other as possible: The computation proceeds by way of descent learning with an objective function indicating independence. There are several criteria of independence and their learning rules, among which we take here Infomax approach (Bell and Sejnowski, 1995), but with natural gradient (Amari, 1998). Its learning rule is ÆW = (I + (I — 2g(Wx))(Wx)t)W, (9) where, g(u) = 1/(1 + exp(—u)). 4 Text Categorization Enhanced with Feature Space Restructuring As in most previous works, we adopt Vector Space Model (Salton and McGill, 1983) for representing documents. In this framework, each document d is represented as a vector (fl, • • • , fd) with word-frequencies as its elements. 4.1 Feature Space Restructuring First we reduce the dimension of document vectors using PCA or ICA. As for PCA, we follow the previous work described in , e.g., (Deerwester et al, 1990). In (Isbell and Viola, 1998), they</context>
</contexts>
<marker>Amari, 1998</marker>
<rawString>Amari, S. 1998. Natural Gradient Works Efficiently in Learning. Neural Computation, vol. 10-2, pp. 251{276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A J Bell</author>
<author>T J Sejnowski</author>
</authors>
<title>An Information Maximization Approach to Blind Separation and Blind Deconvolution.</title>
<date>1995</date>
<journal>Neural Computation,</journal>
<volume>7</volume>
<pages>1129--1159</pages>
<contexts>
<context position="1552" citStr="Bell and Sejnowski, 1995" startWordPosition="230" endWordPosition="233">large number of classified data. However, it is hard to collect classified data, so considering real applications, text categorization must be realized even with a small number of labeled data. Several methods to realize it have been proposed so far (Nigam et al, 2000), but they need to be further developed. For that purpose, we have to take advantage of invaluable information offered by the property of unlabeled data. In this paper, we propose a new categorization method based on Support Vector Machines (SVMs) (Vapnik, 1995) and Independent Component Analysis (ICA) (Herault and Jutten, 1986; Bell and Sejnowski, 1995). SVM is gaining popularity as a classifier with high performance, and ICA is one of the most prospective algorithms in the field of signal processing, which extracts independent components from mixed signals. SVM has been applied in many applications such as Image Processing and Natural Language Processing. The idea to apply SVM for text categorization was first introduced in (Joachims, 1998). However, when the number of labeled data are small, SVM often fails to produce a good result, although several efforts against this problem have been made. There are two strategies for improving perform</context>
<context position="8407" citStr="Bell and Sejnowski, 1995" startWordPosition="1395" endWordPosition="1398">es s E R&apos; are statistically independent of each other and that the observed signals x E Rn are linear mixtures of the sources: x = As. (7) Here the matrix A is called a mixing matrix. We observe x as a time series and estimate both A and s = (s1, • • • , s,,). So our purpose here is to find a demixing matrix W such that sl, • • • , s., are as independent of each other as possible: The computation proceeds by way of descent learning with an objective function indicating independence. There are several criteria of independence and their learning rules, among which we take here Infomax approach (Bell and Sejnowski, 1995), but with natural gradient (Amari, 1998). Its learning rule is ÆW = (I + (I — 2g(Wx))(Wx)t)W, (9) where, g(u) = 1/(1 + exp(—u)). 4 Text Categorization Enhanced with Feature Space Restructuring As in most previous works, we adopt Vector Space Model (Salton and McGill, 1983) for representing documents. In this framework, each document d is represented as a vector (fl, • • • , fd) with word-frequencies as its elements. 4.1 Feature Space Restructuring First we reduce the dimension of document vectors using PCA or ICA. As for PCA, we follow the previous work described in , e.g., (Deerwester et al,</context>
</contexts>
<marker>Bell, Sejnowski, 1995</marker>
<rawString>Bell, A. J. and Sejnowski, T. J. 1995. An Information Maximization Approach to Blind Separation and Blind Deconvolution. Neural Computation, 7, 1129{1159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A J Bell</author>
<author>T J Sejnowski</author>
</authors>
<title>The &apos;Independent Components&apos; of Natural Scenes are Edge Filters.</title>
<date>1997</date>
<journal>Vision Research,</journal>
<volume>37</volume>
<issue>23</issue>
<pages>3327--3338</pages>
<contexts>
<context position="3158" citStr="Bell and Sejnowski, 1997" startWordPosition="493" endWordPosition="496">s (PCA) is often adopted in classifiers such as k-Nearest Neighbor method (Mitchell, 1997). But the conventional dimension-reduction methods fail for SVM as shown by experiments in Section 6. Unlike the conventional ones, our approach uses the components obtained with ICA to augment the dimension of the feature space. ICA is built on the assumptions that the sources are independent of each other and that the signals observed at multiple-points are linear mixtures of the sources. While the theoretical aspects of ICA are being studied, its possibility to applications is often pointed out as in (Bell and Sejnowski, 1997). The idea of using ICA for text clustering is adopted in several works such as in (Isbell and Viola, 1998). In those works, vector representation model is adopted (i.e. each text is represented as a vector with the word-frequencies as the elements). It is reported however that the independent components do not always correspond to the desired classes, but represent some kind of characteristics of texts (Kolenda et al, 2000). In (Kaban and Girolami, 2000), they showed that the number of potential components were larger than that of human-annotated classes. These facts imply that it is not easy</context>
</contexts>
<marker>Bell, Sejnowski, 1997</marker>
<rawString>Bell, A. J. and Sejnowski, T. J. 1997. The &apos;Independent Components&apos; of Natural Scenes are Edge Filters. Vision Research, 37(23), pp. 3327{3338.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Deerwester</author>
<author>T Dumais</author>
<author>T Landauer</author>
<author>W Furnas</author>
<author>A Harshman</author>
</authors>
<title>Indexing by Latent Semantic Analysis.</title>
<date>1990</date>
<journal>Journal of the Society for Information Science,</journal>
<volume>41</volume>
<issue>6</issue>
<pages>391--497</pages>
<contexts>
<context position="9013" citStr="Deerwester et al, 1990" startWordPosition="1502" endWordPosition="1506"> Sejnowski, 1995), but with natural gradient (Amari, 1998). Its learning rule is ÆW = (I + (I — 2g(Wx))(Wx)t)W, (9) where, g(u) = 1/(1 + exp(—u)). 4 Text Categorization Enhanced with Feature Space Restructuring As in most previous works, we adopt Vector Space Model (Salton and McGill, 1983) for representing documents. In this framework, each document d is represented as a vector (fl, • • • , fd) with word-frequencies as its elements. 4.1 Feature Space Restructuring First we reduce the dimension of document vectors using PCA or ICA. As for PCA, we follow the previous work described in , e.g., (Deerwester et al, 1990). In (Isbell and Viola, 1998), they use ICA for dimension reduction and obtain a good result in Information Retrieval. At the first step of our method, where the reduced vectors are obtained, we follow their method. In this framework, each document d is considered as a linear mixture of sources s representing topics. Each wordplays a role of &amp;quot;microphone&amp;quot; and receives a word-frequency in the document as a mixed signal at each time unit. This formulation is represented by the equation: d = As, (10) where A is a mixing matrix. Although both A and s are unknown, they can be obtained using the inde</context>
</contexts>
<marker>Deerwester, Dumais, Landauer, Furnas, Harshman, 1990</marker>
<rawString>Deerwester, S., Dumais, T., Landauer, T., Furnas, W. and Harshman, A. 1990. Indexing by Latent Semantic Analysis. Journal of the Society for Information Science, 41(6), pp. 391{ 497.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Glenn</author>
<author>O Mangasarian</author>
</authors>
<title>SemiSupervised Support Vector Machines for Unlabeled Data Classification. Optimization Methods and Software,</title>
<date>2001</date>
<pages>114</pages>
<contexts>
<context position="2291" citStr="Glenn and Mangasarian, 2001" startWordPosition="350" endWordPosition="353">ithms in the field of signal processing, which extracts independent components from mixed signals. SVM has been applied in many applications such as Image Processing and Natural Language Processing. The idea to apply SVM for text categorization was first introduced in (Joachims, 1998). However, when the number of labeled data are small, SVM often fails to produce a good result, although several efforts against this problem have been made. There are two strategies for improving performance in the case of a limited number of data. One is to modify the learning algorithm itself (Joachims, 1999a; Glenn and Mangasarian, 2001), and the other is to process training data (Weston et al, 2000), including the selection of features. In this paper, we focus on the latter, especially on feature space restructuring. For processing training data, Principal Component Analysis (PCA) is often adopted in classifiers such as k-Nearest Neighbor method (Mitchell, 1997). But the conventional dimension-reduction methods fail for SVM as shown by experiments in Section 6. Unlike the conventional ones, our approach uses the components obtained with ICA to augment the dimension of the feature space. ICA is built on the assumptions that t</context>
</contexts>
<marker>Glenn, Mangasarian, 2001</marker>
<rawString>Glenn, F. and Mangasarian, O. 2001. SemiSupervised Support Vector Machines for Unlabeled Data Classification. Optimization Methods and Software, pp. 114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Herault</author>
<author>J Jutten</author>
</authors>
<title>Space or Time Adaptive Signal Processing by Neural Network Models.</title>
<date>1986</date>
<booktitle>Neural networks for computing: AIP conference proceedings 151,</booktitle>
<pages>206--211</pages>
<contexts>
<context position="1525" citStr="Herault and Jutten, 1986" startWordPosition="226" endWordPosition="229">uccessful works rely on a large number of classified data. However, it is hard to collect classified data, so considering real applications, text categorization must be realized even with a small number of labeled data. Several methods to realize it have been proposed so far (Nigam et al, 2000), but they need to be further developed. For that purpose, we have to take advantage of invaluable information offered by the property of unlabeled data. In this paper, we propose a new categorization method based on Support Vector Machines (SVMs) (Vapnik, 1995) and Independent Component Analysis (ICA) (Herault and Jutten, 1986; Bell and Sejnowski, 1995). SVM is gaining popularity as a classifier with high performance, and ICA is one of the most prospective algorithms in the field of signal processing, which extracts independent components from mixed signals. SVM has been applied in many applications such as Image Processing and Natural Language Processing. The idea to apply SVM for text categorization was first introduced in (Joachims, 1998). However, when the number of labeled data are small, SVM often fails to produce a good result, although several efforts against this problem have been made. There are two strat</context>
</contexts>
<marker>Herault, Jutten, 1986</marker>
<rawString>Herault, J. and Jutten, J. 1986. Space or Time Adaptive Signal Processing by Neural Network Models. Neural networks for computing: AIP conference proceedings 151, pp. 206{211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P</author>
</authors>
<title>Restructuring Sparse High Dimensional Data for Effective Retrieval.</title>
<date>1998</date>
<booktitle>Advances in Neural Information Processing Systems,</booktitle>
<volume>11</volume>
<marker>P, 1998</marker>
<rawString>Isbell, C. and Viola. P. 1998. Restructuring Sparse High Dimensional Data for Effective Retrieval. Advances in Neural Information Processing Systems, volume 11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Text Categorization with Support Vector Machines: Learning with Many Relevant Features.</title>
<date>1998</date>
<booktitle>Proceedings of the European Conference on Machine Learning,</booktitle>
<pages>137--142</pages>
<contexts>
<context position="1948" citStr="Joachims, 1998" startWordPosition="295" endWordPosition="296"> unlabeled data. In this paper, we propose a new categorization method based on Support Vector Machines (SVMs) (Vapnik, 1995) and Independent Component Analysis (ICA) (Herault and Jutten, 1986; Bell and Sejnowski, 1995). SVM is gaining popularity as a classifier with high performance, and ICA is one of the most prospective algorithms in the field of signal processing, which extracts independent components from mixed signals. SVM has been applied in many applications such as Image Processing and Natural Language Processing. The idea to apply SVM for text categorization was first introduced in (Joachims, 1998). However, when the number of labeled data are small, SVM often fails to produce a good result, although several efforts against this problem have been made. There are two strategies for improving performance in the case of a limited number of data. One is to modify the learning algorithm itself (Joachims, 1999a; Glenn and Mangasarian, 2001), and the other is to process training data (Weston et al, 2000), including the selection of features. In this paper, we focus on the latter, especially on feature space restructuring. For processing training data, Principal Component Analysis (PCA) is ofte</context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>Joachims, T. 1998. Text Categorization with Support Vector Machines: Learning with Many Relevant Features. Proceedings of the European Conference on Machine Learning, pp. 137{142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Transductive Inference for Text Classification using Support Vector Machines. Machine Learning -</title>
<date>1999</date>
<booktitle>Proc. 16th Int&apos;l Conf. (ICML &apos;99),</booktitle>
<pages>200--209</pages>
<contexts>
<context position="2260" citStr="Joachims, 1999" startWordPosition="348" endWordPosition="349">prospective algorithms in the field of signal processing, which extracts independent components from mixed signals. SVM has been applied in many applications such as Image Processing and Natural Language Processing. The idea to apply SVM for text categorization was first introduced in (Joachims, 1998). However, when the number of labeled data are small, SVM often fails to produce a good result, although several efforts against this problem have been made. There are two strategies for improving performance in the case of a limited number of data. One is to modify the learning algorithm itself (Joachims, 1999a; Glenn and Mangasarian, 2001), and the other is to process training data (Weston et al, 2000), including the selection of features. In this paper, we focus on the latter, especially on feature space restructuring. For processing training data, Principal Component Analysis (PCA) is often adopted in classifiers such as k-Nearest Neighbor method (Mitchell, 1997). But the conventional dimension-reduction methods fail for SVM as shown by experiments in Section 6. Unlike the conventional ones, our approach uses the components obtained with ICA to augment the dimension of the feature space. ICA is </context>
<context position="4438" citStr="Joachims, 1999" startWordPosition="694" endWordPosition="695">vations into consideration, we take the following strategy: first we perform ICA on input document vectors, and second, create the restructured information by concatenating the reduced vectors (i.e. the values of the independent components) and the original feature vectors. PCA is an alternative restructuring method. So we conducted experiments using SVM with various input vectors: original feature vectors, reduced feature vectors and restructured feature vectors (reduction and restructuring are performed by PCA and ICA). For comparison, we conducted experiments using Transductive SVM (TSVM) (Joachims, 1999a) as well, which is designed for the case of a small number of labeled data. Using the proposed method (SVM with ICA), we obtain better results than ordinary SVM and TSVM, with both small and large numbers of labeled data. 2 Support Vector Machines 2.1 Brief Overview of Support Vector Machines Support Vector Machine (SVM) is one of the large-margin classifiers (Smola et al, 2000). Given a set of pairs, Figure 1: Support Vector Machine (the solid line corresponds to the optimal hyperplane). where ai&apos;s are Lagrange multipliers. Using the ai&apos;s that maximize (4), w is expressed as w=� aiyixi. (5)</context>
<context position="6791" citStr="Joachims, 1999" startWordPosition="1114" endWordPosition="1115">that feature vectors are mapped into a (higher dimensional) Hilbert space and linearly separated there. This mapping structure makes non-linear separation possible, although SVM is basically a linear classifier. Another advantage of kernel method is that although it deals with a high dimensional (possibly infinite) Hilbert space, there is no need to compute high dimensional vectors explicitly. Only the general inner-products of two vectors are needed. This leads to a relatively small computational overhead. 2.3 Transductive SVMs The Transductive Support Vector Machine (TSVM) is introduced in (Joachims, 1999a), which is one realization of transductive learning in (Vapnik, 1995). It is designed for the classification with a small number of labeled data. Its algorithm is approximately as follows: 1. construct a hyperplane using labeled data in the same way as the ordinary SVMs. 2. classify the unlabeled (test) data according to the current hyperplane. 3. select the pair of a positively classified sample and a negatively classified sample that are nearest to the hyperplane. 4. exchange the labels of those samples, if the margin gets larger by exchanging them. 5. terminate if a stopping-criterion is </context>
<context position="12555" citStr="Joachims, 1999" startWordPosition="2106" endWordPosition="2107">words. Considering this situation and the above property of our method, it is not surprising that out method gives a good result. 6 Experiments To evaluate the proposed method, we conducted several experiments. The data used here is the Reuters-21578 dataset. The most frequent 6 categories are extracted from the training-set of the corpus. This leaves 4872 documents (see Table 1). Some part of them is used as training data and the rest is used as test data. Only the words occurring more than twice are used. Both stemming and stop-word removal are performed. For computation, we used SVM-light (Joachims, 1999b). We conducted two kinds of experiments. The first one focuses on evaluating the performance of the proposed method for each category, with a fixed number of labeled data (Section 6.1). The second one is conducted to show that the proposed method gives a good result also when the number of labeled data increases (Section 6.2). The results are evaluated by F-measures. To evaluate the performance across categories, we computed Micro-average and Macro-average (Yang, 1999) of F-measures. Micro-average is obtained by first computing precision and recall for all categories and then using them to c</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Joachims, T. 1999a. Transductive Inference for Text Classification using Support Vector Machines. Machine Learning - Proc. 16th Int&apos;l Conf. (ICML &apos;99), pp. 200{209.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<date>1999</date>
<booktitle>Making large-Scale SVM Learning Practical. Advances in Kernel Methods - Support Vector Learning,</booktitle>
<pages>169--184</pages>
<contexts>
<context position="2260" citStr="Joachims, 1999" startWordPosition="348" endWordPosition="349">prospective algorithms in the field of signal processing, which extracts independent components from mixed signals. SVM has been applied in many applications such as Image Processing and Natural Language Processing. The idea to apply SVM for text categorization was first introduced in (Joachims, 1998). However, when the number of labeled data are small, SVM often fails to produce a good result, although several efforts against this problem have been made. There are two strategies for improving performance in the case of a limited number of data. One is to modify the learning algorithm itself (Joachims, 1999a; Glenn and Mangasarian, 2001), and the other is to process training data (Weston et al, 2000), including the selection of features. In this paper, we focus on the latter, especially on feature space restructuring. For processing training data, Principal Component Analysis (PCA) is often adopted in classifiers such as k-Nearest Neighbor method (Mitchell, 1997). But the conventional dimension-reduction methods fail for SVM as shown by experiments in Section 6. Unlike the conventional ones, our approach uses the components obtained with ICA to augment the dimension of the feature space. ICA is </context>
<context position="4438" citStr="Joachims, 1999" startWordPosition="694" endWordPosition="695">vations into consideration, we take the following strategy: first we perform ICA on input document vectors, and second, create the restructured information by concatenating the reduced vectors (i.e. the values of the independent components) and the original feature vectors. PCA is an alternative restructuring method. So we conducted experiments using SVM with various input vectors: original feature vectors, reduced feature vectors and restructured feature vectors (reduction and restructuring are performed by PCA and ICA). For comparison, we conducted experiments using Transductive SVM (TSVM) (Joachims, 1999a) as well, which is designed for the case of a small number of labeled data. Using the proposed method (SVM with ICA), we obtain better results than ordinary SVM and TSVM, with both small and large numbers of labeled data. 2 Support Vector Machines 2.1 Brief Overview of Support Vector Machines Support Vector Machine (SVM) is one of the large-margin classifiers (Smola et al, 2000). Given a set of pairs, Figure 1: Support Vector Machine (the solid line corresponds to the optimal hyperplane). where ai&apos;s are Lagrange multipliers. Using the ai&apos;s that maximize (4), w is expressed as w=� aiyixi. (5)</context>
<context position="6791" citStr="Joachims, 1999" startWordPosition="1114" endWordPosition="1115">that feature vectors are mapped into a (higher dimensional) Hilbert space and linearly separated there. This mapping structure makes non-linear separation possible, although SVM is basically a linear classifier. Another advantage of kernel method is that although it deals with a high dimensional (possibly infinite) Hilbert space, there is no need to compute high dimensional vectors explicitly. Only the general inner-products of two vectors are needed. This leads to a relatively small computational overhead. 2.3 Transductive SVMs The Transductive Support Vector Machine (TSVM) is introduced in (Joachims, 1999a), which is one realization of transductive learning in (Vapnik, 1995). It is designed for the classification with a small number of labeled data. Its algorithm is approximately as follows: 1. construct a hyperplane using labeled data in the same way as the ordinary SVMs. 2. classify the unlabeled (test) data according to the current hyperplane. 3. select the pair of a positively classified sample and a negatively classified sample that are nearest to the hyperplane. 4. exchange the labels of those samples, if the margin gets larger by exchanging them. 5. terminate if a stopping-criterion is </context>
<context position="12555" citStr="Joachims, 1999" startWordPosition="2106" endWordPosition="2107">words. Considering this situation and the above property of our method, it is not surprising that out method gives a good result. 6 Experiments To evaluate the proposed method, we conducted several experiments. The data used here is the Reuters-21578 dataset. The most frequent 6 categories are extracted from the training-set of the corpus. This leaves 4872 documents (see Table 1). Some part of them is used as training data and the rest is used as test data. Only the words occurring more than twice are used. Both stemming and stop-word removal are performed. For computation, we used SVM-light (Joachims, 1999b). We conducted two kinds of experiments. The first one focuses on evaluating the performance of the proposed method for each category, with a fixed number of labeled data (Section 6.1). The second one is conducted to show that the proposed method gives a good result also when the number of labeled data increases (Section 6.2). The results are evaluated by F-measures. To evaluate the performance across categories, we computed Micro-average and Macro-average (Yang, 1999) of F-measures. Micro-average is obtained by first computing precision and recall for all categories and then using them to c</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Joachims, T. 1999b. Making large-Scale SVM Learning Practical. Advances in Kernel Methods - Support Vector Learning, pp. 169{ 184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kaban</author>
<author>M Girolami</author>
</authors>
<title>Unsupervised Topic Separation and Keyword Identification in Document Collections: A Projection Approach</title>
<date>2000</date>
<tech>Technical Report.</tech>
<contexts>
<context position="3617" citStr="Kaban and Girolami, 2000" startWordPosition="572" endWordPosition="575">mixtures of the sources. While the theoretical aspects of ICA are being studied, its possibility to applications is often pointed out as in (Bell and Sejnowski, 1997). The idea of using ICA for text clustering is adopted in several works such as in (Isbell and Viola, 1998). In those works, vector representation model is adopted (i.e. each text is represented as a vector with the word-frequencies as the elements). It is reported however that the independent components do not always correspond to the desired classes, but represent some kind of characteristics of texts (Kolenda et al, 2000). In (Kaban and Girolami, 2000), they showed that the number of potential components were larger than that of human-annotated classes. These facts imply that it is not easy to apply ICA directly for text classification. Taking these observations into consideration, we take the following strategy: first we perform ICA on input document vectors, and second, create the restructured information by concatenating the reduced vectors (i.e. the values of the independent components) and the original feature vectors. PCA is an alternative restructuring method. So we conducted experiments using SVM with various input vectors: original</context>
</contexts>
<marker>Kaban, Girolami, 2000</marker>
<rawString>Kaban, A. and Girolami, M. 2000. Unsupervised Topic Separation and Keyword Identification in Document Collections: A Projection Approach Technical Report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kolenda</author>
<author>L Hansen</author>
<author>K</author>
<author>S Sigurdsson</author>
</authors>
<date>2000</date>
<booktitle>Indepedent Components in Text . Advances in Independent Component Analysis,</booktitle>
<pages>235--256</pages>
<publisher>Springer-Verlag,</publisher>
<contexts>
<context position="3586" citStr="Kolenda et al, 2000" startWordPosition="567" endWordPosition="570">ultiple-points are linear mixtures of the sources. While the theoretical aspects of ICA are being studied, its possibility to applications is often pointed out as in (Bell and Sejnowski, 1997). The idea of using ICA for text clustering is adopted in several works such as in (Isbell and Viola, 1998). In those works, vector representation model is adopted (i.e. each text is represented as a vector with the word-frequencies as the elements). It is reported however that the independent components do not always correspond to the desired classes, but represent some kind of characteristics of texts (Kolenda et al, 2000). In (Kaban and Girolami, 2000), they showed that the number of potential components were larger than that of human-annotated classes. These facts imply that it is not easy to apply ICA directly for text classification. Taking these observations into consideration, we take the following strategy: first we perform ICA on input document vectors, and second, create the restructured information by concatenating the reduced vectors (i.e. the values of the independent components) and the original feature vectors. PCA is an alternative restructuring method. So we conducted experiments using SVM with </context>
</contexts>
<marker>Kolenda, Hansen, K, Sigurdsson, 2000</marker>
<rawString>Kolenda, T, Hansen, L., K. and Sigurdsson, S. 2000. Indepedent Components in Text . Advances in Independent Component Analysis, Springer-Verlag, pp. 235{256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mitchell</author>
</authors>
<date>1997</date>
<booktitle>Machine Learning,</booktitle>
<publisher>Hill.</publisher>
<location>McGraw</location>
<contexts>
<context position="2623" citStr="Mitchell, 1997" startWordPosition="405" endWordPosition="406">ten fails to produce a good result, although several efforts against this problem have been made. There are two strategies for improving performance in the case of a limited number of data. One is to modify the learning algorithm itself (Joachims, 1999a; Glenn and Mangasarian, 2001), and the other is to process training data (Weston et al, 2000), including the selection of features. In this paper, we focus on the latter, especially on feature space restructuring. For processing training data, Principal Component Analysis (PCA) is often adopted in classifiers such as k-Nearest Neighbor method (Mitchell, 1997). But the conventional dimension-reduction methods fail for SVM as shown by experiments in Section 6. Unlike the conventional ones, our approach uses the components obtained with ICA to augment the dimension of the feature space. ICA is built on the assumptions that the sources are independent of each other and that the signals observed at multiple-points are linear mixtures of the sources. While the theoretical aspects of ICA are being studied, its possibility to applications is often pointed out as in (Bell and Sejnowski, 1997). The idea of using ICA for text clustering is adopted in several</context>
</contexts>
<marker>Mitchell, 1997</marker>
<rawString>Mitchell, T. 1997. Machine Learning, McGraw Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Nigam</author>
<author>A McCallum</author>
<author>S Thrun</author>
<author>T Mitchell</author>
</authors>
<title>Text Classification from Labeled and Unlabeled Documents using EM.</title>
<date>2000</date>
<booktitle>Machine Learning,</booktitle>
<volume>39</volume>
<issue>23</issue>
<pages>103--134</pages>
<contexts>
<context position="1196" citStr="Nigam et al, 2000" startWordPosition="174" endWordPosition="177"> semantic space without losing information given by the original feature space. Using this method, we achieved high performance in text categorization both with small number and large numbers of labeled data. 1 Introduction The task of text categorization has been extensively studied in Natural Language Processing. Most successful works rely on a large number of classified data. However, it is hard to collect classified data, so considering real applications, text categorization must be realized even with a small number of labeled data. Several methods to realize it have been proposed so far (Nigam et al, 2000), but they need to be further developed. For that purpose, we have to take advantage of invaluable information offered by the property of unlabeled data. In this paper, we propose a new categorization method based on Support Vector Machines (SVMs) (Vapnik, 1995) and Independent Component Analysis (ICA) (Herault and Jutten, 1986; Bell and Sejnowski, 1995). SVM is gaining popularity as a classifier with high performance, and ICA is one of the most prospective algorithms in the field of signal processing, which extracts independent components from mixed signals. SVM has been applied in many appli</context>
</contexts>
<marker>Nigam, McCallum, Thrun, Mitchell, 2000</marker>
<rawString>Nigam, K., McCallum, A., Thrun, S. and Mitchell, T. 2000. Text Classification from Labeled and Unlabeled Documents using EM. Machine Learning, 39(23). pp. 103{134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Rissanen</author>
</authors>
<title>Stochastic Complexity.</title>
<date>1987</date>
<journal>Journal of Royal Statistical Society, Series B,</journal>
<volume>49</volume>
<issue>3</issue>
<pages>223--239</pages>
<marker>Rissanen, 1987</marker>
<rawString>Rissanen, J. 1987. Stochastic Complexity. Journal of Royal Statistical Society, Series B, 49(3), pp. 223{239.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>M J McGill</author>
</authors>
<title>Introduction to Modern Information Retrieval.</title>
<date>1983</date>
<publisher>McGrawHill Book Company,</publisher>
<location>New York.</location>
<contexts>
<context position="8681" citStr="Salton and McGill, 1983" startWordPosition="1442" endWordPosition="1445"> here is to find a demixing matrix W such that sl, • • • , s., are as independent of each other as possible: The computation proceeds by way of descent learning with an objective function indicating independence. There are several criteria of independence and their learning rules, among which we take here Infomax approach (Bell and Sejnowski, 1995), but with natural gradient (Amari, 1998). Its learning rule is ÆW = (I + (I — 2g(Wx))(Wx)t)W, (9) where, g(u) = 1/(1 + exp(—u)). 4 Text Categorization Enhanced with Feature Space Restructuring As in most previous works, we adopt Vector Space Model (Salton and McGill, 1983) for representing documents. In this framework, each document d is represented as a vector (fl, • • • , fd) with word-frequencies as its elements. 4.1 Feature Space Restructuring First we reduce the dimension of document vectors using PCA or ICA. As for PCA, we follow the previous work described in , e.g., (Deerwester et al, 1990). In (Isbell and Viola, 1998), they use ICA for dimension reduction and obtain a good result in Information Retrieval. At the first step of our method, where the reduced vectors are obtained, we follow their method. In this framework, each document d is considered as </context>
</contexts>
<marker>Salton, McGill, 1983</marker>
<rawString>Salton, G. and McGill, M. J. 1983. Introduction to Modern Information Retrieval. McGrawHill Book Company, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Smola</author>
<author>P Bartlett</author>
<author>B Scholkopf</author>
<author>D Schuurmans</author>
</authors>
<title>Advances in Large Margin Classifiers.</title>
<date>2000</date>
<publisher>MIT Press</publisher>
<contexts>
<context position="4821" citStr="Smola et al, 2000" startWordPosition="757" endWordPosition="760">tors: original feature vectors, reduced feature vectors and restructured feature vectors (reduction and restructuring are performed by PCA and ICA). For comparison, we conducted experiments using Transductive SVM (TSVM) (Joachims, 1999a) as well, which is designed for the case of a small number of labeled data. Using the proposed method (SVM with ICA), we obtain better results than ordinary SVM and TSVM, with both small and large numbers of labeled data. 2 Support Vector Machines 2.1 Brief Overview of Support Vector Machines Support Vector Machine (SVM) is one of the large-margin classifiers (Smola et al, 2000). Given a set of pairs, Figure 1: Support Vector Machine (the solid line corresponds to the optimal hyperplane). where ai&apos;s are Lagrange multipliers. Using the ai&apos;s that maximize (4), w is expressed as w=� aiyixi. (5) i Substituting (5) into (2), we obtain Margin Positive example Negative example (x1, y1), (x2, y2), ... ,(xn, yn) (1) f(x) = Y, aiyixi • x + b. (6) 8i, xi 2 Rd, yi 2 f-1, 1g of a feature vector and a label, SVM constructs a separating hyperplane with the largest margin (the distance between the hyperplane and the vectors, see Figure 1): f(x) = w • x + b. (2) Finding the largest m</context>
</contexts>
<marker>Smola, Bartlett, Scholkopf, Schuurmans, 2000</marker>
<rawString>Smola, A., Bartlett, P., Scholkopf, B. and Schuurmans, D. 2000. Advances in Large Margin Classifiers. MIT Press</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory.</title>
<date>1995</date>
<publisher>Springer.</publisher>
<contexts>
<context position="1458" citStr="Vapnik, 1995" startWordPosition="219" endWordPosition="220">tensively studied in Natural Language Processing. Most successful works rely on a large number of classified data. However, it is hard to collect classified data, so considering real applications, text categorization must be realized even with a small number of labeled data. Several methods to realize it have been proposed so far (Nigam et al, 2000), but they need to be further developed. For that purpose, we have to take advantage of invaluable information offered by the property of unlabeled data. In this paper, we propose a new categorization method based on Support Vector Machines (SVMs) (Vapnik, 1995) and Independent Component Analysis (ICA) (Herault and Jutten, 1986; Bell and Sejnowski, 1995). SVM is gaining popularity as a classifier with high performance, and ICA is one of the most prospective algorithms in the field of signal processing, which extracts independent components from mixed signals. SVM has been applied in many applications such as Image Processing and Natural Language Processing. The idea to apply SVM for text categorization was first introduced in (Joachims, 1998). However, when the number of labeled data are small, SVM often fails to produce a good result, although sever</context>
<context position="5924" citStr="Vapnik, 1995" startWordPosition="974" endWordPosition="975">he distance between the hyperplane and the vectors, see Figure 1): f(x) = w • x + b. (2) Finding the largest margin is equivalent to minimizing the norm kwk, which is expressed as: min . 2 kwk2, 1(3) s.t. 8i, yi(xi • w + b) — 1 &gt; 0. This is realized by solving the quadratic program (dual problem of (3)): max. Ei ai — 2 Ei,j ai aj yi yj xi - xj (4) s.t. Ei aiyi = 0, 8i, ai &gt; 0, Unlabeled data are classified according to the signs of (6). 2.2 Kernel Method SVM is a linear classifier and its separating ability is limited. To compensate this limitation, Kernel Method is usually combined with SVM (Vapnik, 1995). In Kernel Method, the dot-product in (4) and (6) is replaced by a more general inner-product K(xi, x), called the kernel function. Polynomial kernel (xi • xj + 1)d (d 2 N+) and RBF kernel expf—kxi — xjk2/2Q2g are often used. Using kernel method means that feature vectors are mapped into a (higher dimensional) Hilbert space and linearly separated there. This mapping structure makes non-linear separation possible, although SVM is basically a linear classifier. Another advantage of kernel method is that although it deals with a high dimensional (possibly infinite) Hilbert space, there is no nee</context>
<context position="10962" citStr="Vapnik, 1995" startWordPosition="1847" endWordPosition="1848"> themselves, so we take here the one-versus-rest method to apply them for multi-class classification tasks. 5 Theoretical Perspective 5.1 Validation as a Kernel Function The proposed feature restructuring method can be considered as the use of a certain kernel for the pre-restructured feature space. We give an explanation for the linear case. Given two vectors, d1 and d2, the kernel function K in the restructured space is expressed as, dt 1 d2 = dt1d2 + st1s2 = dt1d2 + d&apos;AtAd2. (12) Considering the fact that each of two terms above is a kernel and that the sum of two kernels is also a kernel (Vapnik, 1995), the proposed restructuring is equivalent to using a certain kernel in the pre-restructured space. 5.2 Interpretation of Feature Space Restructuring The expression (12) shows that weights are put on the latent semantic indices determined by ICA and PCA respectively. The criterion of meaningfulness depends on which of ICA and PCA is used. Note that weighting is different from reducing. In the dimension-reduction methods, only the latent semantic space is considered, but in our method, the original feature space still directly influences the classification result. This property of our method ma</context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>Vapnik, V. 1995. The Nature of Statistical Learning Theory. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Weston</author>
<author>S Mukherjee</author>
<author>O Chapelle</author>
<author>M Pontil</author>
<author>T Poggio</author>
<author>V Vapnik</author>
</authors>
<title>Feature Selection for SVMs.</title>
<date>2000</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<volume>13</volume>
<contexts>
<context position="2355" citStr="Weston et al, 2000" startWordPosition="362" endWordPosition="365">nents from mixed signals. SVM has been applied in many applications such as Image Processing and Natural Language Processing. The idea to apply SVM for text categorization was first introduced in (Joachims, 1998). However, when the number of labeled data are small, SVM often fails to produce a good result, although several efforts against this problem have been made. There are two strategies for improving performance in the case of a limited number of data. One is to modify the learning algorithm itself (Joachims, 1999a; Glenn and Mangasarian, 2001), and the other is to process training data (Weston et al, 2000), including the selection of features. In this paper, we focus on the latter, especially on feature space restructuring. For processing training data, Principal Component Analysis (PCA) is often adopted in classifiers such as k-Nearest Neighbor method (Mitchell, 1997). But the conventional dimension-reduction methods fail for SVM as shown by experiments in Section 6. Unlike the conventional ones, our approach uses the components obtained with ICA to augment the dimension of the feature space. ICA is built on the assumptions that the sources are independent of each other and that the signals ob</context>
</contexts>
<marker>Weston, Mukherjee, Chapelle, Pontil, Poggio, Vapnik, 2000</marker>
<rawString>Weston, J., Mukherjee, S., Chapelle, O., Pontil, M., Poggio, T. and Vapnik, V. 2000. Feature Selection for SVMs. In Advances in Neural Information Processing Systems, volume 13.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Y Yang</author>
</authors>
<title>An Evaluation of Statistical Approaches to Text Categorization.</title>
<journal>Information Retrieval,</journal>
<volume>1</volume>
<pages>1--2</pages>
<marker>Yang, </marker>
<rawString>Yang, Y. An Evaluation of Statistical Approaches to Text Categorization. Information Retrieval, volume 1, 1-2, pp. 69{90.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>