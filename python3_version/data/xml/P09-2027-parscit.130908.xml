<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.195424">
<title confidence="0.980633">
Query-Focused Summaries or Query-Biased Summaries ?
</title>
<author confidence="0.992065">
Rahul Katragadda
</author>
<affiliation confidence="0.893846">
Language Technologies Research Center
IIIT Hyderabad
</affiliation>
<email confidence="0.967847">
rahul k@research.iiit.ac.in
</email>
<author confidence="0.990187">
Vasudeva Varma
</author>
<affiliation confidence="0.894352">
Language Technologies Research Center
IIIT Hyderabad
</affiliation>
<email confidence="0.985078">
vv@iiit.ac.in
</email>
<sectionHeader confidence="0.993561" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999983625">
In the context of the Document Understand-
ing Conferences, the task of Query-Focused
Multi-Document Summarization is intended to
improve agreement in content among human-
generated model summaries. Query-focus also
aids the automated summarizers in directing
the summary at specific topics, which may re-
sult in better agreement with these model sum-
maries. However, while query focus corre-
lates with performance, we show that high-
performing automatic systems produce sum-
maries with disproportionally higher query
term density than human summarizers do. Ex-
perimental evidence suggests that automatic
systems heavily rely on query term occurrence
and repetition to achieve good performance.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999951295081968">
The problem of automatically summarizing text doc-
uments has received a lot of attention since the early
work by Luhn (Luhn, 1958). Most of the current auto-
matic summarization systems rely on a sentence extrac-
tive paradigm, where key sentences in the original text
are selected to form the summary based on the clues (or
heuristics), or learning based approaches.
Common approaches for identifying key sentences
include: training a binary classifier (Kupiec et al.,
1995), training a Markov model or CRF (Conroy et al.,
2004; Shen et al., 2007) or directly assigning weights
to sentences based on a variety of features and heuris-
tically determined feature weights (Toutanova et al.,
2007). But, the question of which components and fea-
tures of automatic summarizers contribute most to their
performance has largely remained unanswered (Marcu
and Gerber, 2001), until Nenkova et al. (Nenkova et
al., 2006) explored the contribution of frequency based
measures. In this paper, we examine the role a query
plays in automated multi-document summarization of
newswire.
One of the issues studied since the inception of auto-
matic summarization is that of human agreement: dif-
ferent people choose different content for their sum-
maries (Rath et al., 1961; van Halteren and Teufel,
2003; Nenkova et al., 2007). Later, it was as-
sumed (Dang, 2005) that having a question/query to
provide focus would improve agreement between any
two human-generated model summaries, as well as be-
tween a model summary and an automated summary.
Starting in 2005 until 2007, a query-focused multi-
document summarization task was conducted as part of
the annual Document Understanding Conference. This
task models a real-world complex question answering
scenario, where systems need to synthesize from a set
of 25 documents, a brief (250 words), well organized
fluent answer to an information need.
Query-focused summarization is a topic of ongoing
importance within the summarization and question an-
swering communities. Most of the work in this area
has been conducted under the guise of “query-focused
multi-document summarization”, “descriptive question
answering”, or even “complex question answering”.
In this paper, based on structured empirical evalu-
ations, we show that most of the systems participat-
ing in DUC’s Query-Focused Multi-Document Sum-
marization (QF-MDS) task have been query-biased in
building extractive summaries. Throughout our discus-
sion, the term ‘query-bias’, with respect to a sentence,
is precisely defined to mean that the sentence has at
least one query term within it. The term ‘query-focus’
is less precisely defined, but is related to the cognitive
task of focusing a summary on the query, which we as-
sume humans do naturally. In other words, the human
generated model summaries are assumed to be query-
focused.
Here we first discuss query-biased content in Sum-
mary Content Units (SCUs) in Section 2 and then in
Section 3 by building formal models on query-bias we
discuss why/how automated systems are query-biased
rather than being query-focused.
</bodyText>
<sectionHeader confidence="0.864266" genericHeader="method">
2 Query-biased content in
</sectionHeader>
<subsectionHeader confidence="0.855525">
Summary Content Units (SCUs)
</subsectionHeader>
<bodyText confidence="0.9999579">
Summary content units, referred as SCUs hereafter, are
semantically motivated subsentential units that are vari-
able in length but not bigger than a sentential clause.
SCUs are constructed from annotation of a collection
of human summaries on a given document collection.
They are identified by noting information that is re-
peated across summaries. The repetition is as small
as a modifier of a noun phrase or as large as a clause.
The evaluation method that is based on overlapping
SCUs in human and automatic summaries is called the
</bodyText>
<page confidence="0.995253">
105
</page>
<note confidence="0.9760125">
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 105–108,
Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP
</note>
<figureCaption confidence="0.99992">
Figure 1: SCU annotation of a source document.
</figureCaption>
<bodyText confidence="0.999893590909091">
pyramid method (Nenkova et al., 2007).
The University of Ottawa has organized the pyramid
annotation data such that for some of the sentences in
the original document collection, a list of correspond-
ing content units is known (Copeck et al., 2006). A
sample of an SCU mapping from topic D0701A of
the DUC 2007 QF-MDS corpus is shown in Figure 1.
Three sentences are seen in the figure among which
two have been annotated with system IDs and SCU
weights wherever applicable. The first sentence has not
been picked by any of the summarizers participating in
Pyramid Evaluations, hence it is unknown if the sen-
tence would have contributed to any SCU. The second
sentence was picked by 8 summarizers and that sen-
tence contributed to an SCU of weight 3. The third
sentence in the example was picked by one summa-
rizer, however, it did not contribute to any SCU. This
example shows all the three types of sentences avail-
able in the corpus: unknown samples, positive samples
and negative samples.
We extracted the positive and negative samples in the
source documents from these annotations; types of sec-
ond and third sentences shown in Figure 1. A total
of 14.8% sentences were annotated to be either posi-
tive or negative. When we analyzed the positive set,
we found that 84.63% sentences in this set were query-
biased. Also, on the negative sample set, we found that
69.12% sentences were query-biased. That is, on an
average, 76.67% of the sentences picked by any au-
tomated summarizer are query-biased. On the other
hand, for human summaries only 58% sentences were
query-biased. All the above numbers are based on the
DUC 2007 dataset shown in boldface in Table 1 1.
There is one caveat: The annotated sentences come
only from the summaries of systems that participated in
the pyramid evaluations. Since only 13 among a total
32 participating systems were evaluated using pyramid
evaluations, the dataset is limited. However, despite
this small issue, it is very clear that at least those sys-
tems that participated in pyramid evaluations have been
biased towards query-terms, or at least, they have been
better at correctly identifying important sentences from
the query-biased sentences than from query-unbiased
sentences.
</bodyText>
<footnote confidence="0.961733">
1We used DUC 2007 dataset for all experiments reported.
</footnote>
<sectionHeader confidence="0.961017" genericHeader="method">
3 Formalizing query-bias
</sectionHeader>
<bodyText confidence="0.9999623125">
Our search for a formal method to capture the relation
between occurrence of query-biased sentences in the
input and in summaries resulted in building binomial
and multinomial model distributions. The distributions
estimated were then used to obtain the likelihood of a
query-biased sentence being emitted into a summary by
each system.
For the DUC 2007 data, there were 45 summaries
for each of the 32 systems (labeled 1-32) among which
2 were baselines (labeled 1 and 2), and 18 summaries
from each of 10 human summarizers (labeled A-J). We
computed the log-likelihood, log(L[summary;p(Ci)]),
of all human and machine summaries from DUC’07
query focused multi-document summarization task,
based on both distributions described below (see Sec-
tions 3.1, 3.2).
</bodyText>
<subsectionHeader confidence="0.989888">
3.1 The binomial model
</subsectionHeader>
<bodyText confidence="0.9998761875">
We represent the set of sentences as a binomial distribu-
tion over type of sentences. Let C0 and C1 denote the
sets of sentences without and with query-bias respec-
tively. Let p(Ci) be the probability of emitting a sen-
tence from a specified set. It is also obvious that query-
biased sentences will be assigned lower emission prob-
abilities, because the occurrence of query-biased sen-
tences in the input is less likely. On average each topic
has 549 sentences, among which 196 contain a query
term; which means only 35.6% sentences in the input
were query-biased. Hence, the likelihood function here
denotes the likelihood of a summary to contain non
query-biased sentences. Humans’ and systems’ sum-
maries must now constitute low likelihood to show that
they rely on query-bias.
The likelihood of a summary then is :
</bodyText>
<equation confidence="0.987562">
L[summary; p(Ci)] = n0!n1!p(C0)n0p(C1)n1 (1)
N!
</equation>
<bodyText confidence="0.998285727272727">
Where N is the number of sentences in the sum-
mary, and n0 + n1 = N; n0 and n1 are the cardinali-
ties of C0 and C1 in the summary. Table 2 shows var-
ious systems with their ranks based on ROUGE-2 and
the average log-likelihood scores. The ROUGE (Lin,
2004) suite of metrics are n-gram overlap based met-
rics that have been shown to highly correlate with hu-
man evaluations on content responsiveness. ROUGE-2
and ROUGE-SU4 are the official ROUGE metrics for
evaluating query-focused multi-document summariza-
tion task since DUC 2005.
</bodyText>
<subsectionHeader confidence="0.995383">
3.2 The multinomial model
</subsectionHeader>
<bodyText confidence="0.9999395">
In the previous section (Section 3.1), we described
the binomial model where we classified each sentence
as being query-biased or not. However, if we were
to quantify the amount of query-bias in a sentence,
we associate each sentence to one among k possible
classes leading to a multinomial distribution. Let Ci ∈
</bodyText>
<page confidence="0.996399">
106
</page>
<table confidence="0.99699525">
Dataset total positive biased positive negative biased negative % bias in positive % bias in negative
DUC 2005 24831 1480 1127 1912 1063 76.15 55.60
DUC 2006 14747 1047 902 1407 908 86.15 71.64
DUC 2007 12832 924 782 975 674 84.63 69.12
</table>
<tableCaption confidence="0.99548">
Table 1: Statistical information on counts of query-biased sentences.
</tableCaption>
<table confidence="0.999913666666667">
ID rank LL ROUGE-2 ID rank LL ROUGE-2 ID rank LL ROUGE-2
1 31 -1.9842 0.06039 J -3.9465 0.13904 24 4 -5.8451 0.11793
C -2.1387 0.15055 E -3.9485 0.13850 9 12 -5.9049 0.10370
16 32 -2.2906 0.03813 10 28 -4.0723 0.07908 14 14 -5.9860 0.10277
27 30 -2.4012 0.06238 21 22 -4.2460 0.08989 5 23 -6.0464 0.08784
6 29 -2.5536 0.07135 G -4.3143 0.13390 4 3 -6.2347 0.11887
12 25 -2.9415 0.08505 25 27 -4.4542 0.08039 20 6 -6.3923 0.10879
I -3.0196 0.13621 B -4.4655 0.13992 29 2 -6.4076 0.12028
11 24 -3.0495 0.08678 19 26 -4.6785 0.08453 3 9 -7.1720 0.10660
28 16 -3.1932 0.09858 26 21 -4.7658 0.08989 8 11 -7.4125 0.10408
2 18 -3.2058 0.09382 23 7 -5.3418 0.10810 17 15 -7.4458 0.10212
D -3.2357 0.17528 30 10 -5.4039 0.10614 13 5 -7.7504 0.11172
H -3.4494 0.13001 7 8 -5.6291 0.10795 32 17 -8.0117 0.09750
A -3.6481 0.13254 18 19 -5.6397 0.09170 22 13 -8.9843 0.10329
F -3.8316 0.13395 15 1 -5.7938 0.12448 31 20 -9.0806 0.09126
</table>
<tableCaption confidence="0.9926405">
Table 2: Rank, Averaged log-likelihood score based on binomial model, true ROUGE-2 score for the summaries
of various systems in DUC’07 query-focused multi-document summarization task.
</tableCaption>
<table confidence="0.999523866666667">
ID rank LL ROUGE-2 ID rank LL ROUGE-2 ID rank LL ROUGE-2
1 31 -4.6770 0.06039 10 28 -8.5004 0.07908 5 23 -14.3259 0.08784
16 32 -4.7390 0.03813 G -9.5593 0.13390 9 12 -14.4732 0.10370
6 29 -5.4809 0.07135 E -9.6831 0.13850 22 13 -14.8557 0.10329
27 30 -5.5110 0.06238 26 21 -9.7163 0.08989 4 3 -14.9307 0.11887
I -6.7662 0.13621 J -9.8386 0.13904 18 19 -15.0114 0.09170
12 25 -6.8631 0.08505 19 26 -10.3226 0.08453 14 14 -15.4863 0.10277
2 18 -6.9363 0.09382 B -10.4152 0.13992 20 6 -15.8697 0.10879
C -7.2497 0.15055 25 27 -10.7693 0.08039 32 17 -15.9318 0.09750
H -7.6657 0.13001 29 2 -12.7595 0.12028 7 8 -15.9927 0.10795
11 24 -7.8048 0.08678 21 22 -13.1686 0.08989 17 15 -17.3737 0.10212
A -7.8690 0.13254 24 4 -13.2842 0.11793 8 11 -17.4454 0.10408
D -8.0266 0.17528 30 10 -13.3632 0.10614 31 20 -17.5615 0.09126
28 16 -8.0307 0.09858 23 7 -13.7781 0.10810 3 9 -19.0495 0.10660
F -8.2633 0.13395 15 1 -14.2832 0.12448 13 5 -19.3089 0.11172
</table>
<tableCaption confidence="0.987619">
Table 3: Rank, Averaged log-likelihood score based on multinomial model, true ROUGE-2 score for the sum-
maries of various systems in DUC’07 query-focused multi-document summarization task.
</tableCaption>
<bodyText confidence="0.965927785714286">
{C0, C1, C2, ... , Ck} denote the k levels of query-
bias. Ci is the set of sentences, each having i query
terms.
The number of sentences participating in each class
varies highly, with C0 bagging a high percentage of
sentences (64.4%) and the rest {C1, C2, ... , Ck} dis-
tributing among themselves the rest 35.6% sentences.
Since the distribution is highly-skewed, distinguish-
ing systems based on log-likelihood scores using this
model is easier and perhaps more accurate. Like be-
fore, Humans’ and systems’ summaries must now con-
stitute low likelihood to show that they rely on query-
bias.
The likelihood of a summary then is :
</bodyText>
<equation confidence="0.972333666666667">
L[summary; p(Ci)] = 1 N1 1P(C0)n p(C1)n1 ··· p(Ck)nk
n0In1...· nk.
(2)
</equation>
<bodyText confidence="0.997238">
Where N is the number of sentences in the sum-
mary, and n0 + n1 + · · · + nk = N; n0, n1,· · · ,nk
are respectively the cardinalities of C0, C1, · · · ,Ck,
in the summary. Table 3 shows various systems with
their ranks based on ROUGE-2 and the average log-
likelihood scores.
</bodyText>
<subsectionHeader confidence="0.9968275">
3.3 Correlation of ROUGE and log-likelihood
scores
</subsectionHeader>
<bodyText confidence="0.999971384615385">
Tables 2 and 3 display log-likelihood scores of vari-
ous systems in the descending order of log-likelihood
scores along with their respective ROUGE-2 scores.
We computed the pearson correlation coefficient (p) of
‘ROUGE-2 and log-likelihood’ and ‘ROUGE-SU4 and
log-likelihood’. This was computed for systems (ID: 1-
32) (r1) and for humans (ID: A-J) (r2) separately, and
for both distributions.
For the binomial model, r1 = -0.66 and r2 = 0.39 was
obtained. This clearly indicates that there is a strong
negative correlation between likelihood of occurrence
of a non-query-term and ROUGE-2 score. That is, a
strong positive correlation between likelihood of occur-
</bodyText>
<page confidence="0.9975">
107
</page>
<bodyText confidence="0.9983603">
rence of a query-term and ROUGE-2 score. Similarly,
for human summarizers there is a weak negative cor-
relation between likelihood of occurrence of a query-
term and ROUGE-2 score. The same correlation anal-
ysis applies to ROUGE-SU4 scores: r1 = -0.66 and r2
= 0.38.
Similar analysis with the multinomial model have
been reported in Tables 4 and 5. Tables 4 and 5 show
the correlation among ROUGE-2 and log-likelihood
scores for systems2 and humans3.
</bodyText>
<table confidence="0.998241">
ρ ROUGE-2 ROUGE-SU4
binomial -0.66 -0.66
multinomial -0.73 -0.73
</table>
<tableCaption confidence="0.999665">
Table 4: Correlation of ROUGE measures with log-
likelihood scores for automated systems
</tableCaption>
<table confidence="0.99939">
ρ ROUGE-2 ROUGE-SU4
binomial 0.39 0.38
multinomial 0.15 0.09
</table>
<tableCaption confidence="0.996055">
Table 5: Correlation of ROUGE measures with log-
likelihood scores for humans
</tableCaption>
<sectionHeader confidence="0.995615" genericHeader="conclusions">
4 Conclusions and Discussion
</sectionHeader>
<bodyText confidence="0.999949814814815">
Our results underscore the differences between human
and machine generated summaries. Based on Sum-
mary Content Unit (SCU) level analysis of query-bias
we argue that most systems are better at finding impor-
tant sentences only from query-biased sentences. More
importantly, we show that on an average, 76.67% of
the sentences picked by any automated summarizer are
query-biased. When asked to produce query-focused
summaries, humans do not rely to the same extent on
the repetition of query terms.
We further confirm based on the likelihood of emit-
ting non query-biased sentence, that there is a strong
(negative) correlation among systems’ likelihood score
and ROUGE score, which suggests that systems are
trying to improve performance based on ROUGE met-
rics by being biased towards the query terms. On the
other hand, humans do not rely on query-bias, though
we do not have statistically significant evidence to sug-
gest it. We have also speculated that the multinomial
model helps in better capturing the variance across the
systems since it distinguishes among query-biased sen-
tences by quantifying the amount of query-bias.
From our point of view, most of the extractive sum-
marization algorithms are formalized based on a bag-
of-words query model. The innovation with individ-
ual approaches has been in formulating the actual algo-
rithm on top of the query model. We speculate that
</bodyText>
<footnote confidence="0.982363">
2All the results in Table 4 are statistically significant with
p-value (p &lt; 0.00004, N=32)
3None of the results in Table 5 are statistically significant
with p-value (p &gt; 0.265, N=10)
</footnote>
<bodyText confidence="0.999920733333333">
the real difference in human summarizers and auto-
mated summarizers could be in the way a query (or rel-
evance) is represented. Traditional query models from
IR literature have been used in summarization research
thus far, and though some previous work (Amini and
Usunier, 2007) tries to address this issue using con-
textual query expansion, new models to represent the
query is perhaps one way to induce topic-focus on the
summary. IR-like query models, which are designed
to handle ‘short keyword queries’, are perhaps not ca-
pable of handling ‘an elaborate query’ in case of sum-
marization. Since the notion of query-focus is appar-
ently missing in any or all of the algorithms, the future
summarization algorithms must try to incorporate this
while designing new algorithms.
</bodyText>
<sectionHeader confidence="0.996933" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9998875">
We thank Dr Charles L A Clarke at the University of
Waterloo for his deep reviews and discussions on ear-
lier versions of the paper. We are also grateful to all the
anonymous reviewers for their valuable comments.
</bodyText>
<sectionHeader confidence="0.999376" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999093477272727">
Massih R. Amini and Nicolas Usunier. 2007. A contextual query expansion
approach by term clustering for robust text summarization. In the proceed-
ings of Document Understanding Conference.
John M. Conroy, Judith D. Schlesinger, Jade Goldstein, and Dianne P. O’leary.
2004. Left-brain/right-brain multi-document summarization. In the pro-
ceedings of Document Understanding Conference (DUC) 2004.
Terry Copeck, D Inkpen, Anna Kazantseva, A Kennedy, D Kipp, Vivi Nastase,
and Stan Szpakowicz. 2006. Leveraging duc. In proceedings of DUC
2006.
Hoa Trang Dang. 2005. Overview of duc 2005. In proceedings of Document
Understanding Conference.
Julian Kupiec, Jan Pedersen, and Francine Chen. 1995. A trainable document
summarizer. In the proceedings ofACM SIGIR’95, pages 68–73. ACM.
Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of sum-
maries. In the proceedings of ACL Workshop on Text Summarization
Branches Out. ACL.
H.P. Luhn. 1958. The automatic creation of literature abstracts. In IBM Jour-
nal of Research and Development, Vol. 2, No. 2, pp. 159-165, April 1958.
Daniel Marcu and Laurie Gerber. 2001. An inquiry into the nature of mul-
tidocument abstracts, extracts, and their evaluation. In Proceedings of the
NAACL-2001 Workshop on Automatic Summarization.
Ani Nenkova, Lucy Vanderwende, and Kathleen McKeown. 2006. A compo-
sitional context sensitive multi-document summarizer: exploring the fac-
tors that influence summarization. In SIGIR ’06: Proceedings of the 29th
annual international ACM SIGIR conference on Research and development
in information retrieval, pages 573–580, New York, NY, USA. ACM.
Ani Nenkova, Rebecca Passonneau, and Kathleen McKeown. 2007. The
pyramid method: Incorporating human content selection variation in sum-
marization evaluation. In ACM Trans. Speech Lang. Process., volume 4,
New York, NY, USA. ACM.
G.J. Rath, A. Resnick, and R. Savage. 1961. The formation of abstracts by the
selection of sentences: Part 1: Sentence selection by man and machines. In
Journal ofAmerican Documentation., pages 139–208.
Dou Shen, Jian-Tao Sun, Hua Li, Qiang Yang, and Zheng Chen. 2007. Doc-
ument summarization using conditional random fields. In the proceedings
of IJCAI ’07., pages 2862–2867. IJCAI.
Kristina Toutanova, Chris Brockett, Michael Gamon, Jagadeesh Jagarlamundi,
Hisami Suzuki, and Lucy Vanderwende. 2007. The pythy summarization
system: Microsoft research at duc 2007. In the proceedings of Document
Understanding Conference.
Hans van Halteren and Simone Teufel. 2003. Examining the consensus be-
tween human summaries: initial experiments with factoid analysis. In
HLT-NAACL 03 Text summarization workshop, pages 57–64, Morristown,
NJ, USA. Association for Computational Linguistics.
</reference>
<page confidence="0.998353">
108
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.381970">
<title confidence="0.994445">Query-Focused Summaries or Query-Biased Summaries ?</title>
<author confidence="0.983738">Rahul Katragadda</author>
<affiliation confidence="0.994099">Language Technologies Research Center</affiliation>
<address confidence="0.678014">IIIT Hyderabad</address>
<email confidence="0.761345">rahulk@research.iiit.ac.in</email>
<author confidence="0.989636">Vasudeva Varma</author>
<affiliation confidence="0.997471">Language Technologies Research Center</affiliation>
<address confidence="0.737688">IIIT Hyderabad</address>
<email confidence="0.95239">vv@iiit.ac.in</email>
<abstract confidence="0.999016235294118">In the context of the Document Understanding Conferences, the task of Query-Focused Multi-Document Summarization is intended to improve agreement in content among humanmodel summaries. aids the automated summarizers in directing the summary at specific topics, which may result in better agreement with these model summaries. However, while query focus correlates with performance, we show that highperforming automatic systems produce summaries with disproportionally higher query term density than human summarizers do. Experimental evidence suggests that automatic systems heavily rely on query term occurrence and repetition to achieve good performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Massih R Amini</author>
<author>Nicolas Usunier</author>
</authors>
<title>A contextual query expansion approach by term clustering for robust text summarization.</title>
<date>2007</date>
<booktitle>In the proceedings of Document Understanding Conference.</booktitle>
<contexts>
<context position="16413" citStr="Amini and Usunier, 2007" startWordPosition="2672" endWordPosition="2675">d on a bagof-words query model. The innovation with individual approaches has been in formulating the actual algorithm on top of the query model. We speculate that 2All the results in Table 4 are statistically significant with p-value (p &lt; 0.00004, N=32) 3None of the results in Table 5 are statistically significant with p-value (p &gt; 0.265, N=10) the real difference in human summarizers and automated summarizers could be in the way a query (or relevance) is represented. Traditional query models from IR literature have been used in summarization research thus far, and though some previous work (Amini and Usunier, 2007) tries to address this issue using contextual query expansion, new models to represent the query is perhaps one way to induce topic-focus on the summary. IR-like query models, which are designed to handle ‘short keyword queries’, are perhaps not capable of handling ‘an elaborate query’ in case of summarization. Since the notion of query-focus is apparently missing in any or all of the algorithms, the future summarization algorithms must try to incorporate this while designing new algorithms. Acknowledgements We thank Dr Charles L A Clarke at the University of Waterloo for his deep reviews and </context>
</contexts>
<marker>Amini, Usunier, 2007</marker>
<rawString>Massih R. Amini and Nicolas Usunier. 2007. A contextual query expansion approach by term clustering for robust text summarization. In the proceedings of Document Understanding Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John M Conroy</author>
<author>Judith D Schlesinger</author>
<author>Jade Goldstein</author>
<author>Dianne P O’leary</author>
</authors>
<title>Left-brain/right-brain multi-document summarization.</title>
<date>2004</date>
<booktitle>In the proceedings of Document Understanding Conference (DUC)</booktitle>
<marker>Conroy, Schlesinger, Goldstein, O’leary, 2004</marker>
<rawString>John M. Conroy, Judith D. Schlesinger, Jade Goldstein, and Dianne P. O’leary. 2004. Left-brain/right-brain multi-document summarization. In the proceedings of Document Understanding Conference (DUC) 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Copeck</author>
<author>D Inkpen</author>
<author>Anna Kazantseva</author>
<author>A Kennedy</author>
<author>D Kipp</author>
<author>Vivi Nastase</author>
<author>Stan Szpakowicz</author>
</authors>
<title>Leveraging duc.</title>
<date>2006</date>
<booktitle>In proceedings of DUC</booktitle>
<contexts>
<context position="5005" citStr="Copeck et al., 2006" startWordPosition="764" endWordPosition="767">ries. The repetition is as small as a modifier of a noun phrase or as large as a clause. The evaluation method that is based on overlapping SCUs in human and automatic summaries is called the 105 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 105–108, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP Figure 1: SCU annotation of a source document. pyramid method (Nenkova et al., 2007). The University of Ottawa has organized the pyramid annotation data such that for some of the sentences in the original document collection, a list of corresponding content units is known (Copeck et al., 2006). A sample of an SCU mapping from topic D0701A of the DUC 2007 QF-MDS corpus is shown in Figure 1. Three sentences are seen in the figure among which two have been annotated with system IDs and SCU weights wherever applicable. The first sentence has not been picked by any of the summarizers participating in Pyramid Evaluations, hence it is unknown if the sentence would have contributed to any SCU. The second sentence was picked by 8 summarizers and that sentence contributed to an SCU of weight 3. The third sentence in the example was picked by one summarizer, however, it did not contribute to </context>
</contexts>
<marker>Copeck, Inkpen, Kazantseva, Kennedy, Kipp, Nastase, Szpakowicz, 2006</marker>
<rawString>Terry Copeck, D Inkpen, Anna Kazantseva, A Kennedy, D Kipp, Vivi Nastase, and Stan Szpakowicz. 2006. Leveraging duc. In proceedings of DUC 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoa Trang Dang</author>
</authors>
<title>Overview of duc</title>
<date>2005</date>
<booktitle>In proceedings of Document Understanding Conference.</booktitle>
<contexts>
<context position="2275" citStr="Dang, 2005" startWordPosition="339" endWordPosition="340">nts and features of automatic summarizers contribute most to their performance has largely remained unanswered (Marcu and Gerber, 2001), until Nenkova et al. (Nenkova et al., 2006) explored the contribution of frequency based measures. In this paper, we examine the role a query plays in automated multi-document summarization of newswire. One of the issues studied since the inception of automatic summarization is that of human agreement: different people choose different content for their summaries (Rath et al., 1961; van Halteren and Teufel, 2003; Nenkova et al., 2007). Later, it was assumed (Dang, 2005) that having a question/query to provide focus would improve agreement between any two human-generated model summaries, as well as between a model summary and an automated summary. Starting in 2005 until 2007, a query-focused multidocument summarization task was conducted as part of the annual Document Understanding Conference. This task models a real-world complex question answering scenario, where systems need to synthesize from a set of 25 documents, a brief (250 words), well organized fluent answer to an information need. Query-focused summarization is a topic of ongoing importance within </context>
</contexts>
<marker>Dang, 2005</marker>
<rawString>Hoa Trang Dang. 2005. Overview of duc 2005. In proceedings of Document Understanding Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian Kupiec</author>
<author>Jan Pedersen</author>
<author>Francine Chen</author>
</authors>
<title>A trainable document summarizer.</title>
<date>1995</date>
<booktitle>In the proceedings ofACM SIGIR’95,</booktitle>
<pages>68--73</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1411" citStr="Kupiec et al., 1995" startWordPosition="198" endWordPosition="201">xperimental evidence suggests that automatic systems heavily rely on query term occurrence and repetition to achieve good performance. 1 Introduction The problem of automatically summarizing text documents has received a lot of attention since the early work by Luhn (Luhn, 1958). Most of the current automatic summarization systems rely on a sentence extractive paradigm, where key sentences in the original text are selected to form the summary based on the clues (or heuristics), or learning based approaches. Common approaches for identifying key sentences include: training a binary classifier (Kupiec et al., 1995), training a Markov model or CRF (Conroy et al., 2004; Shen et al., 2007) or directly assigning weights to sentences based on a variety of features and heuristically determined feature weights (Toutanova et al., 2007). But, the question of which components and features of automatic summarizers contribute most to their performance has largely remained unanswered (Marcu and Gerber, 2001), until Nenkova et al. (Nenkova et al., 2006) explored the contribution of frequency based measures. In this paper, we examine the role a query plays in automated multi-document summarization of newswire. One of </context>
</contexts>
<marker>Kupiec, Pedersen, Chen, 1995</marker>
<rawString>Julian Kupiec, Jan Pedersen, and Francine Chen. 1995. A trainable document summarizer. In the proceedings ofACM SIGIR’95, pages 68–73. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>Rouge: A package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In the proceedings of ACL Workshop on Text Summarization Branches Out. ACL.</booktitle>
<contexts>
<context position="8954" citStr="Lin, 2004" startWordPosition="1424" endWordPosition="1425">ns only 35.6% sentences in the input were query-biased. Hence, the likelihood function here denotes the likelihood of a summary to contain non query-biased sentences. Humans’ and systems’ summaries must now constitute low likelihood to show that they rely on query-bias. The likelihood of a summary then is : L[summary; p(Ci)] = n0!n1!p(C0)n0p(C1)n1 (1) N! Where N is the number of sentences in the summary, and n0 + n1 = N; n0 and n1 are the cardinalities of C0 and C1 in the summary. Table 2 shows various systems with their ranks based on ROUGE-2 and the average log-likelihood scores. The ROUGE (Lin, 2004) suite of metrics are n-gram overlap based metrics that have been shown to highly correlate with human evaluations on content responsiveness. ROUGE-2 and ROUGE-SU4 are the official ROUGE metrics for evaluating query-focused multi-document summarization task since DUC 2005. 3.2 The multinomial model In the previous section (Section 3.1), we described the binomial model where we classified each sentence as being query-biased or not. However, if we were to quantify the amount of query-bias in a sentence, we associate each sentence to one among k possible classes leading to a multinomial distribut</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In the proceedings of ACL Workshop on Text Summarization Branches Out. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Luhn</author>
</authors>
<title>The automatic creation of literature abstracts.</title>
<date>1958</date>
<journal>In IBM Journal of Research and Development,</journal>
<volume>2</volume>
<pages>159--165</pages>
<contexts>
<context position="1070" citStr="Luhn, 1958" startWordPosition="147" endWordPosition="148">he automated summarizers in directing the summary at specific topics, which may result in better agreement with these model summaries. However, while query focus correlates with performance, we show that highperforming automatic systems produce summaries with disproportionally higher query term density than human summarizers do. Experimental evidence suggests that automatic systems heavily rely on query term occurrence and repetition to achieve good performance. 1 Introduction The problem of automatically summarizing text documents has received a lot of attention since the early work by Luhn (Luhn, 1958). Most of the current automatic summarization systems rely on a sentence extractive paradigm, where key sentences in the original text are selected to form the summary based on the clues (or heuristics), or learning based approaches. Common approaches for identifying key sentences include: training a binary classifier (Kupiec et al., 1995), training a Markov model or CRF (Conroy et al., 2004; Shen et al., 2007) or directly assigning weights to sentences based on a variety of features and heuristically determined feature weights (Toutanova et al., 2007). But, the question of which components an</context>
</contexts>
<marker>Luhn, 1958</marker>
<rawString>H.P. Luhn. 1958. The automatic creation of literature abstracts. In IBM Journal of Research and Development, Vol. 2, No. 2, pp. 159-165, April 1958.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>Laurie Gerber</author>
</authors>
<title>An inquiry into the nature of multidocument abstracts, extracts, and their evaluation.</title>
<date>2001</date>
<booktitle>In Proceedings of the NAACL-2001 Workshop on Automatic Summarization.</booktitle>
<contexts>
<context position="1799" citStr="Marcu and Gerber, 2001" startWordPosition="259" endWordPosition="262">ntences in the original text are selected to form the summary based on the clues (or heuristics), or learning based approaches. Common approaches for identifying key sentences include: training a binary classifier (Kupiec et al., 1995), training a Markov model or CRF (Conroy et al., 2004; Shen et al., 2007) or directly assigning weights to sentences based on a variety of features and heuristically determined feature weights (Toutanova et al., 2007). But, the question of which components and features of automatic summarizers contribute most to their performance has largely remained unanswered (Marcu and Gerber, 2001), until Nenkova et al. (Nenkova et al., 2006) explored the contribution of frequency based measures. In this paper, we examine the role a query plays in automated multi-document summarization of newswire. One of the issues studied since the inception of automatic summarization is that of human agreement: different people choose different content for their summaries (Rath et al., 1961; van Halteren and Teufel, 2003; Nenkova et al., 2007). Later, it was assumed (Dang, 2005) that having a question/query to provide focus would improve agreement between any two human-generated model summaries, as w</context>
</contexts>
<marker>Marcu, Gerber, 2001</marker>
<rawString>Daniel Marcu and Laurie Gerber. 2001. An inquiry into the nature of multidocument abstracts, extracts, and their evaluation. In Proceedings of the NAACL-2001 Workshop on Automatic Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Lucy Vanderwende</author>
<author>Kathleen McKeown</author>
</authors>
<title>A compositional context sensitive multi-document summarizer: exploring the factors that influence summarization.</title>
<date>2006</date>
<booktitle>In SIGIR ’06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>573--580</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1844" citStr="Nenkova et al., 2006" startWordPosition="267" endWordPosition="270">rm the summary based on the clues (or heuristics), or learning based approaches. Common approaches for identifying key sentences include: training a binary classifier (Kupiec et al., 1995), training a Markov model or CRF (Conroy et al., 2004; Shen et al., 2007) or directly assigning weights to sentences based on a variety of features and heuristically determined feature weights (Toutanova et al., 2007). But, the question of which components and features of automatic summarizers contribute most to their performance has largely remained unanswered (Marcu and Gerber, 2001), until Nenkova et al. (Nenkova et al., 2006) explored the contribution of frequency based measures. In this paper, we examine the role a query plays in automated multi-document summarization of newswire. One of the issues studied since the inception of automatic summarization is that of human agreement: different people choose different content for their summaries (Rath et al., 1961; van Halteren and Teufel, 2003; Nenkova et al., 2007). Later, it was assumed (Dang, 2005) that having a question/query to provide focus would improve agreement between any two human-generated model summaries, as well as between a model summary and an automat</context>
</contexts>
<marker>Nenkova, Vanderwende, McKeown, 2006</marker>
<rawString>Ani Nenkova, Lucy Vanderwende, and Kathleen McKeown. 2006. A compositional context sensitive multi-document summarizer: exploring the factors that influence summarization. In SIGIR ’06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 573–580, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Rebecca Passonneau</author>
<author>Kathleen McKeown</author>
</authors>
<title>The pyramid method: Incorporating human content selection variation in summarization evaluation.</title>
<date>2007</date>
<journal>In ACM Trans. Speech Lang. Process.,</journal>
<volume>4</volume>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="2239" citStr="Nenkova et al., 2007" startWordPosition="330" endWordPosition="333">al., 2007). But, the question of which components and features of automatic summarizers contribute most to their performance has largely remained unanswered (Marcu and Gerber, 2001), until Nenkova et al. (Nenkova et al., 2006) explored the contribution of frequency based measures. In this paper, we examine the role a query plays in automated multi-document summarization of newswire. One of the issues studied since the inception of automatic summarization is that of human agreement: different people choose different content for their summaries (Rath et al., 1961; van Halteren and Teufel, 2003; Nenkova et al., 2007). Later, it was assumed (Dang, 2005) that having a question/query to provide focus would improve agreement between any two human-generated model summaries, as well as between a model summary and an automated summary. Starting in 2005 until 2007, a query-focused multidocument summarization task was conducted as part of the annual Document Understanding Conference. This task models a real-world complex question answering scenario, where systems need to synthesize from a set of 25 documents, a brief (250 words), well organized fluent answer to an information need. Query-focused summarization is a</context>
<context position="4795" citStr="Nenkova et al., 2007" startWordPosition="729" endWordPosition="732">ot bigger than a sentential clause. SCUs are constructed from annotation of a collection of human summaries on a given document collection. They are identified by noting information that is repeated across summaries. The repetition is as small as a modifier of a noun phrase or as large as a clause. The evaluation method that is based on overlapping SCUs in human and automatic summaries is called the 105 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 105–108, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP Figure 1: SCU annotation of a source document. pyramid method (Nenkova et al., 2007). The University of Ottawa has organized the pyramid annotation data such that for some of the sentences in the original document collection, a list of corresponding content units is known (Copeck et al., 2006). A sample of an SCU mapping from topic D0701A of the DUC 2007 QF-MDS corpus is shown in Figure 1. Three sentences are seen in the figure among which two have been annotated with system IDs and SCU weights wherever applicable. The first sentence has not been picked by any of the summarizers participating in Pyramid Evaluations, hence it is unknown if the sentence would have contributed t</context>
</contexts>
<marker>Nenkova, Passonneau, McKeown, 2007</marker>
<rawString>Ani Nenkova, Rebecca Passonneau, and Kathleen McKeown. 2007. The pyramid method: Incorporating human content selection variation in summarization evaluation. In ACM Trans. Speech Lang. Process., volume 4, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G J Rath</author>
<author>A Resnick</author>
<author>R Savage</author>
</authors>
<title>The formation of abstracts by the selection of sentences: Part 1: Sentence selection by man and machines.</title>
<date>1961</date>
<booktitle>In Journal ofAmerican Documentation.,</booktitle>
<pages>139--208</pages>
<contexts>
<context position="2185" citStr="Rath et al., 1961" startWordPosition="321" endWordPosition="324">stically determined feature weights (Toutanova et al., 2007). But, the question of which components and features of automatic summarizers contribute most to their performance has largely remained unanswered (Marcu and Gerber, 2001), until Nenkova et al. (Nenkova et al., 2006) explored the contribution of frequency based measures. In this paper, we examine the role a query plays in automated multi-document summarization of newswire. One of the issues studied since the inception of automatic summarization is that of human agreement: different people choose different content for their summaries (Rath et al., 1961; van Halteren and Teufel, 2003; Nenkova et al., 2007). Later, it was assumed (Dang, 2005) that having a question/query to provide focus would improve agreement between any two human-generated model summaries, as well as between a model summary and an automated summary. Starting in 2005 until 2007, a query-focused multidocument summarization task was conducted as part of the annual Document Understanding Conference. This task models a real-world complex question answering scenario, where systems need to synthesize from a set of 25 documents, a brief (250 words), well organized fluent answer to</context>
</contexts>
<marker>Rath, Resnick, Savage, 1961</marker>
<rawString>G.J. Rath, A. Resnick, and R. Savage. 1961. The formation of abstracts by the selection of sentences: Part 1: Sentence selection by man and machines. In Journal ofAmerican Documentation., pages 139–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dou Shen</author>
<author>Jian-Tao Sun</author>
<author>Hua Li</author>
<author>Qiang Yang</author>
<author>Zheng Chen</author>
</authors>
<title>Document summarization using conditional random fields.</title>
<date>2007</date>
<booktitle>In the proceedings of IJCAI ’07.,</booktitle>
<pages>2862--2867</pages>
<publisher>IJCAI.</publisher>
<contexts>
<context position="1484" citStr="Shen et al., 2007" startWordPosition="212" endWordPosition="215">term occurrence and repetition to achieve good performance. 1 Introduction The problem of automatically summarizing text documents has received a lot of attention since the early work by Luhn (Luhn, 1958). Most of the current automatic summarization systems rely on a sentence extractive paradigm, where key sentences in the original text are selected to form the summary based on the clues (or heuristics), or learning based approaches. Common approaches for identifying key sentences include: training a binary classifier (Kupiec et al., 1995), training a Markov model or CRF (Conroy et al., 2004; Shen et al., 2007) or directly assigning weights to sentences based on a variety of features and heuristically determined feature weights (Toutanova et al., 2007). But, the question of which components and features of automatic summarizers contribute most to their performance has largely remained unanswered (Marcu and Gerber, 2001), until Nenkova et al. (Nenkova et al., 2006) explored the contribution of frequency based measures. In this paper, we examine the role a query plays in automated multi-document summarization of newswire. One of the issues studied since the inception of automatic summarization is that</context>
</contexts>
<marker>Shen, Sun, Li, Yang, Chen, 2007</marker>
<rawString>Dou Shen, Jian-Tao Sun, Hua Li, Qiang Yang, and Zheng Chen. 2007. Document summarization using conditional random fields. In the proceedings of IJCAI ’07., pages 2862–2867. IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Chris Brockett</author>
<author>Michael Gamon</author>
<author>Jagadeesh Jagarlamundi</author>
<author>Hisami Suzuki</author>
<author>Lucy Vanderwende</author>
</authors>
<title>The pythy summarization system: Microsoft research at duc</title>
<date>2007</date>
<booktitle>In the proceedings of Document Understanding Conference.</booktitle>
<contexts>
<context position="1628" citStr="Toutanova et al., 2007" startWordPosition="234" endWordPosition="237">ived a lot of attention since the early work by Luhn (Luhn, 1958). Most of the current automatic summarization systems rely on a sentence extractive paradigm, where key sentences in the original text are selected to form the summary based on the clues (or heuristics), or learning based approaches. Common approaches for identifying key sentences include: training a binary classifier (Kupiec et al., 1995), training a Markov model or CRF (Conroy et al., 2004; Shen et al., 2007) or directly assigning weights to sentences based on a variety of features and heuristically determined feature weights (Toutanova et al., 2007). But, the question of which components and features of automatic summarizers contribute most to their performance has largely remained unanswered (Marcu and Gerber, 2001), until Nenkova et al. (Nenkova et al., 2006) explored the contribution of frequency based measures. In this paper, we examine the role a query plays in automated multi-document summarization of newswire. One of the issues studied since the inception of automatic summarization is that of human agreement: different people choose different content for their summaries (Rath et al., 1961; van Halteren and Teufel, 2003; Nenkova et</context>
</contexts>
<marker>Toutanova, Brockett, Gamon, Jagarlamundi, Suzuki, Vanderwende, 2007</marker>
<rawString>Kristina Toutanova, Chris Brockett, Michael Gamon, Jagadeesh Jagarlamundi, Hisami Suzuki, and Lucy Vanderwende. 2007. The pythy summarization system: Microsoft research at duc 2007. In the proceedings of Document Understanding Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans van Halteren</author>
<author>Simone Teufel</author>
</authors>
<title>Examining the consensus between human summaries: initial experiments with factoid analysis.</title>
<date>2003</date>
<booktitle>In HLT-NAACL 03 Text summarization workshop,</booktitle>
<pages>57--64</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<marker>van Halteren, Teufel, 2003</marker>
<rawString>Hans van Halteren and Simone Teufel. 2003. Examining the consensus between human summaries: initial experiments with factoid analysis. In HLT-NAACL 03 Text summarization workshop, pages 57–64, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>