<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007578">
<title confidence="0.99849">
A Unified Syntactic Model for Parsing Fluent and Disfluent Speech*
</title>
<author confidence="0.999285">
Tim Miller William Schuler
</author>
<affiliation confidence="0.999928">
University of Minnesota University of Minnesota
</affiliation>
<email confidence="0.999425">
tmill@cs.umn.edu schuler@cs.umn.edu
</email>
<sectionHeader confidence="0.993912" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999161333333333">
This paper describes a syntactic representation
for modeling speech repairs. This representa-
tion makes use of a right corner transform of
syntax trees to produce a tree representation
in which speech repairs require very few spe-
cial syntax rules, making better use of training
data. PCFGs trained on syntax trees using this
model achieve high accuracy on the standard
Switchboard parsing task.
</bodyText>
<sectionHeader confidence="0.998796" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.988176625">
Speech repairs occur when a speaker makes a mis-
take and decides to partially retrace an utterance in
order to correct it. Speech repairs are common in
spontaneous speech – one study found 30% of dia-
logue turns contained repairs (Carletta et al., 1993)
and another study found one repair every 4.8 sec-
onds (Blackmer and Mitton, 1991). Because of the
relatively high frequency of this phenomenon, spon-
taneous speech recognition systems will need to be
able to deal with repairs to achieve high levels of
accuracy.
The speech repair terminology used here follows
that of Shriberg (1994). A speech repair consists of
a reparandum, an interruption point, and the alter-
ation. The reparandum contains the words that the
speaker means to replace, including both words that
are in error and words that will be retraced. The in-
terruption point is the point in time where the stream
of speech is actually stopped, and the repairing of
the mistake can begin. The alteration contains the
This research was supported by NSF CAREER award
0447685. The views expressed are not necessarily endorsed by
the sponsors.
words that are meant to replace the words in the
reparandum.
Recent advances in recognizing spontaneous
speech with repairs (Hale et al., 2006; Johnson and
Charniak, 2004) have used parsing approaches on
transcribed speech to account for the structure in-
herent in speech repairs at the word level and above.
One salient aspect of structure is the fact that there
is often a good deal of overlap in words between
the reparandum and the alteration, as speakers may
trace back several words when restarting after an er-
ror. For instance, in the repair ... a flight to Boston,
uh, I mean, to Denver on Friday... , there is an exact
match of the word ‘to’ between reparandum and re-
pair, and a part of speech match between the words
‘Boston’ and ‘Denver’.
Another sort of structure in repair is what Lev-
elt (1983) called the well-formedness rule. This
rule states that the constituent started in the reparan-
dum and repair are ultimately of syntactic types that
could be grammatically joined by a conjunction. For
example, in the repair above, the well-formedness
rule says that the repair is well formed if the frag-
ment ... a flight to Boston and to Denver... is gram-
matical. In this case the repair is well formed since
the conjunction is grammatical, if not meaningful.
The approach described here makes use of a trans-
form on a tree-annotated corpus to build a syntactic
model of speech repair which takes advantage of the
structure of speech repairs as described above, while
also providing a representation of repair structure
that more closely adheres to intuitions about what
happens when speakers make repairs.
</bodyText>
<page confidence="0.991964">
105
</page>
<affiliation confidence="0.2637825">
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 105–108,
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</affiliation>
<sectionHeader confidence="0.751656" genericHeader="method">
2 Speech repair representation
</sectionHeader>
<bodyText confidence="0.999909785714286">
The representational scheme used for this work
makes use of a right-corner transform, a way of
rewriting syntax trees that turns all right recursion
into left recursion, and leaves left recursion as is.
As a result, constituent structure is built up dur-
ing recognition in a left-to-right fashion, as words
are read in. This arrangement is well-suited to
recognition of speech with repairs, because it al-
lows for constituent structure to be built up using
fluent speech rules up until the moment of interrup-
tion, at which point a special repair rule may be ap-
plied. This property will be examined further in sec-
tion 2.3, following a technical description of the rep-
resentation scheme.
</bodyText>
<subsectionHeader confidence="0.991884">
2.1 Binary branching structure
</subsectionHeader>
<bodyText confidence="0.97609735483871">
In order to obtain a linguistically plausible right-
corner transform representation of incomplete con-
stituents, the Switchboard corpus is subjected to a
pre-process transform to introduce binary-branching
nonterminal projections, and fold empty categories
into nonterminal symbols in a manner similar to that
proposed by Johnson (1998b) and Klein and Man-
ning (2003). This binarization is done in in such
a way as to preserve linguistic intuitions of head
projection, so that the depth requirements of right-
corner transformed trees will be reasonable approx-
imations to the working memory requirements of a
human reader or listener.
Trees containing speech repairs are reduced in ar-
ity by merging repair structure lower in the tree,
when possible. As seen in the left tree below, 1 re-
pair structure is annotated in a flat manner, which
can lead to high-arity rules which are sparsely repre-
sented in the data set, and thus difficult to learn. This
problem can be mitigated by using the rewrite rule
shown below, which turns an EDITED-X constituent
into the leftmost child of a tree of type X, as long as
the original flat tree had X following an EDITED-
X constituent and possibly some editing term (ET)
categories. The INTJ category (‘uh’,‘um’,etc.) and
the PRN category (‘I mean’, ‘that is’, etc.) are con-
sidered to be editing term categories when they lie
1Here, all Ai denote nonterminal symbols, and all αi denote
subtrees; the notation A1:α1 indicates a subtree α1 with label
A1; and all rewrites are applied recursively, from leaves to root.
between EDITED-X and X constituents.
</bodyText>
<equation confidence="0.795639666666667">
EDITED-A1 ET* A1:α2
A1:α1
A1:α1
</equation>
<subsectionHeader confidence="0.984791">
2.2 Right-corner transform
</subsectionHeader>
<bodyText confidence="0.994870181818182">
Binarized trees2 are then transformed into right-
corner trees using transform rules similar to those
described by Johnson(1998a). This right-corner
transform is simply the left-right dual of a left-
corner transform. It transforms all right recursive
sequences in each tree into left recursive sequences
of symbols of the form A1/A2, denoting an incom-
plete instance of category A1 lacking an instance of
category A2 to the right.
Rewrite rules for the right-corner transform are
shown below:
</bodyText>
<equation confidence="0.789598">
A1 A1
</equation>
<bodyText confidence="0.99988505882353">
Here, the first rewrite rule is applied iteratively
(bottom-up on the tree) to flatten all right recursion,
using incomplete constituents to record the original
nonterminal ordering. The second rule is then ap-
plied to generate left recursive structure, preserving
this ordering.
The incomplete constituent categories created by
the right corner transform are similar in form and
meaning to non-constituent categories used in Com-
binatorial Categorial Grammars (CCGs) (Steedman,
2000). Unlike CCGs, however, a right corner trans-
formed grammar does not allow backward function
application, composition, or raising. As a result, it
does not introduce spurious ambiguity between for-
ward and backward operations, but cannot be taken
to explicitly encode argument structure, as CCGs
can.
</bodyText>
<footnote confidence="0.973916">
2All super-binary branches remaining after the above pre-
process are ‘nominally’ decomposed into right-branching struc-
tures by introducing intermediate nodes with labels concate-
nated from the labels of its children, delimited by underscores
</footnote>
<figure confidence="0.983362636363637">
A0
EDITED
ET* A1:α2 α3 ==&gt;
A0
A1
α3
α2 A1/A2:α1 α2
α1
α2
α2 A3:α3
A1
A1/A2:α1 A2/A3
α3 ... ==&gt;
A1/A3
A1
α3 ...
α1 A2 ==&gt; A1/A2
A2/A3
A3:α3
106
EDITED [-NP]
NP [-UNF]
</figure>
<figureCaption confidence="0.986532">
Figure 2: Right-corner transformed tree with repair struc-
ture
</figureCaption>
<subsectionHeader confidence="0.996964">
2.3 Application to speech repair
</subsectionHeader>
<bodyText confidence="0.99997050877193">
An example speech repair from the Switchboard cor-
pus can be seen in Figures 1 and 2, in which the same
repair fragment is shown in a standard state such as
might be used to train a probabilistic context free
grammar, and after the right-corner transform. Fig-
ure 1 also shows, in brackets, the augmented anno-
tation used by Hale et al.(2006). This scheme con-
sisted of adding -X to an EDITED label which pro-
duced a category X, as well as propagating the -UNF
label at the right corner of the tree up through every
parent below the EDITED root.
The standard annotation (without -UNF propaga-
tion) is deficient because even if an unfinished con-
stituent like PP-UNF is correctly recognized, and the
speaker is essentially in an error state, there may be
several partially completed constituents above – in
Figure 1, the NP, PP, and NP above the PP-UNF.
These constituents need to be completed, but using
the standard annotation there is only one chance to
make use of the information about the error that has
occurred – the NP —* NP PP-UNF rule. Thus, by the
time the error section is completed, there is no infor-
mation by which a parsing algorithm could choose
to reduce the topmost NP to EDITED other than in-
dependent rule probabilities.
The approach used by (Hale et al., 2006) works
because the information about the transition to an er-
ror state is propagated up the tree, in the form of the
-UNF tags. As the parsing chart is filled in bottom
up, each rule applied is essentially coming out of a
special repair rule set, and so at the top of the tree
the EDITED hypothesis is much more likely. How-
ever, this requires that several fluent speech rules
from the data set be modified for use in a special
repair grammar, which not only reduces the amount
of available training data, but violates our intuition
that most reparanda are fluent up until the actual edit
occurs.
The right corner transform model works in a dif-
ferent way, by building up constituent structure from
left to right. In Figure 2, the same fragment is
shown as it appears in the training data for this sys-
tem. With this representation, the problem noticed
by Hale and colleagues (2006) has been solved in
a different way, by incrementally building up left-
branching rather than right-branching structure, so
that only a single special error rule is required at the
end of the constituent. Whereas the -UNF propa-
gation scheme often requires the entire reparandum
to be generated from a speech repair rule set, this
scheme only requires one special rule, where the
moment of interruption actually occurred.
This is not only a pleasing parsimony, but it re-
duces the number of special speech repair rules that
need to be learned and saves more potential exam-
ples of fluent speech rules, and therefore potentially
makes better use of limited data.
</bodyText>
<sectionHeader confidence="0.996404" genericHeader="evaluation">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.999184111111111">
The evaluation of this system was performed on
the Switchboard corpus, using the mrg annotations
in directories 2 and 3 for training, and the files
sw4004.mrg to sw4153.mrg in directory 4 for evalu-
ation, following Johnson and Charniak(2004).
The input to the system consists of the terminal
symbols from the trees in the corpus section men-
tioned above. The terminal symbol strings are first
pre-processed by stripping punctuation and other
</bodyText>
<figure confidence="0.9753196">
NN
invasion
PP-UNF
IN
of
</figure>
<figureCaption confidence="0.9904925">
Figure 1: Standard tree repair structure, with -UNF prop-
agation as in (Hale et al., 2006) shown in brackets.
</figureCaption>
<figure confidence="0.999644633333333">
NP [-UNF]
DT
the
JJ
first
NN
kind
IN
of
EDITED-NP
NP
PP [-UNF]
NP/NP
NP/PP
NP
NP/PP
NP
invasion
PP-UNF
of
IN
of
NN
kind
JJ
first
NP/NN
NP/NN
DT
the
</figure>
<page confidence="0.986045">
107
</page>
<table confidence="0.9984658">
System Parseval F EDIT F
Baseline 60.86 42.39
CYK (H06) 71.16 41.7
RCT 68.36 64.41
TAG-based model (JC04) – 79.7
</table>
<tableCaption confidence="0.537188">
Table 1: Baseline results are from a standard CYK parser
with binarized grammar. We were unable to find the cor-
rect configuration to match the baseline results from Hale
et al. RCT results are on the right-corner transformed
grammar (transformed back to flat treebank-style trees
for scoring purposes). CYK and TAG lines show relevant
results from related work.
</tableCaption>
<bodyText confidence="0.99995421875">
non-vocalized terminal symbols, which could not
be expected from the output of a speech recognizer.
Crucially, any information about repair is stripped
from the input, including partial words, repair sym-
bols 3, and interruption point information. While an
integrated system for processing and parsing speech
may use both acoustic and syntactic information to
find repairs, and thus may have access to some of
this information about where interruptions occur,
this experiment is intended to evaluate the use of the
right corner transform and syntactic information on
parsing speech repair. To make a fair comparison to
the CYK baseline of (Hale et al., 2006), the recog-
nizer was given correct part-of-speech tags as input
along with words.
The results presented here use two standard met-
rics for assessing accuracy of transcribed speech
with repairs. The first metric, Parseval F-measure,
takes into account precision and recall of all non-
terminal (and non pre-terminal) constituents in a hy-
pothesized tree relative to the gold standard. The
second metric, EDIT-finding F, measures precision
and recall of the words tagged as EDITED in the
hypothesized tree relative to those tagged EDITED
in the gold standard. F score is defined as usual,
2pr/(p + r) for precision p and recall r.
The results in Table 1 show that this system per-
forms comparably to the state of the art in over-
all parsing accuracy and reasonably well in edit de-
tection. The TAG system (Johnson and Charniak,
2004) achieves a higher EDIT-F score, largely as a
result of its explicit tracking of overlapping words
</bodyText>
<footnote confidence="0.8122475">
3The Switchboard corpus has special terminal symbols indi-
cating e.g. the start and end of the reparandum.
</footnote>
<bodyText confidence="0.9997105">
between reparanda and alterations. A hybrid system
using the right corner transform and keeping infor-
mation about how a repair started may be able to
improve EDIT-F accuracy over this system.
</bodyText>
<sectionHeader confidence="0.999077" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999966333333333">
This paper has described a novel method for pars-
ing speech that contains speech repairs. This system
achieves high accuracy in both parsing and detecting
reparanda in text, by making use of transformations
that create incomplete categories, which model the
reparanda of speech repair well.
</bodyText>
<sectionHeader confidence="0.999429" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999837216216216">
Elizabeth R. Blackmer and Janet L. Mitton. 1991. Theo-
ries of monitoring and the timing of repairs in sponta-
neous speech. Cognition, 39:173–194.
Jean Carletta, Richard Caley, and Stephen Isard. 1993.
A collection of self-repairs from the map task cor-
pus. Technical report, Human Communication Re-
search Centre, University of Edinburgh.
John Hale, Izhak Shafran, Lisa Yung, Bonnie Dorr, Mary
Harper, Anna Krasnyanskaya, Matthew Lease, Yang
Liu, Brian Roark, Matthew Snover, and Robin Stew-
art. 2006. PCFGs with syntactic and prosodic indica-
tors of speech repairs. In Proceedings of the 45th An-
nual Conference of the Association for Computational
Linguistics (COLING-ACL).
Mark Johnson and Eugene Charniak. 2004. A tag-based
noisy channel model of speech repairs. In Proceed-
ings of the 42nd Annual Meeting of the Association
for Computational Linguistics (ACL ’04), pages 33–
39, Barcelona, Spain.
Mark Johnson. 1998a. Finite state approximation of
constraint-based grammars using left-corner grammar
transforms. In Proceedings of COLING/ACL, pages
619–623.
Mark Johnson. 1998b. PCFG models of linguistic tree
representation. Computational Linguistics, 24:613–
632.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics, pages 423–430.
William J.M. Levelt. 1983. Monitoring and self-repair in
speech. Cognition, 14:41–104.
Elizabeth Shriberg. 1994. Preliminaries to a Theory of
Speech Disfluencies. Ph.D. thesis, University of Cali-
fornia at Berkeley.
Mark Steedman. 2000. The syntactic process. MIT
Press/Bradford Books, Cambridge, MA.
</reference>
<page confidence="0.998366">
108
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.964863">
<title confidence="0.999558">Unified Syntactic Model for Parsing Fluent and Disfluent</title>
<author confidence="0.999937">Tim Miller William Schuler</author>
<affiliation confidence="0.999984">University of Minnesota University of Minnesota</affiliation>
<email confidence="0.99864">tmill@cs.umn.eduschuler@cs.umn.edu</email>
<abstract confidence="0.9966438">This paper describes a syntactic representation for modeling speech repairs. This representation makes use of a right corner transform of syntax trees to produce a tree representation in which speech repairs require very few special syntax rules, making better use of training data. PCFGs trained on syntax trees using this model achieve high accuracy on the standard Switchboard parsing task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Elizabeth R Blackmer</author>
<author>Janet L Mitton</author>
</authors>
<title>Theories of monitoring and the timing of repairs in spontaneous speech.</title>
<date>1991</date>
<journal>Cognition,</journal>
<pages>39--173</pages>
<contexts>
<context position="928" citStr="Blackmer and Mitton, 1991" startWordPosition="141" endWordPosition="144">ht corner transform of syntax trees to produce a tree representation in which speech repairs require very few special syntax rules, making better use of training data. PCFGs trained on syntax trees using this model achieve high accuracy on the standard Switchboard parsing task. 1 Introduction Speech repairs occur when a speaker makes a mistake and decides to partially retrace an utterance in order to correct it. Speech repairs are common in spontaneous speech – one study found 30% of dialogue turns contained repairs (Carletta et al., 1993) and another study found one repair every 4.8 seconds (Blackmer and Mitton, 1991). Because of the relatively high frequency of this phenomenon, spontaneous speech recognition systems will need to be able to deal with repairs to achieve high levels of accuracy. The speech repair terminology used here follows that of Shriberg (1994). A speech repair consists of a reparandum, an interruption point, and the alteration. The reparandum contains the words that the speaker means to replace, including both words that are in error and words that will be retraced. The interruption point is the point in time where the stream of speech is actually stopped, and the repairing of the mist</context>
</contexts>
<marker>Blackmer, Mitton, 1991</marker>
<rawString>Elizabeth R. Blackmer and Janet L. Mitton. 1991. Theories of monitoring and the timing of repairs in spontaneous speech. Cognition, 39:173–194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Carletta</author>
<author>Richard Caley</author>
<author>Stephen Isard</author>
</authors>
<title>A collection of self-repairs from the map task corpus.</title>
<date>1993</date>
<tech>Technical report,</tech>
<institution>Human Communication Research Centre, University of Edinburgh.</institution>
<contexts>
<context position="847" citStr="Carletta et al., 1993" startWordPosition="127" endWordPosition="130">sentation for modeling speech repairs. This representation makes use of a right corner transform of syntax trees to produce a tree representation in which speech repairs require very few special syntax rules, making better use of training data. PCFGs trained on syntax trees using this model achieve high accuracy on the standard Switchboard parsing task. 1 Introduction Speech repairs occur when a speaker makes a mistake and decides to partially retrace an utterance in order to correct it. Speech repairs are common in spontaneous speech – one study found 30% of dialogue turns contained repairs (Carletta et al., 1993) and another study found one repair every 4.8 seconds (Blackmer and Mitton, 1991). Because of the relatively high frequency of this phenomenon, spontaneous speech recognition systems will need to be able to deal with repairs to achieve high levels of accuracy. The speech repair terminology used here follows that of Shriberg (1994). A speech repair consists of a reparandum, an interruption point, and the alteration. The reparandum contains the words that the speaker means to replace, including both words that are in error and words that will be retraced. The interruption point is the point in t</context>
</contexts>
<marker>Carletta, Caley, Isard, 1993</marker>
<rawString>Jean Carletta, Richard Caley, and Stephen Isard. 1993. A collection of self-repairs from the map task corpus. Technical report, Human Communication Research Centre, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Hale</author>
<author>Izhak Shafran</author>
<author>Lisa Yung</author>
<author>Bonnie Dorr</author>
<author>Mary Harper</author>
<author>Anna Krasnyanskaya</author>
<author>Matthew Lease</author>
<author>Yang Liu</author>
<author>Brian Roark</author>
<author>Matthew Snover</author>
<author>Robin Stewart</author>
</authors>
<title>PCFGs with syntactic and prosodic indicators of speech repairs.</title>
<date>2006</date>
<booktitle>In Proceedings of the 45th Annual Conference of the Association for Computational Linguistics (COLING-ACL).</booktitle>
<contexts>
<context position="1836" citStr="Hale et al., 2006" startWordPosition="290" endWordPosition="293">um, an interruption point, and the alteration. The reparandum contains the words that the speaker means to replace, including both words that are in error and words that will be retraced. The interruption point is the point in time where the stream of speech is actually stopped, and the repairing of the mistake can begin. The alteration contains the This research was supported by NSF CAREER award 0447685. The views expressed are not necessarily endorsed by the sponsors. words that are meant to replace the words in the reparandum. Recent advances in recognizing spontaneous speech with repairs (Hale et al., 2006; Johnson and Charniak, 2004) have used parsing approaches on transcribed speech to account for the structure inherent in speech repairs at the word level and above. One salient aspect of structure is the fact that there is often a good deal of overlap in words between the reparandum and the alteration, as speakers may trace back several words when restarting after an error. For instance, in the repair ... a flight to Boston, uh, I mean, to Denver on Friday... , there is an exact match of the word ‘to’ between reparandum and repair, and a part of speech match between the words ‘Boston’ and ‘De</context>
<context position="8901" citStr="Hale et al., 2006" startWordPosition="1457" endWordPosition="1460">UNF is correctly recognized, and the speaker is essentially in an error state, there may be several partially completed constituents above – in Figure 1, the NP, PP, and NP above the PP-UNF. These constituents need to be completed, but using the standard annotation there is only one chance to make use of the information about the error that has occurred – the NP —* NP PP-UNF rule. Thus, by the time the error section is completed, there is no information by which a parsing algorithm could choose to reduce the topmost NP to EDITED other than independent rule probabilities. The approach used by (Hale et al., 2006) works because the information about the transition to an error state is propagated up the tree, in the form of the -UNF tags. As the parsing chart is filled in bottom up, each rule applied is essentially coming out of a special repair rule set, and so at the top of the tree the EDITED hypothesis is much more likely. However, this requires that several fluent speech rules from the data set be modified for use in a special repair grammar, which not only reduces the amount of available training data, but violates our intuition that most reparanda are fluent up until the actual edit occurs. The r</context>
<context position="11003" citStr="Hale et al., 2006" startWordPosition="1817" endWordPosition="1820">fore potentially makes better use of limited data. 3 Evaluation The evaluation of this system was performed on the Switchboard corpus, using the mrg annotations in directories 2 and 3 for training, and the files sw4004.mrg to sw4153.mrg in directory 4 for evaluation, following Johnson and Charniak(2004). The input to the system consists of the terminal symbols from the trees in the corpus section mentioned above. The terminal symbol strings are first pre-processed by stripping punctuation and other NN invasion PP-UNF IN of Figure 1: Standard tree repair structure, with -UNF propagation as in (Hale et al., 2006) shown in brackets. NP [-UNF] DT the JJ first NN kind IN of EDITED-NP NP PP [-UNF] NP/NP NP/PP NP NP/PP NP invasion PP-UNF of IN of NN kind JJ first NP/NN NP/NN DT the 107 System Parseval F EDIT F Baseline 60.86 42.39 CYK (H06) 71.16 41.7 RCT 68.36 64.41 TAG-based model (JC04) – 79.7 Table 1: Baseline results are from a standard CYK parser with binarized grammar. We were unable to find the correct configuration to match the baseline results from Hale et al. RCT results are on the right-corner transformed grammar (transformed back to flat treebank-style trees for scoring purposes). CYK and TAG </context>
<context position="12307" citStr="Hale et al., 2006" startWordPosition="2034" endWordPosition="2037">ould not be expected from the output of a speech recognizer. Crucially, any information about repair is stripped from the input, including partial words, repair symbols 3, and interruption point information. While an integrated system for processing and parsing speech may use both acoustic and syntactic information to find repairs, and thus may have access to some of this information about where interruptions occur, this experiment is intended to evaluate the use of the right corner transform and syntactic information on parsing speech repair. To make a fair comparison to the CYK baseline of (Hale et al., 2006), the recognizer was given correct part-of-speech tags as input along with words. The results presented here use two standard metrics for assessing accuracy of transcribed speech with repairs. The first metric, Parseval F-measure, takes into account precision and recall of all nonterminal (and non pre-terminal) constituents in a hypothesized tree relative to the gold standard. The second metric, EDIT-finding F, measures precision and recall of the words tagged as EDITED in the hypothesized tree relative to those tagged EDITED in the gold standard. F score is defined as usual, 2pr/(p + r) for p</context>
</contexts>
<marker>Hale, Shafran, Yung, Dorr, Harper, Krasnyanskaya, Lease, Liu, Roark, Snover, Stewart, 2006</marker>
<rawString>John Hale, Izhak Shafran, Lisa Yung, Bonnie Dorr, Mary Harper, Anna Krasnyanskaya, Matthew Lease, Yang Liu, Brian Roark, Matthew Snover, and Robin Stewart. 2006. PCFGs with syntactic and prosodic indicators of speech repairs. In Proceedings of the 45th Annual Conference of the Association for Computational Linguistics (COLING-ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Eugene Charniak</author>
</authors>
<title>A tag-based noisy channel model of speech repairs.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL ’04),</booktitle>
<pages>33--39</pages>
<location>Barcelona,</location>
<contexts>
<context position="1865" citStr="Johnson and Charniak, 2004" startWordPosition="294" endWordPosition="297"> point, and the alteration. The reparandum contains the words that the speaker means to replace, including both words that are in error and words that will be retraced. The interruption point is the point in time where the stream of speech is actually stopped, and the repairing of the mistake can begin. The alteration contains the This research was supported by NSF CAREER award 0447685. The views expressed are not necessarily endorsed by the sponsors. words that are meant to replace the words in the reparandum. Recent advances in recognizing spontaneous speech with repairs (Hale et al., 2006; Johnson and Charniak, 2004) have used parsing approaches on transcribed speech to account for the structure inherent in speech repairs at the word level and above. One salient aspect of structure is the fact that there is often a good deal of overlap in words between the reparandum and the alteration, as speakers may trace back several words when restarting after an error. For instance, in the repair ... a flight to Boston, uh, I mean, to Denver on Friday... , there is an exact match of the word ‘to’ between reparandum and repair, and a part of speech match between the words ‘Boston’ and ‘Denver’. Another sort of struct</context>
<context position="13131" citStr="Johnson and Charniak, 2004" startWordPosition="2173" endWordPosition="2176">first metric, Parseval F-measure, takes into account precision and recall of all nonterminal (and non pre-terminal) constituents in a hypothesized tree relative to the gold standard. The second metric, EDIT-finding F, measures precision and recall of the words tagged as EDITED in the hypothesized tree relative to those tagged EDITED in the gold standard. F score is defined as usual, 2pr/(p + r) for precision p and recall r. The results in Table 1 show that this system performs comparably to the state of the art in overall parsing accuracy and reasonably well in edit detection. The TAG system (Johnson and Charniak, 2004) achieves a higher EDIT-F score, largely as a result of its explicit tracking of overlapping words 3The Switchboard corpus has special terminal symbols indicating e.g. the start and end of the reparandum. between reparanda and alterations. A hybrid system using the right corner transform and keeping information about how a repair started may be able to improve EDIT-F accuracy over this system. 4 Conclusion This paper has described a novel method for parsing speech that contains speech repairs. This system achieves high accuracy in both parsing and detecting reparanda in text, by making use of </context>
</contexts>
<marker>Johnson, Charniak, 2004</marker>
<rawString>Mark Johnson and Eugene Charniak. 2004. A tag-based noisy channel model of speech repairs. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL ’04), pages 33– 39, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Finite state approximation of constraint-based grammars using left-corner grammar transforms.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING/ACL,</booktitle>
<pages>619--623</pages>
<contexts>
<context position="4545" citStr="Johnson (1998" startWordPosition="734" endWordPosition="735">o be built up using fluent speech rules up until the moment of interruption, at which point a special repair rule may be applied. This property will be examined further in section 2.3, following a technical description of the representation scheme. 2.1 Binary branching structure In order to obtain a linguistically plausible rightcorner transform representation of incomplete constituents, the Switchboard corpus is subjected to a pre-process transform to introduce binary-branching nonterminal projections, and fold empty categories into nonterminal symbols in a manner similar to that proposed by Johnson (1998b) and Klein and Manning (2003). This binarization is done in in such a way as to preserve linguistic intuitions of head projection, so that the depth requirements of rightcorner transformed trees will be reasonable approximations to the working memory requirements of a human reader or listener. Trees containing speech repairs are reduced in arity by merging repair structure lower in the tree, when possible. As seen in the left tree below, 1 repair structure is annotated in a flat manner, which can lead to high-arity rules which are sparsely represented in the data set, and thus difficult to l</context>
</contexts>
<marker>Johnson, 1998</marker>
<rawString>Mark Johnson. 1998a. Finite state approximation of constraint-based grammars using left-corner grammar transforms. In Proceedings of COLING/ACL, pages 619–623.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>PCFG models of linguistic tree representation.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<pages>632</pages>
<contexts>
<context position="4545" citStr="Johnson (1998" startWordPosition="734" endWordPosition="735">o be built up using fluent speech rules up until the moment of interruption, at which point a special repair rule may be applied. This property will be examined further in section 2.3, following a technical description of the representation scheme. 2.1 Binary branching structure In order to obtain a linguistically plausible rightcorner transform representation of incomplete constituents, the Switchboard corpus is subjected to a pre-process transform to introduce binary-branching nonterminal projections, and fold empty categories into nonterminal symbols in a manner similar to that proposed by Johnson (1998b) and Klein and Manning (2003). This binarization is done in in such a way as to preserve linguistic intuitions of head projection, so that the depth requirements of rightcorner transformed trees will be reasonable approximations to the working memory requirements of a human reader or listener. Trees containing speech repairs are reduced in arity by merging repair structure lower in the tree, when possible. As seen in the left tree below, 1 repair structure is annotated in a flat manner, which can lead to high-arity rules which are sparsely represented in the data set, and thus difficult to l</context>
</contexts>
<marker>Johnson, 1998</marker>
<rawString>Mark Johnson. 1998b. PCFG models of linguistic tree representation. Computational Linguistics, 24:613– 632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="4576" citStr="Klein and Manning (2003)" startWordPosition="737" endWordPosition="741">fluent speech rules up until the moment of interruption, at which point a special repair rule may be applied. This property will be examined further in section 2.3, following a technical description of the representation scheme. 2.1 Binary branching structure In order to obtain a linguistically plausible rightcorner transform representation of incomplete constituents, the Switchboard corpus is subjected to a pre-process transform to introduce binary-branching nonterminal projections, and fold empty categories into nonterminal symbols in a manner similar to that proposed by Johnson (1998b) and Klein and Manning (2003). This binarization is done in in such a way as to preserve linguistic intuitions of head projection, so that the depth requirements of rightcorner transformed trees will be reasonable approximations to the working memory requirements of a human reader or listener. Trees containing speech repairs are reduced in arity by merging repair structure lower in the tree, when possible. As seen in the left tree below, 1 repair structure is annotated in a flat manner, which can lead to high-arity rules which are sparsely represented in the data set, and thus difficult to learn. This problem can be mitig</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William J M Levelt</author>
</authors>
<title>Monitoring and self-repair in speech.</title>
<date>1983</date>
<journal>Cognition,</journal>
<pages>14--41</pages>
<contexts>
<context position="2500" citStr="Levelt (1983)" startWordPosition="411" endWordPosition="413">roaches on transcribed speech to account for the structure inherent in speech repairs at the word level and above. One salient aspect of structure is the fact that there is often a good deal of overlap in words between the reparandum and the alteration, as speakers may trace back several words when restarting after an error. For instance, in the repair ... a flight to Boston, uh, I mean, to Denver on Friday... , there is an exact match of the word ‘to’ between reparandum and repair, and a part of speech match between the words ‘Boston’ and ‘Denver’. Another sort of structure in repair is what Levelt (1983) called the well-formedness rule. This rule states that the constituent started in the reparandum and repair are ultimately of syntactic types that could be grammatically joined by a conjunction. For example, in the repair above, the well-formedness rule says that the repair is well formed if the fragment ... a flight to Boston and to Denver... is grammatical. In this case the repair is well formed since the conjunction is grammatical, if not meaningful. The approach described here makes use of a transform on a tree-annotated corpus to build a syntactic model of speech repair which takes advan</context>
</contexts>
<marker>Levelt, 1983</marker>
<rawString>William J.M. Levelt. 1983. Monitoring and self-repair in speech. Cognition, 14:41–104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elizabeth Shriberg</author>
</authors>
<title>Preliminaries to a Theory of Speech Disfluencies.</title>
<date>1994</date>
<tech>Ph.D. thesis,</tech>
<institution>University of California at Berkeley.</institution>
<contexts>
<context position="1179" citStr="Shriberg (1994)" startWordPosition="183" endWordPosition="184">rd parsing task. 1 Introduction Speech repairs occur when a speaker makes a mistake and decides to partially retrace an utterance in order to correct it. Speech repairs are common in spontaneous speech – one study found 30% of dialogue turns contained repairs (Carletta et al., 1993) and another study found one repair every 4.8 seconds (Blackmer and Mitton, 1991). Because of the relatively high frequency of this phenomenon, spontaneous speech recognition systems will need to be able to deal with repairs to achieve high levels of accuracy. The speech repair terminology used here follows that of Shriberg (1994). A speech repair consists of a reparandum, an interruption point, and the alteration. The reparandum contains the words that the speaker means to replace, including both words that are in error and words that will be retraced. The interruption point is the point in time where the stream of speech is actually stopped, and the repairing of the mistake can begin. The alteration contains the This research was supported by NSF CAREER award 0447685. The views expressed are not necessarily endorsed by the sponsors. words that are meant to replace the words in the reparandum. Recent advances in recog</context>
</contexts>
<marker>Shriberg, 1994</marker>
<rawString>Elizabeth Shriberg. 1994. Preliminaries to a Theory of Speech Disfluencies. Ph.D. thesis, University of California at Berkeley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The syntactic process.</title>
<date>2000</date>
<publisher>MIT Press/Bradford Books,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="6828" citStr="Steedman, 2000" startWordPosition="1102" endWordPosition="1103"> of category A1 lacking an instance of category A2 to the right. Rewrite rules for the right-corner transform are shown below: A1 A1 Here, the first rewrite rule is applied iteratively (bottom-up on the tree) to flatten all right recursion, using incomplete constituents to record the original nonterminal ordering. The second rule is then applied to generate left recursive structure, preserving this ordering. The incomplete constituent categories created by the right corner transform are similar in form and meaning to non-constituent categories used in Combinatorial Categorial Grammars (CCGs) (Steedman, 2000). Unlike CCGs, however, a right corner transformed grammar does not allow backward function application, composition, or raising. As a result, it does not introduce spurious ambiguity between forward and backward operations, but cannot be taken to explicitly encode argument structure, as CCGs can. 2All super-binary branches remaining after the above preprocess are ‘nominally’ decomposed into right-branching structures by introducing intermediate nodes with labels concatenated from the labels of its children, delimited by underscores A0 EDITED ET* A1:α2 α3 ==&gt; A0 A1 α3 α2 A1/A2:α1 α2 α1 α2 α2 A</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. 2000. The syntactic process. MIT Press/Bradford Books, Cambridge, MA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>