<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009053">
<title confidence="0.927369">
The SAMMIE System: Multimodal In-Car Dialogue
</title>
<author confidence="0.9662695">
Tilman Becker, Peter Poller,
Jan Schehl
</author>
<affiliation confidence="0.626065">
DFKI
</affiliation>
<email confidence="0.982861">
First.Last@dfki.de
</email>
<sectionHeader confidence="0.993432" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997988">
The SAMMIE&apos; system is an in-car multi-
modal dialogue system for an MP3 ap-
plication. It is used as a testing environ-
ment for our research in natural, intuitive
mixed-initiative interaction, with particu-
lar emphasis on multimodal output plan-
ning and realization aimed to produce out-
put adapted to the context, including the
driver’s attention state w.r.t. the primary
driving task.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999951777777778">
The SAMMIE system, developed in the TALK
project in cooperation between several academic
and industrial partners, employs the Information
State Update paradigm, extended to model collab-
orative problem solving, multimodal context and
the driver’s attention state. We performed exten-
sive user studies in a WOZ setup to guide the sys-
tem design. A formal usability evaluation of the
system’s baseline version in a laboratory environ-
ment has been carried out with overall positive re-
sults. An enhanced version of the system will be
integrated and evaluated in a research car.
In the following sections, we describe the func-
tionality and architecture of the system, point out
its special features in comparison to existing work,
and give more details on the modules that are in
the focus of our research interests. Finally, we
summarize our experiments and evaluation results.
</bodyText>
<sectionHeader confidence="0.995502" genericHeader="introduction">
2 Functionality
</sectionHeader>
<bodyText confidence="0.999737125">
The SAMMIE system provides a multi-modal inter-
face to an in-car MP3 player (see Fig. 1) through
speech and haptic input with a BMW iDrive input
device, a button which can be turned, pushed down
and sideways in four directions (see Fig. 2 left).
System output is provided by speech and a graphi-
cal display integrated into the car’s dashboard. An
example of the system display is shown in Fig. 2.
</bodyText>
<footnote confidence="0.5801365">
&apos;SAMMIE stands for Saarbr¨ucken Multimodal MP3 Player
Interaction Experiment.
</footnote>
<author confidence="0.6834895">
Nate Blaylock, Ciprian Gerstenberger,
Ivana Kruijff-Korbayov´a
</author>
<affiliation confidence="0.712056">
Saarland University
</affiliation>
<email confidence="0.832845">
talk-mit@coli.uni-sb.de
</email>
<figureCaption confidence="0.99901">
Figure 1: User environment in laboratory setup.
</figureCaption>
<bodyText confidence="0.999563260869565">
The MP3 player application offers a wide range
of functions: The user can control the currently
playing song, search and browse an MP3 database
by looking for any of the fields (song, artist, al-
bum, year, etc.), search and select playlists and
even construct and edit playlists.
The user of SAMMIE has complete freedom in
interacting with the system. Input can be through
any modality and is not restricted to answers to
system queries. On the contrary, the user can give
new tasks as well as any information relevant to
the current task at any time. This is achieved by
modeling the interaction as a collaborative prob-
lem solving process, and multi-modal interpreta-
tion that fits user input into the context of the
current task. The user is also free in their use
of multimodality: SAMMIE handles deictic refer-
ences (e.g., Play this title while pushing the iDrive
button) and also cross-modal references, e.g., Play
the third song (on the list). Table 1 shows a typ-
ical interaction with the SAMMIE system; the dis-
played song list is in Fig. 2. SAMMIE supports in-
teraction in German and English.
</bodyText>
<sectionHeader confidence="0.989555" genericHeader="method">
3 Architecture
</sectionHeader>
<bodyText confidence="0.972331333333333">
Our system architecture follows the classical ap-
proach (Bunt et al., 2005) of a pipelined architec-
ture with multimodal interpretation (fusion) and
</bodyText>
<page confidence="0.978241">
57
</page>
<bodyText confidence="0.889934090909091">
Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 57–60,
Sydney, July 2006. c�2006 Association for Computational Linguistics
U: Show me the Beatles albums.
S: I have these four Beatles albums.
[shows a list of album names]
U: Which songs are on this one?
[selects the Red Album]
S: The Red Album contains these songs
[shows a list of the songs]
U: Play the third one.
S: [music plays]
</bodyText>
<tableCaption confidence="0.994081">
Table 1: A typical interaction with SAMMIE.
</tableCaption>
<bodyText confidence="0.999719368421053">
fission modules encapsulating the dialogue man-
ager. Fig. 2 shows the modules and their inter-
action: Modality-specific recognizers and analyz-
ers provide semantically interpreted input to the
multimodal fusion module that interprets them in
the context of the other modalities and the cur-
rent dialogue context. The dialogue manager de-
cides on the next system move, based on its model
of the tasks as collaborative problem solving, the
current context and also the results from calls to
the MP3 database. The turn planning module then
determines an appropriate message to the user by
planning the content, distributing it over the avail-
able output modalities and finally co-ordinating
and synchronizing the output. Modality-specific
output modules generate spoken output and graph-
ical display update. All modules interact with the
extended information state which stores all context
information.
</bodyText>
<figureCaption confidence="0.998671">
Figure 2: SAMMIE system architecture.
</figureCaption>
<bodyText confidence="0.999567625">
Many tasks in the SAMMIE system are mod-
eled by a plan-based approach. Discourse mod-
eling, interpretation management, dialogue man-
agement and linguistic planning, and turn plan-
ning are all based on the production rule system
PATE2 (Pfleger, 2004). It is based on some con-
cepts of the ACT-R 4.0 system, in particular the
goal-oriented application of production rules, the
</bodyText>
<footnote confidence="0.7890015">
2Short for (P)roduction rule system based on (A)ctivation
and (T)yped feature structure (E)lements.
</footnote>
<bodyText confidence="0.999872875">
activation of working memory elements, and the
weighting of production rules. In processing typed
feature structures, PATE provides two operations
that both integrate data and also are suitable for
condition matching in production rule systems,
namely a slightly extended version of the general
unification, but also the discourse-oriented opera-
tion overlay (Alexandersson and Becker, 2001).
</bodyText>
<sectionHeader confidence="0.998635" genericHeader="method">
4 Related Work and Novel Aspects
</sectionHeader>
<bodyText confidence="0.9999871875">
Many dialogue systems deployed today follow a
state-based approach that explicitly models the
full (finite) set of dialogue states and all possible
transitions between them. The VoiceXML3 stan-
dard is a prominent example of this approach. This
has two drawbacks: on the one hand, this approach
is not very flexible and typically allows only so-
called system controlled dialogues where the user
is restricted to choosing their input from provided
menu-like lists and answering specific questions.
The user never is in control of the dialogue. For
restricted tasks with a clear structure, such an ap-
proach is often sufficient and has been applied suc-
cessfully. On the other hand, building such appli-
cations requires a fully specified model of all pos-
sible states and transitions, making larger applica-
tions expensive to build and difficult to test.
In SAMMIE we adopt an approach that mod-
els the interaction on an abstract level as collab-
orative problem solving and adds application spe-
cific knowledge on the possible tasks, available re-
sources and known recipes for achieving the goals.
In addition, all relevant context information is
administered in an Extended Information State.
This is an extension of the Information State Up-
date approach (Traum and Larsson, 2003) to the
multi-modal setting.
Novel aspects in turn planning and realization
include the comprehensive modeling in a sin-
gle, OWL-based ontology and an extended range
of context-sensitive variation, including system
alignment to the user on multiple levels.
</bodyText>
<sectionHeader confidence="0.984239" genericHeader="method">
5 Flexible Multi-modal Interaction
</sectionHeader>
<subsectionHeader confidence="0.727884">
5.1 Extended Information State
</subsectionHeader>
<bodyText confidence="0.999846333333333">
The information state of a multimodal system
needs to contain a representation of contextual in-
formation about discourse, but also a represen-
tation of modality-specific information and user-
specific information which can be used to plan
system output suited to a given context. The over-
</bodyText>
<footnote confidence="0.79466">
3http://www.w3.org/TR/voicexml20
</footnote>
<page confidence="0.922147">
58
</page>
<equation confidence="0.9328545">
�
� � � � � � � � � � � � � � � � � � � �
</equation>
<bodyText confidence="0.999447833333333">
all information state (IS) of the SAMMIE system is
shown in Fig. 3.
The contextual information partition of the IS
represents the multimodal discourse context. It
contains a record of the latest user utterance and
preceding discourse history representing in a uni-
form way the salient discourse entities introduced
in the different modalities. We adopt the three-
tiered multimodal context representation used in
the SmartKom system (Pfleger et al., 2003). The
contents of the task partition are explained in the
next section.
</bodyText>
<subsectionHeader confidence="0.99238">
5.2 Collaborative Problem Solving
</subsectionHeader>
<bodyText confidence="0.999994428571429">
Our dialogue manager is based on an
agent-based model which views dialogue
as collaborative problem-solving (CPS)
(Blaylock and Allen, 2005). The basic building
blocks of the formal CPS model are problem-
solving (PS) objects, which we represent as
typed feature structures. PS object types form a
single-inheritance hierarchy. In our CPS model,
we define types for the upper level of an ontology
of PS objects, which we term abstract PS objects.
There are six abstract PS objects in our model
from which all other domain-specific PS objects
inherit: objective, recipe, constraint, evaluation,
situation, and resource. These are used to model
problem-solving at a domain-independent level
and are taken as arguments by all update opera-
tors of the dialogue manager which implement
conversation acts (Blaylock and Allen, 2005).
The model is then specialized to a domain by
inheriting and instantiating domain-specific types
and instances of the PS objects.
</bodyText>
<subsectionHeader confidence="0.996311">
5.3 Adaptive Turn Planning
</subsectionHeader>
<bodyText confidence="0.999293696969696">
The fission component comprises detailed con-
tent planning, media allocation and coordination
and synchronization. Turn planning takes a set
of CPS-specific conversational acts generated by
the dialogue manager and maps them to modality-
specific communicative acts.
Information on how content should be dis-
tributed over the available modalities (speech or
graphics) is obtained from Pastis, a module which
stores discourse-specific information. Pastis pro-
vides information about (i) the modality on which
the user is currently focused, derived by the cur-
rent discourse context; (ii) the user’s current cog-
nitive load when system interaction becomes a
secondary task (e.g., system interaction while
driving); (iii) the user’s expertise, which is rep-
resented as a state variable. Pastis also contains
information about factors that influence the prepa-
ration of output rendering for a modality, like the
currently used language (German or English) or
the display capabilities (e.g., maximum number of
displayable objects within a table). Together with
the dialogue manager’s embedded part of the in-
formation state, the information stored by Pastis
forms the Extended Information State of the SAM-
MIE system (Fig. 3).
Planning is then executed through a set of pro-
duction rules that determine which kind of infor-
mation should be presented through which of the
available modalities. The rule set is divided in two
subsets, domain-specific and domain-independent
rules which together form the system’s multi-
modal plan library.
</bodyText>
<figure confidence="0.98264725">
contextual-info:
last-user-utterance:
� interp : set(grounding-acts)
: modality-requested : modality
modalities-used : set(msInput)
discourse-history:
: list(discourse-objects)
modality-info:
� speech : speechInfo �
: graphic : graphicInfo
user-info:
r cognitive-load : cogLoadInfo 1
L user-expertise : expertiseInfo J
task-info:
r cps-state : c-situation (see below for details) 1
L pending-sys-utt : list(grounding-acts) J
</figure>
<figureCaption confidence="0.999825">
Figure 3: SAMMIE Information State structure.
</figureCaption>
<subsectionHeader confidence="0.93785">
5.4 Spoken Natural Language Output
Generation
</subsectionHeader>
<bodyText confidence="0.999987294117647">
Our goal is to produce output that varies in the sur-
face realization form and is adapted to the con-
text. A template-based module has been devel-
oped and is sufficient for classes of system output
that do not need fine-tuned context-driven varia-
tion. Our template-based generator can also de-
liver alternative realizations, e.g., alternative syn-
tactic constructions, referring expressions, or lexi-
cal items. It is implemented by a set of straightfor-
ward sentence planning rules in the PATE system
to build the templates, and a set of XSLT trans-
formations to yield the output strings. Output in
German and English is produced by accessing dif-
ferent dictionaries in a uniform way.
In order to facilitate incremental development
of the whole system, our template-based mod-
ule has a full coverage wrt. the classes of sys-
</bodyText>
<equation confidence="0.953492">
�
� � � � � � � � � � � � � � � � � � � �
</equation>
<page confidence="0.98812">
59
</page>
<bodyText confidence="0.99979">
tem output that are needed. In parallel, we are
experimenting with a linguistically more power-
ful grammar-based generator using OpenCCG4,
an open-source natural language processing en-
vironment (Baldridge and Kruijff, 2003). This al-
lows for more fine-grained and controlled choices
between linguistic expressions in order to achieve
contextually appropriate output.
</bodyText>
<subsectionHeader confidence="0.999881">
5.5 Modeling with an Ontology
</subsectionHeader>
<bodyText confidence="0.999983368421053">
We use a full model in OWL as the knowledge rep-
resentation format in the dialogue manager, turn
planner and sentence planner. This model in-
cludes the entities, properties and relations of the
MP3 domain–including the player, data base and
playlists. Also, all possible tasks that the user may
perform are modeled explicitly. This task model
is user centered and not simply a model of the
application’s API.The OWL-based model is trans-
formed automatically to the internal format used
in the PATE rule-interpreter.
We use multiple inheritance to model different
views of concepts and the corresponding presen-
tation possibilities; e.g., a song is a browsable-
object as well as a media-object and thus allows
for very different presentations, depending on con-
text. Thereby PATE provides an efficient and ele-
gant way to create more generic presentation plan-
ning rules.
</bodyText>
<sectionHeader confidence="0.998812" genericHeader="conclusions">
6 Experiments and Evaluation
</sectionHeader>
<bodyText confidence="0.999987227272727">
So far we conducted two WOZ data collection
experiments and one evaluation experiment with
a baseline version of the SAMMIE system. The
SAMMIE-1 WOZ experiment involved only spo-
ken interaction, SAMMIE-2 was multimodal, with
speech and haptic input, and the subjects had
to perform a primary driving task using a Lane
Change simulator (Mattes, 2003) in a half of their
experiment session. The wizard was simulating
an MP3 player application with access to a large
database of information (but not actual music) of
more than 150,000 music albums (almost 1 mil-
lion songs). In order to collect data with a variety
of interaction strategies, we used multiple wizards
and gave them freedom to decide about their re-
sponse and its realization. In the multimodal setup
in SAMMIE-2, the wizards could also freely de-
cide between mono-modal and multimodal output.
(See (Kruijff-Korbayov´a et al., 2005) for details.)
We have just completed a user evaluation to
explore the user-acceptance, usability, and per-
formance of the baseline implementation of the
</bodyText>
<footnote confidence="0.744269">
4http://openccg.sourceforge.net
</footnote>
<bodyText confidence="0.9978834">
SAMMIE multimodal dialogue system. The users
were asked to perform tasks which tested the sys-
tem functionality. The evaluation analyzed the
user’s interaction with the baseline system and
combined objective measurements like task com-
pletion (89%) and subjective ratings from the test
subjects (80% positive).
Acknowledgments This work has been carried
out in the TALK project, funded by the EU 6th
Framework Program, project No. IST-507802.
</bodyText>
<sectionHeader confidence="0.999104" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999825818181818">
[Alexandersson and Becker2001] J. Alexandersson and
T. Becker. 2001. Overlay as the basic operation for
discourse processing in a multimodal dialogue system. In
Proceedings of the 2nd IJCAI Workshop on Knowledge
and Reasoning in Practical Dialogue Systems, Seattle,
Washington, August.
[Baldridge and Kruijff2003] J.M. Baldridge and G.J.M. Krui-
jff. 2003. Multi-Modal Combinatory Categorial Gram-
mar. In Proceedings of the 10th Annual Meeting of the
European Chapter of the Association for Computational
Linguistics (EACL’03), Budapest, Hungary, April.
[Blaylock and Allen2005] N. Blaylock and J. Allen. 2005. A
collaborative problem-solving model of dialogue. In Laila
Dybkjær and Wolfgang Minker, editors, Proceedings of
the 6th SIGdial Workshop on Discourse and Dialogue,
pages 200–211, Lisbon, September 2–3.
[Bunt et al.2005] H. Bunt, M. Kipp, M. Maybury, and
W. Wahlster. 2005. Fusion and coordination for multi-
modal interactive information presentation: Roadmap, ar-
chitecture, tools, semantics. In O. Stock and M. Zanca-
naro, editors, Multimodal Intelligent Information Presen-
tation, volume 27 of Text, Speech and Language Technol-
ogy, pages 325–340. Kluwer Academic.
[Kruijff-Korbayov´a et al.2005] I. Kruijff-Korbayov´a,
T. Becker, N. Blaylock, C. Gerstenberger, M. Kaißer,
P. Poller, J. Schehl, and V. Rieser. 2005. An experiment
setup for collecting data for adaptive output planning in
a multimodal dialogue system. In Proc. of ENLG, pages
191–196.
[Mattes2003] S. Mattes. 2003. The lane-change-task as a tool
for driver distraction evaluation. In Proc. ofIGfA.
[Pfleger et al.2003] N. Pfleger, J. Alexandersson, and
T. Becker. 2003. A robust and generic discourse model
for multimodal dialogue. In Proceedings of the 3rd
Workshop on Knowledge and Reasoning in Practical
Dialogue Systems, Acapulco.
[Pfleger2004] N. Pfleger. 2004. Context based multimodal
fusion. In ICMI ’04: Proceedings of the 6th interna-
tional conference on Multimodal interfaces, pages 265–
272, New York, NY, USA. ACM Press.
[Traum and Larsson2003] David R. Traum and Staffan Lars-
son. 2003. The information state approach to dialog man-
agement. In Current and New Directions in Discourse and
Dialog. Kluwer.
</reference>
<page confidence="0.99841">
60
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.590289">
<title confidence="0.99982">Multimodal In-Car Dialogue</title>
<author confidence="0.9927865">Tilman Becker</author>
<author confidence="0.9927865">Peter Poller</author>
<author confidence="0.9927865">Jan Schehl</author>
<affiliation confidence="0.803259">DFKI</affiliation>
<email confidence="0.767139">First.Last@dfki.de</email>
<abstract confidence="0.998308545454546">is an in-car multimodal dialogue system for an MP3 application. It is used as a testing environment for our research in natural, intuitive mixed-initiative interaction, with particular emphasis on multimodal output planning and realization aimed to produce output adapted to the context, including the driver’s attention state w.r.t. the primary driving task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Alexandersson</author>
<author>T Becker</author>
</authors>
<title>Overlay as the basic operation for discourse processing in a multimodal dialogue system.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2nd IJCAI Workshop on Knowledge and Reasoning in Practical Dialogue Systems,</booktitle>
<location>Seattle, Washington,</location>
<marker>[Alexandersson and Becker2001]</marker>
<rawString>J. Alexandersson and T. Becker. 2001. Overlay as the basic operation for discourse processing in a multimodal dialogue system. In Proceedings of the 2nd IJCAI Workshop on Knowledge and Reasoning in Practical Dialogue Systems, Seattle, Washington, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Baldridge</author>
<author>G J M Kruijff</author>
</authors>
<title>Multi-Modal Combinatory Categorial Grammar.</title>
<date>2003</date>
<booktitle>In Proceedings of the 10th Annual Meeting of the European Chapter of the Association for Computational Linguistics (EACL’03),</booktitle>
<location>Budapest, Hungary,</location>
<marker>[Baldridge and Kruijff2003]</marker>
<rawString>J.M. Baldridge and G.J.M. Kruijff. 2003. Multi-Modal Combinatory Categorial Grammar. In Proceedings of the 10th Annual Meeting of the European Chapter of the Association for Computational Linguistics (EACL’03), Budapest, Hungary, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Blaylock</author>
<author>J Allen</author>
</authors>
<title>A collaborative problem-solving model of dialogue.</title>
<date>2005</date>
<booktitle>In Laila Dybkjær and Wolfgang Minker, editors, Proceedings of the 6th SIGdial Workshop on Discourse and Dialogue,</booktitle>
<pages>200--211</pages>
<location>Lisbon,</location>
<marker>[Blaylock and Allen2005]</marker>
<rawString>N. Blaylock and J. Allen. 2005. A collaborative problem-solving model of dialogue. In Laila Dybkjær and Wolfgang Minker, editors, Proceedings of the 6th SIGdial Workshop on Discourse and Dialogue, pages 200–211, Lisbon, September 2–3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Bunt</author>
<author>M Kipp</author>
<author>M Maybury</author>
<author>W Wahlster</author>
</authors>
<title>Fusion and coordination for multimodal interactive information presentation: Roadmap, architecture, tools, semantics.</title>
<date>2005</date>
<booktitle>Multimodal Intelligent Information Presentation, volume 27 of Text, Speech and Language Technology,</booktitle>
<pages>325--340</pages>
<editor>In O. Stock and M. Zancanaro, editors,</editor>
<publisher>Kluwer Academic.</publisher>
<marker>[Bunt et al.2005]</marker>
<rawString>H. Bunt, M. Kipp, M. Maybury, and W. Wahlster. 2005. Fusion and coordination for multimodal interactive information presentation: Roadmap, architecture, tools, semantics. In O. Stock and M. Zancanaro, editors, Multimodal Intelligent Information Presentation, volume 27 of Text, Speech and Language Technology, pages 325–340. Kluwer Academic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Kruijff-Korbayov´a</author>
<author>T Becker</author>
<author>N Blaylock</author>
<author>C Gerstenberger</author>
<author>M Kaißer</author>
<author>P Poller</author>
<author>J Schehl</author>
<author>V Rieser</author>
</authors>
<title>An experiment setup for collecting data for adaptive output planning in a multimodal dialogue system.</title>
<date>2005</date>
<booktitle>In Proc. of ENLG,</booktitle>
<pages>191--196</pages>
<marker>[Kruijff-Korbayov´a et al.2005]</marker>
<rawString>I. Kruijff-Korbayov´a, T. Becker, N. Blaylock, C. Gerstenberger, M. Kaißer, P. Poller, J. Schehl, and V. Rieser. 2005. An experiment setup for collecting data for adaptive output planning in a multimodal dialogue system. In Proc. of ENLG, pages 191–196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Mattes</author>
</authors>
<title>The lane-change-task as a tool for driver distraction evaluation.</title>
<date>2003</date>
<booktitle>In Proc. ofIGfA.</booktitle>
<marker>[Mattes2003]</marker>
<rawString>S. Mattes. 2003. The lane-change-task as a tool for driver distraction evaluation. In Proc. ofIGfA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Pfleger</author>
<author>J Alexandersson</author>
<author>T Becker</author>
</authors>
<title>A robust and generic discourse model for multimodal dialogue.</title>
<date>2003</date>
<booktitle>In Proceedings of the 3rd Workshop on Knowledge and Reasoning in Practical Dialogue Systems,</booktitle>
<location>Acapulco.</location>
<marker>[Pfleger et al.2003]</marker>
<rawString>N. Pfleger, J. Alexandersson, and T. Becker. 2003. A robust and generic discourse model for multimodal dialogue. In Proceedings of the 3rd Workshop on Knowledge and Reasoning in Practical Dialogue Systems, Acapulco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Pfleger</author>
</authors>
<title>Context based multimodal fusion.</title>
<date>2004</date>
<booktitle>In ICMI ’04: Proceedings of the 6th international conference on Multimodal interfaces,</booktitle>
<pages>265--272</pages>
<publisher>ACM Press.</publisher>
<location>New York, NY, USA.</location>
<marker>[Pfleger2004]</marker>
<rawString>N. Pfleger. 2004. Context based multimodal fusion. In ICMI ’04: Proceedings of the 6th international conference on Multimodal interfaces, pages 265– 272, New York, NY, USA. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David R Traum</author>
<author>Staffan Larsson</author>
</authors>
<title>The information state approach to dialog management.</title>
<date>2003</date>
<booktitle>In Current and New Directions in Discourse and Dialog.</booktitle>
<publisher>Kluwer.</publisher>
<marker>[Traum and Larsson2003]</marker>
<rawString>David R. Traum and Staffan Larsson. 2003. The information state approach to dialog management. In Current and New Directions in Discourse and Dialog. Kluwer.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>