<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000499">
<title confidence="0.991258">
cdec: A Decoder, Alignment, and Learning Framework for
Finite-State and Context-Free translation Models
</title>
<author confidence="0.994627">
Chris Dyer
</author>
<affiliation confidence="0.998112">
University of Maryland
</affiliation>
<email confidence="0.992221">
redpony@umd.edu
</email>
<author confidence="0.986261">
Jonathan Weese
</author>
<affiliation confidence="0.974203">
Johns Hopkins University
</affiliation>
<email confidence="0.996877">
jweese@cs.jhu.edu
</email>
<author confidence="0.994495">
Hendra Setiawan
</author>
<affiliation confidence="0.999158">
University of Maryland
</affiliation>
<email confidence="0.996571">
hendra@umiacs.umd.edu
</email>
<author confidence="0.992533">
Adam Lopez
</author>
<affiliation confidence="0.997168">
University of Edinburgh
</affiliation>
<email confidence="0.980384">
alopez@inf.ed.ac.uk
</email>
<author confidence="0.930959">
Ferhan ture
</author>
<affiliation confidence="0.992391">
University of Maryland
</affiliation>
<email confidence="0.996142">
fture@cs.umd.edu
</email>
<author confidence="0.995914">
Vladimir Eidelman
</author>
<affiliation confidence="0.999266">
University of Maryland
</affiliation>
<email confidence="0.996471">
vlad@umiacs.umd.edu
</email>
<author confidence="0.938769">
Juri Ganitkevitch
</author>
<affiliation confidence="0.928684">
Johns Hopkins University
</affiliation>
<email confidence="0.989902">
juri@cs.jhu.edu
</email>
<author confidence="0.978739">
Phil Blunsom
</author>
<affiliation confidence="0.97805">
Oxford University
</affiliation>
<email confidence="0.991015">
pblunsom@comlab.ox.ac.uk
</email>
<author confidence="0.998444">
Philip Resnik
</author>
<affiliation confidence="0.999379">
University of Maryland
</affiliation>
<email confidence="0.998485">
resnik@umiacs.umd.edu
</email>
<sectionHeader confidence="0.995643" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99993080952381">
We present cdec, an open source frame-
work for decoding, aligning with, and
training a number of statistical machine
translation models, including word-based
models, phrase-based models, and models
based on synchronous context-free gram-
mars. Using a single unified internal
representation for translation forests, the
decoder strictly separates model-specific
translation logic from general rescoring,
pruning, and inference algorithms. From
this unified representation, the decoder can
extract not only the 1- or k-best transla-
tions, but also alignments to a reference,
or the quantities necessary to drive dis-
criminative training using gradient-based
or gradient-free optimization techniques.
Its efficient C++ implementation means
that memory use and runtime performance
are significantly better than comparable
decoders.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999450717391304">
The dominant models used in machine transla-
tion and sequence tagging are formally based
on either weighted finite-state transducers (FSTs)
or weighted synchronous context-free grammars
(SCFGs) (Lopez, 2008). Phrase-based models
(Koehn et al., 2003), lexical translation models
(Brown et al., 1993), and finite-state conditional
random fields (Sha and Pereira, 2003) exemplify
the former, and hierarchical phrase-based models
the latter (Chiang, 2007). We introduce a soft-
ware package called cdec that manipulates both
classes in a unified way.1
Although open source decoders for both phrase-
based and hierarchical translation models have
been available for several years (Koehn et al.,
2007; Li et al., 2009), their extensibility to new
models and algorithms is limited by two sig-
nificant design flaws that we have avoided with
cdec. First, their implementations tightly couple
the translation, language model integration (which
we call rescoring), and pruning algorithms. This
makes it difficult to explore alternative transla-
tion models without also re-implementing rescor-
ing and pruning logic. In cdec, model-specific
code is only required to construct a translation for-
est (§3). General rescoring (with language models
or other models), pruning, inference, and align-
ment algorithms then apply to the unified data
structure (§4). Hence all model types benefit im-
mediately from new algorithms (for rescoring, in-
ference, etc.); new models can be more easily pro-
totyped; and controlled comparison of models is
made easier.
Second, existing open source decoders were de-
signed with the traditional phrase-based parame-
terization using a very small number of dense fea-
tures (typically less than 10). cdec has been de-
signed from the ground up to support any parame-
terization, from those with a handful of dense fea-
tures up to models with millions of sparse features
(Blunsom et al., 2008; Chiang et al., 2009). Since
the inference algorithms necessary to compute a
training objective (e.g. conditional likelihood or
expected BLEU) and its gradient operate on the
unified data structure (§5), any model type can be
trained using with any of the supported training
</bodyText>
<footnote confidence="0.998792">
1The software is released under the Apache License, ver-
sion 2.0, and is available from http://cdec-decoder.org/ .
</footnote>
<page confidence="0.984996">
7
</page>
<note confidence="0.6062865">
Proceedings of the ACL 2010 System Demonstrations, pages 7–12,
Uppsala, Sweden, 13 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.99965825">
criteria. The software package includes general
function optimization utilities that can be used for
discriminative training (§6).
These features are implemented without com-
promising on performance. We show experimen-
tally that cdec uses less memory and time than
comparable decoders on a controlled translation
task (§7).
</bodyText>
<sectionHeader confidence="0.977845" genericHeader="method">
2 Decoder workflow
</sectionHeader>
<bodyText confidence="0.999954657142857">
The decoding pipeline consists of two phases. The
first (Figure 1) transforms input, which may be
represented as a source language sentence, lattice
(Dyer et al., 2008), or context-free forest (Dyer
and Resnik, 2010), into a translation forest that has
been rescored with all applicable models.
In cdec, the only model-specific logic is con-
fined to the first step in the process where an
input string (or lattice, etc.) is transduced into
the unified hypergraph representation. Since the
model-specific code need not worry about integra-
tion with rescoring models, it can be made quite
simple and efficient. Furthermore, prior to lan-
guage model integration (and distortion model in-
tegration, in the case of phrase based translation),
pruning is unnecessary for most kinds of mod-
els, further simplifying the model-specific code.
Once this unscored translation forest has been
generated, any non-coaccessible states (i.e., states
that are not reachable from the goal node) are re-
moved and the resulting structure is rescored with
language models using a user-specified intersec-
tion/pruning strategy (§4) resulting in a rescored
translation forest and completing phase 1.
The second phase of the decoding pipeline (de-
picted in Figure 2) computes a value from the
rescored forest: 1- or k-best derivations, feature
expectations, or intersection with a target language
reference (sentence or lattice). The last option
generates an alignment forest, from which a word
alignment or feature expectations can be extracted.
Most of these values are computed in a time com-
plexity that is linear in the number of edges and
nodes in the translation hypergraph using cdec’s
semiring framework (§5).
</bodyText>
<subsectionHeader confidence="0.998426">
2.1 Alignment forests and alignment
</subsectionHeader>
<bodyText confidence="0.999876411764706">
Alignment is the process of determining if and
how a translation model generates a (source, tar-
get) string pair. To compute an alignment under
a translation model, the phase 1 translation hyper-
graph is reinterpreted as a synchronous context-
free grammar and then used to parse the target
sentence.2 This results in an alignment forest,
which is a compact representation of all the deriva-
tions of the sentence pair under the translation
model. From this forest, the Viterbi or maximum a
posteriori word alignment can be generated. This
alignment algorithm is explored in depth by Dyer
(2010). Note that if the phase 1 forest has been
pruned in some way, or the grammar does not de-
rive the sentence pair, the target intersection parse
may fail, meaning that an alignment will not be
recoverable.
</bodyText>
<sectionHeader confidence="0.974538" genericHeader="method">
3 Translation hypergraphs
</sectionHeader>
<bodyText confidence="0.962907789473684">
Recent research has proposed a unified repre-
sentation for the various translation and tagging
formalisms that is based on weighted logic pro-
gramming (Lopez, 2009). In this view, trans-
lation (or tagging) deductions have the structure
of a context-free forest, or directed hypergraph,
where edges have a single head and 0 or more tail
nodes (Nederhof, 2003). Once a forest has been
constructed representing the possible translations,
general inference algorithms can be applied.
In cdec’s translation hypergraph, a node rep-
resents a contiguous sequence of target language
words. For SCFG models and sequential tag-
ging models, a node also corresponds to a source
span and non-terminal type, but for word-based
and phrase-based models, the relationship to the
source string (or lattice) may be more compli-
cated. In a phrase-based translation hypergraph,
the node will correspond to a source coverage vec-
tor (Koehn et al., 2003). In word-based models, a
single node may derive multiple different source
language coverages since word based models im-
pose no requirements on covering all words in the
input. Figure 3 illustrates two example hyper-
graphs, one generated using a SCFG model and
other from a phrase-based model.
Edges are associated with exactly one syn-
chronous production in the source and target lan-
guage, and alternative translation possibilities are
expressed as alternative edges. Edges are further
annotated with feature values, and are annotated
with the source span vector the edge corresponds
to. An edge’s output label may contain mixtures
of terminal symbol yields and positions indicating
where a child node’s yield should be substituted.
2The parser is smart enough to detect the left-branching
grammars generated by lexical translation and tagging mod-
els, and use a more efficient intersection algorithm.
</bodyText>
<page confidence="0.982075">
8
</page>
<figure confidence="0.999828428571429">
FST rescoring
Output
Cube growing
Full intersection
Translation
hypergraph
No rescoring
Cube pruning
Input Transducers
Source CFG
Source
sentence
Source lattice
FST transducer
Tagger
Lexical transducer
Phrase-based
transducer
SCFG parser
Unscored
hypergraph
</figure>
<figureCaption confidence="0.999716">
Figure 1: Forest generation workflow (first half of decoding pipeline). The decoder’s configuration
</figureCaption>
<bodyText confidence="0.8131075">
specifies what path is taken from the input (one of the bold ovals) to a unified translation hypergraph.
The highlighted path is the workflow used in the test reported in §7.
</bodyText>
<subsubsectionHeader confidence="0.255032">
Translation outputs Alignment outputs
</subsubsectionHeader>
<figureCaption confidence="0.996093">
Figure 2: Output generation workflow (second half of decoding pipeline). Possible output types are
designated with a double box.
</figureCaption>
<figure confidence="0.989828526315789">
feature
expectations
max posterior
alignment
Viterbi alignment
Target
reference
Translation
hypergraph
Viterbi extraction
k-best extraction
max-translation
extraction
intersection by
parsing
feature
expectations
Alignment
hypergraph
</figure>
<bodyText confidence="0.999778">
In the case of SCFG grammars, the edges corre-
spond simply to rules in the synchronous gram-
mar. For non-SCFG translation models, there are
two kinds of edges. The first have zero tail nodes
(i.e., an arity of 0), and correspond to word or
phrase translation pairs (with all translation op-
tions existing on edges deriving the same head
node), or glue rules that glue phrases together.
For tagging, word-based, and phrase-based mod-
els, these are strictly arranged in a monotone, left-
branching structure.
</bodyText>
<sectionHeader confidence="0.964697" genericHeader="method">
4 Rescoring with weighted FSTs
</sectionHeader>
<bodyText confidence="0.999952285714286">
The design of cdec separates the creation of a
translation forest from its rescoring with a lan-
guage models or similar models.3 Since the struc-
ture of the unified search space is context free (§3),
we use the logic for language model rescoring de-
scribed by Chiang (2007), although any weighted
intersection algorithm can be applied. The rescor-
</bodyText>
<footnote confidence="0.622169666666667">
3Other rescoring models that depend on sequential con-
text include distance-based reordering models or Markov fea-
tures in tagging models.
</footnote>
<bodyText confidence="0.982230181818182">
ing models need not be explicitly represented as
FSTs—the state space can be inferred.
Although intersection using the Chiang algo-
rithm runs in polynomial time and space, the re-
sulting rescored forest may still be too large to rep-
resent completely. cdec therefore supports three
pruning strategies that can be used during intersec-
tion: full unpruned intersection (useful for tagging
models to incorporate, e.g., Markov features, but
not generally practical for translation), cube prun-
ing, and cube growing (Huang and Chiang, 2007).
</bodyText>
<sectionHeader confidence="0.99147" genericHeader="method">
5 Semiring framework
</sectionHeader>
<bodyText confidence="0.9998261">
Semirings are a useful mathematical abstraction
for dealing with translation forests since many
useful quantities can be computed using a single
linear-time algorithm but with different semirings.
A semiring is a 5-tuple (K, ®, ®, 0,1) that indi-
cates the set from which the values will be drawn,
K, a generic addition and multiplication operation,
® and ®, and their identities 0 and 1. Multipli-
cation and addition must be associative. Multi-
plication must distribute over addition, and v ® 0
</bodyText>
<page confidence="0.943663">
9
</page>
<figure confidence="0.9997639375">
JJ NN
Goal
a 1 2
a
010
100 101
1 small 1 house
1 shell
1 house
1 little
1 a
110
1 small
1 little
1 shell
Goal
</figure>
<figureCaption confidence="0.953706333333333">
Figure 3: Example unrescored translation hypergraphs generated for the German input ein (a) kleines
(small/little) Haus (house/shell) using a SCFG-based model (left) and phrase-based model with a distor-
tion limit of 1 (right).
</figureCaption>
<bodyText confidence="0.998659882352941">
must equal 0. Values that can be computed using
the semirings include the number of derivations,
the expected translation length, the entropy of the
translation posterior distribution, and the expected
values of feature functions (Li and Eisner, 2009).
Since semirings are such a useful abstraction,
cdec has been designed to facilitate implementa-
tion of new semirings. Table 1 shows the C++ rep-
resentation used for semirings. Note that because
of our representation, built-in types like double,
int, and bool (together with their default op-
erators) are semirings. Beyond these, the type
prob t is provided which stores the logarithm of
the value it represents, which helps avoid under-
flow and overflow problems that may otherwise
be encountered. A generic first-order expectation
semiring is also provided (Li and Eisner, 2009).
</bodyText>
<tableCaption confidence="0.735238">
Table 1: Semiring representation. T is a C++ type
name.
</tableCaption>
<equation confidence="0.984823166666667">
Element C++ representation
K T
T::operator+=
T::operator*=
T()
T(1)
</equation>
<bodyText confidence="0.9999625">
Three standard algorithms parameterized with
semirings are provided: INSIDE, OUTSIDE, and
INSIDEOUTSIDE, and the semiring is specified us-
ing C++ generics (templates). Additionally, each
algorithm takes a weight function that maps from
hypergraph edges to a value in K, making it possi-
ble to use many different semirings without alter-
ing the underlying hypergraph.
</bodyText>
<subsectionHeader confidence="0.978943">
5.1 Viterbi and k-best extraction
</subsectionHeader>
<bodyText confidence="0.9999645">
Although Viterbi and k-best extraction algorithms
are often expressed as INSIDE algorithms with
the tropical semiring, cdec provides a separate
derivation extraction framework that makes use of
a &lt; operator (Huang and Chiang, 2005). Thus,
many of the semiring types define not only the el-
ements shown in Table 1 but T::operator&lt; as
well. The k-best extraction algorithm is also pa-
rameterized by an optional predicate that can filter
out derivations at each node, enabling extraction
of only derivations that yield different strings as in
Huang et al. (2006).
</bodyText>
<sectionHeader confidence="0.958225" genericHeader="method">
6 Model training
</sectionHeader>
<bodyText confidence="0.999850333333333">
Two training pipelines are provided with cdec.
The first, called Viterbi envelope semiring train-
ing, VEST, implements the minimum error rate
training (MERT) algorithm, a gradient-free opti-
mization technique capable of maximizing arbi-
trary loss functions (Och, 2003).
</bodyText>
<subsectionHeader confidence="0.951278">
6.1 VEST
</subsectionHeader>
<bodyText confidence="0.999468684210526">
Rather than computing an error surface using k-
best approximations of the decoder search space,
cdec’s implementation performs inference over
the full hypergraph structure (Kumar et al., 2009).
In particular, by defining a semiring whose values
are sets of line segments, having an addition op-
eration equivalent to union, and a multiplication
operation equivalent to a linear transformation of
the line segments, Och’s line search can be com-
puted simply using the INSIDE algorithm. Since
the translation hypergraphs generated by cdec
may be quite large making inference expensive,
the logic for constructing error surfaces is fac-
tored according to the MapReduce programming
paradigm (Dean and Ghemawat, 2004), enabling
parallelization across a cluster of machines. Im-
plementations of the BLEU and TER loss functions
are provided (Papineni et al., 2002; Snover et al.,
2006).
</bodyText>
<figure confidence="0.87874475">
ED
�
0
1
</figure>
<page confidence="0.929328">
10
</page>
<subsectionHeader confidence="0.99882">
6.2 Large-scale discriminative training
</subsectionHeader>
<bodyText confidence="0.999974934782608">
In addition to the widely used MERT algo-
rithm, cdec also provides a training pipeline for
discriminatively trained probabilistic translation
models (Blunsom et al., 2008; Blunsom and Os-
borne, 2008). In these models, the translation
model is trained to maximize conditional log like-
lihood of the training data under a specified gram-
mar. Since log likelihood is differentiable with
respect to the feature weights in an exponential
model, it is possible to use gradient-based opti-
mization techniques to train the system, enabling
the parameterization of the model using millions
of sparse features. While this training approach
was originally proposed for SCFG-based transla-
tion models, it can be used to train any model
type in cdec. When used with sequential tagging
models, this pipeline is identical to traditional se-
quential CRF training (Sha and Pereira, 2003).
Both the objective (conditional log likelihood)
and its gradient have the form of a difference in
two quantities: each has one term that is com-
puted over the translation hypergraph which is
subtracted from the result of the same computa-
tion over the alignment hypergraph (refer to Fig-
ures 1 and 2). The conditional log likelihood is
the difference in the log partition of the translation
and alignment hypergraph, and is computed using
the INSIDE algorithm. The gradient with respect
to a particular feature is the difference in this fea-
ture’s expected value in the translation and align-
ment hypergraphs, and can be computed using ei-
ther INSIDEOUTSIDE or the expectation semiring
and INSIDE. Since a translation forest is generated
as an intermediate step in generating an alignment
forest (§2) this computation is straightforward.
Since gradient-based optimization techniques
may require thousands of evaluations to converge,
the batch training pipeline is split into map and
reduce components, facilitating distribution over
very large clusters. Briefly, the cdec is run as the
map function, and sentence pairs are mapped over.
The reduce function aggregates the results and per-
forms the optimization using standard algorithms,
including LBFGS (Liu et al., 1989), RPROP (Ried-
miller and Braun, 1993), and stochastic gradient
descent.
</bodyText>
<sectionHeader confidence="0.998488" genericHeader="conclusions">
7 Experiments
</sectionHeader>
<bodyText confidence="0.999359272727273">
Table 2 compares the performance of cdec, Hi-
ero, and Joshua 1.3 (running with 1 or 8 threads)
decoding using a hierarchical phrase-based trans-
lation grammar and identical pruning settings.4
Figure 4 shows the cdec configuration and
weights file used for this test.
The workstation used has two 2GHz quad-core
Intel Xenon processors, 32GB RAM, is running
Linux kernel version 2.6.18 and gcc version 4.1.2.
All decoders use SRI’s language model toolkit,
version 1.5.9 (Stolcke, 2002). Joshua was run on
the Sun HotSpot JVM, version 1.6.0 12. A hierar-
chical phrase-based translation grammar was ex-
tracted for the NIST MT03 Chinese-English trans-
lation using a suffix array rule extractor (Lopez,
2007). A non-terminal span limit of 15 was used,
and all decoders were configured to use cube prun-
ing with a limit of 30 candidates at each node and
no further pruning. All decoders produced a BLEU
score between 31.4 and 31.6 (small differences are
accounted for by different tie-breaking behavior
and OOV handling).
</bodyText>
<tableCaption confidence="0.59338">
Table 2: Memory usage and average per-sentence
running time, in seconds, for decoding a Chinese-
English test set.
</tableCaption>
<table confidence="0.999336">
Decoder Lang. Time (s) Memory
cdec C++ 0.37 1.0Gb
Joshua (1x) Java 0.98 1.5Gb
Joshua (8x) Java 0.35 2.5Gb
Hiero Python 4.04 1.1Gb
</table>
<bodyText confidence="0.855222111111111">
formalism=scfg
grammar=grammar.mt03.scfg.gz
add pass through rules=true
scfg max span limit=15
feature function=LanguageModel
en.3gram.pruned.lm.gz -o 3
feature function=WordPenalty
intersection strategy=cube pruning
cubepruning pop limit=30
</bodyText>
<figureCaption confidence="0.966122333333333">
Figure 4: Configuration file (above) and feature
weights file (below) used for the decoding test de-
scribed in §7.
</figureCaption>
<footnote confidence="0.811175">
4http://sourceforge.net/projects/joshua/
</footnote>
<table confidence="0.793897666666667">
LanguageModel 1.12
WordPenalty -4.26
PhraseModel 0 0.963
PhraseModel 1 0.654
PhraseModel 2 0.773
PassThroughRule -20
</table>
<page confidence="0.957039">
11
</page>
<bodyText confidence="0.9406005">
8 Future work C. Dyer. 2010. Two monolingual parses are better than one
(synchronous parse). In Proc. of HLT-NAACL.
cdec continues to be under active development.
We are taking advantage of its modular design to
study alternative algorithms for language model
integration. Further training pipelines are un-
der development, including minimum risk train-
ing using a linearly decomposable approximation
of BLEU (Li and Eisner, 2009), and MIRA train-
ing (Chiang et al., 2009). All of these will be
made publicly available as the projects progress.
We are also improving support for parallel training
using Hadoop (an open-source implementation of
MapReduce).
</bodyText>
<sectionHeader confidence="0.996919" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999870583333333">
This work was partially supported by the GALE
program of the Defense Advanced Research
Projects Agency, Contract No. HR0011-06-2-001.
Any opinions, findings, conclusions or recommen-
dations expressed in this paper are those of the au-
thors and do not necessarily reflect the views of the
sponsors. Further support was provided the Euro-
Matrix project funded by the European Commis-
sion (7th Framework Programme). Discussions
with Philipp Koehn, Chris Callison-Burch, Zhifei
Li, Lane Schwarz, and Jimmy Lin were likewise
crucial to the successful execution of this project.
</bodyText>
<sectionHeader confidence="0.998549" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99988102631579">
P. Blunsom and M. Osborne. 2008. Probalistic inference for
machine translation. In Proc. of EMNLP.
P. Blunsom, T. Cohn, and M. Osborne. 2008. A discrimina-
tive latent variable model for statistical machine transla-
tion. In Proc. of ACL-HLT.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and R. L.
Mercer. 1993. The mathematics of statistical machine
translation: parameter estimation. Computational Lin-
guistics, 19(2):263–311.
D. Chiang, K. Knight, and W. Wang. 2009. 11,001 new
features for statistical machine translation. In Proc. of
NAACL, pages 218–226.
D. Chiang. 2007. Hierarchical phrase-based translation.
Comp. Ling., 33(2):201–228.
J. Dean and S. Ghemawat. 2004. MapReduce: Simplified
data processing on large clusters. In Proc. of the 6th Sym-
posium on Operating System Design and Implementation
(OSDI2004), pages 137–150.
C. Dyer and P. Resnik. 2010. Context-free reordering, finite-
state translation. In Proc. of HLT-NAACL.
C. Dyer, S. Muresan, and P. Resnik. 2008. Generalizing
word lattice translation. In Proc. of HLT-ACL.
L. Huang and D. Chiang. 2005. Better k-best parsing. In In
Proc. of IWPT, pages 53–64.
L. Huang and D. Chiang. 2007. Forest rescoring: Faster
decoding with integrated language models. In Proc. ACL.
L. Huang, K. Knight, and A. Joshi. 2006. A syntax-directed
translator with extended domain of locality. In Proc. of
AMTA.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrase-
based translation. In Proc. of HLT/NAACL, pages 48–54.
P. Koehn, H. Hoang, A. B. Mayne, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran,
R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst.
2007. Moses: Open source toolkit for statistical ma-
chine translation. In Proc. of ACL, Demonstration Ses-
sion, pages 177–180, June.
S. Kumar, W. Macherey, C. Dyer, and F. Och. 2009. Efficient
minimum error rate training and minimum B ayes-risk de-
coding for translation hypergraphs and lattices. In Proc.
of ACL, pages 163–171.
Z. Li and J. Eisner. 2009. First- and second-order expectation
semirings with applications to minimum-risk training on
translation forests. In Proc. of EMNLP, pages 40–51.
Z. Li, C. Callison-Burch, C. Dyer, J. Ganitkevitch, S. Khu-
danpur, L. Schwartz, W. N. G. Thornton, J. Weese, and
O. F. Zaidan. 2009. Joshua: an open source toolkit for
parsing-based machine translation. In Proc. of the Fourth
Workshop on Stat. Machine Translation, pages 135–139.
D. C. Liu, J. Nocedal, D. C. Liu, and J. Nocedal. 1989. On
the limited memory BFGS method for large scale opti-
mization. Mathematical Programming B, 45(3):503–528.
A. Lopez. 2007. Hierarchical phrase-based translation with
suffix arrays. In Proc. of EMNLP, pages 976–985.
A. Lopez. 2008. Statistical machine translation. ACM Com-
puting Surveys, 40(3), Aug.
A. Lopez. 2009. Translation as weighted deduction. In Proc.
of EACL, pages 532–540.
M.-J. Nederhof. 2003. Weighted deductive parsing and
Knuth’s algorithm. Comp. Ling., 29(1):135–143, Mar.
F. Och. 2003. Minimum error rate training in statistical ma-
chine translation. In Proc. of ACL, pages 160–167.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL, pages 311–318.
M. Riedmiller and H. Braun. 1993. A direct adaptive method
for faster backpropagation learning: The RPROP algo-
rithm. In Proc. of the IEEE international conference on
neural networks, pages 586–591.
F. Sha and F. Pereira. 2003. Shallow parsing with conditional
random fields. In Proc. of NAACL, pages 134–141.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate with
targeted human annotation. In Proc. AMTA.
A. Stolcke. 2002. SRILM – an extensible language modeling
toolkit. In Intl. Conf. on Spoken Language Processing.
</reference>
<page confidence="0.998462">
12
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.725307">
<title confidence="0.9996905">cdec: A Decoder, Alignment, and Learning Framework for Finite-State and Context-Free translation Models</title>
<author confidence="0.999813">Chris Dyer</author>
<affiliation confidence="0.999971">University of Maryland</affiliation>
<email confidence="0.999574">redpony@umd.edu</email>
<author confidence="0.999975">Jonathan Weese</author>
<affiliation confidence="0.998804">Johns Hopkins University</affiliation>
<email confidence="0.998675">jweese@cs.jhu.edu</email>
<author confidence="0.955158">Hendra Setiawan</author>
<affiliation confidence="0.999879">University of Maryland</affiliation>
<email confidence="0.999445">hendra@umiacs.umd.edu</email>
<author confidence="0.995568">Adam Lopez</author>
<affiliation confidence="0.999909">University of Edinburgh</affiliation>
<email confidence="0.990261">alopez@inf.ed.ac.uk</email>
<author confidence="0.911715">Ferhan ture</author>
<affiliation confidence="0.999802">University of Maryland</affiliation>
<email confidence="0.999759">fture@cs.umd.edu</email>
<author confidence="0.989609">Vladimir Eidelman</author>
<affiliation confidence="0.999893">University of Maryland</affiliation>
<email confidence="0.999049">vlad@umiacs.umd.edu</email>
<author confidence="0.945078">Juri Ganitkevitch</author>
<affiliation confidence="0.998565">Johns Hopkins University</affiliation>
<email confidence="0.998901">juri@cs.jhu.edu</email>
<author confidence="0.996464">Phil Blunsom</author>
<affiliation confidence="0.999994">Oxford University</affiliation>
<email confidence="0.980688">pblunsom@comlab.ox.ac.uk</email>
<author confidence="0.999931">Philip Resnik</author>
<affiliation confidence="0.999968">University of Maryland</affiliation>
<email confidence="0.999798">resnik@umiacs.umd.edu</email>
<abstract confidence="0.996900863636364">present an open source framework for decoding, aligning with, and training a number of statistical machine translation models, including word-based models, phrase-based models, and models based on synchronous context-free grammars. Using a single unified internal representation for translation forests, the decoder strictly separates model-specific translation logic from general rescoring, pruning, and inference algorithms. From this unified representation, the decoder can not only the 1or translations, but also alignments to a reference, or the quantities necessary to drive discriminative training using gradient-based or gradient-free optimization techniques. Its efficient C++ implementation means that memory use and runtime performance are significantly better than comparable decoders.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P Blunsom</author>
<author>M Osborne</author>
</authors>
<title>Probalistic inference for machine translation.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="15240" citStr="Blunsom and Osborne, 2008" startWordPosition="2312" endWordPosition="2316">ation hypergraphs generated by cdec may be quite large making inference expensive, the logic for constructing error surfaces is factored according to the MapReduce programming paradigm (Dean and Ghemawat, 2004), enabling parallelization across a cluster of machines. Implementations of the BLEU and TER loss functions are provided (Papineni et al., 2002; Snover et al., 2006). ED � 0 1 10 6.2 Large-scale discriminative training In addition to the widely used MERT algorithm, cdec also provides a training pipeline for discriminatively trained probabilistic translation models (Blunsom et al., 2008; Blunsom and Osborne, 2008). In these models, the translation model is trained to maximize conditional log likelihood of the training data under a specified grammar. Since log likelihood is differentiable with respect to the feature weights in an exponential model, it is possible to use gradient-based optimization techniques to train the system, enabling the parameterization of the model using millions of sparse features. While this training approach was originally proposed for SCFG-based translation models, it can be used to train any model type in cdec. When used with sequential tagging models, this pipeline is identi</context>
</contexts>
<marker>Blunsom, Osborne, 2008</marker>
<rawString>P. Blunsom and M. Osborne. 2008. Probalistic inference for machine translation. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Blunsom</author>
<author>T Cohn</author>
<author>M Osborne</author>
</authors>
<title>A discriminative latent variable model for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proc. of ACL-HLT.</booktitle>
<contexts>
<context position="3344" citStr="Blunsom et al., 2008" startWordPosition="464" endWordPosition="467">nd alignment algorithms then apply to the unified data structure (§4). Hence all model types benefit immediately from new algorithms (for rescoring, inference, etc.); new models can be more easily prototyped; and controlled comparison of models is made easier. Second, existing open source decoders were designed with the traditional phrase-based parameterization using a very small number of dense features (typically less than 10). cdec has been designed from the ground up to support any parameterization, from those with a handful of dense features up to models with millions of sparse features (Blunsom et al., 2008; Chiang et al., 2009). Since the inference algorithms necessary to compute a training objective (e.g. conditional likelihood or expected BLEU) and its gradient operate on the unified data structure (§5), any model type can be trained using with any of the supported training 1The software is released under the Apache License, version 2.0, and is available from http://cdec-decoder.org/ . 7 Proceedings of the ACL 2010 System Demonstrations, pages 7–12, Uppsala, Sweden, 13 July 2010. c�2010 Association for Computational Linguistics criteria. The software package includes general function optimiza</context>
<context position="15212" citStr="Blunsom et al., 2008" startWordPosition="2308" endWordPosition="2311">ithm. Since the translation hypergraphs generated by cdec may be quite large making inference expensive, the logic for constructing error surfaces is factored according to the MapReduce programming paradigm (Dean and Ghemawat, 2004), enabling parallelization across a cluster of machines. Implementations of the BLEU and TER loss functions are provided (Papineni et al., 2002; Snover et al., 2006). ED � 0 1 10 6.2 Large-scale discriminative training In addition to the widely used MERT algorithm, cdec also provides a training pipeline for discriminatively trained probabilistic translation models (Blunsom et al., 2008; Blunsom and Osborne, 2008). In these models, the translation model is trained to maximize conditional log likelihood of the training data under a specified grammar. Since log likelihood is differentiable with respect to the feature weights in an exponential model, it is possible to use gradient-based optimization techniques to train the system, enabling the parameterization of the model using millions of sparse features. While this training approach was originally proposed for SCFG-based translation models, it can be used to train any model type in cdec. When used with sequential tagging mod</context>
</contexts>
<marker>Blunsom, Cohn, Osborne, 2008</marker>
<rawString>P. Blunsom, T. Cohn, and M. Osborne. 2008. A discriminative latent variable model for statistical machine translation. In Proc. of ACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>V J Della Pietra</author>
<author>S A Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1760" citStr="Brown et al., 1993" startWordPosition="217" endWordPosition="220">st translations, but also alignments to a reference, or the quantities necessary to drive discriminative training using gradient-based or gradient-free optimization techniques. Its efficient C++ implementation means that memory use and runtime performance are significantly better than comparable decoders. 1 Introduction The dominant models used in machine translation and sequence tagging are formally based on either weighted finite-state transducers (FSTs) or weighted synchronous context-free grammars (SCFGs) (Lopez, 2008). Phrase-based models (Koehn et al., 2003), lexical translation models (Brown et al., 1993), and finite-state conditional random fields (Sha and Pereira, 2003) exemplify the former, and hierarchical phrase-based models the latter (Chiang, 2007). We introduce a software package called cdec that manipulates both classes in a unified way.1 Although open source decoders for both phrasebased and hierarchical translation models have been available for several years (Koehn et al., 2007; Li et al., 2009), their extensibility to new models and algorithms is limited by two significant design flaws that we have avoided with cdec. First, their implementations tightly couple the translation, lan</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
<author>K Knight</author>
<author>W Wang</author>
</authors>
<title>11,001 new features for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proc. of NAACL,</booktitle>
<pages>218--226</pages>
<contexts>
<context position="3366" citStr="Chiang et al., 2009" startWordPosition="468" endWordPosition="471">s then apply to the unified data structure (§4). Hence all model types benefit immediately from new algorithms (for rescoring, inference, etc.); new models can be more easily prototyped; and controlled comparison of models is made easier. Second, existing open source decoders were designed with the traditional phrase-based parameterization using a very small number of dense features (typically less than 10). cdec has been designed from the ground up to support any parameterization, from those with a handful of dense features up to models with millions of sparse features (Blunsom et al., 2008; Chiang et al., 2009). Since the inference algorithms necessary to compute a training objective (e.g. conditional likelihood or expected BLEU) and its gradient operate on the unified data structure (§5), any model type can be trained using with any of the supported training 1The software is released under the Apache License, version 2.0, and is available from http://cdec-decoder.org/ . 7 Proceedings of the ACL 2010 System Demonstrations, pages 7–12, Uppsala, Sweden, 13 July 2010. c�2010 Association for Computational Linguistics criteria. The software package includes general function optimization utilities that ca</context>
<context position="19495" citStr="Chiang et al., 2009" startWordPosition="2964" endWordPosition="2967">urceforge.net/projects/joshua/ LanguageModel 1.12 WordPenalty -4.26 PhraseModel 0 0.963 PhraseModel 1 0.654 PhraseModel 2 0.773 PassThroughRule -20 11 8 Future work C. Dyer. 2010. Two monolingual parses are better than one (synchronous parse). In Proc. of HLT-NAACL. cdec continues to be under active development. We are taking advantage of its modular design to study alternative algorithms for language model integration. Further training pipelines are under development, including minimum risk training using a linearly decomposable approximation of BLEU (Li and Eisner, 2009), and MIRA training (Chiang et al., 2009). All of these will be made publicly available as the projects progress. We are also improving support for parallel training using Hadoop (an open-source implementation of MapReduce). Acknowledgements This work was partially supported by the GALE program of the Defense Advanced Research Projects Agency, Contract No. HR0011-06-2-001. Any opinions, findings, conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views of the sponsors. Further support was provided the EuroMatrix project funded by the European Commission (7th Framework Pr</context>
</contexts>
<marker>Chiang, Knight, Wang, 2009</marker>
<rawString>D. Chiang, K. Knight, and W. Wang. 2009. 11,001 new features for statistical machine translation. In Proc. of NAACL, pages 218–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Comp. Ling.,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="1913" citStr="Chiang, 2007" startWordPosition="239" endWordPosition="240">ation techniques. Its efficient C++ implementation means that memory use and runtime performance are significantly better than comparable decoders. 1 Introduction The dominant models used in machine translation and sequence tagging are formally based on either weighted finite-state transducers (FSTs) or weighted synchronous context-free grammars (SCFGs) (Lopez, 2008). Phrase-based models (Koehn et al., 2003), lexical translation models (Brown et al., 1993), and finite-state conditional random fields (Sha and Pereira, 2003) exemplify the former, and hierarchical phrase-based models the latter (Chiang, 2007). We introduce a software package called cdec that manipulates both classes in a unified way.1 Although open source decoders for both phrasebased and hierarchical translation models have been available for several years (Koehn et al., 2007; Li et al., 2009), their extensibility to new models and algorithms is limited by two significant design flaws that we have avoided with cdec. First, their implementations tightly couple the translation, language model integration (which we call rescoring), and pruning algorithms. This makes it difficult to explore alternative translation models without also</context>
<context position="10319" citStr="Chiang (2007)" startWordPosition="1544" endWordPosition="1545">(i.e., an arity of 0), and correspond to word or phrase translation pairs (with all translation options existing on edges deriving the same head node), or glue rules that glue phrases together. For tagging, word-based, and phrase-based models, these are strictly arranged in a monotone, leftbranching structure. 4 Rescoring with weighted FSTs The design of cdec separates the creation of a translation forest from its rescoring with a language models or similar models.3 Since the structure of the unified search space is context free (§3), we use the logic for language model rescoring described by Chiang (2007), although any weighted intersection algorithm can be applied. The rescor3Other rescoring models that depend on sequential context include distance-based reordering models or Markov features in tagging models. ing models need not be explicitly represented as FSTs—the state space can be inferred. Although intersection using the Chiang algorithm runs in polynomial time and space, the resulting rescored forest may still be too large to represent completely. cdec therefore supports three pruning strategies that can be used during intersection: full unpruned intersection (useful for tagging models </context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>D. Chiang. 2007. Hierarchical phrase-based translation. Comp. Ling., 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Dean</author>
<author>S Ghemawat</author>
</authors>
<title>MapReduce: Simplified data processing on large clusters.</title>
<date>2004</date>
<booktitle>In Proc. of the 6th Symposium on Operating System Design and Implementation (OSDI2004),</booktitle>
<pages>137--150</pages>
<contexts>
<context position="14824" citStr="Dean and Ghemawat, 2004" startWordPosition="2248" endWordPosition="2251">arch space, cdec’s implementation performs inference over the full hypergraph structure (Kumar et al., 2009). In particular, by defining a semiring whose values are sets of line segments, having an addition operation equivalent to union, and a multiplication operation equivalent to a linear transformation of the line segments, Och’s line search can be computed simply using the INSIDE algorithm. Since the translation hypergraphs generated by cdec may be quite large making inference expensive, the logic for constructing error surfaces is factored according to the MapReduce programming paradigm (Dean and Ghemawat, 2004), enabling parallelization across a cluster of machines. Implementations of the BLEU and TER loss functions are provided (Papineni et al., 2002; Snover et al., 2006). ED � 0 1 10 6.2 Large-scale discriminative training In addition to the widely used MERT algorithm, cdec also provides a training pipeline for discriminatively trained probabilistic translation models (Blunsom et al., 2008; Blunsom and Osborne, 2008). In these models, the translation model is trained to maximize conditional log likelihood of the training data under a specified grammar. Since log likelihood is differentiable with r</context>
</contexts>
<marker>Dean, Ghemawat, 2004</marker>
<rawString>J. Dean and S. Ghemawat. 2004. MapReduce: Simplified data processing on large clusters. In Proc. of the 6th Symposium on Operating System Design and Implementation (OSDI2004), pages 137–150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Dyer</author>
<author>P Resnik</author>
</authors>
<title>Context-free reordering, finitestate translation.</title>
<date>2010</date>
<booktitle>In Proc. of HLT-NAACL.</booktitle>
<contexts>
<context position="4436" citStr="Dyer and Resnik, 2010" startWordPosition="627" endWordPosition="630">, 13 July 2010. c�2010 Association for Computational Linguistics criteria. The software package includes general function optimization utilities that can be used for discriminative training (§6). These features are implemented without compromising on performance. We show experimentally that cdec uses less memory and time than comparable decoders on a controlled translation task (§7). 2 Decoder workflow The decoding pipeline consists of two phases. The first (Figure 1) transforms input, which may be represented as a source language sentence, lattice (Dyer et al., 2008), or context-free forest (Dyer and Resnik, 2010), into a translation forest that has been rescored with all applicable models. In cdec, the only model-specific logic is confined to the first step in the process where an input string (or lattice, etc.) is transduced into the unified hypergraph representation. Since the model-specific code need not worry about integration with rescoring models, it can be made quite simple and efficient. Furthermore, prior to language model integration (and distortion model integration, in the case of phrase based translation), pruning is unnecessary for most kinds of models, further simplifying the model-spec</context>
</contexts>
<marker>Dyer, Resnik, 2010</marker>
<rawString>C. Dyer and P. Resnik. 2010. Context-free reordering, finitestate translation. In Proc. of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Dyer</author>
<author>S Muresan</author>
<author>P Resnik</author>
</authors>
<title>Generalizing word lattice translation.</title>
<date>2008</date>
<booktitle>In Proc. of</booktitle>
<pages>53--64</pages>
<contexts>
<context position="4388" citStr="Dyer et al., 2008" startWordPosition="620" endWordPosition="623"> Demonstrations, pages 7–12, Uppsala, Sweden, 13 July 2010. c�2010 Association for Computational Linguistics criteria. The software package includes general function optimization utilities that can be used for discriminative training (§6). These features are implemented without compromising on performance. We show experimentally that cdec uses less memory and time than comparable decoders on a controlled translation task (§7). 2 Decoder workflow The decoding pipeline consists of two phases. The first (Figure 1) transforms input, which may be represented as a source language sentence, lattice (Dyer et al., 2008), or context-free forest (Dyer and Resnik, 2010), into a translation forest that has been rescored with all applicable models. In cdec, the only model-specific logic is confined to the first step in the process where an input string (or lattice, etc.) is transduced into the unified hypergraph representation. Since the model-specific code need not worry about integration with rescoring models, it can be made quite simple and efficient. Furthermore, prior to language model integration (and distortion model integration, in the case of phrase based translation), pruning is unnecessary for most kin</context>
</contexts>
<marker>Dyer, Muresan, Resnik, 2008</marker>
<rawString>C. Dyer, S. Muresan, and P. Resnik. 2008. Generalizing word lattice translation. In Proc. of HLT-ACL. L. Huang and D. Chiang. 2005. Better k-best parsing. In In Proc. of IWPT, pages 53–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Huang</author>
<author>D Chiang</author>
</authors>
<title>Forest rescoring: Faster decoding with integrated language models.</title>
<date>2007</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="11059" citStr="Huang and Chiang, 2007" startWordPosition="1654" endWordPosition="1657">ntial context include distance-based reordering models or Markov features in tagging models. ing models need not be explicitly represented as FSTs—the state space can be inferred. Although intersection using the Chiang algorithm runs in polynomial time and space, the resulting rescored forest may still be too large to represent completely. cdec therefore supports three pruning strategies that can be used during intersection: full unpruned intersection (useful for tagging models to incorporate, e.g., Markov features, but not generally practical for translation), cube pruning, and cube growing (Huang and Chiang, 2007). 5 Semiring framework Semirings are a useful mathematical abstraction for dealing with translation forests since many useful quantities can be computed using a single linear-time algorithm but with different semirings. A semiring is a 5-tuple (K, ®, ®, 0,1) that indicates the set from which the values will be drawn, K, a generic addition and multiplication operation, ® and ®, and their identities 0 and 1. Multiplication and addition must be associative. Multiplication must distribute over addition, and v ® 0 9 JJ NN Goal a 1 2 a 010 100 101 1 small 1 house 1 shell 1 house 1 little 1 a 110 1 s</context>
</contexts>
<marker>Huang, Chiang, 2007</marker>
<rawString>L. Huang and D. Chiang. 2007. Forest rescoring: Faster decoding with integrated language models. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Huang</author>
<author>K Knight</author>
<author>A Joshi</author>
</authors>
<title>A syntax-directed translator with extended domain of locality.</title>
<date>2006</date>
<booktitle>In Proc. of AMTA.</booktitle>
<contexts>
<context position="13822" citStr="Huang et al. (2006)" startWordPosition="2098" endWordPosition="2101"> the underlying hypergraph. 5.1 Viterbi and k-best extraction Although Viterbi and k-best extraction algorithms are often expressed as INSIDE algorithms with the tropical semiring, cdec provides a separate derivation extraction framework that makes use of a &lt; operator (Huang and Chiang, 2005). Thus, many of the semiring types define not only the elements shown in Table 1 but T::operator&lt; as well. The k-best extraction algorithm is also parameterized by an optional predicate that can filter out derivations at each node, enabling extraction of only derivations that yield different strings as in Huang et al. (2006). 6 Model training Two training pipelines are provided with cdec. The first, called Viterbi envelope semiring training, VEST, implements the minimum error rate training (MERT) algorithm, a gradient-free optimization technique capable of maximizing arbitrary loss functions (Och, 2003). 6.1 VEST Rather than computing an error surface using kbest approximations of the decoder search space, cdec’s implementation performs inference over the full hypergraph structure (Kumar et al., 2009). In particular, by defining a semiring whose values are sets of line segments, having an addition operation equiv</context>
</contexts>
<marker>Huang, Knight, Joshi, 2006</marker>
<rawString>L. Huang, K. Knight, and A. Joshi. 2006. A syntax-directed translator with extended domain of locality. In Proc. of AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrasebased translation.</title>
<date>2003</date>
<booktitle>In Proc. of HLT/NAACL,</booktitle>
<pages>48--54</pages>
<contexts>
<context position="1711" citStr="Koehn et al., 2003" startWordPosition="210" endWordPosition="213">, the decoder can extract not only the 1- or k-best translations, but also alignments to a reference, or the quantities necessary to drive discriminative training using gradient-based or gradient-free optimization techniques. Its efficient C++ implementation means that memory use and runtime performance are significantly better than comparable decoders. 1 Introduction The dominant models used in machine translation and sequence tagging are formally based on either weighted finite-state transducers (FSTs) or weighted synchronous context-free grammars (SCFGs) (Lopez, 2008). Phrase-based models (Koehn et al., 2003), lexical translation models (Brown et al., 1993), and finite-state conditional random fields (Sha and Pereira, 2003) exemplify the former, and hierarchical phrase-based models the latter (Chiang, 2007). We introduce a software package called cdec that manipulates both classes in a unified way.1 Although open source decoders for both phrasebased and hierarchical translation models have been available for several years (Koehn et al., 2007; Li et al., 2009), their extensibility to new models and algorithms is limited by two significant design flaws that we have avoided with cdec. First, their im</context>
<context position="7682" citStr="Koehn et al., 2003" startWordPosition="1143" endWordPosition="1146">ngle head and 0 or more tail nodes (Nederhof, 2003). Once a forest has been constructed representing the possible translations, general inference algorithms can be applied. In cdec’s translation hypergraph, a node represents a contiguous sequence of target language words. For SCFG models and sequential tagging models, a node also corresponds to a source span and non-terminal type, but for word-based and phrase-based models, the relationship to the source string (or lattice) may be more complicated. In a phrase-based translation hypergraph, the node will correspond to a source coverage vector (Koehn et al., 2003). In word-based models, a single node may derive multiple different source language coverages since word based models impose no requirements on covering all words in the input. Figure 3 illustrates two example hypergraphs, one generated using a SCFG model and other from a phrase-based model. Edges are associated with exactly one synchronous production in the source and target language, and alternative translation possibilities are expressed as alternative edges. Edges are further annotated with feature values, and are annotated with the source span vector the edge corresponds to. An edge’s out</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrasebased translation. In Proc. of HLT/NAACL, pages 48–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A B Mayne</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
<author>C Dyer</author>
<author>O Bojar</author>
<author>A Constantin</author>
<author>E Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of ACL, Demonstration Session,</booktitle>
<pages>177--180</pages>
<contexts>
<context position="2152" citStr="Koehn et al., 2007" startWordPosition="275" endWordPosition="278">e formally based on either weighted finite-state transducers (FSTs) or weighted synchronous context-free grammars (SCFGs) (Lopez, 2008). Phrase-based models (Koehn et al., 2003), lexical translation models (Brown et al., 1993), and finite-state conditional random fields (Sha and Pereira, 2003) exemplify the former, and hierarchical phrase-based models the latter (Chiang, 2007). We introduce a software package called cdec that manipulates both classes in a unified way.1 Although open source decoders for both phrasebased and hierarchical translation models have been available for several years (Koehn et al., 2007; Li et al., 2009), their extensibility to new models and algorithms is limited by two significant design flaws that we have avoided with cdec. First, their implementations tightly couple the translation, language model integration (which we call rescoring), and pruning algorithms. This makes it difficult to explore alternative translation models without also re-implementing rescoring and pruning logic. In cdec, model-specific code is only required to construct a translation forest (§3). General rescoring (with language models or other models), pruning, inference, and alignment algorithms then</context>
</contexts>
<marker>Koehn, Hoang, Mayne, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>P. Koehn, H. Hoang, A. B. Mayne, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proc. of ACL, Demonstration Session, pages 177–180, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kumar</author>
<author>W Macherey</author>
<author>C Dyer</author>
<author>F Och</author>
</authors>
<title>Efficient minimum error rate training and minimum B ayes-risk decoding for translation hypergraphs and lattices.</title>
<date>2009</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>163--171</pages>
<contexts>
<context position="14308" citStr="Kumar et al., 2009" startWordPosition="2169" endWordPosition="2172">n filter out derivations at each node, enabling extraction of only derivations that yield different strings as in Huang et al. (2006). 6 Model training Two training pipelines are provided with cdec. The first, called Viterbi envelope semiring training, VEST, implements the minimum error rate training (MERT) algorithm, a gradient-free optimization technique capable of maximizing arbitrary loss functions (Och, 2003). 6.1 VEST Rather than computing an error surface using kbest approximations of the decoder search space, cdec’s implementation performs inference over the full hypergraph structure (Kumar et al., 2009). In particular, by defining a semiring whose values are sets of line segments, having an addition operation equivalent to union, and a multiplication operation equivalent to a linear transformation of the line segments, Och’s line search can be computed simply using the INSIDE algorithm. Since the translation hypergraphs generated by cdec may be quite large making inference expensive, the logic for constructing error surfaces is factored according to the MapReduce programming paradigm (Dean and Ghemawat, 2004), enabling parallelization across a cluster of machines. Implementations of the BLEU</context>
</contexts>
<marker>Kumar, Macherey, Dyer, Och, 2009</marker>
<rawString>S. Kumar, W. Macherey, C. Dyer, and F. Och. 2009. Efficient minimum error rate training and minimum B ayes-risk decoding for translation hypergraphs and lattices. In Proc. of ACL, pages 163–171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Li</author>
<author>J Eisner</author>
</authors>
<title>First- and second-order expectation semirings with applications to minimum-risk training on translation forests.</title>
<date>2009</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>40--51</pages>
<contexts>
<context position="12164" citStr="Li and Eisner, 2009" startWordPosition="1840" endWordPosition="1843">bute over addition, and v ® 0 9 JJ NN Goal a 1 2 a 010 100 101 1 small 1 house 1 shell 1 house 1 little 1 a 110 1 small 1 little 1 shell Goal Figure 3: Example unrescored translation hypergraphs generated for the German input ein (a) kleines (small/little) Haus (house/shell) using a SCFG-based model (left) and phrase-based model with a distortion limit of 1 (right). must equal 0. Values that can be computed using the semirings include the number of derivations, the expected translation length, the entropy of the translation posterior distribution, and the expected values of feature functions (Li and Eisner, 2009). Since semirings are such a useful abstraction, cdec has been designed to facilitate implementation of new semirings. Table 1 shows the C++ representation used for semirings. Note that because of our representation, built-in types like double, int, and bool (together with their default operators) are semirings. Beyond these, the type prob t is provided which stores the logarithm of the value it represents, which helps avoid underflow and overflow problems that may otherwise be encountered. A generic first-order expectation semiring is also provided (Li and Eisner, 2009). Table 1: Semiring rep</context>
<context position="19454" citStr="Li and Eisner, 2009" startWordPosition="2956" endWordPosition="2959">decoding test described in §7. 4http://sourceforge.net/projects/joshua/ LanguageModel 1.12 WordPenalty -4.26 PhraseModel 0 0.963 PhraseModel 1 0.654 PhraseModel 2 0.773 PassThroughRule -20 11 8 Future work C. Dyer. 2010. Two monolingual parses are better than one (synchronous parse). In Proc. of HLT-NAACL. cdec continues to be under active development. We are taking advantage of its modular design to study alternative algorithms for language model integration. Further training pipelines are under development, including minimum risk training using a linearly decomposable approximation of BLEU (Li and Eisner, 2009), and MIRA training (Chiang et al., 2009). All of these will be made publicly available as the projects progress. We are also improving support for parallel training using Hadoop (an open-source implementation of MapReduce). Acknowledgements This work was partially supported by the GALE program of the Defense Advanced Research Projects Agency, Contract No. HR0011-06-2-001. Any opinions, findings, conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views of the sponsors. Further support was provided the EuroMatrix project funded by </context>
</contexts>
<marker>Li, Eisner, 2009</marker>
<rawString>Z. Li and J. Eisner. 2009. First- and second-order expectation semirings with applications to minimum-risk training on translation forests. In Proc. of EMNLP, pages 40–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Li</author>
<author>C Callison-Burch</author>
<author>C Dyer</author>
<author>J Ganitkevitch</author>
<author>S Khudanpur</author>
<author>L Schwartz</author>
<author>W N G Thornton</author>
<author>J Weese</author>
<author>O F Zaidan</author>
</authors>
<title>Joshua: an open source toolkit for parsing-based machine translation.</title>
<date>2009</date>
<booktitle>In Proc. of the Fourth Workshop on Stat. Machine Translation,</booktitle>
<pages>135--139</pages>
<contexts>
<context position="2170" citStr="Li et al., 2009" startWordPosition="279" endWordPosition="282">either weighted finite-state transducers (FSTs) or weighted synchronous context-free grammars (SCFGs) (Lopez, 2008). Phrase-based models (Koehn et al., 2003), lexical translation models (Brown et al., 1993), and finite-state conditional random fields (Sha and Pereira, 2003) exemplify the former, and hierarchical phrase-based models the latter (Chiang, 2007). We introduce a software package called cdec that manipulates both classes in a unified way.1 Although open source decoders for both phrasebased and hierarchical translation models have been available for several years (Koehn et al., 2007; Li et al., 2009), their extensibility to new models and algorithms is limited by two significant design flaws that we have avoided with cdec. First, their implementations tightly couple the translation, language model integration (which we call rescoring), and pruning algorithms. This makes it difficult to explore alternative translation models without also re-implementing rescoring and pruning logic. In cdec, model-specific code is only required to construct a translation forest (§3). General rescoring (with language models or other models), pruning, inference, and alignment algorithms then apply to the unif</context>
</contexts>
<marker>Li, Callison-Burch, Dyer, Ganitkevitch, Khudanpur, Schwartz, Thornton, Weese, Zaidan, 2009</marker>
<rawString>Z. Li, C. Callison-Burch, C. Dyer, J. Ganitkevitch, S. Khudanpur, L. Schwartz, W. N. G. Thornton, J. Weese, and O. F. Zaidan. 2009. Joshua: an open source toolkit for parsing-based machine translation. In Proc. of the Fourth Workshop on Stat. Machine Translation, pages 135–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D C Liu</author>
<author>J Nocedal</author>
<author>D C Liu</author>
<author>J Nocedal</author>
</authors>
<title>On the limited memory BFGS method for large scale optimization.</title>
<date>1989</date>
<journal>Mathematical Programming B,</journal>
<volume>45</volume>
<issue>3</issue>
<contexts>
<context position="17172" citStr="Liu et al., 1989" startWordPosition="2615" endWordPosition="2618">SIDE or the expectation semiring and INSIDE. Since a translation forest is generated as an intermediate step in generating an alignment forest (§2) this computation is straightforward. Since gradient-based optimization techniques may require thousands of evaluations to converge, the batch training pipeline is split into map and reduce components, facilitating distribution over very large clusters. Briefly, the cdec is run as the map function, and sentence pairs are mapped over. The reduce function aggregates the results and performs the optimization using standard algorithms, including LBFGS (Liu et al., 1989), RPROP (Riedmiller and Braun, 1993), and stochastic gradient descent. 7 Experiments Table 2 compares the performance of cdec, Hiero, and Joshua 1.3 (running with 1 or 8 threads) decoding using a hierarchical phrase-based translation grammar and identical pruning settings.4 Figure 4 shows the cdec configuration and weights file used for this test. The workstation used has two 2GHz quad-core Intel Xenon processors, 32GB RAM, is running Linux kernel version 2.6.18 and gcc version 4.1.2. All decoders use SRI’s language model toolkit, version 1.5.9 (Stolcke, 2002). Joshua was run on the Sun HotSpo</context>
</contexts>
<marker>Liu, Nocedal, Liu, Nocedal, 1989</marker>
<rawString>D. C. Liu, J. Nocedal, D. C. Liu, and J. Nocedal. 1989. On the limited memory BFGS method for large scale optimization. Mathematical Programming B, 45(3):503–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lopez</author>
</authors>
<title>Hierarchical phrase-based translation with suffix arrays.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>976--985</pages>
<contexts>
<context position="17954" citStr="Lopez, 2007" startWordPosition="2740" endWordPosition="2741">threads) decoding using a hierarchical phrase-based translation grammar and identical pruning settings.4 Figure 4 shows the cdec configuration and weights file used for this test. The workstation used has two 2GHz quad-core Intel Xenon processors, 32GB RAM, is running Linux kernel version 2.6.18 and gcc version 4.1.2. All decoders use SRI’s language model toolkit, version 1.5.9 (Stolcke, 2002). Joshua was run on the Sun HotSpot JVM, version 1.6.0 12. A hierarchical phrase-based translation grammar was extracted for the NIST MT03 Chinese-English translation using a suffix array rule extractor (Lopez, 2007). A non-terminal span limit of 15 was used, and all decoders were configured to use cube pruning with a limit of 30 candidates at each node and no further pruning. All decoders produced a BLEU score between 31.4 and 31.6 (small differences are accounted for by different tie-breaking behavior and OOV handling). Table 2: Memory usage and average per-sentence running time, in seconds, for decoding a ChineseEnglish test set. Decoder Lang. Time (s) Memory cdec C++ 0.37 1.0Gb Joshua (1x) Java 0.98 1.5Gb Joshua (8x) Java 0.35 2.5Gb Hiero Python 4.04 1.1Gb formalism=scfg grammar=grammar.mt03.scfg.gz a</context>
</contexts>
<marker>Lopez, 2007</marker>
<rawString>A. Lopez. 2007. Hierarchical phrase-based translation with suffix arrays. In Proc. of EMNLP, pages 976–985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lopez</author>
</authors>
<title>Statistical machine translation.</title>
<date>2008</date>
<journal>ACM Computing Surveys,</journal>
<volume>40</volume>
<issue>3</issue>
<contexts>
<context position="1669" citStr="Lopez, 2008" startWordPosition="206" endWordPosition="207">s. From this unified representation, the decoder can extract not only the 1- or k-best translations, but also alignments to a reference, or the quantities necessary to drive discriminative training using gradient-based or gradient-free optimization techniques. Its efficient C++ implementation means that memory use and runtime performance are significantly better than comparable decoders. 1 Introduction The dominant models used in machine translation and sequence tagging are formally based on either weighted finite-state transducers (FSTs) or weighted synchronous context-free grammars (SCFGs) (Lopez, 2008). Phrase-based models (Koehn et al., 2003), lexical translation models (Brown et al., 1993), and finite-state conditional random fields (Sha and Pereira, 2003) exemplify the former, and hierarchical phrase-based models the latter (Chiang, 2007). We introduce a software package called cdec that manipulates both classes in a unified way.1 Although open source decoders for both phrasebased and hierarchical translation models have been available for several years (Koehn et al., 2007; Li et al., 2009), their extensibility to new models and algorithms is limited by two significant design flaws that </context>
</contexts>
<marker>Lopez, 2008</marker>
<rawString>A. Lopez. 2008. Statistical machine translation. ACM Computing Surveys, 40(3), Aug.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lopez</author>
</authors>
<title>Translation as weighted deduction.</title>
<date>2009</date>
<booktitle>In Proc. of EACL,</booktitle>
<pages>532--540</pages>
<contexts>
<context position="6921" citStr="Lopez, 2009" startWordPosition="1025" endWordPosition="1026">of all the derivations of the sentence pair under the translation model. From this forest, the Viterbi or maximum a posteriori word alignment can be generated. This alignment algorithm is explored in depth by Dyer (2010). Note that if the phase 1 forest has been pruned in some way, or the grammar does not derive the sentence pair, the target intersection parse may fail, meaning that an alignment will not be recoverable. 3 Translation hypergraphs Recent research has proposed a unified representation for the various translation and tagging formalisms that is based on weighted logic programming (Lopez, 2009). In this view, translation (or tagging) deductions have the structure of a context-free forest, or directed hypergraph, where edges have a single head and 0 or more tail nodes (Nederhof, 2003). Once a forest has been constructed representing the possible translations, general inference algorithms can be applied. In cdec’s translation hypergraph, a node represents a contiguous sequence of target language words. For SCFG models and sequential tagging models, a node also corresponds to a source span and non-terminal type, but for word-based and phrase-based models, the relationship to the source</context>
</contexts>
<marker>Lopez, 2009</marker>
<rawString>A. Lopez. 2009. Translation as weighted deduction. In Proc. of EACL, pages 532–540.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M-J Nederhof</author>
</authors>
<title>Weighted deductive parsing and Knuth’s algorithm.</title>
<date>2003</date>
<journal>Comp. Ling.,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="7114" citStr="Nederhof, 2003" startWordPosition="1057" endWordPosition="1058">plored in depth by Dyer (2010). Note that if the phase 1 forest has been pruned in some way, or the grammar does not derive the sentence pair, the target intersection parse may fail, meaning that an alignment will not be recoverable. 3 Translation hypergraphs Recent research has proposed a unified representation for the various translation and tagging formalisms that is based on weighted logic programming (Lopez, 2009). In this view, translation (or tagging) deductions have the structure of a context-free forest, or directed hypergraph, where edges have a single head and 0 or more tail nodes (Nederhof, 2003). Once a forest has been constructed representing the possible translations, general inference algorithms can be applied. In cdec’s translation hypergraph, a node represents a contiguous sequence of target language words. For SCFG models and sequential tagging models, a node also corresponds to a source span and non-terminal type, but for word-based and phrase-based models, the relationship to the source string (or lattice) may be more complicated. In a phrase-based translation hypergraph, the node will correspond to a source coverage vector (Koehn et al., 2003). In word-based models, a single</context>
</contexts>
<marker>Nederhof, 2003</marker>
<rawString>M.-J. Nederhof. 2003. Weighted deductive parsing and Knuth’s algorithm. Comp. Ling., 29(1):135–143, Mar.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="14106" citStr="Och, 2003" startWordPosition="2141" endWordPosition="2142">. Thus, many of the semiring types define not only the elements shown in Table 1 but T::operator&lt; as well. The k-best extraction algorithm is also parameterized by an optional predicate that can filter out derivations at each node, enabling extraction of only derivations that yield different strings as in Huang et al. (2006). 6 Model training Two training pipelines are provided with cdec. The first, called Viterbi envelope semiring training, VEST, implements the minimum error rate training (MERT) algorithm, a gradient-free optimization technique capable of maximizing arbitrary loss functions (Och, 2003). 6.1 VEST Rather than computing an error surface using kbest approximations of the decoder search space, cdec’s implementation performs inference over the full hypergraph structure (Kumar et al., 2009). In particular, by defining a semiring whose values are sets of line segments, having an addition operation equivalent to union, and a multiplication operation equivalent to a linear transformation of the line segments, Och’s line search can be computed simply using the INSIDE algorithm. Since the translation hypergraphs generated by cdec may be quite large making inference expensive, the logic</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F. Och. 2003. Minimum error rate training in statistical machine translation. In Proc. of ACL, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="14967" citStr="Papineni et al., 2002" startWordPosition="2270" endWordPosition="2273">g whose values are sets of line segments, having an addition operation equivalent to union, and a multiplication operation equivalent to a linear transformation of the line segments, Och’s line search can be computed simply using the INSIDE algorithm. Since the translation hypergraphs generated by cdec may be quite large making inference expensive, the logic for constructing error surfaces is factored according to the MapReduce programming paradigm (Dean and Ghemawat, 2004), enabling parallelization across a cluster of machines. Implementations of the BLEU and TER loss functions are provided (Papineni et al., 2002; Snover et al., 2006). ED � 0 1 10 6.2 Large-scale discriminative training In addition to the widely used MERT algorithm, cdec also provides a training pipeline for discriminatively trained probabilistic translation models (Blunsom et al., 2008; Blunsom and Osborne, 2008). In these models, the translation model is trained to maximize conditional log likelihood of the training data under a specified grammar. Since log likelihood is differentiable with respect to the feature weights in an exponential model, it is possible to use gradient-based optimization techniques to train the system, enabli</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proc. of ACL, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Riedmiller</author>
<author>H Braun</author>
</authors>
<title>A direct adaptive method for faster backpropagation learning: The RPROP algorithm.</title>
<date>1993</date>
<booktitle>In Proc. of the IEEE international conference on neural networks,</booktitle>
<pages>586--591</pages>
<contexts>
<context position="17208" citStr="Riedmiller and Braun, 1993" startWordPosition="2620" endWordPosition="2624">miring and INSIDE. Since a translation forest is generated as an intermediate step in generating an alignment forest (§2) this computation is straightforward. Since gradient-based optimization techniques may require thousands of evaluations to converge, the batch training pipeline is split into map and reduce components, facilitating distribution over very large clusters. Briefly, the cdec is run as the map function, and sentence pairs are mapped over. The reduce function aggregates the results and performs the optimization using standard algorithms, including LBFGS (Liu et al., 1989), RPROP (Riedmiller and Braun, 1993), and stochastic gradient descent. 7 Experiments Table 2 compares the performance of cdec, Hiero, and Joshua 1.3 (running with 1 or 8 threads) decoding using a hierarchical phrase-based translation grammar and identical pruning settings.4 Figure 4 shows the cdec configuration and weights file used for this test. The workstation used has two 2GHz quad-core Intel Xenon processors, 32GB RAM, is running Linux kernel version 2.6.18 and gcc version 4.1.2. All decoders use SRI’s language model toolkit, version 1.5.9 (Stolcke, 2002). Joshua was run on the Sun HotSpot JVM, version 1.6.0 12. A hierarchi</context>
</contexts>
<marker>Riedmiller, Braun, 1993</marker>
<rawString>M. Riedmiller and H. Braun. 1993. A direct adaptive method for faster backpropagation learning: The RPROP algorithm. In Proc. of the IEEE international conference on neural networks, pages 586–591.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sha</author>
<author>F Pereira</author>
</authors>
<title>Shallow parsing with conditional random fields.</title>
<date>2003</date>
<booktitle>In Proc. of NAACL,</booktitle>
<pages>134--141</pages>
<contexts>
<context position="1828" citStr="Sha and Pereira, 2003" startWordPosition="226" endWordPosition="229">ities necessary to drive discriminative training using gradient-based or gradient-free optimization techniques. Its efficient C++ implementation means that memory use and runtime performance are significantly better than comparable decoders. 1 Introduction The dominant models used in machine translation and sequence tagging are formally based on either weighted finite-state transducers (FSTs) or weighted synchronous context-free grammars (SCFGs) (Lopez, 2008). Phrase-based models (Koehn et al., 2003), lexical translation models (Brown et al., 1993), and finite-state conditional random fields (Sha and Pereira, 2003) exemplify the former, and hierarchical phrase-based models the latter (Chiang, 2007). We introduce a software package called cdec that manipulates both classes in a unified way.1 Although open source decoders for both phrasebased and hierarchical translation models have been available for several years (Koehn et al., 2007; Li et al., 2009), their extensibility to new models and algorithms is limited by two significant design flaws that we have avoided with cdec. First, their implementations tightly couple the translation, language model integration (which we call rescoring), and pruning algor</context>
<context position="15906" citStr="Sha and Pereira, 2003" startWordPosition="2418" endWordPosition="2421">trained to maximize conditional log likelihood of the training data under a specified grammar. Since log likelihood is differentiable with respect to the feature weights in an exponential model, it is possible to use gradient-based optimization techniques to train the system, enabling the parameterization of the model using millions of sparse features. While this training approach was originally proposed for SCFG-based translation models, it can be used to train any model type in cdec. When used with sequential tagging models, this pipeline is identical to traditional sequential CRF training (Sha and Pereira, 2003). Both the objective (conditional log likelihood) and its gradient have the form of a difference in two quantities: each has one term that is computed over the translation hypergraph which is subtracted from the result of the same computation over the alignment hypergraph (refer to Figures 1 and 2). The conditional log likelihood is the difference in the log partition of the translation and alignment hypergraph, and is computed using the INSIDE algorithm. The gradient with respect to a particular feature is the difference in this feature’s expected value in the translation and alignment hyperg</context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>F. Sha and F. Pereira. 2003. Shallow parsing with conditional random fields. In Proc. of NAACL, pages 134–141.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Snover</author>
<author>B Dorr</author>
<author>R Schwartz</author>
<author>L Micciulla</author>
<author>J Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proc. AMTA.</booktitle>
<contexts>
<context position="14989" citStr="Snover et al., 2006" startWordPosition="2274" endWordPosition="2277"> of line segments, having an addition operation equivalent to union, and a multiplication operation equivalent to a linear transformation of the line segments, Och’s line search can be computed simply using the INSIDE algorithm. Since the translation hypergraphs generated by cdec may be quite large making inference expensive, the logic for constructing error surfaces is factored according to the MapReduce programming paradigm (Dean and Ghemawat, 2004), enabling parallelization across a cluster of machines. Implementations of the BLEU and TER loss functions are provided (Papineni et al., 2002; Snover et al., 2006). ED � 0 1 10 6.2 Large-scale discriminative training In addition to the widely used MERT algorithm, cdec also provides a training pipeline for discriminatively trained probabilistic translation models (Blunsom et al., 2008; Blunsom and Osborne, 2008). In these models, the translation model is trained to maximize conditional log likelihood of the training data under a specified grammar. Since log likelihood is differentiable with respect to the feature weights in an exponential model, it is possible to use gradient-based optimization techniques to train the system, enabling the parameterizatio</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and J. Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proc. AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Intl. Conf. on Spoken Language Processing.</booktitle>
<contexts>
<context position="17738" citStr="Stolcke, 2002" startWordPosition="2705" endWordPosition="2706">d algorithms, including LBFGS (Liu et al., 1989), RPROP (Riedmiller and Braun, 1993), and stochastic gradient descent. 7 Experiments Table 2 compares the performance of cdec, Hiero, and Joshua 1.3 (running with 1 or 8 threads) decoding using a hierarchical phrase-based translation grammar and identical pruning settings.4 Figure 4 shows the cdec configuration and weights file used for this test. The workstation used has two 2GHz quad-core Intel Xenon processors, 32GB RAM, is running Linux kernel version 2.6.18 and gcc version 4.1.2. All decoders use SRI’s language model toolkit, version 1.5.9 (Stolcke, 2002). Joshua was run on the Sun HotSpot JVM, version 1.6.0 12. A hierarchical phrase-based translation grammar was extracted for the NIST MT03 Chinese-English translation using a suffix array rule extractor (Lopez, 2007). A non-terminal span limit of 15 was used, and all decoders were configured to use cube pruning with a limit of 30 candidates at each node and no further pruning. All decoders produced a BLEU score between 31.4 and 31.6 (small differences are accounted for by different tie-breaking behavior and OOV handling). Table 2: Memory usage and average per-sentence running time, in seconds,</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. SRILM – an extensible language modeling toolkit. In Intl. Conf. on Spoken Language Processing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>