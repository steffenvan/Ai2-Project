<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001038">
<title confidence="0.896896">
The Linguistic Structure of English Web-Search Queries
</title>
<author confidence="0.589868">
Cory Barr and Rosie Jones
</author>
<affiliation confidence="0.405722">
Yahoo! Inc.
</affiliation>
<address confidence="0.5656085">
701 First Ave
Sunnyvale, CA 94089
</address>
<email confidence="0.685548">
barrc,jonesr@yahoo-inc.com
</email>
<author confidence="0.44754">
Moira Regelson
</author>
<affiliation confidence="0.365033">
Perfect Market, Inc.
</affiliation>
<address confidence="0.559052">
Pasadena, CA 91103
</address>
<email confidence="0.995802">
mregelson@perfectmarket.com
</email>
<sectionHeader confidence="0.999629" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99998346875">
Web-search queries are widely acknowledged to be
short (2.8 words (Spink et al., 2002)) and to be fre-
quently reformulated, but little else is understood
about their grammatical structure. Since search
queries are a fundamental part of the information
retrieval task, it is essential that we interpret them
correctly. However, the variable forms queries take
complicate interpretation significantly. We hypoth-
esize that elucidating the grammatical structure of
search queries would be highly beneficial for the as-
sociated information retrieval task.
Previous work with queries (Allan and Raghavan,
2002) considered that short queries may be ambigu-
ous in their part of speech and that different docu-
ments are relevant depending on how this ambigu-
ity is resolved. For example, the word “boat” in a
query may be intended as subject of a verb, object
of a verb, or as a verb, with each case reflecting
a distinct intent. To distinguish between the possi-
bilities, Allan and Raghavan (Allan and Raghavan,
2002) propose eliciting feedback from the user by
showing them possible contexts for the query terms.
In addition to disambiguating query terms for re-
trieval of suitable documents, part-of-speech tag-
ging can help increase recall by facilitating query
reformulation. Zukerman and Raskutti (Zukerman
and Raskutti, 2002) part-of-speech tag well-formed
questions, and use the part-of-speech tags to substi-
tute synonyms for the content words.
Several authors have leveraged part-of-speech
tagging towards improved index construction
for information retrieval through part-of-speech-
</bodyText>
<sectionHeader confidence="0.666836" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.999870027777778">
Web-search queries are known to be short,
but little else is known about their structure.
In this paper we investigate the applicability
of part-of-speech tagging to typical English-
language web search-engine queries and the
potential value of these tags for improving
search results. We begin by identifying a
set of part-of-speech tags suitable for search
queries and quantifying their occurrence. We
find that proper-nouns constitute 40% of query
terms, and proper nouns and nouns together
constitute over 70% of query terms. We also
show that the majority of queries are noun-
phrases, not unstructured collections of terms.
We then use a set of queries manually la-
beled with these tags to train a Brill tag-
ger and evaluate its performance. In addi-
tion, we investigate classification of search
queries into grammatical classes based on the
syntax of part-of-speech tag sequences. We
also conduct preliminary investigative experi-
ments into the practical applicability of lever-
aging query-trained part-of-speech taggers for
information-retrieval tasks. In particular, we
show that part-of-speech information can be a
significant feature in machine-learned search-
result relevance. These experiments also in-
clude the potential use of the tagger in se-
lecting words for omission or substitution in
query reformulation, actions which can im-
prove recall. We conclude that training a part-
of-speech tagger on labeled corpora of queries
significantly outperforms taggers based on tra-
ditional corpora, and leveraging the unique
linguistic structure of web-search queries can
improve search experience.
</bodyText>
<page confidence="0.956304">
1021
</page>
<note confidence="0.880579">
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1021–1030,
Honolulu, October 2008. c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.9995350625">
based weighting schemas and stopword detection
(Crestani et al., 1998), (Chowdhury and McCabe,
2000), (Dincer and Karaoglan, 2004). Their exper-
iments show degrees of success. Recently, along
with term weighting, Lioma has been using part-
of-speech n-grams for noise and content detection
in indexes (Lioma, 2008). Our study differs from
these in that linguistic and part-of-speech focus is
almost exclusively placed on queries as opposed
to the indexed documents, reflecting our opinion
that queries exhibit their own partially predictable
and unique linguistic structure different from that
of the natural language of indexed documents.
Similarly, (Strzalkowski et al., 1998) added a layer
of natural language processing using part-of-speech
tags and syntactical parsing to the common statis-
tical information-retrieval framework, much like
experiments detailed in sections 4 and 5. Our
system differs in that our syntactic parsing system
was applied to web-search queries and uses rules
derived from the observed linguistic structure of
queries as opposed to natural-language corpora.
By focusing on the part-of-speech distribution and
syntactic structure of queries over tagged indexed
documents, with a simple bijection mapping our
query tags to other tag sets, our system offers a
complementary approach that can be used in tandem
with the techniques referenced above.
Lima and Pederson (de Lima and Pederson, 1999)
conducted related work in which part-of-speech
tagging using morphological analysis was used as
a preprocessing step for labeling tokens of web-
search queries before being parse by a probabilis-
tic context-free grammar tuned to query syntax. We
believe this technique and others relying on part-of-
speech tagging of queries could benefit from using a
query-trained tagger prior to deeper linguistic anal-
ysis.
Pasca (Pasca, 2007) showed that queries can be
used as a linguistic resource for discovering named
entities. In this paper we show that the majority
of query terms are proper nouns, and the majority
of queries are noun-phrases, which may explain the
success of this data source for named-entity discov-
ery.
In this work, we use metrics that assume a unique
correct part-of-speech tagging for each query, im-
plicitly addressing the disambiguation issue through
inter-annotator-agreement scores and tagger gener-
alization error. To identify these tags, we first ana-
lyze the different general forms of queries. In Sec-
tion 2 we determine a suitable set of part-of-speech
labels for use with search queries. We then use man-
ually labeled query data to train a tagger and eval-
uate its performance relative to one trained on the
Brown corpus in Section 3. We make observations
about the syntactic structure of web-search queries
in Section 4, showing that the majority (70%) of
queries are noun-phrases, in contrast with the com-
monly held belief that queries consist of unstruc-
tured collections of terms. Finally, we examine the
potential use of tagging in the tasks of search rele-
vance evaluation and query reformulation in Section
5.
</bodyText>
<sectionHeader confidence="0.992355" genericHeader="introduction">
2 Data
</sectionHeader>
<bodyText confidence="0.999952692307692">
We sampled queries from the Yahoo! search en-
gine recorded in August 2006. Queries were sys-
tematically lower-cased and white-spaced normal-
ized. We removed any query containing a non-
ASCII character. Queries were then passed through
a high-precision proprietary query spelling correc-
tor, followed by the Penn Treebank tokenizer. No
other normalization was carried out. Despite Penn-
tokenization, queries were typical in their average
length (Jansen et al., 2000). We sampled 3,283
queries from our dataset to label, for a total of 2,508
unique queries comprised of 8,423 individual to-
kens.
</bodyText>
<subsectionHeader confidence="0.765314">
2.1 Inter-rater Agreement
</subsectionHeader>
<bodyText confidence="0.999982142857143">
The sparse textual information in search queries
presents difficulties beyond standard corpora, not
only for part-of-speech tagging software but also for
human labelers. To quantify the level of these diffi-
culties we measured inter-rater agreement on a set
of 100 queries labeled by each editor. Since one
labeler annotated 84.4% of the queries, we used a
non-standard metric to determine agreement. One
hundred queries were selected at random from each
of our secondary labelers. Our primary labeler then
re-labeled these queries. Accuracy was then calcu-
lated as a weighted average, specifically the mean of
the agreement between our primary labeler and sec-
ondary labelers, weighted by the number of queries
</bodyText>
<page confidence="0.993269">
1022
</page>
<bodyText confidence="0.999933677419355">
contributed by each secondary labeler. Measuring
agreement with respect to the individual part-of-
speech tag for each token, our corpus has an inter-
rater agreement of 79.3%. If we require agreement
between all tokens in a query, agreement falls to
65.4%. Using Cohen’s kappa coefficient, we have
that token-level agreement is a somewhat low 0.714
and query-level agreement is an even lower 0.641.
We attempted to accurately quantify token-level
ambiguity in queries by examining queries where
chosen labels differ. An author-labeler examined
conflicting labels and made a decision whether the
difference was due to error or genuine ambigu-
ity. Error can be a result of accidentally select-
ing the wrong label, linguistic misunderstanding
(e.g., “chatting” labeled as a verb or gerund), or
lack of consensus between editors (e.g., model num-
bers could be nouns, proper nouns, or even num-
bers). Examples of genuinely ambiguous queries in-
clude “download” and “rent,” both of which could
be a noun or verb. Another major source of gen-
uine token-level ambiguity comes from strings of
proper nouns. For example, some editors consid-
ered “stillwater chamber of commerce” one entity
and hence four proper-noun tokens while others con-
sidered only the first token a proper noun. Of the 99
conflicting token labels in our queries used to mea-
sure inter-annotator agreement, 69 were judged due
to genuine ambiguity. This left us with a metric in-
dicating query ambiguity accounts for 69.7% of la-
beling error.
</bodyText>
<subsectionHeader confidence="0.990244">
2.2 Tags for Part-of-Speech Tagging Queries
</subsectionHeader>
<bodyText confidence="0.999748">
In preliminary labeling experiments we found many
standard part-of-speech tags to be extremely rare in
web-search queries. Adding them to the set of possi-
ble tags made labeling more difficult without adding
any necessary resolution. In Table 1 we give the
set of tags we used for labeling. In general, part-
of-speech tags are defined according to the distribu-
tional behavior of the corresponding parts of speech.
Our tag set differs dramatically from the Brown or
Penn tag sets. Perhaps most noticeably, the sizes of
the tag sets are radically different. The Brown tag set
contains roughly 90 tags. In addition, several tags
can be appended with additional symbols to indicate
negation, genitives, etc. Our tag set contains just 19
unique classes.
</bodyText>
<table confidence="0.99920085">
Tag Example Count (%)
proper-noun texas 3384 (40.2%)
noun pictures 2601 (30.9%)
adjective big 599 (7.1%)
URI ebay.com 495 (5.9%)
preposition in 310 (3.7%)
unknown y 208 (2.5%)
verb get 198 (2.4%)
other conference06-07 174 (2.1%)
comma , 72 (0.9%)
gerund running 69 (0.8%)
number 473 67 (0.8%)
conjunction and 65 (0.8%)
determiner the 56 (0.7%)
pronoun she 53 (0.6%)
adverb quickly 28 (0.3%)
possessive ’s 19 (0.2%)
symbol ( 18 (0.2%)
sentence-ender ? 5 (0.1%)
not n’t 2 (0.0%)
</table>
<tableCaption confidence="0.991853">
Table 1: Tags used for labeling part-of-speech in web-
search queries.
</tableCaption>
<bodyText confidence="0.999976857142857">
Our contrasting tag sets reflect an extremely dif-
ferent use of the English language and correspond-
ing part-of-speech distribution. For example, the
Brown tag set contains unique tags for 35 types of
verbs. We use a single label to indicate all cases of
verbs. However, the corpora the Brown tag set was
designed for consists primarily of complete, natural-
language sentences. Essentially, every sentence con-
tains at least one verb. In contrast, a verb of any type
accounts for only 2.35% of our tags. Similarly, the
Brown corpus contains labels for 15 types of deter-
miners. This class makes up just 0.66% of our data.
Our most common tag is the proper noun, which
constitutes 40% of all query terms, and proper nouns
and nouns together constitute 71% of query terms.
In the Brown corpus, by contrast, the most common
tag, noun, constitutes about 13% of terms. Thus the
distribution of tag types in queries is quite different
from typical edited and published texts, and in par-
ticular, proper nouns are more common than regular
nouns.
</bodyText>
<subsectionHeader confidence="0.995283">
2.3 Capitalization in Query Data
</subsectionHeader>
<bodyText confidence="0.999855">
Although we have chosen to work with lowercase
data, web search queries sometimes contain capi-
</bodyText>
<page confidence="0.862132">
1023
</page>
<table confidence="0.999711">
Use of Capitals Count % Example
Proper-nouns capitalized 48 47% list of Filipino riddles
Query-Initial-Caps 10 10% Nautical map
Init-Caps + Proper-Nouns 7 7% Condos in Yonkers
Acronym 4 4% location by IP address
Total standard capitalization 69 67%
All-caps 26 25% FAX NUMBER FOR
ALLEN CANNING CO
Each word capitalized 6 6% Direct Selling
Mixed 2 2% SONGS OF MEDONA
music feature:audio
Total non-standard capitalization 34 33%
</table>
<tableCaption confidence="0.9878">
Table 2: Ways capitalization is used in web-search
queries.
</tableCaption>
<bodyText confidence="0.999892441176471">
talization information. Since capitalization is fre-
quently used in other corpora to identify proper
nouns, we reviewed its use in web-search queries.
We found that the use of capitalization is inconsis-
tent. On a sample of 290,122 queries from Au-
gust 2006 only 16.8% contained some capitaliza-
tion, with 3.9% of these all-caps. To review the use
of capitalization, we hand-labeled 103 queries con-
taining capital letters (Table 2).
Neither all-lowercase (83.2%) nor all-caps (3.9%)
queries can provide us with any part-of-speech
clues. But we would like to understand the use of
capitalization in queries with varied case. In par-
ticular, how frequently does first-letter capitalization
indicate a proper noun? We manually part-of-speech
tagged 75 mixed-case queries, which contained 289
tokens, 148 of which were proper nouns. The base-
line fraction of proper nouns in this sample is thus
51% (higher than the overall background of 40.2%).
A total of 176 tokens were capitalized, 125 of them
proper nouns. Proper nouns thus made up 73.3%
of capitalized tokens, which is larger than the back-
ground occurrence of proper nouns. We can con-
clude from this that capitalization in a mixed-case
query is a fair indicator that a word is a proper noun.
However, the great majority of queries contain no
informative capitalization, so the great majority of
proper nouns in search queries must be uncapital-
ized. We cannot, therefore, rely on capitalization to
identify proper nouns.
With this knowledge of the infrequent use of capi-
tal letters in search queries in mind, we will examine
the effects of ignoring or using a query’s capitaliza-
tion for part-of-speech tagging in Section 3.4.2.
</bodyText>
<sectionHeader confidence="0.61779" genericHeader="method">
3 Tagger Accuracy on Search Queries
</sectionHeader>
<bodyText confidence="0.999879777777778">
To investigate automation of the tagging process,
we trained taggers on our manually labeled query
set. We used 10-fold cross-validation, with 90% of
the data used for training and the remaining data
used for testing. In the sections below, we used two
datasets. The first consists of 1602 manually labeled
queries. For the experiments in Section 3.5 we la-
beled additional queries, for a total of 2503 manu-
ally labeled queries.
</bodyText>
<subsectionHeader confidence="0.999777">
3.1 Part-of-Speech Tagging Software
</subsectionHeader>
<bodyText confidence="0.999971828571429">
We experimented with two freely available part-
of-speech taggers: The Brill Tagger (Brill, 1995)
and The Stanford Tagger (Toutanova and Manning,
2000; Toutanova et al., 2003).
The Brill tagger works in two stages. The initial
tagger queries a lexicon and labels each token with
its most common part-of-speech tag. If the token
is not in the lexicon, it labels the token with a de-
fault tag, which was “proper noun” in our case. In
the second stage, the tagger applies a set of lexical
rules which examine prefixes, suffixes, and infixes.
The tagger may then exchange the default tag based
on lexical characteristics common to particular parts
of speech. After application of lexical rules, a set
of contextual rules analyze surrounding tokens and
their parts of speech, altering tags accordingly.
We chose to experiment primarily with the Brill
tagger because of its popularity, the human-readable
rules it generates, and its easily modifiable code
base. In addition, the clearly defined stages and in-
corporation of the lexicon provide an accessible way
to supply external lexicons or entity-detection rou-
tines, which could compensate for the sparse con-
textual information of search queries.
We also experimented with the Stanford Log-
Linear Part-of-Speech Tagger, which presently
holds the best published performance in the field at
96.86% on the Penn Treebank corpus. It achieves
this accuracy by expanding information sources for
tagging. In particular, it provides “(i) more exten-
sive treatment of capitalization for unknown words;
(ii) features for the disambiguation of the tense
forms of verbs; (iii) features for disambiguating par-
ticles from prepositions and adverbs.” It uses a
maximum-entropy approach to handle information
</bodyText>
<page confidence="0.985092">
1024
</page>
<bodyText confidence="0.9606055">
diversity without assuming predictor independence
(Toutanova and Manning, 2000).
</bodyText>
<subsectionHeader confidence="0.975558">
3.4 Improving Tagger Accuracy
</subsectionHeader>
<bodyText confidence="0.999939">
We conducted several experiments in improving tag-
ger accuracy, summarized in Table 3 and described
in detail below.
</bodyText>
<subsectionHeader confidence="0.847142">
3.2 Baseline: Most Common Tag
</subsectionHeader>
<bodyText confidence="0.999989611111111">
With proper nouns dominating the distribution, we
first considered using the accuracy of labeling all to-
kens “proper noun” as a baseline. In this case, we
labeled 1953 of 4759 (41.0%) tokens correctly. This
is a significant improvement over the accuracy of
tagging all words as “noun” on the Brown corpus
(approximately 13%), reflecting the frequent occur-
rence of proper nouns in search queries. However, to
examine the grammatical structure of search queries
we must demonstrate that they are not simply col-
lections of words. With this in mind, we chose in-
stead to use the most common part-of-speech tag
for a word as a baseline. We evaluated the baseline
performance on our manually labeled dataset, with
URLs removed. Each token in the set was assigned
its most common part of speech, according to the
Brill lexicon. In this case, 4845 of 7406 tokens were
tagged correctly (65.42%).
</bodyText>
<subsectionHeader confidence="0.999333">
3.3 Effect of Type of Training Data
</subsectionHeader>
<bodyText confidence="0.99998080952381">
The Brill tagger software is pre-trained on the stan-
dard Wall Street Journal corpus, so the simplest pos-
sible approach is to apply it directly to the query data
set. We evaluated this “out-of-the-box” performance
on our 1602 manually labeled queries, after mapping
tags to our reduced tag set. (Our effective training-
set size is 1440 queries, since 10% were held out
to measure accuracy through cross validation.) The
WSJ-trained tagger labeled 2293 of 4759 (48.2%)
tags correctly, a number well below the baseline
performance, demonstrating that application of the
contextual rules that Brill learns from the syntax of
natural-language corpora has a negative effect on ac-
curacy in the context of queries. When we re-trained
Brill’s tagger on a manually labeled set of queries,
we saw accuracy increase to 69.7%. The data used
to train the tagger therefore has a significant effect
on its accuracy (Table 3). The accuracy of the tag-
ger trained on query data is above the baseline, in-
dicating that search queries are somewhat more than
collections of words.
</bodyText>
<subsectionHeader confidence="0.996952">
3.4.1 Adding External Lexicon
</subsectionHeader>
<bodyText confidence="0.999937571428571">
With a training-set size of 1500 queries, compris-
ing a lexicon of roughly 4500 words, it is natural to
question if expanding the lexicon by incorporating
external sources boosts performance. To this end,
we lower-cased the lexicon of 93,696 words pro-
vided by the Brill tagger, mapped the tags to our
own tag set, and merged our lexicon from queries.
This experiment resulted in an accuracy of 71.1%, a
1.4% increase.
One explanation for the limited increase is that
this lexicon is derived from the Brown corpus and
the Penn Treebank tagging of the Wall Street Jour-
nal. These corpora are based on works published
in 1961 and 1989-1992 respectively. As shown in
Table 1, proper nouns dominate the distribution of
search-engine queries. Many of these queries will
involve recent products, celebrities, and other time-
sensitive proper nouns. We speculate that Web-
based information resources could be leveraged to
expand the lexicon of timely proper nouns, thereby
enhancing performance.
</bodyText>
<subsectionHeader confidence="0.920339">
3.4.2 Experiments with Perfect Capitalization
</subsectionHeader>
<bodyText confidence="0.999854105263158">
The overall performance of the pre-trained Brill
tagger on our query set may be due to its poor per-
formance on proper nouns, our most frequent part
of speech. In the WSJ newspaper training data,
proper-nouns always start with a capital letter. As
discussed in Section 2.3, capitalization is rare in
web-search queries. To examine the effect of the
missing capitalization of proper nouns, we evaluated
a pre-trained Brill tagger on our previously men-
tioned manually labeled corpus of 1602 queries al-
tered such that only the proper nouns were capital-
ized. In this case, the tagger reached an extraordi-
nary 89.4% accuracy (Table 3). Unfortunately, the
vast majority of queries do not contain capitalization
information and those that do often contain mislead-
ing information. The pre-trained tagger achieved
only a 45.6% accuracy on non-lowercased queries,
performing even worse than on the set with no capi-
talization at all.
</bodyText>
<page confidence="0.973667">
1025
</page>
<table confidence="0.684534888888889">
Experiment Accuracy
Label-all-proper-noun 41.0%
WSJ-trained 48.2%
most-freq-tag-WSJ 64.4%
re-trained 69.7%
retrained + WSJ lexicon 71.1%
user capitalization 45.6%
oracle capitalization 89.4%
automatic capitalization 70.9%
</table>
<tableCaption confidence="0.808461">
Table 3: Tagging experiments on small labeled corpus.
Experiments were conducted on lower-cased queries ex-
cept where specifically indicated.
</tableCaption>
<subsectionHeader confidence="0.4293">
3.4.3 Automatic Capitalization
</subsectionHeader>
<bodyText confidence="0.999837709677419">
We saw in Section 2.3 that web searchers rarely
use capitalization. We have also seen that a pre-
trained Brill tagger run on queries with perfect
capitalization (“oracle” capitalization) can achieve
89.4% accuracy. We now look at how performance
might be affected if we used an imperfect algorithm
for capitalization.
In order to attempt to capitalize the proper nouns
in queries, we used a machine-learned system which
searches for the query terms and examines how of-
ten they are capitalized in the search results, weight-
ing each capitalization occurrence by various fea-
tures (Bartz et al., 2008). Though the capitalization
system provides 79.3% accuracy, using this system
we see an only a small increase of accuracy in part-
of-speech tagging at 70.9%. This system does not
improve significantly over the tagger trained on the
lower-cased corpus. One explanation is that cap-
italization information of this type could only be
obtained for 81.9% of our queries. Multiplied by
accuracy, this implies that roughly 81.9% * 79.3%
= 65.0% of our proper nouns are correctly cased.
This suggests that any technique for proper-noun de-
tection in search-engine queries must provide over
65.0% accuracy to see any performance increase.
Finally we looked at the capitalization as input by
searchers. We trained on the oracle-capitalized cor-
pus, and tested on raw queries without normaliza-
tion. We saw an accuracy of just 45.6%. Thus using
the capitalization input by web searchers is mislead-
ing and actually hurts performance.
</bodyText>
<figureCaption confidence="0.848409">
Figure 1: Brill’s tagger trained on web-search queries.
</figureCaption>
<bodyText confidence="0.89783425">
We see that the most significant gains in performance are
with the first few hundred labeled examples, but even af-
ter 2500 examples are labeled, more labeled data contin-
ues to improve performance.
</bodyText>
<subsectionHeader confidence="0.990078">
3.5 Learning Curve
</subsectionHeader>
<bodyText confidence="0.999986411764706">
It is important to understand whether tagger accu-
racy is limited by the small size of our manually la-
beled dataset. To examine the effect of dataset size,
we trained Brill’s tagger with increasing numbers of
labeled queries and evaluated accuracy with each set
size. In the interim between conducting the experi-
ments of sections 3.1 through 3.3 and those of sec-
tion 3.5, we were able to obtain 1120 new labeled
queries, allowing us to extend the learning curve.
With our complete corpus of 2722 labeled exam-
ples (for a cross-validated training-set size of 2450
labeled examples, URLs omitted), we see an accu-
racy of 78.6% on a per-token basis. We see the most
significant gains in performance with the first few
hundred labeled examples, but even after 2500 ex-
amples are labeled, more labeled data continues to
improve performance.
</bodyText>
<subsectionHeader confidence="0.8985055">
3.6 Comparing Taggers to Suggest
Methods for Boosting Performance
</subsectionHeader>
<bodyText confidence="0.999960666666667">
In Table 4 we see a comparison of Brill’s tagger to
the Stanford tagger trained on 2450 labeled queries.
The 0.3% performance increase is not statistically
significant. As listed in Section 3, the features
the Stanford tagger adds to achieve high accuracy
in traditional natural-language corpora are not in-
</bodyText>
<page confidence="0.978206">
1026
</page>
<table confidence="0.986866666666667">
Tagger Accuracy
Brill 78.6%
Stanford 78.9%
</table>
<tableCaption confidence="0.990236">
Table 4: Comparison of Brill’s tagger to the Stanford tag-
ger, on our corpus of manually annotated query logs.
</tableCaption>
<bodyText confidence="0.999960257142858">
formative in the domain of search-engine queries.
We believe greater performance on our data will be
achieved primarily through examination of common
sources of inter-rater disagreement (such as consis-
tent handling of ambiguity) and incorporation of ex-
ternal sources to detect proper nouns not in the lexi-
con.
To validate our intuition that expanding the lexi-
con will boost performance, we obtained a propri-
etary list of 7385 known trademarked terms used
in the sponsored-search industry. Treating these
phrases as proper nouns and adding them to the lex-
icon from the Wall Street Journal supplied with the
Brill tagger, we see our cross validated accuracy im-
prove to 80.2% (with a standard deviation of 1.85%),
the highest score achieved in our experiments. We
find it likely that incorporation of more external lex-
ical sources will result in increased performance.
Our experiments also support our hypothesis that
addressing inter-annotator agreement will boost per-
formance. We can see this by examining the results
of the experiments in section 3.3 verses section 3.5.
In section 3.3, we see the accuracy on the query-
trained Brill tagger is 69.7%. As mentioned, for
the experiment in section 3.5, we were able to ob-
tain 1120 new queries. Each of these newly labeled
queries came from the same labeler, who believes
their handling of the ambiguities inherent in search
queries became more consistent over time. With
the same training-set size of 1440 used in section
3.3, Figure 1 shows performance at 1440 queries is
roughly 6% higher. We believe this significant im-
provement is a result of more consistent handling of
query ambiguity obtained through labeling experi-
ence.
</bodyText>
<sectionHeader confidence="0.957791" genericHeader="method">
4 Query Grammar
</sectionHeader>
<bodyText confidence="0.999971117647059">
The above-baseline performance of the Brill tagger
trained on web-search queries suggests that web-
search queries exhibit some degree of syntactical
structure. With a corpus of queries labeled with part-
of-speech information, we are in a position to ana-
lyze this structure and characterize the typical pat-
terns of part-of-speech used by web searchers. To
this end, we randomly sampled and manually la-
beled a set of 222 queries from the part-of-speech
dataset used for tagger training mentioned above.
Each query was labeled with a single meta-tag in-
dicating query type. Two author-judges simultane-
ously labeled queries and created the set of meta-
tags during much discussion, debate, and linguistic
research. A list of our meta-tags and the distribu-
tion of each are provided in Table 5. We can see
that queries consisting of a noun-phrase dominate
the distribution of query types, in contrast with the
popularly held belief that queries consist of unstruc-
tured collections of terms.
To determine how accurately a meta-tag can be
determined based on part-of-speech labels, we cre-
ated a grammar consisting of a set of rules to rewrite
part-of-speech tags into higher-level grammatical
structures. These higher-level grammatical struc-
tures are then rewritten into one of the seven classes
of meta-tags seen in Table 5. Our grammar was con-
structed by testing the output of our rewrite rules on
queries labeled with par-of-speech tags that were not
part of the 222 queries sampled for meta-tag label-
ing. Grammar rules were revised until the failure
rate on previously untested part-of-speech-labeled
queries stabilized. Failure was evaluated by two
means. In the first case, the grammar rules failed
to parse the sequence of part-of-speech tags. In the
second case, the grammar rules led to an inappro-
priate classification for a query type. As during the
labeling phase, the two author-labelers simultane-
ously reached a consensus on whether a parse failed
or succeeded, rendering an inter-annotator score in-
applicable. The resulting grammar was then tested
on the 222 queries with query-type meta-tags.
Our rules function much like production rules in
context-free grammars. As an example, the two-
tag sequence “determiner noun” will be rewritten
as “noun phrase.” This in turn could be re-written
into a larger structure, which will then be rewritten
into a meta-tag of query type. The primary differ-
ence between a context-free grammar or probabilis-
tic context-free grammar (such as that employed by
Lima and Pederson (de Lima and Pederson, 1999))
</bodyText>
<page confidence="0.976574">
1027
</page>
<table confidence="0.999059222222222">
Query Gramm. Type Example Freq (%)
noun-phrase free mp3s 155 (69.8%)
URI http:answers.yahoo.com/ 24 (10.8%)
word salad mp3s free 19 (8.1%)
other-query florida elementary reading 15 (6.8%)
conference2006-2007
unknown nama-nama calon praja ipdn 6 (2.7%)
verb-phrase download free mp3s 3 (1.4%)
question where can I download free mp3s 1 (0.45%)
</table>
<tableCaption confidence="0.986164333333333">
Table 5: Typical grammatical forms of queries used by
web searchers, with distribution based on a sample of 222
hand-labeled queries.
</tableCaption>
<bodyText confidence="0.999992115384615">
and our grammar is that our rules are applied itera-
tively as opposed to recursively. As such, our gram-
mar yields a single parse for each input.
Some of our rules reflect the telegraphic nature of
web queries. For example, it is much more com-
mon to see an abbreviated noun-phrase consisting of
adjective-noun, than one consisting of determiner-
adjective-noun.
Examining the Table 5, we see that just label-
ing a query “noun-phrase” results in an accuracy of
69.8%. Our grammar boosted this high baseline by
14% to yield an final accuracy result of 83.3% at la-
beling queries with their correct meta-type. These
meta-types could be useful in deciding how to han-
dle a query. Further enhancements to the grammar
would likely yield a performance increase. How-
ever, we feel accuracy is currently high enough to
continue with experiments towards application of
leveraging grammar-deduced query types for infor-
mation retrieval.
We can think of some of these meta-types as
elided sentences. For example, the noun-phrase
queries could be interpreted as requests of the form
“how can I obtain X” or “where can I get informa-
tion on X”, while the verb-phrase queries are re-
quests of the form ”I would like to DO-X”.
</bodyText>
<sectionHeader confidence="0.92813" genericHeader="method">
5 Applications of Part-of-Speech Tagging
</sectionHeader>
<bodyText confidence="0.999955428571429">
Since search queries are part of an information re-
trieval task, we would like to demonstrate that part-
of-speech tagging can assist with that task. We con-
ducted two experiments with a large-scale machine-
learned web-search ranking system. In addition, we
considered the applicability of part-of-speech tags to
the question of query reformulation.
</bodyText>
<subsectionHeader confidence="0.974812">
5.1 Web Search Ranking
</subsectionHeader>
<bodyText confidence="0.999951952380953">
We worked with a proprietary experimental testbed
in which features for predicting the relevance of a
query to a document can be tested in a machine-
learning framework. Features can take a wide va-
riety of forms (boolean, real-valued, relational) and
apply to a variety of scopes (the page, the query,
or the combination). These features are evaluated
against editorial judgements and ranked according
to their significance in improving the relevance of
results. We evaluated two part-of-speech tag-based
features in this testbed.
The first experiment involved a simple query-level
feature indicating whether the query contained a
noun or a proper noun. This feature was evaluated
on thousands of queries for the test. At the conclu-
sion of the test, this feature was found to be in the
top 13% of model features, ranked in order of signif-
icance. We believe this significance represents the
importance of recognizing the presence of a noun
in a query and, of course, matching it. Within this
experimental testbed a statistically significant im-
provement of information-retrieval effectiveness is
notoriously difficult to attain. We did not see a sig-
nificant improvement in this metric. However, we
feel that our feature’s high ranking warrants report-
ing and hints at a potentially genuine boost in re-
trieval performance in a system less feature-rich.
The second experiment was more involved and re-
flected more of our intuition about the likely applica-
tion of part-of-speech tagging to the improvement of
search results. In this experiment, we part-of-speech
tagged both queries and documents. Documents
were tagged with a conventionally trained Brill tag-
ger with the resulting Penn-style tags mapped to our
tag set. Many thousands of query-document pairs
were processed in this manner. The feature was
based on the percent of times the part-of-speech tag
of a word in the query matched the part-of-speech
tag of the same word in the document. This feature
was ranked in the top 12% by significance, though
we again saw no statistically significant increase in
overall retrieval performance.
</bodyText>
<subsectionHeader confidence="0.992503">
5.2 Query Reformulation
</subsectionHeader>
<bodyText confidence="0.999959">
We considered the application of part-of-speech tag-
ging to the problem of query reformulation, in which
</bodyText>
<page confidence="0.957796">
1028
</page>
<table confidence="0.999923833333333">
Part-of-speech p(subst) subst / seen
Number 0.49 148 / 302
Adjective 0.46 2877 / 6299
Noun 0.42 15038 / 35515
Proper noun 0.39 21478 / 55331
Gerund 0.37 112 / 300
Verb 0.31 1769 / 5718
Pronoun 0.23 300 / 1319
Conjunction 0.18 85 / 464
Adverb 0.13 105 / 790
Determiner 0.10 22 / 219
Preposition 0.08 369 / 4574
Possessive 0.08 25 / 330
Not 0.03 1 / 32
Symbol 0.02 16 / 879
Other 0.02 78 / 3294
Sentence-ender 0.01 3 / 234
Comma 0.00 4 / 991
</table>
<tableCaption confidence="0.989678">
Table 6: Probability of a word being reformulated from
</tableCaption>
<bodyText confidence="0.99789275">
one query to the next, by part-of-speech tag. While
proper-nouns are the most frequent tag in our corpus, ad-
jectives are more frequently reformulated, reflecting the
fact that the proper nouns carry the core meaning of the
query.
a single word in the query is altered within the
same user session. We used a set of automatically
tagged queries to calculate change probabilities of
each word by part-of-speech tag and the results are
shown in Table 6.
The type of word most likely to be reformu-
lated is “number.” Examples included changing a
year (“most popular baby names 2007” —* “most
popular baby names 2008”), while others included
model, version and edition numbers (“harry potter
6” —* “harry potter 7”) most likely indicating that
the user is looking at variants on a theme, or cor-
recting their search need. Typically a number is a
modifier of the core search meaning. The next most
commonly changed type was “adjective,” perhaps
indicating that adjectives can be used to refine, but
not fundamentally alter, the search intent. Nouns
and proper nouns are the next most commonly mod-
ified types, perhaps reflecting user modification of
their search need, refining the types of documents
retrieved. Other parts of speech are relatively sel-
dom modified, perhaps indicating that they are not
viewed as having a large impact on the documents
retrieved.
We can see from the impact of the search engine
ranking features and from the table of query refor-
mulation likelihood that making use of the grammat-
ical structure of search queries can have an impact
on result relevance. It can also assist with tasks as-
sociated with improving recall, such as query refor-
mulation.
</bodyText>
<sectionHeader confidence="0.999491" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999979461538462">
We have quantified, through a lexicostatistical anal-
ysis, fundamental differences between the natural
language used in standard English-language corpora
and English search-engine queries. These differ-
ences include reduced granularity in part-of-speech
classes as well as the dominance of the noun classes
in queries at the expense of classes such as verbs
frequently found in traditional corpora. In addi-
tion, we have demonstrated the poor performance of
taggers trained on traditional corpora when applied
to search-engine queries, and how this poor perfor-
mance can be overcome through query-based cor-
pora. We have suggested that greater improvement
can be achieved by proper-noun detection through
incorporation of external lexicons or entity detec-
tion. Finally, in preliminary investigations into ap-
plications of our findings, we have shown that query
part-of-speech tagging can be used to create signif-
icant features for improving the relevance of web
search results and may assist with query reformu-
lation. Improvements in accuracy can only increase
the value of POS information for these applications.
We believe that query grammar can be further ex-
ploited to increase query understanding and that this
understanding can improve the overall search expe-
rience.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995759555555556">
James Allan and Hema Raghavan. 2002. Using part-
of-speech patterns to reduce query ambiguity. In Pro-
ceedings of SIGIR, pages 307–314.
Kevin Bartz, Cory Barr, and Adil Aijaz. 2008. Natu-
ral language generation in sponsored-search advertise-
ments. In Proceedings of the 9th ACM Conference on
Electronic Commerce, pages 1–9, Chicago, Illinois.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case
</reference>
<page confidence="0.88219">
1029
</page>
<reference confidence="0.999106529411765">
study in part-of-speech tagging. Computational Lin-
guistics, 21(4):543–565.
Abdur Chowdhury and M. Catherine McCabe. 2000.
Improving information retrieval systems using part of
speech tagging.
Fabio Crestani, Mark Sanderson, and Mounia Lalmas.
1998. Short queries, natural language and spoken doc-
ument retrieval: Experiments at glasgow university.
In Proceedings of the Sixth Text Retrieval Conference
(TREC-6), pages 667–686.
Erika F. de Lima and Jan O. Pederson. 1999. Phrase
recognition and expansion for short, precision-biased
queries based on a query log. In Annual ACM Con-
ference on Research and Development in Informa-
tion Retrieval Proceedings of the 22nd annual inter-
national ACM SIGIR conference on Research and de-
velopment in information retrieval, pages 145–152,
Berkeley, California.
Bekir Taner Dincer and Bahar Karaoglan. 2004. The
effect of part-of-speech tagging on ir performance for
turkish. pages 771–778.
Bernard J. Jansen, Amanda Spink, and Tefko Saracevic.
2000. Real life, real users, and real needs: a study
and analysis of user queries on the web. Information
Processing and Management, 36(2):207–227.
Christina Amalia Lioma. 2008. Part of speech N-grams
for information retrieval. Ph.D. thesis, University of
Glasgow, Glasgow, Scotland, UK.
Marius Pasca. 2007. Weakly-supervised discovery of
named entities using web search queries. In CIKM,
pages 683–690.
Amanda Spink, B. J. Jansen, D. Wolfram, and T. Sarace-
vic. 2002. From e-sex to e-commerce: Web search
changes. IEEE Computer, 35(3):107–109.
Tomek Strzalkowski, Jose Perez Carballo, and Mihnea
Marinescu. 1998. Natural language information re-
trieval: Trec-3 report. In Proceedings of the Sixth Text
Retrieval Conference (TREC-6), page 39.
Kristina Toutanova and Christopher D. Manning. 2000.
Enriching the knowledge sources used in a maximum
entropy part-of-speech tagger. In Proceedings of the
Joint SIGDAT Conference on Empirical Methods in
Natural Language Processing and Very Large Corpora
(EMNLP/VLC-2000).
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of HLT-NAACL, pages 252–259.
Ingrid Zukerman and Bhavani Raskutti. 2002. Lexical
query paraphrasing for document retrieval. In COL-
ING, pages 1177–1183, Taipei, Taiwan.
</reference>
<page confidence="0.99016">
1030
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.314173">
<title confidence="0.999901">The Linguistic Structure of English Web-Search Queries</title>
<author confidence="0.991069">Cory Barr</author>
<author confidence="0.991069">Rosie</author>
<affiliation confidence="0.839332">Yahoo!</affiliation>
<address confidence="0.9514745">701 First Sunnyvale, CA</address>
<email confidence="0.997801">barrc,jonesr@yahoo-inc.com</email>
<author confidence="0.7832355">Moira Perfect Market</author>
<address confidence="0.683606">Pasadena, CA</address>
<email confidence="0.973319">mregelson@perfectmarket.com</email>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James Allan</author>
<author>Hema Raghavan</author>
</authors>
<title>Using partof-speech patterns to reduce query ambiguity.</title>
<date>2002</date>
<booktitle>In Proceedings of SIGIR,</booktitle>
<pages>307--314</pages>
<contexts>
<context position="854" citStr="Allan and Raghavan, 2002" startWordPosition="116" endWordPosition="119">ket.com 1 Introduction Web-search queries are widely acknowledged to be short (2.8 words (Spink et al., 2002)) and to be frequently reformulated, but little else is understood about their grammatical structure. Since search queries are a fundamental part of the information retrieval task, it is essential that we interpret them correctly. However, the variable forms queries take complicate interpretation significantly. We hypothesize that elucidating the grammatical structure of search queries would be highly beneficial for the associated information retrieval task. Previous work with queries (Allan and Raghavan, 2002) considered that short queries may be ambiguous in their part of speech and that different documents are relevant depending on how this ambiguity is resolved. For example, the word “boat” in a query may be intended as subject of a verb, object of a verb, or as a verb, with each case reflecting a distinct intent. To distinguish between the possibilities, Allan and Raghavan (Allan and Raghavan, 2002) propose eliciting feedback from the user by showing them possible contexts for the query terms. In addition to disambiguating query terms for retrieval of suitable documents, part-of-speech tagging </context>
</contexts>
<marker>Allan, Raghavan, 2002</marker>
<rawString>James Allan and Hema Raghavan. 2002. Using partof-speech patterns to reduce query ambiguity. In Proceedings of SIGIR, pages 307–314.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Bartz</author>
<author>Cory Barr</author>
<author>Adil Aijaz</author>
</authors>
<title>Natural language generation in sponsored-search advertisements.</title>
<date>2008</date>
<booktitle>In Proceedings of the 9th ACM Conference on Electronic Commerce,</booktitle>
<pages>1--9</pages>
<location>Chicago, Illinois.</location>
<contexts>
<context position="21594" citStr="Bartz et al., 2008" startWordPosition="3389" endWordPosition="3392">tomatic Capitalization We saw in Section 2.3 that web searchers rarely use capitalization. We have also seen that a pretrained Brill tagger run on queries with perfect capitalization (“oracle” capitalization) can achieve 89.4% accuracy. We now look at how performance might be affected if we used an imperfect algorithm for capitalization. In order to attempt to capitalize the proper nouns in queries, we used a machine-learned system which searches for the query terms and examines how often they are capitalized in the search results, weighting each capitalization occurrence by various features (Bartz et al., 2008). Though the capitalization system provides 79.3% accuracy, using this system we see an only a small increase of accuracy in partof-speech tagging at 70.9%. This system does not improve significantly over the tagger trained on the lower-cased corpus. One explanation is that capitalization information of this type could only be obtained for 81.9% of our queries. Multiplied by accuracy, this implies that roughly 81.9% * 79.3% = 65.0% of our proper nouns are correctly cased. This suggests that any technique for proper-noun detection in search-engine queries must provide over 65.0% accuracy to see</context>
</contexts>
<marker>Bartz, Barr, Aijaz, 2008</marker>
<rawString>Kevin Bartz, Cory Barr, and Adil Aijaz. 2008. Natural language generation in sponsored-search advertisements. In Proceedings of the 9th ACM Conference on Electronic Commerce, pages 1–9, Chicago, Illinois.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Transformation-based error-driven learning and natural language processing: A case study in part-of-speech tagging.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>4</issue>
<contexts>
<context position="14740" citStr="Brill, 1995" startWordPosition="2308" endWordPosition="2309">n Section 3.4.2. 3 Tagger Accuracy on Search Queries To investigate automation of the tagging process, we trained taggers on our manually labeled query set. We used 10-fold cross-validation, with 90% of the data used for training and the remaining data used for testing. In the sections below, we used two datasets. The first consists of 1602 manually labeled queries. For the experiments in Section 3.5 we labeled additional queries, for a total of 2503 manually labeled queries. 3.1 Part-of-Speech Tagging Software We experimented with two freely available partof-speech taggers: The Brill Tagger (Brill, 1995) and The Stanford Tagger (Toutanova and Manning, 2000; Toutanova et al., 2003). The Brill tagger works in two stages. The initial tagger queries a lexicon and labels each token with its most common part-of-speech tag. If the token is not in the lexicon, it labels the token with a default tag, which was “proper noun” in our case. In the second stage, the tagger applies a set of lexical rules which examine prefixes, suffixes, and infixes. The tagger may then exchange the default tag based on lexical characteristics common to particular parts of speech. After application of lexical rules, a set o</context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>Eric Brill. 1995. Transformation-based error-driven learning and natural language processing: A case study in part-of-speech tagging. Computational Linguistics, 21(4):543–565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abdur Chowdhury</author>
<author>M Catherine McCabe</author>
</authors>
<title>Improving information retrieval systems using part of speech tagging.</title>
<date>2000</date>
<contexts>
<context position="3709" citStr="Chowdhury and McCabe, 2000" startWordPosition="542" endWordPosition="545">lecting words for omission or substitution in query reformulation, actions which can improve recall. We conclude that training a partof-speech tagger on labeled corpora of queries significantly outperforms taggers based on traditional corpora, and leveraging the unique linguistic structure of web-search queries can improve search experience. 1021 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1021–1030, Honolulu, October 2008. c�2008 Association for Computational Linguistics based weighting schemas and stopword detection (Crestani et al., 1998), (Chowdhury and McCabe, 2000), (Dincer and Karaoglan, 2004). Their experiments show degrees of success. Recently, along with term weighting, Lioma has been using partof-speech n-grams for noise and content detection in indexes (Lioma, 2008). Our study differs from these in that linguistic and part-of-speech focus is almost exclusively placed on queries as opposed to the indexed documents, reflecting our opinion that queries exhibit their own partially predictable and unique linguistic structure different from that of the natural language of indexed documents. Similarly, (Strzalkowski et al., 1998) added a layer of natural</context>
</contexts>
<marker>Chowdhury, McCabe, 2000</marker>
<rawString>Abdur Chowdhury and M. Catherine McCabe. 2000. Improving information retrieval systems using part of speech tagging.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Crestani</author>
<author>Mark Sanderson</author>
<author>Mounia Lalmas</author>
</authors>
<title>Short queries, natural language and spoken document retrieval: Experiments at glasgow university.</title>
<date>1998</date>
<booktitle>In Proceedings of the Sixth Text Retrieval Conference (TREC-6),</booktitle>
<pages>667--686</pages>
<contexts>
<context position="3679" citStr="Crestani et al., 1998" startWordPosition="538" endWordPosition="541">l use of the tagger in selecting words for omission or substitution in query reformulation, actions which can improve recall. We conclude that training a partof-speech tagger on labeled corpora of queries significantly outperforms taggers based on traditional corpora, and leveraging the unique linguistic structure of web-search queries can improve search experience. 1021 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1021–1030, Honolulu, October 2008. c�2008 Association for Computational Linguistics based weighting schemas and stopword detection (Crestani et al., 1998), (Chowdhury and McCabe, 2000), (Dincer and Karaoglan, 2004). Their experiments show degrees of success. Recently, along with term weighting, Lioma has been using partof-speech n-grams for noise and content detection in indexes (Lioma, 2008). Our study differs from these in that linguistic and part-of-speech focus is almost exclusively placed on queries as opposed to the indexed documents, reflecting our opinion that queries exhibit their own partially predictable and unique linguistic structure different from that of the natural language of indexed documents. Similarly, (Strzalkowski et al., </context>
</contexts>
<marker>Crestani, Sanderson, Lalmas, 1998</marker>
<rawString>Fabio Crestani, Mark Sanderson, and Mounia Lalmas. 1998. Short queries, natural language and spoken document retrieval: Experiments at glasgow university. In Proceedings of the Sixth Text Retrieval Conference (TREC-6), pages 667–686.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erika F de Lima</author>
<author>Jan O Pederson</author>
</authors>
<title>Phrase recognition and expansion for short, precision-biased queries based on a query log.</title>
<date>1999</date>
<booktitle>In Annual ACM Conference on Research and Development in Information Retrieval Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>145--152</pages>
<location>Berkeley, California.</location>
<marker>de Lima, Pederson, 1999</marker>
<rawString>Erika F. de Lima and Jan O. Pederson. 1999. Phrase recognition and expansion for short, precision-biased queries based on a query log. In Annual ACM Conference on Research and Development in Information Retrieval Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, pages 145–152, Berkeley, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bekir Taner Dincer</author>
<author>Bahar Karaoglan</author>
</authors>
<title>The effect of part-of-speech tagging on ir performance for turkish.</title>
<date>2004</date>
<pages>771--778</pages>
<contexts>
<context position="3739" citStr="Dincer and Karaoglan, 2004" startWordPosition="546" endWordPosition="549">substitution in query reformulation, actions which can improve recall. We conclude that training a partof-speech tagger on labeled corpora of queries significantly outperforms taggers based on traditional corpora, and leveraging the unique linguistic structure of web-search queries can improve search experience. 1021 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1021–1030, Honolulu, October 2008. c�2008 Association for Computational Linguistics based weighting schemas and stopword detection (Crestani et al., 1998), (Chowdhury and McCabe, 2000), (Dincer and Karaoglan, 2004). Their experiments show degrees of success. Recently, along with term weighting, Lioma has been using partof-speech n-grams for noise and content detection in indexes (Lioma, 2008). Our study differs from these in that linguistic and part-of-speech focus is almost exclusively placed on queries as opposed to the indexed documents, reflecting our opinion that queries exhibit their own partially predictable and unique linguistic structure different from that of the natural language of indexed documents. Similarly, (Strzalkowski et al., 1998) added a layer of natural language processing using par</context>
</contexts>
<marker>Dincer, Karaoglan, 2004</marker>
<rawString>Bekir Taner Dincer and Bahar Karaoglan. 2004. The effect of part-of-speech tagging on ir performance for turkish. pages 771–778.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard J Jansen</author>
<author>Amanda Spink</author>
<author>Tefko Saracevic</author>
</authors>
<title>Real life, real users, and real needs: a study and analysis of user queries on the web. Information Processing and</title>
<date>2000</date>
<journal>Management,</journal>
<volume>36</volume>
<issue>2</issue>
<contexts>
<context position="7124" citStr="Jansen et al., 2000" startWordPosition="1071" endWordPosition="1074">d collections of terms. Finally, we examine the potential use of tagging in the tasks of search relevance evaluation and query reformulation in Section 5. 2 Data We sampled queries from the Yahoo! search engine recorded in August 2006. Queries were systematically lower-cased and white-spaced normalized. We removed any query containing a nonASCII character. Queries were then passed through a high-precision proprietary query spelling corrector, followed by the Penn Treebank tokenizer. No other normalization was carried out. Despite Penntokenization, queries were typical in their average length (Jansen et al., 2000). We sampled 3,283 queries from our dataset to label, for a total of 2,508 unique queries comprised of 8,423 individual tokens. 2.1 Inter-rater Agreement The sparse textual information in search queries presents difficulties beyond standard corpora, not only for part-of-speech tagging software but also for human labelers. To quantify the level of these difficulties we measured inter-rater agreement on a set of 100 queries labeled by each editor. Since one labeler annotated 84.4% of the queries, we used a non-standard metric to determine agreement. One hundred queries were selected at random fr</context>
</contexts>
<marker>Jansen, Spink, Saracevic, 2000</marker>
<rawString>Bernard J. Jansen, Amanda Spink, and Tefko Saracevic. 2000. Real life, real users, and real needs: a study and analysis of user queries on the web. Information Processing and Management, 36(2):207–227.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christina Amalia Lioma</author>
</authors>
<title>Part of speech N-grams for information retrieval.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Glasgow,</institution>
<location>Glasgow, Scotland, UK.</location>
<contexts>
<context position="3920" citStr="Lioma, 2008" startWordPosition="576" endWordPosition="577">ditional corpora, and leveraging the unique linguistic structure of web-search queries can improve search experience. 1021 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1021–1030, Honolulu, October 2008. c�2008 Association for Computational Linguistics based weighting schemas and stopword detection (Crestani et al., 1998), (Chowdhury and McCabe, 2000), (Dincer and Karaoglan, 2004). Their experiments show degrees of success. Recently, along with term weighting, Lioma has been using partof-speech n-grams for noise and content detection in indexes (Lioma, 2008). Our study differs from these in that linguistic and part-of-speech focus is almost exclusively placed on queries as opposed to the indexed documents, reflecting our opinion that queries exhibit their own partially predictable and unique linguistic structure different from that of the natural language of indexed documents. Similarly, (Strzalkowski et al., 1998) added a layer of natural language processing using part-of-speech tags and syntactical parsing to the common statistical information-retrieval framework, much like experiments detailed in sections 4 and 5. Our system differs in that ou</context>
</contexts>
<marker>Lioma, 2008</marker>
<rawString>Christina Amalia Lioma. 2008. Part of speech N-grams for information retrieval. Ph.D. thesis, University of Glasgow, Glasgow, Scotland, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marius Pasca</author>
</authors>
<title>Weakly-supervised discovery of named entities using web search queries.</title>
<date>2007</date>
<booktitle>In CIKM,</booktitle>
<pages>683--690</pages>
<contexts>
<context position="5453" citStr="Pasca, 2007" startWordPosition="804" endWordPosition="805">r query tags to other tag sets, our system offers a complementary approach that can be used in tandem with the techniques referenced above. Lima and Pederson (de Lima and Pederson, 1999) conducted related work in which part-of-speech tagging using morphological analysis was used as a preprocessing step for labeling tokens of websearch queries before being parse by a probabilistic context-free grammar tuned to query syntax. We believe this technique and others relying on part-ofspeech tagging of queries could benefit from using a query-trained tagger prior to deeper linguistic analysis. Pasca (Pasca, 2007) showed that queries can be used as a linguistic resource for discovering named entities. In this paper we show that the majority of query terms are proper nouns, and the majority of queries are noun-phrases, which may explain the success of this data source for named-entity discovery. In this work, we use metrics that assume a unique correct part-of-speech tagging for each query, implicitly addressing the disambiguation issue through inter-annotator-agreement scores and tagger generalization error. To identify these tags, we first analyze the different general forms of queries. In Section 2 w</context>
</contexts>
<marker>Pasca, 2007</marker>
<rawString>Marius Pasca. 2007. Weakly-supervised discovery of named entities using web search queries. In CIKM, pages 683–690.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amanda Spink</author>
<author>B J Jansen</author>
<author>D Wolfram</author>
<author>T Saracevic</author>
</authors>
<title>From e-sex to e-commerce: Web search changes.</title>
<date>2002</date>
<journal>IEEE Computer,</journal>
<volume>35</volume>
<issue>3</issue>
<marker>Spink, Jansen, Wolfram, Saracevic, 2002</marker>
<rawString>Amanda Spink, B. J. Jansen, D. Wolfram, and T. Saracevic. 2002. From e-sex to e-commerce: Web search changes. IEEE Computer, 35(3):107–109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomek Strzalkowski</author>
<author>Jose Perez Carballo</author>
<author>Mihnea Marinescu</author>
</authors>
<title>Natural language information retrieval: Trec-3 report.</title>
<date>1998</date>
<booktitle>In Proceedings of the Sixth Text Retrieval Conference (TREC-6),</booktitle>
<pages>39</pages>
<contexts>
<context position="4284" citStr="Strzalkowski et al., 1998" startWordPosition="626" endWordPosition="629">restani et al., 1998), (Chowdhury and McCabe, 2000), (Dincer and Karaoglan, 2004). Their experiments show degrees of success. Recently, along with term weighting, Lioma has been using partof-speech n-grams for noise and content detection in indexes (Lioma, 2008). Our study differs from these in that linguistic and part-of-speech focus is almost exclusively placed on queries as opposed to the indexed documents, reflecting our opinion that queries exhibit their own partially predictable and unique linguistic structure different from that of the natural language of indexed documents. Similarly, (Strzalkowski et al., 1998) added a layer of natural language processing using part-of-speech tags and syntactical parsing to the common statistical information-retrieval framework, much like experiments detailed in sections 4 and 5. Our system differs in that our syntactic parsing system was applied to web-search queries and uses rules derived from the observed linguistic structure of queries as opposed to natural-language corpora. By focusing on the part-of-speech distribution and syntactic structure of queries over tagged indexed documents, with a simple bijection mapping our query tags to other tag sets, our system </context>
</contexts>
<marker>Strzalkowski, Carballo, Marinescu, 1998</marker>
<rawString>Tomek Strzalkowski, Jose Perez Carballo, and Mihnea Marinescu. 1998. Natural language information retrieval: Trec-3 report. In Proceedings of the Sixth Text Retrieval Conference (TREC-6), page 39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Christopher D Manning</author>
</authors>
<title>Enriching the knowledge sources used in a maximum entropy part-of-speech tagger.</title>
<date>2000</date>
<booktitle>In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-2000).</booktitle>
<contexts>
<context position="14793" citStr="Toutanova and Manning, 2000" startWordPosition="2314" endWordPosition="2317">Search Queries To investigate automation of the tagging process, we trained taggers on our manually labeled query set. We used 10-fold cross-validation, with 90% of the data used for training and the remaining data used for testing. In the sections below, we used two datasets. The first consists of 1602 manually labeled queries. For the experiments in Section 3.5 we labeled additional queries, for a total of 2503 manually labeled queries. 3.1 Part-of-Speech Tagging Software We experimented with two freely available partof-speech taggers: The Brill Tagger (Brill, 1995) and The Stanford Tagger (Toutanova and Manning, 2000; Toutanova et al., 2003). The Brill tagger works in two stages. The initial tagger queries a lexicon and labels each token with its most common part-of-speech tag. If the token is not in the lexicon, it labels the token with a default tag, which was “proper noun” in our case. In the second stage, the tagger applies a set of lexical rules which examine prefixes, suffixes, and infixes. The tagger may then exchange the default tag based on lexical characteristics common to particular parts of speech. After application of lexical rules, a set of contextual rules analyze surrounding tokens and the</context>
<context position="16461" citStr="Toutanova and Manning, 2000" startWordPosition="2572" endWordPosition="2575">. We also experimented with the Stanford LogLinear Part-of-Speech Tagger, which presently holds the best published performance in the field at 96.86% on the Penn Treebank corpus. It achieves this accuracy by expanding information sources for tagging. In particular, it provides “(i) more extensive treatment of capitalization for unknown words; (ii) features for the disambiguation of the tense forms of verbs; (iii) features for disambiguating particles from prepositions and adverbs.” It uses a maximum-entropy approach to handle information 1024 diversity without assuming predictor independence (Toutanova and Manning, 2000). 3.4 Improving Tagger Accuracy We conducted several experiments in improving tagger accuracy, summarized in Table 3 and described in detail below. 3.2 Baseline: Most Common Tag With proper nouns dominating the distribution, we first considered using the accuracy of labeling all tokens “proper noun” as a baseline. In this case, we labeled 1953 of 4759 (41.0%) tokens correctly. This is a significant improvement over the accuracy of tagging all words as “noun” on the Brown corpus (approximately 13%), reflecting the frequent occurrence of proper nouns in search queries. However, to examine the gr</context>
</contexts>
<marker>Toutanova, Manning, 2000</marker>
<rawString>Kristina Toutanova and Christopher D. Manning. 2000. Enriching the knowledge sources used in a maximum entropy part-of-speech tagger. In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-2000).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>252--259</pages>
<contexts>
<context position="14818" citStr="Toutanova et al., 2003" startWordPosition="2318" endWordPosition="2321"> automation of the tagging process, we trained taggers on our manually labeled query set. We used 10-fold cross-validation, with 90% of the data used for training and the remaining data used for testing. In the sections below, we used two datasets. The first consists of 1602 manually labeled queries. For the experiments in Section 3.5 we labeled additional queries, for a total of 2503 manually labeled queries. 3.1 Part-of-Speech Tagging Software We experimented with two freely available partof-speech taggers: The Brill Tagger (Brill, 1995) and The Stanford Tagger (Toutanova and Manning, 2000; Toutanova et al., 2003). The Brill tagger works in two stages. The initial tagger queries a lexicon and labels each token with its most common part-of-speech tag. If the token is not in the lexicon, it labels the token with a default tag, which was “proper noun” in our case. In the second stage, the tagger applies a set of lexical rules which examine prefixes, suffixes, and infixes. The tagger may then exchange the default tag based on lexical characteristics common to particular parts of speech. After application of lexical rules, a set of contextual rules analyze surrounding tokens and their parts of speech, alter</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In Proceedings of HLT-NAACL, pages 252–259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ingrid Zukerman</author>
<author>Bhavani Raskutti</author>
</authors>
<title>Lexical query paraphrasing for document retrieval.</title>
<date>2002</date>
<booktitle>In COLING,</booktitle>
<pages>1177--1183</pages>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="1567" citStr="Zukerman and Raskutti, 2002" startWordPosition="233" endWordPosition="236">erent documents are relevant depending on how this ambiguity is resolved. For example, the word “boat” in a query may be intended as subject of a verb, object of a verb, or as a verb, with each case reflecting a distinct intent. To distinguish between the possibilities, Allan and Raghavan (Allan and Raghavan, 2002) propose eliciting feedback from the user by showing them possible contexts for the query terms. In addition to disambiguating query terms for retrieval of suitable documents, part-of-speech tagging can help increase recall by facilitating query reformulation. Zukerman and Raskutti (Zukerman and Raskutti, 2002) part-of-speech tag well-formed questions, and use the part-of-speech tags to substitute synonyms for the content words. Several authors have leveraged part-of-speech tagging towards improved index construction for information retrieval through part-of-speechAbstract Web-search queries are known to be short, but little else is known about their structure. In this paper we investigate the applicability of part-of-speech tagging to typical Englishlanguage web search-engine queries and the potential value of these tags for improving search results. We begin by identifying a set of part-of-speech </context>
</contexts>
<marker>Zukerman, Raskutti, 2002</marker>
<rawString>Ingrid Zukerman and Bhavani Raskutti. 2002. Lexical query paraphrasing for document retrieval. In COLING, pages 1177–1183, Taipei, Taiwan.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>