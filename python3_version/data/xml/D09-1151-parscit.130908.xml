<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.996146">
A Probabilistic Model for Associative Anaphora Resolution
</title>
<author confidence="0.913538">
Ryohei Sasano and Sadao Kurohashi
</author>
<affiliation confidence="0.913759">
Graduate School of Informatics, Kyoto University,
</affiliation>
<address confidence="0.519416">
Yoshida-honmachi, Sakyo-ku, Kyoto
</address>
<email confidence="0.999042">
{sasano,kuro}@i.kyoto-u.ac.jp
</email>
<sectionHeader confidence="0.997392" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999705">
This paper proposes a probabilistic model
for associative anaphora resolution in
Japanese. Associative anaphora is a
type of bridging anaphora, in which the
anaphor and its antecedent are not coref-
erent. Our model regards associative
anaphora as a kind of zero anaphora and
resolves it in the same manner as zero
anaphora resolution using automatically
acquired lexical knowledge. Experimen-
tal results show that our model resolves
associative anaphora with good perfor-
mance and the performance is improved
by resolving it simultaneously with zero
anaphora.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9996983125">
The correct interpretation of anaphora is vital
for natural language understanding. Bridging
anaphora (Clark, 1975) represents a special part of
the general problem of anaphora resolution, which
has been studied and discussed for various lan-
guages and domains (Hahn et al., 1996; Murata et
al., 1999; Poesio et al., 2004; Gasperin and Vieira,
2004; Gasperin and Briscoe, 2008).
Usually bridging anaphora considers two
types:1 associative anaphors are noun phrases
(NPs) that have an antecedent that is necessary
to their interpretation but the relation between the
anaphor and its antecedent is different from iden-
tity; and indirect anaphors are those that have
an identity relation with their antecedents but the
anaphor and its antecedent have different head
</bodyText>
<footnote confidence="0.950341">
1The terminology that we use here is introduced by
Hawkins (1978), which is also used in (Vieira et al., 2006).
</footnote>
<bodyText confidence="0.997547428571429">
nouns. In this paper, we focus on associative
anaphora in Japanese.
Associative anaphora resolution is decomposed
into two steps: acquiring lexical knowledge for as-
sociative anaphora resolution, and resolving asso-
ciative anaphora using the acquired knowledge.
Grammatical salience plays a lesser role for
resolving anaphors with full lexical heads, than
for pronominal anaphora (Strube and Hahn, 1999;
Modjeska, 2002). Furthermore, since associative
anaphors and their antecedents usually have differ-
ent head nouns, string matching technique cannot
be applied. Therefore, a large and diverse amount
of lexical knowledge is essential to understand as-
sociative anaphora. For example, to recognize the
meronymic relation between “a house” and “the
roof” in (1), such knowledge as “a roof” is a part
of a building or vehicle is required. To recognize
the attributive relation between “Prius” and “the
price” in (2), such knowledge as “price” is a price
of some goods or service is required.
</bodyText>
<listItem confidence="0.991091">
(1) There was a house. The roof was white.
(2) Toyota launched the hybrid car Prius in
1997. The price was 21.5 million yen.
</listItem>
<bodyText confidence="0.999902166666667">
To acquire such lexical knowledge, various
studies have been carried out. Early studies used
hand-crafted lexical knowledge such as Word-
Net (Strube and Hahn, 1999; Vieira and Poe-
sio, 2000; Meyer and Dale, 2002), but obtained
poor or mediocre results. Hence, Poesio et al.
(2002) proposed to exploit “Nh of Nm” phrases
in large corpora to resolve associative anaphora
in English; Murata et al. (1999) proposed to ex-
ploit “Nm no Nh” phrases to resolve associative
anaphora in Japanese. Here, the Japanese postpo-
sition “no” roughly corresponds to “of,” but it has
</bodyText>
<page confidence="0.924135">
1455
</page>
<note confidence="0.9965555">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1455–1464,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999915628571429">
much broader usage. These studies obtained rea-
sonable results, but the coverage of the acquired
knowledge was not sufficient. Recently, a num-
ber of researchers argued for using the Web as a
source of lexical knowledge, and the Web has been
shown to be a useful resource for anaphora resolu-
tion (Bunescu, 2003; Markert et al., 2003; Poesio
et al., 2004).
Hence, in this study, we acquire the lexi-
cal knowledge for associative anaphora resolution
from “Nm no Nh” phrases in the Web by using the
method described in (Sasano et al., 2004). We pro-
posed a method for acquiring such lexical knowl-
edge, called nominal case frames (NCFs), using
an ordinary language dictionary and “Nm no Nh”
phrases, and constructed NCFs from newspaper
corpora. In this study, we aim to acquire a suffi-
cient amount of lexical knowledge by constructing
NCFs from the Web.
As for associative anaphora resolution itself, we
propose an integrated probabilistic model for zero
anaphora and associative anaphora resolution, in
which associative anaphora is regarded as a kind
of zero anaphora and resolved by using the same
model as zero anaphora. Our model assumes zero
pronouns that represent indispensable entities of
target noun phrases, which are called zero adnom-
inal in (Yamura-Takei, 2003), and conducts zero
pronoun resolution.
Let us consider the associative anaphoric re-
lation between “Prius” and “kakaku” (price).
Although “kakaku” itself is considered as the
anaphor from a conventional point of view (3a),
our model assumes a zero pronoun 0 and consid-
ers it as the anaphor (3b).
</bodyText>
<listItem confidence="0.8938575">
(3) a. Prius - kakaku (price)
[antecedent: Prius, anaphor: kakaku (price)]
b. Prius - (0-no) kakaku (price (of 0))
[antecedent: Prius, anaphor: φ]
</listItem>
<bodyText confidence="0.999327">
The point of this study is three-fold: the ac-
quisition of the lexical knowledge for associative
anaphora resolution from the Web, the application
of zero anaphora resolution model to associative
anaphora resolution, and the integrated resolution
of zero anaphora and associative anaphora.
</bodyText>
<sectionHeader confidence="0.849138" genericHeader="method">
2 Construction of Nominal Case Frames
</sectionHeader>
<bodyText confidence="0.999593142857143">
Most nouns have their indispensable entities:
“price” is a price of some goods or service, “roof”
is a roof of some building, and “coach” is a coach
of some sports. The relation between a noun and
its indispensable entities is parallel to that between
a verb and its arguments or obligatory cases. In
this paper, we call indispensable entities of nouns
obligatory cases. Note that, obligatory does not
mean grammatically obligatory but obligatory to
interpret the meaning of the noun. Associative
anaphora resolution needs comprehensive infor-
mation of obligatory cases of nouns. Nominal case
frames (NCFs) describe such information, and we
construct them from the Web.
</bodyText>
<subsectionHeader confidence="0.988868">
2.1 Automatic Construction of NCFs
</subsectionHeader>
<bodyText confidence="0.9997964375">
First, we briefly introduce our method for con-
structing NCFs from raw corpora proposed in
(Sasano et al., 2004).
Whereas verbal case frame construction uses ar-
guments of each verb (Kawahara and Kurohashi,
2002), nominal case frame construction basically
uses adnominal constituents of each noun. How-
ever, while the meaning of a verbal argument can
be distinguished by the postposition, such as “ga”
(nominative), “wo” (accusative), and “ni” (dative),
the meaning of an adnominal constituent can not
be distinguished easily, because most adnominal
constituents appear with the same postposition
“no” (of). Thus, we first conduct a semantic anal-
ysis of adnominal constituents, and then construct
NCFs using the results as follows:
</bodyText>
<listItem confidence="0.9941284">
1. Collect syntactically unambiguous noun
phrases “Nm no Nh” from the automatic re-
sulting parses of large corpora.
2. Analyze the relation between Nm and Nh
by Kurohashi and Sakai’s method (1999) that
exploits an ordinary language dictionary.
3. Depending on the results, classify Nm, and
obtain preliminary case slots for Nh.
4. Merge case slots if two preliminary case slots
of Nh are similar.
5. Consider frequent case slots as obligatory
cases of Nh. The frequency thresholds are
varied according to semantic analyses.
6. For each meaning of Nh, collect case slots
and construct case frames.
</listItem>
<bodyText confidence="0.9949685">
The point of this method is the integrated use of
an ordinary dictionary and example phrases from
</bodyText>
<page confidence="0.998705">
1456
</page>
<tableCaption confidence="0.999841">
Table 1: Examples of constructed nominal case frames.
</tableCaption>
<table confidence="0.948914125">
Case slot Examples with freq Generalized examples with rate
kakaku (1) Definition: the amount of money you have to pay for something.
(price)
[something] shˆohin(goods):9289, seihin(product):2520, [CT:ARTIFACT]:0.93, · · ·
buhin(part):341, yunyuhin(importation):232, · · ·
yane (1) Definition: the structure that covers or forms the top of a building etc.
(roof)
[building] ie(house):2505, kuruma(car):1565, koya(hut):895, [CT:FACILITY]:0.44,
tatemono(building):883,minka(private house):679, · · · [CT:VEHICLE]:0.13,· · ·
shusho (1) Definition: the elected leader of the government in a country that has a parliament.
(prime minister)
[country] nihon(Japan):2355, kuni(country):272, [NE:LOCATION]:0.82,
doitsu(Germany):157, chˆugoku(China):130, · · · [CT:VEHICLE]:0.13,· · ·
imouto (1) Definition: a girl or woman who has the same parents as you.
(sister)
&lt;relationship&gt; watashi(me):3385, ore(me):1188, boku(me):898, [CT:PERSON]:0.74,
jibun(oneself):341, tomodachi(friend):537, · · · [NE:PERSON]:0.22, · · ·
rebˆa(1) Definition: a stick or handle on a machine.
(lever)
[machine] buˆreki(brake):122, sokketo(sochet):67, [CT:ARTIFACT]:0.61,
waipˆa(wiper):54, souchi(device):52,· · · [CT:VEHICLE]:0.04, · · ·
rebˆa(2) Definition: the liver of an animal, used as food.
(liver)
[animal] niwatori(chicken):153, buta(pig):153, [CT:ANIMAL]:0.98, · · ·
ushi(cattle):62, doubutsu(animal):25,· · ·
senshu(1) Definition: someone who takes part in a sport. ·
(player) · ·
[sport] yakyˆu(baseball):1252, rirˆe(relay):736, [CT:ABSTRACTION]:0.56, · ·
kyˆogi(competition):430, sakkˆa(soccer):394, ···
&lt;affiliation&gt; chˆımu(team):4409, nihon(Japan):3222, [NE:LOCATION]:0.33,
reddu(Reds):771, kankoku(Korea):644,rˆıgu(league) · · · [CT:ORGANIZATION]:0.30, ·
* “[]” and “&lt;&gt;” denote dictionary-based and semantic feature-based analysis respectively. For details see (Sasano et al., 2004).
</table>
<bodyText confidence="0.999471333333333">
large corpora. Dictionary definition sentences are
an informative resource to recognize obligatory
cases of nouns. However, it is difficult to resolve
associative anaphora by using a dictionary as it is,
because all nouns in a definition sentence are not
an obligatory case, and only the frequency infor-
mation of noun phrases tells us which is the oblig-
atory case. On the other hand, a simple method
that just collects and clusters “Nm no Nh” phrases
based on some similarity measure of nouns cannot
construct comprehensive nominal case frames, be-
cause of polysemy and multiple obligatory cases.
For details see (Sasano et al., 2004).
It is desirable to use a probability distribution
for deciding whether a case slot is obligatory or
not. However, it is difficult to estimate a prob-
ability distribution, since we construct nominal
case frames not by using the examples of associa-
tive anaphora itself but by using the examples of
noun phrases “Nm no Nh” (Nh of Nm). We use
such noun phrases because indispensable entities
of noun ”Nh” often appear as ”Nm.” However, we
can say neither frequently appeared ”Nm” is an in-
dispensable entity of ”Nh.” nor an indispensable
entity frequently appears as ”Nm.” For example,
the name of a country is considered as an indis-
pensable entity of ”shusho” (prime minister), but
does not frequently appear as ”Nm.”2 Thus, it is
difficult to estimate a probability distribution and
we use a hard decision.
</bodyText>
<subsectionHeader confidence="0.995286">
2.2 NCF Construction from the Web
</subsectionHeader>
<bodyText confidence="0.9999926">
We constructed nominal case frames from the Web
Corpus (Kawahara and Kurohashi, 2006), which
comprises 1.6 billion unique Japanese sentences.
In this corpus, there were about 390 million noun
phrases “Nm no Nh,” about 100 million unique
noun phrases, and about 17 million unique head
nouns “Nh.” There were about 4.07 million head
nouns that appeared more than 10 times in the cor-
pus, and we used only such head nouns.
The resultant nominal case frames consisted of
about 564,000 nouns including compound nouns.
We show examples of constructed nominal case
frames in Table 1. The average number of case
frames for a noun that has case frames was 1.0031,
and the average number of case slots for a case
frame was 1.0101. However, these statistics dif-
fered with the frequency of the noun. Therefore,
we investigated the statistics of constructed nom-
inal case frames for each group classified by the
frequency of the nouns. Table 2 shows the re-
</bodyText>
<footnote confidence="0.9974305">
2It is because “the prime minister of Japan” is often men-
tioned by simply “the prime minister” in Japanese.
</footnote>
<page confidence="0.996688">
1457
</page>
<tableCaption confidence="0.999742">
Table 2: The statistics of constructed NCFs.
</tableCaption>
<table confidence="0.999754">
Frequency Proportion # of NCFs # of CSs Coverage
ranking of nouns per noun per NCF
with NCF with NCF
-100 56.0% 1.34 1.07 17.3%
-1000 68.8% 1.17 1.16 25.6%
-10000 51.7% 1.11 1.17 27.0%
-100000 14.8% 1.05 1.13 17.6%
100001- 13.7% 1.0009 1.0053 12.5%
all 13.9% 1.0031 1.0101 100%
</table>
<tableCaption confidence="0.999126">
Table 3: Evaluation of constructed NCFs.
</tableCaption>
<table confidence="0.7631885">
Precision Recall F-measure
62/70 (0.89) 62/84 (0.74) 0.81
</table>
<bodyText confidence="0.999949916666667">
sult. As for the 10,000 most frequently appeared
nouns, which occupied about 70% of all noun ap-
pearances, the average number of case frames for
a noun was 1.11, and the average number of case
slots for a case frame was 1.17.
For evaluating the resultant case frames, we ran-
domly selected 100 nouns from the 10,000 most
frequent nouns, and created gold standard case
frames for these nouns by hand. For each noun,
case frames were given if the noun was considered
to have any indispensable entity, and for each case
frame, obligatory case slots were given manually:
70 case frames were created that had 84 case slots;
56 case frames had only one case slot, the other 14
case frames had two case slots. 30 nouns had no
case frames.
We then evaluated the automatically con-
structed case slots for these selected nouns. The
evaluation result is shown in Table 3: the sys-
tem output 70 case slots, and out of them, 62 case
frames were judged as correct. The F-measure was
0.81. Since the boundary between indispensable
cases and optional cases of a noun is not always
obvious, this score is considered to be reasonable.
</bodyText>
<subsectionHeader confidence="0.999783">
2.3 Generalization of Examples
</subsectionHeader>
<bodyText confidence="0.999950035714286">
By using nominal case frames constructed from
the Web, sparseness problem was alleviated to
some extent, but still remained. For instance, there
were thousands of named entities (NEs), which
could not be covered intrinsically. To deal with
this sparseness problem, we generalized the exam-
ples of case slots.
First, we used the categories that Japanese mor-
phological analyzer JUMAN3 adds to common
nouns. In JUMAN, about twenty categories are
defined and tagged to common nouns. For ex-
ample, “kuruma (car),” “niwatori (chicken),” and
“tatemono (building)” are tagged as “VEHICLE,”
“ANIMAL” and “FACILITY,” respectively. For
each category, we calculated the rate of catego-
rized examples among all case slot examples, and
added it to the case slot as “[CT:VEHICLE]:0.13.”
We also generalized NEs. We used a com-
mon standard NE definition for Japanese pro-
vided by IREX workshop (1999). We first rec-
ognized NEs in the source corpus by using an
NE recognizer (Sasano and Kurohashi, 2008), and
then constructed NCFs from the NE-recognized
corpus. As well as categories, for each NE
class, we calculated the NE rate among all case
slot examples, and added it to the case slot as
“[NE:PERSON]:0.22.” The generalized examples
are also included in Table 1.
</bodyText>
<sectionHeader confidence="0.998868" genericHeader="method">
3 Probabilistic Model
</sectionHeader>
<bodyText confidence="0.9997475">
In this study, we apply a lexicalized probabilis-
tic model for zero anaphora resolution proposed in
(Sasano et al., 2008) to associative anaphora reso-
lution.
</bodyText>
<subsectionHeader confidence="0.999034">
3.1 A Lexicalized Probabilistic Model for
Zero Anaphora Resolution
</subsectionHeader>
<bodyText confidence="0.999982538461538">
In English, overt pronouns such as “she” and
definite noun phrases such as “the company”
are anaphors that refer to preceding entities (an-
tecedents). On the other hand, in Japanese,
anaphors are often omitted, which are called zero
pronouns, and zero anaphora resolution is one of
the most important techniques for semantic analy-
sis in Japanese.
Here, we introduce our model for zero anaphora
resolution (Sasano et al., 2008). This model first
resolves coreference and identifies discourse enti-
ties; then from the end of each sentence, analyzes
each predicate by the following steps:
</bodyText>
<listItem confidence="0.992959333333333">
1. Select a case frame temporarily.
2. Consider all possible correspondences be-
tween each input argument and a case slot of
the selected case frame.
3. Regard case slots that have no correspon-
dence as zero pronoun candidates.
</listItem>
<footnote confidence="0.870759">
3http://nlp.kuee.kyoto-u.ac.jp/nl-resource/juman-e.html
</footnote>
<page confidence="0.974681">
1458
</page>
<listItem confidence="0.905412285714286">
4. Consider all possible correspondences be-
tween zero pronoun candidates and existing
entities.
5. For each possible case frame, estimate each
correspondence probabilistically, and select
the most likely case frame and correspon-
dence.
</listItem>
<bodyText confidence="0.972100727272727">
Figure 1 shows an example of correspondences
between case frames and discourse entities.
The probabilistic model gives a probability to
each possible case frame CF and case assignment
CA when target predicate v, input arguments IA
and existing discourse entities ENT are given,
and outputs the case frame and case assignment
that have the highest probability. That is to say,
their model selects the case frame CFbest and the
case assignment CAbest that maximize the proba-
bility P(CF, CA|v, IA, ENT):
</bodyText>
<equation confidence="0.967358666666667">
(CFbest, CAbest)
= argmax P(CF, CA|v, IA, ENT) (i)
CF,CA
</equation>
<bodyText confidence="0.987379857142857">
By decomposing case assignment (CA) into
direct case assignment (DCA) and the indirect
case assignment (ICA) and using several inde-
pendence assumptions, Equation (i) is transformed
into the following equation:4
Here, P(CFl|v) denotes the probability to se-
lect CFl when target predicate v is given, and es-
timated by using case structure analysis of large
raw corpora.
P(DCAk, IA|CFl) denotes the probability to
generate direct case assignment and input argu-
ments when a case frame is given, and estimated
by using case structure analysis of large raw cor-
pora, the frequency of a case slot example in the
automatically constructed verbal case frames, and
the web corpus in which the relation between a
surface case marker and a case slot is manually
annotated.
P(ICAk|ENT, CFl, DCAk) denotes the
probability to generate indirect case assignment
when existing discourse entities, a case frame and
</bodyText>
<footnote confidence="0.7140875">
4For details see (Sasano et al., 2008).
Toyota launched the hybrid car Prius in 1997. Φstarted selling Φ2 overseas in 2000.
</footnote>
<figureCaption confidence="0.98614">
Figure 1: An example of correspondences be-
tween verbal case frames and discourse entities.
</figureCaption>
<bodyText confidence="0.998738764705882">
direct case assignments are given, and estimated
by using several preferences on the relation
between a zero pronoun and an antecedent, such
as a lexical preference, a surface case preferences,
and a locational preference.
For example, the lexical preference represents
how likely an entity that contains nj. as a con-
tent part is considered to be an antecedent and is
estimated by the following equation.
where, the function A&apos;(sj) returns 1 if a case slot
sj is filled with an antecedent of a zero pronoun;
otherwise 0. P(nj|CFl, sj, A&apos;(sj) = 1) is calcu-
lated by using case frames and denotes the proba-
bility of generating a content part nj of a zero pro-
noun, when a case frame and a case slot are given
and the case slot is filled with an antecedent of a
zero pronoun.
</bodyText>
<subsectionHeader confidence="0.8806945">
3.2 Extension to Associative Anaphora
Resolution
</subsectionHeader>
<bodyText confidence="0.999961416666667">
We then extend this probabilistic model to associa-
tive anaphora resolution. In this model, associative
anaphora is regarded as a kind of zero anaphora,
that is, the relation between a noun and its oblig-
atory cases is considered to be parallel to that be-
tween a verb and its arguments. Omitted obliga-
tory cases are considered to be zero pronouns and
resolved by the same process as zero anaphora res-
olution.
We conduct associative anaphora resolution for
only non-coreferent noun phrases. This is because
most of the relationships between coreferent noun
</bodyText>
<figure confidence="0.972851578947369">
{1997-nen}
{Toyota, Φ}
{hybrid car,
Prius, Φ2 }
{2000-nen}
{kaigai}
Entities
:direct case assignment
:indirect case assignment (zero anaphora)
2000-nen-karawa
Toyota-wa
Input sentences
hybrid car
(overseas)
kaigai-demo
1997-nen
hanbai-shiteiru.
Prius-wo
(sell)
hatsubai.
(launch)
Verbal case frames
ga
nominative
ga
nominative
wo
accusative
wo
accusative
hatsubai (launch)
hanbai (sell)
de
locative
de
locative
ni
dative
</figure>
<table confidence="0.935123">
company, Microsoft, ...
[NE:ORGANIZATION] 0.16, ...
goods, product, ticket, ...
[CT:ARTIFACT] 0.40, ...
customer, company, user, ...
[CT:PERSON] 0.28, ...
shop, bookstore, site, ...
[CT:FACILITY] 0.40, ...
company, SONY, firm, ...
[NE:ORGANIZATION] 0.15, ...
area, shop, world, Japan, ...
[CT:FACILITY] 0.13, ...
product, CD, model, car, ...
[CT:ARTIFACT] 0.40, ...
</table>
<equation confidence="0.983457714285714">
(CFbest,DCAbest,ICAbest) =
(argmax P(CF|v) × P(DCA, IA|CF)
CF,DCA,ICA
)×P(ICA|ENT, CF, DCA) (ii)
P(njm|CFl, sj, A&apos;(sj)=1)
(iii)
P(njm)
</equation>
<page confidence="0.965162">
1459
</page>
<bodyText confidence="0.497924">
Toyota launched the hybrid car Prius ・・・. The initial price of Φ2 was 21.5 million yen.
</bodyText>
<figureCaption confidence="0.8962715">
Figure 2: An example of correspondences be-
tween a nominal case frame and discourse entities.
</figureCaption>
<bodyText confidence="0.951862428571429">
phrases and its obligatory entities are easy to rec-
ognize by following up the coreference chains.
For example, the second appearance of ”the roof”
in (4) means ”the roof of the house,” and it is
easy to recognize by looking the first appearance
of ”the roof.”
(4) I saw the roof of the house. The roof was
painted dark green.
While verbal case frames describe both obliga-
tory and optional cases, nominal case frames de-
scribe only obligatory cases. Therefore, we con-
sider all case slots of nominal case frames as the
target of associative anaphora resolution.
Let us consider following example:
</bodyText>
<figure confidence="0.668580714285714">
(5) Toyota-wa 1997-nen hybrid car Prius-wo
year
hatsubai. 2000-nen-kara-wa kaigai-demo
launched year overseas
hanbai-shiteiru. Hatsubai tosho,
selling initial
(φ-no) kakaku-wa 215-man yen-datta.
</figure>
<figureCaption confidence="0.42533">
price ten thousands
</figureCaption>
<bodyText confidence="0.992399857142857">
(Toyota1 launched the hybrid car Prius2 in 1997. 01
started selling 02 overseas in 2000. The initial price
of 02 was 21.5 million yen.)
“Kakaku” (price) in this example has an omitted
obligatory case “[something]” as shown in Table
1. Therefore, our model assumes a zero pronoun
and identifies the antecedent from the existing dis-
course entities, such as {Toyota}, {hybrid-car,
Prius},5 and {kaigai}. Figure 2 shows an exam-
ple of correspondences between the nominal case
frame of “kakaku” (price) and discourse entities.
In addition, as well as zero anaphora resolution,
we exploit generalized examples to estimate lexi-
cal preference. When one mention of an entity is
</bodyText>
<footnote confidence="0.50497">
5“Hybrid car” and “Prius” are in apposition and these two
phrases are considered to refer to the same discourse entity.
</footnote>
<bodyText confidence="0.9996255">
tagged any category or recognized as an NE, our
model also uses the category or the NE class as the
content part of the entity. Specifically, for estimat-
ing Equation (iii), our model also calculates:
</bodyText>
<equation confidence="0.999242">
P(NE:ARTIFACT |kakaku(1), no, A&apos;(no)=1)
P(NE:ARTIFACT)
besides:
P(Prius|kakaku(1), no, A&apos;(no) = 1)
P(Prius)
</equation>
<bodyText confidence="0.966581">
and uses the geometric mean of them.
</bodyText>
<subsectionHeader confidence="0.995609">
3.3 Salience Score Filtering
</subsectionHeader>
<bodyText confidence="0.999883333333333">
Previous work has reported the usefulness of
salience for anaphora resolution (Lappin and Le-
ass, 1994; Mitkov et al., 2002). In order to con-
sider the salience of a discourse entity, we intro-
duce the concept of salience score, which is calcu-
lated by the following set of simple rules, and only
consider the entities that have the salience score no
less than 1 as candidate antecedents of an associa-
tive anaphor.
</bodyText>
<listItem confidence="0.9997516">
• +2 : mentioned with topical marker “wa,” or
at the end of a sentence.
• +1 : mentioned without topical marker “wa.”
• +1 : assigned to a zero pronoun.
• xα : beginning of each sentence.
</listItem>
<bodyText confidence="0.99990175">
We call α a decay rate. If α ≥ 1, we do not
filter out any entities. If α = 0, we only consider
the entities that appears in the same sentence as
candidate antecedents. For example, we consider
the salience score of the discourse entity {hybrid-
car, Prius} in the example (5) when using α=0.6.
In the first sentence, since {hybrid-car, Prius} is
mentioned twice, the salience score is 2.0. At the
beginning of the second sentence it becomes 1.2,
and after the zero anaphora resolution of “hanbai”
it becomes 2.2. At the beginning of the third sen-
tence it becomes 1.32.
Note that, this is an ideal case. Practically, some
zero pronouns are not detected and some pronouns
are assigned wrong antecedent; thus the salience
score varies according to the preceding analysis.
In addition, the salience score also depends on
whether we resolve only associative anaphora or
resolve associative anaphora simultaneously with
zero anaphora. If zero pronoun resolution is not
</bodyText>
<figure confidence="0.970853444444444">
something
goods, product, part,
importation, ...
[CT:ARTIFCAT] 0.40, ...
Nominal case frames
kakaku (price)
:indirect case assignment
(associative anaphora)
215-man-yen-datta.
(ten thousands)
{1997-nen}
{Toyota, Φ}
{hybrid car,
Prius, Φ2 }
{215-man-
yen}
{kaigai}
Entities
Input sentences
Toyota-wa
hybrid car
(initial)
(price)
1997-nen
Prius-wo
Hatsubai-tosho
kakaku-wa
</figure>
<page confidence="0.971548">
1460
</page>
<bodyText confidence="0.996880928571429">
conducted, zero pronouns that represent omitted
cases of verbs are not considered.
For example, in case of {hybrid-car, Prius}
with α = 0.6, if zero anaphora resolution is not
conducted, the salience score at the beginning of
the third sentence becomes 0.72, because the zero
anaphora resolution of “hanbai” is not considered;
and thus {hybrid-car, Prius} is not considered as
an antecedent candidate.
In order to recognize discourse structure more
properly, our model basically resolves associa-
tive anaphora simultaneously with zero anaphora,
and aims to consider zero pronouns that represent
omitted cases of verbs.
</bodyText>
<subsectionHeader confidence="0.992758">
3.4 Summary of Our model
</subsectionHeader>
<bodyText confidence="0.993622">
Our model is summarized as follows:
</bodyText>
<listItem confidence="0.970029285714286">
1. Parse an input text using the Japanese parser
KNP6 and recognize NEs.
2. Resolve coreference, and link each mention
to an entity or create a new entity.
3. From the end of each sentence, zero anaphora
and associative anaphora resolution is con-
ducted for each verb and non-coreferent noun
by the following steps:
(a) Select a case frame temporarily.
(b) Consider all possible correspondences
between each input argument and a case
slot of the selected case frame.
(c) Regard case slots that have no corre-
spondence as zero pronoun candidates.
(d) Consider all possible correspondences
between zero pronoun candidates and
existing entities that has a salience score
no less than 1.0.
(e) Estimate each correspondence proba-
bilistically, and select the most likely
case frame and a correspondence.
</listItem>
<sectionHeader confidence="0.998317" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.997134">
4.1 Setting
</subsectionHeader>
<bodyText confidence="0.9996768">
We created an anaphoric relation-tagged corpus
consisting of 186 web documents (979 sentences),
in which all predicate-argument relations and re-
lations between nouns were manually tagged. We
show some examples:
</bodyText>
<footnote confidence="0.639717">
6http://nlp.kuee.kyoto-u.ac.jp/nl-resource/knp-e.html
</footnote>
<figure confidence="0.9439888">
(6) Toyota-wa 1997-nen Prius-wo hatsubai.
year launch
2000-nen-kara-wa kaigai-demo hanbai.
year overseas sell
(Toyota launched Prius in 1997.
01 started selling 02 overseas in 2000.)
TAG: hatsubai ⇐ ga:Toyota, wo:Prius,
(NOM) (ACC)
hanbai ⇐ ga:Toyota, wo:Prius
(NOM) (ACC)
</figure>
<bodyText confidence="0.999626368421053">
For the predicate “hatsubai” (launch), “Toyota”
is tagged as ga (nominative) case and “Prius” is
tagged as wo (accusative) case. For the predicate
“hanbai” (sell), “Toyota” is tagged as omitted ga
(nominative) case and “Prius” is tagged as omit-
ted wo (accusative) case, which are indicated in
bold, and such omitted cases are the target of zero
anaphora resolution.
As for relations between nouns, both overt and
implicit relations are tagged with the Japanese
case marker “no” (adnominal). In addition, rela-
tions between nouns are classified into three cate-
gories: indispensable, possible, and adjunct. Since
it is not always obvious whether the relations are
indispensable or not, borderline relations between
indispensable and adjunct are tagged possible. We
consider only the implicit relations that are tagged
indispensable as the target of associative anaphora
resolution.
</bodyText>
<figure confidence="0.9445629">
(7) Ken-wa imouto-to yatte-kita.
sister came.
(Ken came with 0’s sister.)
TAG: imouto ⇐ no:Ken (indispensable)
(ADN)
(8) Kˆoen-ni ikuto benchi-ga atta.
park went bench was
(I went to the park. There was a bench in 0.)
TAG: benchi ⇐ no:kˆoen (possible)
(ADN)
</figure>
<bodyText confidence="0.999595384615385">
We used 62 documents for testing and used the
other 124 documents for calculating several prob-
abilities. In the 62 test documents, 110 associa-
tive anaphoric relations were tagged. Each param-
eter for the proposed model was estimated using
maximum likelihood from raw corpora, the tagged
corpus, and case frames. As verbal case frames,
we used the case frames constructed from the Web
corpus comprising 1.6 billion sentences (Sasano et
al., 2009).
In order to concentrate on associative anaphora
resolution, we used the correct morphemes, named
entities, syntactic structures, and coreference re-
</bodyText>
<page confidence="0.985299">
1461
</page>
<figure confidence="0.9253415">
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Decay Rate α
</figure>
<figureCaption confidence="0.998815">
Figure 3: Experimental results of associative
anaphora resolution on several salience decay
rates α.
</figureCaption>
<bodyText confidence="0.999751">
lations that were annotated by hand. Since cor-
rect coreference relations were given, the number
of created entities was the same between the gold
standard data and the system output because zero
anaphora and associative anaphora resolution did
not create new entities.
</bodyText>
<subsectionHeader confidence="0.647581">
4.2 Results
</subsectionHeader>
<bodyText confidence="0.990908928571429">
Figure 3 shows the experimental results of asso-
ciative anaphora resolution, in which we used gen-
eralized examples, resolved zero anaphora auto-
matically, and varied the decay rate α introduced
in Section 3.3 from 0 to 1. When we used the de-
cay rates smaller than 0.5, the recall score wors-
ened clearly. On the other hand, although we ex-
pected to obtain higher precision with small decay
rate, the highest precision was achieved by the de-
cay rate 0.5. Consequently, we obtained the high-
est F-measure of 0.427 with the decay rate 0.5. In
the following experiments, we fixed the decay rate
0.5.
We utilized two baseline models for demon-
strating the effectiveness of our approach: a ran-
dom model and a salience-based model. The ran-
dom model selects a case frame and its correspon-
dence randomly from all possible case frames and
correspondences. The salience-based model se-
lects a case frame and its correspondence that as-
sign a zero pronoun candidate the existing entity
that have highest salience score. In addition, in or-
der to confirm the effectiveness of generalized ex-
amples of NCFs, we conducted experiments with-
out using generalized examples. Table 4 shows
the experimental results. We can confirm that our
proposed model outperforms two baseline mod-
els. Without using any generalized examples, the
</bodyText>
<tableCaption confidence="0.661389333333333">
Table 4: Experimental results of associative
anaphora resolution with two baseline models and
our model with/without generalized examples.
</tableCaption>
<table confidence="0.956532611111111">
Model Recall Precision F-measure
Random* 0.148 0.035 0.056
(16.3/110) (16.3/467.5)
Salience- 0.400 0.135 0.202
based (44/110) (44/325)
Proposed
CT NE
0.318 0.257 0.285
(35/110) (35/136)
√ 0.345 0.268 0.302
(38/110) (38/142)
√ 0.464 0.333 0.388
(51/110) (51/153)
√ √ 0.518 0.363 0.427
(57/110) (57/157)
CT: Using examples generalized by categories.
NE: Using examples generalized by named entities.
* The average of 10 trials is shown.
</table>
<bodyText confidence="0.999946419354839">
F-measure was about 0.14 lower than the method
using generalized examples, and we can also con-
firm the effectiveness of the generalized examples.
While generalization of categories much improved
the F-measure, generalization of NEs contributed
little. This is because the NE rate was smaller than
the common noun rate, and so the effect was lim-
ited. This tendency was also seen in zero anaphora
resolution (Sasano et al., 2008).
In order to investigate the effects of zero
anaphora resolution, we tested our model under
three conditions: without zero anaphora resolu-
tion (no resolution), with zero anaphora resolution
(automatically resolved), and with using correct
zero anaphora relations that are manually tagged
(manually identified). The performance of auto-
matic zero anaphora resolution resulted in a recall
of 0.353, a precision of 0.375, and an F-measure of
0.364. Table 5 shows the experimental results. To
resolve associative anaphora simultaneously with
zero anaphora improved F-measure by 0.072; us-
ing correct zero anaphora relations improved F-
measure by 0.103. We can confirm that the per-
formance of associative anaphora resolution is im-
proved by considering zero anaphora.
Note that, strictly speaking, these comparisons
are not fair because we set the decay rate α to max-
imize the performance when using generalized ex-
amples and resolving zero anaphora automatically.
However, these tendencies described above were
also seen with other decay rates.
</bodyText>
<figure confidence="0.994371">
.427
Recall
F-measure
Precision
0.55
0.50
0.45
0.40
0.35
0.30
0.25
</figure>
<page confidence="0.99073">
1462
</page>
<tableCaption confidence="0.999663">
Table 5: The effects of zero anaphora resolution.
</tableCaption>
<table confidence="0.998156428571429">
Zero anaphora Recall Precision F-measure
No resolution 0.373 0.339 0.355
(41/110) (41/121)
Automatically 0.518 0.363 0.427
resolved (57/110) (57/157)
Manually 0.573 0.382 0.458
identified (63/110) (63/165)
</table>
<subsectionHeader confidence="0.944876">
4.3 Discussion
</subsectionHeader>
<bodyText confidence="0.999978523809524">
By using generalized examples and resolving
simultaneously with zero anaphora, our model
achieved a recall of 0.518 (57/110), but there were
still 53 associative anaphoric relations that were
not recognized. Table 6 shows the causes of them.
22 false negatives were caused by salience score
filtering. Note that, it does not mean that these 22
associative anaphoric relations were always recog-
nized correctly if the correct antecedents were not
filtered by salience score.
Case frame sparseness caused only 5 false neg-
atives. Considering that the recall of nominal case
frames was 74% as shown in Table 3, this seems to
be too few. This is because we do not considered
the relations that tagged possible, and only con-
sidered obviously indispensable relations. From
this result, we can say that coverage of nominal
case frames for nouns that have obviously indis-
pensable entities is much higher than 74%, which
is considered to achieve a coverage of about 95%
(105/110).
</bodyText>
<subsectionHeader confidence="0.999108">
4.4 Comparison with previous work
</subsectionHeader>
<bodyText confidence="0.999963928571429">
Murata et al. (1999) proposed a method of utiliz-
ing “Nm no Nh” phrases for associative anaphora
resolution.7 They basically used all “Nm no Nh”
phrases from corpora as a lexical knowledge, and
used rule-based approach. They obtained a recall
of 0.63 and a precision of 0.68 by using exam-
ples of “X no Y” (Y of X), a recall of 0.71 and a
precision of 0.82 by assuming ideal nominal case
frames. One reason of such high performance may
be that they considered referential properties of
noun phrases, such as generic, indefinite, and defi-
nite, while our model does not. We can also say
that their experiments were conducted on small
and supposedly easy corpora. Half of their corpora
</bodyText>
<footnote confidence="0.942674666666667">
7Murata et al. (1999) and we (Sasano et al., 2004) used
the terminology indirect anaphora, but concerned with the
same phenomena as we concerned with in this paper.
</footnote>
<tableCaption confidence="0.977585">
Table 6: Causes of false negatives.
</tableCaption>
<table confidence="0.9972115">
Causes Num
Filtered by salience score 22 (15)
Judge as non-anaphoric 13 (14)
Select false antecedents 13 (13)
Case frame sparseness 5 (5)
Total 53 (47)
</table>
<bodyText confidence="0.94412825">
*“()“ denotes the number of causes when
using correct zero anaphora tags.
were occupied by fairy tale, against which domain
specific rules are considered to be effective.
We proposed a rule-based approach for asso-
ciative anaphora resolution based on automati-
cally acquired nominal case frames (Sasano et al.,
2004).7 We obtained a recall of 0.633 and a pre-
cision of 0.508 against news paper articles. How-
ever, we regarded some additional relations that
can be interpreted by considering coreference re-
lations as associative anaphoric relations.
</bodyText>
<listItem confidence="0.275459">
(9) Chechen Kyˆowakoku-no shuto-ni ...
</listItem>
<subsectionHeader confidence="0.343152">
Chechen Republic capital
</subsectionHeader>
<bodyText confidence="0.929754933333333">
... shuto seiatsu-no saishu dankai-ni ...
capital conquer last stage
(... to the capital of Chechen Republic ... in the last
stage to conquer the capital ...)
For example, although the second mention of
“shuto” (capital) in example (9) means “Chechen
Kyˆowakoku-no shuto” (the capital of Chechen Re-
public), it can be interpreted by recognizing the
coreference relation between the first and second
mentions of “shuto” (capital). Therefore, as men-
tioned in Section 3.2, we do not consider such re-
lations as associative anaphora in this study; we
included such relations as associative anaphora in
(Sasano et al., 2004). The relatively high score is
caused by this criterion.
</bodyText>
<sectionHeader confidence="0.999597" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999795272727273">
In this paper, we proposed a probabilistic model
for associative anaphora resolution. Our model
regards associative anaphora as a kind of zero
anaphora and resolves it in the same manner as
zero anaphora resolution that uses automatically
acquired case frames. We also showed that the
performance of associative anaphora resolution
can be improved by resolving it simultaneously
with zero anaphora. As future work, we plan to
consider referential properties of noun phrases in
associative anaphora resolution.
</bodyText>
<page confidence="0.960134">
1463
</page>
<sectionHeader confidence="0.998275" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999943969072165">
Razvan Bunescu. 2003. Associative anaphora res-
olution: A web-based approach. In Proc. of
EACL’03: Workshop on The Computational Treat-
ment of Anaphora, pages 47–52.
Herbert H Clark. 1975. Bridging. In Proc. of the Con-
ference on Theoretical Issues in Natural Language
Processing, pages 169–174.
Caroline Gasperin and Ted Briscoe. 2008. Statistical
anaphora resolution in biomedical texts. In Proc. of
COLING’08, pages 257–264.
Caroline Gasperin and Renata Vieira. 2004. Using
word similarity lists for resolving indirect anaphora.
In Proc. ofACL’04: Workshop on Reference Resolu-
tion and its Applications, pages 40–46.
Udo Hahn, Michael Strube, and Katja Markert. 1996.
Bridging textual ellipsis. In Proc. of COLING’96,
pages 496–501.
John A. Hawkins. 1978. Definiteness and indefinite-
ness: a study in reference and grammaticality pre-
diction. Croom Helm Ltd.
IREX Committee, editor. 1999. Proc. of the IREX
Workshop.
Daisuke Kawahara and Sadao Kurohashi. 2002. Fertil-
ization of case frame dictionary for robust Japanese
case analysis. In Proc. of COLING’02, pages 425–
431.
Daisuke Kawahara and Sadao Kurohashi. 2006.
Case frame compilation from the web using high-
performance computing. In Proc. of LREC’06,
pages 1344–1347.
Sadao Kurohashi and Yasuyuki Sakai. 1999. Seman-
tic analysis of Japanese noun phrases: A new ap-
proach to dictionary-based understanding. In Proc.
of ACL’99, pages 481–488.
Shalom Lappin and Herbert J. Leass. 1994. An algo-
rithm for pronominal anaphora resolution. Compu-
tational Linguistics, 20(4):535–562.
Katja Markert, Malvina Nissim, and Natalia N Mod-
jeska. 2003. Using the web for nominal anaphora
resolution. In Proc. of EACL’03: Workshop on the
Computational Treatment of Anaphora, pages 39–
46.
Josef Meyer and Robert Dale. 2002. Using the Word-
Net hierarchy for associative anaphora resolution. In
Proc. of SemaNet’02: Building and Using Semantic
Networks.
Ruslan Mitkov, Richard Evans, and Constantin Or˘asan.
2002. A new, fully automatic version of Mitkov’s
knowledge-poor pronoun resolution method. In
Proc. of CICLing’02.
Natalia N Modjeska. 2002. Lexical and grammati-
cal role constraints in resolution other-anaphora. In
Proc. of DAARC’02.
Masaki Murata, Hitoshi Isahara, and Makoto Nagao.
1999. Resolution of indirect anaphora in Japanese
sentences using examples “X no Y”(Y of X). In
Proc. of ACL’99: Workshop on Coreference and Its
Applications.
Massimo Poesio, Tomonori Ishikawa, Sabine Schulte
im Walde, and Renata Vieira. 2002. Acquiring lex-
ical knowledge for anaphora resolution. In Proc. of
LREC’02, pages 1220–1224.
Massimo Poesio, Pahul Mehta, Axel Maroudas, and
Janet Hitzeman. 2004. Learning to Resolve Bridg-
ing References. In Proc. ofACL’04, pages 143–150.
Ryohei Sasano and Sadao Kurohashi. 2008. Japanese
named entity recognition using structural natural
language processing. In Proc. of IJCNLP’08, pages
607–612.
Ryohei Sasano, Daisuke Kawahara, and Sadao Kuro-
hashi. 2004. Automatic construction of nominal
case frames and its application to indirect anaphora
resolution. In Proc. of COLING’04, pages 1201–
1207.
Ryohei Sasano, Daisuke Kawahara, and Sadao Kuro-
hashi. 2008. A fully-lexicalized probabilistic model
for japanese zero anaphora resolution. In Proc. of
COLING’08, pages 769–776.
Ryohei Sasano, Daisuke Kawahara, and Sadao Kuro-
hashi. 2009. The effect of corpus size on case
frame acquisition for discourse analysis. In Proc.
of NAACL-HLT’09, pages 521–529.
Michael Strube and Udo Hahn. 1999. Functional
centering – grounding referential coherence in in-
formation structure. Computational Linguistics,
25(3):309–344.
Renata Vieira and Massimo Poesio. 2000. An empir-
ically based system for processing definite descrip-
tions. Computational Linguistics, 26(4):539–592.
Renata Vieira, Eckhard Bick, Jorge Coelho, Vinicius
Muller, Sandra Collovini, Jose Souza, and Lucia
Rino. 2006. Semantic tagging for resolution of indi-
rect anaphora. In Proc. of the 7th SIGdial Workshop
on Discourse and Dialogue, pages 76–79.
Mitsuko Yamura-Takei. 2003. Approaches to zero ad-
nominal recognition. In Proc. of ACL’03: Student
Research Workshop, pages 87–94.
</reference>
<page confidence="0.995265">
1464
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.351141">
<title confidence="0.997418">A Probabilistic Model for Associative Anaphora Resolution</title>
<author confidence="0.705797">Sasano</author>
<affiliation confidence="0.712599">Graduate School of Informatics, Kyoto</affiliation>
<address confidence="0.428176">Yoshida-honmachi, Sakyo-ku,</address>
<abstract confidence="0.995436625">This paper proposes a probabilistic for associative anaphora resolution in Japanese. Associative anaphora is type of bridging anaphora, in which the anaphor and its antecedent are not coreferent. Our model regards associative anaphora as a kind of zero anaphora and resolves it in the same manner as zero anaphora resolution using automatically acquired lexical knowledge. Experimental results show that our model resolves associative anaphora with good performance and the performance is improved by resolving it simultaneously with zero anaphora.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
</authors>
<title>Associative anaphora resolution: A web-based approach.</title>
<date>2003</date>
<booktitle>In Proc. of EACL’03: Workshop on The Computational Treatment of Anaphora,</booktitle>
<pages>47--52</pages>
<contexts>
<context position="3800" citStr="Bunescu, 2003" startWordPosition="585" endWordPosition="586">loit “Nm no Nh” phrases to resolve associative anaphora in Japanese. Here, the Japanese postposition “no” roughly corresponds to “of,” but it has 1455 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1455–1464, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP much broader usage. These studies obtained reasonable results, but the coverage of the acquired knowledge was not sufficient. Recently, a number of researchers argued for using the Web as a source of lexical knowledge, and the Web has been shown to be a useful resource for anaphora resolution (Bunescu, 2003; Markert et al., 2003; Poesio et al., 2004). Hence, in this study, we acquire the lexical knowledge for associative anaphora resolution from “Nm no Nh” phrases in the Web by using the method described in (Sasano et al., 2004). We proposed a method for acquiring such lexical knowledge, called nominal case frames (NCFs), using an ordinary language dictionary and “Nm no Nh” phrases, and constructed NCFs from newspaper corpora. In this study, we aim to acquire a sufficient amount of lexical knowledge by constructing NCFs from the Web. As for associative anaphora resolution itself, we propose an i</context>
</contexts>
<marker>Bunescu, 2003</marker>
<rawString>Razvan Bunescu. 2003. Associative anaphora resolution: A web-based approach. In Proc. of EACL’03: Workshop on The Computational Treatment of Anaphora, pages 47–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert H Clark</author>
</authors>
<title>Bridging.</title>
<date>1975</date>
<booktitle>In Proc. of the Conference on Theoretical Issues in Natural Language Processing,</booktitle>
<pages>169--174</pages>
<contexts>
<context position="902" citStr="Clark, 1975" startWordPosition="122" endWordPosition="123">on in Japanese. Associative anaphora is a type of bridging anaphora, in which the anaphor and its antecedent are not coreferent. Our model regards associative anaphora as a kind of zero anaphora and resolves it in the same manner as zero anaphora resolution using automatically acquired lexical knowledge. Experimental results show that our model resolves associative anaphora with good performance and the performance is improved by resolving it simultaneously with zero anaphora. 1 Introduction The correct interpretation of anaphora is vital for natural language understanding. Bridging anaphora (Clark, 1975) represents a special part of the general problem of anaphora resolution, which has been studied and discussed for various languages and domains (Hahn et al., 1996; Murata et al., 1999; Poesio et al., 2004; Gasperin and Vieira, 2004; Gasperin and Briscoe, 2008). Usually bridging anaphora considers two types:1 associative anaphors are noun phrases (NPs) that have an antecedent that is necessary to their interpretation but the relation between the anaphor and its antecedent is different from identity; and indirect anaphors are those that have an identity relation with their antecedents but the a</context>
</contexts>
<marker>Clark, 1975</marker>
<rawString>Herbert H Clark. 1975. Bridging. In Proc. of the Conference on Theoretical Issues in Natural Language Processing, pages 169–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Caroline Gasperin</author>
<author>Ted Briscoe</author>
</authors>
<title>Statistical anaphora resolution in biomedical texts.</title>
<date>2008</date>
<booktitle>In Proc. of COLING’08,</booktitle>
<pages>257--264</pages>
<contexts>
<context position="1163" citStr="Gasperin and Briscoe, 2008" startWordPosition="163" endWordPosition="166"> resolution using automatically acquired lexical knowledge. Experimental results show that our model resolves associative anaphora with good performance and the performance is improved by resolving it simultaneously with zero anaphora. 1 Introduction The correct interpretation of anaphora is vital for natural language understanding. Bridging anaphora (Clark, 1975) represents a special part of the general problem of anaphora resolution, which has been studied and discussed for various languages and domains (Hahn et al., 1996; Murata et al., 1999; Poesio et al., 2004; Gasperin and Vieira, 2004; Gasperin and Briscoe, 2008). Usually bridging anaphora considers two types:1 associative anaphors are noun phrases (NPs) that have an antecedent that is necessary to their interpretation but the relation between the anaphor and its antecedent is different from identity; and indirect anaphors are those that have an identity relation with their antecedents but the anaphor and its antecedent have different head 1The terminology that we use here is introduced by Hawkins (1978), which is also used in (Vieira et al., 2006). nouns. In this paper, we focus on associative anaphora in Japanese. Associative anaphora resolution is </context>
</contexts>
<marker>Gasperin, Briscoe, 2008</marker>
<rawString>Caroline Gasperin and Ted Briscoe. 2008. Statistical anaphora resolution in biomedical texts. In Proc. of COLING’08, pages 257–264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Caroline Gasperin</author>
<author>Renata Vieira</author>
</authors>
<title>Using word similarity lists for resolving indirect anaphora.</title>
<date>2004</date>
<booktitle>In Proc. ofACL’04: Workshop on Reference Resolution and its Applications,</booktitle>
<pages>40--46</pages>
<contexts>
<context position="1134" citStr="Gasperin and Vieira, 2004" startWordPosition="159" endWordPosition="162">ame manner as zero anaphora resolution using automatically acquired lexical knowledge. Experimental results show that our model resolves associative anaphora with good performance and the performance is improved by resolving it simultaneously with zero anaphora. 1 Introduction The correct interpretation of anaphora is vital for natural language understanding. Bridging anaphora (Clark, 1975) represents a special part of the general problem of anaphora resolution, which has been studied and discussed for various languages and domains (Hahn et al., 1996; Murata et al., 1999; Poesio et al., 2004; Gasperin and Vieira, 2004; Gasperin and Briscoe, 2008). Usually bridging anaphora considers two types:1 associative anaphors are noun phrases (NPs) that have an antecedent that is necessary to their interpretation but the relation between the anaphor and its antecedent is different from identity; and indirect anaphors are those that have an identity relation with their antecedents but the anaphor and its antecedent have different head 1The terminology that we use here is introduced by Hawkins (1978), which is also used in (Vieira et al., 2006). nouns. In this paper, we focus on associative anaphora in Japanese. Associ</context>
</contexts>
<marker>Gasperin, Vieira, 2004</marker>
<rawString>Caroline Gasperin and Renata Vieira. 2004. Using word similarity lists for resolving indirect anaphora. In Proc. ofACL’04: Workshop on Reference Resolution and its Applications, pages 40–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Udo Hahn</author>
<author>Michael Strube</author>
<author>Katja Markert</author>
</authors>
<title>Bridging textual ellipsis.</title>
<date>1996</date>
<booktitle>In Proc. of COLING’96,</booktitle>
<pages>496--501</pages>
<contexts>
<context position="1065" citStr="Hahn et al., 1996" startWordPosition="147" endWordPosition="150"> anaphora as a kind of zero anaphora and resolves it in the same manner as zero anaphora resolution using automatically acquired lexical knowledge. Experimental results show that our model resolves associative anaphora with good performance and the performance is improved by resolving it simultaneously with zero anaphora. 1 Introduction The correct interpretation of anaphora is vital for natural language understanding. Bridging anaphora (Clark, 1975) represents a special part of the general problem of anaphora resolution, which has been studied and discussed for various languages and domains (Hahn et al., 1996; Murata et al., 1999; Poesio et al., 2004; Gasperin and Vieira, 2004; Gasperin and Briscoe, 2008). Usually bridging anaphora considers two types:1 associative anaphors are noun phrases (NPs) that have an antecedent that is necessary to their interpretation but the relation between the anaphor and its antecedent is different from identity; and indirect anaphors are those that have an identity relation with their antecedents but the anaphor and its antecedent have different head 1The terminology that we use here is introduced by Hawkins (1978), which is also used in (Vieira et al., 2006). nouns</context>
</contexts>
<marker>Hahn, Strube, Markert, 1996</marker>
<rawString>Udo Hahn, Michael Strube, and Katja Markert. 1996. Bridging textual ellipsis. In Proc. of COLING’96, pages 496–501.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John A Hawkins</author>
</authors>
<title>Definiteness and indefiniteness: a study in reference and grammaticality prediction. Croom Helm Ltd.</title>
<date>1978</date>
<contexts>
<context position="1613" citStr="Hawkins (1978)" startWordPosition="234" endWordPosition="235">and discussed for various languages and domains (Hahn et al., 1996; Murata et al., 1999; Poesio et al., 2004; Gasperin and Vieira, 2004; Gasperin and Briscoe, 2008). Usually bridging anaphora considers two types:1 associative anaphors are noun phrases (NPs) that have an antecedent that is necessary to their interpretation but the relation between the anaphor and its antecedent is different from identity; and indirect anaphors are those that have an identity relation with their antecedents but the anaphor and its antecedent have different head 1The terminology that we use here is introduced by Hawkins (1978), which is also used in (Vieira et al., 2006). nouns. In this paper, we focus on associative anaphora in Japanese. Associative anaphora resolution is decomposed into two steps: acquiring lexical knowledge for associative anaphora resolution, and resolving associative anaphora using the acquired knowledge. Grammatical salience plays a lesser role for resolving anaphors with full lexical heads, than for pronominal anaphora (Strube and Hahn, 1999; Modjeska, 2002). Furthermore, since associative anaphors and their antecedents usually have different head nouns, string matching technique cannot be a</context>
</contexts>
<marker>Hawkins, 1978</marker>
<rawString>John A. Hawkins. 1978. Definiteness and indefiniteness: a study in reference and grammaticality prediction. Croom Helm Ltd.</rawString>
</citation>
<citation valid="true">
<date>1999</date>
<booktitle>Proc. of the IREX Workshop.</booktitle>
<editor>IREX Committee, editor.</editor>
<contexts>
<context position="3171" citStr="(1999)" startWordPosition="483" endWordPosition="483">n (2), such knowledge as “price” is a price of some goods or service is required. (1) There was a house. The roof was white. (2) Toyota launched the hybrid car Prius in 1997. The price was 21.5 million yen. To acquire such lexical knowledge, various studies have been carried out. Early studies used hand-crafted lexical knowledge such as WordNet (Strube and Hahn, 1999; Vieira and Poesio, 2000; Meyer and Dale, 2002), but obtained poor or mediocre results. Hence, Poesio et al. (2002) proposed to exploit “Nh of Nm” phrases in large corpora to resolve associative anaphora in English; Murata et al. (1999) proposed to exploit “Nm no Nh” phrases to resolve associative anaphora in Japanese. Here, the Japanese postposition “no” roughly corresponds to “of,” but it has 1455 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1455–1464, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP much broader usage. These studies obtained reasonable results, but the coverage of the acquired knowledge was not sufficient. Recently, a number of researchers argued for using the Web as a source of lexical knowledge, and the Web has been shown to be a useful resource for anaph</context>
<context position="7156" citStr="(1999)" startWordPosition="1114" endWordPosition="1114">ning of a verbal argument can be distinguished by the postposition, such as “ga” (nominative), “wo” (accusative), and “ni” (dative), the meaning of an adnominal constituent can not be distinguished easily, because most adnominal constituents appear with the same postposition “no” (of). Thus, we first conduct a semantic analysis of adnominal constituents, and then construct NCFs using the results as follows: 1. Collect syntactically unambiguous noun phrases “Nm no Nh” from the automatic resulting parses of large corpora. 2. Analyze the relation between Nm and Nh by Kurohashi and Sakai’s method (1999) that exploits an ordinary language dictionary. 3. Depending on the results, classify Nm, and obtain preliminary case slots for Nh. 4. Merge case slots if two preliminary case slots of Nh are similar. 5. Consider frequent case slots as obligatory cases of Nh. The frequency thresholds are varied according to semantic analyses. 6. For each meaning of Nh, collect case slots and construct case frames. The point of this method is the integrated use of an ordinary dictionary and example phrases from 1456 Table 1: Examples of constructed nominal case frames. Case slot Examples with freq Generalized e</context>
<context position="14552" citStr="(1999)" startWordPosition="2259" endWordPosition="2259">alized the examples of case slots. First, we used the categories that Japanese morphological analyzer JUMAN3 adds to common nouns. In JUMAN, about twenty categories are defined and tagged to common nouns. For example, “kuruma (car),” “niwatori (chicken),” and “tatemono (building)” are tagged as “VEHICLE,” “ANIMAL” and “FACILITY,” respectively. For each category, we calculated the rate of categorized examples among all case slot examples, and added it to the case slot as “[CT:VEHICLE]:0.13.” We also generalized NEs. We used a common standard NE definition for Japanese provided by IREX workshop (1999). We first recognized NEs in the source corpus by using an NE recognizer (Sasano and Kurohashi, 2008), and then constructed NCFs from the NE-recognized corpus. As well as categories, for each NE class, we calculated the NE rate among all case slot examples, and added it to the case slot as “[NE:PERSON]:0.22.” The generalized examples are also included in Table 1. 3 Probabilistic Model In this study, we apply a lexicalized probabilistic model for zero anaphora resolution proposed in (Sasano et al., 2008) to associative anaphora resolution. 3.1 A Lexicalized Probabilistic Model for Zero Anaphora</context>
<context position="33244" citStr="(1999)" startWordPosition="5198" endWordPosition="5198">rrect antecedents were not filtered by salience score. Case frame sparseness caused only 5 false negatives. Considering that the recall of nominal case frames was 74% as shown in Table 3, this seems to be too few. This is because we do not considered the relations that tagged possible, and only considered obviously indispensable relations. From this result, we can say that coverage of nominal case frames for nouns that have obviously indispensable entities is much higher than 74%, which is considered to achieve a coverage of about 95% (105/110). 4.4 Comparison with previous work Murata et al. (1999) proposed a method of utilizing “Nm no Nh” phrases for associative anaphora resolution.7 They basically used all “Nm no Nh” phrases from corpora as a lexical knowledge, and used rule-based approach. They obtained a recall of 0.63 and a precision of 0.68 by using examples of “X no Y” (Y of X), a recall of 0.71 and a precision of 0.82 by assuming ideal nominal case frames. One reason of such high performance may be that they considered referential properties of noun phrases, such as generic, indefinite, and definite, while our model does not. We can also say that their experiments were conducted</context>
</contexts>
<marker>1999</marker>
<rawString>IREX Committee, editor. 1999. Proc. of the IREX Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Fertilization of case frame dictionary for robust Japanese case analysis.</title>
<date>2002</date>
<booktitle>In Proc. of COLING’02,</booktitle>
<pages>425--431</pages>
<contexts>
<context position="6442" citStr="Kawahara and Kurohashi, 2002" startWordPosition="1002" endWordPosition="1005">atory cases. In this paper, we call indispensable entities of nouns obligatory cases. Note that, obligatory does not mean grammatically obligatory but obligatory to interpret the meaning of the noun. Associative anaphora resolution needs comprehensive information of obligatory cases of nouns. Nominal case frames (NCFs) describe such information, and we construct them from the Web. 2.1 Automatic Construction of NCFs First, we briefly introduce our method for constructing NCFs from raw corpora proposed in (Sasano et al., 2004). Whereas verbal case frame construction uses arguments of each verb (Kawahara and Kurohashi, 2002), nominal case frame construction basically uses adnominal constituents of each noun. However, while the meaning of a verbal argument can be distinguished by the postposition, such as “ga” (nominative), “wo” (accusative), and “ni” (dative), the meaning of an adnominal constituent can not be distinguished easily, because most adnominal constituents appear with the same postposition “no” (of). Thus, we first conduct a semantic analysis of adnominal constituents, and then construct NCFs using the results as follows: 1. Collect syntactically unambiguous noun phrases “Nm no Nh” from the automatic r</context>
</contexts>
<marker>Kawahara, Kurohashi, 2002</marker>
<rawString>Daisuke Kawahara and Sadao Kurohashi. 2002. Fertilization of case frame dictionary for robust Japanese case analysis. In Proc. of COLING’02, pages 425– 431.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Case frame compilation from the web using highperformance computing.</title>
<date>2006</date>
<booktitle>In Proc. of LREC’06,</booktitle>
<pages>1344--1347</pages>
<contexts>
<context position="11138" citStr="Kawahara and Kurohashi, 2006" startWordPosition="1678" endWordPosition="1681">f noun phrases “Nm no Nh” (Nh of Nm). We use such noun phrases because indispensable entities of noun ”Nh” often appear as ”Nm.” However, we can say neither frequently appeared ”Nm” is an indispensable entity of ”Nh.” nor an indispensable entity frequently appears as ”Nm.” For example, the name of a country is considered as an indispensable entity of ”shusho” (prime minister), but does not frequently appear as ”Nm.”2 Thus, it is difficult to estimate a probability distribution and we use a hard decision. 2.2 NCF Construction from the Web We constructed nominal case frames from the Web Corpus (Kawahara and Kurohashi, 2006), which comprises 1.6 billion unique Japanese sentences. In this corpus, there were about 390 million noun phrases “Nm no Nh,” about 100 million unique noun phrases, and about 17 million unique head nouns “Nh.” There were about 4.07 million head nouns that appeared more than 10 times in the corpus, and we used only such head nouns. The resultant nominal case frames consisted of about 564,000 nouns including compound nouns. We show examples of constructed nominal case frames in Table 1. The average number of case frames for a noun that has case frames was 1.0031, and the average number of case </context>
</contexts>
<marker>Kawahara, Kurohashi, 2006</marker>
<rawString>Daisuke Kawahara and Sadao Kurohashi. 2006. Case frame compilation from the web using highperformance computing. In Proc. of LREC’06, pages 1344–1347.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sadao Kurohashi</author>
<author>Yasuyuki Sakai</author>
</authors>
<title>Semantic analysis of Japanese noun phrases: A new approach to dictionary-based understanding.</title>
<date>1999</date>
<booktitle>In Proc. of ACL’99,</booktitle>
<pages>481--488</pages>
<marker>Kurohashi, Sakai, 1999</marker>
<rawString>Sadao Kurohashi and Yasuyuki Sakai. 1999. Semantic analysis of Japanese noun phrases: A new approach to dictionary-based understanding. In Proc. of ACL’99, pages 481–488.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shalom Lappin</author>
<author>Herbert J Leass</author>
</authors>
<title>An algorithm for pronominal anaphora resolution.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>4</issue>
<contexts>
<context position="22491" citStr="Lappin and Leass, 1994" startWordPosition="3504" endWordPosition="3508">ne mention of an entity is 5“Hybrid car” and “Prius” are in apposition and these two phrases are considered to refer to the same discourse entity. tagged any category or recognized as an NE, our model also uses the category or the NE class as the content part of the entity. Specifically, for estimating Equation (iii), our model also calculates: P(NE:ARTIFACT |kakaku(1), no, A&apos;(no)=1) P(NE:ARTIFACT) besides: P(Prius|kakaku(1), no, A&apos;(no) = 1) P(Prius) and uses the geometric mean of them. 3.3 Salience Score Filtering Previous work has reported the usefulness of salience for anaphora resolution (Lappin and Leass, 1994; Mitkov et al., 2002). In order to consider the salience of a discourse entity, we introduce the concept of salience score, which is calculated by the following set of simple rules, and only consider the entities that have the salience score no less than 1 as candidate antecedents of an associative anaphor. • +2 : mentioned with topical marker “wa,” or at the end of a sentence. • +1 : mentioned without topical marker “wa.” • +1 : assigned to a zero pronoun. • xα : beginning of each sentence. We call α a decay rate. If α ≥ 1, we do not filter out any entities. If α = 0, we only consider the en</context>
</contexts>
<marker>Lappin, Leass, 1994</marker>
<rawString>Shalom Lappin and Herbert J. Leass. 1994. An algorithm for pronominal anaphora resolution. Computational Linguistics, 20(4):535–562.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Markert</author>
<author>Malvina Nissim</author>
<author>Natalia N Modjeska</author>
</authors>
<title>Using the web for nominal anaphora resolution.</title>
<date>2003</date>
<booktitle>In Proc. of EACL’03: Workshop on the Computational Treatment of Anaphora,</booktitle>
<pages>39--46</pages>
<contexts>
<context position="3822" citStr="Markert et al., 2003" startWordPosition="587" endWordPosition="590"> phrases to resolve associative anaphora in Japanese. Here, the Japanese postposition “no” roughly corresponds to “of,” but it has 1455 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1455–1464, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP much broader usage. These studies obtained reasonable results, but the coverage of the acquired knowledge was not sufficient. Recently, a number of researchers argued for using the Web as a source of lexical knowledge, and the Web has been shown to be a useful resource for anaphora resolution (Bunescu, 2003; Markert et al., 2003; Poesio et al., 2004). Hence, in this study, we acquire the lexical knowledge for associative anaphora resolution from “Nm no Nh” phrases in the Web by using the method described in (Sasano et al., 2004). We proposed a method for acquiring such lexical knowledge, called nominal case frames (NCFs), using an ordinary language dictionary and “Nm no Nh” phrases, and constructed NCFs from newspaper corpora. In this study, we aim to acquire a sufficient amount of lexical knowledge by constructing NCFs from the Web. As for associative anaphora resolution itself, we propose an integrated probabilisti</context>
</contexts>
<marker>Markert, Nissim, Modjeska, 2003</marker>
<rawString>Katja Markert, Malvina Nissim, and Natalia N Modjeska. 2003. Using the web for nominal anaphora resolution. In Proc. of EACL’03: Workshop on the Computational Treatment of Anaphora, pages 39– 46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josef Meyer</author>
<author>Robert Dale</author>
</authors>
<title>Using the WordNet hierarchy for associative anaphora resolution.</title>
<date>2002</date>
<booktitle>In Proc. of SemaNet’02: Building and Using Semantic Networks.</booktitle>
<contexts>
<context position="2982" citStr="Meyer and Dale, 2002" startWordPosition="449" endWordPosition="452">ronymic relation between “a house” and “the roof” in (1), such knowledge as “a roof” is a part of a building or vehicle is required. To recognize the attributive relation between “Prius” and “the price” in (2), such knowledge as “price” is a price of some goods or service is required. (1) There was a house. The roof was white. (2) Toyota launched the hybrid car Prius in 1997. The price was 21.5 million yen. To acquire such lexical knowledge, various studies have been carried out. Early studies used hand-crafted lexical knowledge such as WordNet (Strube and Hahn, 1999; Vieira and Poesio, 2000; Meyer and Dale, 2002), but obtained poor or mediocre results. Hence, Poesio et al. (2002) proposed to exploit “Nh of Nm” phrases in large corpora to resolve associative anaphora in English; Murata et al. (1999) proposed to exploit “Nm no Nh” phrases to resolve associative anaphora in Japanese. Here, the Japanese postposition “no” roughly corresponds to “of,” but it has 1455 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1455–1464, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP much broader usage. These studies obtained reasonable results, but the coverage of the acq</context>
</contexts>
<marker>Meyer, Dale, 2002</marker>
<rawString>Josef Meyer and Robert Dale. 2002. Using the WordNet hierarchy for associative anaphora resolution. In Proc. of SemaNet’02: Building and Using Semantic Networks.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruslan Mitkov</author>
<author>Richard Evans</author>
<author>Constantin Or˘asan</author>
</authors>
<title>A new, fully automatic version of Mitkov’s knowledge-poor pronoun resolution method.</title>
<date>2002</date>
<booktitle>In Proc. of CICLing’02.</booktitle>
<marker>Mitkov, Evans, Or˘asan, 2002</marker>
<rawString>Ruslan Mitkov, Richard Evans, and Constantin Or˘asan. 2002. A new, fully automatic version of Mitkov’s knowledge-poor pronoun resolution method. In Proc. of CICLing’02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Natalia N Modjeska</author>
</authors>
<title>Lexical and grammatical role constraints in resolution other-anaphora.</title>
<date>2002</date>
<booktitle>In Proc. of DAARC’02.</booktitle>
<contexts>
<context position="2077" citStr="Modjeska, 2002" startWordPosition="302" endWordPosition="303">elation with their antecedents but the anaphor and its antecedent have different head 1The terminology that we use here is introduced by Hawkins (1978), which is also used in (Vieira et al., 2006). nouns. In this paper, we focus on associative anaphora in Japanese. Associative anaphora resolution is decomposed into two steps: acquiring lexical knowledge for associative anaphora resolution, and resolving associative anaphora using the acquired knowledge. Grammatical salience plays a lesser role for resolving anaphors with full lexical heads, than for pronominal anaphora (Strube and Hahn, 1999; Modjeska, 2002). Furthermore, since associative anaphors and their antecedents usually have different head nouns, string matching technique cannot be applied. Therefore, a large and diverse amount of lexical knowledge is essential to understand associative anaphora. For example, to recognize the meronymic relation between “a house” and “the roof” in (1), such knowledge as “a roof” is a part of a building or vehicle is required. To recognize the attributive relation between “Prius” and “the price” in (2), such knowledge as “price” is a price of some goods or service is required. (1) There was a house. The roo</context>
</contexts>
<marker>Modjeska, 2002</marker>
<rawString>Natalia N Modjeska. 2002. Lexical and grammatical role constraints in resolution other-anaphora. In Proc. of DAARC’02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaki Murata</author>
<author>Hitoshi Isahara</author>
<author>Makoto Nagao</author>
</authors>
<title>Resolution of indirect anaphora in Japanese sentences using examples “X no Y”(Y of X).</title>
<date>1999</date>
<booktitle>In Proc. of ACL’99: Workshop on Coreference and Its Applications.</booktitle>
<contexts>
<context position="1086" citStr="Murata et al., 1999" startWordPosition="151" endWordPosition="154"> of zero anaphora and resolves it in the same manner as zero anaphora resolution using automatically acquired lexical knowledge. Experimental results show that our model resolves associative anaphora with good performance and the performance is improved by resolving it simultaneously with zero anaphora. 1 Introduction The correct interpretation of anaphora is vital for natural language understanding. Bridging anaphora (Clark, 1975) represents a special part of the general problem of anaphora resolution, which has been studied and discussed for various languages and domains (Hahn et al., 1996; Murata et al., 1999; Poesio et al., 2004; Gasperin and Vieira, 2004; Gasperin and Briscoe, 2008). Usually bridging anaphora considers two types:1 associative anaphors are noun phrases (NPs) that have an antecedent that is necessary to their interpretation but the relation between the anaphor and its antecedent is different from identity; and indirect anaphors are those that have an identity relation with their antecedents but the anaphor and its antecedent have different head 1The terminology that we use here is introduced by Hawkins (1978), which is also used in (Vieira et al., 2006). nouns. In this paper, we f</context>
<context position="3171" citStr="Murata et al. (1999)" startWordPosition="480" endWordPosition="483"> “the price” in (2), such knowledge as “price” is a price of some goods or service is required. (1) There was a house. The roof was white. (2) Toyota launched the hybrid car Prius in 1997. The price was 21.5 million yen. To acquire such lexical knowledge, various studies have been carried out. Early studies used hand-crafted lexical knowledge such as WordNet (Strube and Hahn, 1999; Vieira and Poesio, 2000; Meyer and Dale, 2002), but obtained poor or mediocre results. Hence, Poesio et al. (2002) proposed to exploit “Nh of Nm” phrases in large corpora to resolve associative anaphora in English; Murata et al. (1999) proposed to exploit “Nm no Nh” phrases to resolve associative anaphora in Japanese. Here, the Japanese postposition “no” roughly corresponds to “of,” but it has 1455 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1455–1464, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP much broader usage. These studies obtained reasonable results, but the coverage of the acquired knowledge was not sufficient. Recently, a number of researchers argued for using the Web as a source of lexical knowledge, and the Web has been shown to be a useful resource for anaph</context>
<context position="33244" citStr="Murata et al. (1999)" startWordPosition="5195" endWordPosition="5198">ctly if the correct antecedents were not filtered by salience score. Case frame sparseness caused only 5 false negatives. Considering that the recall of nominal case frames was 74% as shown in Table 3, this seems to be too few. This is because we do not considered the relations that tagged possible, and only considered obviously indispensable relations. From this result, we can say that coverage of nominal case frames for nouns that have obviously indispensable entities is much higher than 74%, which is considered to achieve a coverage of about 95% (105/110). 4.4 Comparison with previous work Murata et al. (1999) proposed a method of utilizing “Nm no Nh” phrases for associative anaphora resolution.7 They basically used all “Nm no Nh” phrases from corpora as a lexical knowledge, and used rule-based approach. They obtained a recall of 0.63 and a precision of 0.68 by using examples of “X no Y” (Y of X), a recall of 0.71 and a precision of 0.82 by assuming ideal nominal case frames. One reason of such high performance may be that they considered referential properties of noun phrases, such as generic, indefinite, and definite, while our model does not. We can also say that their experiments were conducted</context>
</contexts>
<marker>Murata, Isahara, Nagao, 1999</marker>
<rawString>Masaki Murata, Hitoshi Isahara, and Makoto Nagao. 1999. Resolution of indirect anaphora in Japanese sentences using examples “X no Y”(Y of X). In Proc. of ACL’99: Workshop on Coreference and Its Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimo Poesio</author>
<author>Tomonori Ishikawa</author>
<author>Sabine Schulte im Walde</author>
<author>Renata Vieira</author>
</authors>
<title>Acquiring lexical knowledge for anaphora resolution.</title>
<date>2002</date>
<booktitle>In Proc. of LREC’02,</booktitle>
<pages>1220--1224</pages>
<contexts>
<context position="3050" citStr="Poesio et al. (2002)" startWordPosition="460" endWordPosition="463">dge as “a roof” is a part of a building or vehicle is required. To recognize the attributive relation between “Prius” and “the price” in (2), such knowledge as “price” is a price of some goods or service is required. (1) There was a house. The roof was white. (2) Toyota launched the hybrid car Prius in 1997. The price was 21.5 million yen. To acquire such lexical knowledge, various studies have been carried out. Early studies used hand-crafted lexical knowledge such as WordNet (Strube and Hahn, 1999; Vieira and Poesio, 2000; Meyer and Dale, 2002), but obtained poor or mediocre results. Hence, Poesio et al. (2002) proposed to exploit “Nh of Nm” phrases in large corpora to resolve associative anaphora in English; Murata et al. (1999) proposed to exploit “Nm no Nh” phrases to resolve associative anaphora in Japanese. Here, the Japanese postposition “no” roughly corresponds to “of,” but it has 1455 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1455–1464, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP much broader usage. These studies obtained reasonable results, but the coverage of the acquired knowledge was not sufficient. Recently, a number of researcher</context>
</contexts>
<marker>Poesio, Ishikawa, Walde, Vieira, 2002</marker>
<rawString>Massimo Poesio, Tomonori Ishikawa, Sabine Schulte im Walde, and Renata Vieira. 2002. Acquiring lexical knowledge for anaphora resolution. In Proc. of LREC’02, pages 1220–1224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimo Poesio</author>
<author>Pahul Mehta</author>
<author>Axel Maroudas</author>
<author>Janet Hitzeman</author>
</authors>
<title>Learning to Resolve Bridging References.</title>
<date>2004</date>
<booktitle>In Proc. ofACL’04,</booktitle>
<pages>143--150</pages>
<contexts>
<context position="1107" citStr="Poesio et al., 2004" startWordPosition="155" endWordPosition="158"> resolves it in the same manner as zero anaphora resolution using automatically acquired lexical knowledge. Experimental results show that our model resolves associative anaphora with good performance and the performance is improved by resolving it simultaneously with zero anaphora. 1 Introduction The correct interpretation of anaphora is vital for natural language understanding. Bridging anaphora (Clark, 1975) represents a special part of the general problem of anaphora resolution, which has been studied and discussed for various languages and domains (Hahn et al., 1996; Murata et al., 1999; Poesio et al., 2004; Gasperin and Vieira, 2004; Gasperin and Briscoe, 2008). Usually bridging anaphora considers two types:1 associative anaphors are noun phrases (NPs) that have an antecedent that is necessary to their interpretation but the relation between the anaphor and its antecedent is different from identity; and indirect anaphors are those that have an identity relation with their antecedents but the anaphor and its antecedent have different head 1The terminology that we use here is introduced by Hawkins (1978), which is also used in (Vieira et al., 2006). nouns. In this paper, we focus on associative a</context>
<context position="3844" citStr="Poesio et al., 2004" startWordPosition="591" endWordPosition="594">sociative anaphora in Japanese. Here, the Japanese postposition “no” roughly corresponds to “of,” but it has 1455 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1455–1464, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP much broader usage. These studies obtained reasonable results, but the coverage of the acquired knowledge was not sufficient. Recently, a number of researchers argued for using the Web as a source of lexical knowledge, and the Web has been shown to be a useful resource for anaphora resolution (Bunescu, 2003; Markert et al., 2003; Poesio et al., 2004). Hence, in this study, we acquire the lexical knowledge for associative anaphora resolution from “Nm no Nh” phrases in the Web by using the method described in (Sasano et al., 2004). We proposed a method for acquiring such lexical knowledge, called nominal case frames (NCFs), using an ordinary language dictionary and “Nm no Nh” phrases, and constructed NCFs from newspaper corpora. In this study, we aim to acquire a sufficient amount of lexical knowledge by constructing NCFs from the Web. As for associative anaphora resolution itself, we propose an integrated probabilistic model for zero anaph</context>
</contexts>
<marker>Poesio, Mehta, Maroudas, Hitzeman, 2004</marker>
<rawString>Massimo Poesio, Pahul Mehta, Axel Maroudas, and Janet Hitzeman. 2004. Learning to Resolve Bridging References. In Proc. ofACL’04, pages 143–150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryohei Sasano</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Japanese named entity recognition using structural natural language processing.</title>
<date>2008</date>
<booktitle>In Proc. of IJCNLP’08,</booktitle>
<pages>607--612</pages>
<contexts>
<context position="14653" citStr="Sasano and Kurohashi, 2008" startWordPosition="2274" endWordPosition="2277">orphological analyzer JUMAN3 adds to common nouns. In JUMAN, about twenty categories are defined and tagged to common nouns. For example, “kuruma (car),” “niwatori (chicken),” and “tatemono (building)” are tagged as “VEHICLE,” “ANIMAL” and “FACILITY,” respectively. For each category, we calculated the rate of categorized examples among all case slot examples, and added it to the case slot as “[CT:VEHICLE]:0.13.” We also generalized NEs. We used a common standard NE definition for Japanese provided by IREX workshop (1999). We first recognized NEs in the source corpus by using an NE recognizer (Sasano and Kurohashi, 2008), and then constructed NCFs from the NE-recognized corpus. As well as categories, for each NE class, we calculated the NE rate among all case slot examples, and added it to the case slot as “[NE:PERSON]:0.22.” The generalized examples are also included in Table 1. 3 Probabilistic Model In this study, we apply a lexicalized probabilistic model for zero anaphora resolution proposed in (Sasano et al., 2008) to associative anaphora resolution. 3.1 A Lexicalized Probabilistic Model for Zero Anaphora Resolution In English, overt pronouns such as “she” and definite noun phrases such as “the company” </context>
</contexts>
<marker>Sasano, Kurohashi, 2008</marker>
<rawString>Ryohei Sasano and Sadao Kurohashi. 2008. Japanese named entity recognition using structural natural language processing. In Proc. of IJCNLP’08, pages 607–612.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryohei Sasano</author>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Automatic construction of nominal case frames and its application to indirect anaphora resolution.</title>
<date>2004</date>
<booktitle>In Proc. of COLING’04,</booktitle>
<pages>1201--1207</pages>
<contexts>
<context position="4026" citStr="Sasano et al., 2004" startWordPosition="623" endWordPosition="626">anguage Processing, pages 1455–1464, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP much broader usage. These studies obtained reasonable results, but the coverage of the acquired knowledge was not sufficient. Recently, a number of researchers argued for using the Web as a source of lexical knowledge, and the Web has been shown to be a useful resource for anaphora resolution (Bunescu, 2003; Markert et al., 2003; Poesio et al., 2004). Hence, in this study, we acquire the lexical knowledge for associative anaphora resolution from “Nm no Nh” phrases in the Web by using the method described in (Sasano et al., 2004). We proposed a method for acquiring such lexical knowledge, called nominal case frames (NCFs), using an ordinary language dictionary and “Nm no Nh” phrases, and constructed NCFs from newspaper corpora. In this study, we aim to acquire a sufficient amount of lexical knowledge by constructing NCFs from the Web. As for associative anaphora resolution itself, we propose an integrated probabilistic model for zero anaphora and associative anaphora resolution, in which associative anaphora is regarded as a kind of zero anaphora and resolved by using the same model as zero anaphora. Our model assumes</context>
<context position="6343" citStr="Sasano et al., 2004" startWordPosition="987" endWordPosition="990">d its indispensable entities is parallel to that between a verb and its arguments or obligatory cases. In this paper, we call indispensable entities of nouns obligatory cases. Note that, obligatory does not mean grammatically obligatory but obligatory to interpret the meaning of the noun. Associative anaphora resolution needs comprehensive information of obligatory cases of nouns. Nominal case frames (NCFs) describe such information, and we construct them from the Web. 2.1 Automatic Construction of NCFs First, we briefly introduce our method for constructing NCFs from raw corpora proposed in (Sasano et al., 2004). Whereas verbal case frame construction uses arguments of each verb (Kawahara and Kurohashi, 2002), nominal case frame construction basically uses adnominal constituents of each noun. However, while the meaning of a verbal argument can be distinguished by the postposition, such as “ga” (nominative), “wo” (accusative), and “ni” (dative), the meaning of an adnominal constituent can not be distinguished easily, because most adnominal constituents appear with the same postposition “no” (of). Thus, we first conduct a semantic analysis of adnominal constituents, and then construct NCFs using the re</context>
<context position="9579" citStr="Sasano et al., 2004" startWordPosition="1420" endWordPosition="1423">imal, used as food. (liver) [animal] niwatori(chicken):153, buta(pig):153, [CT:ANIMAL]:0.98, · · · ushi(cattle):62, doubutsu(animal):25,· · · senshu(1) Definition: someone who takes part in a sport. · (player) · · [sport] yakyˆu(baseball):1252, rirˆe(relay):736, [CT:ABSTRACTION]:0.56, · · kyˆogi(competition):430, sakkˆa(soccer):394, ··· &lt;affiliation&gt; chˆımu(team):4409, nihon(Japan):3222, [NE:LOCATION]:0.33, reddu(Reds):771, kankoku(Korea):644,rˆıgu(league) · · · [CT:ORGANIZATION]:0.30, · * “[]” and “&lt;&gt;” denote dictionary-based and semantic feature-based analysis respectively. For details see (Sasano et al., 2004). large corpora. Dictionary definition sentences are an informative resource to recognize obligatory cases of nouns. However, it is difficult to resolve associative anaphora by using a dictionary as it is, because all nouns in a definition sentence are not an obligatory case, and only the frequency information of noun phrases tells us which is the obligatory case. On the other hand, a simple method that just collects and clusters “Nm no Nh” phrases based on some similarity measure of nouns cannot construct comprehensive nominal case frames, because of polysemy and multiple obligatory cases. Fo</context>
<context position="33955" citStr="Sasano et al., 2004" startWordPosition="5321" endWordPosition="5324">ey basically used all “Nm no Nh” phrases from corpora as a lexical knowledge, and used rule-based approach. They obtained a recall of 0.63 and a precision of 0.68 by using examples of “X no Y” (Y of X), a recall of 0.71 and a precision of 0.82 by assuming ideal nominal case frames. One reason of such high performance may be that they considered referential properties of noun phrases, such as generic, indefinite, and definite, while our model does not. We can also say that their experiments were conducted on small and supposedly easy corpora. Half of their corpora 7Murata et al. (1999) and we (Sasano et al., 2004) used the terminology indirect anaphora, but concerned with the same phenomena as we concerned with in this paper. Table 6: Causes of false negatives. Causes Num Filtered by salience score 22 (15) Judge as non-anaphoric 13 (14) Select false antecedents 13 (13) Case frame sparseness 5 (5) Total 53 (47) *“()“ denotes the number of causes when using correct zero anaphora tags. were occupied by fairy tale, against which domain specific rules are considered to be effective. We proposed a rule-based approach for associative anaphora resolution based on automatically acquired nominal case frames (Sas</context>
</contexts>
<marker>Sasano, Kawahara, Kurohashi, 2004</marker>
<rawString>Ryohei Sasano, Daisuke Kawahara, and Sadao Kurohashi. 2004. Automatic construction of nominal case frames and its application to indirect anaphora resolution. In Proc. of COLING’04, pages 1201– 1207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryohei Sasano</author>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
</authors>
<title>A fully-lexicalized probabilistic model for japanese zero anaphora resolution.</title>
<date>2008</date>
<booktitle>In Proc. of COLING’08,</booktitle>
<pages>769--776</pages>
<contexts>
<context position="15060" citStr="Sasano et al., 2008" startWordPosition="2341" endWordPosition="2344">” We also generalized NEs. We used a common standard NE definition for Japanese provided by IREX workshop (1999). We first recognized NEs in the source corpus by using an NE recognizer (Sasano and Kurohashi, 2008), and then constructed NCFs from the NE-recognized corpus. As well as categories, for each NE class, we calculated the NE rate among all case slot examples, and added it to the case slot as “[NE:PERSON]:0.22.” The generalized examples are also included in Table 1. 3 Probabilistic Model In this study, we apply a lexicalized probabilistic model for zero anaphora resolution proposed in (Sasano et al., 2008) to associative anaphora resolution. 3.1 A Lexicalized Probabilistic Model for Zero Anaphora Resolution In English, overt pronouns such as “she” and definite noun phrases such as “the company” are anaphors that refer to preceding entities (antecedents). On the other hand, in Japanese, anaphors are often omitted, which are called zero pronouns, and zero anaphora resolution is one of the most important techniques for semantic analysis in Japanese. Here, we introduce our model for zero anaphora resolution (Sasano et al., 2008). This model first resolves coreference and identifies discourse entiti</context>
<context position="17760" citStr="Sasano et al., 2008" startWordPosition="2761" endWordPosition="2764">by using case structure analysis of large raw corpora. P(DCAk, IA|CFl) denotes the probability to generate direct case assignment and input arguments when a case frame is given, and estimated by using case structure analysis of large raw corpora, the frequency of a case slot example in the automatically constructed verbal case frames, and the web corpus in which the relation between a surface case marker and a case slot is manually annotated. P(ICAk|ENT, CFl, DCAk) denotes the probability to generate indirect case assignment when existing discourse entities, a case frame and 4For details see (Sasano et al., 2008). Toyota launched the hybrid car Prius in 1997. Φstarted selling Φ2 overseas in 2000. Figure 1: An example of correspondences between verbal case frames and discourse entities. direct case assignments are given, and estimated by using several preferences on the relation between a zero pronoun and an antecedent, such as a lexical preference, a surface case preferences, and a locational preference. For example, the lexical preference represents how likely an entity that contains nj. as a content part is considered to be an antecedent and is estimated by the following equation. where, the functio</context>
<context position="30840" citStr="Sasano et al., 2008" startWordPosition="4824" endWordPosition="4827">8 (51/110) (51/153) √ √ 0.518 0.363 0.427 (57/110) (57/157) CT: Using examples generalized by categories. NE: Using examples generalized by named entities. * The average of 10 trials is shown. F-measure was about 0.14 lower than the method using generalized examples, and we can also confirm the effectiveness of the generalized examples. While generalization of categories much improved the F-measure, generalization of NEs contributed little. This is because the NE rate was smaller than the common noun rate, and so the effect was limited. This tendency was also seen in zero anaphora resolution (Sasano et al., 2008). In order to investigate the effects of zero anaphora resolution, we tested our model under three conditions: without zero anaphora resolution (no resolution), with zero anaphora resolution (automatically resolved), and with using correct zero anaphora relations that are manually tagged (manually identified). The performance of automatic zero anaphora resolution resulted in a recall of 0.353, a precision of 0.375, and an F-measure of 0.364. Table 5 shows the experimental results. To resolve associative anaphora simultaneously with zero anaphora improved F-measure by 0.072; using correct zero </context>
</contexts>
<marker>Sasano, Kawahara, Kurohashi, 2008</marker>
<rawString>Ryohei Sasano, Daisuke Kawahara, and Sadao Kurohashi. 2008. A fully-lexicalized probabilistic model for japanese zero anaphora resolution. In Proc. of COLING’08, pages 769–776.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryohei Sasano</author>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
</authors>
<title>The effect of corpus size on case frame acquisition for discourse analysis.</title>
<date>2009</date>
<booktitle>In Proc. of NAACL-HLT’09,</booktitle>
<pages>521--529</pages>
<contexts>
<context position="27945" citStr="Sasano et al., 2009" startWordPosition="4361" endWordPosition="4364">uto ⇐ no:Ken (indispensable) (ADN) (8) Kˆoen-ni ikuto benchi-ga atta. park went bench was (I went to the park. There was a bench in 0.) TAG: benchi ⇐ no:kˆoen (possible) (ADN) We used 62 documents for testing and used the other 124 documents for calculating several probabilities. In the 62 test documents, 110 associative anaphoric relations were tagged. Each parameter for the proposed model was estimated using maximum likelihood from raw corpora, the tagged corpus, and case frames. As verbal case frames, we used the case frames constructed from the Web corpus comprising 1.6 billion sentences (Sasano et al., 2009). In order to concentrate on associative anaphora resolution, we used the correct morphemes, named entities, syntactic structures, and coreference re1461 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Decay Rate α Figure 3: Experimental results of associative anaphora resolution on several salience decay rates α. lations that were annotated by hand. Since correct coreference relations were given, the number of created entities was the same between the gold standard data and the system output because zero anaphora and associative anaphora resolution did not create new entities. 4.2 Results Figure </context>
</contexts>
<marker>Sasano, Kawahara, Kurohashi, 2009</marker>
<rawString>Ryohei Sasano, Daisuke Kawahara, and Sadao Kurohashi. 2009. The effect of corpus size on case frame acquisition for discourse analysis. In Proc. of NAACL-HLT’09, pages 521–529.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Strube</author>
<author>Udo Hahn</author>
</authors>
<title>Functional centering – grounding referential coherence in information structure.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>3</issue>
<contexts>
<context position="2060" citStr="Strube and Hahn, 1999" startWordPosition="298" endWordPosition="301">that have an identity relation with their antecedents but the anaphor and its antecedent have different head 1The terminology that we use here is introduced by Hawkins (1978), which is also used in (Vieira et al., 2006). nouns. In this paper, we focus on associative anaphora in Japanese. Associative anaphora resolution is decomposed into two steps: acquiring lexical knowledge for associative anaphora resolution, and resolving associative anaphora using the acquired knowledge. Grammatical salience plays a lesser role for resolving anaphors with full lexical heads, than for pronominal anaphora (Strube and Hahn, 1999; Modjeska, 2002). Furthermore, since associative anaphors and their antecedents usually have different head nouns, string matching technique cannot be applied. Therefore, a large and diverse amount of lexical knowledge is essential to understand associative anaphora. For example, to recognize the meronymic relation between “a house” and “the roof” in (1), such knowledge as “a roof” is a part of a building or vehicle is required. To recognize the attributive relation between “Prius” and “the price” in (2), such knowledge as “price” is a price of some goods or service is required. (1) There was</context>
</contexts>
<marker>Strube, Hahn, 1999</marker>
<rawString>Michael Strube and Udo Hahn. 1999. Functional centering – grounding referential coherence in information structure. Computational Linguistics, 25(3):309–344.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Renata Vieira</author>
<author>Massimo Poesio</author>
</authors>
<title>An empirically based system for processing definite descriptions.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>4</issue>
<contexts>
<context position="2959" citStr="Vieira and Poesio, 2000" startWordPosition="444" endWordPosition="448">mple, to recognize the meronymic relation between “a house” and “the roof” in (1), such knowledge as “a roof” is a part of a building or vehicle is required. To recognize the attributive relation between “Prius” and “the price” in (2), such knowledge as “price” is a price of some goods or service is required. (1) There was a house. The roof was white. (2) Toyota launched the hybrid car Prius in 1997. The price was 21.5 million yen. To acquire such lexical knowledge, various studies have been carried out. Early studies used hand-crafted lexical knowledge such as WordNet (Strube and Hahn, 1999; Vieira and Poesio, 2000; Meyer and Dale, 2002), but obtained poor or mediocre results. Hence, Poesio et al. (2002) proposed to exploit “Nh of Nm” phrases in large corpora to resolve associative anaphora in English; Murata et al. (1999) proposed to exploit “Nm no Nh” phrases to resolve associative anaphora in Japanese. Here, the Japanese postposition “no” roughly corresponds to “of,” but it has 1455 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1455–1464, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP much broader usage. These studies obtained reasonable results, but </context>
</contexts>
<marker>Vieira, Poesio, 2000</marker>
<rawString>Renata Vieira and Massimo Poesio. 2000. An empirically based system for processing definite descriptions. Computational Linguistics, 26(4):539–592.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Renata Vieira</author>
<author>Eckhard Bick</author>
<author>Jorge Coelho</author>
<author>Vinicius Muller</author>
<author>Sandra Collovini</author>
<author>Jose Souza</author>
<author>Lucia Rino</author>
</authors>
<title>Semantic tagging for resolution of indirect anaphora.</title>
<date>2006</date>
<booktitle>In Proc. of the 7th SIGdial Workshop on Discourse and Dialogue,</booktitle>
<pages>76--79</pages>
<contexts>
<context position="1658" citStr="Vieira et al., 2006" startWordPosition="241" endWordPosition="244"> domains (Hahn et al., 1996; Murata et al., 1999; Poesio et al., 2004; Gasperin and Vieira, 2004; Gasperin and Briscoe, 2008). Usually bridging anaphora considers two types:1 associative anaphors are noun phrases (NPs) that have an antecedent that is necessary to their interpretation but the relation between the anaphor and its antecedent is different from identity; and indirect anaphors are those that have an identity relation with their antecedents but the anaphor and its antecedent have different head 1The terminology that we use here is introduced by Hawkins (1978), which is also used in (Vieira et al., 2006). nouns. In this paper, we focus on associative anaphora in Japanese. Associative anaphora resolution is decomposed into two steps: acquiring lexical knowledge for associative anaphora resolution, and resolving associative anaphora using the acquired knowledge. Grammatical salience plays a lesser role for resolving anaphors with full lexical heads, than for pronominal anaphora (Strube and Hahn, 1999; Modjeska, 2002). Furthermore, since associative anaphors and their antecedents usually have different head nouns, string matching technique cannot be applied. Therefore, a large and diverse amount</context>
</contexts>
<marker>Vieira, Bick, Coelho, Muller, Collovini, Souza, Rino, 2006</marker>
<rawString>Renata Vieira, Eckhard Bick, Jorge Coelho, Vinicius Muller, Sandra Collovini, Jose Souza, and Lucia Rino. 2006. Semantic tagging for resolution of indirect anaphora. In Proc. of the 7th SIGdial Workshop on Discourse and Dialogue, pages 76–79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitsuko Yamura-Takei</author>
</authors>
<title>Approaches to zero adnominal recognition.</title>
<date>2003</date>
<booktitle>In Proc. of ACL’03: Student Research Workshop,</booktitle>
<pages>87--94</pages>
<contexts>
<context position="4758" citStr="Yamura-Takei, 2003" startWordPosition="740" endWordPosition="741"> language dictionary and “Nm no Nh” phrases, and constructed NCFs from newspaper corpora. In this study, we aim to acquire a sufficient amount of lexical knowledge by constructing NCFs from the Web. As for associative anaphora resolution itself, we propose an integrated probabilistic model for zero anaphora and associative anaphora resolution, in which associative anaphora is regarded as a kind of zero anaphora and resolved by using the same model as zero anaphora. Our model assumes zero pronouns that represent indispensable entities of target noun phrases, which are called zero adnominal in (Yamura-Takei, 2003), and conducts zero pronoun resolution. Let us consider the associative anaphoric relation between “Prius” and “kakaku” (price). Although “kakaku” itself is considered as the anaphor from a conventional point of view (3a), our model assumes a zero pronoun 0 and considers it as the anaphor (3b). (3) a. Prius - kakaku (price) [antecedent: Prius, anaphor: kakaku (price)] b. Prius - (0-no) kakaku (price (of 0)) [antecedent: Prius, anaphor: φ] The point of this study is three-fold: the acquisition of the lexical knowledge for associative anaphora resolution from the Web, the application of zero ana</context>
</contexts>
<marker>Yamura-Takei, 2003</marker>
<rawString>Mitsuko Yamura-Takei. 2003. Approaches to zero adnominal recognition. In Proc. of ACL’03: Student Research Workshop, pages 87–94.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>