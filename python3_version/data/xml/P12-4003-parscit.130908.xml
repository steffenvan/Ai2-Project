<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.312627">
<title confidence="0.994069333333333">
Topic Models, Latent Space Models, Sparse Coding, and All That: A
systematic understanding of probabilistic semantic extraction in large
corpus
</title>
<author confidence="0.99904">
Eric Xing
</author>
<affiliation confidence="0.995979">
School of Computer Science
Carnegie Mellon University
</affiliation>
<sectionHeader confidence="0.987943" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999863346938776">
Probabilistic topic models have recently
gained much popularity in informational re-
trieval and related areas. Via such mod-
els, one can project high-dimensional objects
such as text documents into a low dimen-
sional space where their latent semantics are
captured and modeled; can integrate multiple
sources of information—to ”share statistical
strength” among components of a hierarchical
probabilistic model; and can structurally dis-
play and classify the otherwise unstructured
object collections. However, to many practi-
tioners, how topic models work, what to and
not to expect from a topic model, how is it dif-
ferent from and related to classical matrix al-
gebraic techniques such as LSI, NMF in NLP,
how to empower topic models to deal with
complex scenarios such as multimodal data,
contractual text in social media, evolving cor-
pus, or presence of supervision such as la-
beling and rating, how to make topic mod-
eling computationally tractable even on web-
scale data, etc., in a principled way, remain un-
clear. In this tutorial, I will demystify the con-
ceptual, mathematical, and computational is-
sues behind all such problems surrounding the
topic models and their applications by present-
ing a systematic overview of the mathemati-
cal foundation of topic modeling, and its con-
nections to a number of related methods pop-
ular in other fields such as the LDA, admix-
ture model, mixed membership model, latent
space models, and sparse coding. I will offer
a simple and unifying view of all these tech-
niques under the framework multi-view latent
space embedding, and online the roadmap of
model extension and algorithmic design to-
ward different applications in IR and NLP. A
main theme of this tutorial that tie together a
wide range of issues and problems will build
on the ”probabilistic graphical model” formal-
ism, a formalism that exploits the conjoined
talents of graph theory and probability theory
to build complex models out of simpler pieces.
I will use this formalism as a main aid to dis-
cuss both the mathematical underpinnings for
the models and the related computational is-
sues in a unified, simplistic, transparent, and
actionable fashion.
</bodyText>
<page confidence="0.830438">
3
</page>
<reference confidence="0.475195">
Tutorial Abstracts of ACL 2012, page 3,
Jeju, Republic of Korea, 8 July 2012. c�2012 Association for Computational Linguistics
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.792549">
<title confidence="0.932670333333333">Topic Models, Latent Space Models, Sparse Coding, and All That: systematic understanding of probabilistic semantic extraction in large corpus</title>
<author confidence="0.999768">Eric Xing</author>
<affiliation confidence="0.999713">School of Computer Carnegie Mellon University</affiliation>
<abstract confidence="0.99985072">Probabilistic topic models have recently gained much popularity in informational retrieval and related areas. Via such models, one can project high-dimensional objects such as text documents into a low dimensional space where their latent semantics are captured and modeled; can integrate multiple sources of information—to ”share statistical strength” among components of a hierarchical probabilistic model; and can structurally display and classify the otherwise unstructured object collections. However, to many practitioners, how topic models work, what to and not to expect from a topic model, how is it different from and related to classical matrix algebraic techniques such as LSI, NMF in NLP, how to empower topic models to deal with complex scenarios such as multimodal data, contractual text in social media, evolving corpus, or presence of supervision such as labeling and rating, how to make topic modeling computationally tractable even on webscale data, etc., in a principled way, remain unclear. In this tutorial, I will demystify the conceptual, mathematical, and computational issues behind all such problems surrounding the topic models and their applications by presenting a systematic overview of the mathematical foundation of topic modeling, and its connections to a number of related methods popular in other fields such as the LDA, admixture model, mixed membership model, latent space models, and sparse coding. I will offer a simple and unifying view of all these techniques under the framework multi-view latent space embedding, and online the roadmap of extension and algorithmic design toward different applications in IR and NLP. A main theme of this tutorial that tie together a wide range of issues and problems will build on the ”probabilistic graphical model” formalism, a formalism that exploits the conjoined talents of graph theory and probability theory to build complex models out of simpler pieces. I will use this formalism as a main aid to discuss both the mathematical underpinnings for the models and the related computational issues in a unified, simplistic, transparent, and actionable fashion.</abstract>
<note confidence="0.950183666666667">3 Abstracts of ACL page 3, Republic of Korea, 8 July 2012. Association for Computational Linguistics</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<booktitle>Tutorial Abstracts of ACL 2012,</booktitle>
<pages>3</pages>
<marker></marker>
<rawString>Tutorial Abstracts of ACL 2012, page 3,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeju</author>
</authors>
<date>2012</date>
<journal>Republic of Korea,</journal>
<booktitle>c�2012 Association for Computational Linguistics</booktitle>
<volume>8</volume>
<marker>Jeju, 2012</marker>
<rawString>Jeju, Republic of Korea, 8 July 2012. c�2012 Association for Computational Linguistics</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>