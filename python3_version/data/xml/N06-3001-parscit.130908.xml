<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.284839">
<title confidence="0.997769">
Incorporating Gesture and Gaze into Multimodal Models of
Human-to-Human Communication
</title>
<author confidence="0.99622">
Lei Chen
</author>
<affiliation confidence="0.9854525">
Dept. of Electrical and Computer Engineering
Purdue University
</affiliation>
<address confidence="0.826363">
West Lafayette, IN 47907
</address>
<email confidence="0.996672">
chenl@ecn.purdue.edu
</email>
<sectionHeader confidence="0.995593" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999962166666667">
Structural information in language is im-
portant for obtaining a better understand-
ing of a human communication (e.g., sen-
tence segmentation, speaker turns, and
topic segmentation). Human communica-
tion involves a variety of multimodal be-
haviors that signal both propositional con-
tent and structure, e.g., gesture, gaze, and
body posture. These non-verbal signals
have tight temporal and semantic links to
spoken content. In my thesis, I am work-
ing on incorporating non-verbal cues into
a multimodal model to better predict the
structural events to further improve the
understanding of human communication.
Some research results are summarized in
this document and my future research plan
is described.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999839348837209">
In human communication, ideas tend to unfold in
a structured way. For example, for an individual
speaker, he/she organizes his/her utterances into sen-
tences. When a speaker makes errors in the dy-
namic speech production process, he/she may cor-
rect these errors using a speech repair scheme. A
group of speakers in a meeting organize their ut-
terances by following a floor control scheme. All
these structures are helpful for building better mod-
els of human communication but are not explicit in
the spontaneous speech or the corresponding tran-
scription word string. In order to utilize these struc-
tures, it is necessary to first detect them, and to do
so as efficiently as possible. Utilization of various
kinds of knowledge is important; For example, lex-
ical and prosodic knowledge (Liu, 2004; Liu et al.,
2005) have been used to detect structural events.
Human communication tends to utilize not only
speech but also visual cues such as gesture, gaze,
and so on. Some studies (McNeill, 1992; Cassell
and Stone, 1999) suggest that gesture and speech
stem from a single underlying mental process, and
they are related both temporally and semantically.
Gestures play an important role in human commu-
nication but use quite different expressive mecha-
nisms than spoken language. Gaze has been found
to be widely used in coordinating multi-party con-
versations (Argyle and Cook, 1976; Novick, 2005).
Given the close relationship between non-verbal
cues and speech and the special expressive capac-
ity of non-verbal cues, we believe that these cues
are likely to provide additional important informa-
tion that can be exploited when modeling structural
events. Hence, in my Ph.D thesis, I have been in-
vestigating the combination of lexical, prosodic, and
non-verbal cues for detection of the following struc-
tural events: sentence units, speech repairs, and
meeting floor control.
This paper is organized as follows: Section 1 has
described the research goals of my thesis. Section 2
summarizes the efforts made related to these goals.
Section 3 lays out the research work needed to com-
plete my thesis.
</bodyText>
<sectionHeader confidence="0.992339" genericHeader="method">
2 Completed Works
</sectionHeader>
<bodyText confidence="0.996867666666667">
Our previous research efforts related to multimodal
analysis of human communication can be roughly
grouped to three fields: (1) multimodal corpus col-
</bodyText>
<page confidence="0.976441">
211
</page>
<note confidence="0.9372465">
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 211–214,
New York, June 2006. c�2006 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999811">
Figure 1: VACE meeting corpus production
</figureCaption>
<bodyText confidence="0.999967583333333">
lection, annotation, and data processing, (2) mea-
surement studies to enrich knowledge of non-verbal
cues to structural events, and (3) model construc-
tion using a data-driven approach. Utilizing non-
verbal cues in human communication processing is
quite new and there is no standard data or off-the-
shelf evaluation method. Hence, the first part of my
research has focused on corpus building. Through
measurement investigations, we then obtain a bet-
ter understanding of the non-verbal cues associated
with structural events in order to model those struc-
tural events more effectively.
</bodyText>
<subsectionHeader confidence="0.980302">
2.1 Multimodal Corpus Collection
</subsectionHeader>
<bodyText confidence="0.99998802631579">
Under NSF KDI award (Quek and et al., ), we col-
lected a multimodal dialogue corpus. The corpus
contains calibrated stereo video recordings, time-
aligned word transcriptions, prosodic analyses, and
hand positions tracked by a video tracking algo-
rithm (Quek et al., 2002). To improve the speed
of producing a corpus while maintaining its qual-
ity, we have investigated factors impacting the ac-
curacy of the forced alignment of transcriptions to
audio files (Chen et al., 2004a).
Meetings, in which several participants commu-
nicate with each other, play an important role in our
daily life but increase the challenges to current infor-
mation processing techniques. Understanding hu-
man multimodal communicative behavior, and how
witting and unwitting visual displays (e.g., gesture,
head orientation, gaze) relate to spoken content is
critical to the analysis of meetings. These multi-
modal behaviors may reveal static and dynamic so-
cial structure of the meeting participants, the flow
of topics being discussed, the control of floor of
the meeting, and so on. For this purpose, we have
been collecting a multimodal meeting corpus un-
der the sponsorship of ARDA VACE II (Chen et al.,
2005). In a room equipped with synchronized mul-
tichannel audio,video and motion-tracking record-
ing devices, participants (from 5 to 8 civilian, mil-
itary, or mixed) engage in planning exercises, such
as managing rocket launch emergency, exploring a
foreign weapon component, and collaborating to se-
lect awardees for fellowships. we have collected and
continued to do multichannel time synchronized au-
dio and video recordings. Using a series of audio
and video processing techniques, we obtain the word
transcriptions and prosodic features, as well as head,
torso and hand 3D tracking traces from visual track-
ers and Vicon motion capture device. Figure 1 de-
picts our meeting corpus collection process.
</bodyText>
<subsectionHeader confidence="0.993532">
2.2 Gesture Patterns during Speech Repairs
</subsectionHeader>
<bodyText confidence="0.9998305">
In the dynamic speech production process, speak-
ers may make errors or totally change the content
of what is being expressed. In either of these cases,
speakers need refocus or revise what they are saying
</bodyText>
<page confidence="0.991481">
212
</page>
<bodyText confidence="0.999994909090909">
and therefore speech repairs appear in overt speech.
A typical speech repair contains a reparandum, an
optional editing phrase, and a correction. Based
on the relationship between the reparandum and the
correction, speech repairs can be classified into three
types: repetitions, content replacements, and false
starts. Since utterance content has been modified
in last two repair types, we call them content mod-
ification (CM) repairs. We carried out a measure-
ment study (Chen et al., 2002) to identify patterns of
gestures that co-occur with speech repairs that can
be exploited by a multimodal processing system to
more effectively process spontaneous speech. We
observed that modification gestures (MGs), which
exhibit a change in gesture state during speech re-
pair, have a high correlation with content modifica-
tion (CM) speech repairs, but rarely occur with con-
tent repetitions. This study does not only provide ev-
idence that gesture and speech are tightly linked in
production, but also provides evidence that gestures
provide an important additional cue for identifying
speech repairs and their types.
</bodyText>
<subsectionHeader confidence="0.999567">
2.3 Incorporating Gesture in SU Detection
</subsectionHeader>
<bodyText confidence="0.999848545454546">
A sentence unit (SU) is defined as the complete ex-
pression of a speaker’s thought or idea. It can be ei-
ther a complete sentence or a semantically complete
smaller unit. We have conducted an experiment that
integrates lexical, prosodic and gestural cues in or-
der to more effectively detect sentence unit bound-
aries in conversational dialog (Chen et al., 2004b).
As can be seen in Figure 2, our multimodal model
combines lexical, prosodic, and gestural knowl-
edge sources, with each knowledge source imple-
mented as a separate model. A hidden event lan-
guage model (LM) was trained to serve as lexical
model (P (W, E)). Using a direct modeling ap-
proach (Shriberg and Stolcke, 2004), prosodic fea-
tures were extracted using the SRI prosodic fea-
ture extraction tool1 by collaborators at ICSI and
then were used to train a CART decision tree as the
prosodic model (P(EIF)). Similarly to the prosodic
model, we computed gesture features directly from
visual tracking measurements (Quek et al., 1999;
Bryll et al., 2001): 3D hand position, Hold (a state
when there is no hand motion beyond some adaptive
</bodyText>
<footnote confidence="0.66431">
1A similar prosody feature extraction tool has been devel-
oped in our lab (Huang et al., 2006) using Praat.
</footnote>
<bodyText confidence="0.951344307692308">
threshold results), and Effort (analogous to the ki-
netic energy of hand movement). Using gestural fea-
tures, we trained a CART tree to serve as the gestu-
ral model (P(EIG)). Finally, an HMM based model
combination scheme was used to integrate predic-
tions from individual models to obtain an overall SU
prediction (argmax(EIW, F, G)). In our investiga-
tions, we found that gesture features complement the
prosodic and lexical knowledge sources; by using
all of the knowledge sources, the model is able to
achieve the lowest overall detection error rate.
Figure 2: Data flow diagram of multimodal SU
model using lexical, prosodic and gestural cues
</bodyText>
<subsectionHeader confidence="0.981095">
2.4 Floor Control Investigation on Meetings
</subsectionHeader>
<bodyText confidence="0.999977277777778">
An underlying, auto-regulatory mechanism known
as “floor control”, allows participants communicate
with each other coherently and smoothly. A person
controlling the floor bears the burden of moving the
discourse along. By increasing our understanding of
floor control in meetings, there is a potential to im-
pact two active research areas: human-like conver-
sational agent design and automatic meeting analy-
sis. We have recently investigated floor control in
multi-party meetings (Chen et al., 2006). In particu-
lar, we analyzed patterns of speech (e.g., the use of
discourse markers) and visual cues (e.g., eye gaze
exchange, pointing gesture for next speaker) that are
often involved in floor control changes. From this
analysis, we identified some multimodal cues that
will be helpful for predicting floor control events.
Discourse markers are found to occur frequently at
the beginning of a floor. During floor transitions, the
</bodyText>
<page confidence="0.997883">
213
</page>
<bodyText confidence="0.999980857142857">
previous holder often gazes at the next floor holder
and vice verse. The well-known mutual gaze break
pattern in dyadic conversations is also found in some
meetings. A special participant, an active meeting
manager, is found to play a role in floor transitions.
Gesture cues are also found to play a role, especially
with respect to floor capturing gestures.
</bodyText>
<sectionHeader confidence="0.991787" genericHeader="method">
3 Research Directions
</sectionHeader>
<bodyText confidence="0.9999765">
In the next stage of my research, I will focus on inte-
grating previous efforts into a complete multimodal
model for structural event detection. In particular, I
will improve current gesture feature extraction, and
expand the non-verbal features to include both eye
gaze and body posture. I will also investigate alter-
native integration architectures to the HMM shown
in Figure 2. In my thesis, I hope to better understand
the role that the non-verbal cues play in assisting
structural event detection. My research is expected
to support adding multimodal perception capabili-
ties to current human communication systems that
rely mostly on speech. I am also interested in inves-
tigating mutual impacts among the structural events.
For example, we will study SUs and their relation-
ship to floor control structure. Given progress in
structural event detection in human communication,
I also plan to utilize the detected structural events
to further enhance meeting understanding. A par-
ticularly interesting task is to locate salient portions
of a meeting from multimodal cues (Chen, 2005) to
summarize it.
</bodyText>
<sectionHeader confidence="0.998583" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999794126984127">
M. Argyle and M. Cook. 1976. Gaze and Mutual Gaze.
Cambridge Univ. Press.
R. Bryll, F. Quek, and A. Esposito. 2001. Automatic
hand hold detection in natural conversation. In IEEE
Workshop on Cues in Communication, Kauai,Hawaii,
Dec.
J. Cassell and M. Stone. 1999. Living Hand to Mouth:
Psychological Theories about Speech and Gesture in
Interactive Dialogue Systems. In AAAI.
L. Chen, M. Harper, and F. Quek. 2002. Gesture pat-
terns during speech repairs. In Proc. of Int. Conf. on
Multimodal Interface (ICMI), Pittsburg, PA, Oct.
L. Chen, Y. Liu, M. Harper, E. Maia, and S. McRoy.
2004a. Evaluating factors impacting the accuracy of
forced alignments in a multimodal corpus. In Proc. of
Language Resource and Evaluation Conference, Lis-
bon, Portugal, June.
L. Chen, Y. Liu, M. Harper, and E. Shriberg. 2004b.
Multimodal model integration for sentence unit detec-
tion. In Proc. of Int. Conf. on Multimodal Interface
(ICMI), University Park, PA, Oct.
L. Chen, T.R. Rose, F. Parrill, X. Han, J. Tu, Z.Q. Huang,
I. Kimbara, H. Welji, M. Harper, F. Quek, D. McNeill,
S. Duncan, R. Tuttle, and T. Huang. 2005. VACE
multimodal meeting corpus. In Proceeding of MLMI
2005 Workshop.
L. Chen, M. Harper, A. Franklin, T. R. Rose, I. Kimbara,
Z. Q. Huang, and F. Quek. 2006. A multimodal anal-
ysis of floor control in meetings. In Proc. ofMLMI 06,
Washington, DC, USA, May.
L. Chen. 2005. Locating salient portions of meeting us-
ing multimodal cues. Research proposal submitted to
AMI training program, Dec.
Z. Q. Huang, L. Chen, and M. Harper. 2006. An open
source prosodic feature extraction tool. In Proc. of
Language Resource and Evaluation Conference, May
2006.
Y. Liu, E. Shriberg, A. Stolcke, B. Peskin, J. Ang, Hillard
D., M. Ostendorf, M. Tomalin, P. Woodland, and
M. Harper. 2005. Structural Metadata Research in
the EARS Program. In Proc. ofICASSP.
Y. Liu. 2004. Structural Event Detection for Rich Tran-
scription of Speech. Ph.D. thesis, Purdue University.
D. McNeill. 1992. Hand and Mind: What Gestures Re-
veal about Thought. Univ. Chicago Press.
D. G. Novick. 2005. Models of gaze in multi-party dis-
course. In Proc. of CHI 2005 Workshop on the Virtu-
ality Continuum Revisted, Portland OR, April 3.
F. Quek and et al. KDI: Cross-model Analysis
Signal and Sense- Data and Computational Re-
sources for Gesture, Speech and Gaze Research,
http://vislab.cs.vt.edu/kdi.
F. Quek, R. Bryll, and X. F. Ma. 1999. A parallel algo-
righm for dynamic gesture tracking. In ICCV Work-
shop on RATFG-RTS, Gorfu,Greece.
F. Quek, D. McNeill, R. Bryll, S. Duncan, X. Ma, C. Kir-
bas, K. E. McCullough, and R. Ansari. 2002. Mul-
timodal human discourse: gesture and speech. ACM
Trans. Comput.-Hum. Interact., 9(3):171–193.
E. Shriberg and A. Stolcke. 2004. Direct modeling of
prosody: An overview of applications in automatic
speech processing. In International Conference on
Speech Prosody.
</reference>
<page confidence="0.998953">
214
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.718597">
<title confidence="0.9935415">Incorporating Gesture and Gaze into Multimodal Models Human-to-Human Communication</title>
<author confidence="0.975555">Lei</author>
<affiliation confidence="0.9021475">Dept. of Electrical and Computer Purdue</affiliation>
<address confidence="0.876751">West Lafayette, IN</address>
<email confidence="0.999217">chenl@ecn.purdue.edu</email>
<abstract confidence="0.997595578947368">Structural information in language is important for obtaining a better understanding of a human communication (e.g., sentence segmentation, speaker turns, and topic segmentation). Human communication involves a variety of multimodal behaviors that signal both propositional content and structure, e.g., gesture, gaze, and body posture. These non-verbal signals have tight temporal and semantic links to spoken content. In my thesis, I am working on incorporating non-verbal cues into a multimodal model to better predict the structural events to further improve the understanding of human communication. Some research results are summarized in this document and my future research plan is described.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Argyle</author>
<author>M Cook</author>
</authors>
<title>Gaze and Mutual Gaze.</title>
<date>1976</date>
<publisher>Cambridge Univ. Press.</publisher>
<contexts>
<context position="2303" citStr="Argyle and Cook, 1976" startWordPosition="355" endWordPosition="358">cal and prosodic knowledge (Liu, 2004; Liu et al., 2005) have been used to detect structural events. Human communication tends to utilize not only speech but also visual cues such as gesture, gaze, and so on. Some studies (McNeill, 1992; Cassell and Stone, 1999) suggest that gesture and speech stem from a single underlying mental process, and they are related both temporally and semantically. Gestures play an important role in human communication but use quite different expressive mechanisms than spoken language. Gaze has been found to be widely used in coordinating multi-party conversations (Argyle and Cook, 1976; Novick, 2005). Given the close relationship between non-verbal cues and speech and the special expressive capacity of non-verbal cues, we believe that these cues are likely to provide additional important information that can be exploited when modeling structural events. Hence, in my Ph.D thesis, I have been investigating the combination of lexical, prosodic, and non-verbal cues for detection of the following structural events: sentence units, speech repairs, and meeting floor control. This paper is organized as follows: Section 1 has described the research goals of my thesis. Section 2 summ</context>
</contexts>
<marker>Argyle, Cook, 1976</marker>
<rawString>M. Argyle and M. Cook. 1976. Gaze and Mutual Gaze. Cambridge Univ. Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bryll</author>
<author>F Quek</author>
<author>A Esposito</author>
</authors>
<title>Automatic hand hold detection in natural conversation.</title>
<date>2001</date>
<booktitle>In IEEE Workshop on Cues in Communication, Kauai,Hawaii,</booktitle>
<contexts>
<context position="8312" citStr="Bryll et al., 2001" startWordPosition="1305" endWordPosition="1308">ur multimodal model combines lexical, prosodic, and gestural knowledge sources, with each knowledge source implemented as a separate model. A hidden event language model (LM) was trained to serve as lexical model (P (W, E)). Using a direct modeling approach (Shriberg and Stolcke, 2004), prosodic features were extracted using the SRI prosodic feature extraction tool1 by collaborators at ICSI and then were used to train a CART decision tree as the prosodic model (P(EIF)). Similarly to the prosodic model, we computed gesture features directly from visual tracking measurements (Quek et al., 1999; Bryll et al., 2001): 3D hand position, Hold (a state when there is no hand motion beyond some adaptive 1A similar prosody feature extraction tool has been developed in our lab (Huang et al., 2006) using Praat. threshold results), and Effort (analogous to the kinetic energy of hand movement). Using gestural features, we trained a CART tree to serve as the gestural model (P(EIG)). Finally, an HMM based model combination scheme was used to integrate predictions from individual models to obtain an overall SU prediction (argmax(EIW, F, G)). In our investigations, we found that gesture features complement the prosodic</context>
</contexts>
<marker>Bryll, Quek, Esposito, 2001</marker>
<rawString>R. Bryll, F. Quek, and A. Esposito. 2001. Automatic hand hold detection in natural conversation. In IEEE Workshop on Cues in Communication, Kauai,Hawaii, Dec.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cassell</author>
<author>M Stone</author>
</authors>
<title>Living Hand to Mouth: Psychological Theories about Speech and Gesture in Interactive Dialogue Systems.</title>
<date>1999</date>
<booktitle>In AAAI.</booktitle>
<contexts>
<context position="1944" citStr="Cassell and Stone, 1999" startWordPosition="299" endWordPosition="302">ese structures are helpful for building better models of human communication but are not explicit in the spontaneous speech or the corresponding transcription word string. In order to utilize these structures, it is necessary to first detect them, and to do so as efficiently as possible. Utilization of various kinds of knowledge is important; For example, lexical and prosodic knowledge (Liu, 2004; Liu et al., 2005) have been used to detect structural events. Human communication tends to utilize not only speech but also visual cues such as gesture, gaze, and so on. Some studies (McNeill, 1992; Cassell and Stone, 1999) suggest that gesture and speech stem from a single underlying mental process, and they are related both temporally and semantically. Gestures play an important role in human communication but use quite different expressive mechanisms than spoken language. Gaze has been found to be widely used in coordinating multi-party conversations (Argyle and Cook, 1976; Novick, 2005). Given the close relationship between non-verbal cues and speech and the special expressive capacity of non-verbal cues, we believe that these cues are likely to provide additional important information that can be exploited </context>
</contexts>
<marker>Cassell, Stone, 1999</marker>
<rawString>J. Cassell and M. Stone. 1999. Living Hand to Mouth: Psychological Theories about Speech and Gesture in Interactive Dialogue Systems. In AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Chen</author>
<author>M Harper</author>
<author>F Quek</author>
</authors>
<title>Gesture patterns during speech repairs.</title>
<date>2002</date>
<booktitle>In Proc. of Int. Conf. on Multimodal Interface (ICMI),</booktitle>
<location>Pittsburg, PA,</location>
<contexts>
<context position="6642" citStr="Chen et al., 2002" startWordPosition="1034" endWordPosition="1037">hange the content of what is being expressed. In either of these cases, speakers need refocus or revise what they are saying 212 and therefore speech repairs appear in overt speech. A typical speech repair contains a reparandum, an optional editing phrase, and a correction. Based on the relationship between the reparandum and the correction, speech repairs can be classified into three types: repetitions, content replacements, and false starts. Since utterance content has been modified in last two repair types, we call them content modification (CM) repairs. We carried out a measurement study (Chen et al., 2002) to identify patterns of gestures that co-occur with speech repairs that can be exploited by a multimodal processing system to more effectively process spontaneous speech. We observed that modification gestures (MGs), which exhibit a change in gesture state during speech repair, have a high correlation with content modification (CM) speech repairs, but rarely occur with content repetitions. This study does not only provide evidence that gesture and speech are tightly linked in production, but also provides evidence that gestures provide an important additional cue for identifying speech repair</context>
</contexts>
<marker>Chen, Harper, Quek, 2002</marker>
<rawString>L. Chen, M. Harper, and F. Quek. 2002. Gesture patterns during speech repairs. In Proc. of Int. Conf. on Multimodal Interface (ICMI), Pittsburg, PA, Oct.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Chen</author>
<author>Y Liu</author>
<author>M Harper</author>
<author>E Maia</author>
<author>S McRoy</author>
</authors>
<title>Evaluating factors impacting the accuracy of forced alignments in a multimodal corpus.</title>
<date>2004</date>
<booktitle>In Proc. of Language Resource and Evaluation Conference,</booktitle>
<location>Lisbon, Portugal,</location>
<contexts>
<context position="4498" citStr="Chen et al., 2004" startWordPosition="697" endWordPosition="700">he non-verbal cues associated with structural events in order to model those structural events more effectively. 2.1 Multimodal Corpus Collection Under NSF KDI award (Quek and et al., ), we collected a multimodal dialogue corpus. The corpus contains calibrated stereo video recordings, timealigned word transcriptions, prosodic analyses, and hand positions tracked by a video tracking algorithm (Quek et al., 2002). To improve the speed of producing a corpus while maintaining its quality, we have investigated factors impacting the accuracy of the forced alignment of transcriptions to audio files (Chen et al., 2004a). Meetings, in which several participants communicate with each other, play an important role in our daily life but increase the challenges to current information processing techniques. Understanding human multimodal communicative behavior, and how witting and unwitting visual displays (e.g., gesture, head orientation, gaze) relate to spoken content is critical to the analysis of meetings. These multimodal behaviors may reveal static and dynamic social structure of the meeting participants, the flow of topics being discussed, the control of floor of the meeting, and so on. For this purpose, </context>
<context position="7660" citStr="Chen et al., 2004" startWordPosition="1196" endWordPosition="1199">udy does not only provide evidence that gesture and speech are tightly linked in production, but also provides evidence that gestures provide an important additional cue for identifying speech repairs and their types. 2.3 Incorporating Gesture in SU Detection A sentence unit (SU) is defined as the complete expression of a speaker’s thought or idea. It can be either a complete sentence or a semantically complete smaller unit. We have conducted an experiment that integrates lexical, prosodic and gestural cues in order to more effectively detect sentence unit boundaries in conversational dialog (Chen et al., 2004b). As can be seen in Figure 2, our multimodal model combines lexical, prosodic, and gestural knowledge sources, with each knowledge source implemented as a separate model. A hidden event language model (LM) was trained to serve as lexical model (P (W, E)). Using a direct modeling approach (Shriberg and Stolcke, 2004), prosodic features were extracted using the SRI prosodic feature extraction tool1 by collaborators at ICSI and then were used to train a CART decision tree as the prosodic model (P(EIF)). Similarly to the prosodic model, we computed gesture features directly from visual tracking </context>
</contexts>
<marker>Chen, Liu, Harper, Maia, McRoy, 2004</marker>
<rawString>L. Chen, Y. Liu, M. Harper, E. Maia, and S. McRoy. 2004a. Evaluating factors impacting the accuracy of forced alignments in a multimodal corpus. In Proc. of Language Resource and Evaluation Conference, Lisbon, Portugal, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Chen</author>
<author>Y Liu</author>
<author>M Harper</author>
<author>E Shriberg</author>
</authors>
<title>Multimodal model integration for sentence unit detection.</title>
<date>2004</date>
<booktitle>In Proc. of Int. Conf. on Multimodal Interface (ICMI),</booktitle>
<location>University Park, PA,</location>
<contexts>
<context position="4498" citStr="Chen et al., 2004" startWordPosition="697" endWordPosition="700">he non-verbal cues associated with structural events in order to model those structural events more effectively. 2.1 Multimodal Corpus Collection Under NSF KDI award (Quek and et al., ), we collected a multimodal dialogue corpus. The corpus contains calibrated stereo video recordings, timealigned word transcriptions, prosodic analyses, and hand positions tracked by a video tracking algorithm (Quek et al., 2002). To improve the speed of producing a corpus while maintaining its quality, we have investigated factors impacting the accuracy of the forced alignment of transcriptions to audio files (Chen et al., 2004a). Meetings, in which several participants communicate with each other, play an important role in our daily life but increase the challenges to current information processing techniques. Understanding human multimodal communicative behavior, and how witting and unwitting visual displays (e.g., gesture, head orientation, gaze) relate to spoken content is critical to the analysis of meetings. These multimodal behaviors may reveal static and dynamic social structure of the meeting participants, the flow of topics being discussed, the control of floor of the meeting, and so on. For this purpose, </context>
<context position="7660" citStr="Chen et al., 2004" startWordPosition="1196" endWordPosition="1199">udy does not only provide evidence that gesture and speech are tightly linked in production, but also provides evidence that gestures provide an important additional cue for identifying speech repairs and their types. 2.3 Incorporating Gesture in SU Detection A sentence unit (SU) is defined as the complete expression of a speaker’s thought or idea. It can be either a complete sentence or a semantically complete smaller unit. We have conducted an experiment that integrates lexical, prosodic and gestural cues in order to more effectively detect sentence unit boundaries in conversational dialog (Chen et al., 2004b). As can be seen in Figure 2, our multimodal model combines lexical, prosodic, and gestural knowledge sources, with each knowledge source implemented as a separate model. A hidden event language model (LM) was trained to serve as lexical model (P (W, E)). Using a direct modeling approach (Shriberg and Stolcke, 2004), prosodic features were extracted using the SRI prosodic feature extraction tool1 by collaborators at ICSI and then were used to train a CART decision tree as the prosodic model (P(EIF)). Similarly to the prosodic model, we computed gesture features directly from visual tracking </context>
</contexts>
<marker>Chen, Liu, Harper, Shriberg, 2004</marker>
<rawString>L. Chen, Y. Liu, M. Harper, and E. Shriberg. 2004b. Multimodal model integration for sentence unit detection. In Proc. of Int. Conf. on Multimodal Interface (ICMI), University Park, PA, Oct.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Chen</author>
<author>T R Rose</author>
<author>F Parrill</author>
<author>X Han</author>
<author>J Tu</author>
<author>Z Q Huang</author>
<author>I Kimbara</author>
<author>H Welji</author>
<author>M Harper</author>
<author>F Quek</author>
<author>D McNeill</author>
<author>S Duncan</author>
<author>R Tuttle</author>
<author>T Huang</author>
</authors>
<title>VACE multimodal meeting corpus.</title>
<date>2005</date>
<booktitle>In Proceeding of MLMI 2005 Workshop.</booktitle>
<contexts>
<context position="5207" citStr="Chen et al., 2005" startWordPosition="809" endWordPosition="812">role in our daily life but increase the challenges to current information processing techniques. Understanding human multimodal communicative behavior, and how witting and unwitting visual displays (e.g., gesture, head orientation, gaze) relate to spoken content is critical to the analysis of meetings. These multimodal behaviors may reveal static and dynamic social structure of the meeting participants, the flow of topics being discussed, the control of floor of the meeting, and so on. For this purpose, we have been collecting a multimodal meeting corpus under the sponsorship of ARDA VACE II (Chen et al., 2005). In a room equipped with synchronized multichannel audio,video and motion-tracking recording devices, participants (from 5 to 8 civilian, military, or mixed) engage in planning exercises, such as managing rocket launch emergency, exploring a foreign weapon component, and collaborating to select awardees for fellowships. we have collected and continued to do multichannel time synchronized audio and video recordings. Using a series of audio and video processing techniques, we obtain the word transcriptions and prosodic features, as well as head, torso and hand 3D tracking traces from visual tra</context>
</contexts>
<marker>Chen, Rose, Parrill, Han, Tu, Huang, Kimbara, Welji, Harper, Quek, McNeill, Duncan, Tuttle, Huang, 2005</marker>
<rawString>L. Chen, T.R. Rose, F. Parrill, X. Han, J. Tu, Z.Q. Huang, I. Kimbara, H. Welji, M. Harper, F. Quek, D. McNeill, S. Duncan, R. Tuttle, and T. Huang. 2005. VACE multimodal meeting corpus. In Proceeding of MLMI 2005 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Chen</author>
<author>M Harper</author>
<author>A Franklin</author>
<author>T R Rose</author>
<author>I Kimbara</author>
<author>Z Q Huang</author>
<author>F Quek</author>
</authors>
<title>A multimodal analysis of floor control in meetings.</title>
<date>2006</date>
<booktitle>In Proc. ofMLMI 06,</booktitle>
<location>Washington, DC, USA,</location>
<contexts>
<context position="9687" citStr="Chen et al., 2006" startWordPosition="1523" endWordPosition="1526">low diagram of multimodal SU model using lexical, prosodic and gestural cues 2.4 Floor Control Investigation on Meetings An underlying, auto-regulatory mechanism known as “floor control”, allows participants communicate with each other coherently and smoothly. A person controlling the floor bears the burden of moving the discourse along. By increasing our understanding of floor control in meetings, there is a potential to impact two active research areas: human-like conversational agent design and automatic meeting analysis. We have recently investigated floor control in multi-party meetings (Chen et al., 2006). In particular, we analyzed patterns of speech (e.g., the use of discourse markers) and visual cues (e.g., eye gaze exchange, pointing gesture for next speaker) that are often involved in floor control changes. From this analysis, we identified some multimodal cues that will be helpful for predicting floor control events. Discourse markers are found to occur frequently at the beginning of a floor. During floor transitions, the 213 previous holder often gazes at the next floor holder and vice verse. The well-known mutual gaze break pattern in dyadic conversations is also found in some meetings</context>
</contexts>
<marker>Chen, Harper, Franklin, Rose, Kimbara, Huang, Quek, 2006</marker>
<rawString>L. Chen, M. Harper, A. Franklin, T. R. Rose, I. Kimbara, Z. Q. Huang, and F. Quek. 2006. A multimodal analysis of floor control in meetings. In Proc. ofMLMI 06, Washington, DC, USA, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Chen</author>
</authors>
<title>Locating salient portions of meeting using multimodal cues. Research proposal submitted to AMI training program,</title>
<date>2005</date>
<marker>Chen, 2005</marker>
<rawString>L. Chen. 2005. Locating salient portions of meeting using multimodal cues. Research proposal submitted to AMI training program, Dec.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Q Huang</author>
<author>L Chen</author>
<author>M Harper</author>
</authors>
<title>An open source prosodic feature extraction tool.</title>
<date>2006</date>
<booktitle>In Proc. of Language Resource and Evaluation Conference,</booktitle>
<contexts>
<context position="8489" citStr="Huang et al., 2006" startWordPosition="1337" endWordPosition="1340">s trained to serve as lexical model (P (W, E)). Using a direct modeling approach (Shriberg and Stolcke, 2004), prosodic features were extracted using the SRI prosodic feature extraction tool1 by collaborators at ICSI and then were used to train a CART decision tree as the prosodic model (P(EIF)). Similarly to the prosodic model, we computed gesture features directly from visual tracking measurements (Quek et al., 1999; Bryll et al., 2001): 3D hand position, Hold (a state when there is no hand motion beyond some adaptive 1A similar prosody feature extraction tool has been developed in our lab (Huang et al., 2006) using Praat. threshold results), and Effort (analogous to the kinetic energy of hand movement). Using gestural features, we trained a CART tree to serve as the gestural model (P(EIG)). Finally, an HMM based model combination scheme was used to integrate predictions from individual models to obtain an overall SU prediction (argmax(EIW, F, G)). In our investigations, we found that gesture features complement the prosodic and lexical knowledge sources; by using all of the knowledge sources, the model is able to achieve the lowest overall detection error rate. Figure 2: Data flow diagram of multi</context>
</contexts>
<marker>Huang, Chen, Harper, 2006</marker>
<rawString>Z. Q. Huang, L. Chen, and M. Harper. 2006. An open source prosodic feature extraction tool. In Proc. of Language Resource and Evaluation Conference, May 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Liu</author>
<author>E Shriberg</author>
<author>A Stolcke</author>
<author>B Peskin</author>
<author>J Ang</author>
<author>D Hillard</author>
<author>M Ostendorf</author>
<author>M Tomalin</author>
<author>P Woodland</author>
<author>M Harper</author>
</authors>
<date>2005</date>
<booktitle>Structural Metadata Research in the EARS Program. In Proc. ofICASSP.</booktitle>
<contexts>
<context position="1738" citStr="Liu et al., 2005" startWordPosition="265" endWordPosition="268">dynamic speech production process, he/she may correct these errors using a speech repair scheme. A group of speakers in a meeting organize their utterances by following a floor control scheme. All these structures are helpful for building better models of human communication but are not explicit in the spontaneous speech or the corresponding transcription word string. In order to utilize these structures, it is necessary to first detect them, and to do so as efficiently as possible. Utilization of various kinds of knowledge is important; For example, lexical and prosodic knowledge (Liu, 2004; Liu et al., 2005) have been used to detect structural events. Human communication tends to utilize not only speech but also visual cues such as gesture, gaze, and so on. Some studies (McNeill, 1992; Cassell and Stone, 1999) suggest that gesture and speech stem from a single underlying mental process, and they are related both temporally and semantically. Gestures play an important role in human communication but use quite different expressive mechanisms than spoken language. Gaze has been found to be widely used in coordinating multi-party conversations (Argyle and Cook, 1976; Novick, 2005). Given the close re</context>
</contexts>
<marker>Liu, Shriberg, Stolcke, Peskin, Ang, Hillard, Ostendorf, Tomalin, Woodland, Harper, 2005</marker>
<rawString>Y. Liu, E. Shriberg, A. Stolcke, B. Peskin, J. Ang, Hillard D., M. Ostendorf, M. Tomalin, P. Woodland, and M. Harper. 2005. Structural Metadata Research in the EARS Program. In Proc. ofICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Liu</author>
</authors>
<title>Structural Event Detection for Rich Transcription of Speech.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<publisher>Univ. Chicago Press.</publisher>
<institution>Purdue University.</institution>
<contexts>
<context position="1719" citStr="Liu, 2004" startWordPosition="263" endWordPosition="264">ors in the dynamic speech production process, he/she may correct these errors using a speech repair scheme. A group of speakers in a meeting organize their utterances by following a floor control scheme. All these structures are helpful for building better models of human communication but are not explicit in the spontaneous speech or the corresponding transcription word string. In order to utilize these structures, it is necessary to first detect them, and to do so as efficiently as possible. Utilization of various kinds of knowledge is important; For example, lexical and prosodic knowledge (Liu, 2004; Liu et al., 2005) have been used to detect structural events. Human communication tends to utilize not only speech but also visual cues such as gesture, gaze, and so on. Some studies (McNeill, 1992; Cassell and Stone, 1999) suggest that gesture and speech stem from a single underlying mental process, and they are related both temporally and semantically. Gestures play an important role in human communication but use quite different expressive mechanisms than spoken language. Gaze has been found to be widely used in coordinating multi-party conversations (Argyle and Cook, 1976; Novick, 2005).</context>
</contexts>
<marker>Liu, 2004</marker>
<rawString>Y. Liu. 2004. Structural Event Detection for Rich Transcription of Speech. Ph.D. thesis, Purdue University. D. McNeill. 1992. Hand and Mind: What Gestures Reveal about Thought. Univ. Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D G Novick</author>
</authors>
<title>Models of gaze in multi-party discourse.</title>
<date>2005</date>
<booktitle>In Proc. of CHI 2005 Workshop on the Virtuality Continuum Revisted,</booktitle>
<location>Portland OR,</location>
<contexts>
<context position="2318" citStr="Novick, 2005" startWordPosition="359" endWordPosition="360">dge (Liu, 2004; Liu et al., 2005) have been used to detect structural events. Human communication tends to utilize not only speech but also visual cues such as gesture, gaze, and so on. Some studies (McNeill, 1992; Cassell and Stone, 1999) suggest that gesture and speech stem from a single underlying mental process, and they are related both temporally and semantically. Gestures play an important role in human communication but use quite different expressive mechanisms than spoken language. Gaze has been found to be widely used in coordinating multi-party conversations (Argyle and Cook, 1976; Novick, 2005). Given the close relationship between non-verbal cues and speech and the special expressive capacity of non-verbal cues, we believe that these cues are likely to provide additional important information that can be exploited when modeling structural events. Hence, in my Ph.D thesis, I have been investigating the combination of lexical, prosodic, and non-verbal cues for detection of the following structural events: sentence units, speech repairs, and meeting floor control. This paper is organized as follows: Section 1 has described the research goals of my thesis. Section 2 summarizes the effo</context>
</contexts>
<marker>Novick, 2005</marker>
<rawString>D. G. Novick. 2005. Models of gaze in multi-party discourse. In Proc. of CHI 2005 Workshop on the Virtuality Continuum Revisted, Portland OR, April 3.</rawString>
</citation>
<citation valid="false">
<authors>
<author>F Quek</author>
</authors>
<title>KDI: Cross-model Analysis Signal and Sense-</title>
<booktitle>Data and Computational Resources for Gesture, Speech and Gaze Research,</booktitle>
<location>http://vislab.cs.vt.edu/kdi.</location>
<marker>Quek, </marker>
<rawString>F. Quek and et al. KDI: Cross-model Analysis Signal and Sense- Data and Computational Resources for Gesture, Speech and Gaze Research, http://vislab.cs.vt.edu/kdi.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Quek</author>
<author>R Bryll</author>
<author>X F Ma</author>
</authors>
<title>A parallel algorighm for dynamic gesture tracking.</title>
<date>1999</date>
<booktitle>In ICCV Workshop on RATFG-RTS, Gorfu,Greece.</booktitle>
<contexts>
<context position="8291" citStr="Quek et al., 1999" startWordPosition="1301" endWordPosition="1304">seen in Figure 2, our multimodal model combines lexical, prosodic, and gestural knowledge sources, with each knowledge source implemented as a separate model. A hidden event language model (LM) was trained to serve as lexical model (P (W, E)). Using a direct modeling approach (Shriberg and Stolcke, 2004), prosodic features were extracted using the SRI prosodic feature extraction tool1 by collaborators at ICSI and then were used to train a CART decision tree as the prosodic model (P(EIF)). Similarly to the prosodic model, we computed gesture features directly from visual tracking measurements (Quek et al., 1999; Bryll et al., 2001): 3D hand position, Hold (a state when there is no hand motion beyond some adaptive 1A similar prosody feature extraction tool has been developed in our lab (Huang et al., 2006) using Praat. threshold results), and Effort (analogous to the kinetic energy of hand movement). Using gestural features, we trained a CART tree to serve as the gestural model (P(EIG)). Finally, an HMM based model combination scheme was used to integrate predictions from individual models to obtain an overall SU prediction (argmax(EIW, F, G)). In our investigations, we found that gesture features co</context>
</contexts>
<marker>Quek, Bryll, Ma, 1999</marker>
<rawString>F. Quek, R. Bryll, and X. F. Ma. 1999. A parallel algorighm for dynamic gesture tracking. In ICCV Workshop on RATFG-RTS, Gorfu,Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Quek</author>
<author>D McNeill</author>
<author>R Bryll</author>
<author>S Duncan</author>
<author>X Ma</author>
<author>C Kirbas</author>
<author>K E McCullough</author>
<author>R Ansari</author>
</authors>
<title>Multimodal human discourse: gesture and speech.</title>
<date>2002</date>
<journal>ACM Trans. Comput.-Hum. Interact.,</journal>
<volume>9</volume>
<issue>3</issue>
<contexts>
<context position="4295" citStr="Quek et al., 2002" startWordPosition="663" endWordPosition="666">e is no standard data or off-theshelf evaluation method. Hence, the first part of my research has focused on corpus building. Through measurement investigations, we then obtain a better understanding of the non-verbal cues associated with structural events in order to model those structural events more effectively. 2.1 Multimodal Corpus Collection Under NSF KDI award (Quek and et al., ), we collected a multimodal dialogue corpus. The corpus contains calibrated stereo video recordings, timealigned word transcriptions, prosodic analyses, and hand positions tracked by a video tracking algorithm (Quek et al., 2002). To improve the speed of producing a corpus while maintaining its quality, we have investigated factors impacting the accuracy of the forced alignment of transcriptions to audio files (Chen et al., 2004a). Meetings, in which several participants communicate with each other, play an important role in our daily life but increase the challenges to current information processing techniques. Understanding human multimodal communicative behavior, and how witting and unwitting visual displays (e.g., gesture, head orientation, gaze) relate to spoken content is critical to the analysis of meetings. Th</context>
</contexts>
<marker>Quek, McNeill, Bryll, Duncan, Ma, Kirbas, McCullough, Ansari, 2002</marker>
<rawString>F. Quek, D. McNeill, R. Bryll, S. Duncan, X. Ma, C. Kirbas, K. E. McCullough, and R. Ansari. 2002. Multimodal human discourse: gesture and speech. ACM Trans. Comput.-Hum. Interact., 9(3):171–193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Shriberg</author>
<author>A Stolcke</author>
</authors>
<title>Direct modeling of prosody: An overview of applications in automatic speech processing.</title>
<date>2004</date>
<booktitle>In International Conference on Speech Prosody.</booktitle>
<contexts>
<context position="7979" citStr="Shriberg and Stolcke, 2004" startWordPosition="1251" endWordPosition="1254">xpression of a speaker’s thought or idea. It can be either a complete sentence or a semantically complete smaller unit. We have conducted an experiment that integrates lexical, prosodic and gestural cues in order to more effectively detect sentence unit boundaries in conversational dialog (Chen et al., 2004b). As can be seen in Figure 2, our multimodal model combines lexical, prosodic, and gestural knowledge sources, with each knowledge source implemented as a separate model. A hidden event language model (LM) was trained to serve as lexical model (P (W, E)). Using a direct modeling approach (Shriberg and Stolcke, 2004), prosodic features were extracted using the SRI prosodic feature extraction tool1 by collaborators at ICSI and then were used to train a CART decision tree as the prosodic model (P(EIF)). Similarly to the prosodic model, we computed gesture features directly from visual tracking measurements (Quek et al., 1999; Bryll et al., 2001): 3D hand position, Hold (a state when there is no hand motion beyond some adaptive 1A similar prosody feature extraction tool has been developed in our lab (Huang et al., 2006) using Praat. threshold results), and Effort (analogous to the kinetic energy of hand move</context>
</contexts>
<marker>Shriberg, Stolcke, 2004</marker>
<rawString>E. Shriberg and A. Stolcke. 2004. Direct modeling of prosody: An overview of applications in automatic speech processing. In International Conference on Speech Prosody.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>