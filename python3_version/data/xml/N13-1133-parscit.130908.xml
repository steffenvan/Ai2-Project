<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.980313">
Supervised All-Words Lexical Substitution using Delexicalized Features
</title>
<author confidence="0.971742">
Gy¨orgy Szarvas1 Chris Biemann2 Iryna Gurevych3,4
</author>
<affiliation confidence="0.5649625">
(1) Nuance Communications Deutschland GmbH
Kackertstrasse 10, D-52072 Aachen, Germany
(2) FG Language Technology
Department of Computer Science, Technische Universit¨at Darmstadt
(3) Ubiquitous Knowledge Processing Lab (UKP-TUDA)
Department of Computer Science, Technische Universit¨at Darmstadt
</affiliation>
<author confidence="0.309569">
(4) Ubiquitous Knowledge Processing Lab (UKP-DIPF)
</author>
<affiliation confidence="0.692268">
German Institute for Educational Research and Educational Information
</affiliation>
<email confidence="0.935982">
http://www.nuance.com, http://www.ukp.tu-darmstadt.de
</email>
<sectionHeader confidence="0.99468" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999965">
We propose a supervised lexical substitu-
tion system that does not use separate clas-
sifiers per word and is therefore applicable
to any word in the vocabulary. Instead of
learning word-specific substitution patterns, a
global model for lexical substitution is trained
on delexicalized (i.e., non lexical) features,
which allows to exploit the power of super-
vised methods while being able to general-
ize beyond target words in the training set.
This way, our approach remains technically
straightforward, provides better performance
and similar coverage in comparison to unsu-
pervised approaches. Using features from lex-
ical resources, as well as a variety of features
computed from large corpora (n-gram counts,
distributional similarity) and a ranking method
based on the posterior probabilities obtained
from a Maximum Entropy classifier, we im-
prove over the state of the art in the LexSub
Best-Precision metric and the Generalized Av-
erage Precision measure. Robustness of our
approach is demonstrated by evaluating it suc-
cessfully on two different datasets.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999982255813954">
In recent years, the task of automatically providing
lexical substitutions in context (McCarthy and Nav-
igli, 2007) received much attention. The premise
to be able to replace words in a sentence with-
out changing its meaning gave rise to applications
like linguistic steganography (Topkara et al., 2006;
Chang and Clark, 2010), semantic text similarity
(Agirre et al., 2012), and plagiarism detection (Gipp
et al., 2011).
Lexical substitution, a special form of contex-
tual paraphrasing where only a single word is re-
placed, is closely related to word sense disambigua-
tion (WSD): polysemous words have possible sub-
stitutions reflecting several senses, and the correct
sense has to be picked to avoid spurious system be-
havior. However, no explicit word sense inventory is
required for lexical substitution (Dagan et al., 2006).
The prominent tasks in a lexical substitution sys-
tem are generation and ranking, i.e. to generate a set
of possible substitutions for the target word and then
to rank this set of possible substitutions according to
their contextual fitness. The task to generate a high
quality set of possible substitutions is challenging in
itself, for two reasons. First, the available lexical
resources are seldom complete in listing synonyms.
Second, manually annotated substitutions show that
not all synonyms of a word are appropriate in a given
context, and many good substitutions have other lex-
ical relation than synonymy to the original word.
In this work, we present a supervised lexical sub-
stitution system that, unlike the usual lexical sam-
ple supervised approaches, can produce substitu-
tions for targets that are not contained in the train-
ing material. We reach this by using non-lexical
features from heterogeneous evidence, including
lexical-semantic resources and distributional simi-
larity, n-gram and shallow syntactic features based
on large, unannotated background corpora. In light
of the existence of lexical resources such as Word-
Net (Fellbaum, 1998) or machine readable dictio-
naries that can serve as the source for lexical infor-
mation, and with the ever-increasing availability of
large unannotated corpora for many languages and
</bodyText>
<page confidence="0.944889">
1131
</page>
<note confidence="0.470116">
Proceedings of NAACL-HLT 2013, pages 1131–1141,
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.9997443">
domains, our proposal enables us to leverage the
quality gain of supervised machine learning while
generalizing over a large vocabulary through the
avoidance of lexicalized features. Using a single
classifier for all substitution targets in this way re-
sults in an all-words substitution system. As our re-
sults demonstrate, our model improves over the state
of the art in lexical substitution with practically no
open parameters that have to be optimized and se-
lected carefully according to the dataset at hand.
</bodyText>
<sectionHeader confidence="0.99977" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999801843137255">
Previous works in lexical substitution either ad-
dress both the generation and the ranking tasks, and
are therefore applicable to any word without pre-
labeled data (c.f. the Semeval 2007 task (McCarthy
and Navigli, 2007) and related work) or focus on
the more challenging ranking step only (c.f. Erk
and Pad´o (2008) and related work). The latter ap-
proaches take the list of possible substitutions di-
rectly from the testing data as a workaround to gen-
erating the possible substitutions, and merely evalu-
ate the ranking capabilities of these methods.
The most accurate lexical substitution systems
use supervised machine learning to train (and test)
a separate classifier per target word, using lexical
and shallow syntactic features. These systems rely
on the existence of a large number of annotated
examples (i.e. sentences together with the con-
textually valid substitutions) for each word. Bie-
mann (2012) describes a supervised lexical sub-
stitution system for frequent nouns. Exploiting a
large amount of sense tagged examples and (sense-
specific) data annotated with substitutions, an ac-
curate coarse-grained WSD model is trained and
then the most frequent substitutions of the predicted
sense are assigned to the new occurrences of the tar-
get words. The results demonstrate that lexical sub-
stitution of noun targets can be attained with very
high precision (over 90%) if sufficient training ma-
terial is available. However, due to high annotation
costs, methods that do not require labeled training
data per target scale better to a large vocabulary.
Knowledge-based systems like e.g. by Hassan et
al. (2007), who use a number of knowledge-based
and unsupervised methods and combine these clues
using a voting scheme, do not need training data per
target. The combination of different signals, how-
ever, has to be done manually. Unsupervised sys-
tems that rely on distributional similarity (Thater et
al., 2011) or topic models (Li et al., 2010) are single
signals in this sense, and their development is guided
by the performance and observations on standard
datasets. Such signals, however, can also be kept
simple avoiding any task-specific optimization and
can be integrated in a single model for all words us-
ing a limited amount of training data and delexical-
ized features, as in Senselearner (Mihalcea and Cso-
mai, 2005) for weakly supervised all-words disam-
biguation. This way, task specific development can
be replaced by a machine learning component and
the resulting model applies also to unseen words,
similar to the knowledge-based approaches.
</bodyText>
<subsectionHeader confidence="0.990744">
2.1 Full Lexical Substitution Systems
</subsectionHeader>
<bodyText confidence="0.999949733333333">
Related works that address the lexical substitution
problem according to the settings established by the
English Lexical Substitution Task (McCarthy and
Navigli, 2007) at Semeval 2007 (LexSub) typically
employ a simple ranking strategy based on local
n-gram frequencies and focus on finding an opti-
mal source of possible substitutions, as the selec-
tion of lexical resources has largest impact on the
overall system performance: Sinha and Mihalcea
(2009) systematically explored the benefits of mul-
tiple lexical resources and found that a supervised
combination of several resources lead to statisti-
cally significant improvements in accuracy (about
3.5% points over the best single resource, WordNet).
They tested LSA (Deerwester et al., 1990), ESA
(Gabrilovich and Markovitch, 2007) and n-gram fre-
quencies for contextualization and found n-gram fre-
quencies to be more effective than dimensionality
reduction techniques by a large margin. Their im-
provements were obtained by supervised learning on
the combination of several lexical resources. Our
work, on the other hand, is concerned with using
more advanced features and we obtain significant
improvements based on a diverse set of features and
a different learning setup: we train a model for con-
textualization, rather than to combine substitutions
from several different resources.
A recent work by Sinha and Mihalcea (2011) used
an approach based on graph centrality to rank the
candidates and achieved comparable performance
</bodyText>
<page confidence="0.992077">
1132
</page>
<bodyText confidence="0.999746222222222">
to n-gram-frequency-based ranking. To summarize,
the use of n-gram frequencies for ranking and Word-
Net as the (most appropriate single) source of syn-
onyms is competitive to more complex solutions and
provides a simple and strong lexical substitution sys-
tem. This motivated the follow-up work by Chang
and Clark (2010) to use WordNet and n-grams in a
linguistic steganography application and this moti-
vates us to use this method as our baseline.
</bodyText>
<subsectionHeader confidence="0.999844">
2.2 Ranking Word Meaning in Context
</subsectionHeader>
<bodyText confidence="0.999986719298246">
Another prominent line of related work focused
solely on the accurate ranking of a pre-given set of
possible synonyms, according to their plausibility as
a substitution in a given context. Typically, lexi-
cal substitution data is used for evaluation purposes,
taking the candidate substitutions directly from the
test data. This choice is motivated by the assump-
tion that better semantic models should rank near-
synonyms more accurately according to how they fit
in the original word’s context.
Erk and Pad´o (2008) proposed the use of multiple
vector representations of words, where the basic rep-
resentation corresponds to a standard co-occurrence
vector, while further vectors are used to characterize
words according to their inverse selectional prefer-
ence statistics for typical dependency relations. The
representation of a word in its context is computed
via combining the basic representation of a word
with the inverse selectional preference vectors of its
related words from the context. Ranking is done by
comparing vectors of possible substitutions with the
substitution target. Thater et al. (2010) took a sim-
ilar approach but used second order co-occurrence
vectors and report improved performance.
An exemplar-based approach is presented by Erk
and Pad´o (2010) and Reisinger and Mooney (2010b)
to model word meaning with respect to its context:
instead of representing the word and the context as
separate vectors and combining them, a set of word
occurrences in similar contexts is picked first, and
then only these exemplars are used to represent the
word in context. While this approach provides good
results with relatively simple and transparent mod-
els, each occurrence of a word has a unique repre-
sentation (that can only be computed at testing time),
and it is computationally expensive to scale these
models to a large number of examples.
Dinu and Lapata (2010) used a bag of words la-
tent variable model to characterize the meaning of a
word as a distribution over a set of latent variables
(that is, probabilistic senses). Contextualized repre-
sentation of word meaning is then attained by con-
ditioning the model on the context words in which
the target word occurs. A similar approach has
been evaluated for word similarity (Reisinger and
Mooney, 2010a) and word sense disambiguation (Li
et al., 2010).
Although our main goal here is to develop a full-
fledged lexical substitution system, we mainly fo-
cus on the construction of better ranking models
based on supervised machine learning and delexi-
calized features that scale well for unseen words.
This approach has similar properties (applicability
to all words without word-specific training data) to
the knowledge-based and unsupervised models de-
scribed above, so we will also refer to these systems
for comparison.
</bodyText>
<sectionHeader confidence="0.998623" genericHeader="method">
3 Datasets
</sectionHeader>
<bodyText confidence="0.999954666666667">
In our work, we use two major freely available
datasets that contain human-annotated substitutions
for single words in their full-sentence context.
</bodyText>
<subsectionHeader confidence="0.999254">
3.1 LexSub dataset
</subsectionHeader>
<bodyText confidence="0.9999858">
This dataset was introduced in the Lexical Substi-
tution task at Semeval 20071. It consists of 2002
sentences for a total of 201 words (10 sentences
per word, but 8 sentences does not have gold stan-
dard labels). Each sentence was assigned to 5 na-
tive speaker annotators, who entered as many para-
phrases or substitutions as they found appropriate
for the word in context. Paraphrases are assigned a
weight (or frequency) that denotes how many anno-
tators suggested that particular word as a substitute.
</bodyText>
<subsectionHeader confidence="0.990341">
3.2 TWSI
</subsectionHeader>
<bodyText confidence="0.99997575">
A similar, but larger dataset is the Turk Bootstrap
Word Sense Inventory (TWSI2, (Biemann, 2012)).
The data was collected through a three-step crowd-
sourcing process and comprises 24,647 sentences
</bodyText>
<footnote confidence="0.9956745">
1download at http://nlp.cs.swarthmore.edu/
semeval/tasks/task10/data.shtml
2http://www.ukp.tu-darmstadt.de/data/
lexical-resources/twsi-lexical-substitutions/
</footnote>
<page confidence="0.991588">
1133
</page>
<tableCaption confidence="0.993302">
Table 1: Details of the datasets: WN=WordNet
</tableCaption>
<figure confidence="0.986590625">
LexSub
source WN Gold St.
# words 201 201
#inst 2002 2002
avg. set 21 17
# empty 508 17
#0 39465 27300
#1 1302 4698
#2 582 1251
#3 308 571
#4 212 319
#5+ 129 179
TWSI
WN Gold St.
908 1007
22543 24643
7.5 22
11165 620
151538 443993
10678 77417
4171 17585
2069 5629
74 325
121 411
</figure>
<bodyText confidence="0.999862571428571">
for a total of 1,012 target nouns, where crowdwork-
ers have provided substitutions for a target word in
context. We did not use the roughly 150,000 sense-
labeled contexts and the sense inventory of this re-
source, i.e. this dataset – as used in this study – is
transparent to the LexSub data. For the majority of
the data, responses from 3 annotators were collected
per context, and there are on average 24 sentences
per target word in the dataset. Due to this, the aver-
age weight of good substitutions is somewhat lower
than in the LexSub dataset (1.27 vs. 1.58 in Lex-
Sub), but the average number of unique substitutions
per target word is slightly higher in TWSI (average
of 22 words / target vs. 17 in LexSub).
</bodyText>
<subsectionHeader confidence="0.999946">
3.3 Source of Possible Substitutions
</subsectionHeader>
<bodyText confidence="0.990729323529412">
In our lexical substitution system, we used WordNet
as the source for candidate synonyms. For each sub-
stitution target, we took all synonyms from all of the
word’s WordNet synsets as candidates, together with
the words from synsets in similar to, entailment and
also see relation to these synsets3. In order to evalu-
ate and compare our ranking methodology in a trans-
parent way with those studies that focused just on
the candidate ranking task, we also performed exper-
iments where we pooled the set of candidates from
the gold standard dataset. This setting ensures that
each set contains a positive candidate, and that all
human-suggested paraphrases are available as posi-
tive examples for a given sentence.
The main characteristics of the datasets (with both
WordNet or the gold standard as the source of candi-
date substitutions) are summarized in Table 1. The
rows in the table indicate the source of possible sub-
stitutions, number of target words, instances with at
least one non-multiword possible substitution, aver-
age size of candidate sets, and number of instances
with no good candidate and frequency of different
labels. The labels denote how many annotators pro-
posed a particular word as substitution in the given
context and can be interpreted as a measure of good-
ness: the higher the value, the better the candidate
fits in the context. Similarly, the label 0 denotes the
total number of negative examples in our datasets,
i.e. bad substitutions – words that belong to the can-
3This candidate set was found best for WordNet by Martinez
et al. (2007).
didate set for a particular target word, but are not
listed as good substitutions in the given context in
the dataset.
</bodyText>
<sectionHeader confidence="0.999639" genericHeader="method">
4 Methodology
</sectionHeader>
<subsectionHeader confidence="0.999891">
4.1 Experimental Setup and Evaluation
</subsectionHeader>
<bodyText confidence="0.999963285714286">
We follow previous works in lexical substitution and
evaluate our models using the Generalized Average
Precision (GAP) (Kishida, 2005) measure which as-
sesses the quality of the entire ranked list. In addi-
tion, we also provide the precision of our system at
the first rank (P@1), i.e. the percentage of correct
paraphrases at rank 1. This is a realistic evaluation
criterion for many applications, such as paraphras-
ing for linguistic steganography: it is the highest-
ranked candidate that can be used to replace the orig-
inal word (the manipulated text should preserve the
original meaning) and there is no straightforward
way to exploit multiple correct answers. In addition,
we also provide the Semeval 2007 best precision4
metric (McCarthy and Navigli, 2007) for the Lex-
Sub dataset for comparison to Semeval 2007 partic-
ipants. This metric also evaluates the first guess of
a system (per context), but gives less credit to easier
contexts, where several good options exist. This fact
motivates us to use P@1 rather than the best preci-
sion metric in all other experiments.
</bodyText>
<footnote confidence="0.951679">
4Since our system always provides an answer, the Semeval
2007 best recall equals best precision.
</footnote>
<page confidence="0.989557">
1134
</page>
<subsectionHeader confidence="0.958236">
4.2 Machine Learning on Delexicalized
Features
</subsectionHeader>
<bodyText confidence="0.999995925925926">
After the list of potential substitutions is obtained,
lexical substitution is cast as a ranking task where
the goal is to prefer contextually plausible substitu-
tions over implausible ones. The goal of this study
is to learn a ranking model that is applicable to any
word, for which a list of synonyms is available. A
supervised model can generalize over the example
target words in the datasets, if aggregate features
can be defined that have the same semantics regard-
less of the actual context, target word or candidate
substitution they are computed from. Having such a
representation, one can expect to learn patterns that
generalize over the words/contexts seen in the train-
ing dataset, and thus the setup constitutes a super-
vised all-word system.
To simulate an all-word scenario, we perform a
10-fold cross validation in our experiments, splitting
the dataset into equal-sized folds randomly on the
target word level. That is, all sentences for a particu-
lar target word fall into the same fold and thus either
the training or the test set (but never both). This way
we always train and test the model on disjoint sets of
words and as such, the learnt models cannot exploit
word-specific properties. This makes our results re-
alistic estimates of an open vocabulary paraphrasing
system, where we would apply the models (mostly)
to words that were not in the training material.
</bodyText>
<sectionHeader confidence="0.525919" genericHeader="method">
4.2.1 Machine Learning Model
</sectionHeader>
<bodyText confidence="0.999991027777777">
In our experiments, we used a Maximum Entropy
(MaxEnt) classifier model implemented in the Mal-
let (McCallum, 2002) package and trained a binary
classifier to predict if a given substitution is valid in
a particular context or not.
We chose to use Maximum Entropy models for
two main reasons: MaxEnt is not sensitive to param-
eter settings and handles correlated features well,
which is crucial in our situation where many features
are highly correlated.
Due to the low number of positive examples in the
datasets (see Table 1, labels 1-5+) and to emphasize
better paraphrases suggested by several annotators,
we assigned a weight to positive instances during the
training process equal to their score (the number of
annotators suggesting that paraphrase; the weight of
negative instances was set to 1).
The output of the MaxEnt classifier is a posterior
probability distribution for each target/substitution
pair, denoting the probabilities of the instance to
be a good or a bad substitution, given the feature
values that describe both the words and their con-
text. The ranking over a set of candidates can be
naturally induced based on their posterior scores for
the positive class, i.e. a number that denotes ’how
good the candidate is, given the context’. That is,
the best substitution candidate s (characterized by a
set of features F) from a set of candidates S is ob-
tained as argmax3ES[P(goodIF)], the next best as
the argmax of the remaining elements, and so on.
This pointwise approach to subset ranking (Cos-
sock and Zhang, 2008) is arguably simplistic, but
several studies (c.f. Li et al. (2007; Busa-Fekete
et al. (2011)) found this approach to perform rea-
sonably well given that the model provides accurate
probability estimates, which is the case for MaxEnt.
</bodyText>
<subsectionHeader confidence="0.994473">
4.3 Delexicalized Features
</subsectionHeader>
<bodyText confidence="0.999988481481482">
We use heterogeneous sources of information to de-
scribe each target word/candidate substitution pair
in its context. The most important features describe
the syntagmatic coherence of the substitute in con-
text, measured as local n-gram frequencies obtained
from web data, in a sliding window around the tar-
get word. In addition we use features to describe the
(non-positional, i.e. non-local) distributional simi-
larity of the target and its candidate substitution in
terms of sentence level co-occurrence statistics col-
lected from newspaper texts. A further set of fea-
tures captures the properties of the target and can-
didate word in a lexical resource (WordNet), such
as their number of senses, how frequent senses are
synonymous, etc. Lastly, we use part of speech pat-
terns to describe the target word in context. This
way, unlike many other methods suggested in previ-
ous works (Thater et al., 2011; Erk and Pad´o, 2008),
our model does not require deep syntactic analysis
of the test sentences in order to rank the candidates.
Even though we make intensive use of WordNet to
compute some of our feature functions, this is not
a severe restriction for a practical paraphrasing sys-
tem: one has to have a decent lexical resource in or-
der to mine a reasonable set of candidate synonyms
and such a resource can also serve as a source for
features in the classifier. The rest of the feature func-
</bodyText>
<page confidence="0.96511">
1135
</page>
<bodyText confidence="0.9997763">
tions exploit only large unannotated corpora and a
POS tagger at application time.
For a target word t, and candidate substitution si
from a set of candidates S, we used the features be-
low. Each numeric feature is used both in the form
given below, and set-wise scaled to [0, 1] (we leave
it to the classifier to pick the more useful form of
information). For the LexSub dataset, each feature
is defined once for all instances, and once specific
to the four POS categories in the dataset. That is
each instance would have the described features de-
fined twice, once the general form (defined for every
instance) and once the form according to the pre-
dicted POS category of the target word. This allows
the model to learn general and also POS-specific
patterns based on the information described below
(i.e. frequency thresholds, distributional properties
etc. for nouns or verbs etc. in particular). We denote
the left and right contexts around t and all words in
the sentence except t with cl, cr and c, respectively)
</bodyText>
<subsectionHeader confidence="0.93293">
4.3.1 Lexical Resource Features
</subsectionHeader>
<bodyText confidence="0.999663923076923">
We used Wordnet 3.0 as the source for substi-
tution candidates and as a source for delexicalized
features. We found the measure of ambiguity and
the sense number to provide useful information in
a more general context: it is informative how many
senses a word has, and it is informative from which
sense number of the substitution target the substitu-
tion candidate came from, since they are ordered by
corpus frequency. In addition, we used the synsets
IDs of the words’ hypernyms as features, which can
capture more general semantics (the word to replace
is ’animate’, ’abstract’, etc.). The following features
were extracted from WordNet:
</bodyText>
<listItem confidence="0.995811">
• number of senses of t and si in WordNet
• the sense numbers of t and si which are syn-
onymous (in case they are direct synonyms, c.f.
WN sense numbers encode sense frequencies)
• binary features for synset IDs of the hypernyms
of the synset containing t and si (this feature
type did not significantly improve results)
</listItem>
<subsubsectionHeader confidence="0.456413">
4.3.2 Corpus-based Features
</subsubsectionHeader>
<bodyText confidence="0.999875260869565">
In order to create a Distributional Thesaurus (DT)
similar to Lin (1998), we parsed a source corpus
of 120M sentence English newspaper texts from
the LCC5 (Richter et al., 2006) with the Stanford
parser (de Marneffe et al., 2006) and used depen-
dencies to extract features for words: each depen-
dency triple (w1, r, w2) denoting a dependency of
type r between words w1 and w2 results in a fea-
ture (r, w2) characterizing w1, and a feature (w1, r)
characterizing w26. After counting the frequency
of each feature for each word, we apply a signifi-
cance measure (log-likelihood test (LL), (Dunning,
1993)), rank features per word according to their
significance, and prune the data, keeping only the
1000 most salient features (Fw) per word7. The sim-
ilarity of two words is then given by the number
of their common features. Our distributional the-
saurus provides a list of the 1000 most salient fea-
tures and a ranked list of up to 200 similar words
(simw, based on the number of shared features) for
all words above a certain frequency in the source
corpus. We compute the following features to char-
acterize a target word / substitution pair:
</bodyText>
<listItem confidence="0.9958968">
• To what extent the context c characterizes si:
L-�cEFsi LL(Fsi (c))
Esj ES EcEFsj LL(Fsj (c))
• percentage of shared words among
the top k similar words to t and
</listItem>
<equation confidence="0.392882666666667">
|simt|k∩|simsi|k
to si: max(|simt|k,|simsi|k), for k =
1, 5, 10, 20, 50,100, 2008
</equation>
<listItem confidence="0.981385333333333">
• percentage of shared salient features among the
top k features of t and si, globally and re-
stricted to the words from the target sentence:
</listItem>
<equation confidence="0.934622333333333">
|Ft|k∩|Fsi|kand |Ft|k∩|Fsi|k∩|c |for k =
max(|Ft|k,|Fsi|k) |c|
1, 5,10, 20, 50,100,1000
</equation>
<listItem confidence="0.9983315">
• boolean feature indicating whether si E simt
or not (in top 100 similar words)
</listItem>
<footnote confidence="0.997936714285714">
5http://corpora.informatik.uni-leipzig.de/
6open source implementation and data available at
http://sourceforge.net/p/jobimtext
7The pruning operation greatly reduces runtime at the-
saurus collection, rendering memory reduction techniques like
(Charikar et al., 2004) as unnecessary.
8The various values for k trade off the salience of this fea-
</footnote>
<bodyText confidence="0.529468">
ture for coverage: only very few substitutions have overlap in
the top 1-5 similar words set, but if this happens, it is a very
strong indicator of contextual fitness, whereas overlap within
the top 100-200 similar words is present for much more tar-
get/substitution pairs, but it is a weaker indicator of fitness.
</bodyText>
<page confidence="0.987348">
1136
</page>
<subsectionHeader confidence="0.847669">
4.3.3 Local n-gram Features (from Web 1T)
</subsectionHeader>
<bodyText confidence="0.999747833333333">
Syntagmatic coherence, measured as the n-gram
frequency of the context with the candidate substi-
tution serves as the basis of ranking in the best Se-
meval 2007 system (Giuliano et al., 2007), which is
also our baseline method here. We use the same n-
grams as features in our supervised model:
</bodyText>
<listItem confidence="0.997322545454545">
• 1-5-gram frequencies in a sliding window
around t: freq(cisicr)/freq(citcr), normal-
ized w.r.t t
• 1-5-gram frequencies in a sliding window
around t: freq(cisicr)/E freq(ci5cr), nor-
malized w.r.t. 5
• for each of x in {’and’, ’or’, ’,’�, 3-5-
gram frequencies in a sliding window around
t: freq(citxsicr)/freq(citcr) (how frequently
the target and candidate are part of a list or con-
junctive phrase)
</listItem>
<subsectionHeader confidence="0.915138">
4.3.4 Shallow Syntactic Features
</subsectionHeader>
<bodyText confidence="0.998410285714286">
We also use part of speech information (from
TreeTagger (Schmid, 1994)) as features, in order
to enable the model to learn POS-specific patterns.
This is especially important for the LexSub dataset,
which contains examples from all major parts of
speech (the TWSI dataset contains only noun tar-
gets). Specifically, we use:
</bodyText>
<listItem confidence="0.99956375">
• 1-3-grams of main POS categories in a window
around t, e.g. NVV for a noun, verb, verb con-
text
• Penn Treebank POS code of t
</listItem>
<subsectionHeader confidence="0.389074">
4.3.5 Example
</subsectionHeader>
<bodyText confidence="0.9999274">
For clarity, we exemplify our delexicalized fea-
tures briefly. Using WordNet as a source for the
word bright, we considered the 11 words brilliant,
vivid, smart, burnished, lustrous, shining, shiny,
undimmed, brilliant, hopeful, promising from the
synsets of bright, and 64 further words from its re-
lated synsets (e.g. intelligent, glimmery, polished,
happy, ...) as potential paraphrases. That is, for
the sentence ”He was bright and independent and
proud.”, where the human annotators listed intelli-
gent, clever as suitable paraphrases, our system had
1 correct (intelligent) and 74 incorrect substituions
in the candidate set (that is, clever is not found in
WordNet in the above described way). The substitu-
tion intelligent in this context is characterized by a
total of 178 active features. Of those, 112 features
are based on local n-gram features (Sect. 4.3.3),
where the large number stems from different n in
n-gram, as well as the different variants of normal-
ization and copies for the particular POS (here: JJ)
and for all POS. For instance, ”bright” and ”intelli-
gent” are frequently occurring in comma-separated
enumerations, and ”intelligent” fits well in the target
context based on n-gram probabilities. The second
largest block of features is constituted by 48 active
distributional similarity features (Sect. 4.3.2), which
are also available per POS and for different normal-
izations. Here is e.g. captured that the candidate
has a high distributional similarity to the target with
respect to our background corpus. The 12 shallow
syntatic features (Sect 4.3.4) capture various present
POS patterns around the target, and the 6 resource-
based features (Sect. 4.3.1) e.g. inform about the
number of senses of the target (10) and the candi-
date (4).
</bodyText>
<subsectionHeader confidence="0.697029">
4.4 Results
</subsectionHeader>
<bodyText confidence="0.999997">
Now, we describe our results in detail. First we com-
pare our system on two datasets with a competitive
baseline, which uses the same candidate set as our
ML-based model, and the simple and effective rank-
ing function based on Google n-grams described by
Giuliano et al. (2007). Later on we analyze how the
four major feature groups contribute to the results in
a feature ablation experiment, and then we provide
a detailed and thorough comparison to earlier works
that are similar to the model presented here and used
the same dataset (LexSub) for evaluation.
</bodyText>
<subsectionHeader confidence="0.93722">
4.4.1 Semeval 2007 Lexical Substitution
</subsectionHeader>
<bodyText confidence="0.9999395">
In Table 2 we report results on the LexSub dataset.
As can be seen, our model outperforms the baseline
by a significant margin (p &lt; 0.01 for all measures,
using a paired t-test for significance). Both the over-
all rankings and the P@1 scores are of higher quality
than the rankings based only on n-grams.
</bodyText>
<subsectionHeader confidence="0.909987">
4.4.2 Turk Bootstrap Word Sense Inventory
</subsectionHeader>
<bodyText confidence="0.792166">
The results on the TWSI dataset are provided in
Table 3. Our model outperforms the baseline in all
</bodyText>
<page confidence="0.933325">
1137
</page>
<table confidence="0.9989325">
cand. from WN from Gold St.
GAP P@1 GAP P@1
Baseline 36.8 31.1 46.9 49.5
Our model 43.8 40.2 52.4 57.7
</table>
<tableCaption confidence="0.996132">
Table 2: Comparison to the baseline on LexSub 2007.
</tableCaption>
<table confidence="0.998703">
cand. from WN from Gold St.
GAP P@1 GAP P@1
Baseline 33.8 28.2 44.4 44.5
Our model 36.6 32.4 47.2 49.5
</table>
<tableCaption confidence="0.999985">
Table 3: Comparison to the baseline on the TWSI dataset.
</tableCaption>
<bodyText confidence="0.999953611111111">
the comparisons similar to the LexSub dataset. The
differences are not so pronounced but still highly
significant (p &lt; 0.01). This is consistent with the
observation by several Semeval 2007 participants
and with a per-POS analysis of our results on Lex-
Sub: the ranking task seems to be more challenging
for nouns than for other parts of speech. When us-
ing WordNet, for about half (11165/22543) of the
instances, individual scores are 0 (cf. Table 1). For
the other half, avg. P@1 score is around 0.7, which
results in 0.324 overall. Note that the task of ranking
in avg. 7.5 items is considerably easier than rank-
ing in avg. 22 items, which explains the high P@1
scores for cases where good candidates exist – also,
a random ranker would score higher in this case.
These results demonstrate that the proposed
delexicalized approach is superior to a competitive
baseline across two datasets.
</bodyText>
<subsectionHeader confidence="0.997105">
4.5 Feature Exploration
</subsectionHeader>
<bodyText confidence="0.99986475">
We explored the contribution of our various fea-
ture types on the LexSub dataset with candidate set
from the gold standard. Our MaxEnt model rely-
ing only on local n-gram frequency features, i.e. the
</bodyText>
<footnote confidence="0.686181833333333">
GAP P@1
w/o n-gram features
w/o distr. thesaurus
w/o POS features
w/o WN features
Our model (all)
</footnote>
<tableCaption confidence="0.9949175">
Table 4: Feature ablation experiment (on LexSub dataset,
with candidates from Gold Standard).
</tableCaption>
<bodyText confidence="0.999966352941177">
same information as the baseline model, achieved a
GAP score of 48.3 and P@1 of 52.1, respectively.
This result is significantly better than the baseline
(p &lt; 0.01), i.e. the machine learnt ranking model
is better than a state-of-the-art handcrafted ranker
based on the same data. All single feature groups,
when combined with n-grams, lead to significant im-
provements (p &lt; 0.01), which proves the usefulness
of each feature group. In order to assess the contri-
bution of each group to the overall system perfor-
mance, we performed a feature ablation experiment.
That is, we trained the MaxEnt model with using
all feature groups (this equals the model in Table 2)
and then with leaving each of the feature groups out
once. As can be seen, all feature groups improve the
overall results in a noticeable way, i.e. their contri-
bution is complementary.
</bodyText>
<subsectionHeader confidence="0.797051">
4.5.1 Comparison to Previous Works
</subsectionHeader>
<bodyText confidence="0.998577">
In Table 5 we compare our method with previous
works in the field, using the LexSub dataset.
</bodyText>
<table confidence="0.965436857142857">
candidates from WN
Best-P
Giuliano 12.93
Martinez 12.68
Sinha 13.60
Baseline 11.75
Our model 15.94
</table>
<tableCaption confidence="0.999923">
Table 5: Comparison to previous works (LexSub dataset).
</tableCaption>
<bodyText confidence="0.9999245">
In the left column of Table 5, we compare the per-
formance of our system to representative Semeval
2007 participants, namely Martinez et al. (2007) and
Giuliano et al. (2007). In order to make a fair com-
parison, we report scores for the official test data
of Semeval 2007, using a 10-fold cross-validation
scheme. Martinez et al. (2007) developed their sys-
tem based on WordNet and we use the same can-
didate set here that they proposed in their system
description. Our reimplementation of (Giuliano et
al., 2007) performs below the original scores, due
to the more restricted source of substitution can-
didates (they use more lexical resources), yet uses
the same ranking methodology based on Google n-
grams that we adopted here as our baseline. We also
report the best previous result for this task, which
</bodyText>
<figure confidence="0.823531384615385">
47.3 48.9
49.8 55.0
51.6 56.3
51.7 57.0
52.4 57.7
from Gold Standard
GAP
Pad´oErk10 38.6
DinuLapata 42.9
Thater10 46.0
Thater11 51.7
Baseline 46.9
Our model 52.4
</figure>
<page confidence="0.989417">
1138
</page>
<bodyText confidence="0.999995828571429">
was achieved via the (supervised) combination of
lexical resources to improve the performance (Sinha
and Mihalcea, 2009). Our model outperforms this
result by a large margin for the best-precision eval-
uation (mode-P, precision measured on those exam-
ples where there is a clear best substitution provided
by humans was 26.3%, compared to 21.3% reported
by Sinha and Mihalcea (2009). This is especially
promising in light of the fact that we use only a sin-
gle source (WordNet) for synonyms and achieve our
improvements through more advanced delexicalized
features in an improved ranking model. Sinha and
Mihalcea (2009), on the other hand, used compara-
bly simple features for contextualization, of which
n-gram features were deemed most successful. As
Sinha and Mihalcea (2009) showed improvements
through utilizing several synonym sources, a combi-
nation of their approach with ours should allow for
further improvements in the future.
In the right column of Table 5, we compare our
model to previous works that addressed only the
ranking task, and report performance on the whole
dataset (i.e. trial and test). As can be seen, the
methodology proposed here outperforms previous
ranking models, without the need to develop a high-
quality ranking model by hand, and without the need
to parse the test sentences. Our delexicalized super-
vised model only requires the development of fea-
tures, and achieves excellent results without major
task-specific tuning or customization: we omitted
the optimization of the feature set and the parame-
ters of the learning model. This fact makes us as-
sume that the proposed model can be applied more
quickly and easily than previous models that have
several important design aspects to choose from.
</bodyText>
<sectionHeader confidence="0.993532" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999989106382979">
In this study, we presented a supervised approach to
all-words lexical substitution based on delexicalized
features, which enables us to fully exploit the power
of supervised models while ensuring applicability to
a large, open vocabulary.
Results demonstrate the feasibility of this method:
our MaxEnt-based ranking approach improved over
the baseline in all settings and yielded – to our
knowledge – the best scores for lexical substitu-
tion with automatically gathered synonyms on the
Semeval 2007 LexSub dataset. Also, it performed
slightly better than the state of the art for candidates
pooled from the gold standard without any parame-
ter tuning or empirical design choices.
In this study, we established transparency be-
tween Semeval-style and ranking-only studies in
lexical substitution – two lines of work that were dif-
ficult to compare in the past. Further, we observe
similar improvements on two different datasets,
showing the robustness of the approach.
While previous works showed the potential of
more/improved lexical resources for lexical substi-
tution, we improved over the best Semeval-style per-
formance just by exploiting an improved ranking
model over a standard WordNet-based candidate set.
These results indicate that improvements from lexi-
cal resources and better ranking models are additive,
thus we want to add more lexical resources in our
system in the future.
Of course there are several other ways to improve
further the work described here. First of all, simi-
lar to the best ranking approaches (e.g. Thater et al.
(2011)), one could use contextualized feature func-
tions to make global information from the distri-
butional thesaurus more accurate. Instead of using
globally calculated similarities, information from
the distributional thesaurus could be contextualized
via constraining the statistics with words from the
context.
Other natural ways to improve the model de-
scribed here are to make heavier use of parser infor-
mation or to employ pair-wise or list-wise machine
learning models (Cao et al., 2007), which are specif-
ically designed for subset ranking. Lastly, while in-
trinsic evaluation of lexical substitution is important,
we would like to show its practicability in tasks such
as steganography or information retrieval.
</bodyText>
<sectionHeader confidence="0.99495" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999822">
This work has been supported by the Hes-
sian research excellence program Landes-Offensive
zur Entwicklung Wissenschaftlich-¨okonomischer
Exzellenz (LOEWE) as part of the research center
Digital Humanities, and by the German Ministry of
Education and Research under grant SiDiM (grant
no. 01IS10054G).
</bodyText>
<page confidence="0.99527">
1139
</page>
<sectionHeader confidence="0.988813" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999457377358491">
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A
pilot on semantic textual similarity. In *SEM 2012:
The First Joint Conference on Lexical and Computa-
tional Semantics – Volume 1: Proceedings of the main
conference and the shared task, and Volume 2: Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 385–393,
Montr´eal, Canada.
Chris Biemann. 2012. Creating a System for Lexi-
cal Substitutions from Scratch using Crowdsourcing.
Language Resources and Evaluation: Special Issue
on Collaboratively Constructed Language Resources,
46(2).
R´obert Busa-Fekete, Bal´azs K´egl, ´Eltet˝o Yam´as, and
Gy¨orgi Szarvas. 2011. A robust ranking method-
ology based on diverse calibration of adaboost. In
European Conference on Machine Learning, volume
LNCS, 6911, pages 263–279.
Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and
Hang Li. 2007. Learning to rank: from pairwise
approach to listwise approach. In Proceedings of the
24rd International Conference on Machine Learning,
pages 129–136.
Ching-Yun Chang and Stephen Clark. 2010. Practi-
cal linguistic steganography using contextual synonym
substitution and vertex colour coding. In Proceedings
of the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1194–1203, Cam-
bridge, MA.
Moses Charikar, Kevin Chen, and Martin Farach-Colton.
2004. Finding frequent items in data streams. Theor.
Comput. Sci., 312(1):3–15.
D. Cossock and T. Zhang. 2008. Statistical analysis of
Bayes optimal subset ranking. IEEE Transactions on
Information Theory, 54(11):5140–5154.
Ido Dagan, Oren Glickman, Alfio Gliozzo, Efrat Mar-
morshtein, and Carlo Strapparava. 2006. Direct word
sense matching for lexical substitution. In Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and the 44th annual meeting of the
Association for Computational Linguistics, ACL-44,
pages 449–456, Sydney, Australia.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
LREC 2006, Genova, Italy.
Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas K. Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal of the
American Society for Information Science, 41(6):391–
407.
Georgiana Dinu and Mirella Lapata. 2010. Measuring
distributional similarity in context. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 1162–1172, Cambridge,
MA.
Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Linguis-
tics, 19(1):61–74.
Katrin Erk and Sebastian Pad´o. 2008. A structured vec-
tor space model for word meaning in context. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 897–906,
Honolulu, Hawaii.
Katrin Erk and Sebastian Pad´o. 2010. Exemplar-based
models for word meaning in context. In Proceedings
of the ACL 2010 Conference Short Papers, pages 92–
97, Uppsala, Sweden.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting Semantic Relatedness using Wikipedia-based
Explicit Semantic Analysis. In Proceedings of the
20th International Joint Conference on Artificial In-
telligence, pages 1606–1611.
Bela Gipp, Norman Meuschke, and Joeran Beel. 2011.
Comparative Evaluation of Text- and Citation-based
Plagiarism Detection Approaches using GuttenPlag.
In Proceedings of 11th ACM/IEEE-CS Joint Confer-
ence on Digital Libraries (JCDL’11), pages 255–258,
Ottawa, Canada. ACM New York, NY, USA. Avail-
able at http://sciplore.org/pub/.
Claudio Giuliano, Alfio Gliozzo, and Carlo Strapparava.
2007. FBK-irst: Lexical substitution task exploit-
ing domain and syntagmatic coherence. In Proceed-
ings of the Fourth International Workshop on Semantic
Evaluations (SemEval-2007), pages 145–148, Prague,
Czech Republic.
Samer Hassan, Andras Csomai, Carmen Banea, Ravi
Sinha, and Rada Mihalcea. 2007. UNT: SubFinder:
Combining knowledge sources for automatic lexical
substitution. In Proceedings of the Fourth Interna-
tional Workshop on Semantic Evaluations (SemEval-
2007), pages 410–413, Prague, Czech Republic.
Kazuaki Kishida. 2005. Property of Average Precision
and Its Generalization: An Examination of Evaluation
Indicator for Information Retrieval Experiments. NII
technical report. National Institute of Informatics.
Ping Li, Christopher J.C. Burges, and Qiang Wu. 2007.
McRank: Learning to rank using multiple classifica-
tion and gradient boosting. In Advances in Neural In-
formation Processing Systems, volume 19, pages 897–
904. The MIT Press.
Linlin Li, Benjamin Roth, and Caroline Sporleder. 2010.
Topic models for word sense disambiguation and
</reference>
<page confidence="0.802777">
1140
</page>
<reference confidence="0.999822027397261">
token-based idiom detection. In Proceedings of the
48th Annual Meeting of the Association for Computa-
tional Linguistics, ACL ’10, pages 1138–1147.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th Annual
Meeting of the Association for Computational Linguis-
tics and the 17th International Conference on Compu-
tational Linguistics, volume 2 of ACL ’98, pages 768–
774, Montreal, Quebec, Canada.
David Martinez, Su Nam Kim, and Timothy Bald-
win. 2007. MELB-MKB: Lexical substitution system
based on relatives in context. In Proceedings of the
Fourth International Workshop on Semantic Evalua-
tions (SemEval-2007), pages 237–240, Prague, Czech
Republic.
Andrew Kachites McCallum. 2002. MALLET:
A Machine Learning for Language Toolkit.
http://mallet.cs.umass.edu.
Diana McCarthy and Roberto Navigli. 2007. Semeval-
2007 task 10: English lexical substitution task. In
Proceedings of the Fourth International Workshop on
Semantic Evaluations (SemEval-2007), pages 48–53,
Prague, Czech Republic.
Rada Mihalcea and Andras Csomai. 2005. Senselearner:
word sense disambiguation for all words in unre-
stricted text. In Proceedings of the ACL 2005 on Inter-
active poster and demonstration sessions, ACLdemo
’05, pages 53–56.
Joseph Reisinger and Raymond Mooney. 2010a. A mix-
ture model with sharing for lexical semantics. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1173–
1182, Cambridge, MA.
Joseph Reisinger and Raymond J. Mooney. 2010b.
Multi-prototype vector-space models of word mean-
ing. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
109–117, Los Angeles, California.
M. Richter, U. Quasthoff, E. Hallsteinsd´ottir, and C. Bie-
mann. 2006. Exploiting the leipzig corpora collection.
In Proceesings of the IS-LTC 2006. Ljubljana, Slove-
nia.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of the In-
ternational Conference on New Methods in Language
Processing, Manchester, UK.
Ravi Sinha and Rada Mihalcea. 2009. Combining lex-
ical resources for contextual synonym expansion. In
Proceedings of the International Conference RANLP-
2009, pages 404–410, Borovets, Bulgaria.
Ravi Som Sinha and Rada Flavia Mihalcea. 2011. Using
centrality algorithms on directed graphs for synonym
expansion. In R. Charles Murray and Philip M. Mc-
Carthy, editors, FLAIRS Conference. AAAI Press.
Stefan Thater, Hagen F¨urstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations us-
ing syntactically enriched vector models. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 948–957, Uppsala,
Sweden.
Stefan Thater, Hagen F¨urstenau, and Manfred Pinkal.
2011. Word meaning in context: A simple and effec-
tive vector model. In Proceedings of the Fifth Interna-
tional Joint Conference on Natural Language Process-
ing : IJCNLP 2011, pages 1134–1143, Chiang Mai,
Thailand. MP, ISSN 978-974-466-564-5.
Umut Topkara, Mercan Topkara, and Mikhail J. Atal-
lah. 2006. The hiding virtues of ambiguity: quan-
tifiably resilient watermarking of natural language text
through synonym substitutions. In Proceedings of the
8th workshop on Multimedia and security, pages 164–
174, New York, NY, USA. ACM.
</reference>
<page confidence="0.994149">
1141
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.618157">
<title confidence="0.996532">Supervised All-Words Lexical Substitution using Delexicalized Features</title>
<author confidence="0.996432">Chris Iryna</author>
<affiliation confidence="0.834134">(1) Nuance Communications Deutschland</affiliation>
<address confidence="0.887333">Kackertstrasse 10, D-52072 Aachen,</address>
<email confidence="0.83811">(2)FGLanguage</email>
<affiliation confidence="0.9862938">Department of Computer Science, Technische Universit¨at Darmstadt (3) Ubiquitous Knowledge Processing Lab Department of Computer Science, Technische Universit¨at Darmstadt (4) Ubiquitous Knowledge Processing Lab (UKP-DIPF) German Institute for Educational Research and Educational Information</affiliation>
<web confidence="0.997838">http://www.nuance.com, http://www.ukp.tu-darmstadt.de</web>
<abstract confidence="0.9986154">We propose a supervised lexical substitution system that does not use separate classifiers per word and is therefore applicable to any word in the vocabulary. Instead of learning word-specific substitution patterns, a global model for lexical substitution is trained on delexicalized (i.e., non lexical) features, which allows to exploit the power of supervised methods while being able to generalize beyond target words in the training set. This way, our approach remains technically straightforward, provides better performance and similar coverage in comparison to unsupervised approaches. Using features from lexical resources, as well as a variety of features computed from large corpora (n-gram counts, distributional similarity) and a ranking method based on the posterior probabilities obtained from a Maximum Entropy classifier, we improve over the state of the art in the LexSub Best-Precision metric and the Generalized Average Precision measure. Robustness of our approach is demonstrated by evaluating it successfully on two different datasets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor Gonzalez-Agirre</author>
</authors>
<title>Semeval-2012 task 6: A pilot on semantic textual similarity.</title>
<date>2012</date>
<booktitle>In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics – Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>385--393</pages>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="2046" citStr="Agirre et al., 2012" startWordPosition="281" endWordPosition="284"> classifier, we improve over the state of the art in the LexSub Best-Precision metric and the Generalized Average Precision measure. Robustness of our approach is demonstrated by evaluating it successfully on two different datasets. 1 Introduction In recent years, the task of automatically providing lexical substitutions in context (McCarthy and Navigli, 2007) received much attention. The premise to be able to replace words in a sentence without changing its meaning gave rise to applications like linguistic steganography (Topkara et al., 2006; Chang and Clark, 2010), semantic text similarity (Agirre et al., 2012), and plagiarism detection (Gipp et al., 2011). Lexical substitution, a special form of contextual paraphrasing where only a single word is replaced, is closely related to word sense disambiguation (WSD): polysemous words have possible substitutions reflecting several senses, and the correct sense has to be picked to avoid spurious system behavior. However, no explicit word sense inventory is required for lexical substitution (Dagan et al., 2006). The prominent tasks in a lexical substitution system are generation and ranking, i.e. to generate a set of possible substitutions for the target wor</context>
</contexts>
<marker>Agirre, Cer, Diab, Gonzalez-Agirre, 2012</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pilot on semantic textual similarity. In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics – Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 385–393, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Biemann</author>
</authors>
<title>Creating a System for Lexical Substitutions from Scratch using Crowdsourcing. Language Resources and Evaluation: Special Issue on Collaboratively Constructed Language Resources,</title>
<date>2012</date>
<volume>46</volume>
<issue>2</issue>
<contexts>
<context position="5403" citStr="Biemann (2012)" startWordPosition="804" endWordPosition="806">f. Erk and Pad´o (2008) and related work). The latter approaches take the list of possible substitutions directly from the testing data as a workaround to generating the possible substitutions, and merely evaluate the ranking capabilities of these methods. The most accurate lexical substitution systems use supervised machine learning to train (and test) a separate classifier per target word, using lexical and shallow syntactic features. These systems rely on the existence of a large number of annotated examples (i.e. sentences together with the contextually valid substitutions) for each word. Biemann (2012) describes a supervised lexical substitution system for frequent nouns. Exploiting a large amount of sense tagged examples and (sensespecific) data annotated with substitutions, an accurate coarse-grained WSD model is trained and then the most frequent substitutions of the predicted sense are assigned to the new occurrences of the target words. The results demonstrate that lexical substitution of noun targets can be attained with very high precision (over 90%) if sufficient training material is available. However, due to high annotation costs, methods that do not require labeled training data </context>
<context position="12630" citStr="Biemann, 2012" startWordPosition="1942" endWordPosition="1943"> dataset This dataset was introduced in the Lexical Substitution task at Semeval 20071. It consists of 2002 sentences for a total of 201 words (10 sentences per word, but 8 sentences does not have gold standard labels). Each sentence was assigned to 5 native speaker annotators, who entered as many paraphrases or substitutions as they found appropriate for the word in context. Paraphrases are assigned a weight (or frequency) that denotes how many annotators suggested that particular word as a substitute. 3.2 TWSI A similar, but larger dataset is the Turk Bootstrap Word Sense Inventory (TWSI2, (Biemann, 2012)). The data was collected through a three-step crowdsourcing process and comprises 24,647 sentences 1download at http://nlp.cs.swarthmore.edu/ semeval/tasks/task10/data.shtml 2http://www.ukp.tu-darmstadt.de/data/ lexical-resources/twsi-lexical-substitutions/ 1133 Table 1: Details of the datasets: WN=WordNet LexSub source WN Gold St. # words 201 201 #inst 2002 2002 avg. set 21 17 # empty 508 17 #0 39465 27300 #1 1302 4698 #2 582 1251 #3 308 571 #4 212 319 #5+ 129 179 TWSI WN Gold St. 908 1007 22543 24643 7.5 22 11165 620 151538 443993 10678 77417 4171 17585 2069 5629 74 325 121 411 for a total </context>
</contexts>
<marker>Biemann, 2012</marker>
<rawString>Chris Biemann. 2012. Creating a System for Lexical Substitutions from Scratch using Crowdsourcing. Language Resources and Evaluation: Special Issue on Collaboratively Constructed Language Resources, 46(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R´obert Busa-Fekete</author>
<author>Bal´azs K´egl</author>
<author>´Eltet˝o Yam´as</author>
<author>Gy¨orgi Szarvas</author>
</authors>
<title>A robust ranking methodology based on diverse calibration of adaboost.</title>
<date>2011</date>
<booktitle>In European Conference on Machine Learning,</booktitle>
<volume>6911</volume>
<pages>263--279</pages>
<marker>Busa-Fekete, K´egl, Yam´as, Szarvas, 2011</marker>
<rawString>R´obert Busa-Fekete, Bal´azs K´egl, ´Eltet˝o Yam´as, and Gy¨orgi Szarvas. 2011. A robust ranking methodology based on diverse calibration of adaboost. In European Conference on Machine Learning, volume LNCS, 6911, pages 263–279.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhe Cao</author>
<author>Tao Qin</author>
<author>Tie-Yan Liu</author>
<author>Ming-Feng Tsai</author>
<author>Hang Li</author>
</authors>
<title>Learning to rank: from pairwise approach to listwise approach.</title>
<date>2007</date>
<booktitle>In Proceedings of the 24rd International Conference on Machine Learning,</booktitle>
<pages>129--136</pages>
<marker>Cao, Qin, Liu, Tsai, Li, 2007</marker>
<rawString>Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li. 2007. Learning to rank: from pairwise approach to listwise approach. In Proceedings of the 24rd International Conference on Machine Learning, pages 129–136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ching-Yun Chang</author>
<author>Stephen Clark</author>
</authors>
<title>Practical linguistic steganography using contextual synonym substitution and vertex colour coding.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1194--1203</pages>
<location>Cambridge, MA.</location>
<contexts>
<context position="1998" citStr="Chang and Clark, 2010" startWordPosition="274" endWordPosition="277">rior probabilities obtained from a Maximum Entropy classifier, we improve over the state of the art in the LexSub Best-Precision metric and the Generalized Average Precision measure. Robustness of our approach is demonstrated by evaluating it successfully on two different datasets. 1 Introduction In recent years, the task of automatically providing lexical substitutions in context (McCarthy and Navigli, 2007) received much attention. The premise to be able to replace words in a sentence without changing its meaning gave rise to applications like linguistic steganography (Topkara et al., 2006; Chang and Clark, 2010), semantic text similarity (Agirre et al., 2012), and plagiarism detection (Gipp et al., 2011). Lexical substitution, a special form of contextual paraphrasing where only a single word is replaced, is closely related to word sense disambiguation (WSD): polysemous words have possible substitutions reflecting several senses, and the correct sense has to be picked to avoid spurious system behavior. However, no explicit word sense inventory is required for lexical substitution (Dagan et al., 2006). The prominent tasks in a lexical substitution system are generation and ranking, i.e. to generate a </context>
<context position="8893" citStr="Chang and Clark (2010)" startWordPosition="1342" endWordPosition="1345">of features and a different learning setup: we train a model for contextualization, rather than to combine substitutions from several different resources. A recent work by Sinha and Mihalcea (2011) used an approach based on graph centrality to rank the candidates and achieved comparable performance 1132 to n-gram-frequency-based ranking. To summarize, the use of n-gram frequencies for ranking and WordNet as the (most appropriate single) source of synonyms is competitive to more complex solutions and provides a simple and strong lexical substitution system. This motivated the follow-up work by Chang and Clark (2010) to use WordNet and n-grams in a linguistic steganography application and this motivates us to use this method as our baseline. 2.2 Ranking Word Meaning in Context Another prominent line of related work focused solely on the accurate ranking of a pre-given set of possible synonyms, according to their plausibility as a substitution in a given context. Typically, lexical substitution data is used for evaluation purposes, taking the candidate substitutions directly from the test data. This choice is motivated by the assumption that better semantic models should rank nearsynonyms more accurately a</context>
</contexts>
<marker>Chang, Clark, 2010</marker>
<rawString>Ching-Yun Chang and Stephen Clark. 2010. Practical linguistic steganography using contextual synonym substitution and vertex colour coding. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1194–1203, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Moses Charikar</author>
<author>Kevin Chen</author>
<author>Martin Farach-Colton</author>
</authors>
<title>Finding frequent items in data streams.</title>
<date>2004</date>
<journal>Theor. Comput. Sci.,</journal>
<volume>312</volume>
<issue>1</issue>
<contexts>
<context position="25522" citStr="Charikar et al., 2004" startWordPosition="4082" endWordPosition="4085">|simsi|k), for k = 1, 5, 10, 20, 50,100, 2008 • percentage of shared salient features among the top k features of t and si, globally and restricted to the words from the target sentence: |Ft|k∩|Fsi|kand |Ft|k∩|Fsi|k∩|c |for k = max(|Ft|k,|Fsi|k) |c| 1, 5,10, 20, 50,100,1000 • boolean feature indicating whether si E simt or not (in top 100 similar words) 5http://corpora.informatik.uni-leipzig.de/ 6open source implementation and data available at http://sourceforge.net/p/jobimtext 7The pruning operation greatly reduces runtime at thesaurus collection, rendering memory reduction techniques like (Charikar et al., 2004) as unnecessary. 8The various values for k trade off the salience of this feature for coverage: only very few substitutions have overlap in the top 1-5 similar words set, but if this happens, it is a very strong indicator of contextual fitness, whereas overlap within the top 100-200 similar words is present for much more target/substitution pairs, but it is a weaker indicator of fitness. 1136 4.3.3 Local n-gram Features (from Web 1T) Syntagmatic coherence, measured as the n-gram frequency of the context with the candidate substitution serves as the basis of ranking in the best Semeval 2007 sys</context>
</contexts>
<marker>Charikar, Chen, Farach-Colton, 2004</marker>
<rawString>Moses Charikar, Kevin Chen, and Martin Farach-Colton. 2004. Finding frequent items in data streams. Theor. Comput. Sci., 312(1):3–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Cossock</author>
<author>T Zhang</author>
</authors>
<title>Statistical analysis of Bayes optimal subset ranking.</title>
<date>2008</date>
<journal>IEEE Transactions on Information Theory,</journal>
<volume>54</volume>
<issue>11</issue>
<contexts>
<context position="19868" citStr="Cossock and Zhang, 2008" startWordPosition="3137" endWordPosition="3141">denoting the probabilities of the instance to be a good or a bad substitution, given the feature values that describe both the words and their context. The ranking over a set of candidates can be naturally induced based on their posterior scores for the positive class, i.e. a number that denotes ’how good the candidate is, given the context’. That is, the best substitution candidate s (characterized by a set of features F) from a set of candidates S is obtained as argmax3ES[P(goodIF)], the next best as the argmax of the remaining elements, and so on. This pointwise approach to subset ranking (Cossock and Zhang, 2008) is arguably simplistic, but several studies (c.f. Li et al. (2007; Busa-Fekete et al. (2011)) found this approach to perform reasonably well given that the model provides accurate probability estimates, which is the case for MaxEnt. 4.3 Delexicalized Features We use heterogeneous sources of information to describe each target word/candidate substitution pair in its context. The most important features describe the syntagmatic coherence of the substitute in context, measured as local n-gram frequencies obtained from web data, in a sliding window around the target word. In addition we use featu</context>
</contexts>
<marker>Cossock, Zhang, 2008</marker>
<rawString>D. Cossock and T. Zhang. 2008. Statistical analysis of Bayes optimal subset ranking. IEEE Transactions on Information Theory, 54(11):5140–5154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Alfio Gliozzo</author>
<author>Efrat Marmorshtein</author>
<author>Carlo Strapparava</author>
</authors>
<title>Direct word sense matching for lexical substitution.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL-44,</booktitle>
<pages>449--456</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="2496" citStr="Dagan et al., 2006" startWordPosition="352" endWordPosition="355">hanging its meaning gave rise to applications like linguistic steganography (Topkara et al., 2006; Chang and Clark, 2010), semantic text similarity (Agirre et al., 2012), and plagiarism detection (Gipp et al., 2011). Lexical substitution, a special form of contextual paraphrasing where only a single word is replaced, is closely related to word sense disambiguation (WSD): polysemous words have possible substitutions reflecting several senses, and the correct sense has to be picked to avoid spurious system behavior. However, no explicit word sense inventory is required for lexical substitution (Dagan et al., 2006). The prominent tasks in a lexical substitution system are generation and ranking, i.e. to generate a set of possible substitutions for the target word and then to rank this set of possible substitutions according to their contextual fitness. The task to generate a high quality set of possible substitutions is challenging in itself, for two reasons. First, the available lexical resources are seldom complete in listing synonyms. Second, manually annotated substitutions show that not all synonyms of a word are appropriate in a given context, and many good substitutions have other lexical relatio</context>
</contexts>
<marker>Dagan, Glickman, Gliozzo, Marmorshtein, Strapparava, 2006</marker>
<rawString>Ido Dagan, Oren Glickman, Alfio Gliozzo, Efrat Marmorshtein, and Carlo Strapparava. 2006. Direct word sense matching for lexical substitution. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL-44, pages 449–456, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In LREC 2006,</booktitle>
<location>Genova, Italy.</location>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In LREC 2006, Genova, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Deerwester</author>
<author>Susan T Dumais</author>
<author>George W Furnas</author>
<author>Thomas K Landauer</author>
<author>Richard Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>41</volume>
<issue>6</issue>
<pages>407</pages>
<contexts>
<context position="7832" citStr="Deerwester et al., 1990" startWordPosition="1179" endWordPosition="1182">al Substitution Task (McCarthy and Navigli, 2007) at Semeval 2007 (LexSub) typically employ a simple ranking strategy based on local n-gram frequencies and focus on finding an optimal source of possible substitutions, as the selection of lexical resources has largest impact on the overall system performance: Sinha and Mihalcea (2009) systematically explored the benefits of multiple lexical resources and found that a supervised combination of several resources lead to statistically significant improvements in accuracy (about 3.5% points over the best single resource, WordNet). They tested LSA (Deerwester et al., 1990), ESA (Gabrilovich and Markovitch, 2007) and n-gram frequencies for contextualization and found n-gram frequencies to be more effective than dimensionality reduction techniques by a large margin. Their improvements were obtained by supervised learning on the combination of several lexical resources. Our work, on the other hand, is concerned with using more advanced features and we obtain significant improvements based on a diverse set of features and a different learning setup: we train a model for contextualization, rather than to combine substitutions from several different resources. A rece</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society for Information Science, 41(6):391– 407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgiana Dinu</author>
<author>Mirella Lapata</author>
</authors>
<title>Measuring distributional similarity in context.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1162--1172</pages>
<location>Cambridge, MA.</location>
<contexts>
<context position="10939" citStr="Dinu and Lapata (2010)" startWordPosition="1666" endWordPosition="1669"> Pad´o (2010) and Reisinger and Mooney (2010b) to model word meaning with respect to its context: instead of representing the word and the context as separate vectors and combining them, a set of word occurrences in similar contexts is picked first, and then only these exemplars are used to represent the word in context. While this approach provides good results with relatively simple and transparent models, each occurrence of a word has a unique representation (that can only be computed at testing time), and it is computationally expensive to scale these models to a large number of examples. Dinu and Lapata (2010) used a bag of words latent variable model to characterize the meaning of a word as a distribution over a set of latent variables (that is, probabilistic senses). Contextualized representation of word meaning is then attained by conditioning the model on the context words in which the target word occurs. A similar approach has been evaluated for word similarity (Reisinger and Mooney, 2010a) and word sense disambiguation (Li et al., 2010). Although our main goal here is to develop a fullfledged lexical substitution system, we mainly focus on the construction of better ranking models based on su</context>
</contexts>
<marker>Dinu, Lapata, 2010</marker>
<rawString>Georgiana Dinu and Mirella Lapata. 2010. Measuring distributional similarity in context. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1162–1172, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Dunning</author>
</authors>
<title>Accurate methods for the statistics of surprise and coincidence.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="24161" citStr="Dunning, 1993" startWordPosition="3865" endWordPosition="3866">-based Features In order to create a Distributional Thesaurus (DT) similar to Lin (1998), we parsed a source corpus of 120M sentence English newspaper texts from the LCC5 (Richter et al., 2006) with the Stanford parser (de Marneffe et al., 2006) and used dependencies to extract features for words: each dependency triple (w1, r, w2) denoting a dependency of type r between words w1 and w2 results in a feature (r, w2) characterizing w1, and a feature (w1, r) characterizing w26. After counting the frequency of each feature for each word, we apply a significance measure (log-likelihood test (LL), (Dunning, 1993)), rank features per word according to their significance, and prune the data, keeping only the 1000 most salient features (Fw) per word7. The similarity of two words is then given by the number of their common features. Our distributional thesaurus provides a list of the 1000 most salient features and a ranked list of up to 200 similar words (simw, based on the number of shared features) for all words above a certain frequency in the source corpus. We compute the following features to characterize a target word / substitution pair: • To what extent the context c characterizes si: L-�cEFsi LL(</context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>Ted Dunning. 1993. Accurate methods for the statistics of surprise and coincidence. Computational Linguistics, 19(1):61–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Pad´o</author>
</authors>
<title>A structured vector space model for word meaning in context.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>897--906</pages>
<location>Honolulu, Hawaii.</location>
<marker>Erk, Pad´o, 2008</marker>
<rawString>Katrin Erk and Sebastian Pad´o. 2008. A structured vector space model for word meaning in context. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 897–906, Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Pad´o</author>
</authors>
<title>Exemplar-based models for word meaning in context.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Conference Short Papers,</booktitle>
<pages>92--97</pages>
<location>Uppsala,</location>
<marker>Erk, Pad´o, 2010</marker>
<rawString>Katrin Erk and Sebastian Pad´o. 2010. Exemplar-based models for word meaning in context. In Proceedings of the ACL 2010 Conference Short Papers, pages 92– 97, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="3654" citStr="Fellbaum, 1998" startWordPosition="532" endWordPosition="533">t, and many good substitutions have other lexical relation than synonymy to the original word. In this work, we present a supervised lexical substitution system that, unlike the usual lexical sample supervised approaches, can produce substitutions for targets that are not contained in the training material. We reach this by using non-lexical features from heterogeneous evidence, including lexical-semantic resources and distributional similarity, n-gram and shallow syntactic features based on large, unannotated background corpora. In light of the existence of lexical resources such as WordNet (Fellbaum, 1998) or machine readable dictionaries that can serve as the source for lexical information, and with the ever-increasing availability of large unannotated corpora for many languages and 1131 Proceedings of NAACL-HLT 2013, pages 1131–1141, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics domains, our proposal enables us to leverage the quality gain of supervised machine learning while generalizing over a large vocabulary through the avoidance of lexicalized features. Using a single classifier for all substitution targets in this way results in an all-words substitu</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeniy Gabrilovich</author>
<author>Shaul Markovitch</author>
</authors>
<title>Computing Semantic Relatedness using Wikipedia-based Explicit Semantic Analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of the 20th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>1606--1611</pages>
<contexts>
<context position="7872" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="1184" endWordPosition="1187">and Navigli, 2007) at Semeval 2007 (LexSub) typically employ a simple ranking strategy based on local n-gram frequencies and focus on finding an optimal source of possible substitutions, as the selection of lexical resources has largest impact on the overall system performance: Sinha and Mihalcea (2009) systematically explored the benefits of multiple lexical resources and found that a supervised combination of several resources lead to statistically significant improvements in accuracy (about 3.5% points over the best single resource, WordNet). They tested LSA (Deerwester et al., 1990), ESA (Gabrilovich and Markovitch, 2007) and n-gram frequencies for contextualization and found n-gram frequencies to be more effective than dimensionality reduction techniques by a large margin. Their improvements were obtained by supervised learning on the combination of several lexical resources. Our work, on the other hand, is concerned with using more advanced features and we obtain significant improvements based on a diverse set of features and a different learning setup: we train a model for contextualization, rather than to combine substitutions from several different resources. A recent work by Sinha and Mihalcea (2011) use</context>
</contexts>
<marker>Gabrilovich, Markovitch, 2007</marker>
<rawString>Evgeniy Gabrilovich and Shaul Markovitch. 2007. Computing Semantic Relatedness using Wikipedia-based Explicit Semantic Analysis. In Proceedings of the 20th International Joint Conference on Artificial Intelligence, pages 1606–1611.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bela Gipp</author>
<author>Norman Meuschke</author>
<author>Joeran Beel</author>
</authors>
<title>Comparative Evaluation of Text- and Citation-based Plagiarism Detection Approaches using GuttenPlag.</title>
<date>2011</date>
<booktitle>In Proceedings of 11th ACM/IEEE-CS Joint Conference on Digital Libraries (JCDL’11),</booktitle>
<pages>255--258</pages>
<publisher>ACM</publisher>
<location>Ottawa,</location>
<note>Available at http://sciplore.org/pub/.</note>
<contexts>
<context position="2092" citStr="Gipp et al., 2011" startWordPosition="288" endWordPosition="291">t in the LexSub Best-Precision metric and the Generalized Average Precision measure. Robustness of our approach is demonstrated by evaluating it successfully on two different datasets. 1 Introduction In recent years, the task of automatically providing lexical substitutions in context (McCarthy and Navigli, 2007) received much attention. The premise to be able to replace words in a sentence without changing its meaning gave rise to applications like linguistic steganography (Topkara et al., 2006; Chang and Clark, 2010), semantic text similarity (Agirre et al., 2012), and plagiarism detection (Gipp et al., 2011). Lexical substitution, a special form of contextual paraphrasing where only a single word is replaced, is closely related to word sense disambiguation (WSD): polysemous words have possible substitutions reflecting several senses, and the correct sense has to be picked to avoid spurious system behavior. However, no explicit word sense inventory is required for lexical substitution (Dagan et al., 2006). The prominent tasks in a lexical substitution system are generation and ranking, i.e. to generate a set of possible substitutions for the target word and then to rank this set of possible substi</context>
</contexts>
<marker>Gipp, Meuschke, Beel, 2011</marker>
<rawString>Bela Gipp, Norman Meuschke, and Joeran Beel. 2011. Comparative Evaluation of Text- and Citation-based Plagiarism Detection Approaches using GuttenPlag. In Proceedings of 11th ACM/IEEE-CS Joint Conference on Digital Libraries (JCDL’11), pages 255–258, Ottawa, Canada. ACM New York, NY, USA. Available at http://sciplore.org/pub/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudio Giuliano</author>
<author>Alfio Gliozzo</author>
<author>Carlo Strapparava</author>
</authors>
<title>FBK-irst: Lexical substitution task exploiting domain and syntagmatic coherence.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>145--148</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="26149" citStr="Giuliano et al., 2007" startWordPosition="4189" endWordPosition="4192">nnecessary. 8The various values for k trade off the salience of this feature for coverage: only very few substitutions have overlap in the top 1-5 similar words set, but if this happens, it is a very strong indicator of contextual fitness, whereas overlap within the top 100-200 similar words is present for much more target/substitution pairs, but it is a weaker indicator of fitness. 1136 4.3.3 Local n-gram Features (from Web 1T) Syntagmatic coherence, measured as the n-gram frequency of the context with the candidate substitution serves as the basis of ranking in the best Semeval 2007 system (Giuliano et al., 2007), which is also our baseline method here. We use the same ngrams as features in our supervised model: • 1-5-gram frequencies in a sliding window around t: freq(cisicr)/freq(citcr), normalized w.r.t t • 1-5-gram frequencies in a sliding window around t: freq(cisicr)/E freq(ci5cr), normalized w.r.t. 5 • for each of x in {’and’, ’or’, ’,’�, 3-5- gram frequencies in a sliding window around t: freq(citxsicr)/freq(citcr) (how frequently the target and candidate are part of a list or conjunctive phrase) 4.3.4 Shallow Syntactic Features We also use part of speech information (from TreeTagger (Schmid, </context>
<context position="29196" citStr="Giuliano et al. (2007)" startWordPosition="4682" endWordPosition="4685"> the candidate has a high distributional similarity to the target with respect to our background corpus. The 12 shallow syntatic features (Sect 4.3.4) capture various present POS patterns around the target, and the 6 resourcebased features (Sect. 4.3.1) e.g. inform about the number of senses of the target (10) and the candidate (4). 4.4 Results Now, we describe our results in detail. First we compare our system on two datasets with a competitive baseline, which uses the same candidate set as our ML-based model, and the simple and effective ranking function based on Google n-grams described by Giuliano et al. (2007). Later on we analyze how the four major feature groups contribute to the results in a feature ablation experiment, and then we provide a detailed and thorough comparison to earlier works that are similar to the model presented here and used the same dataset (LexSub) for evaluation. 4.4.1 Semeval 2007 Lexical Substitution In Table 2 we report results on the LexSub dataset. As can be seen, our model outperforms the baseline by a significant margin (p &lt; 0.01 for all measures, using a paired t-test for significance). Both the overall rankings and the P@1 scores are of higher quality than the rank</context>
<context position="32894" citStr="Giuliano et al. (2007)" startWordPosition="5311" endWordPosition="5314">ng each of the feature groups out once. As can be seen, all feature groups improve the overall results in a noticeable way, i.e. their contribution is complementary. 4.5.1 Comparison to Previous Works In Table 5 we compare our method with previous works in the field, using the LexSub dataset. candidates from WN Best-P Giuliano 12.93 Martinez 12.68 Sinha 13.60 Baseline 11.75 Our model 15.94 Table 5: Comparison to previous works (LexSub dataset). In the left column of Table 5, we compare the performance of our system to representative Semeval 2007 participants, namely Martinez et al. (2007) and Giuliano et al. (2007). In order to make a fair comparison, we report scores for the official test data of Semeval 2007, using a 10-fold cross-validation scheme. Martinez et al. (2007) developed their system based on WordNet and we use the same candidate set here that they proposed in their system description. Our reimplementation of (Giuliano et al., 2007) performs below the original scores, due to the more restricted source of substitution candidates (they use more lexical resources), yet uses the same ranking methodology based on Google ngrams that we adopted here as our baseline. We also report the best previou</context>
</contexts>
<marker>Giuliano, Gliozzo, Strapparava, 2007</marker>
<rawString>Claudio Giuliano, Alfio Gliozzo, and Carlo Strapparava. 2007. FBK-irst: Lexical substitution task exploiting domain and syntagmatic coherence. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pages 145–148, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samer Hassan</author>
<author>Andras Csomai</author>
<author>Carmen Banea</author>
<author>Ravi Sinha</author>
<author>Rada Mihalcea</author>
</authors>
<title>UNT: SubFinder: Combining knowledge sources for automatic lexical substitution.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval2007),</booktitle>
<pages>410--413</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="6107" citStr="Hassan et al. (2007)" startWordPosition="914" endWordPosition="917"> large amount of sense tagged examples and (sensespecific) data annotated with substitutions, an accurate coarse-grained WSD model is trained and then the most frequent substitutions of the predicted sense are assigned to the new occurrences of the target words. The results demonstrate that lexical substitution of noun targets can be attained with very high precision (over 90%) if sufficient training material is available. However, due to high annotation costs, methods that do not require labeled training data per target scale better to a large vocabulary. Knowledge-based systems like e.g. by Hassan et al. (2007), who use a number of knowledge-based and unsupervised methods and combine these clues using a voting scheme, do not need training data per target. The combination of different signals, however, has to be done manually. Unsupervised systems that rely on distributional similarity (Thater et al., 2011) or topic models (Li et al., 2010) are single signals in this sense, and their development is guided by the performance and observations on standard datasets. Such signals, however, can also be kept simple avoiding any task-specific optimization and can be integrated in a single model for all words</context>
</contexts>
<marker>Hassan, Csomai, Banea, Sinha, Mihalcea, 2007</marker>
<rawString>Samer Hassan, Andras Csomai, Carmen Banea, Ravi Sinha, and Rada Mihalcea. 2007. UNT: SubFinder: Combining knowledge sources for automatic lexical substitution. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval2007), pages 410–413, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazuaki Kishida</author>
</authors>
<title>Property of Average Precision and Its Generalization: An Examination of Evaluation Indicator for Information Retrieval Experiments. NII technical report.</title>
<date>2005</date>
<institution>National Institute of Informatics.</institution>
<contexts>
<context position="15834" citStr="Kishida, 2005" startWordPosition="2474" endWordPosition="2475">d as a measure of goodness: the higher the value, the better the candidate fits in the context. Similarly, the label 0 denotes the total number of negative examples in our datasets, i.e. bad substitutions – words that belong to the can3This candidate set was found best for WordNet by Martinez et al. (2007). didate set for a particular target word, but are not listed as good substitutions in the given context in the dataset. 4 Methodology 4.1 Experimental Setup and Evaluation We follow previous works in lexical substitution and evaluate our models using the Generalized Average Precision (GAP) (Kishida, 2005) measure which assesses the quality of the entire ranked list. In addition, we also provide the precision of our system at the first rank (P@1), i.e. the percentage of correct paraphrases at rank 1. This is a realistic evaluation criterion for many applications, such as paraphrasing for linguistic steganography: it is the highestranked candidate that can be used to replace the original word (the manipulated text should preserve the original meaning) and there is no straightforward way to exploit multiple correct answers. In addition, we also provide the Semeval 2007 best precision4 metric (McC</context>
</contexts>
<marker>Kishida, 2005</marker>
<rawString>Kazuaki Kishida. 2005. Property of Average Precision and Its Generalization: An Examination of Evaluation Indicator for Information Retrieval Experiments. NII technical report. National Institute of Informatics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ping Li</author>
<author>Christopher J C Burges</author>
<author>Qiang Wu</author>
</authors>
<title>McRank: Learning to rank using multiple classification and gradient boosting.</title>
<date>2007</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<volume>19</volume>
<pages>897--904</pages>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="19934" citStr="Li et al. (2007" startWordPosition="3149" endWordPosition="3152">on, given the feature values that describe both the words and their context. The ranking over a set of candidates can be naturally induced based on their posterior scores for the positive class, i.e. a number that denotes ’how good the candidate is, given the context’. That is, the best substitution candidate s (characterized by a set of features F) from a set of candidates S is obtained as argmax3ES[P(goodIF)], the next best as the argmax of the remaining elements, and so on. This pointwise approach to subset ranking (Cossock and Zhang, 2008) is arguably simplistic, but several studies (c.f. Li et al. (2007; Busa-Fekete et al. (2011)) found this approach to perform reasonably well given that the model provides accurate probability estimates, which is the case for MaxEnt. 4.3 Delexicalized Features We use heterogeneous sources of information to describe each target word/candidate substitution pair in its context. The most important features describe the syntagmatic coherence of the substitute in context, measured as local n-gram frequencies obtained from web data, in a sliding window around the target word. In addition we use features to describe the (non-positional, i.e. non-local) distributiona</context>
</contexts>
<marker>Li, Burges, Wu, 2007</marker>
<rawString>Ping Li, Christopher J.C. Burges, and Qiang Wu. 2007. McRank: Learning to rank using multiple classification and gradient boosting. In Advances in Neural Information Processing Systems, volume 19, pages 897– 904. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Linlin Li</author>
<author>Benjamin Roth</author>
<author>Caroline Sporleder</author>
</authors>
<title>Topic models for word sense disambiguation and token-based idiom detection.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10,</booktitle>
<pages>1138--1147</pages>
<contexts>
<context position="6442" citStr="Li et al., 2010" startWordPosition="969" endWordPosition="972">ained with very high precision (over 90%) if sufficient training material is available. However, due to high annotation costs, methods that do not require labeled training data per target scale better to a large vocabulary. Knowledge-based systems like e.g. by Hassan et al. (2007), who use a number of knowledge-based and unsupervised methods and combine these clues using a voting scheme, do not need training data per target. The combination of different signals, however, has to be done manually. Unsupervised systems that rely on distributional similarity (Thater et al., 2011) or topic models (Li et al., 2010) are single signals in this sense, and their development is guided by the performance and observations on standard datasets. Such signals, however, can also be kept simple avoiding any task-specific optimization and can be integrated in a single model for all words using a limited amount of training data and delexicalized features, as in Senselearner (Mihalcea and Csomai, 2005) for weakly supervised all-words disambiguation. This way, task specific development can be replaced by a machine learning component and the resulting model applies also to unseen words, similar to the knowledge-based ap</context>
<context position="11380" citStr="Li et al., 2010" startWordPosition="1740" endWordPosition="1743">que representation (that can only be computed at testing time), and it is computationally expensive to scale these models to a large number of examples. Dinu and Lapata (2010) used a bag of words latent variable model to characterize the meaning of a word as a distribution over a set of latent variables (that is, probabilistic senses). Contextualized representation of word meaning is then attained by conditioning the model on the context words in which the target word occurs. A similar approach has been evaluated for word similarity (Reisinger and Mooney, 2010a) and word sense disambiguation (Li et al., 2010). Although our main goal here is to develop a fullfledged lexical substitution system, we mainly focus on the construction of better ranking models based on supervised machine learning and delexicalized features that scale well for unseen words. This approach has similar properties (applicability to all words without word-specific training data) to the knowledge-based and unsupervised models described above, so we will also refer to these systems for comparison. 3 Datasets In our work, we use two major freely available datasets that contain human-annotated substitutions for single words in the</context>
</contexts>
<marker>Li, Roth, Sporleder, 2010</marker>
<rawString>Linlin Li, Benjamin Roth, and Caroline Sporleder. 2010. Topic models for word sense disambiguation and token-based idiom detection. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 1138–1147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and the 17th International Conference on Computational Linguistics,</booktitle>
<volume>2</volume>
<pages>768--774</pages>
<location>Montreal, Quebec, Canada.</location>
<contexts>
<context position="23635" citStr="Lin (1998)" startWordPosition="3774" endWordPosition="3775">s IDs of the words’ hypernyms as features, which can capture more general semantics (the word to replace is ’animate’, ’abstract’, etc.). The following features were extracted from WordNet: • number of senses of t and si in WordNet • the sense numbers of t and si which are synonymous (in case they are direct synonyms, c.f. WN sense numbers encode sense frequencies) • binary features for synset IDs of the hypernyms of the synset containing t and si (this feature type did not significantly improve results) 4.3.2 Corpus-based Features In order to create a Distributional Thesaurus (DT) similar to Lin (1998), we parsed a source corpus of 120M sentence English newspaper texts from the LCC5 (Richter et al., 2006) with the Stanford parser (de Marneffe et al., 2006) and used dependencies to extract features for words: each dependency triple (w1, r, w2) denoting a dependency of type r between words w1 and w2 results in a feature (r, w2) characterizing w1, and a feature (w1, r) characterizing w26. After counting the frequency of each feature for each word, we apply a significance measure (log-likelihood test (LL), (Dunning, 1993)), rank features per word according to their significance, and prune the d</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and the 17th International Conference on Computational Linguistics, volume 2 of ACL ’98, pages 768– 774, Montreal, Quebec, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Martinez</author>
<author>Su Nam Kim</author>
<author>Timothy Baldwin</author>
</authors>
<title>MELB-MKB: Lexical substitution system based on relatives in context.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>237--240</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="15527" citStr="Martinez et al. (2007)" startWordPosition="2425" endWordPosition="2428">arget words, instances with at least one non-multiword possible substitution, average size of candidate sets, and number of instances with no good candidate and frequency of different labels. The labels denote how many annotators proposed a particular word as substitution in the given context and can be interpreted as a measure of goodness: the higher the value, the better the candidate fits in the context. Similarly, the label 0 denotes the total number of negative examples in our datasets, i.e. bad substitutions – words that belong to the can3This candidate set was found best for WordNet by Martinez et al. (2007). didate set for a particular target word, but are not listed as good substitutions in the given context in the dataset. 4 Methodology 4.1 Experimental Setup and Evaluation We follow previous works in lexical substitution and evaluate our models using the Generalized Average Precision (GAP) (Kishida, 2005) measure which assesses the quality of the entire ranked list. In addition, we also provide the precision of our system at the first rank (P@1), i.e. the percentage of correct paraphrases at rank 1. This is a realistic evaluation criterion for many applications, such as paraphrasing for lingu</context>
<context position="32867" citStr="Martinez et al. (2007)" startWordPosition="5306" endWordPosition="5309">able 2) and then with leaving each of the feature groups out once. As can be seen, all feature groups improve the overall results in a noticeable way, i.e. their contribution is complementary. 4.5.1 Comparison to Previous Works In Table 5 we compare our method with previous works in the field, using the LexSub dataset. candidates from WN Best-P Giuliano 12.93 Martinez 12.68 Sinha 13.60 Baseline 11.75 Our model 15.94 Table 5: Comparison to previous works (LexSub dataset). In the left column of Table 5, we compare the performance of our system to representative Semeval 2007 participants, namely Martinez et al. (2007) and Giuliano et al. (2007). In order to make a fair comparison, we report scores for the official test data of Semeval 2007, using a 10-fold cross-validation scheme. Martinez et al. (2007) developed their system based on WordNet and we use the same candidate set here that they proposed in their system description. Our reimplementation of (Giuliano et al., 2007) performs below the original scores, due to the more restricted source of substitution candidates (they use more lexical resources), yet uses the same ranking methodology based on Google ngrams that we adopted here as our baseline. We a</context>
</contexts>
<marker>Martinez, Kim, Baldwin, 2007</marker>
<rawString>David Martinez, Su Nam Kim, and Timothy Baldwin. 2007. MELB-MKB: Lexical substitution system based on relatives in context. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pages 237–240, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kachites McCallum</author>
</authors>
<title>MALLET: A Machine Learning for Language Toolkit.</title>
<date>2002</date>
<location>http://mallet.cs.umass.edu.</location>
<contexts>
<context position="18445" citStr="McCallum, 2002" startWordPosition="2904" endWordPosition="2905">et word level. That is, all sentences for a particular target word fall into the same fold and thus either the training or the test set (but never both). This way we always train and test the model on disjoint sets of words and as such, the learnt models cannot exploit word-specific properties. This makes our results realistic estimates of an open vocabulary paraphrasing system, where we would apply the models (mostly) to words that were not in the training material. 4.2.1 Machine Learning Model In our experiments, we used a Maximum Entropy (MaxEnt) classifier model implemented in the Mallet (McCallum, 2002) package and trained a binary classifier to predict if a given substitution is valid in a particular context or not. We chose to use Maximum Entropy models for two main reasons: MaxEnt is not sensitive to parameter settings and handles correlated features well, which is crucial in our situation where many features are highly correlated. Due to the low number of positive examples in the datasets (see Table 1, labels 1-5+) and to emphasize better paraphrases suggested by several annotators, we assigned a weight to positive instances during the training process equal to their score (the number of</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew Kachites McCallum. 2002. MALLET: A Machine Learning for Language Toolkit. http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Roberto Navigli</author>
</authors>
<title>Semeval2007 task 10: English lexical substitution task.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>48--53</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="1788" citStr="McCarthy and Navigli, 2007" startWordPosition="239" endWordPosition="243">ison to unsupervised approaches. Using features from lexical resources, as well as a variety of features computed from large corpora (n-gram counts, distributional similarity) and a ranking method based on the posterior probabilities obtained from a Maximum Entropy classifier, we improve over the state of the art in the LexSub Best-Precision metric and the Generalized Average Precision measure. Robustness of our approach is demonstrated by evaluating it successfully on two different datasets. 1 Introduction In recent years, the task of automatically providing lexical substitutions in context (McCarthy and Navigli, 2007) received much attention. The premise to be able to replace words in a sentence without changing its meaning gave rise to applications like linguistic steganography (Topkara et al., 2006; Chang and Clark, 2010), semantic text similarity (Agirre et al., 2012), and plagiarism detection (Gipp et al., 2011). Lexical substitution, a special form of contextual paraphrasing where only a single word is replaced, is closely related to word sense disambiguation (WSD): polysemous words have possible substitutions reflecting several senses, and the correct sense has to be picked to avoid spurious system b</context>
<context position="4716" citStr="McCarthy and Navigli, 2007" startWordPosition="694" endWordPosition="697">a large vocabulary through the avoidance of lexicalized features. Using a single classifier for all substitution targets in this way results in an all-words substitution system. As our results demonstrate, our model improves over the state of the art in lexical substitution with practically no open parameters that have to be optimized and selected carefully according to the dataset at hand. 2 Related Work Previous works in lexical substitution either address both the generation and the ranking tasks, and are therefore applicable to any word without prelabeled data (c.f. the Semeval 2007 task (McCarthy and Navigli, 2007) and related work) or focus on the more challenging ranking step only (c.f. Erk and Pad´o (2008) and related work). The latter approaches take the list of possible substitutions directly from the testing data as a workaround to generating the possible substitutions, and merely evaluate the ranking capabilities of these methods. The most accurate lexical substitution systems use supervised machine learning to train (and test) a separate classifier per target word, using lexical and shallow syntactic features. These systems rely on the existence of a large number of annotated examples (i.e. sent</context>
<context position="7257" citStr="McCarthy and Navigli, 2007" startWordPosition="1092" endWordPosition="1095">k-specific optimization and can be integrated in a single model for all words using a limited amount of training data and delexicalized features, as in Senselearner (Mihalcea and Csomai, 2005) for weakly supervised all-words disambiguation. This way, task specific development can be replaced by a machine learning component and the resulting model applies also to unseen words, similar to the knowledge-based approaches. 2.1 Full Lexical Substitution Systems Related works that address the lexical substitution problem according to the settings established by the English Lexical Substitution Task (McCarthy and Navigli, 2007) at Semeval 2007 (LexSub) typically employ a simple ranking strategy based on local n-gram frequencies and focus on finding an optimal source of possible substitutions, as the selection of lexical resources has largest impact on the overall system performance: Sinha and Mihalcea (2009) systematically explored the benefits of multiple lexical resources and found that a supervised combination of several resources lead to statistically significant improvements in accuracy (about 3.5% points over the best single resource, WordNet). They tested LSA (Deerwester et al., 1990), ESA (Gabrilovich and Ma</context>
<context position="16458" citStr="McCarthy and Navigli, 2007" startWordPosition="2574" endWordPosition="2577">05) measure which assesses the quality of the entire ranked list. In addition, we also provide the precision of our system at the first rank (P@1), i.e. the percentage of correct paraphrases at rank 1. This is a realistic evaluation criterion for many applications, such as paraphrasing for linguistic steganography: it is the highestranked candidate that can be used to replace the original word (the manipulated text should preserve the original meaning) and there is no straightforward way to exploit multiple correct answers. In addition, we also provide the Semeval 2007 best precision4 metric (McCarthy and Navigli, 2007) for the LexSub dataset for comparison to Semeval 2007 participants. This metric also evaluates the first guess of a system (per context), but gives less credit to easier contexts, where several good options exist. This fact motivates us to use P@1 rather than the best precision metric in all other experiments. 4Since our system always provides an answer, the Semeval 2007 best recall equals best precision. 1134 4.2 Machine Learning on Delexicalized Features After the list of potential substitutions is obtained, lexical substitution is cast as a ranking task where the goal is to prefer contextu</context>
</contexts>
<marker>McCarthy, Navigli, 2007</marker>
<rawString>Diana McCarthy and Roberto Navigli. 2007. Semeval2007 task 10: English lexical substitution task. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pages 48–53, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Andras Csomai</author>
</authors>
<title>Senselearner: word sense disambiguation for all words in unrestricted text.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL</booktitle>
<pages>53--56</pages>
<contexts>
<context position="6822" citStr="Mihalcea and Csomai, 2005" startWordPosition="1030" endWordPosition="1034">ing a voting scheme, do not need training data per target. The combination of different signals, however, has to be done manually. Unsupervised systems that rely on distributional similarity (Thater et al., 2011) or topic models (Li et al., 2010) are single signals in this sense, and their development is guided by the performance and observations on standard datasets. Such signals, however, can also be kept simple avoiding any task-specific optimization and can be integrated in a single model for all words using a limited amount of training data and delexicalized features, as in Senselearner (Mihalcea and Csomai, 2005) for weakly supervised all-words disambiguation. This way, task specific development can be replaced by a machine learning component and the resulting model applies also to unseen words, similar to the knowledge-based approaches. 2.1 Full Lexical Substitution Systems Related works that address the lexical substitution problem according to the settings established by the English Lexical Substitution Task (McCarthy and Navigli, 2007) at Semeval 2007 (LexSub) typically employ a simple ranking strategy based on local n-gram frequencies and focus on finding an optimal source of possible substitutio</context>
</contexts>
<marker>Mihalcea, Csomai, 2005</marker>
<rawString>Rada Mihalcea and Andras Csomai. 2005. Senselearner: word sense disambiguation for all words in unrestricted text. In Proceedings of the ACL 2005 on Interactive poster and demonstration sessions, ACLdemo ’05, pages 53–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Reisinger</author>
<author>Raymond Mooney</author>
</authors>
<title>A mixture model with sharing for lexical semantics.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1173--1182</pages>
<location>Cambridge, MA.</location>
<contexts>
<context position="10361" citStr="Reisinger and Mooney (2010" startWordPosition="1569" endWordPosition="1572">re used to characterize words according to their inverse selectional preference statistics for typical dependency relations. The representation of a word in its context is computed via combining the basic representation of a word with the inverse selectional preference vectors of its related words from the context. Ranking is done by comparing vectors of possible substitutions with the substitution target. Thater et al. (2010) took a similar approach but used second order co-occurrence vectors and report improved performance. An exemplar-based approach is presented by Erk and Pad´o (2010) and Reisinger and Mooney (2010b) to model word meaning with respect to its context: instead of representing the word and the context as separate vectors and combining them, a set of word occurrences in similar contexts is picked first, and then only these exemplars are used to represent the word in context. While this approach provides good results with relatively simple and transparent models, each occurrence of a word has a unique representation (that can only be computed at testing time), and it is computationally expensive to scale these models to a large number of examples. Dinu and Lapata (2010) used a bag of words l</context>
</contexts>
<marker>Reisinger, Mooney, 2010</marker>
<rawString>Joseph Reisinger and Raymond Mooney. 2010a. A mixture model with sharing for lexical semantics. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1173– 1182, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Reisinger</author>
<author>Raymond J Mooney</author>
</authors>
<title>Multi-prototype vector-space models of word meaning.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>109--117</pages>
<location>Los Angeles, California.</location>
<contexts>
<context position="10361" citStr="Reisinger and Mooney (2010" startWordPosition="1569" endWordPosition="1572">re used to characterize words according to their inverse selectional preference statistics for typical dependency relations. The representation of a word in its context is computed via combining the basic representation of a word with the inverse selectional preference vectors of its related words from the context. Ranking is done by comparing vectors of possible substitutions with the substitution target. Thater et al. (2010) took a similar approach but used second order co-occurrence vectors and report improved performance. An exemplar-based approach is presented by Erk and Pad´o (2010) and Reisinger and Mooney (2010b) to model word meaning with respect to its context: instead of representing the word and the context as separate vectors and combining them, a set of word occurrences in similar contexts is picked first, and then only these exemplars are used to represent the word in context. While this approach provides good results with relatively simple and transparent models, each occurrence of a word has a unique representation (that can only be computed at testing time), and it is computationally expensive to scale these models to a large number of examples. Dinu and Lapata (2010) used a bag of words l</context>
</contexts>
<marker>Reisinger, Mooney, 2010</marker>
<rawString>Joseph Reisinger and Raymond J. Mooney. 2010b. Multi-prototype vector-space models of word meaning. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 109–117, Los Angeles, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Richter</author>
<author>U Quasthoff</author>
<author>E Hallsteinsd´ottir</author>
<author>C Biemann</author>
</authors>
<title>Exploiting the leipzig corpora collection.</title>
<date>2006</date>
<booktitle>In Proceesings of the IS-LTC</booktitle>
<location>Ljubljana, Slovenia.</location>
<marker>Richter, Quasthoff, Hallsteinsd´ottir, Biemann, 2006</marker>
<rawString>M. Richter, U. Quasthoff, E. Hallsteinsd´ottir, and C. Biemann. 2006. Exploiting the leipzig corpora collection. In Proceesings of the IS-LTC 2006. Ljubljana, Slovenia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In Proceedings of the International Conference on New Methods in Language Processing,</booktitle>
<location>Manchester, UK.</location>
<contexts>
<context position="26754" citStr="Schmid, 1994" startWordPosition="4288" endWordPosition="4289">., 2007), which is also our baseline method here. We use the same ngrams as features in our supervised model: • 1-5-gram frequencies in a sliding window around t: freq(cisicr)/freq(citcr), normalized w.r.t t • 1-5-gram frequencies in a sliding window around t: freq(cisicr)/E freq(ci5cr), normalized w.r.t. 5 • for each of x in {’and’, ’or’, ’,’�, 3-5- gram frequencies in a sliding window around t: freq(citxsicr)/freq(citcr) (how frequently the target and candidate are part of a list or conjunctive phrase) 4.3.4 Shallow Syntactic Features We also use part of speech information (from TreeTagger (Schmid, 1994)) as features, in order to enable the model to learn POS-specific patterns. This is especially important for the LexSub dataset, which contains examples from all major parts of speech (the TWSI dataset contains only noun targets). Specifically, we use: • 1-3-grams of main POS categories in a window around t, e.g. NVV for a noun, verb, verb context • Penn Treebank POS code of t 4.3.5 Example For clarity, we exemplify our delexicalized features briefly. Using WordNet as a source for the word bright, we considered the 11 words brilliant, vivid, smart, burnished, lustrous, shining, shiny, undimmed</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of the International Conference on New Methods in Language Processing, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ravi Sinha</author>
<author>Rada Mihalcea</author>
</authors>
<title>Combining lexical resources for contextual synonym expansion.</title>
<date>2009</date>
<booktitle>In Proceedings of the International Conference RANLP2009,</booktitle>
<pages>404--410</pages>
<location>Borovets, Bulgaria.</location>
<contexts>
<context position="7543" citStr="Sinha and Mihalcea (2009)" startWordPosition="1137" endWordPosition="1140">aced by a machine learning component and the resulting model applies also to unseen words, similar to the knowledge-based approaches. 2.1 Full Lexical Substitution Systems Related works that address the lexical substitution problem according to the settings established by the English Lexical Substitution Task (McCarthy and Navigli, 2007) at Semeval 2007 (LexSub) typically employ a simple ranking strategy based on local n-gram frequencies and focus on finding an optimal source of possible substitutions, as the selection of lexical resources has largest impact on the overall system performance: Sinha and Mihalcea (2009) systematically explored the benefits of multiple lexical resources and found that a supervised combination of several resources lead to statistically significant improvements in accuracy (about 3.5% points over the best single resource, WordNet). They tested LSA (Deerwester et al., 1990), ESA (Gabrilovich and Markovitch, 2007) and n-gram frequencies for contextualization and found n-gram frequencies to be more effective than dimensionality reduction techniques by a large margin. Their improvements were obtained by supervised learning on the combination of several lexical resources. Our work, </context>
<context position="33811" citStr="Sinha and Mihalcea, 2009" startWordPosition="5463" endWordPosition="5466">r reimplementation of (Giuliano et al., 2007) performs below the original scores, due to the more restricted source of substitution candidates (they use more lexical resources), yet uses the same ranking methodology based on Google ngrams that we adopted here as our baseline. We also report the best previous result for this task, which 47.3 48.9 49.8 55.0 51.6 56.3 51.7 57.0 52.4 57.7 from Gold Standard GAP Pad´oErk10 38.6 DinuLapata 42.9 Thater10 46.0 Thater11 51.7 Baseline 46.9 Our model 52.4 1138 was achieved via the (supervised) combination of lexical resources to improve the performance (Sinha and Mihalcea, 2009). Our model outperforms this result by a large margin for the best-precision evaluation (mode-P, precision measured on those examples where there is a clear best substitution provided by humans was 26.3%, compared to 21.3% reported by Sinha and Mihalcea (2009). This is especially promising in light of the fact that we use only a single source (WordNet) for synonyms and achieve our improvements through more advanced delexicalized features in an improved ranking model. Sinha and Mihalcea (2009), on the other hand, used comparably simple features for contextualization, of which n-gram features we</context>
</contexts>
<marker>Sinha, Mihalcea, 2009</marker>
<rawString>Ravi Sinha and Rada Mihalcea. 2009. Combining lexical resources for contextual synonym expansion. In Proceedings of the International Conference RANLP2009, pages 404–410, Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ravi Som</author>
</authors>
<title>Sinha and Rada Flavia Mihalcea.</title>
<date>2011</date>
<editor>R. Charles Murray and Philip M. McCarthy, editors, FLAIRS Conference.</editor>
<publisher>AAAI Press.</publisher>
<marker>Som, 2011</marker>
<rawString>Ravi Som Sinha and Rada Flavia Mihalcea. 2011. Using centrality algorithms on directed graphs for synonym expansion. In R. Charles Murray and Philip M. McCarthy, editors, FLAIRS Conference. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Thater</author>
<author>Hagen F¨urstenau</author>
<author>Manfred Pinkal</author>
</authors>
<title>Contextualizing semantic representations using syntactically enriched vector models.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>948--957</pages>
<location>Uppsala,</location>
<marker>Thater, F¨urstenau, Pinkal, 2010</marker>
<rawString>Stefan Thater, Hagen F¨urstenau, and Manfred Pinkal. 2010. Contextualizing semantic representations using syntactically enriched vector models. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 948–957, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Thater</author>
<author>Hagen F¨urstenau</author>
<author>Manfred Pinkal</author>
</authors>
<title>Word meaning in context: A simple and effective vector model.</title>
<date>2011</date>
<journal>Chiang Mai, Thailand. MP, ISSN</journal>
<booktitle>In Proceedings of the Fifth International Joint Conference on Natural Language Processing : IJCNLP 2011,</booktitle>
<pages>1134--1143</pages>
<marker>Thater, F¨urstenau, Pinkal, 2011</marker>
<rawString>Stefan Thater, Hagen F¨urstenau, and Manfred Pinkal. 2011. Word meaning in context: A simple and effective vector model. In Proceedings of the Fifth International Joint Conference on Natural Language Processing : IJCNLP 2011, pages 1134–1143, Chiang Mai, Thailand. MP, ISSN 978-974-466-564-5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Umut Topkara</author>
<author>Mercan Topkara</author>
<author>Mikhail J Atallah</author>
</authors>
<title>The hiding virtues of ambiguity: quantifiably resilient watermarking of natural language text through synonym substitutions.</title>
<date>2006</date>
<booktitle>In Proceedings of the 8th workshop on Multimedia and security,</booktitle>
<pages>164--174</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1974" citStr="Topkara et al., 2006" startWordPosition="270" endWordPosition="273">hod based on the posterior probabilities obtained from a Maximum Entropy classifier, we improve over the state of the art in the LexSub Best-Precision metric and the Generalized Average Precision measure. Robustness of our approach is demonstrated by evaluating it successfully on two different datasets. 1 Introduction In recent years, the task of automatically providing lexical substitutions in context (McCarthy and Navigli, 2007) received much attention. The premise to be able to replace words in a sentence without changing its meaning gave rise to applications like linguistic steganography (Topkara et al., 2006; Chang and Clark, 2010), semantic text similarity (Agirre et al., 2012), and plagiarism detection (Gipp et al., 2011). Lexical substitution, a special form of contextual paraphrasing where only a single word is replaced, is closely related to word sense disambiguation (WSD): polysemous words have possible substitutions reflecting several senses, and the correct sense has to be picked to avoid spurious system behavior. However, no explicit word sense inventory is required for lexical substitution (Dagan et al., 2006). The prominent tasks in a lexical substitution system are generation and rank</context>
</contexts>
<marker>Topkara, Topkara, Atallah, 2006</marker>
<rawString>Umut Topkara, Mercan Topkara, and Mikhail J. Atallah. 2006. The hiding virtues of ambiguity: quantifiably resilient watermarking of natural language text through synonym substitutions. In Proceedings of the 8th workshop on Multimedia and security, pages 164– 174, New York, NY, USA. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>