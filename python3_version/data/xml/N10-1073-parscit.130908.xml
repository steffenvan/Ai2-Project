<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000251">
<title confidence="0.99862">
Improving the Multilingual User Experience of Wikipedia Using
Cross-Language Name Search
</title>
<author confidence="0.986985">
Raghavendra Udupa Mitesh Khapra *
</author>
<affiliation confidence="0.9870105">
Microsoft Research India Indian Institute of Technology Bombay
Bangalore, India. Powai, India.
</affiliation>
<sectionHeader confidence="0.987757" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999606095238095">
Although Wikipedia has emerged as a power-
ful collaborative Encyclopedia on the Web, it
is only partially multilingual as most of the
content is in English and a small number of
other languages. In real-life scenarios, non-
English users in general and ESL/EFL 1 users
in particular, have a need to search for rele-
vant English Wikipedia articles as no relevant
articles are available in their language. The
multilingual experience of such users can be
significantly improved if they could express
their information need in their native language
while searching for English Wikipedia arti-
cles. In this paper, we propose a novel cross-
language name search algorithm and employ
it for searching English Wikipedia articles in
a diverse set of languages including Hebrew,
Hindi, Russian, Kannada, Bangla and Tamil.
Our empirical study shows that the multilin-
gual experience of users is significantly im-
proved by our approach.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999609571428571">
Since its inception in 2001, Wikipedia has emerged
as the most famous free, web-based, collaborative,
and multilingual encyclopedia with over 13 million
articles in over 270 languages. However, Wikipedia
exhibits severe asymmetry in the distribution of its
content in the languages of the world with only a
small number of languages dominating (see Table
</bodyText>
<footnote confidence="0.9189805">
&amp;quot;This work was done when the author was a summer intern
at Microsoft Research India.
1English as Second Language and English as Foreign Lan-
guage.
</footnote>
<bodyText confidence="0.9632425">
1). As a consequence, most users of the under-
represented languages of the world have no choice
but to consult foreign language Wikipedia articles
for satisfying their information needs.
</bodyText>
<tableCaption confidence="0.999092">
Table 1: Linguistic asymmetry of Wikipedia
</tableCaption>
<table confidence="0.988417">
Language Speakers Contributors Articles
English 1500M 47.1% 3,072,373
Russian 278M 5.2% 441,860
Hebrew 10M 0.7% 97,987
Hindi 550M 0.06% 50,926
Bangla 230M 0.02% 20,342
Tamil 66M 0.04% 19,472
Kannada 47M 0.02% 7,185
</table>
<bodyText confidence="0.999868352941177">
Although consulting foreign language Wikipedia
is not a solution for the problem of linguistic asym-
metry, in the specific case of ESL/EFL users who
form a sizable fraction of Internet users of the world
2, it is arguably the most practical option today. Typ-
ically, ESL/EFL users are reasonably good at read-
ing and extracting relevant information from English
content but not so good at expressing their infor-
mation needs in English. In particular, getting the
spellings of foreign names in English correctly is
very difficult for most ESL/EFL users due to the dif-
ferences in the way a foreign name is pronounced
in the native languages. For instance, Japanese
EFL speakers often break consonant clusters in for-
eign names using vowels (see Table 2) and Hindi
ESL speakers find it difficult to differentiate between
‘an’, ‘en’, and ‘on’ in English names (such as ‘Clin-
</bodyText>
<footnote confidence="0.935751">
2As per some estimates, there are about 1 Billion ESL and
EFL speakers in the world today and their number is growing.
</footnote>
<page confidence="0.926036">
492
</page>
<note confidence="0.815381666666667">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 492–500,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
ton’) and will most likely use ‘an’ (‘Clintan’).
</note>
<tableCaption confidence="0.908907">
Table 2: Influence of native language on the English
spelling of names.
</tableCaption>
<table confidence="0.9973908">
Wikipedia Hindi Japanese Kannada
Entity
Stephen Stefan Suchifun Steephan
Hawking Hoking Houkingu Haakimg
Paul Krug- Pol Crugmun Pooru Paal Kraga-
man Kuruguman man
Haroun Haroon Haruun Haroon
al-Rashid al-Rashid aru-Rasheedo al-Rasheed
Subrahmaniya Subramaniya Suburaamaniya Subrahmanya
Bharati Bharati Bahaarachi Bharathi
</table>
<bodyText confidence="0.999450411764706">
In principle, English spell-checkers (Ahmad and
Kondrak, 2005) can handle the problem of incor-
rect spellings in the queries formed by ESL/EFL
users. But in practice, there are two difficulties.
Firstly, most English spell-checkers do not have a
good coverage of names which form the bulk of user
queries. Secondly, spelling correction of names is
difficult because spelling mistakes are markedly in-
fluenced by the native language of the user. Not
surprisingly, Wikipedia’s inbuilt spell-checker sug-
gests “Suchin Housing” as the only alternative to the
query “Suchifun Houkingu” instead of the correct
entity “Stephen Hawking” (See Table 3 for more ex-
amples).
The inability of ESL/EFL speakers to express
their information needs correctly in English and the
poor performance of spell-checkers highlight the
need for a practical solution for the linguistic asym-
metry problem of Wikipedia. In this work, we argue
the multilingual user experience of ESL/EFL users
can be significantly improved by allowing them to
express their information need in their native lan-
guage. While it might seem that we would need
a fully functional cross-language retrieval system
that supports translation of non-English queries to
English, we note that a good number of the pages
in Wikipedia are on people. This empirical fact
allows us to improve the multilingual experience
of ESL/EFL Wikipedia users by means of cross-
language name search which is less resource de-
manding than a fully functional cross-language re-
trieval system.
There are several challenges that need to be ad-
dressed in order to enable cross-language name
</bodyText>
<tableCaption confidence="0.999601">
Table 3: Spelling suggestions by Wikipedia.
</tableCaption>
<table confidence="0.966404777777778">
User Input Wikipedia’s Correct Spelling
Suggestion
Suchifun Houkingu Suchin Housing Stephen Hawking
Stefan Hoking Stefan Ho king Stephen Hawking
Pol Crugman Poll Krugman Paul Krugman
Paal Kragaman Paul Krugman Paul Krugman
Suburaamaniya Ba- Subramaniya Subrahmaniya
haarachi Baracchi Bharati
search in Wikipedia.
</table>
<listItem confidence="0.999789129032258">
• Firstly, name queries are expressed by
ESL/EFL users in the native languages using
the orthography of those languages. Translit-
erating the name into Latin script using a
Machine Transliteration system is an option
but state-of-the-art Machine Transliteration
technologies are still far away from producing
the correct transliteration. Further, as pointed
out by (Udupa et al., 2009a), it is not enough
if a Machine Transliteration system generates
a correct transliteration; it must produce the
transliteration that is present in the Wikipedia
title.
• Secondly, there are about 6 million titles (in-
cluding redirects) in English Wikipedia which
rules out the naive approach of comparing the
query with every one of the English Wikipedia
titles for transliteration equivalence as is done
typically in transliteration mining tasks. A
practical cross-language name search system
for Wikipedia must be able to search millions
of Wikipedia titles in a fraction of a second and
return the most relevant titles.
• Thirdly, names are typically multi-word and
as a consequence there might not be an ex-
act match between the query and English
Wikipedia titles. Any cross-language name
search system for Wikipedia must be able
to deal with multi-word names and partial
matches effectively.
• Fourthly, the cross-language name search sys-
</listItem>
<page confidence="0.999117">
493
</page>
<bodyText confidence="0.999883153846154">
tem must be tolerant to spelling variations in
the query as well as the Wikipedia titles.
In this work, we propose a novel approach to
cross-language name search in Wikipedia that ad-
dresses all the challenges described above. Fur-
ther, our approach does not depend on either spell-
checkers or Machine Transliteration. Rather we
transform the problem into a geometric search prob-
lem and employ a state-of-the-art geometric algo-
rithm for searching a very large database of names.
This enables us to accurately search the relevant
Wikipedia titles for a given user query in a fraction
of a second even on a single processor.
</bodyText>
<subsectionHeader confidence="0.990157">
1.1 Our Contributions
</subsectionHeader>
<bodyText confidence="0.990689">
Our contributions can be summarized as follows:
</bodyText>
<listItem confidence="0.670553166666667">
1. We introduce a language and orthography in-
dependent geometric representation for single-
word names (Section 3.1).
2. We model the problem of learning the geo-
metric representation of names as a multi-view
learning problem and employ the machinery
of Canonical Correlation Analysis (CCA) to
compute a low-dimensional Euclidean feature
space. We map both foreign single-word names
and English single-word names to points in the
common feature space and the similarity be-
tween two single-word names is an exponen-
tially decaying function of the squared geomet-
ric distance between the corresponding points
(Section 3).
3. We model the problem of searching a database
of names as a geometric nearest neighbor prob-
lem in low-dimensional Euclidean space and
employ the well-known ANN algorithm for
approximate nearest neighbors to search for
the equivalent of a query name in the English
Wikipedia titles (Arya et al., 1998) (Section
3.3).
4. We introduce a simple and efficient algorithm
for computing the similarity scores of multi-
word names from the single-word similarity
scores (Section 3.4).
5. We show experimentally that our approach sig-
nificantly improves the multilingual experience
of ESL/EFL users (Section 4).
</listItem>
<sectionHeader confidence="0.995988" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999690880952381">
Although approximate similarity search is well-
studied, we are not aware of any non-trivial cross-
language name search algorithm in the litera-
ture. However, several techniques for mining name
transliterations from monolingual and comparable
corpora have been studied (Pasternack and Roth,
2009), (Goldwasser and Roth, 2008), (Klementiev
and Roth, 2006), (Sproat et al., 2006), (Udupa et al.,
2009b). These techniques employ various translit-
eration similarity models. Character unigrams and
bigrams were used as features to learn a discrimi-
native transliteration model and time series similar-
ity was combined with the transliteration similarity
model (Klementiev and Roth, 2006). A generative
transliteration model was proposed and used along
with cross-language information retrieval to mine
named entity transliterations from large comparable
corpora (Udupa et al., 2009b). However, none of
these transliteration similarity models are applicable
for searching very large name databases as they rely
on brute-force search. Not surprisingly, (Pasternack
and Roth, 2009) report that “.. testing [727 single
word English names] with fifty thousand [Russian]
candidates is a large computational hurdle (it takes
our model about seven hours)”.
Several algorithms for string similarity search
have been proposed and applied to various problems
(Jin et al., 2005). None of them are directly applica-
ble to cross-language name search as they are based
on the assumption that the query string shares the
same alphabet as the database strings.
Machine Transliteration has been studied exten-
sively in the context of Machine Translation and
Cross-Language Information Retrieval (Knight and
Graehl, 1998), (Virga and Khudanpur, 2003), (Kuo
et al., 2006), (Sherif and Kondrak, 2007), (Ravi and
Knight, 2009), (Li et al., 2009), (Khapra and Bhat-
tacharyya, 2009). However, Machine Transliteration
followed by string similarity search gives less-than-
satisfactory solution for the cross-language name
search problem as we will see later in Section 4.
CCA was introduced by Hotelling in 1936 and has
</bodyText>
<page confidence="0.995166">
494
</page>
<bodyText confidence="0.999908375">
been applied to various problems including CLIR,
Text Clustering, and Image Retrieval (Hardoon et
al., 2004). Recently, CCA has gained importance
in the Machine Learning community as a technique
for multi-view learning. CCA computes a common
semantic feature space for two-view data and al-
lows users to query a database using either of the
two views. CCA has been used in bilingual lexi-
con extraction from comparable corpora (Gaussier
et al., 2004) and monolingual corpora (Haghighi et
al., 2008).
Nearest neighbor search is a fundamental prob-
lem where challenge is to preprocess a set of points
in some metric space into a geometric data struc-
ture so that given a query point, its k-nearest neigh-
bors in the set can be reported as fast as possi-
ble. It has applications in many areas including pat-
tern recognition and classification, machine learn-
ing, data compression, data mining, document re-
trieval and statistics. The brute-force search algo-
rithm can find the nearest neighbors in running time
proportional to the product of the number of points
and the dimension of the metric space. When the di-
mension of the metric space is small, there exist al-
gorithms which give better running time than brute-
force search. However, the search time grows expo-
nentially with the dimension and none of the algo-
rithms do significantly better than brute-force search
for high-dimensional data. Fortunately, efficient al-
gorithms exist if instead of exact nearest neighbors,
we ask for approximate nearest neighbors (Arya et
al., 1998).
</bodyText>
<sectionHeader confidence="0.8380155" genericHeader="method">
3 Cross-Language Name Search as a
Geometric Search Problem
</sectionHeader>
<bodyText confidence="0.999776136363636">
The key idea behind our approach is the following:
if we can embed names as points (or equivalently
as vectors) in a suitable geometric space, then the
problem of searching a very large database of names
can be casted as a geometric search problem, i.e. one
of finding the nearest neighbors of the query point in
the database.
As illustrative examples, consider the names
Stephen and Steven. A simple geometric represen-
tation for these names is the one induced by their
corresponding features: {5t, te, ep, ph, he, en} and
{5t, te, ev, ve, en} 3. In this representation, each
character bigram constitutes a dimension of the geo-
metric feature space whose coordinate value is the
number of times the bigram appears in the name.
It is possible to find a low-dimensional representa-
tion for the names by using Principal Components
Analysis or any other dimensionality reduction tech-
nique on the bigram feature vectors. However, the
key point to note is that once we have an appropri-
ate geometric representation for names, the similar-
ity between two names can be computed as
</bodyText>
<equation confidence="0.993738">
Kmono (name1, name2) = e−||01−02||2/2E2 (1)
</equation>
<bodyText confidence="0.999610454545455">
where 01 and 02 are the feature vectors of the two
names and E is a constant. Armed with the geomet-
ric similarity measure, we can leverage geometric
search techniques for finding names similar to the
query.
In the case of cross-language name search, we
need a feature representation of names that is lan-
guage/script independent. Once we map names in
different languages/scripts to the same feature space,
we can essentially treat similarity search as a geo-
metric search problem.
</bodyText>
<subsectionHeader confidence="0.9866695">
3.1 Language/Script Independent Geometric
Representation of Names
</subsectionHeader>
<bodyText confidence="0.9999102">
To obtain language/script independent geometric
representation of names, we start by forming the lan-
guage/script specific feature vectors as described in
Section 3. Given two names, Stephen in Latin script
and ?�O th -1 in Devanagari script, we form the corre-
sponding character bigram feature vectors 0 (using
features {5t, te, ep, ph, en}) and 0 (using features
{-V, VF,FP, Pn}) respectively. We then map these
vectors to a common geometric feature space using
two linear transformations A and B:
</bodyText>
<equation confidence="0.8630285">
0 , AT o = 0s E Rd (2)
� , BT4&apos; = 0s E Rd (3)
</equation>
<bodyText confidence="0.932103166666667">
The vectors 0s and 0s can be viewed as lan-
guage/script independent representation of the
names Stephen and ?�O th -1.
3Here, we have employed character bigrams as features. In
principle, we can use any suitable set of features including pho-
netic features extracted from the strings.
</bodyText>
<page confidence="0.995827">
495
</page>
<subsectionHeader confidence="0.681653">
3.1.1 Cross-Language Similarity of Names
</subsectionHeader>
<bodyText confidence="0.9722824">
In order to search a database of names in English
when the query is in a native language, say Hindi, we
need to be able to measure the similarity of a name in
Devangari script with names in Latin script. The lan-
guage/script independent representation gives a nat-
ural way to measure the similarity of names across
languages. By embedding the language/script spe-
cific feature vectors 0 and 0 in a common feature
space via the projections A and B, we can com-
pute the similarity of the corresponding names as
follows:
Kcross (name1, name2) = e−||φ3−ψ3||2/2E2 (4)
It is easy to see from Equation 4 that the similarity
score of two names is small when the projections of
the names are negatively correlated.
</bodyText>
<subsectionHeader confidence="0.7389305">
3.2 Learning Common Feature Space using
CCA
</subsectionHeader>
<bodyText confidence="0.99995324">
Ideally, the transformations A and B should be such
that similar names in the two languages are mapped
to close-by points in the common geometric fea-
ture space. It is possible to learn such transforma-
tions from a training set of name transliterations in
the two languages using the well-known multi-view
learning framework of Canonical Correlation Anal-
ysis (Hardoon et al., 2004). By viewing the lan-
guage/script specific feature vectors as two represen-
tations/views of the same semantic object, the entity
whose name is written as Stephen in English and as
?�O th -f in Hindi, we can employ the machinery of
CCA to find the transformations A and B.
Given a sample of multivariate data with two
views, CCA finds a linear transformation for each
view such that the correlation between the projec-
tions of the two views is maximized. Consider
a sample Z = {(xi, yi)}Ni=1 of multivariate data
where xi ∈ Rm and yi ∈ Rn are two views of the
object. Let X = {xi}Ni=1 and Y = {yi}Ni=1. As-
sume that X and Y are centered4, i.e., they have zero
mean. Let a and b be two directions. We can project
X onto the direction a to get U = {ui}Ni=1 where
ui = aT xi. Similarly, we can project Y onto the di-
rection b to get the projections V = {vi}ni=1 where
</bodyText>
<footnote confidence="0.894631">
4If X and Y are not centered, they can be centered by sub-
tracting the respective means.
</footnote>
<bodyText confidence="0.971538">
vi = bT yi. The aim of CCA is to find a pair of di-
rections (a, b) such that the projections U and V are
maximally correlated. This is achieved by solving
the following optimization problem:
&lt;Xa,Xb&gt;
The objective function of Equation 5 can be max-
imized by solving the following generalized eigen
value problem (Hardoon et al., 2004):
</bodyText>
<equation confidence="0.975553">
XYT (Y Y T)−1 YXTa = A2XXTa
(Y Y T)−1 YXTa = Ab
</equation>
<bodyText confidence="0.996679">
The subsequent basis vectors can be found
by adding the orthogonality of bases con-
straint to the objective function. Although
the number of basis vectors can be as high as
min{Rank(X), Rank(Y )}, in practice, only the
first few basis vectors are used since the correlation
of the projections is high for these vectors and small
for the remaining vectors.
Let A and B be the first d &gt; 0 basis vectors com-
puted by CCA.
</bodyText>
<figureCaption confidence="0.999704">
Figure 1: Projected names (English-Hindi).
</figureCaption>
<subsectionHeader confidence="0.830875">
3.2.1 Common Geometric Feature Space
</subsectionHeader>
<bodyText confidence="0.999905666666667">
As described in Section 3.1, we represent names
as points in the common geometric feature space de-
fined by the projection matrices A and B. Figure 1
</bodyText>
<equation confidence="0.962243">
P = max(a,b)
= max(a,b)
||Xa |Xb||
aT XYT b
√ √
aTXXTa bTY YTb
</equation>
<page confidence="0.989541">
496
</page>
<bodyText confidence="0.976504">
shows a 2-dimensional common feature space com-
puted by CCA for English (Latin script) and Hindi
(Devanagari script) names. As can be seen from the
figure, names that are transliterations of each other
are mapped to near-by points in the common feature
space.
Figure 2 shows a 2-dimensional common feature
space for English (Latin script) and Russian (Cyrillic
script) names. As can be seen from the figure, names
that are transliterations of each other are mapped to
near-by points in the common feature space.
</bodyText>
<figureCaption confidence="0.998462">
Figure 2: Projected names (English-Russian).
</figureCaption>
<subsectionHeader confidence="0.999271">
3.3 Querying the Name Database
</subsectionHeader>
<bodyText confidence="0.99754175">
Given a database D = {ei}Mi=1 of single-word
names in English, we first compute their lan-
guage/script specific feature vectors 0(i), i =
1,... , M. We then compute the projections �(i)
</bodyText>
<equation confidence="0.952788">
s =
AT O(i). Thus, we transform the name database D
into a set of vectors {�(1)
s , ... , �(M)
s } in Rd.
</equation>
<bodyText confidence="0.999499666666667">
Given a query name h in Hindi, we compute its
language/script specific feature vector 0 and project
it on to the common feature space to get 0s =
BTo E Rd. Names similar to h in the database D
can be found as solutions of the k-nearest neighbor
problem:
</bodyText>
<equation confidence="0.931676666666667">
= (i) − , /,
argmi1Ze.ED_ffei.11k−1 ||s 4sll
a l a7 J j=1
</equation>
<bodyText confidence="0.913089714285714">
Unfortunately, computing exact k-nearest neigh-
bors in dimensions much higher than 8 is difficult
and the best-known methods are only marginally
better than brute-force search (Arya et al., 1998).
Fortunately, there exist very efficient algorithms for
computing approximate nearest neighbors and in
practice they do nearly as well as the exact near-
est neighbors algorithms (Arya et al., 1998). It is
also possible to control the tradeoff between accu-
racy and running time by specifiying a maximum
approximation error bound. We employ the well-
known Approximate Nearest Neighbors (aka ANN)
algorithm by Arya and Mount which is known to do
well in practice when d &lt; 100 (Arya et al., 1998).
</bodyText>
<subsectionHeader confidence="0.99954">
3.4 Combining Single-Word Similarities
</subsectionHeader>
<bodyText confidence="0.999942428571429">
The approach described in the previous sections
works only for single-word names. We need to com-
bine the similarities at the level of individual words
into a similarity function for multi-word names. To-
wards this end, we form a weighted bipartite graph
from the two multi-word names as follows:
We first tokenize the Hindi query name into sin-
gle word tokens and find the nearest English neigh-
bors for each of these Hindi tokens using the method
outlined section 3.3. We then find out all the En-
glish Words which contain one or more of the En-
glish neighbors thus fetched. Let E = e1e2 ... eI
be one such multi-word English name and H =
h1h2 ... hJ be the multi-word Hindi query. We form
a weighted bipartite graph G = (5 U T, W) with a
node si for the ith word ei in E and node tj for the
jth word hj in H. The weight of the edge (si, tj) is
set as wij = Kcross (ei, hj).
Let w be the weight of the maximum weighted
bipartite matching in the graph G. We define the
similarity between E and H as follows:
</bodyText>
<equation confidence="0.980519">
w
Kcross (E, H) = |I − J |+ 1. (5)
</equation>
<bodyText confidence="0.999933625">
The numerator of the right hand side of Equation
5 favors name pairs which have a good number of
high quality matches at the individual word level
whereas the denominator penalizes pairs that have
disproportionate lengths.
Note that, in practice, both I and J are small and
hence we can find the maximum weighted bipartite
matching very easily. Further, most edge weights in
</bodyText>
<equation confidence="0.95270425">
eik = argmaxei∈D−{eij }k−1
j=1 Kcross (ei,h)
= argmaxei∈D−{eij }k−1 e−||φ(i)
j=1 s −ψs||2/2,2
</equation>
<page confidence="0.994119">
497
</page>
<figureCaption confidence="0.999159">
Figure 3: Combining Single-Word Similarities.
</figureCaption>
<bodyText confidence="0.982846333333333">
the bipartite graph are negligibly small. Therefore,
even a greedy matching algorithm suffices in prac-
tice.
</bodyText>
<sectionHeader confidence="0.994005" genericHeader="evaluation">
4 Experiments and Results
</sectionHeader>
<bodyText confidence="0.997325">
In the remainder of this section, we refer to our sys-
tem by GEOM-SEARCH.
</bodyText>
<subsectionHeader confidence="0.985103">
4.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.99998536">
We tested our cross language name search system
using six native languages, viz., Russian, Hebrew,
Hindi, Kannada, Tamil and Bangla. For each of
these languages, we created a test set consisting of
1000 multi-word name queries and found manually
the most relevant Wikipedia article for each query in
the test set. The Wikipedia articles thus found and
all the redirect titles that linked to them formed the
gold standard for evaluating the performance of our
system.
In order to compare the performance of GEOM-
SEARCH with a reasonable baseline, we imple-
mented the following baseline: We used a state-of-
the art Machine Transliteration system to generate
the best transliteration of each of the queries. We
used the edit distance between the transliteration and
the single-word English name as the similarity score.
We combined single word similarities using the ap-
proach described in Section 3.4. We refer to this
baseline by TRANS-SEARCH.
Note that several English Wikipedia names some-
times get the same score for a query. Therefore,
we used a tie-aware mean-reciprocal rank measure
to evaluate the performance (McSherry and Najork,
2008).
</bodyText>
<subsectionHeader confidence="0.972229">
4.2 GEOM-SEARCH
</subsectionHeader>
<bodyText confidence="0.9992715">
The training and search procedure employed by
GEOM-SEARCH are described below.
</bodyText>
<subsubsectionHeader confidence="0.58493">
4.2.1 CCA Training
</subsubsectionHeader>
<bodyText confidence="0.99997">
We learnt the linear transformations A and B that
project the language/script specific feature vectors to
the common feature space using the approach dis-
cussed in Section 3.2. The learning algorithm re-
quires a training set consisting of pairs of single-
word names in English and the respective native lan-
guage. We used approximately 15, 000 name pairs
for each native language.
A key parameter in CCA training is the number of
dimensions of the common feature space. We found
the optimal number of dimensions using a tuning set
consisting of 1, 000 correct name pairs and 1, 000
incorrect name pairs for each native language. We
found that d = 50 is a very good choice for each
native language.
Another key aspect of training is the choice of
language/script specific features. For the six lan-
guages we experimented with and also for English,
we found that character bigrams formed a good set
of features. We note that for languages such as Chi-
nese, Japanese, and Korean, unigrams are the best
choice. Also, for these languages, it may help to
syllabify the English name.
</bodyText>
<subsectionHeader confidence="0.617649">
4.2.2 Search
</subsectionHeader>
<bodyText confidence="0.9999715625">
As a pre-processing step, we extracted a list of 1.3
million unique words from the Wikipedia titles. We
computed the language/script specific feature vector
for each word in this list and projected the vector to
the common feature space as described in Section
3.1. The low-dimensional embeddings thus com-
puted formed the input to the ANN algorithm.
We tokenized each query in the native language
into constituent words. For each constituent, we first
computed the language/script specific feature vector,
projected it to the common feature space, and found
the k-nearest neighbors using the ANN algorithm.
We used k=100 for all our experiments.
After finding the nearest neighbors and the corre-
sponding similarity scores, we combined the scores
using the approach described in Section 3.4.
</bodyText>
<subsectionHeader confidence="0.962026">
4.3 TRANS-SEARCH
</subsectionHeader>
<bodyText confidence="0.998636">
The training and search procedure employed by
TRANS-SEARCH are described below.
</bodyText>
<page confidence="0.997821">
498
</page>
<figureCaption confidence="0.996245">
Figure 4: Top scoring English Wikipedia page retrieved by GEOM-SEARCH
</figureCaption>
<subsubsectionHeader confidence="0.9009">
4.3.1 Transliteration Training
</subsubsectionHeader>
<bodyText confidence="0.99998">
We used a state-of-the-art CRF-based translitera-
tion technique for transliterating the native language
names (Khapra and Bhattacharyya, 2009). We used
CRF++, an open-source CRF training tool, to train
the transliteration system. We used exactly the
same features and parameter settings as described in
(Khapra and Bhattacharyya, 2009). As in the case of
CCA, we use around 15, 000 single word name pairs
in the training.
</bodyText>
<subsectionHeader confidence="0.815553">
4.3.2 Search
</subsectionHeader>
<bodyText confidence="0.999990666666667">
The preprocessing step for TRANS-SEARCH is
the same as that for GEOM-SEARCH. We translit-
erated each constituent of the query into English and
find all single-word English names that are at an edit
distance of at most 3. We computed the similarity
score as described in Section 3.4.
</bodyText>
<subsectionHeader confidence="0.962059">
4.4 Evaluation
</subsectionHeader>
<bodyText confidence="0.999845846153846">
We evaluated the performance of GEOM-SEARCH
and TRANS-SEARCH using a tie-aware mean re-
ciprocal rank (MRR). Table 4 compares the average
time per query and the MRR of the two systems.
GEOM-SEARCH performed significantly better
than the transliteration based baseline system for all
the six languages. On an average, the relevant En-
glish Wikipedia page was found in the top 2 re-
sults produced by GEOM-SEARCH for all the six
native languages. Clearly, this shows that GEOM-
SEARCH is highly effective as a cross-langauge
name search system. The good results also validate
our claim that cross-language name search can im-
</bodyText>
<tableCaption confidence="0.9678065">
Table 4: MRR and average time per query (in seconds)
for the two systems.
</tableCaption>
<table confidence="0.998717625">
Language GEOM TRANS
Time MRR Time MRR
Hin 0.51 0.686 2.39 0.485
Tam 0.23 0.494 2.16 0.291
Kan 1.08 0.689 2.17 0.522
Ben 1.30 0.495 – –
Rus 0.15 0.563 1.65 0.476
Heb 0.65 0.723 – –
</table>
<bodyText confidence="0.960173">
prove the multi-lingual user experience of ESL/EFL
users.
</bodyText>
<sectionHeader confidence="0.999545" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999763636363636">
GEOM-SEARCH, a geometry-based cross-
language name search system for Wikipedia,
improves the multilingual experience of ESL/EFL
users of Wikipedia by allowing them to formulate
queries in their native languages. Further, it is easy
to integrate a Machine Translation system with
GEOM-SEARCH. Such a system would find the
relevant English Wikipedia page for a query using
GEOM-SEARCH and then translate the relevant
Wikipedia pages to the native language using the
Machine Translation system.
</bodyText>
<sectionHeader confidence="0.999348" genericHeader="acknowledgments">
6 Acknowledgement
</sectionHeader>
<bodyText confidence="0.9779505">
We thank Jagadeesh Jagarlamudi and Shaishav Ku-
mar for their help.
</bodyText>
<page confidence="0.998982">
499
</page>
<sectionHeader confidence="0.995876" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.983534494117647">
Farooq Ahmad and Grzegorz Kondrak. 2005. Learn-
ing a spelling error model from search query logs. In
HLT ’05: Proceedings of the conference on Human
Language Technology and Empirical Methods in Nat-
ural Language Processing, pages 955–962, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Sunil Arya, David M. Mount, Nathan S. Netanyahu, Ruth
Silverman, and Angela Y. Wu. 1998. An optimal
algorithm for approximate nearest neighbor searching
fixed dimensions. J. ACM, 45(6):891–923.
´Eric Gaussier, Jean-Michel Renders, Irina Matveeva,
Cyril Goutte, and Herv´e D´ejean. 2004. A geometric
view on bilingual lexicon extraction from comparable
corpora. In ACL, pages 526–533.
Dan Goldwasser and Dan Roth. 2008. Transliteration as
constrained optimization. In EMNLP, pages 353–362.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proceedings of ACL-
08: HLT, pages 771–779, Columbus, Ohio, June. As-
sociation for Computational Linguistics.
David R. Hardoon, S´andor Szedm´ak, and John Shawe-
Taylor. 2004. Canonical correlation analysis: An
overview with application to learning methods. Neu-
ral Computation, 16(12):2639–2664.
Liang Jin, Nick Koudas, Chen Li, and Anthony K. H.
Tung. 2005. Indexing mixed types for approximate
retrieval. In VLDB, pages 793–804.
Mitesh Khapra and Pushpak Bhattacharyya. 2009. Im-
proving transliteration accuracy using word-origin de-
tection and lexicon lookup. In Proceedings of the 2009
Named Entities Workshop: Shared Task on Translit-
eration (NEWS 2009). Association for Computational
Linguistics.
Alexandre Klementiev and Dan Roth. 2006. Named
entity transliteration and discovery from multilingual
comparable corpora. In HLT-NAACL.
Kevin Knight and Jonathan Graehl. 1998. Machine
transliteration. Computational Linguistics, 24(4):599–
612.
Jin-Shea Kuo, Haizhou Li, and Ying-Kuei Yang. 2006.
Learning transliteration lexicons from the web. In
ACL.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In ICML, pages 282–289.
Haizhou Li, A Kumaran, Vladimir Pervouchine, and
Min Zhang. 2009. Report of news 2009 machine
transliteration shared task. In Proceedings of the 2009
Named Entities Workshop: Shared Task on Translit-
eration (NEWS 2009). Association for Computational
Linguistics.
Frank McSherry and Marc Najork. 2008. Computing
information retrieval performance measures efficiently
in the presence of tied scores. In ECIR, pages 414–
421.
Jeff Pasternack and Dan Roth. 2009. Learning better
transliterations. In CIKM, pages 177–186.
Sujith Ravi and Kevin Knight. 2009. Learning phoneme
mappings for transliteration without parallel data. In
NAACL-HLT.
Hanan Samet. 2006. Foundations of Multidimensional
and Metric Data Structures (The Morgan Kaufmann
Series in Computer Graphics). Morgan Kaufmann,
August.
Tarek Sherif and Grzegorz Kondrak. 2007. Substring-
based transliteration. In ACL.
Richard Sproat, Tao Tao, and ChengXiang Zhai. 2006.
Named entity transliteration with comparable corpora.
In ACL.
Raghavendra Udupa, K. Saravanan, Anton Bakalov, and
Abhijit Bhole. 2009a. “they are out there, if you
know where to look”: Mining transliterations of oov
query terms for cross-language information retrieval.
In ECIR, pages 437–448.
Raghavendra Udupa, K. Saravanan, A. Kumaran, and Ja-
gadeesh Jagarlamudi. 2009b. Mint: A method for ef-
fective and scalable mining of named entity transliter-
ations from large comparable corpora. In EACL, pages
799–807.
Paola Virga and Sanjeev Khudanpur. 2003. Transliter-
ation of proper names in cross-language applications.
In SIGIR, pages 365–366.
</reference>
<page confidence="0.995098">
500
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.670602">
<title confidence="0.9500845">Improving the Multilingual User Experience of Wikipedia Cross-Language Name Search</title>
<author confidence="0.903045">Udupa Mitesh Khapra</author>
<affiliation confidence="0.999152">Microsoft Research India Indian Institute of Technology Bombay</affiliation>
<address confidence="0.991763">Bangalore, India. Powai, India.</address>
<abstract confidence="0.987807">Although Wikipedia has emerged as a powerful collaborative Encyclopedia on the Web, it is only partially multilingual as most of the content is in English and a small number of other languages. In real-life scenarios, nonusers in general and ESL/EFL 1users in particular, have a need to search for relevant English Wikipedia articles as no relevant articles are available in their language. The multilingual experience of such users can be significantly improved if they could express their information need in their native language while searching for English Wikipedia articles. In this paper, we propose a novel crosslanguage name search algorithm and employ it for searching English Wikipedia articles in a diverse set of languages including Hebrew, Hindi, Russian, Kannada, Bangla and Tamil. Our empirical study shows that the multilingual experience of users is significantly improved by our approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Farooq Ahmad</author>
<author>Grzegorz Kondrak</author>
</authors>
<title>Learning a spelling error model from search query logs.</title>
<date>2005</date>
<booktitle>In HLT ’05: Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>955--962</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="3795" citStr="Ahmad and Kondrak, 2005" startWordPosition="582" endWordPosition="585">Chapter of the ACL, pages 492–500, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics ton’) and will most likely use ‘an’ (‘Clintan’). Table 2: Influence of native language on the English spelling of names. Wikipedia Hindi Japanese Kannada Entity Stephen Stefan Suchifun Steephan Hawking Hoking Houkingu Haakimg Paul Krug- Pol Crugmun Pooru Paal Kragaman Kuruguman man Haroun Haroon Haruun Haroon al-Rashid al-Rashid aru-Rasheedo al-Rasheed Subrahmaniya Subramaniya Suburaamaniya Subrahmanya Bharati Bharati Bahaarachi Bharathi In principle, English spell-checkers (Ahmad and Kondrak, 2005) can handle the problem of incorrect spellings in the queries formed by ESL/EFL users. But in practice, there are two difficulties. Firstly, most English spell-checkers do not have a good coverage of names which form the bulk of user queries. Secondly, spelling correction of names is difficult because spelling mistakes are markedly influenced by the native language of the user. Not surprisingly, Wikipedia’s inbuilt spell-checker suggests “Suchin Housing” as the only alternative to the query “Suchifun Houkingu” instead of the correct entity “Stephen Hawking” (See Table 3 for more examples). The</context>
</contexts>
<marker>Ahmad, Kondrak, 2005</marker>
<rawString>Farooq Ahmad and Grzegorz Kondrak. 2005. Learning a spelling error model from search query logs. In HLT ’05: Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 955–962, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sunil Arya</author>
<author>David M Mount</author>
<author>Nathan S Netanyahu</author>
<author>Ruth Silverman</author>
<author>Angela Y Wu</author>
</authors>
<title>An optimal algorithm for approximate nearest neighbor searching fixed dimensions.</title>
<date>1998</date>
<journal>J. ACM,</journal>
<volume>45</volume>
<issue>6</issue>
<contexts>
<context position="8631" citStr="Arya et al., 1998" startWordPosition="1337" endWordPosition="1340">a low-dimensional Euclidean feature space. We map both foreign single-word names and English single-word names to points in the common feature space and the similarity between two single-word names is an exponentially decaying function of the squared geometric distance between the corresponding points (Section 3). 3. We model the problem of searching a database of names as a geometric nearest neighbor problem in low-dimensional Euclidean space and employ the well-known ANN algorithm for approximate nearest neighbors to search for the equivalent of a query name in the English Wikipedia titles (Arya et al., 1998) (Section 3.3). 4. We introduce a simple and efficient algorithm for computing the similarity scores of multiword names from the single-word similarity scores (Section 3.4). 5. We show experimentally that our approach significantly improves the multilingual experience of ESL/EFL users (Section 4). 2 Related Work Although approximate similarity search is wellstudied, we are not aware of any non-trivial crosslanguage name search algorithm in the literature. However, several techniques for mining name transliterations from monolingual and comparable corpora have been studied (Pasternack and Roth,</context>
<context position="12548" citStr="Arya et al., 1998" startWordPosition="1939" endWordPosition="1942">nd statistics. The brute-force search algorithm can find the nearest neighbors in running time proportional to the product of the number of points and the dimension of the metric space. When the dimension of the metric space is small, there exist algorithms which give better running time than bruteforce search. However, the search time grows exponentially with the dimension and none of the algorithms do significantly better than brute-force search for high-dimensional data. Fortunately, efficient algorithms exist if instead of exact nearest neighbors, we ask for approximate nearest neighbors (Arya et al., 1998). 3 Cross-Language Name Search as a Geometric Search Problem The key idea behind our approach is the following: if we can embed names as points (or equivalently as vectors) in a suitable geometric space, then the problem of searching a very large database of names can be casted as a geometric search problem, i.e. one of finding the nearest neighbors of the query point in the database. As illustrative examples, consider the names Stephen and Steven. A simple geometric representation for these names is the one induced by their corresponding features: {5t, te, ep, ph, he, en} and {5t, te, ev, ve,</context>
<context position="19689" citStr="Arya et al., 1998" startWordPosition="3195" endWordPosition="3198"> �(i) s = AT O(i). Thus, we transform the name database D into a set of vectors {�(1) s , ... , �(M) s } in Rd. Given a query name h in Hindi, we compute its language/script specific feature vector 0 and project it on to the common feature space to get 0s = BTo E Rd. Names similar to h in the database D can be found as solutions of the k-nearest neighbor problem: = (i) − , /, argmi1Ze.ED_ffei.11k−1 ||s 4sll a l a7 J j=1 Unfortunately, computing exact k-nearest neighbors in dimensions much higher than 8 is difficult and the best-known methods are only marginally better than brute-force search (Arya et al., 1998). Fortunately, there exist very efficient algorithms for computing approximate nearest neighbors and in practice they do nearly as well as the exact nearest neighbors algorithms (Arya et al., 1998). It is also possible to control the tradeoff between accuracy and running time by specifiying a maximum approximation error bound. We employ the wellknown Approximate Nearest Neighbors (aka ANN) algorithm by Arya and Mount which is known to do well in practice when d &lt; 100 (Arya et al., 1998). 3.4 Combining Single-Word Similarities The approach described in the previous sections works only for singl</context>
</contexts>
<marker>Arya, Mount, Netanyahu, Silverman, Wu, 1998</marker>
<rawString>Sunil Arya, David M. Mount, Nathan S. Netanyahu, Ruth Silverman, and Angela Y. Wu. 1998. An optimal algorithm for approximate nearest neighbor searching fixed dimensions. J. ACM, 45(6):891–923.</rawString>
</citation>
<citation valid="true">
<authors>
<author>´Eric Gaussier</author>
<author>Jean-Michel Renders</author>
<author>Irina Matveeva</author>
<author>Cyril Goutte</author>
<author>Herv´e D´ejean</author>
</authors>
<title>A geometric view on bilingual lexicon extraction from comparable corpora.</title>
<date>2004</date>
<booktitle>In ACL,</booktitle>
<pages>526--533</pages>
<marker>Gaussier, Renders, Matveeva, Goutte, D´ejean, 2004</marker>
<rawString>´Eric Gaussier, Jean-Michel Renders, Irina Matveeva, Cyril Goutte, and Herv´e D´ejean. 2004. A geometric view on bilingual lexicon extraction from comparable corpora. In ACL, pages 526–533.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Goldwasser</author>
<author>Dan Roth</author>
</authors>
<title>Transliteration as constrained optimization.</title>
<date>2008</date>
<booktitle>In EMNLP,</booktitle>
<pages>353--362</pages>
<contexts>
<context position="9266" citStr="Goldwasser and Roth, 2008" startWordPosition="1430" endWordPosition="1433"> 3.3). 4. We introduce a simple and efficient algorithm for computing the similarity scores of multiword names from the single-word similarity scores (Section 3.4). 5. We show experimentally that our approach significantly improves the multilingual experience of ESL/EFL users (Section 4). 2 Related Work Although approximate similarity search is wellstudied, we are not aware of any non-trivial crosslanguage name search algorithm in the literature. However, several techniques for mining name transliterations from monolingual and comparable corpora have been studied (Pasternack and Roth, 2009), (Goldwasser and Roth, 2008), (Klementiev and Roth, 2006), (Sproat et al., 2006), (Udupa et al., 2009b). These techniques employ various transliteration similarity models. Character unigrams and bigrams were used as features to learn a discriminative transliteration model and time series similarity was combined with the transliteration similarity model (Klementiev and Roth, 2006). A generative transliteration model was proposed and used along with cross-language information retrieval to mine named entity transliterations from large comparable corpora (Udupa et al., 2009b). However, none of these transliteration similarit</context>
</contexts>
<marker>Goldwasser, Roth, 2008</marker>
<rawString>Dan Goldwasser and Dan Roth. 2008. Transliteration as constrained optimization. In EMNLP, pages 353–362.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Percy Liang</author>
<author>Taylor Berg-Kirkpatrick</author>
<author>Dan Klein</author>
</authors>
<title>Learning bilingual lexicons from monolingual corpora.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL08: HLT,</booktitle>
<pages>771--779</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="11523" citStr="Haghighi et al., 2008" startWordPosition="1769" endWordPosition="1772">the cross-language name search problem as we will see later in Section 4. CCA was introduced by Hotelling in 1936 and has 494 been applied to various problems including CLIR, Text Clustering, and Image Retrieval (Hardoon et al., 2004). Recently, CCA has gained importance in the Machine Learning community as a technique for multi-view learning. CCA computes a common semantic feature space for two-view data and allows users to query a database using either of the two views. CCA has been used in bilingual lexicon extraction from comparable corpora (Gaussier et al., 2004) and monolingual corpora (Haghighi et al., 2008). Nearest neighbor search is a fundamental problem where challenge is to preprocess a set of points in some metric space into a geometric data structure so that given a query point, its k-nearest neighbors in the set can be reported as fast as possible. It has applications in many areas including pattern recognition and classification, machine learning, data compression, data mining, document retrieval and statistics. The brute-force search algorithm can find the nearest neighbors in running time proportional to the product of the number of points and the dimension of the metric space. When th</context>
</contexts>
<marker>Haghighi, Liang, Berg-Kirkpatrick, Klein, 2008</marker>
<rawString>Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick, and Dan Klein. 2008. Learning bilingual lexicons from monolingual corpora. In Proceedings of ACL08: HLT, pages 771–779, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David R Hardoon</author>
<author>S´andor Szedm´ak</author>
<author>John ShaweTaylor</author>
</authors>
<title>Canonical correlation analysis: An overview with application to learning methods.</title>
<date>2004</date>
<journal>Neural Computation,</journal>
<volume>16</volume>
<issue>12</issue>
<marker>Hardoon, Szedm´ak, ShaweTaylor, 2004</marker>
<rawString>David R. Hardoon, S´andor Szedm´ak, and John ShaweTaylor. 2004. Canonical correlation analysis: An overview with application to learning methods. Neural Computation, 16(12):2639–2664.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Jin</author>
<author>Nick Koudas</author>
<author>Chen Li</author>
<author>Anthony K H Tung</author>
</authors>
<title>Indexing mixed types for approximate retrieval.</title>
<date>2005</date>
<booktitle>In VLDB,</booktitle>
<pages>793--804</pages>
<contexts>
<context position="10299" citStr="Jin et al., 2005" startWordPosition="1578" endWordPosition="1581">ng with cross-language information retrieval to mine named entity transliterations from large comparable corpora (Udupa et al., 2009b). However, none of these transliteration similarity models are applicable for searching very large name databases as they rely on brute-force search. Not surprisingly, (Pasternack and Roth, 2009) report that “.. testing [727 single word English names] with fifty thousand [Russian] candidates is a large computational hurdle (it takes our model about seven hours)”. Several algorithms for string similarity search have been proposed and applied to various problems (Jin et al., 2005). None of them are directly applicable to cross-language name search as they are based on the assumption that the query string shares the same alphabet as the database strings. Machine Transliteration has been studied extensively in the context of Machine Translation and Cross-Language Information Retrieval (Knight and Graehl, 1998), (Virga and Khudanpur, 2003), (Kuo et al., 2006), (Sherif and Kondrak, 2007), (Ravi and Knight, 2009), (Li et al., 2009), (Khapra and Bhattacharyya, 2009). However, Machine Transliteration followed by string similarity search gives less-thansatisfactory solution fo</context>
</contexts>
<marker>Jin, Koudas, Li, Tung, 2005</marker>
<rawString>Liang Jin, Nick Koudas, Chen Li, and Anthony K. H. Tung. 2005. Indexing mixed types for approximate retrieval. In VLDB, pages 793–804.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitesh Khapra</author>
<author>Pushpak Bhattacharyya</author>
</authors>
<title>Improving transliteration accuracy using word-origin detection and lexicon lookup.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration (NEWS 2009). Association for Computational Linguistics.</booktitle>
<contexts>
<context position="10788" citStr="Khapra and Bhattacharyya, 2009" startWordPosition="1653" endWordPosition="1657">del about seven hours)”. Several algorithms for string similarity search have been proposed and applied to various problems (Jin et al., 2005). None of them are directly applicable to cross-language name search as they are based on the assumption that the query string shares the same alphabet as the database strings. Machine Transliteration has been studied extensively in the context of Machine Translation and Cross-Language Information Retrieval (Knight and Graehl, 1998), (Virga and Khudanpur, 2003), (Kuo et al., 2006), (Sherif and Kondrak, 2007), (Ravi and Knight, 2009), (Li et al., 2009), (Khapra and Bhattacharyya, 2009). However, Machine Transliteration followed by string similarity search gives less-thansatisfactory solution for the cross-language name search problem as we will see later in Section 4. CCA was introduced by Hotelling in 1936 and has 494 been applied to various problems including CLIR, Text Clustering, and Image Retrieval (Hardoon et al., 2004). Recently, CCA has gained importance in the Machine Learning community as a technique for multi-view learning. CCA computes a common semantic feature space for two-view data and allows users to query a database using either of the two views. CCA has be</context>
<context position="25480" citStr="Khapra and Bhattacharyya, 2009" startWordPosition="4163" endWordPosition="4166">r, projected it to the common feature space, and found the k-nearest neighbors using the ANN algorithm. We used k=100 for all our experiments. After finding the nearest neighbors and the corresponding similarity scores, we combined the scores using the approach described in Section 3.4. 4.3 TRANS-SEARCH The training and search procedure employed by TRANS-SEARCH are described below. 498 Figure 4: Top scoring English Wikipedia page retrieved by GEOM-SEARCH 4.3.1 Transliteration Training We used a state-of-the-art CRF-based transliteration technique for transliterating the native language names (Khapra and Bhattacharyya, 2009). We used CRF++, an open-source CRF training tool, to train the transliteration system. We used exactly the same features and parameter settings as described in (Khapra and Bhattacharyya, 2009). As in the case of CCA, we use around 15, 000 single word name pairs in the training. 4.3.2 Search The preprocessing step for TRANS-SEARCH is the same as that for GEOM-SEARCH. We transliterated each constituent of the query into English and find all single-word English names that are at an edit distance of at most 3. We computed the similarity score as described in Section 3.4. 4.4 Evaluation We evaluat</context>
</contexts>
<marker>Khapra, Bhattacharyya, 2009</marker>
<rawString>Mitesh Khapra and Pushpak Bhattacharyya. 2009. Improving transliteration accuracy using word-origin detection and lexicon lookup. In Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration (NEWS 2009). Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandre Klementiev</author>
<author>Dan Roth</author>
</authors>
<title>Named entity transliteration and discovery from multilingual comparable corpora.</title>
<date>2006</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="9295" citStr="Klementiev and Roth, 2006" startWordPosition="1434" endWordPosition="1437">le and efficient algorithm for computing the similarity scores of multiword names from the single-word similarity scores (Section 3.4). 5. We show experimentally that our approach significantly improves the multilingual experience of ESL/EFL users (Section 4). 2 Related Work Although approximate similarity search is wellstudied, we are not aware of any non-trivial crosslanguage name search algorithm in the literature. However, several techniques for mining name transliterations from monolingual and comparable corpora have been studied (Pasternack and Roth, 2009), (Goldwasser and Roth, 2008), (Klementiev and Roth, 2006), (Sproat et al., 2006), (Udupa et al., 2009b). These techniques employ various transliteration similarity models. Character unigrams and bigrams were used as features to learn a discriminative transliteration model and time series similarity was combined with the transliteration similarity model (Klementiev and Roth, 2006). A generative transliteration model was proposed and used along with cross-language information retrieval to mine named entity transliterations from large comparable corpora (Udupa et al., 2009b). However, none of these transliteration similarity models are applicable for s</context>
</contexts>
<marker>Klementiev, Roth, 2006</marker>
<rawString>Alexandre Klementiev and Dan Roth. 2006. Named entity transliteration and discovery from multilingual comparable corpora. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Jonathan Graehl</author>
</authors>
<date>1998</date>
<booktitle>Machine transliteration. Computational Linguistics,</booktitle>
<volume>24</volume>
<issue>4</issue>
<pages>612</pages>
<contexts>
<context position="10633" citStr="Knight and Graehl, 1998" startWordPosition="1629" endWordPosition="1632">9) report that “.. testing [727 single word English names] with fifty thousand [Russian] candidates is a large computational hurdle (it takes our model about seven hours)”. Several algorithms for string similarity search have been proposed and applied to various problems (Jin et al., 2005). None of them are directly applicable to cross-language name search as they are based on the assumption that the query string shares the same alphabet as the database strings. Machine Transliteration has been studied extensively in the context of Machine Translation and Cross-Language Information Retrieval (Knight and Graehl, 1998), (Virga and Khudanpur, 2003), (Kuo et al., 2006), (Sherif and Kondrak, 2007), (Ravi and Knight, 2009), (Li et al., 2009), (Khapra and Bhattacharyya, 2009). However, Machine Transliteration followed by string similarity search gives less-thansatisfactory solution for the cross-language name search problem as we will see later in Section 4. CCA was introduced by Hotelling in 1936 and has 494 been applied to various problems including CLIR, Text Clustering, and Image Retrieval (Hardoon et al., 2004). Recently, CCA has gained importance in the Machine Learning community as a technique for multi-v</context>
</contexts>
<marker>Knight, Graehl, 1998</marker>
<rawString>Kevin Knight and Jonathan Graehl. 1998. Machine transliteration. Computational Linguistics, 24(4):599– 612.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin-Shea Kuo</author>
<author>Haizhou Li</author>
<author>Ying-Kuei Yang</author>
</authors>
<title>Learning transliteration lexicons from the web. In ACL.</title>
<date>2006</date>
<contexts>
<context position="10682" citStr="Kuo et al., 2006" startWordPosition="1637" endWordPosition="1640">s] with fifty thousand [Russian] candidates is a large computational hurdle (it takes our model about seven hours)”. Several algorithms for string similarity search have been proposed and applied to various problems (Jin et al., 2005). None of them are directly applicable to cross-language name search as they are based on the assumption that the query string shares the same alphabet as the database strings. Machine Transliteration has been studied extensively in the context of Machine Translation and Cross-Language Information Retrieval (Knight and Graehl, 1998), (Virga and Khudanpur, 2003), (Kuo et al., 2006), (Sherif and Kondrak, 2007), (Ravi and Knight, 2009), (Li et al., 2009), (Khapra and Bhattacharyya, 2009). However, Machine Transliteration followed by string similarity search gives less-thansatisfactory solution for the cross-language name search problem as we will see later in Section 4. CCA was introduced by Hotelling in 1936 and has 494 been applied to various problems including CLIR, Text Clustering, and Image Retrieval (Hardoon et al., 2004). Recently, CCA has gained importance in the Machine Learning community as a technique for multi-view learning. CCA computes a common semantic feat</context>
</contexts>
<marker>Kuo, Li, Yang, 2006</marker>
<rawString>Jin-Shea Kuo, Haizhou Li, and Ying-Kuei Yang. 2006. Learning transliteration lexicons from the web. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In</title>
<date>2001</date>
<booktitle>ICML,</booktitle>
<pages>282--289</pages>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In ICML, pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haizhou Li</author>
<author>A Kumaran</author>
<author>Vladimir Pervouchine</author>
<author>Min Zhang</author>
</authors>
<title>Report of news 2009 machine transliteration shared task.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration (NEWS 2009). Association for Computational Linguistics.</booktitle>
<contexts>
<context position="10754" citStr="Li et al., 2009" startWordPosition="1649" endWordPosition="1652">le (it takes our model about seven hours)”. Several algorithms for string similarity search have been proposed and applied to various problems (Jin et al., 2005). None of them are directly applicable to cross-language name search as they are based on the assumption that the query string shares the same alphabet as the database strings. Machine Transliteration has been studied extensively in the context of Machine Translation and Cross-Language Information Retrieval (Knight and Graehl, 1998), (Virga and Khudanpur, 2003), (Kuo et al., 2006), (Sherif and Kondrak, 2007), (Ravi and Knight, 2009), (Li et al., 2009), (Khapra and Bhattacharyya, 2009). However, Machine Transliteration followed by string similarity search gives less-thansatisfactory solution for the cross-language name search problem as we will see later in Section 4. CCA was introduced by Hotelling in 1936 and has 494 been applied to various problems including CLIR, Text Clustering, and Image Retrieval (Hardoon et al., 2004). Recently, CCA has gained importance in the Machine Learning community as a technique for multi-view learning. CCA computes a common semantic feature space for two-view data and allows users to query a database using e</context>
</contexts>
<marker>Li, Kumaran, Pervouchine, Zhang, 2009</marker>
<rawString>Haizhou Li, A Kumaran, Vladimir Pervouchine, and Min Zhang. 2009. Report of news 2009 machine transliteration shared task. In Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration (NEWS 2009). Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank McSherry</author>
<author>Marc Najork</author>
</authors>
<title>Computing information retrieval performance measures efficiently in the presence of tied scores.</title>
<date>2008</date>
<booktitle>In ECIR,</booktitle>
<pages>414--421</pages>
<contexts>
<context position="23146" citStr="McSherry and Najork, 2008" startWordPosition="3791" endWordPosition="3794">ARCH with a reasonable baseline, we implemented the following baseline: We used a state-ofthe art Machine Transliteration system to generate the best transliteration of each of the queries. We used the edit distance between the transliteration and the single-word English name as the similarity score. We combined single word similarities using the approach described in Section 3.4. We refer to this baseline by TRANS-SEARCH. Note that several English Wikipedia names sometimes get the same score for a query. Therefore, we used a tie-aware mean-reciprocal rank measure to evaluate the performance (McSherry and Najork, 2008). 4.2 GEOM-SEARCH The training and search procedure employed by GEOM-SEARCH are described below. 4.2.1 CCA Training We learnt the linear transformations A and B that project the language/script specific feature vectors to the common feature space using the approach discussed in Section 3.2. The learning algorithm requires a training set consisting of pairs of singleword names in English and the respective native language. We used approximately 15, 000 name pairs for each native language. A key parameter in CCA training is the number of dimensions of the common feature space. We found the optim</context>
</contexts>
<marker>McSherry, Najork, 2008</marker>
<rawString>Frank McSherry and Marc Najork. 2008. Computing information retrieval performance measures efficiently in the presence of tied scores. In ECIR, pages 414– 421.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Pasternack</author>
<author>Dan Roth</author>
</authors>
<title>Learning better transliterations.</title>
<date>2009</date>
<booktitle>In CIKM,</booktitle>
<pages>177--186</pages>
<contexts>
<context position="9237" citStr="Pasternack and Roth, 2009" startWordPosition="1426" endWordPosition="1429"> (Arya et al., 1998) (Section 3.3). 4. We introduce a simple and efficient algorithm for computing the similarity scores of multiword names from the single-word similarity scores (Section 3.4). 5. We show experimentally that our approach significantly improves the multilingual experience of ESL/EFL users (Section 4). 2 Related Work Although approximate similarity search is wellstudied, we are not aware of any non-trivial crosslanguage name search algorithm in the literature. However, several techniques for mining name transliterations from monolingual and comparable corpora have been studied (Pasternack and Roth, 2009), (Goldwasser and Roth, 2008), (Klementiev and Roth, 2006), (Sproat et al., 2006), (Udupa et al., 2009b). These techniques employ various transliteration similarity models. Character unigrams and bigrams were used as features to learn a discriminative transliteration model and time series similarity was combined with the transliteration similarity model (Klementiev and Roth, 2006). A generative transliteration model was proposed and used along with cross-language information retrieval to mine named entity transliterations from large comparable corpora (Udupa et al., 2009b). However, none of th</context>
</contexts>
<marker>Pasternack, Roth, 2009</marker>
<rawString>Jeff Pasternack and Dan Roth. 2009. Learning better transliterations. In CIKM, pages 177–186.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujith Ravi</author>
<author>Kevin Knight</author>
</authors>
<title>Learning phoneme mappings for transliteration without parallel data.</title>
<date>2009</date>
<booktitle>In NAACL-HLT.</booktitle>
<contexts>
<context position="10735" citStr="Ravi and Knight, 2009" startWordPosition="1645" endWordPosition="1648"> large computational hurdle (it takes our model about seven hours)”. Several algorithms for string similarity search have been proposed and applied to various problems (Jin et al., 2005). None of them are directly applicable to cross-language name search as they are based on the assumption that the query string shares the same alphabet as the database strings. Machine Transliteration has been studied extensively in the context of Machine Translation and Cross-Language Information Retrieval (Knight and Graehl, 1998), (Virga and Khudanpur, 2003), (Kuo et al., 2006), (Sherif and Kondrak, 2007), (Ravi and Knight, 2009), (Li et al., 2009), (Khapra and Bhattacharyya, 2009). However, Machine Transliteration followed by string similarity search gives less-thansatisfactory solution for the cross-language name search problem as we will see later in Section 4. CCA was introduced by Hotelling in 1936 and has 494 been applied to various problems including CLIR, Text Clustering, and Image Retrieval (Hardoon et al., 2004). Recently, CCA has gained importance in the Machine Learning community as a technique for multi-view learning. CCA computes a common semantic feature space for two-view data and allows users to query</context>
</contexts>
<marker>Ravi, Knight, 2009</marker>
<rawString>Sujith Ravi and Kevin Knight. 2009. Learning phoneme mappings for transliteration without parallel data. In NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hanan Samet</author>
</authors>
<date>2006</date>
<booktitle>Foundations of Multidimensional and Metric Data Structures (The Morgan Kaufmann Series in Computer Graphics).</booktitle>
<publisher>Morgan Kaufmann,</publisher>
<marker>Samet, 2006</marker>
<rawString>Hanan Samet. 2006. Foundations of Multidimensional and Metric Data Structures (The Morgan Kaufmann Series in Computer Graphics). Morgan Kaufmann, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tarek Sherif</author>
<author>Grzegorz Kondrak</author>
</authors>
<title>Substringbased transliteration.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="10710" citStr="Sherif and Kondrak, 2007" startWordPosition="1641" endWordPosition="1644">nd [Russian] candidates is a large computational hurdle (it takes our model about seven hours)”. Several algorithms for string similarity search have been proposed and applied to various problems (Jin et al., 2005). None of them are directly applicable to cross-language name search as they are based on the assumption that the query string shares the same alphabet as the database strings. Machine Transliteration has been studied extensively in the context of Machine Translation and Cross-Language Information Retrieval (Knight and Graehl, 1998), (Virga and Khudanpur, 2003), (Kuo et al., 2006), (Sherif and Kondrak, 2007), (Ravi and Knight, 2009), (Li et al., 2009), (Khapra and Bhattacharyya, 2009). However, Machine Transliteration followed by string similarity search gives less-thansatisfactory solution for the cross-language name search problem as we will see later in Section 4. CCA was introduced by Hotelling in 1936 and has 494 been applied to various problems including CLIR, Text Clustering, and Image Retrieval (Hardoon et al., 2004). Recently, CCA has gained importance in the Machine Learning community as a technique for multi-view learning. CCA computes a common semantic feature space for two-view data </context>
</contexts>
<marker>Sherif, Kondrak, 2007</marker>
<rawString>Tarek Sherif and Grzegorz Kondrak. 2007. Substringbased transliteration. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sproat</author>
<author>Tao Tao</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Named entity transliteration with comparable corpora.</title>
<date>2006</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="9318" citStr="Sproat et al., 2006" startWordPosition="1438" endWordPosition="1441">r computing the similarity scores of multiword names from the single-word similarity scores (Section 3.4). 5. We show experimentally that our approach significantly improves the multilingual experience of ESL/EFL users (Section 4). 2 Related Work Although approximate similarity search is wellstudied, we are not aware of any non-trivial crosslanguage name search algorithm in the literature. However, several techniques for mining name transliterations from monolingual and comparable corpora have been studied (Pasternack and Roth, 2009), (Goldwasser and Roth, 2008), (Klementiev and Roth, 2006), (Sproat et al., 2006), (Udupa et al., 2009b). These techniques employ various transliteration similarity models. Character unigrams and bigrams were used as features to learn a discriminative transliteration model and time series similarity was combined with the transliteration similarity model (Klementiev and Roth, 2006). A generative transliteration model was proposed and used along with cross-language information retrieval to mine named entity transliterations from large comparable corpora (Udupa et al., 2009b). However, none of these transliteration similarity models are applicable for searching very large nam</context>
</contexts>
<marker>Sproat, Tao, Zhai, 2006</marker>
<rawString>Richard Sproat, Tao Tao, and ChengXiang Zhai. 2006. Named entity transliteration with comparable corpora. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raghavendra Udupa</author>
<author>K Saravanan</author>
<author>Anton Bakalov</author>
<author>Abhijit Bhole</author>
</authors>
<title>they are out there, if you know where to look”: Mining transliterations of oov query terms for cross-language information retrieval.</title>
<date>2009</date>
<booktitle>In ECIR,</booktitle>
<pages>437--448</pages>
<contexts>
<context position="6077" citStr="Udupa et al., 2009" startWordPosition="930" endWordPosition="933">using Stephen Hawking Stefan Hoking Stefan Ho king Stephen Hawking Pol Crugman Poll Krugman Paul Krugman Paal Kragaman Paul Krugman Paul Krugman Suburaamaniya Ba- Subramaniya Subrahmaniya haarachi Baracchi Bharati search in Wikipedia. • Firstly, name queries are expressed by ESL/EFL users in the native languages using the orthography of those languages. Transliterating the name into Latin script using a Machine Transliteration system is an option but state-of-the-art Machine Transliteration technologies are still far away from producing the correct transliteration. Further, as pointed out by (Udupa et al., 2009a), it is not enough if a Machine Transliteration system generates a correct transliteration; it must produce the transliteration that is present in the Wikipedia title. • Secondly, there are about 6 million titles (including redirects) in English Wikipedia which rules out the naive approach of comparing the query with every one of the English Wikipedia titles for transliteration equivalence as is done typically in transliteration mining tasks. A practical cross-language name search system for Wikipedia must be able to search millions of Wikipedia titles in a fraction of a second and return th</context>
<context position="9339" citStr="Udupa et al., 2009" startWordPosition="1442" endWordPosition="1445">ity scores of multiword names from the single-word similarity scores (Section 3.4). 5. We show experimentally that our approach significantly improves the multilingual experience of ESL/EFL users (Section 4). 2 Related Work Although approximate similarity search is wellstudied, we are not aware of any non-trivial crosslanguage name search algorithm in the literature. However, several techniques for mining name transliterations from monolingual and comparable corpora have been studied (Pasternack and Roth, 2009), (Goldwasser and Roth, 2008), (Klementiev and Roth, 2006), (Sproat et al., 2006), (Udupa et al., 2009b). These techniques employ various transliteration similarity models. Character unigrams and bigrams were used as features to learn a discriminative transliteration model and time series similarity was combined with the transliteration similarity model (Klementiev and Roth, 2006). A generative transliteration model was proposed and used along with cross-language information retrieval to mine named entity transliterations from large comparable corpora (Udupa et al., 2009b). However, none of these transliteration similarity models are applicable for searching very large name databases as they r</context>
</contexts>
<marker>Udupa, Saravanan, Bakalov, Bhole, 2009</marker>
<rawString>Raghavendra Udupa, K. Saravanan, Anton Bakalov, and Abhijit Bhole. 2009a. “they are out there, if you know where to look”: Mining transliterations of oov query terms for cross-language information retrieval. In ECIR, pages 437–448.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raghavendra Udupa</author>
<author>K Saravanan</author>
<author>A Kumaran</author>
<author>Jagadeesh Jagarlamudi</author>
</authors>
<title>Mint: A method for effective and scalable mining of named entity transliterations from large comparable corpora.</title>
<date>2009</date>
<booktitle>In EACL,</booktitle>
<pages>799--807</pages>
<contexts>
<context position="6077" citStr="Udupa et al., 2009" startWordPosition="930" endWordPosition="933">using Stephen Hawking Stefan Hoking Stefan Ho king Stephen Hawking Pol Crugman Poll Krugman Paul Krugman Paal Kragaman Paul Krugman Paul Krugman Suburaamaniya Ba- Subramaniya Subrahmaniya haarachi Baracchi Bharati search in Wikipedia. • Firstly, name queries are expressed by ESL/EFL users in the native languages using the orthography of those languages. Transliterating the name into Latin script using a Machine Transliteration system is an option but state-of-the-art Machine Transliteration technologies are still far away from producing the correct transliteration. Further, as pointed out by (Udupa et al., 2009a), it is not enough if a Machine Transliteration system generates a correct transliteration; it must produce the transliteration that is present in the Wikipedia title. • Secondly, there are about 6 million titles (including redirects) in English Wikipedia which rules out the naive approach of comparing the query with every one of the English Wikipedia titles for transliteration equivalence as is done typically in transliteration mining tasks. A practical cross-language name search system for Wikipedia must be able to search millions of Wikipedia titles in a fraction of a second and return th</context>
<context position="9339" citStr="Udupa et al., 2009" startWordPosition="1442" endWordPosition="1445">ity scores of multiword names from the single-word similarity scores (Section 3.4). 5. We show experimentally that our approach significantly improves the multilingual experience of ESL/EFL users (Section 4). 2 Related Work Although approximate similarity search is wellstudied, we are not aware of any non-trivial crosslanguage name search algorithm in the literature. However, several techniques for mining name transliterations from monolingual and comparable corpora have been studied (Pasternack and Roth, 2009), (Goldwasser and Roth, 2008), (Klementiev and Roth, 2006), (Sproat et al., 2006), (Udupa et al., 2009b). These techniques employ various transliteration similarity models. Character unigrams and bigrams were used as features to learn a discriminative transliteration model and time series similarity was combined with the transliteration similarity model (Klementiev and Roth, 2006). A generative transliteration model was proposed and used along with cross-language information retrieval to mine named entity transliterations from large comparable corpora (Udupa et al., 2009b). However, none of these transliteration similarity models are applicable for searching very large name databases as they r</context>
</contexts>
<marker>Udupa, Saravanan, Kumaran, Jagarlamudi, 2009</marker>
<rawString>Raghavendra Udupa, K. Saravanan, A. Kumaran, and Jagadeesh Jagarlamudi. 2009b. Mint: A method for effective and scalable mining of named entity transliterations from large comparable corpora. In EACL, pages 799–807.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paola Virga</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Transliteration of proper names in cross-language applications.</title>
<date>2003</date>
<booktitle>In SIGIR,</booktitle>
<pages>365--366</pages>
<contexts>
<context position="10662" citStr="Virga and Khudanpur, 2003" startWordPosition="1633" endWordPosition="1636">[727 single word English names] with fifty thousand [Russian] candidates is a large computational hurdle (it takes our model about seven hours)”. Several algorithms for string similarity search have been proposed and applied to various problems (Jin et al., 2005). None of them are directly applicable to cross-language name search as they are based on the assumption that the query string shares the same alphabet as the database strings. Machine Transliteration has been studied extensively in the context of Machine Translation and Cross-Language Information Retrieval (Knight and Graehl, 1998), (Virga and Khudanpur, 2003), (Kuo et al., 2006), (Sherif and Kondrak, 2007), (Ravi and Knight, 2009), (Li et al., 2009), (Khapra and Bhattacharyya, 2009). However, Machine Transliteration followed by string similarity search gives less-thansatisfactory solution for the cross-language name search problem as we will see later in Section 4. CCA was introduced by Hotelling in 1936 and has 494 been applied to various problems including CLIR, Text Clustering, and Image Retrieval (Hardoon et al., 2004). Recently, CCA has gained importance in the Machine Learning community as a technique for multi-view learning. CCA computes a </context>
</contexts>
<marker>Virga, Khudanpur, 2003</marker>
<rawString>Paola Virga and Sanjeev Khudanpur. 2003. Transliteration of proper names in cross-language applications. In SIGIR, pages 365–366.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>