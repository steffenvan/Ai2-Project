<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.92986">
Influences and Inferences
</title>
<author confidence="0.998803">
Jerry R. Hobbs*
</author>
<affiliation confidence="0.986258333333333">
Information Sciences Institute
University of Southern California
Marina del Rey, California
</affiliation>
<sectionHeader confidence="0.704371" genericHeader="abstract">
1. False and True Starts
</sectionHeader>
<bodyText confidence="0.999670151515151">
I am deeply honored to receive the ACL’s Lifetime Achievement Award. I’m especially
honored when I look back at the list of previous winners—Chuck Fillmore, Eugene
Charniak, Eva Hajiˇcov´a, Fred Jelinek, Martin Kay, Aravind Joshi, and the others—
they’re all my heroes.
I was of course delighted to learn of this award. The most we can hope for in life is
to take part in the conversation, and an award like this means that you’ve taken part in
the conversation.
It seems to be a tradition to begin with a few formative anecdotes from childhood.
For me, it all begins before I was born. My grandfather, a crusty old country lawyer in
southern Indiana, told my father not to bother trying to go to law school. “You don’t
know English grammar,” he said. “You’ll flunk out.” My dad accepted the challenge,
bought a book entitled English Grammar, by Smith, Magee, and Seward (1928), and
mastered it. He went on to become a very successful lawyer.
Fast-forward to when I was in junior high school. My dad was distressed that my
English classes looked to him more like social studies, and barely touched on grammar.
So he persuaded me—actually, he probably bribed me, but I can’t remember what
with—to master that same book, English Grammar by Smith, Magee, and Seward. This
was a concession, because I was a math nerd, reading only textbooks on trigonometry
and calculus, as my way of avoiding the humiliation of playing baseball. But I read the
book, and I was amazed. English grammar was just like math! It had the same sorts of
rules, the same kinds of abstractions, the same types of puzzles. It was actually fun!
In my junior or senior year of high school we had to take something called the
Kuder Preference Test, which would help us decide what career to choose. I scored high
in math and in language. So my high school counselor told me I should write math
books. In fact, she got it exactly backwards. It wasn’t that I should do language about
math. It was that I should do math about language.
I’ve met any number of computational linguists with a similar story. They grew
up not knowing whether they wanted to be a physicist or a poet. They just knew both
sounded fascinating. Then they discovered our field.
My last near miss happened the week I was drafted into the Army. They gave us a
battery of aptitude tests to see what specialties we’d be best for. One of the tests was to
see if we should be sent to the Monterey Language School. Looking back on it, I realize
now it was testing how well you could understand formal language theory. They’d give
</bodyText>
<footnote confidence="0.576693">
* USC/ISI, 4676 Admiralty Way, Marina del Rey, CA 90292, USA. E-mail: hobbs@isi.edu.
doi:10.1162/COLI a 00171
</footnote>
<note confidence="0.834469">
© 2013 Association for Computational Linguistics
Computational Linguistics Volume 39, Number 4
</note>
<bodyText confidence="0.99976988">
you a bunch of rules for an artificial language, and you’d have to say whether different
strings were in or not in the language. I’d never seen anything like it, but it was really
fun to do. Later I met with a personnel specialist who went over my test scores. I got a 46
out of 50. He ignored that until I pointed it out to him. Then he said, “That’s a mistake.
Nobody ever gets more than 6 or 7 points on that test.”
I said, “No, I think it might be correct.”
He said, “It doesn’t matter. You’re not going to the Monterey Language School.
You’re going to South Vietnam.”
Actually, I didn’t go to the Monterey Language School or to South Vietnam. I spent
two years in South Carolina, and was glad to be there. How I managed that is a story
for another occasion.
So I didn’t really discover computational linguistics until my third year in grad-
uate school at New York University. In October I passed my oral exam in topics like
algebraic topology and complex analysis, by one generous yes and two abstensions. In
the subsequent months I discovered more and more facts about myself—for example,
that I was never going to figure out a faster way of multiplying matrices, and that
fascinating though recursion theory might be, I was never going to prove a theorem
that Hartley Rogers would be compelled to include in his next edition. As I surveyed
vaguely plausible fields, I realized I had no idea what the next problem to solve would
be or even what makes a problem interesting.
Then in April, when I had nearly resigned myself to becoming a taxi driver, I discov-
ered New York University’s best-kept secret: Naomi Sager’s Linguistic String Project. I
think it is also computational linguistics’ best kept secret as well. She was motivated by
the science, not by the performance, and her very impressive work is nowhere near as
well-known as it should be. I think her Linguistic String Grammar (Sager 1981) ranks,
as a computational specification of English syntax, with Pollard and Sag’s Head-driven
Phrase Structure Grammar (1994), for thoroughness, insight, and elegance. So, for exam-
ple, in 1992 when we developed the FASTUS system for information extraction using
cascaded finite-state transducers (Hobbs et al. 1997), it was straightforward to copy the
rules for Noun Groups straight from her grammar. It’s no accident that in the late 1980s
during the Strategic Computing Initiative and in the early 1990s in the Message Under-
standing Conferences, three of the most important efforts were led by Linguistic String
Project alumni—Ralph Grishman’s group at New York University, Lynette Hirschman’s
at Unisys, and my group at SRI International. I think the most important lesson I learned
from Naomi Sager was to look closely at the data and to take it seriously.
My other thesis advisor was Jack Schwartz. He was a polymath, so to speak. I
took a course in logic from him. I knew about his book on compilers and the classic
Dunford and Schwartz on functional analysis. But when I saw his book on mathematical
economics and his book on the theory of relativity, I did some research to see if there
was more than one Jack Schwartz. Among his writings was an unpublished Chapter 9
of his compilers book, on parsing natural language, which I of course read.
My thesis was on Earley’s algorithm applied to natural language. It quickly became
apparent that the constraints on phrase structure rules had to be expressed and that
one could do that with fairly simple operations on vectors of features, where among
the features were what I called the “cores” of the constituents, since they bundled many
of the relevant features. My “core” was what linguists came to call “head.” Years later,
I ran across Chapter 9 again and reread it, and realized that all the ideas in my thesis
were there. So when in 1987 Schwartz told someone that I had anticipated head-driven
phrase structure grammar, that was his way of saying he had anticipated head-driven
phrase structure grammar.
</bodyText>
<page confidence="0.981861">
782
</page>
<note confidence="0.800739">
Hobbs Influences and Inferences
</note>
<bodyText confidence="0.999465625">
My first job was at Yale University as a very temporary instructor—I think the
position is now called “post-doc.” Over the course of the year I became convinced
that syntax was a solved problem—something I still believe. But that left me adrift for
problems to work on. I became discouraged, and found myself thinking again about
driving that taxi. Then late one afternoon, just as I was about to go home, a graduate
student named Fred Howard came into my office to ask a couple of questions. That
triggered a discussion that lasted until 11 o’clock that evening. One of the wheels we
reinvented was a recognition of the pervasiveness of spatial metaphor in discourse.
(This was before Lakoff and Johnson [1980], but after similar observations by the
18th-century Italian philosopher Giambattista Vico (1968 [1744]) and the 20th-century
English literary critic I. A. Richards [1936].) But within a year, everything else of value
that remained of the content of that discussion could be compressed into a long footnote
in a technical report. In any case, this conversation lit a fire that fueled my research for
the next 15 or 20 years.
In particular, I began looking at texts, trying to understand how we understand
them. No doubt influenced by Chuck Rieger’s thesis (Rieger 1974), I asked what infer-
ences we draw in the course of comprehension, and, an issue Rieger did not address,
what inferences we do not draw. This culminated in 1976 in an unreadable (and unread)
technical report (Hobbs 1976), microanalyzing one paragraph from Newsweek, trying to
specify every bit of knowledge required for understanding the text and describing how
every linguistic problem in the text invokes that knowledge to arrive at solutions. One
could say that the rest of my career has been a matter of cleaning up and extending that
technical report, in terms of representation, the process of inference and interpretation,
and the specification of common-sense knowledge.
</bodyText>
<sectionHeader confidence="0.989928" genericHeader="keywords">
2. Representation
</sectionHeader>
<bodyText confidence="0.9939612">
In 1977 I moved to SRI, where I fell under the influence of Nils Nilsson and Bob Moore,
and of John McCarthy at nearby Stanford. They were campaigning to replace the ad hoc
styles of representation of early AI with representations based on first-order logic. But
the problem in a nutshell is this: When we are trying to represent an English sentence
like Pat believes Chris is tall, we really want to write
</bodyText>
<equation confidence="0.899735">
(1) believe(Pat, tall(Chris))
</equation>
<bodyText confidence="0.715355714285714">
The difficulty is that tall is a predicate and tall(Chris) evaluates to true or false, so we
are left with Pat believes a truth value, with not a hint of Chris’s tallness to be found. A
common solution to this is to treat believe not as a predicate but as an opaque operator
that blocks evaluation of its operands.
Many special logics have been developed for such operators. For example, knowing
about modal and temporal logics, Russell’s iota operator, functionals, lambda expres-
sions, and so on, we might represent the sentence
</bodyText>
<listItem confidence="0.999535666666667">
(2) Maybe the boy wanted to build a boat quickly.
by the expression
(3) (ιx : BOY)[oPAST[WANT(x,λz[(∃y : BOAT)Quick(build)(z,y)]]]
</listItem>
<bodyText confidence="0.818679">
This bothered me because it seemed like we were introducing a new operator with its
own special logic every time we encountered a new word to define or characterize. For
</bodyText>
<page confidence="0.993653">
783
</page>
<note confidence="0.259183">
Computational Linguistics Volume 39, Number 4
</note>
<bodyText confidence="0.673768">
20,000 words would we have to introduce 20,000 new operators? It seemed to me that
we should rather stay within first-order logic, abiding by two principles:
</bodyText>
<listItem confidence="0.969178555555556">
1. All morphemes are created equal.
2. Every morpheme conveys a predication.
We could achieve this kind of representation by means of reification. Thus, if
tall&apos;(e,Chris) says that eventuality e is the state or eventuality of Chris being tall, then
we can represent Pat believes Chris is tall by
(4) believe(Pat,e) n tall&apos;(e,Chris)
Sentence (2) is then represented
(5) maybe(e5) n the(x, e3) n boy&apos;(e3, x) n want&apos;(e4, x, e6) n Past&apos;(e5, e4)
nbuild&apos;(e6,x,y) n a(y,e8) n boat&apos;(e8,y) n slow(e6)
</listItem>
<bodyText confidence="0.999770117647059">
There’s nothing exotic here (other than reification). It’s all first-order logic, predicates
applied to arguments where the arguments are existentially quantified variables with
widest possible scope, ranging over a universe of possible individuals.
The extremes to which we go in identifying morphemes with predications can
be seen in the predication the(x,e3). What could that possibly mean? Well, ask what
information is being conveyed by the word the. It is a relation between an entity x and
a description e3, and it says the entity is uniquely mutually identifiable in context by
means of the description. We can give this relation a name. We could call it something
like uniquely-mutually-identifiable-in-context. But why not keep it simple, and name the
predicate after the morpheme that conveys it – the?
Knowledge representation schemes that use extensive reification are often called
“Davidsonian,” after the philosopher Donald Davidson (1967), who proposed reifying
events. But he balked at reifying states, let alone negations of states and events. He
would not have treated Chris’s tallness as a thing. By contrast, I adopted a position that,
because I was young and wild, I called “ontological promiscuity.” Now that I’m older
and more domesticated, I would probably call it something like “ontological prosperity”
or “ontological comfortable circumstances” or maybe “ontological glut.”
Many balk at such abandonment of ontological scruples. No doubt I was influenced
by the near solipsism that infected many researchers in the early days of AI. Our brains
could be fooling us, just as we often fool computers to test our programs. Yes, there is
probably a world out there that occasionally bites back. But the world is benevolent—
after all, we evolved in it. When we breathe, there is almost always oxygen there. That’s
no accident. So it doesn’t matter very much what we believe. We can believe all sorts of
crazy things and be completely ignorant of apparently real and pervasive phenomena.
Until the recent past we believed in the spirits of the dead, and we were entirely ignorant
of 98% of the electromagnetic spectrum. If you are willing to admit the existence of
physical objects, sets, numbers, and possible worlds, what ontological scruples do you
have anyway? So why should we give any credence at all to our intuitions about what
exists and what doesn’t? Why not simply stipulate that everything that can be talked
about exists in a Platonic universe of possible individuals, since that makes it so much
easier to represent and reason about the content of natural language discourse?
The result of this move and similar reifications to eliminate quantifier scopings
is that the logical form of a sentence is a flat conjunction of existentially quantified
propositions, with one predication per morpheme.
</bodyText>
<page confidence="0.993195">
784
</page>
<figure confidence="0.9653276">
Hobbs Influences and Inferences
But there is a problem. The sentence
(6) John is tall.
would be represented
(7) John&apos;(e1, x) n tall&apos;(e3, x)
whereas the sentence
(8) John is not tall.
would be represented
(9) John&apos;(e1,x) n not&apos;(e2,e3) n tall&apos;(e3,x)
But P n Q n R implies P n R, so it would seem that John is not tall implies John is tall.
</figure>
<construct confidence="0.877579176470588">
The wrinkle is that tall&apos;(e3, x) does not say that x is tall. It says that e3 is a possible
eventuality of x’s being tall. The eventuality e3 may or may not exist in the real world,
and if it does, that is one of its properties – Rexist(e3).
This means that we have to distinguish between the content of a sentence and its
claim. Sentences (6) and (8) have highly overlapping content. But the claim of sen-
tence (6) is e3, the tall-ness, while the claim of sentence (8) is e2, the negation of the
tall-ness.
The general procedure for deciding on whether or not an eventuality really exists is
as follows:
Step 1: Identify the claim.
Step 2: Propagate truth and falsity through implicatives.
Step 3: As a courtesy to the speaker, assume the other propositions are true. (But
note that in modal contexts there is an ambiguity in whether the grammatically
subordinated material holds in the real world [de re] or in the modal context [de
dicto].)
For example, in
(10) The lazy man did not manage to avoid attending the meeting.
</construct>
<bodyText confidence="0.883150222222222">
Step 1 says the claim is the “not.” Step 2 says that therefore “manage” is false, “avoid”
is false, and “attend” is true. Step 3 says that “lazy,” “man,” and “meeting” are all
true.
This kind of representation has the advantage of yielding a very elegant view
of compositional semantics. In traditional approaches to compositional semantics, the
meanings of constituents are lambda expressions, and composition happens by func-
tion application. With a flat logical form, the only role function application plays is
identifying variables with each other. This gives us a two-part account of compositional
semantics.
</bodyText>
<footnote confidence="0.71517775">
1. The lexicon provides predicate–argument relations.
2. Syntax identifies variables.
For the sentence
(11) The man attended the meeting.
</footnote>
<page confidence="0.986534">
785
</page>
<note confidence="0.292773">
Computational Linguistics Volume 39, Number 4
</note>
<bodyText confidence="0.9946906">
ignoring the and tense, we get from the individual words the propositions
(12) man&apos;(e1, x1), attend&apos;(e2, x2, y2), meeting&apos;(e3, y3)
When we recognize that attended the meeting is a verb phrase, this amounts to recogniz-
ing that y2 = y3. When we recognize the man attended the meeting as a clause, we have
recognized that x1 = x2.
</bodyText>
<sectionHeader confidence="0.989962" genericHeader="introduction">
3. Interpretation
</sectionHeader>
<bodyText confidence="0.999865363636364">
In 1979 and 1980, I had the huge good fortune to participate in a biweekly discussion
group on discourse, alternating between Stanford and Berkeley, consisting of some of
the most illustrious scholars of language in the world, including Mike Agar, Dwight
Bolinger, Eve and Herb Clark, Chuck Fillmore, Paul Kay, George Lakoff, Geoff Nunberg,
Ivan Sag, Dan Slobin, Elizabeth Traugott, and Tom Wasow. For me personally, the high
point in these meetings, and one of the high points in my entire career, was when the
sociologist Irving Goffman, visiting Berkeley at the time, used my paper “Conversation
as Planned Behavior” (Hobbs and Evans 1980) as a club to beat the sociolinguist John
Gumperz over the head with. Metaphorically speaking. We read and discussed mem-
bers’ papers on interpreting nominal compounds, metonymy or deferred reference,
de-nominalized nouns, metaphor, and other phenomena that came to be clustered by
linguists under the name of “Radical Pragmatics” (Cole 1981). (I thought a better name
would be “Run-of-the-mill AI”.)
Around this time, I was concerned with the problem of how we delimit the
set of inferences we draw as we understand a text. The answer that seemed most
promising was that we need to draw those inferences required to resolve interpre-
tation problems of the sort we were examining in the discussion group. But what
systematicity was there to this set of problems? How would you know if your list was
complete?
The scheme that made the most sense to me goes like this. A text conveys predi-
cations, that is, a predicate applied to one or more arguments – p(x). This gives rise to
three sorts of problems:
</bodyText>
<listItem confidence="0.9973538">
1. What is the predicate? What is p? This question subsumes the problems of
lexical ambiguity, the interpretation of vague predicates like prepositions
and have, and the interpretation of the implicit relation in nominal
compounds.
2. What is the argument? What is x? This question subsumes the problems of
coreference and syntactic ambiguity. (Recall that syntactic structure is a
matter of identifying variables in the right way.)
3. In what way are the predicate and argument congruent? What about p and
x would allow p to be true of x? This question subsumes the problems of
metaphor and metonymy.
</listItem>
<bodyText confidence="0.9998675">
This collection of problems I called “local pragmatics.” They are problems that are
presented within the scope of single sentences, but they often require for their solution
the entire discourse, the external context, and world knowledge. (My term never caught
on probably because no one else saw this class of problems as a natural kind.)
</bodyText>
<page confidence="0.961145">
786
</page>
<bodyText confidence="0.98762462745098">
Hobbs Influences and Inferences
Another issue I was thinking about during these years was the structure of dis-
course, in particular, that structure arising out of coherence relations between discourse
segments. In this I was very much influenced by the work of the linguists Joseph Grimes
(1975) and Robert Longacre (1976). I began collaborating with the anthropologist Mike
Agar around this time, and we called this level of structure “local coherence” (Agar and
Hobbs 1982).
In the mid-1970s Ray Perrault and Phil Cohen (Cohen and Perrault 1979) at the
University of Toronto, later to be my colleagues at SRI, and Chip Bruce (Bruce and
Newman 1978) at BBN were doing very exciting work analyzing the structure of
discourse as arising out of the speaker’s or writer’s plan, employing formalizations
of planning from artificial intelligence. In work with David Evans and work with
Mike Agar I tried to apply these insights to the complexities of ordinary conversa-
tion and to ethnographic interviews. Agar and I called this level of structure “global
coherence.”
All along in investigating all three of these problems—local pragmatics, local coher-
ence, and global coherence—it was clear that a key role was played by the notions of
implicature (Grice 1975), accommodation (Lewis 1979; Thomason 1985), and abduction
(Peirce 1955). To solve even elementary problems like pronoun coreference, one had to
make assumptions to get a good interpretation of the text, where the only justification
for the assumptions was that they led to a good interpretation.
In the fall of 1987 at SRI we organized a discussion group on abduction, reading
the classic papers by Peirce, recent attempts in AI to use abduction in, for exam-
ple, medical diagnosis (Pople 1973; Cox and Pietrzykowski 1986), and contemporary
philosophers like Paul Thagard (1978), as well as work by Wilensky and Norvig at
Berkeley (Wilensky 1983; Norvig 1987) and Charniak and Goldman at Brown (Charniak
and Goldman 1988) that seemed to be taking an approach similar to ours. Among the
people in our group were Mark Stickel, Doug Edwards, and the pragmatics scholar
Steve Levinson, who was visiting Stanford at the time. We argued about what we were
calling identity implicatures and referential implicatures, and about how to distinguish
new from given information in discourse, and how to choose the best interpretation
of a text.
Then late one afternoon in October 1987 Mark Stickel came into my office to say
that he thought he had the answer to all our problems. He described his algorithm for
weighted abduction. It struck me immediately as the double helix of computational
linguistics, a feeling that has not entirely abandoned me today. First of all, it gave us a
characterization of what constituted the interpretation of a stream of discourse. It gave
us a clear criterion for what inferences to draw and not draw. The interpretation was the
most economical explanation for what would make the text true, and an inference was
appropriate if and only if it contributed to that explanation.
On my way home that night, I began driving a little more carefully. In the next
few days, I saw how one would approach all the local pragmatics and local and global
coherence problems in this framework. In discussions with Stu Shieber in the next few
days it became apparent how one could integrate syntax smoothly into the framework.
A big picture emerged (Hobbs et al. 1993).
In the early 1990s I saw an advertisement in a magazine for Polaroid cameras
(quite obsolete now). It showed a man standing by the ocean, holding a camera, and
looking at a scene in which the branch of a tree is on the ground and a small boat
is stuck in the top of another tree. When we see this, we immediately interpret it
by coming up with the best explanation for the observables (abduction). There was
a storm that blew the branch down and blew the boat into the tree. There are other
</bodyText>
<page confidence="0.947306">
787
</page>
<note confidence="0.272066">
Computational Linguistics Volume 39, Number 4
</note>
<bodyText confidence="0.999244818181818">
possible explanations. Maybe someone chopped the branch down, and maybe the boat
was lifted into the tree with a crane. But this is not as good an interpretation because
we have to assume two things (the chopping and the crane) rather than just one (the
storm). The first interpretation is better because it is more economical. Less explains
more.
But this isn’t the end of the story. There is another observable to be explained. Why
is this picture in the magazine? The explanation is that it is an advertisement. That
means there was an ad agency involved in posing the picture, and they very well could
have done the chopping and used the crane, rather than wait for the rare event of a
storm to arrange the picture for them.
We could call the first explanation the “informational” one. It explains the content
of the picture, thereby explicating the information conveyed by the picture. We could
call the second explanation the “intentional” one. It explains why the message occurs
at all. Note that both interpretations need to be discovered if the advertisement is to be
fully appreciated.
The big picture that emerges is this (see Figure 1). The brain is an abduction
machine, continuously trying to prove abductively (i.e., by making necessary assump-
tions) that the observables in its environment constitute a coherent situation. (We can
encompass action as well as perception by adding to what is proved the proposition
that the owner of the brain will thrive in that situation.)
Sometimes among the observables is another agent’s utterance. What is to be ex-
plained is the proposition utter(i, u, w)—that is, a speaker i utters to a hearer u a string
</bodyText>
<figureCaption confidence="0.834933">
Figure 1
</figureCaption>
<bodyText confidence="0.731427">
Interpretation as abduction, the big picture.
</bodyText>
<page confidence="0.976414">
788
</page>
<note confidence="0.571169">
Hobbs Influences and Inferences
</note>
<bodyText confidence="0.982306">
of words w. Generally the best explanation for an utterance is that it is an intentional act
aimed at conveying information. We can capture this with the axiom
</bodyText>
<listItem confidence="0.763559">
(13) Segment(w, e) n goal(i, c) n cog&apos;(c, u, e) D utter(i, u, w)
</listItem>
<bodyText confidence="0.998932916666667">
That is, if w is an interpretable segment of discourse describing a situation e, and a
speaker i has the goal c that a hearer u adopt some cognitive stance toward e, then
(defeasibly) i will utter to u the string of words w. The first conjunct in the antecedent
is the entry point into the informational side of an interpretation: What is the content
of the message? The second two conjuncts are the entry point into the intentional side:
Why is the speaker conveying this content?
The reason that the speaker has this particular goal is usually that it plays some role
in, or is a subgoal of, a larger plan the speaker is executing in the world. This is where
that reasoning occurs. It encompasses what Agar and I called “global coherence”—how
does the utterance fit in with what else is going on in the world?
The next level of analysis happens when we decompose the segment of discourse
into smaller segments, using the axiom
</bodyText>
<equation confidence="0.5261865">
(14) Segment(w1, e1) n Segment(w2, e2) n CoRel(e1, e2, e)
D Segment(w1w2,e)
</equation>
<bodyText confidence="0.995045275862069">
This axiom says that if w1 is a segment describing situation e1, and w2 is a segment
describing situation e2, and there is a relation between e1 and e2, then the concatenation
is a segment describing a situation e somehow derivable from the relation. When we
backchain on this axiom, we are explaining an interpretable segment of discourse by
breaking it into parts, explaining the parts, and explaining the relation between them.
The possible coherence relations are just the sort of relations that frequently obtain
between two states or events: causality, similarity, identity, a strong sort of temporal suc-
cession I have called “occasion,” the figure–ground relation, and predicate–argument
relations. These are similar to other catalogues of discourse relations that others have
come up with. However, the intent is to capture the information that can be conveyed
by adjacency. By contrast, the relations of Rhetorical Structure Theory (Mann and
Thompson 1986) are a mixture of informational relations like similarity and intentional
relations like justification. The first is what is conveyed by adjacency; the second is
what the speaker is using adjacency to do. Often the coherence relation conveyed by
adjacency is expressed redundantly (and with less ambiguity) in a conjunction (so), an
adverb (consequently), or a referential expression (That made ... ). This does not pose a
problem, assuming the two do not conflict; discourse is rife with redundancy.
Decomposition of a discourse in this fashion yields a tree or tree-like structure. It
bottoms out in individual clauses, and this is where syntax takes over. Adjacency in
larger stretches of discourse can convey a variety of possible relations. As we saw at the
end of Section 2, adjacency within clauses conveys predicate–argument relations. Syntax
is a set of rules that enable us to convey and interpret complex predicate–argument
relations with the rather crude device of concatenation. The best explanation of a clause
is the decomposition given to us by compositional semantics. The best explanation for
an individual morpheme is that it is intended to convey its corresponding predication.
Thus, the syntactic analysis of a clause bottoms out in its logical form.
Now all that remains to be explained is the logical form. It was the original insight of
the “Interpretation as Abduction” framework that the best abductive proof (i.e., the best
explanation) of the logical form solved the local pragmatics problems as a side effect.
</bodyText>
<page confidence="0.983085">
789
</page>
<note confidence="0.32822">
Computational Linguistics Volume 39, Number 4
</note>
<bodyText confidence="0.851456625">
I won’t make an extended argument for that here, but one example should convey the
basic idea.
The sentence, due to Hirst (1987),
(15) The plane taxied to the terminal.
has three lexical ambiguities. A plane could be an airplane or a wood-smoother, a
terminal could be an airport terminal or a computer terminal, and taxiing could be a
plane moving on the ground or a person riding in a cab.
We assume we have axioms expressing these possibilities
</bodyText>
<equation confidence="0.998604125">
airplane(x) ⊃ plane(x)
wood-smoother(x) ⊃ plane(x)
airport-terminal(y) ⊃ terminal(y)
computer-terminal(y) ⊃ terminal(y)
move-on-ground(x,y) ∧ airplane(x) ⊃ taxi(x,y)
ride-in-cab(x,y) ∧ person(x) ⊃ taxi(x,y)
together with a rule that says airports have airplanes and airport terminals.
airport(z) ⊃ airplane(x) ∧ airport-terminal(y)
</equation>
<bodyText confidence="0.98882575">
Then the most economical explanation (Figure 2) is constructed by assuming there
is an airport and that an airplane we expect to find there is moving on the ground
to the airport terminal we expect to find there. Note that the ambiguous words are
disambiguated as a by-product by virtue of the axioms that are used in the explanation.
The predicate airport-terminal plays a role; the predicate computer-terminal doesn’t.
All of this raises a question. If the framework is so elegant and so all-encompassing,
why isn’t it more widely adopted?
I think there are three reasons for this, historically.
</bodyText>
<listItem confidence="0.998718">
1. Parsers were not accurate enough to produce good logical forms from
which inference could start.
2. Algorithms for abduction were too inefficient.
3. There was a lack of an adequate knowledge base.
</listItem>
<bodyText confidence="0.996841466666667">
Each of these problems has been alleviated somewhat in the past few years. There
are now highly accurate statistical parsers, and for several of these (e.g., Boxer; Bos 2008)
a component for translating into a flat logical form has been implemented.
Recent work by Naoya Inoue and Kentaro Inui (2011) implements weighted abduc-
tion as a problem in integer linear programming, building on earlier work by Charniak
and Santos (Santos 1996). Our experience with this is that when we switched from a
naive backchaining implementation to the ILP implementation, we got a speed-up of
two orders of magnitude.
Finally, there have been ongoing efforts to build large knowledge bases, manually
and automatically, from a number of different perspectives. Efforts to use Cyc for
natural language processing applications have had mixed success at best. But Schubert’s
efforts (2002) to build a knowledge base by analyzing language use looks very promis-
ing. Some applications have attempted to use OpenMind. WordNet hierarchies are used
very widely and Harabagiu and Moldovan (2002) developed XWN, a conversion of
WordNet glosses into logical axioms, and reported success with its use in question-
</bodyText>
<page confidence="0.969884">
790
</page>
<table confidence="0.972056666666667">
Hobbs Influences and Inferences
Logical Form:
plane(x) n taxi(x, y) n terminal(y)
Knowledge Base: ■❅ ♦ ❙
airplane(x) D plane(x) ❅ ❙
✻ ❅ ❙
❅ ❙
❅ ❙
❅ ❙
❅ ❙
❅ ❙
n airplane(x) D taxi(x, y) ❙
❙
❙
airport-terminal(y) D terminal(y)
✻
move-on-ground(x, y)
✚✚✚✚✚✚✚❃
</table>
<equation confidence="0.8039925">
airport(z) D airplane(x) n airport-terminal(y)
wood-smoother(x) D plane(x)
ride-in-cab(x,y) n person(x) D taxi(x,y)
computer-terminal(y) D terminal(y)
</equation>
<figureCaption confidence="0.94953">
Figure 2
</figureCaption>
<bodyText confidence="0.900080857142857">
Interpretation of The plane taxied to the terminal.
answering. FrameNet has been converted into logical axioms by Ovchinnikova et al.
(2013), and she and her colleagues have shown that an abduction engine using a
knowledge base derived from these sources is competitive with the best of the statistical
systems in textual entailment and semantic role labeling.
My own particular take on building a knowledge base for inferential NLP is de-
scribed in the next section.
</bodyText>
<sectionHeader confidence="0.987228" genericHeader="acknowledgments">
4. Knowledge
</sectionHeader>
<bodyText confidence="0.998437">
We understand discourse so well because we know so much. Thus, one of the central
problems in the study of language is how we use our knowledge of language and the
world to interpret discourse. This breaks into two subproblems:
</bodyText>
<listItem confidence="0.991413333333333">
1. How do we encode the common-sense knowledge required for
understanding discourse?
2. How do we use this knowledge in the processing of discourse?
</listItem>
<footnote confidence="0.538494">
I had a conversation with Eugene Charniak in the early 1990s in which I said I
thought the second of these is a solved problem. The answer is abduction. He agreed
</footnote>
<page confidence="0.98873">
791
</page>
<note confidence="0.486381">
Computational Linguistics Volume 39, Number 4
</note>
<bodyText confidence="0.865671666666667">
with me. We both agreed that the first problem was now the most important focus of
research. But he said that he despaired of encoding that knowledge manually, and that’s
why he had reoriented his research toward statistical methods. I disagreed for two
reasons. I think the kind of knowledge that we want at the very core of a knowledge
base for NLP can only be done manually by thoughtful people and cannot be done by
any automatic methods currently imaginable. And I think there is systematicity that
will make the task more tractable than we might believe at the outset.
As to the first point, suppose we want to define or characterize the word range, as in
(16) The scores on the test ranged from 38 to 96.
From the definition we would want to be able to answer the questions
Did someone get a 96 on the test? Yes.
Did someone get a 54 on the test? Maybe.
Did someone get a 25 on the test? No.
In addition, we want to capture generalizations and we don’t want to multiply word
senses needlessly. So we would like to have the same definition work for the sentences
</bodyText>
<listItem confidence="0.925181">
(17) The timber wolf ranges from northern Mexico to southern Alaska.
(18) His behavior ranges from sullen to downright hostile.
(19) The hepatitis cases range from moderate to severe.
</listItem>
<bodyText confidence="0.911565666666667">
We don’t want a “species” sense of range, and a “behavioral” sense and an “epidemio-
logical” sense.
The sort of axiom we need for this is as follows:
</bodyText>
<equation confidence="0.97383375">
(20) (∀ x, y, z)range(x, y, z) ≡
(∃ s, s1, u1, u2)scale(s) ∧ subscale(s1, s) ∧ bottom(y, s1)
∧ top(z, s1) ∧ u1 ∈ x ∧ at(u1, y) ∧ u2 ∈ x ∧ at(u2, z)
∧ (∀u ∈ x)(∃v ∈ s1)at(u,v)
</equation>
<bodyText confidence="0.9928941">
That is, x ranges from y to z if and only if there is a scale s with a subscale s1 whose
bottom is y and whose top is z, such that some member u1 of x is at y, some member u2
of x is at z, and every member u of x is at some point v in s1. (I’ll discuss the predicate at
subsequently.)
It is difficult for me to believe we will any time soon be able to discover automat-
ically rules of this complexity and at the same time rules of this level of abstractness.
I’m sure we’ll be able to discover automatically facts such as “One has to be married
before getting divorced,” and “Houses normally have thermostats.” But facts like the
definition of “range” require human brains.
My formative experience in encoding common-sense knowledge came when I was
at Yale in the early 1970s and immersed myself in the linguistics literature. Among
the papers that struck a chord the most were the Generative Semanticists, like Jeffrey
Gruber, George Lakoff, Haj Ross, James McCawley, and others. They were analyzing
the verb kill into cause to become not alive and the verb move, as in x moves y from z to
w, into x causes a change from y being at z to y being at w. They also speculated on the
abstract nature of the at relation as a source for many of the frozen spatial metaphors
that pervade language.
Generative semantics dropped out of favor rather soon, but I think their funda-
mental insights were exactly right. To my mind, they failed for two reasons. First,
they were doing in tree transformations what they should have been doing in logic,
</bodyText>
<page confidence="0.984449">
792
</page>
<note confidence="0.564752">
Hobbs Influences and Inferences
</note>
<bodyText confidence="0.99978522">
a mistake being repeated today by those working on so-called “natural language infer-
ence.” Second, they lacked a notion of defeasibility, so that when they found examples
of when X killed Y and Y didn’t end up not alive, they thought their theory was
refuted.
These interests in lexical semantics got put on the back burner for several years. I
returned to it in the mid-1980s when Bill Croft, Doug Edwards, Ken Laws, and I worked
on building up a knowledge base. Our goal, which we almost achieved, was to be able
to prove as a theorem that wear on a component of an artifact can cause the artifact to
fail, because wear is a loss of material and this causes a change of shape, and shape in
artifacts is normally functional. At the time we were working on U.S. Navy texts dealing
with worn-out air compressors.
Then back to the back burner until 1999 or 2000, since which time knowledge
encoding has been the principal focus of my research. It is not easy research to get
funding for, because its payoff in comparison to building special-purpose applications
is very long-term. One has to find short-term applications that would be helped by
general knowledge in the next logical domain to attempt. For example, I was able to
work with people like George Ferguson, Pat Hayes, and Drew McDermott on devel-
oping the so-called “OWL-Time,” a comprehensive ontology of time (Hobbs and Pan
2004), for DARPA’s DAML program on the Semantic Web, and ARDA’s AQUAINT
program on question-answering provided the resources for my work with Feng Pan and
Rutu Mulkar-Mehta on vague durations of events. Ram Nevatia’s ARDA-sponsored
MOVER project provided the opportunity to develop an ontology of event structure
called VERL (Video Event Representation Language, Alexandre et al. 2005), and this
led to work with Chris Welty, Mike Gruninger, and people at Cycorp on the ARDA-
sponsored IKRIS project for developing an interlingua among several event and process
ontologies. DARPA’s Machine Reading Program supported my student Rutu Mulkar-
Mehta’s work on granular or “how-to” causality (Mulkar-Mehta, Hobbs, and Hovy
2011) and Niloofar Montazeri’s work defining or characterizing several hundred com-
mon event-related words (Montazeri and Hobbs 2011). My work with Andrew Gordon
on encoding common-sense psychology (Gordon and Hobbs 2004) has been funded by
various agencies over the years, most recently by ONR. But some of the research has
been “stealth” research—work you don’t tell anyone about until it’s finished for fear
your boss will find out and make you work on other stuff. My papers on causality and
modality (Hobbs 2005) and on scales and half orders of magnitude (Hobbs 2000) were
like this.
The goal is to develop what I have come to call “Deep Lexical Semantics” (Hobbs
2008). It is not enough to decompose “move” into “cause - change - at.” It is not good
enough to simply stipulate these as primitives. We need to explicate these concepts in
core theories, a theory of causality, a theory of change of state, and a theory of composite
entities and the figure–ground relation. Lexical decompositions have to be anchored in
such theories so we can not only decompose meanings but also be able to reason with
the decomposed meanings.
The structure of the effort is this: We have the predicates corresponding to the
morphemes of the language. We have the underlying core theories. And we have axioms
defining or characterizing the former in terms of the latter. Thus, in the “range” example,
range is the predicate corresponding to the morpheme. There is a core theory of scales
that provides the predicates scale, lessThan, subscale, top, bottom, and at. Axiom (20) is the
rule that links the lexical predicate with the core theory.
Next I will sketch several very basic core theories and show their utility in defining
words for the textual entailment task.
</bodyText>
<page confidence="0.994647">
793
</page>
<note confidence="0.591781">
Computational Linguistics Volume 39, Number 4
</note>
<bodyText confidence="0.9991575">
Composite Entities and the Figure–Ground Relation: A composite entity is a
thing made of other things. This is intended to cover physical objects like a telephone,
mixed objects like a book, abstract objects like a theory, and events like a concert. It is
characterized by a set of components, a set of properties of the components, a set of
relations among its components (the structure), and relations between the entity as a
whole and its environment (including its function). The predicate at relates an external
entity, the figure, to a component in a composite entity, the ground. Different figures
and different grounds give us different meanings for at.
</bodyText>
<listItem confidence="0.999119714285714">
(21) Spatial location: Pat is at the back of the store.
(22) Location on a scale: Nuance closed at 58.
(23) Membership in an organization: Pat is now at Google.
(24) Location in a text: The table is at the end of the article.
(25) Time of an event: At that moment, Pat stood up.
(26) Event at event: Let’s discuss that at lunch.
(27) At a predication: She was at ease in his company.
</listItem>
<bodyText confidence="0.913025285714286">
When at is specialized in this way, we tap into a whole vocabulary for talking about the
domain, including concepts like move and range.
Change of State: The predication change(e1,e2) says that state e1 changes into state
e2. Its principal properties are that e1 and e2 should have an entity in common—a change
of state is a change of state of something. States e1 and e2 are not the same unless there is
an intermediate state. The predicate change is defeasibly transitive; in fact, backchaining
on the transitivity axiom is one way to refine the granularity on processes.
Causality: We distinguish between the “causal complex” for an effect and the
concept “cause.” A causal complex includes all the states and events that have to happen
or hold in order for the effect to occur. We say that flipping a switch causes the light to
go on. But many other conditions must be in the causal complex—the light bulb can’t
be burnt out, the wiring has to be intact, the power has to be on in the city, and so
on. The two key properties of a causal complex are that when everything in the causal
complex happens or holds, so will the effect, and that everything that is in the causal
complex is relevant in a sense that can be made precise. “Causal complex” is a rigorous
or monotonic notion, but its utility in everyday life is limited because we almost never
can specify everything in it.
“Cause” by contrast is a defeasible or nonmonotonic notion. It selects out of a
causal complex a particular eventuality that in a sense is the “active” part of the causal
complex, the thing that isn’t necessarily normally true. Flipping the switch, in most
contexts, is the action that causes the light to come on. Causes are the focus of plan-
ning, prediction, explanation, and interpreting discourse, but not diagnosis, because in
diagnosis, something that normally happens or holds, doesn’t.
Let us now define a few words in terms of these predicates. The verb let as in x lets
e happen means x does not cause e not to happen.
(28) let(x, e1) - not(e8) n cause&apos;(e8, x, e9) n not&apos;(e9, e1)
In its most abstract sense go just means a change of state, as in Sometimes I go crazy.
(29) go&apos;(e1, y, e2, e3) - change&apos;(e1, e2, e3)
</bodyText>
<page confidence="0.990939">
794
</page>
<table confidence="0.508659875">
Hobbs Influences and Inferences
Something y is free of a set of constraints c to do e5 means that it is not the case that the
constraints cause e5 not to happen.
(30) free&apos;(e3, y, c, e5) - not&apos;(e3, e2) n cause&apos;(e2, c, e4) n not&apos;(e4, e5)
Something x holds y at z means that x causes y not to change from being at z.
(31) hold&apos;(e2,x,y,z)
- cause&apos;(e2, x, e4) n not&apos;(e4, e5) n change&apos;(e5, e6, e7) n at&apos;(e6, y, z)
Something x releases y from z means that there is a change from x’s holding y at z.
</table>
<listItem confidence="0.389816">
(32) release&apos;(e1, x, y, z) - change&apos;(e1, e2, e3) n hold&apos;(e2, x, y, z)
</listItem>
<bodyText confidence="0.945702117647059">
The Recognizing Textual Entailment task is to determine from a text whether a
hypothesis follows from it or not. For example, from the text A Filippino hostage in Iraq
was released we would like to be able conclude the hypothesis The captors let the hostage
go free. Figure 3 illustrates the proof of this entailment relation, using the five axioms
we just wrote, together with rules from the core theories saying if something exists,
nothing causes it to not exist, and if there is a change from a state, that state no longer
holds (Montazeri and Hobbs 2011).
The final set of examples I’ll give of Deep Lexical Semantics come from work I have
been doing with Andrew Gordon on axiomatizing common-sense psychology, or how
we think we think. We have developed approaches to memory, belief, and mutual belief,
envisioning causal chains in explanation and prediction, perception and control of the
body, and goals and plans. I will focus on goals.
We adopt the strong AI position that people are in a sense planning mechanisms. We
have goals, we develop plans to achieve these goals, we execute the plans, we monitor
the execution, and if things go wrong, we modify our plans and execute the new plans.
Figure 3
A textual entailment example.
</bodyText>
<page confidence="0.990695">
795
</page>
<note confidence="0.590234">
Computational Linguistics Volume 39, Number 4
</note>
<bodyText confidence="0.99997696">
There are two chief properties of goals. The first says that if an agent has a goal
e2 and believes e1 causes e2, then defeasibly that will cause the agent to have e1 as a
subgoal. The second property is a similar rule for enablement. These are the planning
axioms; they generate hierarchical plans.
The word help can be explicated in terms of theories of goals and causality. We can
distinguish three levels of helping. At the lowest level, inadvertant helping, you help
someone when you do an action that is in a causal complex for one of their goals. In
this sense, John McCain helped Barack Obama get elected by picking Sarah Palin as
his running mate. A second level, intentional helping, is like the first with the addition
that the helper performs the action in the service of the helpee’s goal. For example, if I
take away a drunk friend’s car keys, I help him survive, a goal of his, but not driving is
no part of his plan to survive. The third level, collaborative helping, happens when the
helper and helpee engage in a shared plan together, as in helping someone carry a sofa.
We can define the word try, as in X tries to do E, as having the goal that E be
accomplished, and having it be a goal cause one to accomplish one of its subgoals. To
succeed at doing E is to try to do E, and to have that trying cause E to be accomplished.
To fail to do E is to try to do E and have E not happen.
Often in comprehending discourse about artifacts we need to know a great deal
about the structure and function of the artifact. The theory of goals and planning gives
us a handle on this, because normally the structure of an artifact reflects a plan to
achieve its functionality, a goal. For example, the function of a coffee cup is to move
coffee. We achieve this by breaking it into two subgoals: having a cup contain the coffee
and moving the cup. We achieve moving the cup by attaching a handle to the cup and
moving the handle. Artifacts are plans made concrete (or in the case of a coffee cup,
ceramic).
One of the most salient aspects of common-sense psychology is our emotions. What
about emotions? Is it possible to formalize a theory of emotions? I think it is, and
much of it involves goals. In general, we can characterize emotions in terms of what
causes them and what they cause. Emotions, like cognition more generally, mediate
between perception and action. Thus, for a particular emotion, we specify an abstract
type of perceived eventualities that cause the emotion, and we specify an abstract
class of typical responses. This is basically the knowledge we humans need to fake
emotions.
Consider happiness. Happiness occurs when one’s goals are being satisfied (or
sometimes merely when we anticipate that). That must mean that one’s beliefs in the
relevant area are working, especially one’s beliefs about what causes what. So one effect
of happiness is a higher level of activity—we plan to do more because our planning
process is in good working order. A second effect of happiness is that we are not very
open to a change of beliefs. If our beliefs are working, why should we change them?
Other emotions can be defined in similar terms. Sadness is the opposite of happiness
in cause and effects. Fear, anger, and disgust can be seen as various responses to a threat,
given the properties of the threat, where a threat is something that will cause one’s goals
to be defeated.
I’ll close with an obvious question. How long will it be before we are able to
automatically analyze texts in the manner I have described? When I wrote my technical
report analyzing one paragraph of Newsweek in 1976, I thought the answer was that the
goal was ten years away. When we began to implement a system based on weighted
abduction, I thought the goal was ten years away. So now I will show myself to be
consistently optimistic. I think a concerted effort along these lines would yield some
measure of success in about ten years.
</bodyText>
<page confidence="0.993116">
796
</page>
<note confidence="0.779069">
Hobbs Influences and Inferences
</note>
<sectionHeader confidence="0.93784" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998171769230769">
Agar, Michael, and Jerry R. Hobbs. 1982.
Interpreting discourse: Coherence and
the analysis of ethnographic interviews.
Discourse Processes, 5(1):1–32.
Alexandre, Francois R. J., Ram Nevatia,
Jerry R. Hobbs, and Robert C. Bolles.
2005. VERL: An ontology framework
for representing and annotating
video events. IEEE Multimedia,
12:76–86.
Bos, Johan. 2008. Wide-coverage semantic
analysis with Boxer. In J. Bos and R.
Delmonte, editors, Semantics in Text
Processing. Proceedings of STEP 2008
Conference, Research in Computational
Semantics, College Publications, Venice,
Italy, pages 277–286.
Bruce, Bertram C., and Dennis Newman.
1978. Interacting plans. Cognitive Science,
2:195–233.
Charniak, Eugene, and Robert Goldman.
1988. A logic for semantic interpretation.
In Proceedings of the 26th Annual Meeting of
the Association for Computational Linguistics,
Buffalo, NY, pages 87–94.
Cohen, Philip, and C. Raymond Perrault.
1979. Elements of a plan-based theory
of speech acts. Cognitive Science,
3(3):177–212.
Cole, Peter, editor. 1981. Radical Pragmatics.
Academic Press, New York.
Cox, P. T., and T. Pietrzykowski. 1986.
Causes for events: Their computation
and applications. In Proceedings of Eighth
International Conference on Automated
Deduction (CADE-8), pages 608–621,
Oxford.
Davidson, Donald. 1967. The logical form of
action sentences. In N. Rescher, editor,
The Logic of Decision and Action, University
of Pittsburgh Press, Pittsburgh, PA,
pages 81–95.
Gordon, Andrew, and Jerry R. Hobbs.
2004. Formalizations of commonsense
psychology. AI Magazine, 25:49–62.
Grice, H. P. 1975. Logic and conversation.
In P. Cole and J. Morgan, editors, Syntax
and Semantics, Academic Press, New York,
pages 41–58.
Grimes, Joseph. 1975. The Thread of Discourse.
Mouton and Company, The Hague,
Netherlands.
Hirst, Graeme, 1987. Semantic Interpretation
and the Resolution of Ambiguity. Cambridge
University Press, Cambridge, UK.
Hobbs, Jerry R. 1976. A computational
approach to discourse analysis. Research
Report 76-2, Department of Computer
Sciences, City College, City University
of New York, New York, pages 28–38.
Hobbs, Jerry R. 2000. Half orders of
magnitude. In Proceedings of KR-2000
Workshop on Semantic Approximation,
Granularity, and Vagueness,
Breckenridge, CO.
Hobbs, Jerry R. 2005. Toward a useful
concept of causality for lexical semantics.
Journal of Semantics, 22:181–209.
Hobbs, Jerry R. 2008. Deep lexical
semantics. Proceedings of 9th International
Conference on Intelligent Text Processing and
Computational Linguistics (CICLing-2008),
Haifa, Israel, pages 183–193.
Hobbs, Jerry R., Douglas E. Appelt, John
Bear, David Israel, Megumi Kameyama,
Mark Stickel, and Mabry Tyson. 1997.
FASTUS: A cascaded finite-state
transducer for extracting information from
natural-language text. In E. Roche and
Y. Schabes, editors. Finite State Devices for
Natural Language Processing. MIT Press,
Cambridge, MA, pages 383–406.
Hobbs, Jerry R., and David Andreoff Evans.
1980. Conversation as planned behavior.
Cognitive Science, 4(4):349–377.
Hobbs, Jerry R., and Feng Pan. 2004. An
ontology of time for the Semantic Web.
ACM Transactions on Asian Language
Information Processing, 3(1):66–85.
Hobbs, Jerry R., Mark Stickel, Douglas
Appelt, and Paul Martin. 1993.
Interpretation as abduction. Artificial
Intelligence, 63(1-2):69–142.
Inoue, Naoya, and Kentaro Inui. 2011.
ILP-based reasoning for weighted
abduction. In Proceedings of AAAI
Workshop on Plan, Activity and Intent
Recognition, San Francisco, CA, pages
25–32.
Lakoff, George, and Mark Johnson. 1980.
Metaphors We Live By. University of
Chicago Press, Chicago.
Lewis, David. 1979. Scorekeeping in a
language game. Journal of Philosophical
Logic, 6:339–359.
Longacre, Robert. 1976. An Anatomy of Speech
Notions. The Peter de Ridder Press,
Ghent, Belguim.
Mann, William, and Sandra Thompson. 1986.
Relational propositions in discourse.
Discourse Processes, 9(1):57–90.
Moldovan, Dan, Sanda Harabagiu, Roxana
Girju, Paul Morarescu, Finley Lacatusu,
Adrian Novischi, Adriana Badulescu,
Orest Bolohan. 2002. LCC tools for
question answering. In Proceedings of the
Eleventh Text Retrieval Conference (TREC
</reference>
<page confidence="0.928506">
797
</page>
<reference confidence="0.993473010989011">
Computational Linguistics Volume 39, Number 4
2002), Gaithersburg, MD. Available at
http://trec.nist.gov/pubs/trec11/
t11 proceedings.html, 21:1–10.
Montazeri, Niloofar, and Jerry R. Hobbs.
2011. Elaborating a knowledge base for
deep lexical semantics. In Proceedings
of the Ninth International Conference on
Computational Semantics (IWCS 2011),
Oxford, England, pages 195–204.
Mulkar-Mehta, Rutu, Jerry R. Hobbs, and
Eduard Hovy. 2011. Applications and
discovery of granularity structures
in natural language discourse.
In Proceedings of the 10th Symposium
on Logical Formalizations of
Commonsense Reasoning. Available at
http://commonsensereasoning.
org/2011/proceedings.html
Norvig, Peter. 1987. Inference in text
understanding. In Proceedings,
AAAI-87, Sixth National Conference on
Artificial Intelligence, pages 561–565,
Seattle, WA.
Ovchinnikova, Ekaterina, Niloofar
Montazeri, Teodor Alexandrov, Jerry R.
Hobbs, Michael C. McCord, and
Rutu Mulkar-Mehta. 2013. Abductive
reasoning with a large knowledge
base for discourse processing.
Computing Meaning, 4:104–124.
Pan, Feng, Rutu Mulkar-Mehta, and Jerry
Hobbs. 2011. Annotating and learning
event durations in text. Computational
Linguistics, 37(4):727–752.
Peirce, Charles Sanders. 1955. Abduction
and induction. In Justus Buchler,
editor, Philosophical Writings of
Peirce, Dover Books, New York,
pages 150–156.
Pollard, Carl, and Ivan A. Sag. 1994.
Head-Driven Phrase Structure Grammar.
University of Chicago Press and
CSLI Publications.
Pople, Harry E., Jr. 1973, On the
mechanization of abductive logic.
In Proceedings of Third International Joint
Conference on Artificial Intelligence,
pages 147–152, Stanford, California.
Richards, I. A. 1936. The Philosophy of
Rhetoric, Oxford University Press, Oxford.
Rieger, Charles J., III. 1974. Conceptual
memory: A theory and computer program
for processing the meaning content of
natural language utterances. Memo
AIM-233, Stanford Artificial Intelligence
Laboratory, Stanford University.
Sager, Naomi. 1981. Natural Language
Information Processing: A Computer
Grammar of English and Its Applications.
Addison-Wesley, Reading, MA.
Santos, Eugene. 1996. Polynomial solvability
of cost-based abduction. Artificial
Intelligence, 86:157–170.
Schubert, Lenhart K. 2002. Can we derive
general world knowledge from texts?
In Proceedings of the 2nd International
Conference on Human Language Technology
Research (HLT 2002), pages 94–97, San
Diego, CA.
Smith, Kate, Ethel B. Magee, and S. S.
Seward, Jr. 1928. English Grammar: Correct
and Effective Use. The Athenaeum Press,
Ginn and Co., Boston, MA.
Thagard, Paul R. 1978. The best explanation:
Criteria for theory choice. The Journal of
Philosophy, 75(2):76–92.
Thomason, Richmond H. 1985.
Accommodation, conversational planning,
and implicature. In Proceedings of Workshop
on Theoretical Approaches to Natural
Language Understanding, Halifax,
Nova Scotia, pages 117–126.
Vico, Giambattista, 1968 [1744]. The New
Science of Giambattista Vico, T. Bergin and
M. Frisch, translators. Cornell University
Press, Ithaca, NY.
Wilensky, Robert. 1983. Planning and
Understanding: A Computational Approach to
Human Reasoning. Addison-Wesley,
Reading, MA.
</reference>
<page confidence="0.997053">
798
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.997627">Influences and Inferences</title>
<author confidence="0.611751">R</author>
<affiliation confidence="0.999257">Information Sciences Institute University of Southern California</affiliation>
<address confidence="0.880644">Marina del Rey, California</address>
<abstract confidence="0.975897741839765">1. False and True Starts I am deeply honored to receive the ACL’s Lifetime Achievement Award. I’m especially honored when I look back at the list of previous winners—Chuck Fillmore, Eugene Charniak, Eva Hajiˇcov´a, Fred Jelinek, Martin Kay, Aravind Joshi, and the others— they’re all my heroes. I was of course delighted to learn of this award. The most we can hope for in life is to take part in the conversation, and an award like this means that you’ve taken part in the conversation. It seems to be a tradition to begin with a few formative anecdotes from childhood. For me, it all begins before I was born. My grandfather, a crusty old country lawyer in southern Indiana, told my father not to bother trying to go to law school. “You don’t know English grammar,” he said. “You’ll flunk out.” My dad accepted the challenge, a book entitled by Smith, Magee, and Seward (1928), and mastered it. He went on to become a very successful lawyer. Fast-forward to when I was in junior high school. My dad was distressed that my English classes looked to him more like social studies, and barely touched on grammar. So he persuaded me—actually, he probably bribed me, but I can’t remember what master that same book, Grammar Smith, Magee, and Seward. This was a concession, because I was a math nerd, reading only textbooks on trigonometry and calculus, as my way of avoiding the humiliation of playing baseball. But I read the book, and I was amazed. English grammar was just like math! It had the same sorts of rules, the same kinds of abstractions, the same types of puzzles. It was actually fun! In my junior or senior year of high school we had to take something called the Kuder Preference Test, which would help us decide what career to choose. I scored high in math and in language. So my high school counselor told me I should write math books. In fact, she got it exactly backwards. It wasn’t that I should do language about math. It was that I should do math about language. I’ve met any number of computational linguists with a similar story. They grew up not knowing whether they wanted to be a physicist or a poet. They just knew both sounded fascinating. Then they discovered our field. My last near miss happened the week I was drafted into the Army. They gave us a battery of aptitude tests to see what specialties we’d be best for. One of the tests was to see if we should be sent to the Monterey Language School. Looking back on it, I realize now it was testing how well you could understand formal language theory. They’d give 4676 Admiralty Way, Marina del Rey, CA 90292, USA. E-mail: doi:10.1162/COLI a 00171 © 2013 Association for Computational Linguistics Computational Linguistics Volume 39, Number 4 you a bunch of rules for an artificial language, and you’d have to say whether different strings were in or not in the language. I’d never seen anything like it, but it was really fun to do. Later I met with a personnel specialist who went over my test scores. I got a 46 out of 50. He ignored that until I pointed it out to him. Then he said, “That’s a mistake. Nobody ever gets more than 6 or 7 points on that test.” I said, “No, I think it might be correct.” He said, “It doesn’t matter. You’re not going to the Monterey Language School. You’re going to South Vietnam.” I didn’t go to the Monterey Language School South Vietnam. I spent two years in South Carolina, and was glad to be there. How I managed that is a story for another occasion. So I didn’t really discover computational linguistics until my third year in graduate school at New York University. In October I passed my oral exam in topics like algebraic topology and complex analysis, by one generous yes and two abstensions. In the subsequent months I discovered more and more facts about myself—for example, that I was never going to figure out a faster way of multiplying matrices, and that fascinating though recursion theory might be, I was never going to prove a theorem that Hartley Rogers would be compelled to include in his next edition. As I surveyed vaguely plausible fields, I realized I had no idea what the next problem to solve would be or even what makes a problem interesting. Then in April, when I had nearly resigned myself to becoming a taxi driver, I discovered New York University’s best-kept secret: Naomi Sager’s Linguistic String Project. I think it is also computational linguistics’ best kept secret as well. She was motivated by the science, not by the performance, and her very impressive work is nowhere near as well-known as it should be. I think her Linguistic String Grammar (Sager 1981) ranks, a computational specification of English syntax, with Pollard and Sag’s Structure Grammar for thoroughness, insight, and elegance. So, for example, in 1992 when we developed the FASTUS system for information extraction using cascaded finite-state transducers (Hobbs et al. 1997), it was straightforward to copy the rules for Noun Groups straight from her grammar. It’s no accident that in the late 1980s during the Strategic Computing Initiative and in the early 1990s in the Message Understanding Conferences, three of the most important efforts were led by Linguistic String Project alumni—Ralph Grishman’s group at New York University, Lynette Hirschman’s at Unisys, and my group at SRI International. I think the most important lesson I learned from Naomi Sager was to look closely at the data and to take it seriously. My other thesis advisor was Jack Schwartz. He was a polymath, so to speak. I took a course in logic from him. I knew about his book on compilers and the classic Dunford and Schwartz on functional analysis. But when I saw his book on mathematical economics and his book on the theory of relativity, I did some research to see if there was more than one Jack Schwartz. Among his writings was an unpublished Chapter 9 of his compilers book, on parsing natural language, which I of course read. My thesis was on Earley’s algorithm applied to natural language. It quickly became apparent that the constraints on phrase structure rules had to be expressed and that one could do that with fairly simple operations on vectors of features, where among the features were what I called the “cores” of the constituents, since they bundled many of the relevant features. My “core” was what linguists came to call “head.” Years later, I ran across Chapter 9 again and reread it, and realized that all the ideas in my thesis were there. So when in 1987 Schwartz told someone that I had anticipated head-driven structure grammar, that was his way of saying anticipated head-driven phrase structure grammar. 782 Hobbs Influences and Inferences My first job was at Yale University as a very temporary instructor—I think the position is now called “post-doc.” Over the course of the year I became convinced that syntax was a solved problem—something I still believe. But that left me adrift for problems to work on. I became discouraged, and found myself thinking again about driving that taxi. Then late one afternoon, just as I was about to go home, a graduate student named Fred Howard came into my office to ask a couple of questions. That triggered a discussion that lasted until 11 o’clock that evening. One of the wheels we reinvented was a recognition of the pervasiveness of spatial metaphor in discourse. (This was before Lakoff and Johnson [1980], but after similar observations by the 18th-century Italian philosopher Giambattista Vico (1968 [1744]) and the 20th-century English literary critic I. A. Richards [1936].) But within a year, everything else of value that remained of the content of that discussion could be compressed into a long footnote in a technical report. In any case, this conversation lit a fire that fueled my research for the next 15 or 20 years. In particular, I began looking at texts, trying to understand how we understand them. No doubt influenced by Chuck Rieger’s thesis (Rieger 1974), I asked what inferences we draw in the course of comprehension, and, an issue Rieger did not address, what inferences we do not draw. This culminated in 1976 in an unreadable (and unread) report (Hobbs 1976), microanalyzing one paragraph from trying to specify every bit of knowledge required for understanding the text and describing how every linguistic problem in the text invokes that knowledge to arrive at solutions. One could say that the rest of my career has been a matter of cleaning up and extending that technical report, in terms of representation, the process of inference and interpretation, and the specification of common-sense knowledge. 2. Representation In 1977 I moved to SRI, where I fell under the influence of Nils Nilsson and Bob Moore, and of John McCarthy at nearby Stanford. They were campaigning to replace the ad hoc styles of representation of early AI with representations based on first-order logic. But the problem in a nutshell is this: When we are trying to represent an English sentence believes Chris is we really want to write (1) believe(Pat, tall(Chris)) difficulty is that a predicate and to true or false, so we are left with Pat believes a truth value, with not a hint of Chris’s tallness to be found. A solution to this is to treat as a predicate but as an opaque operator that blocks evaluation of its operands. Many special logics have been developed for such operators. For example, knowing about modal and temporal logics, Russell’s iota operator, functionals, lambda expressions, and so on, we might represent the sentence (2) Maybe the boy wanted to build a boat quickly. by the expression (3) This bothered me because it seemed like we were introducing a new operator with its own special logic every time we encountered a new word to define or characterize. For 783 Computational Linguistics Volume 39, Number 4 20,000 words would we have to introduce 20,000 new operators? It seemed to me that we should rather stay within first-order logic, abiding by two principles: 1. All morphemes are created equal. 2. Every morpheme conveys a predication. We could achieve this kind of representation by means of reification. Thus, if says that eventuality the state or eventuality of Chris being tall, then can represent believes Chris is tall (4) Sentence (2) is then represented (5) There’s nothing exotic here (other than reification). It’s all first-order logic, predicates applied to arguments where the arguments are existentially quantified variables with widest possible scope, ranging over a universe of possible individuals. The extremes to which we go in identifying morphemes with predications can seen in the predication What could that possibly mean? Well, ask what is being conveyed by the word It is a relation between an entity description and it says the entity is uniquely mutually identifiable in context by means of the description. We can give this relation a name. We could call it something But why not keep it simple, and name the after the morpheme that conveys it – Knowledge representation schemes that use extensive reification are often called “Davidsonian,” after the philosopher Donald Davidson (1967), who proposed reifying events. But he balked at reifying states, let alone negations of states and events. He would not have treated Chris’s tallness as a thing. By contrast, I adopted a position that, because I was young and wild, I called “ontological promiscuity.” Now that I’m older and more domesticated, I would probably call it something like “ontological prosperity” or “ontological comfortable circumstances” or maybe “ontological glut.” Many balk at such abandonment of ontological scruples. No doubt I was influenced by the near solipsism that infected many researchers in the early days of AI. Our brains could be fooling us, just as we often fool computers to test our programs. Yes, there is probably a world out there that occasionally bites back. But the world is benevolent— after all, we evolved in it. When we breathe, there is almost always oxygen there. That’s no accident. So it doesn’t matter very much what we believe. We can believe all sorts of crazy things and be completely ignorant of apparently real and pervasive phenomena. Until the recent past we believed in the spirits of the dead, and we were entirely ignorant of 98% of the electromagnetic spectrum. If you are willing to admit the existence of physical objects, sets, numbers, and possible worlds, what ontological scruples do you have anyway? So why should we give any credence at all to our intuitions about what exists and what doesn’t? Why not simply stipulate that everything that can be talked about exists in a Platonic universe of possible individuals, since that makes it so much easier to represent and reason about the content of natural language discourse? The result of this move and similar reifications to eliminate quantifier scopings is that the logical form of a sentence is a flat conjunction of existentially quantified propositions, with one predication per morpheme. 784 Hobbs Influences and Inferences But there is a problem. The sentence (6) John is tall. would be represented whereas the sentence (8) John is not tall. would be represented so it would seem that is not tall is wrinkle is that does not say that tall. It says that a possible of being tall. The eventuality or may not exist in the real world, if it does, that is one of its properties – means that we have to distinguish between the a sentence and its Sentences (6) and (8) have highly overlapping content. But the claim of sen- (6) is the tall-ness, while the claim of sentence (8) is the negation of the tall-ness. The general procedure for deciding on whether or not an eventuality really exists is as follows: 1: the claim. 2: truth and falsity through implicatives. 3: a courtesy to the speaker, assume the other propositions are true. (But note that in modal contexts there is an ambiguity in whether the grammatically material holds in the real world or in the modal context For example, in (10) The lazy man did not manage to avoid attending the meeting. Step 1 says the claim is the “not.” Step 2 says that therefore “manage” is false, “avoid” is false, and “attend” is true. Step 3 says that “lazy,” “man,” and “meeting” are all true. This kind of representation has the advantage of yielding a very elegant view of compositional semantics. In traditional approaches to compositional semantics, the meanings of constituents are lambda expressions, and composition happens by function application. With a flat logical form, the only role function application plays is identifying variables with each other. This gives us a two-part account of compositional semantics. 1. The lexicon provides predicate–argument relations. 2. Syntax identifies variables. For the sentence (11) The man attended the meeting. 785 Computational Linguistics Volume 39, Number 4 tense, we get from the individual words the propositions we recognize that the meeting a verb phrase, this amounts to recognizthat When we recognize man attended the meeting a clause, we have that 3. Interpretation In 1979 and 1980, I had the huge good fortune to participate in a biweekly discussion group on discourse, alternating between Stanford and Berkeley, consisting of some of the most illustrious scholars of language in the world, including Mike Agar, Dwight Bolinger, Eve and Herb Clark, Chuck Fillmore, Paul Kay, George Lakoff, Geoff Nunberg, Ivan Sag, Dan Slobin, Elizabeth Traugott, and Tom Wasow. For me personally, the high point in these meetings, and one of the high points in my entire career, was when the sociologist Irving Goffman, visiting Berkeley at the time, used my paper “Conversation as Planned Behavior” (Hobbs and Evans 1980) as a club to beat the sociolinguist John Gumperz over the head with. Metaphorically speaking. We read and discussed members’ papers on interpreting nominal compounds, metonymy or deferred reference, de-nominalized nouns, metaphor, and other phenomena that came to be clustered by linguists under the name of “Radical Pragmatics” (Cole 1981). (I thought a better name would be “Run-of-the-mill AI”.) Around this time, I was concerned with the problem of how we delimit the set of inferences we draw as we understand a text. The answer that seemed most promising was that we need to draw those inferences required to resolve interpretation problems of the sort we were examining in the discussion group. But what systematicity was there to this set of problems? How would you know if your list was complete? The scheme that made the most sense to me goes like this. A text conveys predithat is, a predicate applied to one or more arguments – This gives rise to three sorts of problems: What is the predicate? What is This question subsumes the problems of lexical ambiguity, the interpretation of vague predicates like prepositions and the interpretation of the implicit relation in nominal compounds. What is the argument? What is This question subsumes the problems of coreference and syntactic ambiguity. (Recall that syntactic structure is a matter of identifying variables in the right way.) In what way are the predicate and argument congruent? What about allow be true of This question subsumes the problems of metaphor and metonymy. This collection of problems I called “local pragmatics.” They are problems that are presented within the scope of single sentences, but they often require for their solution the entire discourse, the external context, and world knowledge. (My term never caught on probably because no one else saw this class of problems as a natural kind.) 786 Hobbs Influences and Inferences Another issue I was thinking about during these years was the structure of discourse, in particular, that structure arising out of coherence relations between discourse segments. In this I was very much influenced by the work of the linguists Joseph Grimes (1975) and Robert Longacre (1976). I began collaborating with the anthropologist Mike Agar around this time, and we called this level of structure “local coherence” (Agar and Hobbs 1982). In the mid-1970s Ray Perrault and Phil Cohen (Cohen and Perrault 1979) at the University of Toronto, later to be my colleagues at SRI, and Chip Bruce (Bruce and Newman 1978) at BBN were doing very exciting work analyzing the structure of discourse as arising out of the speaker’s or writer’s plan, employing formalizations of planning from artificial intelligence. In work with David Evans and work with Mike Agar I tried to apply these insights to the complexities of ordinary conversation and to ethnographic interviews. Agar and I called this level of structure “global coherence.” All along in investigating all three of these problems—local pragmatics, local coherence, and global coherence—it was clear that a key role was played by the notions of implicature (Grice 1975), accommodation (Lewis 1979; Thomason 1985), and abduction (Peirce 1955). To solve even elementary problems like pronoun coreference, one had to make assumptions to get a good interpretation of the text, where the only justification for the assumptions was that they led to a good interpretation. In the fall of 1987 at SRI we organized a discussion group on abduction, reading the classic papers by Peirce, recent attempts in AI to use abduction in, for example, medical diagnosis (Pople 1973; Cox and Pietrzykowski 1986), and contemporary philosophers like Paul Thagard (1978), as well as work by Wilensky and Norvig at Berkeley (Wilensky 1983; Norvig 1987) and Charniak and Goldman at Brown (Charniak and Goldman 1988) that seemed to be taking an approach similar to ours. Among the people in our group were Mark Stickel, Doug Edwards, and the pragmatics scholar Steve Levinson, who was visiting Stanford at the time. We argued about what we were calling identity implicatures and referential implicatures, and about how to distinguish new from given information in discourse, and how to choose the best interpretation of a text. Then late one afternoon in October 1987 Mark Stickel came into my office to say that he thought he had the answer to all our problems. He described his algorithm for weighted abduction. It struck me immediately as the double helix of computational linguistics, a feeling that has not entirely abandoned me today. First of all, it gave us a characterization of what constituted the interpretation of a stream of discourse. It gave us a clear criterion for what inferences to draw and not draw. The interpretation was the most economical explanation for what would make the text true, and an inference was appropriate if and only if it contributed to that explanation. On my way home that night, I began driving a little more carefully. In the next few days, I saw how one would approach all the local pragmatics and local and global coherence problems in this framework. In discussions with Stu Shieber in the next few days it became apparent how one could integrate syntax smoothly into the framework. A big picture emerged (Hobbs et al. 1993). In the early 1990s I saw an advertisement in a magazine for Polaroid cameras (quite obsolete now). It showed a man standing by the ocean, holding a camera, and looking at a scene in which the branch of a tree is on the ground and a small boat is stuck in the top of another tree. When we see this, we immediately interpret it by coming up with the best explanation for the observables (abduction). There was a storm that blew the branch down and blew the boat into the tree. There are other 787 Computational Linguistics Volume 39, Number 4 possible explanations. Maybe someone chopped the branch down, and maybe the boat was lifted into the tree with a crane. But this is not as good an interpretation because we have to assume two things (the chopping and the crane) rather than just one (the storm). The first interpretation is better because it is more economical. Less explains more. But this isn’t the end of the story. There is another observable to be explained. Why is this picture in the magazine? The explanation is that it is an advertisement. That means there was an ad agency involved in posing the picture, and they very well could have done the chopping and used the crane, rather than wait for the rare event of a storm to arrange the picture for them. We could call the first explanation the “informational” one. It explains the content of the picture, thereby explicating the information conveyed by the picture. We could call the second explanation the “intentional” one. It explains why the message occurs at all. Note that both interpretations need to be discovered if the advertisement is to be fully appreciated. The big picture that emerges is this (see Figure 1). The brain is an abduction machine, continuously trying to prove abductively (i.e., by making necessary assumptions) that the observables in its environment constitute a coherent situation. (We can encompass action as well as perception by adding to what is proved the proposition that the owner of the brain will thrive in that situation.) Sometimes among the observables is another agent’s utterance. What is to be exis the proposition is, a speaker to a hearer string Figure 1 Interpretation as abduction, the big picture. 788 Hobbs Influences and Inferences words Generally the best explanation for an utterance is that it is an intentional act aimed at conveying information. We can capture this with the axiom (13) is, if an interpretable segment of discourse describing a situation and a the goal a hearer some cognitive stance toward then utter to string of words The first conjunct in the antecedent is the entry point into the informational side of an interpretation: What is the content of the message? The second two conjuncts are the entry point into the intentional side: Why is the speaker conveying this content? The reason that the speaker has this particular goal is usually that it plays some role in, or is a subgoal of, a larger plan the speaker is executing in the world. This is where that reasoning occurs. It encompasses what Agar and I called “global coherence”—how does the utterance fit in with what else is going on in the world? The next level of analysis happens when we decompose the segment of discourse into smaller segments, using the axiom (14) axiom says that if a segment describing situation and a segment situation and there is a relation between then the concatenation a segment describing a situation derivable from the relation. When we backchain on this axiom, we are explaining an interpretable segment of discourse by breaking it into parts, explaining the parts, and explaining the relation between them. The possible coherence relations are just the sort of relations that frequently obtain between two states or events: causality, similarity, identity, a strong sort of temporal succession I have called “occasion,” the figure–ground relation, and predicate–argument relations. These are similar to other catalogues of discourse relations that others have come up with. However, the intent is to capture the information that can be conveyed by adjacency. By contrast, the relations of Rhetorical Structure Theory (Mann and Thompson 1986) are a mixture of informational relations like similarity and intentional relations like justification. The first is what is conveyed by adjacency; the second is what the speaker is using adjacency to do. Often the coherence relation conveyed by is expressed redundantly (and with less ambiguity) in a conjunction an or a referential expression made ... This does not pose a problem, assuming the two do not conflict; discourse is rife with redundancy. Decomposition of a discourse in this fashion yields a tree or tree-like structure. It bottoms out in individual clauses, and this is where syntax takes over. Adjacency in larger stretches of discourse can convey a variety of possible relations. As we saw at the end of Section 2, adjacency within clauses conveys predicate–argument relations. Syntax is a set of rules that enable us to convey and interpret complex predicate–argument relations with the rather crude device of concatenation. The best explanation of a clause is the decomposition given to us by compositional semantics. The best explanation for an individual morpheme is that it is intended to convey its corresponding predication. Thus, the syntactic analysis of a clause bottoms out in its logical form. Now all that remains to be explained is the logical form. It was the original insight of the “Interpretation as Abduction” framework that the best abductive proof (i.e., the best explanation) of the logical form solved the local pragmatics problems as a side effect. 789 Computational Linguistics Volume 39, Number 4 I won’t make an extended argument for that here, but one example should convey the basic idea. The sentence, due to Hirst (1987), (15) The plane taxied to the terminal. has three lexical ambiguities. A plane could be an airplane or a wood-smoother, a terminal could be an airport terminal or a computer terminal, and taxiing could be a plane moving on the ground or a person riding in a cab. We assume we have axioms expressing these possibilities together with a rule that says airports have airplanes and airport terminals. Then the most economical explanation (Figure 2) is constructed by assuming there is an airport and that an airplane we expect to find there is moving on the ground to the airport terminal we expect to find there. Note that the ambiguous words are disambiguated as a by-product by virtue of the axioms that are used in the explanation. predicate a role; the predicate All of this raises a question. If the framework is so elegant and so all-encompassing, why isn’t it more widely adopted? I think there are three reasons for this, historically. 1. Parsers were not accurate enough to produce good logical forms from which inference could start. 2. Algorithms for abduction were too inefficient. 3. There was a lack of an adequate knowledge base. Each of these problems has been alleviated somewhat in the past few years. There are now highly accurate statistical parsers, and for several of these (e.g., Boxer; Bos 2008) a component for translating into a flat logical form has been implemented. Recent work by Naoya Inoue and Kentaro Inui (2011) implements weighted abduction as a problem in integer linear programming, building on earlier work by Charniak and Santos (Santos 1996). Our experience with this is that when we switched from a naive backchaining implementation to the ILP implementation, we got a speed-up of two orders of magnitude. Finally, there have been ongoing efforts to build large knowledge bases, manually and automatically, from a number of different perspectives. Efforts to use Cyc for natural language processing applications have had mixed success at best. But Schubert’s efforts (2002) to build a knowledge base by analyzing language use looks very promising. Some applications have attempted to use OpenMind. WordNet hierarchies are used very widely and Harabagiu and Moldovan (2002) developed XWN, a conversion of glosses into logical axioms, and reported success with its use in question- 790 Hobbs Influences and Inferences Logical Form: Base: ❅ ❙ ❙ ✻ ❙ ❙ ❙ ❙ ❙ ❅ ❙ n ❙ ❙ ❙ ✚✚✚✚✚✚✚❃ Figure 2 of plane taxied to the answering. FrameNet has been converted into logical axioms by Ovchinnikova et al. (2013), and she and her colleagues have shown that an abduction engine using a knowledge base derived from these sources is competitive with the best of the statistical systems in textual entailment and semantic role labeling. My own particular take on building a knowledge base for inferential NLP is described in the next section. 4. Knowledge We understand discourse so well because we know so much. Thus, one of the central problems in the study of language is how we use our knowledge of language and the world to interpret discourse. This breaks into two subproblems: 1. How do we encode the common-sense knowledge required for understanding discourse? 2. How do we use this knowledge in the processing of discourse? I had a conversation with Eugene Charniak in the early 1990s in which I said I thought the second of these is a solved problem. The answer is abduction. He agreed 791 Computational Linguistics Volume 39, Number 4 with me. We both agreed that the first problem was now the most important focus of research. But he said that he despaired of encoding that knowledge manually, and that’s why he had reoriented his research toward statistical methods. I disagreed for two reasons. I think the kind of knowledge that we want at the very core of a knowledge base for NLP can only be done manually by thoughtful people and cannot be done by any automatic methods currently imaginable. And I think there is systematicity that will make the task more tractable than we might believe at the outset. to the first point, suppose we want to define or characterize the word as in (16) The scores on the test ranged from 38 to 96. From the definition we would want to be able to answer the questions Did someone get a 96 on the test? Yes. Did someone get a 54 on the test? Maybe. Did someone get a 25 on the test? No. In addition, we want to capture generalizations and we don’t want to multiply word senses needlessly. So we would like to have the same definition work for the sentences (17) The timber wolf ranges from northern Mexico to southern Alaska. (18) His behavior ranges from sullen to downright hostile. (19) The hepatitis cases range from moderate to severe. don’t want a “species” sense of and a “behavioral” sense and an “epidemiological” sense. The sort of axiom we need for this is as follows: (20) is, from and only if there is a scale a subscale is whose top is such that some member at some member at and every member at some point (I’ll discuss the predicate subsequently.) It is difficult for me to believe we will any time soon be able to discover automatically rules of this complexity and at the same time rules of this level of abstractness. I’m sure we’ll be able to discover automatically facts such as “One has to be married before getting divorced,” and “Houses normally have thermostats.” But facts like the definition of “range” require human brains. My formative experience in encoding common-sense knowledge came when I was at Yale in the early 1970s and immersed myself in the linguistics literature. Among the papers that struck a chord the most were the Generative Semanticists, like Jeffrey Gruber, George Lakoff, Haj Ross, James McCawley, and others. They were analyzing verb to become not alive the verb as in moves y from z to into causes a change from y being at z to y being at They also speculated on the nature of the as a source for many of the frozen spatial metaphors that pervade language. Generative semantics dropped out of favor rather soon, but I think their fundamental insights were exactly right. To my mind, they failed for two reasons. First, they were doing in tree transformations what they should have been doing in logic, 792 Hobbs Influences and Inferences a mistake being repeated today by those working on so-called “natural language inference.” Second, they lacked a notion of defeasibility, so that when they found examples when killed Y Y didn’t end up not alive, they thought their theory was refuted. These interests in lexical semantics got put on the back burner for several years. I returned to it in the mid-1980s when Bill Croft, Doug Edwards, Ken Laws, and I worked on building up a knowledge base. Our goal, which we almost achieved, was to be able to prove as a theorem that wear on a component of an artifact can cause the artifact to fail, because wear is a loss of material and this causes a change of shape, and shape in artifacts is normally functional. At the time we were working on U.S. Navy texts dealing with worn-out air compressors. Then back to the back burner until 1999 or 2000, since which time knowledge encoding has been the principal focus of my research. It is not easy research to get funding for, because its payoff in comparison to building special-purpose applications is very long-term. One has to find short-term applications that would be helped by general knowledge in the next logical domain to attempt. For example, I was able to work with people like George Ferguson, Pat Hayes, and Drew McDermott on developing the so-called “OWL-Time,” a comprehensive ontology of time (Hobbs and Pan 2004), for DARPA’s DAML program on the Semantic Web, and ARDA’s AQUAINT program on question-answering provided the resources for my work with Feng Pan and Rutu Mulkar-Mehta on vague durations of events. Ram Nevatia’s ARDA-sponsored MOVER project provided the opportunity to develop an ontology of event structure called VERL (Video Event Representation Language, Alexandre et al. 2005), and this led to work with Chris Welty, Mike Gruninger, and people at Cycorp on the ARDAsponsored IKRIS project for developing an interlingua among several event and process ontologies. DARPA’s Machine Reading Program supported my student Rutu Mulkar- Mehta’s work on granular or “how-to” causality (Mulkar-Mehta, Hobbs, and Hovy 2011) and Niloofar Montazeri’s work defining or characterizing several hundred common event-related words (Montazeri and Hobbs 2011). My work with Andrew Gordon on encoding common-sense psychology (Gordon and Hobbs 2004) has been funded by various agencies over the years, most recently by ONR. But some of the research has been “stealth” research—work you don’t tell anyone about until it’s finished for fear your boss will find out and make you work on other stuff. My papers on causality and modality (Hobbs 2005) and on scales and half orders of magnitude (Hobbs 2000) were like this. The goal is to develop what I have come to call “Deep Lexical Semantics” (Hobbs 2008). It is not enough to decompose “move” into “cause change at.” It is not good enough to simply stipulate these as primitives. We need to explicate these concepts in core theories, a theory of causality, a theory of change of state, and a theory of composite entities and the figure–ground relation. Lexical decompositions have to be anchored in such theories so we can not only decompose meanings but also be able to reason with the decomposed meanings. The structure of the effort is this: We have the predicates corresponding to the morphemes of the language. We have the underlying core theories. And we have axioms defining or characterizing the former in terms of the latter. Thus, in the “range” example, the predicate corresponding to the morpheme. There is a core theory of scales provides the predicates and Axiom (20) is the rule that links the lexical predicate with the core theory. Next I will sketch several very basic core theories and show their utility in defining words for the textual entailment task. 793 Computational Linguistics Volume 39, Number 4 Entities and the Figure–Ground Relation: composite entity is a thing made of other things. This is intended to cover physical objects like a telephone, mixed objects like a book, abstract objects like a theory, and events like a concert. It is characterized by a set of components, a set of properties of the components, a set of relations among its components (the structure), and relations between the entity as a and its environment (including its function). The predicate an external entity, the figure, to a component in a composite entity, the ground. Different figures different grounds give us different meanings for (21) Spatial location: Pat is at the back of the store. (22) Location on a scale: Nuance closed at 58. (23) Membership in an organization: Pat is now at Google. (24) Location in a text: The table is at the end of the article. (25) Time of an event: At that moment, Pat stood up. (26) Event at event: Let’s discuss that at lunch. (27) At a predication: She was at ease in his company. specialized in this way, we tap into a whole vocabulary for talking about the including concepts like of The predication says that state into state Its principal properties are that have an entity in common—a change state is a change of state of States not the same unless there is intermediate state. The predicate defeasibly transitive; in fact, backchaining on the transitivity axiom is one way to refine the granularity on processes. distinguish between the “causal complex” for an effect and the concept “cause.” A causal complex includes all the states and events that have to happen or hold in order for the effect to occur. We say that flipping a switch causes the light to go on. But many other conditions must be in the causal complex—the light bulb can’t be burnt out, the wiring has to be intact, the power has to be on in the city, and so on. The two key properties of a causal complex are that when everything in the causal complex happens or holds, so will the effect, and that everything that is in the causal complex is relevant in a sense that can be made precise. “Causal complex” is a rigorous or monotonic notion, but its utility in everyday life is limited because we almost never can specify everything in it. “Cause” by contrast is a defeasible or nonmonotonic notion. It selects out of a causal complex a particular eventuality that in a sense is the “active” part of the causal complex, the thing that isn’t necessarily normally true. Flipping the switch, in most is the action that light to come on. Causes are the focus of planning, prediction, explanation, and interpreting discourse, but not diagnosis, because in diagnosis, something that normally happens or holds, doesn’t. us now define a few words in terms of these predicates. The verb in lets happen not cause to happen. (28) its most abstract sense means a change of state, as in I go (29) 794 Hobbs Influences and Inferences a set of constraints do that it is not the case that the cause to happen. (30) that to change from being at (31) that there is a change from holding (32) The Recognizing Textual Entailment task is to determine from a text whether a follows from it or not. For example, from the text Filippino hostage in Iraq released would like to be able conclude the hypothesis captors let the hostage Figure 3 illustrates the proof of this entailment relation, using the five axioms we just wrote, together with rules from the core theories saying if something exists, nothing causes it to not exist, and if there is a change from a state, that state no longer holds (Montazeri and Hobbs 2011). The final set of examples I’ll give of Deep Lexical Semantics come from work I have been doing with Andrew Gordon on axiomatizing common-sense psychology, or how we think we think. We have developed approaches to memory, belief, and mutual belief, envisioning causal chains in explanation and prediction, perception and control of the body, and goals and plans. I will focus on goals. We adopt the strong AI position that people are in a sense planning mechanisms. We have goals, we develop plans to achieve these goals, we execute the plans, we monitor the execution, and if things go wrong, we modify our plans and execute the new plans. Figure 3 A textual entailment example. 795 Computational Linguistics Volume 39, Number 4 There are two chief properties of goals. The first says that if an agent has a goal believes then defeasibly that will cause the agent to have a subgoal. The second property is a similar rule for enablement. These are the planning axioms; they generate hierarchical plans. word be explicated in terms of theories of goals and causality. We can distinguish three levels of helping. At the lowest level, inadvertant helping, you help someone when you do an action that is in a causal complex for one of their goals. In this sense, John McCain helped Barack Obama get elected by picking Sarah Palin as his running mate. A second level, intentional helping, is like the first with the addition that the helper performs the action in the service of the helpee’s goal. For example, if I take away a drunk friend’s car keys, I help him survive, a goal of his, but not driving is no part of his plan to survive. The third level, collaborative helping, happens when the helper and helpee engage in a shared plan together, as in helping someone carry a sofa. can define the word as in tries to do as having the goal that E be accomplished, and having it be a goal cause one to accomplish one of its subgoals. To doing E is to try to do E, and to have that trying cause E to be accomplished. do E is to try to do E and have E not happen. Often in comprehending discourse about artifacts we need to know a great deal about the structure and function of the artifact. The theory of goals and planning gives us a handle on this, because normally the structure of an artifact reflects a plan to achieve its functionality, a goal. For example, the function of a coffee cup is to move coffee. We achieve this by breaking it into two subgoals: having a cup contain the coffee and moving the cup. We achieve moving the cup by attaching a handle to the cup and moving the handle. Artifacts are plans made concrete (or in the case of a coffee cup, ceramic). One of the most salient aspects of common-sense psychology is our emotions. What about emotions? Is it possible to formalize a theory of emotions? I think it is, and much of it involves goals. In general, we can characterize emotions in terms of what causes them and what they cause. Emotions, like cognition more generally, mediate between perception and action. Thus, for a particular emotion, we specify an abstract type of perceived eventualities that cause the emotion, and we specify an abstract class of typical responses. This is basically the knowledge we humans need to fake emotions. Consider happiness. Happiness occurs when one’s goals are being satisfied (or sometimes merely when we anticipate that). That must mean that one’s beliefs in the relevant area are working, especially one’s beliefs about what causes what. So one effect of happiness is a higher level of activity—we plan to do more because our planning process is in good working order. A second effect of happiness is that we are not very open to a change of beliefs. If our beliefs are working, why should we change them? Other emotions can be defined in similar terms. Sadness is the opposite of happiness in cause and effects. Fear, anger, and disgust can be seen as various responses to a threat, given the properties of the threat, where a threat is something that will cause one’s goals to be defeated. I’ll close with an obvious question. How long will it be before we are able to automatically analyze texts in the manner I have described? When I wrote my technical analyzing one paragraph of 1976, I thought the answer was that the goal was ten years away. When we began to implement a system based on weighted abduction, I thought the goal was ten years away. So now I will show myself to be consistently optimistic. I think a concerted effort along these lines would yield some measure of success in about ten years.</abstract>
<note confidence="0.950143285714286">796 Hobbs Influences and Inferences References Agar, Michael, and Jerry R. Hobbs. 1982. Interpreting discourse: Coherence and the analysis of ethnographic interviews. 5(1):1–32.</note>
<author confidence="0.8277365">Francois R J Alexandre</author>
<author confidence="0.8277365">Ram Nevatia</author>
<author confidence="0.8277365">Jerry R Hobbs</author>
<author confidence="0.8277365">Robert C Bolles</author>
<abstract confidence="0.7629955">2005. VERL: An ontology framework for representing and annotating events. 12:76–86. Bos, Johan. 2008. Wide-coverage semantic analysis with Boxer. In J. Bos and R.</abstract>
<note confidence="0.926744541666667">editors, in Text Proceedings of STEP 2008 Conference, Research in Computational Semantics, College Publications, Venice, Italy, pages 277–286. Bruce, Bertram C., and Dennis Newman. Interacting plans. 2:195–233. Charniak, Eugene, and Robert Goldman. 1988. A logic for semantic interpretation. of the 26th Annual Meeting of Association for Computational Buffalo, NY, pages 87–94. Cohen, Philip, and C. Raymond Perrault. 1979. Elements of a plan-based theory speech acts. 3(3):177–212. Peter, editor. 1981. Academic Press, New York. Cox, P. T., and T. Pietrzykowski. 1986. Causes for events: Their computation applications. In of Eighth International Conference on Automated pages 608–621,</note>
<address confidence="0.5226305">Oxford. Davidson, Donald. 1967. The logical form of</address>
<email confidence="0.465642">actionsentences.InN.Rescher,editor,</email>
<affiliation confidence="0.84291">Logic of Decision and University</affiliation>
<address confidence="0.750039">of Pittsburgh Press, Pittsburgh, PA,</address>
<note confidence="0.860608709677419">pages 81–95. Gordon, Andrew, and Jerry R. Hobbs. 2004. Formalizations of commonsense 25:49–62. Grice, H. P. 1975. Logic and conversation. P. Cole and J. Morgan, editors, Academic Press, New York, pages 41–58. Joseph. 1975. Thread of Mouton and Company, The Hague, Netherlands. Graeme, 1987. Interpretation the Resolution of Cambridge University Press, Cambridge, UK. Hobbs, Jerry R. 1976. A computational approach to discourse analysis. Research Report 76-2, Department of Computer Sciences, City College, City University of New York, New York, pages 28–38. Hobbs, Jerry R. 2000. Half orders of In of KR-2000 Workshop on Semantic Approximation, and Breckenridge, CO. Hobbs, Jerry R. 2005. Toward a useful concept of causality for lexical semantics. of 22:181–209. Hobbs, Jerry R. 2008. Deep lexical of 9th International Conference on Intelligent Text Processing and Linguistics</note>
<address confidence="0.886687666666667">Haifa, Israel, pages 183–193. Hobbs, Jerry R., Douglas E. Appelt, John Bear, David Israel, Megumi Kameyama,</address>
<abstract confidence="0.71665525">FASTUS: A cascaded finite-state transducer for extracting information from natural-language text. In E. Roche and Schabes, editors. State Devices for</abstract>
<affiliation confidence="0.959696">Language MIT Press,</affiliation>
<address confidence="0.9271745">Cambridge, MA, pages 383–406. Hobbs, Jerry R., and David Andreoff Evans.</address>
<note confidence="0.883942162162162">1980. Conversation as planned behavior. 4(4):349–377. Hobbs, Jerry R., and Feng Pan. 2004. An ontology of time for the Semantic Web. ACM Transactions on Asian Language 3(1):66–85. Hobbs, Jerry R., Mark Stickel, Douglas Appelt, and Paul Martin. 1993. as abduction. 63(1-2):69–142. Inoue, Naoya, and Kentaro Inui. 2011. ILP-based reasoning for weighted In of AAAI Workshop on Plan, Activity and Intent San Francisco, CA, pages 25–32. Lakoff, George, and Mark Johnson. 1980. We Live University of Chicago Press, Chicago. Lewis, David. 1979. Scorekeeping in a game. of Philosophical 6:339–359. Robert. 1976. Anatomy of Speech The Peter de Ridder Press, Ghent, Belguim. Mann, William, and Sandra Thompson. 1986. Relational propositions in discourse. 9(1):57–90. Moldovan, Dan, Sanda Harabagiu, Roxana Girju, Paul Morarescu, Finley Lacatusu, Adrian Novischi, Adriana Badulescu, Orest Bolohan. 2002. LCC tools for answering. In of the Text Retrieval Conference 797 Computational Linguistics Volume 39, Number 4 2002), Gaithersburg, MD. Available at</note>
<web confidence="0.898682">http://trec.nist.gov/pubs/trec11/</web>
<abstract confidence="0.623354">21:1–10. Montazeri, Niloofar, and Jerry R. Hobbs. 2011. Elaborating a knowledge base for lexical semantics. In of the Ninth International Conference on</abstract>
<note confidence="0.6653195">Semantics (IWCS Oxford, England, pages 195–204. Mulkar-Mehta, Rutu, Jerry R. Hobbs, and Eduard Hovy. 2011. Applications and</note>
<abstract confidence="0.424454">discovery of granularity structures in natural language discourse. of the 10th Symposium on Logical Formalizations of Available at http://commonsensereasoning. org/2011/proceedings.html Norvig, Peter. 1987. Inference in text In AAAI-87, Sixth National Conference on pages 561–565,</abstract>
<address confidence="0.537701">Seattle, WA. Ovchinnikova, Ekaterina, Niloofar Montazeri, Teodor Alexandrov, Jerry R. Hobbs, Michael C. McCord, and</address>
<abstract confidence="0.88193475">Rutu Mulkar-Mehta. 2013. Abductive reasoning with a large knowledge base for discourse processing. 4:104–124. Pan, Feng, Rutu Mulkar-Mehta, and Jerry Hobbs. 2011. Annotating and learning durations in text. 37(4):727–752.</abstract>
<author confidence="0.7072195">Abduction</author>
<author confidence="0.7072195">induction In Justus Buchler</author>
<affiliation confidence="0.74408">Writings of Dover Books, New York,</affiliation>
<note confidence="0.85503">pages 150–156. Pollard, Carl, and Ivan A. Sag. 1994.</note>
<title confidence="0.571793">Phrase Structure</title>
<affiliation confidence="0.9296575">University of Chicago Press and CSLI Publications.</affiliation>
<address confidence="0.838436">Pople, Harry E., Jr. 1973, On the</address>
<abstract confidence="0.762509333333333">mechanization of abductive logic. of Third International Joint on Artificial</abstract>
<note confidence="0.848550159090909">pages 147–152, Stanford, California. I. A. 1936. Philosophy of Oxford University Press, Oxford. Rieger, Charles J., III. 1974. Conceptual memory: A theory and computer program for processing the meaning content of natural language utterances. Memo AIM-233, Stanford Artificial Intelligence Laboratory, Stanford University. Naomi. 1981. Language Information Processing: A Computer of English and Its Addison-Wesley, Reading, MA. Santos, Eugene. 1996. Polynomial solvability cost-based abduction. 86:157–170. Schubert, Lenhart K. 2002. Can we derive general world knowledge from texts? of the 2nd International Conference on Human Language Technology (HLT pages 94–97, San Diego, CA. Smith, Kate, Ethel B. Magee, and S. S. Jr. 1928. Grammar: Correct Effective The Athenaeum Press, Ginn and Co., Boston, MA. Thagard, Paul R. 1978. The best explanation: for theory choice. Journal of 75(2):76–92. Thomason, Richmond H. 1985. Accommodation, conversational planning, implicature. In of Workshop on Theoretical Approaches to Natural Halifax, Nova Scotia, pages 117–126. Giambattista, 1968 [1744]. New of Giambattista T. Bergin and M. Frisch, translators. Cornell University Press, Ithaca, NY. Robert. 1983. and Understanding: A Computational Approach to Addison-Wesley, Reading, MA. 798</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michael Agar</author>
<author>Jerry R Hobbs</author>
</authors>
<title>Interpreting discourse: Coherence and the analysis of ethnographic interviews.</title>
<date>1982</date>
<booktitle>Discourse Processes,</booktitle>
<pages>5--1</pages>
<contexts>
<context position="19285" citStr="Agar and Hobbs 1982" startWordPosition="3235" endWordPosition="3238"> external context, and world knowledge. (My term never caught on probably because no one else saw this class of problems as a natural kind.) 786 Hobbs Influences and Inferences Another issue I was thinking about during these years was the structure of discourse, in particular, that structure arising out of coherence relations between discourse segments. In this I was very much influenced by the work of the linguists Joseph Grimes (1975) and Robert Longacre (1976). I began collaborating with the anthropologist Mike Agar around this time, and we called this level of structure “local coherence” (Agar and Hobbs 1982). In the mid-1970s Ray Perrault and Phil Cohen (Cohen and Perrault 1979) at the University of Toronto, later to be my colleagues at SRI, and Chip Bruce (Bruce and Newman 1978) at BBN were doing very exciting work analyzing the structure of discourse as arising out of the speaker’s or writer’s plan, employing formalizations of planning from artificial intelligence. In work with David Evans and work with Mike Agar I tried to apply these insights to the complexities of ordinary conversation and to ethnographic interviews. Agar and I called this level of structure “global coherence.” All along in </context>
</contexts>
<marker>Agar, Hobbs, 1982</marker>
<rawString>Agar, Michael, and Jerry R. Hobbs. 1982. Interpreting discourse: Coherence and the analysis of ethnographic interviews. Discourse Processes, 5(1):1–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francois R J Alexandre</author>
<author>Ram Nevatia</author>
<author>Jerry R Hobbs</author>
<author>Robert C Bolles</author>
</authors>
<title>VERL: An ontology framework for representing and annotating video events.</title>
<date>2005</date>
<journal>IEEE Multimedia,</journal>
<pages>12--76</pages>
<contexts>
<context position="37488" citStr="Alexandre et al. 2005" startWordPosition="6315" endWordPosition="6318">ral knowledge in the next logical domain to attempt. For example, I was able to work with people like George Ferguson, Pat Hayes, and Drew McDermott on developing the so-called “OWL-Time,” a comprehensive ontology of time (Hobbs and Pan 2004), for DARPA’s DAML program on the Semantic Web, and ARDA’s AQUAINT program on question-answering provided the resources for my work with Feng Pan and Rutu Mulkar-Mehta on vague durations of events. Ram Nevatia’s ARDA-sponsored MOVER project provided the opportunity to develop an ontology of event structure called VERL (Video Event Representation Language, Alexandre et al. 2005), and this led to work with Chris Welty, Mike Gruninger, and people at Cycorp on the ARDAsponsored IKRIS project for developing an interlingua among several event and process ontologies. DARPA’s Machine Reading Program supported my student Rutu MulkarMehta’s work on granular or “how-to” causality (Mulkar-Mehta, Hobbs, and Hovy 2011) and Niloofar Montazeri’s work defining or characterizing several hundred common event-related words (Montazeri and Hobbs 2011). My work with Andrew Gordon on encoding common-sense psychology (Gordon and Hobbs 2004) has been funded by various agencies over the years</context>
</contexts>
<marker>Alexandre, Nevatia, Hobbs, Bolles, 2005</marker>
<rawString>Alexandre, Francois R. J., Ram Nevatia, Jerry R. Hobbs, and Robert C. Bolles. 2005. VERL: An ontology framework for representing and annotating video events. IEEE Multimedia, 12:76–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
</authors>
<title>Wide-coverage semantic analysis with Boxer. In</title>
<date>2008</date>
<booktitle>Semantics in Text Processing. Proceedings of STEP 2008 Conference, Research in Computational Semantics, College Publications,</booktitle>
<pages>277--286</pages>
<editor>J. Bos and R. Delmonte, editors,</editor>
<location>Venice, Italy,</location>
<contexts>
<context position="30072" citStr="Bos 2008" startWordPosition="5024" endWordPosition="5025">l plays a role; the predicate computer-terminal doesn’t. All of this raises a question. If the framework is so elegant and so all-encompassing, why isn’t it more widely adopted? I think there are three reasons for this, historically. 1. Parsers were not accurate enough to produce good logical forms from which inference could start. 2. Algorithms for abduction were too inefficient. 3. There was a lack of an adequate knowledge base. Each of these problems has been alleviated somewhat in the past few years. There are now highly accurate statistical parsers, and for several of these (e.g., Boxer; Bos 2008) a component for translating into a flat logical form has been implemented. Recent work by Naoya Inoue and Kentaro Inui (2011) implements weighted abduction as a problem in integer linear programming, building on earlier work by Charniak and Santos (Santos 1996). Our experience with this is that when we switched from a naive backchaining implementation to the ILP implementation, we got a speed-up of two orders of magnitude. Finally, there have been ongoing efforts to build large knowledge bases, manually and automatically, from a number of different perspectives. Efforts to use Cyc for natural</context>
</contexts>
<marker>Bos, 2008</marker>
<rawString>Bos, Johan. 2008. Wide-coverage semantic analysis with Boxer. In J. Bos and R. Delmonte, editors, Semantics in Text Processing. Proceedings of STEP 2008 Conference, Research in Computational Semantics, College Publications, Venice, Italy, pages 277–286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bertram C Bruce</author>
<author>Dennis Newman</author>
</authors>
<title>Interacting plans.</title>
<date>1978</date>
<journal>Cognitive Science,</journal>
<pages>2--195</pages>
<contexts>
<context position="19460" citStr="Bruce and Newman 1978" startWordPosition="3266" endWordPosition="3269">ces Another issue I was thinking about during these years was the structure of discourse, in particular, that structure arising out of coherence relations between discourse segments. In this I was very much influenced by the work of the linguists Joseph Grimes (1975) and Robert Longacre (1976). I began collaborating with the anthropologist Mike Agar around this time, and we called this level of structure “local coherence” (Agar and Hobbs 1982). In the mid-1970s Ray Perrault and Phil Cohen (Cohen and Perrault 1979) at the University of Toronto, later to be my colleagues at SRI, and Chip Bruce (Bruce and Newman 1978) at BBN were doing very exciting work analyzing the structure of discourse as arising out of the speaker’s or writer’s plan, employing formalizations of planning from artificial intelligence. In work with David Evans and work with Mike Agar I tried to apply these insights to the complexities of ordinary conversation and to ethnographic interviews. Agar and I called this level of structure “global coherence.” All along in investigating all three of these problems—local pragmatics, local coherence, and global coherence—it was clear that a key role was played by the notions of implicature (Grice </context>
</contexts>
<marker>Bruce, Newman, 1978</marker>
<rawString>Bruce, Bertram C., and Dennis Newman. 1978. Interacting plans. Cognitive Science, 2:195–233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Robert Goldman</author>
</authors>
<title>A logic for semantic interpretation.</title>
<date>1988</date>
<booktitle>In Proceedings of the 26th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>87--94</pages>
<location>Buffalo, NY,</location>
<contexts>
<context position="20786" citStr="Charniak and Goldman 1988" startWordPosition="3477" endWordPosition="3480">ary problems like pronoun coreference, one had to make assumptions to get a good interpretation of the text, where the only justification for the assumptions was that they led to a good interpretation. In the fall of 1987 at SRI we organized a discussion group on abduction, reading the classic papers by Peirce, recent attempts in AI to use abduction in, for example, medical diagnosis (Pople 1973; Cox and Pietrzykowski 1986), and contemporary philosophers like Paul Thagard (1978), as well as work by Wilensky and Norvig at Berkeley (Wilensky 1983; Norvig 1987) and Charniak and Goldman at Brown (Charniak and Goldman 1988) that seemed to be taking an approach similar to ours. Among the people in our group were Mark Stickel, Doug Edwards, and the pragmatics scholar Steve Levinson, who was visiting Stanford at the time. We argued about what we were calling identity implicatures and referential implicatures, and about how to distinguish new from given information in discourse, and how to choose the best interpretation of a text. Then late one afternoon in October 1987 Mark Stickel came into my office to say that he thought he had the answer to all our problems. He described his algorithm for weighted abduction. It</context>
</contexts>
<marker>Charniak, Goldman, 1988</marker>
<rawString>Charniak, Eugene, and Robert Goldman. 1988. A logic for semantic interpretation. In Proceedings of the 26th Annual Meeting of the Association for Computational Linguistics, Buffalo, NY, pages 87–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Cohen</author>
<author>C Raymond Perrault</author>
</authors>
<title>Elements of a plan-based theory of speech acts.</title>
<date>1979</date>
<journal>Cognitive Science,</journal>
<volume>3</volume>
<issue>3</issue>
<contexts>
<context position="19357" citStr="Cohen and Perrault 1979" startWordPosition="3247" endWordPosition="3250">bably because no one else saw this class of problems as a natural kind.) 786 Hobbs Influences and Inferences Another issue I was thinking about during these years was the structure of discourse, in particular, that structure arising out of coherence relations between discourse segments. In this I was very much influenced by the work of the linguists Joseph Grimes (1975) and Robert Longacre (1976). I began collaborating with the anthropologist Mike Agar around this time, and we called this level of structure “local coherence” (Agar and Hobbs 1982). In the mid-1970s Ray Perrault and Phil Cohen (Cohen and Perrault 1979) at the University of Toronto, later to be my colleagues at SRI, and Chip Bruce (Bruce and Newman 1978) at BBN were doing very exciting work analyzing the structure of discourse as arising out of the speaker’s or writer’s plan, employing formalizations of planning from artificial intelligence. In work with David Evans and work with Mike Agar I tried to apply these insights to the complexities of ordinary conversation and to ethnographic interviews. Agar and I called this level of structure “global coherence.” All along in investigating all three of these problems—local pragmatics, local cohere</context>
</contexts>
<marker>Cohen, Perrault, 1979</marker>
<rawString>Cohen, Philip, and C. Raymond Perrault. 1979. Elements of a plan-based theory of speech acts. Cognitive Science, 3(3):177–212.</rawString>
</citation>
<citation valid="true">
<title>Radical Pragmatics.</title>
<date>1981</date>
<editor>Cole, Peter, editor.</editor>
<publisher>Academic Press,</publisher>
<location>New York.</location>
<marker>1981</marker>
<rawString>Cole, Peter, editor. 1981. Radical Pragmatics. Academic Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P T Cox</author>
<author>T Pietrzykowski</author>
</authors>
<title>Causes for events: Their computation and applications.</title>
<date>1986</date>
<booktitle>In Proceedings of Eighth International Conference on Automated Deduction (CADE-8),</booktitle>
<pages>608--621</pages>
<location>Oxford.</location>
<contexts>
<context position="20587" citStr="Cox and Pietrzykowski 1986" startWordPosition="3446" endWordPosition="3449">, and global coherence—it was clear that a key role was played by the notions of implicature (Grice 1975), accommodation (Lewis 1979; Thomason 1985), and abduction (Peirce 1955). To solve even elementary problems like pronoun coreference, one had to make assumptions to get a good interpretation of the text, where the only justification for the assumptions was that they led to a good interpretation. In the fall of 1987 at SRI we organized a discussion group on abduction, reading the classic papers by Peirce, recent attempts in AI to use abduction in, for example, medical diagnosis (Pople 1973; Cox and Pietrzykowski 1986), and contemporary philosophers like Paul Thagard (1978), as well as work by Wilensky and Norvig at Berkeley (Wilensky 1983; Norvig 1987) and Charniak and Goldman at Brown (Charniak and Goldman 1988) that seemed to be taking an approach similar to ours. Among the people in our group were Mark Stickel, Doug Edwards, and the pragmatics scholar Steve Levinson, who was visiting Stanford at the time. We argued about what we were calling identity implicatures and referential implicatures, and about how to distinguish new from given information in discourse, and how to choose the best interpretation </context>
</contexts>
<marker>Cox, Pietrzykowski, 1986</marker>
<rawString>Cox, P. T., and T. Pietrzykowski. 1986. Causes for events: Their computation and applications. In Proceedings of Eighth International Conference on Automated Deduction (CADE-8), pages 608–621, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Davidson</author>
</authors>
<title>The logical form of action sentences.</title>
<date>1967</date>
<booktitle>The Logic of Decision and Action, University of</booktitle>
<pages>81--95</pages>
<editor>In N. Rescher, editor,</editor>
<publisher>Pittsburgh Press,</publisher>
<location>Pittsburgh, PA,</location>
<contexts>
<context position="11830" citStr="Davidson (1967)" startWordPosition="1994" endWordPosition="1995"> predication the(x,e3). What could that possibly mean? Well, ask what information is being conveyed by the word the. It is a relation between an entity x and a description e3, and it says the entity is uniquely mutually identifiable in context by means of the description. We can give this relation a name. We could call it something like uniquely-mutually-identifiable-in-context. But why not keep it simple, and name the predicate after the morpheme that conveys it – the? Knowledge representation schemes that use extensive reification are often called “Davidsonian,” after the philosopher Donald Davidson (1967), who proposed reifying events. But he balked at reifying states, let alone negations of states and events. He would not have treated Chris’s tallness as a thing. By contrast, I adopted a position that, because I was young and wild, I called “ontological promiscuity.” Now that I’m older and more domesticated, I would probably call it something like “ontological prosperity” or “ontological comfortable circumstances” or maybe “ontological glut.” Many balk at such abandonment of ontological scruples. No doubt I was influenced by the near solipsism that infected many researchers in the early days </context>
</contexts>
<marker>Davidson, 1967</marker>
<rawString>Davidson, Donald. 1967. The logical form of action sentences. In N. Rescher, editor, The Logic of Decision and Action, University of Pittsburgh Press, Pittsburgh, PA, pages 81–95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Gordon</author>
<author>Jerry R Hobbs</author>
</authors>
<title>Formalizations of commonsense psychology.</title>
<date>2004</date>
<journal>AI Magazine,</journal>
<pages>25--49</pages>
<contexts>
<context position="38037" citStr="Gordon and Hobbs 2004" startWordPosition="6396" endWordPosition="6399"> called VERL (Video Event Representation Language, Alexandre et al. 2005), and this led to work with Chris Welty, Mike Gruninger, and people at Cycorp on the ARDAsponsored IKRIS project for developing an interlingua among several event and process ontologies. DARPA’s Machine Reading Program supported my student Rutu MulkarMehta’s work on granular or “how-to” causality (Mulkar-Mehta, Hobbs, and Hovy 2011) and Niloofar Montazeri’s work defining or characterizing several hundred common event-related words (Montazeri and Hobbs 2011). My work with Andrew Gordon on encoding common-sense psychology (Gordon and Hobbs 2004) has been funded by various agencies over the years, most recently by ONR. But some of the research has been “stealth” research—work you don’t tell anyone about until it’s finished for fear your boss will find out and make you work on other stuff. My papers on causality and modality (Hobbs 2005) and on scales and half orders of magnitude (Hobbs 2000) were like this. The goal is to develop what I have come to call “Deep Lexical Semantics” (Hobbs 2008). It is not enough to decompose “move” into “cause - change - at.” It is not good enough to simply stipulate these as primitives. We need to expli</context>
</contexts>
<marker>Gordon, Hobbs, 2004</marker>
<rawString>Gordon, Andrew, and Jerry R. Hobbs. 2004. Formalizations of commonsense psychology. AI Magazine, 25:49–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Grice</author>
</authors>
<title>Logic and conversation.</title>
<date>1975</date>
<pages>41--58</pages>
<editor>In P. Cole and J. Morgan, editors, Syntax and Semantics,</editor>
<publisher>Academic Press,</publisher>
<location>New York,</location>
<contexts>
<context position="20065" citStr="Grice 1975" startWordPosition="3364" endWordPosition="3365"> 1978) at BBN were doing very exciting work analyzing the structure of discourse as arising out of the speaker’s or writer’s plan, employing formalizations of planning from artificial intelligence. In work with David Evans and work with Mike Agar I tried to apply these insights to the complexities of ordinary conversation and to ethnographic interviews. Agar and I called this level of structure “global coherence.” All along in investigating all three of these problems—local pragmatics, local coherence, and global coherence—it was clear that a key role was played by the notions of implicature (Grice 1975), accommodation (Lewis 1979; Thomason 1985), and abduction (Peirce 1955). To solve even elementary problems like pronoun coreference, one had to make assumptions to get a good interpretation of the text, where the only justification for the assumptions was that they led to a good interpretation. In the fall of 1987 at SRI we organized a discussion group on abduction, reading the classic papers by Peirce, recent attempts in AI to use abduction in, for example, medical diagnosis (Pople 1973; Cox and Pietrzykowski 1986), and contemporary philosophers like Paul Thagard (1978), as well as work by W</context>
</contexts>
<marker>Grice, 1975</marker>
<rawString>Grice, H. P. 1975. Logic and conversation. In P. Cole and J. Morgan, editors, Syntax and Semantics, Academic Press, New York, pages 41–58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Grimes</author>
</authors>
<title>The Thread of Discourse.</title>
<date>1975</date>
<publisher>Mouton and Company, The</publisher>
<location>Hague, Netherlands.</location>
<contexts>
<context position="19105" citStr="Grimes (1975)" startWordPosition="3209" endWordPosition="3210">s I called “local pragmatics.” They are problems that are presented within the scope of single sentences, but they often require for their solution the entire discourse, the external context, and world knowledge. (My term never caught on probably because no one else saw this class of problems as a natural kind.) 786 Hobbs Influences and Inferences Another issue I was thinking about during these years was the structure of discourse, in particular, that structure arising out of coherence relations between discourse segments. In this I was very much influenced by the work of the linguists Joseph Grimes (1975) and Robert Longacre (1976). I began collaborating with the anthropologist Mike Agar around this time, and we called this level of structure “local coherence” (Agar and Hobbs 1982). In the mid-1970s Ray Perrault and Phil Cohen (Cohen and Perrault 1979) at the University of Toronto, later to be my colleagues at SRI, and Chip Bruce (Bruce and Newman 1978) at BBN were doing very exciting work analyzing the structure of discourse as arising out of the speaker’s or writer’s plan, employing formalizations of planning from artificial intelligence. In work with David Evans and work with Mike Agar I tr</context>
</contexts>
<marker>Grimes, 1975</marker>
<rawString>Grimes, Joseph. 1975. The Thread of Discourse. Mouton and Company, The Hague, Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graeme Hirst</author>
</authors>
<title>Semantic Interpretation and the Resolution of Ambiguity.</title>
<date>1987</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<contexts>
<context position="28447" citStr="Hirst (1987)" startWordPosition="4768" endWordPosition="4769">n for an individual morpheme is that it is intended to convey its corresponding predication. Thus, the syntactic analysis of a clause bottoms out in its logical form. Now all that remains to be explained is the logical form. It was the original insight of the “Interpretation as Abduction” framework that the best abductive proof (i.e., the best explanation) of the logical form solved the local pragmatics problems as a side effect. 789 Computational Linguistics Volume 39, Number 4 I won’t make an extended argument for that here, but one example should convey the basic idea. The sentence, due to Hirst (1987), (15) The plane taxied to the terminal. has three lexical ambiguities. A plane could be an airplane or a wood-smoother, a terminal could be an airport terminal or a computer terminal, and taxiing could be a plane moving on the ground or a person riding in a cab. We assume we have axioms expressing these possibilities airplane(x) ⊃ plane(x) wood-smoother(x) ⊃ plane(x) airport-terminal(y) ⊃ terminal(y) computer-terminal(y) ⊃ terminal(y) move-on-ground(x,y) ∧ airplane(x) ⊃ taxi(x,y) ride-in-cab(x,y) ∧ person(x) ⊃ taxi(x,y) together with a rule that says airports have airplanes and airport termin</context>
</contexts>
<marker>Hirst, 1987</marker>
<rawString>Hirst, Graeme, 1987. Semantic Interpretation and the Resolution of Ambiguity. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry R Hobbs</author>
</authors>
<title>A computational approach to discourse analysis.</title>
<date>1976</date>
<tech>Research Report 76-2,</tech>
<pages>28--38</pages>
<institution>Department of Computer Sciences, City College, City University of New</institution>
<location>York, New York,</location>
<contexts>
<context position="8431" citStr="Hobbs 1976" startWordPosition="1449" endWordPosition="1450"> within a year, everything else of value that remained of the content of that discussion could be compressed into a long footnote in a technical report. In any case, this conversation lit a fire that fueled my research for the next 15 or 20 years. In particular, I began looking at texts, trying to understand how we understand them. No doubt influenced by Chuck Rieger’s thesis (Rieger 1974), I asked what inferences we draw in the course of comprehension, and, an issue Rieger did not address, what inferences we do not draw. This culminated in 1976 in an unreadable (and unread) technical report (Hobbs 1976), microanalyzing one paragraph from Newsweek, trying to specify every bit of knowledge required for understanding the text and describing how every linguistic problem in the text invokes that knowledge to arrive at solutions. One could say that the rest of my career has been a matter of cleaning up and extending that technical report, in terms of representation, the process of inference and interpretation, and the specification of common-sense knowledge. 2. Representation In 1977 I moved to SRI, where I fell under the influence of Nils Nilsson and Bob Moore, and of John McCarthy at nearby Stan</context>
</contexts>
<marker>Hobbs, 1976</marker>
<rawString>Hobbs, Jerry R. 1976. A computational approach to discourse analysis. Research Report 76-2, Department of Computer Sciences, City College, City University of New York, New York, pages 28–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry R Hobbs</author>
</authors>
<title>Half orders of magnitude.</title>
<date>2000</date>
<booktitle>In Proceedings of KR-2000 Workshop on Semantic Approximation, Granularity, and Vagueness,</booktitle>
<location>Breckenridge, CO.</location>
<contexts>
<context position="38389" citStr="Hobbs 2000" startWordPosition="6460" endWordPosition="6461">usality (Mulkar-Mehta, Hobbs, and Hovy 2011) and Niloofar Montazeri’s work defining or characterizing several hundred common event-related words (Montazeri and Hobbs 2011). My work with Andrew Gordon on encoding common-sense psychology (Gordon and Hobbs 2004) has been funded by various agencies over the years, most recently by ONR. But some of the research has been “stealth” research—work you don’t tell anyone about until it’s finished for fear your boss will find out and make you work on other stuff. My papers on causality and modality (Hobbs 2005) and on scales and half orders of magnitude (Hobbs 2000) were like this. The goal is to develop what I have come to call “Deep Lexical Semantics” (Hobbs 2008). It is not enough to decompose “move” into “cause - change - at.” It is not good enough to simply stipulate these as primitives. We need to explicate these concepts in core theories, a theory of causality, a theory of change of state, and a theory of composite entities and the figure–ground relation. Lexical decompositions have to be anchored in such theories so we can not only decompose meanings but also be able to reason with the decomposed meanings. The structure of the effort is this: We </context>
</contexts>
<marker>Hobbs, 2000</marker>
<rawString>Hobbs, Jerry R. 2000. Half orders of magnitude. In Proceedings of KR-2000 Workshop on Semantic Approximation, Granularity, and Vagueness, Breckenridge, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry R Hobbs</author>
</authors>
<title>Toward a useful concept of causality for lexical semantics.</title>
<date>2005</date>
<journal>Journal of Semantics,</journal>
<pages>22--181</pages>
<contexts>
<context position="38333" citStr="Hobbs 2005" startWordPosition="6450" endWordPosition="6451">udent Rutu MulkarMehta’s work on granular or “how-to” causality (Mulkar-Mehta, Hobbs, and Hovy 2011) and Niloofar Montazeri’s work defining or characterizing several hundred common event-related words (Montazeri and Hobbs 2011). My work with Andrew Gordon on encoding common-sense psychology (Gordon and Hobbs 2004) has been funded by various agencies over the years, most recently by ONR. But some of the research has been “stealth” research—work you don’t tell anyone about until it’s finished for fear your boss will find out and make you work on other stuff. My papers on causality and modality (Hobbs 2005) and on scales and half orders of magnitude (Hobbs 2000) were like this. The goal is to develop what I have come to call “Deep Lexical Semantics” (Hobbs 2008). It is not enough to decompose “move” into “cause - change - at.” It is not good enough to simply stipulate these as primitives. We need to explicate these concepts in core theories, a theory of causality, a theory of change of state, and a theory of composite entities and the figure–ground relation. Lexical decompositions have to be anchored in such theories so we can not only decompose meanings but also be able to reason with the decom</context>
</contexts>
<marker>Hobbs, 2005</marker>
<rawString>Hobbs, Jerry R. 2005. Toward a useful concept of causality for lexical semantics. Journal of Semantics, 22:181–209.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry R Hobbs</author>
</authors>
<title>Deep lexical semantics.</title>
<date>2008</date>
<booktitle>Proceedings of 9th International Conference on Intelligent Text Processing and Computational Linguistics (CICLing-2008),</booktitle>
<pages>183--193</pages>
<location>Haifa,</location>
<contexts>
<context position="38491" citStr="Hobbs 2008" startWordPosition="6479" endWordPosition="6480">several hundred common event-related words (Montazeri and Hobbs 2011). My work with Andrew Gordon on encoding common-sense psychology (Gordon and Hobbs 2004) has been funded by various agencies over the years, most recently by ONR. But some of the research has been “stealth” research—work you don’t tell anyone about until it’s finished for fear your boss will find out and make you work on other stuff. My papers on causality and modality (Hobbs 2005) and on scales and half orders of magnitude (Hobbs 2000) were like this. The goal is to develop what I have come to call “Deep Lexical Semantics” (Hobbs 2008). It is not enough to decompose “move” into “cause - change - at.” It is not good enough to simply stipulate these as primitives. We need to explicate these concepts in core theories, a theory of causality, a theory of change of state, and a theory of composite entities and the figure–ground relation. Lexical decompositions have to be anchored in such theories so we can not only decompose meanings but also be able to reason with the decomposed meanings. The structure of the effort is this: We have the predicates corresponding to the morphemes of the language. We have the underlying core theori</context>
</contexts>
<marker>Hobbs, 2008</marker>
<rawString>Hobbs, Jerry R. 2008. Deep lexical semantics. Proceedings of 9th International Conference on Intelligent Text Processing and Computational Linguistics (CICLing-2008), Haifa, Israel, pages 183–193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry R Hobbs</author>
<author>Douglas E Appelt</author>
<author>John Bear</author>
<author>David Israel</author>
<author>Megumi Kameyama</author>
<author>Mark Stickel</author>
<author>Mabry Tyson</author>
</authors>
<title>FASTUS: A cascaded finite-state transducer for extracting information from natural-language text.</title>
<date>1997</date>
<booktitle>Finite State Devices for Natural Language Processing.</booktitle>
<pages>383--406</pages>
<editor>In E. Roche and Y. Schabes, editors.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA,</location>
<contexts>
<context position="5145" citStr="Hobbs et al. 1997" startWordPosition="891" endWordPosition="894">ret: Naomi Sager’s Linguistic String Project. I think it is also computational linguistics’ best kept secret as well. She was motivated by the science, not by the performance, and her very impressive work is nowhere near as well-known as it should be. I think her Linguistic String Grammar (Sager 1981) ranks, as a computational specification of English syntax, with Pollard and Sag’s Head-driven Phrase Structure Grammar (1994), for thoroughness, insight, and elegance. So, for example, in 1992 when we developed the FASTUS system for information extraction using cascaded finite-state transducers (Hobbs et al. 1997), it was straightforward to copy the rules for Noun Groups straight from her grammar. It’s no accident that in the late 1980s during the Strategic Computing Initiative and in the early 1990s in the Message Understanding Conferences, three of the most important efforts were led by Linguistic String Project alumni—Ralph Grishman’s group at New York University, Lynette Hirschman’s at Unisys, and my group at SRI International. I think the most important lesson I learned from Naomi Sager was to look closely at the data and to take it seriously. My other thesis advisor was Jack Schwartz. He was a po</context>
</contexts>
<marker>Hobbs, Appelt, Bear, Israel, Kameyama, Stickel, Tyson, 1997</marker>
<rawString>Hobbs, Jerry R., Douglas E. Appelt, John Bear, David Israel, Megumi Kameyama, Mark Stickel, and Mabry Tyson. 1997. FASTUS: A cascaded finite-state transducer for extracting information from natural-language text. In E. Roche and Y. Schabes, editors. Finite State Devices for Natural Language Processing. MIT Press, Cambridge, MA, pages 383–406.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry R Hobbs</author>
<author>David Andreoff Evans</author>
</authors>
<title>Conversation as planned behavior.</title>
<date>1980</date>
<journal>Cognitive Science,</journal>
<volume>4</volume>
<issue>4</issue>
<contexts>
<context position="16864" citStr="Hobbs and Evans 1980" startWordPosition="2835" endWordPosition="2838">ad the huge good fortune to participate in a biweekly discussion group on discourse, alternating between Stanford and Berkeley, consisting of some of the most illustrious scholars of language in the world, including Mike Agar, Dwight Bolinger, Eve and Herb Clark, Chuck Fillmore, Paul Kay, George Lakoff, Geoff Nunberg, Ivan Sag, Dan Slobin, Elizabeth Traugott, and Tom Wasow. For me personally, the high point in these meetings, and one of the high points in my entire career, was when the sociologist Irving Goffman, visiting Berkeley at the time, used my paper “Conversation as Planned Behavior” (Hobbs and Evans 1980) as a club to beat the sociolinguist John Gumperz over the head with. Metaphorically speaking. We read and discussed members’ papers on interpreting nominal compounds, metonymy or deferred reference, de-nominalized nouns, metaphor, and other phenomena that came to be clustered by linguists under the name of “Radical Pragmatics” (Cole 1981). (I thought a better name would be “Run-of-the-mill AI”.) Around this time, I was concerned with the problem of how we delimit the set of inferences we draw as we understand a text. The answer that seemed most promising was that we need to draw those inferen</context>
</contexts>
<marker>Hobbs, Evans, 1980</marker>
<rawString>Hobbs, Jerry R., and David Andreoff Evans. 1980. Conversation as planned behavior. Cognitive Science, 4(4):349–377.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry R Hobbs</author>
<author>Feng Pan</author>
</authors>
<title>An ontology of time for the Semantic Web.</title>
<date>2004</date>
<journal>ACM Transactions on Asian Language Information Processing,</journal>
<volume>3</volume>
<issue>1</issue>
<contexts>
<context position="37108" citStr="Hobbs and Pan 2004" startWordPosition="6259" endWordPosition="6262">ts dealing with worn-out air compressors. Then back to the back burner until 1999 or 2000, since which time knowledge encoding has been the principal focus of my research. It is not easy research to get funding for, because its payoff in comparison to building special-purpose applications is very long-term. One has to find short-term applications that would be helped by general knowledge in the next logical domain to attempt. For example, I was able to work with people like George Ferguson, Pat Hayes, and Drew McDermott on developing the so-called “OWL-Time,” a comprehensive ontology of time (Hobbs and Pan 2004), for DARPA’s DAML program on the Semantic Web, and ARDA’s AQUAINT program on question-answering provided the resources for my work with Feng Pan and Rutu Mulkar-Mehta on vague durations of events. Ram Nevatia’s ARDA-sponsored MOVER project provided the opportunity to develop an ontology of event structure called VERL (Video Event Representation Language, Alexandre et al. 2005), and this led to work with Chris Welty, Mike Gruninger, and people at Cycorp on the ARDAsponsored IKRIS project for developing an interlingua among several event and process ontologies. DARPA’s Machine Reading Program s</context>
</contexts>
<marker>Hobbs, Pan, 2004</marker>
<rawString>Hobbs, Jerry R., and Feng Pan. 2004. An ontology of time for the Semantic Web. ACM Transactions on Asian Language Information Processing, 3(1):66–85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry R Hobbs</author>
<author>Mark Stickel</author>
<author>Douglas Appelt</author>
<author>Paul Martin</author>
</authors>
<title>Interpretation as abduction.</title>
<date>1993</date>
<journal>Artificial Intelligence,</journal>
<pages>63--1</pages>
<contexts>
<context position="22241" citStr="Hobbs et al. 1993" startWordPosition="3724" endWordPosition="3727">gave us a clear criterion for what inferences to draw and not draw. The interpretation was the most economical explanation for what would make the text true, and an inference was appropriate if and only if it contributed to that explanation. On my way home that night, I began driving a little more carefully. In the next few days, I saw how one would approach all the local pragmatics and local and global coherence problems in this framework. In discussions with Stu Shieber in the next few days it became apparent how one could integrate syntax smoothly into the framework. A big picture emerged (Hobbs et al. 1993). In the early 1990s I saw an advertisement in a magazine for Polaroid cameras (quite obsolete now). It showed a man standing by the ocean, holding a camera, and looking at a scene in which the branch of a tree is on the ground and a small boat is stuck in the top of another tree. When we see this, we immediately interpret it by coming up with the best explanation for the observables (abduction). There was a storm that blew the branch down and blew the boat into the tree. There are other 787 Computational Linguistics Volume 39, Number 4 possible explanations. Maybe someone chopped the branch d</context>
</contexts>
<marker>Hobbs, Stickel, Appelt, Martin, 1993</marker>
<rawString>Hobbs, Jerry R., Mark Stickel, Douglas Appelt, and Paul Martin. 1993. Interpretation as abduction. Artificial Intelligence, 63(1-2):69–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naoya Inoue</author>
<author>Kentaro Inui</author>
</authors>
<title>ILP-based reasoning for weighted abduction.</title>
<date>2011</date>
<booktitle>In Proceedings of AAAI Workshop on Plan, Activity and Intent Recognition,</booktitle>
<pages>25--32</pages>
<location>San Francisco, CA,</location>
<marker>Inoue, Inui, 2011</marker>
<rawString>Inoue, Naoya, and Kentaro Inui. 2011. ILP-based reasoning for weighted abduction. In Proceedings of AAAI Workshop on Plan, Activity and Intent Recognition, San Francisco, CA, pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Lakoff</author>
<author>Mark Johnson</author>
</authors>
<title>Metaphors We Live By.</title>
<date>1980</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago.</location>
<marker>Lakoff, Johnson, 1980</marker>
<rawString>Lakoff, George, and Mark Johnson. 1980. Metaphors We Live By. University of Chicago Press, Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Lewis</author>
</authors>
<title>Scorekeeping in a language game.</title>
<date>1979</date>
<journal>Journal of Philosophical Logic,</journal>
<pages>6--339</pages>
<contexts>
<context position="20092" citStr="Lewis 1979" startWordPosition="3367" endWordPosition="3368">y exciting work analyzing the structure of discourse as arising out of the speaker’s or writer’s plan, employing formalizations of planning from artificial intelligence. In work with David Evans and work with Mike Agar I tried to apply these insights to the complexities of ordinary conversation and to ethnographic interviews. Agar and I called this level of structure “global coherence.” All along in investigating all three of these problems—local pragmatics, local coherence, and global coherence—it was clear that a key role was played by the notions of implicature (Grice 1975), accommodation (Lewis 1979; Thomason 1985), and abduction (Peirce 1955). To solve even elementary problems like pronoun coreference, one had to make assumptions to get a good interpretation of the text, where the only justification for the assumptions was that they led to a good interpretation. In the fall of 1987 at SRI we organized a discussion group on abduction, reading the classic papers by Peirce, recent attempts in AI to use abduction in, for example, medical diagnosis (Pople 1973; Cox and Pietrzykowski 1986), and contemporary philosophers like Paul Thagard (1978), as well as work by Wilensky and Norvig at Berke</context>
</contexts>
<marker>Lewis, 1979</marker>
<rawString>Lewis, David. 1979. Scorekeeping in a language game. Journal of Philosophical Logic, 6:339–359.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Longacre</author>
</authors>
<title>An Anatomy of Speech Notions. The Peter de Ridder Press,</title>
<date>1976</date>
<location>Ghent, Belguim.</location>
<contexts>
<context position="19132" citStr="Longacre (1976)" startWordPosition="3213" endWordPosition="3214">ics.” They are problems that are presented within the scope of single sentences, but they often require for their solution the entire discourse, the external context, and world knowledge. (My term never caught on probably because no one else saw this class of problems as a natural kind.) 786 Hobbs Influences and Inferences Another issue I was thinking about during these years was the structure of discourse, in particular, that structure arising out of coherence relations between discourse segments. In this I was very much influenced by the work of the linguists Joseph Grimes (1975) and Robert Longacre (1976). I began collaborating with the anthropologist Mike Agar around this time, and we called this level of structure “local coherence” (Agar and Hobbs 1982). In the mid-1970s Ray Perrault and Phil Cohen (Cohen and Perrault 1979) at the University of Toronto, later to be my colleagues at SRI, and Chip Bruce (Bruce and Newman 1978) at BBN were doing very exciting work analyzing the structure of discourse as arising out of the speaker’s or writer’s plan, employing formalizations of planning from artificial intelligence. In work with David Evans and work with Mike Agar I tried to apply these insights</context>
</contexts>
<marker>Longacre, 1976</marker>
<rawString>Longacre, Robert. 1976. An Anatomy of Speech Notions. The Peter de Ridder Press, Ghent, Belguim.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Mann</author>
<author>Sandra Thompson</author>
</authors>
<title>Relational propositions in discourse.</title>
<date>1986</date>
<booktitle>Discourse Processes,</booktitle>
<pages>9--1</pages>
<contexts>
<context position="26730" citStr="Mann and Thompson 1986" startWordPosition="4492" endWordPosition="4495"> breaking it into parts, explaining the parts, and explaining the relation between them. The possible coherence relations are just the sort of relations that frequently obtain between two states or events: causality, similarity, identity, a strong sort of temporal succession I have called “occasion,” the figure–ground relation, and predicate–argument relations. These are similar to other catalogues of discourse relations that others have come up with. However, the intent is to capture the information that can be conveyed by adjacency. By contrast, the relations of Rhetorical Structure Theory (Mann and Thompson 1986) are a mixture of informational relations like similarity and intentional relations like justification. The first is what is conveyed by adjacency; the second is what the speaker is using adjacency to do. Often the coherence relation conveyed by adjacency is expressed redundantly (and with less ambiguity) in a conjunction (so), an adverb (consequently), or a referential expression (That made ... ). This does not pose a problem, assuming the two do not conflict; discourse is rife with redundancy. Decomposition of a discourse in this fashion yields a tree or tree-like structure. It bottoms out i</context>
</contexts>
<marker>Mann, Thompson, 1986</marker>
<rawString>Mann, William, and Sandra Thompson. 1986. Relational propositions in discourse. Discourse Processes, 9(1):57–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Moldovan</author>
<author>Sanda Harabagiu</author>
<author>Roxana Girju</author>
<author>Paul Morarescu</author>
<author>Finley Lacatusu</author>
<author>Adrian Novischi</author>
<author>Adriana Badulescu</author>
<author>Orest Bolohan</author>
</authors>
<title>LCC tools for question answering.</title>
<date>2002</date>
<booktitle>In Proceedings of the Eleventh Text Retrieval Conference (TREC Computational Linguistics Volume 39, Number</booktitle>
<volume>4</volume>
<pages>21--1</pages>
<location>Gaithersburg, MD.</location>
<note>Available at http://trec.nist.gov/pubs/trec11/ t11 proceedings.html,</note>
<marker>Moldovan, Harabagiu, Girju, Morarescu, Lacatusu, Novischi, Badulescu, Bolohan, 2002</marker>
<rawString>Moldovan, Dan, Sanda Harabagiu, Roxana Girju, Paul Morarescu, Finley Lacatusu, Adrian Novischi, Adriana Badulescu, Orest Bolohan. 2002. LCC tools for question answering. In Proceedings of the Eleventh Text Retrieval Conference (TREC Computational Linguistics Volume 39, Number 4 2002), Gaithersburg, MD. Available at http://trec.nist.gov/pubs/trec11/ t11 proceedings.html, 21:1–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Niloofar Montazeri</author>
<author>Jerry R Hobbs</author>
</authors>
<title>Elaborating a knowledge base for deep lexical semantics.</title>
<date>2011</date>
<booktitle>In Proceedings of the Ninth International Conference on Computational Semantics (IWCS 2011),</booktitle>
<pages>195--204</pages>
<location>Oxford, England,</location>
<contexts>
<context position="37949" citStr="Montazeri and Hobbs 2011" startWordPosition="6383" endWordPosition="6386">-sponsored MOVER project provided the opportunity to develop an ontology of event structure called VERL (Video Event Representation Language, Alexandre et al. 2005), and this led to work with Chris Welty, Mike Gruninger, and people at Cycorp on the ARDAsponsored IKRIS project for developing an interlingua among several event and process ontologies. DARPA’s Machine Reading Program supported my student Rutu MulkarMehta’s work on granular or “how-to” causality (Mulkar-Mehta, Hobbs, and Hovy 2011) and Niloofar Montazeri’s work defining or characterizing several hundred common event-related words (Montazeri and Hobbs 2011). My work with Andrew Gordon on encoding common-sense psychology (Gordon and Hobbs 2004) has been funded by various agencies over the years, most recently by ONR. But some of the research has been “stealth” research—work you don’t tell anyone about until it’s finished for fear your boss will find out and make you work on other stuff. My papers on causality and modality (Hobbs 2005) and on scales and half orders of magnitude (Hobbs 2000) were like this. The goal is to develop what I have come to call “Deep Lexical Semantics” (Hobbs 2008). It is not enough to decompose “move” into “cause - chang</context>
<context position="44003" citStr="Montazeri and Hobbs 2011" startWordPosition="7458" endWordPosition="7461"> release&apos;(e1, x, y, z) - change&apos;(e1, e2, e3) n hold&apos;(e2, x, y, z) The Recognizing Textual Entailment task is to determine from a text whether a hypothesis follows from it or not. For example, from the text A Filippino hostage in Iraq was released we would like to be able conclude the hypothesis The captors let the hostage go free. Figure 3 illustrates the proof of this entailment relation, using the five axioms we just wrote, together with rules from the core theories saying if something exists, nothing causes it to not exist, and if there is a change from a state, that state no longer holds (Montazeri and Hobbs 2011). The final set of examples I’ll give of Deep Lexical Semantics come from work I have been doing with Andrew Gordon on axiomatizing common-sense psychology, or how we think we think. We have developed approaches to memory, belief, and mutual belief, envisioning causal chains in explanation and prediction, perception and control of the body, and goals and plans. I will focus on goals. We adopt the strong AI position that people are in a sense planning mechanisms. We have goals, we develop plans to achieve these goals, we execute the plans, we monitor the execution, and if things go wrong, we mo</context>
</contexts>
<marker>Montazeri, Hobbs, 2011</marker>
<rawString>Montazeri, Niloofar, and Jerry R. Hobbs. 2011. Elaborating a knowledge base for deep lexical semantics. In Proceedings of the Ninth International Conference on Computational Semantics (IWCS 2011), Oxford, England, pages 195–204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rutu Mulkar-Mehta</author>
<author>Jerry R Hobbs</author>
<author>Eduard Hovy</author>
</authors>
<title>Applications and discovery of granularity structures in natural language discourse.</title>
<date>2011</date>
<marker>Mulkar-Mehta, Hobbs, Hovy, 2011</marker>
<rawString>Mulkar-Mehta, Rutu, Jerry R. Hobbs, and Eduard Hovy. 2011. Applications and discovery of granularity structures in natural language discourse.</rawString>
</citation>
<citation valid="false">
<booktitle>In Proceedings of the 10th Symposium on Logical Formalizations of Commonsense Reasoning. Available at http://commonsensereasoning. org/2011/proceedings.html</booktitle>
<marker></marker>
<rawString>In Proceedings of the 10th Symposium on Logical Formalizations of Commonsense Reasoning. Available at http://commonsensereasoning. org/2011/proceedings.html</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Norvig</author>
</authors>
<title>Inference in text understanding.</title>
<date>1987</date>
<booktitle>In Proceedings, AAAI-87, Sixth National Conference on Artificial Intelligence,</booktitle>
<pages>561--565</pages>
<location>Seattle, WA.</location>
<contexts>
<context position="20724" citStr="Norvig 1987" startWordPosition="3469" endWordPosition="3470">d abduction (Peirce 1955). To solve even elementary problems like pronoun coreference, one had to make assumptions to get a good interpretation of the text, where the only justification for the assumptions was that they led to a good interpretation. In the fall of 1987 at SRI we organized a discussion group on abduction, reading the classic papers by Peirce, recent attempts in AI to use abduction in, for example, medical diagnosis (Pople 1973; Cox and Pietrzykowski 1986), and contemporary philosophers like Paul Thagard (1978), as well as work by Wilensky and Norvig at Berkeley (Wilensky 1983; Norvig 1987) and Charniak and Goldman at Brown (Charniak and Goldman 1988) that seemed to be taking an approach similar to ours. Among the people in our group were Mark Stickel, Doug Edwards, and the pragmatics scholar Steve Levinson, who was visiting Stanford at the time. We argued about what we were calling identity implicatures and referential implicatures, and about how to distinguish new from given information in discourse, and how to choose the best interpretation of a text. Then late one afternoon in October 1987 Mark Stickel came into my office to say that he thought he had the answer to all our p</context>
</contexts>
<marker>Norvig, 1987</marker>
<rawString>Norvig, Peter. 1987. Inference in text understanding. In Proceedings, AAAI-87, Sixth National Conference on Artificial Intelligence, pages 561–565, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ekaterina Ovchinnikova</author>
<author>Niloofar Montazeri</author>
<author>Teodor Alexandrov</author>
<author>Jerry R Hobbs</author>
<author>Michael C McCord</author>
<author>Rutu Mulkar-Mehta</author>
</authors>
<title>Abductive reasoning with a large knowledge base for discourse processing. Computing Meaning,</title>
<date>2013</date>
<pages>4--104</pages>
<contexts>
<context position="31641" citStr="Ovchinnikova et al. (2013)" startWordPosition="5269" endWordPosition="5272">nto logical axioms, and reported success with its use in question790 Hobbs Influences and Inferences Logical Form: plane(x) n taxi(x, y) n terminal(y) Knowledge Base: ■❅ ♦ ❙ airplane(x) D plane(x) ❅ ❙ ✻ ❅ ❙ ❅ ❙ ❅ ❙ ❅ ❙ ❅ ❙ ❅ ❙ n airplane(x) D taxi(x, y) ❙ ❙ ❙ airport-terminal(y) D terminal(y) ✻ move-on-ground(x, y) ✚✚✚✚✚✚✚❃ airport(z) D airplane(x) n airport-terminal(y) wood-smoother(x) D plane(x) ride-in-cab(x,y) n person(x) D taxi(x,y) computer-terminal(y) D terminal(y) Figure 2 Interpretation of The plane taxied to the terminal. answering. FrameNet has been converted into logical axioms by Ovchinnikova et al. (2013), and she and her colleagues have shown that an abduction engine using a knowledge base derived from these sources is competitive with the best of the statistical systems in textual entailment and semantic role labeling. My own particular take on building a knowledge base for inferential NLP is described in the next section. 4. Knowledge We understand discourse so well because we know so much. Thus, one of the central problems in the study of language is how we use our knowledge of language and the world to interpret discourse. This breaks into two subproblems: 1. How do we encode the common-s</context>
</contexts>
<marker>Ovchinnikova, Montazeri, Alexandrov, Hobbs, McCord, Mulkar-Mehta, 2013</marker>
<rawString>Ovchinnikova, Ekaterina, Niloofar Montazeri, Teodor Alexandrov, Jerry R. Hobbs, Michael C. McCord, and Rutu Mulkar-Mehta. 2013. Abductive reasoning with a large knowledge base for discourse processing. Computing Meaning, 4:104–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Feng Pan</author>
<author>Rutu Mulkar-Mehta</author>
<author>Jerry Hobbs</author>
</authors>
<title>Annotating and learning event durations in text.</title>
<date>2011</date>
<journal>Computational Linguistics,</journal>
<volume>37</volume>
<issue>4</issue>
<marker>Pan, Mulkar-Mehta, Hobbs, 2011</marker>
<rawString>Pan, Feng, Rutu Mulkar-Mehta, and Jerry Hobbs. 2011. Annotating and learning event durations in text. Computational Linguistics, 37(4):727–752.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Sanders Peirce</author>
</authors>
<title>Abduction and induction.</title>
<date>1955</date>
<booktitle>Philosophical Writings of Peirce,</booktitle>
<pages>150--156</pages>
<editor>In Justus Buchler, editor,</editor>
<publisher>Books,</publisher>
<location>Dover</location>
<contexts>
<context position="20137" citStr="Peirce 1955" startWordPosition="3373" endWordPosition="3374">discourse as arising out of the speaker’s or writer’s plan, employing formalizations of planning from artificial intelligence. In work with David Evans and work with Mike Agar I tried to apply these insights to the complexities of ordinary conversation and to ethnographic interviews. Agar and I called this level of structure “global coherence.” All along in investigating all three of these problems—local pragmatics, local coherence, and global coherence—it was clear that a key role was played by the notions of implicature (Grice 1975), accommodation (Lewis 1979; Thomason 1985), and abduction (Peirce 1955). To solve even elementary problems like pronoun coreference, one had to make assumptions to get a good interpretation of the text, where the only justification for the assumptions was that they led to a good interpretation. In the fall of 1987 at SRI we organized a discussion group on abduction, reading the classic papers by Peirce, recent attempts in AI to use abduction in, for example, medical diagnosis (Pople 1973; Cox and Pietrzykowski 1986), and contemporary philosophers like Paul Thagard (1978), as well as work by Wilensky and Norvig at Berkeley (Wilensky 1983; Norvig 1987) and Charniak</context>
</contexts>
<marker>Peirce, 1955</marker>
<rawString>Peirce, Charles Sanders. 1955. Abduction and induction. In Justus Buchler, editor, Philosophical Writings of Peirce, Dover Books, New York, pages 150–156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan A Sag</author>
</authors>
<title>Head-Driven Phrase Structure Grammar.</title>
<date>1994</date>
<publisher>University of Chicago Press and CSLI Publications.</publisher>
<marker>Pollard, Sag, 1994</marker>
<rawString>Pollard, Carl, and Ivan A. Sag. 1994. Head-Driven Phrase Structure Grammar. University of Chicago Press and CSLI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harry E Pople</author>
</authors>
<title>On the mechanization of abductive logic.</title>
<date>1973</date>
<booktitle>In Proceedings of Third International Joint Conference on Artificial Intelligence,</booktitle>
<pages>147--152</pages>
<location>Stanford, California.</location>
<contexts>
<context position="20558" citStr="Pople 1973" startWordPosition="3444" endWordPosition="3445">al coherence, and global coherence—it was clear that a key role was played by the notions of implicature (Grice 1975), accommodation (Lewis 1979; Thomason 1985), and abduction (Peirce 1955). To solve even elementary problems like pronoun coreference, one had to make assumptions to get a good interpretation of the text, where the only justification for the assumptions was that they led to a good interpretation. In the fall of 1987 at SRI we organized a discussion group on abduction, reading the classic papers by Peirce, recent attempts in AI to use abduction in, for example, medical diagnosis (Pople 1973; Cox and Pietrzykowski 1986), and contemporary philosophers like Paul Thagard (1978), as well as work by Wilensky and Norvig at Berkeley (Wilensky 1983; Norvig 1987) and Charniak and Goldman at Brown (Charniak and Goldman 1988) that seemed to be taking an approach similar to ours. Among the people in our group were Mark Stickel, Doug Edwards, and the pragmatics scholar Steve Levinson, who was visiting Stanford at the time. We argued about what we were calling identity implicatures and referential implicatures, and about how to distinguish new from given information in discourse, and how to ch</context>
</contexts>
<marker>Pople, 1973</marker>
<rawString>Pople, Harry E., Jr. 1973, On the mechanization of abductive logic. In Proceedings of Third International Joint Conference on Artificial Intelligence, pages 147–152, Stanford, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I A Richards</author>
</authors>
<title>The Philosophy of Rhetoric,</title>
<date>1936</date>
<publisher>University Press,</publisher>
<location>Oxford</location>
<marker>Richards, 1936</marker>
<rawString>Richards, I. A. 1936. The Philosophy of Rhetoric, Oxford University Press, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles J Rieger</author>
</authors>
<title>Conceptual memory: A theory and computer program for processing the meaning content of natural language utterances. Memo AIM-233,</title>
<date>1974</date>
<institution>Stanford Artificial Intelligence Laboratory, Stanford University.</institution>
<contexts>
<context position="8212" citStr="Rieger 1974" startWordPosition="1411" endWordPosition="1412"> (This was before Lakoff and Johnson [1980], but after similar observations by the 18th-century Italian philosopher Giambattista Vico (1968 [1744]) and the 20th-century English literary critic I. A. Richards [1936].) But within a year, everything else of value that remained of the content of that discussion could be compressed into a long footnote in a technical report. In any case, this conversation lit a fire that fueled my research for the next 15 or 20 years. In particular, I began looking at texts, trying to understand how we understand them. No doubt influenced by Chuck Rieger’s thesis (Rieger 1974), I asked what inferences we draw in the course of comprehension, and, an issue Rieger did not address, what inferences we do not draw. This culminated in 1976 in an unreadable (and unread) technical report (Hobbs 1976), microanalyzing one paragraph from Newsweek, trying to specify every bit of knowledge required for understanding the text and describing how every linguistic problem in the text invokes that knowledge to arrive at solutions. One could say that the rest of my career has been a matter of cleaning up and extending that technical report, in terms of representation, the process of i</context>
</contexts>
<marker>Rieger, 1974</marker>
<rawString>Rieger, Charles J., III. 1974. Conceptual memory: A theory and computer program for processing the meaning content of natural language utterances. Memo AIM-233, Stanford Artificial Intelligence Laboratory, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naomi Sager</author>
</authors>
<title>Natural Language Information Processing: A Computer Grammar of English and Its Applications.</title>
<date>1981</date>
<publisher>Addison-Wesley,</publisher>
<location>Reading, MA.</location>
<contexts>
<context position="4829" citStr="Sager 1981" startWordPosition="848" endWordPosition="849">elled to include in his next edition. As I surveyed vaguely plausible fields, I realized I had no idea what the next problem to solve would be or even what makes a problem interesting. Then in April, when I had nearly resigned myself to becoming a taxi driver, I discovered New York University’s best-kept secret: Naomi Sager’s Linguistic String Project. I think it is also computational linguistics’ best kept secret as well. She was motivated by the science, not by the performance, and her very impressive work is nowhere near as well-known as it should be. I think her Linguistic String Grammar (Sager 1981) ranks, as a computational specification of English syntax, with Pollard and Sag’s Head-driven Phrase Structure Grammar (1994), for thoroughness, insight, and elegance. So, for example, in 1992 when we developed the FASTUS system for information extraction using cascaded finite-state transducers (Hobbs et al. 1997), it was straightforward to copy the rules for Noun Groups straight from her grammar. It’s no accident that in the late 1980s during the Strategic Computing Initiative and in the early 1990s in the Message Understanding Conferences, three of the most important efforts were led by Lin</context>
</contexts>
<marker>Sager, 1981</marker>
<rawString>Sager, Naomi. 1981. Natural Language Information Processing: A Computer Grammar of English and Its Applications. Addison-Wesley, Reading, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Santos</author>
</authors>
<title>Polynomial solvability of cost-based abduction.</title>
<date>1996</date>
<journal>Artificial Intelligence,</journal>
<pages>86--157</pages>
<contexts>
<context position="30334" citStr="Santos 1996" startWordPosition="5066" endWordPosition="5067">ate enough to produce good logical forms from which inference could start. 2. Algorithms for abduction were too inefficient. 3. There was a lack of an adequate knowledge base. Each of these problems has been alleviated somewhat in the past few years. There are now highly accurate statistical parsers, and for several of these (e.g., Boxer; Bos 2008) a component for translating into a flat logical form has been implemented. Recent work by Naoya Inoue and Kentaro Inui (2011) implements weighted abduction as a problem in integer linear programming, building on earlier work by Charniak and Santos (Santos 1996). Our experience with this is that when we switched from a naive backchaining implementation to the ILP implementation, we got a speed-up of two orders of magnitude. Finally, there have been ongoing efforts to build large knowledge bases, manually and automatically, from a number of different perspectives. Efforts to use Cyc for natural language processing applications have had mixed success at best. But Schubert’s efforts (2002) to build a knowledge base by analyzing language use looks very promising. Some applications have attempted to use OpenMind. WordNet hierarchies are used very widely a</context>
</contexts>
<marker>Santos, 1996</marker>
<rawString>Santos, Eugene. 1996. Polynomial solvability of cost-based abduction. Artificial Intelligence, 86:157–170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lenhart K Schubert</author>
</authors>
<title>Can we derive general world knowledge from texts?</title>
<date>2002</date>
<booktitle>In Proceedings of the 2nd International Conference on Human Language Technology Research (HLT</booktitle>
<pages>94--97</pages>
<location>San Diego, CA.</location>
<marker>Schubert, 2002</marker>
<rawString>Schubert, Lenhart K. 2002. Can we derive general world knowledge from texts? In Proceedings of the 2nd International Conference on Human Language Technology Research (HLT 2002), pages 94–97, San Diego, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kate Smith</author>
<author>Ethel B Magee</author>
<author>S S Seward</author>
</authors>
<title>English Grammar: Correct and Effective Use. The Athenaeum Press, Ginn and Co.,</title>
<date>1928</date>
<location>Boston, MA.</location>
<marker>Smith, Magee, Seward, 1928</marker>
<rawString>Smith, Kate, Ethel B. Magee, and S. S. Seward, Jr. 1928. English Grammar: Correct and Effective Use. The Athenaeum Press, Ginn and Co., Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul R Thagard</author>
</authors>
<title>The best explanation: Criteria for theory choice.</title>
<date>1978</date>
<journal>The Journal of Philosophy,</journal>
<volume>75</volume>
<issue>2</issue>
<contexts>
<context position="20643" citStr="Thagard (1978)" startWordPosition="3455" endWordPosition="3456"> notions of implicature (Grice 1975), accommodation (Lewis 1979; Thomason 1985), and abduction (Peirce 1955). To solve even elementary problems like pronoun coreference, one had to make assumptions to get a good interpretation of the text, where the only justification for the assumptions was that they led to a good interpretation. In the fall of 1987 at SRI we organized a discussion group on abduction, reading the classic papers by Peirce, recent attempts in AI to use abduction in, for example, medical diagnosis (Pople 1973; Cox and Pietrzykowski 1986), and contemporary philosophers like Paul Thagard (1978), as well as work by Wilensky and Norvig at Berkeley (Wilensky 1983; Norvig 1987) and Charniak and Goldman at Brown (Charniak and Goldman 1988) that seemed to be taking an approach similar to ours. Among the people in our group were Mark Stickel, Doug Edwards, and the pragmatics scholar Steve Levinson, who was visiting Stanford at the time. We argued about what we were calling identity implicatures and referential implicatures, and about how to distinguish new from given information in discourse, and how to choose the best interpretation of a text. Then late one afternoon in October 1987 Mark </context>
</contexts>
<marker>Thagard, 1978</marker>
<rawString>Thagard, Paul R. 1978. The best explanation: Criteria for theory choice. The Journal of Philosophy, 75(2):76–92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richmond H Thomason</author>
</authors>
<title>Accommodation, conversational planning, and implicature.</title>
<date>1985</date>
<booktitle>In Proceedings of Workshop on Theoretical Approaches to Natural Language Understanding,</booktitle>
<pages>117--126</pages>
<location>Halifax, Nova Scotia,</location>
<contexts>
<context position="20108" citStr="Thomason 1985" startWordPosition="3369" endWordPosition="3370">ork analyzing the structure of discourse as arising out of the speaker’s or writer’s plan, employing formalizations of planning from artificial intelligence. In work with David Evans and work with Mike Agar I tried to apply these insights to the complexities of ordinary conversation and to ethnographic interviews. Agar and I called this level of structure “global coherence.” All along in investigating all three of these problems—local pragmatics, local coherence, and global coherence—it was clear that a key role was played by the notions of implicature (Grice 1975), accommodation (Lewis 1979; Thomason 1985), and abduction (Peirce 1955). To solve even elementary problems like pronoun coreference, one had to make assumptions to get a good interpretation of the text, where the only justification for the assumptions was that they led to a good interpretation. In the fall of 1987 at SRI we organized a discussion group on abduction, reading the classic papers by Peirce, recent attempts in AI to use abduction in, for example, medical diagnosis (Pople 1973; Cox and Pietrzykowski 1986), and contemporary philosophers like Paul Thagard (1978), as well as work by Wilensky and Norvig at Berkeley (Wilensky 19</context>
</contexts>
<marker>Thomason, 1985</marker>
<rawString>Thomason, Richmond H. 1985. Accommodation, conversational planning, and implicature. In Proceedings of Workshop on Theoretical Approaches to Natural Language Understanding, Halifax, Nova Scotia, pages 117–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giambattista Vico</author>
</authors>
<title>[1744]. The New Science of</title>
<date>1968</date>
<publisher>Cornell University Press,</publisher>
<location>Ithaca, NY.</location>
<contexts>
<context position="7739" citStr="Vico (1968" startWordPosition="1331" endWordPosition="1332">ething I still believe. But that left me adrift for problems to work on. I became discouraged, and found myself thinking again about driving that taxi. Then late one afternoon, just as I was about to go home, a graduate student named Fred Howard came into my office to ask a couple of questions. That triggered a discussion that lasted until 11 o’clock that evening. One of the wheels we reinvented was a recognition of the pervasiveness of spatial metaphor in discourse. (This was before Lakoff and Johnson [1980], but after similar observations by the 18th-century Italian philosopher Giambattista Vico (1968 [1744]) and the 20th-century English literary critic I. A. Richards [1936].) But within a year, everything else of value that remained of the content of that discussion could be compressed into a long footnote in a technical report. In any case, this conversation lit a fire that fueled my research for the next 15 or 20 years. In particular, I began looking at texts, trying to understand how we understand them. No doubt influenced by Chuck Rieger’s thesis (Rieger 1974), I asked what inferences we draw in the course of comprehension, and, an issue Rieger did not address, what inferences we do n</context>
</contexts>
<marker>Vico, 1968</marker>
<rawString>Vico, Giambattista, 1968 [1744]. The New Science of Giambattista Vico, T. Bergin and M. Frisch, translators. Cornell University Press, Ithaca, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Wilensky</author>
</authors>
<title>Planning and Understanding: A Computational Approach to Human Reasoning.</title>
<date>1983</date>
<publisher>Addison-Wesley,</publisher>
<location>Reading, MA.</location>
<contexts>
<context position="20710" citStr="Wilensky 1983" startWordPosition="3467" endWordPosition="3468">mason 1985), and abduction (Peirce 1955). To solve even elementary problems like pronoun coreference, one had to make assumptions to get a good interpretation of the text, where the only justification for the assumptions was that they led to a good interpretation. In the fall of 1987 at SRI we organized a discussion group on abduction, reading the classic papers by Peirce, recent attempts in AI to use abduction in, for example, medical diagnosis (Pople 1973; Cox and Pietrzykowski 1986), and contemporary philosophers like Paul Thagard (1978), as well as work by Wilensky and Norvig at Berkeley (Wilensky 1983; Norvig 1987) and Charniak and Goldman at Brown (Charniak and Goldman 1988) that seemed to be taking an approach similar to ours. Among the people in our group were Mark Stickel, Doug Edwards, and the pragmatics scholar Steve Levinson, who was visiting Stanford at the time. We argued about what we were calling identity implicatures and referential implicatures, and about how to distinguish new from given information in discourse, and how to choose the best interpretation of a text. Then late one afternoon in October 1987 Mark Stickel came into my office to say that he thought he had the answe</context>
</contexts>
<marker>Wilensky, 1983</marker>
<rawString>Wilensky, Robert. 1983. Planning and Understanding: A Computational Approach to Human Reasoning. Addison-Wesley, Reading, MA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>