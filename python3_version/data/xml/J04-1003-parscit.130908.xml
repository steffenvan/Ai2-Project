<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9988545">
Verb Class Disambiguation Using
Informative Priors
</title>
<author confidence="0.999802">
Mirella Lapata∗ Chris Brew†
</author>
<affiliation confidence="0.997765">
University of Sheffield Ohio State University
</affiliation>
<bodyText confidence="0.985222">
Levin’s (1993) study of verb classes is a widely used resource for lexical semantics. In her frame-
work, some verbs, such as give, exhibit no class ambiguity. But other verbs, such as write, have
several alternative classes. We extend Levin’s inventory to a simple statistical model of verb class
ambiguity. Using this model we are able to generate preferences for ambiguous verbs without the
use of a disambiguated corpus. We additionally show that these preferences are useful as priors
for a verb sense disambiguator.
</bodyText>
<sectionHeader confidence="0.999265" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999648615384615">
Much research in lexical semantics has concentrated on the relation between verbs and
their arguments. Many scholars hypothesize that the behavior of a verb, particularly
with respect to the expression and interpretation of its arguments, is to a large extent
determined by its meaning (Talmy 1985; Jackendoff 1983; Goldberg 1995; Levin 1993;
Pinker 1989; Green 1974; Gropen et al. 1989; Fillmore 1965). The correspondence be-
tween verbal meaning and syntax has been extensively studied in Levin (1993), which
argues that verbs which display the same diathesis alternations—alternations in the
realization of their argument structure—can be assumed to share certain meaning
components and to form a semantically coherent class.
The converse of this assumption is that verb behavior (i.e., participation in diathe-
sis alternations) can be used to provide clues about aspects of meaning, which in turn
can be exploited to characterize verb senses (referred to as classes in Levin’s [1993] ter-
minology). A major advantage of this approach is that criteria for assigning senses can
be more concrete than is traditionally assumed in lexicographic work (e.g., WordNet or
machine-readable dictionaries) concerned with sense distinctions (Palmer 2000). As an
example consider sentences (1)–(4), taken from Levin. Examples (1) and (2) illustrate
the dative and benefactive alternations, respectively. Dative verbs alternate between
the prepositional frame “NP1 V NP2 to NP3” (see (1a)) and the double-object frame
“NP1 V NP2 NP3” (see (1b)), whereas benefactive verbs alternate between the double-
object frame (see (2a)) and the prepositional frame “NP1 V NP2 for NP3” (see (2b)).
To decide whether a verb is benefactive or dative it suffices to test the acceptability
of the for and to frames. Verbs undergoing the conative alternation can be attested
either as transitive or as intransitive with a prepositional phrase headed by the word
at.1 The role filled by the object of the transitive variant is shared by the noun phrase
complement of at in the intransitive variant (see (3)). This example makes explicit that
class assignment depends not only on syntactic facts but also on judgments about
</bodyText>
<footnote confidence="0.6725755">
∗ Department of Computer Science, Regent Court, 211 Portobello Street, Sheffield, S1 4DP, UK. E-mail:
mlap@dcs.shef.ac.uk.
† Department of Linguistics, Oxley Hall,1712 Neil Avenue, Columbus, OH. E-mail: cbrew@ling.ohio-
state.edu.
1 At is the most likely choice, but for some conative verbs the preposition is instead on or onto.
© 2004 Association for Computational Linguistics
</footnote>
<note confidence="0.525117">
Computational Linguistics Volume 30, Number 1
</note>
<bodyText confidence="0.979252666666667">
semantic roles. Similarly, the possessor object alternation involves a possessor and a
possessed attribute that can be manifested either as the verbal object or as the object
of a prepositional phrase headed by for (see (4)).
</bodyText>
<listItem confidence="0.9818905">
(1) a. Bill sold a car to Tom.
b. Bill sold Tom a car.
(2) a. Martha carved the baby a toy.
b. Martha carved a toy for the baby.
(3) a. Paula hit the fence.
b. Paula hit at the fence.
(4) a. I admired his honesty.
b. I admired him for his honesty.
</listItem>
<bodyText confidence="0.985163705882353">
Observation of the semantic and syntactic behavior of pay and give reveals that
they pattern with sell in licensing the dative alternation. These verbs are all members
of the GIVE class. Verbs like make and build behave similarly to carve in licensing the
benefactive alternation and are members of the class of BUILD verbs. The verbs beat,
kick, and hit undergo the conative alternation; they are all members of the HIT verb
class. By grouping together verbs that pattern together with respect to diathesis alter-
nations, Levin (1993) defines approximately 200 verb classes, which she argues reflect
important semantic regularities. These analyses (and many similar ones by Levin and
her successors) rely primarily on straightforward syntactic and syntactico-semantic cri-
teria. To adopt this approach is to accept some limitations on the reach of our analyses,
since not all semantically interesting differences will have the appropriate reflexes in
syntax. Nevertheless, the emphasis on concretely available observables makes Levin’s
methodology a good candidate for automation (Palmer 2000).
Therefore, Levin’s (1993) classification has formed the basis for many efforts that
aim to acquire lexical semantic information from corpora. These exploit syntactic cues,
or at least cues that are plausibly related to syntax (Merlo and Stevenson 2001; Schulte
im Walde 2000; Lapata 1999; McCarthy 2000). Other work has used Levin’s classifi-
cation (in conjunction with other lexical resources) to create dictionaries that express
the systematic correspondence between syntax and meaning (Dorr 1997; Dang, Rosen-
zweig, and Palmer 1997; Dorr and Jones 1996). Levin’s inventory of verbs and classes
has been also useful for applications such as machine translation (Dorr 1997; Palmer
and Wu 1995), generation (Stede 1998), information retrieval (Levow, Dorr, and Lin
2000), and document classification (Klavans and Kan 1998).
Although the classification provides a general framework for describing verbal
meaning, it says only which verb meanings are possible, staying silent on the relative
likelihoods of the different meanings. The inventory captures systematic regularities
in the meaning of words and phrases but falls short of providing a probabilistic model
of these regularities. Such a model would be useful in applications that need to resolve
ambiguity in the presence of multiple and conflicting probabilistic constraints.
More precisely, Levin (1993) provides an index of 3,024 verbs for which she lists the
semantic classes and diathesis alternations. The mapping between verbs and classes
is not one to one. Of the 3,024 verbs which she covers, 784 are listed as having more
than one class. Even though Levin’s monosemous verbs outnumber her polysemous
verbs by a factor of nearly four to one, the total frequency of the former (4,252,715)
</bodyText>
<page confidence="0.998861">
46
</page>
<note confidence="0.96523">
Lapata and Brew Verb Class Disambiguation Using Informative Priors
</note>
<tableCaption confidence="0.991236">
Table 1
</tableCaption>
<figure confidence="0.8942526">
Polysemous verbs according to Levin.
Classes Verbs BNC frequency
1 2,239 4,252,715
2 536 2,325,982
3 173 738,854
4 43 395,212
5 23 222,747
6 7 272,669
7 2 26,123
10 1 4,427
</figure>
<figureCaption confidence="0.994247">
Figure 1
</figureCaption>
<subsectionHeader confidence="0.499202">
Relation between number of classes and alternations.
</subsectionHeader>
<bodyText confidence="0.999884411764706">
is comparable to the total frequency of the latter (3,986,014). This means that close
to half of the cases processed by a semantic tagger would manifest some degree of
ambiguity. The frequencies are detailed in Table 1 and were compiled from a lemma-
tized version of the British National Corpus (BNC) (Burnard 1995). Furthermore, as
shown in Figure 1, the level of ambiguity increases in tandem with the number of
alternations licensed by a given verb. Consider, for example, verbs participating in
one alternation only: Of these, 90.4% have one semantic class, 8.6% have two classes,
0.7% have three classes, and 0.3% have four classes. In contrast, of the verbs licensing
six different alternations, 14% have one class, 17% have two classes, 12.4% have three
classes, 53.6% have four classes, 2% have six classes, and 1% have seven classes. As
ambiguity increases, so does the availability and potential utility of information about
diathesis alternations.
Palmer (2000) and Dang et al. (1998) argue that syntactic frames and verb classes
are useful for developing principled classifications of verbs. We go beyond this, show-
ing that they can also be of assistance in disambiguation. Consider, for instance, the
verb serve, which is a member of four Levin classes: GIVE, FIT, MASQUERADE, and
FULFILLING. Each of these classes can in turn license four distinct syntactic frames.
</bodyText>
<page confidence="0.998244">
47
</page>
<note confidence="0.728027">
Computational Linguistics Volume 30, Number 1
</note>
<bodyText confidence="0.999645875">
As shown in the examples2 below, in (5a) serve appears ditransitively and belongs to
the semantic class of GIVE verbs, in (5b) it occurs transitively and is a member of the
class of FIT verbs, in (5c) it takes the predicative complement as minister of the interior
and is a member of the class of MASQUERADE verbs. Finally, in sentence (5d) serve is
a FULFILLING verb and takes two complements, a noun phrase (an apprenticeship) and
a prepositional phrase headed by to (to a still-life photographer). In the case of verbs like
serve, we can guess their semantic class solely on the basis of the frame with which
they appear.
</bodyText>
<listItem confidence="0.6591045">
(5) a. I’m desperately trying to find a venue for the reception which can
serve our guests an authentic Italian meal. NP1 V NP2 NP3
b. The airline serves 164 destinations in over 75 countries. NP1 V NP2
c. Jean-Antoine Chaptal was a brilliant chemist and technocrat who
served Napoleon as minister of the interior from 1800 to 1805. NP1 V NP2
as NP3
d. Before her brief exposure to pop stardom, she served an
apprenticeship to a still-life photographer. NP1 V NP2 to NP3
</listItem>
<bodyText confidence="0.9999412">
But sometimes we do not have the syntactic information that would provide cues
for semantic disambiguation. Consider example (6). The verb write is a member of three
Levin classes, two of which (MESSAGE TRANSFER, PERFORMANCE) take the double-
object frame. In this case, we have the choice between the MESSAGE TRANSFER reading
(see (6a)) and the PERFORMANCE reading (see (6b)). The same situation arises with the
verb toast, which is listed as a PREPARE verb and a JUDGMENT verb; both these classes
license the prepositional frame “NP1 V NP2 for NP3.” In sentence (7a) the preferred
reading is that of PREPARE rather than that of JUDGMENT (see sentence (7b)). The verb
study is ambiguous among three classes when attested in the transitive frame: LEARN
(see example (8a)), SIGHT (see example (8b)), and ASSESSMENT (see example (8c)). The
verb convey, when attested in the prepositional frame “NP1 V NP2 to NP3,” can be
ambiguous between the SAY class (see example (9a)) and the SEND class (see exam-
ple (9b)). In order to correctly decide the semantic class for a given ambiguous verb,
we would need not only detailed semantic information about the verb’s arguments,
but also a considerable amount of world knowledge. Admittedly, selectional restric-
tions are sufficient for distinguishing (7a) from (7b) (one normally heats up inanimate
entities and salutes animate ones), but selectional restrictions alone are probably not
enough to disambiguate (6a) from (6b), since both letter and screenplay are likely to be
described as written material. Rather, we need fine-grained world knowledge: Both
scripts and letters can be written for someone: only letters can be written to someone.
</bodyText>
<listItem confidence="0.96937975">
(6) a. A solicitor wrote him a letter at the airport.
b. I want you to write me a screenplay called “The Trip.”
(7) a. He sat by the fire and toasted a piece of bread for himself.
b. We all toasted Nigel for his recovery.
</listItem>
<footnote confidence="0.46487">
2 Unless otherwise stated, our example sentences were taken (possibly in simplified form) from the BNC.
</footnote>
<page confidence="0.996426">
48
</page>
<note confidence="0.860634">
Lapata and Brew Verb Class Disambiguation Using Informative Priors
</note>
<listItem confidence="0.979117">
(8) a. Chapman studied medicine at Cambridge.
b. Romanov studied the old man carefully, looking for some sign that he
knew exactly what had been awaiting him at the bank.
c. The alliance will also study the possibility of providing service to other
high-volume products, such as IBM and multi-vendor workstations.
(9) a. By conveying the news to her sister, she would convey by implication
</listItem>
<bodyText confidence="0.997278785714286">
something of her own anxiety.
b. The judge signed the committal warrant and the police conveyed Mr.
Butler to prison, giving the warrant to the governor.
This need for world knowledge (or at least a convenient way of approximating
this knowledge) is not an isolated phenomenon but manifests itself across a variety
of classes and frames (e.g., double object, transitive, prepositional frame: see exam-
ples (6)–(9)). We have argued that the concreteness of Levin-style verb classes is an
advantage, but this advantage would be compromised if we tried to fold too much
world knowledge into the classification. We do not do this. Instead, Section 5 of the
current article describes disambiguation experiments in which our probabilistic Levin
classes are used in tandem with proxies for appropriate world knowledge.
It is important to point out that Levin’s (1993) classification is not intended as an
exhaustive description of English verbs, their meanings, and their likelihood. Many
other classifications could have been built using the same principles. A different group-
ing might, for example, have occurred if finer or coarser semantic distinctions were
taken into account (see Merlo and Stevenson [2001] and Dang, Rosenzweig, and Palmer
[1997] for alternative classifications) or if the containment of ambiguity was one of the
classification objectives. As pointed out by Kipper, Dang, and Palmer (2000), Levin
classes exhibit inconsistencies, and verbs are listed in multiple classes, some of which
have conflicting sets of syntactic frames. This means that some ambiguities may also
arise as a result of accidental errors or inconsistencies. The classification was created
not with computational uses in mind, but for human readers, so it has not been nec-
essary to remedy all the errors and omissions that might cause trouble for machines.
Similar issues arise in almost all efforts to make use of preexisting lexical resources for
computational purposes (Briscoe and Carroll 1997), so none of the above comments
should be taken as criticisms of Levin’s achievement.
The objective of this article is to show how to train and use a probabilistic version
of Levin’s classification in verb sense disambiguation. We treat errors and inconsis-
tencies in the classification as noise. Although all our tests have used Levin’s classes
and the British National Corpus, the method itself depends neither on the details of
Levin’s classification nor on parochial facts about the English language. Our future
work will include tests on other languages, other classifications, and other corpora.
The model developed in this article takes as input a partially parsed corpus and
generates, for each combination of a verb and its syntactic frame, a probability distri-
bution over the available verb classes. The corpus itself does not have to be labeled
with classes. This makes it feasible to use large corpora. Our model is not immediately
useful for disambiguation, because it cannot discriminate among different occurrences
of the same verb and frame, but it can (as we show in Section 5) be used as a prior
in a full disambiguation system that does take appropriate account of context. The
model relies on several gross simplifications; it does not take selectional restrictions,
discourse, or pragmatic information into account but is demonstrably superior to sim-
pler priors that make no use of subcategorization.
</bodyText>
<page confidence="0.997293">
49
</page>
<note confidence="0.218629">
Computational Linguistics Volume 30, Number 1
</note>
<bodyText confidence="0.999616888888889">
The remainder of this article is organized as follows. In Section 2 we describe the
probabilistic model and the estimation of the various model parameters. In Sections 3
and 4 we report on the results of two experiments that use the model to derive the
dominant class for polysemous verbs. Sections 5 and 6 discuss our verb class disam-
biguation experiments. We base our results on the BNC, a 100-million-word collection
of samples of written and spoken language from a wide range of sources designed
to represent a wide cross-section of current British English, both spoken and writ-
ten (Burnard 1995). We discuss our results in Section 7 and review related work in
Section 8.
</bodyText>
<sectionHeader confidence="0.994301" genericHeader="categories and subject descriptors">
2. The Prior Model
</sectionHeader>
<bodyText confidence="0.986403333333333">
Consider again the sentences in (6). Assuming that we more often write something to
someone rather than for someone, we would like to derive MESSAGE TRANSFER as
the prevalent class for write rather than PERFORMANCE. We view the choice of a class
for a polysemous verb in a given frame as maximizing the joint probability P(c,f,v),
where v is a verb subcategorizing for the frame f and inhabiting more than one Levin
class c:
</bodyText>
<equation confidence="0.999558">
P(c,f,v) = P(v) · P(f |v) · P(c|v,f) (10)
</equation>
<bodyText confidence="0.998319142857143">
Although the terms P(v) and P(f |v) can be estimated from the BNC (P(v) reduces
to the number of times a verb is attested in the corpus, and P(f |v) can be obtained
through parsing), the estimation of P(c|v,f) is somewhat problematic, since it relies
on the frequency F(c, v,f). The latter could be obtained straightforwardly if we had
access to a parsed corpus annotated with subcategorization and semantic-class infor-
mation. Lacking such a corpus we will assume that the semantic class determines the
subcategorization patterns of its members independently of their identity (see (11)):
</bodyText>
<equation confidence="0.753376">
P(c|v, f ) ≈ P(c|f) (11)
</equation>
<bodyText confidence="0.999803625">
The independence assumption is a simplification of Levin’s (1993) hypothesis that the
argument structure of a given verb is a direct reflection of its meaning. The rationale
behind the approximation in (11) is that since class formation is determined on the
basis of diathesis alternations, it is the differences in subcategorization structure, rather
than the identity of the individual verbs, that determine class likelihood. For example,
if we know that some verb subcategorizes for the double object and the prepositional
“NP1 V NP2 to NP3” frames, we can guess that it is a member of the GIVE class or
the MESSAGE TRANSFER class without knowing whether this verb is give, write, or tell.
Note that the approximation in (11) assumes that verbs of the same class uniformly
subcategorize (or not) for a given frame. This is evidently not true for all classes of
verbs. For example, all GIVE verbs undergo the dative diathesis alternation, and there-
fore we would expect them to be attested in both the double-object and prepositional
frame, but only a subset of CREATE verbs undergo the benefactive alternation. For
example, the verb invent is a CREATE verb and can be attested only in the benefactive
prepositional frame (I will invent a tool for you versus ?I will invent you a tool; see Levin
[1993] for details). By applying Bayes’ law we write P(c|f) as
</bodyText>
<equation confidence="0.961298142857143">
P(c|f) = P(f |c) · P(c) (12)
P(f )
By substituting (12) into (10), we can write P(c,f, v) as
P(c,f,v) =
P(v) · P(f |v)
·P(f |c) · P(c) (13)
P(f )
</equation>
<page confidence="0.935544">
50
</page>
<note confidence="0.734079">
Lapata and Brew Verb Class Disambiguation Using Informative Priors
</note>
<tableCaption confidence="0.908661">
Table 2
</tableCaption>
<equation confidence="0.889216777777778">
Estimation of model parameters.
(a) ˆP(v) = F(v) ˆP(f |v) = F(f , v) (c) ˆP(f |c) = F(f , c)
(d) � F(vi) (b) F(v) F(c)
i P(f) = F(f)) (f) F(f, c) = E F(c, f, vi)
ˆP(c) = F(c) i
� F(ci) (e) i
i
(g) F(c) = F(vi, c) (h) F(v, c) = F(v) · P(c|v)
i
</equation>
<bodyText confidence="0.999953882352941">
It is easy to obtain P(v) from the lemmatized BNC (see (a) in Table 2). In order to es-
timate the probability P(f |v), we need to know how many times a verb is attested with
a given frame. We acquired Levin-compatible subcategorization frames from the BNC
after performing a coarse-grained mapping between Levin’s frame descriptions and
surface syntactic patterns without preserving detailed semantic information about ar-
gument structure and thematic roles. This resulted in 80 frame types that were grossly
compatible with Levin. We used Gsearch (Corley et al. 2001), a tool that facilitates the
search of arbitrary part-of-speech-tagged corpora for shallow syntactic patterns based
on a user-specified context-free grammar and a syntactic query. We specified a chunk
grammar for recognizing the verbal complex, NPs, and PPs and used Gsearch to ex-
tract tokens matching the frames specified in Levin. We discarded all frames with a
frequency smaller than five, as they were likely to be unreliable given our heuristic
approach. The frame probability P(f) (see the denominator in (13) and equation (e)
in Table 2) was also estimated on the basis of the Levin-compatible subcategorization
frames that were acquired from the BNC.
We cannot read off P(f|c) in (13) directly from the corpus, because the corpus
is not annotated with verb classes. Nevertheless Levin’s (1993) classification records
the syntactic frames that are licensed by a given verb class (for example, GIVE verbs
license the double object and the “NP1 V NP2 to NP3” frame) and also the number and
type of classes a given verb exhibits (e.g., write inhabits two classes, PERFORMANCE
and MESSAGE TRANSFER). Furthermore, we know how many times a given verb is
attested with a certain frame in the corpus, as we have acquired Levin-compatible
frames from the BNC (see (b) in Table 2). We first explain how we obtain F(f, c), which
we rewrite as the sum of all occurrences of verbs v that are members of class c and
are attested in the corpus with frame f (see (c) and (f) in Table 2).
For monosemous verbs the count F(c,f,v) reduces to the number of times these
verbs have been attested in the corpus with a certain frame. For polysemous verbs,
we additionally need to know the class in which they were attested in the corpus.
Note that we don’t necessarily need an annotated corpus for class-ambiguous verbs
whose classes license distinct frames (see example (5)), provided that we have extracted
verb frames relatively accurately. For genuinely ambiguous verbs (i.e., verbs licensed
by classes that take the same frame), given that we don’t have access to a corpus
annotated with verb class information, we distribute the frequency of the verb and its
frame evenly across its semantic classes:
</bodyText>
<equation confidence="0.957869333333333">
F(f,v)
F(c, f , v) = (14)
|classes(v,f)|
</equation>
<bodyText confidence="0.916097">
Here F(f, v) is the co-occurrence frequency of a verb and its frame and |classes(v,f)|
is the number of classes verb v is a member of when found with frame f. The joint
</bodyText>
<page confidence="0.997234">
51
</page>
<table confidence="0.490739">
Computational Linguistics Volume 30, Number 1
</table>
<tableCaption confidence="0.990943">
Table 3
</tableCaption>
<table confidence="0.979632933333333">
Estimation of F(c,f, v) and F(v, c).
GIVE F(GIVE, NPVNPNP, v) F(GIVE, NPVNPtoNP, v) F(v, GIVE)
feed 98 40 3, 263
4
2 2
give 25, 705 7, 502 126, 894
lend 343 648 2, 650
6 1, 060
2
rent 2 10
181 256 19, 459
pass
3 3 4
58 15, 457
serve 85
</table>
<page confidence="0.7418445">
4
2
</page>
<bodyText confidence="0.99993368">
frequency of a class and its frame F(f, c) is then the sum of all verbs that are mem-
bers of the class c and are attested with frame f in the corpus (see (f) in Table 2).
Table 3 shows the estimation of the frequency F(c,f, v) for six verbs that are mem-
bers of the GIVE class. Consider for example feed, which is a member of four classes:
GIVE, GORGE, FEEDING, and FIT. Of these classes, only FEEDING and GIVE license
the double-object and prepositional frames. This is why the co-occurrence frequency
of feed with these frames is divided by two. The verb serve inhabits four classes. The
double-object frame is licensed by the GIVE class, whereas the prepositional frame
is additionally licensed by the FULFILLING class, and therefore the co-occurrence fre-
quency F(NPVNPtoNP,serve) is equally distributed between these two classes. This
is clearly a simplification, since one would expect F(c,f, v) to vary for different verb
classes. However, note that according to this estimation, F(f, c) will vary across frames
reflecting differences in the likelihood of a class being attested with a certain frame.
Both terms P(f |c) and P(c) in (13) rely on the class frequency F(c) (see (c) and (d)
in Table 2). We rewrite F(c) as the sum of all verbs attested in the corpus with class c
(see (g) in Table 2). For monosemous verbs the estimate of F(v, c) reduces to the count
of the verb in the corpus. Once again we cannot estimate F(v, c) for polysemous verbs
directly. The task would be straightforward if we had a corpus of verbs, each labeled
explicitly with class information. All we have is the overall frequency of a given verb
in the BNC and the number of classes it is a member of according to Levin (1993).
Since polysemous verbs can generally be the realization of more than one semantic
class, counts of semantic classes can be constructed by dividing the contribution from
the verb by the number of classes it belongs to (Resnik 1993; Lauer 1995). We rewrite
the frequency F(v, c) as shown in (h) in Table 2 and approximate P(c|v), the true
distribution of the verb and its classes, as follows:
</bodyText>
<equation confidence="0.994564333333333">
F(v)
P(c|v) ≈ (15)
|classes(v)|
</equation>
<bodyText confidence="0.998765428571429">
Here, F(v) is the number of times the verb v was observed in the corpus and |classes(v)|
is the number of classes c it belongs to. For example, in order to estimate the frequency
of the class GIVE, we consider all verbs that are listed as members of this class in Levin
(1993). The class contains thirteen verbs, among which six are polysemous. We will
obtain F(GIVE) by taking into account the verb frequency of the monosemous verbs
(|classes(v) |is one in this case) as well as distributing the frequency of the polyse-
mous verbs among their classes. For example, feed inhabits the classes GIVE, GORGE,
</bodyText>
<page confidence="0.957355">
52
</page>
<bodyText confidence="0.970834315789474">
Lapata and Brew Verb Class Disambiguation Using Informative Priors
FEEDING, and FIT and occurs in the corpus 3,263 times. We will increment the count
of F(GIVE) by 3,263
4 . Table 3 illustrates the estimation of F(v, c) for six members of the
GIVE class. The total frequency of the class is obtained by summing over the individual
values of F(v, c) (see equation (g) in Table 2).
The approach in (15) relies on the simplifying assumption that the frequency of
a verb is distributed evenly across its semantic classes. This is clearly not true for all
verbs. Consider, for example, the verb rent, which inhabits classes GIVE (Frank rented
Peter his room) and GET (I rented my flat for my sister). Intuitively speaking, the GIVE
sense of rent is more frequent than the GET sense, however, this is not taken into
account in (15), primarily because we do not know the true distribution of the classes
for rent. An alternative to (15) is to distribute the verb frequency unequally among verb
classes. Even though we don’t know how likely classes are in relation to a particular
verb, we can approximate how likely classes are in general on the basis of their size
(i.e., number of verbs that are members of each class). So then we can distribute a
verb’s frequency unequally, according to class size. This time we approximate P(c|v)
(see (h) in Table 2) by P(c|amb class), the probability of class c given the ambiguity
class3 amb class. The latter represents the set of classes a verb might inhabit:
</bodyText>
<equation confidence="0.970467">
F(v, c) ,: F(v) · P(c|amb class) (16)
</equation>
<bodyText confidence="0.99991775">
We collapse verbs into ambiguity classes in order to reduce the number of parameters
that must be estimated; we certainly lose information, but the approximation makes it
easier to get reliable estimates from limited data. We simply approximate P(c|amb class)
using a heuristic based on class size:
</bodyText>
<equation confidence="0.969690666666667">
P(c|amb class) ,: |c |(17)
� |c|
c ∈ amb class
</equation>
<bodyText confidence="0.995423285714286">
For each class we recorded the number of its members after discarding verbs
whose frequency was less than one per million in the BNC. This gave us a first ap-
proximation of the size of each class. We then computed, for each polysemous verb,
the total size of the classes of which it was a member. We calculated P(c|amb class) by
dividing the former by the latter (see equation (17)). We obtained the class frequency
F(c) by multiplying P(c|amb class) by the observed frequency of the verb in the BNC
(see equation (16)). As an example, consider again F(GIVE), which is calculated by
summing over all verbs that are members of this class (see (g) in Table 2). In order
to add the contribution of the verb feed, we need to distribute its corpus frequency
among the classes GIVE, GORGE, FEED, and FIT. The respective P(c|amb class) values
for these classes are 15
38, 8 38, 3 38, and 12
38. By multiplying these by the frequency of feed in
the BNC (3,263), we obtain the values of F(v, c) given in Table 4. Only the frequency
F(feed, GIVE) is relevant for F(GIVE).
The estimation process just described involves at least one gross simplification,
since P(c|amb class) is calculated without reference to the identity of the verb in ques-
tion. For any two verbs that fall into the same set of classes, P(c|amb class) will be the
same, even though one or both may be atypical in its distribution across the classes.
Furthermore, the estimation tends to favor large classes, again irrespectively of the
identity of the verb in question. For example, the verb carry has three classes, CARRY,
</bodyText>
<footnote confidence="0.6683855">
3 Our use of ambiguity classes is inspired by a similar use in hidden Markov model–based
part-of-speech tagging (Kupiec 1992).
</footnote>
<page confidence="0.993174">
53
</page>
<table confidence="0.481073">
Computational Linguistics Volume 30, Number 1
</table>
<tableCaption confidence="0.998317">
Table 4
</tableCaption>
<table confidence="0.995090333333333">
Estimation of F(v, c) for the verb feed.
c Icl P(clamb class) F(v, c)
GIVE 15 .39 1,272.57
GORGE 8 .21 685.23
FEED 3 .08 261.04
FIT 12 .32 1,044.16
</table>
<tableCaption confidence="0.997711">
Table 5
</tableCaption>
<table confidence="0.977480833333333">
Ten most frequent classes using equal distribution of verb frequencies.
c F(c)
CHARACTERIZE 601,647.4
GET 514,308.0
SAY 450,444.6
CONJECTURE 390,618.4
FUTURE HAVING 369,229.3
DECLARE 264,923.6
AMUSE 258,857.9
DIRECTED MOTION 252,775.6
MESSAGE TRANSFER 248,238.7
GIVE 208,884.1
</table>
<tableCaption confidence="0.995053">
Table 6
</tableCaption>
<table confidence="0.959346666666667">
Ten most frequent classes using unequal distribution of verb frequencies.
c F(c)
GET 453,843.6
SAY 447,044.2
CHARACTERIZE 404,734.2
CONJECTURE 382,193.8
FUTURE HAVING 370,717.7
DECLARE 285,431.7
DIRECTED MOTION 255,821.6
POCKET 247,392.7
AMUSE 205,729.4
GIVE 197,828.8
</table>
<bodyText confidence="0.941645615384616">
FIT, and COST. Intuitively speaking, the CARRY class is the most frequent (e.g., Smok-
ing can impair the blood which carries oxygen to the brain; I carry sugar lumps around with me).
However, since the FIT class (e.g., Thameslink presently carries 20,000 passengers daily)
is larger than the CARRY class, it will be given a higher probability (.45 versus .4).
Our estimation scheme is clearly a simplification, but it is an empirical question how
much it matters. Tables 5 and 6 show the ten most frequent classes as estimated us-
ing (15) and (16). We explore the contribution of the two estimation schemes for P(c)
in Experiments 1 and 2.
The probabilities P(f 1c) and P(f jv) will be unreliable when the frequencies F(f, v)
and F(f, c) are small and will be undefined when the frequencies are zero. Following
Hindle and Rooth (1993), we smooth the observed frequencies as shown in Table 7.
When F(f, v) is zero, the estimate used is proportional to the average F(f,V)
F(V) across
</bodyText>
<page confidence="0.98886">
54
</page>
<note confidence="0.773414">
Lapata and Brew Verb Class Disambiguation Using Informative Priors
</note>
<tableCaption confidence="0.946481">
Table 7
</tableCaption>
<equation confidence="0.796315181818182">
Smoothed estimates.
(a) P(f |v) ≈ F(f, v) + FM (b) F(f, V) = F( f, vi)
F(v) + 1
i
F(f, c) + Ff ,C)F(C) (d) F(C) = F( f, ci)
F(c) + 1
(c) P(f |c) ≈
i
all verbs. Similarly, when F(f, c) is zero, our estimate is proportional to the average
F(f,C)
F(C) across all classes. We do not claim that this scheme is perfect, but any deficien-
</equation>
<bodyText confidence="0.998651045454546">
cies it may have are almost certainly masked by the effects of approximations and
simplifications elsewhere in the system.
We evaluated the performance of the model on all verbs listed in Levin (1993) that
are polysemous (i.e., members of more than one class) and take frames characteristic
of the widely studied dative and benefactive alternations (Pinker 1989; Boguraev and
Briscoe 1989; Levin 1993; Goldberg 1995; Briscoe and Copestake 1999) and of the less
well-known conative and possessor object alternations (see examples (1)–(4)). All four
alternations seem fairly productive; that is, a large number of verbs undergo these
alternations, according to Levin. A large number of classes license the frames that
are relevant for these alternations and the verbs that inhabit these classes are likely
to exhibit class ambiguity: 20 classes license the double object frame, 22 license the
prepositional frame “NP1 V NP2 to NP3,” 17 classes license the benefactive “NP1 V
NP2 for NP3” frame, 118 (out of 200) classes license the transitive frame, and 15 classes
license the conative “NP1 V at NP2” frame.
In Experiment 1 we use the model to test the hypothesis that subcategorization
information can be used to disambiguate polysemous verbs. In particular, we concen-
trate on verbs like serve (see example (5)) that can be disambiguated solely on the
basis of their frame. In Experiment 2 we focus on verbs that are genuinely ambiguous;
that is, they inhabit a single frame and yet can be members of more than one seman-
tic class (e.g., write, study, see examples (6)–(9)). In this case, we use the probabilistic
model to assign a probability to each class the verb inhabits. The class with the highest
probability represents the dominant meaning for a given verb.
</bodyText>
<sectionHeader confidence="0.949403" genericHeader="general terms">
3. Experiment 1: Using Subcategorization to Resolve Verb Class Ambiguity
</sectionHeader>
<subsectionHeader confidence="0.999943">
3.1 Method
</subsectionHeader>
<bodyText confidence="0.99997675">
In this experiment we focused solely on verbs whose meaning can be potentially
disambiguated by taking into account their subcategorization frame. A model that
performs badly on this task cannot be expected to produce any meaningful results for
genuinely ambiguous verbs.
We considered 128 verbs with the double-object frame (2.72 average class ambi-
guity), 101 verbs with the prepositional frame “NP1 V NP2 to NP3” (2.59 average
class ambiguity), 113 verbs with the frame “NP1 V NP2 for NP3” (2.63 average class
ambiguity), 42 verbs with the frame “NP1 V at NP3” (3.05 average class ambiguity),
and 39 verbs with the transitive frame (2.28 average class ambiguity). The task was the
following: Given that we know the frame of a given verb, can we predict its semantic
class? In other words by varying the class c in the term P(c,f, v), we are trying to see
whether the class that maximizes it is the one predicted by the lexical semantics and
</bodyText>
<page confidence="0.99265">
55
</page>
<table confidence="0.479506">
Computational Linguistics Volume 30, Number 1
</table>
<tableCaption confidence="0.995899">
Table 8
</tableCaption>
<table confidence="0.939473">
Model accuracy using equal distribution of verb frequencies for the estimation of P(c).
Frame Baseline Model
NP1 V NP2 NP3 60.9% 93.8%
NP1 V NP to NP3 63.3% 95.0%
NP1 V NP for NP3 63.6% 98.2%
NP1 V at NP2 2.4% 83.3%
NP1 V NP2 43.6% 87.2%
Combined 55.8% 93.9%
</table>
<tableCaption confidence="0.7493945">
Table 9
Model accuracy using unequal distribution of verb frequencies for the estimation of P(c).
</tableCaption>
<table confidence="0.998280142857143">
Frame Baseline Model
NP1 V NP2 NP3 62.5% 93.8%
NP1 V NP to NP3 67.3% 95.0%
NP1 V NP for NP3 66.4% 98.2%
NP1 V at NP2 2.4% 85.7%
NP1 V NP2 41.0% 84.6%
Combined 56.7% 93.9%
</table>
<bodyText confidence="0.994384666666667">
the argument structure of the verb in question. The model’s responses were evaluated
against Levin’s (1993) classification. The model’s performance was considered correct
if it agreed with Levin in assigning a verb to an appropriate class given a particular
frame. Recall from Section 2 that we proposed two approaches for the estimation of
the class probability P(c). We explore the influence of P(c) by obtaining two sets of
results corresponding to the two estimation schemes.
</bodyText>
<subsectionHeader confidence="0.807609">
3.2 Results
</subsectionHeader>
<bodyText confidence="0.999980052631579">
The model’s accuracy is shown in Tables 8 and 9. The results in Table 8 were ob-
tained using the estimation scheme for P(c) that relies on the even distribution of
the frequency of a verb across its semantic classes (see equation (15)). The results
in Table 9 were obtained using an alternative scheme that distributes verb frequency
unequally among verb classes by taking class size into account (see equation (16)).
As mentioned in Section 3.1, the results were based on comparison of the model’s
performance against Levin’s (1993) classification. We also compared the results to the
baseline of choosing the most likely class P(c) (without taking subcategorization in-
formation into account). The latter was determined on the basis of the approximations
described in Section 2 (see equation (9) in Table 2, as well as equations (15), (16),
and (17)).
The model achieved an accuracy of 93.9% using either type of estimation for P(c).
It also outperformed the baseline by 38.1% (see Table 8) and 37.2% (see Table 9). One
might expect an accuracy of 100%, since these verbs can be disambiguated solely on the
basis of their frame. However, the performance of our model achieves a lesser accuracy,
mainly because of the way we estimate the terms P(c) and P(fIc): We overemphasize
the importance of class information without taking into account how individual verbs
distribute across classes. Furthermore, we rely on frame frequencies acquired from the
BNC, using shallow syntactic analysis, which means that the correspondence between
</bodyText>
<page confidence="0.961945">
56
</page>
<note confidence="0.67082">
Lapata and Brew Verb Class Disambiguation Using Informative Priors
</note>
<bodyText confidence="0.99955225">
Levin’s (1993) frames and our acquired frames is not one to one. Except for the fact
that our frames do not preserve much of the linguistic information detailed Levin,
the number of frames acquired for a given verb can be a subset or superset of the
frames available in Levin. Note that the two estimation schemes yield comparable
performances. This is a positive result given the importance of P(c) in the estimation
of P(c,f,v).
A more demanding task for our probabilistic model will be with genuinely am-
biguous verbs (i.e., verbs for which the mapping between meaning and subcatego-
rization is not one to one). Although native speakers may have intuitions about the
dominant interpretation for a given verb, this information is entirely absent from Levin
(1993) and from the corpus on which our model is trained. In Experiment 2 we show
how our model can be used to recover this information.
</bodyText>
<sectionHeader confidence="0.991215" genericHeader="keywords">
4. Experiment 2: Using Corpus Distributions to Derive Verb Class Preferences
</sectionHeader>
<subsectionHeader confidence="0.998734">
4.1 Method
</subsectionHeader>
<bodyText confidence="0.999909064516129">
We evaluated the performance of our model on 67 genuinely ambiguous verbs, that
is, verbs that inhabit a single frame and can be members of more than one seman-
tic class (e.g., write). These verbs are listed in Levin (1993) and undergo the dative,
benefactive, conative, and possessor object alternations. As in Experiment 1, we con-
sidered verbs with the double-object frame (3.27 average class ambiguity), verbs with
the frame “NP1 V NP2 to NP3” (2.94 average class ambiguity), verbs with the frame
“NP1 V NP2 for NP3” (2.42 average class ambiguity), verbs with the frame “NP1 V
at NP3” (2.71 average class ambiguity), and transitive verbs (2.77 average class am-
biguity). The model’s predictions were compared against manually annotated data
that was used only for testing purposes. The model was trained without access to a
disambiguated corpus. More specifically, corpus tokens characteristic of the verb and
frame in question were randomly sampled from the BNC and annotated with class
information so as to derive the true distribution of the verb’s classes in a particular
frame. We describe the verb selection procedure as follows.
Given the restriction that these verbs be semantically ambiguous in a specific
syntactic frame, we could not simply sample from the entire BNC, since this would
decrease the chances of finding the verb in the frame we are interested in. Instead,
a stratified sample was used: For all class-ambiguous verbs, tokens were randomly
sampled from the parsed data used for the acquisition of verb frame frequencies. The
model was evaluated on verbs for which a reliable sample could be obtained. This
meant that verbs had to have a frame frequency larger than 50. For verbs exceeding
this threshold 100 tokens were randomly selected and annotated with verb class infor-
mation. For verbs with frame frequency less than 100 and more than 50, no sampling
took place; the entire set of tokens was manually annotated. This selection procedure
resulted in 14 verbs with the double-object frame, 16 verbs with the frame “NP1 V
NP2 to NP3,” 2 verbs with the frame “NP1 V NP2 for NP3,” 1 verb with the frame
“NP1 V at NP3,” and 80 verbs with the transitive frame. From the transitive verbs
we further randomly selected 34 verbs; these were manually annotated and used for
evaluating the model’s performance.4
The selected tokens were annotated with class information by two judges, both
linguistics graduate students. The classes were taken from Levin (1993) and augmented
</bodyText>
<footnote confidence="0.616253">
4 Although the model can yield predictions for any number of verbs, evaluation could not be performed
for all 80 verbs, as to perform such evaluation, our judges would have had to annotate 8,000 corpus
tokens.
</footnote>
<page confidence="0.983372">
57
</page>
<note confidence="0.385303">
Computational Linguistics Volume 30, Number 1
</note>
<bodyText confidence="0.999822111111111">
with the class OTHER, which was reserved for either corpus tokens that had the wrong
frame or those for which the classes in question were not applicable. The judges were
given annotation guidelines (for each verb) but no prior training (for details on the
annotation study see Lapata [2001]). The annotation provided a gold standard for
evaluating the model’s performance and enabled us to test whether humans agree
on the class annotation task. We measured the judges’ agreement on the annotation
task using the kappa coefficient (Cohen 1960). In general, the agreement on the class
annotation task was good, with kappa values ranging from .66 to 1.00 (the mean kappa
was .80, SD = .09).
</bodyText>
<sectionHeader confidence="0.623264" genericHeader="introduction">
4.2 Results
</sectionHeader>
<bodyText confidence="0.999910333333333">
We counted the performance of our model as correct if it agreed with the “most pre-
ferred,” that is, the most frequent, verb class, as determined in the manually annotated
corpus sample by taking the average of the responses of both judges. As an example,
consider the verb feed, which in the double-object frame is ambiguous between the
classes FEED and GIVE. According to the model, FEED is the most likely class for feed.
Out of 100 instances of the verb feed in the double-object frame, 61 were manually
assigned the FEED class, 32 were assigned the GIVE class, and 6 were parsing mis-
takes (and therefore assigned the class OTHER). In this case the model’s outcome is
considered correct given that the corpus tokens also reveal a preference for the FEED
(i.e., the FEED instances outnumber the GIVE ones).
As in Experiment 1, we explored the influence of the parameter P(c) on the model’s
performance by obtaining two sets of results corresponding to the two estimation
schemes discussed in Section 2. The model’s accuracy is shown in Tables 10 and 11.
The results in Table 10 were obtained using the estimation scheme for P(c) that
relies on the even distribution of a verb’s frequency across its semantic classes (see
</bodyText>
<tableCaption confidence="0.997349">
Table 10
</tableCaption>
<table confidence="0.94286175">
Model accuracy using equal distribution of verb frequencies for the estimation of P(c).
Frame Baseline Model
NP1 V NP2 NP3 50.0% 78.6%
NP1 V NP to NP3 43.8% 68.8%
NP1 V NP for NP3 00.0% 100.0%
NP1 V at NP2 100.0% 100.0%
NP1 V NP2 47.1% 73.5%
Combined 46.2% 74.6%
</table>
<tableCaption confidence="0.994561">
Table 11
</tableCaption>
<table confidence="0.95043375">
Model accuracy using unequal distribution of verb frequencies for the estimation of P(c).
Frame Baseline Model
NP1 V NP2 NP3 50.0% 78.6%
NP1 V NP to NP3 43.8% 75.0%
NP1 V NP for NP3 00.0% 100.0%
NP1 V at NP2 100.0% 100.0%
NP1 V NP2 47.1% 67.6%
Combined 46.2% 73.1%
</table>
<page confidence="0.598672">
58
</page>
<note confidence="0.498244">
Lapata and Brew Verb Class Disambiguation Using Informative Priors
</note>
<tableCaption confidence="0.983813">
Table 12
</tableCaption>
<figure confidence="0.84810372">
Semantic preferences for verbs with the double-object frame.
Verb Class K
call √ DUB GET OTHER .82
93 -7.59 3 -8.12 4
cook BUILD PREPARE OTHER 1.00
28 -11.68 33 -11.50 1
declare √ DECLARE REF. APPEAR. OTHER .89
35 -10.51 18 -12.18 5
feed √ FEED GIVE OTHER .73
61 -10.63 32 -12.16 6
find √ DECLARE GET OTHER .70
36 -7.69 47 -7.43 17
leave √ GET FULFILL F. HAVE OTHER .67
6 -7.91 14 -10.40 56 −7.66 23
make √ BUILD DUB OTHER .79
21 -7.25 66 -6.13 13
pass √ GIVE SEND THROW OTHER .93
81 -8.84 0 -8.96 0 −9.98 19
save √ BILL GET OTHER .74
24 -9.74 62 -9.59 14
shoot THROW GET OTHER 1.00
91 -10.94 0 -9.99 5
take BRING-TAKE PERFORM OTHER .77
15 -7.02 40 -7.38 45
write √ MSG. TRANS. PERFORM OTHER
</figure>
<bodyText confidence="0.965584428571428">
equation (15)). The results in Table 11 were obtained using a scheme that distributes
verb frequency unequally among verb classes by taking class size into account (see
equation (16)). As in Experiment 1, the results were compared to a simple baseline that
defaults to the most likely class without taking verb frame information into account
(see equation (g) in Table 2 as well as equations (15), (16), and (17)).
The model achieved an accuracy of 74.6% using the estimation scheme of equal
distribution and a accuracy of 73.1% using the estimation scheme of unequal distribu-
tion. The difference between the two estimation schemes is not statistically significant
( x2(67) = 2.17, p = .84). Table 12 gives the distribution of classes for 12 polysemous
verbs taking the double-object frame as obtained from the manual annotation of corpus
tokens together with interannotator agreement (rc). We also give the (log-transformed)
probabilities of these classes as derived by the model.5 The presence of the symbol V/
indicates that the model’s class preference for a given verb agrees with its distribution
in the corpus. The absence of V/ indicates disagreement. For the comparison shown in
Table 12, model class preferences were derived using the equal-distribution estimation
scheme for P(c) (see equation (15)).
As shown in Table 12, the model’s predictions are generally borne out in the corpus
data. Misclassifications are due mainly to the fact that the model does not take verb
class dependencies into account. Consider, for example, the verb cook. According to the
model the most likely class for cook is BUILD. Although it may generally be the case
that BUILD verbs (e.g., make, assemble, build) are more frequent than PREPARE verbs
</bodyText>
<tableCaption confidence="0.6719485">
5 No probabilities are given for the OTHER class; this is not a Levin class, however, it was used by the
annotators, mainly to indicate parsing errors.
</tableCaption>
<figure confidence="0.6135915">
.85
54 -8.79 19 -9.05 18
59
Computational Linguistics Volume 30, Number 1
</figure>
<bodyText confidence="0.999679555555556">
(e.g., bake, roast, boil), the situation is reversed for cook. The same is true for the verb
shoot, which when attested in the double-object frame is more likely to be a THROW
verb (Jamie shot Mary a glance) rather than a GET verb (I will shoot you two birds).
Notice that our model is not context sensitive; that is, it does not derive class
rankings tailored to specific verbs, primarily because this information is not readily
available in the corpus, as explained in Section 2. However, we have effectively built a
prior model of the joint distribution of verbs, their classes, and their syntactic frames
that can be useful for disambiguating polysemous verbs in context. We describe our
class disambiguation experiments as follows.
</bodyText>
<sectionHeader confidence="0.985127" genericHeader="method">
5. Class Disambiguation
</sectionHeader>
<bodyText confidence="0.99998375">
In the previous sections we focused on deriving a model of the distribution of Levin
classes without relying on annotated data and showed that this model infers the right
class for genuinely ambiguous verbs 74.6% of the time without taking the local context
of their occurrence into account. An obvious question is whether this information is
useful for disambiguating tokens rather than types. In the following we report on a
disambiguation experiment that takes advantage of this prior information.
Word sense disambiguation is often cast as a problem in supervised learning,
where a disambiguator is induced from a corpus of manually sense-tagged text. The
context within which the ambiguous word occurs is typically represented by a set
of linguistically motivated features from which a learning algorithm induces a repre-
sentative model that performs the disambiguation. A variety of classifiers have been
employed for this task (see Mooney [1996] and Ide and Veronis [1998] for overviews),
the most popular being decision lists (Yarowsky 1994, 1995) and naive Bayesian classi-
fiers (Pedersen 2000; Ng 1997; Pedersen and Bruce 1998; Mooney 1996; Cucerzan and
Yarowsky 2002). We employed a naive Bayesian classifier (Duda and Hart 1973) for
our experiments, as it is a very convenient framework for incorporating prior knowl-
edge and studying its influence on the classification task. In Section 5.1 we describe
a basic naive Bayesian classifier and show how it can be extended with informative
priors. In Section 5.2 we discuss the types of contextual features we use. We report on
our experimental results in Section 6.
</bodyText>
<subsectionHeader confidence="0.978495">
5.1 Naive Bayes Classification
</subsectionHeader>
<bodyText confidence="0.9999255">
A naive Bayesian classifier assumes that all the feature variables representing a prob-
lem are conditionally independent, given the value of the classification variable. In
word sense disambiguation, the features (a1, a2, ... , an) represent the context surround-
ing the ambiguous word, and the classification variable c is the sense (Levin class in
our case) of the ambiguous word in this particular context. Within a naive Bayes
approach, the probability of the class c given its context can be expressed as
</bodyText>
<equation confidence="0.992005">
(18)
P(ai)
</equation>
<bodyText confidence="0.999811">
where P(ai|c) is the probability that a test example is of class c given the contextual
features ai. Since the denominator P(ai) is constant for all classes c, the problem reduces
to finding the class c with the maximum value for the numerator:
</bodyText>
<equation confidence="0.994594375">
n
P(c|ai) � P(c) P(ai|c) (19)
i=1
P(c)
P(c|ai) =
n
P(ai|c)
i=1
</equation>
<page confidence="0.973171">
60
</page>
<note confidence="0.656652">
Lapata and Brew Verb Class Disambiguation Using Informative Priors
</note>
<bodyText confidence="0.870214333333333">
If we choose the prior P(c) to be uniform (P(c) = 1
,c, for all c E C), (19) can be
further simplified to
</bodyText>
<equation confidence="0.930403285714286">
n
P(c|a) � P(ai|c) (20)
i=1
Assuming a uniform prior, a basic naive Bayesian classifier is as follows:
n
* c = P(ai|c) (21)
i=1
</equation>
<bodyText confidence="0.998324666666667">
Note, however, that we developed in the previous section two types of non-
uniform prior models. The first model derives P(c) heuristically from the BNC, ig-
noring the identity of the polysemous verb and its subcategorization profile, and the
second model estimates the class distribution P(c, v,f) by taking the frame distribu-
tion into account. So, the naive Bayesian classifier in (21) can be extended with a
nonuniform prior:
</bodyText>
<equation confidence="0.998051">
n
* c = P(c) P(ai|c) (22)
i=1
n
*c=P(c,v,f) P(ai|c, f , v) (23)
i=1
</equation>
<bodyText confidence="0.999948714285714">
where P(c) is estimated as shown in (d)–(g) in Table 2 and P(c, v,f), the prior for
each class c corresponding to verb v in frame f, is estimated as explained in Sec-
tion 2 (see (13)). As before, ai are the contextual features. The probabilities P(ai|c)
and P(ai|c, f , v) can be estimated from the training data simply by counting the co-
occurrence of feature ai with class c (for (22)) or the co-occurrence of ai with class
c, verb v, and frame f (for (23)). For features that have zero counts, we use add-k
smoothing (Johnson 1932), where k is a number less than one.
</bodyText>
<subsectionHeader confidence="0.999304">
5.2 Feature Space
</subsectionHeader>
<bodyText confidence="0.999991833333333">
As is common in word sense disambiguation studies, we experimented with two types
of context representations, collocations and co-occurrences. Co-occurrences simply in-
dicate whether a given word occurs within some number of words to the left or right
of an ambiguous word. In this case the contextual features are binary and represent
the presence or absence of a particular word in the current or preceding sentence. We
used four types of context in our experiments: left context (i.e., words occurring to
the left of the ambiguous word), right context (i.e., words occurring to the right of the
ambiguous word), the current sentence (i.e., words surrounding the ambiguous word),
and the current sentence together with its immediately preceding sentence. Punctua-
tion and capitalization were removed from the windows of context; noncontent words
were included. The context words were represented as lemmas or parts of speech.
Collocations are words that are frequently adjacent to the word to be disam-
biguated. We considered 12 types of collocations. Examples of collocations for the
verb write are illustrated in Table 13. The L columns in the table indicate the number
of words to the left of the ambiguous words, and the R columns, the number of words
to the right. So for example, the collocation 1L3R represents one word to the left and
three words to the right of the ambiguous word. Collocations again were represented
as lemmas (see Table 13) or parts of speech.
</bodyText>
<page confidence="0.994771">
61
</page>
<figure confidence="0.8602286">
Computational Linguistics Volume 30, Number 1
Table 13
Features for collocations.
L R Example L R Example
0 1 write you 1 1 can write you
1 0 can write 1 2 can write you a
0 2 write you a 2 1 I can write you
2 0 I can write 1 3 can write you a story
0 3 write you a story 3 1 perhaps I can write you
3 0 perhaps I can write 2 4 I can write you a story sunshine
</figure>
<sectionHeader confidence="0.922806" genericHeader="method">
6. Experiment 3: Disambiguating Polysemous Verbs
</sectionHeader>
<subsectionHeader confidence="0.999212">
6.1 Method
</subsectionHeader>
<bodyText confidence="0.999960590909091">
We tested the performance of our naive Bayesian classifiers on the 67 genuinely am-
biguous verbs on which the prior models were tested. Recall that these models were
trained without access to a disambiguated corpus, which was used only to determine
for a given verb and its frame its most likely meaning overall (i.e., across the corpus)
instead of focusing on the meaning of individual corpus tokens. The same corpus
was used for the disambiguation of individual tokens, excluding tokens assigned the
class OTHER. The naive Bayes classifiers were trained and tested using 10-fold cross-
validation on a set of 5,002 examples. These were representative of the frames “NP1
V NP2,” “NP1 V NP2 NP3,” “NP1 V NP2 to NP3,” and “NP1 V NP2 for NP3.” The
frame “NP1 V at NP2” was excluded from our disambiguation experiments as it was
represented solely by the verb kick (50 instances).
In this study we compare a naive Bayesian classifier that relies on a uniform
prior (see (20)) against two classifiers that make use of nonuniform prior models:
The classifier in (22) effectively uses as prior the baseline model P(c) from Section 2,
whereas the classifier in (23) relies on the more informative model P(c,f, v). As a
baseline for the disambiguation task, we simply assign the most common class in the
training data to every instance in the test data, ignoring context and any form of prior
information (Pedersen 2001; Gale, Church, and Yarowsky 1992a). We also report an
upper bound on disambiguation performance by measuring how well human judges
agree with one another (percentage agreement) on the class assignment task. Recall
from Section 4.1 that our corpus was annotated by two judges with Levin-compatible
verb classes.
</bodyText>
<subsectionHeader confidence="0.887514">
6.2 Results
</subsectionHeader>
<bodyText confidence="0.999350666666666">
The results of our class disambiguation experiments are summarized in Figures 2–5.
In order to investigate differences among different frames, we show how the naive
Bayesian classifiers perform for each frame individually. Figures 2–5 (x-axis) also re-
veal the influence of collocational features of different sizes (see Table 13) on the
classification task. Panel (b) in the figures presents the classifiers’ accuracy when the
collocational features are encoded as lemmas; in panel (c) of the figures, the context
is represented as parts of speech, whereas in panel (a) of the figures, the context is
represented by both lemmas and parts of speech.
As can be seen in the figures, the naive Bayesian classifier with our informa-
tive prior (P(c,f, v), IPrior in Figures 2–5) generally outperforms the baseline prior
(P(c), BPrior in Figures 2–5), the uniform prior (UPrior in Figures 2–5), and the base-
line (Baseline in Figures 2–5) for all frames. Good performances are attained with
</bodyText>
<page confidence="0.989216">
62
</page>
<figure confidence="0.907269875">
Lapata and Brew Verb Class Disambiguation Using Informative Priors
(a) (b) (c)
Figure 2
Word sense disambiguation accuracy for “NP1 V NP2” frame.
(a) (b) (c)
Figure 3
Word sense disambiguation accuracy for “NP1 V NP2 NP3” frame.
(a) (b) (c)
</figure>
<figureCaption confidence="0.99279">
Figure 4
</figureCaption>
<bodyText confidence="0.963813833333333">
Word sense disambiguation accuracy for “NP1 V NP2 to NP3” frame.
lemmas, parts of speech, and combination of the two. The naive Bayesian classifier
(IPrior) reaches the upper bound (UpBound in Figures 2–5) for the ditransitive frames
“NP1 V NP2 NP3,” “NP1 V NP2 to NP3,” and “NP1 V NP2 for NP3.”
The best accuracy (87.8%) for the transitive frame is achieved with the collocational
features 0L2R, 1L2R, and 1L3R (see Figures 2(a)–(c)). For the double-object frame, the
highest accuracy (90.8%) is obtained with features 0LR1 and 0L3R (see Figures 3(b)
and 3(c)). Similarly, for the ditransitive “NP1 V NP2 to NP3” frame, the features 0L3R
and 0L1R yield the best accuracies (88.8%, see Figures 4(a)–(c)). Finally, for the “NP1 V
NP2 for NP3” frame, accuracy (94.4%) is generally good for most features when an
informative prior is used. In fact, neither the uniform prior nor the baseline P(c)
outperforms the baseline for this frame.
</bodyText>
<page confidence="0.993382">
63
</page>
<figure confidence="0.9972505">
Computational Linguistics Volume 30, Number 1
(a) (b) (c)
</figure>
<figureCaption confidence="0.956647">
Figure 5
</figureCaption>
<bodyText confidence="0.97514095">
Word sense disambiguation accuracy for “NP1 V NP2 for NP3” frame.
The context encoding (lemmas versus parts of speech) does not seem to have a
great influence on the disambiguation performance. Good accuracies are obtained with
either parts of speech or lemmas; combination of the two does not yield better results.
The classifier with the informative prior P(c,f,v) outperforms the baseline prior
P(c) and the uniform prior also when co-occurrences are used. However, the co-
occurrences never outperform the collocational features, for all four types of context.
The classifiers (regardless of the type of prior being used) never beat the baseline for
the frames “NP1 V NP2” and “NP1 V NP2 to NP3”. Accuracies above the baseline
are achieved for the frames “NP1 V NP2 NP3” and “NP1 V NP2 for NP3” when
an informative prior is used. Detailed results are summarized in the Appendix. Co-
occurrences and windows of large sizes traditionally work well for topical distinctions
(Gale, Church, and Yarowsky 1992b). Levin classes, however, typically capture differ-
ences in argument structure, that is, the types of objects or subjects that verbs select
for. Argument structure is approximated by our collocational features. For example, a
verb often taking a reflexive pronoun as its object is more likely to be a REFLEXIVE
VERB OF APPEARANCE than a verb that never subcategorizes for a reflexive object.
There is not enough variability among the wider contexts surrounding a polysemous
verb to inform the class-disambiguation task, as the Levin classes often do not cross
topical boundaries.
</bodyText>
<sectionHeader confidence="0.979958" genericHeader="method">
7. Discussion
</sectionHeader>
<bodyText confidence="0.999939461538461">
In this article, we have presented a probabilistic model of verb class ambiguity based
on Levin’s (1993) semantic classification. Our results show that subcategorization in-
formation acquired automatically from corpora provides important cues for verb class
disambiguation (Experiment 1). In the absence of subcategorization cues, corpus-based
distributions and quantitative approximations of linguistic concepts can be used to de-
rive a preference ordering on the set of verbal meanings (Experiment 2). The semantic
preferences that we have generated can be thought of as default semantic knowledge,
to be used in the absence of any explicit contextual or lexical semantic information to
the contrary (see Table 12). We have also shown that these preferences are useful for
disambiguating polysemous verbs within their local contexts of occurrence (Experi-
ment 3).
The approach is promising in that it achieves satisfactory results with a simple
model that has a straightforward interpretation in a Bayesian framework and does not
</bodyText>
<page confidence="0.997209">
64
</page>
<note confidence="0.75668">
Lapata and Brew Verb Class Disambiguation Using Informative Priors
</note>
<bodyText confidence="0.999951162162162">
rely on the availability of annotated data. The model’s parameters are estimated using
simple distributions that can be extracted easily from corpora. Our model achieved an
accuracy of 93.9% (over a baseline of 56.7%) on the class disambiguation task (Exper-
iment 1) and an accuracy of 74.6% (over a baseline of 46.2%) on the task of deriving
dominant verb classes (Experiment 2). Our disambiguation experiments reveal that
this default semantic knowledge, when incorporated as a prior in a naive Bayes classi-
fier, outperforms the uniform prior and the baseline of always defaulting to the most
frequent class (Experiment 3). In fact, for three out of the four frames under study,
our classifier with the informative prior achieved upper-bound performance.
Although our results are promising, it remains to be shown that they generalize
across frames and alternations. Four types of alternations were investigated in this
study. However, Levin lists 79 alternations and approximately 200 classes. Although
distributions for different class/frame combinations can easily be derived automati-
cally, it remains to be shown that these distributions are useful for all verbs, frames,
and classes. Also note that the models described in the previous sections crucially rely
on the acquisition of relatively accurate frames from the corpus. It is a matter of future
work to examine how the quality of the acquired frames influences the disambiguation
task. Also, the assumption that the semantic class determines the subcategorization
patterns of its class members independently of their identity may not be harmless for
all classes and frames.
Although our original aim was to develop a probabilistic framework that exploits
Levin’s (1993) linguistic classification and the systematic correspondence between syn-
tax and semantics, a limitation of the model is that it cannot infer class information for
verbs not listed in Levin. For these verbs, P(c), and hence P(c, f , v), will be zero. Recent
work in computational linguistics (e.g., Sch¨utze 1998) and cognitive psychology (e.g.,
Landauer and Dumais 1997) has shown that large corpora implicitly contain semantic
information, which can be extracted and manipulated in the form of co-occurrence
vectors. One possible approach would be to compute the centroid (geometric mean)
of the vectors of all members of a semantic class. Given an unknown verb (i.e., a verb
not listed in Levin), we can decide its semantic class by comparing its semantic vector
to the centroids of all semantic classes. For example, we could determine class mem-
bership on the basis of the distance to the closest centroid representing a semantic
class (see Patel, Billinaria, and Levy [1998] for a proposal similar in spirit). Another
approach put forward by Dorr and Jones (1996) utilizes WordNet (Miller and Charles
1991) to find similarities (via synonymy) between unknown verbs and verbs listed in
Levin. Once we have chosen a class for an unknown verb, we are entitled to assume
that it will share the broad syntactic and semantic properties of that class.
</bodyText>
<sectionHeader confidence="0.997804" genericHeader="related work">
8. Related Work
</sectionHeader>
<bodyText confidence="0.999869090909091">
Levin’s (1993) seminal study on diathesis alternations and verb semantic classes has
recently influenced work in dictionary creation (Dorr 1997; Dang et al. 1998; Dorr
and Jones 1996) and notably lexicon acquisition on the basis of the assumption that
verbal meaning can be gleaned from corpora using cues pertaining to syntactic struc-
ture (Merlo and Stevenson 2001; Schulte im Walde 2000; Lapata 1999; McCarthy 2000).
Previous work in word sense disambiguation has not tackled explicitly the ambiguity
problems arising from Levin’s classification, although methods for deriving informa-
tive priors in an unsupervised manner have been proposed by Ciaramita and Johnson
(2000) and Chao and Dyer (2000) within the context of noun and adjective sense dis-
ambiguation, respectively. In this section we review related work on classification and
lexicon acquisition and compare it to our own work.
</bodyText>
<page confidence="0.997977">
65
</page>
<note confidence="0.496052">
Computational Linguistics Volume 30, Number 1
</note>
<bodyText confidence="0.998739196078432">
Dang et al. (1998) observe that verbs in Levin’s (1993) database are listed in more
than one class. The precise meaning of this ambiguity is left open to interpretation
in Levin, as it may indicate that the verb has more than one sense or that one sense
(i.e., class) is primary and the alternations for this class should take precedence over
the alternations for the other classes for which the verb is listed. Dang et al. augment
Levin’s semantic classes with a set of “intersective” classes that are created by grouping
together sets of existing classes that share a minimum of three overlapping members.
Intersective classes are more fine-grained than the original Levin classes and exhibit
more-coherent sets of syntactic frames and associated semantic components. Dang et
al. further argue that intersective classes are more compatible with WordNet than the
broader Levin classes and thus make it possible to attribute the semantic components
and associated sets of syntactic frames to specific WordNet senses as well, thereby
enriching the WordNet representation and providing explicit criteria for word sense
disambiguation.
Most statistical approaches, including ours, treat verbal-meaning assignment as a
semantic classification task. The underlying question is the following: How can corpus
information be exploited in deriving the semantic class for a given verb? Despite the
unifying theme of using corpora and corpus distributions for the acquisition task, the
approaches differ in the inventory of classes they employ, in the methodology used
for inferring semantic classes, and in the specific assumptions concerning the verbs to
be classified (e.g., can they be polysemous or not).
Merlo and Stevenson (2001) use grammatical features (acquired from corpora) to
classify verbs into three semantic classes: unergative, unaccusative, and object drop.
These classes are abstractions of Levin’s (1993) classes and as a result yield a coarser
classification. For example, object-drop verbs comprise a variety of Levin classes such
as GESTURE verbs, CARING verbs, LOAD verbs, PUSH-PULL verbs, MEET verbs, SO-
CIAL INTERACTION verbs, and AMUSE verbs. Unergative, unaccusative, and object-drop
verbs have identical subcategorization patterns (i.e., they alternate between the tran-
sitive and intransitive frame), yet distinct argument structures, and therefore differ in
the thematic roles they assign to their arguments. For example, when attested in the
intransitive frame, the subject of an object-drop verb is an agent, whereas the subject
of an unaccusative verb is a theme. Under the assumption that differences in thematic
role assignment uniquely identify semantic classes, numeric approximations of argu-
ment structure are derived from corpora and used in a machine-learning paradigm to
place verbs in their semantic classes. The approach is evaluated on 59 verbs manually
selected from Levin (20 unergatives, 20 object drops, and 19 unaccusatives). It is as-
sumed that these verbs are monosemous, that is, they can be ergative, unergative, or
object drop. A decision-tree learner achieves an accuracy of 69.8% on the classification
task over a chance baseline of 34%.
Schulte im Walde (2000) uses subcategorization information and selectional re-
strictions to cluster verbs into Levin (1993)–compatible semantic classes. Subcatego-
rization frames are induced from the BNC using a robust statistical parser (Carroll and
Rooth 1998). The selectional restrictions are acquired using Resnik’s (1993) information-
theoretic measure of selectional association, which combines distributional and taxo-
nomic information (e.g., WordNet) to formalize how well a predicate associates with
a given argument. Two sets of experiments are run to evaluate the contribution of se-
lectional restrictions using two types of clustering algorithms: iterative clustering and
latent-class clustering (see Schulte im Walde [2000] for details). The approach is evalu-
ated on 153 verbs taken from Levin, 53 of which are polysemous (i.e., belong to more
than one class). The size of the derived clusters is restricted to four verbs and compared
to Levin: Verbs are classified correctly if they are members of a nonsingleton cluster
</bodyText>
<page confidence="0.95623">
66
</page>
<note confidence="0.731565">
Lapata and Brew Verb Class Disambiguation Using Informative Priors
</note>
<bodyText confidence="0.99993531372549">
that is a subset of a Levin class. Polysemous verbs can be assigned to distinct clusters
only using the latent-class clustering method. The best results achieve a recall of 36%
and a precision of 61% (over a baseline of 5%, calculated as the number of randomly
created clusters that are subsets of a Levin class) using subcategorization information
only and iterative clustering. Inclusion of information about selectional restrictions
yields a lower accuracy of 38% (with a recall of 20%), again using iterative clustering.
Dorr and Jones (1996) use Levin’s (1993) classification to show that there is a pre-
dictable relationship between verbal meaning and syntactic behavior. They create a
database of Levin verb classes and the sentences exemplifying them (including both
positive and negative examples, i.e., examples marked with asterisks). A parser is used
to extract basic syntactic patterns for each semantic class. These patterns form the syn-
tactic signature of the class. Dorr and Jones show that 97.9% of the semantic classes can
be identified uniquely by their syntactic signatures. Grouping verbs (instead of classes)
with identical signatures to form a semantic class yields a 6.3% overlap with Levin
classes. Dorr and Jones’s results are somewhat difficult to interpret, since in practice
information about a verb and its syntactic signature is not available, and it is pre-
cisely this information that is crucial for classifying verbs into Levin classes. Schulte
im Walde’s study and our own study show that acquisition of syntactic signatures
(i.e., subcategorization frames) from corpora is feasible; however, these acquired sig-
natures are not necessarily compatible with Levin and in most cases will depart from
those derived by Dorr and Jones, as negative examples are not available in real corpora.
Ciaramita and Johnson (2000) propose an unsupervised Bayesian model for dis-
ambiguating verbal objects that uses WordNet’s inventory of senses. For each verb
the model creates a Bayesian network whose architecture is determined by WordNet’s
hierarchy and whose parameters are estimated from a list of verb-object pairs found in
a corpus. A common problem for unsupervised models trained on verb-object tuples
is that the objects can belong to more than one semantic class. The class ambiguity
problem is commonly resolved by considering each observation of an object as evi-
dence for each of the classes the word belongs to. The formalization of the problem in
terms of Bayesian networks allows the contribution of different senses to be weighted
via explaining away (Pearl 1988): If A is a hyponym of B and C is a hyponym of B,
and B is true, then finding that C is true makes A less likely.
Prior knowledge about the likelihoods of concepts is hand coded in the network
according to the following principles: (1) It is unlikely that any given class will be
a priori selected for; (2) if a class is selected, then its hyponyms are also likely to be
selected; (3) a word is likely as the object of a verb, if at least one of its classes is selected
for. Likely and unlikely here correspond to numbers that sum up to to one. Ciaramita
and Johnson show that their model outperforms other word sense disambiguation
approaches that do not make use of prior knowledge.
Chao and Dyer (2000) propose a method for the disambiguation of polysemous
adjectives in adjective-noun combinations that also uses Bayesian networks and Word-
Net’s taxonomic information. Prior knowledge about the likelihood of different senses
or semantic classes is derived heuristically by submitting queries (e.g., great hurricane)
to the AltaVista search engine and extrapolating from the number of returned doc-
uments the frequency of the adjective-noun pair (see Mihalcea and Moldovan [1998]
for details of this technique). For each polysemous adjective-noun combination, the
synonyms representative of each sense are retrieved from WordNet (e.g., {great, large,
big} vs. {great, neat, good}). Queries are submitted to AltaVista for each synonym-noun
pair; the number of documents returned is used then as an estimate of how likely
the different adjective senses are. Chao and Dyer obtain better results when prior
knowledge is factored into their Bayesian network.
</bodyText>
<page confidence="0.996987">
67
</page>
<note confidence="0.494632">
Computational Linguistics Volume 30, Number 1
</note>
<bodyText confidence="0.999979913043478">
Our work focuses on the ambiguity inherently present in Levin’s (1993) classifi-
cation. The problem is ignored by Merlo and Stevenson (2001), who focus only on
monosemous verbs. Polysemous verbs are included in Schulte im Walde’s (2000) ex-
periments: The clustering approach can go so far as to identify more than one class
for a given verb without, however, providing information about its dominant class.
We recast Levin’s classification in a statistical framework and show in agreement with
Merlo and Stevenson and Schulte im Walde that corpus-based distributions provide
important information for semantic classification, especially in the case of polysemous
verbs whose meaning cannot be easily inferred from the immediate surrounding con-
text (i.e., subcategorization). We additionally show that the derived model is useful
not only for determining the most likely overall class for a given verb (i.e., across the
corpus), but also for disambiguating polysemous verb tokens in context.
Like Schulte im Walde (2000), our approach relies on subcategorization frames
extracted from the BNC (although using a different methodology). We employ Levin’s
inventory of semantic classes, arriving at a finer-grained classification than Merlo and
Stevenson (2001). In contrast to Schulte im Walde, we do not attempt to discover Levin
classes from corpora; instead, we exploit Levin’s classification and corpus frequencies
in order to derive a distribution of verbs, classes, and their frames that is not known a
priori but is approximated using simplifications. Our approach is not particularly tied
to Levin’s exact classification. We have presented in this article a general framework
that could be extended to related classifications such as the semantic hierarchy pro-
posed by Dang et al. (1998). In fact the latter may be more appropriate than Levin’s
original classification for our disambiguation experiments, as it is based on a tighter
correspondence between syntactic frames and semantic components and contains links
to the WordNet taxonomy.
Prior knowledge with regard to the likelihood of polysemous verb classes is ac-
quired automatically in an unsupervised manner by combining corpus frequencies
estimated from the BNC and information inherent in Levin. The models proposed by
Chao and Dyer (2000) and Ciaramita and Johnson (2000) are not directly applicable
to Levin’s classification, as the latter is not a hierarchy (and therefore not a DAG) and
cannot be straightforwardly mapped into a Bayesian network. However, in agreement
with Chao and Dyer and Ciaramita and Johnson, we show that prior knowledge about
class preferences improves word sense disambiguation performance.
Unlike Schulte im Walde (2000) and Merlo and Stevenson (2001), we ignore infor-
mation about the arguments of a given verb in the form of either selectional restric-
tions or argument structure while building our prior models. The latter information is,
however, indirectly taken into account in our disambiguation experiments: The verbs’
arguments are features for our naive Bayesian classifiers. Such information can be also
incorporated into the prior model in the form of conditional probabilities, where the
verb is, for example, conditioned on the thematic role of its arguments if this is known
(see Gildea and Jurafsky [2000] for a method that automatically labels thematic roles).
Unlike Stevenson and Merlo, Schulte im Walde, and Dorr and Jones (1996), we pro-
vide a general probabilistic model that assigns a probability to each class of a given
verb by calculating the probability of a complex expression in terms of the probability
of simpler expressions that compose it. We further show that this model is useful for
disambiguating polysemous verbs in context.
</bodyText>
<sectionHeader confidence="0.987038" genericHeader="conclusions">
Appendix: Disambiguation Results with Co-occurrences
</sectionHeader>
<bodyText confidence="0.995724">
Figures 6–9 show the performances of our naive Bayesian classifier when co-occurrences
are used as features. We experimented with four types of context: left context (Left),
</bodyText>
<page confidence="0.995273">
68
</page>
<note confidence="0.727318">
Lapata and Brew Verb Class Disambiguation Using Informative Priors
</note>
<bodyText confidence="0.991595">
right context (Right), sentential context (Sentence), and the sentence within which
the ambiguous verb is found together with its immediately preceding sentence
(PSentence). The context was encoded as lemmas or parts of speech.
</bodyText>
<figure confidence="0.8503645">
(a) (b)
Figure 6
Word sense disambiguation accuracy for “NP1 V NP2” frame.
(a) (b)
</figure>
<figureCaption confidence="0.999034">
Figure 7
</figureCaption>
<bodyText confidence="0.419559">
Word sense disambiguation accuracy for “NP1 V NP2 NP3” frame.
</bodyText>
<page confidence="0.959685">
69
</page>
<figure confidence="0.9090296">
Computational Linguistics Volume 30, Number 1
(a) (b)
Figure 8
Word sense disambiguation accuracy for “NP1 V to NP2 NP3” frame.
(a) (b)
</figure>
<figureCaption confidence="0.936518">
Figure 9
</figureCaption>
<bodyText confidence="0.508246">
Word sense disambiguation accuracy for “NP1 V for NP2 NP3” frame.
</bodyText>
<page confidence="0.985566">
70
</page>
<note confidence="0.87923">
Lapata and Brew Verb Class Disambiguation Using Informative Priors
</note>
<sectionHeader confidence="0.97698" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<reference confidence="0.876152">
Mirella Lapata was supported by ESRC
grant number R000237772. Thanks to Frank
Keller, Alex Lascarides, Katja Markert, Paola
Merlo, Sabine Schulte im Walde, Stacey
Bailey, Markus Dickinson, Anna Feldman,
Anton Rytting, and two anonymous
reviewers for valuable comments.
</reference>
<sectionHeader confidence="0.833543" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998216803571429">
Boguraev, Branimir K. and Ted Briscoe. 1989.
Utilising the LDOCE grammar codes. In
Ted Briscoe and Branimir K. Boguraev,
editors, Computational Lexicography for
Natural Language Processing. Longman,
London, pages 85–116.
Briscoe, Ted and John Carroll. 1997.
Automatic extraction of subcategorization
from corpora. In Proceedings of the Fifth
Conference on Applied Natural Language
Processing, pages 356–363, Washington,
DC.
Briscoe, Ted and Ann Copestake. 1999.
Lexical rules in constraint-based grammar.
Computational Linguistics, 25(4):487–526.
Burnard, Lou, 1995. The Users Reference Guide
for the British National Corpus. British
National Corpus Consortium, Oxford
University Computing Service.
Carroll, Glenn and Mats Rooth. 1998.
Valence induction with a head-lexicalized
PCFG. In Nancy Ide and Atro Voutilainen,
editors, Proceedings of the Third Conference on
Empirical Methods in Natural Language
Processing, pages 36–45, Granada, Spain.
Chao, Gerald and Michael G. Dyer. 2000.
Word sense disambiguation of adjectives
using probabilistic networks. In
Proceedings of the 18th International
Conference on Computational Linguistics,
pages 152–158, Saarbr¨ucken, Germany.
Ciaramita, Massimiliano and Mark Johnson.
2000. Explaining away ambiguity:
Learning verb selectional restrictions with
Bayesian networks. In Proceedings of the
18th International Conference on
Computational Linguistics, pages 187–193,
Saarbr¨ucken, Germany.
Cohen, J. 1960. A coefficient of agreement for
nominal scales. Educational and
Psychological Measurement, 20:37–46.
Corley, Steffan, Martin Corley, Frank Keller,
Matthew W. Crocker, and Shari Trewin.
2001. Finding syntactic structure in
unparsed corpora: The Gsearch corpus
query system. Computers and the
Humanities, 35(2):81–94.
Cucerzan, Silviu and David Yarowsky. 2002.
Augmented mixture models for lexical
disambiguation. In Jan Hajiˇc and Yuji
Matsumoto, editors, Proceedings of the
Conference on Empirical Methods in Natural
Language Processing, pages 33–40,
Philadelphia, PA.
Dang, Hoa Trang, Karin Kipper, Martha
Palmer, and Joseph Rosenzweig. 1998.
Investigating regular sense extensions
based on intersective Levin classes. In
Proceedings of the 17th International
Conference on Computational Linguistics and
36th Annual Meeting of the Association for
Computational Linguistics, pages 293–299,
Montr ´eal, Qu ´ebec, Canada.
Dang, Hoa Trang, Joseph Rosenzweig, and
Martha Palmer. 1997. Associating
semantic components with intersective
Levin classes. In Proceedings of the First
AMTA SIG-IL Workshop on Interlinguas,
pages 1–8, San Diego, CA.
Dorr, Bonnie J. 1997. Large-scale dictionary
construction for foreign language tutoring
and interlingual machine translation.
Machine Translation, 12(4):371–322.
Dorr, Bonnie J. and Doug Jones. 1996. Role
of word sense disambiguation in lexical
acquisition: Predicting semantics from
syntactic cues. In Proceedings of the 16th
International Conference on Computational
Linguistics, pages 322–327, Copenhagen,
Denmark.
Duda, Richard O. and Peter E. Hart. 1973.
Pattern Classification and Scene Analysis.
Wiley, New York.
Fillmore, Charles. 1965. Indirect Object
Constructions and the Ordering of
Transformations. Mouton, The Hague.
Gale, William, Kenneth W. Church, and
David Yarowsky. 1992a. Estimating upper
and lower bounds on the performance of
word-sense disambiguation programs. In
Proceedings of the 30th Annual Meeting of the
Association for Computational Linguistics,
pages 249–256, Columbus, OH.
Gale, William A., Kenneth W. Church, and
David Yarowsky. 1992b. A method for
disambiguating word senses in a large
corpus. Computers and the Humanities,
26(5–6):415–439.
Gildea, Daniel and Daniel Jurafsky. 2000.
Automatic labelling of semantic roles. In
Proceedings of the 38th Annual Meeting of the
Association for Computational Linguistics,
Hong Kong.
Goldberg, Adele. 1995. Constructions.
University of Chicago Press, Chicago.
Green, Georgia. 1974. Semantics and Syntactic
Regularity. Indiana University Press,
Bloomington.
Gropen, Jess, Steven Pinker,
Michelle Hollander, Richard M. Goldberg,
and Ronald Wilson. 1989. The learnability
and acquisition of the dative alternation.
</reference>
<page confidence="0.954864">
71
</page>
<reference confidence="0.994401609756097">
Computational Linguistics Volume 30, Number 1
Language, 65(2):203–257.
Hindle, Donald and Mats Rooth. 1993.
Structural ambiguity and lexical relations.
Computational Linguistics, 19(1):103–120.
Ide, Nancy and Jean V ´eronis. 1998.
Introduction to the special issue on word
sense disambiguation: The state of the art.
Computational Linguistics, 24(1):1–40.
Jackendoff, Ray. 1983. Semantics and
Cognition. MIT Press, Cambridge, MA.
Johnson, William E. 1932. Probability: The
deductive and inductive problems. Mind,
49:409–423.
Kipper, Karin, Hoa Trang Dang, and Martha
Palmer. 2000. Class-based construction of a
verb lexicon. In Proceedings of the 17th
National Conference on Artificial Intelligence,
pages 691–696, Austin, TX.
Klavans, Judith and Min-Yen Kan. 1998. Role
of verbs in document analysis. In
Proceedings of the 17th International
Conference on Computational Linguistics and
36th Annual Meeting of the Association for
Computational Linguistics, pages 680–688,
Montr ´eal, Qu ´ebec, Canada.
Kupiec, Julian. 1992. Robust part-of-speech
tagging using a hidden Markov model.
Computer Speech and Language, 6(3):225–242.
Landauer, Thomas K. and Susan T. Dumais.
1997. A solution to Plato’s problem: The
latent semantic analysis theory of
acquisition, induction and representation
of knowledge. Psychological Review,
104(2):211–240.
Lapata, Maria. 1999. Acquiring lexical
generalizations from corpora: A case study
for diathesis alternations. In Proceedings of
the 37th Annual Meeting of the Association for
Computational Linguistics, pages 397–404,
College Park, MD.
Lapata, Maria. 2001. The Acquisition and
Modeling of Lexical Knowledge: A
Corpus-Based Investigation of Systematic
Polysemy. Ph.D. thesis, University of
Edinburgh.
Lauer, Mark. 1995. Designing Statistical
Language Learners: Experiments on Compound
Nouns. Ph.D. thesis, Macquarie University,
Sydney, Australia.
Levin, Beth. 1993. English Verb Classes and
Alternations: A Preliminary Investigation.
University of Chicago Press, Chicago.
Levow, Gina-Anne, Bonnie Dorr, and
Dekang Lin. 2000. Construction of
Chinese-English semantic hierarchy for
information retrieval. Technical report,
University of Maryland, College Park.
McCarthy, Diana. 2000. Using semantic
preferences to identify verbal participation
in role switching alternations. In
Proceedings of the First North American
Annual Meeting of the Association for
Computational Linguistics, pages 256–263,
Seattle, WA.
Merlo, Paola and Susanne Stevenson. 2001.
Automatic verb classification based on
statistical distribution of argument
structure. Computational Linguistics,
27(3):373–408.
Mihalcea, Rada and Dan Moldovan. 1998.
Word sense disambiguation based on
semantic density. In Sanda Harabagiu,
editor, Proceedings of COLING/ACL
Workshop on Usage of WordNet in Natural
Language Processing, pages 16–22,
Montr ´eal, Qu ´ebec, Canada.
Miller, George A. and William G. Charles.
1991. Contextual correlates of semantic
similarity. Language and Cognitive Processes,
6(1):1–28.
Mooney, Raymond J. 1996. Comparative
experiments on disambiguating word
senses: An illustration of the role of bias
in machine learning. In Eric Brill and
Kenneth Church, editors, Proceedings of the
First Conference on Empirical Methods in
Natural Language Processing, pages 82–91,
Philadelphia, PA.
Ng, Hwee Tou. 1997. Exemplar-based word
sense disambiguation: Some recent
improvements. In Claire Cardie and
Ralph Weischedel, editors, Proceedings of
the Second Conference on Empirical Methods
in Natural Language Processing, pages
208–216, Providence, RI.
Palmer, Martha. 2000. Consistent criteria for
sense distinctions. Computers and the
Humanities, 34(1–2):217–222.
Palmer, Martha and Zhibiao Wu. 1995. Verb
semantics for English-Chinese translation.
Machine Translation, 10:59–92.
Patel, Malti, John A. Bullinaria, and
Joseph P. Levy. 1998. Extracting semantic
representations from large text corpora. In
John A. Bullinaria, D. W. Glasspool, and
G. Houghton, editors, Proceedings of the
Fourth Workshop on Neural Computation and
Psychology, pages 199–212. Springer,
Berlin.
Pearl, Judea. 1988. Probabilistic Reasoning in
Intelligent Systems. Morgan Kaufmann,
San Mateo, CA.
Pedersen, Ted. 2000. A simple approach to
building ensembles of naive Bayesian
classifiers for word sense disambiguation.
In Proceedings of the First North American
Annual Meeting of the Association for
Computational Linguistics, pages 63–69,
Seattle, WA.
Pedersen, Ted. 2001. A decision tree of
bigrams is an accurate predictor of word
sense. In Proceedings of the Second North
</reference>
<page confidence="0.976397">
72
</page>
<note confidence="0.697951">
Lapata and Brew Verb Class Disambiguation Using Informative Priors
</note>
<reference confidence="0.998920065217391">
American Annual Meeting of the Association
for Computational Linguistics, pages 63–69,
Pittsburgh, PA.
Pedersen, Ted and Rebecca Bruce. 1998.
Knowledge lean word-sense
disambiguation. In Proceedings of the 17th
National Conference on Artificial Intelligence,
pages 800–805, Madison, WI.
Pinker, Steven. 1989. Learnability and
Cognition: The Acquisition of Argument
Structure. MIT Press, Cambridge, MA.
Resnik, Philip Stuart. 1993. Selection and
Information: A Class-Based Approach to Lexical
Relationships. Ph.D. thesis, University of
Pennsylvania.
Schulte im Walde, Sabine. 2000. Clustering
verbs semantically according to their
alternation behaviour. In Proceedings of the
18th International Conference on
Computational Linguistics, pages 747–753,
Saarbr¨ucken, Germany.
Sch¨utze, Hinrich. 1998. Automatic word
sense discrimination. Computational
Linguistics, 24(1):97–124.
Stede, Manfred. 1998. A generative
perspective on verb alternations.
Computational Linguistics, 24(3):401–430.
Talmy, Leonard. 1985. Lexicalisation
patterns: Semantic structure in lexical
forms. In Timothy Shopen, editor,
Language Typology and Syntactic Description,
III: Grammatical Categories and the Lexicon.
Cambrige University Press, Cambridge,
pages 57–149.
Yarowsky, David. 1994. Decision lists for
lexical ambiuguity resolution: Application
to accent restoration in Spanish and
French. In Proceedings of the 32nd Annual
Meeting of the Association for Computational
Linguistics, pages 88–95, Las Cruces, NM.
Yarowsky, David. 1995. Unsupervised word
sense disambiguation rivaling supervised
methods. In Proceedings of the 33rd Annual
Meeting of the Association for Computational
Linguistics, pages 189–196, Cambridge,
MA.
</reference>
<page confidence="0.99928">
73
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.774157">
<title confidence="0.999407">Verb Class Disambiguation Using Informative Priors</title>
<author confidence="0.990537">Chris</author>
<affiliation confidence="0.999956">University of Sheffield Ohio State University</affiliation>
<abstract confidence="0.9629795">Levin’s (1993) study of verb classes is a widely used resource for lexical semantics. In her framesome verbs, such as exhibit no class ambiguity. But other verbs, such as have several alternative classes. We extend Levin’s inventory to a simple statistical model of verb class ambiguity. Using this model we are able to generate preferences for ambiguous verbs without the use of a disambiguated corpus. We additionally show that these preferences are useful as priors for a verb sense disambiguator.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Mirella</author>
</authors>
<title>Lapata was supported by ESRC grant number R000237772. Thanks to Frank Keller, Alex Lascarides,</title>
<location>Katja Markert, Paola Merlo, Sabine Schulte im Walde, Stacey Bailey, Markus Dickinson, Anna Feldman, Anton</location>
<marker>Mirella, </marker>
<rawString>Mirella Lapata was supported by ESRC grant number R000237772. Thanks to Frank Keller, Alex Lascarides, Katja Markert, Paola Merlo, Sabine Schulte im Walde, Stacey Bailey, Markus Dickinson, Anna Feldman, Anton Rytting, and two anonymous reviewers for valuable comments.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Branimir K Boguraev</author>
<author>Ted Briscoe</author>
</authors>
<title>Utilising the LDOCE grammar codes.</title>
<date>1989</date>
<booktitle>In Ted Briscoe and Branimir</booktitle>
<pages>85--116</pages>
<editor>K. Boguraev, editors,</editor>
<publisher>Longman,</publisher>
<location>London,</location>
<contexts>
<context position="31088" citStr="Boguraev and Briscoe 1989" startWordPosition="5184" endWordPosition="5187">f ,C)F(C) (d) F(C) = F( f, ci) F(c) + 1 (c) P(f |c) ≈ i all verbs. Similarly, when F(f, c) is zero, our estimate is proportional to the average F(f,C) F(C) across all classes. We do not claim that this scheme is perfect, but any deficiencies it may have are almost certainly masked by the effects of approximations and simplifications elsewhere in the system. We evaluated the performance of the model on all verbs listed in Levin (1993) that are polysemous (i.e., members of more than one class) and take frames characteristic of the widely studied dative and benefactive alternations (Pinker 1989; Boguraev and Briscoe 1989; Levin 1993; Goldberg 1995; Briscoe and Copestake 1999) and of the less well-known conative and possessor object alternations (see examples (1)–(4)). All four alternations seem fairly productive; that is, a large number of verbs undergo these alternations, according to Levin. A large number of classes license the frames that are relevant for these alternations and the verbs that inhabit these classes are likely to exhibit class ambiguity: 20 classes license the double object frame, 22 license the prepositional frame “NP1 V NP2 to NP3,” 17 classes license the benefactive “NP1 V NP2 for NP3” fr</context>
</contexts>
<marker>Boguraev, Briscoe, 1989</marker>
<rawString>Boguraev, Branimir K. and Ted Briscoe. 1989. Utilising the LDOCE grammar codes. In Ted Briscoe and Branimir K. Boguraev, editors, Computational Lexicography for Natural Language Processing. Longman, London, pages 85–116.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
</authors>
<title>Automatic extraction of subcategorization from corpora.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fifth Conference on Applied Natural Language Processing,</booktitle>
<pages>356--363</pages>
<location>Washington, DC.</location>
<contexts>
<context position="13879" citStr="Briscoe and Carroll 1997" startWordPosition="2214" endWordPosition="2217">ointed out by Kipper, Dang, and Palmer (2000), Levin classes exhibit inconsistencies, and verbs are listed in multiple classes, some of which have conflicting sets of syntactic frames. This means that some ambiguities may also arise as a result of accidental errors or inconsistencies. The classification was created not with computational uses in mind, but for human readers, so it has not been necessary to remedy all the errors and omissions that might cause trouble for machines. Similar issues arise in almost all efforts to make use of preexisting lexical resources for computational purposes (Briscoe and Carroll 1997), so none of the above comments should be taken as criticisms of Levin’s achievement. The objective of this article is to show how to train and use a probabilistic version of Levin’s classification in verb sense disambiguation. We treat errors and inconsistencies in the classification as noise. Although all our tests have used Levin’s classes and the British National Corpus, the method itself depends neither on the details of Levin’s classification nor on parochial facts about the English language. Our future work will include tests on other languages, other classifications, and other corpora.</context>
</contexts>
<marker>Briscoe, Carroll, 1997</marker>
<rawString>Briscoe, Ted and John Carroll. 1997. Automatic extraction of subcategorization from corpora. In Proceedings of the Fifth Conference on Applied Natural Language Processing, pages 356–363, Washington, DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>Ann Copestake</author>
</authors>
<title>Lexical rules in constraint-based grammar.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>4</issue>
<contexts>
<context position="31144" citStr="Briscoe and Copestake 1999" startWordPosition="5192" endWordPosition="5195">i all verbs. Similarly, when F(f, c) is zero, our estimate is proportional to the average F(f,C) F(C) across all classes. We do not claim that this scheme is perfect, but any deficiencies it may have are almost certainly masked by the effects of approximations and simplifications elsewhere in the system. We evaluated the performance of the model on all verbs listed in Levin (1993) that are polysemous (i.e., members of more than one class) and take frames characteristic of the widely studied dative and benefactive alternations (Pinker 1989; Boguraev and Briscoe 1989; Levin 1993; Goldberg 1995; Briscoe and Copestake 1999) and of the less well-known conative and possessor object alternations (see examples (1)–(4)). All four alternations seem fairly productive; that is, a large number of verbs undergo these alternations, according to Levin. A large number of classes license the frames that are relevant for these alternations and the verbs that inhabit these classes are likely to exhibit class ambiguity: 20 classes license the double object frame, 22 license the prepositional frame “NP1 V NP2 to NP3,” 17 classes license the benefactive “NP1 V NP2 for NP3” frame, 118 (out of 200) classes license the transitive fra</context>
</contexts>
<marker>Briscoe, Copestake, 1999</marker>
<rawString>Briscoe, Ted and Ann Copestake. 1999. Lexical rules in constraint-based grammar. Computational Linguistics, 25(4):487–526.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lou Burnard</author>
</authors>
<title>The Users Reference Guide for the British National Corpus.</title>
<date>1995</date>
<institution>British National Corpus Consortium, Oxford University Computing Service.</institution>
<contexts>
<context position="7218" citStr="Burnard 1995" startWordPosition="1129" endWordPosition="1130">Brew Verb Class Disambiguation Using Informative Priors Table 1 Polysemous verbs according to Levin. Classes Verbs BNC frequency 1 2,239 4,252,715 2 536 2,325,982 3 173 738,854 4 43 395,212 5 23 222,747 6 7 272,669 7 2 26,123 10 1 4,427 Figure 1 Relation between number of classes and alternations. is comparable to the total frequency of the latter (3,986,014). This means that close to half of the cases processed by a semantic tagger would manifest some degree of ambiguity. The frequencies are detailed in Table 1 and were compiled from a lemmatized version of the British National Corpus (BNC) (Burnard 1995). Furthermore, as shown in Figure 1, the level of ambiguity increases in tandem with the number of alternations licensed by a given verb. Consider, for example, verbs participating in one alternation only: Of these, 90.4% have one semantic class, 8.6% have two classes, 0.7% have three classes, and 0.3% have four classes. In contrast, of the verbs licensing six different alternations, 14% have one class, 17% have two classes, 12.4% have three classes, 53.6% have four classes, 2% have six classes, and 1% have seven classes. As ambiguity increases, so does the availability and potential utility o</context>
<context position="15949" citStr="Burnard 1995" startWordPosition="2551" endWordPosition="2552">30, Number 1 The remainder of this article is organized as follows. In Section 2 we describe the probabilistic model and the estimation of the various model parameters. In Sections 3 and 4 we report on the results of two experiments that use the model to derive the dominant class for polysemous verbs. Sections 5 and 6 discuss our verb class disambiguation experiments. We base our results on the BNC, a 100-million-word collection of samples of written and spoken language from a wide range of sources designed to represent a wide cross-section of current British English, both spoken and written (Burnard 1995). We discuss our results in Section 7 and review related work in Section 8. 2. The Prior Model Consider again the sentences in (6). Assuming that we more often write something to someone rather than for someone, we would like to derive MESSAGE TRANSFER as the prevalent class for write rather than PERFORMANCE. We view the choice of a class for a polysemous verb in a given frame as maximizing the joint probability P(c,f,v), where v is a verb subcategorizing for the frame f and inhabiting more than one Levin class c: P(c,f,v) = P(v) · P(f |v) · P(c|v,f) (10) Although the terms P(v) and P(f |v) ca</context>
</contexts>
<marker>Burnard, 1995</marker>
<rawString>Burnard, Lou, 1995. The Users Reference Guide for the British National Corpus. British National Corpus Consortium, Oxford University Computing Service.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Glenn Carroll</author>
<author>Mats Rooth</author>
</authors>
<title>Valence induction with a head-lexicalized PCFG.</title>
<date>1998</date>
<booktitle>In Nancy Ide and Atro Voutilainen, editors, Proceedings of the Third Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>36--45</pages>
<location>Granada, Spain.</location>
<contexts>
<context position="65778" citStr="Carroll and Rooth 1998" startWordPosition="10896" endWordPosition="10899">ir semantic classes. The approach is evaluated on 59 verbs manually selected from Levin (20 unergatives, 20 object drops, and 19 unaccusatives). It is assumed that these verbs are monosemous, that is, they can be ergative, unergative, or object drop. A decision-tree learner achieves an accuracy of 69.8% on the classification task over a chance baseline of 34%. Schulte im Walde (2000) uses subcategorization information and selectional restrictions to cluster verbs into Levin (1993)–compatible semantic classes. Subcategorization frames are induced from the BNC using a robust statistical parser (Carroll and Rooth 1998). The selectional restrictions are acquired using Resnik’s (1993) informationtheoretic measure of selectional association, which combines distributional and taxonomic information (e.g., WordNet) to formalize how well a predicate associates with a given argument. Two sets of experiments are run to evaluate the contribution of selectional restrictions using two types of clustering algorithms: iterative clustering and latent-class clustering (see Schulte im Walde [2000] for details). The approach is evaluated on 153 verbs taken from Levin, 53 of which are polysemous (i.e., belong to more than one</context>
</contexts>
<marker>Carroll, Rooth, 1998</marker>
<rawString>Carroll, Glenn and Mats Rooth. 1998. Valence induction with a head-lexicalized PCFG. In Nancy Ide and Atro Voutilainen, editors, Proceedings of the Third Conference on Empirical Methods in Natural Language Processing, pages 36–45, Granada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald Chao</author>
<author>Michael G Dyer</author>
</authors>
<title>Word sense disambiguation of adjectives using probabilistic networks.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International Conference on Computational Linguistics,</booktitle>
<pages>152--158</pages>
<location>Saarbr¨ucken, Germany.</location>
<contexts>
<context position="62097" citStr="Chao and Dyer (2000)" startWordPosition="10340" endWordPosition="10343">s has recently influenced work in dictionary creation (Dorr 1997; Dang et al. 1998; Dorr and Jones 1996) and notably lexicon acquisition on the basis of the assumption that verbal meaning can be gleaned from corpora using cues pertaining to syntactic structure (Merlo and Stevenson 2001; Schulte im Walde 2000; Lapata 1999; McCarthy 2000). Previous work in word sense disambiguation has not tackled explicitly the ambiguity problems arising from Levin’s classification, although methods for deriving informative priors in an unsupervised manner have been proposed by Ciaramita and Johnson (2000) and Chao and Dyer (2000) within the context of noun and adjective sense disambiguation, respectively. In this section we review related work on classification and lexicon acquisition and compare it to our own work. 65 Computational Linguistics Volume 30, Number 1 Dang et al. (1998) observe that verbs in Levin’s (1993) database are listed in more than one class. The precise meaning of this ambiguity is left open to interpretation in Levin, as it may indicate that the verb has more than one sense or that one sense (i.e., class) is primary and the alternations for this class should take precedence over the alternations </context>
<context position="69918" citStr="Chao and Dyer (2000)" startWordPosition="11563" endWordPosition="11566">is true makes A less likely. Prior knowledge about the likelihoods of concepts is hand coded in the network according to the following principles: (1) It is unlikely that any given class will be a priori selected for; (2) if a class is selected, then its hyponyms are also likely to be selected; (3) a word is likely as the object of a verb, if at least one of its classes is selected for. Likely and unlikely here correspond to numbers that sum up to to one. Ciaramita and Johnson show that their model outperforms other word sense disambiguation approaches that do not make use of prior knowledge. Chao and Dyer (2000) propose a method for the disambiguation of polysemous adjectives in adjective-noun combinations that also uses Bayesian networks and WordNet’s taxonomic information. Prior knowledge about the likelihood of different senses or semantic classes is derived heuristically by submitting queries (e.g., great hurricane) to the AltaVista search engine and extrapolating from the number of returned documents the frequency of the adjective-noun pair (see Mihalcea and Moldovan [1998] for details of this technique). For each polysemous adjective-noun combination, the synonyms representative of each sense a</context>
<context position="73220" citStr="Chao and Dyer (2000)" startWordPosition="12057" endWordPosition="12060">t could be extended to related classifications such as the semantic hierarchy proposed by Dang et al. (1998). In fact the latter may be more appropriate than Levin’s original classification for our disambiguation experiments, as it is based on a tighter correspondence between syntactic frames and semantic components and contains links to the WordNet taxonomy. Prior knowledge with regard to the likelihood of polysemous verb classes is acquired automatically in an unsupervised manner by combining corpus frequencies estimated from the BNC and information inherent in Levin. The models proposed by Chao and Dyer (2000) and Ciaramita and Johnson (2000) are not directly applicable to Levin’s classification, as the latter is not a hierarchy (and therefore not a DAG) and cannot be straightforwardly mapped into a Bayesian network. However, in agreement with Chao and Dyer and Ciaramita and Johnson, we show that prior knowledge about class preferences improves word sense disambiguation performance. Unlike Schulte im Walde (2000) and Merlo and Stevenson (2001), we ignore information about the arguments of a given verb in the form of either selectional restrictions or argument structure while building our prior mode</context>
</contexts>
<marker>Chao, Dyer, 2000</marker>
<rawString>Chao, Gerald and Michael G. Dyer. 2000. Word sense disambiguation of adjectives using probabilistic networks. In Proceedings of the 18th International Conference on Computational Linguistics, pages 152–158, Saarbr¨ucken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimiliano Ciaramita</author>
<author>Mark Johnson</author>
</authors>
<title>Explaining away ambiguity: Learning verb selectional restrictions with Bayesian networks.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International Conference on Computational Linguistics,</booktitle>
<pages>187--193</pages>
<location>Saarbr¨ucken, Germany.</location>
<contexts>
<context position="62072" citStr="Ciaramita and Johnson (2000)" startWordPosition="10335" endWordPosition="10338">rnations and verb semantic classes has recently influenced work in dictionary creation (Dorr 1997; Dang et al. 1998; Dorr and Jones 1996) and notably lexicon acquisition on the basis of the assumption that verbal meaning can be gleaned from corpora using cues pertaining to syntactic structure (Merlo and Stevenson 2001; Schulte im Walde 2000; Lapata 1999; McCarthy 2000). Previous work in word sense disambiguation has not tackled explicitly the ambiguity problems arising from Levin’s classification, although methods for deriving informative priors in an unsupervised manner have been proposed by Ciaramita and Johnson (2000) and Chao and Dyer (2000) within the context of noun and adjective sense disambiguation, respectively. In this section we review related work on classification and lexicon acquisition and compare it to our own work. 65 Computational Linguistics Volume 30, Number 1 Dang et al. (1998) observe that verbs in Levin’s (1993) database are listed in more than one class. The precise meaning of this ambiguity is left open to interpretation in Levin, as it may indicate that the verb has more than one sense or that one sense (i.e., class) is primary and the alternations for this class should take preceden</context>
<context position="68464" citStr="Ciaramita and Johnson (2000)" startWordPosition="11310" endWordPosition="11313">ses. Dorr and Jones’s results are somewhat difficult to interpret, since in practice information about a verb and its syntactic signature is not available, and it is precisely this information that is crucial for classifying verbs into Levin classes. Schulte im Walde’s study and our own study show that acquisition of syntactic signatures (i.e., subcategorization frames) from corpora is feasible; however, these acquired signatures are not necessarily compatible with Levin and in most cases will depart from those derived by Dorr and Jones, as negative examples are not available in real corpora. Ciaramita and Johnson (2000) propose an unsupervised Bayesian model for disambiguating verbal objects that uses WordNet’s inventory of senses. For each verb the model creates a Bayesian network whose architecture is determined by WordNet’s hierarchy and whose parameters are estimated from a list of verb-object pairs found in a corpus. A common problem for unsupervised models trained on verb-object tuples is that the objects can belong to more than one semantic class. The class ambiguity problem is commonly resolved by considering each observation of an object as evidence for each of the classes the word belongs to. The f</context>
<context position="73253" citStr="Ciaramita and Johnson (2000)" startWordPosition="12062" endWordPosition="12065">lated classifications such as the semantic hierarchy proposed by Dang et al. (1998). In fact the latter may be more appropriate than Levin’s original classification for our disambiguation experiments, as it is based on a tighter correspondence between syntactic frames and semantic components and contains links to the WordNet taxonomy. Prior knowledge with regard to the likelihood of polysemous verb classes is acquired automatically in an unsupervised manner by combining corpus frequencies estimated from the BNC and information inherent in Levin. The models proposed by Chao and Dyer (2000) and Ciaramita and Johnson (2000) are not directly applicable to Levin’s classification, as the latter is not a hierarchy (and therefore not a DAG) and cannot be straightforwardly mapped into a Bayesian network. However, in agreement with Chao and Dyer and Ciaramita and Johnson, we show that prior knowledge about class preferences improves word sense disambiguation performance. Unlike Schulte im Walde (2000) and Merlo and Stevenson (2001), we ignore information about the arguments of a given verb in the form of either selectional restrictions or argument structure while building our prior models. The latter information is, ho</context>
</contexts>
<marker>Ciaramita, Johnson, 2000</marker>
<rawString>Ciaramita, Massimiliano and Mark Johnson. 2000. Explaining away ambiguity: Learning verb selectional restrictions with Bayesian networks. In Proceedings of the 18th International Conference on Computational Linguistics, pages 187–193, Saarbr¨ucken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cohen</author>
</authors>
<title>A coefficient of agreement for nominal scales.</title>
<date>1960</date>
<booktitle>Educational and Psychological Measurement,</booktitle>
<pages>20--37</pages>
<contexts>
<context position="40448" citStr="Cohen 1960" startWordPosition="6738" endWordPosition="6739">00 corpus tokens. 57 Computational Linguistics Volume 30, Number 1 with the class OTHER, which was reserved for either corpus tokens that had the wrong frame or those for which the classes in question were not applicable. The judges were given annotation guidelines (for each verb) but no prior training (for details on the annotation study see Lapata [2001]). The annotation provided a gold standard for evaluating the model’s performance and enabled us to test whether humans agree on the class annotation task. We measured the judges’ agreement on the annotation task using the kappa coefficient (Cohen 1960). In general, the agreement on the class annotation task was good, with kappa values ranging from .66 to 1.00 (the mean kappa was .80, SD = .09). 4.2 Results We counted the performance of our model as correct if it agreed with the “most preferred,” that is, the most frequent, verb class, as determined in the manually annotated corpus sample by taking the average of the responses of both judges. As an example, consider the verb feed, which in the double-object frame is ambiguous between the classes FEED and GIVE. According to the model, FEED is the most likely class for feed. Out of 100 instanc</context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>Cohen, J. 1960. A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20:37–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steffan Corley</author>
<author>Martin Corley</author>
<author>Frank Keller</author>
<author>Matthew W Crocker</author>
<author>Shari Trewin</author>
</authors>
<title>Finding syntactic structure in unparsed corpora: The Gsearch corpus query system.</title>
<date>2001</date>
<booktitle>Computers and the Humanities,</booktitle>
<pages>35--2</pages>
<contexts>
<context position="19518" citStr="Corley et al. 2001" startWordPosition="3176" endWordPosition="3179"> i (g) F(c) = F(vi, c) (h) F(v, c) = F(v) · P(c|v) i It is easy to obtain P(v) from the lemmatized BNC (see (a) in Table 2). In order to estimate the probability P(f |v), we need to know how many times a verb is attested with a given frame. We acquired Levin-compatible subcategorization frames from the BNC after performing a coarse-grained mapping between Levin’s frame descriptions and surface syntactic patterns without preserving detailed semantic information about argument structure and thematic roles. This resulted in 80 frame types that were grossly compatible with Levin. We used Gsearch (Corley et al. 2001), a tool that facilitates the search of arbitrary part-of-speech-tagged corpora for shallow syntactic patterns based on a user-specified context-free grammar and a syntactic query. We specified a chunk grammar for recognizing the verbal complex, NPs, and PPs and used Gsearch to extract tokens matching the frames specified in Levin. We discarded all frames with a frequency smaller than five, as they were likely to be unreliable given our heuristic approach. The frame probability P(f) (see the denominator in (13) and equation (e) in Table 2) was also estimated on the basis of the Levin-compatibl</context>
</contexts>
<marker>Corley, Corley, Keller, Crocker, Trewin, 2001</marker>
<rawString>Corley, Steffan, Martin Corley, Frank Keller, Matthew W. Crocker, and Shari Trewin. 2001. Finding syntactic structure in unparsed corpora: The Gsearch corpus query system. Computers and the Humanities, 35(2):81–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silviu Cucerzan</author>
<author>David Yarowsky</author>
</authors>
<title>Augmented mixture models for lexical disambiguation.</title>
<date>2002</date>
<booktitle>In Jan Hajiˇc and Yuji Matsumoto, editors, Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>33--40</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="47048" citStr="Cucerzan and Yarowsky 2002" startWordPosition="7863" endWordPosition="7866">blem in supervised learning, where a disambiguator is induced from a corpus of manually sense-tagged text. The context within which the ambiguous word occurs is typically represented by a set of linguistically motivated features from which a learning algorithm induces a representative model that performs the disambiguation. A variety of classifiers have been employed for this task (see Mooney [1996] and Ide and Veronis [1998] for overviews), the most popular being decision lists (Yarowsky 1994, 1995) and naive Bayesian classifiers (Pedersen 2000; Ng 1997; Pedersen and Bruce 1998; Mooney 1996; Cucerzan and Yarowsky 2002). We employed a naive Bayesian classifier (Duda and Hart 1973) for our experiments, as it is a very convenient framework for incorporating prior knowledge and studying its influence on the classification task. In Section 5.1 we describe a basic naive Bayesian classifier and show how it can be extended with informative priors. In Section 5.2 we discuss the types of contextual features we use. We report on our experimental results in Section 6. 5.1 Naive Bayes Classification A naive Bayesian classifier assumes that all the feature variables representing a problem are conditionally independent, g</context>
</contexts>
<marker>Cucerzan, Yarowsky, 2002</marker>
<rawString>Cucerzan, Silviu and David Yarowsky. 2002. Augmented mixture models for lexical disambiguation. In Jan Hajiˇc and Yuji Matsumoto, editors, Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 33–40, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoa Trang Dang</author>
<author>Karin Kipper</author>
<author>Martha Palmer</author>
<author>Joseph Rosenzweig</author>
</authors>
<title>Investigating regular sense extensions based on intersective Levin classes.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th International Conference on Computational Linguistics and 36th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>293--299</pages>
<location>Montr ´eal, Qu ´ebec, Canada.</location>
<contexts>
<context position="7898" citStr="Dang et al. (1998)" startWordPosition="1235" endWordPosition="1238">creases in tandem with the number of alternations licensed by a given verb. Consider, for example, verbs participating in one alternation only: Of these, 90.4% have one semantic class, 8.6% have two classes, 0.7% have three classes, and 0.3% have four classes. In contrast, of the verbs licensing six different alternations, 14% have one class, 17% have two classes, 12.4% have three classes, 53.6% have four classes, 2% have six classes, and 1% have seven classes. As ambiguity increases, so does the availability and potential utility of information about diathesis alternations. Palmer (2000) and Dang et al. (1998) argue that syntactic frames and verb classes are useful for developing principled classifications of verbs. We go beyond this, showing that they can also be of assistance in disambiguation. Consider, for instance, the verb serve, which is a member of four Levin classes: GIVE, FIT, MASQUERADE, and FULFILLING. Each of these classes can in turn license four distinct syntactic frames. 47 Computational Linguistics Volume 30, Number 1 As shown in the examples2 below, in (5a) serve appears ditransitively and belongs to the semantic class of GIVE verbs, in (5b) it occurs transitively and is a member </context>
<context position="61559" citStr="Dang et al. 1998" startWordPosition="10258" endWordPosition="10261">epresenting a semantic class (see Patel, Billinaria, and Levy [1998] for a proposal similar in spirit). Another approach put forward by Dorr and Jones (1996) utilizes WordNet (Miller and Charles 1991) to find similarities (via synonymy) between unknown verbs and verbs listed in Levin. Once we have chosen a class for an unknown verb, we are entitled to assume that it will share the broad syntactic and semantic properties of that class. 8. Related Work Levin’s (1993) seminal study on diathesis alternations and verb semantic classes has recently influenced work in dictionary creation (Dorr 1997; Dang et al. 1998; Dorr and Jones 1996) and notably lexicon acquisition on the basis of the assumption that verbal meaning can be gleaned from corpora using cues pertaining to syntactic structure (Merlo and Stevenson 2001; Schulte im Walde 2000; Lapata 1999; McCarthy 2000). Previous work in word sense disambiguation has not tackled explicitly the ambiguity problems arising from Levin’s classification, although methods for deriving informative priors in an unsupervised manner have been proposed by Ciaramita and Johnson (2000) and Chao and Dyer (2000) within the context of noun and adjective sense disambiguation</context>
<context position="72708" citStr="Dang et al. (1998)" startWordPosition="11980" endWordPosition="11983">classes, arriving at a finer-grained classification than Merlo and Stevenson (2001). In contrast to Schulte im Walde, we do not attempt to discover Levin classes from corpora; instead, we exploit Levin’s classification and corpus frequencies in order to derive a distribution of verbs, classes, and their frames that is not known a priori but is approximated using simplifications. Our approach is not particularly tied to Levin’s exact classification. We have presented in this article a general framework that could be extended to related classifications such as the semantic hierarchy proposed by Dang et al. (1998). In fact the latter may be more appropriate than Levin’s original classification for our disambiguation experiments, as it is based on a tighter correspondence between syntactic frames and semantic components and contains links to the WordNet taxonomy. Prior knowledge with regard to the likelihood of polysemous verb classes is acquired automatically in an unsupervised manner by combining corpus frequencies estimated from the BNC and information inherent in Levin. The models proposed by Chao and Dyer (2000) and Ciaramita and Johnson (2000) are not directly applicable to Levin’s classification,</context>
</contexts>
<marker>Dang, Kipper, Palmer, Rosenzweig, 1998</marker>
<rawString>Dang, Hoa Trang, Karin Kipper, Martha Palmer, and Joseph Rosenzweig. 1998. Investigating regular sense extensions based on intersective Levin classes. In Proceedings of the 17th International Conference on Computational Linguistics and 36th Annual Meeting of the Association for Computational Linguistics, pages 293–299, Montr ´eal, Qu ´ebec, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoa Trang Dang</author>
<author>Joseph Rosenzweig</author>
<author>Martha Palmer</author>
</authors>
<title>Associating semantic components with intersective Levin classes.</title>
<date>1997</date>
<booktitle>In Proceedings of the First AMTA SIG-IL Workshop on Interlinguas,</booktitle>
<pages>1--8</pages>
<location>San Diego, CA.</location>
<marker>Dang, Rosenzweig, Palmer, 1997</marker>
<rawString>Dang, Hoa Trang, Joseph Rosenzweig, and Martha Palmer. 1997. Associating semantic components with intersective Levin classes. In Proceedings of the First AMTA SIG-IL Workshop on Interlinguas, pages 1–8, San Diego, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie J Dorr</author>
</authors>
<title>Large-scale dictionary construction for foreign language tutoring and interlingual machine translation.</title>
<date>1997</date>
<journal>Machine Translation,</journal>
<volume>12</volume>
<issue>4</issue>
<contexts>
<context position="5337" citStr="Dorr 1997" startWordPosition="828" endWordPosition="829">is on concretely available observables makes Levin’s methodology a good candidate for automation (Palmer 2000). Therefore, Levin’s (1993) classification has formed the basis for many efforts that aim to acquire lexical semantic information from corpora. These exploit syntactic cues, or at least cues that are plausibly related to syntax (Merlo and Stevenson 2001; Schulte im Walde 2000; Lapata 1999; McCarthy 2000). Other work has used Levin’s classification (in conjunction with other lexical resources) to create dictionaries that express the systematic correspondence between syntax and meaning (Dorr 1997; Dang, Rosenzweig, and Palmer 1997; Dorr and Jones 1996). Levin’s inventory of verbs and classes has been also useful for applications such as machine translation (Dorr 1997; Palmer and Wu 1995), generation (Stede 1998), information retrieval (Levow, Dorr, and Lin 2000), and document classification (Klavans and Kan 1998). Although the classification provides a general framework for describing verbal meaning, it says only which verb meanings are possible, staying silent on the relative likelihoods of the different meanings. The inventory captures systematic regularities in the meaning of words</context>
<context position="61541" citStr="Dorr 1997" startWordPosition="10256" endWordPosition="10257"> centroid representing a semantic class (see Patel, Billinaria, and Levy [1998] for a proposal similar in spirit). Another approach put forward by Dorr and Jones (1996) utilizes WordNet (Miller and Charles 1991) to find similarities (via synonymy) between unknown verbs and verbs listed in Levin. Once we have chosen a class for an unknown verb, we are entitled to assume that it will share the broad syntactic and semantic properties of that class. 8. Related Work Levin’s (1993) seminal study on diathesis alternations and verb semantic classes has recently influenced work in dictionary creation (Dorr 1997; Dang et al. 1998; Dorr and Jones 1996) and notably lexicon acquisition on the basis of the assumption that verbal meaning can be gleaned from corpora using cues pertaining to syntactic structure (Merlo and Stevenson 2001; Schulte im Walde 2000; Lapata 1999; McCarthy 2000). Previous work in word sense disambiguation has not tackled explicitly the ambiguity problems arising from Levin’s classification, although methods for deriving informative priors in an unsupervised manner have been proposed by Ciaramita and Johnson (2000) and Chao and Dyer (2000) within the context of noun and adjective se</context>
</contexts>
<marker>Dorr, 1997</marker>
<rawString>Dorr, Bonnie J. 1997. Large-scale dictionary construction for foreign language tutoring and interlingual machine translation. Machine Translation, 12(4):371–322.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie J Dorr</author>
<author>Doug Jones</author>
</authors>
<title>Role of word sense disambiguation in lexical acquisition: Predicting semantics from syntactic cues.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th International Conference on Computational Linguistics,</booktitle>
<pages>322--327</pages>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="5394" citStr="Dorr and Jones 1996" startWordPosition="836" endWordPosition="839">evin’s methodology a good candidate for automation (Palmer 2000). Therefore, Levin’s (1993) classification has formed the basis for many efforts that aim to acquire lexical semantic information from corpora. These exploit syntactic cues, or at least cues that are plausibly related to syntax (Merlo and Stevenson 2001; Schulte im Walde 2000; Lapata 1999; McCarthy 2000). Other work has used Levin’s classification (in conjunction with other lexical resources) to create dictionaries that express the systematic correspondence between syntax and meaning (Dorr 1997; Dang, Rosenzweig, and Palmer 1997; Dorr and Jones 1996). Levin’s inventory of verbs and classes has been also useful for applications such as machine translation (Dorr 1997; Palmer and Wu 1995), generation (Stede 1998), information retrieval (Levow, Dorr, and Lin 2000), and document classification (Klavans and Kan 1998). Although the classification provides a general framework for describing verbal meaning, it says only which verb meanings are possible, staying silent on the relative likelihoods of the different meanings. The inventory captures systematic regularities in the meaning of words and phrases but falls short of providing a probabilistic</context>
<context position="61100" citStr="Dorr and Jones (1996)" startWordPosition="10184" endWordPosition="10187"> can be extracted and manipulated in the form of co-occurrence vectors. One possible approach would be to compute the centroid (geometric mean) of the vectors of all members of a semantic class. Given an unknown verb (i.e., a verb not listed in Levin), we can decide its semantic class by comparing its semantic vector to the centroids of all semantic classes. For example, we could determine class membership on the basis of the distance to the closest centroid representing a semantic class (see Patel, Billinaria, and Levy [1998] for a proposal similar in spirit). Another approach put forward by Dorr and Jones (1996) utilizes WordNet (Miller and Charles 1991) to find similarities (via synonymy) between unknown verbs and verbs listed in Levin. Once we have chosen a class for an unknown verb, we are entitled to assume that it will share the broad syntactic and semantic properties of that class. 8. Related Work Levin’s (1993) seminal study on diathesis alternations and verb semantic classes has recently influenced work in dictionary creation (Dorr 1997; Dang et al. 1998; Dorr and Jones 1996) and notably lexicon acquisition on the basis of the assumption that verbal meaning can be gleaned from corpora using c</context>
<context position="67164" citStr="Dorr and Jones (1996)" startWordPosition="11109" endWordPosition="11112">ster 66 Lapata and Brew Verb Class Disambiguation Using Informative Priors that is a subset of a Levin class. Polysemous verbs can be assigned to distinct clusters only using the latent-class clustering method. The best results achieve a recall of 36% and a precision of 61% (over a baseline of 5%, calculated as the number of randomly created clusters that are subsets of a Levin class) using subcategorization information only and iterative clustering. Inclusion of information about selectional restrictions yields a lower accuracy of 38% (with a recall of 20%), again using iterative clustering. Dorr and Jones (1996) use Levin’s (1993) classification to show that there is a predictable relationship between verbal meaning and syntactic behavior. They create a database of Levin verb classes and the sentences exemplifying them (including both positive and negative examples, i.e., examples marked with asterisks). A parser is used to extract basic syntactic patterns for each semantic class. These patterns form the syntactic signature of the class. Dorr and Jones show that 97.9% of the semantic classes can be identified uniquely by their syntactic signatures. Grouping verbs (instead of classes) with identical s</context>
<context position="74358" citStr="Dorr and Jones (1996)" startWordPosition="12233" endWordPosition="12236">either selectional restrictions or argument structure while building our prior models. The latter information is, however, indirectly taken into account in our disambiguation experiments: The verbs’ arguments are features for our naive Bayesian classifiers. Such information can be also incorporated into the prior model in the form of conditional probabilities, where the verb is, for example, conditioned on the thematic role of its arguments if this is known (see Gildea and Jurafsky [2000] for a method that automatically labels thematic roles). Unlike Stevenson and Merlo, Schulte im Walde, and Dorr and Jones (1996), we provide a general probabilistic model that assigns a probability to each class of a given verb by calculating the probability of a complex expression in terms of the probability of simpler expressions that compose it. We further show that this model is useful for disambiguating polysemous verbs in context. Appendix: Disambiguation Results with Co-occurrences Figures 6–9 show the performances of our naive Bayesian classifier when co-occurrences are used as features. We experimented with four types of context: left context (Left), 68 Lapata and Brew Verb Class Disambiguation Using Informati</context>
</contexts>
<marker>Dorr, Jones, 1996</marker>
<rawString>Dorr, Bonnie J. and Doug Jones. 1996. Role of word sense disambiguation in lexical acquisition: Predicting semantics from syntactic cues. In Proceedings of the 16th International Conference on Computational Linguistics, pages 322–327, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard O Duda</author>
<author>Peter E Hart</author>
</authors>
<title>Pattern Classification and Scene Analysis.</title>
<date>1973</date>
<publisher>Wiley,</publisher>
<location>New York.</location>
<contexts>
<context position="47110" citStr="Duda and Hart 1973" startWordPosition="7873" endWordPosition="7876">orpus of manually sense-tagged text. The context within which the ambiguous word occurs is typically represented by a set of linguistically motivated features from which a learning algorithm induces a representative model that performs the disambiguation. A variety of classifiers have been employed for this task (see Mooney [1996] and Ide and Veronis [1998] for overviews), the most popular being decision lists (Yarowsky 1994, 1995) and naive Bayesian classifiers (Pedersen 2000; Ng 1997; Pedersen and Bruce 1998; Mooney 1996; Cucerzan and Yarowsky 2002). We employed a naive Bayesian classifier (Duda and Hart 1973) for our experiments, as it is a very convenient framework for incorporating prior knowledge and studying its influence on the classification task. In Section 5.1 we describe a basic naive Bayesian classifier and show how it can be extended with informative priors. In Section 5.2 we discuss the types of contextual features we use. We report on our experimental results in Section 6. 5.1 Naive Bayes Classification A naive Bayesian classifier assumes that all the feature variables representing a problem are conditionally independent, given the value of the classification variable. In word sense d</context>
</contexts>
<marker>Duda, Hart, 1973</marker>
<rawString>Duda, Richard O. and Peter E. Hart. 1973. Pattern Classification and Scene Analysis. Wiley, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Fillmore</author>
</authors>
<title>Indirect Object Constructions and the Ordering of Transformations.</title>
<date>1965</date>
<publisher>Mouton, The Hague.</publisher>
<contexts>
<context position="1063" citStr="Fillmore 1965" startWordPosition="162" endWordPosition="163">we are able to generate preferences for ambiguous verbs without the use of a disambiguated corpus. We additionally show that these preferences are useful as priors for a verb sense disambiguator. 1. Introduction Much research in lexical semantics has concentrated on the relation between verbs and their arguments. Many scholars hypothesize that the behavior of a verb, particularly with respect to the expression and interpretation of its arguments, is to a large extent determined by its meaning (Talmy 1985; Jackendoff 1983; Goldberg 1995; Levin 1993; Pinker 1989; Green 1974; Gropen et al. 1989; Fillmore 1965). The correspondence between verbal meaning and syntax has been extensively studied in Levin (1993), which argues that verbs which display the same diathesis alternations—alternations in the realization of their argument structure—can be assumed to share certain meaning components and to form a semantically coherent class. The converse of this assumption is that verb behavior (i.e., participation in diathesis alternations) can be used to provide clues about aspects of meaning, which in turn can be exploited to characterize verb senses (referred to as classes in Levin’s [1993] terminology). A m</context>
</contexts>
<marker>Fillmore, 1965</marker>
<rawString>Fillmore, Charles. 1965. Indirect Object Constructions and the Ordering of Transformations. Mouton, The Hague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Gale</author>
<author>Kenneth W Church</author>
<author>David Yarowsky</author>
</authors>
<title>Estimating upper and lower bounds on the performance of word-sense disambiguation programs.</title>
<date>1992</date>
<booktitle>In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>249--256</pages>
<location>Columbus, OH.</location>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>Gale, William, Kenneth W. Church, and David Yarowsky. 1992a. Estimating upper and lower bounds on the performance of word-sense disambiguation programs. In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics, pages 249–256, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Gale</author>
<author>Kenneth W Church</author>
<author>David Yarowsky</author>
</authors>
<title>A method for disambiguating word senses in a large corpus. Computers and the Humanities,</title>
<date>1992</date>
<pages>26--5</pages>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>Gale, William A., Kenneth W. Church, and David Yarowsky. 1992b. A method for disambiguating word senses in a large corpus. Computers and the Humanities, 26(5–6):415–439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labelling of semantic roles.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Hong Kong.</location>
<marker>Gildea, Jurafsky, 2000</marker>
<rawString>Gildea, Daniel and Daniel Jurafsky. 2000. Automatic labelling of semantic roles. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adele Goldberg</author>
</authors>
<date>1995</date>
<publisher>Constructions. University of Chicago Press,</publisher>
<location>Chicago.</location>
<contexts>
<context position="990" citStr="Goldberg 1995" startWordPosition="150" endWordPosition="151">to a simple statistical model of verb class ambiguity. Using this model we are able to generate preferences for ambiguous verbs without the use of a disambiguated corpus. We additionally show that these preferences are useful as priors for a verb sense disambiguator. 1. Introduction Much research in lexical semantics has concentrated on the relation between verbs and their arguments. Many scholars hypothesize that the behavior of a verb, particularly with respect to the expression and interpretation of its arguments, is to a large extent determined by its meaning (Talmy 1985; Jackendoff 1983; Goldberg 1995; Levin 1993; Pinker 1989; Green 1974; Gropen et al. 1989; Fillmore 1965). The correspondence between verbal meaning and syntax has been extensively studied in Levin (1993), which argues that verbs which display the same diathesis alternations—alternations in the realization of their argument structure—can be assumed to share certain meaning components and to form a semantically coherent class. The converse of this assumption is that verb behavior (i.e., participation in diathesis alternations) can be used to provide clues about aspects of meaning, which in turn can be exploited to characteriz</context>
<context position="31115" citStr="Goldberg 1995" startWordPosition="5190" endWordPosition="5191"> (c) P(f |c) ≈ i all verbs. Similarly, when F(f, c) is zero, our estimate is proportional to the average F(f,C) F(C) across all classes. We do not claim that this scheme is perfect, but any deficiencies it may have are almost certainly masked by the effects of approximations and simplifications elsewhere in the system. We evaluated the performance of the model on all verbs listed in Levin (1993) that are polysemous (i.e., members of more than one class) and take frames characteristic of the widely studied dative and benefactive alternations (Pinker 1989; Boguraev and Briscoe 1989; Levin 1993; Goldberg 1995; Briscoe and Copestake 1999) and of the less well-known conative and possessor object alternations (see examples (1)–(4)). All four alternations seem fairly productive; that is, a large number of verbs undergo these alternations, according to Levin. A large number of classes license the frames that are relevant for these alternations and the verbs that inhabit these classes are likely to exhibit class ambiguity: 20 classes license the double object frame, 22 license the prepositional frame “NP1 V NP2 to NP3,” 17 classes license the benefactive “NP1 V NP2 for NP3” frame, 118 (out of 200) class</context>
</contexts>
<marker>Goldberg, 1995</marker>
<rawString>Goldberg, Adele. 1995. Constructions. University of Chicago Press, Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgia Green</author>
</authors>
<title>Semantics and Syntactic Regularity.</title>
<date>1974</date>
<publisher>Indiana University Press,</publisher>
<location>Bloomington.</location>
<contexts>
<context position="1027" citStr="Green 1974" startWordPosition="156" endWordPosition="157">ass ambiguity. Using this model we are able to generate preferences for ambiguous verbs without the use of a disambiguated corpus. We additionally show that these preferences are useful as priors for a verb sense disambiguator. 1. Introduction Much research in lexical semantics has concentrated on the relation between verbs and their arguments. Many scholars hypothesize that the behavior of a verb, particularly with respect to the expression and interpretation of its arguments, is to a large extent determined by its meaning (Talmy 1985; Jackendoff 1983; Goldberg 1995; Levin 1993; Pinker 1989; Green 1974; Gropen et al. 1989; Fillmore 1965). The correspondence between verbal meaning and syntax has been extensively studied in Levin (1993), which argues that verbs which display the same diathesis alternations—alternations in the realization of their argument structure—can be assumed to share certain meaning components and to form a semantically coherent class. The converse of this assumption is that verb behavior (i.e., participation in diathesis alternations) can be used to provide clues about aspects of meaning, which in turn can be exploited to characterize verb senses (referred to as classes</context>
</contexts>
<marker>Green, 1974</marker>
<rawString>Green, Georgia. 1974. Semantics and Syntactic Regularity. Indiana University Press, Bloomington.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jess Gropen</author>
</authors>
<location>Steven Pinker,</location>
<marker>Gropen, </marker>
<rawString>Gropen, Jess, Steven Pinker,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michelle Hollander</author>
<author>Richard M Goldberg</author>
<author>Ronald Wilson</author>
</authors>
<title>The learnability and acquisition of the dative alternation.</title>
<date>1989</date>
<booktitle>Computational Linguistics Volume 30, Number 1 Language,</booktitle>
<volume>65</volume>
<issue>2</issue>
<marker>Hollander, Goldberg, Wilson, 1989</marker>
<rawString>Michelle Hollander, Richard M. Goldberg, and Ronald Wilson. 1989. The learnability and acquisition of the dative alternation. Computational Linguistics Volume 30, Number 1 Language, 65(2):203–257.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Hindle</author>
<author>Mats Rooth</author>
</authors>
<title>Structural ambiguity and lexical relations.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="30144" citStr="Hindle and Rooth (1993)" startWordPosition="5013" endWordPosition="5016">s (e.g., Thameslink presently carries 20,000 passengers daily) is larger than the CARRY class, it will be given a higher probability (.45 versus .4). Our estimation scheme is clearly a simplification, but it is an empirical question how much it matters. Tables 5 and 6 show the ten most frequent classes as estimated using (15) and (16). We explore the contribution of the two estimation schemes for P(c) in Experiments 1 and 2. The probabilities P(f 1c) and P(f jv) will be unreliable when the frequencies F(f, v) and F(f, c) are small and will be undefined when the frequencies are zero. Following Hindle and Rooth (1993), we smooth the observed frequencies as shown in Table 7. When F(f, v) is zero, the estimate used is proportional to the average F(f,V) F(V) across 54 Lapata and Brew Verb Class Disambiguation Using Informative Priors Table 7 Smoothed estimates. (a) P(f |v) ≈ F(f, v) + FM (b) F(f, V) = F( f, vi) F(v) + 1 i F(f, c) + Ff ,C)F(C) (d) F(C) = F( f, ci) F(c) + 1 (c) P(f |c) ≈ i all verbs. Similarly, when F(f, c) is zero, our estimate is proportional to the average F(f,C) F(C) across all classes. We do not claim that this scheme is perfect, but any deficiencies it may have are almost certainly masked</context>
</contexts>
<marker>Hindle, Rooth, 1993</marker>
<rawString>Hindle, Donald and Mats Rooth. 1993. Structural ambiguity and lexical relations. Computational Linguistics, 19(1):103–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Ide</author>
<author>Jean V ´eronis</author>
</authors>
<title>Introduction to the special issue on word sense disambiguation: The state of the art.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<marker>Ide, ´eronis, 1998</marker>
<rawString>Ide, Nancy and Jean V ´eronis. 1998. Introduction to the special issue on word sense disambiguation: The state of the art. Computational Linguistics, 24(1):1–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ray Jackendoff</author>
</authors>
<title>Semantics and Cognition.</title>
<date>1983</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="975" citStr="Jackendoff 1983" startWordPosition="148" endWordPosition="149">evin’s inventory to a simple statistical model of verb class ambiguity. Using this model we are able to generate preferences for ambiguous verbs without the use of a disambiguated corpus. We additionally show that these preferences are useful as priors for a verb sense disambiguator. 1. Introduction Much research in lexical semantics has concentrated on the relation between verbs and their arguments. Many scholars hypothesize that the behavior of a verb, particularly with respect to the expression and interpretation of its arguments, is to a large extent determined by its meaning (Talmy 1985; Jackendoff 1983; Goldberg 1995; Levin 1993; Pinker 1989; Green 1974; Gropen et al. 1989; Fillmore 1965). The correspondence between verbal meaning and syntax has been extensively studied in Levin (1993), which argues that verbs which display the same diathesis alternations—alternations in the realization of their argument structure—can be assumed to share certain meaning components and to form a semantically coherent class. The converse of this assumption is that verb behavior (i.e., participation in diathesis alternations) can be used to provide clues about aspects of meaning, which in turn can be exploited</context>
</contexts>
<marker>Jackendoff, 1983</marker>
<rawString>Jackendoff, Ray. 1983. Semantics and Cognition. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William E Johnson</author>
</authors>
<title>Probability: The deductive and inductive problems.</title>
<date>1932</date>
<journal>Mind,</journal>
<pages>49--409</pages>
<contexts>
<context position="49683" citStr="Johnson 1932" startWordPosition="8327" endWordPosition="8328">iform prior: n * c = P(c) P(ai|c) (22) i=1 n *c=P(c,v,f) P(ai|c, f , v) (23) i=1 where P(c) is estimated as shown in (d)–(g) in Table 2 and P(c, v,f), the prior for each class c corresponding to verb v in frame f, is estimated as explained in Section 2 (see (13)). As before, ai are the contextual features. The probabilities P(ai|c) and P(ai|c, f , v) can be estimated from the training data simply by counting the cooccurrence of feature ai with class c (for (22)) or the co-occurrence of ai with class c, verb v, and frame f (for (23)). For features that have zero counts, we use add-k smoothing (Johnson 1932), where k is a number less than one. 5.2 Feature Space As is common in word sense disambiguation studies, we experimented with two types of context representations, collocations and co-occurrences. Co-occurrences simply indicate whether a given word occurs within some number of words to the left or right of an ambiguous word. In this case the contextual features are binary and represent the presence or absence of a particular word in the current or preceding sentence. We used four types of context in our experiments: left context (i.e., words occurring to the left of the ambiguous word), right</context>
</contexts>
<marker>Johnson, 1932</marker>
<rawString>Johnson, William E. 1932. Probability: The deductive and inductive problems. Mind, 49:409–423.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karin Kipper</author>
<author>Hoa Trang Dang</author>
<author>Martha Palmer</author>
</authors>
<title>Class-based construction of a verb lexicon.</title>
<date>2000</date>
<booktitle>In Proceedings of the 17th National Conference on Artificial Intelligence,</booktitle>
<pages>691--696</pages>
<location>Austin, TX.</location>
<marker>Kipper, Dang, Palmer, 2000</marker>
<rawString>Kipper, Karin, Hoa Trang Dang, and Martha Palmer. 2000. Class-based construction of a verb lexicon. In Proceedings of the 17th National Conference on Artificial Intelligence, pages 691–696, Austin, TX.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judith Klavans</author>
<author>Min-Yen Kan</author>
</authors>
<title>Role of verbs in document analysis.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th International Conference on Computational Linguistics and 36th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>680--688</pages>
<location>Montr ´eal, Qu ´ebec, Canada.</location>
<contexts>
<context position="5660" citStr="Klavans and Kan 1998" startWordPosition="875" endWordPosition="878">lausibly related to syntax (Merlo and Stevenson 2001; Schulte im Walde 2000; Lapata 1999; McCarthy 2000). Other work has used Levin’s classification (in conjunction with other lexical resources) to create dictionaries that express the systematic correspondence between syntax and meaning (Dorr 1997; Dang, Rosenzweig, and Palmer 1997; Dorr and Jones 1996). Levin’s inventory of verbs and classes has been also useful for applications such as machine translation (Dorr 1997; Palmer and Wu 1995), generation (Stede 1998), information retrieval (Levow, Dorr, and Lin 2000), and document classification (Klavans and Kan 1998). Although the classification provides a general framework for describing verbal meaning, it says only which verb meanings are possible, staying silent on the relative likelihoods of the different meanings. The inventory captures systematic regularities in the meaning of words and phrases but falls short of providing a probabilistic model of these regularities. Such a model would be useful in applications that need to resolve ambiguity in the presence of multiple and conflicting probabilistic constraints. More precisely, Levin (1993) provides an index of 3,024 verbs for which she lists the sem</context>
</contexts>
<marker>Klavans, Kan, 1998</marker>
<rawString>Klavans, Judith and Min-Yen Kan. 1998. Role of verbs in document analysis. In Proceedings of the 17th International Conference on Computational Linguistics and 36th Annual Meeting of the Association for Computational Linguistics, pages 680–688, Montr ´eal, Qu ´ebec, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian Kupiec</author>
</authors>
<title>Robust part-of-speech tagging using a hidden Markov model.</title>
<date>1992</date>
<journal>Computer Speech and Language,</journal>
<volume>6</volume>
<issue>3</issue>
<contexts>
<context position="28543" citStr="Kupiec 1992" startWordPosition="4759" endWordPosition="4760">bed involves at least one gross simplification, since P(c|amb class) is calculated without reference to the identity of the verb in question. For any two verbs that fall into the same set of classes, P(c|amb class) will be the same, even though one or both may be atypical in its distribution across the classes. Furthermore, the estimation tends to favor large classes, again irrespectively of the identity of the verb in question. For example, the verb carry has three classes, CARRY, 3 Our use of ambiguity classes is inspired by a similar use in hidden Markov model–based part-of-speech tagging (Kupiec 1992). 53 Computational Linguistics Volume 30, Number 1 Table 4 Estimation of F(v, c) for the verb feed. c Icl P(clamb class) F(v, c) GIVE 15 .39 1,272.57 GORGE 8 .21 685.23 FEED 3 .08 261.04 FIT 12 .32 1,044.16 Table 5 Ten most frequent classes using equal distribution of verb frequencies. c F(c) CHARACTERIZE 601,647.4 GET 514,308.0 SAY 450,444.6 CONJECTURE 390,618.4 FUTURE HAVING 369,229.3 DECLARE 264,923.6 AMUSE 258,857.9 DIRECTED MOTION 252,775.6 MESSAGE TRANSFER 248,238.7 GIVE 208,884.1 Table 6 Ten most frequent classes using unequal distribution of verb frequencies. c F(c) GET 453,843.6 SAY 4</context>
</contexts>
<marker>Kupiec, 1992</marker>
<rawString>Kupiec, Julian. 1992. Robust part-of-speech tagging using a hidden Markov model. Computer Speech and Language, 6(3):225–242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Susan T Dumais</author>
</authors>
<title>A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<issue>2</issue>
<contexts>
<context position="60403" citStr="Landauer and Dumais 1997" startWordPosition="10070" endWordPosition="10073">e semantic class determines the subcategorization patterns of its class members independently of their identity may not be harmless for all classes and frames. Although our original aim was to develop a probabilistic framework that exploits Levin’s (1993) linguistic classification and the systematic correspondence between syntax and semantics, a limitation of the model is that it cannot infer class information for verbs not listed in Levin. For these verbs, P(c), and hence P(c, f , v), will be zero. Recent work in computational linguistics (e.g., Sch¨utze 1998) and cognitive psychology (e.g., Landauer and Dumais 1997) has shown that large corpora implicitly contain semantic information, which can be extracted and manipulated in the form of co-occurrence vectors. One possible approach would be to compute the centroid (geometric mean) of the vectors of all members of a semantic class. Given an unknown verb (i.e., a verb not listed in Levin), we can decide its semantic class by comparing its semantic vector to the centroids of all semantic classes. For example, we could determine class membership on the basis of the distance to the closest centroid representing a semantic class (see Patel, Billinaria, and Lev</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Landauer, Thomas K. and Susan T. Dumais. 1997. A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction and representation of knowledge. Psychological Review, 104(2):211–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Lapata</author>
</authors>
<title>Acquiring lexical generalizations from corpora: A case study for diathesis alternations.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>397--404</pages>
<location>College Park, MD.</location>
<contexts>
<context position="5127" citStr="Lapata 1999" startWordPosition="799" endWordPosition="800">iteria. To adopt this approach is to accept some limitations on the reach of our analyses, since not all semantically interesting differences will have the appropriate reflexes in syntax. Nevertheless, the emphasis on concretely available observables makes Levin’s methodology a good candidate for automation (Palmer 2000). Therefore, Levin’s (1993) classification has formed the basis for many efforts that aim to acquire lexical semantic information from corpora. These exploit syntactic cues, or at least cues that are plausibly related to syntax (Merlo and Stevenson 2001; Schulte im Walde 2000; Lapata 1999; McCarthy 2000). Other work has used Levin’s classification (in conjunction with other lexical resources) to create dictionaries that express the systematic correspondence between syntax and meaning (Dorr 1997; Dang, Rosenzweig, and Palmer 1997; Dorr and Jones 1996). Levin’s inventory of verbs and classes has been also useful for applications such as machine translation (Dorr 1997; Palmer and Wu 1995), generation (Stede 1998), information retrieval (Levow, Dorr, and Lin 2000), and document classification (Klavans and Kan 1998). Although the classification provides a general framework for desc</context>
<context position="61799" citStr="Lapata 1999" startWordPosition="10299" endWordPosition="10300">unknown verbs and verbs listed in Levin. Once we have chosen a class for an unknown verb, we are entitled to assume that it will share the broad syntactic and semantic properties of that class. 8. Related Work Levin’s (1993) seminal study on diathesis alternations and verb semantic classes has recently influenced work in dictionary creation (Dorr 1997; Dang et al. 1998; Dorr and Jones 1996) and notably lexicon acquisition on the basis of the assumption that verbal meaning can be gleaned from corpora using cues pertaining to syntactic structure (Merlo and Stevenson 2001; Schulte im Walde 2000; Lapata 1999; McCarthy 2000). Previous work in word sense disambiguation has not tackled explicitly the ambiguity problems arising from Levin’s classification, although methods for deriving informative priors in an unsupervised manner have been proposed by Ciaramita and Johnson (2000) and Chao and Dyer (2000) within the context of noun and adjective sense disambiguation, respectively. In this section we review related work on classification and lexicon acquisition and compare it to our own work. 65 Computational Linguistics Volume 30, Number 1 Dang et al. (1998) observe that verbs in Levin’s (1993) databa</context>
</contexts>
<marker>Lapata, 1999</marker>
<rawString>Lapata, Maria. 1999. Acquiring lexical generalizations from corpora: A case study for diathesis alternations. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pages 397–404, College Park, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Lapata</author>
</authors>
<title>The Acquisition and Modeling of Lexical Knowledge: A Corpus-Based Investigation of Systematic Polysemy.</title>
<date>2001</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<marker>Lapata, 2001</marker>
<rawString>Lapata, Maria. 2001. The Acquisition and Modeling of Lexical Knowledge: A Corpus-Based Investigation of Systematic Polysemy. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Lauer</author>
</authors>
<title>Designing Statistical Language Learners: Experiments on Compound Nouns.</title>
<date>1995</date>
<tech>Ph.D. thesis,</tech>
<institution>Macquarie University,</institution>
<location>Sydney, Australia.</location>
<contexts>
<context position="24168" citStr="Lauer 1995" startWordPosition="3994" endWordPosition="3995">v, c) reduces to the count of the verb in the corpus. Once again we cannot estimate F(v, c) for polysemous verbs directly. The task would be straightforward if we had a corpus of verbs, each labeled explicitly with class information. All we have is the overall frequency of a given verb in the BNC and the number of classes it is a member of according to Levin (1993). Since polysemous verbs can generally be the realization of more than one semantic class, counts of semantic classes can be constructed by dividing the contribution from the verb by the number of classes it belongs to (Resnik 1993; Lauer 1995). We rewrite the frequency F(v, c) as shown in (h) in Table 2 and approximate P(c|v), the true distribution of the verb and its classes, as follows: F(v) P(c|v) ≈ (15) |classes(v)| Here, F(v) is the number of times the verb v was observed in the corpus and |classes(v)| is the number of classes c it belongs to. For example, in order to estimate the frequency of the class GIVE, we consider all verbs that are listed as members of this class in Levin (1993). The class contains thirteen verbs, among which six are polysemous. We will obtain F(GIVE) by taking into account the verb frequency of the mo</context>
</contexts>
<marker>Lauer, 1995</marker>
<rawString>Lauer, Mark. 1995. Designing Statistical Language Learners: Experiments on Compound Nouns. Ph.D. thesis, Macquarie University, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Levin</author>
</authors>
<title>English Verb Classes and Alternations: A Preliminary Investigation.</title>
<date>1993</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago.</location>
<contexts>
<context position="1002" citStr="Levin 1993" startWordPosition="152" endWordPosition="153">tistical model of verb class ambiguity. Using this model we are able to generate preferences for ambiguous verbs without the use of a disambiguated corpus. We additionally show that these preferences are useful as priors for a verb sense disambiguator. 1. Introduction Much research in lexical semantics has concentrated on the relation between verbs and their arguments. Many scholars hypothesize that the behavior of a verb, particularly with respect to the expression and interpretation of its arguments, is to a large extent determined by its meaning (Talmy 1985; Jackendoff 1983; Goldberg 1995; Levin 1993; Pinker 1989; Green 1974; Gropen et al. 1989; Fillmore 1965). The correspondence between verbal meaning and syntax has been extensively studied in Levin (1993), which argues that verbs which display the same diathesis alternations—alternations in the realization of their argument structure—can be assumed to share certain meaning components and to form a semantically coherent class. The converse of this assumption is that verb behavior (i.e., participation in diathesis alternations) can be used to provide clues about aspects of meaning, which in turn can be exploited to characterize verb sense</context>
<context position="4280" citStr="Levin (1993)" startWordPosition="678" endWordPosition="679">at the fence. (4) a. I admired his honesty. b. I admired him for his honesty. Observation of the semantic and syntactic behavior of pay and give reveals that they pattern with sell in licensing the dative alternation. These verbs are all members of the GIVE class. Verbs like make and build behave similarly to carve in licensing the benefactive alternation and are members of the class of BUILD verbs. The verbs beat, kick, and hit undergo the conative alternation; they are all members of the HIT verb class. By grouping together verbs that pattern together with respect to diathesis alternations, Levin (1993) defines approximately 200 verb classes, which she argues reflect important semantic regularities. These analyses (and many similar ones by Levin and her successors) rely primarily on straightforward syntactic and syntactico-semantic criteria. To adopt this approach is to accept some limitations on the reach of our analyses, since not all semantically interesting differences will have the appropriate reflexes in syntax. Nevertheless, the emphasis on concretely available observables makes Levin’s methodology a good candidate for automation (Palmer 2000). Therefore, Levin’s (1993) classification</context>
<context position="6199" citStr="Levin (1993)" startWordPosition="955" endWordPosition="956">, Dorr, and Lin 2000), and document classification (Klavans and Kan 1998). Although the classification provides a general framework for describing verbal meaning, it says only which verb meanings are possible, staying silent on the relative likelihoods of the different meanings. The inventory captures systematic regularities in the meaning of words and phrases but falls short of providing a probabilistic model of these regularities. Such a model would be useful in applications that need to resolve ambiguity in the presence of multiple and conflicting probabilistic constraints. More precisely, Levin (1993) provides an index of 3,024 verbs for which she lists the semantic classes and diathesis alternations. The mapping between verbs and classes is not one to one. Of the 3,024 verbs which she covers, 784 are listed as having more than one class. Even though Levin’s monosemous verbs outnumber her polysemous verbs by a factor of nearly four to one, the total frequency of the former (4,252,715) 46 Lapata and Brew Verb Class Disambiguation Using Informative Priors Table 1 Polysemous verbs according to Levin. Classes Verbs BNC frequency 1 2,239 4,252,715 2 536 2,325,982 3 173 738,854 4 43 395,212 5 23</context>
<context position="23924" citStr="Levin (1993)" startWordPosition="3954" endWordPosition="3955">n frame. Both terms P(f |c) and P(c) in (13) rely on the class frequency F(c) (see (c) and (d) in Table 2). We rewrite F(c) as the sum of all verbs attested in the corpus with class c (see (g) in Table 2). For monosemous verbs the estimate of F(v, c) reduces to the count of the verb in the corpus. Once again we cannot estimate F(v, c) for polysemous verbs directly. The task would be straightforward if we had a corpus of verbs, each labeled explicitly with class information. All we have is the overall frequency of a given verb in the BNC and the number of classes it is a member of according to Levin (1993). Since polysemous verbs can generally be the realization of more than one semantic class, counts of semantic classes can be constructed by dividing the contribution from the verb by the number of classes it belongs to (Resnik 1993; Lauer 1995). We rewrite the frequency F(v, c) as shown in (h) in Table 2 and approximate P(c|v), the true distribution of the verb and its classes, as follows: F(v) P(c|v) ≈ (15) |classes(v)| Here, F(v) is the number of times the verb v was observed in the corpus and |classes(v)| is the number of classes c it belongs to. For example, in order to estimate the freque</context>
<context position="30900" citStr="Levin (1993)" startWordPosition="5158" endWordPosition="5159">oss 54 Lapata and Brew Verb Class Disambiguation Using Informative Priors Table 7 Smoothed estimates. (a) P(f |v) ≈ F(f, v) + FM (b) F(f, V) = F( f, vi) F(v) + 1 i F(f, c) + Ff ,C)F(C) (d) F(C) = F( f, ci) F(c) + 1 (c) P(f |c) ≈ i all verbs. Similarly, when F(f, c) is zero, our estimate is proportional to the average F(f,C) F(C) across all classes. We do not claim that this scheme is perfect, but any deficiencies it may have are almost certainly masked by the effects of approximations and simplifications elsewhere in the system. We evaluated the performance of the model on all verbs listed in Levin (1993) that are polysemous (i.e., members of more than one class) and take frames characteristic of the widely studied dative and benefactive alternations (Pinker 1989; Boguraev and Briscoe 1989; Levin 1993; Goldberg 1995; Briscoe and Copestake 1999) and of the less well-known conative and possessor object alternations (see examples (1)–(4)). All four alternations seem fairly productive; that is, a large number of verbs undergo these alternations, according to Levin. A large number of classes license the frames that are relevant for these alternations and the verbs that inhabit these classes are lik</context>
<context position="36933" citStr="Levin (1993)" startWordPosition="6159" endWordPosition="6160"> detailed Levin, the number of frames acquired for a given verb can be a subset or superset of the frames available in Levin. Note that the two estimation schemes yield comparable performances. This is a positive result given the importance of P(c) in the estimation of P(c,f,v). A more demanding task for our probabilistic model will be with genuinely ambiguous verbs (i.e., verbs for which the mapping between meaning and subcategorization is not one to one). Although native speakers may have intuitions about the dominant interpretation for a given verb, this information is entirely absent from Levin (1993) and from the corpus on which our model is trained. In Experiment 2 we show how our model can be used to recover this information. 4. Experiment 2: Using Corpus Distributions to Derive Verb Class Preferences 4.1 Method We evaluated the performance of our model on 67 genuinely ambiguous verbs, that is, verbs that inhabit a single frame and can be members of more than one semantic class (e.g., write). These verbs are listed in Levin (1993) and undergo the dative, benefactive, conative, and possessor object alternations. As in Experiment 1, we considered verbs with the double-object frame (3.27 a</context>
<context position="39630" citStr="Levin (1993)" startWordPosition="6605" endWordPosition="6606">0, no sampling took place; the entire set of tokens was manually annotated. This selection procedure resulted in 14 verbs with the double-object frame, 16 verbs with the frame “NP1 V NP2 to NP3,” 2 verbs with the frame “NP1 V NP2 for NP3,” 1 verb with the frame “NP1 V at NP3,” and 80 verbs with the transitive frame. From the transitive verbs we further randomly selected 34 verbs; these were manually annotated and used for evaluating the model’s performance.4 The selected tokens were annotated with class information by two judges, both linguistics graduate students. The classes were taken from Levin (1993) and augmented 4 Although the model can yield predictions for any number of verbs, evaluation could not be performed for all 80 verbs, as to perform such evaluation, our judges would have had to annotate 8,000 corpus tokens. 57 Computational Linguistics Volume 30, Number 1 with the class OTHER, which was reserved for either corpus tokens that had the wrong frame or those for which the classes in question were not applicable. The judges were given annotation guidelines (for each verb) but no prior training (for details on the annotation study see Lapata [2001]). The annotation provided a gold s</context>
<context position="65640" citStr="Levin (1993)" startWordPosition="10879" endWordPosition="10880">ric approximations of argument structure are derived from corpora and used in a machine-learning paradigm to place verbs in their semantic classes. The approach is evaluated on 59 verbs manually selected from Levin (20 unergatives, 20 object drops, and 19 unaccusatives). It is assumed that these verbs are monosemous, that is, they can be ergative, unergative, or object drop. A decision-tree learner achieves an accuracy of 69.8% on the classification task over a chance baseline of 34%. Schulte im Walde (2000) uses subcategorization information and selectional restrictions to cluster verbs into Levin (1993)–compatible semantic classes. Subcategorization frames are induced from the BNC using a robust statistical parser (Carroll and Rooth 1998). The selectional restrictions are acquired using Resnik’s (1993) informationtheoretic measure of selectional association, which combines distributional and taxonomic information (e.g., WordNet) to formalize how well a predicate associates with a given argument. Two sets of experiments are run to evaluate the contribution of selectional restrictions using two types of clustering algorithms: iterative clustering and latent-class clustering (see Schulte im Wal</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>Levin, Beth. 1993. English Verb Classes and Alternations: A Preliminary Investigation. University of Chicago Press, Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gina-Anne Levow</author>
<author>Bonnie Dorr</author>
<author>Dekang Lin</author>
</authors>
<title>Construction of Chinese-English semantic hierarchy for information retrieval.</title>
<date>2000</date>
<tech>Technical report,</tech>
<institution>University of Maryland, College Park.</institution>
<marker>Levow, Dorr, Lin, 2000</marker>
<rawString>Levow, Gina-Anne, Bonnie Dorr, and Dekang Lin. 2000. Construction of Chinese-English semantic hierarchy for information retrieval. Technical report, University of Maryland, College Park.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
</authors>
<title>Using semantic preferences to identify verbal participation in role switching alternations.</title>
<date>2000</date>
<booktitle>In Proceedings of the First North American Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>256--263</pages>
<location>Seattle, WA.</location>
<contexts>
<context position="5143" citStr="McCarthy 2000" startWordPosition="801" endWordPosition="802">opt this approach is to accept some limitations on the reach of our analyses, since not all semantically interesting differences will have the appropriate reflexes in syntax. Nevertheless, the emphasis on concretely available observables makes Levin’s methodology a good candidate for automation (Palmer 2000). Therefore, Levin’s (1993) classification has formed the basis for many efforts that aim to acquire lexical semantic information from corpora. These exploit syntactic cues, or at least cues that are plausibly related to syntax (Merlo and Stevenson 2001; Schulte im Walde 2000; Lapata 1999; McCarthy 2000). Other work has used Levin’s classification (in conjunction with other lexical resources) to create dictionaries that express the systematic correspondence between syntax and meaning (Dorr 1997; Dang, Rosenzweig, and Palmer 1997; Dorr and Jones 1996). Levin’s inventory of verbs and classes has been also useful for applications such as machine translation (Dorr 1997; Palmer and Wu 1995), generation (Stede 1998), information retrieval (Levow, Dorr, and Lin 2000), and document classification (Klavans and Kan 1998). Although the classification provides a general framework for describing verbal me</context>
<context position="61815" citStr="McCarthy 2000" startWordPosition="10301" endWordPosition="10302"> and verbs listed in Levin. Once we have chosen a class for an unknown verb, we are entitled to assume that it will share the broad syntactic and semantic properties of that class. 8. Related Work Levin’s (1993) seminal study on diathesis alternations and verb semantic classes has recently influenced work in dictionary creation (Dorr 1997; Dang et al. 1998; Dorr and Jones 1996) and notably lexicon acquisition on the basis of the assumption that verbal meaning can be gleaned from corpora using cues pertaining to syntactic structure (Merlo and Stevenson 2001; Schulte im Walde 2000; Lapata 1999; McCarthy 2000). Previous work in word sense disambiguation has not tackled explicitly the ambiguity problems arising from Levin’s classification, although methods for deriving informative priors in an unsupervised manner have been proposed by Ciaramita and Johnson (2000) and Chao and Dyer (2000) within the context of noun and adjective sense disambiguation, respectively. In this section we review related work on classification and lexicon acquisition and compare it to our own work. 65 Computational Linguistics Volume 30, Number 1 Dang et al. (1998) observe that verbs in Levin’s (1993) database are listed in</context>
</contexts>
<marker>McCarthy, 2000</marker>
<rawString>McCarthy, Diana. 2000. Using semantic preferences to identify verbal participation in role switching alternations. In Proceedings of the First North American Annual Meeting of the Association for Computational Linguistics, pages 256–263, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paola Merlo</author>
<author>Susanne Stevenson</author>
</authors>
<title>Automatic verb classification based on statistical distribution of argument structure.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>3</issue>
<contexts>
<context position="5091" citStr="Merlo and Stevenson 2001" startWordPosition="791" endWordPosition="794">aightforward syntactic and syntactico-semantic criteria. To adopt this approach is to accept some limitations on the reach of our analyses, since not all semantically interesting differences will have the appropriate reflexes in syntax. Nevertheless, the emphasis on concretely available observables makes Levin’s methodology a good candidate for automation (Palmer 2000). Therefore, Levin’s (1993) classification has formed the basis for many efforts that aim to acquire lexical semantic information from corpora. These exploit syntactic cues, or at least cues that are plausibly related to syntax (Merlo and Stevenson 2001; Schulte im Walde 2000; Lapata 1999; McCarthy 2000). Other work has used Levin’s classification (in conjunction with other lexical resources) to create dictionaries that express the systematic correspondence between syntax and meaning (Dorr 1997; Dang, Rosenzweig, and Palmer 1997; Dorr and Jones 1996). Levin’s inventory of verbs and classes has been also useful for applications such as machine translation (Dorr 1997; Palmer and Wu 1995), generation (Stede 1998), information retrieval (Levow, Dorr, and Lin 2000), and document classification (Klavans and Kan 1998). Although the classification p</context>
<context position="61763" citStr="Merlo and Stevenson 2001" startWordPosition="10291" endWordPosition="10294">991) to find similarities (via synonymy) between unknown verbs and verbs listed in Levin. Once we have chosen a class for an unknown verb, we are entitled to assume that it will share the broad syntactic and semantic properties of that class. 8. Related Work Levin’s (1993) seminal study on diathesis alternations and verb semantic classes has recently influenced work in dictionary creation (Dorr 1997; Dang et al. 1998; Dorr and Jones 1996) and notably lexicon acquisition on the basis of the assumption that verbal meaning can be gleaned from corpora using cues pertaining to syntactic structure (Merlo and Stevenson 2001; Schulte im Walde 2000; Lapata 1999; McCarthy 2000). Previous work in word sense disambiguation has not tackled explicitly the ambiguity problems arising from Levin’s classification, although methods for deriving informative priors in an unsupervised manner have been proposed by Ciaramita and Johnson (2000) and Chao and Dyer (2000) within the context of noun and adjective sense disambiguation, respectively. In this section we review related work on classification and lexicon acquisition and compare it to our own work. 65 Computational Linguistics Volume 30, Number 1 Dang et al. (1998) observe</context>
<context position="64062" citStr="Merlo and Stevenson (2001)" startWordPosition="10643" endWordPosition="10646">ia for word sense disambiguation. Most statistical approaches, including ours, treat verbal-meaning assignment as a semantic classification task. The underlying question is the following: How can corpus information be exploited in deriving the semantic class for a given verb? Despite the unifying theme of using corpora and corpus distributions for the acquisition task, the approaches differ in the inventory of classes they employ, in the methodology used for inferring semantic classes, and in the specific assumptions concerning the verbs to be classified (e.g., can they be polysemous or not). Merlo and Stevenson (2001) use grammatical features (acquired from corpora) to classify verbs into three semantic classes: unergative, unaccusative, and object drop. These classes are abstractions of Levin’s (1993) classes and as a result yield a coarser classification. For example, object-drop verbs comprise a variety of Levin classes such as GESTURE verbs, CARING verbs, LOAD verbs, PUSH-PULL verbs, MEET verbs, SOCIAL INTERACTION verbs, and AMUSE verbs. Unergative, unaccusative, and object-drop verbs have identical subcategorization patterns (i.e., they alternate between the transitive and intransitive frame), yet dis</context>
<context position="71057" citStr="Merlo and Stevenson (2001)" startWordPosition="11730" endWordPosition="11733">each polysemous adjective-noun combination, the synonyms representative of each sense are retrieved from WordNet (e.g., {great, large, big} vs. {great, neat, good}). Queries are submitted to AltaVista for each synonym-noun pair; the number of documents returned is used then as an estimate of how likely the different adjective senses are. Chao and Dyer obtain better results when prior knowledge is factored into their Bayesian network. 67 Computational Linguistics Volume 30, Number 1 Our work focuses on the ambiguity inherently present in Levin’s (1993) classification. The problem is ignored by Merlo and Stevenson (2001), who focus only on monosemous verbs. Polysemous verbs are included in Schulte im Walde’s (2000) experiments: The clustering approach can go so far as to identify more than one class for a given verb without, however, providing information about its dominant class. We recast Levin’s classification in a statistical framework and show in agreement with Merlo and Stevenson and Schulte im Walde that corpus-based distributions provide important information for semantic classification, especially in the case of polysemous verbs whose meaning cannot be easily inferred from the immediate surrounding c</context>
<context position="73662" citStr="Merlo and Stevenson (2001)" startWordPosition="12124" endWordPosition="12127">s acquired automatically in an unsupervised manner by combining corpus frequencies estimated from the BNC and information inherent in Levin. The models proposed by Chao and Dyer (2000) and Ciaramita and Johnson (2000) are not directly applicable to Levin’s classification, as the latter is not a hierarchy (and therefore not a DAG) and cannot be straightforwardly mapped into a Bayesian network. However, in agreement with Chao and Dyer and Ciaramita and Johnson, we show that prior knowledge about class preferences improves word sense disambiguation performance. Unlike Schulte im Walde (2000) and Merlo and Stevenson (2001), we ignore information about the arguments of a given verb in the form of either selectional restrictions or argument structure while building our prior models. The latter information is, however, indirectly taken into account in our disambiguation experiments: The verbs’ arguments are features for our naive Bayesian classifiers. Such information can be also incorporated into the prior model in the form of conditional probabilities, where the verb is, for example, conditioned on the thematic role of its arguments if this is known (see Gildea and Jurafsky [2000] for a method that automatically</context>
</contexts>
<marker>Merlo, Stevenson, 2001</marker>
<rawString>Merlo, Paola and Susanne Stevenson. 2001. Automatic verb classification based on statistical distribution of argument structure. Computational Linguistics, 27(3):373–408.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Dan Moldovan</author>
</authors>
<title>Word sense disambiguation based on semantic density. In</title>
<date>1998</date>
<booktitle>Proceedings of COLING/ACL Workshop on Usage of WordNet in Natural Language Processing,</booktitle>
<pages>16--22</pages>
<editor>Sanda Harabagiu, editor,</editor>
<location>Montr ´eal, Qu ´ebec, Canada.</location>
<marker>Mihalcea, Moldovan, 1998</marker>
<rawString>Mihalcea, Rada and Dan Moldovan. 1998. Word sense disambiguation based on semantic density. In Sanda Harabagiu, editor, Proceedings of COLING/ACL Workshop on Usage of WordNet in Natural Language Processing, pages 16–22, Montr ´eal, Qu ´ebec, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>William G Charles</author>
</authors>
<title>Contextual correlates of semantic similarity.</title>
<date>1991</date>
<booktitle>Language and Cognitive Processes,</booktitle>
<pages>6--1</pages>
<contexts>
<context position="61143" citStr="Miller and Charles 1991" startWordPosition="10190" endWordPosition="10193"> form of co-occurrence vectors. One possible approach would be to compute the centroid (geometric mean) of the vectors of all members of a semantic class. Given an unknown verb (i.e., a verb not listed in Levin), we can decide its semantic class by comparing its semantic vector to the centroids of all semantic classes. For example, we could determine class membership on the basis of the distance to the closest centroid representing a semantic class (see Patel, Billinaria, and Levy [1998] for a proposal similar in spirit). Another approach put forward by Dorr and Jones (1996) utilizes WordNet (Miller and Charles 1991) to find similarities (via synonymy) between unknown verbs and verbs listed in Levin. Once we have chosen a class for an unknown verb, we are entitled to assume that it will share the broad syntactic and semantic properties of that class. 8. Related Work Levin’s (1993) seminal study on diathesis alternations and verb semantic classes has recently influenced work in dictionary creation (Dorr 1997; Dang et al. 1998; Dorr and Jones 1996) and notably lexicon acquisition on the basis of the assumption that verbal meaning can be gleaned from corpora using cues pertaining to syntactic structure (Merl</context>
</contexts>
<marker>Miller, Charles, 1991</marker>
<rawString>Miller, George A. and William G. Charles. 1991. Contextual correlates of semantic similarity. Language and Cognitive Processes, 6(1):1–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raymond J Mooney</author>
</authors>
<title>Comparative experiments on disambiguating word senses: An illustration of the role of bias in machine learning.</title>
<date>1996</date>
<booktitle>In Eric Brill and Kenneth Church, editors, Proceedings of the First Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>82--91</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="47019" citStr="Mooney 1996" startWordPosition="7861" endWordPosition="7862">cast as a problem in supervised learning, where a disambiguator is induced from a corpus of manually sense-tagged text. The context within which the ambiguous word occurs is typically represented by a set of linguistically motivated features from which a learning algorithm induces a representative model that performs the disambiguation. A variety of classifiers have been employed for this task (see Mooney [1996] and Ide and Veronis [1998] for overviews), the most popular being decision lists (Yarowsky 1994, 1995) and naive Bayesian classifiers (Pedersen 2000; Ng 1997; Pedersen and Bruce 1998; Mooney 1996; Cucerzan and Yarowsky 2002). We employed a naive Bayesian classifier (Duda and Hart 1973) for our experiments, as it is a very convenient framework for incorporating prior knowledge and studying its influence on the classification task. In Section 5.1 we describe a basic naive Bayesian classifier and show how it can be extended with informative priors. In Section 5.2 we discuss the types of contextual features we use. We report on our experimental results in Section 6. 5.1 Naive Bayes Classification A naive Bayesian classifier assumes that all the feature variables representing a problem are</context>
</contexts>
<marker>Mooney, 1996</marker>
<rawString>Mooney, Raymond J. 1996. Comparative experiments on disambiguating word senses: An illustration of the role of bias in machine learning. In Eric Brill and Kenneth Church, editors, Proceedings of the First Conference on Empirical Methods in Natural Language Processing, pages 82–91, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
</authors>
<title>Exemplar-based word sense disambiguation: Some recent improvements.</title>
<date>1997</date>
<booktitle>In Claire Cardie and Ralph Weischedel, editors, Proceedings of the Second Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>208--216</pages>
<location>Providence, RI.</location>
<contexts>
<context position="46981" citStr="Ng 1997" startWordPosition="7855" endWordPosition="7856">ord sense disambiguation is often cast as a problem in supervised learning, where a disambiguator is induced from a corpus of manually sense-tagged text. The context within which the ambiguous word occurs is typically represented by a set of linguistically motivated features from which a learning algorithm induces a representative model that performs the disambiguation. A variety of classifiers have been employed for this task (see Mooney [1996] and Ide and Veronis [1998] for overviews), the most popular being decision lists (Yarowsky 1994, 1995) and naive Bayesian classifiers (Pedersen 2000; Ng 1997; Pedersen and Bruce 1998; Mooney 1996; Cucerzan and Yarowsky 2002). We employed a naive Bayesian classifier (Duda and Hart 1973) for our experiments, as it is a very convenient framework for incorporating prior knowledge and studying its influence on the classification task. In Section 5.1 we describe a basic naive Bayesian classifier and show how it can be extended with informative priors. In Section 5.2 we discuss the types of contextual features we use. We report on our experimental results in Section 6. 5.1 Naive Bayes Classification A naive Bayesian classifier assumes that all the featur</context>
</contexts>
<marker>Ng, 1997</marker>
<rawString>Ng, Hwee Tou. 1997. Exemplar-based word sense disambiguation: Some recent improvements. In Claire Cardie and Ralph Weischedel, editors, Proceedings of the Second Conference on Empirical Methods in Natural Language Processing, pages 208–216, Providence, RI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
</authors>
<title>Consistent criteria for sense distinctions. Computers and the Humanities,</title>
<date>2000</date>
<pages>34--1</pages>
<contexts>
<context position="1902" citStr="Palmer 2000" startWordPosition="286" endWordPosition="287">ure—can be assumed to share certain meaning components and to form a semantically coherent class. The converse of this assumption is that verb behavior (i.e., participation in diathesis alternations) can be used to provide clues about aspects of meaning, which in turn can be exploited to characterize verb senses (referred to as classes in Levin’s [1993] terminology). A major advantage of this approach is that criteria for assigning senses can be more concrete than is traditionally assumed in lexicographic work (e.g., WordNet or machine-readable dictionaries) concerned with sense distinctions (Palmer 2000). As an example consider sentences (1)–(4), taken from Levin. Examples (1) and (2) illustrate the dative and benefactive alternations, respectively. Dative verbs alternate between the prepositional frame “NP1 V NP2 to NP3” (see (1a)) and the double-object frame “NP1 V NP2 NP3” (see (1b)), whereas benefactive verbs alternate between the doubleobject frame (see (2a)) and the prepositional frame “NP1 V NP2 for NP3” (see (2b)). To decide whether a verb is benefactive or dative it suffices to test the acceptability of the for and to frames. Verbs undergoing the conative alternation can be attested </context>
<context position="4838" citStr="Palmer 2000" startWordPosition="755" endWordPosition="756">r with respect to diathesis alternations, Levin (1993) defines approximately 200 verb classes, which she argues reflect important semantic regularities. These analyses (and many similar ones by Levin and her successors) rely primarily on straightforward syntactic and syntactico-semantic criteria. To adopt this approach is to accept some limitations on the reach of our analyses, since not all semantically interesting differences will have the appropriate reflexes in syntax. Nevertheless, the emphasis on concretely available observables makes Levin’s methodology a good candidate for automation (Palmer 2000). Therefore, Levin’s (1993) classification has formed the basis for many efforts that aim to acquire lexical semantic information from corpora. These exploit syntactic cues, or at least cues that are plausibly related to syntax (Merlo and Stevenson 2001; Schulte im Walde 2000; Lapata 1999; McCarthy 2000). Other work has used Levin’s classification (in conjunction with other lexical resources) to create dictionaries that express the systematic correspondence between syntax and meaning (Dorr 1997; Dang, Rosenzweig, and Palmer 1997; Dorr and Jones 1996). Levin’s inventory of verbs and classes has</context>
<context position="7875" citStr="Palmer (2000)" startWordPosition="1232" endWordPosition="1233">el of ambiguity increases in tandem with the number of alternations licensed by a given verb. Consider, for example, verbs participating in one alternation only: Of these, 90.4% have one semantic class, 8.6% have two classes, 0.7% have three classes, and 0.3% have four classes. In contrast, of the verbs licensing six different alternations, 14% have one class, 17% have two classes, 12.4% have three classes, 53.6% have four classes, 2% have six classes, and 1% have seven classes. As ambiguity increases, so does the availability and potential utility of information about diathesis alternations. Palmer (2000) and Dang et al. (1998) argue that syntactic frames and verb classes are useful for developing principled classifications of verbs. We go beyond this, showing that they can also be of assistance in disambiguation. Consider, for instance, the verb serve, which is a member of four Levin classes: GIVE, FIT, MASQUERADE, and FULFILLING. Each of these classes can in turn license four distinct syntactic frames. 47 Computational Linguistics Volume 30, Number 1 As shown in the examples2 below, in (5a) serve appears ditransitively and belongs to the semantic class of GIVE verbs, in (5b) it occurs transi</context>
<context position="13299" citStr="Palmer (2000)" startWordPosition="2125" endWordPosition="2126">te world knowledge. It is important to point out that Levin’s (1993) classification is not intended as an exhaustive description of English verbs, their meanings, and their likelihood. Many other classifications could have been built using the same principles. A different grouping might, for example, have occurred if finer or coarser semantic distinctions were taken into account (see Merlo and Stevenson [2001] and Dang, Rosenzweig, and Palmer [1997] for alternative classifications) or if the containment of ambiguity was one of the classification objectives. As pointed out by Kipper, Dang, and Palmer (2000), Levin classes exhibit inconsistencies, and verbs are listed in multiple classes, some of which have conflicting sets of syntactic frames. This means that some ambiguities may also arise as a result of accidental errors or inconsistencies. The classification was created not with computational uses in mind, but for human readers, so it has not been necessary to remedy all the errors and omissions that might cause trouble for machines. Similar issues arise in almost all efforts to make use of preexisting lexical resources for computational purposes (Briscoe and Carroll 1997), so none of the abo</context>
</contexts>
<marker>Palmer, 2000</marker>
<rawString>Palmer, Martha. 2000. Consistent criteria for sense distinctions. Computers and the Humanities, 34(1–2):217–222.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Zhibiao Wu</author>
</authors>
<title>Verb semantics for English-Chinese translation.</title>
<date>1995</date>
<journal>Machine Translation,</journal>
<pages>10--59</pages>
<contexts>
<context position="5532" citStr="Palmer and Wu 1995" startWordPosition="858" endWordPosition="861">forts that aim to acquire lexical semantic information from corpora. These exploit syntactic cues, or at least cues that are plausibly related to syntax (Merlo and Stevenson 2001; Schulte im Walde 2000; Lapata 1999; McCarthy 2000). Other work has used Levin’s classification (in conjunction with other lexical resources) to create dictionaries that express the systematic correspondence between syntax and meaning (Dorr 1997; Dang, Rosenzweig, and Palmer 1997; Dorr and Jones 1996). Levin’s inventory of verbs and classes has been also useful for applications such as machine translation (Dorr 1997; Palmer and Wu 1995), generation (Stede 1998), information retrieval (Levow, Dorr, and Lin 2000), and document classification (Klavans and Kan 1998). Although the classification provides a general framework for describing verbal meaning, it says only which verb meanings are possible, staying silent on the relative likelihoods of the different meanings. The inventory captures systematic regularities in the meaning of words and phrases but falls short of providing a probabilistic model of these regularities. Such a model would be useful in applications that need to resolve ambiguity in the presence of multiple and </context>
</contexts>
<marker>Palmer, Wu, 1995</marker>
<rawString>Palmer, Martha and Zhibiao Wu. 1995. Verb semantics for English-Chinese translation. Machine Translation, 10:59–92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Malti Patel</author>
<author>John A Bullinaria</author>
<author>Joseph P Levy</author>
</authors>
<title>Extracting semantic representations from large text corpora. In</title>
<date>1998</date>
<booktitle>Proceedings of the Fourth Workshop on Neural Computation and Psychology,</booktitle>
<pages>199--212</pages>
<editor>John A. Bullinaria, D. W. Glasspool, and G. Houghton, editors,</editor>
<publisher>Springer,</publisher>
<location>Berlin.</location>
<marker>Patel, Bullinaria, Levy, 1998</marker>
<rawString>Patel, Malti, John A. Bullinaria, and Joseph P. Levy. 1998. Extracting semantic representations from large text corpora. In John A. Bullinaria, D. W. Glasspool, and G. Houghton, editors, Proceedings of the Fourth Workshop on Neural Computation and Psychology, pages 199–212. Springer, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judea Pearl</author>
</authors>
<title>Probabilistic Reasoning in Intelligent Systems.</title>
<date>1988</date>
<publisher>Morgan Kaufmann,</publisher>
<location>San Mateo, CA.</location>
<contexts>
<context position="69213" citStr="Pearl 1988" startWordPosition="11431" endWordPosition="11432"> creates a Bayesian network whose architecture is determined by WordNet’s hierarchy and whose parameters are estimated from a list of verb-object pairs found in a corpus. A common problem for unsupervised models trained on verb-object tuples is that the objects can belong to more than one semantic class. The class ambiguity problem is commonly resolved by considering each observation of an object as evidence for each of the classes the word belongs to. The formalization of the problem in terms of Bayesian networks allows the contribution of different senses to be weighted via explaining away (Pearl 1988): If A is a hyponym of B and C is a hyponym of B, and B is true, then finding that C is true makes A less likely. Prior knowledge about the likelihoods of concepts is hand coded in the network according to the following principles: (1) It is unlikely that any given class will be a priori selected for; (2) if a class is selected, then its hyponyms are also likely to be selected; (3) a word is likely as the object of a verb, if at least one of its classes is selected for. Likely and unlikely here correspond to numbers that sum up to to one. Ciaramita and Johnson show that their model outperforms</context>
</contexts>
<marker>Pearl, 1988</marker>
<rawString>Pearl, Judea. 1988. Probabilistic Reasoning in Intelligent Systems. Morgan Kaufmann, San Mateo, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
</authors>
<title>A simple approach to building ensembles of naive Bayesian classifiers for word sense disambiguation.</title>
<date>2000</date>
<booktitle>In Proceedings of the First North American Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>63--69</pages>
<location>Seattle, WA.</location>
<contexts>
<context position="46972" citStr="Pedersen 2000" startWordPosition="7853" endWordPosition="7854"> information. Word sense disambiguation is often cast as a problem in supervised learning, where a disambiguator is induced from a corpus of manually sense-tagged text. The context within which the ambiguous word occurs is typically represented by a set of linguistically motivated features from which a learning algorithm induces a representative model that performs the disambiguation. A variety of classifiers have been employed for this task (see Mooney [1996] and Ide and Veronis [1998] for overviews), the most popular being decision lists (Yarowsky 1994, 1995) and naive Bayesian classifiers (Pedersen 2000; Ng 1997; Pedersen and Bruce 1998; Mooney 1996; Cucerzan and Yarowsky 2002). We employed a naive Bayesian classifier (Duda and Hart 1973) for our experiments, as it is a very convenient framework for incorporating prior knowledge and studying its influence on the classification task. In Section 5.1 we describe a basic naive Bayesian classifier and show how it can be extended with informative priors. In Section 5.2 we discuss the types of contextual features we use. We report on our experimental results in Section 6. 5.1 Naive Bayes Classification A naive Bayesian classifier assumes that all t</context>
</contexts>
<marker>Pedersen, 2000</marker>
<rawString>Pedersen, Ted. 2000. A simple approach to building ensembles of naive Bayesian classifiers for word sense disambiguation. In Proceedings of the First North American Annual Meeting of the Association for Computational Linguistics, pages 63–69, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
</authors>
<title>A decision tree of bigrams is an accurate predictor of word sense.</title>
<date>2001</date>
<booktitle>In Proceedings of the Second</booktitle>
<publisher>North</publisher>
<contexts>
<context position="53041" citStr="Pedersen 2001" startWordPosition="8913" endWordPosition="8914">ation experiments as it was represented solely by the verb kick (50 instances). In this study we compare a naive Bayesian classifier that relies on a uniform prior (see (20)) against two classifiers that make use of nonuniform prior models: The classifier in (22) effectively uses as prior the baseline model P(c) from Section 2, whereas the classifier in (23) relies on the more informative model P(c,f, v). As a baseline for the disambiguation task, we simply assign the most common class in the training data to every instance in the test data, ignoring context and any form of prior information (Pedersen 2001; Gale, Church, and Yarowsky 1992a). We also report an upper bound on disambiguation performance by measuring how well human judges agree with one another (percentage agreement) on the class assignment task. Recall from Section 4.1 that our corpus was annotated by two judges with Levin-compatible verb classes. 6.2 Results The results of our class disambiguation experiments are summarized in Figures 2–5. In order to investigate differences among different frames, we show how the naive Bayesian classifiers perform for each frame individually. Figures 2–5 (x-axis) also reveal the influence of col</context>
</contexts>
<marker>Pedersen, 2001</marker>
<rawString>Pedersen, Ted. 2001. A decision tree of bigrams is an accurate predictor of word sense. In Proceedings of the Second North</rawString>
</citation>
<citation valid="false">
<booktitle>American Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>63--69</pages>
<location>Pittsburgh, PA.</location>
<marker></marker>
<rawString>American Annual Meeting of the Association for Computational Linguistics, pages 63–69, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Rebecca Bruce</author>
</authors>
<title>Knowledge lean word-sense disambiguation.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th National Conference on Artificial Intelligence,</booktitle>
<pages>800--805</pages>
<location>Madison, WI.</location>
<contexts>
<context position="47006" citStr="Pedersen and Bruce 1998" startWordPosition="7857" endWordPosition="7860"> disambiguation is often cast as a problem in supervised learning, where a disambiguator is induced from a corpus of manually sense-tagged text. The context within which the ambiguous word occurs is typically represented by a set of linguistically motivated features from which a learning algorithm induces a representative model that performs the disambiguation. A variety of classifiers have been employed for this task (see Mooney [1996] and Ide and Veronis [1998] for overviews), the most popular being decision lists (Yarowsky 1994, 1995) and naive Bayesian classifiers (Pedersen 2000; Ng 1997; Pedersen and Bruce 1998; Mooney 1996; Cucerzan and Yarowsky 2002). We employed a naive Bayesian classifier (Duda and Hart 1973) for our experiments, as it is a very convenient framework for incorporating prior knowledge and studying its influence on the classification task. In Section 5.1 we describe a basic naive Bayesian classifier and show how it can be extended with informative priors. In Section 5.2 we discuss the types of contextual features we use. We report on our experimental results in Section 6. 5.1 Naive Bayes Classification A naive Bayesian classifier assumes that all the feature variables representing </context>
</contexts>
<marker>Pedersen, Bruce, 1998</marker>
<rawString>Pedersen, Ted and Rebecca Bruce. 1998. Knowledge lean word-sense disambiguation. In Proceedings of the 17th National Conference on Artificial Intelligence, pages 800–805, Madison, WI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Pinker</author>
</authors>
<title>Learnability and Cognition: The Acquisition of Argument Structure.</title>
<date>1989</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="1015" citStr="Pinker 1989" startWordPosition="154" endWordPosition="155">el of verb class ambiguity. Using this model we are able to generate preferences for ambiguous verbs without the use of a disambiguated corpus. We additionally show that these preferences are useful as priors for a verb sense disambiguator. 1. Introduction Much research in lexical semantics has concentrated on the relation between verbs and their arguments. Many scholars hypothesize that the behavior of a verb, particularly with respect to the expression and interpretation of its arguments, is to a large extent determined by its meaning (Talmy 1985; Jackendoff 1983; Goldberg 1995; Levin 1993; Pinker 1989; Green 1974; Gropen et al. 1989; Fillmore 1965). The correspondence between verbal meaning and syntax has been extensively studied in Levin (1993), which argues that verbs which display the same diathesis alternations—alternations in the realization of their argument structure—can be assumed to share certain meaning components and to form a semantically coherent class. The converse of this assumption is that verb behavior (i.e., participation in diathesis alternations) can be used to provide clues about aspects of meaning, which in turn can be exploited to characterize verb senses (referred t</context>
<context position="31061" citStr="Pinker 1989" startWordPosition="5182" endWordPosition="5183">i F(f, c) + Ff ,C)F(C) (d) F(C) = F( f, ci) F(c) + 1 (c) P(f |c) ≈ i all verbs. Similarly, when F(f, c) is zero, our estimate is proportional to the average F(f,C) F(C) across all classes. We do not claim that this scheme is perfect, but any deficiencies it may have are almost certainly masked by the effects of approximations and simplifications elsewhere in the system. We evaluated the performance of the model on all verbs listed in Levin (1993) that are polysemous (i.e., members of more than one class) and take frames characteristic of the widely studied dative and benefactive alternations (Pinker 1989; Boguraev and Briscoe 1989; Levin 1993; Goldberg 1995; Briscoe and Copestake 1999) and of the less well-known conative and possessor object alternations (see examples (1)–(4)). All four alternations seem fairly productive; that is, a large number of verbs undergo these alternations, according to Levin. A large number of classes license the frames that are relevant for these alternations and the verbs that inhabit these classes are likely to exhibit class ambiguity: 20 classes license the double object frame, 22 license the prepositional frame “NP1 V NP2 to NP3,” 17 classes license the benefac</context>
</contexts>
<marker>Pinker, 1989</marker>
<rawString>Pinker, Steven. 1989. Learnability and Cognition: The Acquisition of Argument Structure. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Stuart Resnik</author>
</authors>
<title>Selection and Information: A Class-Based Approach to Lexical Relationships.</title>
<date>1993</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="24155" citStr="Resnik 1993" startWordPosition="3992" endWordPosition="3993">stimate of F(v, c) reduces to the count of the verb in the corpus. Once again we cannot estimate F(v, c) for polysemous verbs directly. The task would be straightforward if we had a corpus of verbs, each labeled explicitly with class information. All we have is the overall frequency of a given verb in the BNC and the number of classes it is a member of according to Levin (1993). Since polysemous verbs can generally be the realization of more than one semantic class, counts of semantic classes can be constructed by dividing the contribution from the verb by the number of classes it belongs to (Resnik 1993; Lauer 1995). We rewrite the frequency F(v, c) as shown in (h) in Table 2 and approximate P(c|v), the true distribution of the verb and its classes, as follows: F(v) P(c|v) ≈ (15) |classes(v)| Here, F(v) is the number of times the verb v was observed in the corpus and |classes(v)| is the number of classes c it belongs to. For example, in order to estimate the frequency of the class GIVE, we consider all verbs that are listed as members of this class in Levin (1993). The class contains thirteen verbs, among which six are polysemous. We will obtain F(GIVE) by taking into account the verb freque</context>
</contexts>
<marker>Resnik, 1993</marker>
<rawString>Resnik, Philip Stuart. 1993. Selection and Information: A Class-Based Approach to Lexical Relationships. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Schulte im Walde</author>
<author>Sabine</author>
</authors>
<title>Clustering verbs semantically according to their alternation behaviour.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International Conference on Computational Linguistics,</booktitle>
<pages>747--753</pages>
<location>Saarbr¨ucken, Germany.</location>
<marker>Walde, Sabine, 2000</marker>
<rawString>Schulte im Walde, Sabine. 2000. Clustering verbs semantically according to their alternation behaviour. In Proceedings of the 18th International Conference on Computational Linguistics, pages 747–753, Saarbr¨ucken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<marker>Sch¨utze, 1998</marker>
<rawString>Sch¨utze, Hinrich. 1998. Automatic word sense discrimination. Computational Linguistics, 24(1):97–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manfred Stede</author>
</authors>
<title>A generative perspective on verb alternations.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>3</issue>
<contexts>
<context position="5557" citStr="Stede 1998" startWordPosition="863" endWordPosition="864"> semantic information from corpora. These exploit syntactic cues, or at least cues that are plausibly related to syntax (Merlo and Stevenson 2001; Schulte im Walde 2000; Lapata 1999; McCarthy 2000). Other work has used Levin’s classification (in conjunction with other lexical resources) to create dictionaries that express the systematic correspondence between syntax and meaning (Dorr 1997; Dang, Rosenzweig, and Palmer 1997; Dorr and Jones 1996). Levin’s inventory of verbs and classes has been also useful for applications such as machine translation (Dorr 1997; Palmer and Wu 1995), generation (Stede 1998), information retrieval (Levow, Dorr, and Lin 2000), and document classification (Klavans and Kan 1998). Although the classification provides a general framework for describing verbal meaning, it says only which verb meanings are possible, staying silent on the relative likelihoods of the different meanings. The inventory captures systematic regularities in the meaning of words and phrases but falls short of providing a probabilistic model of these regularities. Such a model would be useful in applications that need to resolve ambiguity in the presence of multiple and conflicting probabilistic</context>
</contexts>
<marker>Stede, 1998</marker>
<rawString>Stede, Manfred. 1998. A generative perspective on verb alternations. Computational Linguistics, 24(3):401–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonard Talmy</author>
</authors>
<title>Lexicalisation patterns: Semantic structure in lexical forms.</title>
<date>1985</date>
<booktitle>Language Typology and Syntactic Description, III: Grammatical Categories and the Lexicon.</booktitle>
<pages>57--149</pages>
<editor>In Timothy Shopen, editor,</editor>
<publisher>Cambrige University Press,</publisher>
<location>Cambridge,</location>
<contexts>
<context position="958" citStr="Talmy 1985" startWordPosition="146" endWordPosition="147"> We extend Levin’s inventory to a simple statistical model of verb class ambiguity. Using this model we are able to generate preferences for ambiguous verbs without the use of a disambiguated corpus. We additionally show that these preferences are useful as priors for a verb sense disambiguator. 1. Introduction Much research in lexical semantics has concentrated on the relation between verbs and their arguments. Many scholars hypothesize that the behavior of a verb, particularly with respect to the expression and interpretation of its arguments, is to a large extent determined by its meaning (Talmy 1985; Jackendoff 1983; Goldberg 1995; Levin 1993; Pinker 1989; Green 1974; Gropen et al. 1989; Fillmore 1965). The correspondence between verbal meaning and syntax has been extensively studied in Levin (1993), which argues that verbs which display the same diathesis alternations—alternations in the realization of their argument structure—can be assumed to share certain meaning components and to form a semantically coherent class. The converse of this assumption is that verb behavior (i.e., participation in diathesis alternations) can be used to provide clues about aspects of meaning, which in turn</context>
</contexts>
<marker>Talmy, 1985</marker>
<rawString>Talmy, Leonard. 1985. Lexicalisation patterns: Semantic structure in lexical forms. In Timothy Shopen, editor, Language Typology and Syntactic Description, III: Grammatical Categories and the Lexicon. Cambrige University Press, Cambridge, pages 57–149.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Decision lists for lexical ambiuguity resolution: Application to accent restoration in Spanish and French.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>88--95</pages>
<location>Las Cruces, NM.</location>
<contexts>
<context position="46919" citStr="Yarowsky 1994" startWordPosition="7845" endWordPosition="7846">guation experiment that takes advantage of this prior information. Word sense disambiguation is often cast as a problem in supervised learning, where a disambiguator is induced from a corpus of manually sense-tagged text. The context within which the ambiguous word occurs is typically represented by a set of linguistically motivated features from which a learning algorithm induces a representative model that performs the disambiguation. A variety of classifiers have been employed for this task (see Mooney [1996] and Ide and Veronis [1998] for overviews), the most popular being decision lists (Yarowsky 1994, 1995) and naive Bayesian classifiers (Pedersen 2000; Ng 1997; Pedersen and Bruce 1998; Mooney 1996; Cucerzan and Yarowsky 2002). We employed a naive Bayesian classifier (Duda and Hart 1973) for our experiments, as it is a very convenient framework for incorporating prior knowledge and studying its influence on the classification task. In Section 5.1 we describe a basic naive Bayesian classifier and show how it can be extended with informative priors. In Section 5.2 we discuss the types of contextual features we use. We report on our experimental results in Section 6. 5.1 Naive Bayes Classifi</context>
</contexts>
<marker>Yarowsky, 1994</marker>
<rawString>Yarowsky, David. 1994. Decision lists for lexical ambiuguity resolution: Application to accent restoration in Spanish and French. In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, pages 88–95, Las Cruces, NM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>189--196</pages>
<location>Cambridge, MA.</location>
<marker>Yarowsky, 1995</marker>
<rawString>Yarowsky, David. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, pages 189–196, Cambridge, MA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>