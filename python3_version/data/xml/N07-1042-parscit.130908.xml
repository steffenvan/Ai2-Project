<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.97001">
Multi-document Relationship Fusion via
Constraints on Probabilistic Databases
</title>
<author confidence="0.999109">
Gideon Mann
</author>
<affiliation confidence="0.9985945">
Department of Computer Science
University of Massachusetts
</affiliation>
<address confidence="0.802814">
Amherst, MA 01003
</address>
<email confidence="0.984451">
gideon.mann@gmail.com
</email>
<sectionHeader confidence="0.994688" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999592230769231">
Previous multi-document relationship ex-
traction and fusion research has focused
on single relationships. Shifting the fo-
cus to multiple relationships allows for the
use of mutual constraints to aid extrac-
tion. This paper presents a fusion method
which uses a probabilistic database model
to pick relationships which violate few
constraints. This model allows improved
performance on constructing corporate
succession timelines from multiple doc-
uments with respect to a multi-document
fusion baseline.
</bodyText>
<sectionHeader confidence="0.998121" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9960984">
Single document information extraction of named
entities and relationships has received much atten-
tion since the MUC evaluations1 in the mid-90s (Ap-
pelt et al., 1993; Grishman and Sundheim, 1996).
Recently, there has been increased interest in the
extraction of named entities and relationships from
multiple documents, since the redundancy of infor-
mation across documents has been shown to be a
powerful resource for obtaining high quality infor-
mation even when the extractors have access to little
or no training data (Etzioni et al., 2004; Hasegawa
et al., 2004). Much of the recent work in multi-
document relationship extraction has focused on
the extraction of isolated relationships (Agichtein,
2005; Pasca et al., 2006), but often the goal, as in
</bodyText>
<footnote confidence="0.865276">
1http://www.itl.nist.gov/iaui/894.02/related projects/muc/
</footnote>
<bodyText confidence="0.999837272727273">
single document tasks like MUC, is to extract a tem-
plate or a relational database composed of related
facts.
With databases containing multiple relationships,
the semantics of the database impose constraints
on possible database configurations. This paper
presents a statistical method which picks relation-
ships which violate few constraints as measured by
a probabilistic database model. The constraints are
hard constraints, and robust estimates are achieved
by accounting for the underlying extraction/fusion
uncertainty.
This method is applied to the problem of con-
structing management succession timelines which
have a rich set of semantic constraints. Using con-
straints on probabilistic databases yields F-Measure
improvements of 5 to 18 points on a per-relationship
basis over a state-of-the-art multi-document extrac-
tion/fusion baseline. The constraints proposed in
this paper are used in a context of minimally super-
vised information extractors and present an alterna-
tive to costly manual annotation.
</bodyText>
<sectionHeader confidence="0.954104" genericHeader="method">
2 Semantic Constraints on Databases
</sectionHeader>
<bodyText confidence="0.9988515">
This paper considers management succession
databases where each record has three fields: a
CEO’s name and the start and end years for that
person’s tenure as CEO (Table 1 Column 1). Each
record is represented by three binary logical pred-
icates: ceo(c,x), start(x,y&apos;), end(x,y2), where c is
a company, x is a CEO’s name, and y&apos; and y2 are
years.2
</bodyText>
<note confidence="0.8012098">
2All of the relationships in this paper are defined to be bi-
nary relationships. When extracting relationships of higher ar-
332
Proceedings of NAACL HLT 2007, pages 332–339,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<table confidence="0.998771666666667">
Explicit Implicit Logical Constraints
Relationships Relationships (a partial list)
ceo(c, x) precedes(x1,x2) ceo(c,x1), precedes(x1,x2) ⇔ ceo(c,x2), end(x1,y), start(x2, y)
start(x, y) inoffice(x, y) start(x,y1), inoffice(x,y2), end(x, y3) ⇒ y1 ≤ y2 ≤ y3
end(x, y) predates(x1, x2) inoffice(x1, y1), inoffice(x2, y2) , y1 &lt; y2 ⇒ predates(x1,x2)
precedes(x1,x2) , inoffice(x1,y1), inoffice(x2,y2) ⇒ y1 ≤ y2
</table>
<tableCaption confidence="0.998592">
Table 1: Database semantics can provide 1) a method to augment the explicit relationships in the database
</tableCaption>
<bodyText confidence="0.992767266666667">
with implicit relationships and 2) logical constraints on these explicit and implicit relationships. In the above
table, c is a company, xi is a person, and yi is a time.
In this setting, database semantics allow for the
derivation of other implicit relationships from the
database: the immediate predecessor of a given CEO
(precedes(x1,x2)), all predecessors of a given CEO
(predates(x1,x2)) and all years the CEO was in of-
fice (inoffice(x,y3)), where x2 is a CEO’s name and
t3 a year (Table 1 Column 2).
These implicit relationships and the original ex-
plicit relationships are governed by a series of se-
mantic relations which impose constraints on the
permissible database configurations (Table 1 Col-
umn 3). For example, it will always be true that a
CEO’s start date precedes their end date:
</bodyText>
<equation confidence="0.767606">
∀x : start(x, y1), end(x, y2) ⇒ y1 ≤ y2.
</equation>
<bodyText confidence="0.999822272727273">
Multi-document extraction of single relationships
exploits redundancy and variety of expression to ex-
tract accurate information from across many docu-
ments. However, these are not the only benefits of
extraction from a large document collection. As well
as being rich in redundant information, large docu-
ment collections also contain a wealth of secondary
relationships which are related to the relationship of
interest via constraints as described above. These
secondary relationships can yield benefits to aug-
ment those achieved by redundancy.
</bodyText>
<sectionHeader confidence="0.936047" genericHeader="method">
3 Multi-Document Database Fusion
</sectionHeader>
<bodyText confidence="0.989687785714286">
There are typically two steps in the extraction of sin-
gle relationships from multiple documents. In the
first step, a relationship extractor goes through the
corpus, finds all possible relationships r in all sen-
tences s and gives them a score p(r|s). Next, the
ity, typically binary relationships are combined (McDonald et
al., 2005).
relationships are fused across sentences to generate
one score Or for each relationship.
This paper proposes a third step which combines
the fusion scores across relationships. This section
first presents a probabilistic database model gener-
ated from fusion scores and then shows how to use
this model for multi-document fusion.
</bodyText>
<subsectionHeader confidence="0.998372">
3.1 A Probabilistic Database Model
</subsectionHeader>
<bodyText confidence="0.999909">
A relationship r is defined to be a 3-tuple rt,a,b =
r(t, a, b), where t is the type of the relationship (e.g.
start), and a and b are the arguments of the binary
relationship.3
To construct a probabilistic database for a given
corpus, the weights generated in relationship fusion
are normalized to provide the conditional probability
of a relationship given its type:
</bodyText>
<equation confidence="0.985889833333333">
Ort,a,b
p( t,a,bl ) 1
1
r t1 = ,
Eri:rti=t Ort,a,b
i
</equation>
<bodyText confidence="0.960975714285714">
where Or is the fusion score generated by the extrac-
tion/fusion system.4 By applying a prior over types
p(t), a distribution p(r1, t1) can be derived. Given
strong independence assumptions, the probability of
an ordered database configuration R = r1..n of types
t1..n is:
p(ri, ti). (1)
</bodyText>
<footnote confidence="0.881019333333333">
3For readibility in future examples, “a” and “b” are replaced
by the types of their arguments. For example, for start the year
in which the CEO starts is referred to as r&apos;Y�a�.
4The following fusion method does not depend on a par-
ticular extraction/fusion architecture or training methodology,
merely this conditional probability distribution.
</footnote>
<equation confidence="0.88438325">
n
p(r1..n, t1..n) =
i=1
333
</equation>
<bodyText confidence="0.999958076923077">
As proposed, the model in Equation 1 is faulty
since the relationships in a database are not inde-
pendent. Given a set of database constraints, certain
database configurations are illegal and should be as-
signed zero probability. To address this, the model
in Equation 1 is augmented with constraints that ex-
plicitly set the probability of a database configura-
tion to zero when they are violated.
A database constraint is a logical formula
η(r1..π(η)), where π(η) is the arity of the constraint
η. For the constraints presented in this paper, all
constraints η are modeled with two terms ηα and ηβ
where:
</bodyText>
<equation confidence="0.99756">
� �
η(r1..π(η)) = ηα(r1..π(η)) =&gt;. ηβ(r1..π(η))
</equation>
<bodyText confidence="0.9997875">
For a set of relationships, a constraint holds if η(·)
is true, and the constraint applies if ηα(·) is true. A
constraint η(·) can only be violated (false) when the
constraint applies, since: (false =&gt;. X) = true.
In application to a database, each constraint η is
quantified over the database to become a quantified
constraint ηr1..n. For example, the constraints that a
person’s start date must come before their end date is
universally quantified over all pairs of relationships
in a configuration R = r1..n:
</bodyText>
<equation confidence="0.998751">
ηr1..n = br1,r2 E R : η(r1, r2) =
(rt1 = start, rt2 = end, rceo
1 = rceo
2 )
=&gt;. (r year&lt; ryear
1 2 )-
</equation>
<bodyText confidence="0.999709375">
This constraint applies to start and end relationships
whose CEO argument matches and is violated when
the years are not in order. If the quantified constraint
ηr1..n is true for a given database configuration r1..n
then it holds.
To ensure that only legal database configurations
are assigned positive probabilities, Equation 1 is
augmented with a factor
</bodyText>
<equation confidence="0.899256">
(
1 if ηr1..nholds
φηr1..n = 0 otherwise.
To include a constraint η, the database model in
Equation 1 is extended to be:
1 Y pη(r1..n, t1..n) = Z !p(ri, ti) φηr1..nr
i
</equation>
<bodyText confidence="0.99943375">
where Z is the partition function and corresponds to
the total probability of all database configurations. A
set of constraints η1..Q = η1..ηQ can be integrated
similarly:
</bodyText>
<equation confidence="0.7340914">
1 Y pη1..Q(r1..n, t1..n) = Z ! Y
i φηq
p(ri, ti) r1..n
q
(2)
</equation>
<bodyText confidence="0.999948333333333">
With these added constraints, the probabilistic
database model assigns non-zero probability only to
databases which don’t violate any constraints.
</bodyText>
<subsectionHeader confidence="0.998645">
3.2 Constraints on Probabilistic Databases for
Relationship Rescoring
</subsectionHeader>
<bodyText confidence="0.9998787">
Though the constrained probabilistic database
model in Equation 2 is theoretically appealing, it
would be infeasible to calculate its partition func-
tion which requires enumeration of all legal 2n
databases. This section proposes two methods for
re-scoring relationships with regards to how likely
they are to be present in a legal database configu-
ration using the model proposed above. The first
method is a confidence estimate based on how likely
it is that η holds for a given relationship r1:
</bodyText>
<equation confidence="0.996594375">
h i
Λη(r1, t1) = Ep(r2..n,t2..n) φη r1..π(η)
P �Qπ(η) �
i=2 p(ri, ti) φη
r2..π(η) r1..π(η)
=
Pr2..π(η) pη(r1..n, t1..n)
= Pr2..π(η) p(r1..n, t1..n) r
</equation>
<bodyText confidence="0.9532">
where the expectation that the constraint holds
is equivalent to the likelihood ratio between the
database probability models with and without con-
straints. In effect, this model measures the expec-
tation that the constraint holds for a finite database
“look-ahead” of size π(η) − 1.
With this method, for a constraint to reduce the
confidence in a particular relationship by half, half
of all configurations would have to violate the con-
straint.5 Since inconsistencies are relatively rare, for
a given relationship Λη(r, t) pz� 1 (i.e. almost all
small databases are legal).
5Assuming equal probability for all relationships.
</bodyText>
<equation confidence="0.89674075">
P �Qπ(η) �
i=2 p(ri, ti)
r2..π(η)
334
</equation>
<bodyText confidence="0.9994195">
To remedy this, another factor φα is defined simi-
larly to φη, except that it takes a value of 1 only if the
constraint applies to that database configuration. An
applicability probability model is then defined as:
</bodyText>
<equation confidence="0.9978756">
1 Y pα(r1..n, t1..n) = Z
i
α
p(ri, ti)!
φr1..n
tionships via:
Yˆcη1..Q,α1..Q(r1, t1) = p(r1, t1) Ληq,αq(r1, t1)
q
(3)
list.7
</equation>
<bodyText confidence="0.98861552">
The second confidence estimate is based on how
likely it is that the constraint is holds in cases where
it applies (i.e. is not violated):
When the constraint doesn’t apply it cannot be vio-
figurations that
be affected by the constraint.
Recall that
t) is the likelihood ratio be-
tween the probability of configurations in which r
holds for constraint
and all configurations. In con-
trast,
t) is the likelihood ratio between the
database configurations where r applies and holds
for
and the database configurations where
ap-
plies. In the later ratio, for confidence in a particular
relationship to be cut in half, only half of the con-
figurations which might actually contain an incon-
sistency would be required to produce a violation.6
As a result,
t) gives a much higher penalty to
relationships which create inconsistencies than does
can’t
</bodyText>
<equation confidence="0.955949714285714">
Λη(r,
η
Λη,α(r,
η
η
Λη,α(r,
Λη(r, t).
</equation>
<bodyText confidence="0.997925">
In order to apply multiple constraints, indepen-
dent database look-aheads are generated for each
constraint q:
task is complicated to start
A corpus was created for each company by is-
suing aGoogle query for
OR
and collecting the top ranked doc-
uments, generating up to 1000 documents per com-
pany. The data was then split randomly into training,
development and testing sets of 6, 4, an
</bodyText>
<table confidence="0.705552230769231">
with.8
“CEO-of-Company
Company-CEO”,
d 8 compa-
nies.
Training : Anheuser-Busch, Hewlett-Packard,
Lenner, McGraw-Hill, Pfizer, Raytheon
Dev.: Boeing, Heinz, Staples, Textr
|XML |xmlLoc_0 xmlBold_no xmlItalic_no xmlFontSize_smaller xmlPic_no xmlTable_no xmlBullet_no bi_xmlSFBIA_new bi_xmlPara_continue
on
Test : General Electric, General Motors,
Gannett, The Home Depot, IBM,
Kroger, Sears, UPS
</table>
<equation confidence="0.854951">
h i
= Epα(r2..n,t2..n) φη r1..π(η)�Qπ(η) �
P p(ri ti) φα φη
=
P �Qπ(η) �
i=2 p(ri, ti) φα
r2..π(η) r1..π(η)
YΛη1..Q,α1..Q(r1, t1) = Ληq,αq(r1, t1). icular,
q
</equation>
<bodyText confidence="0.8944025">
For a particular relationship type, these confidence
scores are calculated and then used to rank the rela-
the same CEO. The confidence in a particular start
date would
be halved if half of the proposed end dates for a given CEO
occurred before it.
</bodyText>
<equation confidence="0.370676666666667">
Λη,α(r1, t1)
lated, so this confidence estimate ignores those con-
6For example, for a start relationship and the constraint that
</equation>
<tableCaption confidence="0.6276805">
a CEO must start before they end, this method would only ex-
amine configurations of one start and one end relationship for
</tableCaption>
<bodyText confidence="0.969954">
Databases with different precision/recall trade offs
can be selected by descending the ranked
</bodyText>
<sectionHeader confidence="0.99946" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.974764095238095">
In order to test the fusion method proposed above,
human annotators manually constructed truth data
of complete chief executive histories for 18 Fortune-
500 companies using online resources. Extraction
from these documents is particularly difficult be-
cause these data have vast differences in genre and
style and are considerably noisy. Furthermore, the
Ground truth was created from the entire web, but
since the corpus for each company is only a small
web snapshot, the experimental results are not simi-
lar to extraction tasks like MUC and ACE in that the
corpus is not guaranteed to contain the information
necessary to build the entire database. In part
thing to note is that since all relationships are given
confidence estimates separately, this process may result ulti-
mately in a database where constraints are violated. A potential
solution, which is not explored here, would be to incrementally
add relationships to the database from the ranked list only if
their addition
make the database inconsistent.
example, in certain companies, the title of the chief
</bodyText>
<figure confidence="0.895064777777778">
executive has changed over the years, often going from
to
Executive
To make things more com-
plicated, after the change, the role of
may still hang
on as
7One
doesn’t
8For
“Presi-
dent”
“Chief
Officer”.
“President”
a subordinate to the CEO!
335
1) Only one start or end per person.
</figure>
<table confidence="0.996282">
1 = rtype 2= (start ∪ end), rceo
∀r1, r2 : η(r1, r2) = (rtype
2 ) ⇒ (ryear 1= ryear
1 = rceo
2 )
2) Only a CEO’s start or end dates belong in the database.
∀r1∃r2 : η(r1, r2) = (rtype 1= start ∪ end, rtype 2= ceo) ⇒ (rceo 1= rceo
2 )
3) Start dates come before end dates.
∀r1, r2 : η(r1, r2) = (rtype 1 = rceo
2 ) ⇒ (rY ear ≤ ryear)
1 = start, rtype 2= end, rceo
\\
4) Can’t be in the middle of someone else’s tenure.
∀r1, r2, r3 : η(r1, r2, r3) = (rtype 1= start ∪ inoffice, rtype
2 = end ∪ inoffice, rtype
3 = start ∪ inoffice ∪ end,
rceo 2 =6 r3 ceo, ryear
1 = rceo
1 &lt; ryear
1 ∪ ryear
3 ≤ ryear
2 ) ⇒ (ryear
3 ≥ ryear
2 )
5) CEO’s are only in office after their start.
2 = inoffice, rceo
∀r1, r2 : η(r1, r2) = (rtype 1 = rceo
2 ) ⇒ (rYear ≤ ryear)
1 = start, rtype
\\
6) CEO’s are only in office before their end.
2 = end, rceo
∀r1, r2 : η(r1, r2) = (rtype 1 = rceo
1 ≤ ryear
2 ) ⇒ (ryear
1 = inoffice, rtype
2 )
7) Someone’s end is the same as their successor’s start.
∀r1, r2, r3 : η(r1, r2, r3) =
1 = ryear
1 = rfirst
(rtype 1= end, rtype 2= start, rtype 3= precedes, rceo 2 = rsecond
3 ) ⇒ (ryear
3 , rceo
2 )
S) All of the someone’s dates (start, inoffice, end) are before their successors.
∀r1, r2, r3 : η(r1, r2, r3) = (rtype
1 = start ∪ end ∪ inoffice, rtype 2= start ∪ inoffice ∪ end, rtype
3 = precedes,
rceo 1 = rfirst
3 ) ⇒ (ryear
2 = rsecond
3 , rceo
1 ≤ ryear 2 )
9) Only CEO succession in the database.
∀r1∃r1, r2 : η(r1, r2, r3) = (rtype
2 , rsecond = rceo
3 = ceo) ⇒ (rfirst
1 = rceo
2 = rtype
1 = precedes, rtype
1 3 )
</table>
<tableCaption confidence="0.920179">
Table 2: For a CEO succession database like the one presented in Table 1, the above constraints must hold
</tableCaption>
<bodyText confidence="0.9648125">
if the database is consistent.
many CEOs from pre-Internet years were either in-
frequently mentioned or not mentioned at all in the
database.9 In the following experiments, recall is re-
ported for facts that were retrieved by the extraction
system.
</bodyText>
<subsectionHeader confidence="0.99894">
4.1 Relationship Extraction and Fusion
</subsectionHeader>
<bodyText confidence="0.998624763157895">
A two-class maximum-entropy classifier was trained
for each relationship type. Each classifier takes a
sentence and two marked entities (e.g. a person and
a year)10 and gives the probability that a relation-
ship between the two entities is supported by the
sentence. For each relationship type, one of the ele-
ments is designated as the “hook” in order to gener-
ate likely negative examples.11 In training, all entity
pairs are collected from the corpus. The pairs whose
“hook” element doesn’t appear in the database are
thrown out. The remaining pairs are then marked
9Another consequence is that assessing the effectiveness of
the relationships extraction on a per-extraction basis is difficult.
Because there are no training sentences where it is known that
the sentence contains the relationship of interest, grading per-
extraction results can be deceptive.
10The person tagger used is the named-entity tagger from
OpenNLP tools and the year tagger simply finds any four digit
numbers between 1950 and 2010.
11For the CEO relationship, the company was taken to be
the hook. For the other relationships the hook was the primary
CEO.
by exact match to the database. In testing, the rela-
tionship extractor yields the probability p(r s) of an
entity pair relationship r in a particular sentence s.
The features used in the classifier are: unigrams
between the given information and the target, dis-
tance in words between the given information and
the target, and the exact string between the given in-
formation and the target (if less that 3 words long).
After extraction from individual sentences, the re-
lationships are fused together such that there is one
score for each unique entity pair. In the case of per-
son names, normalization was performed to merge
coreferent but lexically distinct names (e.g. “Phil
Condit” and “Philip M. Condit”).
In the following experiments, the baseline fusion
score is:
</bodyText>
<equation confidence="0.9888275">
�φ� � p(r|s) (4)
s
</equation>
<subsectionHeader confidence="0.985734">
4.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999910125">
Given the management succession database pro-
posed in Section 2, Table 2 enumerates a set of quan-
tified constraints. Information extraction and fusion
were run separately for each company to create a
probabilistic database. In this section, various con-
straint sets are applied, either individually or jointly,
and evaluated in two ways. The first measures per-
relationship precision/recall using the model pro-
</bodyText>
<figure confidence="0.734108">
336
0 0.2 0.4 0.6 0.8 1
</figure>
<figureCaption confidence="0.997655">
Figure 1: Precision/Recall curve for start(x,t) rela-
</figureCaption>
<bodyText confidence="0.9739565">
tionships. The joint constraint “2,3,5,8” is the best
performing, even though constraints “3” and “8”
(not pictured) alone don’t perform well.
posed and the second looks at the precision/recall
of a heterogeneous database with many relationship
types. Both evaluations examine the ranked lists of
relationships, where the relationships are ranked by
rescoring via constraints on probabilistic databases
(Equation 3) and compared to the baseline fusion
score (Equation 4). The evaluations use two stan-
dard metrics, interpolated precision at recall level i
(PRz), and MaxF1:
</bodyText>
<equation confidence="0.99013225">
PRz = max
j&gt;i
MaxF1 = max
i
</equation>
<bodyText confidence="0.998089277777778">
Figures 1, 2, and 3 show precision/recall curves
for the application of various sets of constraints. Ta-
ble 3 lists the MaxF1 scores for each of the con-
straint variants. For start and end, the majority of
constraints are beneficial. For precedes, only the
constraint that improved performance constraints
both people in the relationship to be CEOs. Across
all relationships, performance is hurt when using the
constraint that there could only be one relationship
of each type for a given CEO. The reason behind
this is that the confidence estimate based on this
constraint favors relationships with few competitors,
and those relationships are typically for people who
are infrequent in the corpus (and therefore unlikely
to be CEOs).
The best-performing constraint sets yield between
5 and 18 points of improvement on Max F1 (Ta-
ble 3). Surprisingly, the gains from joint con-
</bodyText>
<figure confidence="0.867731">
0 0.2 0.4 0.6 0.8 1
</figure>
<figureCaption confidence="0.87643425">
Figure 2: Precision/Recall curve for end(x,t) rela-
tionships alone. The joint constraint “2,6” is the best
performing.
Figure 3: Precision/Recall for precedes(x,y) rela-
tionships alone. Though the constraint “First Before
Second (8)” helps performance on start(x,t) relation-
ships, the only constraint which aids here is “Only
CEOs Succession (9)”.
</figureCaption>
<bodyText confidence="0.999922133333333">
straints are sometimes more than their additive
gains. “2,3,5,6,8” is 6 points better for the start rela-
tionship than “2,3,5,6”, but the gains from “8” alone
are negligible.
These performance gains on the individual rela-
tionship types also lead to gains when generating an
entire database (Figure 4). The highest performing
constraint is the “CEOs Only (2)” constraint, which
outperforms the joint constraints of the previous sec-
tion. One reason the joint constraints don’t do as
well here is that each constraint makes the confi-
dence estimate smaller and smaller. This doesn’t
have an effect when judging the relationship types
individually, but when combining the relationships
results, the fused relationships types (start, end) be-
</bodyText>
<figure confidence="0.98396118367347">
Baseline
Only One (1)
CEOs Only (2)
Start Before End (3)
Inoffice After Start (5)
2,5
2,3,5,
First Before
Second (8)
Baseline
Only One (1)
CEOs Only (2)
Inoffice Before End (6)
2,6
0 0.2 0.4 0.6 0.8 1
0.6
0.5
0.4
0.3
0.2
0.1
0
Baseline
End is Start (7)
First Before Second (8)
Only CEO Succession (9)
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
PR,,
2
1 1.
PRz + Rz
337
</figure>
<table confidence="0.986359142857143">
Constraint Set Start Max F1 DB
End Pre.
0 (baseline) 31.2 35.8 34.5 37.9
Only One (1) 10.5 7.2 - 38.1
CEOs Only (2) or (9) 43.3 39.4 39.4 42.9
Start Before End (3) 40.8 32.8 - 40.9
No Overlaps (4) 31.5 35.9 - 36.8
Inoffice After Start (5) 32.5 - - 38.2
Inoffice Before End (6) - 36.5 - 37.4
End is Start (7) 7.3 8.0 20.7 39.2
First before Second (8) 31.4 35.6 26.3 38.1
2,5,6 43.3 40.8 - 42.7
2,3,5,6 43.9 43.3 - 42.2
2,3,5,6,8 49.3 43.9 26.3 40.9
</table>
<figure confidence="0.973961636363636">
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
</figure>
<figureCaption confidence="0.989235333333333">
Figure 4: Precision/Recall curve for whole database
reconstruction. Performance curves using con-
straints dominate the baseline.
</figureCaption>
<figure confidence="0.642125857142857">
Baseline
CEOs Only (2)
Start Before End (3)
2, Inoffice After Start (5)
Inoffice Before End (6)
2,3,5,6
0 0.2 0.4 0.6 0.8 1
</figure>
<tableCaption confidence="0.953674">
Table 3: Max F1 scores for three relationships
</tableCaption>
<bodyText confidence="0.996900818181818">
Start(x,t), End(x,t) and Precedes(x,y)) in isolation
and within the context of whole database DB. The
joint constraints perform best for the explicit rela-
tionships in isolation. Using constraints on implicit
derived fields (Inoffice and Precedes) provides ad-
ditional benefit above constraints strictly on explicit
database fields (start, end, ceo).
come artificially lower ranked than the unfused rela-
tionship type (ceo). The best performing contrained
probabilistic database approach beats the baseline
by 5 points.
</bodyText>
<sectionHeader confidence="0.999963" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999700897959184">
Techniques for information extraction from min-
imally supervised data have been explored by
Brin (1998), Agichtein and Gravano (2000), and
Ravichandran and Hovy (2002). Those techniques
propose methods for estimating extractors from ex-
ample relationships and a corpus which contains in-
stances of those relationships.
Nahm and Mooney (2002) explore techniques for
extracting multiple relationships in single document
extraction. They learn rules for predicting certain
fields given other extracted fields (i.e. a someone
who lists Windows as a specialty is likely to know
Microsoft Word).
Perhaps the most related work to what is pre-
sented here is previous research which uses database
information as co-occurrence features for informa-
tion extraction in a multi-document setting. Mann
and Yarowsky (2005) present an incremental ap-
proach where co-occurrence with a known relation-
ship is a feature added in training and test. Cu-
lotta et al. (2006) introduce a data mining approach
where discovered relationships from a database are
used as features in extracting new relationships. The
database constraints presented in this paper provide
a more general framework for jointly conditioning
multiple relationships. Additionally, this constraint-
based approach can be applied without special train-
ing of the extraction/fusion system.
In the context of information fusion of single rela-
tionships across multiple documents, Downey et al.
(2005) propose a method that models the probabili-
ties of positive and negative extracted classifications.
More distantly related, Sutton and McCallum (2004)
and Finkel et al. (2005) propose graphical models
for combining information about a given entity from
multiple mentions.
In the field of question answering, Prager et al.
(2004) answer a question about the list of composi-
tions produced by a given subject by looking for re-
lated information about the subject’s birth and death.
Their method treats supporting information as fixed
hard constraints on the original questions and are ap-
plied in an ad-hoc fashion. This paper proposes a
probabilistic method for using constraints in the con-
text of database extraction and applies this method
over a larger set of relations.
Richardson and Domingos (2006) propose a
method for reasoning about databases and logical
constraints using Markov Random Fields. Their
</bodyText>
<page confidence="0.803153">
338
</page>
<bodyText confidence="0.99969975">
model applies reasoning starting from a known
database. In this paper the database is built from ex-
traction/fusion of relationships from web pages and
contains a significant amount of noise.
</bodyText>
<sectionHeader confidence="0.999498" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999988642857143">
This paper has presented a probabilistic method for
fusing extracted facts in the context of database ex-
traction when there exist logical constraints between
the fields in the database. The method estimates the
probability than the inclusion of a given relationship
will violate database constraints by taking into ac-
count the uncertainty of the other extracted relation-
ships. Along with the relationships explicitly listed
in the database, constraints are formed over implicit
fields directly recoverable from the explicit listed re-
lationships.
The construction of CEO succession timelines us-
ing minimally trained extractors from web text is a
particularly challenging problem because of noise
resulting from the wide variation in genre in the cor-
pora and errors in extraction. The use of constraints
on probabilistic databases is effective in resolving
many of these errors, leading to improved precision
and recall of retrieved facts, with F-measure gains of
5 to 18 points.
The method presented in this paper combines
symbolic and statistical approaches to natural lan-
guage processing. Logical constraints are made
more robust by taking into account the uncertainty
of the extracted information. An interesting area
of future work is the application of data mining to
search for appropriate constraints to integrate into
this model.
</bodyText>
<sectionHeader confidence="0.997293" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99266075">
This work was supported in part by DoD contract #HM1582-
06-1-2013. Any opinions, findings and conclusions or recom-
mendations expressed in this material belong to the author and
do not necessarily reflect those of the sponsor.
</bodyText>
<sectionHeader confidence="0.99786" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999801666666666">
E. Agichtein and L. Gravano. 2000. Snowball: Extracting re-
lations from large plain-text collections. In Proceedings of
ICDL, pages 85–94.
E. Agichtein. 2005. Extracting Relations from Large Text Col-
lections. Ph.D. thesis, Columbia University.
D. Appelt, J. Hobbs, J. Bear, D. Israel, and M. Tyson. 1993.
FASTUS: a finite-state processor for information extraction
from real-world text. In Proceedings of IJCAI.
S. Brin. 1998. Extracting patterns and relations from the world
wide web. In WebDB Workshop at 6th International Confer-
ence on Extending Database Technology, EDBT’98, pages
172–183.
A. Culotta, A. McCallum, and J. Betz. 2006. Integrating prob-
abilistic extraction models and data mining to discover rela-
tions and patterns in text. In HLT-NAACL, pages 296–303,
New York, NY, June.
D. Downey, O. Etzioni, and S. Soderland. 2005. A probabilistic
model of redundancy in information extraction. In IJCAI.
O. Etzioni, M. Cafarella, D. Downey, S. Kok, A-M. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates. 2004. Web-
scale information extraction in knowitall. In WWW.
J. Finkel, T. Grenager, , and C. Manning. 2005. Incorporating
non-local information into information extraction systems by
gibbs sampling. In ACL.
R. Grishman and B. Sundheim. 1996. Message understanding
conference-6: A brief history. In Proceedings of COLING.
T. Hasegawa, S. Sekine, and R. Grishman. 2004. Discovering
relations amoung named entities from large corpora. In ACL.
G. Mann and D. Yarowsky. 2005. Multi-field information ex-
traction and cross-document fusion. In ACL.
R. McDonald, F. Pereira, S. Kulick, S. Winters, Y. Jin, and
P. White. 2005. Simple algorithms for complex relationship
extraction with applications to biomedical ie. In Proceed-
ings of ACL.
U. Nahm and R. Mooney. 2002. Text mining with information
extraction. In Proceedings of the AAAI 2220 Spring Sympo-
sium on Mining Answers from Texts and Knowledge Bases,
pages 60–67.
M. Pasca, D. Lin, J. Bigham, A. Lifchits, and A. Jain. 2006.
Organizing and searching the world wide web of facts - step
one: The one-million fact extracion challenge. In AAAI.
J. Prager, J. Chu-Carroll, and K. Czuba. 2004. Question an-
swering by constraint satisfaction: Qa-by-dossier with con-
straints. In Proceedings of ACL, pages 574–581.
D. Ravichandran and E. Hovy. 2002. Learning surface text
patterns for a question answering system. In Proceedings of
ACL, pages 41–47.
M. Richardson and P. Domingos. 2006. Markov logic net-
works. Machine Learning, 62:107–136.
C. Sutton and A. McCallum. 2004. Collective segmenta-
tion and labeling of distant entities in information extraction.
Technical Report TR # 04-49, University of Massachusetts,
July. Presented at ICML Workshop on Statistical Relational
Learning and Its Connections to Other Fields.
</reference>
<page confidence="0.954717">
339
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.939965">
<title confidence="0.99993">Multi-document Relationship Fusion Constraints on Probabilistic Databases</title>
<author confidence="0.996585">Gideon</author>
<affiliation confidence="0.9998435">Department of Computer University of</affiliation>
<address confidence="0.987826">Amherst, MA</address>
<email confidence="0.99974">gideon.mann@gmail.com</email>
<abstract confidence="0.996771714285714">Previous multi-document relationship extraction and fusion research has focused on single relationships. Shifting the focus to multiple relationships allows for the use of mutual constraints to aid extraction. This paper presents a fusion method which uses a probabilistic database model to pick relationships which violate few constraints. This model allows improved performance on constructing corporate succession timelines from multiple documents with respect to a multi-document fusion baseline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agichtein</author>
<author>L Gravano</author>
</authors>
<title>Snowball: Extracting relations from large plain-text collections.</title>
<date>2000</date>
<booktitle>In Proceedings of ICDL,</booktitle>
<pages>85--94</pages>
<contexts>
<context position="23299" citStr="Agichtein and Gravano (2000)" startWordPosition="3867" endWordPosition="3870">y)) in isolation and within the context of whole database DB. The joint constraints perform best for the explicit relationships in isolation. Using constraints on implicit derived fields (Inoffice and Precedes) provides additional benefit above constraints strictly on explicit database fields (start, end, ceo). come artificially lower ranked than the unfused relationship type (ceo). The best performing contrained probabilistic database approach beats the baseline by 5 points. 5 Related Work Techniques for information extraction from minimally supervised data have been explored by Brin (1998), Agichtein and Gravano (2000), and Ravichandran and Hovy (2002). Those techniques propose methods for estimating extractors from example relationships and a corpus which contains instances of those relationships. Nahm and Mooney (2002) explore techniques for extracting multiple relationships in single document extraction. They learn rules for predicting certain fields given other extracted fields (i.e. a someone who lists Windows as a specialty is likely to know Microsoft Word). Perhaps the most related work to what is presented here is previous research which uses database information as co-occurrence features for inform</context>
</contexts>
<marker>Agichtein, Gravano, 2000</marker>
<rawString>E. Agichtein and L. Gravano. 2000. Snowball: Extracting relations from large plain-text collections. In Proceedings of ICDL, pages 85–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Agichtein</author>
</authors>
<title>Extracting Relations from Large Text Collections.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>Columbia University.</institution>
<contexts>
<context position="1418" citStr="Agichtein, 2005" startWordPosition="202" endWordPosition="203">uch attention since the MUC evaluations1 in the mid-90s (Appelt et al., 1993; Grishman and Sundheim, 1996). Recently, there has been increased interest in the extraction of named entities and relationships from multiple documents, since the redundancy of information across documents has been shown to be a powerful resource for obtaining high quality information even when the extractors have access to little or no training data (Etzioni et al., 2004; Hasegawa et al., 2004). Much of the recent work in multidocument relationship extraction has focused on the extraction of isolated relationships (Agichtein, 2005; Pasca et al., 2006), but often the goal, as in 1http://www.itl.nist.gov/iaui/894.02/related projects/muc/ single document tasks like MUC, is to extract a template or a relational database composed of related facts. With databases containing multiple relationships, the semantics of the database impose constraints on possible database configurations. This paper presents a statistical method which picks relationships which violate few constraints as measured by a probabilistic database model. The constraints are hard constraints, and robust estimates are achieved by accounting for the underlyin</context>
</contexts>
<marker>Agichtein, 2005</marker>
<rawString>E. Agichtein. 2005. Extracting Relations from Large Text Collections. Ph.D. thesis, Columbia University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Appelt</author>
<author>J Hobbs</author>
<author>J Bear</author>
<author>D Israel</author>
<author>M Tyson</author>
</authors>
<title>FASTUS: a finite-state processor for information extraction from real-world text.</title>
<date>1993</date>
<booktitle>In Proceedings of IJCAI.</booktitle>
<contexts>
<context position="879" citStr="Appelt et al., 1993" startWordPosition="116" endWordPosition="120">earch has focused on single relationships. Shifting the focus to multiple relationships allows for the use of mutual constraints to aid extraction. This paper presents a fusion method which uses a probabilistic database model to pick relationships which violate few constraints. This model allows improved performance on constructing corporate succession timelines from multiple documents with respect to a multi-document fusion baseline. 1 Introduction Single document information extraction of named entities and relationships has received much attention since the MUC evaluations1 in the mid-90s (Appelt et al., 1993; Grishman and Sundheim, 1996). Recently, there has been increased interest in the extraction of named entities and relationships from multiple documents, since the redundancy of information across documents has been shown to be a powerful resource for obtaining high quality information even when the extractors have access to little or no training data (Etzioni et al., 2004; Hasegawa et al., 2004). Much of the recent work in multidocument relationship extraction has focused on the extraction of isolated relationships (Agichtein, 2005; Pasca et al., 2006), but often the goal, as in 1http://www.</context>
</contexts>
<marker>Appelt, Hobbs, Bear, Israel, Tyson, 1993</marker>
<rawString>D. Appelt, J. Hobbs, J. Bear, D. Israel, and M. Tyson. 1993. FASTUS: a finite-state processor for information extraction from real-world text. In Proceedings of IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Brin</author>
</authors>
<title>Extracting patterns and relations from the world wide web.</title>
<date>1998</date>
<booktitle>In WebDB Workshop at 6th International Conference on Extending Database Technology, EDBT’98,</booktitle>
<pages>172--183</pages>
<contexts>
<context position="23269" citStr="Brin (1998)" startWordPosition="3865" endWordPosition="3866">d Precedes(x,y)) in isolation and within the context of whole database DB. The joint constraints perform best for the explicit relationships in isolation. Using constraints on implicit derived fields (Inoffice and Precedes) provides additional benefit above constraints strictly on explicit database fields (start, end, ceo). come artificially lower ranked than the unfused relationship type (ceo). The best performing contrained probabilistic database approach beats the baseline by 5 points. 5 Related Work Techniques for information extraction from minimally supervised data have been explored by Brin (1998), Agichtein and Gravano (2000), and Ravichandran and Hovy (2002). Those techniques propose methods for estimating extractors from example relationships and a corpus which contains instances of those relationships. Nahm and Mooney (2002) explore techniques for extracting multiple relationships in single document extraction. They learn rules for predicting certain fields given other extracted fields (i.e. a someone who lists Windows as a specialty is likely to know Microsoft Word). Perhaps the most related work to what is presented here is previous research which uses database information as co-</context>
</contexts>
<marker>Brin, 1998</marker>
<rawString>S. Brin. 1998. Extracting patterns and relations from the world wide web. In WebDB Workshop at 6th International Conference on Extending Database Technology, EDBT’98, pages 172–183.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Culotta</author>
<author>A McCallum</author>
<author>J Betz</author>
</authors>
<title>Integrating probabilistic extraction models and data mining to discover relations and patterns in text.</title>
<date>2006</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>296--303</pages>
<location>New York, NY,</location>
<contexts>
<context position="24110" citStr="Culotta et al. (2006)" startWordPosition="3990" endWordPosition="3994">nd Mooney (2002) explore techniques for extracting multiple relationships in single document extraction. They learn rules for predicting certain fields given other extracted fields (i.e. a someone who lists Windows as a specialty is likely to know Microsoft Word). Perhaps the most related work to what is presented here is previous research which uses database information as co-occurrence features for information extraction in a multi-document setting. Mann and Yarowsky (2005) present an incremental approach where co-occurrence with a known relationship is a feature added in training and test. Culotta et al. (2006) introduce a data mining approach where discovered relationships from a database are used as features in extracting new relationships. The database constraints presented in this paper provide a more general framework for jointly conditioning multiple relationships. Additionally, this constraintbased approach can be applied without special training of the extraction/fusion system. In the context of information fusion of single relationships across multiple documents, Downey et al. (2005) propose a method that models the probabilities of positive and negative extracted classifications. More dist</context>
</contexts>
<marker>Culotta, McCallum, Betz, 2006</marker>
<rawString>A. Culotta, A. McCallum, and J. Betz. 2006. Integrating probabilistic extraction models and data mining to discover relations and patterns in text. In HLT-NAACL, pages 296–303, New York, NY, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Downey</author>
<author>O Etzioni</author>
<author>S Soderland</author>
</authors>
<title>A probabilistic model of redundancy in information extraction.</title>
<date>2005</date>
<booktitle>In IJCAI.</booktitle>
<contexts>
<context position="24601" citStr="Downey et al. (2005)" startWordPosition="4060" endWordPosition="4063">incremental approach where co-occurrence with a known relationship is a feature added in training and test. Culotta et al. (2006) introduce a data mining approach where discovered relationships from a database are used as features in extracting new relationships. The database constraints presented in this paper provide a more general framework for jointly conditioning multiple relationships. Additionally, this constraintbased approach can be applied without special training of the extraction/fusion system. In the context of information fusion of single relationships across multiple documents, Downey et al. (2005) propose a method that models the probabilities of positive and negative extracted classifications. More distantly related, Sutton and McCallum (2004) and Finkel et al. (2005) propose graphical models for combining information about a given entity from multiple mentions. In the field of question answering, Prager et al. (2004) answer a question about the list of compositions produced by a given subject by looking for related information about the subject’s birth and death. Their method treats supporting information as fixed hard constraints on the original questions and are applied in an ad-ho</context>
</contexts>
<marker>Downey, Etzioni, Soderland, 2005</marker>
<rawString>D. Downey, O. Etzioni, and S. Soderland. 2005. A probabilistic model of redundancy in information extraction. In IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Etzioni</author>
<author>M Cafarella</author>
<author>D Downey</author>
<author>S Kok</author>
<author>A-M Popescu</author>
<author>T Shaked</author>
<author>S Soderland</author>
<author>D Weld</author>
<author>A Yates</author>
</authors>
<title>Webscale information extraction in knowitall.</title>
<date>2004</date>
<booktitle>In WWW.</booktitle>
<contexts>
<context position="1255" citStr="Etzioni et al., 2004" startWordPosition="176" endWordPosition="179">ple documents with respect to a multi-document fusion baseline. 1 Introduction Single document information extraction of named entities and relationships has received much attention since the MUC evaluations1 in the mid-90s (Appelt et al., 1993; Grishman and Sundheim, 1996). Recently, there has been increased interest in the extraction of named entities and relationships from multiple documents, since the redundancy of information across documents has been shown to be a powerful resource for obtaining high quality information even when the extractors have access to little or no training data (Etzioni et al., 2004; Hasegawa et al., 2004). Much of the recent work in multidocument relationship extraction has focused on the extraction of isolated relationships (Agichtein, 2005; Pasca et al., 2006), but often the goal, as in 1http://www.itl.nist.gov/iaui/894.02/related projects/muc/ single document tasks like MUC, is to extract a template or a relational database composed of related facts. With databases containing multiple relationships, the semantics of the database impose constraints on possible database configurations. This paper presents a statistical method which picks relationships which violate few</context>
</contexts>
<marker>Etzioni, Cafarella, Downey, Kok, Popescu, Shaked, Soderland, Weld, Yates, 2004</marker>
<rawString>O. Etzioni, M. Cafarella, D. Downey, S. Kok, A-M. Popescu, T. Shaked, S. Soderland, D. Weld, and A. Yates. 2004. Webscale information extraction in knowitall. In WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Finkel</author>
<author>T Grenager</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In ACL.</booktitle>
<marker>Finkel, Grenager, 2005</marker>
<rawString>J. Finkel, T. Grenager, , and C. Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Grishman</author>
<author>B Sundheim</author>
</authors>
<title>Message understanding conference-6: A brief history.</title>
<date>1996</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="909" citStr="Grishman and Sundheim, 1996" startWordPosition="121" endWordPosition="124">single relationships. Shifting the focus to multiple relationships allows for the use of mutual constraints to aid extraction. This paper presents a fusion method which uses a probabilistic database model to pick relationships which violate few constraints. This model allows improved performance on constructing corporate succession timelines from multiple documents with respect to a multi-document fusion baseline. 1 Introduction Single document information extraction of named entities and relationships has received much attention since the MUC evaluations1 in the mid-90s (Appelt et al., 1993; Grishman and Sundheim, 1996). Recently, there has been increased interest in the extraction of named entities and relationships from multiple documents, since the redundancy of information across documents has been shown to be a powerful resource for obtaining high quality information even when the extractors have access to little or no training data (Etzioni et al., 2004; Hasegawa et al., 2004). Much of the recent work in multidocument relationship extraction has focused on the extraction of isolated relationships (Agichtein, 2005; Pasca et al., 2006), but often the goal, as in 1http://www.itl.nist.gov/iaui/894.02/relat</context>
</contexts>
<marker>Grishman, Sundheim, 1996</marker>
<rawString>R. Grishman and B. Sundheim. 1996. Message understanding conference-6: A brief history. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hasegawa</author>
<author>S Sekine</author>
<author>R Grishman</author>
</authors>
<title>Discovering relations amoung named entities from large corpora.</title>
<date>2004</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1279" citStr="Hasegawa et al., 2004" startWordPosition="180" endWordPosition="183">pect to a multi-document fusion baseline. 1 Introduction Single document information extraction of named entities and relationships has received much attention since the MUC evaluations1 in the mid-90s (Appelt et al., 1993; Grishman and Sundheim, 1996). Recently, there has been increased interest in the extraction of named entities and relationships from multiple documents, since the redundancy of information across documents has been shown to be a powerful resource for obtaining high quality information even when the extractors have access to little or no training data (Etzioni et al., 2004; Hasegawa et al., 2004). Much of the recent work in multidocument relationship extraction has focused on the extraction of isolated relationships (Agichtein, 2005; Pasca et al., 2006), but often the goal, as in 1http://www.itl.nist.gov/iaui/894.02/related projects/muc/ single document tasks like MUC, is to extract a template or a relational database composed of related facts. With databases containing multiple relationships, the semantics of the database impose constraints on possible database configurations. This paper presents a statistical method which picks relationships which violate few constraints as measured</context>
</contexts>
<marker>Hasegawa, Sekine, Grishman, 2004</marker>
<rawString>T. Hasegawa, S. Sekine, and R. Grishman. 2004. Discovering relations amoung named entities from large corpora. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Mann</author>
<author>D Yarowsky</author>
</authors>
<title>Multi-field information extraction and cross-document fusion.</title>
<date>2005</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="23969" citStr="Mann and Yarowsky (2005)" startWordPosition="3966" endWordPosition="3969">niques propose methods for estimating extractors from example relationships and a corpus which contains instances of those relationships. Nahm and Mooney (2002) explore techniques for extracting multiple relationships in single document extraction. They learn rules for predicting certain fields given other extracted fields (i.e. a someone who lists Windows as a specialty is likely to know Microsoft Word). Perhaps the most related work to what is presented here is previous research which uses database information as co-occurrence features for information extraction in a multi-document setting. Mann and Yarowsky (2005) present an incremental approach where co-occurrence with a known relationship is a feature added in training and test. Culotta et al. (2006) introduce a data mining approach where discovered relationships from a database are used as features in extracting new relationships. The database constraints presented in this paper provide a more general framework for jointly conditioning multiple relationships. Additionally, this constraintbased approach can be applied without special training of the extraction/fusion system. In the context of information fusion of single relationships across multiple</context>
</contexts>
<marker>Mann, Yarowsky, 2005</marker>
<rawString>G. Mann and D. Yarowsky. 2005. Multi-field information extraction and cross-document fusion. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>F Pereira</author>
<author>S Kulick</author>
<author>S Winters</author>
<author>Y Jin</author>
<author>P White</author>
</authors>
<title>Simple algorithms for complex relationship extraction with applications to biomedical ie.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="5425" citStr="McDonald et al., 2005" startWordPosition="811" endWordPosition="814">on, large document collections also contain a wealth of secondary relationships which are related to the relationship of interest via constraints as described above. These secondary relationships can yield benefits to augment those achieved by redundancy. 3 Multi-Document Database Fusion There are typically two steps in the extraction of single relationships from multiple documents. In the first step, a relationship extractor goes through the corpus, finds all possible relationships r in all sentences s and gives them a score p(r|s). Next, the ity, typically binary relationships are combined (McDonald et al., 2005). relationships are fused across sentences to generate one score Or for each relationship. This paper proposes a third step which combines the fusion scores across relationships. This section first presents a probabilistic database model generated from fusion scores and then shows how to use this model for multi-document fusion. 3.1 A Probabilistic Database Model A relationship r is defined to be a 3-tuple rt,a,b = r(t, a, b), where t is the type of the relationship (e.g. start), and a and b are the arguments of the binary relationship.3 To construct a probabilistic database for a given corpus</context>
</contexts>
<marker>McDonald, Pereira, Kulick, Winters, Jin, White, 2005</marker>
<rawString>R. McDonald, F. Pereira, S. Kulick, S. Winters, Y. Jin, and P. White. 2005. Simple algorithms for complex relationship extraction with applications to biomedical ie. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Nahm</author>
<author>R Mooney</author>
</authors>
<title>Text mining with information extraction.</title>
<date>2002</date>
<booktitle>In Proceedings of the AAAI 2220 Spring Symposium on Mining Answers from Texts and Knowledge Bases,</booktitle>
<pages>60--67</pages>
<contexts>
<context position="23505" citStr="Nahm and Mooney (2002)" startWordPosition="3897" endWordPosition="3900">rovides additional benefit above constraints strictly on explicit database fields (start, end, ceo). come artificially lower ranked than the unfused relationship type (ceo). The best performing contrained probabilistic database approach beats the baseline by 5 points. 5 Related Work Techniques for information extraction from minimally supervised data have been explored by Brin (1998), Agichtein and Gravano (2000), and Ravichandran and Hovy (2002). Those techniques propose methods for estimating extractors from example relationships and a corpus which contains instances of those relationships. Nahm and Mooney (2002) explore techniques for extracting multiple relationships in single document extraction. They learn rules for predicting certain fields given other extracted fields (i.e. a someone who lists Windows as a specialty is likely to know Microsoft Word). Perhaps the most related work to what is presented here is previous research which uses database information as co-occurrence features for information extraction in a multi-document setting. Mann and Yarowsky (2005) present an incremental approach where co-occurrence with a known relationship is a feature added in training and test. Culotta et al. (</context>
</contexts>
<marker>Nahm, Mooney, 2002</marker>
<rawString>U. Nahm and R. Mooney. 2002. Text mining with information extraction. In Proceedings of the AAAI 2220 Spring Symposium on Mining Answers from Texts and Knowledge Bases, pages 60–67.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pasca</author>
<author>D Lin</author>
<author>J Bigham</author>
<author>A Lifchits</author>
<author>A Jain</author>
</authors>
<title>Organizing and searching the world wide web of facts - step one: The one-million fact extracion challenge.</title>
<date>2006</date>
<booktitle>In AAAI.</booktitle>
<contexts>
<context position="1439" citStr="Pasca et al., 2006" startWordPosition="204" endWordPosition="207">ce the MUC evaluations1 in the mid-90s (Appelt et al., 1993; Grishman and Sundheim, 1996). Recently, there has been increased interest in the extraction of named entities and relationships from multiple documents, since the redundancy of information across documents has been shown to be a powerful resource for obtaining high quality information even when the extractors have access to little or no training data (Etzioni et al., 2004; Hasegawa et al., 2004). Much of the recent work in multidocument relationship extraction has focused on the extraction of isolated relationships (Agichtein, 2005; Pasca et al., 2006), but often the goal, as in 1http://www.itl.nist.gov/iaui/894.02/related projects/muc/ single document tasks like MUC, is to extract a template or a relational database composed of related facts. With databases containing multiple relationships, the semantics of the database impose constraints on possible database configurations. This paper presents a statistical method which picks relationships which violate few constraints as measured by a probabilistic database model. The constraints are hard constraints, and robust estimates are achieved by accounting for the underlying extraction/fusion u</context>
</contexts>
<marker>Pasca, Lin, Bigham, Lifchits, Jain, 2006</marker>
<rawString>M. Pasca, D. Lin, J. Bigham, A. Lifchits, and A. Jain. 2006. Organizing and searching the world wide web of facts - step one: The one-million fact extracion challenge. In AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Prager</author>
<author>J Chu-Carroll</author>
<author>K Czuba</author>
</authors>
<title>Question answering by constraint satisfaction: Qa-by-dossier with constraints.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>574--581</pages>
<contexts>
<context position="24929" citStr="Prager et al. (2004)" startWordPosition="4109" endWordPosition="4112">general framework for jointly conditioning multiple relationships. Additionally, this constraintbased approach can be applied without special training of the extraction/fusion system. In the context of information fusion of single relationships across multiple documents, Downey et al. (2005) propose a method that models the probabilities of positive and negative extracted classifications. More distantly related, Sutton and McCallum (2004) and Finkel et al. (2005) propose graphical models for combining information about a given entity from multiple mentions. In the field of question answering, Prager et al. (2004) answer a question about the list of compositions produced by a given subject by looking for related information about the subject’s birth and death. Their method treats supporting information as fixed hard constraints on the original questions and are applied in an ad-hoc fashion. This paper proposes a probabilistic method for using constraints in the context of database extraction and applies this method over a larger set of relations. Richardson and Domingos (2006) propose a method for reasoning about databases and logical constraints using Markov Random Fields. Their 338 model applies reas</context>
</contexts>
<marker>Prager, Chu-Carroll, Czuba, 2004</marker>
<rawString>J. Prager, J. Chu-Carroll, and K. Czuba. 2004. Question answering by constraint satisfaction: Qa-by-dossier with constraints. In Proceedings of ACL, pages 574–581.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ravichandran</author>
<author>E Hovy</author>
</authors>
<title>Learning surface text patterns for a question answering system.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>41--47</pages>
<contexts>
<context position="23333" citStr="Ravichandran and Hovy (2002)" startWordPosition="3872" endWordPosition="3875">ntext of whole database DB. The joint constraints perform best for the explicit relationships in isolation. Using constraints on implicit derived fields (Inoffice and Precedes) provides additional benefit above constraints strictly on explicit database fields (start, end, ceo). come artificially lower ranked than the unfused relationship type (ceo). The best performing contrained probabilistic database approach beats the baseline by 5 points. 5 Related Work Techniques for information extraction from minimally supervised data have been explored by Brin (1998), Agichtein and Gravano (2000), and Ravichandran and Hovy (2002). Those techniques propose methods for estimating extractors from example relationships and a corpus which contains instances of those relationships. Nahm and Mooney (2002) explore techniques for extracting multiple relationships in single document extraction. They learn rules for predicting certain fields given other extracted fields (i.e. a someone who lists Windows as a specialty is likely to know Microsoft Word). Perhaps the most related work to what is presented here is previous research which uses database information as co-occurrence features for information extraction in a multi-docume</context>
</contexts>
<marker>Ravichandran, Hovy, 2002</marker>
<rawString>D. Ravichandran and E. Hovy. 2002. Learning surface text patterns for a question answering system. In Proceedings of ACL, pages 41–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Richardson</author>
<author>P Domingos</author>
</authors>
<title>Markov logic networks.</title>
<date>2006</date>
<booktitle>Machine Learning,</booktitle>
<pages>62--107</pages>
<contexts>
<context position="25401" citStr="Richardson and Domingos (2006)" startWordPosition="4186" endWordPosition="4189">2005) propose graphical models for combining information about a given entity from multiple mentions. In the field of question answering, Prager et al. (2004) answer a question about the list of compositions produced by a given subject by looking for related information about the subject’s birth and death. Their method treats supporting information as fixed hard constraints on the original questions and are applied in an ad-hoc fashion. This paper proposes a probabilistic method for using constraints in the context of database extraction and applies this method over a larger set of relations. Richardson and Domingos (2006) propose a method for reasoning about databases and logical constraints using Markov Random Fields. Their 338 model applies reasoning starting from a known database. In this paper the database is built from extraction/fusion of relationships from web pages and contains a significant amount of noise. 6 Conclusion This paper has presented a probabilistic method for fusing extracted facts in the context of database extraction when there exist logical constraints between the fields in the database. The method estimates the probability than the inclusion of a given relationship will violate databas</context>
</contexts>
<marker>Richardson, Domingos, 2006</marker>
<rawString>M. Richardson and P. Domingos. 2006. Markov logic networks. Machine Learning, 62:107–136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Sutton</author>
<author>A McCallum</author>
</authors>
<title>Collective segmentation and labeling of distant entities in information extraction.</title>
<date>2004</date>
<booktitle>Presented at ICML Workshop on Statistical Relational Learning and Its Connections to Other</booktitle>
<tech>Technical Report TR # 04-49,</tech>
<institution>University of Massachusetts,</institution>
<location>Fields.</location>
<contexts>
<context position="24751" citStr="Sutton and McCallum (2004)" startWordPosition="4081" endWordPosition="4084">ta mining approach where discovered relationships from a database are used as features in extracting new relationships. The database constraints presented in this paper provide a more general framework for jointly conditioning multiple relationships. Additionally, this constraintbased approach can be applied without special training of the extraction/fusion system. In the context of information fusion of single relationships across multiple documents, Downey et al. (2005) propose a method that models the probabilities of positive and negative extracted classifications. More distantly related, Sutton and McCallum (2004) and Finkel et al. (2005) propose graphical models for combining information about a given entity from multiple mentions. In the field of question answering, Prager et al. (2004) answer a question about the list of compositions produced by a given subject by looking for related information about the subject’s birth and death. Their method treats supporting information as fixed hard constraints on the original questions and are applied in an ad-hoc fashion. This paper proposes a probabilistic method for using constraints in the context of database extraction and applies this method over a large</context>
</contexts>
<marker>Sutton, McCallum, 2004</marker>
<rawString>C. Sutton and A. McCallum. 2004. Collective segmentation and labeling of distant entities in information extraction. Technical Report TR # 04-49, University of Massachusetts, July. Presented at ICML Workshop on Statistical Relational Learning and Its Connections to Other Fields.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>