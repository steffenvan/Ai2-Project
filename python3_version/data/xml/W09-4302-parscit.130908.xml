<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002942">
<title confidence="0.989004">
Event Detection in Blogs using Temporal Random Indexing
</title>
<author confidence="0.994309">
David Jurgens and Keith Stevens
</author>
<affiliation confidence="0.998471">
University of California, Los Angeles
</affiliation>
<address confidence="0.8116995">
4732 Boelter Hall
Los Angeles, CA 90095
</address>
<email confidence="0.998962">
{jurgens, kstevensl@cs.ucla.edu
</email>
<sectionHeader confidence="0.994793" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997936">
Automatic event detection aims to identify novel, in-
teresting topics as they are published online. While
existing algorithms for event detection have focused
on newswire releases, we examine how event detec-
tion can work on less structured corpora of blogs.
The proliferation of blogs and other forms of self-
published media have given rise to an ever-growing
corpus of news, commentary and opinion texts. Blogs
offer a major advantage for event detection as their
content may be rapidly updated. However, blogs
texts also pose a significant challenge in that the de-
scribed events may be less easy to detect given the va-
riety of topics, writing styles and possible author bi-
ases. We propose a new way of detecting events in
this media by looking for changes in word semantics.
We first outline a new algorithm that makes use of a
temporally-annotated semantic space for tracking how
words change semantics. Then we demonstrate how
identified changes could be used to detect new events
and their associated blog entries.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999981012658228">
Automated event detection is a form of information re-
trieval where given a time-ordered set of documents, an al-
gorithm must select those which represent recent news or
changes to existing information. An automated approach
to event detection has many practical applications; given
the large amount of text being written daily, readers want
to be informed of which developments and topics are the
most recent and important without having to manually sift
through all the documents written on the topic. In addition,
a robust system should be able to detect multiple kinds of
events, such as international conflicts, product releases or
sports results. The main challenge in automating this task
is detecting what makes a new document sufficiently novel
to be described as a new event.
Current event detection approaches have focused on
identifying concrete events that occur within newswire
text[11]. However, in recent years, blogs have become
an important source of both news and commentary. Un-
like news reports, blog content expresses a wide range of
topics, opinions, vocabulary and writing styles; the change
in editorial requirements allows blog authors to comment
freely on local, national and international issues, while still
expressing their personal sentiment. Accordingly, blogs of-
fer a rich opportunity for detecting events that may not be
covered in traditional newswire text. These forms of self
published media might also allow event detection systems
to identify developing events before official news reports
can be written.
Several forms of event detection have focused on an-
alyzing named entities, such as “Bill Clinton” or “Iraq,”
and the contexts or documents in which they appear, e.g.
[10, 11, 5]. We propose a more general approach that looks
at all words and their contexts, rather than a predetermined
set of words. Specifically, we argue that event detection
can be done by measuring the semantic change in a word or
phrase. To track changes in the semantics, we use a seman-
tic space model of meaning, which is an automated method
of building distributed representations of word meaning.
Semantic space models of meaning offer three notable
advantages for event detection. First, the models are capa-
ble of automatically determining the semantics of a word
by examining the contexts in which the word appears. Such
automated understanding of semantics is required for ana-
lyzing these new sources of data due to the much wider
vocabulary used by authors. Second, the models offer a
well defined method for comparing the semantics between
words. These semantic comparisons have been shown to be
similar to human judgments[13]. We argue that reporting
words which have a notable changes in semantics should
correlate well with a reader’s expectations of interesting de-
velopments. Third, the models are well-established at de-
tecting association such as synonymy among words, which
can allow models to detect events that are referred to by
multiple names. Given these advantages, we introduce a
new semantic space algorithm for assessing how the mean-
ing of a word changes through time for the purpose of event
detection.
We illustrate our approach to topic detection with a hy-
pothetical example of the product release of a toy named
“blick.” At the start of the toy’s popularity, the word
“blick” has not occurred before and therefore its seman-
tics would be undefined. As “blick” appears in more blogs,
the word acquires consistent semantics, and the algorithm
can report a new event for “blick.” Our approach differs
from simple occurrence monitoring in that we require the
word to have a consistent meaning; unless the algorithm is
capable of determining what concepts the word refers to,
knowing that the word relates to an event is impossible.
However, consider detecting a second event for “blick”
soon after its release in which the toy is discovered to have
toxic properties. Since the toy’s name was already present
in the blogs, the novelty of the name is not enough to detect
the point at which the toxic chemical was revealed. How-
ever, our approach, which looks at the semantic shift of
words over time, would detect a shift based on the new
kinds of words that would be likely to co-occur with the
toy’s name, e.g. toxicity, a toy recall, or lawsuit. Intuitively
speaking, this approach associates news events with notice-
able changes in both what authors talk about and how they
</bodyText>
<page confidence="0.954761">
9
</page>
<bodyText confidence="0.995068">
Events in Emerging Text Types (eETTs) - Borovets, Bulgaria, pages 9–16
talk about those subject.
In this paper we present a new algorithm, Temporal
Random Indexing, that effectively captures the semantic
changes for words and phrases over time. We first briefly
review the semantic space model that underlies this ap-
proach and then present the algorithm. Following, we
demonstrate several examples of semantic change extracted
from a large blog corpus and illustrate one method for re-
porting the events.
</bodyText>
<sectionHeader confidence="0.930189" genericHeader="method">
2 Semantic Space Models
</sectionHeader>
<bodyText confidence="0.999651285714286">
Semantic space models of meaning are born from the dis-
tributed hypothesis: For two words, their similarity in
meaning is predicted by the similarity of their distributions
of co-occurring words[6], or as Firth puts it, “you shall
know a word by the company it keeps,”[4]. Creating se-
mantics from co-occurring words forms the basis for how
our algorithm represents changes in semantics.
</bodyText>
<subsectionHeader confidence="0.999451">
2.1 Semantics as Co-occurrence
</subsectionHeader>
<bodyText confidence="0.99998371875">
In a semantic space, a word’s semantics are mapped to high
dimensional vectors in a geometric space. The dimensions
of the space represent distinctions between the meanings
of words; accordingly, words with similar semantics have
similar vector representations. Semantic space representa-
tions have proven effective at a variety of information re-
trieval tasks such as identifying synonymous queries[21]
and multi-language retrieval[14, 23]. For a recent survey
of applications of semantic spaces to information retrieval
see Cohen and Widdows[3]. To illustrate the basics of co-
occurrence based semantic space models, we can further
explore the example of “blick”, the new yet toxic toy.
Consider the documents describing “blick” when it is
first introduced during a holiday season. A potential line
from several blogs might read “A perfect gift this holiday
season is blick, one of the newest toys available!” Using
a simple co-occurrence semantic space, the semantics of
“blick” would be a count of how frequently it co-occurs
with key words such as: gift, holiday, perfect and toys.
Examining later blog posts written when this same toy is
discovered to have toxic elements, several posts might now
have the line: “the toxic elements in blick make the toy
dangerous.” The semantics of the toy should now focus
primarily on the co-occurrence of words such as toxic and
dangerous, and should no longer be associated with pos-
itive words such as holiday and perfect. Figure 1 illus-
trates a simplified two-dimensional semantic space and the
changes to semantics that would occur as “blick” begins
to co-occur with toxic-related words. A standard semantic
space model would define the semantics of the new toy as a
combination of all co-occurrences, in this case the positive
new semantics and the negative semantics of toxicity.
</bodyText>
<subsectionHeader confidence="0.999119">
2.2 Random Indexing
</subsectionHeader>
<bodyText confidence="0.998413">
Using simple co-occurrence is rarely done in practice
for large corpora. In such models, each unique word
would be assigned its own dimension (corresponding to co-
occurrence with that word), which results in vectors with
hundreds of thousands to millions of dimensions. Basing
the number of dimensions on the number of unique words
</bodyText>
<figureCaption confidence="0.912055666666667">
Fig. 1: Word semantics projected into two dimensions, il-
lustrating the hypothetical change in meaning for “blick”
based on its nearest neighbors
</figureCaption>
<bodyText confidence="0.999976674418605">
is particularly problematic for blog corpora, as writers fre-
quently introduce misspellings, slang, or topic-specific jar-
gon. Accordingly, many approaches have focused on re-
ducing the dimensionality of the semantic space. Dimen-
sionality reduction often has the additional benefits such
as making the resulting vector more general, or reducing
computation time.
Early successful approaches such as Latent Semantic
Analysis[13] use the Singular Value Decomposition (SVD)
to reduce the number of dimensions. While the SVD re-
sults in significant improvements in information retrieval,
the fastest algorithms for the SVD are O(mn2) [7], which
make them impractical for large corpora. Moreover, the
SVD and other forms of principle component analysis must
have the entire corpus present at once, which makes it dif-
ficult to update the space as new words and contexts are
added. This is particularly problematic for event detec-
tion, as the corpus is expected to continuously grow as new
events occur.
Random Indexing[9, 18] offers an alternative method for
reducing the dimensionality of the semantic space by us-
ing a random projection of the full co-occurrence matrix
onto a lower dimensional space. Random Indexing oper-
ates as follows. Each unique word is assigned an index
vector, which is a random, sparse vector in a high dimen-
sional space, often 2000-10000 dimensions. The size of the
index vectors sets the number of dimensions used in the
resulting semantic space. Index vectors are created such
that any two arbitrary index vectors have a high probability
of being orthogonal. This property is necessary to accu-
rately approximate the original word co-occurrence matrix
in a lower dimension. The semantics of each word are cal-
culated by summing the index vectors of all co-occurring
words within a small window of text. Random Indexing
works well in practice as the dimensionality reduction oc-
curs as the corpus is being processed, rather than requiring
an explicit step after all the corpus has been seen.
More formally, let w be a focus word, wi be a co-
occurring word with a word distance of i and index(wi)
be the co-occurring word’s index vector. For the current
word, we define a window of size n words before and after,
which are counted as co-occurring. The semantics of w are
then defined as:
</bodyText>
<equation confidence="0.962903">
�semantics(w) =
VIED −n&lt;i&lt;n
</equation>
<bodyText confidence="0.817424">
where c is each occurrence of w in the corpus D.
</bodyText>
<equation confidence="0.704449">
toy
Blick
new
buy
lawsuit
toxic
Blick’
index(wi) (1)
</equation>
<page confidence="0.973765">
10
</page>
<subsectionHeader confidence="0.999725">
2.3 Adding Time to Semantic Space Models
</subsectionHeader>
<bodyText confidence="0.999956806451613">
Augmenting a semantic space with time has been rec-
ognized as an effective method for tracking changes in
semantics[20]. Two methods have been used to add tempo-
ral semantics. The first approach builds a separate seman-
tic space for each specific time range. Semantics are then
compared across spaces by defining some common context
which occurs in both spaces. The second approach builds
a single semantic space but provides the ability to segment
it based on time. The key difference between these ap-
proaches lies in the meaning of each semantic dimension;
when multiple spaces are used, there is no guarantee that
the specific semantic meaning associated with some dimen-
sion i will be the same for dimension i in another space.
Kontostathis et al.[10] and Fortuna et al.[5] have inde-
pendently proposed two successful semantic space algo-
rithms that use the first approach of processing several
distinct corpora. Both approaches collect several corpora
which span unique time ranges, and construct a semantic
space for each corpus using LSA. Using LSA is a notable
challenge as the space defined by LSA is based on the
SVD of a word x document matrix; with documents be-
ing unique to each time-span’s corpus, direct comparison
of vectors between spaces is not feasible.
Kontostathis et al.[10] use data mining to overcome the
change in dimension-meaning by first clustering the se-
mantics from each year. With this clustering, key attributes
are extracted from several time ranges, and significant dif-
ferences are used to infer an event or trend. In essence,
vector comparisons between the semantic spaces are by-
passed by using cross-space meta-statistics for each word
generated from each space. This approach is limited to be-
ing an offline approach due to the costly machine learning
techniques, and is further limited by key sets of attributes.
Another approach for comparing semantics from seman-
tic spaces has been introduced by Fortuna et al.[5]. Their
approach focused on finding key words that existed in mul-
tiple spaces, and defining a concrete set of semantics for
these landmark words. As semantics from distinct spaces
are created, they can be evaluated according to their re-
lation to these landmark terms, and at any point in time,
the words most closely associated to the landmark provide
terms describing events related to the landmarks.
Sagi et al. propose an alternate approach of uses a sin-
gle corpus and includes temporal semantics after generat-
ing an initial set of semantics[17]. This generates semantic
vectors for a corpus spanning many time ranges of inter-
est and reducing dimensionality via SVD. Then, to develop
temporal semantics for a term, documents from a specific
time range are used to generate temporal vectors through a
process very similar to Random Indexing; in this process
the first set of semantic vectors generated are used in place
of index vectors when using equation (1).
While these approaches allow for accurate representa-
tions of semantic shifts, they face significant challenges
when scaling to a large streaming set of documents, due to
a reliance on the SVD for dimensionality reduction. Addi-
tionally, none of the algorithms are able to change the time-
spans used for analysis without reprocessing some portion
of a corpus. Given these limitations, a computationally ef-
ficient modification to how a semantic space is produced
is necessary to permit more detailed analysis of changing
semantics.
</bodyText>
<figure confidence="0.526938">
semantics
</figure>
<figureCaption confidence="0.987329">
Fig. 2: The tensor representation of the semantic space
</figureCaption>
<sectionHeader confidence="0.953498" genericHeader="method">
3 Temporal Random Indexing
</sectionHeader>
<bodyText confidence="0.972847434782609">
Temporal Random Indexing (TRI) incorporates time by
building a single semantic space. However, instead of using
a word x semantics matrix, TRI uses a word x semantics
x time tensor. Figure 2 illustrates this change. The time
dimension records the semantic vectors of a word for each
time unit (e.g. a week or month).
TRI offers three major advantages over existing mod-
els. First, the semantic space is built incrementally so new
documents may be added at any time. Second, the ten-
sor representation allows for arbitrary time-range compar-
isons. Third, we use a dimensionality reduction similar to
that of Random Indexing, which operates as the documents
are processed, rather than all at once. This results in greatly
reduced time and memory requirements compared to those
methods that rely on the SVD for dimensionality reduction.
To associate specific time values with semantics, TRI
does not immediately perform the summation as defined in
equation (1). Instead TRI only accumulates the semantics
for contexts which occur in the same time period. These
time period semantics are then stored in chronologically as-
cending order to produce a semantic slice for a word. The
plane in figure 2 represents the semantic slice of a single
word, which covers all the time periods in which that word
has been observed.
Summing a semantic slice along the time dimension pro-
duces a vector equivalent to the results of Random Index-
ing, which would simply sum all the values, ignoring time,
to create a single vector for the word. TRI, on the other
hand, allows for more precise summations to be computed,
such as a summation over the entire known time range, a
single point in time, or several separate time ranges. Con-
sidering the example of a new toy that is first introduced
and later found to be toxic, figure 3 shows how one could
produce two semantic vectors for the toy “blick“ using a
semantic slice. The first semantic vector is a summation of
temporal semantic vectors that describe the introduction of
the new toy, and the second semantic vector is a summation
of temporal semantic vectors that describe the toxic nature
of the toy. Using this technique, TRI can produce two dis-
tinct semantic representations of the same word, based on
a simple partition of the temporal dimension. More over,
these two vectors can be still be directly compared because
they are built from the same index vectors.
Temporal Random Indexing can be formally described
as a modification of equation (1), with three additional
equations. As input, it takes an annotated collection of
</bodyText>
<figure confidence="0.995029785714286">
words
...
...
... ...
0 3 1 7 0 2
1
5
... ...
...
4
...
...
semantics of a word
from time i to j
</figure>
<page confidence="0.738882">
11
</page>
<figureCaption confidence="0.9955585">
Fig. 3: The semantic slice of “blick“ as the meaning
changes due to shifts in word co-occurrencepatterns
</figureCaption>
<bodyText confidence="0.918431833333333">
documents D = ((t0, d0), (t1, d1), (t2, d2), ..., (tk, dk)),
where di is the set of documents occurring at time ti. Let
WD be the set of all unique words in the collection. Just as
in Random Indexing, for each word w E WD, we assign a
unique index index(w).
Equation(1) can then be extended to be:
</bodyText>
<equation confidence="0.983869">
�semantics(w, t) =
ctEd; −n&lt;i&lt;n
</equation>
<bodyText confidence="0.999855333333333">
where t is a unique timestamp, and ct is the context for
an occurrence of w at time t. Using this new definition of
semantics, a word slice can be defined as:
</bodyText>
<equation confidence="0.995796">
slice(w) = {(ti, semantics(w, ti)|w E di, i = 1, k}
(3)
</equation>
<bodyText confidence="0.9605825">
The semantics of a word for some range of time can then
easily be computed with:
</bodyText>
<equation confidence="0.9837745">
�snapshot(w, ti, tj) = sm (4)
(t,,,,s,,,)Eslice(w),t;&lt;t,,,&lt;tj
</equation>
<sectionHeader confidence="0.999095" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9998354">
We applied TRI to the task of detecting events for a man-
ually selected set of 199 words from a variety of topics.
We selected words based on how frequently it was used in
a corpus and knowledge that it would be likely to be dis-
cussed in blogs. However, limiting the word selection was
done for efficiency, and this approach could be applied to
tracking events for a larger set of words. Due to limited
space, we illustrate the performance using a set of six word
of divergent topics that includes both abstract and specific
concepts: college, Lebanon, nuclear, Wii, PS3, and XP.
</bodyText>
<subsectionHeader confidence="0.990552">
4.1 The Corpus
</subsectionHeader>
<bodyText confidence="0.999850769230769">
Our approach can be applied to any corpus that has a known
date of authorship of each article, at the granularity desired
for analysis of semantic shifts. For the purpose of detecting
changes in public opinion over the course of recent events,
and the detection of previously unknown, but still interest-
ing, events, we have utilized a portion of an already existing
corpus[22].
The corpus comes from a collection of blog postings
from 2004 on. These blog postings come from around
the world, and in a variety of languages. We view this as
an excellent example of an unstructured corpus for event
detection since it is composed of blog articles harvested
by BlogLines1. The documents come from some standard
</bodyText>
<footnote confidence="0.863092">
1 http://www.bloglines.com
</footnote>
<bodyText confidence="0.999787147058823">
news sources, but also from any blogging service which
provides rss feeds, such as livejournal, local newspapers,
wordpress, and many more.
For this experiment, we collected only English articles
from the blog corpus, but the algorithm could be used in
practice with any language. The date of authorship for each
document in this collection is estimated to be the most re-
cent date the document has been updated.
Overall we expect this corpus to be well fitted to the
challenge of detecting events while handling multiple view
points beyond editorial control. Table 1 provides three
sample blog posts which exemplify the issue. Each of the
posts were written near the release of the Wii game console,
each with a significantly different usage of words, and sen-
timent. There is a clear range of styles, from the mechan-
ical description of the device, to opinions on the company
releasing the system, and finally to adoration of the sys-
tem. Beyond this sample set of posts, the corpus meets our
expectations in other ways. First, the lack of editorial over-
sight in the documents leads to grammatical and spelling
errors, and frequently to the introduction of new terms or
phrases unique to the author along with other issues2. Sec-
ond, the corpus has a large number of discussed topics,
ranging from international events, to product releases, and
to personal musings.
Before the corpus is used for performing event detec-
tion, the corpus is preprocessed to render it more uniform.
Similar to other semantic space approaches that used web-
gathered data[16], this pre-processing allows the model
to gracefully handle several irregularities in writing style,
such as inconsistent use of punctuation and capitalization.
Additionally, this process removes many tokens such as
html mark-up, which have little or no semantic content in
themselves3. The corpus is processed as follows:
</bodyText>
<listItem confidence="0.9914785">
1. Replace all numbers with &lt;num&gt;
2. Remove all html mark-up and email addresses
3. Remove unusual punctuation, and separate all other punctu-
ation from words
4. Remove words of 20 characters in length
5. Converting all words to lower case
6. Replacing $5 to &lt;num&gt; dollars
7. Discard articles with fewer than some threshold percentage
of correctly spelled English words
8. Associate each entry with a numeric timestamp
</listItem>
<bodyText confidence="0.9991816">
When computing the semantics, we also impose two fil-
ters on corpus during processing: any word in a list of fre-
quent closed-classed words and those words not in the most
frequent 250,000 words in the blog corpus were removed.
This step is both practical and empirically motivated.
Removing closed-class is a common practice in seman-
tic spaces models[16, 19], due to the low semantic value;
words such as “the” or “of” so frequently appear that they
do not serve to distinguish the meaning of any co-occurring
word. Similarly, infrequent words can safely be removed
for initial uses due to the small effect they would have on
other semantic vectors.
For both stop words and infrequent words, their origi-
nal position is preserved after removal. This ensures that
the window for counting co-occurrence takes into account
</bodyText>
<footnote confidence="0.9966545">
2 This may lead to an increase in polysemy and synonomy amongst
words, potentially impacting our approach, but exploration ofthis topic
is left for future work
3 We note that the HTML might be interpreted to yield more informa-
tion however, TRI is agnostic to its input, and so no special HTML
processing is done
</footnote>
<note confidence="0.913127">
The &amp;quot;blick&amp;quot; toy is
discovered to be toxic
The &amp;quot;blick&amp;quot; toy is introduced
</note>
<equation confidence="0.954532">
index(wi) (2)
</equation>
<page confidence="0.943657">
12
</page>
<bodyText confidence="0.972872666666667">
For those of you who went, I hope you guys With motion-sensing controls and While I agree that expanding video games beyond its core
had just as much fun as I did. One of the best three-dimensional movements on audience is certainly an intriguing idea (if not necessary),
parts was actually being able to play the Wii. screen, the upcoming Nintendo Wii it doesn’t exactly thrill me as a member of said audience.
When I picked up that controller, I was sold game platform is changing the way Nintendo has already done a fine job of turning us all into
instantly. The other awesome part was be- video game developers think about their little marketing minions with the DS. None of this
ing able to demo for Enchanted Arms in the games. will change with the Wii. I call it exploitation of the weak
</bodyText>
<table confidence="0.665505">
Ubisoft area. I have a bunch of pictures up on spot in our hearts for the big N.
my Flickr Account if people are interested.
</table>
<tableCaption confidence="0.997513">
Table 1: Contrasting blog entries about the Wii gaming console prior to its release in November 2006
</tableCaption>
<bodyText confidence="0.999836285714286">
the words originally within the window distance. All re-
maining words and tokens are assigned an index vector for
computing the semantics.
We limited the analysis to the 2006 postings in the cor-
pus; this constituted 15,725,511 blog entries and a total of
2.62 billion tokens (both words and punctuation) after the
normalization process.
</bodyText>
<subsectionHeader confidence="0.996067">
4.2 Detecting Events using TRI
</subsectionHeader>
<bodyText confidence="0.999934903225807">
Events are extracted using a three step process. First, TRI
is used to convert the corpus into semantic slices. A month-
long time span was selected after an empirical analysis of
the particular corpus showed that the the reduced frequency
of words in smaller time spans led to semantics that per-
formed less well. TRI was configured using 10,000 dimen-
sional vectors with a f3 word window. Index vectors had
the values of 4 dimensions randomly assigned to +1 or −1,
and the rest to be 0. Processing the entire corpus using TRI
took approximately 100 minutes on a 2.4GHz Intel Core 2
processor with 8 gigabytes of RAM.
In the second step, the semantic shift is calculated for
each word. To detect the shift, a word’s semantic vec-
tors for slices at time ti and ti+1 are compared using the
cosine similarity, which measures the similarity in angle
between vectors. The cosine similarity ranges between 1
and −1, indicating identical and opposing angles, respec-
tively. The semantic shift is defined as the 1− the cosine
similarity. Changes in angle reflect a change in a word’s
meaning, which in this system can signify the presence of
an event. Changes in magnitude were also tracked but an
analysis showed they were not correlated with events. Ta-
ble 2 shows semantic shifts for several test words.
The third step selects those topic words that undergo
a significant semantic shift and associate the topic words
with documents. We define the significance for a shift in
terms of its deviation from the mean semantic shift using a
simple time series analysis. Specifically, we calculate the
mean and standard deviation for the semantic shift of all
words in the two slices. If a word’s shift is greater than one
standard deviation away from the mean, then it the word is
marked as undergoing a significant shift. The bold values
in table 2 note these shifts for five example words.
To form the association between documents and topic
words, each word that undergoes a significant shift has its
nearest neighbors calculated. These neighbors are often
words associated with the topic word, but are not neces-
sarily synonyms. We posit that the neighbors provide con-
text about the nature of an event by virtue of reflecting the
frequent co-occurrences in the documents. To retrieve the
event-related documents, the topic word and its neighbors
are used as query terms to search the corpus during the
month that the event occurred. Documents are retrieved us-
ing a simple technique that returns the posts containing the
event term and the highest frequency of the related terms.
Accurately evaluating event detection requires a set of
events that are known a priori to bein the corpus. For large
corpora with millions of documents, such as the Bloglines
corpus used here, it is infeasible to determine the complete
set of events that are present. Furthermore, determining
what kinds of events may be present can prove problematic,
as blogs frequently discuss many topics outside the range
of normal news events. To create a baseline for evaluation,
We constructed a limited set of significant news events that
were likely to be in the corpus and then manually verified
their presence. Descriptive keywords for each event were
then to evaluate TRI. Ultimately, the evaluation is an anal-
ysis of not only TRI, but also the corpus itself, as some
terms, or events, may not be present at all within the cor-
pus. We plan to use this initial methodology to identify a
better means of analyzing massive corpora and the diverse
set of events contained therein.
</bodyText>
<subsectionHeader confidence="0.903582">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.999844787878788">
Several semantic shifts correlated well with known events
of 2006. We discuss the results by analyzing the events
detected for the words in table 2. Table 3 lists some of
the highest rated blogs associated with specific events our
technique detected.
Both the “Wii” and the “PS3” are gaming consoles re-
leased in North America in November 2006. However,
only the Wii experienced a significant semantic shift. The
stabilization of the semantics correlates with the products
demonstration at the Electronics Entertainment Expo, a
major gaming event. The corpus contained many examples
of attendees describing their experiences with both con-
soles at the convention. Notably the PS3 underwent only
a slight shift, indicating a fairly stable meaning. Further
analysis showed that the change in “Wii” was due to the
console being renamed from “Revolution” to “Wii” in late
April.
The 2006 Lebanon War took place in July 2006, which
was detected by a significant shift in meaning and is further
supported by a change in the nearest neighbors. In July, the
nearest neighbors of “Lebanon” were terms associated with
war, such as “Hezbollah”, “soldiers”, and “rockets”. How-
ever, before, and after the war, the ten closest neighbors to
“Lebanon” in 2006 were names of countries, revealing that
during the course of the war, the semantics of “Lebanon”
shifted dramaticly to a different class of words, and then
returned to it’s original class once the war concluded.
The changes for “nuclear” correspond directly to claims
that North Korea conducted nuclear tests in October 2006.
Until October, the related terms of “nuclear” are focused
on Iran, and nuclear power; during October, the neighbors
shift towards terms such as “Korea”, “atomic”, “sanctions”,
and “bomb.”
</bodyText>
<page confidence="0.998102">
13
</page>
<table confidence="0.991259571428571">
Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec
college 0.00 0.00 0.00 0.00 0.05 0.05 0.00 0.00 0.00 0.00 0.00
Lebanon 0.04 0.05 0.05 0.06 0.16 0.25 0.01 0.01 0.02 0.03 0.01
nuclear 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.01 0.11 0.06 0.02
PS3 0.06 0.04 0.06 0.05 0.03 0.04 0.03 0.02 0.02 0.03 0.01
Wii 0.12 0.14 0.15 0.06 0.03 0.04 0.04 0.02 0.01 0.01 0.01
XP 0.03 0.04 0.03 0.03 0.19 0.20 0.02 0.02 0.02 0.02 0.01
</table>
<tableCaption confidence="0.997739">
Table 2: Semantic shift values for six example words where bold indicates a significant change
</tableCaption>
<bodyText confidence="0.999950333333333">
Throughout the year, “college” experienced no notice-
able semantic shifts, despite the annual events of beginning
and graduating college. We view this as example of a con-
sistent word which acts as a reference point to other words.
An analysis for “XP” showed a semantic shift caused by
an unlikely change in corpus content; during the month of
June, spammers added such a high number of advertise-
ments for Windows XP copies, shifting the semantics to
unimportant terms. An examination of the nearest neigh-
bors to “XP” showed a dramatic change from related op-
erating system terms such as “Windows,” “Linux” and
“Vista” to numbers and currency abbreviations.
Overall, using the cosine similarity metric, and then fur-
ther examining the sets of nearest neighbors proved to be
an effective method of catching semantic shifts. Further-
more combining the nearest neighbors with a simple docu-
ment retrieval algorithm generated relevant documents cor-
responding to the events which caused the shift.
</bodyText>
<sectionHeader confidence="0.99985" genericHeader="method">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999951103896104">
Among the many systems that perform event detection,
several use a related technique that maps documents, rather
than words, into a vector space. Documents are represented
by a vector of their term frequencies, with most approaches
using some form of weighting the vectors, such as term
frequency-inverse document frequency (TF-IDF) weight-
ing. Additionally, many event detection systems restrict
themselves to processing newswire text, instead of blogs.
Most document based event detection algorithms extend
a core usage of TF-IDF weighting. In the core event de-
tection algorithm, each document is analyzed to produce
TF-IDF values for each word occurring in the document.
This set of values can then be compared against TF-IDF
values of other documents using the same similarity mea-
sures used in semantic space models, with cosine similar-
ity being one of the most common. In general, an event is
detected if the current document is significantly different
from all other processed documents, based on a threshold
of similarity values between documents[1, 11].
Brants et al. introduced some significant improvements
to the standard document based model[1]. The first im-
provement was to compute the TF-IDF values on a docu-
ment by document basis, allowing the system to continu-
ously process new documents. The second improvement
was computing a set of TF-IDF values which were depen-
dent on the source of the document, under the assumption
that some words, such as CNN, would be more frequent
based on who wrote the document. Beyond modifying the
TF-IDF values, the similarity measure was also extended to
include some normalization techniques that take into con-
sideration the source of the documents, and the average
similarity of documents. Finally, their model was extended
to compare segmentations of documents, rather than entire
documents. Overall, these modifications showed notice-
able improvements over the basic usage of TF-IDF values
and similarity metrics.
Kumaran and Allan expand on Brants et al. by consider-
ing not only the term frequencies when computing the sim-
ilarity between documents, but also named entities, such as
“President Obama,” in the documents[11]. Two additional
vectors are created for each document: One composed of
just the named entities occurring in a document, and an-
other composed of all words that are not named entities.
When comparing the similarity between two documents,
the standard vector is initially used, and then the similarity
between the additional vectors are used to provide finer dis-
tinctions, such as whether two documents refer to the same
set of named entities, and the same set of general topics,
i.e. all the non named entities.
In [12], Lam et al. extend a document-space approach
by associating each document with three vectors: a TF-
IDF weighted vector; a TF-IDF score of named entities
present in the document, similar to [11]; and a concept vec-
tor, which details which abstract concepts are contained in
the document, using TF-IDF scores based on the frequency
of concepts rather than words. The key terms in a docu-
ment are each given a weight based on which key terms the
document contains. Event detection is done by clustering
documents as they appear. Each cluster is said to repre-
sent a specific event; and documents that do not fit into one
cluster are said to be new events. Chen et al. use a similar
clustering for event detection but use sentences rather than
entire document[2].
Makkonen et al. augment the document representa-
tion by using an existing ontology to extract out loca-
tions, proper names, and temporal references from the
document[15]. These three, combined with the remaining
terms in the document are used as the basis for comparison.
Overall, the current event detection systems that do not
utilize a semantic space have the key benefit of being able
to process documents continuously, since no reduction step
is required for vector representations. But the key differ-
ence is the focus on comparisons between documents, and
not words that occur in documents. These approaches must
handle different challenges, such as documents that discuss
multiple events and elements of documents that are vague
but important for distinguishing events.
</bodyText>
<sectionHeader confidence="0.998975" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.9997848">
The semantic space model we have presented has a num-
ber of benefits and drawbacks compared to other semantic
space and document based techniques for automatic event
detection. The most significant outstanding question is how
to analyze all the semantic slices produced in an efficient
</bodyText>
<page confidence="0.997339">
14
</page>
<subsectionHeader confidence="0.645424">
Lebanon nuclear Wii
</subsectionHeader>
<bodyText confidence="0.998033193548387">
Exercising great restraint, they instantly
launched airstrikes on Lebanon, damaging
critical roads, [...], power stations, etc. Hezbol-
lah retaliated with a stream of rockets that
penetrated as far Ashaifa, causing a great deal
of terror to Israeli civilians.
[T]heir goal is to move public opinion in
Lebanon against Hezbollah due to the destruc-
tion “they” caused the country, establish a
strong deterrence for any future attacks, and ,
of course, destroy as much of Hezbollah’s in-
frastructure and weapons as possible.
Pyongyang would not hold negotiations to re-
solve the outstanding issues with Washington,
[...] “the Americans never recognized our se-
curity and we were forced to conduct nuclear
test to defend ourselves.”
Korea nuclear test hasn’t tipped military bal-
ance [...] hours ago questions surrounding
North Korea and its nascent nuclear weapons
program took center stage Monday night
The Wii was the name of the console.. It was
time to see if it could deliver its promise of
“changing the way we game.” Their presenta-
tion was probably the most “fun,” cutting right
to the chase and demonstrating uses of the con-
troller in games.
It turns out, according to eyewitness reports
from the show floor, that we might not have
been playing Wii consoles at the Nintendo
booth...should I feel betrayed?
</bodyText>
<tableCaption confidence="0.926218">
Table 3: Blog snippets describing events associated with “Lebanon” in July, “nuclear” in October, and “Wii” in May
</tableCaption>
<bodyText confidence="0.999981056179776">
manner that exposes events. While our preliminary anal-
ysis has shown that events can be detected using TRI, our
approach does not currently scale to searching across all
terms, nor to identifying events for new words that infre-
quently occur. However, we argue that the advantages pro-
vided by TRI outweigh the outstanding issues and merits
further work to address these limitations.
Event detection systems that use semantic spaces have
two notable challenges due to how time is integrated. First,
the space must be easily modifiable as new documents are
produced. Existing approaches use a single dimensional-
ity reduction step after a corpus had been processed to im-
prove information retrieval. However this step limits the
integration of new documents into the semantic space; to
integrate new documents, the space must be completely
recomputed. The second challenge stems from compar-
ing word meanings and documents that occur in different
times. Approaches such as [10, 5] that arbitrarily segment
the corpora used into different semantic spaces artificially
limit both the types of comparisons available and the spe-
cific time ranges of the semantics. TRI addresses both of
these challenges efficiently. By being based on Random In-
dexing, dimensionality reduction is done concurrently with
developing semantic vectors. Additionally, by utilizing the
same set of index vectors over all documents analyzed, ev-
ery semantic slice is contained within the same semantic
space, avoiding the need for reference only those vectors
that are common to several time periods.
Conversely, the document based methods discussed in
section 5 provided a means of avoiding a post processing
stage by incrementally determining the TF-IDF values for
words in the corpus. While these approaches efficiently
allow the inclusion of more documents over time, each
document vector encounters similar problems seen in ba-
sic co-occurrence semantic space models, most notably the
requirement that two documents have the same exact words
for them to be declared similar.
The introduction of additional vector representations of a
document, such as the named entity vectors, or the concept
vectors, attempt to address this issue, but these additions
allude to benefits provided by a semantic space model. For
instance, if two documents describe the same events, but
without using the same set of words, and instead use highly
similar words to describe the event differently, the doc-
ument based event detection methods would either report
two distinct events, or rely on some system which can de-
termine the similarity between two words. Being based on
word semantics, TRI avoids this problem, and provides a
way of determining how similar two terms are, or which
concepts a word refers to. With TRI, synonymous key
words describing the event are modified in a similar man-
ner, and words with similar meanings will have similar ef-
fects on the semantics. It may also be possible with TRI to
detect synonymous event names by identifying words with
similar shifts and similar neighbors. However, further in-
vestigation is needed.
While TRI provides elegant solutions to several prob-
lems in event detection, significant questions still remain.
First, a suitable method of analyzing the semantic shift
between vectors is needed. Our initial experiment illus-
trates tracking outliers based on cosine similarity works
well in practice; however, this does not utilize all the in-
formation present and could leave some events undetected.
Time series analysis or probability distribution analysis are
two techniques which might be well suited for similarity
comparisons between semantic slices. However, it remains
an open question of what limitations exist to the types of
events TRI can be detected, and whether the method of
comparison can be targeted to find specific kinds of events.
As a second issue, the relationship should be established
between the corpus, the duration of a semantic slice, and
the types of events that are detected. Our current sys-
tem was able to detect changes at a monthly granularity,
but real-time event detection must operate on a much finer
scale. Further work is needed to determine how brief a se-
mantic slice can be while still adequately representing the
semantics necessary for event detection.
Regarding the granularity of semantic slices and seman-
tic vectors, we suspect that the optimal granularity is highly
dependent on how dense documents are with regards to
time in the corpus. One drawback of Random Indexing
is the need for a large amount of data, and if there is not
enough data, semantic vectors become poorly defined and
produce weak similarity scores. We found that the cor-
pus used in the experiment was sparse enough to produce a
degradation in the semantics when our semantic slices were
set to a time range shorter than a month. Ideally the corpus
should have a dense enough set of topics for very narrow
semantic slices.
</bodyText>
<sectionHeader confidence="0.998933" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9991428">
Unstructured, unfiltered corpora such as blogs present an
ideal opportunity for automated event-detection systems to
identify new events before they can be reported through
more formal sources. We have presented an algorithm
that uses changes in word semantics to detect new events
</bodyText>
<page confidence="0.990967">
15
</page>
<bodyText confidence="0.999762666666667">
in blog posts. Our approach utilizes simple word co-
occurrence and scales well to processing millions of blog
posts. Additionally, initial experiments to identify events
for specific words proved successful. Further work is
needed to identify the strengths and weakness of this ap-
proach and quantify its ability to detect events. However,
we plan to address these issues in future work. Last we
plan to release the implementation of TRI as a part of the
S-Space Package[8].
</bodyText>
<sectionHeader confidence="0.996389" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.991609666666667">
We thank John Cho and his lab for access to the blogline
corpus used in this work. We also thank the anonymous
reviewers for their comments and suggestions.
</bodyText>
<sectionHeader confidence="0.999127" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999972979381444">
[1] T. Brants, F. Chen, and A. Farahat. A system for
new event detection. In SIGIR ’03: Proceedings of
the 26th annual international ACMSIGIR conference
on Research and development in informaion retrieval,
pages 330–337, New York, NY, USA, 2003. ACM.
[2] K.-Y. Chen, L. Luesukprasert, Seng-cho, and
T. Chou. Hot Topic Extraction Based on Timeline
Analysis and Multidimensional Sentence Modeling.
IEEE Transactions on Knowledge and Data Engi-
neering, 19(8):1016–1025, 2007.
[3] T. Cohen and D. Widdows. Empirical distribu-
tional semantics: Methods and biomedical applica-
tions. Journal of Biomedial Informatics, 42(2):390–
405, 2009.
[4] J. R. Firth. A synopsis of linguistic theory 1930-1955.
Oxford: Philological Society, 1957. Reprinted in F.
R. Palmer (Ed.), (1968). Selected papers of J. R. Firth
1952-1959, London: Longman.
[5] B. Fortuna, D. Mladeni´c, , and M. Grobelnik. Visu-
alization of temporal semantic spaces. In J. Davies,
M. Grobelnik, and D. Mladeni´c, editors, Semantic
Knowledge Management, pages 155–169. Springer
Berlin Heidelberg, 2009.
[6] Z. Harris. Mathematical Structures ofLanguage. Wi-
ley, New York, 1968.
[7] D. B. III and L. N. Trefethen. Numerical linear alge-
bra. Philadelphia: Society for Industrial and Applied
Mathematics, 1997.
[8] D. Jurgens and K. Stevens. The S-Space Pack-
age: An open source package for semantic spaces.
http://code.google.com/p/airhead-research/.
[9] P. Kanerva, J. Kristoferson, and A. Holst. Random
indexing of text samples for latent semantic analysis.
In L. R. Gleitman and A. K. Josh, editors, Proceed-
ings of the 22nd Annual Conference of the Cognitive
Science Society, page 1036, 2000.
[10] A. Kontostathis, I. De, L. E. Holzman, and W. M. Pot-
tenger. Use of term clusters for emerging trend detec-
tion. Technical report, Lehigh University, 2004.
[11] G. Kumaran and J. Allan. Text classification and
named entities for new event detection. In SIGIR ’04:
Proceedings of the 27th annual international ACM
SIGIR conference on Research and development in in-
formation retrieval, pages 297–304. ACM, 2004.
[12] W. Lam, H. Meng, K. L. Wong, and J. Yen. Using
contextual analysis for news event detection. Inter-
national Journal on Intelligent Systems, 16(44):525–
546, 2001.
[13] T. K. Landauer and S. T. Dumais. A solution to
Plato’s problem: The Latent Semantic Analysis the-
ory of the acquisition, induction, and representation
of knowledge. Psychological Review, 104:211–240,
1997.
[14] M. L. Littman, S. T. Dumais, and T. K. Landauer.
Automatic cross-language information retrieval using
latent semantic indexing. In G. Grefenstette, editor,
Cross language information retrieval. Kluwer, 1998.
[15] J. Makkonen, H. Ahonen-Myka, and M. Salmenkivi.
Simple Semantics in Topic Detection and Tracking.
Information Retrieval, 7:347–368, 2004.
[16] D. L. T. Rohde, L. M. Gonnerman, and D. C. Plaut.
An improved model of semantic similarity based on
lexical co-occurrence. Cognitive Science, 2009. sub-
mitted.
[17] E. Sagi, S. Kaufmann, and B. Clark. Semantic density
analysis: Comparing word meaning across time and
phonetic space. In Proceedings of the Workshop on
Geometrical Models ofNatural Language Semantics,
pages 104–111, Athens, Greece, March 2009. Asso-
ciation for Computational Linguistics.
[18] M. Sahlgren. Vector-based semantic analysis: Repre-
senting word meanings based on random labels. In
Proceedings of the ESSLLI 2001 Workshop on Se-
mantic Knowledge Acquisition and Categorisation,
Helsinki, Finland, 2001.
[19] M. Sahlgren, A. Holst, and P. Kanerva. Permutations
as a means to encode order in word space. In Pro-
ceedings of the 30th Annual Meeting of the Cognitive
Science Society (CogSci’08), 2008.
[20] M. Sahlgren and J. Karlgren. Buzz Monitoring in
Word Space. In Proceedings of European Confer-
ence on Intelligence and Security Informatics (Eu-
roISI 2008), Esbjerg, Denmark, 2008.
[21] H. Sch¨utze and J. O. Pedersen. A cooccurrence-
based thesaurus and two applications to information
retrieval. Information Processing and Management,
33(3):307–318, 1997.
[22] K. C. Sia, J. Cho, Y. Chi, and B. L. Tseng. Effi-
cient computation of personal aggregate queries on
blogs. In Proceedings of the ACMSIGKDD Interna-
tional Conference on Knowledge Discovery and Data
Mining (KDD), August 2008.
[23] A. Vinokourov, J. Shawe-Taylor, and N. Cristianini.
Finding Language-Independent Semantic Represen-
tation of Text using Kernel Canonical Correlation
Analysis. Technical report, Neurocolt, 2002. Neu-
roCOLT Technical Report NC-TR-02-119.
</reference>
<page confidence="0.998701">
16
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.902982">
<title confidence="0.997821">Event Detection in Blogs using Temporal Random Indexing</title>
<author confidence="0.936988">Jurgens</author>
<affiliation confidence="0.99796">University of California, Los</affiliation>
<address confidence="0.985314">4732 Boelter Los Angeles, CA</address>
<abstract confidence="0.999705666666667">Automatic event detection aims to identify novel, interesting topics as they are published online. While existing algorithms for event detection have focused on newswire releases, we examine how event detection can work on less structured corpora of blogs. The proliferation of blogs and other forms of selfpublished media have given rise to an ever-growing corpus of news, commentary and opinion texts. Blogs offer a major advantage for event detection as their content may be rapidly updated. However, blogs texts also pose a significant challenge in that the described events may be less easy to detect given the variety of topics, writing styles and possible author biases. We propose a new way of detecting events in this media by looking for changes in word semantics. We first outline a new algorithm that makes use of a temporally-annotated semantic space for tracking how words change semantics. Then we demonstrate how identified changes could be used to detect new events and their associated blog entries.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>T Brants</author>
<author>F Chen</author>
<author>A Farahat</author>
</authors>
<title>A system for new event detection.</title>
<date>2003</date>
<booktitle>In SIGIR ’03: Proceedings of the 26th annual international ACMSIGIR conference on Research and development in informaion retrieval,</booktitle>
<pages>330--337</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA,</location>
<contexts>
<context position="32492" citStr="[1, 11]" startWordPosition="5378" endWordPosition="5379">. Most document based event detection algorithms extend a core usage of TF-IDF weighting. In the core event detection algorithm, each document is analyzed to produce TF-IDF values for each word occurring in the document. This set of values can then be compared against TF-IDF values of other documents using the same similarity measures used in semantic space models, with cosine similarity being one of the most common. In general, an event is detected if the current document is significantly different from all other processed documents, based on a threshold of similarity values between documents[1, 11]. Brants et al. introduced some significant improvements to the standard document based model[1]. The first improvement was to compute the TF-IDF values on a document by document basis, allowing the system to continuously process new documents. The second improvement was computing a set of TF-IDF values which were dependent on the source of the document, under the assumption that some words, such as CNN, would be more frequent based on who wrote the document. Beyond modifying the TF-IDF values, the similarity measure was also extended to include some normalization techniques that take into con</context>
</contexts>
<marker>[1]</marker>
<rawString>T. Brants, F. Chen, and A. Farahat. A system for new event detection. In SIGIR ’03: Proceedings of the 26th annual international ACMSIGIR conference on Research and development in informaion retrieval, pages 330–337, New York, NY, USA, 2003. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K-Y Chen</author>
<author>L Luesukprasert</author>
<author>Seng-cho</author>
<author>T Chou</author>
</authors>
<title>Hot Topic Extraction Based on Timeline Analysis and Multidimensional Sentence Modeling.</title>
<date>2007</date>
<journal>IEEE Transactions on Knowledge and Data Engineering,</journal>
<volume>19</volume>
<issue>8</issue>
<contexts>
<context position="34879" citStr="[2]" startWordPosition="5772" endWordPosition="5772"> present in the document, similar to [11]; and a concept vector, which details which abstract concepts are contained in the document, using TF-IDF scores based on the frequency of concepts rather than words. The key terms in a document are each given a weight based on which key terms the document contains. Event detection is done by clustering documents as they appear. Each cluster is said to represent a specific event; and documents that do not fit into one cluster are said to be new events. Chen et al. use a similar clustering for event detection but use sentences rather than entire document[2]. Makkonen et al. augment the document representation by using an existing ontology to extract out locations, proper names, and temporal references from the document[15]. These three, combined with the remaining terms in the document are used as the basis for comparison. Overall, the current event detection systems that do not utilize a semantic space have the key benefit of being able to process documents continuously, since no reduction step is required for vector representations. But the key difference is the focus on comparisons between documents, and not words that occur in documents. The</context>
</contexts>
<marker>[2]</marker>
<rawString>K.-Y. Chen, L. Luesukprasert, Seng-cho, and T. Chou. Hot Topic Extraction Based on Timeline Analysis and Multidimensional Sentence Modeling. IEEE Transactions on Knowledge and Data Engineering, 19(8):1016–1025, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Cohen</author>
<author>D Widdows</author>
</authors>
<title>Empirical distributional semantics: Methods and biomedical applications.</title>
<date>2009</date>
<journal>Journal of Biomedial Informatics,</journal>
<volume>42</volume>
<issue>2</issue>
<pages>405</pages>
<contexts>
<context position="7143" citStr="[3]" startWordPosition="1143" endWordPosition="1143">sents changes in semantics. 2.1 Semantics as Co-occurrence In a semantic space, a word’s semantics are mapped to high dimensional vectors in a geometric space. The dimensions of the space represent distinctions between the meanings of words; accordingly, words with similar semantics have similar vector representations. Semantic space representations have proven effective at a variety of information retrieval tasks such as identifying synonymous queries[21] and multi-language retrieval[14, 23]. For a recent survey of applications of semantic spaces to information retrieval see Cohen and Widdows[3]. To illustrate the basics of cooccurrence based semantic space models, we can further explore the example of “blick”, the new yet toxic toy. Consider the documents describing “blick” when it is first introduced during a holiday season. A potential line from several blogs might read “A perfect gift this holiday season is blick, one of the newest toys available!” Using a simple co-occurrence semantic space, the semantics of “blick” would be a count of how frequently it co-occurs with key words such as: gift, holiday, perfect and toys. Examining later blog posts written when this same toy is dis</context>
</contexts>
<marker>[3]</marker>
<rawString>T. Cohen and D. Widdows. Empirical distributional semantics: Methods and biomedical applications. Journal of Biomedial Informatics, 42(2):390– 405, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Firth</author>
</authors>
<title>A synopsis of linguistic theory 1930-1955.</title>
<date>1957</date>
<publisher>Philological Society,</publisher>
<location>Oxford:</location>
<note>Reprinted in</note>
<contexts>
<context position="6452" citStr="[4]" startWordPosition="1044" endWordPosition="1044">semantic changes for words and phrases over time. We first briefly review the semantic space model that underlies this approach and then present the algorithm. Following, we demonstrate several examples of semantic change extracted from a large blog corpus and illustrate one method for reporting the events. 2 Semantic Space Models Semantic space models of meaning are born from the distributed hypothesis: For two words, their similarity in meaning is predicted by the similarity of their distributions of co-occurring words[6], or as Firth puts it, “you shall know a word by the company it keeps,”[4]. Creating semantics from co-occurring words forms the basis for how our algorithm represents changes in semantics. 2.1 Semantics as Co-occurrence In a semantic space, a word’s semantics are mapped to high dimensional vectors in a geometric space. The dimensions of the space represent distinctions between the meanings of words; accordingly, words with similar semantics have similar vector representations. Semantic space representations have proven effective at a variety of information retrieval tasks such as identifying synonymous queries[21] and multi-language retrieval[14, 23]. For a recent </context>
</contexts>
<marker>[4]</marker>
<rawString>J. R. Firth. A synopsis of linguistic theory 1930-1955. Oxford: Philological Society, 1957. Reprinted in F. R. Palmer (Ed.), (1968). Selected papers of J. R. Firth 1952-1959, London: Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Fortuna</author>
<author>D Mladeni´c</author>
</authors>
<title>Visualization of temporal semantic spaces. In</title>
<date>2009</date>
<booktitle>Semantic Knowledge Management,</booktitle>
<pages>155--169</pages>
<editor>J. Davies, M. Grobelnik, and D. Mladeni´c, editors,</editor>
<publisher>Springer</publisher>
<location>Berlin Heidelberg,</location>
<contexts>
<context position="2959" citStr="[10, 11, 5]" startWordPosition="464" endWordPosition="466">ange in editorial requirements allows blog authors to comment freely on local, national and international issues, while still expressing their personal sentiment. Accordingly, blogs offer a rich opportunity for detecting events that may not be covered in traditional newswire text. These forms of self published media might also allow event detection systems to identify developing events before official news reports can be written. Several forms of event detection have focused on analyzing named entities, such as “Bill Clinton” or “Iraq,” and the contexts or documents in which they appear, e.g. [10, 11, 5]. We propose a more general approach that looks at all words and their contexts, rather than a predetermined set of words. Specifically, we argue that event detection can be done by measuring the semantic change in a word or phrase. To track changes in the semantics, we use a semantic space model of meaning, which is an automated method of building distributed representations of word meaning. Semantic space models of meaning offer three notable advantages for event detection. First, the models are capable of automatically determining the semantics of a word by examining the contexts in which t</context>
<context position="12168" citStr="[5]" startWordPosition="1967" endWordPosition="1967">cs. The first approach builds a separate semantic space for each specific time range. Semantics are then compared across spaces by defining some common context which occurs in both spaces. The second approach builds a single semantic space but provides the ability to segment it based on time. The key difference between these approaches lies in the meaning of each semantic dimension; when multiple spaces are used, there is no guarantee that the specific semantic meaning associated with some dimension i will be the same for dimension i in another space. Kontostathis et al.[10] and Fortuna et al.[5] have independently proposed two successful semantic space algorithms that use the first approach of processing several distinct corpora. Both approaches collect several corpora which span unique time ranges, and construct a semantic space for each corpus using LSA. Using LSA is a notable challenge as the space defined by LSA is based on the SVD of a word x document matrix; with documents being unique to each time-span’s corpus, direct comparison of vectors between spaces is not feasible. Kontostathis et al.[10] use data mining to overcome the change in dimension-meaning by first clustering th</context>
<context position="38371" citStr="[10, 5]" startWordPosition="6328" endWordPosition="6329">e limitations. Event detection systems that use semantic spaces have two notable challenges due to how time is integrated. First, the space must be easily modifiable as new documents are produced. Existing approaches use a single dimensionality reduction step after a corpus had been processed to improve information retrieval. However this step limits the integration of new documents into the semantic space; to integrate new documents, the space must be completely recomputed. The second challenge stems from comparing word meanings and documents that occur in different times. Approaches such as [10, 5] that arbitrarily segment the corpora used into different semantic spaces artificially limit both the types of comparisons available and the specific time ranges of the semantics. TRI addresses both of these challenges efficiently. By being based on Random Indexing, dimensionality reduction is done concurrently with developing semantic vectors. Additionally, by utilizing the same set of index vectors over all documents analyzed, every semantic slice is contained within the same semantic space, avoiding the need for reference only those vectors that are common to several time periods. Conversel</context>
</contexts>
<marker>[5]</marker>
<rawString>B. Fortuna, D. Mladeni´c, , and M. Grobelnik. Visualization of temporal semantic spaces. In J. Davies, M. Grobelnik, and D. Mladeni´c, editors, Semantic Knowledge Management, pages 155–169. Springer Berlin Heidelberg, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Harris</author>
</authors>
<title>Mathematical Structures ofLanguage.</title>
<date>1968</date>
<publisher>Wiley,</publisher>
<location>New York,</location>
<contexts>
<context position="6378" citStr="[6]" startWordPosition="1029" endWordPosition="1029"> a new algorithm, Temporal Random Indexing, that effectively captures the semantic changes for words and phrases over time. We first briefly review the semantic space model that underlies this approach and then present the algorithm. Following, we demonstrate several examples of semantic change extracted from a large blog corpus and illustrate one method for reporting the events. 2 Semantic Space Models Semantic space models of meaning are born from the distributed hypothesis: For two words, their similarity in meaning is predicted by the similarity of their distributions of co-occurring words[6], or as Firth puts it, “you shall know a word by the company it keeps,”[4]. Creating semantics from co-occurring words forms the basis for how our algorithm represents changes in semantics. 2.1 Semantics as Co-occurrence In a semantic space, a word’s semantics are mapped to high dimensional vectors in a geometric space. The dimensions of the space represent distinctions between the meanings of words; accordingly, words with similar semantics have similar vector representations. Semantic space representations have proven effective at a variety of information retrieval tasks such as identifying </context>
</contexts>
<marker>[6]</marker>
<rawString>Z. Harris. Mathematical Structures ofLanguage. Wiley, New York, 1968.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D B</author>
<author>L N Trefethen</author>
</authors>
<title>Numerical linear algebra. Philadelphia: Society for Industrial and Applied Mathematics,</title>
<date>1997</date>
<contexts>
<context position="9549" citStr="[7]" startWordPosition="1523" endWordPosition="1523">c for blog corpora, as writers frequently introduce misspellings, slang, or topic-specific jargon. Accordingly, many approaches have focused on reducing the dimensionality of the semantic space. Dimensionality reduction often has the additional benefits such as making the resulting vector more general, or reducing computation time. Early successful approaches such as Latent Semantic Analysis[13] use the Singular Value Decomposition (SVD) to reduce the number of dimensions. While the SVD results in significant improvements in information retrieval, the fastest algorithms for the SVD are O(mn2) [7], which make them impractical for large corpora. Moreover, the SVD and other forms of principle component analysis must have the entire corpus present at once, which makes it difficult to update the space as new words and contexts are added. This is particularly problematic for event detection, as the corpus is expected to continuously grow as new events occur. Random Indexing[9, 18] offers an alternative method for reducing the dimensionality of the semantic space by using a random projection of the full co-occurrence matrix onto a lower dimensional space. Random Indexing operates as follows.</context>
</contexts>
<marker>[7]</marker>
<rawString>D. B. III and L. N. Trefethen. Numerical linear algebra. Philadelphia: Society for Industrial and Applied Mathematics, 1997.</rawString>
</citation>
<citation valid="false">
<authors>
<author>D Jurgens</author>
<author>K Stevens</author>
</authors>
<title>The S-Space Package: An open source package for semantic spaces.</title>
<note>http://code.google.com/p/airhead-research/.</note>
<marker>[8]</marker>
<rawString>D. Jurgens and K. Stevens. The S-Space Package: An open source package for semantic spaces. http://code.google.com/p/airhead-research/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Kanerva</author>
<author>J Kristoferson</author>
<author>A Holst</author>
</authors>
<title>Random indexing of text samples for latent semantic analysis.</title>
<date>2000</date>
<booktitle>Proceedings of the 22nd Annual Conference of the Cognitive Science Society,</booktitle>
<pages>1036</pages>
<editor>In L. R. Gleitman and A. K. Josh, editors,</editor>
<contexts>
<context position="9935" citStr="[9, 18]" startWordPosition="1586" endWordPosition="1587">tic Analysis[13] use the Singular Value Decomposition (SVD) to reduce the number of dimensions. While the SVD results in significant improvements in information retrieval, the fastest algorithms for the SVD are O(mn2) [7], which make them impractical for large corpora. Moreover, the SVD and other forms of principle component analysis must have the entire corpus present at once, which makes it difficult to update the space as new words and contexts are added. This is particularly problematic for event detection, as the corpus is expected to continuously grow as new events occur. Random Indexing[9, 18] offers an alternative method for reducing the dimensionality of the semantic space by using a random projection of the full co-occurrence matrix onto a lower dimensional space. Random Indexing operates as follows. Each unique word is assigned an index vector, which is a random, sparse vector in a high dimensional space, often 2000-10000 dimensions. The size of the index vectors sets the number of dimensions used in the resulting semantic space. Index vectors are created such that any two arbitrary index vectors have a high probability of being orthogonal. This property is necessary to accurat</context>
</contexts>
<marker>[9]</marker>
<rawString>P. Kanerva, J. Kristoferson, and A. Holst. Random indexing of text samples for latent semantic analysis. In L. R. Gleitman and A. K. Josh, editors, Proceedings of the 22nd Annual Conference of the Cognitive Science Society, page 1036, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kontostathis</author>
<author>I De</author>
<author>L E Holzman</author>
<author>W M Pottenger</author>
</authors>
<title>Use of term clusters for emerging trend detection.</title>
<date>2004</date>
<tech>Technical report,</tech>
<institution>Lehigh University,</institution>
<contexts>
<context position="2959" citStr="[10, 11, 5]" startWordPosition="464" endWordPosition="466">ange in editorial requirements allows blog authors to comment freely on local, national and international issues, while still expressing their personal sentiment. Accordingly, blogs offer a rich opportunity for detecting events that may not be covered in traditional newswire text. These forms of self published media might also allow event detection systems to identify developing events before official news reports can be written. Several forms of event detection have focused on analyzing named entities, such as “Bill Clinton” or “Iraq,” and the contexts or documents in which they appear, e.g. [10, 11, 5]. We propose a more general approach that looks at all words and their contexts, rather than a predetermined set of words. Specifically, we argue that event detection can be done by measuring the semantic change in a word or phrase. To track changes in the semantics, we use a semantic space model of meaning, which is an automated method of building distributed representations of word meaning. Semantic space models of meaning offer three notable advantages for event detection. First, the models are capable of automatically determining the semantics of a word by examining the contexts in which t</context>
<context position="12146" citStr="[10]" startWordPosition="1963" endWordPosition="1963">to add temporal semantics. The first approach builds a separate semantic space for each specific time range. Semantics are then compared across spaces by defining some common context which occurs in both spaces. The second approach builds a single semantic space but provides the ability to segment it based on time. The key difference between these approaches lies in the meaning of each semantic dimension; when multiple spaces are used, there is no guarantee that the specific semantic meaning associated with some dimension i will be the same for dimension i in another space. Kontostathis et al.[10] and Fortuna et al.[5] have independently proposed two successful semantic space algorithms that use the first approach of processing several distinct corpora. Both approaches collect several corpora which span unique time ranges, and construct a semantic space for each corpus using LSA. Using LSA is a notable challenge as the space defined by LSA is based on the SVD of a word x document matrix; with documents being unique to each time-span’s corpus, direct comparison of vectors between spaces is not feasible. Kontostathis et al.[10] use data mining to overcome the change in dimension-meaning </context>
<context position="38371" citStr="[10, 5]" startWordPosition="6328" endWordPosition="6329">e limitations. Event detection systems that use semantic spaces have two notable challenges due to how time is integrated. First, the space must be easily modifiable as new documents are produced. Existing approaches use a single dimensionality reduction step after a corpus had been processed to improve information retrieval. However this step limits the integration of new documents into the semantic space; to integrate new documents, the space must be completely recomputed. The second challenge stems from comparing word meanings and documents that occur in different times. Approaches such as [10, 5] that arbitrarily segment the corpora used into different semantic spaces artificially limit both the types of comparisons available and the specific time ranges of the semantics. TRI addresses both of these challenges efficiently. By being based on Random Indexing, dimensionality reduction is done concurrently with developing semantic vectors. Additionally, by utilizing the same set of index vectors over all documents analyzed, every semantic slice is contained within the same semantic space, avoiding the need for reference only those vectors that are common to several time periods. Conversel</context>
</contexts>
<marker>[10]</marker>
<rawString>A. Kontostathis, I. De, L. E. Holzman, and W. M. Pottenger. Use of term clusters for emerging trend detection. Technical report, Lehigh University, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Kumaran</author>
<author>J Allan</author>
</authors>
<title>Text classification and named entities for new event detection.</title>
<date>2004</date>
<booktitle>In SIGIR ’04: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>297--304</pages>
<publisher>ACM,</publisher>
<contexts>
<context position="2138" citStr="[11]" startWordPosition="338" endWordPosition="338">unt of text being written daily, readers want to be informed of which developments and topics are the most recent and important without having to manually sift through all the documents written on the topic. In addition, a robust system should be able to detect multiple kinds of events, such as international conflicts, product releases or sports results. The main challenge in automating this task is detecting what makes a new document sufficiently novel to be described as a new event. Current event detection approaches have focused on identifying concrete events that occur within newswire text[11]. However, in recent years, blogs have become an important source of both news and commentary. Unlike news reports, blog content expresses a wide range of topics, opinions, vocabulary and writing styles; the change in editorial requirements allows blog authors to comment freely on local, national and international issues, while still expressing their personal sentiment. Accordingly, blogs offer a rich opportunity for detecting events that may not be covered in traditional newswire text. These forms of self published media might also allow event detection systems to identify developing events b</context>
<context position="32492" citStr="[1, 11]" startWordPosition="5378" endWordPosition="5379">. Most document based event detection algorithms extend a core usage of TF-IDF weighting. In the core event detection algorithm, each document is analyzed to produce TF-IDF values for each word occurring in the document. This set of values can then be compared against TF-IDF values of other documents using the same similarity measures used in semantic space models, with cosine similarity being one of the most common. In general, an event is detected if the current document is significantly different from all other processed documents, based on a threshold of similarity values between documents[1, 11]. Brants et al. introduced some significant improvements to the standard document based model[1]. The first improvement was to compute the TF-IDF values on a document by document basis, allowing the system to continuously process new documents. The second improvement was computing a set of TF-IDF values which were dependent on the source of the document, under the assumption that some words, such as CNN, would be more frequent based on who wrote the document. Beyond modifying the TF-IDF values, the similarity measure was also extended to include some normalization techniques that take into con</context>
<context position="34317" citStr="[11]" startWordPosition="5673" endWordPosition="5673">n a document, and another composed of all words that are not named entities. When comparing the similarity between two documents, the standard vector is initially used, and then the similarity between the additional vectors are used to provide finer distinctions, such as whether two documents refer to the same set of named entities, and the same set of general topics, i.e. all the non named entities. In [12], Lam et al. extend a document-space approach by associating each document with three vectors: a TFIDF weighted vector; a TF-IDF score of named entities present in the document, similar to [11]; and a concept vector, which details which abstract concepts are contained in the document, using TF-IDF scores based on the frequency of concepts rather than words. The key terms in a document are each given a weight based on which key terms the document contains. Event detection is done by clustering documents as they appear. Each cluster is said to represent a specific event; and documents that do not fit into one cluster are said to be new events. Chen et al. use a similar clustering for event detection but use sentences rather than entire document[2]. Makkonen et al. augment the document</context>
</contexts>
<marker>[11]</marker>
<rawString>G. Kumaran and J. Allan. Text classification and named entities for new event detection. In SIGIR ’04: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 297–304. ACM, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Lam</author>
<author>H Meng</author>
<author>K L Wong</author>
<author>J Yen</author>
</authors>
<title>Using contextual analysis for news event detection.</title>
<date>2001</date>
<journal>International Journal on Intelligent Systems,</journal>
<volume>16</volume>
<issue>44</issue>
<pages>546</pages>
<contexts>
<context position="34124" citStr="[12]" startWordPosition="5641" endWordPosition="5641">en documents, but also named entities, such as “President Obama,” in the documents[11]. Two additional vectors are created for each document: One composed of just the named entities occurring in a document, and another composed of all words that are not named entities. When comparing the similarity between two documents, the standard vector is initially used, and then the similarity between the additional vectors are used to provide finer distinctions, such as whether two documents refer to the same set of named entities, and the same set of general topics, i.e. all the non named entities. In [12], Lam et al. extend a document-space approach by associating each document with three vectors: a TFIDF weighted vector; a TF-IDF score of named entities present in the document, similar to [11]; and a concept vector, which details which abstract concepts are contained in the document, using TF-IDF scores based on the frequency of concepts rather than words. The key terms in a document are each given a weight based on which key terms the document contains. Event detection is done by clustering documents as they appear. Each cluster is said to represent a specific event; and documents that do no</context>
</contexts>
<marker>[12]</marker>
<rawString>W. Lam, H. Meng, K. L. Wong, and J. Yen. Using contextual analysis for news event detection. International Journal on Intelligent Systems, 16(44):525– 546, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T K Landauer</author>
<author>S T Dumais</author>
</authors>
<title>A solution to Plato’s problem: The Latent Semantic Analysis theory of the acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<contexts>
<context position="3889" citStr="[13]" startWordPosition="618" endWordPosition="618">mated method of building distributed representations of word meaning. Semantic space models of meaning offer three notable advantages for event detection. First, the models are capable of automatically determining the semantics of a word by examining the contexts in which the word appears. Such automated understanding of semantics is required for analyzing these new sources of data due to the much wider vocabulary used by authors. Second, the models offer a well defined method for comparing the semantics between words. These semantic comparisons have been shown to be similar to human judgments[13]. We argue that reporting words which have a notable changes in semantics should correlate well with a reader’s expectations of interesting developments. Third, the models are well-established at detecting association such as synonymy among words, which can allow models to detect events that are referred to by multiple names. Given these advantages, we introduce a new semantic space algorithm for assessing how the meaning of a word changes through time for the purpose of event detection. We illustrate our approach to topic detection with a hypothetical example of the product release of a toy n</context>
<context position="9344" citStr="[13]" startWordPosition="1491" endWordPosition="1491">ions on the number of unique words Fig. 1: Word semantics projected into two dimensions, illustrating the hypothetical change in meaning for “blick” based on its nearest neighbors is particularly problematic for blog corpora, as writers frequently introduce misspellings, slang, or topic-specific jargon. Accordingly, many approaches have focused on reducing the dimensionality of the semantic space. Dimensionality reduction often has the additional benefits such as making the resulting vector more general, or reducing computation time. Early successful approaches such as Latent Semantic Analysis[13] use the Singular Value Decomposition (SVD) to reduce the number of dimensions. While the SVD results in significant improvements in information retrieval, the fastest algorithms for the SVD are O(mn2) [7], which make them impractical for large corpora. Moreover, the SVD and other forms of principle component analysis must have the entire corpus present at once, which makes it difficult to update the space as new words and contexts are added. This is particularly problematic for event detection, as the corpus is expected to continuously grow as new events occur. Random Indexing[9, 18] offers a</context>
</contexts>
<marker>[13]</marker>
<rawString>T. K. Landauer and S. T. Dumais. A solution to Plato’s problem: The Latent Semantic Analysis theory of the acquisition, induction, and representation of knowledge. Psychological Review, 104:211–240, 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M L Littman</author>
<author>S T Dumais</author>
<author>T K Landauer</author>
</authors>
<title>Automatic cross-language information retrieval using latent semantic indexing.</title>
<date>1998</date>
<editor>In G. Grefenstette, editor,</editor>
<publisher>Kluwer,</publisher>
<contexts>
<context position="7037" citStr="[14, 23]" startWordPosition="1126" endWordPosition="1127">e company it keeps,”[4]. Creating semantics from co-occurring words forms the basis for how our algorithm represents changes in semantics. 2.1 Semantics as Co-occurrence In a semantic space, a word’s semantics are mapped to high dimensional vectors in a geometric space. The dimensions of the space represent distinctions between the meanings of words; accordingly, words with similar semantics have similar vector representations. Semantic space representations have proven effective at a variety of information retrieval tasks such as identifying synonymous queries[21] and multi-language retrieval[14, 23]. For a recent survey of applications of semantic spaces to information retrieval see Cohen and Widdows[3]. To illustrate the basics of cooccurrence based semantic space models, we can further explore the example of “blick”, the new yet toxic toy. Consider the documents describing “blick” when it is first introduced during a holiday season. A potential line from several blogs might read “A perfect gift this holiday season is blick, one of the newest toys available!” Using a simple co-occurrence semantic space, the semantics of “blick” would be a count of how frequently it co-occurs with key wo</context>
</contexts>
<marker>[14]</marker>
<rawString>M. L. Littman, S. T. Dumais, and T. K. Landauer. Automatic cross-language information retrieval using latent semantic indexing. In G. Grefenstette, editor, Cross language information retrieval. Kluwer, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Makkonen</author>
<author>H Ahonen-Myka</author>
<author>M Salmenkivi</author>
</authors>
<title>Simple Semantics in Topic Detection and Tracking. Information Retrieval,</title>
<date>2004</date>
<location>7:347–368,</location>
<contexts>
<context position="35048" citStr="[15]" startWordPosition="5798" endWordPosition="5798">requency of concepts rather than words. The key terms in a document are each given a weight based on which key terms the document contains. Event detection is done by clustering documents as they appear. Each cluster is said to represent a specific event; and documents that do not fit into one cluster are said to be new events. Chen et al. use a similar clustering for event detection but use sentences rather than entire document[2]. Makkonen et al. augment the document representation by using an existing ontology to extract out locations, proper names, and temporal references from the document[15]. These three, combined with the remaining terms in the document are used as the basis for comparison. Overall, the current event detection systems that do not utilize a semantic space have the key benefit of being able to process documents continuously, since no reduction step is required for vector representations. But the key difference is the focus on comparisons between documents, and not words that occur in documents. These approaches must handle different challenges, such as documents that discuss multiple events and elements of documents that are vague but important for distinguishing </context>
</contexts>
<marker>[15]</marker>
<rawString>J. Makkonen, H. Ahonen-Myka, and M. Salmenkivi. Simple Semantics in Topic Detection and Tracking. Information Retrieval, 7:347–368, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D L T Rohde</author>
<author>L M Gonnerman</author>
<author>D C Plaut</author>
</authors>
<title>An improved model of semantic similarity based on lexical co-occurrence. Cognitive Science,</title>
<date>2009</date>
<note>submitted.</note>
<contexts>
<context position="21270" citStr="[16]" startWordPosition="3505" endWordPosition="3505">this sample set of posts, the corpus meets our expectations in other ways. First, the lack of editorial oversight in the documents leads to grammatical and spelling errors, and frequently to the introduction of new terms or phrases unique to the author along with other issues2. Second, the corpus has a large number of discussed topics, ranging from international events, to product releases, and to personal musings. Before the corpus is used for performing event detection, the corpus is preprocessed to render it more uniform. Similar to other semantic space approaches that used webgathered data[16], this pre-processing allows the model to gracefully handle several irregularities in writing style, such as inconsistent use of punctuation and capitalization. Additionally, this process removes many tokens such as html mark-up, which have little or no semantic content in themselves3. The corpus is processed as follows: 1. Replace all numbers with &lt;num&gt; 2. Remove all html mark-up and email addresses 3. Remove unusual punctuation, and separate all other punctuation from words 4. Remove words of 20 characters in length 5. Converting all words to lower case 6. Replacing $5 to &lt;num&gt; dollars 7. Di</context>
</contexts>
<marker>[16]</marker>
<rawString>D. L. T. Rohde, L. M. Gonnerman, and D. C. Plaut. An improved model of semantic similarity based on lexical co-occurrence. Cognitive Science, 2009. submitted.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Sagi</author>
<author>S Kaufmann</author>
<author>B Clark</author>
</authors>
<title>Semantic density analysis: Comparing word meaning across time and phonetic space.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Geometrical Models ofNatural Language Semantics,</booktitle>
<pages>104--111</pages>
<location>Athens, Greece,</location>
<contexts>
<context position="13889" citStr="[17]" startWordPosition="2247" endWordPosition="2247">semantic spaces has been introduced by Fortuna et al.[5]. Their approach focused on finding key words that existed in multiple spaces, and defining a concrete set of semantics for these landmark words. As semantics from distinct spaces are created, they can be evaluated according to their relation to these landmark terms, and at any point in time, the words most closely associated to the landmark provide terms describing events related to the landmarks. Sagi et al. propose an alternate approach of uses a single corpus and includes temporal semantics after generating an initial set of semantics[17]. This generates semantic vectors for a corpus spanning many time ranges of interest and reducing dimensionality via SVD. Then, to develop temporal semantics for a term, documents from a specific time range are used to generate temporal vectors through a process very similar to Random Indexing; in this process the first set of semantic vectors generated are used in place of index vectors when using equation (1). While these approaches allow for accurate representations of semantic shifts, they face significant challenges when scaling to a large streaming set of documents, due to a reliance on </context>
</contexts>
<marker>[17]</marker>
<rawString>E. Sagi, S. Kaufmann, and B. Clark. Semantic density analysis: Comparing word meaning across time and phonetic space. In Proceedings of the Workshop on Geometrical Models ofNatural Language Semantics, pages 104–111, Athens, Greece, March 2009. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Sahlgren</author>
</authors>
<title>Vector-based semantic analysis: Representing word meanings based on random labels.</title>
<date>2001</date>
<booktitle>In Proceedings of the ESSLLI 2001 Workshop on Semantic Knowledge Acquisition and Categorisation,</booktitle>
<location>Helsinki, Finland,</location>
<contexts>
<context position="9935" citStr="[9, 18]" startWordPosition="1586" endWordPosition="1587">tic Analysis[13] use the Singular Value Decomposition (SVD) to reduce the number of dimensions. While the SVD results in significant improvements in information retrieval, the fastest algorithms for the SVD are O(mn2) [7], which make them impractical for large corpora. Moreover, the SVD and other forms of principle component analysis must have the entire corpus present at once, which makes it difficult to update the space as new words and contexts are added. This is particularly problematic for event detection, as the corpus is expected to continuously grow as new events occur. Random Indexing[9, 18] offers an alternative method for reducing the dimensionality of the semantic space by using a random projection of the full co-occurrence matrix onto a lower dimensional space. Random Indexing operates as follows. Each unique word is assigned an index vector, which is a random, sparse vector in a high dimensional space, often 2000-10000 dimensions. The size of the index vectors sets the number of dimensions used in the resulting semantic space. Index vectors are created such that any two arbitrary index vectors have a high probability of being orthogonal. This property is necessary to accurat</context>
</contexts>
<marker>[18]</marker>
<rawString>M. Sahlgren. Vector-based semantic analysis: Representing word meanings based on random labels. In Proceedings of the ESSLLI 2001 Workshop on Semantic Knowledge Acquisition and Categorisation, Helsinki, Finland, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Sahlgren</author>
<author>A Holst</author>
<author>P Kanerva</author>
</authors>
<title>Permutations as a means to encode order in word space.</title>
<date>2008</date>
<booktitle>In Proceedings of the 30th Annual Meeting of the Cognitive Science Society (CogSci’08),</booktitle>
<contexts>
<context position="22368" citStr="[16, 19]" startWordPosition="3679" endWordPosition="3680"> words of 20 characters in length 5. Converting all words to lower case 6. Replacing $5 to &lt;num&gt; dollars 7. Discard articles with fewer than some threshold percentage of correctly spelled English words 8. Associate each entry with a numeric timestamp When computing the semantics, we also impose two filters on corpus during processing: any word in a list of frequent closed-classed words and those words not in the most frequent 250,000 words in the blog corpus were removed. This step is both practical and empirically motivated. Removing closed-class is a common practice in semantic spaces models[16, 19], due to the low semantic value; words such as “the” or “of” so frequently appear that they do not serve to distinguish the meaning of any co-occurring word. Similarly, infrequent words can safely be removed for initial uses due to the small effect they would have on other semantic vectors. For both stop words and infrequent words, their original position is preserved after removal. This ensures that the window for counting co-occurrence takes into account 2 This may lead to an increase in polysemy and synonomy amongst words, potentially impacting our approach, but exploration ofthis topic is </context>
</contexts>
<marker>[19]</marker>
<rawString>M. Sahlgren, A. Holst, and P. Kanerva. Permutations as a means to encode order in word space. In Proceedings of the 30th Annual Meeting of the Cognitive Science Society (CogSci’08), 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Sahlgren</author>
<author>J Karlgren</author>
</authors>
<title>Buzz Monitoring in Word Space.</title>
<date>2008</date>
<booktitle>In Proceedings of European Conference on Intelligence and Security Informatics (EuroISI</booktitle>
<location>Esbjerg, Denmark,</location>
<contexts>
<context position="11513" citStr="[20]" startWordPosition="1857" endWordPosition="1857">us has been seen. More formally, let w be a focus word, wi be a cooccurring word with a word distance of i and index(wi) be the co-occurring word’s index vector. For the current word, we define a window of size n words before and after, which are counted as co-occurring. The semantics of w are then defined as: �semantics(w) = VIED −n&lt;i&lt;n where c is each occurrence of w in the corpus D. toy Blick new buy lawsuit toxic Blick’ index(wi) (1) 10 2.3 Adding Time to Semantic Space Models Augmenting a semantic space with time has been recognized as an effective method for tracking changes in semantics[20]. Two methods have been used to add temporal semantics. The first approach builds a separate semantic space for each specific time range. Semantics are then compared across spaces by defining some common context which occurs in both spaces. The second approach builds a single semantic space but provides the ability to segment it based on time. The key difference between these approaches lies in the meaning of each semantic dimension; when multiple spaces are used, there is no guarantee that the specific semantic meaning associated with some dimension i will be the same for dimension i in anoth</context>
</contexts>
<marker>[20]</marker>
<rawString>M. Sahlgren and J. Karlgren. Buzz Monitoring in Word Space. In Proceedings of European Conference on Intelligence and Security Informatics (EuroISI 2008), Esbjerg, Denmark, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Sch¨utze</author>
<author>J O Pedersen</author>
</authors>
<title>A cooccurrencebased thesaurus and two applications to information retrieval.</title>
<date>1997</date>
<booktitle>Information Processing and Management,</booktitle>
<volume>33</volume>
<issue>3</issue>
<contexts>
<context position="7000" citStr="[21]" startWordPosition="1123" endWordPosition="1123"> it, “you shall know a word by the company it keeps,”[4]. Creating semantics from co-occurring words forms the basis for how our algorithm represents changes in semantics. 2.1 Semantics as Co-occurrence In a semantic space, a word’s semantics are mapped to high dimensional vectors in a geometric space. The dimensions of the space represent distinctions between the meanings of words; accordingly, words with similar semantics have similar vector representations. Semantic space representations have proven effective at a variety of information retrieval tasks such as identifying synonymous queries[21] and multi-language retrieval[14, 23]. For a recent survey of applications of semantic spaces to information retrieval see Cohen and Widdows[3]. To illustrate the basics of cooccurrence based semantic space models, we can further explore the example of “blick”, the new yet toxic toy. Consider the documents describing “blick” when it is first introduced during a holiday season. A potential line from several blogs might read “A perfect gift this holiday season is blick, one of the newest toys available!” Using a simple co-occurrence semantic space, the semantics of “blick” would be a count of ho</context>
</contexts>
<marker>[21]</marker>
<rawString>H. Sch¨utze and J. O. Pedersen. A cooccurrencebased thesaurus and two applications to information retrieval. Information Processing and Management, 33(3):307–318, 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K C Sia</author>
<author>J Cho</author>
<author>Y Chi</author>
<author>B L Tseng</author>
</authors>
<title>Efficient computation of personal aggregate queries on blogs.</title>
<date>2008</date>
<booktitle>In Proceedings of the ACMSIGKDD International Conference on Knowledge Discovery and Data Mining (KDD),</booktitle>
<contexts>
<context position="19364" citStr="[22]" startWordPosition="3189" endWordPosition="3189">larger set of words. Due to limited space, we illustrate the performance using a set of six word of divergent topics that includes both abstract and specific concepts: college, Lebanon, nuclear, Wii, PS3, and XP. 4.1 The Corpus Our approach can be applied to any corpus that has a known date of authorship of each article, at the granularity desired for analysis of semantic shifts. For the purpose of detecting changes in public opinion over the course of recent events, and the detection of previously unknown, but still interesting, events, we have utilized a portion of an already existing corpus[22]. The corpus comes from a collection of blog postings from 2004 on. These blog postings come from around the world, and in a variety of languages. We view this as an excellent example of an unstructured corpus for event detection since it is composed of blog articles harvested by BlogLines1. The documents come from some standard 1 http://www.bloglines.com news sources, but also from any blogging service which provides rss feeds, such as livejournal, local newspapers, wordpress, and many more. For this experiment, we collected only English articles from the blog corpus, but the algorithm could </context>
</contexts>
<marker>[22]</marker>
<rawString>K. C. Sia, J. Cho, Y. Chi, and B. L. Tseng. Efficient computation of personal aggregate queries on blogs. In Proceedings of the ACMSIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), August 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Vinokourov</author>
<author>J Shawe-Taylor</author>
<author>N Cristianini</author>
</authors>
<title>Finding Language-Independent Semantic Representation of Text using Kernel Canonical Correlation Analysis.</title>
<date>2002</date>
<tech>Technical report, Neurocolt,</tech>
<contexts>
<context position="7037" citStr="[14, 23]" startWordPosition="1126" endWordPosition="1127">e company it keeps,”[4]. Creating semantics from co-occurring words forms the basis for how our algorithm represents changes in semantics. 2.1 Semantics as Co-occurrence In a semantic space, a word’s semantics are mapped to high dimensional vectors in a geometric space. The dimensions of the space represent distinctions between the meanings of words; accordingly, words with similar semantics have similar vector representations. Semantic space representations have proven effective at a variety of information retrieval tasks such as identifying synonymous queries[21] and multi-language retrieval[14, 23]. For a recent survey of applications of semantic spaces to information retrieval see Cohen and Widdows[3]. To illustrate the basics of cooccurrence based semantic space models, we can further explore the example of “blick”, the new yet toxic toy. Consider the documents describing “blick” when it is first introduced during a holiday season. A potential line from several blogs might read “A perfect gift this holiday season is blick, one of the newest toys available!” Using a simple co-occurrence semantic space, the semantics of “blick” would be a count of how frequently it co-occurs with key wo</context>
</contexts>
<marker>[23]</marker>
<rawString>A. Vinokourov, J. Shawe-Taylor, and N. Cristianini. Finding Language-Independent Semantic Representation of Text using Kernel Canonical Correlation Analysis. Technical report, Neurocolt, 2002. NeuroCOLT Technical Report NC-TR-02-119.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>