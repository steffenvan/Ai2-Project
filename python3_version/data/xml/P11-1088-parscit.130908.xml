<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001522">
<title confidence="0.9991525">
Using Deep Morphology to Improve Automatic Error Detection
in Arabic Handwriting Recognition
</title>
<author confidence="0.996638">
Nizar Habash and Ryan M. Roth
</author>
<affiliation confidence="0.997798">
Center for Computational Learning Systems
Columbia University
</affiliation>
<email confidence="0.997628">
{habash,ryanr}@ccls.columbia.edu
</email>
<sectionHeader confidence="0.997383" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999763">
Arabic handwriting recognition (HR) is a
challenging problem due to Arabic’s con-
nected letter forms, consonantal diacritics and
rich morphology. In this paper we isolate
the task of identification of erroneous words
in HR from the task of producing corrections
for these words. We consider a variety of
linguistic (morphological and syntactic) and
non-linguistic features to automatically iden-
tify these errors. Our best approach achieves
a roughly —15% absolute increase in F-score
over a simple but reasonable baseline. A de-
tailed error analysis shows that linguistic fea-
tures, such as lemma (i.e., citation form) mod-
els, help improve HR-error detection precisely
where we expect them to: semantically inco-
herent error words.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999822714285715">
After years of development, optical character recog-
nition (OCR) for Latin-character languages, such as
English, has been refined greatly. Arabic, however,
possesses a complex orthography and morphology
that makes OCR more difficult (Märgner and Abed,
2009; Halima and Alimi, 2009; Magdy and Dar-
wish, 2006). Because of this, only a few systems
for Arabic OCR of printed text have been devel-
oped, and these have not been thoroughly evalu-
ated (Märgner and Abed, 2009). OCR of Arabic
handwritten text (handwriting recognition, or HR),
whether online or offline, is even more challenging
compared to printed Arabic OCR, where the unifor-
mity of letter shapes and other factors allow for eas-
ier recognition (Biadsy et al., 2006; Natarajan et al.,
2008; Saleem et al., 2009).
OCR and HR systems are often improved by per-
forming post-processing; these are attempts to eval-
uate whether each word, phrase or sentence in the
OCR/HR output is legal and/or probable. When an
illegal word or phrase is discovered (error detec-
tion), these systems usually attempt to generate a le-
gal alternative (error correction). In this paper, we
present a HR error detection system that uses deep
lexical and morphological feature models to locate
possible &amp;quot;problem zones&amp;quot; – words or phrases that
are likely incorrect – in Arabic HR output. We use
an off-the-shelf HR system (Natarajan et al., 2008;
Saleem et al., 2009) to generate an N-best list of hy-
potheses for each of several scanned segments of
Arabic handwriting. Our problem zone detection
(PZD) system then tags the potentially erroneous
(problem) words. A subsequent HR post-processing
system can then focus its effort on these words when
generating additional alternative hypotheses. We
only discuss the PZD system and not the task of
new hypothesis generation; the evaluation is on er-
ror/problem identification. PZD can also be useful in
highlighting erroneous text for human post-editors.
This paper is structured as follows: Section 2 pro-
vides background on the difficulties of the Arabic
HR task. Section 3 presents an analysis of HR er-
rors and defines what is considered a problem zone
to be tagged. The experimental features, data and
other variables are outlined in Section 4. The exper-
iments are presented and discussed in Section 5. We
discuss and compare to some related work in detail
in Section 6. Conclusions and suggested avenues of
for future progress are presented in Section 7.
</bodyText>
<sectionHeader confidence="0.992821" genericHeader="method">
2 Arabic Handwriting Recognition
Challenges
</sectionHeader>
<bodyText confidence="0.9350855">
Arabic has several orthographic and morphological
properties that make HR challenging (Darwish and
Oard, 2002; Magdy and Darwish, 2006; Märgner
and Abed, 2009).
</bodyText>
<page confidence="0.980832">
875
</page>
<note confidence="0.9857605">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 875–884,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<subsectionHeader confidence="0.957825">
2.1 Arabic Orthography Challenges
</subsectionHeader>
<bodyText confidence="0.999970913043478">
The use of cursive, connected script creates prob-
lems in that it becomes more difficult for a machine
to distinguish between individual characters. This is
certainly not a property unique to Arabic; methods
developed for other cursive script languages (such
as Hidden Markov Models) can be applied success-
fully to Arabic (Natarajan et al., 2008; Saleem et al.,
2009; Märgner and Abed, 2009; Lu et al., 1999).
Arabic writers often make use of elongation
(tatweel/kashida) to beautify the script. Arabic also
contains certain ligature constructions that require
consideration during OCR/HR (Darwish and Oard,
2002). Sets of dots and optional diacritic markers
are used to create character distinctions in Arabic.
However, trace amounts of dust or dirt on the origi-
nal document scan can be easily mistaken for these
markers (Darwish and Oard, 2002). Alternatively,
these markers in handwritten text may be too small,
light or closely-spaced to readily distinguish, caus-
ing the system to drop them entirely. While Arabic
disconnective letters may make it hard to determine
word boundaries, they could plausibly contribute to
reduced ambiguity of otherwise similar shapes.
</bodyText>
<subsectionHeader confidence="0.996483">
2.2 Arabic Morphology Challenges
</subsectionHeader>
<bodyText confidence="0.999431235294117">
Arabic words can be described in terms of their
morphemes. In addition to concatenative prefixes
and suffixes, Arabic has templatic morphemes called
roots and patterns. For example, the word ÑîD.�KA3/4Ò»ð
wkmkAtbhm1 (w+k+mkAtb+hm) ‘and like their of-
fices’ has two prefixes and one suffix, in addition
to a stem composed of the root H. u1/4 k-t-b ‘writ-
ing related’ and the pattern m1A23.2 Arabic words
can also be described in terms of lexemes and inflec-
tional features. The set of word forms that only vary
inflectionally among each other is called the lexeme.
A lemma is a particular word form used to represent
the lexeme word set – a citation form that stands
in for the class (Habash, 2010). For instance, the
lemma I. �JºÓ mktb ‘office’ represents the class of
all forms sharing the core meaning ‘office’: I. �KA3/4Ó
mkAtb ‘offices’ (irregular plural), I. �JºÖÏ@ Almktb ‘the
</bodyText>
<footnote confidence="0.99735175">
1All Arabic transliterations are presented in the HSB
transliteration scheme (Habash et al., 2007).
2The digits in the pattern correspond to positions where root
radicals are inserted.
</footnote>
<bodyText confidence="0.971808384615384">
office’, AîD.�JºÖÏ lmktbhA ‘for her office’, and so on.
Just as the lemma abstracts over inflectional mor-
phology, the root abstracts over both inflectional
and derivational morphology and thus provides a
very high level of lexical abstraction, indicating the
“core” meaning of the word. The Arabic root H. u1/4
k-t-b ‘writing related’, e.g., relates words likeI.�JºÓ
mktb ‘office’, H. ñ�JºÓ mktwb ‘letter’, and kJ.~
-.rktybh
‘military unit (of conscripts)’.
Arabic morphology allows for tens of billions of
potential, legal words (Magdy and Darwish, 2006;
Moftah et al., 2009). The large potential vocabulary
size by itself complicates HR methods that rely on
conventional, word-based dictionary lookup strate-
gies. In this paper we consider the value of morpho-
lexical and morpho-syntactic features such as lem-
mas and part-of-speech tags, respectively, that may
allow machine learning algorithms to learn general-
izations. We do not consider the root since it has
been shown to be too general for NLP purposes
(Larkey et al., 2007). Other researchers have used
stems for OCR correction (Magdy and Darwish,
2006); we discuss their work and compare to it in
Section 6, but we do not present a direct experimen-
tal comparison.
</bodyText>
<sectionHeader confidence="0.9985465" genericHeader="method">
3 Problem Zones in Handwriting
Recognition
</sectionHeader>
<subsectionHeader confidence="0.999613">
3.1 HR Error Classifications
</subsectionHeader>
<bodyText confidence="0.9999615">
We can classify three types of HR errors: substi-
tutions, insertions and deletions. Substitutions in-
volve replacing the correct word by another incor-
rect form. Insertions are words that are incorrectly
added into the HR hypothesis. An insertion error
is typically paired with a substitution error, where
the two errors reflect a mis-identification of a single
word as two words. Deletions are simply missing
words. Examples of these different types of errors
appear in Table 1. In the dev set that we study here
(see Section 4.1), 25.8% of the words are marked as
problematic. Of these, 87.2% are letter-based words
(henceforth words), as opposed to 9.3% punctuation
and 3.5% digits.
Orthogonally, 81.4% of all problem words are
substitution errors, 10.6% are insertion errors and
7.9% are deletion errors. Whereas punctuation sym-
bols are 9.3% of all errors, they represent over 38%
</bodyText>
<page confidence="0.998172">
876
</page>
<table confidence="0.999114125">
REF ! oXñm.Ì @ ZJ�ËA« èYK éJ�.Ê« @ñª�J’i @ �à�B@ð €PA�¯ð •ËY�K�A�¯AK� ú �jñÒÊ‚ÖÏ@ AîE��@ �
! Aljwdh ςAlyh �. P� ςlbh tSnςwA Ân wAlÂndls wfArs �éJ���J�¢�J¢‚�®Ë@ m~~ Almslmwn ÂyhA Õç� �Qj. «@
zbdh AlqsTnTynyh yAfAtHy Âςjztm
HYP èXñm.Ì @ �éJ�ËA« èñK� à éJ�Ê« @ñ�¯Qå”�� à@ �áÖÏ �� €PA�¯ð �éJ���J�¢�J¢‚�®Ë@ ú�GA�KAK. �àñÒÊ‚ÖÏ@ AîE�@ Õç~ Q~
Aljwdh ςAlyh ywh n ςlyh tSrfwA An lmn Y �gB@ð wfArs AlqsTnTynyh bAtAný Almslmwn AyhA θm «@
wAlAxð Âςyr
PZD PROB OK PROB PROB PROB PROB PROB PROB PROB OK OK PROB OK PROB PROB PROB
DELX INS SUB DOTS SUB ORTH INS SUB SUB ORTH INS SUB
</table>
<tableCaption confidence="0.77244975">
Table 1: An example highlighting the different types of Arabic HR errors. The first row shows the reference sentence
(right-to-left). The second row shows an automatically generated hypothesis of the same sentence. The last row shows
which words in the hypothesis are marked as problematic (PROB) by the system and the specific category of the
problem (illustrative, not used by system): SUB (substituted), ORTH (substituted by an orthographic variant), DOTS
(substituted by a word with different dotting), INS (inserted), and DELX (adjacent to a deleted word). The remaining
words are tagged as OK. The reference translates as ‘Are you unable O’Moslems, you who conquered Constantinople
and Persia and Andalusia, to manufacture a tub of high quality butter!’. The hypothesis roughly translates as ‘I loan
then O’Moslems Pattani Constantinople, and Persia and taking from whom that you spend on him N Yeoh high quality’.
</tableCaption>
<bodyText confidence="0.948065358974359">
of all deletion errors, almost 22% of all insertion er-
rors and less than 5% of substitution errors. Simi-
larly digits, which are 3.5% of all errors, are almost
14% of deletions, 7% of insertions and just over 2%
of all substitutions. Punctuation and digits bring dif-
ferent challenges: whereas punctuation marks are a
small class, their shape is often confusable with Ara-
bic letters or letter components, e.g., �@ Aˇ and ! or
P r and ,. Digits on the other hand are a hard class
to language model since the vocabulary (of multi-
digit numbers) is infinite. Potentially this can be
addressed using a pattern-based model that captures
forms of digit sequences (such as date and currency
formats); we leave this as future work.
Words (non-digit, non-punctuation) still consti-
tute the majority in every category of error: 47.7%
of deletions, 71.3% of insertions and over 93%
of substitutions. Among substitutions, 26.5% are
simple orthographic variants that are often normal-
ized in Arabic NLP because they result from fre-
quent inconsistencies in spelling: Alef Hamza forms
�
@�/@ A/Â/ˇA/¯A) and Ya/Alef-Maqsura (ø
/ø y/ý). If
we consider whether the lemma of the correct word
and its incorrect form are matchable, an additional
6.9% can be added to the orthographic variant sum
(since all of these cases can share the same lemmas).
The rest of the cases, or 59.7% of the words, in-
volve complex orthographic errors. Simple dot mis-
placement can only account for 2.4% of all substi-
tution errors. The HR system output does not con-
tain any illegal non-words since its vocabulary is re-
stricted by its training data and language models.
The large proportion of errors involving lemma dif-
ferences is consistent with the perception that most
OCR/HR errors create semantically incoherent sen-
tences. This suggests that lemma models can be
helpful in identifying such errors.
</bodyText>
<subsectionHeader confidence="0.999275">
3.2 Problem Zone Definition
</subsectionHeader>
<bodyText confidence="0.999997176470589">
Prior to developing a model for PZD, it is necessary
to define what is considered a ‘problem’. Once a
definition is chosen, gold problem tags can be gen-
erated for the training and test data by comparing the
hypotheses to their references.3 We decided in this
paper to use a simple binary problem tag: a hypothe-
sis word is tagged as &amp;quot;PROB&amp;quot; if it is the result of an
insertion or substitution of a word. Deleted words
in a hypothesis, which cannot be tagged themselves,
cause their adjacent words to be marked as PROB in-
stead. In this way, a subsequent HR post-processing
system can be alerted to the possibility of a miss-
ing word via its surroundings (hence the idea of a
problem ‘zone’). Any words not marked as PROB
are given an &amp;quot;OK&amp;quot; tag (see the PZD row of Table 1).
We describe in Section 5.6 some preliminary exper-
iments we conducted using more fine-grained tags.
</bodyText>
<sectionHeader confidence="0.999446" genericHeader="method">
4 Experimental Settings
</sectionHeader>
<subsectionHeader confidence="0.999554">
4.1 Training and Evaluation Data
</subsectionHeader>
<bodyText confidence="0.999953">
The data used in this paper is derived from im-
age scans provided by the Linguistic Data Consor-
tium (LDC) (Strassel, 2009). This data consists of
high-resolution (600 dpi) handwriting scans of Ara-
bic text taken from newswire articles, web logs and
</bodyText>
<footnote confidence="0.8446365">
3For clarity, we refer to these tags as ‘gold’, whereas the cor-
rect segment for a given hypothesis set is called the ‘reference’.
</footnote>
<equation confidence="0.8272925">
�
(@/ @/
</equation>
<page confidence="0.976455">
877
</page>
<bodyText confidence="0.999957740740741">
newsgroups, along with ground truth annotations
and word bounding box information. The scans in-
clude variations in scribe demographic background,
writing instrument, paper and writing speed.
The BBN Byblos HR system (Natarajan et al.,
2008; Saleem et al., 2009) is then used to pro-
cess these scanned images into sequences of seg-
ments (sentence fragments). The system generates
a ranked N-best list of hypotheses for each segment,
where N could be as high as 300. On average, a seg-
ment has 6.87 words (including punctuation).
We divide the N-best list data into training, de-
velopment (dev) and test sets.4 For training, we
consider two sets of size 2000 and 4000 segments
(5) with the 10 top-ranked hypotheses (H) for each
segment to provide additional variations.5 The ref-
erences are also included in the training sets to pro-
vide examples of perfect text. The dev and test sets
use 500 segments with one top-ranked hypothesis
each {H=1}. We can construct a trivial PZD base-
line by assuming all the input words are PROBs;
this results in baseline % Precision/Recall/F-scores
of 25.8/100/41.1 and 26.0/100/41.2 for the dev and
test sets, respectively. Note that in this paper we
eschew these baselines in favor of comparison to
a non-trivial baseline generated by a simple PZD
model.
</bodyText>
<subsectionHeader confidence="0.993189">
4.2 PZD Models and Features
</subsectionHeader>
<bodyText confidence="0.985163135135135">
The PZD system relies on a set of SVM classi-
fiers trained using morphological and lexical fea-
tures. The SVM classifiers are built using Yamcha
(Kudo and Matsumoto, 2003). The SVMs use a
quadratic polynomial kernel. For the models pre-
sented in this paper, the static feature window con-
text size is set to +/- 2 words; the previous two (dy-
namic) classifications (i.e. targets) are also used as
features. Experiments with smaller window sizes re-
sult in poorer performance, while a larger window
size (+/- 6 words) yields roughly the same perfor-
mance at the expense of an order-of-magnitude in-
crease in required training time. Over 30 different
4Naturally, we do not use data that the BBN Byblos HR sys-
tem was trained on.
5We conducted additional experiments where we varied the
number of segments and hypotheses and found that the system
benefited from added variety of segments more than hypotheses.
We also modified training composition in terms of the ratio of
problem/non-problem words; this did not help performance.
Simple Description
word The surface word form
nw Normalized word: the word after Alef,
Ya and digit normalization
pos The part-of-speech (POS) of the word
lem The lemma of the word
na No-analysis: a binary feature indicat-
ing whether the morphological analyzer
produced any analyses for the word
Binned Description
nw N-grams Normword 1/2/3-gram probabilities
lem N-grams Lemma 1/2/3-gram probabilities
pos N-grams POS 1/2/3-gram probabilities
conf Word confidence: the ratio of the num-
ber of hypotheses in the N-best list that
contain the word over the total number
of hypotheses
</bodyText>
<tableCaption confidence="0.9841705">
Table 2: PZD model features. Simple features are used
directly by the PZD SVM models, whereas Binned fea-
tures’ (numerical) values are reduced to a small, labeled
category set whose labels are used as model features.
</tableCaption>
<bodyText confidence="0.999567222222222">
combinations of features were considered. Table 2
shows the individual feature definitions.
In order to obtain the morphological features,
all of the training and test data is passed through
MADA 3.0, a software tool for Arabic morpholog-
ical analysis disambiguation (Habash and Rambow,
2005; Roth et al., 2008; Habash et al., 2010). For
these experiments, MADA provides the pos (using
MADA’s native 34-tag set) and the lemma for each
word. Occasionally MADA will not be able to pro-
duce any interpretations (analyses) for a word; since
this is often a sign that the word is misspelled or un-
common, we define a binary na feature to indicate
when MADA fails to generate analyses.
In addition to using the MADA features directly,
we also develop a set of nine N-gram models (where
N=1, 2, and 3) for the nw, pos, and lem features de-
fined in Table 2. We train these models using 220M
words from the Arabic Gigaword 3 corpus (Graff,
2007) which had also been run through MADA 3.0
to extract the pos and lem information. The models
are built using the SRI Language Modeling Toolkit
(Stolcke, 2002). Each word in a hypothesis can then
be assigned a probability by each of these nine mod-
els. We reduce these probabilities into one of nine
bins, with each successive bin representing an order
of magnitude drop in probability (the final bin is re-
</bodyText>
<page confidence="0.99144">
878
</page>
<bodyText confidence="0.99994975">
served for word N-grams which did not appear in
the models). The bin labels are used as the SVM
features.
Finally, we also use a word confidence (conf)
feature, which is aimed at measuring the frequency
with which a given word is chosen by the HR system
for a given segment scan. The conf is defined here
as the ratio of the number of hypotheses in the N-
best list that the word appears in to the total number
of hypotheses. These numbers are calculated using
the original N-best hypothesis list, before the data is
trimmed to H={1, 10}. Like the N-grams, this num-
ber is binned; in this case there are 11 bins, with 10
spread evenly over the [0,1) range, and an extra bin
for values of exactly 1 (i.e., when the word appears
in every hypothesis in the set).
</bodyText>
<sectionHeader confidence="0.99998" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.9999658">
We describe next different experiments conducted
by varying the features used in the PZD model. We
present the results in terms of F-score only for sim-
plicity; we then conduct an error analysis that exam-
ines precision and recall.
</bodyText>
<subsectionHeader confidence="0.998818">
5.1 Effect of Feature Set Choice
</subsectionHeader>
<bodyText confidence="0.99913175">
Selecting an appropriate set of features for PZD re-
quires extensive testing. Even when only consider-
ing the few features described in Table 2, the param-
eter space is quite large. Rather then exhaustively
test every possible feature combination, we selec-
tively choose feature subsets that can be compared
to gain a sense of the incremental benefit provided
by individual features.
</bodyText>
<subsubsectionHeader confidence="0.837037">
5.1.1 Simple Features
</subsubsectionHeader>
<bodyText confidence="0.999514">
Table 3 illustrates the result of taking a baseline fea-
ture set (containing word as the only feature) and
adding a single feature from the Simple set to it. The
result of combining all the Simple features is also in-
dicated. From this, we see that Simple features, even
collectively, provide only minor improvements.
</bodyText>
<subsubsectionHeader confidence="0.940735">
5.1.2 Binned Features
</subsubsectionHeader>
<bodyText confidence="0.9994602">
Table 4 shows models which include both Simple
and Binned features. First, Table 4 shows the effect
of adding nw N-grams of successively higher orders
to the word baseline. Here we see that even a sim-
ple unigram provides a significant benefit (compared
</bodyText>
<table confidence="0.999886714285714">
Feature Set F-score %Imp
word 43.85 –
word+nw 43.86 -0
word+na 44.78 2.1
word+lem 45.85 4.6
word+pos 45.91 4.7
word+nw+pos+lem+na 46.34 5.7
</table>
<tableCaption confidence="0.9737756">
Table 3: PZD F-scores for simple feature combinations.
The training set used was {S=2000, H=10} and the mod-
els were evaluated on the dev set. The improvement over
the word baseline case is also indicated. %Imp is the rel-
ative improvement over the first row.
</tableCaption>
<table confidence="0.999988307692308">
Feature Set F-score %Imp
word 43.85 –
word+nw 1-gram 49.51 12.9
word+nw 1-gram+nw 2-gram 59.26 35.2
word+nw N-grams 59.33 35.3
+pos 58.50 33.4
+pos N-grams 57.35 30.8
+lem+lem N-grams 59.63 36.0
+lem+lem N-grams+na 59.93 36.7
+lem+lem N-grams+na+nw 59.77 36.3
+lem 60.92 38.9
+lem+na 60.47 37.9
+lem+lem N-grams 60.44 37.9
</table>
<tableCaption confidence="0.782275875">
Table 4: PZD F-scores for models that include Binned
features. The training set used was {S=2000, H=10} and
the models were evaluated on the dev set. The improve-
ment over the word baseline case is also indicated. The
label &amp;quot;N-grams&amp;quot; following a Binned feature refers to us-
ing 1, 2 and 3-grams of that feature. Indentation marks
accumulative features in model. The best performing row
(with bolded score) is word+nw N-grams+lem.
</tableCaption>
<bodyText confidence="0.999972285714286">
to the improvements gained in Table 3). The largest
improvement comes with the addition of the bigram
(thus introducing context into the model), but the tri-
gram provides only a slight improvement above that.
This implies that pursuing higher order N-grams will
result in negligible returns.
In the next part of Table 4, we see that the sin-
gle feature (pos) which provided the highest single-
feature benefit in Table 3 does not provide simi-
lar improvements under these combinations, and in
fact seems detrimental. We also note that using
all the features in one model is outperformed by
more selective choices. Here, the best performer is
the model which utilizes the word, nw N-grams,
</bodyText>
<page confidence="0.998226">
879
</page>
<table confidence="0.999330111111111">
Base Feature Set F-score %Imp
+conf
word 43.85 55.83 27.3
+nw N-grams 59.33 61.71 4.0
+lem 60.92 62.60 2.8
+lem+na 60.47 63.14 4.4
+lem+lem N-grams 60.44 62.88 4.0
+pos+pos N-grams 59.77 62.44 4.5
+na+nw (all system)
</table>
<tableCaption confidence="0.952976">
Table 5: PZD F-scores for models when word confi-
</tableCaption>
<figureCaption confidence="0.983750625">
dence is added to the feature set. The training set used
was {S=2000, H=10} and the models were evaluated on
the dev set. The improvement generated by including
word confidence is indicated. The label &amp;quot;N-grams&amp;quot; fol-
lowing a Binned feature refers to using 1, 2 and 3-grams
of that feature. Indentation marks accumulative features
in model. %Imp is the relative improvement gained by
adding the conf feature.
</figureCaption>
<bodyText confidence="0.999924333333333">
and lem as the only features. However, the dif-
ferences among this model and the other models
using lem Table 4 are not statistically significant.
The differences between this model and the other
lower performing models are statistically significant
(p&lt;0.05).
</bodyText>
<subsubsectionHeader confidence="0.670843">
5.1.3 Word Confidence
</subsubsectionHeader>
<bodyText confidence="0.9999611875">
The conf feature deserves special consideration be-
cause it is the only feature which draws on informa-
tion from across the entire hypothesis set. In Table 5,
we show the effect of adding conf as a feature to
several base feature sets taken from Table 4. Except
for the baseline case, conf provides a relatively
consistent benefit. The large (27.3%) improvement
gained by adding conf to the word baseline shows
that conf is a valuable feature, but the smaller im-
provements in the other models indicate that the in-
formation it provides largely overlaps with the in-
formation already present in those models. The dif-
ferences among the last four models (all including
lem) in Table 5 are not statistically significant. The
differences between these four models and the first
two are statistically significant (p&lt;0.05).
</bodyText>
<subsectionHeader confidence="0.999559">
5.2 Effect of Training Data Size
</subsectionHeader>
<bodyText confidence="0.99949625">
In order to allow for rapid examination of multi-
ple feature combinations, we restricted the size of
the training set (S) to maintain manageable train-
ing times. With this decision comes the implicit as-
</bodyText>
<table confidence="0.997221">
Feature Set S = 2000 S = 4000 %Imp
F-score F-score
word 43.85 52.08 18.8
word+conf 55.83 57.50 3.0
word+nw N-grams+lem 62.60 66.34 6.0
+conf (best system)
+na 63.14 66.21 4.9
+lem N-grams 62.88 64.43 2.5
all 62.44 65.62 5.1
</table>
<tableCaption confidence="0.977284714285714">
Table 6: PZD F-scores for selected models when the
number of training segments (S) is doubled. The training
set used was {S=2000, H=10} and {S=4000, H=10},
and the models were evaluated on the dev set. The label
&amp;quot;N-grams&amp;quot; following a Binned feature refers to using 1, 2
and 3-grams of that feature. Indentation marks accumu-
lative features in model.
</tableCaption>
<bodyText confidence="0.9999965">
sumption that the results obtained will scale with ad-
ditional training data. We test this assumption by
taking the best-performing feature sets from Table 5
and training new models using twice the training
data {S=4000}. The results are shown in Table 6.
In each case, the improvements are relatively con-
sistent (and on the order of the gains provided by the
inclusion of conf as seen in Table 5), indicating that
the model performance does scale with data size.
However, these improvements come with a cost of
a roughly 4-7x increase in training time. We note
that the value of doubling S is roughly 3-6x times
greater for the word baseline than the others; how-
ever, simply adding conf to the baseline provides
an even greater improvement than doubling S. The
differences between the final four models in Table 6
are not statistically significant. The differences be-
tween these models and the first two models in the
table are statistically significant (p&lt;0.05). For con-
venience, in the next section we refer to the third
model listed in Table 6 as the best system (because
it has the highest absolute F-score on the large data
set), but readers should recall that these four models
are roughly equivalent in performance.
</bodyText>
<subsectionHeader confidence="0.945724">
5.3 Error Analysis
</subsectionHeader>
<bodyText confidence="0.9999774">
In this section, we look closely at the performance
of a subset of systems on different types of prob-
lem words. We compare the following model set-
tings: for {S=4000} training, we use word, word
+ conf, the best system from Table 6 and the model
</bodyText>
<page confidence="0.995116">
880
</page>
<table confidence="0.994206125">
(a) S=4000 S=2000
word wconf best all all
Precision 54.7 59.5 67.1 67.4 62.4
Recall 49.7 55.7 65.6 64.0 62.5
F-score 52.1 57.5 66.3 65.6 62.4
Accuracy 76.4 78.7 82.8 82.7 80.6
(b) %Prob word wconf best all all
Words 87.2 51.8 57.3 68.5 67.1 64.9
Punc. 9.3 39.5 44.7 50.0 46.1 40.8
Digits 3.5 24.1 44.8 34.5 34.5 62.1
INS 10.6 46.0 49.4 62.1 62.1 55.2
DEL 7.9 29.2 20.0 24.6 21.5 27.7
SUB 81.4 52.2 60.0 70.0 68.4 66.9
Ortho 21.6 63.3 51.4 52.5 53.7 48.6
Lemma 5.6 45.7 52.2 63.0 52.2 54.4
Semantic 54.2 48.4 64.2 77.7 75.9 75.5
</table>
<tableCaption confidence="0.736451">
Table 7: Error analysis results comparing the perfor-
mance of multiple systems over different metrics (a) and
word/error types (b). %Prob shows the distribution of
</tableCaption>
<bodyText confidence="0.997529682539683">
problem words into different word types (word, punctua-
tion and digit) and error types. INS, DEL and SUB stand
for insertion, deletion and substitution error types, re-
spectively. Ortho stands for orthographic variant. Lemma
stands for ‘shared lemma’. The columns to the right
of the %Prob column show recall percentage for each
word/error type.
using all possible features (word, wconf, best and
all, respectively); and we also use all trained with
{S=2000}. We consider the performance in terms
of precision and recall in addition to F-score – see
Table 7 (a). We also consider the percentage of re-
call per error type, such as word/punctuation/digit or
deletion/insertion/substitution and different types of
substitution errors – see Table 7 (b). The second col-
umn in this table (%Prob) shows the distribution of
gold-tagged problem words into word and error type
categories.
Overall, there is no major tradeoff between preci-
sion and recall across the different settings; although
we can observe the following: (i) adding more train-
ing data helps precision more than recall (over three
times more) – compare the last two columns in Ta-
ble 7 (a); and (ii) the best setting has a slightly lower
precision than all features, although a much better
recall – compare columns 4 and 5 in Table 7 (a).
The performance of different settings on words is
generally better than punctuation and that is better
than digits. The only exceptions are in the digit cat-
egory, which may be explained by that category’s
small count which makes it prone to large percent-
age fluctuations.
In terms of error type, the performance on sub-
stitutions is better than insertions, which is in turn
better than deletions, for all systems compared. This
makes sense since deletions are rather hard to de-
tect and they are marked on possibly correct adja-
cent words, which may confuse the classifiers. One
insight for future work is to develop systems for
different types of errors. Considering substitutions
in more detail, we see that surprisingly, the simple
approach of using the word feature only (without
conf) correctly recalls a bigger proportion of prob-
lems involving orthographic variants than other set-
tings. It seems that the more complex the model,
the harder it is to model these cases correctly. Er-
ror types that include semantic variations (different
lemmas) or shared lemmas (but not explained by or-
thographic variation), are by contrast much harder
for the simple models. The more complex models do
quite well recalling errors involving semantically in-
coherent substitutions (around 77.7% of those cases)
and words that share the same lemma but vary in in-
flectional features (63% of those cases). These two
results are quite a jump from the basic word baseline
(around 29% and 18% respectively).
The simple addition of data seems to contribute
more towards the orthographic variation errors and
less towards semantic errors. The different settings
we use (training size and features) show some de-
gree of complementarity in how they identify errors.
We try to exploit this fact in Section 5.5 exploring
some simple system combination ideas.
</bodyText>
<subsectionHeader confidence="0.997534">
5.4 Blind Test Set
</subsectionHeader>
<bodyText confidence="0.999385818181818">
Table 8 shows the results of applying the same mod-
els described in Table 7 to a blind test set of yet un-
seen data. As mentioned in Section 4.1, the trivial
baseline of the test set is comparable to the dev set.
However, the test set is harder to tag than the dev
set; this can be seen in the overall lower F-scores.
That said, the relative order of performing features
is the same as with the dev set, confirming that our
best model is optimal for test too. On further study,
we noticed that the reason for the test set difference
is that the overlap in word forms between test and
</bodyText>
<page confidence="0.99458">
881
</page>
<table confidence="0.9985814">
word wconf best all
Precision 37.55 51.48 57.01 55.46
Recall 51.73 53.39 61.97 60.44
F-score 43.51 52.42 59.39 57.84
Accuracy 65.13 74.83 77.99 77.13
</table>
<tableCaption confidence="0.994501333333333">
Table 8: Results on test set of 500 segments with one hy-
pothesis each. The models were trained on the {S=4000,
H=10} training set.
</tableCaption>
<bodyText confidence="0.930677">
train is less than dev and train: 63% versus 81%,
respectively on {S=4000}.
</bodyText>
<subsectionHeader confidence="0.991739">
5.5 Preliminary Combination Analysis
</subsectionHeader>
<bodyText confidence="0.999970482758621">
In a preliminarily investigation of the value of com-
plementarity across these different systems, we tried
two simple model combination techniques. We re-
stricted the search to the systems in the error analy-
sis (Table 7).
First, we considered a sliding voting scheme
where a word is marked as problematic if at least
n systems agreed to that. Naturally, as n increases,
precision increases and recall decreases, provid-
ing multiple tradeoff options. The range spans
49.1/83.2/61.8 (% Precision/Recall/F-score) at one
end (n = 1) to 80.4/27.5/41.0 on the other (n = all).
The best F-score combination was with n = 2 (any
two agree) producing 62.8/72.4/67.3, an almost 1%
higher than our best system.
In a different combination exploration, we ex-
haustively sought the best three systems from which
any agreement (2 or 3) can produce an even better
system. The best combination included the word
model, the best model (both in {S=4000} training)
and the all model (in {S=2000}). This combination
yields 70.2/64.0/66.9, a lower F-score than the best
general voting approach discussed above, but with a
different bias towards better precision.
These basic exploratory experiments show that
there is a lot of value in pursuing combinations of
systems, if not for overall improvement, then at least
to benefit from tradeoffs in precision and recall that
may be appropriate for different applications.
</bodyText>
<subsectionHeader confidence="0.972175">
5.6 Preliminary Tag Set Exploration
</subsectionHeader>
<bodyText confidence="0.99995964">
In all of the experiments described so far, the
PZD models tag words using a binary tag set
of PROB/OK. We may also consider more com-
plex tag sets based on problem subtypes, such
as SUB/INS/DEL/OK (where all the problem sub-
types are differentiated), SUB/INS/OK (ignores
deletions), and SUB/OK (ignores deletions and in-
sertions). Care must be taken when comparing these
systems, because the differences in tag set definition
results in different baselines. Therefore we com-
pare the % error reduction over the trivial baseline
achieved in each case.
For an all model trained on the {S=2000, H=10}
set, using the PROB/OK tag set results in a 36.3%
error reduction over its trivial baseline (using the
dev set). The corresponding SUB/INS/DEL/OK tag
set only achieves a 34.8% error reduction. The
SUB/INS/OK tag set manages a 40.1% error re-
duction, however. The SUB/OK tag set achieves
a 38.9% error reduction. We suspect that the very
low relative number of deletions (7.9% in the dev
data) and the awkwardness of a DEL tag indicating
a neighboring deletion (rather than the current word)
may be confusing the models, and so ignoring them
seems to result in a clearer picture.
</bodyText>
<sectionHeader confidence="0.999985" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.9998128">
Common OCR/HR post-processing strategies are
similar to spelling correction solutions involving
dictionary lookup (Kukich, 1992; Jurafsky and Mar-
tin, 2000) and morphological restrictions (Domeij
et al., 1994; Oflazer, 1996). Error detection sys-
tems using dictionary lookup can sometimes be im-
proved by adding entries representing morphologi-
cal variations of root words, particularly if the lan-
guage involved has a complex morphology (Pal et
al., 2000). Alternatively, morphological information
can be used to construct supplemental lexicons or
language models (Sari and Sellami, 2002; Magdy
and Darwish, 2006).
In comparison to (Magdy and Darwish, 2006),
our paper is about error detection only (done in us-
ing discriminative machine learning); whereas their
work is on error correction (done in a standard gen-
erative manner (Kolak and Resnik, 2002)) with no
assumptions of some cases being correct or incor-
rect. In essence, their method of detection is the
same as our trivial baseline. The morphological fea-
tures they use are shallow and restricted to breaking
up a word into prefix+stem+suffix; whereas we ana-
lyze words into their lemmas, abstracting away over
a large number of variations. We also made use of
</bodyText>
<page confidence="0.927265">
882
</page>
<bodyText confidence="0.997976041666667">
part-of-speech tags, which they do not use, but sug- for producing correction hypotheses. We also will
gest may help. In their work, the morphological fea- consider different uses for the basic system setup
tures did not help (and even hurt a little), whereas for we developed to identify other types of text errors,
us, the lemma feature actually helped. Their hypoth- such as spelling errors or code-switching between
esis that their large language model (16M words) languages and dialects.
may be responsible for why the word-based mod- Acknowledgments
els outperformed stem-based (morphological) mod- We would like to thank Premkumar Natarajan, Rohit
els is challenged by the fact that our language model Prasad, Matin Kamali, Shirin Saleem, Katrin Kirch-
data (220M words) is an order of magnitude larger, hoff, and Andreas Stolcke. This work was funded
but we are still able to show benefit for using mor- under DARPA project number HR0011-08-C-0004.
phology. We cannot directly compare to their re- References
sults because of the different training/test sets and Fadi Biadsy, Jihad El-Sana, and Nizar Habash. 2006.
target (correction vs detection); however, we should Online Arabic handwriting recognition using Hid-
note that their starting error rate was quite high (39% den Markov Models. In The 10th International
on Alef/Ya normalized words), whereas our start- Workshop on Frontiers in Handwriting Recognition
ing error rate is almost half of that (-26% with un- (IWFHR’10), La Baule, France.
normalized Alef/Yas, which account for almost 5% Kareem Darwish and Douglas W. Oard. 2002. Term Se-
absolute of the errors). Perhaps a combination of lection for Searching Printed Arabic. In SIGIR ’02:
the two kinds of efforts can push the perfomance on Proceedings of the 25th annual international ACM SI-
correction even further by biasing towards problem- GIR conference on Research and development in in-
atic words and avoiding incorrectly changing correct formation retrieval, pages 261–268, New York, NY,
words. Magdy and Darwish (2006) do not report on USA. ACM.
percentages of words that they incorrectly modify. Rickard Domeij, Joachim Hollman, and Viggo Kann.
7 Conclusions and Future Work 1994. Detection of spelling errors in Swedish not us-
We presented a study with various settings (linguis- ing a word list en clair. J. Quantitative Linguistics,
tic and non-linguistic features and learning curve) 1:1–195.
for automatically detecting problem words in Ara- David Graff. 2007. Arabic Gigaword 3, LDC Catalog
bic handwriting recognition. Our best approach No.: LDC2003T40. Linguistic Data Consortium, Uni-
achieves a roughly —15% absolute increase in F- versity of Pennsylvania.
score over a simple baseline. A detailed error anal- Nizar Habash and Owen Rambow. 2005. Arabic Tok-
ysis shows that linguistic features, such as lemma enization, Part-of-Speech Tagging and Morphological
models, help improve HR-error detection specifi- Disambiguation in One Fell Swoop. In Proceedings of
cally where we expect them to: identifying semanti- the 43rd Annual Meeting of the Association for Com-
cally inconsistent error words. putational Linguistics (ACL’05), pages 573–580, Ann
In the future, we plan to continue improving our Arbor, Michigan, June. Association for Computational
system by considering smarter trainable combina- Linguistics.
tion techniques and by separating the training for Nizar Habash, Abdelhadi Soudi, and Tim Buckwalter.
different types of errors, particularly deletions from 2007. On Arabic Transliteration. In A. van den Bosch
insertions and substitutions. We would also like and A. Soudi, editors, Arabic Computational Mor-
to conduct an extended evaluation comparing other phology: Knowledge-based and Empirical Methods.
types of morphological features, such as roots and Springer.
stems, directly. One additional idea is to implement Nizar Habash, Owen Rambow, and Ryan Roth. 2010.
a lemma-confidence feature that examines lemma MADA+TOKAN Manual. Technical Report CCLS-
use in hypotheses across the document. This could 10-01, Center for Computational Learning Systems
potentially provide valuable semantic information at (CCLS), Columbia University.
the document level. Nizar Habash. 2010. Introduction to Arabic Natural
We also plan to integrate our system with a system Language Processing. Morgan &amp; Claypool Publish-
883 ers.
</bodyText>
<reference confidence="0.999151268817205">
Mohamed Ben Halima and Adel M. Alimi. 2009. A
multi-agent system for recognizing printed Arabic
words. In Khalid Choukri and Bente Maegaard, edi-
tors, Proceedings of the Second International Confer-
ence on Arabic Language Resources and Tools, Cairo,
Egypt, April. The MEDAR Consortium.
Daniel Jurafsky and James H. Martin. 2000. Speech
and Language Processing. Prentice Hall, New Jersey,
USA.
Okan Kolak and Philip Resnik. 2002. OCR error cor-
rection using a noisy channel model. In Proceedings
of the second international conference on Human Lan-
guage Technology Research.
Taku Kudo and Yuji Matsumoto. 2003. Fast Methods for
Kernel-Based Text Analysis. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics (ACL’03), pages 24–31, Sapporo,
Japan, July. Association for Computational Linguis-
tics.
Karen Kukich. 1992. Techniques for Automatically
Correcting Words in Text. ACM Computing Surveys,
24(4).
Leah S. Larkey, Lisa Ballesteros, and Margaret E.
Connell, 2007. Arabic Computational Morphology:
Knowledge-based and Empirical Methods, chapter
Light Stemming for Arabic Information Retrieval.
Springer Netherlands, Kluwer/Springer edition.
Zhidong Lu, Issam Bazzi, Andras Kornai, John Makhoul,
Premkumar Natarajan, and Richard Schwartz. 1999.
A Robust, Language-Independent OCR System. In the
27th AIPR Workshop: Advances in Computer Assisted
Recognition, SPIE.
Walid Magdy and Kareem Darwish. 2006. Arabic OCR
Error Correction Using Character Segment Correction,
Language Modeling, and Shallow Morphology. In
Proceedings of 2006 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP 2006),
pages 408–414, Sydney, Austrailia.
Volker Märgner and Haikal El Abed. 2009. Arabic
Word and Text Recognition - Current Developments.
In Khalid Choukri and Bente Maegaard, editors, Pro-
ceedings of the Second International Conference on
Arabic Language Resources and Tools, Cairo, Egypt,
April. The MEDAR Consortium.
Mohsen Moftah, Waleed Fakhr, Sherif Abdou, and
Mohsen Rashwan. 2009. Stem-based Arabic lan-
guage models experiments. In Khalid Choukri and
Bente Maegaard, editors, Proceedings of the Second
International Conference on Arabic Language Re-
sources and Tools, Cairo, Egypt, April. The MEDAR
Consortium.
Prem Natarajan, Shirin Saleem, Rohit Prasad, Ehry
MacRostie, and Krishna Subramanian, 2008. Arabic
and Chinese Handwriting Recognition, volume 4768
of Lecture Notes in Computer Science, pages 231–250.
Springer, Berlin, Germany.
Kemal Oflazer. 1996. Error-tolerant finite-state recog-
nition with applications to morphological analysis
and spelling correction. Computational Linguistics,
22:73–90.
U. Pal, P. K. Kundu, and B. B. Chaudhuri. 2000. OCR er-
ror correction of an inflectional Indian language using
morphological parsing. J. Information Sci. and Eng.,
16:903–922.
Ryan Roth, Owen Rambow, Nizar Habash, Mona Diab,
and Cynthia Rudin. 2008. Arabic Morphological Tag-
ging, Diacritization, and Lemmatization Using Lex-
eme Models and Feature Ranking. In Proceedings of
ACL-08: HLT, Short Papers, pages 117–120, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Shirin Saleem, Huaigu Cao, Krishna Subramanian, Marin
Kamali, Rohit Prasad, and Prem Natarajan. 2009.
Improvements in BBN’s HMM-based Offline Hand-
writing Recognition System. In Khalid Choukri and
Bente Maegaard, editors, 10th International Confer-
ence on DocumentAnalysis and Recognition (ICDAR),
Barcelona, Spain, July.
Toufik Sari and Mokhtar Sellami. 2002. MOrpho-
LEXical analysis for correcting OCR-generated Ara-
bic words (MOLEX). In The 8th International
Workshop on Frontiers in Handwriting Recognition
(IWFHR’02), Niagara-on-the-Lake, Canada.
Andreas Stolcke. 2002. SRILM - an Extensible Lan-
guage Modeling Toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing
(ICSLP), volume 2, pages 901–904, Denver, CO.
Stephanie Strassel. 2009. Linguistic Resources for
Arabic Handwriting Recognition. In Khalid Choukri
and Bente Maegaard, editors, Proceedings of the Sec-
ond International Conference on Arabic Language Re-
sources and Tools, Cairo, Egypt, April. The MEDAR
Consortium.
</reference>
<page confidence="0.99875">
884
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.596240">
<title confidence="0.9998195">Using Deep Morphology to Improve Automatic Error in Arabic Handwriting Recognition</title>
<author confidence="0.998064">M Habash</author>
<affiliation confidence="0.813227">Center for Computational Learning Columbia</affiliation>
<email confidence="0.999833">habash@ccls.columbia.edu</email>
<email confidence="0.999833">ryanr@ccls.columbia.edu</email>
<abstract confidence="0.997406666666667">Arabic handwriting recognition (HR) is a challenging problem due to Arabic’s connected letter forms, consonantal diacritics and rich morphology. In this paper we isolate the task of identification of erroneous words in HR from the task of producing corrections for these words. We consider a variety of linguistic (morphological and syntactic) and non-linguistic features to automatically identify these errors. Our best approach achieves roughly absolute increase in F-score over a simple but reasonable baseline. A detailed error analysis shows that linguistic features, such as lemma (i.e., citation form) models, help improve HR-error detection precisely where we expect them to: semantically incoherent error words.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mohamed Ben Halima</author>
<author>Adel M Alimi</author>
</authors>
<title>A multi-agent system for recognizing printed Arabic words.</title>
<date>2009</date>
<booktitle>In Khalid Choukri and Bente Maegaard, editors, Proceedings of the Second International Conference on Arabic Language Resources and Tools,</booktitle>
<publisher>The MEDAR Consortium.</publisher>
<location>Cairo, Egypt,</location>
<contexts>
<context position="1249" citStr="Halima and Alimi, 2009" startWordPosition="176" endWordPosition="179">ese errors. Our best approach achieves a roughly —15% absolute increase in F-score over a simple but reasonable baseline. A detailed error analysis shows that linguistic features, such as lemma (i.e., citation form) models, help improve HR-error detection precisely where we expect them to: semantically incoherent error words. 1 Introduction After years of development, optical character recognition (OCR) for Latin-character languages, such as English, has been refined greatly. Arabic, however, possesses a complex orthography and morphology that makes OCR more difficult (Märgner and Abed, 2009; Halima and Alimi, 2009; Magdy and Darwish, 2006). Because of this, only a few systems for Arabic OCR of printed text have been developed, and these have not been thoroughly evaluated (Märgner and Abed, 2009). OCR of Arabic handwritten text (handwriting recognition, or HR), whether online or offline, is even more challenging compared to printed Arabic OCR, where the uniformity of letter shapes and other factors allow for easier recognition (Biadsy et al., 2006; Natarajan et al., 2008; Saleem et al., 2009). OCR and HR systems are often improved by performing post-processing; these are attempts to evaluate whether eac</context>
</contexts>
<marker>Halima, Alimi, 2009</marker>
<rawString>Mohamed Ben Halima and Adel M. Alimi. 2009. A multi-agent system for recognizing printed Arabic words. In Khalid Choukri and Bente Maegaard, editors, Proceedings of the Second International Conference on Arabic Language Resources and Tools, Cairo, Egypt, April. The MEDAR Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Jurafsky</author>
<author>James H Martin</author>
</authors>
<title>Speech and Language Processing.</title>
<date>2000</date>
<publisher>Prentice Hall,</publisher>
<location>New Jersey, USA.</location>
<contexts>
<context position="32929" citStr="Jurafsky and Martin, 2000" startWordPosition="5444" endWordPosition="5448">sponding SUB/INS/DEL/OK tag set only achieves a 34.8% error reduction. The SUB/INS/OK tag set manages a 40.1% error reduction, however. The SUB/OK tag set achieves a 38.9% error reduction. We suspect that the very low relative number of deletions (7.9% in the dev data) and the awkwardness of a DEL tag indicating a neighboring deletion (rather than the current word) may be confusing the models, and so ignoring them seems to result in a clearer picture. 6 Related Work Common OCR/HR post-processing strategies are similar to spelling correction solutions involving dictionary lookup (Kukich, 1992; Jurafsky and Martin, 2000) and morphological restrictions (Domeij et al., 1994; Oflazer, 1996). Error detection systems using dictionary lookup can sometimes be improved by adding entries representing morphological variations of root words, particularly if the language involved has a complex morphology (Pal et al., 2000). Alternatively, morphological information can be used to construct supplemental lexicons or language models (Sari and Sellami, 2002; Magdy and Darwish, 2006). In comparison to (Magdy and Darwish, 2006), our paper is about error detection only (done in using discriminative machine learning); whereas the</context>
</contexts>
<marker>Jurafsky, Martin, 2000</marker>
<rawString>Daniel Jurafsky and James H. Martin. 2000. Speech and Language Processing. Prentice Hall, New Jersey, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Okan Kolak</author>
<author>Philip Resnik</author>
</authors>
<title>OCR error correction using a noisy channel model.</title>
<date>2002</date>
<booktitle>In Proceedings of the second international conference on Human Language Technology Research.</booktitle>
<contexts>
<context position="33622" citStr="Kolak and Resnik, 2002" startWordPosition="5550" endWordPosition="5553">Error detection systems using dictionary lookup can sometimes be improved by adding entries representing morphological variations of root words, particularly if the language involved has a complex morphology (Pal et al., 2000). Alternatively, morphological information can be used to construct supplemental lexicons or language models (Sari and Sellami, 2002; Magdy and Darwish, 2006). In comparison to (Magdy and Darwish, 2006), our paper is about error detection only (done in using discriminative machine learning); whereas their work is on error correction (done in a standard generative manner (Kolak and Resnik, 2002)) with no assumptions of some cases being correct or incorrect. In essence, their method of detection is the same as our trivial baseline. The morphological features they use are shallow and restricted to breaking up a word into prefix+stem+suffix; whereas we analyze words into their lemmas, abstracting away over a large number of variations. We also made use of 882 part-of-speech tags, which they do not use, but sug- for producing correction hypotheses. We also will gest may help. In their work, the morphological fea- consider different uses for the basic system setup tures did not help (and </context>
</contexts>
<marker>Kolak, Resnik, 2002</marker>
<rawString>Okan Kolak and Philip Resnik. 2002. OCR error correction using a noisy channel model. In Proceedings of the second international conference on Human Language Technology Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Fast Methods for Kernel-Based Text Analysis.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL’03),</booktitle>
<pages>24--31</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sapporo, Japan,</location>
<contexts>
<context position="14440" citStr="Kudo and Matsumoto, 2003" startWordPosition="2333" endWordPosition="2336">. The dev and test sets use 500 segments with one top-ranked hypothesis each {H=1}. We can construct a trivial PZD baseline by assuming all the input words are PROBs; this results in baseline % Precision/Recall/F-scores of 25.8/100/41.1 and 26.0/100/41.2 for the dev and test sets, respectively. Note that in this paper we eschew these baselines in favor of comparison to a non-trivial baseline generated by a simple PZD model. 4.2 PZD Models and Features The PZD system relies on a set of SVM classifiers trained using morphological and lexical features. The SVM classifiers are built using Yamcha (Kudo and Matsumoto, 2003). The SVMs use a quadratic polynomial kernel. For the models presented in this paper, the static feature window context size is set to +/- 2 words; the previous two (dynamic) classifications (i.e. targets) are also used as features. Experiments with smaller window sizes result in poorer performance, while a larger window size (+/- 6 words) yields roughly the same performance at the expense of an order-of-magnitude increase in required training time. Over 30 different 4Naturally, we do not use data that the BBN Byblos HR system was trained on. 5We conducted additional experiments where we varie</context>
</contexts>
<marker>Kudo, Matsumoto, 2003</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2003. Fast Methods for Kernel-Based Text Analysis. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL’03), pages 24–31, Sapporo, Japan, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Kukich</author>
</authors>
<title>Techniques for Automatically Correcting Words in Text.</title>
<date>1992</date>
<journal>ACM Computing Surveys,</journal>
<volume>24</volume>
<issue>4</issue>
<contexts>
<context position="32901" citStr="Kukich, 1992" startWordPosition="5442" endWordPosition="5443">et). The corresponding SUB/INS/DEL/OK tag set only achieves a 34.8% error reduction. The SUB/INS/OK tag set manages a 40.1% error reduction, however. The SUB/OK tag set achieves a 38.9% error reduction. We suspect that the very low relative number of deletions (7.9% in the dev data) and the awkwardness of a DEL tag indicating a neighboring deletion (rather than the current word) may be confusing the models, and so ignoring them seems to result in a clearer picture. 6 Related Work Common OCR/HR post-processing strategies are similar to spelling correction solutions involving dictionary lookup (Kukich, 1992; Jurafsky and Martin, 2000) and morphological restrictions (Domeij et al., 1994; Oflazer, 1996). Error detection systems using dictionary lookup can sometimes be improved by adding entries representing morphological variations of root words, particularly if the language involved has a complex morphology (Pal et al., 2000). Alternatively, morphological information can be used to construct supplemental lexicons or language models (Sari and Sellami, 2002; Magdy and Darwish, 2006). In comparison to (Magdy and Darwish, 2006), our paper is about error detection only (done in using discriminative ma</context>
</contexts>
<marker>Kukich, 1992</marker>
<rawString>Karen Kukich. 1992. Techniques for Automatically Correcting Words in Text. ACM Computing Surveys, 24(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leah S Larkey</author>
<author>Lisa Ballesteros</author>
<author>Margaret E Connell</author>
</authors>
<title>Arabic Computational Morphology: Knowledge-based and Empirical Methods, chapter Light Stemming for Arabic Information Retrieval.</title>
<date>2007</date>
<publisher>Springer</publisher>
<contexts>
<context position="7125" citStr="Larkey et al., 2007" startWordPosition="1106" endWordPosition="1109">d kJ.~ -.rktybh ‘military unit (of conscripts)’. Arabic morphology allows for tens of billions of potential, legal words (Magdy and Darwish, 2006; Moftah et al., 2009). The large potential vocabulary size by itself complicates HR methods that rely on conventional, word-based dictionary lookup strategies. In this paper we consider the value of morpholexical and morpho-syntactic features such as lemmas and part-of-speech tags, respectively, that may allow machine learning algorithms to learn generalizations. We do not consider the root since it has been shown to be too general for NLP purposes (Larkey et al., 2007). Other researchers have used stems for OCR correction (Magdy and Darwish, 2006); we discuss their work and compare to it in Section 6, but we do not present a direct experimental comparison. 3 Problem Zones in Handwriting Recognition 3.1 HR Error Classifications We can classify three types of HR errors: substitutions, insertions and deletions. Substitutions involve replacing the correct word by another incorrect form. Insertions are words that are incorrectly added into the HR hypothesis. An insertion error is typically paired with a substitution error, where the two errors reflect a mis-iden</context>
</contexts>
<marker>Larkey, Ballesteros, Connell, 2007</marker>
<rawString>Leah S. Larkey, Lisa Ballesteros, and Margaret E. Connell, 2007. Arabic Computational Morphology: Knowledge-based and Empirical Methods, chapter Light Stemming for Arabic Information Retrieval. Springer Netherlands, Kluwer/Springer edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhidong Lu</author>
<author>Issam Bazzi</author>
<author>Andras Kornai</author>
<author>John Makhoul</author>
<author>Premkumar Natarajan</author>
<author>Richard Schwartz</author>
</authors>
<title>A Robust, Language-Independent OCR System.</title>
<date>1999</date>
<booktitle>In the 27th AIPR Workshop: Advances in Computer Assisted Recognition, SPIE.</booktitle>
<contexts>
<context position="4237" citStr="Lu et al., 1999" startWordPosition="655" endWordPosition="658"> the 49th Annual Meeting of the Association for Computational Linguistics, pages 875–884, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics 2.1 Arabic Orthography Challenges The use of cursive, connected script creates problems in that it becomes more difficult for a machine to distinguish between individual characters. This is certainly not a property unique to Arabic; methods developed for other cursive script languages (such as Hidden Markov Models) can be applied successfully to Arabic (Natarajan et al., 2008; Saleem et al., 2009; Märgner and Abed, 2009; Lu et al., 1999). Arabic writers often make use of elongation (tatweel/kashida) to beautify the script. Arabic also contains certain ligature constructions that require consideration during OCR/HR (Darwish and Oard, 2002). Sets of dots and optional diacritic markers are used to create character distinctions in Arabic. However, trace amounts of dust or dirt on the original document scan can be easily mistaken for these markers (Darwish and Oard, 2002). Alternatively, these markers in handwritten text may be too small, light or closely-spaced to readily distinguish, causing the system to drop them entirely. Whi</context>
</contexts>
<marker>Lu, Bazzi, Kornai, Makhoul, Natarajan, Schwartz, 1999</marker>
<rawString>Zhidong Lu, Issam Bazzi, Andras Kornai, John Makhoul, Premkumar Natarajan, and Richard Schwartz. 1999. A Robust, Language-Independent OCR System. In the 27th AIPR Workshop: Advances in Computer Assisted Recognition, SPIE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walid Magdy</author>
<author>Kareem Darwish</author>
</authors>
<title>Arabic OCR Error Correction Using Character Segment Correction, Language Modeling, and Shallow Morphology.</title>
<date>2006</date>
<booktitle>In Proceedings of 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<pages>408--414</pages>
<location>Sydney, Austrailia.</location>
<contexts>
<context position="1275" citStr="Magdy and Darwish, 2006" startWordPosition="180" endWordPosition="184">roach achieves a roughly —15% absolute increase in F-score over a simple but reasonable baseline. A detailed error analysis shows that linguistic features, such as lemma (i.e., citation form) models, help improve HR-error detection precisely where we expect them to: semantically incoherent error words. 1 Introduction After years of development, optical character recognition (OCR) for Latin-character languages, such as English, has been refined greatly. Arabic, however, possesses a complex orthography and morphology that makes OCR more difficult (Märgner and Abed, 2009; Halima and Alimi, 2009; Magdy and Darwish, 2006). Because of this, only a few systems for Arabic OCR of printed text have been developed, and these have not been thoroughly evaluated (Märgner and Abed, 2009). OCR of Arabic handwritten text (handwriting recognition, or HR), whether online or offline, is even more challenging compared to printed Arabic OCR, where the uniformity of letter shapes and other factors allow for easier recognition (Biadsy et al., 2006; Natarajan et al., 2008; Saleem et al., 2009). OCR and HR systems are often improved by performing post-processing; these are attempts to evaluate whether each word, phrase or sentence</context>
<context position="3576" citStr="Magdy and Darwish, 2006" startWordPosition="555" endWordPosition="558">kground on the difficulties of the Arabic HR task. Section 3 presents an analysis of HR errors and defines what is considered a problem zone to be tagged. The experimental features, data and other variables are outlined in Section 4. The experiments are presented and discussed in Section 5. We discuss and compare to some related work in detail in Section 6. Conclusions and suggested avenues of for future progress are presented in Section 7. 2 Arabic Handwriting Recognition Challenges Arabic has several orthographic and morphological properties that make HR challenging (Darwish and Oard, 2002; Magdy and Darwish, 2006; Märgner and Abed, 2009). 875 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 875–884, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics 2.1 Arabic Orthography Challenges The use of cursive, connected script creates problems in that it becomes more difficult for a machine to distinguish between individual characters. This is certainly not a property unique to Arabic; methods developed for other cursive script languages (such as Hidden Markov Models) can be applied successfully to Arabic (Natarajan et al., 2008; </context>
<context position="6650" citStr="Magdy and Darwish, 2006" startWordPosition="1030" endWordPosition="1033">he pattern correspond to positions where root radicals are inserted. office’, AîD.�JºÖÏ lmktbhA ‘for her office’, and so on. Just as the lemma abstracts over inflectional morphology, the root abstracts over both inflectional and derivational morphology and thus provides a very high level of lexical abstraction, indicating the “core” meaning of the word. The Arabic root H. u1/4 k-t-b ‘writing related’, e.g., relates words likeI.�JºÓ mktb ‘office’, H. ñ�JºÓ mktwb ‘letter’, and kJ.~ -.rktybh ‘military unit (of conscripts)’. Arabic morphology allows for tens of billions of potential, legal words (Magdy and Darwish, 2006; Moftah et al., 2009). The large potential vocabulary size by itself complicates HR methods that rely on conventional, word-based dictionary lookup strategies. In this paper we consider the value of morpholexical and morpho-syntactic features such as lemmas and part-of-speech tags, respectively, that may allow machine learning algorithms to learn generalizations. We do not consider the root since it has been shown to be too general for NLP purposes (Larkey et al., 2007). Other researchers have used stems for OCR correction (Magdy and Darwish, 2006); we discuss their work and compare to it in </context>
<context position="33383" citStr="Magdy and Darwish, 2006" startWordPosition="5511" endWordPosition="5514">6 Related Work Common OCR/HR post-processing strategies are similar to spelling correction solutions involving dictionary lookup (Kukich, 1992; Jurafsky and Martin, 2000) and morphological restrictions (Domeij et al., 1994; Oflazer, 1996). Error detection systems using dictionary lookup can sometimes be improved by adding entries representing morphological variations of root words, particularly if the language involved has a complex morphology (Pal et al., 2000). Alternatively, morphological information can be used to construct supplemental lexicons or language models (Sari and Sellami, 2002; Magdy and Darwish, 2006). In comparison to (Magdy and Darwish, 2006), our paper is about error detection only (done in using discriminative machine learning); whereas their work is on error correction (done in a standard generative manner (Kolak and Resnik, 2002)) with no assumptions of some cases being correct or incorrect. In essence, their method of detection is the same as our trivial baseline. The morphological features they use are shallow and restricted to breaking up a word into prefix+stem+suffix; whereas we analyze words into their lemmas, abstracting away over a large number of variations. We also made use</context>
<context position="36031" citStr="Magdy and Darwish (2006)" startWordPosition="5931" endWordPosition="5934">ion ing error rate is almost half of that (-26% with un- (IWFHR’10), La Baule, France. normalized Alef/Yas, which account for almost 5% Kareem Darwish and Douglas W. Oard. 2002. Term Seabsolute of the errors). Perhaps a combination of lection for Searching Printed Arabic. In SIGIR ’02: the two kinds of efforts can push the perfomance on Proceedings of the 25th annual international ACM SIcorrection even further by biasing towards problem- GIR conference on Research and development in inatic words and avoiding incorrectly changing correct formation retrieval, pages 261–268, New York, NY, words. Magdy and Darwish (2006) do not report on USA. ACM. percentages of words that they incorrectly modify. Rickard Domeij, Joachim Hollman, and Viggo Kann. 7 Conclusions and Future Work 1994. Detection of spelling errors in Swedish not usWe presented a study with various settings (linguis- ing a word list en clair. J. Quantitative Linguistics, tic and non-linguistic features and learning curve) 1:1–195. for automatically detecting problem words in Ara- David Graff. 2007. Arabic Gigaword 3, LDC Catalog bic handwriting recognition. Our best approach No.: LDC2003T40. Linguistic Data Consortium, Uniachieves a roughly —15% ab</context>
</contexts>
<marker>Magdy, Darwish, 2006</marker>
<rawString>Walid Magdy and Kareem Darwish. 2006. Arabic OCR Error Correction Using Character Segment Correction, Language Modeling, and Shallow Morphology. In Proceedings of 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 408–414, Sydney, Austrailia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Volker Märgner</author>
<author>Haikal El Abed</author>
</authors>
<title>Arabic Word and Text Recognition - Current Developments.</title>
<date>2009</date>
<booktitle>In Khalid Choukri and Bente Maegaard, editors, Proceedings of the Second International Conference on Arabic Language Resources and Tools,</booktitle>
<publisher>The MEDAR Consortium.</publisher>
<location>Cairo, Egypt,</location>
<marker>Märgner, El Abed, 2009</marker>
<rawString>Volker Märgner and Haikal El Abed. 2009. Arabic Word and Text Recognition - Current Developments. In Khalid Choukri and Bente Maegaard, editors, Proceedings of the Second International Conference on Arabic Language Resources and Tools, Cairo, Egypt, April. The MEDAR Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohsen Moftah</author>
<author>Waleed Fakhr</author>
<author>Sherif Abdou</author>
<author>Mohsen Rashwan</author>
</authors>
<title>Stem-based Arabic language models experiments.</title>
<date>2009</date>
<booktitle>In Khalid Choukri and Bente Maegaard, editors, Proceedings of the Second International Conference on Arabic Language Resources and Tools,</booktitle>
<publisher>The MEDAR Consortium.</publisher>
<location>Cairo, Egypt,</location>
<contexts>
<context position="6672" citStr="Moftah et al., 2009" startWordPosition="1034" endWordPosition="1037">positions where root radicals are inserted. office’, AîD.�JºÖÏ lmktbhA ‘for her office’, and so on. Just as the lemma abstracts over inflectional morphology, the root abstracts over both inflectional and derivational morphology and thus provides a very high level of lexical abstraction, indicating the “core” meaning of the word. The Arabic root H. u1/4 k-t-b ‘writing related’, e.g., relates words likeI.�JºÓ mktb ‘office’, H. ñ�JºÓ mktwb ‘letter’, and kJ.~ -.rktybh ‘military unit (of conscripts)’. Arabic morphology allows for tens of billions of potential, legal words (Magdy and Darwish, 2006; Moftah et al., 2009). The large potential vocabulary size by itself complicates HR methods that rely on conventional, word-based dictionary lookup strategies. In this paper we consider the value of morpholexical and morpho-syntactic features such as lemmas and part-of-speech tags, respectively, that may allow machine learning algorithms to learn generalizations. We do not consider the root since it has been shown to be too general for NLP purposes (Larkey et al., 2007). Other researchers have used stems for OCR correction (Magdy and Darwish, 2006); we discuss their work and compare to it in Section 6, but we do n</context>
</contexts>
<marker>Moftah, Fakhr, Abdou, Rashwan, 2009</marker>
<rawString>Mohsen Moftah, Waleed Fakhr, Sherif Abdou, and Mohsen Rashwan. 2009. Stem-based Arabic language models experiments. In Khalid Choukri and Bente Maegaard, editors, Proceedings of the Second International Conference on Arabic Language Resources and Tools, Cairo, Egypt, April. The MEDAR Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Prem Natarajan</author>
<author>Shirin Saleem</author>
<author>Rohit Prasad</author>
<author>Ehry MacRostie</author>
<author>Krishna Subramanian</author>
</authors>
<title>Arabic and Chinese Handwriting Recognition,</title>
<date>2008</date>
<booktitle>of Lecture Notes in Computer Science,</booktitle>
<volume>4768</volume>
<pages>231--250</pages>
<publisher>Springer,</publisher>
<location>Berlin, Germany.</location>
<contexts>
<context position="1714" citStr="Natarajan et al., 2008" startWordPosition="255" endWordPosition="258">efined greatly. Arabic, however, possesses a complex orthography and morphology that makes OCR more difficult (Märgner and Abed, 2009; Halima and Alimi, 2009; Magdy and Darwish, 2006). Because of this, only a few systems for Arabic OCR of printed text have been developed, and these have not been thoroughly evaluated (Märgner and Abed, 2009). OCR of Arabic handwritten text (handwriting recognition, or HR), whether online or offline, is even more challenging compared to printed Arabic OCR, where the uniformity of letter shapes and other factors allow for easier recognition (Biadsy et al., 2006; Natarajan et al., 2008; Saleem et al., 2009). OCR and HR systems are often improved by performing post-processing; these are attempts to evaluate whether each word, phrase or sentence in the OCR/HR output is legal and/or probable. When an illegal word or phrase is discovered (error detection), these systems usually attempt to generate a legal alternative (error correction). In this paper, we present a HR error detection system that uses deep lexical and morphological feature models to locate possible &amp;quot;problem zones&amp;quot; – words or phrases that are likely incorrect – in Arabic HR output. We use an off-the-shelf HR syste</context>
<context position="4174" citStr="Natarajan et al., 2008" startWordPosition="643" endWordPosition="646"> Magdy and Darwish, 2006; Märgner and Abed, 2009). 875 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 875–884, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics 2.1 Arabic Orthography Challenges The use of cursive, connected script creates problems in that it becomes more difficult for a machine to distinguish between individual characters. This is certainly not a property unique to Arabic; methods developed for other cursive script languages (such as Hidden Markov Models) can be applied successfully to Arabic (Natarajan et al., 2008; Saleem et al., 2009; Märgner and Abed, 2009; Lu et al., 1999). Arabic writers often make use of elongation (tatweel/kashida) to beautify the script. Arabic also contains certain ligature constructions that require consideration during OCR/HR (Darwish and Oard, 2002). Sets of dots and optional diacritic markers are used to create character distinctions in Arabic. However, trace amounts of dust or dirt on the original document scan can be easily mistaken for these markers (Darwish and Oard, 2002). Alternatively, these markers in handwritten text may be too small, light or closely-spaced to rea</context>
<context position="13200" citStr="Natarajan et al., 2008" startWordPosition="2122" endWordPosition="2125">a used in this paper is derived from image scans provided by the Linguistic Data Consortium (LDC) (Strassel, 2009). This data consists of high-resolution (600 dpi) handwriting scans of Arabic text taken from newswire articles, web logs and 3For clarity, we refer to these tags as ‘gold’, whereas the correct segment for a given hypothesis set is called the ‘reference’. � (@/ @/ 877 newsgroups, along with ground truth annotations and word bounding box information. The scans include variations in scribe demographic background, writing instrument, paper and writing speed. The BBN Byblos HR system (Natarajan et al., 2008; Saleem et al., 2009) is then used to process these scanned images into sequences of segments (sentence fragments). The system generates a ranked N-best list of hypotheses for each segment, where N could be as high as 300. On average, a segment has 6.87 words (including punctuation). We divide the N-best list data into training, development (dev) and test sets.4 For training, we consider two sets of size 2000 and 4000 segments (5) with the 10 top-ranked hypotheses (H) for each segment to provide additional variations.5 The references are also included in the training sets to provide examples </context>
</contexts>
<marker>Natarajan, Saleem, Prasad, MacRostie, Subramanian, 2008</marker>
<rawString>Prem Natarajan, Shirin Saleem, Rohit Prasad, Ehry MacRostie, and Krishna Subramanian, 2008. Arabic and Chinese Handwriting Recognition, volume 4768 of Lecture Notes in Computer Science, pages 231–250. Springer, Berlin, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kemal Oflazer</author>
</authors>
<title>Error-tolerant finite-state recognition with applications to morphological analysis and spelling correction.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<pages>22--73</pages>
<contexts>
<context position="32997" citStr="Oflazer, 1996" startWordPosition="5456" endWordPosition="5457">NS/OK tag set manages a 40.1% error reduction, however. The SUB/OK tag set achieves a 38.9% error reduction. We suspect that the very low relative number of deletions (7.9% in the dev data) and the awkwardness of a DEL tag indicating a neighboring deletion (rather than the current word) may be confusing the models, and so ignoring them seems to result in a clearer picture. 6 Related Work Common OCR/HR post-processing strategies are similar to spelling correction solutions involving dictionary lookup (Kukich, 1992; Jurafsky and Martin, 2000) and morphological restrictions (Domeij et al., 1994; Oflazer, 1996). Error detection systems using dictionary lookup can sometimes be improved by adding entries representing morphological variations of root words, particularly if the language involved has a complex morphology (Pal et al., 2000). Alternatively, morphological information can be used to construct supplemental lexicons or language models (Sari and Sellami, 2002; Magdy and Darwish, 2006). In comparison to (Magdy and Darwish, 2006), our paper is about error detection only (done in using discriminative machine learning); whereas their work is on error correction (done in a standard generative manner</context>
</contexts>
<marker>Oflazer, 1996</marker>
<rawString>Kemal Oflazer. 1996. Error-tolerant finite-state recognition with applications to morphological analysis and spelling correction. Computational Linguistics, 22:73–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Pal</author>
<author>P K Kundu</author>
<author>B B Chaudhuri</author>
</authors>
<title>OCR error correction of an inflectional Indian language using morphological parsing.</title>
<date>2000</date>
<journal>J. Information Sci. and Eng.,</journal>
<pages>16--903</pages>
<contexts>
<context position="33225" citStr="Pal et al., 2000" startWordPosition="5490" endWordPosition="5493">dicating a neighboring deletion (rather than the current word) may be confusing the models, and so ignoring them seems to result in a clearer picture. 6 Related Work Common OCR/HR post-processing strategies are similar to spelling correction solutions involving dictionary lookup (Kukich, 1992; Jurafsky and Martin, 2000) and morphological restrictions (Domeij et al., 1994; Oflazer, 1996). Error detection systems using dictionary lookup can sometimes be improved by adding entries representing morphological variations of root words, particularly if the language involved has a complex morphology (Pal et al., 2000). Alternatively, morphological information can be used to construct supplemental lexicons or language models (Sari and Sellami, 2002; Magdy and Darwish, 2006). In comparison to (Magdy and Darwish, 2006), our paper is about error detection only (done in using discriminative machine learning); whereas their work is on error correction (done in a standard generative manner (Kolak and Resnik, 2002)) with no assumptions of some cases being correct or incorrect. In essence, their method of detection is the same as our trivial baseline. The morphological features they use are shallow and restricted t</context>
</contexts>
<marker>Pal, Kundu, Chaudhuri, 2000</marker>
<rawString>U. Pal, P. K. Kundu, and B. B. Chaudhuri. 2000. OCR error correction of an inflectional Indian language using morphological parsing. J. Information Sci. and Eng., 16:903–922.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan Roth</author>
<author>Owen Rambow</author>
<author>Nizar Habash</author>
<author>Mona Diab</author>
<author>Cynthia Rudin</author>
</authors>
<title>Arabic Morphological Tagging, Diacritization, and Lemmatization Using Lexeme Models and Feature Ranking.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT, Short Papers,</booktitle>
<pages>117--120</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="16395" citStr="Roth et al., 2008" startWordPosition="2648" endWordPosition="2651"> of hypotheses in the N-best list that contain the word over the total number of hypotheses Table 2: PZD model features. Simple features are used directly by the PZD SVM models, whereas Binned features’ (numerical) values are reduced to a small, labeled category set whose labels are used as model features. combinations of features were considered. Table 2 shows the individual feature definitions. In order to obtain the morphological features, all of the training and test data is passed through MADA 3.0, a software tool for Arabic morphological analysis disambiguation (Habash and Rambow, 2005; Roth et al., 2008; Habash et al., 2010). For these experiments, MADA provides the pos (using MADA’s native 34-tag set) and the lemma for each word. Occasionally MADA will not be able to produce any interpretations (analyses) for a word; since this is often a sign that the word is misspelled or uncommon, we define a binary na feature to indicate when MADA fails to generate analyses. In addition to using the MADA features directly, we also develop a set of nine N-gram models (where N=1, 2, and 3) for the nw, pos, and lem features defined in Table 2. We train these models using 220M words from the Arabic Gigaword</context>
</contexts>
<marker>Roth, Rambow, Habash, Diab, Rudin, 2008</marker>
<rawString>Ryan Roth, Owen Rambow, Nizar Habash, Mona Diab, and Cynthia Rudin. 2008. Arabic Morphological Tagging, Diacritization, and Lemmatization Using Lexeme Models and Feature Ranking. In Proceedings of ACL-08: HLT, Short Papers, pages 117–120, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shirin Saleem</author>
<author>Huaigu Cao</author>
<author>Krishna Subramanian</author>
<author>Marin Kamali</author>
<author>Rohit Prasad</author>
<author>Prem Natarajan</author>
</authors>
<title>Improvements in BBN’s HMM-based Offline Handwriting Recognition System.</title>
<date>2009</date>
<booktitle>In Khalid Choukri and Bente Maegaard, editors, 10th International Conference on DocumentAnalysis and Recognition (ICDAR),</booktitle>
<location>Barcelona, Spain,</location>
<contexts>
<context position="1736" citStr="Saleem et al., 2009" startWordPosition="259" endWordPosition="262">however, possesses a complex orthography and morphology that makes OCR more difficult (Märgner and Abed, 2009; Halima and Alimi, 2009; Magdy and Darwish, 2006). Because of this, only a few systems for Arabic OCR of printed text have been developed, and these have not been thoroughly evaluated (Märgner and Abed, 2009). OCR of Arabic handwritten text (handwriting recognition, or HR), whether online or offline, is even more challenging compared to printed Arabic OCR, where the uniformity of letter shapes and other factors allow for easier recognition (Biadsy et al., 2006; Natarajan et al., 2008; Saleem et al., 2009). OCR and HR systems are often improved by performing post-processing; these are attempts to evaluate whether each word, phrase or sentence in the OCR/HR output is legal and/or probable. When an illegal word or phrase is discovered (error detection), these systems usually attempt to generate a legal alternative (error correction). In this paper, we present a HR error detection system that uses deep lexical and morphological feature models to locate possible &amp;quot;problem zones&amp;quot; – words or phrases that are likely incorrect – in Arabic HR output. We use an off-the-shelf HR system (Natarajan et al., 2</context>
<context position="4195" citStr="Saleem et al., 2009" startWordPosition="647" endWordPosition="650">; Märgner and Abed, 2009). 875 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 875–884, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics 2.1 Arabic Orthography Challenges The use of cursive, connected script creates problems in that it becomes more difficult for a machine to distinguish between individual characters. This is certainly not a property unique to Arabic; methods developed for other cursive script languages (such as Hidden Markov Models) can be applied successfully to Arabic (Natarajan et al., 2008; Saleem et al., 2009; Märgner and Abed, 2009; Lu et al., 1999). Arabic writers often make use of elongation (tatweel/kashida) to beautify the script. Arabic also contains certain ligature constructions that require consideration during OCR/HR (Darwish and Oard, 2002). Sets of dots and optional diacritic markers are used to create character distinctions in Arabic. However, trace amounts of dust or dirt on the original document scan can be easily mistaken for these markers (Darwish and Oard, 2002). Alternatively, these markers in handwritten text may be too small, light or closely-spaced to readily distinguish, cau</context>
<context position="13222" citStr="Saleem et al., 2009" startWordPosition="2126" endWordPosition="2129">derived from image scans provided by the Linguistic Data Consortium (LDC) (Strassel, 2009). This data consists of high-resolution (600 dpi) handwriting scans of Arabic text taken from newswire articles, web logs and 3For clarity, we refer to these tags as ‘gold’, whereas the correct segment for a given hypothesis set is called the ‘reference’. � (@/ @/ 877 newsgroups, along with ground truth annotations and word bounding box information. The scans include variations in scribe demographic background, writing instrument, paper and writing speed. The BBN Byblos HR system (Natarajan et al., 2008; Saleem et al., 2009) is then used to process these scanned images into sequences of segments (sentence fragments). The system generates a ranked N-best list of hypotheses for each segment, where N could be as high as 300. On average, a segment has 6.87 words (including punctuation). We divide the N-best list data into training, development (dev) and test sets.4 For training, we consider two sets of size 2000 and 4000 segments (5) with the 10 top-ranked hypotheses (H) for each segment to provide additional variations.5 The references are also included in the training sets to provide examples of perfect text. The d</context>
</contexts>
<marker>Saleem, Cao, Subramanian, Kamali, Prasad, Natarajan, 2009</marker>
<rawString>Shirin Saleem, Huaigu Cao, Krishna Subramanian, Marin Kamali, Rohit Prasad, and Prem Natarajan. 2009. Improvements in BBN’s HMM-based Offline Handwriting Recognition System. In Khalid Choukri and Bente Maegaard, editors, 10th International Conference on DocumentAnalysis and Recognition (ICDAR), Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Toufik Sari</author>
<author>Mokhtar Sellami</author>
</authors>
<title>MOrphoLEXical analysis for correcting OCR-generated Arabic words (MOLEX).</title>
<date>2002</date>
<booktitle>In The 8th International Workshop on Frontiers in Handwriting Recognition (IWFHR’02),</booktitle>
<location>Niagara-on-the-Lake, Canada.</location>
<contexts>
<context position="33357" citStr="Sari and Sellami, 2002" startWordPosition="5507" endWordPosition="5510">t in a clearer picture. 6 Related Work Common OCR/HR post-processing strategies are similar to spelling correction solutions involving dictionary lookup (Kukich, 1992; Jurafsky and Martin, 2000) and morphological restrictions (Domeij et al., 1994; Oflazer, 1996). Error detection systems using dictionary lookup can sometimes be improved by adding entries representing morphological variations of root words, particularly if the language involved has a complex morphology (Pal et al., 2000). Alternatively, morphological information can be used to construct supplemental lexicons or language models (Sari and Sellami, 2002; Magdy and Darwish, 2006). In comparison to (Magdy and Darwish, 2006), our paper is about error detection only (done in using discriminative machine learning); whereas their work is on error correction (done in a standard generative manner (Kolak and Resnik, 2002)) with no assumptions of some cases being correct or incorrect. In essence, their method of detection is the same as our trivial baseline. The morphological features they use are shallow and restricted to breaking up a word into prefix+stem+suffix; whereas we analyze words into their lemmas, abstracting away over a large number of va</context>
</contexts>
<marker>Sari, Sellami, 2002</marker>
<rawString>Toufik Sari and Mokhtar Sellami. 2002. MOrphoLEXical analysis for correcting OCR-generated Arabic words (MOLEX). In The 8th International Workshop on Frontiers in Handwriting Recognition (IWFHR’02), Niagara-on-the-Lake, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - an Extensible Language Modeling Toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing (ICSLP),</booktitle>
<volume>2</volume>
<pages>901--904</pages>
<location>Denver, CO.</location>
<contexts>
<context position="17176" citStr="Stolcke, 2002" startWordPosition="2791" endWordPosition="2792">o produce any interpretations (analyses) for a word; since this is often a sign that the word is misspelled or uncommon, we define a binary na feature to indicate when MADA fails to generate analyses. In addition to using the MADA features directly, we also develop a set of nine N-gram models (where N=1, 2, and 3) for the nw, pos, and lem features defined in Table 2. We train these models using 220M words from the Arabic Gigaword 3 corpus (Graff, 2007) which had also been run through MADA 3.0 to extract the pos and lem information. The models are built using the SRI Language Modeling Toolkit (Stolcke, 2002). Each word in a hypothesis can then be assigned a probability by each of these nine models. We reduce these probabilities into one of nine bins, with each successive bin representing an order of magnitude drop in probability (the final bin is re878 served for word N-grams which did not appear in the models). The bin labels are used as the SVM features. Finally, we also use a word confidence (conf) feature, which is aimed at measuring the frequency with which a given word is chosen by the HR system for a given segment scan. The conf is defined here as the ratio of the number of hypotheses in t</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - an Extensible Language Modeling Toolkit. In Proceedings of the International Conference on Spoken Language Processing (ICSLP), volume 2, pages 901–904, Denver, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephanie Strassel</author>
</authors>
<title>Linguistic Resources for Arabic Handwriting Recognition.</title>
<date>2009</date>
<booktitle>In Khalid Choukri and Bente Maegaard, editors, Proceedings of the Second International Conference on Arabic Language Resources and Tools,</booktitle>
<publisher>The MEDAR Consortium.</publisher>
<location>Cairo, Egypt,</location>
<contexts>
<context position="12692" citStr="Strassel, 2009" startWordPosition="2042" endWordPosition="2043">, which cannot be tagged themselves, cause their adjacent words to be marked as PROB instead. In this way, a subsequent HR post-processing system can be alerted to the possibility of a missing word via its surroundings (hence the idea of a problem ‘zone’). Any words not marked as PROB are given an &amp;quot;OK&amp;quot; tag (see the PZD row of Table 1). We describe in Section 5.6 some preliminary experiments we conducted using more fine-grained tags. 4 Experimental Settings 4.1 Training and Evaluation Data The data used in this paper is derived from image scans provided by the Linguistic Data Consortium (LDC) (Strassel, 2009). This data consists of high-resolution (600 dpi) handwriting scans of Arabic text taken from newswire articles, web logs and 3For clarity, we refer to these tags as ‘gold’, whereas the correct segment for a given hypothesis set is called the ‘reference’. � (@/ @/ 877 newsgroups, along with ground truth annotations and word bounding box information. The scans include variations in scribe demographic background, writing instrument, paper and writing speed. The BBN Byblos HR system (Natarajan et al., 2008; Saleem et al., 2009) is then used to process these scanned images into sequences of segmen</context>
</contexts>
<marker>Strassel, 2009</marker>
<rawString>Stephanie Strassel. 2009. Linguistic Resources for Arabic Handwriting Recognition. In Khalid Choukri and Bente Maegaard, editors, Proceedings of the Second International Conference on Arabic Language Resources and Tools, Cairo, Egypt, April. The MEDAR Consortium.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>