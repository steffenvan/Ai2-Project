<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.993938">
Towards Open-Domain Semantic Role Labeling
</title>
<author confidence="0.9573">
Danilo Croce, Cristina Giannone, Paolo Annesi, Roberto Basili
</author>
<affiliation confidence="0.817258666666667">
{croce,giannone,annesi,basili}@info.uniroma2.it
Department of Computer Science, Systems and Production
University of Roma, Tor Vergata
</affiliation>
<sectionHeader confidence="0.988921" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.996522647058824">
Current Semantic Role Labeling technolo-
gies are based on inductive algorithms
trained over large scale repositories of
annotated examples. Frame-based sys-
tems currently make use of the FrameNet
database but fail to show suitable general-
ization capabilities in out-of-domain sce-
narios. In this paper, a state-of-art system
for frame-based SRL is extended through
the encapsulation of a distributional model
of semantic similarity. The resulting argu-
ment classification model promotes a sim-
pler feature space that limits the potential
overfitting effects. The large scale em-
pirical study here discussed confirms that
state-of-art accuracy can be obtained for
out-of-domain evaluations.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999921636363636">
The availability of large scale semantic lexicons,
such as FrameNet (Baker et al., 1998), allowed the
adoption of a wide family of learning paradigms
in the automation of semantic parsing. Building
upon the so called frame semantic model (Fill-
more, 1985), the Berkeley FrameNet project has
developed a semantic lexicon for the core vocab-
ulary of English, since 1997. A frame is evoked
in texts through the occurrence of its lexical units
(LU), i.e. predicate words such verbs, nouns, or
adjectives, and specifies the participants and prop-
erties of the situation it describes, the so called
frame elements (FEs).
Semantic Role Labeling (SRL) is the task of
automatic recognition of individual predicates to-
gether with their major roles (e.g. frame ele-
ments) as they are grammatically realized in in-
put sentences. It has been a popular task since
the availability of the PropBank and FrameNet an-
notated corpora (Palmer et al., 2005), the seminal
work of (Gildea and Jurafsky, 2002) and the suc-
cessful CoNLL evaluation campaigns (Carreras
and M`arquez, 2005). Statistical machine learning
methods, ranging from joint probabilistic models
to support vector machines, have been success-
fully adopted to provide very accurate semantic
labeling, e.g. (Carreras and M`arquez, 2005).
SRL based on FrameNet is thus not a novel task,
although very few systems are known capable of
completing a general frame-based annotation pro-
cess over raw texts, noticeable exceptions being
discussed for example in (Erk and Pado, 2006),
(Johansson and Nugues, 2008b) and (Coppola et
al., 2009). Some critical limitations have been out-
lined in literature, some of them independent from
the underlying semantic paradigm.
Parsing Accuracy. Most of the employed
learning algorithms are based on complex sets of
syntagmatic features, as deeply investigated in (Jo-
hansson and Nugues, 2008b). The resulting recog-
nition is thus highly dependent on the accuracy of
the underlying parser, whereas wrong structures
returned by the parser usually imply large misclas-
sification errors.
Annotation costs. Statistical learning ap-
proaches applied to SRL are very demanding with
respect to the amount and quality of the train-
ing material. The complex SRL architectures
proposed (usually combining local and global,
i.e. joint, models of argument classification, e.g.
(Toutanova et al., 2008)) require a large number
of annotated examples. The amount and quality of
the training data required to reach a significant ac-
curacy is a serious limitation to the exploitation of
SRL in many NLP applications.
</bodyText>
<subsectionHeader confidence="0.503059">
Limited Linguistic Generalization. Several
</subsectionHeader>
<bodyText confidence="0.9999074">
studies showed that even when large training
sets exist the corresponding learning exhibits
poor generalization power. Most of the CoNLL
2005 systems show a significant performance drop
when the tested corpus, i.e. Brown, differs from
</bodyText>
<page confidence="0.955816">
237
</page>
<note confidence="0.943199">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 237–246,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999836460317461">
the training one (i.e. Wall Street Journal), e.g.
(Toutanova et al., 2008). More recently, the state-
of-art frame-based semantic role labeling system
discussed in (Johansson and Nugues, 2008b) re-
ports a 19% drop in accuracy for the argument
classification task when a different test domain is
targeted (i.e. NTI corpus). Out-of-domain tests
seem to suggest the models trained on BNC do not
generalize well to novel grammatical and lexical
phenomena. As also suggested in (Pradhan et al.,
2008), the major drawback is the poor generaliza-
tion power affecting lexical features. Notice how
this is also a general problem of statistical learning
processes, as large fine grain feature sets are more
exposed to the risks of overfitting.
The above problems are particularly critical
for frame-based shallow semantic parsing where,
as opposed to more syntactic-oriented semantic
labeling schemes (as Propbank (Palmer et al.,
2005)), a significant mismatch exists between the
semantic descriptors and the underlying syntac-
tic annotation level. In (Johansson and Nugues,
2008b) an upper bound of about 83.9% for the ac-
curacy of the argument identification task is re-
ported, it is due to the complexity in projecting
frame element boundaries out from the depen-
dency graph: more than 16% of the roles in the
annotated material lack of a clear grammatical sta-
tus.
The limited level of linguistic generalization
outlined above is still an open research problem.
Existing solutions have been proposed in litera-
ture along different lines. Learning from richer
linguistic descriptions of more complex structures
is proposed in (Toutanova et al., 2008). Limit-
ing the cost required for developing large domain-
specific training data sets has been also studied,
e.g., (F¨urstenau and Lapata, 2009). Finally, the ap-
plication of semi-supervised learning is attempted
to increase the lexical expressiveness of the model,
e.g. (Goldberg and Elhadad, 2009).
In this paper, this last direction is pursued. A
semi-supervised statistical model exploiting use-
ful lexical information from unlabeled corpora is
proposed. The model adopts a simple feature
space by relying on a limited set of grammati-
cal properties, thus reducing its learning capac-
ity. Moreover, it generalizes lexical information
about the annotated examples by applying a ge-
ometrical model, in a Latent Semantic Analysis
style, inspired by a distributional paradigm (Pado
and Lapata, 2007). As we will see, the accu-
racy reachable through a restricted feature space is
still quite close to the state-of-art, but interestingly
the performance drops in out-of-domain tests are
avoided.
In the following, after discussing existing ap-
proaches to SRL (Section 2), a distributional ap-
proach is defined in Section 3. Section 3.2 dis-
cusses the proposed HMM-based treatment of
joint inferences in argument classification. The
large scale experiments described in Section 4 will
allow to draw the conclusions of Section 5.
</bodyText>
<sectionHeader confidence="0.999875" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999955432432432">
State-of-art approaches to frame-based SRL are
based on Support Vector Machines, trained over
linear models of syntactic features, e.g. (Jo-
hansson and Nugues, 2008b), or tree-kernels, e.g.
(Coppola et al., 2009). SRL proceeds through two
main steps: the localization of arguments in a sen-
tence, called boundary detection (BD), and the as-
signment of the proper role to the detected con-
stituents, that is the argument classification, (AC)
step. In (Toutanova et al., 2008) a SRL model
over Propbank that effectively exploits the seman-
tic argument frame as a joint structure, is pre-
sented. It incorporates strong dependencies within
a comprehensive statistical joint model with a rich
set of features over multiple argument phrases.
This approach effectively introduces a new step
in SRL, also called Joint Re-ranking, (RR), e.g.
(Toutanova et al., 2008) or (Moschitti et al., 2008).
First local models are applied to produce role
labels over individual arguments, then the joint
model is used to decide the entire argument se-
quence among the set of the n-best competing
solutions. While these approaches increase the
expressive power of the models to capture more
general linguistic properties, they rely on com-
plex feature sets, are more demanding about the
amount of training information and increase the
overall exposure to overfitting effects.
In (Johansson and Nugues, 2008b) the impact of
different grammatical representations on the task
of frame-based shallow semantic parsing is stud-
ied and the poor lexical generalization problem
is outlined. An argument classification accuracy
of 89.9% over the FrameNet (i.e. BNC) dataset
is shown to decrease to 71.1% when a different
test domain is evaluated (i.e. the Nuclear Threat
Initiative corpus). The argument classification
</bodyText>
<page confidence="0.995461">
238
</page>
<bodyText confidence="0.99998575">
component is thus shown to be heavily domain-
dependent whereas the inclusion of grammatical
function features is just able to mitigate this sen-
sitivity. In line with (Pradhan et al., 2008), it is
suggested that lexical features are domain specific
and their suitable generalization is not achieved.
The lack of suitable lexical information is also
discussed in (F¨urstenau and Lapata, 2009) through
an approach aiming to support the creation of
novel annotated resources. Accordingly a semi-
supervised approach for reducing the costs of the
manual annotation effort is proposed. Through a
graph alignment algorithm triggered by annotated
resources, the method acquires training instances
from an unlabeled corpus also for verbs not listed
as existing FrameNet predicates.
</bodyText>
<subsectionHeader confidence="0.992276">
2.1 The role of Lexical Semantic Information
</subsectionHeader>
<bodyText confidence="0.999989130434783">
It is widely accepted that lexical information (as
features directly derived from word forms) is cru-
cial for training accurate systems in a number of
NLP tasks. Indeed, all the best systems in the
CoNLL shared task competitions (e.g. Chunk-
ing (Tjong Kim Sang and Buchholz, 2000)) make
extensive use of lexical information. Also lexi-
cal features are beneficial in SRL usually either
for systems on Propbank as well as for FrameNet-
based annotation.
In (Goldberg and Elhadad, 2009), a different
strategy to incorporate lexical features into clas-
sification models is proposed. A more expres-
sive training algorithm (i.e. anchored SVM) cou-
pled with an aggressive feature pruning strategy
is shown to achieve high accuracy over a chunk-
ing and named entity recognition task. The sug-
gested perspective here is that effective semantic
knowledge can be collected from sources exter-
nal to the annotated corpora (very large unanno-
tated corpora or on manually constructed lexical
resources) rather than learned from the raw lexi-
cal counts of the annotated corpus. Notice how
this is also the strategy pursued in recent work on
deep learning approaches to NLP tasks. In (Col-
lobert and Weston, 2008) a unified architecture
for Natural Language Processing that learns fea-
tures relevant to the tasks at hand given very lim-
ited prior knowledge is presented. It embodies the
idea that a multitask learning architecture coupled
with semi-supervised learning can be effectively
applied even to complex linguistic tasks such as
SRL. In particular, (Collobert and Weston, 2008)
proposes an embedding of lexical information us-
ing Wikipedia as source, and exploiting the result-
ing language model within the multitask learning
process. The idea of (Collobert and Weston, 2008)
to obtain an embedding of lexical information by
acquiring a language model from unlabeled data is
an interesting approach to the problem of perfor-
mance degradation in out-of-domain tests, as al-
ready pursued by (Deschacht and Moens, 2009).
The extensive use of unlabeled texts allows to
achieve a significant level of lexical generalization
that seems better capitalize the smaller annotated
data sets.
</bodyText>
<sectionHeader confidence="0.987777" genericHeader="method">
3 A Distributional Model for Argument
Classification
</sectionHeader>
<bodyText confidence="0.97548575">
High quality lexical information is crucial for ro-
bust open-domain SRL, as semantic generaliza-
tion highly depends on lexical information. For
example, the following two sentences evoke the
STATEMENT frame, through the LUs say and
state, where the FEs, SPEAKER and MEDIUM, are
shown.
[President Kennedy] SPEAKER said to an astronaut, ”Man
is still the most extraordinary computer of all.” (1)
[The report] MEDIUM stated, that some problems needed
to be solved. (2)
In sentence (1), for example, President Kennedy
is the grammatical subject of the verb say and
this justifies its role of SPEAKER. However, syn-
tax does not entirely characterize argument seman-
tics. In (1) and (2), the same syntactic relation is
observed. It is the semantics of the grammatical
heads, i.e. report and Kennedy, the main respon-
sible for the difference between the two resulting
proto-agentive roles, SPEAKER and MEDIUM.
In this work we explore two different aspects.
First, we propose a model that does not depend
on complex syntactic information in order to min-
imize the risk of overfitting. Second, we improve
the lexical semantic information available to the
learning algorithm. The proposed ”minimalistic”
approach will consider only two independent fea-
tures:
</bodyText>
<listItem confidence="0.986312">
• the semantic head (h) of a role, as it can
be observed in the grammatical structure. In
sentence (2), for example, the MEDIUM FE is
realized as the logical subject, whose head is
report.
</listItem>
<page confidence="0.970711">
239
</page>
<bodyText confidence="0.983169">
• the dependency relation (r) connecting the
semantic head to the predicate words. In (2),
the semantic head report is connected to the
LU stated through the subject (SBJ) relation.
In the rest of the section the distributional model
for the argument classification step is presented.
A lexicalized model for individual semantic roles
is first defined in order to achieve robust seman-
tic classification local to each argument. Then a
Hidden Markov Model is introduced in order to
exploit the local probability estimators, sensitive
to lexical similarity, as well as the global informa-
tion on the entire argument sequence.
</bodyText>
<subsectionHeader confidence="0.997184">
3.1 Distributional Local Models
</subsectionHeader>
<bodyText confidence="0.996415696969697">
As the classification of semantic roles is strictly
related to the lexical meaning of argument heads,
we adopt a distributional perspective, where the
meaning is described by the set of textual con-
texts in which words appear. In distributional
models, words are thus represented through vec-
tors built over these observable contexts: similar
vectors suggest semantic relatedness as a func-
tion of the distance between two words, capturing
paradigmatic (e.g. synonymy) or syntagmatic re-
lations (Pado, 2007). Vectors h are described by
an adjacency matrix M, whose rows describe tar-
get words (h) and whose columns describe their
corpus contexts. Latent Semantic Analysis (LSA)
(Landauer and Dumais, 1997), is then applied to
��
M to acquire meaningful representations h . LSA
exploits the linear transformation called Singular
Value Decomposition (SVD) and produces an ap-
proximation of the original matrix M, capturing
(semantic) dependencies between context vectors.
M is replaced by a lower dimensional matrix Ml,
capturing the same statistical information in a new
l-dimensional space, where each dimension is a
linear combination of some of the original fea-
tures (i.e. contexts). These derived features may
be thought as artificial concepts, each one repre-
senting an emerging meaning component, as the
linear combination of many different words.
In the argument classification task, the similar-
ity between two argument heads h1 and h2 ob-
�� h1 and
notated examples as they are immerse in the LSA
</bodyText>
<table confidence="0.998637555555556">
Role, FEk Clusters of semantic heads
MEDIUM c1: {article, report, statement}
c2: {constitution, decree, rule}
SPEAKER c3: {brother, father, mother, sister }
c4: {biographer, philosopher, ....}
c5: {he, she, we, you}
cs: {friend}
TOPIC c7: {privilege, unresponsiveness}
ca: {pattern}
</table>
<tableCaption confidence="0.99689">
Table 1: Clusters of semantic heads in the Subj
</tableCaption>
<bodyText confidence="0.984896">
position for the frame STATEMENT with σ = 0.5
space acquired from the unlabeled texts. More-
over, given FEk, a model for each individual syn-
tactic relation r (i.e. that links h labeled as FEk
to their corresponding predicates) is a partition of
the set HFEk called HF Ek
r , i.e. the subset of
HFEk produced by examples of the relation r (e.g.
Subj). Given the annotated sentence (2), we have
that report E HMEDIUM
SBJ .
As the LSA vectors h are available for the se-
mantic heads h, a vector representation
the role FEk can be obtained from the annotated
data. However, one single vector is a too simplis-
tic representation given the rich nature of seman-
tic roles FEk. In order to better represent FEk,
multiple regions in the semantic space are used.
They are obtained by a clustering process applied
to the set HF Ek
r according to the Quality Thresh-
old (QT) algorithm (Heyer et al., 1999). QT is a
generalization of k-mean where a variable number
of clusters can be obtained. This number depends
on the minimal value of intra-cluster similarity ac-
cepted by the algorithm and controlled by a pa-
rameter, σ: lower values of σ correspond to more
heterogeneous (i.e. larger grain) clusters, while
values close to 1 characterize stricter policies and
more fine-grained results. Given a syntactic rela-
tion r, CF Ek
r denotes the clusters derived by QT
clustering over HF Ek
r . Each cluster c E CF Ek
</bodyText>
<equation confidence="0.598586">
r
</equation>
<bodyText confidence="0.998575111111111">
is represented by a vector -C-+ , computed as the
geometric centroid of its semantic heads h E c.
For a frame F, clusters define a geometric model
of every frame elements FEk: it consists of cen-
troids c with c C_ HF Ek
r . Each c represents FEk
through a set of similar heads, as role fillers ob-
served in FrameNet. Table 1 represents clusters
for the heads HF Ek
Subj of the STATEMENT frame.
In argument classification we assume that the
evoking predicate word for the frame F in an
input sentence s is known. A sentence s can
be seen as a sequence of role-relation pairs:
served in FrameNet can be computed over
�� h2. The model for a given frame element FEk
is built around the semantic heads h observed in
the role FEk: they form a set denoted by HFEk.
</bodyText>
<figure confidence="0.430982">
��
These LSA vectors h express the individual an-
F Ek for
���
240
s = {(r1, h1), ..., (rn, hn)} where the heads hi
</figure>
<bodyText confidence="0.8879138">
are in the syntactic relation ri with the underlying
lexical unit of F.
For every head h in s, the vector �� h can be then
used to estimate its similarity with the different
candidate roles FEk. Given the syntactic relation
r, the clusters c E CF Ek
r whose centroid vector c�
�
is closer to h are selected. Dr,h is the set of the
representations semantically related to h:
</bodyText>
<equation confidence="0.990475">
�Dr,h = {ckj E CF Ek
r |sim(h, ckj) &gt; T} (3)
k
</equation>
<bodyText confidence="0.957587">
where the similarity between the j-th cluster for
the FEk, i.e. ckj E CF Ek
r , and h is the usual
</bodyText>
<equation confidence="0.520755">
�� h ·C kj
11h1111&apos;c kj11
</equation>
<bodyText confidence="0.973664333333333">
Then, through a k-nearest neighbours (k-NN)
strategy within Dr,h, the m clusters ckj most simi-
lar to h are retained in the set D(m)
r,h . A probabilis-
tic preference for the role FEk is estimated for h
through a cluster-based voting scheme,
</bodyText>
<equation confidence="0.9337366">
Ek (m)
prob(FEk I CrF l r, h) = D( )Ir,h (4)
r,h |
or, alternatively, an instance-based one over D(m)
r,h :
</equation>
<bodyText confidence="0.999740238095238">
In Fig. 1 the preference estimation for the
incoming head h = professor connected to
a LU by the Subj relation is shown. Clus-
ters for the heads in Table 1 are also reported.
First, in the set of clusters whose similarity
with professor is higher than a threshold T the
m = 5 most similar clusters are selected. Ac-
cordingly, the preferences given by Eq. 4 are
prob(SPEAKERISBJ, h) = 3/5, prob(MEDIUMISBJ, h) =
2/5 and prob(TOPICISBJ, h) = 0. The strategy mod-
eled by Eq. 5 amplifies the role of larger
clusters, e.g. prob(SPEAKERISBJ, h) = 9/14 and
prob(MEDIUMISBJ, h) = 5/14. We call Distribu-
tional, the model that applies Eq. 5 to the source
(r, h) arguments, by rejecting cases only when no
information about the head h is available from the
unlabeled corpus or no example of relation r for
the role FEk is available from the annotated cor-
pus. Eq. 4 and 5 in fact do not cover all possible
cases. Often the incoming head h or the relation r
may be unavailable:
</bodyText>
<listItem confidence="0.9511043">
1. If the head h has never been met in the un-
labeled corpus or the high grammatical am-
biguity of the sentence does not allow to
locate it reliably, Eq. 4 (or 5) should be
backed off to a purely syntactic model, that
is prob(FEk|r)
2. If the relation r can not be properly located
in s, h is also unknown: the prior probability
of individual arguments, i.e. prob(FEk), is
here employed.
</listItem>
<bodyText confidence="0.8736792">
Both prob(FEk|r) and prob(FEk) can be esti-
mated from the training set and smoothing can be
also applied1. A more robust argument preference
function for all arguments (ri, hi) E s of the frame
F is thus given by:
</bodyText>
<equation confidence="0.9987955">
prob(FEk|ri, hi) = A1prob(FEk|ri, hi) +
A2prob(FEk|ri) + A3prob(FEk) (6)
</equation>
<bodyText confidence="0.999913833333333">
where weights A1, A2, A3 can be heuristically as-
signed or estimated from the training set2. The
resulting model is hereafter called Backoff model:
although simply based on a single feature (i.e. the
syntactic relation r), it accounts for information at
different reliability degrees.
</bodyText>
<subsectionHeader confidence="0.8712005">
3.2 A Joint Model for Argument
Classification
</subsectionHeader>
<bodyText confidence="0.996282736842105">
Eq. 6 defines roles preferences local to individual
arguments (ri, hi). However, an argument frame
is a joint structure, with strong dependencies be-
tween arguments. We thus propose to model the
reranking phase (RR) as a HMM sequence label-
ing task. It defines a stochastic inference over
multiple (locally justified) alternative sequences
through a Hidden Markov Model (HMM). It in-
fers the best sequence FE(k1,...,kn) over all the
possible hidden state sequences (i.e. made by the
target FEki) given the observable emissions, i.e.
the arguments (ri, hi). Viterbi inference is applied
to build the best (role) interpretation for the input
sentence.
Once Eq. 6 is available, the best frame element
sequence FE(θ(1),...,θ(n)) for the entire sentence s
can be selected by defining the function B(·) that
maps arguments (ri, hi) E s to frame elements
FEk:
</bodyText>
<equation confidence="0.940721">
B(i) = k s.t. FEk E F (7)
</equation>
<footnote confidence="0.98737225">
1Lindstone smoothing was applied with S = 1.
2In each test discussed hereafter, A1, A2, A3 were assigned
to .9,.09 and .01, in order to impose a strict priority to the
model contributions.
</footnote>
<figure confidence="0.603632285714286">
cosine similarity: simcos(h, ckj) =
EcECr Ek nD(m)
r,h |c|
EcED( h)
prob(FEk|r, h) =
(5)
|c|
</figure>
<page confidence="0.886197">
241
</page>
<figureCaption confidence="0.999308">
Figure 1: A k-NN approach to the role classification for hi = professor
</figureCaption>
<figure confidence="0.998920935483871">
decree rule
constitution
brother
sister
manifesto
report
review
article
statement
survey
you
she
philosopher
professor
translator
unresponsiveness
pattern
we
he
father
biographer
archaeologist
mother
friend
SPEAKER
TOPIC
MEDIUM
target head
president
king
privilege
</figure>
<bodyText confidence="0.994269">
Notice that different transfer functions 0(·)
are usually possible. By computing their prob-
ability we can solve the SRL task by select-
ing the most likely interpretation, 0(·), via
</bodyText>
<equation confidence="0.872215333333333">
argmaxθ P(0(·)  |s), as follows:
�0(·) = argmax P(s|0(·))P(0(·)) (8)
θ
</equation>
<bodyText confidence="0.997146666666667">
In Eq. 8, the emission probability P(s|0(·)) and
the transition probability P (0(·)) are explicit. No-
tice that the emission probability corresponds to
an argument interpretation (e.g. Eq. 5) and it is
assigned independently from the rest of the sen-
tence. On the other hand, transition probabilities
model role sequences and support the expectations
about argument frames of a sentence.
The emission probability is approximated as:
</bodyText>
<equation confidence="0.976633333333333">
n
P(s  |0(1) ... 0(n))≈ P(ri7 hi  |FEθ(i))
i=1
</equation>
<listItem confidence="0.984471833333333">
as it is made independent from previous states in
a Viterbi path. Again the emission probability can
be rewritten as:
Since P(ri7 hi) does not depend on the role la-
beling, maximizing Eq. 10 corresponds to maxi-
mize:
</listItem>
<equation confidence="0.960578111111111">
P(FEθ(i)|ri7 hi) (11)
P(FEθ(i))
whereas P(FEθ(i)|ri7 hi) is thus estimated
through Eq. 6.
The transition probability, estimated through
P(0(1) ... 0(n))≈
n
� P(FEθ(i)|FEθ(i−1)7 FEθ(i−2)) (12)
i=1
</equation>
<bodyText confidence="0.950913">
accounts FEs sequence via a 3-gram model3 .
</bodyText>
<sectionHeader confidence="0.997495" genericHeader="method">
4 Empirical Analysis
</sectionHeader>
<bodyText confidence="0.999751421052632">
The aim of the evaluation is to measure the reach-
able accuracy of the simple model proposed and
to compare its impact over in-domain and out-of-
domain semantic role labeling tasks. In particular,
we will evaluate the argument classification (AC)
task in Section 4.2.
Experimental Set-Up. The in-domain test has
been run over the FrameNet annotated corpus, de-
rived from the British National Corpus (BNC).
The splitting between train and test set is 90%-
10% according to the same data set of (Johans-
son and Nugues, 2008b). In all experiments,
the FrameNet 1.3 version and the dependency-
based system using the LTH parser (Johansson
and Nugues, 2008a) have been employed. Out-
of-domain tests are run over the two training cor-
pora as made available by the Semeval 2007 Task
194 (Baker et al., 2007): the Nuclear Threat Ini-
tiative (NTI) and the American National Corpus
</bodyText>
<footnote confidence="0.9970354">
3Two empty states are added at the beginning of any se-
quence. Moreover, Laplace smoothing was also applied to
each estimator.
4The NTI and ANC annotated collections are download-
able at:
</footnote>
<equation confidence="0.844378333333333">
nlp.cs.swarthmore.edu/semeval/tasks/task19/data/train.tar.gz
P(ri7 hi|FEθ(i)) = P(FEθ(i))
P(FEθ(i)|ri7 hi) P(ri7 hi)
</equation>
<page confidence="0.995858">
242
</page>
<table confidence="0.9993">
Corpus Predicates Arguments
training FN-BNC 134,697 271,560
test
in-domain FN-BNC 14,952 30,173
out-of-domain NTI 8,208 14,422
ANC 760 1,389
</table>
<tableCaption confidence="0.99955">
Table 2: Training and Testing data sets
</tableCaption>
<bodyText confidence="0.971518153846154">
(ANC)5. Table 2 shows the predicates and argu-
ments in each data set. All null-instantiated ar-
guments were removed from the training and test
sets.
�
Vectors h representing semantic heads have
been computed according to the ”dependency-
based” vector space discussed in (Pado and La-
pata, 2007). The entire BNC corpus has been
parsed and the dependency graphs derived from
individual sentences provided the basic observ-
able contexts: every co-occurrence is thus syntac-
tically justified by a dependency arc. The most
frequent 30,000 basic features, i.e. (syntactic re-
lation,lemma) pairs, have been used to build the
matrix M, vector components corresponding to
point-wise mutual information scores. Finally, the
final space is obtained by applying the SVD reduc-
tion over M, with a dimensionality cut of l = 250.
In the evaluation of the AC task, accuracy is
computed over the nodes of the dependency graph,
in line with (Johansson and Nugues, 2008b) or
(Coppola et al., 2009). Accordingly, also recall,
precision and F-measure are reported on a per
node basis, against the binary BD task or for the
full BD + AC chain.
</bodyText>
<subsectionHeader confidence="0.999069">
4.1 The Role of Lexical Clustering
</subsectionHeader>
<bodyText confidence="0.9998355">
The first study aims at detecting the impact of dif-
ferent clustering policies on the resulting AC ac-
curacy. Clustering, as discussed in Section 3.1,
allows to generalize lexical information: similar
heads within the latent semantic space are built
from the annotated examples and they allow to
predict the behavior of new unseen words as found
in the test sentences. The system performances
have been here measured under different cluster-
ing conditions, i.e. grains at which the clustering
of annotated examples is applied. This grain is de-
termined by the parameter Q of the applied Quality
Threshold algorithm (Heyer et al., 1999). Notice
that small values of Q imply large clusters, while if
</bodyText>
<footnote confidence="0.8066275">
5Sentences whose arguments were not represented in the
FrameNet training material were removed from all tests.
</footnote>
<table confidence="0.9745342">
Eq. -a Frames with a number of annotated examples
&gt;0 &gt;100 &gt;500 &gt;1K &gt;3K &gt;5K
(5) - .85 86.3 86.5 87.2 88.3 85.9 82.0
(4) - .5 85.1 85.5 85.8 87.2 83.5 79.4
(4) - .1 84.5 84.8 85.1 86.5 83.0 78.7
</table>
<tableCaption confidence="0.7454055">
Table 3: Accuracy on Arg classification tasks wrt
different clustering policies
</tableCaption>
<bodyText confidence="0.999763275862069">
Q pz� 1 then many singleton clusters are promoted
(i.e. one cluster for each example). By varying the
threshold Q we thus account for prototype-based
as well exemplar-based strategies, as discussed in
(Erk, 2009).
We measured the performance on the argument
classification tasks of different models obtained by
combing different choices of Q with Eq. (4) or (5).
Results are reported in Table 3. The leftmost col-
umn reports the different clustering settings, while
in the remaining columns we see performances
over test sentences related to different frames: we
selected frames for which an increasing number of
annotated examples are available: from all frames
(for more than 0 examples) to the only frame (i.e.
SELF MOTION) that has more than 5,000 exam-
ples in our training data set.
The reported accuracies suggest that Eq. (5),
promoting an example driven strategy, better cap-
tures the role preference, as it always outperforms
alternative settings (i.e. more prototype oriented
methods). It limits overgeneralization and pro-
motes fine grained clusters. An interesting result is
that a per-node accuracy of 86.3 (i.e. only 3 points
under the state of-the art on the same data set,
(Johansson and Nugues, 2008b)) is achieved. All
the remaining tests have been run with the clus-
tering configuration characterized by Eq. (5) and
Q = 0.85.
</bodyText>
<subsectionHeader confidence="0.996663">
4.2 Argument Classification Accuracy
</subsectionHeader>
<bodyText confidence="0.99998875">
In these experiments we evaluate the quality of
the argument classification step against the lexi-
cal knowledge acquired from unlabeled texts and
the reranking step. The accuracy reachable on the
gold standard argument boundaries has been com-
pared across several experimental settings. Two
baseline systems have been obtained. The Local
Prior model outputs the sequence that maximizes
the prior probability locally to individual argu-
ments. The Global Prior model is obtained by ap-
plying re-ranking (Section 3.2) to the best n = 10
candidates provided by the Local Prior model. Fi-
</bodyText>
<page confidence="0.996785">
243
</page>
<table confidence="0.999597857142857">
Model FN-BNC NTI ANC
Local Prior 43.9 50.9 50.4
Global Prior 67.7 (+54.2%) 75.9 (+49.0%) 68.8 (+36.4%)
Distributional 81.1 (+19.8%) 82.3 (+8.4%) 69.7 (+1.3%)
Backoff 84.6 (+4.3%) 87.2 (+6.0%) 76.2 (+9.3%)
Backoff+HMMRR 86.3 (+2.0%) 90.5 (+3.8%) 79.9 (+5.0%)
(Johansson&amp;Nugues, 2008) 89.9 71.1 -
</table>
<tableCaption confidence="0.8416665">
Table 4: Accuracy of the Argument Classification task over the different corpora. In parenthesis the
relative increment with respect to the immediately simpler model, previous row
</tableCaption>
<bodyText confidence="0.999609777777778">
nally, the application of the backoff strategies (as
in Eq. 6) and the HMM-based reranking character-
ize the final two configurations. Table 4 reports the
accuracy results obtained over the three corpora
(defined as in Table 2): the accuracy scores are av-
eraged over different values of m in Eq. 5, ranging
from 3 to 30. In the in-domain scenario, i.e. the
FN-BNC dataset reported in column 2, it is worth
noticing that the proposed model, with backoff and
global reranking, is quite effective with respect to
the state-of-the-art.
Although results on the FN-BNC do not outper-
form the state-of-the-art for the FrameNet corpus,
we still need to study the generalization capabil-
ity of our SRL model in out-of-domain conditions.
In a further experiment, we applied the same sys-
tem, as trained over the FN-BNC data, to the other
corpora, i.e. NTI and ANC, used entirely as test
sets. Results, reported in column 3 and 4 of Ta-
ble 4 and shown in Figure 2, confirm that no ma-
jor drop in performance is observed. Notice how
the positive impact of the backoff models and the
HMM reranking policy is similarly reflected by all
the collections. Moreover, the results on the NTI
corpus are even better than those obtained on the
BNC, with a resulting 90.5% accuracy on the AC
task.
</bodyText>
<figureCaption confidence="0.9707025">
Figure 2: Accuracy of the AC task over different
corpora
</figureCaption>
<subsectionHeader confidence="0.997695">
4.3 Discussion
</subsectionHeader>
<bodyText confidence="0.997789513513513">
The above empirical findings are relevant if com-
pared with the outcome of a similar test on the NTI
collection, discussed in (Johansson and Nugues,
2008b)6. There, under the same training condi-
tions, a performance drop of about -19% is re-
ported (from 89.9 to 71.1%) over gold standard
argument boundaries. The model proposed in this
paper exhibits no such drop in any collection (NTI
and ANC). This seems to confirm the hypothesis
that the model is able to properly generalize the
required lexical information across different do-
mains.
It is interesting to outline that the individual
stages of the proposed model play different roles
in the different domains, as Table 4 suggests. Al-
though the positive contributions of the individual
processing stages are uniformly confirmed, some
differences can be outlined:
• The beneficial impact of the lexical informa-
tion (i.e. the distributional model) applies dif-
ferently across the different domains. The
ANC domain seems not to significantly ben-
efit when the distributional model (Eq. 5) is
applied. Notice how Eq. 5 depends both from
the evidence gathered in the corpus about lex-
ical heads h as well as about the relation r. In
ANC the percentage of times that the Eq. 5 is
backed off against test instances (as h or r are
not available from the training data) is twice
as high as in the BNC-FN or in the NTI do-
main (i.e. 15.5 vs. 7.2 or 8.7, respectively).
The different syntactic style of ANC seems
thus the main responsible of the poor impact
of distributional information, as it is often un-
applicable to ANC test cases.
• The complexity of the three test sets is dif-
ferent, as the three plots show. The NTI col-
</bodyText>
<footnote confidence="0.978441">
6Notice that in this paper only the training portion of the
NTI data set is employed as reported in Table 2 and results are
not directly comparable to (Johansson and Nugues, 2008b).
</footnote>
<figure confidence="0.994366105263158">
100,0%
90,0%
80,0%
60,0%
50,0%
40,0%
70,0%
Local
Prior
Global
Prior
Distributional Backoff Backoff
+HMMRR
FN-BNC
NTI
ANC
90,5%
86,3%
79,9%
</figure>
<page confidence="0.996061">
244
</page>
<bodyText confidence="0.99967002">
lections seems characterized by a lower level
of complexity (see for example the accuracy
of the Local prior model, that is about 51%
as for the ANC). It then gets benefits from
all the analysis stages, in particular the final
HMM reranking. The BNC-FN test collec-
tion seems the most complex one, and the im-
pact of the lexical information brought by the
distributional model is here maximal. This
is mainly due to the coherence between the
distributions of lexical and grammatical phe-
nomena in the test and training data.
• The role of HMM reranking is an effective
way to compensate errors in the local argu-
ment classifications for all the three domains.
However, it is particularly effective for the
outside domain cases, while, in the BNC cor-
pus, it produces just a small improvement in-
stead (i.e. +2%, as shown in Table 4 ). It is
worth noticing that the average length of the
sentences in the BNC test collection is about
23 words per sentence, while it is higher for
the NTI and ANC data sets (i.e. 34 and 31,
respectively). It seems that the HMM model
well captures some information on the global
semantic structure of a sentence: this is help-
ful in cases where errors in the grammati-
cal recognition (of individual arguments or
at sentence level) are more frequent and af-
flict the local distributional model. The more
complex is the syntax of a corpus (e.g. in the
NTI and ANC data sets), the higher seems the
impact of the reranking phase.
The significant performance of the AC model
here presented suggest to test it when integrated
within a full SRL architecture. Table 5 reports the
results of the processing cascade over three col-
lections. Results on the Boundary Detection BD
task are obtained by training an SVM model on
the same feature set presented in (Johansson and
Nugues, 2008b) and are slightly below the state-
of-the art BD accuracy reported in (Coppola et
al., 2009). However, the accuracy of the complete
BD + AC + RR chain (i.e. 68%) improves the
corresponding results of (Coppola et al., 2009).
Given the relatively simple feature set adopted
here, this result is very significant as for its result-
ing efficiency. The overall BD recognition pro-
cess is, on a standard architecture, performed at
about 6.74 sentences per second, that is basically
</bodyText>
<table confidence="0.9797888">
Corpus Eval. Setting Recall Precision F1
BD 72.6 85.1 78.4
BNC
BD+AC+RR 62.6 74.5 68.0
BD 63.9 80.0 71.0
NTI
BD+AC+RR 56.7 72.1 63.5
BD 64.0 81.5 71.7
ANC
BD+AC+RR 47.4 62.5 53.9
</table>
<tableCaption confidence="0.969072">
Table 5: Accuracy of the full cascade of the SRL
system over three domain
</tableCaption>
<bodyText confidence="0.996986666666667">
the same as the time needed for applying the en-
tire BD + AC + RR chain, i.e. 6.21 sentence per
second.
</bodyText>
<sectionHeader confidence="0.999777" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.9999115">
In this paper, a distributional approach for acquir-
ing a semi-supervised model of argument classi-
fication (AC) preferences has been proposed. It
aims at improving the generalization capability of
the inductive SRL approach by reducing the com-
plexity of the employed grammatical features and
through a distributional representation of lexical
features. The obtained results are close to the
state-of-art in FrameNet semantic parsing. State
of the art accuracy is obtained instead in out-of-
domain experiments. The model seems to cap-
italize from simple methods of lexical modeling
(i.e. the estimation of lexico-grammatical pref-
erences through distributional analysis over unla-
beled data), estimation (through syntactic or lexi-
cal back-off where necessary) and reranking. The
result is an accurate and highly portable SRL cas-
cade. Experiments on the integrated SRL archi-
tecture (i.e. BD + AC + RR chain) show that
state-of-art accuracy (i.e. 68%) can be obtained
on raw texts. This result is also very significant
as for the achieved efficiency. The system is able
to apply the entire BD + AC + RR chain at a
speed of 6.21 sentences per second. This signif-
icant efficiency confirms the applicability of the
SRL approach proposed here in large scale NLP
applications. Future work will study the appli-
cation of the flexible SRL method proposed to
other languages, for which less resources are avail-
able and worst training conditions are the norm.
Moreover, dimensionality reduction methods al-
ternative to LSA, as currently studied on semi-
supervised spectral learning (Johnson and Zhang,
2008), will be experimented.
</bodyText>
<page confidence="0.997642">
245
</page>
<sectionHeader confidence="0.998346" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9999211875">
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proc. of
COLING-ACL, Montreal, Canada.
Collin Baker, Michael Ellsworth, and Katrin Erk.
2007. Semeval-2007 task 19: Frame semantic struc-
ture extraction. In Proceedings of SemEval-2007,
pages 99–104, Prague, Czech Republic, June. Asso-
ciation for Computational Linguistics.
Xavier Carreras and Lluis M`arquez. 2005. Intro-
duction to the CoNLL-2005 Shared Task: Seman-
tic Role Labeling. In Proc. of CoNLL-2005, pages
152–164, Ann Arbor, Michigan, June.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: deep
neural networks with multitask learning. In In Pro-
ceedings of ICML ’08, pages 160–167, New York,
NY, USA. ACM.
Bonaventura Coppola, Alessandro Moschitti, and
Giuseppe Riccardi. 2009. Shallow semantic parsing
for spoken language understanding. In Proceedings
of NAACL ’09, pages 85–88, Morristown, NJ, USA.
Koen Deschacht and Marie-Francine Moens. 2009.
Semi-supervised semantic role labeling using the la-
tent words language model. In EMNLP ’09: Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing, pages 21–29,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Katrin Erk and Sebastian Pado. 2006. Shalmaneser -
a flexible toolbox for semantic role assignment. In
Proceedings of LREC 2006, Genoa, Italy.
Katrin Erk. 2009. Representing words as regions
in vector space. In In Proceedings of CoNLL ’09,
pages 57–65, Morristown, NJ, USA. Association for
Computational Linguistics.
Charles J. Fillmore. 1985. Frames and the semantics of
understanding. Quaderni di Semantica, 4(2):222–
254.
Hagen F¨urstenau and Mirella Lapata. 2009. Graph
alignment for semi-supervised semantic role label-
ing. In In Proceedings of EMNLP ’09, pages 11–20,
Morristown, NJ, USA.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
Labeling of Semantic Roles. Computational Lin-
guistics, 28(3):245–288.
Yoav Goldberg and Michael Elhadad. 2009. On the
role of lexical features in sequence labeling. In In
Proceedings of EMNLP ’09, pages 1142–1151, Sin-
gapore, August. Association for Computational Lin-
guistics.
L.J. Heyer, S. Kruglyak, and S. Yooseph. 1999. Ex-
ploring expression data: Identification and analysis
of coexpressed genes. Genome Research, (9):1106–
1115.
Richard Johansson and Pierre Nugues. 2008a.
Dependency-based syntactic-semantic analysis with
propbank and nombank. In Proceedings of CoNLL-
2008, Manchester, UK, August 16-17.
Richard Johansson and Pierre Nugues. 2008b. The
effect of syntactic representation on semantic role
labeling. In Proceedings of COLING, Manchester,
UK, August 18-22.
Rie Johnson and Tong Zhang. 2008. Graph-based
semi-supervised learning and spectral kernel de-
sign. IEEE Transactions on Information Theory,
54(1):275–288.
Tom Landauer and Sue Dumais. 1997. A solution to
plato’s problem: The latent semantic analysis the-
ory of acquisition, induction and representation of
knowledge. Psychological Review, 104.
A. Moschitti, D. Pighin, and R. Basili. 2008. Tree
kernels for semantic role labeling. Computational
Linguistics, 34.
Sebastian Pado and Mirella Lapata. 2007.
Dependency-based construction of semantic
space models. Computational Linguistics, 33(2).
Sebastian Pado. 2007. Cross-Lingual Annotation
Projection Models for Role-Semantic Information.
Ph.D. thesis, Saarland University.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005.
The proposition bank: A corpus annotated with
semantic roles. Computational Linguistics, 31(1),
March.
Sameer S. Pradhan, Wayne Ward, and James H. Mar-
tin. 2008. Towards robust semantic role labeling.
Comput. Linguist., 34(2):289–310.
Erik F. Tjong Kim Sang and Sabine Buchholz. 2000.
Introduction to the conll-2000 shared task: chunk-
ing. In Proceedings of the 2nd workshop on Learn-
ing language in logic and the 4th conference on
Computational natural language learning, pages
127–132, Morristown, NJ, USA. Association for
Computational Linguistics.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2008. A global joint model for semantic
role labeling. Comput. Linguist., 34(2):161–191.
</reference>
<page confidence="0.998728">
246
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.850547">
<title confidence="0.999911">Towards Open-Domain Semantic Role Labeling</title>
<author confidence="0.999568">Danilo Croce</author>
<author confidence="0.999568">Cristina Giannone</author>
<author confidence="0.999568">Paolo Annesi</author>
<author confidence="0.999568">Roberto Basili</author>
<email confidence="0.989038">croce@info.uniroma2.it</email>
<email confidence="0.989038">giannone@info.uniroma2.it</email>
<email confidence="0.989038">annesi@info.uniroma2.it</email>
<email confidence="0.989038">basili@info.uniroma2.it</email>
<affiliation confidence="0.957649">Department of Computer Science, Systems and Production</affiliation>
<address confidence="0.871516">of Roma, Vergata</address>
<abstract confidence="0.999479722222222">Current Semantic Role Labeling technologies are based on inductive algorithms trained over large scale repositories of annotated examples. Frame-based systems currently make use of the FrameNet database but fail to show suitable generalization capabilities in out-of-domain scenarios. In this paper, a state-of-art system for frame-based SRL is extended through the encapsulation of a distributional model of semantic similarity. The resulting argument classification model promotes a simpler feature space that limits the potential overfitting effects. The large scale empirical study here discussed confirms that state-of-art accuracy can be obtained for out-of-domain evaluations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The Berkeley FrameNet project.</title>
<date>1998</date>
<booktitle>In Proc. of COLING-ACL,</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="1036" citStr="Baker et al., 1998" startWordPosition="137" endWordPosition="140"> make use of the FrameNet database but fail to show suitable generalization capabilities in out-of-domain scenarios. In this paper, a state-of-art system for frame-based SRL is extended through the encapsulation of a distributional model of semantic similarity. The resulting argument classification model promotes a simpler feature space that limits the potential overfitting effects. The large scale empirical study here discussed confirms that state-of-art accuracy can be obtained for out-of-domain evaluations. 1 Introduction The availability of large scale semantic lexicons, such as FrameNet (Baker et al., 1998), allowed the adoption of a wide family of learning paradigms in the automation of semantic parsing. Building upon the so called frame semantic model (Fillmore, 1985), the Berkeley FrameNet project has developed a semantic lexicon for the core vocabulary of English, since 1997. A frame is evoked in texts through the occurrence of its lexical units (LU), i.e. predicate words such verbs, nouns, or adjectives, and specifies the participants and properties of the situation it describes, the so called frame elements (FEs). Semantic Role Labeling (SRL) is the task of automatic recognition of individ</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The Berkeley FrameNet project. In Proc. of COLING-ACL, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Collin Baker</author>
<author>Michael Ellsworth</author>
<author>Katrin Erk</author>
</authors>
<title>Semeval-2007 task 19: Frame semantic structure extraction.</title>
<date>2007</date>
<booktitle>In Proceedings of SemEval-2007,</booktitle>
<pages>99--104</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="24274" citStr="Baker et al., 2007" startWordPosition="3927" endWordPosition="3930">tasks. In particular, we will evaluate the argument classification (AC) task in Section 4.2. Experimental Set-Up. The in-domain test has been run over the FrameNet annotated corpus, derived from the British National Corpus (BNC). The splitting between train and test set is 90%- 10% according to the same data set of (Johansson and Nugues, 2008b). In all experiments, the FrameNet 1.3 version and the dependencybased system using the LTH parser (Johansson and Nugues, 2008a) have been employed. Outof-domain tests are run over the two training corpora as made available by the Semeval 2007 Task 194 (Baker et al., 2007): the Nuclear Threat Initiative (NTI) and the American National Corpus 3Two empty states are added at the beginning of any sequence. Moreover, Laplace smoothing was also applied to each estimator. 4The NTI and ANC annotated collections are downloadable at: nlp.cs.swarthmore.edu/semeval/tasks/task19/data/train.tar.gz P(ri7 hi|FEθ(i)) = P(FEθ(i)) P(FEθ(i)|ri7 hi) P(ri7 hi) 242 Corpus Predicates Arguments training FN-BNC 134,697 271,560 test in-domain FN-BNC 14,952 30,173 out-of-domain NTI 8,208 14,422 ANC 760 1,389 Table 2: Training and Testing data sets (ANC)5. Table 2 shows the predicates and </context>
</contexts>
<marker>Baker, Ellsworth, Erk, 2007</marker>
<rawString>Collin Baker, Michael Ellsworth, and Katrin Erk. 2007. Semeval-2007 task 19: Frame semantic structure extraction. In Proceedings of SemEval-2007, pages 99–104, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Lluis M`arquez</author>
</authors>
<title>Introduction to the CoNLL-2005 Shared Task: Semantic Role Labeling.</title>
<date>2005</date>
<booktitle>In Proc. of CoNLL-2005,</booktitle>
<pages>152--164</pages>
<location>Ann Arbor, Michigan,</location>
<marker>Carreras, M`arquez, 2005</marker>
<rawString>Xavier Carreras and Lluis M`arquez. 2005. Introduction to the CoNLL-2005 Shared Task: Semantic Role Labeling. In Proc. of CoNLL-2005, pages 152–164, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In In Proceedings of ICML ’08,</booktitle>
<pages>160--167</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="10719" citStr="Collobert and Weston, 2008" startWordPosition="1636" endWordPosition="1640">els is proposed. A more expressive training algorithm (i.e. anchored SVM) coupled with an aggressive feature pruning strategy is shown to achieve high accuracy over a chunking and named entity recognition task. The suggested perspective here is that effective semantic knowledge can be collected from sources external to the annotated corpora (very large unannotated corpora or on manually constructed lexical resources) rather than learned from the raw lexical counts of the annotated corpus. Notice how this is also the strategy pursued in recent work on deep learning approaches to NLP tasks. In (Collobert and Weston, 2008) a unified architecture for Natural Language Processing that learns features relevant to the tasks at hand given very limited prior knowledge is presented. It embodies the idea that a multitask learning architecture coupled with semi-supervised learning can be effectively applied even to complex linguistic tasks such as SRL. In particular, (Collobert and Weston, 2008) proposes an embedding of lexical information using Wikipedia as source, and exploiting the resulting language model within the multitask learning process. The idea of (Collobert and Weston, 2008) to obtain an embedding of lexical</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: deep neural networks with multitask learning. In In Proceedings of ICML ’08, pages 160–167, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonaventura Coppola</author>
<author>Alessandro Moschitti</author>
<author>Giuseppe Riccardi</author>
</authors>
<title>Shallow semantic parsing for spoken language understanding.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL ’09,</booktitle>
<pages>85--88</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="2516" citStr="Coppola et al., 2009" startWordPosition="371" endWordPosition="374">k of (Gildea and Jurafsky, 2002) and the successful CoNLL evaluation campaigns (Carreras and M`arquez, 2005). Statistical machine learning methods, ranging from joint probabilistic models to support vector machines, have been successfully adopted to provide very accurate semantic labeling, e.g. (Carreras and M`arquez, 2005). SRL based on FrameNet is thus not a novel task, although very few systems are known capable of completing a general frame-based annotation process over raw texts, noticeable exceptions being discussed for example in (Erk and Pado, 2006), (Johansson and Nugues, 2008b) and (Coppola et al., 2009). Some critical limitations have been outlined in literature, some of them independent from the underlying semantic paradigm. Parsing Accuracy. Most of the employed learning algorithms are based on complex sets of syntagmatic features, as deeply investigated in (Johansson and Nugues, 2008b). The resulting recognition is thus highly dependent on the accuracy of the underlying parser, whereas wrong structures returned by the parser usually imply large misclassification errors. Annotation costs. Statistical learning approaches applied to SRL are very demanding with respect to the amount and quali</context>
<context position="7150" citStr="Coppola et al., 2009" startWordPosition="1076" endWordPosition="1079">stingly the performance drops in out-of-domain tests are avoided. In the following, after discussing existing approaches to SRL (Section 2), a distributional approach is defined in Section 3. Section 3.2 discusses the proposed HMM-based treatment of joint inferences in argument classification. The large scale experiments described in Section 4 will allow to draw the conclusions of Section 5. 2 Related Work State-of-art approaches to frame-based SRL are based on Support Vector Machines, trained over linear models of syntactic features, e.g. (Johansson and Nugues, 2008b), or tree-kernels, e.g. (Coppola et al., 2009). SRL proceeds through two main steps: the localization of arguments in a sentence, called boundary detection (BD), and the assignment of the proper role to the detected constituents, that is the argument classification, (AC) step. In (Toutanova et al., 2008) a SRL model over Propbank that effectively exploits the semantic argument frame as a joint structure, is presented. It incorporates strong dependencies within a comprehensive statistical joint model with a rich set of features over multiple argument phrases. This approach effectively introduces a new step in SRL, also called Joint Re-rank</context>
<context position="25803" citStr="Coppola et al., 2009" startWordPosition="4163" endWordPosition="4166">graphs derived from individual sentences provided the basic observable contexts: every co-occurrence is thus syntactically justified by a dependency arc. The most frequent 30,000 basic features, i.e. (syntactic relation,lemma) pairs, have been used to build the matrix M, vector components corresponding to point-wise mutual information scores. Finally, the final space is obtained by applying the SVD reduction over M, with a dimensionality cut of l = 250. In the evaluation of the AC task, accuracy is computed over the nodes of the dependency graph, in line with (Johansson and Nugues, 2008b) or (Coppola et al., 2009). Accordingly, also recall, precision and F-measure are reported on a per node basis, against the binary BD task or for the full BD + AC chain. 4.1 The Role of Lexical Clustering The first study aims at detecting the impact of different clustering policies on the resulting AC accuracy. Clustering, as discussed in Section 3.1, allows to generalize lexical information: similar heads within the latent semantic space are built from the annotated examples and they allow to predict the behavior of new unseen words as found in the test sentences. The system performances have been here measured under </context>
<context position="34709" citStr="Coppola et al., 2009" startWordPosition="5665" endWordPosition="5668">uent and afflict the local distributional model. The more complex is the syntax of a corpus (e.g. in the NTI and ANC data sets), the higher seems the impact of the reranking phase. The significant performance of the AC model here presented suggest to test it when integrated within a full SRL architecture. Table 5 reports the results of the processing cascade over three collections. Results on the Boundary Detection BD task are obtained by training an SVM model on the same feature set presented in (Johansson and Nugues, 2008b) and are slightly below the stateof-the art BD accuracy reported in (Coppola et al., 2009). However, the accuracy of the complete BD + AC + RR chain (i.e. 68%) improves the corresponding results of (Coppola et al., 2009). Given the relatively simple feature set adopted here, this result is very significant as for its resulting efficiency. The overall BD recognition process is, on a standard architecture, performed at about 6.74 sentences per second, that is basically Corpus Eval. Setting Recall Precision F1 BD 72.6 85.1 78.4 BNC BD+AC+RR 62.6 74.5 68.0 BD 63.9 80.0 71.0 NTI BD+AC+RR 56.7 72.1 63.5 BD 64.0 81.5 71.7 ANC BD+AC+RR 47.4 62.5 53.9 Table 5: Accuracy of the full cascade o</context>
</contexts>
<marker>Coppola, Moschitti, Riccardi, 2009</marker>
<rawString>Bonaventura Coppola, Alessandro Moschitti, and Giuseppe Riccardi. 2009. Shallow semantic parsing for spoken language understanding. In Proceedings of NAACL ’09, pages 85–88, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koen Deschacht</author>
<author>Marie-Francine Moens</author>
</authors>
<title>Semi-supervised semantic role labeling using the latent words language model.</title>
<date>2009</date>
<booktitle>In EMNLP ’09: Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>21--29</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="11524" citStr="Deschacht and Moens, 2009" startWordPosition="1761" endWordPosition="1764">at a multitask learning architecture coupled with semi-supervised learning can be effectively applied even to complex linguistic tasks such as SRL. In particular, (Collobert and Weston, 2008) proposes an embedding of lexical information using Wikipedia as source, and exploiting the resulting language model within the multitask learning process. The idea of (Collobert and Weston, 2008) to obtain an embedding of lexical information by acquiring a language model from unlabeled data is an interesting approach to the problem of performance degradation in out-of-domain tests, as already pursued by (Deschacht and Moens, 2009). The extensive use of unlabeled texts allows to achieve a significant level of lexical generalization that seems better capitalize the smaller annotated data sets. 3 A Distributional Model for Argument Classification High quality lexical information is crucial for robust open-domain SRL, as semantic generalization highly depends on lexical information. For example, the following two sentences evoke the STATEMENT frame, through the LUs say and state, where the FEs, SPEAKER and MEDIUM, are shown. [President Kennedy] SPEAKER said to an astronaut, ”Man is still the most extraordinary computer of </context>
</contexts>
<marker>Deschacht, Moens, 2009</marker>
<rawString>Koen Deschacht and Marie-Francine Moens. 2009. Semi-supervised semantic role labeling using the latent words language model. In EMNLP ’09: Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 21–29, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Pado</author>
</authors>
<title>Shalmaneser -a flexible toolbox for semantic role assignment.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC 2006,</booktitle>
<location>Genoa, Italy.</location>
<contexts>
<context position="2458" citStr="Erk and Pado, 2006" startWordPosition="362" endWordPosition="365">annotated corpora (Palmer et al., 2005), the seminal work of (Gildea and Jurafsky, 2002) and the successful CoNLL evaluation campaigns (Carreras and M`arquez, 2005). Statistical machine learning methods, ranging from joint probabilistic models to support vector machines, have been successfully adopted to provide very accurate semantic labeling, e.g. (Carreras and M`arquez, 2005). SRL based on FrameNet is thus not a novel task, although very few systems are known capable of completing a general frame-based annotation process over raw texts, noticeable exceptions being discussed for example in (Erk and Pado, 2006), (Johansson and Nugues, 2008b) and (Coppola et al., 2009). Some critical limitations have been outlined in literature, some of them independent from the underlying semantic paradigm. Parsing Accuracy. Most of the employed learning algorithms are based on complex sets of syntagmatic features, as deeply investigated in (Johansson and Nugues, 2008b). The resulting recognition is thus highly dependent on the accuracy of the underlying parser, whereas wrong structures returned by the parser usually imply large misclassification errors. Annotation costs. Statistical learning approaches applied to S</context>
</contexts>
<marker>Erk, Pado, 2006</marker>
<rawString>Katrin Erk and Sebastian Pado. 2006. Shalmaneser -a flexible toolbox for semantic role assignment. In Proceedings of LREC 2006, Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
</authors>
<title>Representing words as regions in vector space. In</title>
<date>2009</date>
<booktitle>In Proceedings of CoNLL ’09,</booktitle>
<pages>57--65</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="27272" citStr="Erk, 2009" startWordPosition="4413" endWordPosition="4414">ers, while if 5Sentences whose arguments were not represented in the FrameNet training material were removed from all tests. Eq. -a Frames with a number of annotated examples &gt;0 &gt;100 &gt;500 &gt;1K &gt;3K &gt;5K (5) - .85 86.3 86.5 87.2 88.3 85.9 82.0 (4) - .5 85.1 85.5 85.8 87.2 83.5 79.4 (4) - .1 84.5 84.8 85.1 86.5 83.0 78.7 Table 3: Accuracy on Arg classification tasks wrt different clustering policies Q pz� 1 then many singleton clusters are promoted (i.e. one cluster for each example). By varying the threshold Q we thus account for prototype-based as well exemplar-based strategies, as discussed in (Erk, 2009). We measured the performance on the argument classification tasks of different models obtained by combing different choices of Q with Eq. (4) or (5). Results are reported in Table 3. The leftmost column reports the different clustering settings, while in the remaining columns we see performances over test sentences related to different frames: we selected frames for which an increasing number of annotated examples are available: from all frames (for more than 0 examples) to the only frame (i.e. SELF MOTION) that has more than 5,000 examples in our training data set. The reported accuracies su</context>
</contexts>
<marker>Erk, 2009</marker>
<rawString>Katrin Erk. 2009. Representing words as regions in vector space. In In Proceedings of CoNLL ’09, pages 57–65, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles J Fillmore</author>
</authors>
<title>Frames and the semantics of understanding.</title>
<date>1985</date>
<journal>Quaderni di Semantica,</journal>
<volume>4</volume>
<issue>2</issue>
<pages>254</pages>
<contexts>
<context position="1202" citStr="Fillmore, 1985" startWordPosition="165" endWordPosition="167">SRL is extended through the encapsulation of a distributional model of semantic similarity. The resulting argument classification model promotes a simpler feature space that limits the potential overfitting effects. The large scale empirical study here discussed confirms that state-of-art accuracy can be obtained for out-of-domain evaluations. 1 Introduction The availability of large scale semantic lexicons, such as FrameNet (Baker et al., 1998), allowed the adoption of a wide family of learning paradigms in the automation of semantic parsing. Building upon the so called frame semantic model (Fillmore, 1985), the Berkeley FrameNet project has developed a semantic lexicon for the core vocabulary of English, since 1997. A frame is evoked in texts through the occurrence of its lexical units (LU), i.e. predicate words such verbs, nouns, or adjectives, and specifies the participants and properties of the situation it describes, the so called frame elements (FEs). Semantic Role Labeling (SRL) is the task of automatic recognition of individual predicates together with their major roles (e.g. frame elements) as they are grammatically realized in input sentences. It has been a popular task since the avail</context>
</contexts>
<marker>Fillmore, 1985</marker>
<rawString>Charles J. Fillmore. 1985. Frames and the semantics of understanding. Quaderni di Semantica, 4(2):222– 254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hagen F¨urstenau</author>
<author>Mirella Lapata</author>
</authors>
<title>Graph alignment for semi-supervised semantic role labeling. In</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP ’09,</booktitle>
<pages>11--20</pages>
<location>Morristown, NJ, USA.</location>
<marker>F¨urstenau, Lapata, 2009</marker>
<rawString>Hagen F¨urstenau and Mirella Lapata. 2009. Graph alignment for semi-supervised semantic role labeling. In In Proceedings of EMNLP ’09, pages 11–20, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic Labeling of Semantic Roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="1927" citStr="Gildea and Jurafsky, 2002" startWordPosition="283" endWordPosition="286">, since 1997. A frame is evoked in texts through the occurrence of its lexical units (LU), i.e. predicate words such verbs, nouns, or adjectives, and specifies the participants and properties of the situation it describes, the so called frame elements (FEs). Semantic Role Labeling (SRL) is the task of automatic recognition of individual predicates together with their major roles (e.g. frame elements) as they are grammatically realized in input sentences. It has been a popular task since the availability of the PropBank and FrameNet annotated corpora (Palmer et al., 2005), the seminal work of (Gildea and Jurafsky, 2002) and the successful CoNLL evaluation campaigns (Carreras and M`arquez, 2005). Statistical machine learning methods, ranging from joint probabilistic models to support vector machines, have been successfully adopted to provide very accurate semantic labeling, e.g. (Carreras and M`arquez, 2005). SRL based on FrameNet is thus not a novel task, although very few systems are known capable of completing a general frame-based annotation process over raw texts, noticeable exceptions being discussed for example in (Erk and Pado, 2006), (Johansson and Nugues, 2008b) and (Coppola et al., 2009). Some crit</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2002. Automatic Labeling of Semantic Roles. Computational Linguistics, 28(3):245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Michael Elhadad</author>
</authors>
<title>On the role of lexical features in sequence labeling. In</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP ’09,</booktitle>
<pages>1142--1151</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5905" citStr="Goldberg and Elhadad, 2009" startWordPosition="885" endWordPosition="888">ed material lack of a clear grammatical status. The limited level of linguistic generalization outlined above is still an open research problem. Existing solutions have been proposed in literature along different lines. Learning from richer linguistic descriptions of more complex structures is proposed in (Toutanova et al., 2008). Limiting the cost required for developing large domainspecific training data sets has been also studied, e.g., (F¨urstenau and Lapata, 2009). Finally, the application of semi-supervised learning is attempted to increase the lexical expressiveness of the model, e.g. (Goldberg and Elhadad, 2009). In this paper, this last direction is pursued. A semi-supervised statistical model exploiting useful lexical information from unlabeled corpora is proposed. The model adopts a simple feature space by relying on a limited set of grammatical properties, thus reducing its learning capacity. Moreover, it generalizes lexical information about the annotated examples by applying a geometrical model, in a Latent Semantic Analysis style, inspired by a distributional paradigm (Pado and Lapata, 2007). As we will see, the accuracy reachable through a restricted feature space is still quite close to the </context>
<context position="10014" citStr="Goldberg and Elhadad, 2009" startWordPosition="1523" endWordPosition="1526"> training instances from an unlabeled corpus also for verbs not listed as existing FrameNet predicates. 2.1 The role of Lexical Semantic Information It is widely accepted that lexical information (as features directly derived from word forms) is crucial for training accurate systems in a number of NLP tasks. Indeed, all the best systems in the CoNLL shared task competitions (e.g. Chunking (Tjong Kim Sang and Buchholz, 2000)) make extensive use of lexical information. Also lexical features are beneficial in SRL usually either for systems on Propbank as well as for FrameNetbased annotation. In (Goldberg and Elhadad, 2009), a different strategy to incorporate lexical features into classification models is proposed. A more expressive training algorithm (i.e. anchored SVM) coupled with an aggressive feature pruning strategy is shown to achieve high accuracy over a chunking and named entity recognition task. The suggested perspective here is that effective semantic knowledge can be collected from sources external to the annotated corpora (very large unannotated corpora or on manually constructed lexical resources) rather than learned from the raw lexical counts of the annotated corpus. Notice how this is also the </context>
</contexts>
<marker>Goldberg, Elhadad, 2009</marker>
<rawString>Yoav Goldberg and Michael Elhadad. 2009. On the role of lexical features in sequence labeling. In In Proceedings of EMNLP ’09, pages 1142–1151, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L J Heyer</author>
<author>S Kruglyak</author>
<author>S Yooseph</author>
</authors>
<title>Exploring expression data: Identification and analysis of coexpressed genes.</title>
<date>1999</date>
<journal>Genome Research,</journal>
<volume>9</volume>
<pages>1115</pages>
<contexts>
<context position="16545" citStr="Heyer et al., 1999" startWordPosition="2572" endWordPosition="2575"> r , i.e. the subset of HFEk produced by examples of the relation r (e.g. Subj). Given the annotated sentence (2), we have that report E HMEDIUM SBJ . As the LSA vectors h are available for the semantic heads h, a vector representation the role FEk can be obtained from the annotated data. However, one single vector is a too simplistic representation given the rich nature of semantic roles FEk. In order to better represent FEk, multiple regions in the semantic space are used. They are obtained by a clustering process applied to the set HF Ek r according to the Quality Threshold (QT) algorithm (Heyer et al., 1999). QT is a generalization of k-mean where a variable number of clusters can be obtained. This number depends on the minimal value of intra-cluster similarity accepted by the algorithm and controlled by a parameter, σ: lower values of σ correspond to more heterogeneous (i.e. larger grain) clusters, while values close to 1 characterize stricter policies and more fine-grained results. Given a syntactic relation r, CF Ek r denotes the clusters derived by QT clustering over HF Ek r . Each cluster c E CF Ek r is represented by a vector -C-+ , computed as the geometric centroid of its semantic heads h</context>
<context position="26613" citStr="Heyer et al., 1999" startWordPosition="4297" endWordPosition="4300">udy aims at detecting the impact of different clustering policies on the resulting AC accuracy. Clustering, as discussed in Section 3.1, allows to generalize lexical information: similar heads within the latent semantic space are built from the annotated examples and they allow to predict the behavior of new unseen words as found in the test sentences. The system performances have been here measured under different clustering conditions, i.e. grains at which the clustering of annotated examples is applied. This grain is determined by the parameter Q of the applied Quality Threshold algorithm (Heyer et al., 1999). Notice that small values of Q imply large clusters, while if 5Sentences whose arguments were not represented in the FrameNet training material were removed from all tests. Eq. -a Frames with a number of annotated examples &gt;0 &gt;100 &gt;500 &gt;1K &gt;3K &gt;5K (5) - .85 86.3 86.5 87.2 88.3 85.9 82.0 (4) - .5 85.1 85.5 85.8 87.2 83.5 79.4 (4) - .1 84.5 84.8 85.1 86.5 83.0 78.7 Table 3: Accuracy on Arg classification tasks wrt different clustering policies Q pz� 1 then many singleton clusters are promoted (i.e. one cluster for each example). By varying the threshold Q we thus account for prototype-based as </context>
</contexts>
<marker>Heyer, Kruglyak, Yooseph, 1999</marker>
<rawString>L.J. Heyer, S. Kruglyak, and S. Yooseph. 1999. Exploring expression data: Identification and analysis of coexpressed genes. Genome Research, (9):1106– 1115.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Dependency-based syntactic-semantic analysis with propbank and nombank.</title>
<date>2008</date>
<booktitle>In Proceedings of CoNLL2008,</booktitle>
<pages>16--17</pages>
<location>Manchester, UK,</location>
<contexts>
<context position="2487" citStr="Johansson and Nugues, 2008" startWordPosition="366" endWordPosition="369">mer et al., 2005), the seminal work of (Gildea and Jurafsky, 2002) and the successful CoNLL evaluation campaigns (Carreras and M`arquez, 2005). Statistical machine learning methods, ranging from joint probabilistic models to support vector machines, have been successfully adopted to provide very accurate semantic labeling, e.g. (Carreras and M`arquez, 2005). SRL based on FrameNet is thus not a novel task, although very few systems are known capable of completing a general frame-based annotation process over raw texts, noticeable exceptions being discussed for example in (Erk and Pado, 2006), (Johansson and Nugues, 2008b) and (Coppola et al., 2009). Some critical limitations have been outlined in literature, some of them independent from the underlying semantic paradigm. Parsing Accuracy. Most of the employed learning algorithms are based on complex sets of syntagmatic features, as deeply investigated in (Johansson and Nugues, 2008b). The resulting recognition is thus highly dependent on the accuracy of the underlying parser, whereas wrong structures returned by the parser usually imply large misclassification errors. Annotation costs. Statistical learning approaches applied to SRL are very demanding with re</context>
<context position="4165" citStr="Johansson and Nugues, 2008" startWordPosition="614" endWordPosition="617">n. Several studies showed that even when large training sets exist the corresponding learning exhibits poor generalization power. Most of the CoNLL 2005 systems show a significant performance drop when the tested corpus, i.e. Brown, differs from 237 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 237–246, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics the training one (i.e. Wall Street Journal), e.g. (Toutanova et al., 2008). More recently, the stateof-art frame-based semantic role labeling system discussed in (Johansson and Nugues, 2008b) reports a 19% drop in accuracy for the argument classification task when a different test domain is targeted (i.e. NTI corpus). Out-of-domain tests seem to suggest the models trained on BNC do not generalize well to novel grammatical and lexical phenomena. As also suggested in (Pradhan et al., 2008), the major drawback is the poor generalization power affecting lexical features. Notice how this is also a general problem of statistical learning processes, as large fine grain feature sets are more exposed to the risks of overfitting. The above problems are particularly critical for frame-base</context>
<context position="7102" citStr="Johansson and Nugues, 2008" startWordPosition="1068" endWordPosition="1072"> is still quite close to the state-of-art, but interestingly the performance drops in out-of-domain tests are avoided. In the following, after discussing existing approaches to SRL (Section 2), a distributional approach is defined in Section 3. Section 3.2 discusses the proposed HMM-based treatment of joint inferences in argument classification. The large scale experiments described in Section 4 will allow to draw the conclusions of Section 5. 2 Related Work State-of-art approaches to frame-based SRL are based on Support Vector Machines, trained over linear models of syntactic features, e.g. (Johansson and Nugues, 2008b), or tree-kernels, e.g. (Coppola et al., 2009). SRL proceeds through two main steps: the localization of arguments in a sentence, called boundary detection (BD), and the assignment of the proper role to the detected constituents, that is the argument classification, (AC) step. In (Toutanova et al., 2008) a SRL model over Propbank that effectively exploits the semantic argument frame as a joint structure, is presented. It incorporates strong dependencies within a comprehensive statistical joint model with a rich set of features over multiple argument phrases. This approach effectively introdu</context>
<context position="23999" citStr="Johansson and Nugues, 2008" startWordPosition="3879" endWordPosition="3883">≈ n � P(FEθ(i)|FEθ(i−1)7 FEθ(i−2)) (12) i=1 accounts FEs sequence via a 3-gram model3 . 4 Empirical Analysis The aim of the evaluation is to measure the reachable accuracy of the simple model proposed and to compare its impact over in-domain and out-ofdomain semantic role labeling tasks. In particular, we will evaluate the argument classification (AC) task in Section 4.2. Experimental Set-Up. The in-domain test has been run over the FrameNet annotated corpus, derived from the British National Corpus (BNC). The splitting between train and test set is 90%- 10% according to the same data set of (Johansson and Nugues, 2008b). In all experiments, the FrameNet 1.3 version and the dependencybased system using the LTH parser (Johansson and Nugues, 2008a) have been employed. Outof-domain tests are run over the two training corpora as made available by the Semeval 2007 Task 194 (Baker et al., 2007): the Nuclear Threat Initiative (NTI) and the American National Corpus 3Two empty states are added at the beginning of any sequence. Moreover, Laplace smoothing was also applied to each estimator. 4The NTI and ANC annotated collections are downloadable at: nlp.cs.swarthmore.edu/semeval/tasks/task19/data/train.tar.gz P(ri7 h</context>
<context position="25775" citStr="Johansson and Nugues, 2008" startWordPosition="4158" endWordPosition="4161">s been parsed and the dependency graphs derived from individual sentences provided the basic observable contexts: every co-occurrence is thus syntactically justified by a dependency arc. The most frequent 30,000 basic features, i.e. (syntactic relation,lemma) pairs, have been used to build the matrix M, vector components corresponding to point-wise mutual information scores. Finally, the final space is obtained by applying the SVD reduction over M, with a dimensionality cut of l = 250. In the evaluation of the AC task, accuracy is computed over the nodes of the dependency graph, in line with (Johansson and Nugues, 2008b) or (Coppola et al., 2009). Accordingly, also recall, precision and F-measure are reported on a per node basis, against the binary BD task or for the full BD + AC chain. 4.1 The Role of Lexical Clustering The first study aims at detecting the impact of different clustering policies on the resulting AC accuracy. Clustering, as discussed in Section 3.1, allows to generalize lexical information: similar heads within the latent semantic space are built from the annotated examples and they allow to predict the behavior of new unseen words as found in the test sentences. The system performances ha</context>
<context position="28272" citStr="Johansson and Nugues, 2008" startWordPosition="4571" endWordPosition="4574">h an increasing number of annotated examples are available: from all frames (for more than 0 examples) to the only frame (i.e. SELF MOTION) that has more than 5,000 examples in our training data set. The reported accuracies suggest that Eq. (5), promoting an example driven strategy, better captures the role preference, as it always outperforms alternative settings (i.e. more prototype oriented methods). It limits overgeneralization and promotes fine grained clusters. An interesting result is that a per-node accuracy of 86.3 (i.e. only 3 points under the state of-the art on the same data set, (Johansson and Nugues, 2008b)) is achieved. All the remaining tests have been run with the clustering configuration characterized by Eq. (5) and Q = 0.85. 4.2 Argument Classification Accuracy In these experiments we evaluate the quality of the argument classification step against the lexical knowledge acquired from unlabeled texts and the reranking step. The accuracy reachable on the gold standard argument boundaries has been compared across several experimental settings. Two baseline systems have been obtained. The Local Prior model outputs the sequence that maximizes the prior probability locally to individual argumen</context>
<context position="30987" citStr="Johansson and Nugues, 2008" startWordPosition="5017" endWordPosition="5020"> test sets. Results, reported in column 3 and 4 of Table 4 and shown in Figure 2, confirm that no major drop in performance is observed. Notice how the positive impact of the backoff models and the HMM reranking policy is similarly reflected by all the collections. Moreover, the results on the NTI corpus are even better than those obtained on the BNC, with a resulting 90.5% accuracy on the AC task. Figure 2: Accuracy of the AC task over different corpora 4.3 Discussion The above empirical findings are relevant if compared with the outcome of a similar test on the NTI collection, discussed in (Johansson and Nugues, 2008b)6. There, under the same training conditions, a performance drop of about -19% is reported (from 89.9 to 71.1%) over gold standard argument boundaries. The model proposed in this paper exhibits no such drop in any collection (NTI and ANC). This seems to confirm the hypothesis that the model is able to properly generalize the required lexical information across different domains. It is interesting to outline that the individual stages of the proposed model play different roles in the different domains, as Table 4 suggests. Although the positive contributions of the individual processing stage</context>
<context position="32674" citStr="Johansson and Nugues, 2008" startWordPosition="5311" endWordPosition="5314">q. 5 is backed off against test instances (as h or r are not available from the training data) is twice as high as in the BNC-FN or in the NTI domain (i.e. 15.5 vs. 7.2 or 8.7, respectively). The different syntactic style of ANC seems thus the main responsible of the poor impact of distributional information, as it is often unapplicable to ANC test cases. • The complexity of the three test sets is different, as the three plots show. The NTI col6Notice that in this paper only the training portion of the NTI data set is employed as reported in Table 2 and results are not directly comparable to (Johansson and Nugues, 2008b). 100,0% 90,0% 80,0% 60,0% 50,0% 40,0% 70,0% Local Prior Global Prior Distributional Backoff Backoff +HMMRR FN-BNC NTI ANC 90,5% 86,3% 79,9% 244 lections seems characterized by a lower level of complexity (see for example the accuracy of the Local prior model, that is about 51% as for the ANC). It then gets benefits from all the analysis stages, in particular the final HMM reranking. The BNC-FN test collection seems the most complex one, and the impact of the lexical information brought by the distributional model is here maximal. This is mainly due to the coherence between the distributions</context>
<context position="34617" citStr="Johansson and Nugues, 2008" startWordPosition="5649" endWordPosition="5652">rrors in the grammatical recognition (of individual arguments or at sentence level) are more frequent and afflict the local distributional model. The more complex is the syntax of a corpus (e.g. in the NTI and ANC data sets), the higher seems the impact of the reranking phase. The significant performance of the AC model here presented suggest to test it when integrated within a full SRL architecture. Table 5 reports the results of the processing cascade over three collections. Results on the Boundary Detection BD task are obtained by training an SVM model on the same feature set presented in (Johansson and Nugues, 2008b) and are slightly below the stateof-the art BD accuracy reported in (Coppola et al., 2009). However, the accuracy of the complete BD + AC + RR chain (i.e. 68%) improves the corresponding results of (Coppola et al., 2009). Given the relatively simple feature set adopted here, this result is very significant as for its resulting efficiency. The overall BD recognition process is, on a standard architecture, performed at about 6.74 sentences per second, that is basically Corpus Eval. Setting Recall Precision F1 BD 72.6 85.1 78.4 BNC BD+AC+RR 62.6 74.5 68.0 BD 63.9 80.0 71.0 NTI BD+AC+RR 56.7 72.</context>
</contexts>
<marker>Johansson, Nugues, 2008</marker>
<rawString>Richard Johansson and Pierre Nugues. 2008a. Dependency-based syntactic-semantic analysis with propbank and nombank. In Proceedings of CoNLL2008, Manchester, UK, August 16-17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>The effect of syntactic representation on semantic role labeling.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>18--22</pages>
<location>Manchester, UK,</location>
<contexts>
<context position="2487" citStr="Johansson and Nugues, 2008" startWordPosition="366" endWordPosition="369">mer et al., 2005), the seminal work of (Gildea and Jurafsky, 2002) and the successful CoNLL evaluation campaigns (Carreras and M`arquez, 2005). Statistical machine learning methods, ranging from joint probabilistic models to support vector machines, have been successfully adopted to provide very accurate semantic labeling, e.g. (Carreras and M`arquez, 2005). SRL based on FrameNet is thus not a novel task, although very few systems are known capable of completing a general frame-based annotation process over raw texts, noticeable exceptions being discussed for example in (Erk and Pado, 2006), (Johansson and Nugues, 2008b) and (Coppola et al., 2009). Some critical limitations have been outlined in literature, some of them independent from the underlying semantic paradigm. Parsing Accuracy. Most of the employed learning algorithms are based on complex sets of syntagmatic features, as deeply investigated in (Johansson and Nugues, 2008b). The resulting recognition is thus highly dependent on the accuracy of the underlying parser, whereas wrong structures returned by the parser usually imply large misclassification errors. Annotation costs. Statistical learning approaches applied to SRL are very demanding with re</context>
<context position="4165" citStr="Johansson and Nugues, 2008" startWordPosition="614" endWordPosition="617">n. Several studies showed that even when large training sets exist the corresponding learning exhibits poor generalization power. Most of the CoNLL 2005 systems show a significant performance drop when the tested corpus, i.e. Brown, differs from 237 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 237–246, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics the training one (i.e. Wall Street Journal), e.g. (Toutanova et al., 2008). More recently, the stateof-art frame-based semantic role labeling system discussed in (Johansson and Nugues, 2008b) reports a 19% drop in accuracy for the argument classification task when a different test domain is targeted (i.e. NTI corpus). Out-of-domain tests seem to suggest the models trained on BNC do not generalize well to novel grammatical and lexical phenomena. As also suggested in (Pradhan et al., 2008), the major drawback is the poor generalization power affecting lexical features. Notice how this is also a general problem of statistical learning processes, as large fine grain feature sets are more exposed to the risks of overfitting. The above problems are particularly critical for frame-base</context>
<context position="7102" citStr="Johansson and Nugues, 2008" startWordPosition="1068" endWordPosition="1072"> is still quite close to the state-of-art, but interestingly the performance drops in out-of-domain tests are avoided. In the following, after discussing existing approaches to SRL (Section 2), a distributional approach is defined in Section 3. Section 3.2 discusses the proposed HMM-based treatment of joint inferences in argument classification. The large scale experiments described in Section 4 will allow to draw the conclusions of Section 5. 2 Related Work State-of-art approaches to frame-based SRL are based on Support Vector Machines, trained over linear models of syntactic features, e.g. (Johansson and Nugues, 2008b), or tree-kernels, e.g. (Coppola et al., 2009). SRL proceeds through two main steps: the localization of arguments in a sentence, called boundary detection (BD), and the assignment of the proper role to the detected constituents, that is the argument classification, (AC) step. In (Toutanova et al., 2008) a SRL model over Propbank that effectively exploits the semantic argument frame as a joint structure, is presented. It incorporates strong dependencies within a comprehensive statistical joint model with a rich set of features over multiple argument phrases. This approach effectively introdu</context>
<context position="23999" citStr="Johansson and Nugues, 2008" startWordPosition="3879" endWordPosition="3883">≈ n � P(FEθ(i)|FEθ(i−1)7 FEθ(i−2)) (12) i=1 accounts FEs sequence via a 3-gram model3 . 4 Empirical Analysis The aim of the evaluation is to measure the reachable accuracy of the simple model proposed and to compare its impact over in-domain and out-ofdomain semantic role labeling tasks. In particular, we will evaluate the argument classification (AC) task in Section 4.2. Experimental Set-Up. The in-domain test has been run over the FrameNet annotated corpus, derived from the British National Corpus (BNC). The splitting between train and test set is 90%- 10% according to the same data set of (Johansson and Nugues, 2008b). In all experiments, the FrameNet 1.3 version and the dependencybased system using the LTH parser (Johansson and Nugues, 2008a) have been employed. Outof-domain tests are run over the two training corpora as made available by the Semeval 2007 Task 194 (Baker et al., 2007): the Nuclear Threat Initiative (NTI) and the American National Corpus 3Two empty states are added at the beginning of any sequence. Moreover, Laplace smoothing was also applied to each estimator. 4The NTI and ANC annotated collections are downloadable at: nlp.cs.swarthmore.edu/semeval/tasks/task19/data/train.tar.gz P(ri7 h</context>
<context position="25775" citStr="Johansson and Nugues, 2008" startWordPosition="4158" endWordPosition="4161">s been parsed and the dependency graphs derived from individual sentences provided the basic observable contexts: every co-occurrence is thus syntactically justified by a dependency arc. The most frequent 30,000 basic features, i.e. (syntactic relation,lemma) pairs, have been used to build the matrix M, vector components corresponding to point-wise mutual information scores. Finally, the final space is obtained by applying the SVD reduction over M, with a dimensionality cut of l = 250. In the evaluation of the AC task, accuracy is computed over the nodes of the dependency graph, in line with (Johansson and Nugues, 2008b) or (Coppola et al., 2009). Accordingly, also recall, precision and F-measure are reported on a per node basis, against the binary BD task or for the full BD + AC chain. 4.1 The Role of Lexical Clustering The first study aims at detecting the impact of different clustering policies on the resulting AC accuracy. Clustering, as discussed in Section 3.1, allows to generalize lexical information: similar heads within the latent semantic space are built from the annotated examples and they allow to predict the behavior of new unseen words as found in the test sentences. The system performances ha</context>
<context position="28272" citStr="Johansson and Nugues, 2008" startWordPosition="4571" endWordPosition="4574">h an increasing number of annotated examples are available: from all frames (for more than 0 examples) to the only frame (i.e. SELF MOTION) that has more than 5,000 examples in our training data set. The reported accuracies suggest that Eq. (5), promoting an example driven strategy, better captures the role preference, as it always outperforms alternative settings (i.e. more prototype oriented methods). It limits overgeneralization and promotes fine grained clusters. An interesting result is that a per-node accuracy of 86.3 (i.e. only 3 points under the state of-the art on the same data set, (Johansson and Nugues, 2008b)) is achieved. All the remaining tests have been run with the clustering configuration characterized by Eq. (5) and Q = 0.85. 4.2 Argument Classification Accuracy In these experiments we evaluate the quality of the argument classification step against the lexical knowledge acquired from unlabeled texts and the reranking step. The accuracy reachable on the gold standard argument boundaries has been compared across several experimental settings. Two baseline systems have been obtained. The Local Prior model outputs the sequence that maximizes the prior probability locally to individual argumen</context>
<context position="30987" citStr="Johansson and Nugues, 2008" startWordPosition="5017" endWordPosition="5020"> test sets. Results, reported in column 3 and 4 of Table 4 and shown in Figure 2, confirm that no major drop in performance is observed. Notice how the positive impact of the backoff models and the HMM reranking policy is similarly reflected by all the collections. Moreover, the results on the NTI corpus are even better than those obtained on the BNC, with a resulting 90.5% accuracy on the AC task. Figure 2: Accuracy of the AC task over different corpora 4.3 Discussion The above empirical findings are relevant if compared with the outcome of a similar test on the NTI collection, discussed in (Johansson and Nugues, 2008b)6. There, under the same training conditions, a performance drop of about -19% is reported (from 89.9 to 71.1%) over gold standard argument boundaries. The model proposed in this paper exhibits no such drop in any collection (NTI and ANC). This seems to confirm the hypothesis that the model is able to properly generalize the required lexical information across different domains. It is interesting to outline that the individual stages of the proposed model play different roles in the different domains, as Table 4 suggests. Although the positive contributions of the individual processing stage</context>
<context position="32674" citStr="Johansson and Nugues, 2008" startWordPosition="5311" endWordPosition="5314">q. 5 is backed off against test instances (as h or r are not available from the training data) is twice as high as in the BNC-FN or in the NTI domain (i.e. 15.5 vs. 7.2 or 8.7, respectively). The different syntactic style of ANC seems thus the main responsible of the poor impact of distributional information, as it is often unapplicable to ANC test cases. • The complexity of the three test sets is different, as the three plots show. The NTI col6Notice that in this paper only the training portion of the NTI data set is employed as reported in Table 2 and results are not directly comparable to (Johansson and Nugues, 2008b). 100,0% 90,0% 80,0% 60,0% 50,0% 40,0% 70,0% Local Prior Global Prior Distributional Backoff Backoff +HMMRR FN-BNC NTI ANC 90,5% 86,3% 79,9% 244 lections seems characterized by a lower level of complexity (see for example the accuracy of the Local prior model, that is about 51% as for the ANC). It then gets benefits from all the analysis stages, in particular the final HMM reranking. The BNC-FN test collection seems the most complex one, and the impact of the lexical information brought by the distributional model is here maximal. This is mainly due to the coherence between the distributions</context>
<context position="34617" citStr="Johansson and Nugues, 2008" startWordPosition="5649" endWordPosition="5652">rrors in the grammatical recognition (of individual arguments or at sentence level) are more frequent and afflict the local distributional model. The more complex is the syntax of a corpus (e.g. in the NTI and ANC data sets), the higher seems the impact of the reranking phase. The significant performance of the AC model here presented suggest to test it when integrated within a full SRL architecture. Table 5 reports the results of the processing cascade over three collections. Results on the Boundary Detection BD task are obtained by training an SVM model on the same feature set presented in (Johansson and Nugues, 2008b) and are slightly below the stateof-the art BD accuracy reported in (Coppola et al., 2009). However, the accuracy of the complete BD + AC + RR chain (i.e. 68%) improves the corresponding results of (Coppola et al., 2009). Given the relatively simple feature set adopted here, this result is very significant as for its resulting efficiency. The overall BD recognition process is, on a standard architecture, performed at about 6.74 sentences per second, that is basically Corpus Eval. Setting Recall Precision F1 BD 72.6 85.1 78.4 BNC BD+AC+RR 62.6 74.5 68.0 BD 63.9 80.0 71.0 NTI BD+AC+RR 56.7 72.</context>
</contexts>
<marker>Johansson, Nugues, 2008</marker>
<rawString>Richard Johansson and Pierre Nugues. 2008b. The effect of syntactic representation on semantic role labeling. In Proceedings of COLING, Manchester, UK, August 18-22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rie Johnson</author>
<author>Tong Zhang</author>
</authors>
<title>Graph-based semi-supervised learning and spectral kernel design.</title>
<date>2008</date>
<journal>IEEE Transactions on Information Theory,</journal>
<volume>54</volume>
<issue>1</issue>
<marker>Johnson, Zhang, 2008</marker>
<rawString>Rie Johnson and Tong Zhang. 2008. Graph-based semi-supervised learning and spectral kernel design. IEEE Transactions on Information Theory, 54(1):275–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Landauer</author>
<author>Sue Dumais</author>
</authors>
<title>A solution to plato’s problem: The latent semantic analysis theory of acquisition, induction and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<contexts>
<context position="14531" citStr="Landauer and Dumais, 1997" startWordPosition="2235" endWordPosition="2238"> meaning of argument heads, we adopt a distributional perspective, where the meaning is described by the set of textual contexts in which words appear. In distributional models, words are thus represented through vectors built over these observable contexts: similar vectors suggest semantic relatedness as a function of the distance between two words, capturing paradigmatic (e.g. synonymy) or syntagmatic relations (Pado, 2007). Vectors h are described by an adjacency matrix M, whose rows describe target words (h) and whose columns describe their corpus contexts. Latent Semantic Analysis (LSA) (Landauer and Dumais, 1997), is then applied to �� M to acquire meaningful representations h . LSA exploits the linear transformation called Singular Value Decomposition (SVD) and produces an approximation of the original matrix M, capturing (semantic) dependencies between context vectors. M is replaced by a lower dimensional matrix Ml, capturing the same statistical information in a new l-dimensional space, where each dimension is a linear combination of some of the original features (i.e. contexts). These derived features may be thought as artificial concepts, each one representing an emerging meaning component, as th</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Tom Landauer and Sue Dumais. 1997. A solution to plato’s problem: The latent semantic analysis theory of acquisition, induction and representation of knowledge. Psychological Review, 104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Moschitti</author>
<author>D Pighin</author>
<author>R Basili</author>
</authors>
<title>Tree kernels for semantic role labeling.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<contexts>
<context position="7818" citStr="Moschitti et al., 2008" startWordPosition="1183" endWordPosition="1186">calization of arguments in a sentence, called boundary detection (BD), and the assignment of the proper role to the detected constituents, that is the argument classification, (AC) step. In (Toutanova et al., 2008) a SRL model over Propbank that effectively exploits the semantic argument frame as a joint structure, is presented. It incorporates strong dependencies within a comprehensive statistical joint model with a rich set of features over multiple argument phrases. This approach effectively introduces a new step in SRL, also called Joint Re-ranking, (RR), e.g. (Toutanova et al., 2008) or (Moschitti et al., 2008). First local models are applied to produce role labels over individual arguments, then the joint model is used to decide the entire argument sequence among the set of the n-best competing solutions. While these approaches increase the expressive power of the models to capture more general linguistic properties, they rely on complex feature sets, are more demanding about the amount of training information and increase the overall exposure to overfitting effects. In (Johansson and Nugues, 2008b) the impact of different grammatical representations on the task of frame-based shallow semantic pars</context>
</contexts>
<marker>Moschitti, Pighin, Basili, 2008</marker>
<rawString>A. Moschitti, D. Pighin, and R. Basili. 2008. Tree kernels for semantic role labeling. Computational Linguistics, 34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pado</author>
<author>Mirella Lapata</author>
</authors>
<title>Dependency-based construction of semantic space models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="6401" citStr="Pado and Lapata, 2007" startWordPosition="960" endWordPosition="963">f semi-supervised learning is attempted to increase the lexical expressiveness of the model, e.g. (Goldberg and Elhadad, 2009). In this paper, this last direction is pursued. A semi-supervised statistical model exploiting useful lexical information from unlabeled corpora is proposed. The model adopts a simple feature space by relying on a limited set of grammatical properties, thus reducing its learning capacity. Moreover, it generalizes lexical information about the annotated examples by applying a geometrical model, in a Latent Semantic Analysis style, inspired by a distributional paradigm (Pado and Lapata, 2007). As we will see, the accuracy reachable through a restricted feature space is still quite close to the state-of-art, but interestingly the performance drops in out-of-domain tests are avoided. In the following, after discussing existing approaches to SRL (Section 2), a distributional approach is defined in Section 3. Section 3.2 discusses the proposed HMM-based treatment of joint inferences in argument classification. The large scale experiments described in Section 4 will allow to draw the conclusions of Section 5. 2 Related Work State-of-art approaches to frame-based SRL are based on Suppor</context>
<context position="25123" citStr="Pado and Lapata, 2007" startWordPosition="4052" endWordPosition="4056">lections are downloadable at: nlp.cs.swarthmore.edu/semeval/tasks/task19/data/train.tar.gz P(ri7 hi|FEθ(i)) = P(FEθ(i)) P(FEθ(i)|ri7 hi) P(ri7 hi) 242 Corpus Predicates Arguments training FN-BNC 134,697 271,560 test in-domain FN-BNC 14,952 30,173 out-of-domain NTI 8,208 14,422 ANC 760 1,389 Table 2: Training and Testing data sets (ANC)5. Table 2 shows the predicates and arguments in each data set. All null-instantiated arguments were removed from the training and test sets. � Vectors h representing semantic heads have been computed according to the ”dependencybased” vector space discussed in (Pado and Lapata, 2007). The entire BNC corpus has been parsed and the dependency graphs derived from individual sentences provided the basic observable contexts: every co-occurrence is thus syntactically justified by a dependency arc. The most frequent 30,000 basic features, i.e. (syntactic relation,lemma) pairs, have been used to build the matrix M, vector components corresponding to point-wise mutual information scores. Finally, the final space is obtained by applying the SVD reduction over M, with a dimensionality cut of l = 250. In the evaluation of the AC task, accuracy is computed over the nodes of the depend</context>
</contexts>
<marker>Pado, Lapata, 2007</marker>
<rawString>Sebastian Pado and Mirella Lapata. 2007. Dependency-based construction of semantic space models. Computational Linguistics, 33(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pado</author>
</authors>
<title>Cross-Lingual Annotation Projection Models for Role-Semantic Information.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>Saarland University.</institution>
<contexts>
<context position="14334" citStr="Pado, 2007" startWordPosition="2206" endWordPosition="2207">ilarity, as well as the global information on the entire argument sequence. 3.1 Distributional Local Models As the classification of semantic roles is strictly related to the lexical meaning of argument heads, we adopt a distributional perspective, where the meaning is described by the set of textual contexts in which words appear. In distributional models, words are thus represented through vectors built over these observable contexts: similar vectors suggest semantic relatedness as a function of the distance between two words, capturing paradigmatic (e.g. synonymy) or syntagmatic relations (Pado, 2007). Vectors h are described by an adjacency matrix M, whose rows describe target words (h) and whose columns describe their corpus contexts. Latent Semantic Analysis (LSA) (Landauer and Dumais, 1997), is then applied to �� M to acquire meaningful representations h . LSA exploits the linear transformation called Singular Value Decomposition (SVD) and produces an approximation of the original matrix M, capturing (semantic) dependencies between context vectors. M is replaced by a lower dimensional matrix Ml, capturing the same statistical information in a new l-dimensional space, where each dimensi</context>
</contexts>
<marker>Pado, 2007</marker>
<rawString>Sebastian Pado. 2007. Cross-Lingual Annotation Projection Models for Role-Semantic Information. Ph.D. thesis, Saarland University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Dan Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The proposition bank: A corpus annotated with semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="1878" citStr="Palmer et al., 2005" startWordPosition="275" endWordPosition="278"> lexicon for the core vocabulary of English, since 1997. A frame is evoked in texts through the occurrence of its lexical units (LU), i.e. predicate words such verbs, nouns, or adjectives, and specifies the participants and properties of the situation it describes, the so called frame elements (FEs). Semantic Role Labeling (SRL) is the task of automatic recognition of individual predicates together with their major roles (e.g. frame elements) as they are grammatically realized in input sentences. It has been a popular task since the availability of the PropBank and FrameNet annotated corpora (Palmer et al., 2005), the seminal work of (Gildea and Jurafsky, 2002) and the successful CoNLL evaluation campaigns (Carreras and M`arquez, 2005). Statistical machine learning methods, ranging from joint probabilistic models to support vector machines, have been successfully adopted to provide very accurate semantic labeling, e.g. (Carreras and M`arquez, 2005). SRL based on FrameNet is thus not a novel task, although very few systems are known capable of completing a general frame-based annotation process over raw texts, noticeable exceptions being discussed for example in (Erk and Pado, 2006), (Johansson and Nug</context>
<context position="4897" citStr="Palmer et al., 2005" startWordPosition="728" endWordPosition="731">(i.e. NTI corpus). Out-of-domain tests seem to suggest the models trained on BNC do not generalize well to novel grammatical and lexical phenomena. As also suggested in (Pradhan et al., 2008), the major drawback is the poor generalization power affecting lexical features. Notice how this is also a general problem of statistical learning processes, as large fine grain feature sets are more exposed to the risks of overfitting. The above problems are particularly critical for frame-based shallow semantic parsing where, as opposed to more syntactic-oriented semantic labeling schemes (as Propbank (Palmer et al., 2005)), a significant mismatch exists between the semantic descriptors and the underlying syntactic annotation level. In (Johansson and Nugues, 2008b) an upper bound of about 83.9% for the accuracy of the argument identification task is reported, it is due to the complexity in projecting frame element boundaries out from the dependency graph: more than 16% of the roles in the annotated material lack of a clear grammatical status. The limited level of linguistic generalization outlined above is still an open research problem. Existing solutions have been proposed in literature along different lines.</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005. The proposition bank: A corpus annotated with semantic roles. Computational Linguistics, 31(1), March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer S Pradhan</author>
<author>Wayne Ward</author>
<author>James H Martin</author>
</authors>
<title>Towards robust semantic role labeling.</title>
<date>2008</date>
<journal>Comput. Linguist.,</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="4468" citStr="Pradhan et al., 2008" startWordPosition="664" endWordPosition="667">for Computational Linguistics, pages 237–246, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics the training one (i.e. Wall Street Journal), e.g. (Toutanova et al., 2008). More recently, the stateof-art frame-based semantic role labeling system discussed in (Johansson and Nugues, 2008b) reports a 19% drop in accuracy for the argument classification task when a different test domain is targeted (i.e. NTI corpus). Out-of-domain tests seem to suggest the models trained on BNC do not generalize well to novel grammatical and lexical phenomena. As also suggested in (Pradhan et al., 2008), the major drawback is the poor generalization power affecting lexical features. Notice how this is also a general problem of statistical learning processes, as large fine grain feature sets are more exposed to the risks of overfitting. The above problems are particularly critical for frame-based shallow semantic parsing where, as opposed to more syntactic-oriented semantic labeling schemes (as Propbank (Palmer et al., 2005)), a significant mismatch exists between the semantic descriptors and the underlying syntactic annotation level. In (Johansson and Nugues, 2008b) an upper bound of about 8</context>
<context position="8908" citStr="Pradhan et al., 2008" startWordPosition="1352" endWordPosition="1355"> (Johansson and Nugues, 2008b) the impact of different grammatical representations on the task of frame-based shallow semantic parsing is studied and the poor lexical generalization problem is outlined. An argument classification accuracy of 89.9% over the FrameNet (i.e. BNC) dataset is shown to decrease to 71.1% when a different test domain is evaluated (i.e. the Nuclear Threat Initiative corpus). The argument classification 238 component is thus shown to be heavily domaindependent whereas the inclusion of grammatical function features is just able to mitigate this sensitivity. In line with (Pradhan et al., 2008), it is suggested that lexical features are domain specific and their suitable generalization is not achieved. The lack of suitable lexical information is also discussed in (F¨urstenau and Lapata, 2009) through an approach aiming to support the creation of novel annotated resources. Accordingly a semisupervised approach for reducing the costs of the manual annotation effort is proposed. Through a graph alignment algorithm triggered by annotated resources, the method acquires training instances from an unlabeled corpus also for verbs not listed as existing FrameNet predicates. 2.1 The role of L</context>
</contexts>
<marker>Pradhan, Ward, Martin, 2008</marker>
<rawString>Sameer S. Pradhan, Wayne Ward, and James H. Martin. 2008. Towards robust semantic role labeling. Comput. Linguist., 34(2):289–310.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F Tjong Kim Sang</author>
<author>Sabine Buchholz</author>
</authors>
<title>Introduction to the conll-2000 shared task: chunking.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2nd workshop on Learning language in logic and the 4th conference on Computational natural language learning,</booktitle>
<pages>127--132</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="9814" citStr="Sang and Buchholz, 2000" startWordPosition="1491" endWordPosition="1494">. Accordingly a semisupervised approach for reducing the costs of the manual annotation effort is proposed. Through a graph alignment algorithm triggered by annotated resources, the method acquires training instances from an unlabeled corpus also for verbs not listed as existing FrameNet predicates. 2.1 The role of Lexical Semantic Information It is widely accepted that lexical information (as features directly derived from word forms) is crucial for training accurate systems in a number of NLP tasks. Indeed, all the best systems in the CoNLL shared task competitions (e.g. Chunking (Tjong Kim Sang and Buchholz, 2000)) make extensive use of lexical information. Also lexical features are beneficial in SRL usually either for systems on Propbank as well as for FrameNetbased annotation. In (Goldberg and Elhadad, 2009), a different strategy to incorporate lexical features into classification models is proposed. A more expressive training algorithm (i.e. anchored SVM) coupled with an aggressive feature pruning strategy is shown to achieve high accuracy over a chunking and named entity recognition task. The suggested perspective here is that effective semantic knowledge can be collected from sources external to t</context>
</contexts>
<marker>Sang, Buchholz, 2000</marker>
<rawString>Erik F. Tjong Kim Sang and Sabine Buchholz. 2000. Introduction to the conll-2000 shared task: chunking. In Proceedings of the 2nd workshop on Learning language in logic and the 4th conference on Computational natural language learning, pages 127–132, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Aria Haghighi</author>
<author>Christopher D Manning</author>
</authors>
<title>A global joint model for semantic role labeling.</title>
<date>2008</date>
<journal>Comput. Linguist.,</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="3297" citStr="Toutanova et al., 2008" startWordPosition="487" endWordPosition="490">ployed learning algorithms are based on complex sets of syntagmatic features, as deeply investigated in (Johansson and Nugues, 2008b). The resulting recognition is thus highly dependent on the accuracy of the underlying parser, whereas wrong structures returned by the parser usually imply large misclassification errors. Annotation costs. Statistical learning approaches applied to SRL are very demanding with respect to the amount and quality of the training material. The complex SRL architectures proposed (usually combining local and global, i.e. joint, models of argument classification, e.g. (Toutanova et al., 2008)) require a large number of annotated examples. The amount and quality of the training data required to reach a significant accuracy is a serious limitation to the exploitation of SRL in many NLP applications. Limited Linguistic Generalization. Several studies showed that even when large training sets exist the corresponding learning exhibits poor generalization power. Most of the CoNLL 2005 systems show a significant performance drop when the tested corpus, i.e. Brown, differs from 237 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 237–246, Upps</context>
<context position="5609" citStr="Toutanova et al., 2008" startWordPosition="841" endWordPosition="844">actic annotation level. In (Johansson and Nugues, 2008b) an upper bound of about 83.9% for the accuracy of the argument identification task is reported, it is due to the complexity in projecting frame element boundaries out from the dependency graph: more than 16% of the roles in the annotated material lack of a clear grammatical status. The limited level of linguistic generalization outlined above is still an open research problem. Existing solutions have been proposed in literature along different lines. Learning from richer linguistic descriptions of more complex structures is proposed in (Toutanova et al., 2008). Limiting the cost required for developing large domainspecific training data sets has been also studied, e.g., (F¨urstenau and Lapata, 2009). Finally, the application of semi-supervised learning is attempted to increase the lexical expressiveness of the model, e.g. (Goldberg and Elhadad, 2009). In this paper, this last direction is pursued. A semi-supervised statistical model exploiting useful lexical information from unlabeled corpora is proposed. The model adopts a simple feature space by relying on a limited set of grammatical properties, thus reducing its learning capacity. Moreover, it </context>
<context position="7409" citStr="Toutanova et al., 2008" startWordPosition="1119" endWordPosition="1122">nferences in argument classification. The large scale experiments described in Section 4 will allow to draw the conclusions of Section 5. 2 Related Work State-of-art approaches to frame-based SRL are based on Support Vector Machines, trained over linear models of syntactic features, e.g. (Johansson and Nugues, 2008b), or tree-kernels, e.g. (Coppola et al., 2009). SRL proceeds through two main steps: the localization of arguments in a sentence, called boundary detection (BD), and the assignment of the proper role to the detected constituents, that is the argument classification, (AC) step. In (Toutanova et al., 2008) a SRL model over Propbank that effectively exploits the semantic argument frame as a joint structure, is presented. It incorporates strong dependencies within a comprehensive statistical joint model with a rich set of features over multiple argument phrases. This approach effectively introduces a new step in SRL, also called Joint Re-ranking, (RR), e.g. (Toutanova et al., 2008) or (Moschitti et al., 2008). First local models are applied to produce role labels over individual arguments, then the joint model is used to decide the entire argument sequence among the set of the n-best competing so</context>
</contexts>
<marker>Toutanova, Haghighi, Manning, 2008</marker>
<rawString>Kristina Toutanova, Aria Haghighi, and Christopher D. Manning. 2008. A global joint model for semantic role labeling. Comput. Linguist., 34(2):161–191.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>