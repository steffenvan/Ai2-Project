<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000194">
<title confidence="0.8908695">
Seed and Grow: Augmenting Statistically Generated Summary Sentences
using Schematic Word Patterns
</title>
<figure confidence="0.316276666666667">
C´ecile Paris$
$ICT Centre
CSIRO
</figure>
<address confidence="0.711162">
Sydney, Australia
</address>
<email confidence="0.910054">
Cecile.Paris@csiro.au
</email>
<author confidence="0.998927">
Stephen Wan†$ Robert Dale† Mark Dras††Centre for Language Technology
</author>
<affiliation confidence="0.9911015">
Department of Computing
Macquarie University
</affiliation>
<address confidence="0.917359">
Sydney, NSW 2113
</address>
<email confidence="0.997937">
swan,madras,rdale@ics.mq.edu.au
</email>
<sectionHeader confidence="0.993879" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998013125">
We examine the problem of content selection
in statistical novel sentence generation. Our
approach models the processes performed by
professional editors when incorporating ma-
terial from additional sentences to support
some initially chosen key summary sentence,
a process we refer to as Sentence Augmen-
tation. We propose and evaluate a method
called “Seed and Grow” for selecting such
auxiliary information. Additionally, we argue
that this can be performed using schemata, as
represented by word-pair co-occurrences, and
demonstrate its use in statistical summary sen-
tence generation. Evaluation results are sup-
portive, indicating that a schemata model sig-
nificantly improves over the baseline.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999962705882353">
In the context of automatic text summarisation, we
examine the problem of statistical novel sentence
generation, with the aim of moving from the current
state-of-the-art of sentence extraction to abstract-
like summaries. In particular, we focus on the task of
selecting content to include within a generated sen-
tence.
Our approach to novel sentence generation is to
model the processes underlying summarisation as
performed by professional editors and abstractors.
An example of the target output of this kind of gen-
eration is presented in Figure 1. In this example, the
human authored summary sentence was taken verba-
tim from the executive summary of a United Nations
proposal for the provision of aid addressing a partic-
ular humanitarian crisis. Such documents typically
exceed a hundred pages.
</bodyText>
<subsubsectionHeader confidence="0.766031">
Human-Authored Summary Sentence:
</subsubsectionHeader>
<bodyText confidence="0.9904642">
Repeated [poor seasonal rains]1 [in 2004]2, culminating
in [food insecurity]3, indicate [another year]4 of crisis,
the scale of which is larger than last year’s and is further
[exacerbated by diminishing coping assets]5 [in both
rural and urban areas]6.
</bodyText>
<subsubsectionHeader confidence="0.5072">
Key Source Sentence:
</subsubsectionHeader>
<bodyText confidence="0.9209865">
The consequences of [another year]4 of [poor rains]1 on
[food security]3 are severe.
</bodyText>
<subsubsectionHeader confidence="0.669362">
Auxiliary Source Sentence(s):
</subsubsectionHeader>
<bodyText confidence="0.9843088">
However in addition to the needs of economic recovery
activities for IDPs, [food insecurity]3 [over the major-
ity of 2004]2 [has created great stress]5 on the poorest
families in the country, [both within the urban and rural
settings]6.
</bodyText>
<figureCaption confidence="0.983898">
Figure 1: Alignment of a summary sentence to sentences
in the full document. Phrases of similar meaning are co-
indexed.
</figureCaption>
<bodyText confidence="0.9999565">
To write such summaries, we assume that the hu-
man abstractor begins by choosing key sentences
from the full document. Then, for each key sen-
tence, a set of auxiliary material is identified. The
key sentence is revised incorporating these auxil-
iary sentences to produce the eventual summary sen-
tence.
To study this phenomenon, a corpus of UN docu-
ments was collected and analysed.1 Each document
was divided into two parts comprising its executive
summary, and the remainder, referred to here as the
source. We manually aligned each executive sum-
mary sentence with one or more sentences from the
source, by choosing a key sentence that provided
</bodyText>
<footnote confidence="0.948946">
1This corpus is described in detail in Section 5.1.
</footnote>
<page confidence="0.946256">
543
</page>
<note confidence="0.9623345">
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 543–552,
Honolulu, October 2008.c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.954001857142857">
evidence for the content of the summary sentence
along with additional sentences that provided sup-
porting material.
We refer to the resulting corpus as the UN Con-
solidated Appeals Process (UN CAP) corpus. It is
a collection of sentence alignments, each referred to
as an aligned sentence tuple, which consists of:
</bodyText>
<listItem confidence="0.9928052">
1. A human authored summary sentence from the
executive summary;
2. A key sentence from the source;
3. Zero or more auxiliary sentences from the
source.
</listItem>
<bodyText confidence="0.999832612500001">
The key and any auxiliary sentences are referred to
collectively as the aligned source sentences.
We argue that some process that combines infor-
mation from multiple sentences is required if we are
to generate summary sentences similar to that por-
trayed in Figure 1. This is supported by our analysis
of the UN CAP corpus. Of the 580 aligned sentence
tuples, the majority, 61% of cases, appear to be ex-
amples of such a process.
Furthermore, the auxiliary sentences are clearly
necessary. We found that only 30% of the open-class
words in the summary are found in the key sentence.
If one selects all the open-class words from aligned
source sentences, recall increases to an upper limit
of 45% without yet accounting for stemming. This
upper bound is consistent with the upper limit of
50% found by Daum´e III and Marcu (2005) which
takes into account stemming differences.
This demonstrates that the auxiliary material is
a valuable source of content which should be inte-
grated into the summary sentence, allowing an im-
provement in recall of up to 15% prior to account-
ing for morphological, synonym and paraphrase dif-
ferences. Of course, the trick is to improve recall
without hurting precision. A naive addition of all
words in the aligned source sentences incurs a drop
in precision from 30% to 23%. The problem thus is
one of selecting the relevant auxiliary content words
without introducing unimportant content. We refer
to this problem of incorporating material from aux-
iliary sentences to supplement a key sentence as Sen-
tence Augmentation.
In this paper, sentence augmentation is modelled
as a noisy channel process and has two facets: con-
tent selection and language modelling. This paper
focuses on the former, in which the system must
rank text segments—in this case, words—for inclu-
sion in the generated sentence. Given a ranked se-
lection of words, a language model would then order
them appropriately, as described in work on sentence
regeneration (for example, see Soricut and Marcu
(2005); Wan et al. (2005)).
Provided with an aligned sentence tuple, the prob-
lem lies in effectively selecting words from the aux-
iliary sentences to bolster those taken from the key
sentence. Given that there are on average 2.7 aux-
iliary sentences per aligned sentence tuple, this ad-
ditional influx of words poses a considerable chal-
lenge.
We begin with the premise that, for documents
of a homogeneous type (in this case, the genre is
a funding proposal, and the domain is humanitarian
aid), it may be possible to identify patterns in the or-
ganisation of information in summaries. For exam-
ple, Figure 2 presents three summary sentences from
our corpus that share the same patterned juxtapo-
sition of two concepts DisplacedPersons and Host-
ingCommunities. Documents may exhibit common
patterns since they have a similar goal: namely, to
convince donors to give financial support. In the
above example, the juxtaposition highlights the fact
that those in need are not just those people from the
‘epicenter’ of the crisis but also those that look after
them.
We propose and evaluate a method called “Seed
and Grow” for selecting content from auxiliary sen-
tences. That is, we first select the core meaning of
the summary, given here by the key sentence, and
then we find those pieces of additional information
that are conventionally juxtaposed with it.
Such patterns are reminiscent of Schemata, the or-
ganisations of propositional content introduced by
McKeown (1985). Schemata typically involve a
symbolic representation of each proposition’s se-
mantics. However, in our case, a text-to-text gener-
ation scenario, we are without such representations
and so must find other means to encode these pat-
terns.
To alleviate the situation, we turn to word-pair co-
occurrences to approximate schematic patterns. Fig-
</bodyText>
<page confidence="0.994861">
544
</page>
<tableCaption confidence="0.540316">
Sentence 1:
</tableCaption>
<bodyText confidence="0.98387225">
The increased number of [internally displaced persons]1
and the continued presence of refugees have fur-
ther strained the scarce natural resources of [host
communities]2, stretching their capacity to the limit.
</bodyText>
<tableCaption confidence="0.806373416666667">
Sentence 2:
100,000 people, a significant portion of the population,
remain [displaced]1, burdening the already precarious
living conditions of [host families]2 in Dili and the
Districts.
Sentence 3:
The current humanitarian situation in Timor-Leste is
characterised by: An estimated [100,000 displaced
people]1 (10% of the population) living in camps and
with [host families]2 in the districts; A total or partial de-
struction of over 3,000 homes in Dili affecting at least
14,000 IDPs
</tableCaption>
<figureCaption confidence="0.96181">
Figure 2: Examples of the pattern (DisplacedPersons[1],
HostingCommunities[2]).
</figureCaption>
<bodyText confidence="0.996447121212121">
ure 2 showed that mentions of the plight of interna-
tionally displaced persons are often followed by de-
scriptions of the impact on the host communities that
look after them. In this particular example, this is
realised lexically in the co-occurrences of the words
displaced and host.
Corpus-based methods inspired by the notion of
schemata have been explored in the past by Lap-
ata (2003) and Barzilay and Lee (2004) for order-
ing sentences extracted in a multi-document sum-
marisation application. However, to our knowledge,
using word co-occurrences in this manner to repre-
sent schematic knowledge for the purposes of select-
ing content in a statistically-generated summary sen-
tence has not previously been explored.
This paper seeks to determine whether or not such
patterns exist in homogeneous data; and further-
more, whether such patterns can be used to better
select words from auxiliary sentences. In particular,
we propose the “Seed and Grow” approach for this
task. The results show that even simple modelling
approaches are able to model this schematic infor-
mation.
In the remainder of this paper, we contrast our ap-
proach to related text-to-text research in Section 2.
The Content Selection model is presented in Section
3. Section 4 describes how a binary classification
model is used in a statistical text generation system.
Section 5 describes our evaluation of the model for a
summary generation task. We conclude, in Section
6, that domain-specific schematic patterns can be ac-
quired and applied to content selection for statistical
sentence generation.
</bodyText>
<sectionHeader confidence="0.999917" genericHeader="introduction">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.999906">
2.1 Content Selection in Text-to-Text Systems
</subsectionHeader>
<bodyText confidence="0.999972216216216">
Statistical text-to-text summarisation applications
have borrowed much from the related field of statis-
tical machine translation. In one of the first works to
present summarisation as a noisy channel approach,
Witbrock and Mittal (1999) presented a conditional
model for learning the suitability of words from a
news article for inclusion in headlines, or ‘ultra-
summaries’. Inspired by this approach, and with
the intention of designing a robust statistical gener-
ation system, our work is also based on the noisy
channel model. Into this, we incorporate our con-
tent selection model, which includes Witbrock and
Mittal’s model supplemented with schema-based in-
formation.
Roughly, text-to-text transformations fall into
three categories: those in which information is com-
pressed, conserved, and augmented. We use these
distinctions to organise this overview of the litera-
ture.
In Sentence Compression work, a single sentence
undergoes pruning to shorten its length. Previ-
ous approaches have focused on statistical syntactic
transformations (Knight and Marcu, 2002). For con-
tent selection, discourse-level considerations were
proposed by Daum´e III and Marcu (2002), who ex-
plored the use of Rhetorical Structure Theory (Mann
and Thompson, 1988). More recently, Clarke and
Lapata (2007) use Centering Theory (Grosz et al.,
1995) and Lexical Chains (Morris and Hirst, 1991)
to identify which information to prune. Our work is
similar in incorporating discourse-level phenomena
for content selection. However, we look at schema-
like information as opposed to chains of references
and focus on the sentence augmentation task.
The work of Barzilay and McKeown (2005) on
Sentence Fusion introduced the problem of convert-
ing multiple sentences into a single summary sen-
</bodyText>
<page confidence="0.990503">
545
</page>
<bodyText confidence="0.99989575862069">
tence. Each sentence set ideally tightly clusters
around a single news event. Thus, there is one gen-
eral proposition to be realised in the summary sen-
tence, identified by finding the common elements in
the input sentences. We see this as an example of
conservation. In our work, this general proposition
is equivalent to the core information for the sum-
mary sentence before the incorporation of supple-
mentary material.
In contrast to both compression and conservation
work, we focus on augmenting the information in
a key sentence. The closest work is that of Jing
and McKeown (1999) and Daum´e III and Marcu
(2005), in which multiple sentences are processed,
with fragments within them being recycled to gener-
ate the novel generated text.
In both works, recyclable fragments are identified
by automatic means. Jing and McKeown (1999) use
models that are based on “copy-and-paste” opera-
tions learnt from the behaviour of human abstrac-
tors as found in a corpus. Daum´e III and Marcu
(2005) propose a model that encodes how likely it
is that different sized spans of text are skipped to
reach words and phrases to recycle.
While similar in task, our models differ substan-
tially in the nature of the phenomenon modelled. In
this work, we focus on content-based considerations
that model which words can be combined to build
up a new sentence.
</bodyText>
<subsectionHeader confidence="0.996576">
2.2 Schemata and Text Generation
</subsectionHeader>
<bodyText confidence="0.99997744">
There exists related work from Natural Language
Generation (NLG) in finding material to build up
sentences. As mentioned above, our content selec-
tion model is inspired by work on schemata from
NLG (McKeown, 1985). Barzilay and Lee (2004)
showed that it is possible to obtain schema-like
knowledge automatically from a corpus for the pur-
poses of extracting sentences and ordering them.
However, their work represents patterns at the sen-
tence level, and is thus not directly comparable to
our work, given our focus on sentence generation.
In our system, what is required is a means to rank
words for use in generation. Thus, we focus on com-
monly occurring word co-occurrences, with the aim
of encoding conventions in the texts we are trying to
generate. In this respect, this is similar to work by
Lapata (2003), who builds a conditional model of
words across adjacent sentences, focusing on words
in particular semantic roles. Like Barzilay and Lee
(2004), this model was used to order extracted sen-
tences in summaries. In contrast, our work focuses
on word patterns found within a summary sentence,
not between sentences. Additionally, our tasks dif-
fer as we examine the statistical sentence generation
instead of sentence ordering.
</bodyText>
<sectionHeader confidence="0.9254745" genericHeader="method">
3 Linguistic Intuitions behind Word
Selection
</sectionHeader>
<bodyText confidence="0.999944">
The “Seed and Grow” approach proposed in this pa-
per divides the word-level content selection prob-
lem into two underlying subproblems. We address
these with two separate models, called the salience
and schematic models. The salience model chooses
the key content for the summary sentence while the
schematic model attempts to identify what else is
typically mentioned given those salient pieces of in-
formation.
</bodyText>
<subsectionHeader confidence="0.998874">
3.1 A Salience Model: Learning “Buzzwords”
</subsectionHeader>
<bodyText confidence="0.999966956521739">
There are a variety of methods for determining the
salient information in a text, and these underpin
most work in automatic text summarisation. As an
example of a salience model trained on corpus data,
Witbrock and Mittal (1999) introduced a method for
scoring summary words for inclusion within news
headlines. In their model, headlines were treated as
‘ultra-summaries’. Their model learns which words
are typically used in headlines and encodes, at least
to some degree, which words are attention grabbing.
In the domain of funding proposals, key words
that grab attention may amount to domain-specific
buzzwords. Intuitively, a reader, perhaps someone
in charge of allocating donations, tends to look for
certain types of key information matching donation
criteria, and so human abstract authors will target
their summaries for this purpose.
We thus adapt the Witbrock and Mittal (1999)
model to identify such domain specific buzzwords
(BWM, for ‘buzzword model’). For an aligned sen-
tence tuple, the probability that a word is selected
based on the salience of a word with respect to the
domain is defined as:
</bodyText>
<equation confidence="0.9963395">
probbwm(select = 1|w) = |summaryw|
|sourcew |(1)
</equation>
<page confidence="0.972114">
546
</page>
<bodyText confidence="0.999995076923077">
where summaryw is the set of aligned sentence tu-
ples that contain the word w in the summary sen-
tence and in the source sentences. The denomina-
tor, sourcew, is the set of aligned sentence tuples that
have the word w in either the key or an auxiliary sen-
tence.
As is implicit in this equation, we could just use
this buzzword model to select content not only from
the key sentence, but from the auxiliary sentences
as well. While it is intended ultimately to find the
key content of the summary, it can also serve as an
alternative baseline for auxiliary content selection to
compare against the “Seed and Grow” model.
</bodyText>
<subsectionHeader confidence="0.9421855">
3.2 A Schema Model: Approximation via
Word co-Occurrences
</subsectionHeader>
<bodyText confidence="0.99993105">
To restate the problem at hand: the task is one
of finding elements of secondary importance that
schematically elaborate on the key information. We
do this by examining sample summary sentences for
conventional juxtapositions of concepts. As men-
tioned in Section 1, schemata are approximated here
with patterns of word-pair co-occurrences. Using a
corpus of human-authored summaries in the domain
of our application, it is thus possible to learn what
those common combinations of words are.
Roughly, the process is as follows. To begin with,
a seed set of words is chosen. The purpose of the
seed set is to represent the core proposition of the
summary sentence.
In this work, this core proposition is given by the
key sentence and so the non-stopwords belonging to
it are used to populate the seed set. In the “Seed and
Grow” approach, we check to see which words from
auxiliary sentences pair well with words in the seed
set.
</bodyText>
<subsectionHeader confidence="0.956741">
3.2.1 Collecting Word-level Patterns
</subsectionHeader>
<bodyText confidence="0.999788">
Each training case in the corpus contains a single
human-authored summary sentence that can be used
to learn which pairs of words conventionally occur
in a summary. For each summary sentence, stop-
words are removed. Then, each pairing of words in
the sentence is used to update a pair-wise word co-
occurrence frequency table. When looking up and
storing a frequency, the order of words is ignored.
</bodyText>
<subsectionHeader confidence="0.7357855">
3.2.2 Scoring Word-Pair Co-occurrence
Strength
</subsectionHeader>
<bodyText confidence="0.999702666666667">
For any two words, w1 from the seed set and w2 from
an auxiliary sentence, the word-pair co-occurrence
probability is defined as follows:
</bodyText>
<equation confidence="0.9988095">
probco-oc(w1,w2)
freq(w1,w2)
�
freq(w1) + freq(w2) − freq(w1,w2) (2)
</equation>
<bodyText confidence="0.999356">
where freq(w1,w2) is a lookup in the word-pair co-
occurrence frequency table. This table stores co-
occurrence word pairs occurring in the summary
sentence.
</bodyText>
<subsectionHeader confidence="0.907784">
3.2.3 Combining a Set of Co-occurrence Scores
</subsectionHeader>
<bodyText confidence="0.99997659375">
Each auxiliary word now has a series of scores,
one for each comparison with a seed word. To rank
each auxiliary word, these need to be combined into
a single score for sorting.
When combining the set of co-occurrence scores,
one might want to account for the fact that each pair-
ing of a seed word with an auxiliary word might
not contribute equally to the overall selection of that
auxiliary word. Intuitively, a word in the seed set,
derived from the key sentence, may only make a
minor contribution to the core meaning of the sum-
mary sentence. For example, words that are part of
an adjunct phrase in the key sentence might not be
good candidates to elaborate upon. Thus, one might
want to weight these seed words lower, to reduce
their influence on triggering schematically associ-
ated words.
To allow for this, a seed weight vector is main-
tained, storing a weight per seed word. Different
weighting schemes are possible. For example, a
scheme might indicate the salience of a word. In
addition to the buzzword model (BWM) described
earlier, one might employ a standard vector space
approach (Salton and McGill, 1983) from Informa-
tion Retrieval, which uses term frequency scores
weighted with an inverse document frequency fac-
tor, or tf-idf. We also implement the case in which all
seed words are treated equally using binary weights,
where 1 indicates the presence of a seed word, and
0 indicates its absence. In the evaluations described
in Section 5, we refer to these three seed weighting
schemes as bwm and tf-idf, and binary respectively.
</bodyText>
<page confidence="0.987338">
547
</page>
<bodyText confidence="0.997680833333333">
To find the probability of selecting an auxiliary
word using the schematic word-pair co-occurrence
model (WCM), an averaged probability is found
by normalising the sum of the weighted probabili-
ties, where weights are provided by one of the three
schemes above:
</bodyText>
<equation confidence="0.999711">
probwcm(wi) =
weightsk × probco-oc(wi,wk) (3)
</equation>
<bodyText confidence="0.99971525">
where seed is the set of seed words and wk is the kth
word in that set. The vector, weights, stores the seed
weights. The normalisation factor for the weighted
average, Z, is the number of auxiliary words.
Finally, since the WCM model only serves to se-
lect words from the auxiliary sentences, words from
the key sentence must be given scores as well. For
these words, the scoring is as follows:
</bodyText>
<equation confidence="0.99899275">
� 1 �
1
probwcm(w) = |seed |+ probwcm(w) (4)
Z
</equation>
<bodyText confidence="0.964933">
where Z is a normalisation across the set of seed
words.
</bodyText>
<sectionHeader confidence="0.985365" genericHeader="method">
4 Combining Buzzwords and Word-Pair
Co-Occurrence Models for Generation
</sectionHeader>
<bodyText confidence="0.999924076923077">
As mentioned above, the noisy channel approach
is used for producing the augmented sentence. Al-
though the focus of this paper is on Content Selec-
tion, an overview of the end-to-end generation pro-
cess is presented for completeness.
Sentence augmentation is essentially a text-to-text
process: A key sentence and auxiliary material are
transformed into a single summary sentence. Fol-
lowing Witbrock and Mittal (1999), the task is to
search for the string of words that maximises the
probability prob(summary|source). Standardly re-
formulating this probability using Bayes’ rule re-
sults in the following:
</bodyText>
<equation confidence="0.978318">
probcm(source|summary) × problm(summary) (5)
</equation>
<bodyText confidence="0.999888388888889">
In this paper, we are concerned with the first
factor, probcm(source|summary), referred to as the
channel model (CM), which combines both the
buzzword (BWM) and word-pair co-occurrence
(WCM) models. An examination of differences be-
tween the two approaches revealed only a 20% word
overlap on the Jaccard metric.
In order to combine multiple models, we intend
to use machine learning approaches to combine the
information in each model in a similar manner to
Berger et al. (1996). We are currently exploring the
use of logistic regression methods to learn a func-
tion that would treat, as features, the probabilities
defined by the salience and schematic content selec-
tion models. Although generation is possible using
each content selection model in isolation, evalua-
tions of the combined model are on-going and are
not presented in this paper.
</bodyText>
<sectionHeader confidence="0.99866" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.9999845">
In this evaluation, the task is to select n words from
the aligned source sentences for inclusion in a sum-
mary. As a gold-standard for comparison, we sim-
ply examine what words were actually chosen in the
summary sentence of the aligned sentence tuple. We
are specifically interested in open-class words, and
so a stopword list of closed-class words is used to
filter the sentences in each test case.
We evaluate against the set of open-class words
in the human-authored summary sentence using re-
call and precision metrics. Recall is the size of
the intersection of the selected and gold-standard
sets, normalised by the length of the gold-standard
sentence (in words). This recall metric is similar
to the ROUGE-1 metric, the unigram version of
the ROUGE metric (Lin and Hovy, 2003) used in
the Document Understanding Conferences2 (DUC).
Precision is the size of the intersection normalised
by the number of words selected. We also report the
F-measure, which is the harmonic mean of the recall
and precision scores.
Recall, precision and F-measure are measured at
various values of n ranging from 1 to the number of
open-class words in the gold-standard summary sen-
tence for a particular test case. For the purposes of
evaluation, differences in tokens due to morphology
were explored crudely via the use of Porter’s stem-
ming algorithm. However, the results from stem-
ming are not that different from exact token matches
when examining performance on the entire data set
</bodyText>
<footnote confidence="0.562407">
2http://duc.nist.gov
</footnote>
<figure confidence="0.977290166666667">
×
1
Z
|seed|
∑
k=0
</figure>
<page confidence="0.979255">
548
</page>
<table confidence="0.998969428571429">
Number of training cases 530
Average words in summary sentence 27.0
Average stopwords in summary sentence 10.3
Average number of auxiliary sentences 2.75
Word count: summary sentences 4630
Word count: source sentences 21356
Word type count in corpus 3800
</table>
<tableCaption confidence="0.999199">
Table 1: Statistics for the UN CAP training set
</tableCaption>
<bodyText confidence="0.928523">
and so, for simplicity, these are omitted in this dis-
cussion.
</bodyText>
<subsectionHeader confidence="0.986372">
5.1 The Data
</subsectionHeader>
<bodyText confidence="0.999799275862069">
The corpus is made up of a number of humanitar-
ian aid proposals called Consolidated Appeals Pro-
cess (UN CAP) documents, which are archived at
the United Nations website.3 135 documents from
the period 2002 to 2007 were downloaded by the au-
thors. A preprocessing stage extracted text from the
PDF files and segmented the documents into execu-
tive summary and source sections. These were then
automatically segmented further into sentences.
Executive summary sentences were manually
aligned by the authors to source key and auxiliary
sentences, producing a corpus of 580 aligned sen-
tence tuples referred to here as the UN CAP cor-
pus. Of these, 230 tuples were paraphrase cases (i.e.
without aligned auxiliary sentences). The remaining
550 cases were instances of sentence augmentation
(with at least one auxiliary sentence).
Of the 580 cases, 50 cases were set aside for test-
ing. The remaining 530 cases were used for train-
ing. Statistics for the training portion of the sentence
augmentation set are provided in Table 1.
In this paper, aligned sentence tuples are obtained
via manual annotation. Automatic construction
of these sentence-level alignments is possible and
has been explored by Jing and McKeown (1999).
We also envisage using tools for scoring sentence
similarity (for example, see Hatzivassiloglou et al.
(2001)) for automatically constructing them; this is
the focus of work by Wan and Paris (2008).
</bodyText>
<footnote confidence="0.901643">
3http://ochaonline3.un.org/humanitarianappeal/index.htm
</footnote>
<subsectionHeader confidence="0.99794">
5.2 The Baselines
</subsectionHeader>
<bodyText confidence="0.999959888888889">
Three baselines were used in this work: the random,
tf-idf and position baselines. A random word selec-
tor shows what performance might be achieved in
the absence of any linguistic knowledge.
We also sorted all words in the aligned source sen-
tences by their weighted tf-idf scores. This baseline
selects words in order until the desired word limit
is reached. This baseline is referred to as the tf-idf
baseline.
Finally, we selected words based on their sen-
tence order, choosing first those words from the key
sentence. When these are exhausted, auxiliary sen-
tences are sorted by their sentence positions in the
original document. Words from the first auxiliary
sentence are then chosen. This continues until ei-
ther the desired number of words have been chosen,
or no words remain. This baseline is known as the
position baseline.
</bodyText>
<subsectionHeader confidence="0.99934">
5.3 Content Selection Results
</subsectionHeader>
<bodyText confidence="0.999905730769231">
We compare the three baselines to the two mod-
els presented in Section 3. These are the buzzword
salience model (BWM) and the schematic word-pair
co-occurrence model (WCM).
We begin by presenting recall, precision and F-
measure graphs when selecting from the aligned
source sentences, comprising the key and auxiliary
sentences. Figure 3 shows the results for the two
models against the three baselines. The two mod-
els, the positional, and the tf-idf baselines perform
better than the random baseline, as measured by a
two-tailed Wilcoxon Matched Pairs Signed Ranks
test (α = 0.05).
The WCM consistently out-performs the BWM
on all metrics, and the differences are statistically
significant. In fact, the BWM also generally per-
forms worse than the position and tf-idf baselines.
WCM and the position baseline both significantly
outperform the tf-idf baseline on all metrics for
longer sentence lengths.
That the position baseline and WCM should per-
form similarly is not really surprising since, in ef-
fect, the position baseline first chooses words from
the key sentence and then selects auxiliary words.
The difference essentially lies in how the auxiliary
words are chosen.
</bodyText>
<page confidence="0.994306">
549
</page>
<figure confidence="0.999196833333333">
0 5 10 15 20 25 30
Number of Open-class Words Selected
0 5 10 15 20 25 30
Number of Open-class Words Selected
0 5 10 15 20 25 30
Number of Open-class Words Selected
</figure>
<figureCaption confidence="0.9993624">
Figure 3: Recall, Precision and F-measure performance
for open-class words from the entire input set (key and
auxiliary). Models presented are the Buzzword Model
(BWM), the Word-Pair Co-occurrence Model (WCM)
and position, tf-idf and random baselines.
</figureCaption>
<figure confidence="0.9978005">
0 5 10 15 20 25 30
Number of Open-class Words Selected
</figure>
<figureCaption confidence="0.9995715">
Figure 4: F-measure scores for content selection on just
the auxiliary sentences. Models presented are the Word-
Pair Co-occurrence model (WCM) and the position base-
line.
</figureCaption>
<bodyText confidence="0.999064870967742">
The results of Figure 3 weakly support the
hypothesis that using schematic word-pair co-
occurrences helps improve performance over mod-
els without discourse-related features. The graphs
show that WCM edges above the position base-
line when the number of selected open-class words
ranges from 10 to 15. Note that the average num-
ber of open-class words in a human authored sum-
mary sentence is 16. The only significant difference
found was in the F-measure and precision scores for
19 selected open-class words. Nevertheless, a gen-
eral trend can be observed in which WCM performs
better than the position baseline.
Ultimately, however, what we want to do is select
auxiliary content to supplement the key sentence.
To examine the effect of two best performing ap-
proaches, WCM and the position baseline, on this
task, were both modified so that the key sentence
words were explicitly given a zero probability. Thus,
the recall, precision and F-measure scores obtained
are based solely on the ability of either to select aux-
iliary words. The F-measure scores are presented
Figure 4. WCM consistently outperforms the po-
sition baseline for the selection of auxiliary words.
Differences are significant for 6 or more selected
open-class words.
The results show that even when considering only
exact token matches, we can improve on the re-
call of open-class words, and do so without penalty
in precision. Our working hypothesis is that such
gains are possible because the corpus has a homo-
</bodyText>
<figure confidence="0.999699826923077">
Recall
0.35
0.25
0.15
0.05
0.4
0.3
0.2
0.1
0
WCM
BWM
Position
tf.idf
Random
Precision
0.35
0.25
0.15
0.05
0.4
0.3
0.2
0.1
0
WCM
BWM
Position
tf.idf
Random
Fmeasure
0.25
0.15
0.05
0.3
0.2
0.1
0
WCM
BWM
Position
tf.idf
Random
Fmeasure
0.25
0.15
0.05
0.2
0.1
0
WCM
position
</figure>
<page confidence="0.988555">
550
</page>
<bodyText confidence="0.9999692">
geneous quality and key patterns are sufficiently re-
peated even when the overall data set is of the or-
der of hundreds of cases. The benefit of using a
model encoding some schematic information is fur-
ther shown by the performance of WCM over the
position baseline when selecting words from auxil-
iary sentences.
This is an interesting finding given that do-
main independent methods are increasingly used
on domain-specific corpora such as financial and
biomedical texts, for which we may have access to
only a limited amount of data. We anticipate that as
we introduce methods to account for paraphrase and
synonym differences, performance might rise fur-
ther still.
</bodyText>
<subsectionHeader confidence="0.991412">
5.4 Testing Seed Weighting Schemes
</subsectionHeader>
<bodyText confidence="0.999994772727273">
We can also weight seed words in the “Seed and
Grow” approach in a variety of ways. To test
whether weighting schemes have any effect on con-
tent selection performance, we examined the use
of three schemes. We were particularly interested
in those schemes that indicate the contribution of
a seed word to the core meaning of a sentence.
These are the binary, tf-idf and buzzword weight-
ing schemes described in Section 3. We present
the F-measure graph for these three variants of the
schematic word-pair co-occurrence model (WCM)
in Figure 5.
The graphs show that there is no discernible dif-
ference between the seed weighting schemes. No
scheme significantly outperforms another. Thus, we
conclude that the choice of these particular seed
weighting schemes has no effect on performance. In
future work, we intend to examine whether weight-
ing schemes encoding syntactic information might
fare better, since such information might more accu-
rately represent the contribution of a substring to the
main clause of the sentence.
</bodyText>
<sectionHeader confidence="0.998968" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999919666666667">
In this paper, we argued a case for sentence augmen-
tation, a component that facilitates abstract-like text
summarisation. We showed that such a process can
account for summary sentences as authored by pro-
fessional editors. We proposed the use of schemata,
as approximated with a word-pair co-occurrence
</bodyText>
<figure confidence="0.993106">
0 5 10 15 20 25 30
Number of Open-class Words Selected
</figure>
<figureCaption confidence="0.99314725">
Figure 5: F-measure performance for open-class words
from the entire input set (key and auxiliary). Models
presented are variants of the Word-Pair Co-occurrence
Model (WCM) that differ in the seed weighting schemes.
</figureCaption>
<bodyText confidence="0.9995465">
model, and advocated a new schema-based “Seed
and Grow” content selection model used for statisti-
cal sentence generation.
We also showed that domain-specific patterns,
schematic word-pair co-occurrences in this case, can
be acquired from a limited amount of data as indi-
cated by modest performance gains for content se-
lection using schemata information. We postulate
that this is particularly true when dealing with ho-
mogeneous data.
In future work, we intend to explore other string
matches corresponding to variations due to para-
phrases and synonymy. We would also like to study
the effects of corpus size when learning schematic
patterns. Finally, we are currently investigating the
use of machine learning methods to combine the
best of the Salience and Schemata models in order
to provide a single model for use in decoding.
</bodyText>
<sectionHeader confidence="0.999249" genericHeader="acknowledgments">
7 Acknowledgments
</sectionHeader>
<bodyText confidence="0.9988635">
We would like to thank the reviewers for their in-
sightful comments. This work was funded by the
CSIRO ICT Centre and Centre for Language Tech-
nology at Macquarie University.
</bodyText>
<sectionHeader confidence="0.999444" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9978662">
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In Daniel Marcu Su-
san Dumais and Salim Roukos, editors, HLT-NAACL
2004: Main Proceedings, pages 113–120, Boston,
</reference>
<figure confidence="0.998706636363636">
F����u��
0.25
0.15
0.05
0.3
0.2
0.1
0
binary
tfidf
BWM
</figure>
<page confidence="0.980512">
551
</page>
<reference confidence="0.999774909090909">
Massachusetts, USA, May 2 - May 7. Association for
Computational Linguistics.
Regina Barzilay and Kathleen R. McKeown. 2005. Sen-
tence fusion for multidocument news summarization.
Computational Linguistics, 31(3):297–328.
Adam L. Berger, Stephen Della Pietra, and Vincent
J. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Computational Lin-
guistics, 22(1):39–71.
James Clarke and Mirella Lapata. 2007. Modelling com-
pression with discourse constraints. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL), pages
1–11.
Hal Daum´e III and Daniel Marcu. 2002. A noisy-channel
model for document compression. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics (ACL – 2002), pages 449 – 456,
Philadelphia, PA, July 6 – 12.
Hal Daum´e III and Daniel Marcu. 2005. Induction
of word and phrase alignments for automatic doc-
ument summarization. Computational Linguistics,
31(4):505–530, December.
Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.
1995. Centering: A framework for modeling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2):203–225.
V. Hatzivassiloglou, J. Klavans, M. Holcombe, R. Barzi-
lay, M. Kan, and K. McKeown. 2001. Simfinder: A
flexible clustering tool for summarization. pages 41–
49. Association for Computational Linguistics.
Hongyan Jing and Kathleen McKeown. 1999. The de-
composition of human-written summary sentences. In
Research and Development in Information Retrieval,
pages 129–136.
Kevin Knight and Daniel Marcu. 2002. Summariza-
tion beyond sentence extraction: a probabilistic ap-
proach to sentence compression. Artificial Intelli-
gence, 139(1):91–107.
Mirella Lapata. 2003. Probabilistic text structuring: Ex-
periments with sentence ordering. In Proceedings of
the 41st Annual Meeting of the Associationfor Compu-
tational Linguistics, pages 545–552, Sapporo, Japan.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic evalu-
ation of summaries using n-gram co-occurrence statis-
tics. In NAACL ’03: Proceedings of the 2003 Confer-
ence of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology, pages 71–78, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
W. C. Mann and S. A. Thompson. 1988. Rhetorical
structure theory: Toward a functional theory of text
organization. Text, 8(3):243–281.
Kathleen R McKeown. 1985. Text Generation: Using
Discourse Strategies and Focus Constraints to Gen-
erate Natural Language Text. Cambridge University
Press.
Jane Morris and Graeme Hirst. 1991. Lexical cohe-
sion computed by thesaural relations as an indicator
of the structure of text. Computational Linguistics,
17(1):21–48.
G. Salton and M. J. McGill. 1983. Introduction to mod-
ern information retrieval. McGraw-Hill, New York.
Radu Soricut and Daniel Marcu. 2005. Towards de-
veloping generation algorithms for text-to-text appli-
cations. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL’05), pages 66–74, Ann Arbor, Michigan, June.
Association for Computational Linguistics.
Stephen Wan and C´ecile Paris. 2008. In-browser sum-
marisation: Generating elaborative summaries biased
towards the reading context. In Proceedings of ACL-
08: HLT, Short Papers, pages 129–132, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
Stephen Wan, Robert Dale Mark Dras, and C´ecile Paris.
2005. Towards statistical paraphrase generation: pre-
liminary evaluations of grammaticality. In Proceed-
ings of The 3rd International Workshop on Paraphras-
ing (IWP2005), pages 88–95, Jeju Island, South Korea.
Michael J. Witbrock and Vibhu O. Mittal. 1999. Ultra-
summarization (poster abstract): a statistical approach
to generating highly condensed non-extractive sum-
maries. In SIGIR ’99: Proceedings of the 22nd annual
international ACM SIGIR conference on Research and
development in information retrieval, pages 315–316,
New York, NY, USA. ACM Press.
</reference>
<page confidence="0.997639">
552
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.097354">
<title confidence="0.975652">Seed and Grow: Augmenting Statistically Generated Summary using Schematic Word Patterns</title>
<address confidence="0.629406">Sydney, Australia</address>
<email confidence="0.437104">Cecile.Paris@csiro.au</email>
<title confidence="0.545791333333333">for Language Department of Macquarie</title>
<address confidence="0.922018">Sydney, NSW 2113</address>
<email confidence="0.996682">swan,madras,rdale@ics.mq.edu.au</email>
<abstract confidence="0.997593">We examine the problem of content selection in statistical novel sentence generation. Our approach models the processes performed by professional editors when incorporating material from additional sentences to support some initially chosen key summary sentence, process we refer to as Augmen- We propose and evaluate a method called “Seed and Grow” for selecting such auxiliary information. Additionally, we argue that this can be performed using schemata, as represented by word-pair co-occurrences, and demonstrate its use in statistical summary sentence generation. Evaluation results are supportive, indicating that a schemata model significantly improves over the baseline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Catching the drift: Probabilistic content models, with applications to generation and summarization.</title>
<date>2004</date>
<booktitle>In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLT-NAACL 2004: Main Proceedings,</booktitle>
<volume>2</volume>
<pages>113--120</pages>
<location>Boston, Massachusetts, USA,</location>
<contexts>
<context position="8973" citStr="Barzilay and Lee (2004)" startWordPosition="1402" endWordPosition="1405">ith [host families]2 in the districts; A total or partial destruction of over 3,000 homes in Dili affecting at least 14,000 IDPs Figure 2: Examples of the pattern (DisplacedPersons[1], HostingCommunities[2]). ure 2 showed that mentions of the plight of internationally displaced persons are often followed by descriptions of the impact on the host communities that look after them. In this particular example, this is realised lexically in the co-occurrences of the words displaced and host. Corpus-based methods inspired by the notion of schemata have been explored in the past by Lapata (2003) and Barzilay and Lee (2004) for ordering sentences extracted in a multi-document summarisation application. However, to our knowledge, using word co-occurrences in this manner to represent schematic knowledge for the purposes of selecting content in a statistically-generated summary sentence has not previously been explored. This paper seeks to determine whether or not such patterns exist in homogeneous data; and furthermore, whether such patterns can be used to better select words from auxiliary sentences. In particular, we propose the “Seed and Grow” approach for this task. The results show that even simple modelling </context>
<context position="13557" citStr="Barzilay and Lee (2004)" startWordPosition="2122" endWordPosition="2125">) propose a model that encodes how likely it is that different sized spans of text are skipped to reach words and phrases to recycle. While similar in task, our models differ substantially in the nature of the phenomenon modelled. In this work, we focus on content-based considerations that model which words can be combined to build up a new sentence. 2.2 Schemata and Text Generation There exists related work from Natural Language Generation (NLG) in finding material to build up sentences. As mentioned above, our content selection model is inspired by work on schemata from NLG (McKeown, 1985). Barzilay and Lee (2004) showed that it is possible to obtain schema-like knowledge automatically from a corpus for the purposes of extracting sentences and ordering them. However, their work represents patterns at the sentence level, and is thus not directly comparable to our work, given our focus on sentence generation. In our system, what is required is a means to rank words for use in generation. Thus, we focus on commonly occurring word co-occurrences, with the aim of encoding conventions in the texts we are trying to generate. In this respect, this is similar to work by Lapata (2003), who builds a conditional m</context>
</contexts>
<marker>Barzilay, Lee, 2004</marker>
<rawString>Regina Barzilay and Lillian Lee. 2004. Catching the drift: Probabilistic content models, with applications to generation and summarization. In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLT-NAACL 2004: Main Proceedings, pages 113–120, Boston, Massachusetts, USA, May 2 - May 7. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Sentence fusion for multidocument news summarization.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>3</issue>
<contexts>
<context position="11842" citStr="Barzilay and McKeown (2005)" startWordPosition="1838" endWordPosition="1841">mations (Knight and Marcu, 2002). For content selection, discourse-level considerations were proposed by Daum´e III and Marcu (2002), who explored the use of Rhetorical Structure Theory (Mann and Thompson, 1988). More recently, Clarke and Lapata (2007) use Centering Theory (Grosz et al., 1995) and Lexical Chains (Morris and Hirst, 1991) to identify which information to prune. Our work is similar in incorporating discourse-level phenomena for content selection. However, we look at schemalike information as opposed to chains of references and focus on the sentence augmentation task. The work of Barzilay and McKeown (2005) on Sentence Fusion introduced the problem of converting multiple sentences into a single summary sen545 tence. Each sentence set ideally tightly clusters around a single news event. Thus, there is one general proposition to be realised in the summary sentence, identified by finding the common elements in the input sentences. We see this as an example of conservation. In our work, this general proposition is equivalent to the core information for the summary sentence before the incorporation of supplementary material. In contrast to both compression and conservation work, we focus on augmentin</context>
</contexts>
<marker>Barzilay, McKeown, 2005</marker>
<rawString>Regina Barzilay and Kathleen R. McKeown. 2005. Sentence fusion for multidocument news summarization. Computational Linguistics, 31(3):297–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Stephen Della Pietra</author>
<author>Vincent J Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="22263" citStr="Berger et al. (1996)" startWordPosition="3552" endWordPosition="3555">y|source). Standardly reformulating this probability using Bayes’ rule results in the following: probcm(source|summary) × problm(summary) (5) In this paper, we are concerned with the first factor, probcm(source|summary), referred to as the channel model (CM), which combines both the buzzword (BWM) and word-pair co-occurrence (WCM) models. An examination of differences between the two approaches revealed only a 20% word overlap on the Jaccard metric. In order to combine multiple models, we intend to use machine learning approaches to combine the information in each model in a similar manner to Berger et al. (1996). We are currently exploring the use of logistic regression methods to learn a function that would treat, as features, the probabilities defined by the salience and schematic content selection models. Although generation is possible using each content selection model in isolation, evaluations of the combined model are on-going and are not presented in this paper. 5 Evaluation In this evaluation, the task is to select n words from the aligned source sentences for inclusion in a summary. As a gold-standard for comparison, we simply examine what words were actually chosen in the summary sentence </context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam L. Berger, Stephen Della Pietra, and Vincent J. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Clarke</author>
<author>Mirella Lapata</author>
</authors>
<title>Modelling compression with discourse constraints.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>1--11</pages>
<contexts>
<context position="11467" citStr="Clarke and Lapata (2007)" startWordPosition="1780" endWordPosition="1783">nformation. Roughly, text-to-text transformations fall into three categories: those in which information is compressed, conserved, and augmented. We use these distinctions to organise this overview of the literature. In Sentence Compression work, a single sentence undergoes pruning to shorten its length. Previous approaches have focused on statistical syntactic transformations (Knight and Marcu, 2002). For content selection, discourse-level considerations were proposed by Daum´e III and Marcu (2002), who explored the use of Rhetorical Structure Theory (Mann and Thompson, 1988). More recently, Clarke and Lapata (2007) use Centering Theory (Grosz et al., 1995) and Lexical Chains (Morris and Hirst, 1991) to identify which information to prune. Our work is similar in incorporating discourse-level phenomena for content selection. However, we look at schemalike information as opposed to chains of references and focus on the sentence augmentation task. The work of Barzilay and McKeown (2005) on Sentence Fusion introduced the problem of converting multiple sentences into a single summary sen545 tence. Each sentence set ideally tightly clusters around a single news event. Thus, there is one general proposition to </context>
</contexts>
<marker>Clarke, Lapata, 2007</marker>
<rawString>James Clarke and Mirella Lapata. 2007. Modelling compression with discourse constraints. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 1–11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>A noisy-channel model for document compression.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL –</booktitle>
<volume>6</volume>
<pages>449--456</pages>
<location>Philadelphia, PA,</location>
<marker>Daum´e, Marcu, 2002</marker>
<rawString>Hal Daum´e III and Daniel Marcu. 2002. A noisy-channel model for document compression. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL – 2002), pages 449 – 456, Philadelphia, PA, July 6 – 12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>Induction of word and phrase alignments for automatic document summarization.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>4</issue>
<marker>Daum´e, Marcu, 2005</marker>
<rawString>Hal Daum´e III and Daniel Marcu. 2005. Induction of word and phrase alignments for automatic document summarization. Computational Linguistics, 31(4):505–530, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Aravind K Joshi</author>
<author>Scott Weinstein</author>
</authors>
<title>Centering: A framework for modeling the local coherence of discourse.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="11509" citStr="Grosz et al., 1995" startWordPosition="1787" endWordPosition="1790">ns fall into three categories: those in which information is compressed, conserved, and augmented. We use these distinctions to organise this overview of the literature. In Sentence Compression work, a single sentence undergoes pruning to shorten its length. Previous approaches have focused on statistical syntactic transformations (Knight and Marcu, 2002). For content selection, discourse-level considerations were proposed by Daum´e III and Marcu (2002), who explored the use of Rhetorical Structure Theory (Mann and Thompson, 1988). More recently, Clarke and Lapata (2007) use Centering Theory (Grosz et al., 1995) and Lexical Chains (Morris and Hirst, 1991) to identify which information to prune. Our work is similar in incorporating discourse-level phenomena for content selection. However, we look at schemalike information as opposed to chains of references and focus on the sentence augmentation task. The work of Barzilay and McKeown (2005) on Sentence Fusion introduced the problem of converting multiple sentences into a single summary sen545 tence. Each sentence set ideally tightly clusters around a single news event. Thus, there is one general proposition to be realised in the summary sentence, ident</context>
</contexts>
<marker>Grosz, Joshi, Weinstein, 1995</marker>
<rawString>Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein. 1995. Centering: A framework for modeling the local coherence of discourse. Computational Linguistics, 21(2):203–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Hatzivassiloglou</author>
<author>J Klavans</author>
<author>M Holcombe</author>
<author>R Barzilay</author>
<author>M Kan</author>
<author>K McKeown</author>
</authors>
<title>Simfinder: A flexible clustering tool for summarization. pages 41– 49. Association for Computational Linguistics.</title>
<date>2001</date>
<contexts>
<context position="25858" citStr="Hatzivassiloglou et al. (2001)" startWordPosition="4136" endWordPosition="4139">ed auxiliary sentences). The remaining 550 cases were instances of sentence augmentation (with at least one auxiliary sentence). Of the 580 cases, 50 cases were set aside for testing. The remaining 530 cases were used for training. Statistics for the training portion of the sentence augmentation set are provided in Table 1. In this paper, aligned sentence tuples are obtained via manual annotation. Automatic construction of these sentence-level alignments is possible and has been explored by Jing and McKeown (1999). We also envisage using tools for scoring sentence similarity (for example, see Hatzivassiloglou et al. (2001)) for automatically constructing them; this is the focus of work by Wan and Paris (2008). 3http://ochaonline3.un.org/humanitarianappeal/index.htm 5.2 The Baselines Three baselines were used in this work: the random, tf-idf and position baselines. A random word selector shows what performance might be achieved in the absence of any linguistic knowledge. We also sorted all words in the aligned source sentences by their weighted tf-idf scores. This baseline selects words in order until the desired word limit is reached. This baseline is referred to as the tf-idf baseline. Finally, we selected wor</context>
</contexts>
<marker>Hatzivassiloglou, Klavans, Holcombe, Barzilay, Kan, McKeown, 2001</marker>
<rawString>V. Hatzivassiloglou, J. Klavans, M. Holcombe, R. Barzilay, M. Kan, and K. McKeown. 2001. Simfinder: A flexible clustering tool for summarization. pages 41– 49. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyan Jing</author>
<author>Kathleen McKeown</author>
</authors>
<title>The decomposition of human-written summary sentences.</title>
<date>1999</date>
<booktitle>In Research and Development in Information Retrieval,</booktitle>
<pages>129--136</pages>
<contexts>
<context position="12530" citStr="Jing and McKeown (1999)" startWordPosition="1952" endWordPosition="1955"> sentences into a single summary sen545 tence. Each sentence set ideally tightly clusters around a single news event. Thus, there is one general proposition to be realised in the summary sentence, identified by finding the common elements in the input sentences. We see this as an example of conservation. In our work, this general proposition is equivalent to the core information for the summary sentence before the incorporation of supplementary material. In contrast to both compression and conservation work, we focus on augmenting the information in a key sentence. The closest work is that of Jing and McKeown (1999) and Daum´e III and Marcu (2005), in which multiple sentences are processed, with fragments within them being recycled to generate the novel generated text. In both works, recyclable fragments are identified by automatic means. Jing and McKeown (1999) use models that are based on “copy-and-paste” operations learnt from the behaviour of human abstractors as found in a corpus. Daum´e III and Marcu (2005) propose a model that encodes how likely it is that different sized spans of text are skipped to reach words and phrases to recycle. While similar in task, our models differ substantially in the </context>
<context position="25747" citStr="Jing and McKeown (1999)" startWordPosition="4120" endWordPosition="4123">es referred to here as the UN CAP corpus. Of these, 230 tuples were paraphrase cases (i.e. without aligned auxiliary sentences). The remaining 550 cases were instances of sentence augmentation (with at least one auxiliary sentence). Of the 580 cases, 50 cases were set aside for testing. The remaining 530 cases were used for training. Statistics for the training portion of the sentence augmentation set are provided in Table 1. In this paper, aligned sentence tuples are obtained via manual annotation. Automatic construction of these sentence-level alignments is possible and has been explored by Jing and McKeown (1999). We also envisage using tools for scoring sentence similarity (for example, see Hatzivassiloglou et al. (2001)) for automatically constructing them; this is the focus of work by Wan and Paris (2008). 3http://ochaonline3.un.org/humanitarianappeal/index.htm 5.2 The Baselines Three baselines were used in this work: the random, tf-idf and position baselines. A random word selector shows what performance might be achieved in the absence of any linguistic knowledge. We also sorted all words in the aligned source sentences by their weighted tf-idf scores. This baseline selects words in order until t</context>
</contexts>
<marker>Jing, McKeown, 1999</marker>
<rawString>Hongyan Jing and Kathleen McKeown. 1999. The decomposition of human-written summary sentences. In Research and Development in Information Retrieval, pages 129–136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Summarization beyond sentence extraction: a probabilistic approach to sentence compression.</title>
<date>2002</date>
<journal>Artificial Intelligence,</journal>
<volume>139</volume>
<issue>1</issue>
<contexts>
<context position="11247" citStr="Knight and Marcu, 2002" startWordPosition="1747" endWordPosition="1750">robust statistical generation system, our work is also based on the noisy channel model. Into this, we incorporate our content selection model, which includes Witbrock and Mittal’s model supplemented with schema-based information. Roughly, text-to-text transformations fall into three categories: those in which information is compressed, conserved, and augmented. We use these distinctions to organise this overview of the literature. In Sentence Compression work, a single sentence undergoes pruning to shorten its length. Previous approaches have focused on statistical syntactic transformations (Knight and Marcu, 2002). For content selection, discourse-level considerations were proposed by Daum´e III and Marcu (2002), who explored the use of Rhetorical Structure Theory (Mann and Thompson, 1988). More recently, Clarke and Lapata (2007) use Centering Theory (Grosz et al., 1995) and Lexical Chains (Morris and Hirst, 1991) to identify which information to prune. Our work is similar in incorporating discourse-level phenomena for content selection. However, we look at schemalike information as opposed to chains of references and focus on the sentence augmentation task. The work of Barzilay and McKeown (2005) on S</context>
</contexts>
<marker>Knight, Marcu, 2002</marker>
<rawString>Kevin Knight and Daniel Marcu. 2002. Summarization beyond sentence extraction: a probabilistic approach to sentence compression. Artificial Intelligence, 139(1):91–107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
</authors>
<title>Probabilistic text structuring: Experiments with sentence ordering.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Associationfor Computational Linguistics,</booktitle>
<pages>545--552</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="8945" citStr="Lapata (2003)" startWordPosition="1398" endWordPosition="1400">ing in camps and with [host families]2 in the districts; A total or partial destruction of over 3,000 homes in Dili affecting at least 14,000 IDPs Figure 2: Examples of the pattern (DisplacedPersons[1], HostingCommunities[2]). ure 2 showed that mentions of the plight of internationally displaced persons are often followed by descriptions of the impact on the host communities that look after them. In this particular example, this is realised lexically in the co-occurrences of the words displaced and host. Corpus-based methods inspired by the notion of schemata have been explored in the past by Lapata (2003) and Barzilay and Lee (2004) for ordering sentences extracted in a multi-document summarisation application. However, to our knowledge, using word co-occurrences in this manner to represent schematic knowledge for the purposes of selecting content in a statistically-generated summary sentence has not previously been explored. This paper seeks to determine whether or not such patterns exist in homogeneous data; and furthermore, whether such patterns can be used to better select words from auxiliary sentences. In particular, we propose the “Seed and Grow” approach for this task. The results show</context>
<context position="14129" citStr="Lapata (2003)" startWordPosition="2222" endWordPosition="2223"> (McKeown, 1985). Barzilay and Lee (2004) showed that it is possible to obtain schema-like knowledge automatically from a corpus for the purposes of extracting sentences and ordering them. However, their work represents patterns at the sentence level, and is thus not directly comparable to our work, given our focus on sentence generation. In our system, what is required is a means to rank words for use in generation. Thus, we focus on commonly occurring word co-occurrences, with the aim of encoding conventions in the texts we are trying to generate. In this respect, this is similar to work by Lapata (2003), who builds a conditional model of words across adjacent sentences, focusing on words in particular semantic roles. Like Barzilay and Lee (2004), this model was used to order extracted sentences in summaries. In contrast, our work focuses on word patterns found within a summary sentence, not between sentences. Additionally, our tasks differ as we examine the statistical sentence generation instead of sentence ordering. 3 Linguistic Intuitions behind Word Selection The “Seed and Grow” approach proposed in this paper divides the word-level content selection problem into two underlying subproble</context>
</contexts>
<marker>Lapata, 2003</marker>
<rawString>Mirella Lapata. 2003. Probabilistic text structuring: Experiments with sentence ordering. In Proceedings of the 41st Annual Meeting of the Associationfor Computational Linguistics, pages 545–552, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>Automatic evaluation of summaries using n-gram co-occurrence statistics.</title>
<date>2003</date>
<booktitle>In NAACL ’03: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology,</booktitle>
<pages>71--78</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="23424" citStr="Lin and Hovy, 2003" startWordPosition="3743" endWordPosition="3746">e what words were actually chosen in the summary sentence of the aligned sentence tuple. We are specifically interested in open-class words, and so a stopword list of closed-class words is used to filter the sentences in each test case. We evaluate against the set of open-class words in the human-authored summary sentence using recall and precision metrics. Recall is the size of the intersection of the selected and gold-standard sets, normalised by the length of the gold-standard sentence (in words). This recall metric is similar to the ROUGE-1 metric, the unigram version of the ROUGE metric (Lin and Hovy, 2003) used in the Document Understanding Conferences2 (DUC). Precision is the size of the intersection normalised by the number of words selected. We also report the F-measure, which is the harmonic mean of the recall and precision scores. Recall, precision and F-measure are measured at various values of n ranging from 1 to the number of open-class words in the gold-standard summary sentence for a particular test case. For the purposes of evaluation, differences in tokens due to morphology were explored crudely via the use of Porter’s stemming algorithm. However, the results from stemming are not t</context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>Chin-Yew Lin and Eduard Hovy. 2003. Automatic evaluation of summaries using n-gram co-occurrence statistics. In NAACL ’03: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, pages 71–78, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W C Mann</author>
<author>S A Thompson</author>
</authors>
<title>Rhetorical structure theory: Toward a functional theory of text organization.</title>
<date>1988</date>
<tech>Text, 8(3):243–281.</tech>
<contexts>
<context position="11426" citStr="Mann and Thompson, 1988" startWordPosition="1774" endWordPosition="1777">’s model supplemented with schema-based information. Roughly, text-to-text transformations fall into three categories: those in which information is compressed, conserved, and augmented. We use these distinctions to organise this overview of the literature. In Sentence Compression work, a single sentence undergoes pruning to shorten its length. Previous approaches have focused on statistical syntactic transformations (Knight and Marcu, 2002). For content selection, discourse-level considerations were proposed by Daum´e III and Marcu (2002), who explored the use of Rhetorical Structure Theory (Mann and Thompson, 1988). More recently, Clarke and Lapata (2007) use Centering Theory (Grosz et al., 1995) and Lexical Chains (Morris and Hirst, 1991) to identify which information to prune. Our work is similar in incorporating discourse-level phenomena for content selection. However, we look at schemalike information as opposed to chains of references and focus on the sentence augmentation task. The work of Barzilay and McKeown (2005) on Sentence Fusion introduced the problem of converting multiple sentences into a single summary sen545 tence. Each sentence set ideally tightly clusters around a single news event. T</context>
</contexts>
<marker>Mann, Thompson, 1988</marker>
<rawString>W. C. Mann and S. A. Thompson. 1988. Rhetorical structure theory: Toward a functional theory of text organization. Text, 8(3):243–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen R McKeown</author>
</authors>
<title>Text Generation: Using Discourse Strategies and Focus Constraints to Generate Natural Language Text.</title>
<date>1985</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="7429" citStr="McKeown (1985)" startWordPosition="1169" endWordPosition="1170">s to give financial support. In the above example, the juxtaposition highlights the fact that those in need are not just those people from the ‘epicenter’ of the crisis but also those that look after them. We propose and evaluate a method called “Seed and Grow” for selecting content from auxiliary sentences. That is, we first select the core meaning of the summary, given here by the key sentence, and then we find those pieces of additional information that are conventionally juxtaposed with it. Such patterns are reminiscent of Schemata, the organisations of propositional content introduced by McKeown (1985). Schemata typically involve a symbolic representation of each proposition’s semantics. However, in our case, a text-to-text generation scenario, we are without such representations and so must find other means to encode these patterns. To alleviate the situation, we turn to word-pair cooccurrences to approximate schematic patterns. Fig544 Sentence 1: The increased number of [internally displaced persons]1 and the continued presence of refugees have further strained the scarce natural resources of [host communities]2, stretching their capacity to the limit. Sentence 2: 100,000 people, a signif</context>
<context position="13532" citStr="McKeown, 1985" startWordPosition="2120" endWordPosition="2121"> and Marcu (2005) propose a model that encodes how likely it is that different sized spans of text are skipped to reach words and phrases to recycle. While similar in task, our models differ substantially in the nature of the phenomenon modelled. In this work, we focus on content-based considerations that model which words can be combined to build up a new sentence. 2.2 Schemata and Text Generation There exists related work from Natural Language Generation (NLG) in finding material to build up sentences. As mentioned above, our content selection model is inspired by work on schemata from NLG (McKeown, 1985). Barzilay and Lee (2004) showed that it is possible to obtain schema-like knowledge automatically from a corpus for the purposes of extracting sentences and ordering them. However, their work represents patterns at the sentence level, and is thus not directly comparable to our work, given our focus on sentence generation. In our system, what is required is a means to rank words for use in generation. Thus, we focus on commonly occurring word co-occurrences, with the aim of encoding conventions in the texts we are trying to generate. In this respect, this is similar to work by Lapata (2003), w</context>
</contexts>
<marker>McKeown, 1985</marker>
<rawString>Kathleen R McKeown. 1985. Text Generation: Using Discourse Strategies and Focus Constraints to Generate Natural Language Text. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane Morris</author>
<author>Graeme Hirst</author>
</authors>
<title>Lexical cohesion computed by thesaural relations as an indicator of the structure of text.</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<volume>17</volume>
<issue>1</issue>
<contexts>
<context position="11553" citStr="Morris and Hirst, 1991" startWordPosition="1794" endWordPosition="1797">which information is compressed, conserved, and augmented. We use these distinctions to organise this overview of the literature. In Sentence Compression work, a single sentence undergoes pruning to shorten its length. Previous approaches have focused on statistical syntactic transformations (Knight and Marcu, 2002). For content selection, discourse-level considerations were proposed by Daum´e III and Marcu (2002), who explored the use of Rhetorical Structure Theory (Mann and Thompson, 1988). More recently, Clarke and Lapata (2007) use Centering Theory (Grosz et al., 1995) and Lexical Chains (Morris and Hirst, 1991) to identify which information to prune. Our work is similar in incorporating discourse-level phenomena for content selection. However, we look at schemalike information as opposed to chains of references and focus on the sentence augmentation task. The work of Barzilay and McKeown (2005) on Sentence Fusion introduced the problem of converting multiple sentences into a single summary sen545 tence. Each sentence set ideally tightly clusters around a single news event. Thus, there is one general proposition to be realised in the summary sentence, identified by finding the common elements in the </context>
</contexts>
<marker>Morris, Hirst, 1991</marker>
<rawString>Jane Morris and Graeme Hirst. 1991. Lexical cohesion computed by thesaural relations as an indicator of the structure of text. Computational Linguistics, 17(1):21–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>M J McGill</author>
</authors>
<title>Introduction to modern information retrieval.</title>
<date>1983</date>
<publisher>McGraw-Hill,</publisher>
<location>New York.</location>
<contexts>
<context position="19829" citStr="Salton and McGill, 1983" startWordPosition="3157" endWordPosition="3160">ution to the core meaning of the summary sentence. For example, words that are part of an adjunct phrase in the key sentence might not be good candidates to elaborate upon. Thus, one might want to weight these seed words lower, to reduce their influence on triggering schematically associated words. To allow for this, a seed weight vector is maintained, storing a weight per seed word. Different weighting schemes are possible. For example, a scheme might indicate the salience of a word. In addition to the buzzword model (BWM) described earlier, one might employ a standard vector space approach (Salton and McGill, 1983) from Information Retrieval, which uses term frequency scores weighted with an inverse document frequency factor, or tf-idf. We also implement the case in which all seed words are treated equally using binary weights, where 1 indicates the presence of a seed word, and 0 indicates its absence. In the evaluations described in Section 5, we refer to these three seed weighting schemes as bwm and tf-idf, and binary respectively. 547 To find the probability of selecting an auxiliary word using the schematic word-pair co-occurrence model (WCM), an averaged probability is found by normalising the sum </context>
</contexts>
<marker>Salton, McGill, 1983</marker>
<rawString>G. Salton and M. J. McGill. 1983. Introduction to modern information retrieval. McGraw-Hill, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Soricut</author>
<author>Daniel Marcu</author>
</authors>
<title>Towards developing generation algorithms for text-to-text applications.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>66--74</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="5972" citStr="Soricut and Marcu (2005)" startWordPosition="928" endWordPosition="931">thout introducing unimportant content. We refer to this problem of incorporating material from auxiliary sentences to supplement a key sentence as Sentence Augmentation. In this paper, sentence augmentation is modelled as a noisy channel process and has two facets: content selection and language modelling. This paper focuses on the former, in which the system must rank text segments—in this case, words—for inclusion in the generated sentence. Given a ranked selection of words, a language model would then order them appropriately, as described in work on sentence regeneration (for example, see Soricut and Marcu (2005); Wan et al. (2005)). Provided with an aligned sentence tuple, the problem lies in effectively selecting words from the auxiliary sentences to bolster those taken from the key sentence. Given that there are on average 2.7 auxiliary sentences per aligned sentence tuple, this additional influx of words poses a considerable challenge. We begin with the premise that, for documents of a homogeneous type (in this case, the genre is a funding proposal, and the domain is humanitarian aid), it may be possible to identify patterns in the organisation of information in summaries. For example, Figure 2 pr</context>
</contexts>
<marker>Soricut, Marcu, 2005</marker>
<rawString>Radu Soricut and Daniel Marcu. 2005. Towards developing generation algorithms for text-to-text applications. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 66–74, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Wan</author>
<author>C´ecile Paris</author>
</authors>
<title>In-browser summarisation: Generating elaborative summaries biased towards the reading context.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL08: HLT, Short Papers,</booktitle>
<pages>129--132</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="25946" citStr="Wan and Paris (2008)" startWordPosition="4151" endWordPosition="4154">least one auxiliary sentence). Of the 580 cases, 50 cases were set aside for testing. The remaining 530 cases were used for training. Statistics for the training portion of the sentence augmentation set are provided in Table 1. In this paper, aligned sentence tuples are obtained via manual annotation. Automatic construction of these sentence-level alignments is possible and has been explored by Jing and McKeown (1999). We also envisage using tools for scoring sentence similarity (for example, see Hatzivassiloglou et al. (2001)) for automatically constructing them; this is the focus of work by Wan and Paris (2008). 3http://ochaonline3.un.org/humanitarianappeal/index.htm 5.2 The Baselines Three baselines were used in this work: the random, tf-idf and position baselines. A random word selector shows what performance might be achieved in the absence of any linguistic knowledge. We also sorted all words in the aligned source sentences by their weighted tf-idf scores. This baseline selects words in order until the desired word limit is reached. This baseline is referred to as the tf-idf baseline. Finally, we selected words based on their sentence order, choosing first those words from the key sentence. When</context>
</contexts>
<marker>Wan, Paris, 2008</marker>
<rawString>Stephen Wan and C´ecile Paris. 2008. In-browser summarisation: Generating elaborative summaries biased towards the reading context. In Proceedings of ACL08: HLT, Short Papers, pages 129–132, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Wan</author>
<author>Robert Dale Mark Dras</author>
<author>C´ecile Paris</author>
</authors>
<title>Towards statistical paraphrase generation: preliminary evaluations of grammaticality.</title>
<date>2005</date>
<booktitle>In Proceedings of The 3rd International Workshop on Paraphrasing (IWP2005),</booktitle>
<pages>88--95</pages>
<location>Jeju Island, South</location>
<contexts>
<context position="5991" citStr="Wan et al. (2005)" startWordPosition="932" endWordPosition="935">ant content. We refer to this problem of incorporating material from auxiliary sentences to supplement a key sentence as Sentence Augmentation. In this paper, sentence augmentation is modelled as a noisy channel process and has two facets: content selection and language modelling. This paper focuses on the former, in which the system must rank text segments—in this case, words—for inclusion in the generated sentence. Given a ranked selection of words, a language model would then order them appropriately, as described in work on sentence regeneration (for example, see Soricut and Marcu (2005); Wan et al. (2005)). Provided with an aligned sentence tuple, the problem lies in effectively selecting words from the auxiliary sentences to bolster those taken from the key sentence. Given that there are on average 2.7 auxiliary sentences per aligned sentence tuple, this additional influx of words poses a considerable challenge. We begin with the premise that, for documents of a homogeneous type (in this case, the genre is a funding proposal, and the domain is humanitarian aid), it may be possible to identify patterns in the organisation of information in summaries. For example, Figure 2 presents three summar</context>
</contexts>
<marker>Wan, Dras, Paris, 2005</marker>
<rawString>Stephen Wan, Robert Dale Mark Dras, and C´ecile Paris. 2005. Towards statistical paraphrase generation: preliminary evaluations of grammaticality. In Proceedings of The 3rd International Workshop on Paraphrasing (IWP2005), pages 88–95, Jeju Island, South Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael J Witbrock</author>
<author>Vibhu O Mittal</author>
</authors>
<title>Ultrasummarization (poster abstract): a statistical approach to generating highly condensed non-extractive summaries.</title>
<date>1999</date>
<booktitle>In SIGIR ’99: Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>315--316</pages>
<publisher>ACM Press.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="10421" citStr="Witbrock and Mittal (1999)" startWordPosition="1625" endWordPosition="1628">n 4 describes how a binary classification model is used in a statistical text generation system. Section 5 describes our evaluation of the model for a summary generation task. We conclude, in Section 6, that domain-specific schematic patterns can be acquired and applied to content selection for statistical sentence generation. 2 Related Work 2.1 Content Selection in Text-to-Text Systems Statistical text-to-text summarisation applications have borrowed much from the related field of statistical machine translation. In one of the first works to present summarisation as a noisy channel approach, Witbrock and Mittal (1999) presented a conditional model for learning the suitability of words from a news article for inclusion in headlines, or ‘ultrasummaries’. Inspired by this approach, and with the intention of designing a robust statistical generation system, our work is also based on the noisy channel model. Into this, we incorporate our content selection model, which includes Witbrock and Mittal’s model supplemented with schema-based information. Roughly, text-to-text transformations fall into three categories: those in which information is compressed, conserved, and augmented. We use these distinctions to org</context>
<context position="15280" citStr="Witbrock and Mittal (1999)" startWordPosition="2399" endWordPosition="2402">ides the word-level content selection problem into two underlying subproblems. We address these with two separate models, called the salience and schematic models. The salience model chooses the key content for the summary sentence while the schematic model attempts to identify what else is typically mentioned given those salient pieces of information. 3.1 A Salience Model: Learning “Buzzwords” There are a variety of methods for determining the salient information in a text, and these underpin most work in automatic text summarisation. As an example of a salience model trained on corpus data, Witbrock and Mittal (1999) introduced a method for scoring summary words for inclusion within news headlines. In their model, headlines were treated as ‘ultra-summaries’. Their model learns which words are typically used in headlines and encodes, at least to some degree, which words are attention grabbing. In the domain of funding proposals, key words that grab attention may amount to domain-specific buzzwords. Intuitively, a reader, perhaps someone in charge of allocating donations, tends to look for certain types of key information matching donation criteria, and so human abstract authors will target their summaries </context>
<context position="21553" citStr="Witbrock and Mittal (1999)" startWordPosition="3443" endWordPosition="3446">e words, the scoring is as follows: � 1 � 1 probwcm(w) = |seed |+ probwcm(w) (4) Z where Z is a normalisation across the set of seed words. 4 Combining Buzzwords and Word-Pair Co-Occurrence Models for Generation As mentioned above, the noisy channel approach is used for producing the augmented sentence. Although the focus of this paper is on Content Selection, an overview of the end-to-end generation process is presented for completeness. Sentence augmentation is essentially a text-to-text process: A key sentence and auxiliary material are transformed into a single summary sentence. Following Witbrock and Mittal (1999), the task is to search for the string of words that maximises the probability prob(summary|source). Standardly reformulating this probability using Bayes’ rule results in the following: probcm(source|summary) × problm(summary) (5) In this paper, we are concerned with the first factor, probcm(source|summary), referred to as the channel model (CM), which combines both the buzzword (BWM) and word-pair co-occurrence (WCM) models. An examination of differences between the two approaches revealed only a 20% word overlap on the Jaccard metric. In order to combine multiple models, we intend to use ma</context>
</contexts>
<marker>Witbrock, Mittal, 1999</marker>
<rawString>Michael J. Witbrock and Vibhu O. Mittal. 1999. Ultrasummarization (poster abstract): a statistical approach to generating highly condensed non-extractive summaries. In SIGIR ’99: Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, pages 315–316, New York, NY, USA. ACM Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>