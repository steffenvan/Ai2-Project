<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.321216">
<title confidence="0.998281">
Why Not Grab a Free Lunch? Mining Large Corpora for
Parallel Sentences to Improve Translation Modeling
</title>
<author confidence="0.995257">
Ferhan Ture Jimmy Lin
</author>
<affiliation confidence="0.9997205">
Dept. of Computer Science, The iSchool
University of Maryland University of Maryland
</affiliation>
<email confidence="0.998968">
fture@cs.umd.edu jimmylin@umd.edu
</email>
<sectionHeader confidence="0.995619" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999311166666667">
It is well known that the output quality of
statistical machine translation (SMT) systems
increases with more training data. To ob-
tain more parallel text for translation mod-
eling, researchers have turned to the web to
mine parallel sentences, but most previous ap-
proaches have avoided the difficult problem
of pairwise similarity on cross-lingual docu-
ments and instead rely on heuristics. In con-
trast, we confront this challenge head on us-
ing the MapReduce framework. On a mod-
est cluster, our scalable end-to-end processing
pipeline was able to automatically gather 5.8m
parallel sentence pairs from English and Ger-
man Wikipedia. Augmenting existing bitext
with these data yielded significant improve-
ments over a state-of-the-art baseline (2.39
BLEU points in the best case).
</bodyText>
<sectionHeader confidence="0.998989" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999983552631579">
It has been repeatedly shown that “throwing more
data at the problem” is effective in increasing SMT
output quality, both for translation modeling (Dyer
et al., 2008) and for language modeling (Brants et
al., 2007). In this paper, we bring together two re-
lated research threads to gather parallel sentences for
improved translation modeling: cross-lingual pair-
wise similarity to mine comparable documents and
classification to identify sentence pairs that are mu-
tual translations.
Unlike most previous work, which sidesteps the
computationally-intensive task of pairwise compar-
isons to mine comparable documents and instead re-
lies on heuristics, we tackle the challenge head on.
This paper describes a fully open-source, scalable
MapReduce-based processing pipeline that is able to
automatically extract large quantities of parallel sen-
tences. Experiments examine the impact data size
has on a state-of-the-art SMT system.
We acknowledge that different components of this
work are not novel and the general principles behind
“big data” MT are well known. However, when con-
sidered together with our previous work (Ture et al.,
2011), to our knowledge this is the first exposition
in which all the pieces have been “put together” in
an end-to-end pipeline that is accessible to academic
research groups. The framework described in this
paper is entirely open source, and the computational
resources necessary to replicate our results are rela-
tively modest.
Starting from nothing more than two corpora in
different languages (in German and English, in our
case), we are able to extract bitext and improve
translation quality by a significant margin (2.39
BLEU points), essentially “for free”. By varying
both the quantity and quality of the bitext, we char-
acterize the tradeoffs between the amount of data,
computational costs, and translation quality.
</bodyText>
<sectionHeader confidence="0.999766" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9996785">
The idea of mining parallel sentences, particularly
from the web, is of course not new. Most adopt a
two step process: 1. identify comparable documents
and generate candidate sentence pairs, and 2. filter
candidate pairs to retain parallel sentences.
The general solution to the first step involves com-
puting pairwise similarities across multi-lingual cor-
pora. As this is computationally intensive, most
</bodyText>
<page confidence="0.90774">
626
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 626–630,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</page>
<bodyText confidence="0.983121617021277">
studies fall back to heuristics, e.g., comparing news
articles close in time (Munteanu and Marcu, 2005),
exploiting “inter-wiki” links in Wikipedia (Smith et
al., 2010), or bootstrapping off an existing search
engine (Resnik and Smith, 2003). In contrast, we
adopt a more exhaustive approach by directly tack-
ling the cross-lingual pairwise similarity problem,
using MapReduce on a modest cluster. We perform
experiments on German and English Wikipedia (two
largest available), but our technique is general and
does not depend on sparse, manually-created inter-
wiki links. Thus, compared to those approaches, we
achieve much higher recall.
The second step (filtering candidate sentence
pairs) is relatively straightforward, and we adopt
the classification approach of Munteanu and
Marcu (2005). However, unlike in previous work,
we need to classify large volumes of data (due to
higher recall in the first step). Therefore, we care
about the relationship between classification accu-
racy and the speed of the classifier. Our two-stage
approach gives us both high effectiveness (accuracy)
and efficiency (speed).
A recent study from Google describes a general
solution to our problem that scales to web collec-
tions (Uszkoreit et al., 2010). The authors translate
all documents from one language into another, thus
transforming the problem into identifying similar
mono-lingual document pairs. Nevertheless, our ap-
proach makes several additional contributions. First,
we explore the effect of dataset size on results. Our
conclusions are more nuanced than simply “more
data is better”, since there is a tradeoff between qual-
ity and quantity. Our experiments involve orders
of magnitude less data, but we nevertheless observe
significant gains over a strong baseline. Overall, our
approach requires far less computational resources
and thus is within the reach of academic research
groups: we do not require running an MT system
on one side of the entire collection, and we care-
fully evaluate and control the speed of sentence-
classification. Finally, in support of open science,
our code1 and data2 are available as part of Ivory, an
open-source Hadoop toolkit for web-scale informa-
tion retrieval (Lin et al., 2009).
1ivory.cc
2github.com/ferhanture/WikiBitext
</bodyText>
<sectionHeader confidence="0.953903" genericHeader="method">
3 Generating Candidate Sentences
</sectionHeader>
<bodyText confidence="0.999984586956522">
We applied our approach on English Wikipedia
(10.9m documents, 30.6GB) and German Wikipedia
(2.4m articles, 8.5GB), using XML dumps from Jan-
uary 2011. English and German Wikipedia were se-
lected because they are the largest Wikipedia collec-
tions available, and we want to measure effects in a
language for which we already have lots of bitext.
In both collections, redirect pages and stub articles
were discarded.
To mine comparable documents, we used our
previously described algorithm (Ture et al., 2011),
based on local-sensitive hashing, also implemented
in Hadoop MapReduce. The reader is referred to
the paper for details. On a 16 node (96 core) cluster,
we were able to extract 64m (de, df) document pairs
(with cosine similarity &gt; 0.3) in 8.8 hours.
For each of the (de, df) pairs, the next process-
ing step involves generating the Cartesian product of
sentences in both documents as candidate sentence
pairs: this itself is a non-trivial problem. Although
in this particular case it may be possible to load both
document collections in memory, we envision scal-
ing up to collections in the future for which this is
not possible. Therefore, we devised a scalable, dis-
tributed, out-of-memory solution using Hadoop.
The algorithm works as follows: We map over
(docid n, document d) pairs from both the German
and English collections. In each mapper all (de, df)
similarity pairs are loaded in memory. If the input
document is not found in any of these pairs, no work
is performed. Otherwise, we extract all sentences
and retain only those that have at least 5 terms and
at least 3 unique terms. Sentences are converted into
BM25-weighted vectors in the English term space;
for German sentences, translation into English is ac-
complished using the technique proposed by Dar-
wish and Oard (2003). For every (de, df) pair that
the input document is found in, the mapper emits the
list of weighted sentence vectors, with the (de, df)
pair as the key. As all intermediate key-value pairs
in MapReduce are grouped by their keys for reduce-
side processing, the reducer receives the key (de, df)
and weighted sentence vectors for both the German
and English articles. From there, we generate the
Cartesian product of sentences in both languages.
As an initial filtering step, we discard all pairs where
</bodyText>
<page confidence="0.992819">
627
</page>
<bodyText confidence="0.999935307692308">
the ratio of sentence lengths is more than two, a
heuristic proposed in (Munteanu and Marcu, 2005).
Each of the remaining candidate sentences are then
processed by two separate classifiers: a less accurate,
fast classifier and a more accurate, slow classifier.
This is described in the next section.
This algorithm is a variant of what is commonly
known as a reduce-side join in MapReduce (Lin
and Dyer, 2010), where (de, df) serves as the
join key. Note that in this algorithm, sentence
vectors are emitted multiple times, one for each
(de, df) pair that they participate in: this results
in increased network traffic during the sort/shuffle
phase. We experimented with an alternative algo-
rithm that processes all foreign documents similar
to the same English document together, e.g., pro-
cessing (de, [df1, df2, ...1) together. This approach,
counter-intuitively, was slower despite reduced net-
work traffic, due to skew in the distribution of sim-
ilar document pairs. In our experiments, half of the
source collection was not linked to any target docu-
ment, whereas 4% had more than 100 links. This re-
sults in reduce-side load imbalance, and while most
of the reducers finish quickly, a few reducers end
up performing substantially more computation, and
these “stragglers” increase end-to-end running time.
</bodyText>
<sectionHeader confidence="0.992214" genericHeader="method">
4 Parallel Sentence Classification
</sectionHeader>
<bodyText confidence="0.999766421052632">
We built two MaxEnt parallel sentence classifiers us-
ing the OpenNLP package, with data from a sam-
ple of the Europarl corpus of European parliament
speeches. For training, we sampled 1000 parallel
sentences from the German-English subset of the
corpus as positive instances, and 5000 non-parallel
sentence pairs as negative instances. For testing, we
sampled another 1000 parallel pairs and generated
all possible non-parallel pairs by the Cartesian prod-
uct of these samples. This provides a better approx-
imation of the task we’re interested in, since most of
the candidate sentence pairs will be non-parallel in a
comparable corpus. We report precision, recall, and
F-score, using different classifier confidence scores
as the decision threshold (see Table 1).
Our first, simple classifier, which uses cosine sim-
ilarity between the sentences as the only feature,
achieved a maximum F-score of 74%, with 80%
precision and 69% recall. Following previous work
</bodyText>
<table confidence="0.999271714285714">
Classifier Measure Value
Recall @ P90 0.59
Simple Recall @ P80 0.69
Best F-score 0.74
Recall @ P90 0.69
Complex Recall @ P80 0.79
Best F-score 0.80
</table>
<tableCaption confidence="0.998427">
Table 1: Accuracy of the simple and complex sentence
classifiers on Europarl data.
</tableCaption>
<bodyText confidence="0.992850105263158">
(Smith et al., 2010), we also report recall with pre-
cision at 80% and 90% in Table 1; the classifier ef-
fectiveness is comparable to the previous work. The
second, complex classifier uses the following addi-
tional features: ratio of sentence lengths, ratio of
source-side tokens that have translations on the tar-
get side, ratio of target-side tokens that have trans-
lations on the source side. We also experimented
with features using the word alignment output, but
there was no improvement in accuracy. The com-
plex classifier showed better performance: recall of
79% at 80% precision and 69% at precision of 90%,
with a maximum F-score of 80%.
Due to the large amounts of data involved in our
experiments, we were interested in speed/accuracy
tradeoffs between the two classifiers. Microbench-
marks were performed on a commodity laptop run-
ning Mac OS X on a 2.26GHz Intel Core Duo CPU,
measuring per-instance classification speed (includ-
ing feature computation time). The complex classi-
fier took 100 µs per instance, about 4 times slower
than the simple one, which took 27 µs.
The initial input of 64m similar document pairs
yielded 400b raw candidate sentence pairs, which
were first reduced to 214b by the per-sentence length
filter, and then to 132b by enforcing a maximum sen-
tence length ratio of 2. The simple classifier was
applied to the remaining pairs, with different confi-
dence thresholds. We adjusted the threshold to ob-
tain different amounts of bitext, to see the effect on
translation quality (this condition is called 51 here-
after). The positive results of the first classifier was
then processed by the second classifier (this two-
level approach is called 52 hereafter).
Candidate generation was completed in 2.4 hours
on our cluster with 96 cores. These candidates went
through the MapReduce shuffle-and-sort process in
0.75 hours, which were then classified in 4 hours.
</bodyText>
<page confidence="0.997593">
628
</page>
<bodyText confidence="0.998454">
Processing by the more complex classifier in 52 took
an additional 0.52 hours.
</bodyText>
<sectionHeader confidence="0.985202" genericHeader="method">
5 End-to-End MT Experiments
</sectionHeader>
<bodyText confidence="0.999866465116279">
In all experiments, our MT system learned a syn-
chronous context-free grammar (Chiang, 2007), us-
ing GIZA++ for word alignments, MIRA for pa-
rameter tuning (Crammer et al., 2006), cdec for de-
coding (Dyer et al., 2010), a 5-gram SRILM for
language modeling, and single-reference BLEU for
evaluation. The baseline system was trained on the
German-English WMT10 training data, consisting
of 3.1m sentence pairs. For development and test-
ing, we used the newswire datasets provided for
WMT10, including 2525 sentences for tuning and
2489 sentences for testing.
Our baseline system includes all standard fea-
tures, including phrase translation probabilities in
both directions, word and arity penalties, and lan-
guage model scores. It achieves a BLEU score
of 21.37 on the test set, which would place it 51h
out of 9 systems that reported comparable results
in WMT10 (only three systems achieved a BLEU
score over 22). Many of these systems used tech-
niques that exploited the specific aspects of the task,
e.g., German-specific morphological analysis. In
contrast, we present a knowledge-impoverished, en-
tirely data-driven approach, by simply looking for
more data in large collections.
For both experimental conditions (one-step classi-
fication, 51, and two-step classification, 52) we var-
ied the decision threshold to generate new bitext col-
lections of different sizes. Each of these collections
was added to the baseline training data to induce
an entirely new translation model (note that GIZA
additionally filtered out some of the pairs based on
length). The final dataset sizes, along with BLEU
scores on the test data, are shown in Fig. 1. In 51, we
observe that increasing the amount of data (by low-
ering the decision threshold) initially leads to lower
BLEU scores (due to increased noise), but there is a
threshold after which the improvement coming from
the added data supersedes the noise. The 52 condi-
tion increases the quality of bitext by reducing this
noise: the best run, with 5.8m pairs added to the
baseline (final dataset has 8.1m pairs), yields 23.76
BLEU (labeled P on figure), 2.39 points above the
</bodyText>
<figure confidence="0.991017">
3 3.5 4 4.5 5 5.5 6 6.5 7 7.5 8 8.5
Training data size (millions)
</figure>
<figureCaption confidence="0.999956">
Figure 1: Evaluation results on the WMT10 test set.
</figureCaption>
<bodyText confidence="0.999989588235294">
baseline (and higher than the best WMT10 result).
These results show that the two-step classification
process, while slower, is worth the additional pro-
cessing time.
Our approach yields solid improvements even
with less data added: with only 382k pairs added
to the baseline, the BLEU score increases by 1.84
points. In order to better examine the effect of
data size alone, we created partial datasets from P
by randomly sampling sentence pairs, and then re-
peated experiments, also shown in Fig. 1. We see
an increasing trend of BLEU scores with respect to
data size. By comparing the three plots, we see that
52 and random sampling from P work better than
51. Also, random sampling is not always worse than
52, since some pairs that receive low classifier con-
fidence turn out to be helpful.
</bodyText>
<sectionHeader confidence="0.999626" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999892066666667">
In this paper, we describe a scalable MapReduce im-
plementation for automatically mining parallel sen-
tences from arbitrary comparable corpora. We show,
at least for German-English MT, that an impover-
ished, data-driven approach is more effective than
task-specific engineering. With the distributed bi-
text mining machinery described in this paper, im-
provements come basically “for free” (the only cost
is a modest amount of cluster resources). Given the
availability of data and computing power, there is
simply no reason why MT researchers should not
ride the large-data “tide” that lifts all boats. For the
benefit of the community, all code necessary to repli-
cate these results have been open sourced, as well as
the bitext we’ve gathered.
</bodyText>
<figure confidence="0.96169875">
Baseline BLEU = 21.37
S1 (1-step classification)
S2 (2-step classification)
Sampling data from training set P
P
23.5
BLEU score
23
</figure>
<page confidence="0.995274">
629
</page>
<sectionHeader confidence="0.998284" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9997387">
This research was supported in part by the BOLT
program of the Defense Advanced Research Projects
Agency, Contract No. HR0011-12-C-0015; NSF un-
der awards IIS-0916043 and CCF-1018625. Any
opinions, findings, conclusions, or recommenda-
tions expressed in this paper are those of the authors
and do not necessarily reflect the view of the spon-
sors. The second author is grateful to Esther and
Kiri for their loving support and dedicates this work
to Joshua and Jacob.
</bodyText>
<sectionHeader confidence="0.998945" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999855923076923">
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 858–867, Prague, Czech Re-
public.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33:201–228.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551–585.
Kareem Darwish and Douglas W. Oard. 2003. Analysis
of anchor text for web search. Proceedings of the 26th
Annual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval (SI-
GIR 2003), pages 261–268, Toronto, Canada.
Chris Dyer, Aaron Cordova, Alex Mont, and Jimmy Lin.
2008. Fast, easy, and cheap: Construction of statisti-
cal machine translation models with MapReduce. Pro-
ceedings of the Third Workshop on Statistical Machine
Translation at ACL 2008, pages 199–207, Columbus,
Ohio.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec: A
decoder, alignment, and learning framework for finite-
state and context-free translation models. Proceedings
of the ACL 2010 System Demonstrations, pages 7–12,
Uppsala, Sweden, July.
Jimmy Lin and Chris Dyer. 2010. Data-Intensive Text
Processing with MapReduce. Morgan &amp; Claypool
Publishers.
Jimmy Lin, Donald Metzler, Tamer Elsayed, and Lidan
Wang. 2009. Of Ivory and Smurfs: Loxodontan
MapReduce experiments for web search. Proceedings
of the Eighteenth Text REtrieval Conference (TREC
2009), Gaithersburg, Maryland.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguistics,
31(4):477–504.
Philip Resnik and Noah A. Smith. 2003. The web
as a parallel corpus. Computational Linguistics,
29(3):349–380.
Jason R. Smith, Chris Quirk, and Kristina Toutanova.
2010. Extracting parallel sentences from comparable
corpora using document level alignment. Proceedings
of Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics (HLT/NAACL
2010), pages 403–411, Los Angeles, California.
Ferhan Ture, Tamer Elsayed, and Jimmy Lin. 2011.
No free lunch: Brute force vs. locality-sensitive hash-
ing for cross-lingual pairwise similarity. Proceedings
of the 34th Annual International ACM SIGIR Confer-
ence on Research and Development in Information Re-
trieval (SIGIR 2011), pages 943–952, Beijing, China.
Jakob Uszkoreit, Jay M. Ponte, Ashok C. Popat, and
Moshe Dubiner. 2010. Large scale parallel document
mining for machine translation. Proceedings of the
23rd International Conference on Computational Lin-
guistics (COLING 2010), pages 1101–1109, Beijing,
China.
</reference>
<page confidence="0.997602">
630
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.443600">
<title confidence="0.999556">Why Not Grab a Free Lunch? Mining Large Corpora for Parallel Sentences to Improve Translation Modeling</title>
<author confidence="0.995596">Ferhan Ture Jimmy Lin</author>
<affiliation confidence="0.9734855">Dept. of Computer Science, The iSchool University of Maryland University of Maryland</affiliation>
<email confidence="0.99983">fture@cs.umd.edujimmylin@umd.edu</email>
<abstract confidence="0.947367684210526">It is well known that the output quality of statistical machine translation (SMT) systems increases with more training data. To obtain more parallel text for translation modeling, researchers have turned to the web to mine parallel sentences, but most previous approaches have avoided the difficult problem of pairwise similarity on cross-lingual documents and instead rely on heuristics. In contrast, we confront this challenge head on using the MapReduce framework. On a modest cluster, our scalable end-to-end processing pipeline was able to automatically gather 5.8m parallel sentence pairs from English and German Wikipedia. Augmenting existing bitext with these data yielded significant improvements over a state-of-the-art baseline (2.39 BLEU points in the best case).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Ashok C Popat</author>
<author>Peng Xu</author>
<author>Franz J Och</author>
<author>Jeffrey Dean</author>
</authors>
<title>Large language models in machine translation.</title>
<date>2007</date>
<booktitle>Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>858--867</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="1258" citStr="Brants et al., 2007" startWordPosition="189" endWordPosition="192">cs. In contrast, we confront this challenge head on using the MapReduce framework. On a modest cluster, our scalable end-to-end processing pipeline was able to automatically gather 5.8m parallel sentence pairs from English and German Wikipedia. Augmenting existing bitext with these data yielded significant improvements over a state-of-the-art baseline (2.39 BLEU points in the best case). 1 Introduction It has been repeatedly shown that “throwing more data at the problem” is effective in increasing SMT output quality, both for translation modeling (Dyer et al., 2008) and for language modeling (Brants et al., 2007). In this paper, we bring together two related research threads to gather parallel sentences for improved translation modeling: cross-lingual pairwise similarity to mine comparable documents and classification to identify sentence pairs that are mutual translations. Unlike most previous work, which sidesteps the computationally-intensive task of pairwise comparisons to mine comparable documents and instead relies on heuristics, we tackle the challenge head on. This paper describes a fully open-source, scalable MapReduce-based processing pipeline that is able to automatically extract large quan</context>
</contexts>
<marker>Brants, Popat, Xu, Och, Dean, 2007</marker>
<rawString>Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och, and Jeffrey Dean. 2007. Large language models in machine translation. Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 858–867, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<pages>33--201</pages>
<contexts>
<context position="12742" citStr="Chiang, 2007" startWordPosition="2003" endWordPosition="2004"> effect on translation quality (this condition is called 51 hereafter). The positive results of the first classifier was then processed by the second classifier (this twolevel approach is called 52 hereafter). Candidate generation was completed in 2.4 hours on our cluster with 96 cores. These candidates went through the MapReduce shuffle-and-sort process in 0.75 hours, which were then classified in 4 hours. 628 Processing by the more complex classifier in 52 took an additional 0.52 hours. 5 End-to-End MT Experiments In all experiments, our MT system learned a synchronous context-free grammar (Chiang, 2007), using GIZA++ for word alignments, MIRA for parameter tuning (Crammer et al., 2006), cdec for decoding (Dyer et al., 2010), a 5-gram SRILM for language modeling, and single-reference BLEU for evaluation. The baseline system was trained on the German-English WMT10 training data, consisting of 3.1m sentence pairs. For development and testing, we used the newswire datasets provided for WMT10, including 2525 sentences for tuning and 2489 sentences for testing. Our baseline system includes all standard features, including phrase translation probabilities in both directions, word and arity penaltie</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33:201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai ShalevShwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online passiveaggressive algorithms.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>7--551</pages>
<contexts>
<context position="12826" citStr="Crammer et al., 2006" startWordPosition="2016" endWordPosition="2019">positive results of the first classifier was then processed by the second classifier (this twolevel approach is called 52 hereafter). Candidate generation was completed in 2.4 hours on our cluster with 96 cores. These candidates went through the MapReduce shuffle-and-sort process in 0.75 hours, which were then classified in 4 hours. 628 Processing by the more complex classifier in 52 took an additional 0.52 hours. 5 End-to-End MT Experiments In all experiments, our MT system learned a synchronous context-free grammar (Chiang, 2007), using GIZA++ for word alignments, MIRA for parameter tuning (Crammer et al., 2006), cdec for decoding (Dyer et al., 2010), a 5-gram SRILM for language modeling, and single-reference BLEU for evaluation. The baseline system was trained on the German-English WMT10 training data, consisting of 3.1m sentence pairs. For development and testing, we used the newswire datasets provided for WMT10, including 2525 sentences for tuning and 2489 sentences for testing. Our baseline system includes all standard features, including phrase translation probabilities in both directions, word and arity penalties, and language model scores. It achieves a BLEU score of 21.37 on the test set, whi</context>
</contexts>
<marker>Crammer, Dekel, Keshet, ShalevShwartz, Singer, 2006</marker>
<rawString>Koby Crammer, Ofer Dekel, Joseph Keshet, Shai ShalevShwartz, and Yoram Singer. 2006. Online passiveaggressive algorithms. Journal of Machine Learning Research, 7:551–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kareem Darwish</author>
<author>Douglas W Oard</author>
</authors>
<title>Analysis of anchor text for web search.</title>
<date>2003</date>
<booktitle>Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR</booktitle>
<pages>261--268</pages>
<location>Toronto, Canada.</location>
<contexts>
<context position="7623" citStr="Darwish and Oard (2003)" startWordPosition="1171" endWordPosition="1175">stributed, out-of-memory solution using Hadoop. The algorithm works as follows: We map over (docid n, document d) pairs from both the German and English collections. In each mapper all (de, df) similarity pairs are loaded in memory. If the input document is not found in any of these pairs, no work is performed. Otherwise, we extract all sentences and retain only those that have at least 5 terms and at least 3 unique terms. Sentences are converted into BM25-weighted vectors in the English term space; for German sentences, translation into English is accomplished using the technique proposed by Darwish and Oard (2003). For every (de, df) pair that the input document is found in, the mapper emits the list of weighted sentence vectors, with the (de, df) pair as the key. As all intermediate key-value pairs in MapReduce are grouped by their keys for reduceside processing, the reducer receives the key (de, df) and weighted sentence vectors for both the German and English articles. From there, we generate the Cartesian product of sentences in both languages. As an initial filtering step, we discard all pairs where 627 the ratio of sentence lengths is more than two, a heuristic proposed in (Munteanu and Marcu, 20</context>
</contexts>
<marker>Darwish, Oard, 2003</marker>
<rawString>Kareem Darwish and Douglas W. Oard. 2003. Analysis of anchor text for web search. Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2003), pages 261–268, Toronto, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Aaron Cordova</author>
<author>Alex Mont</author>
<author>Jimmy Lin</author>
</authors>
<title>Fast, easy, and cheap: Construction of statistical machine translation models with MapReduce.</title>
<date>2008</date>
<booktitle>Proceedings of the Third Workshop on Statistical Machine Translation at ACL</booktitle>
<pages>199--207</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="1210" citStr="Dyer et al., 2008" startWordPosition="181" endWordPosition="184">lingual documents and instead rely on heuristics. In contrast, we confront this challenge head on using the MapReduce framework. On a modest cluster, our scalable end-to-end processing pipeline was able to automatically gather 5.8m parallel sentence pairs from English and German Wikipedia. Augmenting existing bitext with these data yielded significant improvements over a state-of-the-art baseline (2.39 BLEU points in the best case). 1 Introduction It has been repeatedly shown that “throwing more data at the problem” is effective in increasing SMT output quality, both for translation modeling (Dyer et al., 2008) and for language modeling (Brants et al., 2007). In this paper, we bring together two related research threads to gather parallel sentences for improved translation modeling: cross-lingual pairwise similarity to mine comparable documents and classification to identify sentence pairs that are mutual translations. Unlike most previous work, which sidesteps the computationally-intensive task of pairwise comparisons to mine comparable documents and instead relies on heuristics, we tackle the challenge head on. This paper describes a fully open-source, scalable MapReduce-based processing pipeline </context>
</contexts>
<marker>Dyer, Cordova, Mont, Lin, 2008</marker>
<rawString>Chris Dyer, Aaron Cordova, Alex Mont, and Jimmy Lin. 2008. Fast, easy, and cheap: Construction of statistical machine translation models with MapReduce. Proceedings of the Third Workshop on Statistical Machine Translation at ACL 2008, pages 199–207, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Adam Lopez</author>
<author>Juri Ganitkevitch</author>
<author>Jonathan Weese</author>
<author>Ferhan Ture</author>
<author>Phil Blunsom</author>
<author>Hendra Setiawan</author>
<author>Vladimir Eidelman</author>
<author>Philip Resnik</author>
</authors>
<title>cdec: A decoder, alignment, and learning framework for finitestate and context-free translation models.</title>
<date>2010</date>
<booktitle>Proceedings of the ACL 2010 System Demonstrations,</booktitle>
<pages>7--12</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="12865" citStr="Dyer et al., 2010" startWordPosition="2024" endWordPosition="2027">as then processed by the second classifier (this twolevel approach is called 52 hereafter). Candidate generation was completed in 2.4 hours on our cluster with 96 cores. These candidates went through the MapReduce shuffle-and-sort process in 0.75 hours, which were then classified in 4 hours. 628 Processing by the more complex classifier in 52 took an additional 0.52 hours. 5 End-to-End MT Experiments In all experiments, our MT system learned a synchronous context-free grammar (Chiang, 2007), using GIZA++ for word alignments, MIRA for parameter tuning (Crammer et al., 2006), cdec for decoding (Dyer et al., 2010), a 5-gram SRILM for language modeling, and single-reference BLEU for evaluation. The baseline system was trained on the German-English WMT10 training data, consisting of 3.1m sentence pairs. For development and testing, we used the newswire datasets provided for WMT10, including 2525 sentences for tuning and 2489 sentences for testing. Our baseline system includes all standard features, including phrase translation probabilities in both directions, word and arity penalties, and language model scores. It achieves a BLEU score of 21.37 on the test set, which would place it 51h out of 9 systems </context>
</contexts>
<marker>Dyer, Lopez, Ganitkevitch, Weese, Ture, Blunsom, Setiawan, Eidelman, Resnik, 2010</marker>
<rawString>Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan, Vladimir Eidelman, and Philip Resnik. 2010. cdec: A decoder, alignment, and learning framework for finitestate and context-free translation models. Proceedings of the ACL 2010 System Demonstrations, pages 7–12, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jimmy Lin</author>
<author>Chris Dyer</author>
</authors>
<date>2010</date>
<booktitle>Data-Intensive Text Processing with MapReduce.</booktitle>
<publisher>Morgan &amp; Claypool Publishers.</publisher>
<contexts>
<context position="8537" citStr="Lin and Dyer, 2010" startWordPosition="1326" endWordPosition="1329">nd weighted sentence vectors for both the German and English articles. From there, we generate the Cartesian product of sentences in both languages. As an initial filtering step, we discard all pairs where 627 the ratio of sentence lengths is more than two, a heuristic proposed in (Munteanu and Marcu, 2005). Each of the remaining candidate sentences are then processed by two separate classifiers: a less accurate, fast classifier and a more accurate, slow classifier. This is described in the next section. This algorithm is a variant of what is commonly known as a reduce-side join in MapReduce (Lin and Dyer, 2010), where (de, df) serves as the join key. Note that in this algorithm, sentence vectors are emitted multiple times, one for each (de, df) pair that they participate in: this results in increased network traffic during the sort/shuffle phase. We experimented with an alternative algorithm that processes all foreign documents similar to the same English document together, e.g., processing (de, [df1, df2, ...1) together. This approach, counter-intuitively, was slower despite reduced network traffic, due to skew in the distribution of similar document pairs. In our experiments, half of the source co</context>
</contexts>
<marker>Lin, Dyer, 2010</marker>
<rawString>Jimmy Lin and Chris Dyer. 2010. Data-Intensive Text Processing with MapReduce. Morgan &amp; Claypool Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jimmy Lin</author>
<author>Donald Metzler</author>
<author>Tamer Elsayed</author>
<author>Lidan Wang</author>
</authors>
<title>Of Ivory and Smurfs: Loxodontan MapReduce experiments for web search.</title>
<date>2009</date>
<booktitle>Proceedings of the Eighteenth Text REtrieval Conference (TREC</booktitle>
<location>Gaithersburg, Maryland.</location>
<contexts>
<context position="5750" citStr="Lin et al., 2009" startWordPosition="869" endWordPosition="872"> is a tradeoff between quality and quantity. Our experiments involve orders of magnitude less data, but we nevertheless observe significant gains over a strong baseline. Overall, our approach requires far less computational resources and thus is within the reach of academic research groups: we do not require running an MT system on one side of the entire collection, and we carefully evaluate and control the speed of sentenceclassification. Finally, in support of open science, our code1 and data2 are available as part of Ivory, an open-source Hadoop toolkit for web-scale information retrieval (Lin et al., 2009). 1ivory.cc 2github.com/ferhanture/WikiBitext 3 Generating Candidate Sentences We applied our approach on English Wikipedia (10.9m documents, 30.6GB) and German Wikipedia (2.4m articles, 8.5GB), using XML dumps from January 2011. English and German Wikipedia were selected because they are the largest Wikipedia collections available, and we want to measure effects in a language for which we already have lots of bitext. In both collections, redirect pages and stub articles were discarded. To mine comparable documents, we used our previously described algorithm (Ture et al., 2011), based on local</context>
</contexts>
<marker>Lin, Metzler, Elsayed, Wang, 2009</marker>
<rawString>Jimmy Lin, Donald Metzler, Tamer Elsayed, and Lidan Wang. 2009. Of Ivory and Smurfs: Loxodontan MapReduce experiments for web search. Proceedings of the Eighteenth Text REtrieval Conference (TREC 2009), Gaithersburg, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragos Stefan Munteanu</author>
<author>Daniel Marcu</author>
</authors>
<title>Improving machine translation performance by exploiting non-parallel corpora.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>4</issue>
<contexts>
<context position="3644" citStr="Munteanu and Marcu, 2005" startWordPosition="547" endWordPosition="550">process: 1. identify comparable documents and generate candidate sentence pairs, and 2. filter candidate pairs to retain parallel sentences. The general solution to the first step involves computing pairwise similarities across multi-lingual corpora. As this is computationally intensive, most 626 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 626–630, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics studies fall back to heuristics, e.g., comparing news articles close in time (Munteanu and Marcu, 2005), exploiting “inter-wiki” links in Wikipedia (Smith et al., 2010), or bootstrapping off an existing search engine (Resnik and Smith, 2003). In contrast, we adopt a more exhaustive approach by directly tackling the cross-lingual pairwise similarity problem, using MapReduce on a modest cluster. We perform experiments on German and English Wikipedia (two largest available), but our technique is general and does not depend on sparse, manually-created interwiki links. Thus, compared to those approaches, we achieve much higher recall. The second step (filtering candidate sentence pairs) is relativel</context>
<context position="8226" citStr="Munteanu and Marcu, 2005" startWordPosition="1275" endWordPosition="1278">arwish and Oard (2003). For every (de, df) pair that the input document is found in, the mapper emits the list of weighted sentence vectors, with the (de, df) pair as the key. As all intermediate key-value pairs in MapReduce are grouped by their keys for reduceside processing, the reducer receives the key (de, df) and weighted sentence vectors for both the German and English articles. From there, we generate the Cartesian product of sentences in both languages. As an initial filtering step, we discard all pairs where 627 the ratio of sentence lengths is more than two, a heuristic proposed in (Munteanu and Marcu, 2005). Each of the remaining candidate sentences are then processed by two separate classifiers: a less accurate, fast classifier and a more accurate, slow classifier. This is described in the next section. This algorithm is a variant of what is commonly known as a reduce-side join in MapReduce (Lin and Dyer, 2010), where (de, df) serves as the join key. Note that in this algorithm, sentence vectors are emitted multiple times, one for each (de, df) pair that they participate in: this results in increased network traffic during the sort/shuffle phase. We experimented with an alternative algorithm th</context>
</contexts>
<marker>Munteanu, Marcu, 2005</marker>
<rawString>Dragos Stefan Munteanu and Daniel Marcu. 2005. Improving machine translation performance by exploiting non-parallel corpora. Computational Linguistics, 31(4):477–504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
<author>Noah A Smith</author>
</authors>
<title>The web as a parallel corpus.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>3</issue>
<contexts>
<context position="3782" citStr="Resnik and Smith, 2003" startWordPosition="567" endWordPosition="570"> The general solution to the first step involves computing pairwise similarities across multi-lingual corpora. As this is computationally intensive, most 626 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 626–630, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics studies fall back to heuristics, e.g., comparing news articles close in time (Munteanu and Marcu, 2005), exploiting “inter-wiki” links in Wikipedia (Smith et al., 2010), or bootstrapping off an existing search engine (Resnik and Smith, 2003). In contrast, we adopt a more exhaustive approach by directly tackling the cross-lingual pairwise similarity problem, using MapReduce on a modest cluster. We perform experiments on German and English Wikipedia (two largest available), but our technique is general and does not depend on sparse, manually-created interwiki links. Thus, compared to those approaches, we achieve much higher recall. The second step (filtering candidate sentence pairs) is relatively straightforward, and we adopt the classification approach of Munteanu and Marcu (2005). However, unlike in previous work, we need to cla</context>
</contexts>
<marker>Resnik, Smith, 2003</marker>
<rawString>Philip Resnik and Noah A. Smith. 2003. The web as a parallel corpus. Computational Linguistics, 29(3):349–380.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason R Smith</author>
<author>Chris Quirk</author>
<author>Kristina Toutanova</author>
</authors>
<title>Extracting parallel sentences from comparable corpora using document level alignment.</title>
<date>2010</date>
<booktitle>Proceedings of Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics (HLT/NAACL 2010),</booktitle>
<pages>403--411</pages>
<location>Los Angeles, California.</location>
<contexts>
<context position="3709" citStr="Smith et al., 2010" startWordPosition="556" endWordPosition="559">ce pairs, and 2. filter candidate pairs to retain parallel sentences. The general solution to the first step involves computing pairwise similarities across multi-lingual corpora. As this is computationally intensive, most 626 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 626–630, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics studies fall back to heuristics, e.g., comparing news articles close in time (Munteanu and Marcu, 2005), exploiting “inter-wiki” links in Wikipedia (Smith et al., 2010), or bootstrapping off an existing search engine (Resnik and Smith, 2003). In contrast, we adopt a more exhaustive approach by directly tackling the cross-lingual pairwise similarity problem, using MapReduce on a modest cluster. We perform experiments on German and English Wikipedia (two largest available), but our technique is general and does not depend on sparse, manually-created interwiki links. Thus, compared to those approaches, we achieve much higher recall. The second step (filtering candidate sentence pairs) is relatively straightforward, and we adopt the classification approach of Mu</context>
<context position="10677" citStr="Smith et al., 2010" startWordPosition="1663" endWordPosition="1666">ill be non-parallel in a comparable corpus. We report precision, recall, and F-score, using different classifier confidence scores as the decision threshold (see Table 1). Our first, simple classifier, which uses cosine similarity between the sentences as the only feature, achieved a maximum F-score of 74%, with 80% precision and 69% recall. Following previous work Classifier Measure Value Recall @ P90 0.59 Simple Recall @ P80 0.69 Best F-score 0.74 Recall @ P90 0.69 Complex Recall @ P80 0.79 Best F-score 0.80 Table 1: Accuracy of the simple and complex sentence classifiers on Europarl data. (Smith et al., 2010), we also report recall with precision at 80% and 90% in Table 1; the classifier effectiveness is comparable to the previous work. The second, complex classifier uses the following additional features: ratio of sentence lengths, ratio of source-side tokens that have translations on the target side, ratio of target-side tokens that have translations on the source side. We also experimented with features using the word alignment output, but there was no improvement in accuracy. The complex classifier showed better performance: recall of 79% at 80% precision and 69% at precision of 90%, with a ma</context>
</contexts>
<marker>Smith, Quirk, Toutanova, 2010</marker>
<rawString>Jason R. Smith, Chris Quirk, and Kristina Toutanova. 2010. Extracting parallel sentences from comparable corpora using document level alignment. Proceedings of Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics (HLT/NAACL 2010), pages 403–411, Los Angeles, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ferhan Ture</author>
<author>Tamer Elsayed</author>
<author>Jimmy Lin</author>
</authors>
<title>No free lunch: Brute force vs. locality-sensitive hashing for cross-lingual pairwise similarity.</title>
<date>2011</date>
<booktitle>Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2011),</booktitle>
<pages>943--952</pages>
<location>Beijing, China.</location>
<contexts>
<context position="2175" citStr="Ture et al., 2011" startWordPosition="325" endWordPosition="328"> sidesteps the computationally-intensive task of pairwise comparisons to mine comparable documents and instead relies on heuristics, we tackle the challenge head on. This paper describes a fully open-source, scalable MapReduce-based processing pipeline that is able to automatically extract large quantities of parallel sentences. Experiments examine the impact data size has on a state-of-the-art SMT system. We acknowledge that different components of this work are not novel and the general principles behind “big data” MT are well known. However, when considered together with our previous work (Ture et al., 2011), to our knowledge this is the first exposition in which all the pieces have been “put together” in an end-to-end pipeline that is accessible to academic research groups. The framework described in this paper is entirely open source, and the computational resources necessary to replicate our results are relatively modest. Starting from nothing more than two corpora in different languages (in German and English, in our case), we are able to extract bitext and improve translation quality by a significant margin (2.39 BLEU points), essentially “for free”. By varying both the quantity and quality </context>
<context position="6334" citStr="Ture et al., 2011" startWordPosition="955" endWordPosition="958">ation retrieval (Lin et al., 2009). 1ivory.cc 2github.com/ferhanture/WikiBitext 3 Generating Candidate Sentences We applied our approach on English Wikipedia (10.9m documents, 30.6GB) and German Wikipedia (2.4m articles, 8.5GB), using XML dumps from January 2011. English and German Wikipedia were selected because they are the largest Wikipedia collections available, and we want to measure effects in a language for which we already have lots of bitext. In both collections, redirect pages and stub articles were discarded. To mine comparable documents, we used our previously described algorithm (Ture et al., 2011), based on local-sensitive hashing, also implemented in Hadoop MapReduce. The reader is referred to the paper for details. On a 16 node (96 core) cluster, we were able to extract 64m (de, df) document pairs (with cosine similarity &gt; 0.3) in 8.8 hours. For each of the (de, df) pairs, the next processing step involves generating the Cartesian product of sentences in both documents as candidate sentence pairs: this itself is a non-trivial problem. Although in this particular case it may be possible to load both document collections in memory, we envision scaling up to collections in the future fo</context>
</contexts>
<marker>Ture, Elsayed, Lin, 2011</marker>
<rawString>Ferhan Ture, Tamer Elsayed, and Jimmy Lin. 2011. No free lunch: Brute force vs. locality-sensitive hashing for cross-lingual pairwise similarity. Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2011), pages 943–952, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jakob Uszkoreit</author>
<author>Jay M Ponte</author>
<author>Ashok C Popat</author>
<author>Moshe Dubiner</author>
</authors>
<title>Large scale parallel document mining for machine translation.</title>
<date>2010</date>
<booktitle>Proceedings of the 23rd International Conference on Computational Linguistics (COLING 2010),</booktitle>
<pages>1101--1109</pages>
<location>Beijing, China.</location>
<contexts>
<context position="4776" citStr="Uszkoreit et al., 2010" startWordPosition="718" endWordPosition="721">hieve much higher recall. The second step (filtering candidate sentence pairs) is relatively straightforward, and we adopt the classification approach of Munteanu and Marcu (2005). However, unlike in previous work, we need to classify large volumes of data (due to higher recall in the first step). Therefore, we care about the relationship between classification accuracy and the speed of the classifier. Our two-stage approach gives us both high effectiveness (accuracy) and efficiency (speed). A recent study from Google describes a general solution to our problem that scales to web collections (Uszkoreit et al., 2010). The authors translate all documents from one language into another, thus transforming the problem into identifying similar mono-lingual document pairs. Nevertheless, our approach makes several additional contributions. First, we explore the effect of dataset size on results. Our conclusions are more nuanced than simply “more data is better”, since there is a tradeoff between quality and quantity. Our experiments involve orders of magnitude less data, but we nevertheless observe significant gains over a strong baseline. Overall, our approach requires far less computational resources and thus </context>
</contexts>
<marker>Uszkoreit, Ponte, Popat, Dubiner, 2010</marker>
<rawString>Jakob Uszkoreit, Jay M. Ponte, Ashok C. Popat, and Moshe Dubiner. 2010. Large scale parallel document mining for machine translation. Proceedings of the 23rd International Conference on Computational Linguistics (COLING 2010), pages 1101–1109, Beijing, China.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>