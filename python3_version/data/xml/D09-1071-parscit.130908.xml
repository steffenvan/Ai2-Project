<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009571">
<title confidence="0.911198">
The infinite HMM for unsupervised PoS tagging
</title>
<author confidence="0.988485">
Jurgen Van Gael Andreas Vlachos Zoubin Ghahramani
</author>
<affiliation confidence="0.999605">
Department of Engineering Computer Laboratory Department of Engineering
University of Cambridge University of Cambridge University of Cambridge
</affiliation>
<email confidence="0.959216">
jv249@cam.ac.uk av308@cl.cam.ac.uk zoubin@eng.cam.ac.uk
</email>
<sectionHeader confidence="0.99398" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999956714285714">
We extend previous work on fully unsu-
pervised part-of-speech tagging. Using
a non-parametric version of the HMM,
called the infinite HMM (iHMM), we ad-
dress the problem of choosing the number
of hidden states in unsupervised Markov
models for PoS tagging. We experi-
ment with two non-parametric priors, the
Dirichlet and Pitman-Yor processes, on the
Wall Street Journal dataset using a paral-
lelized implementation of an iHMM in-
ference algorithm. We evaluate the re-
sults with a variety of clustering evalua-
tion metrics and achieve equivalent or bet-
ter performances than previously reported.
Building on this promising result we eval-
uate the output of the unsupervised PoS
tagger as a direct replacement for the out-
put of a fully supervised PoS tagger for the
task of shallow parsing and compare the
two evaluations.
</bodyText>
<sectionHeader confidence="0.998752" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999800701754386">
Many Natural Language Processing (NLP) tasks
are commonly tackled using supervised learning
approaches. These learning methods rely on the
availability of labeled datasets which are usually
produced by expensive manual annotation. For
some tasks, we have the choice to use unsuper-
vised learning approaches. While they do not nec-
essarily achieve the same level of performance,
they are appealing as unlabeled data is usually
abundant. In particular, for the purpose of ex-
ploring new domains and languages, obtainining
labeled material can be prohibitively expensive
and unsupervised learning methods are a very at-
tractive choice. Recent work (Johnson, 2007;
Goldwater and Griffiths, 2007; Gao and Johnson,
2008) explored the task of part-of-speech tagging
(PoS) using unsupervised Hidden Markov Models
(HMMs) with encouraging results. PoS tagging is
a standard component in many linguistic process-
ing pipelines, so any improvement on its perfor-
mance is likely to impact a wide range of tasks.
It is important to point out that a completely
unsupervised learning method will discover the
statistics of a dataset according to a particular
model choice but these statistics might not cor-
respond exactly to our intuition about PoS tags.
Johnson (2007) and Gao &amp; Johnson (2008) as-
sume that words are generated by a hidden Markov
model and find that the resulting states strongly
correlate with POS tags. Nonetheless, identifying
the HMM states with appropriate POS tags is hard.
Because many evaluation methods often require
POS tags (rather than HMM states) this identifica-
tion problem makes unsupervised systems difficult
to evaluate.
One potential solution is to add a small amount
of supervision as in Goldwater &amp; Griffiths (2007)
who assume a dictionary of frequent words asso-
ciated with possible PoS tags extracted from a la-
beled corpus. Although this technique improves
performance, in this paper we explore the com-
pletely unsupervised approach. The reason for this
is that better unsupervised approaches provide us
with better starting points from which to explore
how and where to incorporate supervision.
In previous work on unsupervised PoS tagging
a main question was how to set the number of hid-
den states appropriately. Johnson (2007) reports
results for different numbers of hidden states but it
is unclear how to make this choice a priori, while
Goldwater &amp; Griffiths (2007) leave this question
as future work.
It is not uncommon in statistical machine learn-
ing to distinguish between parameters of a model
and the capacity of a model. E.g. in a clustering
context, the choice for the number of clusters (ca-
pacity) and the parameters of each cluster are often
</bodyText>
<page confidence="0.972813">
678
</page>
<note confidence="0.996613">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 678–687,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.998532111111111">
treated differently: the latter are estimated using
algorithms like EM, MCMC or Variational Bayes
while the former is chosen using common sense,
heuristics or in a Bayesian framework maybe us-
ing evidence maximization.
Non-parametric Bayesian methods are a class of
probability distributions which explicitly treat the
capacity of a model as “just another parameter”.
Potential advantages are
</bodyText>
<listItem confidence="0.980597142857143">
• the model capacity can automatically adjust
to the amount of data: e.g. when clustering
a very small dataset, it is unlikely that many
fine grained clusters can be distinguished,
• inference can be more efficient: e.g. instead
of running full inference for different model
capacities and then choosing the best ca-
pacity (according to some choice of “best”),
inference in non-parametric Bayesian meth-
ods integrates the capacity search in one al-
gorithm. This is particularly advantageous
when parameters other than capacity need to
be explored, since it reduces signifcantly the
number of experiments needed.
</listItem>
<bodyText confidence="0.999946756756757">
None of these potential advantages are guaranteed
and in this paper we investigate these two aspects
for the task of unsupervised PoS tagging.
The contributions in this paper extend previous
work on unsupervised PoS tagging in five ways.
First, we introduce the use of a non-parametric
version of the HMM, namely the infinite HMM
(iHMM) (Beal et al., 2002) for unsupervised PoS
tagging. This answers an open problem from
Goldwater &amp; Griffiths (2007). Second, we care-
fully implemented a parallelized version of the
inference algorithms for the iHMM so we could
use it on the Wall Street Journal Penn Treebank
dataset. Third, we introduce a new variant of
the iHMM that builds on the Pitman-Yor process.
Fourth, we evaluate the results with a variety of
clustering evaluation methods and achieve equiv-
alent or better performances than previously re-
ported. Finally, building on this promising result
we use the output of the unsupervised PoS tagger
as a direct replacement for the output of a fully su-
pervised PoS tagger for the task of shallow pars-
ing. This evaluation enables us to assess the appli-
cability of an unsupervised PoS tagging method
and provides us with means of comparing its per-
formance against a supervised PoS tagger.
The rest of the paper is structured as follows:
in section 2 we introduce the iHMM as a non-
parametric version of the Bayesian HMM used
in previous work on unsupervised PoS tagging.
Then, in section 3 we describe some details of
our implementation of the iHMM. In section 4 we
present a variety of evaluation metrics to compare
our results with previous work. Finally, in sec-
tion 5 we report our experimental results. We con-
clude this paper with a discussion of ongoing work
and experiments.
</bodyText>
<sectionHeader confidence="0.922538" genericHeader="method">
2 The Infinite HMM
</sectionHeader>
<bodyText confidence="0.997498">
In this section, we describe a non-parametric hid-
den Markov model known as the infinite HMM
(iHMM) (Beal et al., 2002; Teh et al., 2006). As
we show below, this model is flexible in the num-
ber of hidden states which it can accomodate. In
other words, the capacity is an uncertain quantity
with an a priori infinite range that is a posteriori
inferred by the data. It is instructive to first re-
view the finite HMM and its Bayesian treatment:
for one, it is the model that has been used in previ-
ous work on unsupervised PoS tagging, secondly
it allows us to better understand the iHMM.
The Bayesian HMM A finite first-order HMM
consists of a hidden state sequence s =
(s1, s2, ... , sT) and a corresponding observation
sequence y = (y1, y2,.. . , yT). Each state vari-
able st can take on a finite number of states, say
1... K. Transitions between states are governed
by Markov dynamics parameterized by the tran-
sition matrix π, where πij = p(st = j|st−1 =
i), while the initial state probabilities are π0i =
p(s1 = i). For each state st E {1... K} there
is a parameter φst which parameterizes the obser-
vation likelihood for that state: yt|st — F (φst).
Given the parameters 1π0, π, φ, K} of the HMM,
the joint distribution over hidden states s and ob-
servations y can be written (with s0 = 0):
</bodyText>
<equation confidence="0.997313">
T
p(s, y|π0, π, φ, K) = H p(st|st−1)p(yt|st)
t=1
</equation>
<bodyText confidence="0.999676875">
As Johnson (2007) clearly explained, training the
HMM with EM leads to poor results in PoS tag-
ging. However, we can easily treat the HMM in a
fully Bayesian way (MacKay, 1997) by introduc-
ing priors on the parameters of the HMM. With
no further prior knowledge, a typical prior for the
transition (and initial) probabilities are symmet-
ric Dirichlet distributions. This corresponds to our
</bodyText>
<page confidence="0.998773">
679
</page>
<bodyText confidence="0.9999294">
belief that, a priori, each state is equally likely to
transition to every other state. Also, it is com-
monly known that the parameter of a Dirichlet
distribution controls how sparse its samples are.
In other words, by making the hyperprior on the
Dirichlet distribution for the rows of the transi-
tion matrix small, we can encode our belief that
any state (corresponding to a PoS tag in this ap-
plication context) will only be followed by a small
number of other states. As we explain below, we
will be able to include this desirable property in
the non-parametric model as well. Secondly, we
need to introduce a prior on the observation pa-
rameters φk. Without any further prior knowl-
edge, a convenient choice here is another sym-
metric Dirichlet distribution with sparsity induc-
ing hyperprior. This encodes our belief that only
a subset of the words correspond to a particular
state.
The Infinite HMM A first naive way to obtain
a non-parametric HMM with an infinite number
of states might be to use symmetric Dirichlet pri-
ors over the transition probabilities with parameter
α/K and take K → ∞. This approach unfortu-
nately does not work: α/K → 0 when K → ∞
and hence the rows of the matrix will become “in-
finitely sparse”. Since the sum of the entries must
sum to one, the rows of the transition matrix will
be zero everywhere and all its mass in a random
location. Unfortunately, this random location is
out of an infinite number of possible locations and
hence with probability 1 will be different for all
the rows. As a consequence, at each timestep the
HMM moves to a new state and will never revisit
old states. As we shall see shortly, we can fix this
by using a hierarchical Bayesian formalism where
the Dirichlet priors on the rows have a shared pa-
rameter.
Before moving on to the iHMM, let us look at
the finite HMM from a different perspective. The
finite HMM of length T with K hidden states can
be seen as a sequence of T finite mixture models.
The following equation illustrates this idea: con-
ditioned on the previous state st−1, the marginal
probability of observation yt can be written as:
</bodyText>
<equation confidence="0.983402666666667">
K
p(yt|st−1 = k) = p(st|st−1 = k)p(yt|st),
st=1
K
πk,stp(yt|φst). (1)
st=1
</equation>
<bodyText confidence="0.99975">
The variable st−1 = k specifies the mixing
weights πk,· for the mixture distribution, while st
indexes the mixture component generating the ob-
servation yt. In other words, equation (1) says that
each row of the transition matrix π specifies a dif-
ferent mixture distribution over the same set of K
mixture components φ.
Our second attempt to define a non-parametric
version of the hidden Markov model is to replace
the finite mixture by an infinite mixture. The
theory of Dirichlet process mixtures (Antoniak,
1974) tells us exactly how to do this. A draw
G ∼ DP(α, H) from a Dirichlet process (DP)
with base measure H and concentration parame-
ter α ≥ 0 is a discrete distribution which can be
written as an infinite mixture of atoms
</bodyText>
<equation confidence="0.9997935">
G(·) = �∞ πiδφi(·)
i=1
</equation>
<bodyText confidence="0.997681">
where the φi are i.i.d. draws from the base mea-
sure H, δφi(·) represents a point distribution at
</bodyText>
<equation confidence="0.5864035">
�i−1
φi and πi = vi l=1(1 − vl) where each vl ∼
</equation>
<bodyText confidence="0.999786266666667">
Beta(1, α). The distribution over πi is called a
stick breaking construction and is essentially an
infinite dimensional version of the Dirichlet dis-
tribution. We refer to Teh et al. (2006) for more
details.
Switching back to the iHMM our next step is to
introduce a DP Gj for each state j ∈ {1 · · · ∞};
we write Gj(·) = E∞i=1 πji δφj i (·). There is now
a parameter for each state j and each index i ∈
{1, 2, · · · , ∞}. Next, we draw the datapoint at
timestep t given that the previous datapoint was in
state st−1 by drawing from DP Gst−1. We first se-
lect a mixture component st from the vector πst−1,·
and then sample a datapoint yt ∼ F(φst−1,st) so
we get the following distribution for yt
</bodyText>
<equation confidence="0.9575205">
p(yt|α, st−1) = �∞ πst−1,stp(yt|φst−1,st).
st=1
</equation>
<bodyText confidence="0.9902742">
This is almost the non-parametric equivalent of
equation (1) but there is a subtle difference: each
Gj selects their own set of parameters φj·. This
is unfortunate as it means that the output distribu-
tion would not be the same for each state, it would
depend on which state we were moving to! Luck-
ily, we can easily fix this: by introducing an in-
termediate distribution G0 ∼ DP(γ, H) and let
Gj ∼ DP(α, G0) we enforce that the i.i.d. draws
φj · are draws from a discrete distribution (since G0
</bodyText>
<page confidence="0.98468">
680
</page>
<bodyText confidence="0.9997382">
is a draw from a Dirichlet process) and hence all
Gj will share the same infinite set of atoms as cho-
sen by G0. Figure 1 illustrates the graphical model
for the iHMM.
The iHMM with Pitman-Yor Prior The
Dirichlet process described above defines a very
specific distribution over the number of states
in the iHMM. One particular generalization of
the Dirichlet process that has been studied in the
NLP literature before is the Pitman-Yor process.
Goldwater et al. (2006) have shown that the
Pitman-Yor distribution can more accurately cap-
ture power-law like distributions that frequently
occur in natural language.
More specifically, a draw G ∼ PY (d, α, H)
from a Pitman-Yor process (PY) with base mea-
sure H, discount parameter 0 ≤ d &lt; 1 and con-
centration parameter α &gt; −d is a discrete distri-
bution which can be written as an infinite mixture
of atoms
</bodyText>
<equation confidence="0.999769">
G(·) = �∞ 7ri6φ,(·)
i=1
</equation>
<bodyText confidence="0.953236592592593">
where the 0i are i.i.d. draws from the base mea-
sure H, 6φ,(·) represents a point distribution at
�i−1
0i and 7ri = vi l=1(1 − vl) where each vl ∼
Beta(1− d, α+ld). Note the similarity to the DP:
in fact, the DP is a special case of PY with d = 0.
In our experiments, we constructed an iHMM
where the DP(α, H) base measure G0 is re-
placed with its two parameter generalization
PY (d, α, H). Because the Dirichlet and Pitman-
Yor processes only differ in the way 7r is con-
structed, without loss of generality we will de-
scribe hyper-parameter choice and inference in the
context of the iHMM with Dirichlet process base
measure.
Hyperparameter Choice The description
above shows that there are 4 parameters which
we must specify: the base measure H, the
output distribution p(yt|0s ), the discount1 and
concentration2 parameters d, -y for G0 and the
concentration parameter α for the DP’s Gj. Just as
in the finite case, the base measure H is the prior
distribution on the parameter 0 of p(yt|0s ). We
chose to use a symmetric Dirichlet distribution
with parameter 6 over the word types in our
corpus. Since we do not know the sparsity level 6
of the output distributions we decided to learn this
</bodyText>
<footnote confidence="0.985427">
1for Pitman-Yor base measure
2for both Dirichlet and Pitman-Yor base measures
</footnote>
<bodyText confidence="0.999957833333333">
parameter from the data. We initially set a vague
Gamma prior over 6 but soon realized that as we
expect hidden states in the iHMM to correspond
to PoS tags, it is unrealistic to expect each state
to have the same sparsity level. Hence we chose
a Dirichlet process as the prior for 6; this way
we end up with a small discrete set of sparsity
levels: e.g. we can learn that states corresponding
to verbs and nouns share one sparsity level
while states correpsonding to determiners have
their own (much sparser) sparsity level. For the
output distribution p(yt|0s ) we chose a simple
multinomial distribution.
The hyperparameters d and -y mostly control the
number of states in the iHMM while - as we dis-
cussed above - α controls the sparsity of the tran-
sition matrix. In the experiments below we report
both fixing the two parameters and learning them
by sampling (using vague Gamma hyperpriors).
Because of computational constraints, we chose to
use vague Bayesian priors for all hyperparameters
rather than run the whole experiment over a grid of
“reasonable” parameter settings and use the best
ones according to cross validation.
</bodyText>
<sectionHeader confidence="0.999587" genericHeader="method">
3 Inference
</sectionHeader>
<bodyText confidence="0.9999122">
The Wall Street Journal part of the Penn Tree-
bank that was used for our experiments contains
about one million words. In the non-parametric
Bayesian literature not many algorithms have been
described that scale into this regime. In this sec-
tion we describe our parallel implementation of
the iHMM which can easily handle a dataset of
this scale.
There is a wealth of evidence (Scott, 2002; Gao
and Johnson, 2008) in the machine learning litera-
ture that Gibbs sampling for Markov models leads
to slow mixing times. Hence we decided our start-
ing point for inference needs to be based on dy-
namic programming. Because we didn’t have a
good idea for the number of states that we were go-
ing to end up with, we prefered the beam sampler
of Van Gael et al. (2008) over a finite truncation
of the iHMM. Moreover, the beam sampler also
introduces a certain amount of sparsity in the dy-
namic program which can speed up computations
(potentially at the cost of slower mixing).
The beam sampler is a blocked Gibbs sampler
where we alternate between sampling the param-
eters (transition matrix, output parameters), the
state sequence and the hyperparameters. Sam-
</bodyText>
<page confidence="0.969718">
681
</page>
<figure confidence="0.997512333333333">
®
H
1/4k
µk
808182
Y1y2
</figure>
<figureCaption confidence="0.999983">
Figure 1: The graphical model for the iHMM. The variable Q represents the mixture for the DP G0.
</figureCaption>
<equation confidence="0.974442">
° ¯
k= 1 ¢ ¢ ¢ oo
</equation>
<bodyText confidence="0.999966517241379">
pling the transition matrix and output distribu-
tion parameters requires computing their sufficient
statistics and sampling from a Dirichlet distribu-
tion; we refer to the beam sampling paper for de-
tails. For the hyperparameters we use standard
Gibbs sampling. We briefly sketch the resam-
pling step for the state sequence for a single se-
quence of data (sentence of words). Running stan-
dard dynamic programming is prohibitive because
the state space of the iHMM is infinitely large.
The central idea of the beam sampler is to adap-
tively truncate the state space of the iHMM and
run dynamic programming. In order to truncate
the state space, we sample an auxilary variable ut
for each word in the sequence from the distribu-
tion ut — Uniform(0, 7rst−1st) where 7r represents
the transition matrix.
Intuitively, when we sample u1:T |s1:T accord-
ing to the distribution above, the only valid sam-
ples are those for which the ut are smaller than
the transition probabilities of the state sequence
s1:T. This means that when we sample s1:T |u1:T
at a later point, it must be the case that the ut’s
are still smaller than the new transition probabil-
ities. This significantly reduces the set of valid
state sequences that we need to consider. More
specifically, Van Gael et al. (2008) show that we
can compute p(st|y1:t, u1:t) using the following
dynamic programming recursion p(st|y1:t, u1:t) =
</bodyText>
<equation confidence="0.8619625">
p(yt|st) � p(st−1|y1:t−1, u1:t−1).
st−1:ut&lt;πst−1,st
</equation>
<bodyText confidence="0.999137868421053">
The summation Est−1:ut&lt;πst−1,st ensures that this
computation remains finite. When we compute
p(st|y1:t, u1:t) for t E 11 · · · T}, we can easily
sample sT and using Bayes rule backtrack sample
every other st. It can be shown that this procedure
produces samples from the exact posterior.
Notice that the dynamic program only needs to
perform computation when ut &lt; 7rst−1,st. A care-
ful implementation of the beam sampler consists
of preprocessing the transition matrix 7r and sort-
ing its elements in descending order. We can then
iterate over the elements of the transition matrix
starting from the largest element and stop once
we reach the first element of the transition matrix
smaller than ut. In our experiments we found that
this optimization reduces the amount of computa-
tion per sentence by an order of magnitutde.
A second optimization which we introduced
is to use the map-reduce paradigm (Dean and
Ghemawat, 2004) to parallelize our computations.
More specifically, after we preprocess the transi-
tion matrix, the dynamic program computations
are independent for each sentence in the dataset.
This means we can perform each dynamic pro-
gram in parallel; in other words our “map” con-
sists of running the dynamic program on one sen-
tence in the dataset. Next, we need to resample
the transition matrix and output distribution pa-
rameters. In order to do so we need to compute
their sufficient statistics: the number of transitions
from state to state and the number of emissions of
each word out of each state. Our “reduce” func-
tion consists of computing the sufficient statistics
for each sentence and then aggregating the statis-
tics for the whole dataset. Our implementation
runs on a quad-core shared memory architecture
and we find an almost linear speedup going from
one to four cores.
</bodyText>
<sectionHeader confidence="0.999251" genericHeader="method">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.9855595">
Evaluating unsupervised PoS tagging is rather dif-
ficult mainly due to the fact that the output of such
</bodyText>
<page confidence="0.993367">
682
</page>
<bodyText confidence="0.999921416666667">
systems are not actual PoS tags but state identi-
fiers. Therefore it is impossible to evaluate per-
formance against a manually annotated gold stan-
dard using accuracy. Recent work (Goldwater and
Griffiths, 2007; Johnson, 2007; Gao and Johnson,
2008) on this task explored a variety of method-
ologies to address this issue.
The most common approach followed in pre-
vious work is to evaluate unsupervised PoS tag-
ging as clustering against a gold standard using
the Variation of Information (VI) (Meil˘a, 2007).
VI assesses homogeneity and completeness us-
ing the quantities H(C|K) (the conditional en-
tropy of the class distribution in the gold stan-
dard given the clustering) and H(K|C) (the con-
ditional entropy of clustering given the class dis-
tribution in the gold standard). However, as Gao
&amp; Johnson (2008) point out, VI is biased to-
wards clusterings with a small number of clus-
ters. A different evaluation measure that uses
the same quantities but weighs them differently is
the V-measure (Rosenberg and Hirschberg, 2007),
which is defined in Equation 2 by setting the pa-
rameter β to 1.
</bodyText>
<equation confidence="0.99691575">
H(C|K)
h = 1 −
H(C)
H(K|C)
c = 1 −
H(K)
Vβ = (1 + β)hc (2)
(βh) + c
</equation>
<bodyText confidence="0.999867535211268">
Vlachos et al. (2009) noted that V-measure favors
clusterings with a large number of clusters. Both
of these biases become crucial in our experiments,
since the number of clusters (states of the iHMM)
is not fixed in advance. Vlachos et al. proposed a
variation of the V-measure, V-beta, that adjusts the
balance between homogeneity and completeness
using the parameter β in Eq. 2.
It is worth mentioning that, unlike V-measure
and V-beta, VI scores are not normalized
and therefore they are difficult to interpret.
Meil˘a (2007) presented two normalizations,
acknowledging the potential disadvantages
they have. The first one normalizes VI by
2log(max(|K|, |C|)), which is inappropriate
when the number of clusters discovered |K|
changes between experiments. The second
normalization involves the quantity log N which
is appropriate when comparing different algo-
rithms on the same dataset (N is the number
of instances). However, this quantity depends
exclusively on the size of the dataset and hence if
the dataset is very large it can result in normalized
VI scores misleadingly close to 100%. This does
not affect rankings, i.e. a better VI score will also
be translated into a better normalized VI score. In
our experiments, we report results only with the
un-normalized VI scores, V-measure and V-beta.
All the evaluation measures mentioned so far
evaluate PoS tagging as a clustering task against
a manually annotated gold standard. While this
is reasonable, it still does not provide means of
assessing the performance in a way that would
allow comparisons with supervised methods that
output actual PoS tags. Even for the normalized
measures V-measure and V-beta, it is unclear how
their values relate to accuracy levels. Gao &amp; John-
son (2008) partially addressed this issue by map-
ping states to PoS tags following two different
strategies, cross-validation accuracy, and greedy
1-to-1 mapping, which both have shortcomings.
We argue that since an unsupervised PoS tagger is
trained without taking any gold standard into ac-
count, it is not appropriate to evaluate against a
particular gold standard, or at least this should not
be the sole criterion. The fact that different authors
use different versions of the same gold standard to
evaluate similar experiments (e.g. Goldwater &amp;
Griffiths (2007) versus Johnson (2007)) supports
this claim. Furthermore, PoS tagging is seldomly
a goal in itself, but it is a component in a linguistic
pipeline.
In order to address these issues, we perform an
extrinsic evaluation using a well-explored task that
involves PoS tags. While PoS tagging is consid-
ered a pre-processing step in many natural lan-
guage processing pipelines, the choice of task is
restricted by the lack of real PoS tags in the out-
put of our system. For our purposes we need a
task that relies on discriminating between PoS tags
rather than the PoS tag semantics themselves, in
other words, a task in which knowing whether a
word is tagged as noun instead of a verb is equiv-
alent to knowing it is tagged as state 1 instead of
state 2. Taking these considerations into account,
in Section 5 we experiment with shallow pars-
ing in the context of the CoNLL-2000 shared task
(Tjong Kim Sang and Buchholz, 2000) in which
very good performances were achieved using only
the words with their PoS tags. Our intuition is that
if the iHMM (or any unsupervised PoS tagging
</bodyText>
<page confidence="0.99856">
683
</page>
<bodyText confidence="0.999970619047619">
method) has a reasonable level of performance, it
should improve on the performance of a system
that does not use PoS tags. Moreover, if the per-
formance is very good indeed, it should get close
to the performance of a system that uses real PoS
tags, provided either by human annotation or by a
good supervised system. Similar extrinsic evalu-
ation was performed by Biemann et al. (2007). It
is of interest to compare the results between the
clustering evaluation and the extrinsic one.
A different approach in evaluating non-
parametric Bayesian models for NLP is state-
splitting (Finkel et al., 2007; Liang et al., 2007).
In this setting, the model is used in order to re-
fine existing annotation of the dataset. While this
approach can provide us with some insights and
interpretable results, the use of existing annotation
influences the output of the model. In this work,
we want to verify whether the output of the iHMM
(without any supervision) can be used instead of
that of a supervised system.
</bodyText>
<sectionHeader confidence="0.999282" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999928329113924">
In all our experiments, the Wall Street Journal
(WSJ) part of the Penn Treebank was used. As ex-
plained in Section 4, we evaluate the output of the
iHMM in two ways, as clustering with respect to a
gold standard and as direct replacement of the PoS
tags in the task of shallow parsing. In each experi-
ment, we obtain a sample from the iHMM over all
the sections of WSJ. The states for sections 15-18
and 20 of the WSJ (training and testing sets re-
spectvely in the CoNLL shared task) are used for
the evaluation based on shallow parsing, while the
remaining sections are used for evaluation against
the WSJ gold standard PoS tags using clustering
evaluation measures.
As described in Section 2 we performed three
runs with the iHMM: one run with DP prior and
fixed -y, α, one with PY prior and fixed d, -y, α and
one with DP prior but where we learn the hyper-
parameters -y, α from the data. Our inference algo-
rithm uses 1000 burn-in iterations after which we
collect a sample every 1000 iterations. Our infer-
ence procedure is annealed during the first 1000
burnin and 2400 iterations by powering the likeli-
hood of the output distribution with a number that
smoothly increases from 0.4 to 1.0 over the 3400
first iterations. The numbers of iterations reported
in the remainder of the section refer to the itera-
tions after burn-in. We initialized the sampler by:
a) sampling the hyperparameters from the prior
where applicable, b) uniformly assign each word
one out of 20 iHMM states. For the DP run with
fixed parameters, we chose α = 0.8 to encourage
some sparsity in the transition matrix and -y = 5.0
to allow for enough hidden states. For the PY run
with fixed parameters, we chose α = 0.8 for simi-
lar reasons and d = 0.1 and -y = 1.0. We point out
that one weakness of MCMC methods is that they
are hard to test for convergence. We chose to run
the simulations until they became prohibitively ex-
pensive to obtain a new sample.
First, we present results using clustering eval-
uation measures which appear in the figures of
Table 1. The three runs exhibit different behav-
ior. The number of states reached by the iHMM
with fixed parameters using the DP prior stabilizes
close to 50 states, while for the experiment with
learnt hyperparameters the number of states grows
more rapidly, reaching 194 states after 8,000 iter-
ations. With the PY prior, the number of states
reached grows less rapidly reaching 90 states. All
runs achieve better performances with respect to
all the measures used as the number of iterations
grows. An exception is that VI scores tend to in-
crease (lower VI scores are better) when the num-
ber of states grows larger than the gold standard.
It is interesting to notice how the measures exhibit
different biases, in particular that VI penalizes the
larger numbers of states discovered in the DP run
with learnt parameters as well as the run with the
PY prior, compared to the more lenient scores pro-
vided by V-measure and V-beta. The latter though
assigns lower scores to the DP run with learnt pa-
rameters because it takes into account that the high
homogeneity is achieved using even more states.
Finally, the interpretability of these scores presents
some interest. For example, in the run with fixed
parameters using the DP prior, after burn-in VI
was 4.6, which corresponds to 76.65% normalized
VI score, while V-measure and V-beta were 12.7%
and 9% respectively. In 8,000 iterations after burn-
in, VI was 3.94 (80.3% when normalized), while
V-measure and V-beta were 53.3%, since the num-
ber of states was almost the same as the number of
unique PoS tags in the gold standard.
The closest experiment to ours is the one by
Gao &amp; Johnson (2008) who run their Bayesian
HMM over the whole WSJ and evaluated against
the full gold standard, the only difference being
is that we exclude the CoNLL shared task sec-
</bodyText>
<page confidence="0.994834">
684
</page>
<figure confidence="0.999079202247191">
0 1 2 3 4 5 6 7 8
200
180
160
140
120
100
80
60
40
20
0
DP-learnt
DP-fixed
PY-fixed
0 1 2 3 4 5 6 7 8
60
55
50
45
40
35
30
25
DP-learnt
DP-fixed
PY-fixed
0 1 2 3 4 5 6 7 8
60
55
50
45
40
35
30
25
20
15
10
DP-learnt
DP-fixed
PY-fixed
70
60
50
40
30
20
10
0
DP-learnt
DP-fixed
PY-fixed
0 1 2 3 4 5 6 7 8
0 1 2 3 4 5 6 7 8
DP-learnt
DP-fixed
PY-fixed
0 1 2 3 4 5 6 7 8
5.2
5
4.8
4.6
4.4
4.2
4
3.8
3.6
DP-learnt
DP-fixed
PY-fixed
60
55
50
45
40
35
30
25
20
15
10
5
homogeneity
VI
V-beta
states
completeness
V-measure
</figure>
<tableCaption confidence="0.8946735">
Table 1: Performance of the three iHMM runs according to clustering evaluation measures against num-
ber of iteretions (in thousands).
</tableCaption>
<figure confidence="0.997695708333333">
0 1 2 3 4 5 6 7 8
0 1 2 3 4 5 6 7 8
accuracy
94.6
94.4
94.2
93.8
93.6
93.4
93.2
94
DP-learnt
DP-fixed
PY-fixed
F-score
90.5
89.5
88.5
91
90
89
DP-learnt
DP-fixed
PY-fixed
</figure>
<tableCaption confidence="0.8347395">
Table 2: Performance of the output of the three iHMM runs when used in shallow parsing against number
of iteretions (in thousands).
</tableCaption>
<bodyText confidence="0.802949166666667">
tions from our evaluation, which leaves us with 19 of states fixed to 50. The VI score achieved by the
tions from our evaluation, which leaves us with 19
of states fixed to 50. The VI score achieved by the
iHMM with fixed parameters using the PY prior
sections instead of 24. Their best VI score was iHMM with fixed parameters using the PY prior
sections instead of 24. Their best VI score was
reaches 3.73, while using the DP prior VI reaches
4.03886 which they achieved using the collapsed, reaches 3.73, while using the DP prior VI reaches
4.03886 which they achieved using the collapsed,
4.32 with learnt parameters and 3.93 with fixed
sentence-blocked Gibbs sampler with the number 4.32 with learnt parameters and 3.93 with fixed
sentence-blocked Gibbs sampler with the number
</bodyText>
<page confidence="0.993476">
685
</page>
<bodyText confidence="0.999942979591837">
parameters. These results, even if they are not
directly comparable, are on par with the state-of-
the-art, which encouraged us to proceed with the
extrinsic evaluation.
For the experiments with shallow parsing we
used the CRF++ toolkit3 which has an efficient
implementation of the model introduced by Sha &amp;
Pereira (2003) for this task. First we ran an experi-
ment using the words and the PoS tags provided in
the shared task data and the performances obtained
were 96.07% accuracy and 93.81% F-measure.
The PoS tags were produced using the Brill tag-
ger (Brill, 1994) which employs tranformation-
based learning and was trained using the WSJ cor-
pus. Then we ran an experiment removing the
PoS tags altogether, and the performances were
93.25% accuracy and 88.58% F-measure respec-
tively. This gave us some indication as to what the
contribution of the PoS tags is in the context of the
shallow parsing task at hand.
The experiments using the output of the iHMM
as PoS tags for shallow parsing are presented in
Table 2. The best performance achieved was
94.48% and 90.98% in accuracy and F-measure,
which is 1.23% and 2.4% better respectively than
just using words, but worse by 1.57% and 2.83%
compared to using the supervised PoS tagger out-
put. Given that the latter is trained on WSJ we be-
lieve that this is a good result. Interestingly, this
was obtained by using the last sample from the
iHMM run using the DP prior with learnt param-
eters which has worse overall clustering evalua-
tion scores, especially in terms of VI. This sample
though has the best homogeneity score (69.39%).
We believe that homogeneity is more important
than the overall clustering score due to the fact
that, in the application considered, it is probably
worse to assign tokens that belong to different PoS
tags to the same state, e.g. verb and adverbs, rather
than generate more than one state for the same
PoS. This is likely to be the case in tasks where
we are interested in distinguishing between PoS
tags rather than the actual tag itself. Also, clus-
tering evaluation measures tend to score leniently
consistent mixing of members of different classes
in the same cluster. However, such mixing results
in consistent noise when the clustering output be-
comes input to a machine learning method, which
is harder to deal with.
</bodyText>
<footnote confidence="0.858629">
3http://crfpp.sourceforge.net/
</footnote>
<sectionHeader confidence="0.99055" genericHeader="conclusions">
6 Conclusions - Future Work
</sectionHeader>
<bodyText confidence="0.999974446808511">
In the context of shallow parsing we saw that the
performance of the iHMM does not match the
performance of a supervised PoS tagger but does
lead to a performance increase over a model us-
ing only words as features. Given that it was con-
structed without any need for human annotation,
we believe this is a good result. At the same time
though, it suggests that it is still some way from
being a direct drop-in replacement for a supervised
method. We argue that the extrinsic evaluation of
unsupervised PoS tagging performed in this paper
is quite informative as it allowed us to assess our
results in a more realistic context. In this work we
used shallow parsing for this, but we are consider-
ing other tasks in which we hope that PoS tagging
performance will be more crucial.
Our experiments also suggest that the number of
states in a Bayesian non-parametric model can be
rather unpredictable. On one hand, this is a strong
warning towards inference algorithms which per-
form finite truncation of non-parametric models.
On the other hand, the remarkable difference in
behavior between the DP with fixed and learned
priors suggests that more research is needed to-
wards understanding the influence of hyperparam-
eters in Bayesian non-parametric models.
We are currently experimenting with a semi-
supervised PoS tagger where we let the transi-
tion matrix of the iHMM depend on annotated
PoS tags. This model allows us to: a) use an-
notations whenever they are available and do un-
supervised learning otherwise; b) use the power
of non-parametric methods to possibly learn more
fine grained statistical structure than tag sets cre-
ated manually.
On the implementation side, it would be in-
teresting to see how our methods scale in a dis-
tributed map-reduce architecture where network
communication overhead becomes an issue.
Finally, the ultimate goal of our investigation is
to do unsupervised PoS tagging using web-scale
datasets. Although the WSJ corpus is reasonably
sized, our computational methods do not currently
scale to problems with one or two order of magni-
tude more data. We will need new breakthroughs
to unleash the full potential of unsupervised learn-
ing for NLP.
</bodyText>
<page confidence="0.998623">
686
</page>
<sectionHeader confidence="0.99388" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999924608695652">
Charles E. Antoniak. 1974. Mixtures of dirichlet pro-
cesses with applications to bayesian nonparametric
problems. The Annals of Statistics, 2(6):1152–1174.
M. J. Beal, Z. Ghahramani, and C. E. Rasmussen.
2002. The infinite hidden markov model. Advances
in Neural Information Processing Systems, 14:577 –
584.
Chris Biemann, Claudio Giuliano, and Alfio Gliozzo.
2007. Unsupervised part-of-speech tagging support-
ing supervised methods. In Proceedings of RANLP.
Eric Brill. 1994. Some advances in transformation-
based part of speech tagging. In National Confer-
ence on Artificial Intelligence, pages 722–727.
Jeffrey Dean and Sanjay Ghemawat. 2004. Mapre-
duce: Simplified data processing on large clusters.
In Sixth Symposium on Operating System Design
and Implementation.
Jenny Rose Finkel, Trond Grenager, and Christo-
pher D. Manning. 2007. The infinite tree. In Pro-
ceedings of the 45th Annual Meeting of the Associ-
ation of Computational Linguistics, pages 272–279,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
J. Van Gael, Y. Saatci, Y. W. Teh, and Z. Ghahra-
mani. 2008. Beam sampling for the infinite hidden
markov model. In Proceedings of the 25th interna-
tional conference on Machine learning, volume 25,
Helsinki.
Jianfeng Gao and Mark Johnson. 2008. A compari-
son of bayesian estimators for unsupervised hidden
markov model pos taggers. In Proceedings of the
2008 Conference on Empirical Methods in Natural
Language Processing, pages 344–352.
Sharon Goldwater and Tom Griffiths. 2007. A fully
bayesian approach to unsupervised part-of-speech
tagging. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 744–751, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.
S. Goldwater, T. Griffiths, and M. Johnson. 2006. In-
terpolating between types and tokens by estimating
power-law generators. Advances in Neural Informa-
tion Processing Systems, 18.
Mark Johnson. 2007. Why Doesn’t EM Find Good
HMM POS-Taggers? In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning, pages 296–305.
P. Liang, S. Petrov, M. I. Jordan, and D. Klein. 2007.
The infinite PCFG using hierarchical Dirichlet pro-
cesses. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning.
D. J. C. MacKay. 1997. Ensemble learning for hidden
Markov models. Technical report, Cavendish Labo-
ratory, University of Cambridge, 1997.
Marina Meil˘a. 2007. Comparing clusterings—an in-
formation based distance. Journal of Multivariate
Analysis, 98(5):873–895.
Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A conditional entropy-based external clus-
ter evaluation measure. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning, pages 410–420, Prague, Czech
Republic, June.
Steven L. Scott. 2002. Bayesian methods for hidden
markov models: Recursive computing in the 21st
century. Journal of the American Statistical Asso-
ciation, 97(457):337–351, March.
Fei Sha and Fernando Pereira. 2003. Shallow pars-
ing with conditional random fields. In Human Lan-
guage Technology Conference and the 4th Meeting
of the North American Association for Computa-
tional Linguistics.
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal,
and D. M. Blei. 2006. Hierarchical Dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101(476):1566–1581.
Erik F. Tjong Kim Sang and Sabine Buchholz.
2000. Introduction to the CoNLL-2000 shared task:
Chunking. In Claire Cardie, Walter Daelemans,
Claire Nedellec, and Erik Tjong Kim Sang, editors,
Proceedings of the Fourth Conference on Computa-
tional Natural Language Learning, pages 127–132.
Lisbon, Portugal, September.
Andreas Vlachos, Anna Korhonen, and Zoubin
Ghahramani. 2009. Unsupervised and Constrained
Dirichlet Process Mixture Models for Verb Cluster-
ing. In Proceedings of the EACL workshop on GEo-
metrical Models of Natural Language Semantics.
</reference>
<page confidence="0.997862">
687
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.965752">
<title confidence="0.999671">The infinite HMM for unsupervised PoS tagging</title>
<author confidence="0.999906">Jurgen Van_Gael Andreas Vlachos Zoubin Ghahramani</author>
<affiliation confidence="0.999735">Department of Engineering Computer Laboratory Department of Engineering University of Cambridge University of Cambridge University of Cambridge</affiliation>
<email confidence="0.97249">jv249@cam.ac.ukav308@cl.cam.ac.ukzoubin@eng.cam.ac.uk</email>
<abstract confidence="0.999710909090909">We extend previous work on fully unsupervised part-of-speech tagging. Using a non-parametric version of the HMM, called the infinite HMM (iHMM), we address the problem of choosing the number of hidden states in unsupervised Markov models for PoS tagging. We experiment with two non-parametric priors, the Dirichlet and Pitman-Yor processes, on the Wall Street Journal dataset using a parallelized implementation of an iHMM inference algorithm. We evaluate the results with a variety of clustering evaluation metrics and achieve equivalent or better performances than previously reported. Building on this promising result we evaluate the output of the unsupervised PoS tagger as a direct replacement for the output of a fully supervised PoS tagger for the task of shallow parsing and compare the two evaluations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Charles E Antoniak</author>
</authors>
<title>Mixtures of dirichlet processes with applications to bayesian nonparametric problems.</title>
<date>1974</date>
<journal>The Annals of Statistics,</journal>
<volume>2</volume>
<issue>6</issue>
<contexts>
<context position="11126" citStr="Antoniak, 1974" startWordPosition="1856" endWordPosition="1857">ion yt can be written as: K p(yt|st−1 = k) = p(st|st−1 = k)p(yt|st), st=1 K πk,stp(yt|φst). (1) st=1 The variable st−1 = k specifies the mixing weights πk,· for the mixture distribution, while st indexes the mixture component generating the observation yt. In other words, equation (1) says that each row of the transition matrix π specifies a different mixture distribution over the same set of K mixture components φ. Our second attempt to define a non-parametric version of the hidden Markov model is to replace the finite mixture by an infinite mixture. The theory of Dirichlet process mixtures (Antoniak, 1974) tells us exactly how to do this. A draw G ∼ DP(α, H) from a Dirichlet process (DP) with base measure H and concentration parameter α ≥ 0 is a discrete distribution which can be written as an infinite mixture of atoms G(·) = �∞ πiδφi(·) i=1 where the φi are i.i.d. draws from the base measure H, δφi(·) represents a point distribution at �i−1 φi and πi = vi l=1(1 − vl) where each vl ∼ Beta(1, α). The distribution over πi is called a stick breaking construction and is essentially an infinite dimensional version of the Dirichlet distribution. We refer to Teh et al. (2006) for more details. Switchi</context>
</contexts>
<marker>Antoniak, 1974</marker>
<rawString>Charles E. Antoniak. 1974. Mixtures of dirichlet processes with applications to bayesian nonparametric problems. The Annals of Statistics, 2(6):1152–1174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Beal</author>
<author>Z Ghahramani</author>
<author>C E Rasmussen</author>
</authors>
<title>The infinite hidden markov model.</title>
<date>2002</date>
<booktitle>Advances in Neural Information Processing Systems, 14:577 –</booktitle>
<pages>584</pages>
<contexts>
<context position="5316" citStr="Beal et al., 2002" startWordPosition="824" endWordPosition="827"> of “best”), inference in non-parametric Bayesian methods integrates the capacity search in one algorithm. This is particularly advantageous when parameters other than capacity need to be explored, since it reduces signifcantly the number of experiments needed. None of these potential advantages are guaranteed and in this paper we investigate these two aspects for the task of unsupervised PoS tagging. The contributions in this paper extend previous work on unsupervised PoS tagging in five ways. First, we introduce the use of a non-parametric version of the HMM, namely the infinite HMM (iHMM) (Beal et al., 2002) for unsupervised PoS tagging. This answers an open problem from Goldwater &amp; Griffiths (2007). Second, we carefully implemented a parallelized version of the inference algorithms for the iHMM so we could use it on the Wall Street Journal Penn Treebank dataset. Third, we introduce a new variant of the iHMM that builds on the Pitman-Yor process. Fourth, we evaluate the results with a variety of clustering evaluation methods and achieve equivalent or better performances than previously reported. Finally, building on this promising result we use the output of the unsupervised PoS tagger as a direc</context>
<context position="6820" citStr="Beal et al., 2002" startWordPosition="1082" endWordPosition="1085">f the paper is structured as follows: in section 2 we introduce the iHMM as a nonparametric version of the Bayesian HMM used in previous work on unsupervised PoS tagging. Then, in section 3 we describe some details of our implementation of the iHMM. In section 4 we present a variety of evaluation metrics to compare our results with previous work. Finally, in section 5 we report our experimental results. We conclude this paper with a discussion of ongoing work and experiments. 2 The Infinite HMM In this section, we describe a non-parametric hidden Markov model known as the infinite HMM (iHMM) (Beal et al., 2002; Teh et al., 2006). As we show below, this model is flexible in the number of hidden states which it can accomodate. In other words, the capacity is an uncertain quantity with an a priori infinite range that is a posteriori inferred by the data. It is instructive to first review the finite HMM and its Bayesian treatment: for one, it is the model that has been used in previous work on unsupervised PoS tagging, secondly it allows us to better understand the iHMM. The Bayesian HMM A finite first-order HMM consists of a hidden state sequence s = (s1, s2, ... , sT) and a corresponding observation </context>
</contexts>
<marker>Beal, Ghahramani, Rasmussen, 2002</marker>
<rawString>M. J. Beal, Z. Ghahramani, and C. E. Rasmussen. 2002. The infinite hidden markov model. Advances in Neural Information Processing Systems, 14:577 – 584.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Biemann</author>
<author>Claudio Giuliano</author>
<author>Alfio Gliozzo</author>
</authors>
<title>Unsupervised part-of-speech tagging supporting supervised methods.</title>
<date>2007</date>
<booktitle>In Proceedings of RANLP.</booktitle>
<contexts>
<context position="25599" citStr="Biemann et al. (2007)" startWordPosition="4360" endWordPosition="4363">he context of the CoNLL-2000 shared task (Tjong Kim Sang and Buchholz, 2000) in which very good performances were achieved using only the words with their PoS tags. Our intuition is that if the iHMM (or any unsupervised PoS tagging 683 method) has a reasonable level of performance, it should improve on the performance of a system that does not use PoS tags. Moreover, if the performance is very good indeed, it should get close to the performance of a system that uses real PoS tags, provided either by human annotation or by a good supervised system. Similar extrinsic evaluation was performed by Biemann et al. (2007). It is of interest to compare the results between the clustering evaluation and the extrinsic one. A different approach in evaluating nonparametric Bayesian models for NLP is statesplitting (Finkel et al., 2007; Liang et al., 2007). In this setting, the model is used in order to refine existing annotation of the dataset. While this approach can provide us with some insights and interpretable results, the use of existing annotation influences the output of the model. In this work, we want to verify whether the output of the iHMM (without any supervision) can be used instead of that of a superv</context>
</contexts>
<marker>Biemann, Giuliano, Gliozzo, 2007</marker>
<rawString>Chris Biemann, Claudio Giuliano, and Alfio Gliozzo. 2007. Unsupervised part-of-speech tagging supporting supervised methods. In Proceedings of RANLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Some advances in transformationbased part of speech tagging.</title>
<date>1994</date>
<booktitle>In National Conference on Artificial Intelligence,</booktitle>
<pages>722--727</pages>
<contexts>
<context position="32359" citStr="Brill, 1994" startWordPosition="5599" endWordPosition="5600">nce-blocked Gibbs sampler with the number 685 parameters. These results, even if they are not directly comparable, are on par with the state-ofthe-art, which encouraged us to proceed with the extrinsic evaluation. For the experiments with shallow parsing we used the CRF++ toolkit3 which has an efficient implementation of the model introduced by Sha &amp; Pereira (2003) for this task. First we ran an experiment using the words and the PoS tags provided in the shared task data and the performances obtained were 96.07% accuracy and 93.81% F-measure. The PoS tags were produced using the Brill tagger (Brill, 1994) which employs tranformationbased learning and was trained using the WSJ corpus. Then we ran an experiment removing the PoS tags altogether, and the performances were 93.25% accuracy and 88.58% F-measure respectively. This gave us some indication as to what the contribution of the PoS tags is in the context of the shallow parsing task at hand. The experiments using the output of the iHMM as PoS tags for shallow parsing are presented in Table 2. The best performance achieved was 94.48% and 90.98% in accuracy and F-measure, which is 1.23% and 2.4% better respectively than just using words, but w</context>
</contexts>
<marker>Brill, 1994</marker>
<rawString>Eric Brill. 1994. Some advances in transformationbased part of speech tagging. In National Conference on Artificial Intelligence, pages 722–727.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Dean</author>
<author>Sanjay Ghemawat</author>
</authors>
<title>Mapreduce: Simplified data processing on large clusters.</title>
<date>2004</date>
<booktitle>In Sixth Symposium on Operating System Design and Implementation.</booktitle>
<contexts>
<context position="19690" citStr="Dean and Ghemawat, 2004" startWordPosition="3370" endWordPosition="3373"> the dynamic program only needs to perform computation when ut &lt; 7rst−1,st. A careful implementation of the beam sampler consists of preprocessing the transition matrix 7r and sorting its elements in descending order. We can then iterate over the elements of the transition matrix starting from the largest element and stop once we reach the first element of the transition matrix smaller than ut. In our experiments we found that this optimization reduces the amount of computation per sentence by an order of magnitutde. A second optimization which we introduced is to use the map-reduce paradigm (Dean and Ghemawat, 2004) to parallelize our computations. More specifically, after we preprocess the transition matrix, the dynamic program computations are independent for each sentence in the dataset. This means we can perform each dynamic program in parallel; in other words our “map” consists of running the dynamic program on one sentence in the dataset. Next, we need to resample the transition matrix and output distribution parameters. In order to do so we need to compute their sufficient statistics: the number of transitions from state to state and the number of emissions of each word out of each state. Our “red</context>
</contexts>
<marker>Dean, Ghemawat, 2004</marker>
<rawString>Jeffrey Dean and Sanjay Ghemawat. 2004. Mapreduce: Simplified data processing on large clusters. In Sixth Symposium on Operating System Design and Implementation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher D Manning</author>
</authors>
<title>The infinite tree.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>272--279</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="25810" citStr="Finkel et al., 2007" startWordPosition="4394" endWordPosition="4397">rvised PoS tagging 683 method) has a reasonable level of performance, it should improve on the performance of a system that does not use PoS tags. Moreover, if the performance is very good indeed, it should get close to the performance of a system that uses real PoS tags, provided either by human annotation or by a good supervised system. Similar extrinsic evaluation was performed by Biemann et al. (2007). It is of interest to compare the results between the clustering evaluation and the extrinsic one. A different approach in evaluating nonparametric Bayesian models for NLP is statesplitting (Finkel et al., 2007; Liang et al., 2007). In this setting, the model is used in order to refine existing annotation of the dataset. While this approach can provide us with some insights and interpretable results, the use of existing annotation influences the output of the model. In this work, we want to verify whether the output of the iHMM (without any supervision) can be used instead of that of a supervised system. 5 Experiments In all our experiments, the Wall Street Journal (WSJ) part of the Penn Treebank was used. As explained in Section 4, we evaluate the output of the iHMM in two ways, as clustering with </context>
</contexts>
<marker>Finkel, Grenager, Manning, 2007</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher D. Manning. 2007. The infinite tree. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 272–279, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Van Gael</author>
<author>Y Saatci</author>
<author>Y W Teh</author>
<author>Z Ghahramani</author>
</authors>
<title>Beam sampling for the infinite hidden markov model.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th international conference on Machine learning,</booktitle>
<volume>25</volume>
<location>Helsinki.</location>
<marker>Van Gael, Saatci, Teh, Ghahramani, 2008</marker>
<rawString>J. Van Gael, Y. Saatci, Y. W. Teh, and Z. Ghahramani. 2008. Beam sampling for the infinite hidden markov model. In Proceedings of the 25th international conference on Machine learning, volume 25, Helsinki.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Mark Johnson</author>
</authors>
<title>A comparison of bayesian estimators for unsupervised hidden markov model pos taggers.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>344--352</pages>
<contexts>
<context position="1843" citStr="Gao and Johnson, 2008" startWordPosition="272" endWordPosition="275">ing approaches. These learning methods rely on the availability of labeled datasets which are usually produced by expensive manual annotation. For some tasks, we have the choice to use unsupervised learning approaches. While they do not necessarily achieve the same level of performance, they are appealing as unlabeled data is usually abundant. In particular, for the purpose of exploring new domains and languages, obtainining labeled material can be prohibitively expensive and unsupervised learning methods are a very attractive choice. Recent work (Johnson, 2007; Goldwater and Griffiths, 2007; Gao and Johnson, 2008) explored the task of part-of-speech tagging (PoS) using unsupervised Hidden Markov Models (HMMs) with encouraging results. PoS tagging is a standard component in many linguistic processing pipelines, so any improvement on its performance is likely to impact a wide range of tasks. It is important to point out that a completely unsupervised learning method will discover the statistics of a dataset according to a particular model choice but these statistics might not correspond exactly to our intuition about PoS tags. Johnson (2007) and Gao &amp; Johnson (2008) assume that words are generated by a h</context>
<context position="16453" citStr="Gao and Johnson, 2008" startWordPosition="2818" endWordPosition="2821">e chose to use vague Bayesian priors for all hyperparameters rather than run the whole experiment over a grid of “reasonable” parameter settings and use the best ones according to cross validation. 3 Inference The Wall Street Journal part of the Penn Treebank that was used for our experiments contains about one million words. In the non-parametric Bayesian literature not many algorithms have been described that scale into this regime. In this section we describe our parallel implementation of the iHMM which can easily handle a dataset of this scale. There is a wealth of evidence (Scott, 2002; Gao and Johnson, 2008) in the machine learning literature that Gibbs sampling for Markov models leads to slow mixing times. Hence we decided our starting point for inference needs to be based on dynamic programming. Because we didn’t have a good idea for the number of states that we were going to end up with, we prefered the beam sampler of Van Gael et al. (2008) over a finite truncation of the iHMM. Moreover, the beam sampler also introduces a certain amount of sparsity in the dynamic program which can speed up computations (potentially at the cost of slower mixing). The beam sampler is a blocked Gibbs sampler whe</context>
<context position="20928" citStr="Gao and Johnson, 2008" startWordPosition="3575" endWordPosition="3578">nsists of computing the sufficient statistics for each sentence and then aggregating the statistics for the whole dataset. Our implementation runs on a quad-core shared memory architecture and we find an almost linear speedup going from one to four cores. 4 Evaluation Evaluating unsupervised PoS tagging is rather difficult mainly due to the fact that the output of such 682 systems are not actual PoS tags but state identifiers. Therefore it is impossible to evaluate performance against a manually annotated gold standard using accuracy. Recent work (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008) on this task explored a variety of methodologies to address this issue. The most common approach followed in previous work is to evaluate unsupervised PoS tagging as clustering against a gold standard using the Variation of Information (VI) (Meil˘a, 2007). VI assesses homogeneity and completeness using the quantities H(C|K) (the conditional entropy of the class distribution in the gold standard given the clustering) and H(K|C) (the conditional entropy of clustering given the class distribution in the gold standard). However, as Gao &amp; Johnson (2008) point out, VI is biased towards clusterings </context>
<context position="2404" citStr="Gao &amp; Johnson (2008)" startWordPosition="362" endWordPosition="365">2007; Goldwater and Griffiths, 2007; Gao and Johnson, 2008) explored the task of part-of-speech tagging (PoS) using unsupervised Hidden Markov Models (HMMs) with encouraging results. PoS tagging is a standard component in many linguistic processing pipelines, so any improvement on its performance is likely to impact a wide range of tasks. It is important to point out that a completely unsupervised learning method will discover the statistics of a dataset according to a particular model choice but these statistics might not correspond exactly to our intuition about PoS tags. Johnson (2007) and Gao &amp; Johnson (2008) assume that words are generated by a hidden Markov model and find that the resulting states strongly correlate with POS tags. Nonetheless, identifying the HMM states with appropriate POS tags is hard. Because many evaluation methods often require POS tags (rather than HMM states) this identification problem makes unsupervised systems difficult to evaluate. One potential solution is to add a small amount of supervision as in Goldwater &amp; Griffiths (2007) who assume a dictionary of frequent words associated with possible PoS tags extracted from a labeled corpus. Although this technique improves </context>
<context position="21483" citStr="Gao &amp; Johnson (2008)" startWordPosition="3667" endWordPosition="3670">dwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008) on this task explored a variety of methodologies to address this issue. The most common approach followed in previous work is to evaluate unsupervised PoS tagging as clustering against a gold standard using the Variation of Information (VI) (Meil˘a, 2007). VI assesses homogeneity and completeness using the quantities H(C|K) (the conditional entropy of the class distribution in the gold standard given the clustering) and H(K|C) (the conditional entropy of clustering given the class distribution in the gold standard). However, as Gao &amp; Johnson (2008) point out, VI is biased towards clusterings with a small number of clusters. A different evaluation measure that uses the same quantities but weighs them differently is the V-measure (Rosenberg and Hirschberg, 2007), which is defined in Equation 2 by setting the parameter β to 1. H(C|K) h = 1 − H(C) H(K|C) c = 1 − H(K) Vβ = (1 + β)hc (2) (βh) + c Vlachos et al. (2009) noted that V-measure favors clusterings with a large number of clusters. Both of these biases become crucial in our experiments, since the number of clusters (states of the iHMM) is not fixed in advance. Vlachos et al. proposed </context>
<context position="23584" citStr="Gao &amp; Johnson (2008)" startWordPosition="4015" endWordPosition="4019">. a better VI score will also be translated into a better normalized VI score. In our experiments, we report results only with the un-normalized VI scores, V-measure and V-beta. All the evaluation measures mentioned so far evaluate PoS tagging as a clustering task against a manually annotated gold standard. While this is reasonable, it still does not provide means of assessing the performance in a way that would allow comparisons with supervised methods that output actual PoS tags. Even for the normalized measures V-measure and V-beta, it is unclear how their values relate to accuracy levels. Gao &amp; Johnson (2008) partially addressed this issue by mapping states to PoS tags following two different strategies, cross-validation accuracy, and greedy 1-to-1 mapping, which both have shortcomings. We argue that since an unsupervised PoS tagger is trained without taking any gold standard into account, it is not appropriate to evaluate against a particular gold standard, or at least this should not be the sole criterion. The fact that different authors use different versions of the same gold standard to evaluate similar experiments (e.g. Goldwater &amp; Griffiths (2007) versus Johnson (2007)) supports this claim. </context>
<context position="29891" citStr="Gao &amp; Johnson (2008)" startWordPosition="5118" endWordPosition="5121">to account that the high homogeneity is achieved using even more states. Finally, the interpretability of these scores presents some interest. For example, in the run with fixed parameters using the DP prior, after burn-in VI was 4.6, which corresponds to 76.65% normalized VI score, while V-measure and V-beta were 12.7% and 9% respectively. In 8,000 iterations after burnin, VI was 3.94 (80.3% when normalized), while V-measure and V-beta were 53.3%, since the number of states was almost the same as the number of unique PoS tags in the gold standard. The closest experiment to ours is the one by Gao &amp; Johnson (2008) who run their Bayesian HMM over the whole WSJ and evaluated against the full gold standard, the only difference being is that we exclude the CoNLL shared task sec684 0 1 2 3 4 5 6 7 8 200 180 160 140 120 100 80 60 40 20 0 DP-learnt DP-fixed PY-fixed 0 1 2 3 4 5 6 7 8 60 55 50 45 40 35 30 25 DP-learnt DP-fixed PY-fixed 0 1 2 3 4 5 6 7 8 60 55 50 45 40 35 30 25 20 15 10 DP-learnt DP-fixed PY-fixed 70 60 50 40 30 20 10 0 DP-learnt DP-fixed PY-fixed 0 1 2 3 4 5 6 7 8 0 1 2 3 4 5 6 7 8 DP-learnt DP-fixed PY-fixed 0 1 2 3 4 5 6 7 8 5.2 5 4.8 4.6 4.4 4.2 4 3.8 3.6 DP-learnt DP-fixed PY-fixed 60 55 5</context>
</contexts>
<marker>Gao, Johnson, 2008</marker>
<rawString>Jianfeng Gao and Mark Johnson. 2008. A comparison of bayesian estimators for unsupervised hidden markov model pos taggers. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 344–352.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Tom Griffiths</author>
</authors>
<title>A fully bayesian approach to unsupervised part-of-speech tagging.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>744--751</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="1819" citStr="Goldwater and Griffiths, 2007" startWordPosition="268" endWordPosition="271"> tackled using supervised learning approaches. These learning methods rely on the availability of labeled datasets which are usually produced by expensive manual annotation. For some tasks, we have the choice to use unsupervised learning approaches. While they do not necessarily achieve the same level of performance, they are appealing as unlabeled data is usually abundant. In particular, for the purpose of exploring new domains and languages, obtainining labeled material can be prohibitively expensive and unsupervised learning methods are a very attractive choice. Recent work (Johnson, 2007; Goldwater and Griffiths, 2007; Gao and Johnson, 2008) explored the task of part-of-speech tagging (PoS) using unsupervised Hidden Markov Models (HMMs) with encouraging results. PoS tagging is a standard component in many linguistic processing pipelines, so any improvement on its performance is likely to impact a wide range of tasks. It is important to point out that a completely unsupervised learning method will discover the statistics of a dataset according to a particular model choice but these statistics might not correspond exactly to our intuition about PoS tags. Johnson (2007) and Gao &amp; Johnson (2008) assume that wo</context>
<context position="20889" citStr="Goldwater and Griffiths, 2007" startWordPosition="3569" endWordPosition="3572">rd out of each state. Our “reduce” function consists of computing the sufficient statistics for each sentence and then aggregating the statistics for the whole dataset. Our implementation runs on a quad-core shared memory architecture and we find an almost linear speedup going from one to four cores. 4 Evaluation Evaluating unsupervised PoS tagging is rather difficult mainly due to the fact that the output of such 682 systems are not actual PoS tags but state identifiers. Therefore it is impossible to evaluate performance against a manually annotated gold standard using accuracy. Recent work (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008) on this task explored a variety of methodologies to address this issue. The most common approach followed in previous work is to evaluate unsupervised PoS tagging as clustering against a gold standard using the Variation of Information (VI) (Meil˘a, 2007). VI assesses homogeneity and completeness using the quantities H(C|K) (the conditional entropy of the class distribution in the gold standard given the clustering) and H(K|C) (the conditional entropy of clustering given the class distribution in the gold standard). However, as Gao &amp; Johnson (2008) point</context>
<context position="2861" citStr="Goldwater &amp; Griffiths (2007)" startWordPosition="434" endWordPosition="437">a dataset according to a particular model choice but these statistics might not correspond exactly to our intuition about PoS tags. Johnson (2007) and Gao &amp; Johnson (2008) assume that words are generated by a hidden Markov model and find that the resulting states strongly correlate with POS tags. Nonetheless, identifying the HMM states with appropriate POS tags is hard. Because many evaluation methods often require POS tags (rather than HMM states) this identification problem makes unsupervised systems difficult to evaluate. One potential solution is to add a small amount of supervision as in Goldwater &amp; Griffiths (2007) who assume a dictionary of frequent words associated with possible PoS tags extracted from a labeled corpus. Although this technique improves performance, in this paper we explore the completely unsupervised approach. The reason for this is that better unsupervised approaches provide us with better starting points from which to explore how and where to incorporate supervision. In previous work on unsupervised PoS tagging a main question was how to set the number of hidden states appropriately. Johnson (2007) reports results for different numbers of hidden states but it is unclear how to make </context>
<context position="5409" citStr="Goldwater &amp; Griffiths (2007)" startWordPosition="838" endWordPosition="841">earch in one algorithm. This is particularly advantageous when parameters other than capacity need to be explored, since it reduces signifcantly the number of experiments needed. None of these potential advantages are guaranteed and in this paper we investigate these two aspects for the task of unsupervised PoS tagging. The contributions in this paper extend previous work on unsupervised PoS tagging in five ways. First, we introduce the use of a non-parametric version of the HMM, namely the infinite HMM (iHMM) (Beal et al., 2002) for unsupervised PoS tagging. This answers an open problem from Goldwater &amp; Griffiths (2007). Second, we carefully implemented a parallelized version of the inference algorithms for the iHMM so we could use it on the Wall Street Journal Penn Treebank dataset. Third, we introduce a new variant of the iHMM that builds on the Pitman-Yor process. Fourth, we evaluate the results with a variety of clustering evaluation methods and achieve equivalent or better performances than previously reported. Finally, building on this promising result we use the output of the unsupervised PoS tagger as a direct replacement for the output of a fully supervised PoS tagger for the task of shallow parsing</context>
<context position="24139" citStr="Goldwater &amp; Griffiths (2007)" startWordPosition="4102" endWordPosition="4105"> unclear how their values relate to accuracy levels. Gao &amp; Johnson (2008) partially addressed this issue by mapping states to PoS tags following two different strategies, cross-validation accuracy, and greedy 1-to-1 mapping, which both have shortcomings. We argue that since an unsupervised PoS tagger is trained without taking any gold standard into account, it is not appropriate to evaluate against a particular gold standard, or at least this should not be the sole criterion. The fact that different authors use different versions of the same gold standard to evaluate similar experiments (e.g. Goldwater &amp; Griffiths (2007) versus Johnson (2007)) supports this claim. Furthermore, PoS tagging is seldomly a goal in itself, but it is a component in a linguistic pipeline. In order to address these issues, we perform an extrinsic evaluation using a well-explored task that involves PoS tags. While PoS tagging is considered a pre-processing step in many natural language processing pipelines, the choice of task is restricted by the lack of real PoS tags in the output of our system. For our purposes we need a task that relies on discriminating between PoS tags rather than the PoS tag semantics themselves, in other words,</context>
</contexts>
<marker>Goldwater, Griffiths, 2007</marker>
<rawString>Sharon Goldwater and Tom Griffiths. 2007. A fully bayesian approach to unsupervised part-of-speech tagging. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 744–751, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Goldwater</author>
<author>T Griffiths</author>
<author>M Johnson</author>
</authors>
<title>Interpolating between types and tokens by estimating power-law generators.</title>
<date>2006</date>
<booktitle>Advances in Neural Information Processing Systems,</booktitle>
<volume>18</volume>
<contexts>
<context position="13220" citStr="Goldwater et al. (2006)" startWordPosition="2250" endWordPosition="2253"> intermediate distribution G0 ∼ DP(γ, H) and let Gj ∼ DP(α, G0) we enforce that the i.i.d. draws φj · are draws from a discrete distribution (since G0 680 is a draw from a Dirichlet process) and hence all Gj will share the same infinite set of atoms as chosen by G0. Figure 1 illustrates the graphical model for the iHMM. The iHMM with Pitman-Yor Prior The Dirichlet process described above defines a very specific distribution over the number of states in the iHMM. One particular generalization of the Dirichlet process that has been studied in the NLP literature before is the Pitman-Yor process. Goldwater et al. (2006) have shown that the Pitman-Yor distribution can more accurately capture power-law like distributions that frequently occur in natural language. More specifically, a draw G ∼ PY (d, α, H) from a Pitman-Yor process (PY) with base measure H, discount parameter 0 ≤ d &lt; 1 and concentration parameter α &gt; −d is a discrete distribution which can be written as an infinite mixture of atoms G(·) = �∞ 7ri6φ,(·) i=1 where the 0i are i.i.d. draws from the base measure H, 6φ,(·) represents a point distribution at �i−1 0i and 7ri = vi l=1(1 − vl) where each vl ∼ Beta(1− d, α+ld). Note the similarity to the D</context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2006</marker>
<rawString>S. Goldwater, T. Griffiths, and M. Johnson. 2006. Interpolating between types and tokens by estimating power-law generators. Advances in Neural Information Processing Systems, 18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Why Doesn’t EM Find Good HMM POS-Taggers?</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>296--305</pages>
<contexts>
<context position="1788" citStr="Johnson, 2007" startWordPosition="266" endWordPosition="267">ks are commonly tackled using supervised learning approaches. These learning methods rely on the availability of labeled datasets which are usually produced by expensive manual annotation. For some tasks, we have the choice to use unsupervised learning approaches. While they do not necessarily achieve the same level of performance, they are appealing as unlabeled data is usually abundant. In particular, for the purpose of exploring new domains and languages, obtainining labeled material can be prohibitively expensive and unsupervised learning methods are a very attractive choice. Recent work (Johnson, 2007; Goldwater and Griffiths, 2007; Gao and Johnson, 2008) explored the task of part-of-speech tagging (PoS) using unsupervised Hidden Markov Models (HMMs) with encouraging results. PoS tagging is a standard component in many linguistic processing pipelines, so any improvement on its performance is likely to impact a wide range of tasks. It is important to point out that a completely unsupervised learning method will discover the statistics of a dataset according to a particular model choice but these statistics might not correspond exactly to our intuition about PoS tags. Johnson (2007) and Gao </context>
<context position="3375" citStr="Johnson (2007)" startWordPosition="518" endWordPosition="519">e. One potential solution is to add a small amount of supervision as in Goldwater &amp; Griffiths (2007) who assume a dictionary of frequent words associated with possible PoS tags extracted from a labeled corpus. Although this technique improves performance, in this paper we explore the completely unsupervised approach. The reason for this is that better unsupervised approaches provide us with better starting points from which to explore how and where to incorporate supervision. In previous work on unsupervised PoS tagging a main question was how to set the number of hidden states appropriately. Johnson (2007) reports results for different numbers of hidden states but it is unclear how to make this choice a priori, while Goldwater &amp; Griffiths (2007) leave this question as future work. It is not uncommon in statistical machine learning to distinguish between parameters of a model and the capacity of a model. E.g. in a clustering context, the choice for the number of clusters (capacity) and the parameters of each cluster are often 678 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 678–687, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP treated differen</context>
<context position="8059" citStr="Johnson (2007)" startWordPosition="1319" endWordPosition="1320"> , yT). Each state variable st can take on a finite number of states, say 1... K. Transitions between states are governed by Markov dynamics parameterized by the transition matrix π, where πij = p(st = j|st−1 = i), while the initial state probabilities are π0i = p(s1 = i). For each state st E {1... K} there is a parameter φst which parameterizes the observation likelihood for that state: yt|st — F (φst). Given the parameters 1π0, π, φ, K} of the HMM, the joint distribution over hidden states s and observations y can be written (with s0 = 0): T p(s, y|π0, π, φ, K) = H p(st|st−1)p(yt|st) t=1 As Johnson (2007) clearly explained, training the HMM with EM leads to poor results in PoS tagging. However, we can easily treat the HMM in a fully Bayesian way (MacKay, 1997) by introducing priors on the parameters of the HMM. With no further prior knowledge, a typical prior for the transition (and initial) probabilities are symmetric Dirichlet distributions. This corresponds to our 679 belief that, a priori, each state is equally likely to transition to every other state. Also, it is commonly known that the parameter of a Dirichlet distribution controls how sparse its samples are. In other words, by making t</context>
<context position="20904" citStr="Johnson, 2007" startWordPosition="3573" endWordPosition="3574">ce” function consists of computing the sufficient statistics for each sentence and then aggregating the statistics for the whole dataset. Our implementation runs on a quad-core shared memory architecture and we find an almost linear speedup going from one to four cores. 4 Evaluation Evaluating unsupervised PoS tagging is rather difficult mainly due to the fact that the output of such 682 systems are not actual PoS tags but state identifiers. Therefore it is impossible to evaluate performance against a manually annotated gold standard using accuracy. Recent work (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008) on this task explored a variety of methodologies to address this issue. The most common approach followed in previous work is to evaluate unsupervised PoS tagging as clustering against a gold standard using the Variation of Information (VI) (Meil˘a, 2007). VI assesses homogeneity and completeness using the quantities H(C|K) (the conditional entropy of the class distribution in the gold standard given the clustering) and H(K|C) (the conditional entropy of clustering given the class distribution in the gold standard). However, as Gao &amp; Johnson (2008) point out, VI is bia</context>
<context position="24161" citStr="Johnson (2007)" startWordPosition="4107" endWordPosition="4108">accuracy levels. Gao &amp; Johnson (2008) partially addressed this issue by mapping states to PoS tags following two different strategies, cross-validation accuracy, and greedy 1-to-1 mapping, which both have shortcomings. We argue that since an unsupervised PoS tagger is trained without taking any gold standard into account, it is not appropriate to evaluate against a particular gold standard, or at least this should not be the sole criterion. The fact that different authors use different versions of the same gold standard to evaluate similar experiments (e.g. Goldwater &amp; Griffiths (2007) versus Johnson (2007)) supports this claim. Furthermore, PoS tagging is seldomly a goal in itself, but it is a component in a linguistic pipeline. In order to address these issues, we perform an extrinsic evaluation using a well-explored task that involves PoS tags. While PoS tagging is considered a pre-processing step in many natural language processing pipelines, the choice of task is restricted by the lack of real PoS tags in the output of our system. For our purposes we need a task that relies on discriminating between PoS tags rather than the PoS tag semantics themselves, in other words, a task in which knowi</context>
</contexts>
<marker>Johnson, 2007</marker>
<rawString>Mark Johnson. 2007. Why Doesn’t EM Find Good HMM POS-Taggers? In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 296–305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>S Petrov</author>
<author>M I Jordan</author>
<author>D Klein</author>
</authors>
<title>The infinite PCFG using hierarchical Dirichlet processes.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.</booktitle>
<contexts>
<context position="25831" citStr="Liang et al., 2007" startWordPosition="4398" endWordPosition="4401">3 method) has a reasonable level of performance, it should improve on the performance of a system that does not use PoS tags. Moreover, if the performance is very good indeed, it should get close to the performance of a system that uses real PoS tags, provided either by human annotation or by a good supervised system. Similar extrinsic evaluation was performed by Biemann et al. (2007). It is of interest to compare the results between the clustering evaluation and the extrinsic one. A different approach in evaluating nonparametric Bayesian models for NLP is statesplitting (Finkel et al., 2007; Liang et al., 2007). In this setting, the model is used in order to refine existing annotation of the dataset. While this approach can provide us with some insights and interpretable results, the use of existing annotation influences the output of the model. In this work, we want to verify whether the output of the iHMM (without any supervision) can be used instead of that of a supervised system. 5 Experiments In all our experiments, the Wall Street Journal (WSJ) part of the Penn Treebank was used. As explained in Section 4, we evaluate the output of the iHMM in two ways, as clustering with respect to a gold sta</context>
</contexts>
<marker>Liang, Petrov, Jordan, Klein, 2007</marker>
<rawString>P. Liang, S. Petrov, M. I. Jordan, and D. Klein. 2007. The infinite PCFG using hierarchical Dirichlet processes. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D J C MacKay</author>
</authors>
<title>Ensemble learning for hidden Markov models.</title>
<date>1997</date>
<tech>Technical report,</tech>
<institution>Cavendish Laboratory, University of Cambridge,</institution>
<contexts>
<context position="8217" citStr="MacKay, 1997" startWordPosition="1348" endWordPosition="1349">he transition matrix π, where πij = p(st = j|st−1 = i), while the initial state probabilities are π0i = p(s1 = i). For each state st E {1... K} there is a parameter φst which parameterizes the observation likelihood for that state: yt|st — F (φst). Given the parameters 1π0, π, φ, K} of the HMM, the joint distribution over hidden states s and observations y can be written (with s0 = 0): T p(s, y|π0, π, φ, K) = H p(st|st−1)p(yt|st) t=1 As Johnson (2007) clearly explained, training the HMM with EM leads to poor results in PoS tagging. However, we can easily treat the HMM in a fully Bayesian way (MacKay, 1997) by introducing priors on the parameters of the HMM. With no further prior knowledge, a typical prior for the transition (and initial) probabilities are symmetric Dirichlet distributions. This corresponds to our 679 belief that, a priori, each state is equally likely to transition to every other state. Also, it is commonly known that the parameter of a Dirichlet distribution controls how sparse its samples are. In other words, by making the hyperprior on the Dirichlet distribution for the rows of the transition matrix small, we can encode our belief that any state (corresponding to a PoS tag i</context>
</contexts>
<marker>MacKay, 1997</marker>
<rawString>D. J. C. MacKay. 1997. Ensemble learning for hidden Markov models. Technical report, Cavendish Laboratory, University of Cambridge, 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marina Meil˘a</author>
</authors>
<title>Comparing clusterings—an information based distance.</title>
<date>2007</date>
<journal>Journal of Multivariate Analysis,</journal>
<volume>98</volume>
<issue>5</issue>
<marker>Meil˘a, 2007</marker>
<rawString>Marina Meil˘a. 2007. Comparing clusterings—an information based distance. Journal of Multivariate Analysis, 98(5):873–895.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Rosenberg</author>
<author>Julia Hirschberg</author>
</authors>
<title>Vmeasure: A conditional entropy-based external cluster evaluation measure.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>410--420</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="21699" citStr="Rosenberg and Hirschberg, 2007" startWordPosition="3702" endWordPosition="3705">nsupervised PoS tagging as clustering against a gold standard using the Variation of Information (VI) (Meil˘a, 2007). VI assesses homogeneity and completeness using the quantities H(C|K) (the conditional entropy of the class distribution in the gold standard given the clustering) and H(K|C) (the conditional entropy of clustering given the class distribution in the gold standard). However, as Gao &amp; Johnson (2008) point out, VI is biased towards clusterings with a small number of clusters. A different evaluation measure that uses the same quantities but weighs them differently is the V-measure (Rosenberg and Hirschberg, 2007), which is defined in Equation 2 by setting the parameter β to 1. H(C|K) h = 1 − H(C) H(K|C) c = 1 − H(K) Vβ = (1 + β)hc (2) (βh) + c Vlachos et al. (2009) noted that V-measure favors clusterings with a large number of clusters. Both of these biases become crucial in our experiments, since the number of clusters (states of the iHMM) is not fixed in advance. Vlachos et al. proposed a variation of the V-measure, V-beta, that adjusts the balance between homogeneity and completeness using the parameter β in Eq. 2. It is worth mentioning that, unlike V-measure and V-beta, VI scores are not normaliz</context>
</contexts>
<marker>Rosenberg, Hirschberg, 2007</marker>
<rawString>Andrew Rosenberg and Julia Hirschberg. 2007. Vmeasure: A conditional entropy-based external cluster evaluation measure. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 410–420, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven L Scott</author>
</authors>
<title>Bayesian methods for hidden markov models: Recursive computing in the 21st century.</title>
<date>2002</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>97</volume>
<issue>457</issue>
<contexts>
<context position="16429" citStr="Scott, 2002" startWordPosition="2816" endWordPosition="2817">onstraints, we chose to use vague Bayesian priors for all hyperparameters rather than run the whole experiment over a grid of “reasonable” parameter settings and use the best ones according to cross validation. 3 Inference The Wall Street Journal part of the Penn Treebank that was used for our experiments contains about one million words. In the non-parametric Bayesian literature not many algorithms have been described that scale into this regime. In this section we describe our parallel implementation of the iHMM which can easily handle a dataset of this scale. There is a wealth of evidence (Scott, 2002; Gao and Johnson, 2008) in the machine learning literature that Gibbs sampling for Markov models leads to slow mixing times. Hence we decided our starting point for inference needs to be based on dynamic programming. Because we didn’t have a good idea for the number of states that we were going to end up with, we prefered the beam sampler of Van Gael et al. (2008) over a finite truncation of the iHMM. Moreover, the beam sampler also introduces a certain amount of sparsity in the dynamic program which can speed up computations (potentially at the cost of slower mixing). The beam sampler is a b</context>
</contexts>
<marker>Scott, 2002</marker>
<rawString>Steven L. Scott. 2002. Bayesian methods for hidden markov models: Recursive computing in the 21st century. Journal of the American Statistical Association, 97(457):337–351, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Sha</author>
<author>Fernando Pereira</author>
</authors>
<title>Shallow parsing with conditional random fields.</title>
<date>2003</date>
<booktitle>In Human Language Technology Conference and the 4th Meeting of the North American Association for Computational Linguistics.</booktitle>
<contexts>
<context position="32114" citStr="Sha &amp; Pereira (2003)" startWordPosition="5553" endWordPosition="5556">psed, reaches 3.73, while using the DP prior VI reaches 4.03886 which they achieved using the collapsed, 4.32 with learnt parameters and 3.93 with fixed sentence-blocked Gibbs sampler with the number 4.32 with learnt parameters and 3.93 with fixed sentence-blocked Gibbs sampler with the number 685 parameters. These results, even if they are not directly comparable, are on par with the state-ofthe-art, which encouraged us to proceed with the extrinsic evaluation. For the experiments with shallow parsing we used the CRF++ toolkit3 which has an efficient implementation of the model introduced by Sha &amp; Pereira (2003) for this task. First we ran an experiment using the words and the PoS tags provided in the shared task data and the performances obtained were 96.07% accuracy and 93.81% F-measure. The PoS tags were produced using the Brill tagger (Brill, 1994) which employs tranformationbased learning and was trained using the WSJ corpus. Then we ran an experiment removing the PoS tags altogether, and the performances were 93.25% accuracy and 88.58% F-measure respectively. This gave us some indication as to what the contribution of the PoS tags is in the context of the shallow parsing task at hand. The exper</context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>Fei Sha and Fernando Pereira. 2003. Shallow parsing with conditional random fields. In Human Language Technology Conference and the 4th Meeting of the North American Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
<author>Michael I Jordan</author>
<author>Matthew J Beal</author>
<author>D M Blei</author>
</authors>
<title>Hierarchical Dirichlet processes.</title>
<date>2006</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>101</volume>
<issue>476</issue>
<contexts>
<context position="6839" citStr="Teh et al., 2006" startWordPosition="1086" endWordPosition="1089">ctured as follows: in section 2 we introduce the iHMM as a nonparametric version of the Bayesian HMM used in previous work on unsupervised PoS tagging. Then, in section 3 we describe some details of our implementation of the iHMM. In section 4 we present a variety of evaluation metrics to compare our results with previous work. Finally, in section 5 we report our experimental results. We conclude this paper with a discussion of ongoing work and experiments. 2 The Infinite HMM In this section, we describe a non-parametric hidden Markov model known as the infinite HMM (iHMM) (Beal et al., 2002; Teh et al., 2006). As we show below, this model is flexible in the number of hidden states which it can accomodate. In other words, the capacity is an uncertain quantity with an a priori infinite range that is a posteriori inferred by the data. It is instructive to first review the finite HMM and its Bayesian treatment: for one, it is the model that has been used in previous work on unsupervised PoS tagging, secondly it allows us to better understand the iHMM. The Bayesian HMM A finite first-order HMM consists of a hidden state sequence s = (s1, s2, ... , sT) and a corresponding observation sequence y = (y1, y</context>
<context position="11700" citStr="Teh et al. (2006)" startWordPosition="1964" endWordPosition="1967">Dirichlet process mixtures (Antoniak, 1974) tells us exactly how to do this. A draw G ∼ DP(α, H) from a Dirichlet process (DP) with base measure H and concentration parameter α ≥ 0 is a discrete distribution which can be written as an infinite mixture of atoms G(·) = �∞ πiδφi(·) i=1 where the φi are i.i.d. draws from the base measure H, δφi(·) represents a point distribution at �i−1 φi and πi = vi l=1(1 − vl) where each vl ∼ Beta(1, α). The distribution over πi is called a stick breaking construction and is essentially an infinite dimensional version of the Dirichlet distribution. We refer to Teh et al. (2006) for more details. Switching back to the iHMM our next step is to introduce a DP Gj for each state j ∈ {1 · · · ∞}; we write Gj(·) = E∞i=1 πji δφj i (·). There is now a parameter for each state j and each index i ∈ {1, 2, · · · , ∞}. Next, we draw the datapoint at timestep t given that the previous datapoint was in state st−1 by drawing from DP Gst−1. We first select a mixture component st from the vector πst−1,· and then sample a datapoint yt ∼ F(φst−1,st) so we get the following distribution for yt p(yt|α, st−1) = �∞ πst−1,stp(yt|φst−1,st). st=1 This is almost the non-parametric equivalent o</context>
</contexts>
<marker>Teh, Jordan, Beal, Blei, 2006</marker>
<rawString>Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and D. M. Blei. 2006. Hierarchical Dirichlet processes. Journal of the American Statistical Association, 101(476):1566–1581.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F Tjong Kim Sang</author>
<author>Sabine Buchholz</author>
</authors>
<title>Introduction to the CoNLL-2000 shared task: Chunking.</title>
<date>2000</date>
<booktitle>Proceedings of the Fourth Conference on Computational Natural Language Learning,</booktitle>
<pages>127--132</pages>
<editor>In Claire Cardie, Walter Daelemans, Claire Nedellec, and Erik Tjong Kim Sang, editors,</editor>
<location>Lisbon, Portugal,</location>
<contexts>
<context position="25054" citStr="Sang and Buchholz, 2000" startWordPosition="4264" endWordPosition="4267">nsidered a pre-processing step in many natural language processing pipelines, the choice of task is restricted by the lack of real PoS tags in the output of our system. For our purposes we need a task that relies on discriminating between PoS tags rather than the PoS tag semantics themselves, in other words, a task in which knowing whether a word is tagged as noun instead of a verb is equivalent to knowing it is tagged as state 1 instead of state 2. Taking these considerations into account, in Section 5 we experiment with shallow parsing in the context of the CoNLL-2000 shared task (Tjong Kim Sang and Buchholz, 2000) in which very good performances were achieved using only the words with their PoS tags. Our intuition is that if the iHMM (or any unsupervised PoS tagging 683 method) has a reasonable level of performance, it should improve on the performance of a system that does not use PoS tags. Moreover, if the performance is very good indeed, it should get close to the performance of a system that uses real PoS tags, provided either by human annotation or by a good supervised system. Similar extrinsic evaluation was performed by Biemann et al. (2007). It is of interest to compare the results between the </context>
</contexts>
<marker>Sang, Buchholz, 2000</marker>
<rawString>Erik F. Tjong Kim Sang and Sabine Buchholz. 2000. Introduction to the CoNLL-2000 shared task: Chunking. In Claire Cardie, Walter Daelemans, Claire Nedellec, and Erik Tjong Kim Sang, editors, Proceedings of the Fourth Conference on Computational Natural Language Learning, pages 127–132. Lisbon, Portugal, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Vlachos</author>
<author>Anna Korhonen</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>Unsupervised and Constrained Dirichlet Process Mixture Models for Verb Clustering.</title>
<date>2009</date>
<booktitle>In Proceedings of the EACL workshop on GEometrical Models of Natural Language Semantics.</booktitle>
<contexts>
<context position="21854" citStr="Vlachos et al. (2009)" startWordPosition="3741" endWordPosition="3744">the quantities H(C|K) (the conditional entropy of the class distribution in the gold standard given the clustering) and H(K|C) (the conditional entropy of clustering given the class distribution in the gold standard). However, as Gao &amp; Johnson (2008) point out, VI is biased towards clusterings with a small number of clusters. A different evaluation measure that uses the same quantities but weighs them differently is the V-measure (Rosenberg and Hirschberg, 2007), which is defined in Equation 2 by setting the parameter β to 1. H(C|K) h = 1 − H(C) H(K|C) c = 1 − H(K) Vβ = (1 + β)hc (2) (βh) + c Vlachos et al. (2009) noted that V-measure favors clusterings with a large number of clusters. Both of these biases become crucial in our experiments, since the number of clusters (states of the iHMM) is not fixed in advance. Vlachos et al. proposed a variation of the V-measure, V-beta, that adjusts the balance between homogeneity and completeness using the parameter β in Eq. 2. It is worth mentioning that, unlike V-measure and V-beta, VI scores are not normalized and therefore they are difficult to interpret. Meil˘a (2007) presented two normalizations, acknowledging the potential disadvantages they have. The firs</context>
</contexts>
<marker>Vlachos, Korhonen, Ghahramani, 2009</marker>
<rawString>Andreas Vlachos, Anna Korhonen, and Zoubin Ghahramani. 2009. Unsupervised and Constrained Dirichlet Process Mixture Models for Verb Clustering. In Proceedings of the EACL workshop on GEometrical Models of Natural Language Semantics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>