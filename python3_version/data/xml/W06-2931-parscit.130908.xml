<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.051193">
<title confidence="0.976018">
Dependency Parsing Based on Dynamic Local Optimization
</title>
<author confidence="0.979417">
Ting Liu Jinshan Ma Huijia Zhu Sheng Li
</author>
<affiliation confidence="0.9587045">
Information Retrieval Lab
Harbin Institute of Technology
</affiliation>
<address confidence="0.881872">
Harbin, 150001, China
</address>
<email confidence="0.999032">
{tliu,mjs,hjzhu,ls}@ir.hit.edu.cn
</email>
<sectionHeader confidence="0.995643" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.996370272727273">
This paper presents a deterministic pars-
ing algorithm for projective dependency
grammar. In a bottom-up way the al-
gorithm finds the local optimum dynam-
ically. A constraint procedure is made
to use more structure information. The
algorithm parses sentences in linear time
and labeling is integrated with the parsing.
This parser achieves 63.29% labeled at-
tachment score on the average in CoNLL-
X Shared Task.
</bodyText>
<sectionHeader confidence="0.998948" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999952276595745">
Recently, dependency grammar has gained renewed
attention in the parsing community. Good results
have been achieved in some dependency parsers
(Yamada and Matsumoto, 2003; Nivre et al., 2004).
With the availability of many dependency treebanks
(van der Beek et al., 2002; Hajiˇc et al., 2004;
B¨ohmov´a et al., 2003; Kromann, 2003; Dˇzeroski et
al., 2006) and more other treebanks which can be
converted to dependency annotation (Brants et al.,
2002; Nilsson et al., 2005; Chen et al., 2003; Kawata
and Bartels, 2000), multi-lingual dependency pars-
ing is proposed in CoNLL shared task (Buchholz et
al., 2006).
Many previous works focus on unlabeled parsing,
in which exhaustive methods are often used (Eis-
ner, 1996). Their global searching performs well
in the unlabeled dependency parsing. But with the
increase of parameters, efficiency has to be consid-
ered in labeled dependency parsing. Thus determin-
istic parsing was proposed as a robust and efficient
method in recent years. Such method breaks the
construction of dependency tree into a series of ac-
tions. A classifier is often used to choose the most
probable action to assemble the dependency tree.
(Yamada and Matsumoto, 2003) defined three ac-
tions and used a SVM classifier to choose one of
them in a bottom-up way. The algorithm in (Nivre
et al., 2004) is a blend of bottom-up and top-down
processing. Its classifier is trained by memory-based
learning.
Deterministic parsing derives an analysis without
redundancy or backtracking, and linear time can be
achieved. But when searching the local optimum in
the order of left-to-right, some wrong reduce may
prevent next analysis with more possibility. (Jin et
al., 2005) used a two-phase shift-reduce to decrease
such errors, and improved the accuracy of long dis-
tance dependencies.
In this paper a deterministic parsing based on dy-
namic local optimization is proposed. According to
the probabilities of dependency arcs, the algorithm
dynamically finds the one with the highest probabil-
ities instead of dealing with the sentence in order.
A procedure of constraint which can integrate more
structure information is made to check the rational-
ity of the reduce. Finally our results and error anal-
ysis are presented.
</bodyText>
<sectionHeader confidence="0.993258" genericHeader="method">
2 Dependency Probabilities
</sectionHeader>
<bodyText confidence="0.998404666666667">
An example of Chinese dependency tree is showed
in Figure1. The tree can be represented as a directed
graph with nodes representing word tokens and arcs
</bodyText>
<page confidence="0.981028">
211
</page>
<note confidence="0.677219">
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 211–215, New York City, June 2006. c�2006 Association for Computational Linguistics
</note>
<figureCaption confidence="0.99966">
Figure 1: A Chinese dependency tree
</figureCaption>
<bodyText confidence="0.999839181818182">
representing dependency relations. The assumption
that the arcs are independent on each other often is
made so that parsing can be handled easily. On the
other side the independence assumption will result
in the loss of information because dependencies are
interrelated on each other actually. Therefore, two
kinds of probabilities are used in our parser. One is
arc probabilities which are the possibility that two
nodes form an arc, and the other is structure proba-
bilities which are used to describe some specific syn-
tactic structures.
</bodyText>
<subsectionHeader confidence="0.989292">
2.1 Arc Probabilities
</subsectionHeader>
<bodyText confidence="0.999907875">
A dependency arc Ai can be expressed as a 4-tuple
Ai = &lt;Nodei, Nodej, D, R&gt;. Nodei and Nodej are
nodes that constitute the directed arc. D is the direc-
tion of the arc, which can be left or right. R is rela-
tion type labeled on the arc. Under the independence
assumption that an arc depends on its two nodes we
can calculate arc probability given two nodes. In our
paper the arc probabilities are calculated as follows:
</bodyText>
<equation confidence="0.999928666666667">
P1 = P(R,D|CTagi, CTagj, Dist)
P2 = P(R,D|FTagi, FTagj)
P3 = P(R,D|CTagi, Wordj)
P4 = P(R,D|Wordi, CTagj)
P5 = P(R,D|Wordi,CTagi, Wordj,CTagj)
P6 = P(R,D|CTagi−1, CTagi, CTagj, CTagj+1)
</equation>
<bodyText confidence="0.99898975">
Where CTag is coarse-grained part of speech tag
and FTag is fine-grained tag. As to Word we choose
its lemma if it exists. Dist is the distance between
Nodei and Nodej. It is divided into four parts:
</bodyText>
<construct confidence="0.98801325">
Dist = 1 if j-i = 1
Dist = 2 if j-i = 2
Dist = 3 if 3j-i6
Dist = 4 if j-i &gt; 6
</construct>
<bodyText confidence="0.99871625">
All the probabilities are obtained by maximum
likelihood estimation from the training data. Then
interpolation smoothing is made to get the final arc
probabilities.
</bodyText>
<subsectionHeader confidence="0.999037">
2.2 Structure Probabilities
</subsectionHeader>
<bodyText confidence="0.999978625">
Structure information plays the critical role in syn-
tactic analysis. Nevertheless the flexibility of syn-
tactic structures and data sparseness pose obstacles
to us. Especially some structures are related to spe-
cific language and cannot be employed in multi-
lingual parsing. We have to find those language-
independent features.
In valency theory “valence” represents the num-
ber of arguments that a verb is able to govern. In
this paper we extend the range of verbs and argu-
ments to all the words. We call the new “valence”
Governing Degree (GD), which means the ability of
one node governing other nodes. In Figure1, the GD
of node “ᅚ⇀” is 2 and the GDs of two other nodes
are 0. The governing degree of nodes in dependency
tree often shows directionality. For example, Chi-
nese token “֥” always governs one left node. Fur-
thermore, we subdivide the GD into Left Governing
Degree (LGD) and Right Governing Degree (RGD),
which are the ability of words governing their left
children or right children. In Figure 1 the LGD and
RGD of verb “ᅚ⇀” are both 1.
In the paper we use the probabilities of GD
over the fine-grained tags. The probabilities of
P(LDG|FTag) and P(RGD|FTag) are calculated
from training data. Then we only reserve the FTags
with large probability because their GDs are stable
and helpful to syntactic analysis. Other FTags with
small probabilities are unstable in GDs and cannot
provide efficient information for syntactic analysis.
If their probabilities are less than 0.65 they will be
ignored in our dependency parsing.
</bodyText>
<sectionHeader confidence="0.983892" genericHeader="method">
3 Dynamic local optimization
</sectionHeader>
<bodyText confidence="0.99982075">
Many previous methods are based on history-based
models. Despite many obvious advantages, these
methods can be awkward to encode some constrains
within their framework (Collins, 2000). Classifiers
are good at encoding more features in the determin-
istic parsing (Yamada and Matsumoto, 2003; Nivre
et al., 2004). However, such algorithm often make
more probable dependencies be prevented by pre-
ceding errors. An example is showed in Figure 2.
Arc a is a frequent dependency and b is an arc with
more probability. Arc b will be prevented by a if the
reduce is carried out in order.
</bodyText>
<page confidence="0.995745">
212
</page>
<figureCaption confidence="0.997673">
Figure 2: A common error in deterministic parsing
</figureCaption>
<subsectionHeader confidence="0.991092">
3.1 Our algorithm
</subsectionHeader>
<bodyText confidence="0.999622">
Our deterministic parsing is based on dynamic local
optimization. The algorithm calculates the arc prob-
abilities of two continuous nodes, and then reduces
the most probable arc. The construction of depen-
dency tree includes four actions: Check, Reduce,
Delete, and Insert. Before a node is reduced, the
Check procedure is made to validate its correctness.
Only if the arc passes the Check procedure it can
be reduced. Otherwise the Reduce will be delayed.
Delete and Insert are then carried out to adjust the
changed arcs. The complete algorithm is depicted
as follows:
</bodyText>
<equation confidence="0.963888823529412">
Input Sentence: S = (w1, w2, , wn)
Initialize:
for i = 1 to n
Ri = GetArcProb(wi,wi+1);
Push(Ri) onto Stack;
Sort(Stack);
Start:
i = 0;
While Stack.empty = false
R = Stack.top+i;
if Check(R) = true
Reduce(R);
Delete(R’);
Insert(R”);
i = 0;
else
i++;
</equation>
<bodyText confidence="0.97052">
The algorithm has following advantages:
</bodyText>
<listItem confidence="0.99983325">
• Projectivity can be guaranteed. The node is
only reduced with its neighboring node. If a
node is reduced as a leaf it will be removed
from the sentence and doesn’t take part in next
Reduce. So no cross arc will occur.
• After n-1 pass a projective dependency tree is
complete. Algorithm is finished in linear time.
• The algorithm always reduces the node with the
</listItem>
<figureCaption confidence="0.997406">
Figure 3: Adjustment
</figureCaption>
<bodyText confidence="0.905903333333333">
highest probability if it passes the Check. No
any limitation on order thus the spread of errors
can be mitigated effectively.
</bodyText>
<listItem confidence="0.6334265">
• Check is an open process. Various constrains
can be encoded in this process. Structural con-
strains, partial parsed information or language-
dependent knowledge can be added.
</listItem>
<bodyText confidence="0.9973135">
Adjustment is illustrated in Figure 3, where “Ӕ
∁” is reduced and arc R’ is deleted. Then the algo-
rithm computes the arc probability of R” and inserts
it to the Stack.
</bodyText>
<subsectionHeader confidence="0.99808">
3.2 Checking
</subsectionHeader>
<bodyText confidence="0.9999573">
The information in parsing falls into two kinds:
static and dynamic. The arc probabilities in 2.1 de-
scribe the static information which is not changed in
parsing. They are obtained from the training data in
advance. The structure probabilities in 2.2 describe
the dynamic information which varies in the process
of parsing. The use of dynamic information often
depends on what current dependency tree is.
Besides the governing degree, Check procedure
also uses another dynamic information–Sequential
Dependency. Whether current arc can be reduced is
relating to previous arc. In Figure 3 the reduce of the
arc R depends on the arc R’. If R’ has been delayed
or its probability is little less than that of R, arc R
will be delayed.
If the arc doesn’t pass the Check it will be de-
layed. The delayed time ranges from 1 to Length
which is the length of sentence. If the arc is delayed
Length times it will be blocked. The Reduce will be
delayed in the following cases:
</bodyText>
<listItem confidence="0.99762825">
• GD(Nodei) &gt; 0 and its probability is P. If
GD(Nodei) = 0 and Nodei is made as child
in the Reduce, the Nodei will be delayed
Length*P times.
• �GD(Nodei)  m (m &gt; 0) and its probability
is P. If GD(Nodei) = m and Nodei is made as
parent in the Reduce, the Nodei will be delayed
Length*P times.
</listItem>
<page confidence="0.999466">
213
</page>
<figureCaption confidence="0.999644">
Figure 4: Token score with size of training data
Figure 5: Token score with sentence length
</figureCaption>
<listItem confidence="0.99989225">
• P(R’) &gt; AP(R), the current arc R will be de-
layed Length*(P(R’)/P(R)) times. R’ is the pre-
ceding arc and A = 0.60.
• If arc R’ is blocking, the arc R will be delayed.
</listItem>
<bodyText confidence="0.534925">
GD is empirical value and GD is current value.
</bodyText>
<sectionHeader confidence="0.752908" genericHeader="evaluation">
4 Experiments and analysis
</sectionHeader>
<bodyText confidence="0.9998382">
Our parsing results and average results are listed
in the Table 1. It can be seen that the attachment
scores vary greatly with different languages. A gen-
eral analysis and a specific analysis are made respec-
tively in this section.
</bodyText>
<subsectionHeader confidence="0.999783">
4.1 General analysis
</subsectionHeader>
<bodyText confidence="0.999937702702703">
We try to find the properties that make the differ-
ence to parsing results in multi-lingual parsing. The
properties of all the training data can be found in
(Buchholz et al., 2006). Intuitively the size of train-
ing data and average length of per sentence would
be influential on dependency parsing. The relation
of these properties and scores are showed in the Fig-
ure 4 and 5.
From the charts we cannot assuredly find the
properties that are proportional to score. Whether
Czech language with the largest size of training data
or Chinese with the shortest sentence length, don’t
achieve the best results. It seems that no any factor is
determining to parsing results but all the properties
exert influence on the dependency parsing together.
Another factor that maybe explain the difference
of scores in multi-lingual parsing is the characteris-
tics of language. For example, the number of tokens
with HEAD=0 in a sentence is not one for some lan-
guages. Table 1 shows the range of governing de-
gree of head. This statistics is somewhat different
with that from organizers because we don’t distin-
guish the scoring tokens and non-scoring tokens.
Another characteristic is the directionality of de-
pendency relations. As Table 1 showed, many rela-
tions in treebanks are bi-directional, which increases
the number of the relation actually. Furthermore, the
flexibility of some grammatical structures poses dif-
ficulties to language model. For instance, subject
can appear in both sides of the predicates in some
treebanks which tends to cause the confusion with
the object (Kromann, 2003; Afonso et al., 2002;
Civit Torruella and MartiAntonin, 2002; Oflazer et
al., 2003; Atalay et al., 2003).
As to our parsing results, which are lower than all
the average results except for Danish. That can be
explained from the following aspects:
</bodyText>
<listItem confidence="0.987656666666667">
(1) Our parser uses a projective parsing algorithm
and cannot deal with the non-projective tokens,
which exist in all the languages except for Chinese.
</listItem>
<bodyText confidence="0.994040857142857">
(2) The information provided by training data is not
fully employed. Only POS and lemma are used. The
morphological and syntactic features may be helpful
to parsing.
(3) We haven’t explored syntactic structures in depth
for multi-lingual parsing and more structural fea-
tures need to be used in the Check procedure.
</bodyText>
<subsectionHeader confidence="0.998951">
4.2 Specific analysis
</subsectionHeader>
<bodyText confidence="0.999980583333333">
Specifically we make error analysis to Chinese and
Turkish. In Chinese result we found many errors
occurred near the auxiliary word “֥”(DE). We call
the noun phrases with “֥” DE Structure. The word
“֥” appears 355 times in the all 4970 dependencies
of the test data. In Table 2 the second row shows the
frequencies of “DE” as the parent of dependencies.
The third row shows the frequencies while it is as
child. Its error rate is 33.1% and 43.4% in our re-
sults respectively. Furthermore, each head error will
result in more than one errors, so the errors from DE
Structures are nearly 9% in our results.
</bodyText>
<page confidence="0.994991">
214
</page>
<table confidence="0.991688">
Ar Ch Cz Da Du Ge Ja Po Sl Sp Sw Tu
our 50.74 75.29 58.52 77.70 59.36 68.11 70.84 71.13 57.21 65.08 63.83 41.72
ave 59.94 78.32 67.17 76.16 70.73 78.58 85.86 80.63 65.16 73.52 76.44 55.95
NH 17 1 28 4 9 1 14 1 11 1 1 5
BD 27/24 78/55 82/72 54/24 26/17 46/40 7/2 55/40 26/23 21/19 64/54 26/23
</table>
<tableCaption confidence="0.997157">
Table 1: The second and third rows are our scores and average scores. The fourth row lists the maximal
number of tokens with HEAD=0 in a sentence. The last row lists the number of relations/the number of
bi-directional relations of them (Our statistics are slightly different from that of organizers).
</tableCaption>
<table confidence="0.998614666666667">
gold system error headerr
parent 320 354 106 106
child 355 355 154 74
</table>
<tableCaption confidence="0.982199">
Table 2: Chinese DE Structure Errors
</tableCaption>
<table confidence="0.998762666666667">
total obj sub mod D.A L.A
Noun-V 1300 494 319 156 102 78
Pron-V 215 91 60 9 37 3
</table>
<tableCaption confidence="0.999921">
Table 3: The distribution of some common relations
</tableCaption>
<bodyText confidence="0.999963851851852">
The high error rate is due to the flexibility of DE
Structure. The children of DE can be nouns and
verbs, thus the ambiguities will occur. For example,
the sequence “V N1 DE N2” is a common ambigu-
ious structure in Chinese. It needs to be solved with
semantic knowledge to some extent. The errors of
DE being child are mostly from noun compounds.
For example, the string “A9 n �as ” results
in the error: “DE” as the child of “11�as”. It will be
better that noun compounds are processed specially.
Our results and average results achieve the low-
est score on Turkish. We try to find some reasons
through the following analysis. Turkish is a typi-
cal head-final language and 81.1% of dependencies
are right-headed. The monotone of directionality in-
creases the difficulties of identification. Another dif-
ficulty is the diversity of the same pair. Taking noun
and pronoun as example, which only achieve the ac-
curacy of 25% and 28% in our results, there are 14
relations in the noun-verb pairs and 11 relations in
the pronoun-verb pairs. Table 3 illustrates the distri-
bution of some common relations in the test data.
The similarity of these dependencies makes our
parser only recognize 23.3% noun-verb structures
and 21.8% pronoun-verb structures. The syntactic
or semantic knowledge maybe helpful to distinguish
these similar structures.
</bodyText>
<sectionHeader confidence="0.999461" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.994909916666667">
This paper has applied a deterministic algorithm
based on dynamic local optimization to multi-
lingual dependency parsing. Through the error
analysis for some languages, we think that the dif-
ference between languages is a main obstacle posed
on multi-lingual dependency parsing. Adopting
different learners according to the type of languages
may be helpful to multi-lingual dependency parsing.
Acknowledgement This work was supported
by the National Natural Science Foundation of
China under Grant No. 60435020,60575042 and
60503072.
</bodyText>
<sectionHeader confidence="0.997766" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997850111111111">
M. Collins. 2000. Discriminative reranking for natural
language parsing. In Proc. ofICML.
M.X. Jin, M.Y. Kim, and J.H. Lee. 2005. Two-phase
shift-reduce deterministic dependency parser of chi-
nese. In Proc. ofIJCNLP: Companion Volume includ-
ing Posters/Demos and tutorial abstracts.
J. Nivre, J. Hall, and J. Nilsson. 2004. Memory-based
dependency parsing. In Proc. of the Eighth Conf. on
Computational Natural Language Learning (CoNLL),
pages 49–56.
J. Eisner. 1996. Three new probabilistic models for
dependency parsing: An exploration. In Proc. of
the 16th Intern. Conf. on Computational Linguistics
(COLING), pages 340–345.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Proc.
of the 8th Intern. Workshop on Parsing Technologies
(IWPT).
</reference>
<page confidence="0.999123">
215
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.785153">
<title confidence="0.999629">Dependency Parsing Based on Dynamic Local Optimization</title>
<author confidence="0.986859">Ting Liu Jinshan Ma Huijia Zhu Sheng</author>
<affiliation confidence="0.9923625">Information Retrieval Harbin Institute of</affiliation>
<address confidence="0.975584">Harbin, 150001,</address>
<abstract confidence="0.999917444444444">This paper presents a deterministic parsing algorithm for projective dependency grammar. In a bottom-up way the algorithm finds the local optimum dynamically. A constraint procedure is made to use more structure information. The algorithm parses sentences in linear time and labeling is integrated with the parsing.</abstract>
<note confidence="0.874023333333333">This parser achieves 63.29% labeled attachment score on the average in CoNLL- X Shared Task.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2000</date>
<booktitle>In Proc. ofICML.</booktitle>
<contexts>
<context position="6644" citStr="Collins, 2000" startWordPosition="1077" endWordPosition="1078">ities of P(LDG|FTag) and P(RGD|FTag) are calculated from training data. Then we only reserve the FTags with large probability because their GDs are stable and helpful to syntactic analysis. Other FTags with small probabilities are unstable in GDs and cannot provide efficient information for syntactic analysis. If their probabilities are less than 0.65 they will be ignored in our dependency parsing. 3 Dynamic local optimization Many previous methods are based on history-based models. Despite many obvious advantages, these methods can be awkward to encode some constrains within their framework (Collins, 2000). Classifiers are good at encoding more features in the deterministic parsing (Yamada and Matsumoto, 2003; Nivre et al., 2004). However, such algorithm often make more probable dependencies be prevented by preceding errors. An example is showed in Figure 2. Arc a is a frequent dependency and b is an arc with more probability. Arc b will be prevented by a if the reduce is carried out in order. 212 Figure 2: A common error in deterministic parsing 3.1 Our algorithm Our deterministic parsing is based on dynamic local optimization. The algorithm calculates the arc probabilities of two continuous n</context>
</contexts>
<marker>Collins, 2000</marker>
<rawString>M. Collins. 2000. Discriminative reranking for natural language parsing. In Proc. ofICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M X Jin</author>
<author>M Y Kim</author>
<author>J H Lee</author>
</authors>
<title>Two-phase shift-reduce deterministic dependency parser of chinese.</title>
<date>2005</date>
<booktitle>In Proc. ofIJCNLP: Companion Volume including Posters/Demos and tutorial abstracts.</booktitle>
<contexts>
<context position="2318" citStr="Jin et al., 2005" startWordPosition="358" endWordPosition="361">A classifier is often used to choose the most probable action to assemble the dependency tree. (Yamada and Matsumoto, 2003) defined three actions and used a SVM classifier to choose one of them in a bottom-up way. The algorithm in (Nivre et al., 2004) is a blend of bottom-up and top-down processing. Its classifier is trained by memory-based learning. Deterministic parsing derives an analysis without redundancy or backtracking, and linear time can be achieved. But when searching the local optimum in the order of left-to-right, some wrong reduce may prevent next analysis with more possibility. (Jin et al., 2005) used a two-phase shift-reduce to decrease such errors, and improved the accuracy of long distance dependencies. In this paper a deterministic parsing based on dynamic local optimization is proposed. According to the probabilities of dependency arcs, the algorithm dynamically finds the one with the highest probabilities instead of dealing with the sentence in order. A procedure of constraint which can integrate more structure information is made to check the rationality of the reduce. Finally our results and error analysis are presented. 2 Dependency Probabilities An example of Chinese depende</context>
</contexts>
<marker>Jin, Kim, Lee, 2005</marker>
<rawString>M.X. Jin, M.Y. Kim, and J.H. Lee. 2005. Two-phase shift-reduce deterministic dependency parser of chinese. In Proc. ofIJCNLP: Companion Volume including Posters/Demos and tutorial abstracts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>J Nilsson</author>
</authors>
<title>Memory-based dependency parsing.</title>
<date>2004</date>
<booktitle>In Proc. of the Eighth Conf. on Computational Natural Language Learning (CoNLL),</booktitle>
<pages>49--56</pages>
<contexts>
<context position="830" citStr="Nivre et al., 2004" startWordPosition="118" endWordPosition="121">tract This paper presents a deterministic parsing algorithm for projective dependency grammar. In a bottom-up way the algorithm finds the local optimum dynamically. A constraint procedure is made to use more structure information. The algorithm parses sentences in linear time and labeling is integrated with the parsing. This parser achieves 63.29% labeled attachment score on the average in CoNLLX Shared Task. 1 Introduction Recently, dependency grammar has gained renewed attention in the parsing community. Good results have been achieved in some dependency parsers (Yamada and Matsumoto, 2003; Nivre et al., 2004). With the availability of many dependency treebanks (van der Beek et al., 2002; Hajiˇc et al., 2004; B¨ohmov´a et al., 2003; Kromann, 2003; Dˇzeroski et al., 2006) and more other treebanks which can be converted to dependency annotation (Brants et al., 2002; Nilsson et al., 2005; Chen et al., 2003; Kawata and Bartels, 2000), multi-lingual dependency parsing is proposed in CoNLL shared task (Buchholz et al., 2006). Many previous works focus on unlabeled parsing, in which exhaustive methods are often used (Eisner, 1996). Their global searching performs well in the unlabeled dependency parsing. </context>
<context position="6770" citStr="Nivre et al., 2004" startWordPosition="1095" endWordPosition="1098">lity because their GDs are stable and helpful to syntactic analysis. Other FTags with small probabilities are unstable in GDs and cannot provide efficient information for syntactic analysis. If their probabilities are less than 0.65 they will be ignored in our dependency parsing. 3 Dynamic local optimization Many previous methods are based on history-based models. Despite many obvious advantages, these methods can be awkward to encode some constrains within their framework (Collins, 2000). Classifiers are good at encoding more features in the deterministic parsing (Yamada and Matsumoto, 2003; Nivre et al., 2004). However, such algorithm often make more probable dependencies be prevented by preceding errors. An example is showed in Figure 2. Arc a is a frequent dependency and b is an arc with more probability. Arc b will be prevented by a if the reduce is carried out in order. 212 Figure 2: A common error in deterministic parsing 3.1 Our algorithm Our deterministic parsing is based on dynamic local optimization. The algorithm calculates the arc probabilities of two continuous nodes, and then reduces the most probable arc. The construction of dependency tree includes four actions: Check, Reduce, Delete</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2004</marker>
<rawString>J. Nivre, J. Hall, and J. Nilsson. 2004. Memory-based dependency parsing. In Proc. of the Eighth Conf. on Computational Natural Language Learning (CoNLL), pages 49–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing: An exploration.</title>
<date>1996</date>
<booktitle>In Proc. of the 16th Intern. Conf. on Computational Linguistics (COLING),</booktitle>
<pages>340--345</pages>
<contexts>
<context position="1354" citStr="Eisner, 1996" startWordPosition="204" endWordPosition="206">been achieved in some dependency parsers (Yamada and Matsumoto, 2003; Nivre et al., 2004). With the availability of many dependency treebanks (van der Beek et al., 2002; Hajiˇc et al., 2004; B¨ohmov´a et al., 2003; Kromann, 2003; Dˇzeroski et al., 2006) and more other treebanks which can be converted to dependency annotation (Brants et al., 2002; Nilsson et al., 2005; Chen et al., 2003; Kawata and Bartels, 2000), multi-lingual dependency parsing is proposed in CoNLL shared task (Buchholz et al., 2006). Many previous works focus on unlabeled parsing, in which exhaustive methods are often used (Eisner, 1996). Their global searching performs well in the unlabeled dependency parsing. But with the increase of parameters, efficiency has to be considered in labeled dependency parsing. Thus deterministic parsing was proposed as a robust and efficient method in recent years. Such method breaks the construction of dependency tree into a series of actions. A classifier is often used to choose the most probable action to assemble the dependency tree. (Yamada and Matsumoto, 2003) defined three actions and used a SVM classifier to choose one of them in a bottom-up way. The algorithm in (Nivre et al., 2004) i</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>J. Eisner. 1996. Three new probabilistic models for dependency parsing: An exploration. In Proc. of the 16th Intern. Conf. on Computational Linguistics (COLING), pages 340–345.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yamada</author>
<author>Y Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In Proc. of the 8th Intern. Workshop on Parsing Technologies (IWPT).</booktitle>
<contexts>
<context position="809" citStr="Yamada and Matsumoto, 2003" startWordPosition="114" endWordPosition="117">,hjzhu,ls}@ir.hit.edu.cn Abstract This paper presents a deterministic parsing algorithm for projective dependency grammar. In a bottom-up way the algorithm finds the local optimum dynamically. A constraint procedure is made to use more structure information. The algorithm parses sentences in linear time and labeling is integrated with the parsing. This parser achieves 63.29% labeled attachment score on the average in CoNLLX Shared Task. 1 Introduction Recently, dependency grammar has gained renewed attention in the parsing community. Good results have been achieved in some dependency parsers (Yamada and Matsumoto, 2003; Nivre et al., 2004). With the availability of many dependency treebanks (van der Beek et al., 2002; Hajiˇc et al., 2004; B¨ohmov´a et al., 2003; Kromann, 2003; Dˇzeroski et al., 2006) and more other treebanks which can be converted to dependency annotation (Brants et al., 2002; Nilsson et al., 2005; Chen et al., 2003; Kawata and Bartels, 2000), multi-lingual dependency parsing is proposed in CoNLL shared task (Buchholz et al., 2006). Many previous works focus on unlabeled parsing, in which exhaustive methods are often used (Eisner, 1996). Their global searching performs well in the unlabeled</context>
<context position="6749" citStr="Yamada and Matsumoto, 2003" startWordPosition="1091" endWordPosition="1094">the FTags with large probability because their GDs are stable and helpful to syntactic analysis. Other FTags with small probabilities are unstable in GDs and cannot provide efficient information for syntactic analysis. If their probabilities are less than 0.65 they will be ignored in our dependency parsing. 3 Dynamic local optimization Many previous methods are based on history-based models. Despite many obvious advantages, these methods can be awkward to encode some constrains within their framework (Collins, 2000). Classifiers are good at encoding more features in the deterministic parsing (Yamada and Matsumoto, 2003; Nivre et al., 2004). However, such algorithm often make more probable dependencies be prevented by preceding errors. An example is showed in Figure 2. Arc a is a frequent dependency and b is an arc with more probability. Arc b will be prevented by a if the reduce is carried out in order. 212 Figure 2: A common error in deterministic parsing 3.1 Our algorithm Our deterministic parsing is based on dynamic local optimization. The algorithm calculates the arc probabilities of two continuous nodes, and then reduces the most probable arc. The construction of dependency tree includes four actions: </context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>H. Yamada and Y. Matsumoto. 2003. Statistical dependency analysis with support vector machines. In Proc. of the 8th Intern. Workshop on Parsing Technologies (IWPT).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>