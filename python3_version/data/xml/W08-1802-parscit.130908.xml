<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.055499">
<title confidence="0.997782">
Exact Phrases in Information Retrieval for Question Answering
</title>
<author confidence="0.99752">
Svetlana Stoyanchev, and Young Chol Song, and William Lahti
</author>
<affiliation confidence="0.9623995">
Department of Computer Science
Stony Brook University
</affiliation>
<address confidence="0.882644">
Stony Brook, NY 11794-4400
</address>
<email confidence="0.988896">
svetastenchikova, nskystars, william.lahti @gmail.com
</email>
<sectionHeader confidence="0.993567" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999960571428572">
Question answering (QA) is the task of
finding a concise answer to a natural lan-
guage question. The first stage of QA in-
volves information retrieval. Therefore,
performance of an information retrieval
subsystem serves as an upper bound for the
performance of a QA system. In this work
we use phrases automatically identified
from questions as exact match constituents
to search queries. Our results show an im-
provement over baseline on several docu-
ment and sentence retrieval measures on
the WEB dataset. We get a 20% relative
improvement in MRR for sentence extrac-
tion on the WEB dataset when using au-
tomatically generated phrases and a fur-
ther 9.5% relative improvement when us-
ing manually annotated phrases. Surpris-
ingly, a separate experiment on the indexed
AQUAINT dataset showed no effect on IR
performance of using exact phrases.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998449">
Question answering can be viewed as a sophisti-
cated information retrieval (IR) task where a sys-
tem automatically generates a search query from
a natural language question and finds a concise
answer from a set of documents. In the open-
domain factoid question answering task systems
answer general questions like Who is the creator of
The Daily Show?, or When was Mozart born?. A
variety of approaches to question answering have
been investigated in TREC competitions in the last
</bodyText>
<note confidence="0.723297">
© 2008. Licensed under the Creative Commons
</note>
<footnote confidence="0.967386333333333">
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
</footnote>
<bodyText confidence="0.999744447368421">
decade from (Vorhees and Harman, 1999) to (Dang
et al., 2006). Most existing question answering
systems add question analysis, sentence retrieval
and answer extraction components to an IR sys-
tem.
Since information retrieval is the first stage of
question answering, its performance is an up-
per bound on the overall question answering sys-
tem’s performance. IR performance depends on
the quality of document indexing and query con-
struction. Question answering systems create a
search query automatically from a user’s question,
through various levels of sophistication. The sim-
plest way of creating a query is to treat the words
in the question as the terms in the query. Some
question answering systems (Srihari and Li, 1999)
apply linguistic processing to the question, iden-
tifying named entities and other query-relevant
phrases. Others (Hovy et al., 2001b) use ontolo-
gies to expand query terms with synonyms and hy-
pernyms.
IR system recall is very important for question
answering. If no correct answers are present in a
document, no further processing will be able to
find an answer. IR system precision and rank-
ing of candidate passages can also affect question
answering performance. If a sentence without a
correct answer is ranked highly, answer extrac-
tion may extract incorrect answers from these erro-
neous candidates. Collins-Thompson et al. (2004)
show that there is a consistent relationship between
the quality of document retrieval and the overall
performance of question answering systems.
In this work we evaluate the use of exact phrases
from a question in document and passage retrieval.
First, we analyze how different parts of a ques-
tion contribute to the performance of the sentence
extraction stage of question answering. We ana-
</bodyText>
<page confidence="0.970794">
9
</page>
<note confidence="0.994404">
Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering (IR4QA), pages 9–16
Manchester, UK. August 2008
</note>
<bodyText confidence="0.99993964">
lyze the match between linguistic constituents of
different types in questions and sentences contain-
ing candidate answers. For this analysis, we use a
set of questions and answers from the TREC 2006
competition as a gold standard.
Second, we evaluate the performance of doc-
ument retrieval in our StoQA question answering
system. We compare the performance of docu-
ment retrieval from the Web and from an indexed
collection of documents using different methods of
query construction, and identify the optimal algo-
rithm for query construction in our system as well
as its limitations.
Third, we evaluate passage extraction from a set
of documents. We analyze how the specificity of a
query affects sentence extraction.
The rest of the paper is organized as follows:
In Section 2, we summarize recent approaches to
question answering. In Section 3, we describe the
dataset used in this experiment. In Section 5, we
describe our method and data analysis. In Sec-
tion 4, we outline the architecture of our question
answering system. In Section 6, we describe our
experiments and present our results. We summa-
rize in Section 7.
</bodyText>
<sectionHeader confidence="0.999692" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999932172413793">
Information retrieval (IR) for question answering
consists of 2 steps: document retrieval and passage
retrieval.
Approaches to passage retrieval include sim-
ple word overlap (Light et al., 2001), density-
based passage retrieval (Clarke et al., 2000), re-
trieval based on the inverse document frequency
(IDF) of matched and mismatched words (Itty-
cheriah et al., 2001), cosine similarity between a
question and a passage (Llopis and Vicedo, 2001),
passage/sentence ranking by weighting different
features (Lee and others, 2001), stemming and
morphological query expansion (2004), and vot-
ing between different retrieval methods (Tellex
et al., 2003). As in previous approaches, we
use words and phrases from a question for pas-
sage extraction and experiment with using exactly
matched phrases in addition to words. Similarly
to Lee (2001), we assign weights to sentences in
retrieved documents according to the number of
matched constituents.
Systems vary in the size of retrieved passages.
Some systems identify multi-sentence and variable
size passages (Ittycheriah et al., 2001; Clarke et
al., 2000). An optimal passage size may depend
on the method of answer extraction. We use single
sentence extraction because our system’s semantic
role labeling-based answer extraction functions on
individual sentences.
White and Sutcliffe (2004) performed a man-
ual analysis of questions and answers for 50 of the
TREC questions. The authors computed frequency
of terms matching exactly, with morphological, or
semantic variation between a question and a an-
swer passage. In this work we perform a similar
analysis automatically. We compare frequencies
of phrases and words matching between a question
and candidate sentences.
Query expansion has been investigated in sys-
tems described in (Hovy et al., 2001a; Harabagiu
et al., 2006). They use WordNet (Miller, 1995) for
query expansion, and incorporate semantic roles in
the answer extraction process. In this experiment
we do not expand query terms.
Corpus pre-processing and encoding informa-
tion useful for retrieval was shown to improve doc-
ument retrieval (Katz and Lin, 2003; Harabagiu
et al., 2006; Chu-Carroll et al., 2006). In our
approach we evaluate linguistic question process-
ing technique which does not require corpus pre-
processing.
Statistical machine translation model is used
for information retrieval by (Murdock and Croft,
2005). The model estimates probability of a ques-
tion given an answer and is trained on &lt;question,
candidate sentence&gt; pairs. It capturing synonymy
and grammar transformations using a statistical
model.
</bodyText>
<sectionHeader confidence="0.996059" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.999990583333333">
In this work we evaluate our question answering
system on two datasets: the AQUAINT corpus, a 3
gigabyte collection of news documents used in the
TREC 2006 competition; and the Web.
We use questions from TREC, a yearly ques-
tion answering competition. We use a subset
of questions with non-empty answers 1 from the
TREC 2006 dataset 2. The dataset provides a list
of matching documents from the AQUAINT cor-
pus and correct answers for each question. The
dataset contains 387 questions; the AQUAINT cor-
pus contains an average of 3.5 documents per ques-
</bodyText>
<footnote confidence="0.998958">
1The questions where an answer was not in the dataset
were not used in this analysis
2http://trec.nist.gov/data/qa/t2006 qadata.html
</footnote>
<page confidence="0.99829">
10
</page>
<bodyText confidence="0.999987869565217">
tion that contain the correct answer to that ques-
tion. Using correct answers we find the correct
sentences from the matching documents. We use
this information as a gold standard for the IR task.
We index the documents in the AQUAINT cor-
pus using the Lucene (Apache, 2004 2008) engine
on the document level. We evaluate document re-
trieval using gold standard documents from the
AQUAINT corpus. We evaluate sentence extrac-
tion on both AQUAINT and the Web automatically
using regular expressions for correct answers pro-
vided by TREC.
In our experiments we use manually and auto-
matically created phrases. Our automatically cre-
ated phrases were obtained by extracting noun,
verb and prepositional phrases and named entities
from the question dataset using then NLTK (Bird
et al., 2008) and Lingpipe (Carpenter and Bald-
win, 2008) tools. Our manually created phrases
were obtained by hand-correcting these automatic
annotations (e.g. to remove extraneous words and
phrases and add missed words and phrases from
the questions).
</bodyText>
<sectionHeader confidence="0.996151" genericHeader="method">
4 System
</sectionHeader>
<bodyText confidence="0.999952194444444">
For the experiments in this paper we use the StoQA
system. This system employs a pipeline architec-
ture with three main stages as illustrated in Fig-
ure 1: question analysis, document and sentence
extraction (IR), and answer extraction. After the
user poses a question, it is analyzed. Target named
entities and semantic roles are determined. A
query is constructed, tailored to the search tools in
use. Sentences containing target terms are then ex-
tracted from the documents retrieved by the query.
The candidate sentences are processed to identify
and extract candidate answers, which are presented
to the user.
We use the NLTK toolkit (Bird et al., 2008)
for question analysis and can add terms to search
queries using WordNet (Miller, 1995). Our system
can currently retrieve documents from either the
Web (using the Yahoo search API (Yahoo!, 2008)),
or the AQUAINT corpus (Graff, 2002) (through
the Lucene indexer and search engine (Apache,
2004 2008)). When using Lucene, we can assign
different weights to different types of search term
(e.g. less weight to terms than to named entities
added to a query) (cf. (Lee and others, 2001)).
We currently have two modules for answer ex-
traction, which can be used separately or together.
Candidate sentences can be tagged with named en-
tity information using the Lydia system (Lloyd et
al., 2005). The tagged word/phrase matching the
target named entity type most frequently found is
chosen as the answer. Our system can also extract
answers through semantic role labeling, using the
SRL toolkit from (Punyakanok et al., 2008). In
this case, the tagged word/phrase matching the tar-
get semantic role most frequently found is chosen
as the answer.
</bodyText>
<figureCaption confidence="0.9160615">
Figure 1: Architecutre of our question answering
system
</figureCaption>
<sectionHeader confidence="0.965101" genericHeader="method">
5 Method
</sectionHeader>
<subsectionHeader confidence="0.978193">
5.1 Motivation
</subsectionHeader>
<bodyText confidence="0.999624">
Question answering is an engineering-intensive
task. System performance improves as more so-
phisticated techniques are applied to data process-
ing. For example, the IR stage in question an-
swering is shown to improve with the help of tech-
niques like predictive annotations and relation ex-
traction; matching of semantic and syntactic re-
</bodyText>
<page confidence="0.995796">
11
</page>
<table confidence="0.999590533333333">
Target United Nations
Question What was the number of member nations of the U.N. in 2000?
Named Entity U.N., United Nations
Phrases “member nations of the U.N.”
Converted Q-phrase “member nations of the U.N. in 2000”
Baseline Query was the number of member nations of the U.N. in 2000
United Nations
Lucene Query with phrases was the number of member nations of the U.N. in 2000
and NE “United Nations”, ”member nations of the u.n.”
Cascaded web query
query1 “member nations of the U.N. in 2000” AND ( United Nations )
query2 ”member nations of the u.n.” AND ( United Nations )
query3 (number of member nations of the U.N. in 2000) AND ( United
Nations )
query4 ( United Nations )
</table>
<tableCaption confidence="0.999927">
Table 1: Question processing example: terms of a query
</tableCaption>
<bodyText confidence="0.990605321428571">
lations in a question and a candidate sentence
are known to improve overall QA system perfor-
mance (Prager et al., 2000; Stenchikova et al.,
2006; Katz and Lin, 2003; Harabagiu et al., 2006;
Chu-Carroll et al., 2006).
In this work we analyze less resource expensive
techniques, such as chunking and named entity de-
tection, for IR in question answering. Linguistic
analysis in our system is applied to questions and
to candidate sentences only. There is no need for
annotation of all documents to be indexed, so our
techniques can be applied to IR on large datasets
such as the Web.
Intuitively, using phrases in query construction
may improve retrieval precision. For example,
if we search for In what year did the movie win
academy awards? using a disjunction of words
as our query we may match irrelevant documents
about the military academy or Nobel prize awards.
However, if we use the phrase “academy awards”
as one of the query terms, documents with this
term will receive a higher ranking. A counterargu-
ment for using phrases is that academy and awards
are highly correlated and therefore the documents
that contain both will be more highly ranked. We
hypothesize that for phrases where constituents are
not highly correlated, exact phrase extraction will
give more benefit.
</bodyText>
<subsectionHeader confidence="0.999177">
5.2 Search Query
</subsectionHeader>
<bodyText confidence="0.998988153846154">
We process each TREC question and target 3 to
identify named entities. Often, the target is a com-
plete named entity (NE), however, in some of the
TREC questions the target contains a named entity,
e.g. tourists massacred at Luxor in 1997, or 1991
eruption of Mount Pinatubo with named entities
Luxor and Mount Pinatubo. For the TREC ques-
tion What was the number of member nations of
the U.N. in 2000?, the identified constituents and
automatically constructed query are shown in Ta-
ble 1. Named entities are identified using Ling-
pipe (Carpenter and Baldwin, 2008), which iden-
tifies named entities of type organization, location
and person. Phrases are identified automatically
using the NLTK toolkit (Bird et al., 2008). We
extract noun phrases, verb phrases and preposi-
tional phrases. The rules for identifying phrases
are mined from a dataset of manually annotated
parse trees (Judge et al., 2006) 4. Converted Q-
phrases are heuristically created phrases that para-
phrase the question in declarative form using a
small set of rules. The rules match a question to a
pattern and transform the question using linguistic
information. For example, one rule matches Who
islwas NOUNIPRONOUN VBD and converts it to
NOUN|PRONOUN is|was VBD. 5
</bodyText>
<footnote confidence="0.918397333333333">
3The TREC dataset also provides a target topic for each
questions, and we include it in the query.
4The test questions are not in this dataset.
5Q-phrase is extracted only for who/when/where ques-
tions. We used a set of 6 transformation patterns in this ex-
periment.
</footnote>
<page confidence="0.994896">
12
</page>
<table confidence="0.8165972">
Named Entities Phrases
great pyramids; frank sinatra; mt. capacity of the ballpark; groath rate; se-
pinatubo; miss america; manchester curity council; tufts university endow-
united; clinton administration ment; family members; terrorist organi-
zation
</table>
<tableCaption confidence="0.997269">
Table 2: Automatically identified named entities and phrases
</tableCaption>
<bodyText confidence="0.999925823529412">
A q-phrase represents how a simple answer is
expected to appear, e. g. a q-phrase for the ques-
tion When was Mozart born? is Mozart was born.
We expect a low probability of encountering a q-
phrase in retrieved documents, but a high prob-
ability of co-occurrence of q-phrases phrase with
correct answers.
In our basic system (baseline), words (trivial
query constituents) from question and target form
the query. In the experimental system, the query is
created from a combination of words, quoted exact
phrases, and quoted named entities. Table 2 shows
some examples of phrases and named entities used
in queries. The goal of our analysis is to evaluate
whether non-trivial query constituents can improve
document and sentence extraction.
We use a back-off mechanism with both of
our IR subsystems to improve document extrac-
tion. The Lucene API allows the user to cre-
ate arbitrarily long queries and assign a weight to
each query constituent. We experiment with as-
signing different weights based on the type of a
query constituent. Assigning a higher weight to
phrase constituents increases the scores for docu-
ments matching a phrase, but if no phrase matches
are found documents matching lower-scored con-
stituents will be returned.
The query construction system for the Web first
produces a query containing only converted q-
phrases which have low recall and high precision
(query 1 in table 1). If this query returns less than
20 results, it then constructs a query using phrases
(query 2 in table 1), if this returns less than 20 re-
sults, queries without exact phrases (queries 3 and
4) are used. Every query contains a conjunction
with the question target to increase precision for
the cases where the target is excluded from con-
verted q-phrase or an exact phrase.
For both our IR subsystems we return a maxi-
mum of 20 documents. We chose this relatively
low number of documents because our answer ex-
traction algorithm relies on semantic tagging of
candidate sentences, which is a relatively time-
consuming operation.
The text from each retrieved documents is split
into sentences using Lingpipe. The same sen-
tence extraction algorithm is used for the output
from both IR subsystems (AQUAINT/Lucene and
Web/Yahoo). The sentence extraction algorithm
assigns a score to each sentence according to the
number of matched terms it contains.
</bodyText>
<subsectionHeader confidence="0.999763">
5.3 Analysis of Constituents
</subsectionHeader>
<bodyText confidence="0.999977291666667">
For our analysis of the impact of different linguis-
tic constituent types on document retrieval we use
the TREC 2006 dataset which consists of ques-
tions, documents containing answers to each ques-
tion, and supporting sentences, sentences from
these documents that contain the answer to each
question.
Table 3 shows the number of times each con-
stituent type appears in a supporting sentence and
the proportion of supporting sentences containing
each constituent type (sent w/answer column). The
“All Sentences” column shows the number of con-
stituents in all sentences of candidate documents.
The precision column displays the chance that a
given sentence is a supporting sentence if a con-
stituent of a particular type is present in it. Con-
verted q-phrase has the highest precision, followed
by phrases, verbs, and named entities. Words have
the highest chance of occurrence in a supporting
sentence (.907), but they also have a high chance
of occurrence in a document (.745).
This analysis supports our hypothesis that using
exact phrases may improve the performance of in-
formation retrieval for question answering.
</bodyText>
<sectionHeader confidence="0.997543" genericHeader="evaluation">
6 Experiment
</sectionHeader>
<bodyText confidence="0.999831857142857">
In these experiments we look at the impact of using
exact phrases on the performance of the document
retrieval and sentence extraction stages of question
answering. We use our StoQA question answering
system. Questions are analyzed as described in the
previous section. For document retrieval we use
the back-off method described in the previous sec-
</bodyText>
<page confidence="0.998514">
13
</page>
<table confidence="0.999232625">
sent w/ answer num all sentences precision
num proportion proportion
Named Entity 907 0.320 4873 0.122 .18
Phrases 350 0.123 1072 0.027 .34
Verbs 396 0.140 1399 0.035 .28
Q-Phrases 11 0.004 15 0.00038 .73
Words 2573 0.907 29576 0.745 .086
Total Sentences 2836 39688
</table>
<tableCaption confidence="0.990622">
Table 3: Query constituents in sentences of correct documents
</tableCaption>
<table confidence="0.999470857142857">
avg doc avg doc overall avg overall avg corr avg corr avg corr
recall MRR doc recall sent sent sent sent sent
MRR recall in top 1 in top 10 in top 50
IR with Lucene on AQUAINT dataset
baseline (words disjunction 0.530 0.631 0.756 0.314 0.627 0.223 1.202 3.464
from target and question)
baseline 0.514 0.617 0.741 0.332 0.653 0.236 1.269 3.759
+ auto phrases
words 0.501 0.604 0.736 0.316 0.653 0.220 1.228 3.705
+ auto NEs &amp; phrases
baseline 0.506 0.621 0.738 0.291 0.609 0.199 1.231 3.378
+ manual phrases
words 0.510 0.625 0.738 0.294 0.609 0.202 1.244 3.368
+ manual NEs &amp; phrases
IR with Yahoo API on WEB
baseline - - - 0.183 0.570 0.101 0.821 2.316
words disjunction
cascaded - - - 0.220 0.604 0.140 0.956 2.725
using auto phrases
cascaded - - - 0.241 0.614 0.155 1.065 3.016
using manual phrases
</table>
<tableCaption confidence="0.997586">
Table 4: Document retrieval evaluation.
</tableCaption>
<bodyText confidence="0.9988796">
tion. We performed the experiments using first au-
tomatically generated phrases, and then manually
corrected phrases.
For document retrieval we report: 1) average re-
call, 2) average mean reciprocal ranking (MRR),
and 3) overall document recall. Each question has
a document retrieval recall score which is the pro-
portion of documents identified from all correct
documents for this question. The average recall
is the individual recall averaged over all questions.
MRR is the inverse index of the first correct doc-
ument. For example, if the first correct document
appears second, the MRR score will be 1/2. MRR
is computed for each question and averaged over
all questions. Overall document recall is the per-
centage of questions for which at least one correct
document was retrieved. This measure indicates
the upper bound on the QA system.
For sentence retrieval we report 1) average sen-
tence MRR, 2) overall sentence recall, 3) average
precision of the first sentence, 4) number of cor-
rect candidate sentences in the top 10 results, and
5) number of correct candidate sentences in the top
50 results 6.
Table 4 shows our experimental results. First,
we evaluate the performance of document retrieval
on the indexed AQUAINT dataset. Average doc-
ument recall for our baseline system is 0.53, in-
dicating that on average half of the correct doc-
uments are retrieved. Average document MRR
is .631, meaning that on average the first correct
document appears first or second. Overall docu-
ment recall indicates that 75.6% of queries con-
tain a correct document among the retrieved docu-
ments. Average sentence recall is lower than docu-
ment recall indicating that some proportion of cor-
rect answers is not retrieved using our heuristic
sentence extraction algorithm. The average sen-
tence MRR is .314 indicating that the first correct
sentence is approximately third on the list. With
</bodyText>
<footnote confidence="0.9037905">
6Although the number of documents is 20, multiple sen-
tences may be extracted from each document.
</footnote>
<page confidence="0.998803">
14
</page>
<bodyText confidence="0.99988934">
the AQUAINT dataset, we notice no improvement
with exact phrases.
Next, we evaluate sentence retrieval from the
WEB. There is no gold standard for the WEB
dataset so we do not report document retrieval
scores. Sentence scores on the WEB dataset are
lower than on the AQUAINT dataset 7.
Using back-off retrieval with automatically cre-
ated phrases and named entities, we see an im-
provement over the baseline system performance
for each of the sentence measures on the WEB
dataset. Average sentence MRR increases 20%
from .183 in the baseline to .220 in the experimen-
tal system. With manually created phrases MRR
improves a further 9.5% to .241. This indicates
that information retrieval on the WEB dataset can
benefit from a better quality of chunker and from a
properly converted question phrase. It also shows
that the improvement is not due to simply match-
ing random substrings from a question, but that
linguistic information is useful in constructing the
exact match phrases. Precision of automatically
detected phrases is affected by errors during auto-
matic part-of-speech tagging of questions. An ex-
ample of an error due to POS tagging is the iden-
tification of a phrase was Rowling born due to a
failure to identify that born is a verb.
Our results emphasize the difference between
the two datasets. AQUAINT dataset is a collec-
tion of a large set of news documents, while WEB
is a much larger resource of information from a
variety of sources. It is reasonable to assume
that on average there are much fewer documents
with query words in AQUAINT corpus than on the
WEB. Proportion of correct documents from all re-
trieved WEB documents on average is likely to be
lower than this proportion in documents retrieved
from AQUAINT. When using words on a query
to AQUAINT dataset, most of the correct docu-
ments are returned in the top matches. Our results
indicate that over 50% of correct documents are
retrieved in the top 20 results. Results in table 3
indicate that exactly matched phrases from a ques-
tion are more precise predictors of presence of an
answer. Using exact matched phrases in a WEB
query allows a search engine to give higher rank to
more relevant documents and increases likelihood
of these documents in the top 20 matches.
Although overall performance on the WEB
dataset is lower than on AQUAINT, there is a po-
</bodyText>
<footnote confidence="0.415758">
7Our decision to use only 20 documents may be a factor.
</footnote>
<bodyText confidence="0.999862666666667">
tential for improvement by using a larger set of
documents and improving our sentence extraction
heuristics.
</bodyText>
<sectionHeader confidence="0.966202" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999612214285714">
In this paper we present a document retrieval ex-
periment on a question answering system. We
evaluate the use of named entities and of noun,
verb, and prepositional phrases as exact match
phrases in a document retrieval query. Our re-
sults indicate that using phrases extracted from
questions improves IR performance on WEB data.
Surprisingly, we find no positive effect of using
phrases on a smaller closed set of data.
Our data analysis shows that linguistic phrases
are more accurate indicators for candidate sen-
tences than words. In future work we plan to evalu-
ate how phrase type (noun vs. verb vs. preposition)
affects IR performance.
</bodyText>
<sectionHeader confidence="0.970828" genericHeader="acknowledgments">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.99988175">
We would like to thank professor Amanda Stent
for suggestions about experiments and proofread-
ing the paper. We would like to thank the reviewers
for useful comments.
</bodyText>
<sectionHeader confidence="0.998151" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999224666666667">
Apache. 2004-2008. Lucene.
http://lucene.apache.org/java/docs/index.html.
Bilotti, M., B. Katz, and J. Lin. 2004. What works
better for question answering: Stemming or morpho-
logical query expansion? In Proc. SIGIR.
Bird, S., E. Loper, and E. Klein. 2008.
Natural Language ToolKit (NLTK).
http://nltk.org/index.php/Main Page.
Carpenter, B. and B. Baldwin. 2008. Lingpipe.
http://alias-i.com/lingpipe/index.html.
Chu-Carroll, J., J. Prager, K. Czuba, D. Ferrucci, and
P. Duboue. 2006. Semantic search via XML frag-
ments: a high-precision approach to IR. In Proc.
SIGIR.
Clarke, C., G. Cormack, D. Kisman, and T. Lynam.
2000. Question answering by passage selection
(multitext experiments for TREC-9). In Proc. TREC.
Collins-Thompson, K., J. Callan, E. Terra, and C. L.A.
Clarke. 2004. The effect of document retrieval qual-
ity on factoid question answering performance. In
Proc. SIGIR.
Dang, H., J. Lin, and D. Kelly. 2006. Overview of
the TREC 2006 question answering track. In Proc.
TREC.
</reference>
<page confidence="0.997883">
15
</page>
<bodyText confidence="0.856814666666667">
Graff, D. 2002. The AQUAINT corpus of English
news text. Technical report, Linguistic Data Con-
sortium, Philadelphia, PA, USA.
Stenchikova, S., D. Hakkani-Tur, and G. Tur. 2006.
QASR: Question answering using semantic roles for
speech interface. In Proc. ICSLP-Interspeech 2006.
</bodyText>
<reference confidence="0.999294586206897">
Harabagiu, S., A. Hickl, J. Williams, J. Bensley,
K. Roberts, Y. Shi, and B. Rink. 2006. Question
answering with LCC’s CHAUCER at TREC 2006.
In Proc. TREC.
Hovy, E., L. Gerber, U. Hermjakob, M. Junk, and C.-Y.
Lin. 2001a. Question answering in Webclopedia. In
Proc. TREC.
Hovy, E., U. Hermjakob, and C.-Y. Lin. 2001b. The
use of external knowledge in factoid QA. In Proc.
TREC.
Ittycheriah, A., M. Franz, and S. Roukos. 2001. IBM’s
statistical question answering system – TREC-10. In
Proc. TREC.
Judge, J., A. Cahill, and J. van Genabith. 2006.
QuestionBank: Creating a corpus of parse-annotated
questions. In Proc. ACL.
Katz, B. and J. Lin. 2003. Selectively using relations to
improve precision in question answering. In Proc. of
the EACL Workshop on Natural Language Process-
ing for Question Answering.
Lee, G. G. et al. 2001. SiteQ: Engineering high per-
formance QA system using lexico-semantic pattern
matching and shallow NLP. In Proc. TREC.
Light, M., G. S. Mann, E. Riloff, and E. Breck. 2001.
Analyses for elucidating current question answering
technology. Journal ofNatural Language Engineer-
ing, 7(4).
Llopis, F. and J. L. Vicedo. 2001. IR-n: A passage re-
trieval system at CLEF-2001. In Proc. of the Second
Workshop of the Cross-Language Evaluation Forum
(CLEF 2001).
Lloyd, L., D. Kechagias, and S. Skiena. 2005. Ly-
dia: A system for large-scale news analysis. In Proc.
SPIRE, pages 161–166.
Miller, George A. 1995. WordNet: a lexical database
for english. Communications of the ACM, 38(11).
Murdock, V. and W. B. Croft. 2005. Simple transla-
tion models for sentence retrieval in factoid question
answering. In Proc. SIGIR.
Prager, J., E. Brown, and A. Coden. 2000. Question-
answering by predictive annotation. In ACM SIGIR.
QA -to site.
Punyakanok, V., D. Roth, and W. Yih. 2008. The im-
portance of syntactic parsing and inference in seman-
tic role labeling. Computational Linguistics, 34(2).
Srihari, R. and W. Li. 1999. Information extraction
supported question answering. In Proc. TREC.
Tellex, S., B. Katz, J. Lin, A. Fernandes, and G. Marton.
2003. Quantitative evaluation of passage retrieval al-
gorithms for question answering. In Proc. SIGIR.
Vorhees, V. and D. Harman. 1999. Overview of the
eighth Text REtrieval Conference (TREC-8). In
”Proc. TREC”.
White, K. and R. Sutcliffe. 2004. Seeking an upper
bound to sentence level retrieval in question answer-
ing. In Proc. SIGIR.
Yahoo!, Inc. 2008. Yahoo! search API.
http://developer.yahoo.com/search/.
</reference>
<page confidence="0.998322">
16
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.828183">
<title confidence="0.998265">Exact Phrases in Information Retrieval for Question Answering</title>
<author confidence="0.95661">Svetlana Stoyanchev</author>
<author confidence="0.95661">Young Chol Song</author>
<author confidence="0.95661">William</author>
<affiliation confidence="0.96436">Department of Computer</affiliation>
<author confidence="0.928488">Stony Brook Stony Brook</author>
<author confidence="0.928488">NY</author>
<email confidence="0.996142">svetastenchikova,nskystars,william.lahti@gmail.com</email>
<abstract confidence="0.9995675">Question answering (QA) is the task of finding a concise answer to a natural language question. The first stage of QA involves information retrieval. Therefore, performance of an information retrieval subsystem serves as an upper bound for the performance of a QA system. In this work we use phrases automatically identified from questions as exact match constituents to search queries. Our results show an improvement over baseline on several document and sentence retrieval measures on the WEB dataset. We get a 20% relative improvement in MRR for sentence extraction on the WEB dataset when using automatically generated phrases and a further 9.5% relative improvement when using manually annotated phrases. Surprisingly, a separate experiment on the indexed AQUAINT dataset showed no effect on IR performance of using exact phrases.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Apache</author>
</authors>
<date>2004</date>
<note>Lucene. http://lucene.apache.org/java/docs/index.html.</note>
<contexts>
<context position="8340" citStr="Apache, 2004" startWordPosition="1303" endWordPosition="1304">set provides a list of matching documents from the AQUAINT corpus and correct answers for each question. The dataset contains 387 questions; the AQUAINT corpus contains an average of 3.5 documents per ques1The questions where an answer was not in the dataset were not used in this analysis 2http://trec.nist.gov/data/qa/t2006 qadata.html 10 tion that contain the correct answer to that question. Using correct answers we find the correct sentences from the matching documents. We use this information as a gold standard for the IR task. We index the documents in the AQUAINT corpus using the Lucene (Apache, 2004 2008) engine on the document level. We evaluate document retrieval using gold standard documents from the AQUAINT corpus. We evaluate sentence extraction on both AQUAINT and the Web automatically using regular expressions for correct answers provided by TREC. In our experiments we use manually and automatically created phrases. Our automatically created phrases were obtained by extracting noun, verb and prepositional phrases and named entities from the question dataset using then NLTK (Bird et al., 2008) and Lingpipe (Carpenter and Baldwin, 2008) tools. Our manually created phrases were obtai</context>
<context position="10047" citStr="Apache, 2004" startWordPosition="1573" endWordPosition="1574">ned. A query is constructed, tailored to the search tools in use. Sentences containing target terms are then extracted from the documents retrieved by the query. The candidate sentences are processed to identify and extract candidate answers, which are presented to the user. We use the NLTK toolkit (Bird et al., 2008) for question analysis and can add terms to search queries using WordNet (Miller, 1995). Our system can currently retrieve documents from either the Web (using the Yahoo search API (Yahoo!, 2008)), or the AQUAINT corpus (Graff, 2002) (through the Lucene indexer and search engine (Apache, 2004 2008)). When using Lucene, we can assign different weights to different types of search term (e.g. less weight to terms than to named entities added to a query) (cf. (Lee and others, 2001)). We currently have two modules for answer extraction, which can be used separately or together. Candidate sentences can be tagged with named entity information using the Lydia system (Lloyd et al., 2005). The tagged word/phrase matching the target named entity type most frequently found is chosen as the answer. Our system can also extract answers through semantic role labeling, using the SRL toolkit from (</context>
</contexts>
<marker>Apache, 2004</marker>
<rawString>Apache. 2004-2008. Lucene. http://lucene.apache.org/java/docs/index.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Bilotti</author>
<author>B Katz</author>
<author>J Lin</author>
</authors>
<title>What works better for question answering: Stemming or morphological query expansion? In</title>
<date>2004</date>
<booktitle>Proc. SIGIR.</booktitle>
<marker>Bilotti, Katz, Lin, 2004</marker>
<rawString>Bilotti, M., B. Katz, and J. Lin. 2004. What works better for question answering: Stemming or morphological query expansion? In Proc. SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bird</author>
<author>E Loper</author>
<author>E Klein</author>
</authors>
<date>2008</date>
<journal>Natural Language ToolKit</journal>
<contexts>
<context position="8850" citStr="Bird et al., 2008" startWordPosition="1381" endWordPosition="1384">a gold standard for the IR task. We index the documents in the AQUAINT corpus using the Lucene (Apache, 2004 2008) engine on the document level. We evaluate document retrieval using gold standard documents from the AQUAINT corpus. We evaluate sentence extraction on both AQUAINT and the Web automatically using regular expressions for correct answers provided by TREC. In our experiments we use manually and automatically created phrases. Our automatically created phrases were obtained by extracting noun, verb and prepositional phrases and named entities from the question dataset using then NLTK (Bird et al., 2008) and Lingpipe (Carpenter and Baldwin, 2008) tools. Our manually created phrases were obtained by hand-correcting these automatic annotations (e.g. to remove extraneous words and phrases and add missed words and phrases from the questions). 4 System For the experiments in this paper we use the StoQA system. This system employs a pipeline architecture with three main stages as illustrated in Figure 1: question analysis, document and sentence extraction (IR), and answer extraction. After the user poses a question, it is analyzed. Target named entities and semantic roles are determined. A query is</context>
<context position="13958" citStr="Bird et al., 2008" startWordPosition="2225" endWordPosition="2228">te named entity (NE), however, in some of the TREC questions the target contains a named entity, e.g. tourists massacred at Luxor in 1997, or 1991 eruption of Mount Pinatubo with named entities Luxor and Mount Pinatubo. For the TREC question What was the number of member nations of the U.N. in 2000?, the identified constituents and automatically constructed query are shown in Table 1. Named entities are identified using Lingpipe (Carpenter and Baldwin, 2008), which identifies named entities of type organization, location and person. Phrases are identified automatically using the NLTK toolkit (Bird et al., 2008). We extract noun phrases, verb phrases and prepositional phrases. The rules for identifying phrases are mined from a dataset of manually annotated parse trees (Judge et al., 2006) 4. Converted Qphrases are heuristically created phrases that paraphrase the question in declarative form using a small set of rules. The rules match a question to a pattern and transform the question using linguistic information. For example, one rule matches Who islwas NOUNIPRONOUN VBD and converts it to NOUN|PRONOUN is|was VBD. 5 3The TREC dataset also provides a target topic for each questions, and we include it </context>
</contexts>
<marker>Bird, Loper, Klein, 2008</marker>
<rawString>Bird, S., E. Loper, and E. Klein. 2008. Natural Language ToolKit (NLTK). http://nltk.org/index.php/Main Page.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Carpenter</author>
<author>B Baldwin</author>
</authors>
<date>2008</date>
<note>Lingpipe. http://alias-i.com/lingpipe/index.html.</note>
<contexts>
<context position="8893" citStr="Carpenter and Baldwin, 2008" startWordPosition="1387" endWordPosition="1391">We index the documents in the AQUAINT corpus using the Lucene (Apache, 2004 2008) engine on the document level. We evaluate document retrieval using gold standard documents from the AQUAINT corpus. We evaluate sentence extraction on both AQUAINT and the Web automatically using regular expressions for correct answers provided by TREC. In our experiments we use manually and automatically created phrases. Our automatically created phrases were obtained by extracting noun, verb and prepositional phrases and named entities from the question dataset using then NLTK (Bird et al., 2008) and Lingpipe (Carpenter and Baldwin, 2008) tools. Our manually created phrases were obtained by hand-correcting these automatic annotations (e.g. to remove extraneous words and phrases and add missed words and phrases from the questions). 4 System For the experiments in this paper we use the StoQA system. This system employs a pipeline architecture with three main stages as illustrated in Figure 1: question analysis, document and sentence extraction (IR), and answer extraction. After the user poses a question, it is analyzed. Target named entities and semantic roles are determined. A query is constructed, tailored to the search tools </context>
<context position="13802" citStr="Carpenter and Baldwin, 2008" startWordPosition="2202" endWordPosition="2205"> exact phrase extraction will give more benefit. 5.2 Search Query We process each TREC question and target 3 to identify named entities. Often, the target is a complete named entity (NE), however, in some of the TREC questions the target contains a named entity, e.g. tourists massacred at Luxor in 1997, or 1991 eruption of Mount Pinatubo with named entities Luxor and Mount Pinatubo. For the TREC question What was the number of member nations of the U.N. in 2000?, the identified constituents and automatically constructed query are shown in Table 1. Named entities are identified using Lingpipe (Carpenter and Baldwin, 2008), which identifies named entities of type organization, location and person. Phrases are identified automatically using the NLTK toolkit (Bird et al., 2008). We extract noun phrases, verb phrases and prepositional phrases. The rules for identifying phrases are mined from a dataset of manually annotated parse trees (Judge et al., 2006) 4. Converted Qphrases are heuristically created phrases that paraphrase the question in declarative form using a small set of rules. The rules match a question to a pattern and transform the question using linguistic information. For example, one rule matches Who</context>
</contexts>
<marker>Carpenter, Baldwin, 2008</marker>
<rawString>Carpenter, B. and B. Baldwin. 2008. Lingpipe. http://alias-i.com/lingpipe/index.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Chu-Carroll</author>
<author>J Prager</author>
<author>K Czuba</author>
<author>D Ferrucci</author>
<author>P Duboue</author>
</authors>
<title>Semantic search via XML fragments: a high-precision approach to IR. In</title>
<date>2006</date>
<booktitle>Proc. SIGIR.</booktitle>
<contexts>
<context position="6965" citStr="Chu-Carroll et al., 2006" startWordPosition="1078" endWordPosition="1081"> answer passage. In this work we perform a similar analysis automatically. We compare frequencies of phrases and words matching between a question and candidate sentences. Query expansion has been investigated in systems described in (Hovy et al., 2001a; Harabagiu et al., 2006). They use WordNet (Miller, 1995) for query expansion, and incorporate semantic roles in the answer extraction process. In this experiment we do not expand query terms. Corpus pre-processing and encoding information useful for retrieval was shown to improve document retrieval (Katz and Lin, 2003; Harabagiu et al., 2006; Chu-Carroll et al., 2006). In our approach we evaluate linguistic question processing technique which does not require corpus preprocessing. Statistical machine translation model is used for information retrieval by (Murdock and Croft, 2005). The model estimates probability of a question given an answer and is trained on &lt;question, candidate sentence&gt; pairs. It capturing synonymy and grammar transformations using a statistical model. 3 Data In this work we evaluate our question answering system on two datasets: the AQUAINT corpus, a 3 gigabyte collection of news documents used in the TREC 2006 competition; and the Web</context>
<context position="12157" citStr="Chu-Carroll et al., 2006" startWordPosition="1927" endWordPosition="1930">was the number of member nations of the U.N. in 2000 and NE “United Nations”, ”member nations of the u.n.” Cascaded web query query1 “member nations of the U.N. in 2000” AND ( United Nations ) query2 ”member nations of the u.n.” AND ( United Nations ) query3 (number of member nations of the U.N. in 2000) AND ( United Nations ) query4 ( United Nations ) Table 1: Question processing example: terms of a query lations in a question and a candidate sentence are known to improve overall QA system performance (Prager et al., 2000; Stenchikova et al., 2006; Katz and Lin, 2003; Harabagiu et al., 2006; Chu-Carroll et al., 2006). In this work we analyze less resource expensive techniques, such as chunking and named entity detection, for IR in question answering. Linguistic analysis in our system is applied to questions and to candidate sentences only. There is no need for annotation of all documents to be indexed, so our techniques can be applied to IR on large datasets such as the Web. Intuitively, using phrases in query construction may improve retrieval precision. For example, if we search for In what year did the movie win academy awards? using a disjunction of words as our query we may match irrelevant documents</context>
</contexts>
<marker>Chu-Carroll, Prager, Czuba, Ferrucci, Duboue, 2006</marker>
<rawString>Chu-Carroll, J., J. Prager, K. Czuba, D. Ferrucci, and P. Duboue. 2006. Semantic search via XML fragments: a high-precision approach to IR. In Proc. SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Clarke</author>
<author>G Cormack</author>
<author>D Kisman</author>
<author>T Lynam</author>
</authors>
<title>Question answering by passage selection (multitext experiments for TREC-9). In</title>
<date>2000</date>
<booktitle>Proc. TREC.</booktitle>
<contexts>
<context position="5048" citStr="Clarke et al., 2000" startWordPosition="786" endWordPosition="789"> In Section 2, we summarize recent approaches to question answering. In Section 3, we describe the dataset used in this experiment. In Section 5, we describe our method and data analysis. In Section 4, we outline the architecture of our question answering system. In Section 6, we describe our experiments and present our results. We summarize in Section 7. 2 Related Work Information retrieval (IR) for question answering consists of 2 steps: document retrieval and passage retrieval. Approaches to passage retrieval include simple word overlap (Light et al., 2001), densitybased passage retrieval (Clarke et al., 2000), retrieval based on the inverse document frequency (IDF) of matched and mismatched words (Ittycheriah et al., 2001), cosine similarity between a question and a passage (Llopis and Vicedo, 2001), passage/sentence ranking by weighting different features (Lee and others, 2001), stemming and morphological query expansion (2004), and voting between different retrieval methods (Tellex et al., 2003). As in previous approaches, we use words and phrases from a question for passage extraction and experiment with using exactly matched phrases in addition to words. Similarly to Lee (2001), we assign weig</context>
</contexts>
<marker>Clarke, Cormack, Kisman, Lynam, 2000</marker>
<rawString>Clarke, C., G. Cormack, D. Kisman, and T. Lynam. 2000. Question answering by passage selection (multitext experiments for TREC-9). In Proc. TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Collins-Thompson</author>
<author>J Callan</author>
<author>E Terra</author>
<author>C L A Clarke</author>
</authors>
<title>The effect of document retrieval quality on factoid question answering performance.</title>
<date>2004</date>
<booktitle>In Proc. SIGIR.</booktitle>
<contexts>
<context position="3131" citStr="Collins-Thompson et al. (2004)" startWordPosition="479" endWordPosition="482">ply linguistic processing to the question, identifying named entities and other query-relevant phrases. Others (Hovy et al., 2001b) use ontologies to expand query terms with synonyms and hypernyms. IR system recall is very important for question answering. If no correct answers are present in a document, no further processing will be able to find an answer. IR system precision and ranking of candidate passages can also affect question answering performance. If a sentence without a correct answer is ranked highly, answer extraction may extract incorrect answers from these erroneous candidates. Collins-Thompson et al. (2004) show that there is a consistent relationship between the quality of document retrieval and the overall performance of question answering systems. In this work we evaluate the use of exact phrases from a question in document and passage retrieval. First, we analyze how different parts of a question contribute to the performance of the sentence extraction stage of question answering. We ana9 Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering (IR4QA), pages 9–16 Manchester, UK. August 2008 lyze the match between linguistic constituents of different types</context>
</contexts>
<marker>Collins-Thompson, Callan, Terra, Clarke, 2004</marker>
<rawString>Collins-Thompson, K., J. Callan, E. Terra, and C. L.A. Clarke. 2004. The effect of document retrieval quality on factoid question answering performance. In Proc. SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Dang</author>
<author>J Lin</author>
<author>D Kelly</author>
</authors>
<title>question answering track.</title>
<date>2006</date>
<journal>Overview of the TREC</journal>
<booktitle>In Proc. TREC.</booktitle>
<contexts>
<context position="1835" citStr="Dang et al., 2006" startWordPosition="275" endWordPosition="278">automatically generates a search query from a natural language question and finds a concise answer from a set of documents. In the opendomain factoid question answering task systems answer general questions like Who is the creator of The Daily Show?, or When was Mozart born?. A variety of approaches to question answering have been investigated in TREC competitions in the last © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. decade from (Vorhees and Harman, 1999) to (Dang et al., 2006). Most existing question answering systems add question analysis, sentence retrieval and answer extraction components to an IR system. Since information retrieval is the first stage of question answering, its performance is an upper bound on the overall question answering system’s performance. IR performance depends on the quality of document indexing and query construction. Question answering systems create a search query automatically from a user’s question, through various levels of sophistication. The simplest way of creating a query is to treat the words in the question as the terms in th</context>
</contexts>
<marker>Dang, Lin, Kelly, 2006</marker>
<rawString>Dang, H., J. Lin, and D. Kelly. 2006. Overview of the TREC 2006 question answering track. In Proc. TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Harabagiu</author>
<author>A Hickl</author>
<author>J Williams</author>
<author>J Bensley</author>
<author>K Roberts</author>
<author>Y Shi</author>
<author>B Rink</author>
</authors>
<title>Question answering with LCC’s CHAUCER at TREC</title>
<date>2006</date>
<booktitle>In Proc. TREC.</booktitle>
<contexts>
<context position="6618" citStr="Harabagiu et al., 2006" startWordPosition="1024" endWordPosition="1027">raction because our system’s semantic role labeling-based answer extraction functions on individual sentences. White and Sutcliffe (2004) performed a manual analysis of questions and answers for 50 of the TREC questions. The authors computed frequency of terms matching exactly, with morphological, or semantic variation between a question and a answer passage. In this work we perform a similar analysis automatically. We compare frequencies of phrases and words matching between a question and candidate sentences. Query expansion has been investigated in systems described in (Hovy et al., 2001a; Harabagiu et al., 2006). They use WordNet (Miller, 1995) for query expansion, and incorporate semantic roles in the answer extraction process. In this experiment we do not expand query terms. Corpus pre-processing and encoding information useful for retrieval was shown to improve document retrieval (Katz and Lin, 2003; Harabagiu et al., 2006; Chu-Carroll et al., 2006). In our approach we evaluate linguistic question processing technique which does not require corpus preprocessing. Statistical machine translation model is used for information retrieval by (Murdock and Croft, 2005). The model estimates probability of </context>
<context position="12130" citStr="Harabagiu et al., 2006" startWordPosition="1923" endWordPosition="1926">cene Query with phrases was the number of member nations of the U.N. in 2000 and NE “United Nations”, ”member nations of the u.n.” Cascaded web query query1 “member nations of the U.N. in 2000” AND ( United Nations ) query2 ”member nations of the u.n.” AND ( United Nations ) query3 (number of member nations of the U.N. in 2000) AND ( United Nations ) query4 ( United Nations ) Table 1: Question processing example: terms of a query lations in a question and a candidate sentence are known to improve overall QA system performance (Prager et al., 2000; Stenchikova et al., 2006; Katz and Lin, 2003; Harabagiu et al., 2006; Chu-Carroll et al., 2006). In this work we analyze less resource expensive techniques, such as chunking and named entity detection, for IR in question answering. Linguistic analysis in our system is applied to questions and to candidate sentences only. There is no need for annotation of all documents to be indexed, so our techniques can be applied to IR on large datasets such as the Web. Intuitively, using phrases in query construction may improve retrieval precision. For example, if we search for In what year did the movie win academy awards? using a disjunction of words as our query we may</context>
</contexts>
<marker>Harabagiu, Hickl, Williams, Bensley, Roberts, Shi, Rink, 2006</marker>
<rawString>Harabagiu, S., A. Hickl, J. Williams, J. Bensley, K. Roberts, Y. Shi, and B. Rink. 2006. Question answering with LCC’s CHAUCER at TREC 2006. In Proc. TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hovy</author>
<author>L Gerber</author>
<author>U Hermjakob</author>
<author>M Junk</author>
<author>C-Y Lin</author>
</authors>
<title>Question answering in Webclopedia. In</title>
<date>2001</date>
<booktitle>Proc. TREC.</booktitle>
<contexts>
<context position="2630" citStr="Hovy et al., 2001" startWordPosition="398" endWordPosition="401">age of question answering, its performance is an upper bound on the overall question answering system’s performance. IR performance depends on the quality of document indexing and query construction. Question answering systems create a search query automatically from a user’s question, through various levels of sophistication. The simplest way of creating a query is to treat the words in the question as the terms in the query. Some question answering systems (Srihari and Li, 1999) apply linguistic processing to the question, identifying named entities and other query-relevant phrases. Others (Hovy et al., 2001b) use ontologies to expand query terms with synonyms and hypernyms. IR system recall is very important for question answering. If no correct answers are present in a document, no further processing will be able to find an answer. IR system precision and ranking of candidate passages can also affect question answering performance. If a sentence without a correct answer is ranked highly, answer extraction may extract incorrect answers from these erroneous candidates. Collins-Thompson et al. (2004) show that there is a consistent relationship between the quality of document retrieval and the ove</context>
<context position="6592" citStr="Hovy et al., 2001" startWordPosition="1020" endWordPosition="1023"> single sentence extraction because our system’s semantic role labeling-based answer extraction functions on individual sentences. White and Sutcliffe (2004) performed a manual analysis of questions and answers for 50 of the TREC questions. The authors computed frequency of terms matching exactly, with morphological, or semantic variation between a question and a answer passage. In this work we perform a similar analysis automatically. We compare frequencies of phrases and words matching between a question and candidate sentences. Query expansion has been investigated in systems described in (Hovy et al., 2001a; Harabagiu et al., 2006). They use WordNet (Miller, 1995) for query expansion, and incorporate semantic roles in the answer extraction process. In this experiment we do not expand query terms. Corpus pre-processing and encoding information useful for retrieval was shown to improve document retrieval (Katz and Lin, 2003; Harabagiu et al., 2006; Chu-Carroll et al., 2006). In our approach we evaluate linguistic question processing technique which does not require corpus preprocessing. Statistical machine translation model is used for information retrieval by (Murdock and Croft, 2005). The model</context>
</contexts>
<marker>Hovy, Gerber, Hermjakob, Junk, Lin, 2001</marker>
<rawString>Hovy, E., L. Gerber, U. Hermjakob, M. Junk, and C.-Y. Lin. 2001a. Question answering in Webclopedia. In Proc. TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hovy</author>
<author>U Hermjakob</author>
<author>C-Y Lin</author>
</authors>
<title>The use of external knowledge in factoid QA.</title>
<date>2001</date>
<booktitle>In Proc. TREC.</booktitle>
<contexts>
<context position="2630" citStr="Hovy et al., 2001" startWordPosition="398" endWordPosition="401">age of question answering, its performance is an upper bound on the overall question answering system’s performance. IR performance depends on the quality of document indexing and query construction. Question answering systems create a search query automatically from a user’s question, through various levels of sophistication. The simplest way of creating a query is to treat the words in the question as the terms in the query. Some question answering systems (Srihari and Li, 1999) apply linguistic processing to the question, identifying named entities and other query-relevant phrases. Others (Hovy et al., 2001b) use ontologies to expand query terms with synonyms and hypernyms. IR system recall is very important for question answering. If no correct answers are present in a document, no further processing will be able to find an answer. IR system precision and ranking of candidate passages can also affect question answering performance. If a sentence without a correct answer is ranked highly, answer extraction may extract incorrect answers from these erroneous candidates. Collins-Thompson et al. (2004) show that there is a consistent relationship between the quality of document retrieval and the ove</context>
<context position="6592" citStr="Hovy et al., 2001" startWordPosition="1020" endWordPosition="1023"> single sentence extraction because our system’s semantic role labeling-based answer extraction functions on individual sentences. White and Sutcliffe (2004) performed a manual analysis of questions and answers for 50 of the TREC questions. The authors computed frequency of terms matching exactly, with morphological, or semantic variation between a question and a answer passage. In this work we perform a similar analysis automatically. We compare frequencies of phrases and words matching between a question and candidate sentences. Query expansion has been investigated in systems described in (Hovy et al., 2001a; Harabagiu et al., 2006). They use WordNet (Miller, 1995) for query expansion, and incorporate semantic roles in the answer extraction process. In this experiment we do not expand query terms. Corpus pre-processing and encoding information useful for retrieval was shown to improve document retrieval (Katz and Lin, 2003; Harabagiu et al., 2006; Chu-Carroll et al., 2006). In our approach we evaluate linguistic question processing technique which does not require corpus preprocessing. Statistical machine translation model is used for information retrieval by (Murdock and Croft, 2005). The model</context>
</contexts>
<marker>Hovy, Hermjakob, Lin, 2001</marker>
<rawString>Hovy, E., U. Hermjakob, and C.-Y. Lin. 2001b. The use of external knowledge in factoid QA. In Proc. TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ittycheriah</author>
<author>M Franz</author>
<author>S Roukos</author>
</authors>
<title>IBM’s statistical question answering system – TREC-10. In</title>
<date>2001</date>
<booktitle>Proc. TREC.</booktitle>
<contexts>
<context position="5164" citStr="Ittycheriah et al., 2001" startWordPosition="804" endWordPosition="808"> in this experiment. In Section 5, we describe our method and data analysis. In Section 4, we outline the architecture of our question answering system. In Section 6, we describe our experiments and present our results. We summarize in Section 7. 2 Related Work Information retrieval (IR) for question answering consists of 2 steps: document retrieval and passage retrieval. Approaches to passage retrieval include simple word overlap (Light et al., 2001), densitybased passage retrieval (Clarke et al., 2000), retrieval based on the inverse document frequency (IDF) of matched and mismatched words (Ittycheriah et al., 2001), cosine similarity between a question and a passage (Llopis and Vicedo, 2001), passage/sentence ranking by weighting different features (Lee and others, 2001), stemming and morphological query expansion (2004), and voting between different retrieval methods (Tellex et al., 2003). As in previous approaches, we use words and phrases from a question for passage extraction and experiment with using exactly matched phrases in addition to words. Similarly to Lee (2001), we assign weights to sentences in retrieved documents according to the number of matched constituents. Systems vary in the size of</context>
</contexts>
<marker>Ittycheriah, Franz, Roukos, 2001</marker>
<rawString>Ittycheriah, A., M. Franz, and S. Roukos. 2001. IBM’s statistical question answering system – TREC-10. In Proc. TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Judge</author>
<author>A Cahill</author>
<author>J van Genabith</author>
</authors>
<title>QuestionBank: Creating a corpus of parse-annotated questions.</title>
<date>2006</date>
<booktitle>In Proc. ACL.</booktitle>
<marker>Judge, Cahill, van Genabith, 2006</marker>
<rawString>Judge, J., A. Cahill, and J. van Genabith. 2006. QuestionBank: Creating a corpus of parse-annotated questions. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Katz</author>
<author>J Lin</author>
</authors>
<title>Selectively using relations to improve precision in question answering.</title>
<date>2003</date>
<booktitle>In Proc. of the EACL Workshop on Natural Language Processing for Question Answering.</booktitle>
<contexts>
<context position="6914" citStr="Katz and Lin, 2003" startWordPosition="1070" endWordPosition="1073"> semantic variation between a question and a answer passage. In this work we perform a similar analysis automatically. We compare frequencies of phrases and words matching between a question and candidate sentences. Query expansion has been investigated in systems described in (Hovy et al., 2001a; Harabagiu et al., 2006). They use WordNet (Miller, 1995) for query expansion, and incorporate semantic roles in the answer extraction process. In this experiment we do not expand query terms. Corpus pre-processing and encoding information useful for retrieval was shown to improve document retrieval (Katz and Lin, 2003; Harabagiu et al., 2006; Chu-Carroll et al., 2006). In our approach we evaluate linguistic question processing technique which does not require corpus preprocessing. Statistical machine translation model is used for information retrieval by (Murdock and Croft, 2005). The model estimates probability of a question given an answer and is trained on &lt;question, candidate sentence&gt; pairs. It capturing synonymy and grammar transformations using a statistical model. 3 Data In this work we evaluate our question answering system on two datasets: the AQUAINT corpus, a 3 gigabyte collection of news docum</context>
<context position="12106" citStr="Katz and Lin, 2003" startWordPosition="1919" endWordPosition="1922">00 United Nations Lucene Query with phrases was the number of member nations of the U.N. in 2000 and NE “United Nations”, ”member nations of the u.n.” Cascaded web query query1 “member nations of the U.N. in 2000” AND ( United Nations ) query2 ”member nations of the u.n.” AND ( United Nations ) query3 (number of member nations of the U.N. in 2000) AND ( United Nations ) query4 ( United Nations ) Table 1: Question processing example: terms of a query lations in a question and a candidate sentence are known to improve overall QA system performance (Prager et al., 2000; Stenchikova et al., 2006; Katz and Lin, 2003; Harabagiu et al., 2006; Chu-Carroll et al., 2006). In this work we analyze less resource expensive techniques, such as chunking and named entity detection, for IR in question answering. Linguistic analysis in our system is applied to questions and to candidate sentences only. There is no need for annotation of all documents to be indexed, so our techniques can be applied to IR on large datasets such as the Web. Intuitively, using phrases in query construction may improve retrieval precision. For example, if we search for In what year did the movie win academy awards? using a disjunction of w</context>
</contexts>
<marker>Katz, Lin, 2003</marker>
<rawString>Katz, B. and J. Lin. 2003. Selectively using relations to improve precision in question answering. In Proc. of the EACL Workshop on Natural Language Processing for Question Answering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G G Lee</author>
</authors>
<title>SiteQ: Engineering high performance QA system using lexico-semantic pattern matching and shallow NLP.</title>
<date>2001</date>
<booktitle>In Proc. TREC.</booktitle>
<contexts>
<context position="5632" citStr="Lee (2001)" startWordPosition="877" endWordPosition="878">eval (Clarke et al., 2000), retrieval based on the inverse document frequency (IDF) of matched and mismatched words (Ittycheriah et al., 2001), cosine similarity between a question and a passage (Llopis and Vicedo, 2001), passage/sentence ranking by weighting different features (Lee and others, 2001), stemming and morphological query expansion (2004), and voting between different retrieval methods (Tellex et al., 2003). As in previous approaches, we use words and phrases from a question for passage extraction and experiment with using exactly matched phrases in addition to words. Similarly to Lee (2001), we assign weights to sentences in retrieved documents according to the number of matched constituents. Systems vary in the size of retrieved passages. Some systems identify multi-sentence and variable size passages (Ittycheriah et al., 2001; Clarke et al., 2000). An optimal passage size may depend on the method of answer extraction. We use single sentence extraction because our system’s semantic role labeling-based answer extraction functions on individual sentences. White and Sutcliffe (2004) performed a manual analysis of questions and answers for 50 of the TREC questions. The authors comp</context>
</contexts>
<marker>Lee, 2001</marker>
<rawString>Lee, G. G. et al. 2001. SiteQ: Engineering high performance QA system using lexico-semantic pattern matching and shallow NLP. In Proc. TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Light</author>
<author>G S Mann</author>
<author>E Riloff</author>
<author>E Breck</author>
</authors>
<title>Analyses for elucidating current question answering technology.</title>
<date>2001</date>
<journal>Journal ofNatural Language Engineering,</journal>
<volume>7</volume>
<issue>4</issue>
<contexts>
<context position="4994" citStr="Light et al., 2001" startWordPosition="778" endWordPosition="781">ction. The rest of the paper is organized as follows: In Section 2, we summarize recent approaches to question answering. In Section 3, we describe the dataset used in this experiment. In Section 5, we describe our method and data analysis. In Section 4, we outline the architecture of our question answering system. In Section 6, we describe our experiments and present our results. We summarize in Section 7. 2 Related Work Information retrieval (IR) for question answering consists of 2 steps: document retrieval and passage retrieval. Approaches to passage retrieval include simple word overlap (Light et al., 2001), densitybased passage retrieval (Clarke et al., 2000), retrieval based on the inverse document frequency (IDF) of matched and mismatched words (Ittycheriah et al., 2001), cosine similarity between a question and a passage (Llopis and Vicedo, 2001), passage/sentence ranking by weighting different features (Lee and others, 2001), stemming and morphological query expansion (2004), and voting between different retrieval methods (Tellex et al., 2003). As in previous approaches, we use words and phrases from a question for passage extraction and experiment with using exactly matched phrases in addi</context>
</contexts>
<marker>Light, Mann, Riloff, Breck, 2001</marker>
<rawString>Light, M., G. S. Mann, E. Riloff, and E. Breck. 2001. Analyses for elucidating current question answering technology. Journal ofNatural Language Engineering, 7(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Llopis</author>
<author>J L Vicedo</author>
</authors>
<title>IR-n: A passage retrieval system at CLEF-2001.</title>
<date>2001</date>
<booktitle>In Proc. of the Second Workshop of the Cross-Language Evaluation Forum (CLEF</booktitle>
<contexts>
<context position="5242" citStr="Llopis and Vicedo, 2001" startWordPosition="817" endWordPosition="820"> Section 4, we outline the architecture of our question answering system. In Section 6, we describe our experiments and present our results. We summarize in Section 7. 2 Related Work Information retrieval (IR) for question answering consists of 2 steps: document retrieval and passage retrieval. Approaches to passage retrieval include simple word overlap (Light et al., 2001), densitybased passage retrieval (Clarke et al., 2000), retrieval based on the inverse document frequency (IDF) of matched and mismatched words (Ittycheriah et al., 2001), cosine similarity between a question and a passage (Llopis and Vicedo, 2001), passage/sentence ranking by weighting different features (Lee and others, 2001), stemming and morphological query expansion (2004), and voting between different retrieval methods (Tellex et al., 2003). As in previous approaches, we use words and phrases from a question for passage extraction and experiment with using exactly matched phrases in addition to words. Similarly to Lee (2001), we assign weights to sentences in retrieved documents according to the number of matched constituents. Systems vary in the size of retrieved passages. Some systems identify multi-sentence and variable size pa</context>
</contexts>
<marker>Llopis, Vicedo, 2001</marker>
<rawString>Llopis, F. and J. L. Vicedo. 2001. IR-n: A passage retrieval system at CLEF-2001. In Proc. of the Second Workshop of the Cross-Language Evaluation Forum (CLEF 2001).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Lloyd</author>
<author>D Kechagias</author>
<author>S Skiena</author>
</authors>
<title>Lydia: A system for large-scale news analysis.</title>
<date>2005</date>
<booktitle>In Proc. SPIRE,</booktitle>
<pages>161--166</pages>
<contexts>
<context position="10441" citStr="Lloyd et al., 2005" startWordPosition="1638" endWordPosition="1641">dNet (Miller, 1995). Our system can currently retrieve documents from either the Web (using the Yahoo search API (Yahoo!, 2008)), or the AQUAINT corpus (Graff, 2002) (through the Lucene indexer and search engine (Apache, 2004 2008)). When using Lucene, we can assign different weights to different types of search term (e.g. less weight to terms than to named entities added to a query) (cf. (Lee and others, 2001)). We currently have two modules for answer extraction, which can be used separately or together. Candidate sentences can be tagged with named entity information using the Lydia system (Lloyd et al., 2005). The tagged word/phrase matching the target named entity type most frequently found is chosen as the answer. Our system can also extract answers through semantic role labeling, using the SRL toolkit from (Punyakanok et al., 2008). In this case, the tagged word/phrase matching the target semantic role most frequently found is chosen as the answer. Figure 1: Architecutre of our question answering system 5 Method 5.1 Motivation Question answering is an engineering-intensive task. System performance improves as more sophisticated techniques are applied to data processing. For example, the IR stag</context>
</contexts>
<marker>Lloyd, Kechagias, Skiena, 2005</marker>
<rawString>Lloyd, L., D. Kechagias, and S. Skiena. 2005. Lydia: A system for large-scale news analysis. In Proc. SPIRE, pages 161–166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>WordNet: a lexical database for english.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="6651" citStr="Miller, 1995" startWordPosition="1031" endWordPosition="1032">labeling-based answer extraction functions on individual sentences. White and Sutcliffe (2004) performed a manual analysis of questions and answers for 50 of the TREC questions. The authors computed frequency of terms matching exactly, with morphological, or semantic variation between a question and a answer passage. In this work we perform a similar analysis automatically. We compare frequencies of phrases and words matching between a question and candidate sentences. Query expansion has been investigated in systems described in (Hovy et al., 2001a; Harabagiu et al., 2006). They use WordNet (Miller, 1995) for query expansion, and incorporate semantic roles in the answer extraction process. In this experiment we do not expand query terms. Corpus pre-processing and encoding information useful for retrieval was shown to improve document retrieval (Katz and Lin, 2003; Harabagiu et al., 2006; Chu-Carroll et al., 2006). In our approach we evaluate linguistic question processing technique which does not require corpus preprocessing. Statistical machine translation model is used for information retrieval by (Murdock and Croft, 2005). The model estimates probability of a question given an answer and is</context>
<context position="9841" citStr="Miller, 1995" startWordPosition="1541" endWordPosition="1542">illustrated in Figure 1: question analysis, document and sentence extraction (IR), and answer extraction. After the user poses a question, it is analyzed. Target named entities and semantic roles are determined. A query is constructed, tailored to the search tools in use. Sentences containing target terms are then extracted from the documents retrieved by the query. The candidate sentences are processed to identify and extract candidate answers, which are presented to the user. We use the NLTK toolkit (Bird et al., 2008) for question analysis and can add terms to search queries using WordNet (Miller, 1995). Our system can currently retrieve documents from either the Web (using the Yahoo search API (Yahoo!, 2008)), or the AQUAINT corpus (Graff, 2002) (through the Lucene indexer and search engine (Apache, 2004 2008)). When using Lucene, we can assign different weights to different types of search term (e.g. less weight to terms than to named entities added to a query) (cf. (Lee and others, 2001)). We currently have two modules for answer extraction, which can be used separately or together. Candidate sentences can be tagged with named entity information using the Lydia system (Lloyd et al., 2005)</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>Miller, George A. 1995. WordNet: a lexical database for english. Communications of the ACM, 38(11).</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Murdock</author>
<author>W B Croft</author>
</authors>
<title>Simple translation models for sentence retrieval in factoid question answering.</title>
<date>2005</date>
<booktitle>In Proc. SIGIR.</booktitle>
<contexts>
<context position="7181" citStr="Murdock and Croft, 2005" startWordPosition="1109" endWordPosition="1112">ems described in (Hovy et al., 2001a; Harabagiu et al., 2006). They use WordNet (Miller, 1995) for query expansion, and incorporate semantic roles in the answer extraction process. In this experiment we do not expand query terms. Corpus pre-processing and encoding information useful for retrieval was shown to improve document retrieval (Katz and Lin, 2003; Harabagiu et al., 2006; Chu-Carroll et al., 2006). In our approach we evaluate linguistic question processing technique which does not require corpus preprocessing. Statistical machine translation model is used for information retrieval by (Murdock and Croft, 2005). The model estimates probability of a question given an answer and is trained on &lt;question, candidate sentence&gt; pairs. It capturing synonymy and grammar transformations using a statistical model. 3 Data In this work we evaluate our question answering system on two datasets: the AQUAINT corpus, a 3 gigabyte collection of news documents used in the TREC 2006 competition; and the Web. We use questions from TREC, a yearly question answering competition. We use a subset of questions with non-empty answers 1 from the TREC 2006 dataset 2. The dataset provides a list of matching documents from the AQ</context>
</contexts>
<marker>Murdock, Croft, 2005</marker>
<rawString>Murdock, V. and W. B. Croft. 2005. Simple translation models for sentence retrieval in factoid question answering. In Proc. SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Prager</author>
<author>E Brown</author>
<author>A Coden</author>
</authors>
<title>Questionanswering by predictive annotation.</title>
<date>2000</date>
<booktitle>In ACM SIGIR. QA</booktitle>
<note>to site.</note>
<contexts>
<context position="12060" citStr="Prager et al., 2000" startWordPosition="1911" endWordPosition="1914"> the number of member nations of the U.N. in 2000 United Nations Lucene Query with phrases was the number of member nations of the U.N. in 2000 and NE “United Nations”, ”member nations of the u.n.” Cascaded web query query1 “member nations of the U.N. in 2000” AND ( United Nations ) query2 ”member nations of the u.n.” AND ( United Nations ) query3 (number of member nations of the U.N. in 2000) AND ( United Nations ) query4 ( United Nations ) Table 1: Question processing example: terms of a query lations in a question and a candidate sentence are known to improve overall QA system performance (Prager et al., 2000; Stenchikova et al., 2006; Katz and Lin, 2003; Harabagiu et al., 2006; Chu-Carroll et al., 2006). In this work we analyze less resource expensive techniques, such as chunking and named entity detection, for IR in question answering. Linguistic analysis in our system is applied to questions and to candidate sentences only. There is no need for annotation of all documents to be indexed, so our techniques can be applied to IR on large datasets such as the Web. Intuitively, using phrases in query construction may improve retrieval precision. For example, if we search for In what year did the movi</context>
</contexts>
<marker>Prager, Brown, Coden, 2000</marker>
<rawString>Prager, J., E. Brown, and A. Coden. 2000. Questionanswering by predictive annotation. In ACM SIGIR. QA -to site.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Punyakanok</author>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>The importance of syntactic parsing and inference in semantic role labeling.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="10671" citStr="Punyakanok et al., 2008" startWordPosition="1674" endWordPosition="1677"> 2008)). When using Lucene, we can assign different weights to different types of search term (e.g. less weight to terms than to named entities added to a query) (cf. (Lee and others, 2001)). We currently have two modules for answer extraction, which can be used separately or together. Candidate sentences can be tagged with named entity information using the Lydia system (Lloyd et al., 2005). The tagged word/phrase matching the target named entity type most frequently found is chosen as the answer. Our system can also extract answers through semantic role labeling, using the SRL toolkit from (Punyakanok et al., 2008). In this case, the tagged word/phrase matching the target semantic role most frequently found is chosen as the answer. Figure 1: Architecutre of our question answering system 5 Method 5.1 Motivation Question answering is an engineering-intensive task. System performance improves as more sophisticated techniques are applied to data processing. For example, the IR stage in question answering is shown to improve with the help of techniques like predictive annotations and relation extraction; matching of semantic and syntactic re11 Target United Nations Question What was the number of member nati</context>
</contexts>
<marker>Punyakanok, Roth, Yih, 2008</marker>
<rawString>Punyakanok, V., D. Roth, and W. Yih. 2008. The importance of syntactic parsing and inference in semantic role labeling. Computational Linguistics, 34(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Srihari</author>
<author>W Li</author>
</authors>
<title>Information extraction supported question answering.</title>
<date>1999</date>
<booktitle>In Proc. TREC.</booktitle>
<contexts>
<context position="2498" citStr="Srihari and Li, 1999" startWordPosition="379" endWordPosition="382"> add question analysis, sentence retrieval and answer extraction components to an IR system. Since information retrieval is the first stage of question answering, its performance is an upper bound on the overall question answering system’s performance. IR performance depends on the quality of document indexing and query construction. Question answering systems create a search query automatically from a user’s question, through various levels of sophistication. The simplest way of creating a query is to treat the words in the question as the terms in the query. Some question answering systems (Srihari and Li, 1999) apply linguistic processing to the question, identifying named entities and other query-relevant phrases. Others (Hovy et al., 2001b) use ontologies to expand query terms with synonyms and hypernyms. IR system recall is very important for question answering. If no correct answers are present in a document, no further processing will be able to find an answer. IR system precision and ranking of candidate passages can also affect question answering performance. If a sentence without a correct answer is ranked highly, answer extraction may extract incorrect answers from these erroneous candidate</context>
</contexts>
<marker>Srihari, Li, 1999</marker>
<rawString>Srihari, R. and W. Li. 1999. Information extraction supported question answering. In Proc. TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Tellex</author>
<author>B Katz</author>
<author>J Lin</author>
<author>A Fernandes</author>
<author>G Marton</author>
</authors>
<title>Quantitative evaluation of passage retrieval algorithms for question answering. In</title>
<date>2003</date>
<booktitle>Proc. SIGIR.</booktitle>
<contexts>
<context position="5444" citStr="Tellex et al., 2003" startWordPosition="844" endWordPosition="847">IR) for question answering consists of 2 steps: document retrieval and passage retrieval. Approaches to passage retrieval include simple word overlap (Light et al., 2001), densitybased passage retrieval (Clarke et al., 2000), retrieval based on the inverse document frequency (IDF) of matched and mismatched words (Ittycheriah et al., 2001), cosine similarity between a question and a passage (Llopis and Vicedo, 2001), passage/sentence ranking by weighting different features (Lee and others, 2001), stemming and morphological query expansion (2004), and voting between different retrieval methods (Tellex et al., 2003). As in previous approaches, we use words and phrases from a question for passage extraction and experiment with using exactly matched phrases in addition to words. Similarly to Lee (2001), we assign weights to sentences in retrieved documents according to the number of matched constituents. Systems vary in the size of retrieved passages. Some systems identify multi-sentence and variable size passages (Ittycheriah et al., 2001; Clarke et al., 2000). An optimal passage size may depend on the method of answer extraction. We use single sentence extraction because our system’s semantic role labeli</context>
</contexts>
<marker>Tellex, Katz, Lin, Fernandes, Marton, 2003</marker>
<rawString>Tellex, S., B. Katz, J. Lin, A. Fernandes, and G. Marton. 2003. Quantitative evaluation of passage retrieval algorithms for question answering. In Proc. SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Vorhees</author>
<author>D Harman</author>
</authors>
<date>1999</date>
<booktitle>Overview of the eighth Text REtrieval Conference (TREC-8). In ”Proc. TREC”.</booktitle>
<contexts>
<context position="1812" citStr="Vorhees and Harman, 1999" startWordPosition="270" endWordPosition="273">eval (IR) task where a system automatically generates a search query from a natural language question and finds a concise answer from a set of documents. In the opendomain factoid question answering task systems answer general questions like Who is the creator of The Daily Show?, or When was Mozart born?. A variety of approaches to question answering have been investigated in TREC competitions in the last © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. decade from (Vorhees and Harman, 1999) to (Dang et al., 2006). Most existing question answering systems add question analysis, sentence retrieval and answer extraction components to an IR system. Since information retrieval is the first stage of question answering, its performance is an upper bound on the overall question answering system’s performance. IR performance depends on the quality of document indexing and query construction. Question answering systems create a search query automatically from a user’s question, through various levels of sophistication. The simplest way of creating a query is to treat the words in the ques</context>
</contexts>
<marker>Vorhees, Harman, 1999</marker>
<rawString>Vorhees, V. and D. Harman. 1999. Overview of the eighth Text REtrieval Conference (TREC-8). In ”Proc. TREC”.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K White</author>
<author>R Sutcliffe</author>
</authors>
<title>Seeking an upper bound to sentence level retrieval in question answering.</title>
<date>2004</date>
<booktitle>In Proc. SIGIR.</booktitle>
<contexts>
<context position="6132" citStr="White and Sutcliffe (2004)" startWordPosition="947" endWordPosition="950">uestion for passage extraction and experiment with using exactly matched phrases in addition to words. Similarly to Lee (2001), we assign weights to sentences in retrieved documents according to the number of matched constituents. Systems vary in the size of retrieved passages. Some systems identify multi-sentence and variable size passages (Ittycheriah et al., 2001; Clarke et al., 2000). An optimal passage size may depend on the method of answer extraction. We use single sentence extraction because our system’s semantic role labeling-based answer extraction functions on individual sentences. White and Sutcliffe (2004) performed a manual analysis of questions and answers for 50 of the TREC questions. The authors computed frequency of terms matching exactly, with morphological, or semantic variation between a question and a answer passage. In this work we perform a similar analysis automatically. We compare frequencies of phrases and words matching between a question and candidate sentences. Query expansion has been investigated in systems described in (Hovy et al., 2001a; Harabagiu et al., 2006). They use WordNet (Miller, 1995) for query expansion, and incorporate semantic roles in the answer extraction pro</context>
</contexts>
<marker>White, Sutcliffe, 2004</marker>
<rawString>White, K. and R. Sutcliffe. 2004. Seeking an upper bound to sentence level retrieval in question answering. In Proc. SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inc Yahoo</author>
</authors>
<date>2008</date>
<note>Yahoo! search API. http://developer.yahoo.com/search/.</note>
<marker>Yahoo, 2008</marker>
<rawString>Yahoo!, Inc. 2008. Yahoo! search API. http://developer.yahoo.com/search/.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>