<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.022129">
<title confidence="0.99876">
Identifying and Tracking Entity Mentions
in a Maximum Entropy Framework
</title>
<author confidence="0.996644">
A. Ittycheriah, L. Lita*, N. Kambhatla, N. Nicolov, S. Roukos, M. Stys
</author>
<affiliation confidence="0.985443">
I.B.M. T.J.Watson Research Center
</affiliation>
<address confidence="0.7257105">
P.O.Box 218, Route 134
Yorktown, NY 10598
</address>
<bodyText confidence="0.963568">
label , nanda , nicolas , roukos , smll@us ibm. com, *llita@cs cmu edu
</bodyText>
<sectionHeader confidence="0.977458" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999719214285714">
We present a system for identifying and tracking
named, nominal, and pronominal mentions of
entities within a text document. Our maximum
entropy model for mention detection combines
two pre-existing named entity taggers (built to
extract different entity categories) and other
syntactic and morphological feature streams to
achieve competitive performance. We developed
a novel maximum entropy model for tracking all
mentions of an entity within a document. We
participated in the Automatic Content Extraction
(ACE) evaluation and performed well. We de-
scribe our system and present results of the ACE
evaluation.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997873647058824">
We present a system for identifying entities in text. En-
tities are groups of mentions where mentions are textual
references to objects. Mentions have one of five types
(person, organization, geo-political entity, location, facil-
ity) and can be named (as in standard Named Entity (NE)
research), nominal and pronominal - the latter dimension
is called the level of a mention. Additionally, mentions
can be generic or specific. We break the original task into
mention detection (finding all mentions in the text and their
type, level and genericity) and mention tracking (combin-
ing mentions into groups of references to the same object
in the document).
Our work is motivated by the requirements of a NIST-
run evaluation on Automatic Content Extraction (ACE,
2002) where the goal is to build systems that detect enti-
ties (groups of mentions), relations among them and events
in which they participate. Our team took part in the En-
tity detection track. The ACE task is inherently different
and arguably harder than traditional named entity recogni-
tion, because of the complexity involved in extracting non-
named mentions and chaining them together with named
mentions.
We investigate maximum entropy models for both tasks.
For mention detection we use a maximum entropy frame-
work for learning semantic trees&apos; (corresponding to the
mentions) combining as features the output of two pre-
existing statistical NE taggers. These taggers have been
trained on different corpora using different categories (us-
ing 31 and 3 categories respectively).
In Section 2 we describe our mention detection com-
ponent. Section 3 presents a novel approach for deciding
when a mention will or will not be chained with previously
created groups of mentions. Section 4 gives the results of
our system from the last ACE evaluation.
</bodyText>
<sectionHeader confidence="0.941427" genericHeader="method">
2 Mention Detection
</sectionHeader>
<bodyText confidence="0.999867454545454">
We use a maximum entropy semantic parser for detecting
mentions. The labels of the tree nodes correspond to the
combination of type, level and genericity, giving rise to
30 = 5 x 3 x 2 categories for the learning framework.
We had two pre-existing statistical NE taggers (Hmm
and WINNOW) built with other applications in mind. Our
strategy was to combine the hypotheses of the existing NE
taggers (using their original models trained on different
training data and with different labels) in a MaxEnt frame-
work as well as use additional syntactic and semantic in-
formation.2
The underlying semantic parser (Ratnaparkhi, 1999)
works in three stages: POS tagging, chunking and struc-
ture building. During chunking (similar to bottom up pars-
ing) the next level of constutuent structure is discovered.
During structure building the rest of the tree is built. All
decisions are modeled using Maximum Entropy models.
The nature of mention detection puts most burden on the
chunking model. The chunking model features include:
unigrams of current word (w0), bigrams in w_1, w0, w+1,
trigrams in w_2, w—i wo, w+1, w+2, unigrams, bigrams,
trigrams on combinations of words and their POS tags in
</bodyText>
<footnote confidence="0.51928">
1 We refer to this as a semantic parser.
</footnote>
<bodyText confidence="0.992675857142857">
2 From an engineering perspective the particular way we take
diverse information into acount is by using multiple synchronized
streams as input to the MaxEnt semantic parser.
[-1,0, +11 window, the previous label, people and loca-
tion suffixes.
As additional features we used unigrams, bigrams and
trigrams on the output of two models. The first is from
an Hmm-based system implementing back-off strategies as
in BBN&apos;s NYMBLE system (Bikel et al., 1999). It uses
31 categories and is trained on a large corpus of 1.5 mil-
lion words. The system is developed as a component of a
question answering system (Ittycheriah, 2001). The sec-
ond system uses a generalized WINNOW approach (Zhang
et al., 2002). It takes additional features: POS tags, lists
of known locations, organizations, and person names. It is
trained on m uc7 data and a subset of the above corpus for
three common classes: person, location and organization.
Additional streams we used were: flags, gazetteers,
chunk, left corner and WordNet: flags specify capitaliza-
tion patterns (Bikel et al., 1999; Borthwick et al., 1998;
Zhou &amp; Su, 2002); the gazetteer stream indicates presence
of a word in lists; chunk states the label of the mother node
of each preterminal in the parse tree3; left corner specifies
whether the current word is inside an NP and the identity of
the leftmost leaf if it has the tag DT (determiner); WordNet
specifies whether triggers have fired for the five mention
types. Here is an example sentence with its corresponding
streams:
</bodyText>
<subsubsectionHeader confidence="0.599052">
Sent The senator visited Rome
</subsubsectionHeader>
<bodyText confidence="0.790195">
Left corner The The NP
</bodyText>
<equation confidence="0.558995">
HMM x LOC-unary
WINNOW X B-LOC
</equation>
<sectionHeader confidence="0.947905" genericHeader="method">
3 Mention Tracking
</sectionHeader>
<bodyText confidence="0.998659869565217">
Mention tracking is the process of recognizing mentions as
belonging to an entity. We used a statistical approach for
tracking mentions of an entity in a document. Mentions
are scored pairwise by a relevancy score and then greed-
ily clustered together into a chain representing a single en-
tity. Resolving pronoun mentions to their antecedents is
a classic NLP problem (Hobbs, 1976; Ge, 2000; Mitkov,
2002). A method similar to ours for merging templates in
the muc-6 task has been described by (Kehler, 1997).
This work differs from the previous research in refer-
ence resolution in three respects: (1) instead of a restric-
tive search of antecedents of a given mention, we apply a
greedy methodology of symmetric pairwise comparison of
all link probabilities (2) we track nominal, pronominal and
named mentions of different semantic types, (3) a large
corpus of mentions has enabled us to produce a trainable
system for mention tracking.
Our approach is based on two elements (1) the relevancy
model introduced in (Ittycheriah, 2001) for question an-
swering and (2) a greedy pairwise linking strategy. In the
current application of the model, we seek to link the cur-
We use a statistical parser trained on the Penn Tree Bank.
rent mention to an entity, 6, which satisfies,
</bodyText>
<equation confidence="0.971044">
= arg maxp(iimi, ) I t=linked
e)
</equation>
<bodyText confidence="0.999865333333333">
where the binary-valued 1 is either &apos;linked&apos; or &apos;-&apos;linked&apos;.
The algorithm operates on the mentions in document order
and from the view of each mention there are:
</bodyText>
<listItem confidence="0.999652">
• partially formed clusters to the left, r
• free, unlabeled mentions to the right, R.
</listItem>
<bodyText confidence="0.9985915">
The algorithm4 for linking the current mention, in, is as
follows:
</bodyText>
<equation confidence="0.971132615384615">
Greedy-Chain(r, R., mc)
for m, in
if p(1 = linkedlm„ null) &lt; threshSingleMention
Add(m„
for m, in r
Rank(m„ r&apos;)
for m, in Z&apos;
if p(1 = linkedime, &gt; thresh[type]
return Merge(mc, m2)
for mi in TZ
if p(1 = linkedjmc, m1) &gt; thresh[type]
return DiscourseNew(nic)
return SingleMention(me)
</equation>
<bodyText confidence="0.999967947368421">
Separate thresholds were established for name, nominal,
and pronoun merging, as well as the number of entities
considered on the left and the number of mentions to the
right.
The model is built on binary-valued features, which are
defined as functions of the form f (link decision, m, m3).
The features in our model can be grouped into proxies rely-
ing on similarity (such as exact and partial matches, over-
lapping word tokens between mention heads), distance
measures (in terms of the word and sentence number be-
tween the two mentions, and string edit distance), text lo-
cation (quantized sentence number containing a mention),
length (e.g. number of words within a mention head), fre-
quency counts (number of times a mention head occurred
within a given document) as well as syntactic (e.g. ap-
positive) and semantic features (WordNet, semantic entity
type, definiteness proxies). A detailed description of the
algorithm along with incremental results with different fea-
tures are presented in (Ittycheriah-Stys, 2003).
</bodyText>
<sectionHeader confidence="0.999477" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.8856625">
In this section we present results of our participation in
the September 2002 NIST Automatic Content Extraction
</bodyText>
<footnote confidence="0.99853625">
4Z is a set of clusters to the left where individual mentions
are potential link candidates, G&apos; is a set of clusters to the left
where mentions have been ranked by their type, and thresh[type]
are thresholds specific to the mention type.
</footnote>
<table confidence="0.994129571428571">
Experiment Mention Detection ACE metric
F-measure % value
All streams 74.0% 60.1%
w/o HMM 65.3% 50.9%
w/o WINNOW 66.3% 45.6%
w/O LC 66.4% 55.0%
w/o HMM-WINNOW-LC 50.1% 34.7%
</table>
<tableCaption confidence="0.790060666666667">
Table 1: Mention detection F-measure and entity detection
ACE value for different models on the non-degraded text
sources of the ACE September 2002 evaluation data set.
</tableCaption>
<bodyText confidence="0.9997758">
evaluation (ACE, 2002). The evaluation measured the per-
formance of systems on entity and relation extraction from
newspaper and news wire articles, and broadcast news seg-
ments.
We report the F-measure of mention detection and
a NIST-defined value metric for entity detection (ACE,
2002) which computes a weighted cost of the misses, false
alarms and errors. The cost is normalized and subtracted
from 1 to arrive at a normalized &amp;quot;value&amp;quot;, with 0 corre-
sponding to no output and 1 corresponding to perfect entity
detection. We present results only of our site&apos;s participa-
tion as per NIST guidelines for the evaluation.
Our training set (provided by NIST) comprised of 417
documents, 191,501 words, 30,492 mentions and 12,630
entities and the evaluation set contained 186 documents,
104,877 words, 10,665 mentions and 4396 entities includ-
ing ASR and OCR versions of broadcast news and newswire
documents. We report results only on the original (not de-
graded) text documents.
Table 1 shows F-measure and ACE value for our sub-
mission system (&amp;quot;All streams&amp;quot;). We also show results
with four other mention detection models trained without
the HMM stream (&amp;quot;w/o HMM&amp;quot;), the Winnow stream (&amp;quot;w/o
WINNOW&amp;quot;), the left corner of NPs (&amp;quot;w/o Lc&amp;quot;), and without
the HMM, WINNOW and LC streams (&amp;quot;w/o HMM-WINNOW-
LC&amp;quot;). For all experiments, we used the same mention
tracking model described in Section 3.
We achieved competitive scores (both F-measure and
ACE value) for this task. As indicated by the results in Ta-
ble 1, we were able to obtain a higher overall performance
by using all streams. The Winnow NE tagger is very good
at detecting person names, which the ACE value metric
weights highly. This may account for the relatively sharp
decrease in ACE value for the model without the Winnow
stream compared to the drop in F-measure, which does not
assign weights to categories. Our results suggest that our
model was able to use the complementary information pro-
vided by the different streams. In particular, the Named
Entity extractions of the two pre-existing NE taggers were
complimentary and helped the overall system.
</bodyText>
<sectionHeader confidence="0.999193" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999991714285714">
We have presented a system for identifying named, nomi-
nal, and pronominal mentions of entities in text and track-
ing them within documents. We participated in the NIST
Automatic Content Extraction evaluation and performed
well.
For mention detection, we pulled together two existing
named entity taggers trained with different categories and
combined them with other syntactic and lexical sources
of information using a maximum entropy framework for
building semantic trees. Combining the complementary
information provided by the pre-existing taggers helped us
rapidly achieve a high F-measure.
For mention tracking, we proposed a novel statistical
technique for tracking named, nominal and pronominal
mentions of an entity within a document. Using a unified
trainable approach helped us perfom well in the evaluation.
Ongoing work includes improving the mention detec-
tion and mention tracking by adding morphological, syn-
tactic (derived from parse trees) and semantic (e.g. Word-
Net) information streams, and extracting relations between
the detected entities using statistical models.
</bodyText>
<sectionHeader confidence="0.998642" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998196862068965">
ACE. 2002. The ACE Evaluation Plan.
www.nist.gov/speech/tests/ace/index.htm
D. Bikel, R. Schwartz &amp; R.M. Weischedel. 1999. An Al-
gorithm that Learns What&apos;s in a Name. Machine Learn-
ing, 34(1-3).
A. Borthwick, J. Sterling, E. Agichtein &amp; R. Grishman
1998. Exploiting Diverse Knowledge Sources via Max-
Entropy in NE Recognition 6th Workshop on Very Large
Corpora, Montreal.
N. Ge. 2000. An Approach to Anaphora Resolution. PhD
Thesis, Dept. of Computer Science, Brown University.
J. Hobbs. 1976. Pronoun resolution. Computer Science
Dept., City College, CUNY, Technical Report TR76-1.
A. Ittycheriah. 2001. Trainable Question Answering Sys-
tems. PhD Thesis, Dept. of Electrical and Computer En-
gineering, Rutgers - The State Univ. of New Jersey.
A. Ittycheriah &amp; M. Stys. 2003. A Greedy Algorithm for
Mention Tracking. Submitted to ACL&apos;2003.
A. Kehler. 1997. Probabilistic Coreference in Information
Extraction. EMNLP-2, 163-173.
R. Mitkov. 2002. Anaphora Resolution. Pearson, London.
A. Ratnaparkhi. 1999. Learning to Parse Natural Lan-
guage with Maximum Entropy. Machine Learning,
34(1-3).
T. Zhang, F. Damerau &amp; D.E. Johnson. 2002. Text Chunk-
ing based on a Generalization of Winnow. Journal of
Machine Learning Research, 2:615-637.
G. Zhou &amp; J. Su. 2002. Named Entity Recognition using
HMM Chunk Tagger. ACL&apos;02, 473-480. Philadelphia.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.580028">
<title confidence="0.999075">Identifying and Tracking Entity in a Maximum Entropy Framework</title>
<author confidence="0.751963">A Ittycheriah</author>
<author confidence="0.751963">L Lita</author>
<author confidence="0.751963">N Kambhatla</author>
<author confidence="0.751963">N Nicolov</author>
<author confidence="0.751963">S Roukos</author>
<author confidence="0.751963">M</author>
<affiliation confidence="0.745944">Research</affiliation>
<address confidence="0.9238155">P.O.Box 218, Route Yorktown, NY 10598</address>
<email confidence="0.962895">label,nanda,nicolas,roukos,smll@usibm.com,*llita@cscmuedu</email>
<abstract confidence="0.991602866666666">We present a system for identifying and tracking named, nominal, and pronominal mentions of entities within a text document. Our maximum entropy model for mention detection combines two pre-existing named entity taggers (built to extract different entity categories) and other syntactic and morphological feature streams to achieve competitive performance. We developed a novel maximum entropy model for tracking all mentions of an entity within a document. We participated in the Automatic Content Extraction (ACE) evaluation and performed well. We describe our system and present results of the ACE evaluation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>ACE</author>
</authors>
<date>2002</date>
<journal>The ACE Evaluation</journal>
<note>Plan. www.nist.gov/speech/tests/ace/index.htm</note>
<contexts>
<context position="1688" citStr="ACE, 2002" startWordPosition="260" endWordPosition="261">ne of five types (person, organization, geo-political entity, location, facility) and can be named (as in standard Named Entity (NE) research), nominal and pronominal - the latter dimension is called the level of a mention. Additionally, mentions can be generic or specific. We break the original task into mention detection (finding all mentions in the text and their type, level and genericity) and mention tracking (combining mentions into groups of references to the same object in the document). Our work is motivated by the requirements of a NISTrun evaluation on Automatic Content Extraction (ACE, 2002) where the goal is to build systems that detect entities (groups of mentions), relations among them and events in which they participate. Our team took part in the Entity detection track. The ACE task is inherently different and arguably harder than traditional named entity recognition, because of the complexity involved in extracting nonnamed mentions and chaining them together with named mentions. We investigate maximum entropy models for both tasks. For mention detection we use a maximum entropy framework for learning semantic trees&apos; (corresponding to the mentions) combining as features the</context>
<context position="9232" citStr="ACE, 2002" startWordPosition="1501" endWordPosition="1502">action 4Z is a set of clusters to the left where individual mentions are potential link candidates, G&apos; is a set of clusters to the left where mentions have been ranked by their type, and thresh[type] are thresholds specific to the mention type. Experiment Mention Detection ACE metric F-measure % value All streams 74.0% 60.1% w/o HMM 65.3% 50.9% w/o WINNOW 66.3% 45.6% w/O LC 66.4% 55.0% w/o HMM-WINNOW-LC 50.1% 34.7% Table 1: Mention detection F-measure and entity detection ACE value for different models on the non-degraded text sources of the ACE September 2002 evaluation data set. evaluation (ACE, 2002). The evaluation measured the performance of systems on entity and relation extraction from newspaper and news wire articles, and broadcast news segments. We report the F-measure of mention detection and a NIST-defined value metric for entity detection (ACE, 2002) which computes a weighted cost of the misses, false alarms and errors. The cost is normalized and subtracted from 1 to arrive at a normalized &amp;quot;value&amp;quot;, with 0 corresponding to no output and 1 corresponding to perfect entity detection. We present results only of our site&apos;s participation as per NIST guidelines for the evaluation. Our tr</context>
</contexts>
<marker>ACE, 2002</marker>
<rawString>ACE. 2002. The ACE Evaluation Plan. www.nist.gov/speech/tests/ace/index.htm</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bikel</author>
<author>R Schwartz</author>
<author>R M Weischedel</author>
</authors>
<title>An Algorithm that Learns What&apos;s in a Name.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="4423" citStr="Bikel et al., 1999" startWordPosition="702" endWordPosition="705">n w_1, w0, w+1, trigrams in w_2, w—i wo, w+1, w+2, unigrams, bigrams, trigrams on combinations of words and their POS tags in 1 We refer to this as a semantic parser. 2 From an engineering perspective the particular way we take diverse information into acount is by using multiple synchronized streams as input to the MaxEnt semantic parser. [-1,0, +11 window, the previous label, people and location suffixes. As additional features we used unigrams, bigrams and trigrams on the output of two models. The first is from an Hmm-based system implementing back-off strategies as in BBN&apos;s NYMBLE system (Bikel et al., 1999). It uses 31 categories and is trained on a large corpus of 1.5 million words. The system is developed as a component of a question answering system (Ittycheriah, 2001). The second system uses a generalized WINNOW approach (Zhang et al., 2002). It takes additional features: POS tags, lists of known locations, organizations, and person names. It is trained on m uc7 data and a subset of the above corpus for three common classes: person, location and organization. Additional streams we used were: flags, gazetteers, chunk, left corner and WordNet: flags specify capitalization patterns (Bikel et al</context>
</contexts>
<marker>Bikel, Schwartz, Weischedel, 1999</marker>
<rawString>D. Bikel, R. Schwartz &amp; R.M. Weischedel. 1999. An Algorithm that Learns What&apos;s in a Name. Machine Learning, 34(1-3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Borthwick</author>
<author>J Sterling</author>
<author>E Agichtein</author>
<author>R Grishman</author>
</authors>
<date>1998</date>
<booktitle>Exploiting Diverse Knowledge Sources via MaxEntropy in NE Recognition 6th Workshop on Very Large Corpora,</booktitle>
<location>Montreal.</location>
<contexts>
<context position="5054" citStr="Borthwick et al., 1998" startWordPosition="806" endWordPosition="809"> 31 categories and is trained on a large corpus of 1.5 million words. The system is developed as a component of a question answering system (Ittycheriah, 2001). The second system uses a generalized WINNOW approach (Zhang et al., 2002). It takes additional features: POS tags, lists of known locations, organizations, and person names. It is trained on m uc7 data and a subset of the above corpus for three common classes: person, location and organization. Additional streams we used were: flags, gazetteers, chunk, left corner and WordNet: flags specify capitalization patterns (Bikel et al., 1999; Borthwick et al., 1998; Zhou &amp; Su, 2002); the gazetteer stream indicates presence of a word in lists; chunk states the label of the mother node of each preterminal in the parse tree3; left corner specifies whether the current word is inside an NP and the identity of the leftmost leaf if it has the tag DT (determiner); WordNet specifies whether triggers have fired for the five mention types. Here is an example sentence with its corresponding streams: Sent The senator visited Rome Left corner The The NP HMM x LOC-unary WINNOW X B-LOC 3 Mention Tracking Mention tracking is the process of recognizing mentions as belong</context>
</contexts>
<marker>Borthwick, Sterling, Agichtein, Grishman, 1998</marker>
<rawString>A. Borthwick, J. Sterling, E. Agichtein &amp; R. Grishman 1998. Exploiting Diverse Knowledge Sources via MaxEntropy in NE Recognition 6th Workshop on Very Large Corpora, Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ge</author>
</authors>
<title>An Approach to Anaphora Resolution.</title>
<date>2000</date>
<tech>PhD Thesis,</tech>
<institution>Dept. of Computer Science, Brown University.</institution>
<contexts>
<context position="5978" citStr="Ge, 2000" startWordPosition="965" endWordPosition="966">whether triggers have fired for the five mention types. Here is an example sentence with its corresponding streams: Sent The senator visited Rome Left corner The The NP HMM x LOC-unary WINNOW X B-LOC 3 Mention Tracking Mention tracking is the process of recognizing mentions as belonging to an entity. We used a statistical approach for tracking mentions of an entity in a document. Mentions are scored pairwise by a relevancy score and then greedily clustered together into a chain representing a single entity. Resolving pronoun mentions to their antecedents is a classic NLP problem (Hobbs, 1976; Ge, 2000; Mitkov, 2002). A method similar to ours for merging templates in the muc-6 task has been described by (Kehler, 1997). This work differs from the previous research in reference resolution in three respects: (1) instead of a restrictive search of antecedents of a given mention, we apply a greedy methodology of symmetric pairwise comparison of all link probabilities (2) we track nominal, pronominal and named mentions of different semantic types, (3) a large corpus of mentions has enabled us to produce a trainable system for mention tracking. Our approach is based on two elements (1) the relevan</context>
</contexts>
<marker>Ge, 2000</marker>
<rawString>N. Ge. 2000. An Approach to Anaphora Resolution. PhD Thesis, Dept. of Computer Science, Brown University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hobbs</author>
</authors>
<title>Pronoun resolution. Computer Science Dept.,</title>
<date>1976</date>
<tech>Technical Report TR76-1.</tech>
<location>City College, CUNY,</location>
<contexts>
<context position="5968" citStr="Hobbs, 1976" startWordPosition="963" endWordPosition="964">et specifies whether triggers have fired for the five mention types. Here is an example sentence with its corresponding streams: Sent The senator visited Rome Left corner The The NP HMM x LOC-unary WINNOW X B-LOC 3 Mention Tracking Mention tracking is the process of recognizing mentions as belonging to an entity. We used a statistical approach for tracking mentions of an entity in a document. Mentions are scored pairwise by a relevancy score and then greedily clustered together into a chain representing a single entity. Resolving pronoun mentions to their antecedents is a classic NLP problem (Hobbs, 1976; Ge, 2000; Mitkov, 2002). A method similar to ours for merging templates in the muc-6 task has been described by (Kehler, 1997). This work differs from the previous research in reference resolution in three respects: (1) instead of a restrictive search of antecedents of a given mention, we apply a greedy methodology of symmetric pairwise comparison of all link probabilities (2) we track nominal, pronominal and named mentions of different semantic types, (3) a large corpus of mentions has enabled us to produce a trainable system for mention tracking. Our approach is based on two elements (1) t</context>
</contexts>
<marker>Hobbs, 1976</marker>
<rawString>J. Hobbs. 1976. Pronoun resolution. Computer Science Dept., City College, CUNY, Technical Report TR76-1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ittycheriah</author>
</authors>
<title>Trainable Question Answering Systems.</title>
<date>2001</date>
<tech>PhD Thesis,</tech>
<institution>Dept. of Electrical and Computer Engineering, Rutgers - The State Univ. of New Jersey.</institution>
<contexts>
<context position="4591" citStr="Ittycheriah, 2001" startWordPosition="734" endWordPosition="735">From an engineering perspective the particular way we take diverse information into acount is by using multiple synchronized streams as input to the MaxEnt semantic parser. [-1,0, +11 window, the previous label, people and location suffixes. As additional features we used unigrams, bigrams and trigrams on the output of two models. The first is from an Hmm-based system implementing back-off strategies as in BBN&apos;s NYMBLE system (Bikel et al., 1999). It uses 31 categories and is trained on a large corpus of 1.5 million words. The system is developed as a component of a question answering system (Ittycheriah, 2001). The second system uses a generalized WINNOW approach (Zhang et al., 2002). It takes additional features: POS tags, lists of known locations, organizations, and person names. It is trained on m uc7 data and a subset of the above corpus for three common classes: person, location and organization. Additional streams we used were: flags, gazetteers, chunk, left corner and WordNet: flags specify capitalization patterns (Bikel et al., 1999; Borthwick et al., 1998; Zhou &amp; Su, 2002); the gazetteer stream indicates presence of a word in lists; chunk states the label of the mother node of each preterm</context>
<context position="6620" citStr="Ittycheriah, 2001" startWordPosition="1069" endWordPosition="1070">d similar to ours for merging templates in the muc-6 task has been described by (Kehler, 1997). This work differs from the previous research in reference resolution in three respects: (1) instead of a restrictive search of antecedents of a given mention, we apply a greedy methodology of symmetric pairwise comparison of all link probabilities (2) we track nominal, pronominal and named mentions of different semantic types, (3) a large corpus of mentions has enabled us to produce a trainable system for mention tracking. Our approach is based on two elements (1) the relevancy model introduced in (Ittycheriah, 2001) for question answering and (2) a greedy pairwise linking strategy. In the current application of the model, we seek to link the curWe use a statistical parser trained on the Penn Tree Bank. rent mention to an entity, 6, which satisfies, = arg maxp(iimi, ) I t=linked e) where the binary-valued 1 is either &apos;linked&apos; or &apos;-&apos;linked&apos;. The algorithm operates on the mentions in document order and from the view of each mention there are: • partially formed clusters to the left, r • free, unlabeled mentions to the right, R. The algorithm4 for linking the current mention, in, is as follows: Greedy-Chain(</context>
</contexts>
<marker>Ittycheriah, 2001</marker>
<rawString>A. Ittycheriah. 2001. Trainable Question Answering Systems. PhD Thesis, Dept. of Electrical and Computer Engineering, Rutgers - The State Univ. of New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ittycheriah</author>
<author>M Stys</author>
</authors>
<title>A Greedy Algorithm for Mention Tracking.</title>
<date>2003</date>
<note>Submitted to ACL&apos;2003.</note>
<marker>Ittycheriah, Stys, 2003</marker>
<rawString>A. Ittycheriah &amp; M. Stys. 2003. A Greedy Algorithm for Mention Tracking. Submitted to ACL&apos;2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kehler</author>
</authors>
<date>1997</date>
<booktitle>Probabilistic Coreference in Information Extraction. EMNLP-2,</booktitle>
<pages>163--173</pages>
<contexts>
<context position="6096" citStr="Kehler, 1997" startWordPosition="985" endWordPosition="986">s: Sent The senator visited Rome Left corner The The NP HMM x LOC-unary WINNOW X B-LOC 3 Mention Tracking Mention tracking is the process of recognizing mentions as belonging to an entity. We used a statistical approach for tracking mentions of an entity in a document. Mentions are scored pairwise by a relevancy score and then greedily clustered together into a chain representing a single entity. Resolving pronoun mentions to their antecedents is a classic NLP problem (Hobbs, 1976; Ge, 2000; Mitkov, 2002). A method similar to ours for merging templates in the muc-6 task has been described by (Kehler, 1997). This work differs from the previous research in reference resolution in three respects: (1) instead of a restrictive search of antecedents of a given mention, we apply a greedy methodology of symmetric pairwise comparison of all link probabilities (2) we track nominal, pronominal and named mentions of different semantic types, (3) a large corpus of mentions has enabled us to produce a trainable system for mention tracking. Our approach is based on two elements (1) the relevancy model introduced in (Ittycheriah, 2001) for question answering and (2) a greedy pairwise linking strategy. In the c</context>
</contexts>
<marker>Kehler, 1997</marker>
<rawString>A. Kehler. 1997. Probabilistic Coreference in Information Extraction. EMNLP-2, 163-173.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mitkov</author>
</authors>
<title>Anaphora Resolution.</title>
<date>2002</date>
<publisher>Pearson,</publisher>
<location>London.</location>
<contexts>
<context position="5993" citStr="Mitkov, 2002" startWordPosition="967" endWordPosition="968">iggers have fired for the five mention types. Here is an example sentence with its corresponding streams: Sent The senator visited Rome Left corner The The NP HMM x LOC-unary WINNOW X B-LOC 3 Mention Tracking Mention tracking is the process of recognizing mentions as belonging to an entity. We used a statistical approach for tracking mentions of an entity in a document. Mentions are scored pairwise by a relevancy score and then greedily clustered together into a chain representing a single entity. Resolving pronoun mentions to their antecedents is a classic NLP problem (Hobbs, 1976; Ge, 2000; Mitkov, 2002). A method similar to ours for merging templates in the muc-6 task has been described by (Kehler, 1997). This work differs from the previous research in reference resolution in three respects: (1) instead of a restrictive search of antecedents of a given mention, we apply a greedy methodology of symmetric pairwise comparison of all link probabilities (2) we track nominal, pronominal and named mentions of different semantic types, (3) a large corpus of mentions has enabled us to produce a trainable system for mention tracking. Our approach is based on two elements (1) the relevancy model introd</context>
</contexts>
<marker>Mitkov, 2002</marker>
<rawString>R. Mitkov. 2002. Anaphora Resolution. Pearson, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>Learning to Parse Natural Language with Maximum Entropy.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="3370" citStr="Ratnaparkhi, 1999" startWordPosition="532" endWordPosition="533">e a maximum entropy semantic parser for detecting mentions. The labels of the tree nodes correspond to the combination of type, level and genericity, giving rise to 30 = 5 x 3 x 2 categories for the learning framework. We had two pre-existing statistical NE taggers (Hmm and WINNOW) built with other applications in mind. Our strategy was to combine the hypotheses of the existing NE taggers (using their original models trained on different training data and with different labels) in a MaxEnt framework as well as use additional syntactic and semantic information.2 The underlying semantic parser (Ratnaparkhi, 1999) works in three stages: POS tagging, chunking and structure building. During chunking (similar to bottom up parsing) the next level of constutuent structure is discovered. During structure building the rest of the tree is built. All decisions are modeled using Maximum Entropy models. The nature of mention detection puts most burden on the chunking model. The chunking model features include: unigrams of current word (w0), bigrams in w_1, w0, w+1, trigrams in w_2, w—i wo, w+1, w+2, unigrams, bigrams, trigrams on combinations of words and their POS tags in 1 We refer to this as a semantic parser.</context>
</contexts>
<marker>Ratnaparkhi, 1999</marker>
<rawString>A. Ratnaparkhi. 1999. Learning to Parse Natural Language with Maximum Entropy. Machine Learning, 34(1-3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Zhang</author>
<author>F Damerau</author>
<author>D E Johnson</author>
</authors>
<title>Text Chunking based on a Generalization of Winnow.</title>
<date>2002</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>2--615</pages>
<contexts>
<context position="4666" citStr="Zhang et al., 2002" startWordPosition="745" endWordPosition="748">tion into acount is by using multiple synchronized streams as input to the MaxEnt semantic parser. [-1,0, +11 window, the previous label, people and location suffixes. As additional features we used unigrams, bigrams and trigrams on the output of two models. The first is from an Hmm-based system implementing back-off strategies as in BBN&apos;s NYMBLE system (Bikel et al., 1999). It uses 31 categories and is trained on a large corpus of 1.5 million words. The system is developed as a component of a question answering system (Ittycheriah, 2001). The second system uses a generalized WINNOW approach (Zhang et al., 2002). It takes additional features: POS tags, lists of known locations, organizations, and person names. It is trained on m uc7 data and a subset of the above corpus for three common classes: person, location and organization. Additional streams we used were: flags, gazetteers, chunk, left corner and WordNet: flags specify capitalization patterns (Bikel et al., 1999; Borthwick et al., 1998; Zhou &amp; Su, 2002); the gazetteer stream indicates presence of a word in lists; chunk states the label of the mother node of each preterminal in the parse tree3; left corner specifies whether the current word is </context>
</contexts>
<marker>Zhang, Damerau, Johnson, 2002</marker>
<rawString>T. Zhang, F. Damerau &amp; D.E. Johnson. 2002. Text Chunking based on a Generalization of Winnow. Journal of Machine Learning Research, 2:615-637.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Zhou</author>
<author>J Su</author>
</authors>
<title>Named Entity Recognition using HMM Chunk Tagger.</title>
<date>2002</date>
<volume>02</volume>
<pages>473--480</pages>
<location>Philadelphia.</location>
<contexts>
<context position="5072" citStr="Zhou &amp; Su, 2002" startWordPosition="810" endWordPosition="813">ained on a large corpus of 1.5 million words. The system is developed as a component of a question answering system (Ittycheriah, 2001). The second system uses a generalized WINNOW approach (Zhang et al., 2002). It takes additional features: POS tags, lists of known locations, organizations, and person names. It is trained on m uc7 data and a subset of the above corpus for three common classes: person, location and organization. Additional streams we used were: flags, gazetteers, chunk, left corner and WordNet: flags specify capitalization patterns (Bikel et al., 1999; Borthwick et al., 1998; Zhou &amp; Su, 2002); the gazetteer stream indicates presence of a word in lists; chunk states the label of the mother node of each preterminal in the parse tree3; left corner specifies whether the current word is inside an NP and the identity of the leftmost leaf if it has the tag DT (determiner); WordNet specifies whether triggers have fired for the five mention types. Here is an example sentence with its corresponding streams: Sent The senator visited Rome Left corner The The NP HMM x LOC-unary WINNOW X B-LOC 3 Mention Tracking Mention tracking is the process of recognizing mentions as belonging to an entity. </context>
</contexts>
<marker>Zhou, Su, 2002</marker>
<rawString>G. Zhou &amp; J. Su. 2002. Named Entity Recognition using HMM Chunk Tagger. ACL&apos;02, 473-480. Philadelphia.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>