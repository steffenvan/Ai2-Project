<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000014">
<title confidence="0.999267">
Estimating Lexical Priors for
Low-Frequency Morphologically
Ambiguous Forms
</title>
<author confidence="0.995137">
Harald Baayen* Richard Sproatt
</author>
<affiliation confidence="0.50338">
Max Planck Institute for Bell Laboratories
Psycholinguistics
</affiliation>
<bodyText confidence="0.999435533333333">
Given a form that is previously unseen in a sufficiently large training corpus, and that is mor-
phologically n-ways ambiguous (serves n different lexical functions) what is the best estimator
for the lexical prior probabilities for the various functions of the form? We argue that the best
estimator is provided by computing the relative frequencies of the various functions among the
hapax legomena—the forms that occur exactly once in a corpus; in particular, a hapax-based
estimator is better than one based on the proportion of the various functions among words of all
frequency ranges. As we shall argue, this is because when one computes an overall measure, one is
including high-frequency words, and high-frequency words tend to have idiosyncratic properties
that are not at all representative of the much larger mass of (productively formed) low-frequency
words. This result has potential importance for various kinds of applications requiring lexical
disambiguation, including, in particular, stochastic taggers. This is especially true when some
initial hand-tagging of a corpus is required: for predicting lexical priors for very low-frequericy
morphologically ambiguous types (most of which would not occur in any given corpus), one
should concentrate on tagging a good representative sample of the hapax legomena, rather than
extensively tagging words of all frequency ranges,
</bodyText>
<sectionHeader confidence="0.992135" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.9993978">
As a number of writers on morphology have noted (most recently and notably Beard
[1995]), it is common to find that a particular affix or other morphological marker
serves more than one function in a language. For example, in many morphologically
complex languages it is often the case that several slots in a paradigm are filled with
the same form; put in another way, it is common to find that a particular morphologi-
cal form is in fact ambiguous between several distinct functions. This phenomenon—
which in the domain of inflectional morphology is termed syncretism—can be illus-
trated by a Dutch example such as lopen &apos;walk&apos;, which can either be the infinitive form
(&apos;to walk&apos;) or the finite plural (present tense) form (&apos;we, you, or they walk&apos;). In some
cases, syncretism is completely systematic: for example the case cited in Dutch, where
the -en suffix can always function in the two ways cited; or in Latin, where the plural
dative and ablative forms of nouns and adjectives are always identical, no matter what
paradigm the noun belongs to. In other cases, a particular instance of syncretism may
be displayed only in some paradigms: for example, Russian feminine nouns, such as
toshad&amp;quot;horse&apos; (Cyrillic .nonia,ab), have the same form for both the genitive singular
</bodyText>
<footnote confidence="0.8695745">
* Wundtlaan 1, 6525 XD, Nijmegen, The Netherlands. E-mail: baayen@mpi.n1
t 600 Mountain Avenue, Murray Hill, NJ 07974, USA. E-mail: rws@research.att.com
</footnote>
<note confidence="0.812963">
C) 1996 Association for Computational Linguistics
Computational Linguistics Volume 22, Number 2
</note>
<bodyText confidence="0.999932815789474">
— loshadi (Cyrillic .11omax4) — and the nominative plural, whereas masculine nouns
typically distinguish these forms. In still other cases, the syncretism may be partial in
that two forms may be identical at one level of representation — say, orthography —
but not another — say, pronunciation. For example the written form goroda in Rus-
sian (Cyrillic ropoaa) may either be the nominative plural or the genitive singular of
&apos;city&apos;. In the genitive singular, the stress is on the first syllable (igioradA/), whereas
in the nominative plural the stress resides on the final syllable UgarAd&apos;an; note that
the difference in stress results in very different vowel qualities for the two forms, as
indicated in the phonetic transcriptions.
Syncretism and related morphological ambiguities present a problem for many
NL applications where lexical disambiguation is important; cases where the ortho-
graphic form is identical but the pronunciations of the various functions differ are
particularly important for speech applications, such as text-to-speech, since appro-
priate word pronunciations must be computed from orthographic forms that under-
specify the necessary information. Ideally one would like to build models that use
contextual information to perform lexical disambiguation (Yarowsky 1992, 1994), but
such models must be trained on specialized tagged corpora (either hand-generated
or semi-automatically generated) and such training corpora are often not available, at
least in the early phases of constructing a particular application. Lacking good contex-
tual models, one is forced to fall back on estimates of the lexical prior probabilities for
the various functions of a form. Following standard terminology, a lexical prior can be
defined as follows: Imagine that a given form is n-ways ambiguous; the lexical prior
probability of sense i of this form is simply the probability of sense i independent of
the context in which the particular instantiation of the form occurs. Assuming one
has a tagged corpus, one can usually get reasonable estimates of the lexical priors
for the frequent forms (such as Dutch lo pen &apos;walk&apos;) by simply counting the number
of times the form occurs in each of its various functions and dividing by the total
number of instances of the form (in any function). This yields the Maximum Like-
lihood Estimate (MLE) for the lexical prior probability. But for infrequent or unseen
forms, it is less clear how to compute the estimate. Consider another Dutch example
like aanlokken &apos;entice, appeal&apos;. This form occurs only once, as an infinitive, in the Uit
den Boogaart (henceforth UdB) corpus (Uit den Boogaart 1975); in other words it is a
hapax legomenon (&lt; Greek hapax &apos;once&apos;, legomenon &apos;said&apos;) in this corpus. Obviously
the lexical prior probability of this form expressing the finite plural is not zero, the
MLE is a poor estimate in such cases. When one considers forms that do not occur
in the training corpus (e.g., bedraden `to wire&apos;) the situation is even worse. The prob-
lem, then, is to provide a more reasonable estimate of the relative probabilities of the
various potential functions of such forms.1
</bodyText>
<sectionHeader confidence="0.57345" genericHeader="keywords">
2. Estimating the Lexical Priors for Rare Forms
</sectionHeader>
<bodyText confidence="0.994498">
For a common form such as lo pen &apos;walk&apos; a reasonable estimate of the lexical prior
probabilities is the MLE, computed over all occurrences of this form. So, in the UdB
corpus, to pen occurs 92 times as an infinitive and 43 times as a finite plural, so the MLE
</bodyText>
<footnote confidence="0.999496666666667">
1 Even models of disambiguation that make use of context, such as statistical n-gram taggers, often
presume some estimate of lexical priors, in addition to requiring estimates of the transition
probabilities of sequences of lexical tags (Church 1988; DeRose 1988; Kupiec 1992), and this again
brings up the question of what to do about unseen or low-frequency forms. In working taggers, a
common approach is simply to apply a uniform small probability to the various senses of unseen or
low-frequency forms: this was done in the tagger discussed in Church (1988), for example.
</footnote>
<page confidence="0.996852">
156
</page>
<figure confidence="0.983114714285714">
Baayen and Sproat Lexical Priors for Low-Frequency Forms
proportion of infinitives
•4,
0
0
2 4 6 8
log frequency class
</figure>
<figureCaption confidence="0.958324142857143">
Figure 1
Relative frequency of Dutch infinitives versus finite plurals in the Uit den Boogaart corpus, as
a function of the (natural) log of the frequency of the word forms. The horizontal solid line
represents the overall MLE, the relative frequency of the infinitive as computed over all
tokens; the horizontal dashed line represents the relative frequency of the infinitive among the
hapax legomena. The solid curve represents a locally weighted regression smoothing
(Cleveland 1979).
</figureCaption>
<bodyText confidence="0.999972">
estimate of the probability of the infinitive is 0.68. For low-frequency forms such as
aanlokken or bed raden, one might consider basing the MLE on the aggregate counts of all
ambiguous forms in the corpus. In the UdB corpus, there are 21,703 infinitive tokens,
and 9,922 finite plural tokens, so the MLE for aanlokken being an infinitive would be
0.69. Note, however, that the application of this overall MLE presupposes that the
relative frequencies of the various functions of a particular form are independent of
the frequency of the form itself. For the Dutch example at hand, this presupposition
predicts that if we were to classify -en forms according to their frequency, and then for
each frequency class thus defined, plot the relative frequency of infinitives and finite
plurals, the regression line should have a slope of approximately zero.
</bodyText>
<subsectionHeader confidence="0.998062">
2.1 Dutch Verb Forms in -en
</subsectionHeader>
<bodyText confidence="0.964852333333333">
Figure 1 shows that this prediction is not borne out. This scatterplot shows the relative
frequency of the infinitive versus the finite plural, as a function of the log-frequency
of the -en form. At the left-hand edge of the graph, the relative frequency of the in-
finitives for the hapax legomena is shown. This proportion is also highlighted by the
dashed horizontal line. As we proceed to the right, we observe that there is a general
downward curvature representing a lowering of the proportion of infinitives for the
</bodyText>
<page confidence="0.988323">
157
</page>
<note confidence="0.419959">
Computational Linguistics Volume 22, Number 2
</note>
<bodyText confidence="0.999966966666667">
higher-frequency words. This trend is captured by the solid nonparametric regression
line; an explanation for this trend will be forthcoming in Section 3. (It will be noted that
in Figure 1 the variance is fairly small for the lower-frequency ranges, higher for the
middle ranges, and then small again for the high-frequency ranges; anticipating some-
what, we note the same trends in Figures 2 and 3. This variance pattern follows from
the high variability in the absolute numbers of types realized, especially in the middle
log-frequency classes, in combination with the assumption that for any log-frequency
class, the proportion for that class is itself a random variable.) The solid horizontal line
represents the proportion of infinitives calculated over all frequency classes, and the
dashed horizontal line represents the proportion of infinitives calculated over just the
hapax legomena. The two horizontal lines can be interpreted as MLEs for the proba-
bility of an -en form being an infinitive: the solid line or overall MLE clearly provides
an estimate based on the whole population, whereas the dashed line or hapax-based
MLE provides an estimate for the hapaxes. The overall MLE computes a lower rel-
ative frequency for the infinitives, compared to the hapax-based MLE. The question,
then, is: Which of these MLEs provides a better estimate for low-frequency types? In
particular, for types that have not been seen in the training corpus, and for which we
therefore have no direct estimate of the word-specific prior probabilities, we would
like to know whether the hapax-based or overall MLE provides a better estimate.
To answer this question we compared the accuracy of the overall and hapax-based
MLEs using tenfold cross-validation. We first randomized the list of -en tokens from
the UdB corpus, then divided the randomized list into ten equal-sized parts. Each of
the ten parts was held out as the test set, and the remaining nine-tenths was used as
the training set over which the two MLE estimates were computed. The results are
shown in Table 1. In this table, No(inf) and No(p1) represent the observed number of
tokens of infinitives and plurals in the held-out portion of the data, representing types
that had not been seen in the training data. The final four rows compare the estimates
for these numbers of tokens given the overall MLE (Eo[No(inf)] and E0[No(p1)]), versus
the hapax-based MLE (Eh[No(inf)] and Eh[No(p0]). For all ten runs, the hapax-based
MLE is clearly a far better predictor than the overall MLE.2
</bodyText>
<subsectionHeader confidence="0.995249">
2.2 English Verb Forms in -ed
</subsectionHeader>
<bodyText confidence="0.999906363636363">
The pattern that we have observed for the Dutch infinitive-plural ambiguity can be
replicated for other cases of morphological ambiguity. Consider the case of English
verbs ending in -ed, which are systematically ambiguous between being simple past
tenses and past participles. The upper panel of Figure 2 shows the distribution of
the relative frequencies of the two functions, plotted against the natural log of the
frequency for the Brown corpus (Francis and Kucera 1982). (All lines, including the
nonparametric regression line are interpretable as in Figure 1.) Results of a tenfold
cross-validation are shown in Table 2. Clearly, in this case the magnitude of the dif-
ference between the overall MLE and the hapax-based MLE is smaller than in the
previous example: indeed in cross validations 6, 8, and 9, the overall MLE is superior.
Nonetheless, the hapax-based MLE remains a significantly better predictor overall.&apos;
</bodyText>
<footnote confidence="0.909455333333333">
2 A paired t-test on the ratios No(inf)/No(p1) versus Eo[No(inf)]/E0[No(p1)] reveals a highly significant
difference (t9 = 13.4,p &lt; 0.001); conversely a comparison of No (inf)/No(p1) and Et[No(inf)]/Eh[No(p1)1
reveals no difference (t9 = 0.96,p &gt; 0.10).
3 A paired t-test on the ratios No(vbn)/No(vbd) versus Eo[No(vbn)]/E0[1\10(vbd)J reveals a significant
difference (t9 = 2.47,p &lt; 0.05); conversely a comparison of No(vbn)/No(vbd) and
EhtNo(vbn)1/Eh[No(vbd)] reveals no difference (t9 = 0.48,p &gt; 0.10).
</footnote>
<page confidence="0.99003">
158
</page>
<note confidence="0.655661">
Baayen and Sproat Lexical Priors for Low-Frequency Forms
</note>
<tableCaption confidence="0.993392">
Table 1
</tableCaption>
<bodyText confidence="0.990168444444445">
Results of tenfold cross-validation for Dutch -en verb forms from the Uit den Boogaart corpus.
Columns represent different cross-validation runs. N(inf) and N(p1) are the number of tokens
of the infinitives and finite plurals, respectively, in the training set. N1(inf) and N1 (pi) are the
number of tokens of the infinitives and finite plurals, respectively, among the hapaxes in the
training set. OMLE and HMLE are, respectively, the overall and hapax-based MLEs. No(inf)
and No(p1) denote the number of tokens in the held-out portion that have not been observed in
the training set. The expected numbers of tokens of infinitives and plurals for types unseen in
the training set, using the overall MLE are denoted as Eo[No(inf)] and Eo[No(p/)]; the
corresponding estimates using the hapax-based MLE are denoted as Eh[No(ing and Eh[No(p1)].
</bodyText>
<table confidence="0.998769769230769">
Run 1 2 3 4 5 6 7 8 9 10
N(inf) 19,509 19,527 19,536 19,526 19,507 19,511 19,533 19,524 19,569 19,585
N(p1) 8,953 8,935 8,926 8,936 8,955 8,952 8,930 8,939 8,894 8,878
OMLE 0.685 0.686 0.686 0.686 0.685 0.685 0.686 0.686 0.688 0.688
Ni(inf) 1,075 1,086 1,066 1,068 1,092 1,091 1,098 1,066 1,094 1,079
Ni(p1) 185 184 180 182 179 185 184 178 179 180
HMLE 0.853 0.855 0.856 0.854 0.859 0.855 0.856 0.857 0.859 0.857
No (inf) 120 114 133 125 133 123 102 118 121 127
No(p1) 24 19 20 18 18 16 15 23 23 21
Eo[No(inf)] 99 91 105 98 103 95 80 97 99 102
Eo [No (PO] 45 42 48 45 48 44 37 44 45 46
Eh[No(ing 123 114 131 122 130 119 100 121 124 127
Eh[No(p1)] 21 19 22 21 21 20 17 20 20 21
</table>
<subsectionHeader confidence="0.999209">
2.3 Dutch Words in -en: A More General Problem
</subsectionHeader>
<bodyText confidence="0.99973775">
In the two examples we have just considered, the hapax-based MLE, while being a
better predictor of the a priori lexical probability for unseen cases than the overall
MLE, does not actually yield a different prediction as to which function of a form is
more likely. This does not hold generally, however, and the bottom panel of Figure 2
presents a case where the hapax-based MLE does yield a different prediction as to
which function is more likely. In this plot we consider Dutch word forms from the
UdB corpus ending in -en. As we have seen, Dutch -en is used as a verb marker: it
marks the infinitive, present plural, and for strong verbs, also the past plural; it is
also used as a marker of noun plurals. The case of noun plurals is somewhat different
from the preceding two cases since it is not, strictly speaking, a case of morphological
syncretism. However, it is a potential source of ambiguity in text analysis, since a low
frequency form in -en, where one may not have seen the stem of the word, could
potentially be either a noun or a verb. Also, systematic ambiguity exists among cases
of noun-verb conversion: for examplefluiten is either a noun meaning &apos;flutes&apos; or a verb
meaning &apos;to play the flute&apos;; spelden means either &apos;pins&apos; or &apos;to pin&apos;; and ploegen means
either &apos;ploughs&apos; or &apos;to plough&apos;. Results for a tenfold cross-validation for these data are
shown in Table 3.4 In this case, the overall MLE would lead one to predict that for
an unseen form in -en, the verbal function would be more likely. Contrariwise, the
hapax-based MLE predicts that the nominal function would be more likely. Again, it
is the hapax-based MLE that proves to be superior.
</bodyText>
<footnote confidence="0.794847">
4 A paired t-test on the ratios No(v)/No(n) versus Eo[No(v)]/E0[No(n)] reveals a highly significant
difference (t9 =- 95.95,p &lt; 0.001); conversely a comparison of No(v)/No(n) and Eh[No(V)]/Eh[NO(n)]
reveals no difference (t9 0.12,p &gt; 0.10).
</footnote>
<page confidence="0.978087">
159
</page>
<figure confidence="0.990181181818182">
Computational Linguistics Volume 22, Number 2
English -ed
. • ..•
• .• .
• • 21 • a
0 1 2 3 4 5 6
log frequency class
Dutch verbs and nouns in -en
. • •
0 2 4 6
log frequency class
</figure>
<figureCaption confidence="0.994543">
Figure 2
</figureCaption>
<bodyText confidence="0.915708333333333">
The top panel displays the distribution in the Brown corpus of the relative frequencies of
English simple past tense verbs in -ed (Brown corpus tag VBD) versus past participles in -ed
(VBN), plotted against log-frequency. The bottom panel displays the relative frequency as a
function of log-frequency of Dutch verbs in -en (infinitives, present plurals, and strong past
tense plurals), versus plural nouns in -en, computed over the Uit den Boogaart corpus. Lines
are interpreted as in Figure 1.
</bodyText>
<table confidence="0.407297666666667">
proportion VBN
160
Baayen and Sproat Lexical Priors for Low-Frequency Forms
</table>
<tableCaption confidence="0.992198">
Table 2
</tableCaption>
<table confidence="0.966130357142857">
Cross-validation statistics for English past participles versus simple past tense verbs.
Run 1 2 3 4 5 6 7 8 9 10
N(vbn) 20,386 20,360 20,376 20,372 20,388 20,451 20,431 20,431 20,426 20,400
N(vbd) 13,845 13,871 13,855 13,859 13,843 13,781 13,801 13,801 13,806 13,832
OMLE 0.596 0.595 0.595 0.595 0.596 0.597 0.597 0.597 0.597 0.596
(vbn) 701 695 678 700 693 705 690 692 710 711
N1(vbd) 395 401 405 406 406 403 404 405 393 403
HMLE 0.640 0.634 0.626 0.633 0.631 0.636 0.631 0.631 0.644 0.638
No (vbn) 80 86 101 83 71 61 85 75 72 77
No (vbd) 49 52 37 41 43 45 41 50 48 42
E, [No (vbn)] 77 82 82 74 68 63 75 75 72 71
E, [No (vbd)] 52 56 56 50 46 43 51 50 48 48
Ery [No(vbn)] 83 88 86 78 72 67 79 79 77 76
Ery [No (vbd)] 46 50 52 46 42 39 47 46 43 43
</table>
<tableCaption confidence="0.782147">
Table 3
Cross-validation statistics for Dutch verbs in -en versus plural nouns in -en.
</tableCaption>
<table confidence="0.999886076923077">
Run 1 2 3 4 5 6 7 8 9 10
N(v) 25,237 25,283 25,267 25,245 25,292 25,267 25,205 25,207 25,261 25,294
N(n) 18,306 18,260 18,277 18,299 18,252 18,277 18,339 18,337 18,283 18,250
OMLE 0.580 0.581 0.580 0.580 0.581 0.580 0.579 0.579 0.580 0.581
Ni (v) 1,312 1,295 1,287 1,317 1,284 1,298 1,298 1,297 1,292 1,298
Ni(n) 2,913 2,910 2,939 2,942 2,901 2,922 2,979 2,969 2,936 2,931
HMLE 0.311 0.308 0.305 0.309 0.307 0.308 0.303 0.304 0.306 0.307
No (v) 124 131 154 142 148 143 148 156 153 139
No (n) 325 344 327 334 352 335 289 301 327 319
E, [No (v)] 260 276 279 276 290 277 253 265 278 266
Ea [No (n)] 189 199 202 200 210 201 184 192 202 192
Eh [NOM] 139 146 146 147 153 147 133 139 147 141
Eh [No (n)] 310 329 335 329 347 331 304 318 333 317
</table>
<subsectionHeader confidence="0.980866">
2.4 Disyllabic Dutch Words Ending in -er
</subsectionHeader>
<bodyText confidence="0.999919733333333">
One final example—also not a case of syncretism—concerns the ambiguity of the
sequence -er in Dutch, which occurs word-finally in monomorphemic nouns (moeder,
&apos;mother&apos;), adjectives (donker, &apos;dark&apos;), and proper names (Pieter, &apos;Peter&apos;), but which
is also used as a suffix to form comparatives (sneller, &apos;faster&apos;) and &amp;quot;agentive&amp;quot; nouns
(schrifver, &apos;writer&apos;). Since monomorphemic nouns and adjectives in this class are mostly
disyllabic, we will restrict our attention to the disyllabic instances of words ending
in -er. Again we find that the hapax-based MLE is superior to the overall MLE for
predicting to which of these five categories an unseen disyllabic word belongs.
Table 4 lists the overall MLE, the hapax-based MLE and the statistics on which
these estimates are based; Figure 3 plots the corresponding proportions as a function of
log-frequency. Table 4 also lists the results of tenfold cross-validation by specifying, for
each category, its contribution to the X2-statistic summing over the ten cross-validation
runs. (A more condensed format was chosen for this table than for the previous tables,
since here we are dealing with a fivefold ambiguity; the previous format would have
resulted in a rather large table in the present case.) Clearly, predictions based on the
</bodyText>
<page confidence="0.995845">
161
</page>
<note confidence="0.53813">
Computational Linguistics Volume 22, Number 2
</note>
<tableCaption confidence="0.998865">
Table 4
Results of tenfold cross-validation for Dutch disyllabic -er words. N and N1 are the number of
tokens and number of hapax legomena in the Uit den Boogaart corpus for simplex and
complex adjectives and nouns, and proper names. OMLE and HMLE are, respectively, the
overall and hapax-based MLEs based on N and N1. For each category, the columns headed by
X2(OMLE) and X2(HMLE) list the summed contribution to the X2-measures over ten
cross-validation runs for the overall and hapax-based estimates.
</tableCaption>
<table confidence="0.965076714285714">
String type N N1 OMLE HMLE X2(OMLE) X2(HMLE)
Simplex noun in -er 2,157 43 0.438 0.206 46.52 1.63
Derived noun in -er 581 51 0.118 0.244 32.02 1.94
Simplex adjective in -er 486 6 0.099 0.029 18.22 14.90
Derived adjective in -er 1,409 41 0.286 0.196 14.97 9.05
Proper name in -er 291 68 0.059 0.325 361.22 5.98
4,924 209 1.000 1.000 462.97 33.50
</table>
<bodyText confidence="0.999435">
hapax-based MLE are superior to those based on the overall MLE (X36) = 462.97, p &lt;
.001 for the overall MLE, Xf36) = 33.50, p&gt; .5 for the hapax-based MLE). In particular,
proper names in -er have a much higher probability of occurrence than the overall
MLE would suggest. Of course, in orthography-based applications, one can rely to
some extent on capitalization to indicate proper names, so one might want to eliminate
those from consideration here on this basis. Removing the category of proper names
from the analysis, a cross-validation test again reveals significantly better predictions
for the hapax-based MLE (X27) = 43.61, p = .023) than for the overall MLE (Xf27) =
120.03, p &lt; .001).
</bodyText>
<subsectionHeader confidence="0.984295">
2.5 Summary
</subsectionHeader>
<bodyText confidence="0.999917958333333">
We have demonstrated with four separate examples that the hapax-based MLE is su-
perior to the overall MLE in predicting the proportions, among unseen forms, of the
various functions of morphologically ambiguous categories. Could an even better es-
timator be obtained by taking not only the proportion for the hapax legomena into
account but also the proportions for other low-log-frequency classes? To answer this
question, note that the scatterplot in the bottom panel of Figure 2 reveals a down-
ward curvature at the very left-hand side: even for the lowest-log-frequency classes,
the likelihood of a word being a verb decreases with decreasing log-frequency. This
suggests that for this particular example the hapax legomena alone should be used to
estimate the probability that an unseen word is a noun or verb, rather than the hapax
legomena in combination with other low-frequency classes (the words occurring twice,
three times, etc.). Interestingly, the top panel of Figure 2 does not reveal even a hint of
a trend among the lowest-log-frequency classes, and in Figure 1 the observed propor-
tions for log-frequency less than 2 also do not reveal a clear pattern. For Figure 3, clear
trends for the lower-log-frequency classes seem to obtain in all cases except the plot
showing the proportion of simplex adjectives. Taken jointly, these observations suggest
informally that an MLE based on the hapax legomena will never be inferior to MLEs
that take additional log-frequency classes at the lower end of the log-frequency range
into account. At the same time, the example of Dutch verb and noun forms in -en
suggests that the hapax-based MLE can be superior to such MLEs—in this particular
case, inclusion of these lower-frequency classes would bring the adjusted MLE more
in line with the overall MLE, resulting in a loss of accuracy. These considerations lead
us to conclude that the hapax-based MLE is to be preferred to an adjusted MLE that
includes other low-log-frequency classes.
</bodyText>
<page confidence="0.978645">
162
</page>
<figure confidence="0.996543583333333">
Baayen and Sproat Lexical Priors for Low-Frequency Forms
0 1 2 3 4 5 6
log frequency class
0 1 2 3 4
log frequency class
0 1 2 3 4 5
log frequency class
0 1 2 3 4 5
log frequency class
• • • • • • • • • • • • • • • •
0 1 2 3 4 5 6
log frequency class
</figure>
<figureCaption confidence="0.981314">
Figure 3
</figureCaption>
<bodyText confidence="0.8112792">
The distribution in the Uit den Boogaart corpus of the relative frequencies of disyllabic words
in -er, plotted against log-frequency. Five types of words are distinguished: monomorphemic
words in -er (moeder, &apos;mother&apos;); bimorphemic nouns in -er (schrijver, &apos;writer&apos;); monomorphemic
adjectives in -er (donker, &apos;dark&apos;); bimorphemic adjectives in -er (groter, &apos;greater&apos;); and proper
names in -er (Pieter, &apos;Peter&apos;). Lines are interpreted as in Figure 1.
</bodyText>
<sectionHeader confidence="0.993925" genericHeader="introduction">
3. Discussion
</sectionHeader>
<bodyText confidence="0.993060333333334">
As we have seen in the four examples discussed above, the MLE computed over hapax
legomena yields a better prediction of lexical prior probabilities for unseen cases than
does an MLE computed over the entire training corpus. We now have to consider
why this result holds. As we shall see, the reasons are different from case to case, but
nonetheless share a commonality: in all four cases, idiosyncratic lexical properties of
high-frequency words dominate the statistical properties of the high-frequency ranges,
thus making the overall MLE a less reliable predictor of the properties of the low-
frequency and unseen cases.
First let us discuss the final case, that of -er ambiguity in Dutch, beginning with the
derived and underived nouns. The hapax-based MLE estimate for derived nouns in
-er is somewhat higher than the overall MLE; for underived nouns, the hapax-based
MLE is significantly lower — half — of the overall MLE. This can be explained by
the observation that a good many of the underived nouns in -er are high-frequency
words such as moeder &apos;mother&apos; and vader &apos;father&apos;. Such words contribute to the overall
proportional mass of the underived nouns, thus boosting the estimate of the overall
MLE for this class. A similar argument holds for the derived and underived adjectives.
Turning to proper names, we see that the hapax-based MLE is much larger than the
overall MLE. Proper names differ from ordinary words in that there are relatively few
</bodyText>
<page confidence="0.996559">
163
</page>
<note confidence="0.71827">
Computational Linguistics Volume 22, Number 2
</note>
<bodyText confidence="0.98599068">
proper names that are highly frequent, in comparison with words in general, but there
are large numbers of types of names that occur rarely. Thus, we expect an imbalance
of the kind we observe.
Consider next the ambiguity in Dutch between -en verb forms and -en plural
nouns. Ceteris paribus, plural nouns are less frequent than singular nouns; on the
other hand, -en for verbs serves both the function of marking plurality and of marking
the infinitive. High-frequency verbs include some very common word forms, such as
the auxiliaries hebben &apos;have&apos;, zullen &apos;will&apos;, kunnen &apos;can&apos;, and moeten &apos;must&apos;. Thus, for the
high-frequency ranges, the data is weighted heavily towards verbs. On the other hand,
while both nouns and verbs are open classes, nouns are far more productive as a class
than are verbs (Baayen and Lieber 1991), and this pattern becomes predominant in the
low-frequency ranges: among low-frequency types, most tokens are nouns. Hence, for
the low-frequency ranges, the data is weighted towards nouns. These two opposing
forces conspire to yield a downward trend in the percentage of verbs as we proceed
from the high- to the low-frequency ranges.
Next, consider the English past tense versus past participle ambiguity. One of the
important functions of the past participle form is as an adjectival modifier or predicate;
for example, the parked car. In this function the past participle has a passive meaning
with transitive verbs, and a perfective meaning with unaccusative intransitive verbs;
see Levin (1993, 86-88) for details. For reasons that are not clear to us, a predomi-
nant number of the high-frequency verbs cannot felicitously be used as prenominal
adjectives. These verbs include unergative intransitives like walk, for which one would
not expect to find the adjectival usage, given the above characterization; but they also
include clear transitives like move, try, and ask, and unaccusative intransitives like ap-
pear, which are not generally felicitous in this usage. Consider: ?a moved car, ?a tried
approach, ?an asked question, ?an appeared ad; but contrast: an oft-tried approach, a frequently
asked question, a recently appeared ad, where an adverbial modifier renders the examples
felicitous.&apos; Among the low-frequency verbs, including accentuate, bottle and incense, the
predominate types are those in which the past participle usage is preferred. What is
clear from the plot in the top panel of Figure 2 is that the downward trend in the
regression curve to the right of the plot is due to the lexical properties of a relatively
small number of high-frequency verbs. For the greater part of the frequency range,
there is a relatively stable proportion of participles to finite past forms. Thus, the
hapax-based MLE yields an estimate that is uncontaminated by the lexical properties
of individual high-frequency forms.
Finally, consider the Dutch verb forms -en that we started with. In Figure 1 the
strong downward trend in the regression curve at the right of the figure is due in
large measure to the inclusion of high-frequency auxiliary verbs, examples of which
have already been given. These verbs, while possible in the infinitival form, occur
predominantly in the finite form. Hence, a form such as hebben &apos;have&apos; is much more
likely to be a plural finite form than it is to be an infinitive. At the low end of the
frequency spectrum, we find a great many verbs derived with separable particles,
such as afzeggen &apos;cancel&apos;; note that separable prefixation is the most productive verb-
forming process in Dutch. In the infinitival form, the particle is always attached to
5 One reviewer has suggested that the infelicity of many adjectival passives relates to the fact that the
action denoted by the base verb is not regarded as producing an enduring result that affects the object
denoted by the (deep) internal argument: contrast a broken vase, where the vase is enduringly affected
by the breaking, with ?a seen movie, where the movie is not affected. However, this cannot be the whole
story since the object denoted by the internal argument of kill is presumably enduringly affected by the
killing, yet ?a killed man seems about as odd as ?a seen movie.
</bodyText>
<page confidence="0.993635">
164
</page>
<note confidence="0.722665">
Baayen and Sproat Lexical Priors for Low-Frequency Forms
</note>
<bodyText confidence="0.999207586206897">
the verb. However, in the finite forms in main clauses, the particle must be separated:
for example, wij zeggen onze afspraak af &apos;we are cancelling our appointment&apos;. These
properties of Dutch separable verbs boost the likelihood of infinitival forms for the
low-frequency ranges, but they also boost the likelihood of (higher-frequency) finite
plural forms such as zeggen: since the separated finite plural form zeggen is identical
to the finite plural of the underived verb zeggen &apos;say&apos;, any separated finite forms will
accrue to the frequency of the generally much more common derivational base.
What all of these cases share is that the statistical properties of the high-frequency
ranges are dominated by lexical properties of particular sets of high-frequency words.
This in turn biases the overall MLE and makes it a poor predictor of novel cases.
For example, auxiliaries such as hebben &apos;have&apos; are among the most common verbs in
Dutch, but they have rather different syntactic, and hence morphological, properties
from other verbs; these properties in turn contaminate the high-frequency ranges and
thus the overall MLE. In contrast, words in the low-frequency ranges, and particu-
larly hapaxes, are heavily populated with (necessarily non-idiosyncratic) neologisms
derived via productive morphological processes (Baayen 1989; Baayen and Renouf
1996). Any lexical biases that are inherent in these morphological processes — for
example, the fact that a low frequency Dutch word ending in -en is more likely to
be a noun than a verb — are well-estimated by the hapaxes. Now, for a sufficiently
large training corpus, we can be very confident that an unseen complex word is non-
idiosyncratic and formed via a productive morphological process, and this confidence
increases as the corpus size increases (Baayen and Renouf 1996). Since the hapaxes of a
particular morphological process mostly consist of non-idiosyncratic formations from
that process, it makes sense that the distribution of a property among the hapaxes is
the least contaminated estimate available for the distribution of that property among
the unseen cases.
The hapax-based MLE that we have proposed is not only observationally prefer-
able to the overall MLE, it is also firmly grounded in probability theory. The probability
of encountering an unseen word given that this word is a word in -en is estimated by:
</bodyText>
<equation confidence="0.9949155">
Pr(unseenl-en) ,,,,__, NLN(-en)
N(-en) &apos; (1)
</equation>
<bodyText confidence="0.999972222222222">
where Ni,N (-en) denotes the number of hapax legomena in -en among the N(-en) tokens
in -en in the training sample; see Baayen (1989), Baayen and Lieber (1991), Good (1953),
and Church and Gale (1991). Of course, this estimate is heavily influenced by the
highest-frequency words in -en, as these words contribute many tokens to N(-en). In our
example, high-frequency auxiliaries such as hebben cause the probability of sampling
unseen types in -en to be low — newly sampled tokens have a high probability of
being an auxiliary rather than some previously unseen word. Interestingly, (1) can be
used to derive an expression for the conditional probability that a word is, say, a noun,
given that it is an unseen type in -en (Baayen 1993):
</bodyText>
<figure confidence="0.868741">
Pr(noun n unseen -en type)
Pr(noun I unseen -en type) = (2)
Pr (unseen -en type)
Ni,N(-en, noun)
m-en)
---- Ni,,,,(-en)
w-en)
Ni,N(-
N1,N (-en) &apos;
</figure>
<bodyText confidence="0.887760666666667">
Note that the estimator exemplified in (1) has been applied twice: once (in the de-
en, noun)
=
</bodyText>
<page confidence="0.990674">
165
</page>
<note confidence="0.688494">
Computational Linguistics Volume 22, Number 2
</note>
<bodyText confidence="0.99947175">
nominator) to the distribution of all -en words; and once (in the numerator) to the
distribution of the -en nouns — after reclassifying all verbal tokens in -en as represent-
ing one (very high-frequency) noun type in the frequency distribution. Similarly, the
probability that an unseen word in -en is a verb is given by
</bodyText>
<equation confidence="0.833748333333333">
(N1&apos;N verb) (3)
Pr(verb I unseen -en type)
N1,N(-en) •
</equation>
<bodyText confidence="0.999725909090909">
Thus the proportion of verbal hapaxes in -en that we have suggested as an adjusted
MLE estimator on the basis of the curve shown in Figure 2 is in fact an estimate of the
conditional probability that a word is a verb, given that it is an unseen type in -en.
The results of the analyses presented in this paper are of potential importance
in various applications that require lexical disambiguation and where an estimate
of lexical priors is required. For high-frequency words, one can obtain fairly reliable
estimates of the lexical priors by tagging a corpus that gives a good coverage to words
of various ranges. For predicting the lexical priors for the much larger mass of very
low-frequency types, most of which would not occur in any such corpus, the results we
have presented suggest that one should concentrate on tagging a good representative
sample of the hapaxes, rather than extensively tagging words of all frequency ranges.
</bodyText>
<sectionHeader confidence="0.991488" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998083333333333">
The authors wish to thank four anonymous
reviewers for Computational Linguistics for
useful comments on this paper.
</bodyText>
<sectionHeader confidence="0.994427" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999871169230769">
Baayen, Harald. 1989. A Corpus-Based
Approach to Morphological Productivity:
Statistical Analysis and Psycholinguistic
Interpretation. Ph.D. thesis, Free
University, Amsterdam.
Baayen, Harald. 1993. On frequency,
transparency and productivity. Yearbook of
Morphology 1992, pages 181-208.
Baayen, Harald and Rochelle Lieber. 1991.
Productivity and English derivation: A
corpus-based study. Linguistics,
29:801-843.
Baayen, Harald and Antoinette Renouf.
1996. Chronicling the Times: Productive
lexical innovations in an English
newspaper. Language, 72:69-96.
Beard, Robert. 1995. Lexeme-Morpheme Base
Morphology. SUNY, Albany.
Church, Kenneth. 1988. A stochastic parts
program and noun phrase parser for
unrestricted text. In Proceedings of the
Second Conference on Applied Natural
Language Processing, pages 136-143,
Morristown, NJ. Association for
Computational Linguistics.
Church, Kenneth Ward and William Gale.
1991. A comparison of the enhanced
Good-Turing and deleted estimation
methods for estimating probabilities of
English bigrams. Computer Speech and
Language, 5(1):19-54.
Cleveland, William. 1979. Robust locally
weighted regression and smoothing
scatterplots. Journal of the Acoustical Society
of America, 74(368):829-836, December.
DeRose, Stephen. 1988. Grammatical
category disambiguation by statistical
optimization. Computational Linguistics,
14:31-39.
Francis, W. Nelson and Henry Kucera. 1982.
Frequency Analysis of English Usage.
Houghton Mifflin, Boston.
Good, I. 1953. The population frequencies of
species and the estimation of population
parameters. Biometrica V. 40(3,4):237-264.
Kupiec, Julian. 1992. Robust part-of-speech
tagging using a hidden Markov model.
Computer Speech and Language, 6:225-242.
Levin, Beth. 1993. English Verb Classes and
Alternations. University of Chicago,
Chicago.
Uit den Boogaart, P. C., editor. 1975.
Woordfrequen ties in Gesproken en Geschreven
Ned erlands. Oosthoek, Scheltema and
Holkema, Utrecht.
Yarowsky, David. 1992. Word-sense
disambiguation using statistical models of
roget&apos;s categories trained on large
corpora. In Proceedings of COLING-92,
Nantes, France, July. COLING.
Yarowsky, David. 1994. Decision lists for
lexical ambiguity resolution. In
Proceedings of the 32nd Annual Meeting.
Association for Computational
Linguistics.
</reference>
<page confidence="0.99876">
166
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.328013">
<title confidence="0.998522666666667">Estimating Lexical Priors for Low-Frequency Morphologically Ambiguous Forms</title>
<author confidence="0.7405645">Harald Baayen Richard Sproatt Max Planck Institute for Bell Laboratories</author>
<abstract confidence="0.9695150625">Psycholinguistics Given a form that is previously unseen in a sufficiently large training corpus, and that is morn-ways ambiguous (serves lexical functions) what is the best estimator for the lexical prior probabilities for the various functions of the form? We argue that the best estimator is provided by computing the relative frequencies of the various functions among the legomena—the that occur exactly once in a corpus; in particular, a hapax-based is better than one based on the proportion of the various functions among words of ranges. As we shall argue, this is because when one computes an one is including high-frequency words, and high-frequency words tend to have idiosyncratic properties that are not at all representative of the much larger mass of (productively formed) low-frequency words. This result has potential importance for various kinds of applications requiring lexical disambiguation, including, in particular, stochastic taggers. This is especially true when some initial hand-tagging of a corpus is required: for predicting lexical priors for very low-frequericy morphologically ambiguous types (most of which would not occur in any given corpus), one should concentrate on tagging a good representative sample of the hapax legomena, rather than extensively tagging words of all frequency ranges,</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Harald Baayen</author>
</authors>
<title>A Corpus-Based Approach to Morphological Productivity: Statistical Analysis and Psycholinguistic Interpretation.</title>
<date>1989</date>
<tech>Ph.D. thesis,</tech>
<institution>Free University,</institution>
<location>Amsterdam.</location>
<contexts>
<context position="31824" citStr="Baayen 1989" startWordPosition="5267" endWordPosition="5268">perties of particular sets of high-frequency words. This in turn biases the overall MLE and makes it a poor predictor of novel cases. For example, auxiliaries such as hebben &apos;have&apos; are among the most common verbs in Dutch, but they have rather different syntactic, and hence morphological, properties from other verbs; these properties in turn contaminate the high-frequency ranges and thus the overall MLE. In contrast, words in the low-frequency ranges, and particularly hapaxes, are heavily populated with (necessarily non-idiosyncratic) neologisms derived via productive morphological processes (Baayen 1989; Baayen and Renouf 1996). Any lexical biases that are inherent in these morphological processes — for example, the fact that a low frequency Dutch word ending in -en is more likely to be a noun than a verb — are well-estimated by the hapaxes. Now, for a sufficiently large training corpus, we can be very confident that an unseen complex word is nonidiosyncratic and formed via a productive morphological process, and this confidence increases as the corpus size increases (Baayen and Renouf 1996). Since the hapaxes of a particular morphological process mostly consist of non-idiosyncratic formatio</context>
<context position="33053" citStr="Baayen (1989)" startWordPosition="5469" endWordPosition="5470">cess, it makes sense that the distribution of a property among the hapaxes is the least contaminated estimate available for the distribution of that property among the unseen cases. The hapax-based MLE that we have proposed is not only observationally preferable to the overall MLE, it is also firmly grounded in probability theory. The probability of encountering an unseen word given that this word is a word in -en is estimated by: Pr(unseenl-en) ,,,,__, NLN(-en) N(-en) &apos; (1) where Ni,N (-en) denotes the number of hapax legomena in -en among the N(-en) tokens in -en in the training sample; see Baayen (1989), Baayen and Lieber (1991), Good (1953), and Church and Gale (1991). Of course, this estimate is heavily influenced by the highest-frequency words in -en, as these words contribute many tokens to N(-en). In our example, high-frequency auxiliaries such as hebben cause the probability of sampling unseen types in -en to be low — newly sampled tokens have a high probability of being an auxiliary rather than some previously unseen word. Interestingly, (1) can be used to derive an expression for the conditional probability that a word is, say, a noun, given that it is an unseen type in -en (Baayen 1</context>
</contexts>
<marker>Baayen, 1989</marker>
<rawString>Baayen, Harald. 1989. A Corpus-Based Approach to Morphological Productivity: Statistical Analysis and Psycholinguistic Interpretation. Ph.D. thesis, Free University, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harald Baayen</author>
</authors>
<title>On frequency, transparency and productivity. Yearbook of Morphology</title>
<date>1993</date>
<pages>181--208</pages>
<contexts>
<context position="33657" citStr="Baayen 1993" startWordPosition="5570" endWordPosition="5571">n (1989), Baayen and Lieber (1991), Good (1953), and Church and Gale (1991). Of course, this estimate is heavily influenced by the highest-frequency words in -en, as these words contribute many tokens to N(-en). In our example, high-frequency auxiliaries such as hebben cause the probability of sampling unseen types in -en to be low — newly sampled tokens have a high probability of being an auxiliary rather than some previously unseen word. Interestingly, (1) can be used to derive an expression for the conditional probability that a word is, say, a noun, given that it is an unseen type in -en (Baayen 1993): Pr(noun n unseen -en type) Pr(noun I unseen -en type) = (2) Pr (unseen -en type) Ni,N(-en, noun) m-en) ---- Ni,,,,(-en) w-en) Ni,N(- N1,N (-en) &apos; Note that the estimator exemplified in (1) has been applied twice: once (in the deen, noun) = 165 Computational Linguistics Volume 22, Number 2 nominator) to the distribution of all -en words; and once (in the numerator) to the distribution of the -en nouns — after reclassifying all verbal tokens in -en as representing one (very high-frequency) noun type in the frequency distribution. Similarly, the probability that an unseen word in -en is a verb </context>
</contexts>
<marker>Baayen, 1993</marker>
<rawString>Baayen, Harald. 1993. On frequency, transparency and productivity. Yearbook of Morphology 1992, pages 181-208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harald Baayen</author>
<author>Rochelle Lieber</author>
</authors>
<title>Productivity and English derivation: A corpus-based study.</title>
<date>1991</date>
<journal>Linguistics,</journal>
<pages>29--801</pages>
<contexts>
<context position="27059" citStr="Baayen and Lieber 1991" startWordPosition="4507" endWordPosition="4510"> next the ambiguity in Dutch between -en verb forms and -en plural nouns. Ceteris paribus, plural nouns are less frequent than singular nouns; on the other hand, -en for verbs serves both the function of marking plurality and of marking the infinitive. High-frequency verbs include some very common word forms, such as the auxiliaries hebben &apos;have&apos;, zullen &apos;will&apos;, kunnen &apos;can&apos;, and moeten &apos;must&apos;. Thus, for the high-frequency ranges, the data is weighted heavily towards verbs. On the other hand, while both nouns and verbs are open classes, nouns are far more productive as a class than are verbs (Baayen and Lieber 1991), and this pattern becomes predominant in the low-frequency ranges: among low-frequency types, most tokens are nouns. Hence, for the low-frequency ranges, the data is weighted towards nouns. These two opposing forces conspire to yield a downward trend in the percentage of verbs as we proceed from the high- to the low-frequency ranges. Next, consider the English past tense versus past participle ambiguity. One of the important functions of the past participle form is as an adjectival modifier or predicate; for example, the parked car. In this function the past participle has a passive meaning w</context>
<context position="33079" citStr="Baayen and Lieber (1991)" startWordPosition="5471" endWordPosition="5474">sense that the distribution of a property among the hapaxes is the least contaminated estimate available for the distribution of that property among the unseen cases. The hapax-based MLE that we have proposed is not only observationally preferable to the overall MLE, it is also firmly grounded in probability theory. The probability of encountering an unseen word given that this word is a word in -en is estimated by: Pr(unseenl-en) ,,,,__, NLN(-en) N(-en) &apos; (1) where Ni,N (-en) denotes the number of hapax legomena in -en among the N(-en) tokens in -en in the training sample; see Baayen (1989), Baayen and Lieber (1991), Good (1953), and Church and Gale (1991). Of course, this estimate is heavily influenced by the highest-frequency words in -en, as these words contribute many tokens to N(-en). In our example, high-frequency auxiliaries such as hebben cause the probability of sampling unseen types in -en to be low — newly sampled tokens have a high probability of being an auxiliary rather than some previously unseen word. Interestingly, (1) can be used to derive an expression for the conditional probability that a word is, say, a noun, given that it is an unseen type in -en (Baayen 1993): Pr(noun n unseen -en</context>
</contexts>
<marker>Baayen, Lieber, 1991</marker>
<rawString>Baayen, Harald and Rochelle Lieber. 1991. Productivity and English derivation: A corpus-based study. Linguistics, 29:801-843.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harald Baayen</author>
<author>Antoinette Renouf</author>
</authors>
<title>Chronicling the Times: Productive lexical innovations in an English newspaper.</title>
<date>1996</date>
<journal>Language,</journal>
<pages>72--69</pages>
<contexts>
<context position="31849" citStr="Baayen and Renouf 1996" startWordPosition="5269" endWordPosition="5272">rticular sets of high-frequency words. This in turn biases the overall MLE and makes it a poor predictor of novel cases. For example, auxiliaries such as hebben &apos;have&apos; are among the most common verbs in Dutch, but they have rather different syntactic, and hence morphological, properties from other verbs; these properties in turn contaminate the high-frequency ranges and thus the overall MLE. In contrast, words in the low-frequency ranges, and particularly hapaxes, are heavily populated with (necessarily non-idiosyncratic) neologisms derived via productive morphological processes (Baayen 1989; Baayen and Renouf 1996). Any lexical biases that are inherent in these morphological processes — for example, the fact that a low frequency Dutch word ending in -en is more likely to be a noun than a verb — are well-estimated by the hapaxes. Now, for a sufficiently large training corpus, we can be very confident that an unseen complex word is nonidiosyncratic and formed via a productive morphological process, and this confidence increases as the corpus size increases (Baayen and Renouf 1996). Since the hapaxes of a particular morphological process mostly consist of non-idiosyncratic formations from that process, it </context>
</contexts>
<marker>Baayen, Renouf, 1996</marker>
<rawString>Baayen, Harald and Antoinette Renouf. 1996. Chronicling the Times: Productive lexical innovations in an English newspaper. Language, 72:69-96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Beard</author>
</authors>
<title>Lexeme-Morpheme Base Morphology.</title>
<date>1995</date>
<publisher>SUNY,</publisher>
<location>Albany.</location>
<marker>Beard, 1995</marker>
<rawString>Beard, Robert. 1995. Lexeme-Morpheme Base Morphology. SUNY, Albany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Church</author>
</authors>
<title>A stochastic parts program and noun phrase parser for unrestricted text.</title>
<date>1988</date>
<booktitle>In Proceedings of the Second Conference on Applied Natural Language Processing,</booktitle>
<pages>136--143</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ.</location>
<contexts>
<context position="6795" citStr="Church 1988" startWordPosition="1068" endWordPosition="1069">es of the various potential functions of such forms.1 2. Estimating the Lexical Priors for Rare Forms For a common form such as lo pen &apos;walk&apos; a reasonable estimate of the lexical prior probabilities is the MLE, computed over all occurrences of this form. So, in the UdB corpus, to pen occurs 92 times as an infinitive and 43 times as a finite plural, so the MLE 1 Even models of disambiguation that make use of context, such as statistical n-gram taggers, often presume some estimate of lexical priors, in addition to requiring estimates of the transition probabilities of sequences of lexical tags (Church 1988; DeRose 1988; Kupiec 1992), and this again brings up the question of what to do about unseen or low-frequency forms. In working taggers, a common approach is simply to apply a uniform small probability to the various senses of unseen or low-frequency forms: this was done in the tagger discussed in Church (1988), for example. 156 Baayen and Sproat Lexical Priors for Low-Frequency Forms proportion of infinitives •4, 0 0 2 4 6 8 log frequency class Figure 1 Relative frequency of Dutch infinitives versus finite plurals in the Uit den Boogaart corpus, as a function of the (natural) log of the freq</context>
</contexts>
<marker>Church, 1988</marker>
<rawString>Church, Kenneth. 1988. A stochastic parts program and noun phrase parser for unrestricted text. In Proceedings of the Second Conference on Applied Natural Language Processing, pages 136-143, Morristown, NJ. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Ward Church</author>
<author>William Gale</author>
</authors>
<title>A comparison of the enhanced Good-Turing and deleted estimation methods for estimating probabilities of English bigrams.</title>
<date>1991</date>
<journal>Computer Speech and Language,</journal>
<pages>5--1</pages>
<contexts>
<context position="33120" citStr="Church and Gale (1991)" startWordPosition="5478" endWordPosition="5481">mong the hapaxes is the least contaminated estimate available for the distribution of that property among the unseen cases. The hapax-based MLE that we have proposed is not only observationally preferable to the overall MLE, it is also firmly grounded in probability theory. The probability of encountering an unseen word given that this word is a word in -en is estimated by: Pr(unseenl-en) ,,,,__, NLN(-en) N(-en) &apos; (1) where Ni,N (-en) denotes the number of hapax legomena in -en among the N(-en) tokens in -en in the training sample; see Baayen (1989), Baayen and Lieber (1991), Good (1953), and Church and Gale (1991). Of course, this estimate is heavily influenced by the highest-frequency words in -en, as these words contribute many tokens to N(-en). In our example, high-frequency auxiliaries such as hebben cause the probability of sampling unseen types in -en to be low — newly sampled tokens have a high probability of being an auxiliary rather than some previously unseen word. Interestingly, (1) can be used to derive an expression for the conditional probability that a word is, say, a noun, given that it is an unseen type in -en (Baayen 1993): Pr(noun n unseen -en type) Pr(noun I unseen -en type) = (2) P</context>
</contexts>
<marker>Church, Gale, 1991</marker>
<rawString>Church, Kenneth Ward and William Gale. 1991. A comparison of the enhanced Good-Turing and deleted estimation methods for estimating probabilities of English bigrams. Computer Speech and Language, 5(1):19-54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Cleveland</author>
</authors>
<title>Robust locally weighted regression and smoothing scatterplots.</title>
<date>1979</date>
<journal>Journal of the Acoustical Society of America,</journal>
<pages>74--368</pages>
<contexts>
<context position="7732" citStr="Cleveland 1979" startWordPosition="1220" endWordPosition="1221">mple. 156 Baayen and Sproat Lexical Priors for Low-Frequency Forms proportion of infinitives •4, 0 0 2 4 6 8 log frequency class Figure 1 Relative frequency of Dutch infinitives versus finite plurals in the Uit den Boogaart corpus, as a function of the (natural) log of the frequency of the word forms. The horizontal solid line represents the overall MLE, the relative frequency of the infinitive as computed over all tokens; the horizontal dashed line represents the relative frequency of the infinitive among the hapax legomena. The solid curve represents a locally weighted regression smoothing (Cleveland 1979). estimate of the probability of the infinitive is 0.68. For low-frequency forms such as aanlokken or bed raden, one might consider basing the MLE on the aggregate counts of all ambiguous forms in the corpus. In the UdB corpus, there are 21,703 infinitive tokens, and 9,922 finite plural tokens, so the MLE for aanlokken being an infinitive would be 0.69. Note, however, that the application of this overall MLE presupposes that the relative frequencies of the various functions of a particular form are independent of the frequency of the form itself. For the Dutch example at hand, this presupposit</context>
</contexts>
<marker>Cleveland, 1979</marker>
<rawString>Cleveland, William. 1979. Robust locally weighted regression and smoothing scatterplots. Journal of the Acoustical Society of America, 74(368):829-836, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen DeRose</author>
</authors>
<title>Grammatical category disambiguation by statistical optimization.</title>
<date>1988</date>
<journal>Computational Linguistics,</journal>
<pages>14--31</pages>
<contexts>
<context position="6808" citStr="DeRose 1988" startWordPosition="1070" endWordPosition="1071">ious potential functions of such forms.1 2. Estimating the Lexical Priors for Rare Forms For a common form such as lo pen &apos;walk&apos; a reasonable estimate of the lexical prior probabilities is the MLE, computed over all occurrences of this form. So, in the UdB corpus, to pen occurs 92 times as an infinitive and 43 times as a finite plural, so the MLE 1 Even models of disambiguation that make use of context, such as statistical n-gram taggers, often presume some estimate of lexical priors, in addition to requiring estimates of the transition probabilities of sequences of lexical tags (Church 1988; DeRose 1988; Kupiec 1992), and this again brings up the question of what to do about unseen or low-frequency forms. In working taggers, a common approach is simply to apply a uniform small probability to the various senses of unseen or low-frequency forms: this was done in the tagger discussed in Church (1988), for example. 156 Baayen and Sproat Lexical Priors for Low-Frequency Forms proportion of infinitives •4, 0 0 2 4 6 8 log frequency class Figure 1 Relative frequency of Dutch infinitives versus finite plurals in the Uit den Boogaart corpus, as a function of the (natural) log of the frequency of the </context>
</contexts>
<marker>DeRose, 1988</marker>
<rawString>DeRose, Stephen. 1988. Grammatical category disambiguation by statistical optimization. Computational Linguistics, 14:31-39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Nelson Francis</author>
<author>Henry Kucera</author>
</authors>
<title>Frequency Analysis of English Usage.</title>
<date>1982</date>
<location>Houghton Mifflin, Boston.</location>
<contexts>
<context position="12231" citStr="Francis and Kucera 1982" startWordPosition="1949" endWordPosition="1952">No(inf)] and Eh[No(p0]). For all ten runs, the hapax-based MLE is clearly a far better predictor than the overall MLE.2 2.2 English Verb Forms in -ed The pattern that we have observed for the Dutch infinitive-plural ambiguity can be replicated for other cases of morphological ambiguity. Consider the case of English verbs ending in -ed, which are systematically ambiguous between being simple past tenses and past participles. The upper panel of Figure 2 shows the distribution of the relative frequencies of the two functions, plotted against the natural log of the frequency for the Brown corpus (Francis and Kucera 1982). (All lines, including the nonparametric regression line are interpretable as in Figure 1.) Results of a tenfold cross-validation are shown in Table 2. Clearly, in this case the magnitude of the difference between the overall MLE and the hapax-based MLE is smaller than in the previous example: indeed in cross validations 6, 8, and 9, the overall MLE is superior. Nonetheless, the hapax-based MLE remains a significantly better predictor overall.&apos; 2 A paired t-test on the ratios No(inf)/No(p1) versus Eo[No(inf)]/E0[No(p1)] reveals a highly significant difference (t9 = 13.4,p &lt; 0.001); conversely</context>
</contexts>
<marker>Francis, Kucera, 1982</marker>
<rawString>Francis, W. Nelson and Henry Kucera. 1982. Frequency Analysis of English Usage. Houghton Mifflin, Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Good</author>
</authors>
<title>The population frequencies of species and the estimation of population parameters.</title>
<date>1953</date>
<journal>Biometrica V.</journal>
<pages>40--3</pages>
<contexts>
<context position="33092" citStr="Good (1953)" startWordPosition="5475" endWordPosition="5476">n of a property among the hapaxes is the least contaminated estimate available for the distribution of that property among the unseen cases. The hapax-based MLE that we have proposed is not only observationally preferable to the overall MLE, it is also firmly grounded in probability theory. The probability of encountering an unseen word given that this word is a word in -en is estimated by: Pr(unseenl-en) ,,,,__, NLN(-en) N(-en) &apos; (1) where Ni,N (-en) denotes the number of hapax legomena in -en among the N(-en) tokens in -en in the training sample; see Baayen (1989), Baayen and Lieber (1991), Good (1953), and Church and Gale (1991). Of course, this estimate is heavily influenced by the highest-frequency words in -en, as these words contribute many tokens to N(-en). In our example, high-frequency auxiliaries such as hebben cause the probability of sampling unseen types in -en to be low — newly sampled tokens have a high probability of being an auxiliary rather than some previously unseen word. Interestingly, (1) can be used to derive an expression for the conditional probability that a word is, say, a noun, given that it is an unseen type in -en (Baayen 1993): Pr(noun n unseen -en type) Pr(nou</context>
</contexts>
<marker>Good, 1953</marker>
<rawString>Good, I. 1953. The population frequencies of species and the estimation of population parameters. Biometrica V. 40(3,4):237-264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian Kupiec</author>
</authors>
<title>Robust part-of-speech tagging using a hidden Markov model.</title>
<date>1992</date>
<journal>Computer Speech and Language,</journal>
<pages>6--225</pages>
<contexts>
<context position="6822" citStr="Kupiec 1992" startWordPosition="1072" endWordPosition="1073">l functions of such forms.1 2. Estimating the Lexical Priors for Rare Forms For a common form such as lo pen &apos;walk&apos; a reasonable estimate of the lexical prior probabilities is the MLE, computed over all occurrences of this form. So, in the UdB corpus, to pen occurs 92 times as an infinitive and 43 times as a finite plural, so the MLE 1 Even models of disambiguation that make use of context, such as statistical n-gram taggers, often presume some estimate of lexical priors, in addition to requiring estimates of the transition probabilities of sequences of lexical tags (Church 1988; DeRose 1988; Kupiec 1992), and this again brings up the question of what to do about unseen or low-frequency forms. In working taggers, a common approach is simply to apply a uniform small probability to the various senses of unseen or low-frequency forms: this was done in the tagger discussed in Church (1988), for example. 156 Baayen and Sproat Lexical Priors for Low-Frequency Forms proportion of infinitives •4, 0 0 2 4 6 8 log frequency class Figure 1 Relative frequency of Dutch infinitives versus finite plurals in the Uit den Boogaart corpus, as a function of the (natural) log of the frequency of the word forms. Th</context>
</contexts>
<marker>Kupiec, 1992</marker>
<rawString>Kupiec, Julian. 1992. Robust part-of-speech tagging using a hidden Markov model. Computer Speech and Language, 6:225-242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Levin</author>
</authors>
<title>English Verb Classes and Alternations.</title>
<date>1993</date>
<institution>University of Chicago,</institution>
<location>Chicago.</location>
<contexts>
<context position="27759" citStr="Levin (1993" startWordPosition="4617" endWordPosition="4618">ypes, most tokens are nouns. Hence, for the low-frequency ranges, the data is weighted towards nouns. These two opposing forces conspire to yield a downward trend in the percentage of verbs as we proceed from the high- to the low-frequency ranges. Next, consider the English past tense versus past participle ambiguity. One of the important functions of the past participle form is as an adjectival modifier or predicate; for example, the parked car. In this function the past participle has a passive meaning with transitive verbs, and a perfective meaning with unaccusative intransitive verbs; see Levin (1993, 86-88) for details. For reasons that are not clear to us, a predominant number of the high-frequency verbs cannot felicitously be used as prenominal adjectives. These verbs include unergative intransitives like walk, for which one would not expect to find the adjectival usage, given the above characterization; but they also include clear transitives like move, try, and ask, and unaccusative intransitives like appear, which are not generally felicitous in this usage. Consider: ?a moved car, ?a tried approach, ?an asked question, ?an appeared ad; but contrast: an oft-tried approach, a frequent</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>Levin, Beth. 1993. English Verb Classes and Alternations. University of Chicago, Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Uit den Boogaart</author>
<author>P C</author>
<author>editor</author>
</authors>
<date>1975</date>
<booktitle>Woordfrequen ties in Gesproken en Geschreven Ned erlands. Oosthoek, Scheltema and Holkema,</booktitle>
<location>Utrecht.</location>
<marker>den Boogaart, C, editor, 1975</marker>
<rawString>Uit den Boogaart, P. C., editor. 1975. Woordfrequen ties in Gesproken en Geschreven Ned erlands. Oosthoek, Scheltema and Holkema, Utrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Word-sense disambiguation using statistical models of roget&apos;s categories trained on large corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of COLING-92,</booktitle>
<publisher>COLING.</publisher>
<location>Nantes, France,</location>
<contexts>
<context position="4386" citStr="Yarowsky 1992" startWordPosition="666" endWordPosition="667"> two forms, as indicated in the phonetic transcriptions. Syncretism and related morphological ambiguities present a problem for many NL applications where lexical disambiguation is important; cases where the orthographic form is identical but the pronunciations of the various functions differ are particularly important for speech applications, such as text-to-speech, since appropriate word pronunciations must be computed from orthographic forms that underspecify the necessary information. Ideally one would like to build models that use contextual information to perform lexical disambiguation (Yarowsky 1992, 1994), but such models must be trained on specialized tagged corpora (either hand-generated or semi-automatically generated) and such training corpora are often not available, at least in the early phases of constructing a particular application. Lacking good contextual models, one is forced to fall back on estimates of the lexical prior probabilities for the various functions of a form. Following standard terminology, a lexical prior can be defined as follows: Imagine that a given form is n-ways ambiguous; the lexical prior probability of sense i of this form is simply the probability of se</context>
</contexts>
<marker>Yarowsky, 1992</marker>
<rawString>Yarowsky, David. 1992. Word-sense disambiguation using statistical models of roget&apos;s categories trained on large corpora. In Proceedings of COLING-92, Nantes, France, July. COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Decision lists for lexical ambiguity resolution.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Meeting. Association for Computational Linguistics.</booktitle>
<marker>Yarowsky, 1994</marker>
<rawString>Yarowsky, David. 1994. Decision lists for lexical ambiguity resolution. In Proceedings of the 32nd Annual Meeting. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>