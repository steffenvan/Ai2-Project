<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000111">
<title confidence="0.944716">
A Fast, Accurate, Non-Projective, Semantically-Enriched Parser
</title>
<author confidence="0.994274">
Stephen Tratz and Eduard Hovy
</author>
<affiliation confidence="0.872521333333333">
Information Sciences Institute
University of Southern California
Marina del Rey, California 90292
</affiliation>
<email confidence="0.999535">
{stratz,hovy}@isi.edu
</email>
<sectionHeader confidence="0.993897" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999026958333333">
Dependency parsers are critical components
within many NLP systems. However, cur-
rently available dependency parsers each ex-
hibit at least one of several weaknesses, in-
cluding high running time, limited accuracy,
vague dependency labels, and lack of non-
projectivity support. Furthermore, no com-
monly used parser provides additional shal-
low semantic interpretation, such as prepo-
sition sense disambiguation and noun com-
pound interpretation. In this paper, we present
a new dependency-tree conversion of the Penn
Treebank along with its associated fine-grain
dependency labels and a fast, accurate parser
trained on it. We explain how a non-projective
extension to shift-reduce parsing can be in-
corporated into non-directional easy-first pars-
ing. The parser performs well when evalu-
ated on the standard test section of the Penn
Treebank, outperforming several popular open
source dependency parsers; it is, to the best
of our knowledge, the first dependency parser
capable of parsing more than 75 sentences per
second at over 93% accuracy.
</bodyText>
<sectionHeader confidence="0.99913" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999860804347826">
Parsers are critical components within many natu-
ral language processing (NLP) systems, including
systems for information extraction, question answer-
ing, machine translation, recognition of textual en-
tailment, summarization, and many others. Unfortu-
nately, currently available dependency parsers suf-
fer from at least one of several weaknesses includ-
ing high running time, limited accuracy, vague de-
pendency labels, and lack of non-projectivity sup-
port. Furthermore, few parsers include any sort of
additional semantic interpretation, such as interpre-
tations for prepositions, possessives, or noun com-
pounds.
In this paper, we describe 1) a new dependency
conversion (Section 3) of the Penn Treebank (Mar-
cus, et al., 1993) along with the associated de-
pendency label scheme, which is based upon the
Stanford parser’s popular scheme (de Marneffe and
Manning, 2008), and a fast, accurate dependency
parser with non-projectivity support (Section 4) and
additional integrated semantic annotation modules
for automatic preposition sense disambiguation and
noun compound interpretation (Section 5). We show
how Nivre’s (2009) swap-based reordering tech-
nique for non-projective shift-reduce-style parsing
can be integrated into the non-directional easy-first
framework of Goldberg and Elhadad (2010) to sup-
port non-projectivity, and we report the results of our
parsing experiments on the standard test section of
the PTB, providing comparisons with several freely
available parsers, including Goldberg and Elhadad’s
(2010) implementation, MALTPARSER (Nivre et al.,
2006), MSTPARSER (McDonald et al., 2005; Mc-
Donald and Pereira, 2006), the Charniak (2000)
parser, and the Berkeley parser (Petrov et al., 2006;
Petrov and Klein, 2007).
The experimental results show that the parser is
substantially more accurate than Goldberg and El-
hadad’s original implementation, with fairly simi-
lar overall speed. Furthermore, the results prove
that Stanford-granularity dependency labels can be
learned by modern dependency parsing systems
when using our Treebank conversion, unlike the
Stanford conversion, for which Cer et al. (2010)
show that this isn’t the case.
The optional semantic annotation modules also
</bodyText>
<page confidence="0.937283">
1257
</page>
<note confidence="0.957699">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1257–1268,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.989301436619718">
perform well, with the preposition sense disam- Though there are many syntactic parsers than can
biguation module exceeding the accuracy of the pre- reconstruct the grammatical structure of a text, there
vious best reported result for fine-grained preposi- are few, if any, accurate and widely accepted sys-
tion sense disambiguation (85.7% vs Hovy et al.’s tems that also produce shallow semantic analysis of
(2010) 84.8%), the possessives interpretation sys- the text. For example, a parser may indicate that,
tem achieving over 85% accuracy, and the noun in the case of ‘ice statue’, ‘ice’ modifies ‘statue’ but
compound interpretation system performing simi- will not indicate that ‘ice’ is the substance of the
larly to an earlier version described by Tratz and statue. Similarly, a parser will indicate which words
Hovy (2010) at just over 79% accuracy. a preposition connects but will not give any seman-
2 Background tic interpretation (e.g., ‘the boy with the pirate hat’
The NLP community has recently seen a surge of → wearing or carrying, ‘wash with cold water’ →
interest in dependency parsing, with several CoNLL means, ‘shave with the grain’ → in the same direc-
shared tasks focusing on it (Buchholz and Marsi, tion as). While, in some cases, it may be possible to
2006; Nivre et al., 2007). One of the main advan- use the output from a separate system for this pur-
tages of dependency parsing is the relative ease with pose, doing so is often difficult in practice due to a
which it can handle non-projectivity1. Additionally, wide variety of complications, including program-
since each word is linked directly to its head via a ming language differences, alternative data formats,
link that, ideally, indicates the syntactic dependency and, sometimes, other parsers.
type, there is no difficulty in determining either the 3 Dependency Conversion
syntactic head of a particular word or the syntactic 3.1 Relations and Structure
relation type, whereas these issues often arise when Most recent English dependency parsers produce
dealing with constituent parses2. one of three sets of dependency types: unlabeled,
Unfortunately, most currently available depen- some variant of the coarse labels used by the
dency parsers produce relatively vague labels or, in CoNLL dependency parsing shared-tasks (Buchholz
many cases, produce no labels at all. While the and Marsi, 2006; Nivre et al., 2007) (e.g., ADV,
Stanford fine-grain dependency scheme (de Marn- NMOD, PMOD), or Stanford’s dependency labels
effe and Manning, 2008) has proven to be popular, (de Marneffe and Manning, 2008). Unlabeled de-
recent experiments by Cer et al. (2010) using the pendencies are clearly too impoverished for many
Stanford conversion of the Penn Treebank indicate tasks. Similarly, the coarse labels of the CoNLL
that it is difficult for current dependency parsers to tasks are not very specific; for example, the same re-
learn. Indeed, the highest scoring parsers trained us- lation, NMOD, is used for determiners, adjectives,
ing the MSTPARSER (McDonald and Pereira, 2006) nouns, participle modifiers, relative clauses, etc. that
and MALTPARSER (Nivre et al., 2006) parsing suites modify nouns. In contrast, the Stanford relations
achieved only 78.8 and 81.1 labeled attachment provide a more reasonable level of granularity.
F1, respectively. This contrasted with the much Our dependency relation scheme is similar to
higher performance obtained using a constituent-to- Stanford’s basic scheme but has several differ-
dependency conversion approach with accurate, but ences. It introduces several new relations including
much slower, constituency parsers such as the Char- ccinit “initial coordinating conjunction”, cleft “cleft
niak and Johnson (2005) and Berkeley (Petrov et clause”, combo “combined term”, extr “extraposed
al., 2006; Petrov and Klein, 2007) parsers, which element”, infmark “infinitive marker ‘to’ ”, objcomp
achieved 89.1 and 87.9 labeled F1 scores, respec- “object complement”, postloc “post-modifying lo-
tively. cation”, sccomp “clausal complement of ‘so’ ”, vch
“verbal chain” and whadvmod “wh- adverbial mod-
ifier”. The nsubjpass, csubjpass, and auxpass rela-
tions of Stanford’s are left out because adding them
up front makes learning more difficult and the fact
1A tree is non-projective if the sequence of words visited in
a left-to-right, depth-first traversal of the sentence’s parse tree is
different than the actual word order of the sentence.
2These latter two issues are not problems for constituent
parses with binarized output and functional tags.
1258
abbrev abbreviation csubjpass clausal subject (passive) pobj prepositional object
acomp adjectival complement det determiner poss possessive
advcl adverbial clause dobj direct object possessive possessive marker
advmod adverbial modifier extr extraposed element postloc post-modifying location
agent ‘by’ agent expl ‘there’ expletive preconj pre conjunct
amod adjectival modifier infmark infinitive marker (‘to’) predet predeterminer
appos appositive infmod infinite modifier prep preposition
attr attributive iobj indirect object prt particle
aux auxillary mark subordinate clause marker punct punctuation
auxpass auxillary (passive) measure measure modifier purpcl purpose clause
cleft cleft clause neg negative quantmod quantifier modifier
cc coordination nn noun compound rcmod relative clause
ccinit initial CC nsubj nominal subject rel relative
ccomp clausal complement nsubjpass nominal subject (passive) sccomp clausal complement of ‘so’
combo combination term num numeric modifier tmod temporal modifier
compl complementizer number compound number vch verbal chain
conj conjunction objcomp object complement whadvmod wh- adverbial
cop copula complement parataxis parataxis xcomp clausal complement w/o subj
csubj clausal subject partmod participle modifier
</bodyText>
<tableCaption confidence="0.99881">
Table 1: Dependency scheme with differences versus basic Stanford dependencies highlighted. Bold indicates the
relation does not exist in the Stanford scheme. Italics indicate the relation appears in Stanford’s scheme but not ours.
</tableCaption>
<bodyText confidence="0.959816272727273">
that a nsubj, csubj, or aux is passive can easily be de-
termined from the final tree. Stanford’s aux depen-
dencies are replaced using verbal chain (vch) links;
conversion of these to Stanford-style aux dependen-
cies is also trivial as a post-processing step.3 The attr
dependency is excluded because it is redundant with
the cop relation due to different handling of copula,
and the dependency scheme does not have an abbrev
label because this information is not provided by the
Penn Treebank. The dependency scheme with dif-
ferences with Stanford highlighted is presented in
Table 1.
In addition to using a slightly different set of de-
pendency names, a handful of relations, notably cop,
conj, and cc, are treated in a different manner. These
differences are illustrated by Figure 1. The Stan-
ford scheme’s treatment of copula may be one rea-
son why dependency parsers have trouble learning
and applying it. Normally, the head of the clause
is a verb, but, under Stanford’s scheme, if the verb
happens to be a copula, the complement of the cop-
ula (cop) is treated as the head of the clause instead.
</bodyText>
<footnote confidence="0.987645666666667">
3The parsing system includes an optional script that can con-
vert vch arcs into aux and auxpass and the subject relations into
csubjpass and nsubjpass.
</footnote>
<figureCaption confidence="0.997225666666667">
Figure 1: Example comparing Stanford’s (top) handling
of copula and coordinating conjunctions with ours (bot-
tom).
</figureCaption>
<subsectionHeader confidence="0.997572">
3.2 Conversion Process
</subsectionHeader>
<bodyText confidence="0.993569923076923">
A three-step process is used to convert the Penn
Treebank (Marcus, et al., 1993) from constituent
parses into dependency trees labeled according to
the dependency scheme presented in the prior sec-
tion. The first step is to apply the noun phrase
structure patch created by Vadas and Curran (2007),
which adds structure to the otherwise flat noun
phrases (NPs) of the Penn Treebank (e.g., ‘(metal
soup pot cover)’ would become ‘(metal (soup pot)
cover)’). The second step is to apply a version
of Johansson and Nugues’ (2007) constituent-to-
dependency converter with some head-finding rule
modifications; these rules, with changes highlighted
</bodyText>
<page confidence="0.993428">
1259
</page>
<table confidence="0.929103285714286">
(WH)?NP|NX|NML|NAC FW|NML|NN* JJR $|# CD|FW QP JJ|NAC JJS PRP ADJP RB[SR] VBG|DT|WP
RB NP-C S|SBAR|UCP|PP SINV|SBARQ|SQ UH VP|NP VB|VBP
ADJP|JJP NNS QP NN $|# JJ VBN VBG (AD|J)JP ADVP JJR NP|NML JJS DT FW RBR RBS SBAR RB
ADVP RB|RBR|JJ|JJR RBS FW ADVP TO CD IN NP|NML JJS NN
PRN S* VP NN*|NX|NML NP W* PP|IN ADJP|JJ ADVP RB NAC VP INTJ
QP $|# NNS NN CD JJ RB DT NCD QP IN CC JJR JJS
SBARQ SQ S SBARQ SINV FRAG
SQ VBZ VBD VBP VB MD *-PRD SQ VP FRAG X
UCP [QNVP]P|S*|UCP|NML|PR[NT]|RRC|NX|NAC|FRAG|INTJ|AD[JV]P|LST|WH*|X
VP VBD|AUX VBN MD VBZ VB VBG VBP VP POS *-PRD ADJP JJ NN NNS NP|NML
WHADJP CC JJ WRB ADJP
WHADVP CC WRB|RB
X [QNVP]P|S*|UCP|NML|PR[NT]|RRC|NX|NAC|FRAG|INTJ|AD[JV]P|LST|WH*|X|CONJP
LST LS : DT|NN|SYM
</table>
<figureCaption confidence="0.852288666666667">
Figure 2: Modified head-finding rules. Underline indicates that the search is performed in a left-to-right fashion instead
of the default right-to-left order. NML and JJP are both products of Vadas and Curran’s (2007) patch. Bold indicates
an added or moved element; for the original rules, see the paper by Johansson and Nugues (2007).
</figureCaption>
<bodyText confidence="0.999942186046512">
in bold, are provided in Figure 2. Finally, an addi-
tional script makes additional changes and converts
the intermediate output into the dependency scheme.
This dependency conversion has several advan-
tages to it. Using the modified head-finding rules for
Johansson and Nugues’ (2007) converter results in
fewer buggy trees than were present in the CoNLL
shared tasks, including fewer trees in which words
are headed by punctuation marks. For sections 2–
21, there are far fewer generic dep/DEP relations
(2,765) than with the Stanford conversion (34,134)
or the CoNLL 2008 shared task conversion (23,811).
Also, the additional conversion script contains vari-
ous rules for correcting part-of-speech (POS) errors
using the syntactic structure as well as additional
rules for some specific word forms, mostly common
words with inconsistent taggings. Many of these
changes cover part-of-speech problems discussed by
Manning (2011), including VBD/VBN, VBZ/NNS,
NNP/NNPS, and IN/WDT/DT issues. In total, the
script changes over 9,500 part-of-speech tags, with
the most common change being to change preposi-
tion tags (IN) into adverb tags (RB) for cases where
there is no prepositional complement/object. The
top fifteen of these changes are presented in Table
2. The conversion script contains a variety of ad-
ditional rules for modifying the parse structure and
fixing erroneous trees as well, including cases where
one or more POS tags were incorrect and, as such,
the initial dependency parse was flawed. Quick
manual inspections of the changes suggested that the
vast majority are accurate.
In the final output from the conversion, the num-
ber of sentences with one or more words dependent
on non-projective arcs in sections 2–21 is 3,245—
about 8.1% of the dataset. About 1.3% of this, or
556 of sentences, is due to the secondary conver-
sion script, with sentences containing approximate
currency amounts (e.g., about $ 10) comprising the
bulk of difference. For these, the quantifying text
(e.g., about, over, nearly), is linked to the number
following the currency symbol instead of to the cur-
rency symbol as it was in the CoNLL 2008 task.
</bodyText>
<table confidence="0.999865375">
Original New # of changes
IN RB 1128
JJ NN 787
VBD VBN 601
RB IN 462
VBN VBD 441
NN JJ 409
NNPS NNP 405
IN WDT 388
VBG NN 223
DT IN 220
RB JJ 214
VB VBP 184
NN NNS 169
RB NN 157
NNS VBZ 148
</table>
<tableCaption confidence="0.9814705">
Table 2: Top 15 part-of-speech tag changes performed by
the conversion script.
</tableCaption>
<page confidence="0.753477">
1260
</page>
<listItem confidence="0.3131855">
4 Parser in practice, especially for languages with relatively
4.1 Algorithm fixed word order such as English.5 Though Gold-
</listItem>
<bodyText confidence="0.990373390243902">
The parsing approach is based upon the non- berg and Elhadad’s (2010) original implementation
directional easy-first algorithm recently presented only supports unlabeled dependencies, the algorithm
by Goldberg and Elhadad (2010). Their original al- itself is in no way limited in this regard, and it is
gorithm behaves as follows. For a sentence of length simple enough to add labeled dependency support
n, the algorithm performs a total of n steps. In each by treating each dependency label as a specific type
step, one of the unattached tokens is added as a child of attach operation (e.g., attach_as_nsubj), which
to one of its current neighbors and is then removed is the method used by this implementation. Pseu-
from the list of unprocessed tokens. When only one docode for the non-directional easy-first algorithm
token remains unprocessed, it is designated as the with non-projective support is given in Algorithm 1.
root. Provided that only a constant number of po- input : w1 ... wn, #the sentence
tential attachments need to be re-evaluated after each m, #the model
step, which is the case if one restricts the context for k, #the context width
feature generation to a constant number of neigh- actions, #the list of parse actions
boring tokens, the algorithm can be implemented to O, #the feature generator
run in O(n log n). However, since only O(n) dot output: tree #a collection of dependency arcs
products must be calculated by the parser and these words= copyOf(s);
have a large constant associated with them, the run- stale = copyOf (s);
ning time will rival O(n) parsers for any reasonable cache; #cache of action scores
n, and, thus, a naive O(n2) implementation will be while |words |&gt; 1 do
nearly as fast as a priority queue implementation in for w ∈ stale do
practice.4 for act ∈ actions do
The algorithm has a couple potential advantages cache[w,act] = score(act, O(w,...),
over standard shift-reduce style parsing algorithms. m);
The first advantage is that performing easy ac- stale.remove(w);
tions first may make the originally difficult deci- best = arg max cache[w,a]
sions easier. The second advantage is that perform- a∈actions&amp;valid(a),w∈words
ing parse actions in a more flexible order than left- if isMove(best) then
to-right/right-to-left shift-reduce parsing reduces the i=
chance of error propagation. words.index(getTokenToMove(best));
Unfortunately, the original algorithm does not words.move (i, isMoveLeft(best) ? -1
support non-projective trees. To extend the algo- : 1);
rithm to support non-projective trees, we introduce
move-right and move-left operations similar to the
stack-to-buffer swaps proposed by Nivre (2009) for
shift-reduce style parsing. Thus, instead of attaching
a token to one of its neighbors at each step, the algo-
rithm may instead decide to move a token past one
of its neighbors. Provided that no node is allowed
to be moved past a token in such a way that a previ-
ous move operation is undone, there can be at most
O(n2) moves and the overall worst-case complexity
</bodyText>
<table confidence="0.918576285714286">
becomes O(n2 log n). While theoretically slower,
this has a limited impact upon actual parsing times
else
arc = createArc(best);
tree.add(arc);
i = words.index(getChild(arc));
words.remove(i);
for x ∈ -k,...,k do
stale.add(words.get(index+x));
return tree
Algorithm 1: Modified version of Goldberg and
Elhadad’s (2010) Easy-First Algorithm with non-
projective support.
5See Nivre (2009) for more information on the effect of re-
ordering operations on parse time.
4See Goldberg and Elhadad (2010) for more explanation.
1261
4.2 Features tion. The general idea of the algorithm is to iterate
One of the key aspects of the parser is the complex over the sentences and, whenever the model predicts
set of features used. The feature set is based off an incorrect action, update the model weights. Fol-
the features used by Goldberg and Elhadad (2010) lowing Goldberg and Elhadad, parameter averaging
</table>
<bodyText confidence="0.94014105">
but has a significant number of extensions. Various is used to reduce overfitting.
feature templates are specifically designed to pro- Our implementation varies slightly from that of
duce features that help with several syntactic issues Goldberg and Elhadad (2010). The difference is
including preposition attachment, coordination, ad- that, at any particular step for a given sentence, the
verbial clauses, clausal complements, and relative algorithm continues to update the weight vector as
clauses. Unfortunately, there is insufficient space in long as any invalid action is scored higher than any
this paper to describe them all here. However, a list valid action, not just the highest scoring valid ac-
of feature templates will be provided with the parser tion; unfortunately, this change significantly slowed
download. down the training process. In early experiments, this
Several of the feature templates use unsupervised change produced a slight improvement in accuracy
word clusters created with the Brown et al. (1992) though it also slowed training significantly. In later
hierarchical clustering algorithm. The use of this al- experiments using additional feature templates, this
gorithm was inspired by Koo et al. (2008), who used change ceased to have any notable impact on the
the top branches of the cluster hierarchy as features. overall accuracy, but it was kept anyway. 6
However, unlike Koo et al.’s (2008) parser, the fine- The oracle used to determine whether a move op-
grained cluster identifiers are used instead of just eration should be considered legal during the train-
the top 4-6 branches of the cluster hierarchy. The ing phase is similar to Nivre et al.’s (2009) improved
175 word clusters utilized by the parser were created oracle based upon maximal projective subcompo-
from the New York Times corpus (Sandhaus, 2008). nents. As an additional restriction, during training,
Some examples from the clusters are presented in move actions were only considered valid either if no
</bodyText>
<figureCaption confidence="0.9489115">
Figure 3. The ideal number of such clusters was not other action was valid or if the token to be moved
thoroughly investigated. already had all its children attached and moving it
</figureCaption>
<table confidence="0.884549777777778">
while where when although despite unless unlike ... caused it to be adjacent to its parent. This fits with
why what whom whatever whoever whomever whence ... Nivre et al.’s (2009) intuition that it is best to delay
based died involved runs ended lived charged born ... word reordering as long as possible.
them him me us himself themselves herself myself ... 4.4 Speed Enhancements
really just almost nearly simply quite fully virtually ... To enhance the speed for practical use, the parser
know think thought feel believe knew felt hope mean ... uses constraints based upon the part-of-speech tags
into through on onto atop astride Saturday/Early thru ... of the adjacent word pairs to eliminate invalid de-
Ms. Mr. Dr. Mrs. Judge Miss Professor Officer Colonel ... pendencies from even being evaluated. A rela-
John President David J. St. Robert Michael James George ... tion is only considered between a pair of words if
wife own husband brother sister grandfather beloved ... such a relation was observed in the training data
often now once recently sometimes clearly apparently ... between a pair of words with the same parts-of-
everyone it everybody somebody anybody nobody hers ... speech (with the exception of the generic dep de-
around over under among near behind outside across ... pendency, which is permitted between any POS tag
Clinton Bush Johnson Smith Brown Williams King ... pair). Early experiments utilizing similar constraints
children companies women people men things students ... showed an improvement in parsing speed of about
Figure 3: High frequency examples from 15 of the Brown 16% with no significant impact on accuracy, regard-
clusters. less of whether the constraints were enforced during
4.3 Training training.
The parsing model is trained using a variant of the
structured perceptron training algorithm used in the
original Goldberg and Elhadad (2010) implementa-
1262
6See Goldberg and Elhadad (2010) for more description of
the general training procedure.
System Arc Accuracy Perfect Non-Proj Arcs
Sentences
Labeled Unlabeled Labeled Unlabeled Labeled Unlabeled
THIS WORK 92.1 (93.3) 93.7 (94.3) 38.4 (42.5) 46.2 (48.5) 66.5 (69.7) 69.3 (71.7)
THIS WORKno clusters
THIS WORKmoves disabled
91.8 (93.1) 93.4 (94.1) 38.2 (42.3) 45.5 (47.3) 67.3 (70.9) 69.3 (72.5)
91.7 (92.9) 93.3 (93.9) 37.1 (40.8) 44.2 (46.2) 21.1 (21.1) 22.7 (21.9)
NON-DIR EASY FIRST * 91.2 (92.0) * 37.8 (39.4) * 15.1 (16.3)
EISNEREMST 90.9 (92.2) 92.8 (93.5) 32.1 (35.6) 40.6 (42.3) 62.5 (65.3) 63.7 (66.9)
CHU-LIU-EDMONDSMST
90.0 (91.2) 91.8 (92.5) 28.4 (31.3) 35.0 (36.4) 62.9 (65.3) 64.1 (66.5)
ARC-EAGERMalt 89.8 (91.1) 91.3 (92.1) 31.6 (34.2) 37.4 (38.5) 19.5 (19.5) 20.3 (19.9)
ARC-STANDARDMalt
STACK-EAGERMalt
STACK-LAZYMalt
88.3 (89.5) 89.7 (90.4) 31.4 (34.1) 36.1 (37.3) 13.1 (12.0) 13.9 (12.7)
90.0 (91.2) 91.5 (92.3) 34.5 (37.5) 40.4 (41.9) 51.8 (53.8) 53.8 (55.4)
90.4 (91.7) 91.9 (92.8) 34.8 (37.7) 40.6 (42.5) 61.8 (63.3) 63.3 (65.3)
CHARNIAKt * 93.2 * 43.5 * 32.3
BERKELEYt * 93.3 * 43.6 * 34.3
</table>
<tableCaption confidence="0.996946666666667">
Table 3: Parsing results for section 23 of the Penn Treebank (punctuation excluded). Results in parentheses were
produced using gold POS tags. EEisner (1996) algorithm with non-projective rewriting and second order features.
tResults not directly comparable; see text. *Labeled dependencies not available/comparable.
</tableCaption>
<subsectionHeader confidence="0.864486">
4.5 Evaluation
</subsectionHeader>
<bodyText confidence="0.991544844827587">
The following split of the Penn Treebank (Marcus,
et al., 1993) was used for the experiments: sections
2–21 for training, 22 for development, and 23 for
testing.
For part-of-speech (POS) tagging, we used an in-
house SVM-based POS tagger modeled after the
work of Giménez and Márquez (2004) 7. The train-
ing data was tagged in a 10-fold fashion; each fold
was tagged using a tagger trained from the nine re-
maining folds. The development and test sections
were tagged by an instance of the tagger trained us-
ing the entire training set. The full details of the
POS tagger are outside the scope of this paper; it is
included with the parser download.
The final parser was trained for 31 iterations,
which is the point at which its performance on the
development set peaked. One test run was per-
formed with non-projectivity support disabled in or-
der to get some idea of the impact of the move opera-
tions on the parser’s overall performance; also, since
the parsers used for comparison had no access to the
unsupervised word clusters, an additional instance
of the parser was trained with every word treated
as belonging to the same cluster so as to facilitate
a more fair comparison.
Seven different dependency parsing models were
797.42% accuracy on traditional POS evaluation (Penn Tree-
bank WSJ sections 22-24).
trained for comparison using the following open
source parsing packages: Goldberg and Elhadad’s
(2010)’s non-directional easy-first parser, MALT-
PARSER (Nivre et al., 2006), and MSTPARSER
(McDonald and Pereira, 2006)8. The model trained
using Goldberg and Elhadad’s (2010) easy-first
parser serves as something of a baseline. The
four MALTPARSER parsing models used the arc-
eager, arc-standard, stack-eager, and stack-lazy al-
gorithms. One of the MSTPARSER models used
the Chu-Liu-Edmonds maximum spanning tree ap-
proach, and the other used the Eisner (1996) al-
gorithm with second order features and a non-
projective rewriting post-processing step.
Unfortunately, it is not possible to directly com-
pare the parser’s accuracy with most popular con-
stituent parsers such as the Charniak (2000) and
Berkeley (Petrov et al., 2006; Petrov and Klein,
2007) parsers9 both because they do not pro-
duce functional tags for subjects, direct objects,
etc., which are required for the final script of the
constituent-to-dependency conversion routine, and
because they determine part-of-speech tags in con-
junction with the parsing. However, it is possible to
compute approximate unlabeled accuracy scores by
training the constituent parsers on the NP-patched
(Vadas and Curran, 2007) version of the data and
then running the test output through just the first
conversion script—that is, the modified version of
Johansson and Nugues’ (2007) converter.
</bodyText>
<page confidence="0.927179">
1263
</page>
<bodyText confidence="0.999855270270271">
The results of the experiment are given in Ta-
ble 3, including accuracy for individual arcs, non-
projective arcs only, and full sentence match. Punc-
tuation is excluded in all the result computations. To
determine whether an arc is non-projective, the fol-
lowing heuristic was used. Traverse the sentence in
a depth-first search, starting from the imaginary root
node and pursuing child arcs in order of increasing
absolute distance from their parent. Whenever an
arc being traversed is found to cross a previously tra-
versed arc, mark it as non-projective and continue.
To evaluate the impact of part-of-speech tagging er-
ror, results for parsing using the gold standard part-
of-speech tags are also included.
We also measured the speed of the parser on the
various sentences in the test collection. For reason-
able sentence lengths, the parser scales quite well.
The scatterplot depicting the relation between sen-
tence length and parsing time is presented in Figure
5.
just over 30 seconds—a rate of over 75 sentences per
second, substantially faster than most of the other
parsers.
Not surprisingly, the results for non-projective
arcs are substantially lower than the results for all
arcs, and the systems that are designed to handle
them outperformed the strictly projective parsers in
this regard.
The negative effect of part-of-speech tagging er-
ror appears to impact the different parsers about the
same amount, with a loss of .6% to .8% in unlabeled
accuracy and 1.1% to 1.3% in labeled accuracy.
The 93.2% and 93.3% accuracy scores achieved
by the Charniak and Berkeley parsers are not too
different from the 93.7% result, but, of course, it is
important to remember that these scores are not di-
rectly comparable.
</bodyText>
<figureCaption confidence="0.983525125">
Figure 5: Sentence length versus parse time. Median
times for five runs over section 23.
Figure 4: Parse times for Penn Treebank section 23 for
the parsers on a PC with a 2.4Ghz Q6600 processor and
8GB RAM. MALTPARSER ran substantially slower than
the others, perhaps due to its use of polynomial kernels,
and isn’t shown. (C-L-E - Chu-Liu-Edmonds, G&amp;E -
Goldberg and Elhadad (2010)).
</figureCaption>
<sectionHeader confidence="0.736565" genericHeader="introduction">
4.5.1 Results Discussion
</sectionHeader>
<bodyText confidence="0.9999772">
The parser achieves 92.1% labeled and 93.7% un-
labeled accuracy on the evaluation, a solid result and
about 2.5% higher than the original easy-first imple-
mentation of Goldberg and Elhadad (2010). Further-
more, the parser processed the entire test section in
</bodyText>
<footnote confidence="0.994734">
8Versions 1.4.1, 0.4.3b, and 0.2, respectively
9Versions 1.1 and 05Aug16, respectively
</footnote>
<sectionHeader confidence="0.949644" genericHeader="method">
5 Shallow Semantic Annotation
</sectionHeader>
<bodyText confidence="0.994789142857143">
To create a more informative parse, the parser in-
cludes four optional modules, a preposition sense
disambiguation (PSD) system, a work-in-progress
’s-possessive interpretation system, a noun com-
pound interpretation system, and a PropBank-based
semantic role labeling system10. Taken together,
these integrated modules enable the parsing sys-
tem to produce substantially more informative out-
put than a traditional parser.
Preposition Sense Disambiguation The PSD
system is a newer version of the system described
10Lack of space prohibits a sufficiently thorough discussion
of these individual components and their evaluations, but addi-
tional information will be available with the system download.
</bodyText>
<page confidence="0.993403">
1264
</page>
<figureCaption confidence="0.978552322580645">
by Tratz and Hovy (2009) and Hovy et al. (2010); it duce non-projectivity into it. Examples of this in-
achieves 85.7% accuracy on the SemEval-2007 fine- clude McDonald and Pereira’s (2006) rewriting of
grain PSD task (Litkowski and Hargraves, 2007), projective trees produced by the Eisner (1996) al-
which is a statistically significant (p&lt;=0.05; upper- gorithm, and Nivre and Nilsson’s (2005) pseudo-
tailed z test) increase over the previous best reported projective approach that creates projective trees with
result for this dataset, Hovy et al.’s (2010) 84.8%. specially marked arcs that are later transformed into
Noun Compound Interpretation The noun com- non-projective dependencies.
pound interpretation system is a newer version of Descriptive dependency labels. While most re-
the system described by Tratz and Hovy (2010) with cent dependency parsing research has used either
similar accuracy (79.6% vs 79.3% using 10-fold vague labels, such as those of the CoNLL shared
cross-validation11). tasks, or no labels at all, some descriptive depen-
Possessives Interpretation The possessive inter- dency label schemes exist. By far the most promi-
pretation system assigns interpretations to ’s pos- nent of these is the Stanford typed dependency
sessives (e.g., John’s arm → PART-OF, Mowgli’s scheme (de Marneffe and Manning, 2008). An-
capture → PATIENT/THEME). The current system other descriptive scheme that exists, but which is
achieves over 85.0% accuracy, but it is important to less widely used in the NLP community, is the one
note that the annotation scheme, automatic classifier, used by Tapanainen and Järvinen’s parser (1997).
and dataset are all still under active development. Unfortunately, the Stanford dependency conversion
PropBank SRL The PropBank-based semantic of the Penn Treebank has proven difficult to learn for
role labeling system achieves 86.8 combined F1 current dependency parsers (Cer et al., 2010), and
measure for automatically-generated parse trees cal- there is no publicly available dependency conversion
culated over both predicate disambiguation and ar- according to Tapanainen and Järvinen’s scheme.
gument/adjunct classification (89.5 F1 on predicate Faster parsing. While the fastest reasonable
disambiguation, 85.6 F1 on argument and adjuncts parsing algorithms are the O(n) shift-reduce algo-
corresponding to dependency links, and 86.8 F1); rithms, such as Nivre’s (2003) algorithm and an ex-
this score is not directly comparable to any previ- pected linear time dynamic programming approach
ous work due to some differences, including differ- presented by Huang and Sagae (2010), a few other
ences in both the parse tree conversion and the Prop- fast alternatives exist. Goldberg and Elhadad’s
Bank conversion. The most similar work is that of (2010) easy-first algorithm is one such example. An-
the CoNLL shared task work (Surdeanu et al., 2008; other example, is Roark and Hollingshead’s (2009)
Hajiˇc et al., 2009). work that uses chart constraints to achieve linear
</figureCaption>
<sectionHeader confidence="0.288384" genericHeader="method">
6 Related Work time complexity for constituency parsing.
</sectionHeader>
<bodyText confidence="0.717069037037037">
Non-projectivity. There are two main approaches Effective features for parsing. A variety of work
used in recent NLP literature for handling non- has investigated the use of more informative fea-
projectivity in parse trees. The first is to use an al- tures for parsing. This includes work that inte-
gorithm, like the one presented in this paper, that grates second and even third order features (McDon-
has inherent support for non-projective trees. Ex- ald et al., 2006; Carreras, 2007; Koo and Collins,
amples of this include the Chu-Liu-Edmonds’ ap- 2010). Also, some work has incorporated unsuper-
proach for maximum spanning tree (MST) parsing vised word clusters as features, including that of Koo
(McDonald et al., 2005) and Nivre’s (2009) swap- et al. (2008) and Suzuki et al. (2009), who utilized
based reordering method for shift-reduce parsing. unsupervised word clusters created using the Brown
The second approach is to create an initial projec- et al. (1992) hierarchical clustering algorithm.
tive parse and then apply transformations to intro- Semantically-enriched output. The 2008 and
2009 CoNLL shared tasks (Surdeanu et al., 2008;
Hajiˇc et al., 2009), which required participants to
build systems capable of both syntactic parsing and
Semantic Role Labeling (SRL) (Gildea and Juraf-
sky, 2002), are the most notable attempts to encour-
11These accuracy figures are higher than what should be ex-
pected for unseen datasets; see Tratz and Hovy (2010) for more
detail.
1265
age the development of parsers with additional se-
mantic annotation. These tasks relied upon Prop-
Bank (2005) and NomBank (2004) for the seman-
tic roles. A variety of other systems have focused
on FrameNet-based (1998) SRL instead, including
those that participated in the SemEval-2007 Task 19
(Baker et al., 2007) and work by Das et al. (2010).
</bodyText>
<sectionHeader confidence="0.990321" genericHeader="method">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999970962264151">
In this paper, we have described a new high-quality
dependency tree conversion of the Penn Treebank
(Marcus, et al., 1993) along with its labeled depen-
dency scheme and presented a parser that is fast, ac-
curate, supports non-projective trees and provides
rich output, including not only informative depen-
dency labels similar to Stanford’s but also additional
semantic annotation for prepositions, possessives,
and noun compound relations. We showed how the
easy-first algorithm of Goldberg and Elhadad (Gold-
berg and Elhadad, 2010) can be extended to support
non-projective trees by adding move actions similar
to Nivre’s (2009) swap-based reordering for shift-
reduce parsing and evaluated our parser on the stan-
dard test section of the Penn Treebank, comparing
with several other freely available parsers.
The Penn Treebank conversion process fixes a
number of buggy trees and part-of-speech tags and
produces dependency trees with a relatively small
percentage of generic dep dependencies. The ex-
perimental results show that dependency parsers can
generally produce Stanford-granularity labels with
high accuracy when using the new dependency con-
version of the Penn Treebank, something which, ac-
cording to the findings of Cer et al. (2010), does
not appear to be the case when training and testing
dependency parsers on the Stanford conversion.
The parser achieves high labeled and unlabeled
accuracy in the evaluation, 92.1% and 93.7%, re-
spectively. The 93.7% result represents a 2.5% in-
crease over the accuracy of Goldberg and Elhadad’s
(2010) implementation. Also, the parser proves to
be quite fast, processing section 23 of the Penn Tree-
bank in just over 30 seconds (a rate of over 75 sen-
tences per second).
The parsing system is capable of not only produc-
ing fine-grained dependency relations, but can also
produce shallow semantic annotations for preposi-
tions, possessives, and noun compounds by using
several optional integrated modules. The preposi-
tion sense disambiguation (PSD) module achieves
85.7% accuracy on the SemEval-2007 PSD task, ex-
ceeding the previous best published result of 84.8%
by a statistically significant margin, the possessives
module is over 85% accurate, the noun compound
interpretation module achieves 79.6% accuracy on
Tratz and Hovy’s (2010) dataset. The PropBank
SRL module achieves 89.5 F1 on predicate disam-
biguation and 85.6 F1 on argument and adjuncts cor-
responding to dependency links, for an overall F1 of
86.8. Combined with the core parser, these modules
allow the system to produce a substantially more in-
formative textual analysis than a standard parser.
</bodyText>
<sectionHeader confidence="0.999532" genericHeader="discussions">
8 Future Work
</sectionHeader>
<bodyText confidence="0.999975428571429">
There are a variety of ways to extend and improve
upon this work. We would like to change our han-
dling of coordinating conjunctions to treat the co-
ordinating conjunction as the head because this has
fewer ambiguities than the current approach and also
add the ability to produce traces for WH- words. It
would also be interesting to examine the impact on
final parsing accuracy of the various differences be-
tween our dependency conversion and Stanford’s.
To aid future NLP research work, the code,
including the treebank converter, part-of-speech
tagger, parser, and semantic annotation add-ons,
will be made publicly available for download via
http://www.isi.edu.
</bodyText>
<sectionHeader confidence="0.99495" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999728">
We would like to thank Richard Johansson for
providing us with the code for the pennconverter
consituent-to-dependency converter. We would also
like to thank Dirk Hovy and Anselmo Peiias for
many valuable dicussions and suggestions.
</bodyText>
<sectionHeader confidence="0.999007" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996954166666667">
Collin Baker, and Michael Ellsworth and Katrin Erk.
2007. SemEval’07 task 19: Frame Semantic Structure
Extraction. In Proc. of the 4th International Workshop
on Semantic Evaluations
Collin Baker, Charles J. Fillmore and John B. Lowe.
1998. The Berkeley FrameNet Project. In Proc. of the
</reference>
<page confidence="0.973495">
1266
</page>
<note confidence="0.732288">
17th international conference on Computational lin-
guistics
</note>
<reference confidence="0.999168990825688">
Adam L. Berger, Vincent J. Della Pietra, and Stephen A.
Della Pietra. 1996. A maximum entropy approach to
natural language processing. In Computational Lin-
guistics 22(1):39–71
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
Based n-gram Models of Natural Language. Compu-
tational Linguistics 18(4):467–479.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proc. of CoNLL 2006.
Xavier Carreras. 2007. Experiments with a Higher-
Order Projective Dependency Parser. In Proc. of the
CoNLL Shared Task Session of EMNLP-CoNLL 2007.
Daniel Cer, Marie-Catherine de Marneffe, Daniel Juraf-
sky, and Christopher D. Manning. 2010. Parsing to
Stanford Dependencies: Trade-offs between speed and
accuracy. In Proc. of LREC 2010.
Eugene Charniak. 2000. A Maximum-Entropy-Inspired
Parser. In Proc. of NAACL 2000.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
find-grained n-best parsing and discriminative rerank-
ing. In Proc. of ACL 2005.
Michael A. Covington. 2001. A Fundamental Algorithm
for Dependency Parsing. In Proc. of the 39th Annual
ACM Southeast Conference.
Dipanjan Das, Nathan Schneider, Desai Chen, and Noah
A. Smith. 2010. Probabilistic Frame-Semantic Pars-
ing. In Proc. of HLT-NAACL 2010.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies repre-
sentation. In COLING Workshop on Cross-framework
and Cross-domain Parser Evaluation.
Jason Eisner. 1996. Three New Probabilistic Models
for Dependency Parsing: An Exploration. In Proc. of
COLING 1996.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics.
28(3):245–288.
Jesús Giménez and Lluís Márquez 2004. SVMTool: A
General POS Tagger Generator Based on Support Vec-
tor Machines. In Proc. of LREC 2004.
Yoav Goldberg and Michael Elhadad. 2010. An Ef-
ficient Algorithm for Easy-First Non-Directional De-
pendency Parsing. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the ACL.
Yoav Goldberg and Michael Elhadad. 2009. The
CoNLL-2009 Shared Task: Syntactic and Semantic
Dependencies in Multiple Languages. In Proc. of
the Thirteenth Conference on Computational Natural
Language Learning: Shared Task.
Dirk Hovy, Stephen Tratz, and Eduard Hovy. 2010.
What’s in a Preposition?—Dimensions of Sense Dis-
ambiguation for an Interesting Word Class. In Proc. of
COLING 2010.
Liang Huang and Kenji Sagae. 2010. Dynamic Program-
ming for Linear-Time Shift-Reduce Parsing. In Proc.
of ACL 2010.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for english. In
Proc. of NODALIDA.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple Semi-supervised Dependency Parsing. In
Proc. of ACL 2008.
Terry Koo and Michael Collins. 2010. Efficient Third-
order Dependency Parsers. In Proc. ofACL 2010.
Ken Litkowski and Orin Hargraves. 2007. SemEval-
2007 Task 06: Word-Sense Disambiguation of Prepo-
sitions. In Proc. of the 4th International Workshop on
Semantic Evaluations.
Christopher D. Manning. 2011. Part-of-Speech Tagging
from 97% to 100%: Is It Time for Some Linguistics?
In Proc. of the 12th International Conference on Intel-
ligent Text Processing and Computational Linguistics
(CICLing 2011).
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: the Penn TreeBank. Computational
Linguistics, 19(2):313–330.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajiˇc. 2005. Non-Projective Dependency Parsing
Using Spanning Tree Algorithms. In Proc. of HLT-
EMNLP 2005.
Ryan McDonald and Fernando Pereira. 2006. Online
Learning of Approximate Dependency Parsing Algo-
rithms. In Proc. of EACL 2006.
Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel
Szekely, Veronika Zielinska, Brian Young and Ralph
Grishman. 2004. The NomBank Project: An Interim
Report. In Proc. of the NAACL/HLT Workshop on
Frontiers in Corpus Annotation.
Joakim Nivre. 2009. Non-Projective Dependency Pars-
ing in Expected Linear Time. In Proc. of the 47th An-
nual Meeting of the ACL and the 4th IJCNLP of the
AFNLP.
Joakim Nivre. 2003. An Efficient Algorithm for Projec-
tive Dependency Parsing. In Proc. of the 8th Interna-
tional Workshop on Parsing Technologies (IWPT).
Joakim Nivre, Marco Kuhlmann, and Johan Hall. 2009.
An Improved Oracle for Dependency Parsing with On-
line Reordering. In Proc. of the 11th International
Conference on Parsing Technologies (IWPT).
Joakim Nivre, Johan Hall, Sandra Kübler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007. The CoNLL 2007 shared task on dependency
parsing. In Proc. of EMNLP-CoNLL 2007.
</reference>
<page confidence="0.807835">
1267
</page>
<reference confidence="0.99982558">
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Malt-
Parser: A Data-Driven Parser-Generator for Depen-
dency Parsing. In Proc. of LREC 2006.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective
dependency parsing. In Proc. of ACL-2005.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An Annotated Cor-
pus of Semantic Roles. In Computational Linguistics.
31(1):71–106.
Slav Petrov and Dan Klein. 2007. Improved Inference
for Unlexicalized Parsing. In Proc. of HLT-NAACL
2007.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proc. of COLING-ACL
2006.
Brian Roark and Kristy Hollingshead. 2009. Linear
complexity context-free parsing pipelines via chart
constraints. In Proc. of HLT-NAACL.
Evan Sandhaus. 2008. The New York Times Annotated
Corpus. Linguistic Data Consortium, Philadelphia.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Lluís Màrquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In Proc. of the Twelfth Confer-
ence on Computational Natural Language Learning.
Jun Suzuki, Hideki Isozaki, Xavier Carrerras, and
Michael Collins. 2009. An Empirical Study of Semi-
supervised Structured Conditional Models for Depen-
dency Parsing. In Proc. of EMNLP.
Pasi Tapanainen and Timo Järvinen. 1997. A non-
projective dependency parser. In Proc. of the fifth con-
ference on applied natural language processing.
Stephen Tratz and Dirk Hovy. 2009. Disambiguation of
Preposition Sense using Linguistically Motivated Fea-
tures. In Proc. of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
Companion Volume: Student Research Workshop and
Doctoral Consortium.
Stephen Tratz and Eduard Hovy. 2010. A Taxonomy,
Dataset, and Classifier for Automatic Noun Com-
pound Interpretation. In Proc. of ACL 2010.
David Vadas and James R. Curran. 2007. Adding Noun
Phrase Structure to the Penn Treebank. In Proc. of
ACL 2007.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical Dependency Analysis With Support Vector Ma-
chines. In Proc. of 8th International Workshop on
Parsing Technologies (IWPT).
</reference>
<page confidence="0.992384">
1268
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.880791">
<title confidence="0.998325">A Fast, Accurate, Non-Projective, Semantically-Enriched Parser</title>
<author confidence="0.967654">Tratz Hovy</author>
<affiliation confidence="0.9994625">Information Sciences Institute University of Southern California</affiliation>
<address confidence="0.992096">Marina del Rey, California 90292</address>
<email confidence="0.999231">stratz@isi.edu</email>
<email confidence="0.999231">hovy@isi.edu</email>
<abstract confidence="0.9967352">Dependency parsers are critical components within many NLP systems. However, currently available dependency parsers each exhibit at least one of several weaknesses, including high running time, limited accuracy, vague dependency labels, and lack of nonprojectivity support. Furthermore, no commonly used parser provides additional shallow semantic interpretation, such as preposition sense disambiguation and noun compound interpretation. In this paper, we present a new dependency-tree conversion of the Penn Treebank along with its associated fine-grain dependency labels and a fast, accurate parser trained on it. We explain how a non-projective extension to shift-reduce parsing can be incorporated into non-directional easy-first parsing. The parser performs well when evaluated on the standard test section of the Penn Treebank, outperforming several popular open source dependency parsers; it is, to the best of our knowledge, the first dependency parser capable of parsing more than 75 sentences per second at over 93% accuracy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Collin Baker</author>
<author>Michael Ellsworth</author>
<author>Katrin Erk</author>
</authors>
<title>SemEval’07 task 19: Frame Semantic Structure Extraction.</title>
<date>2007</date>
<booktitle>In Proc. of the 4th International Workshop on Semantic Evaluations</booktitle>
<contexts>
<context position="35596" citStr="Baker et al., 2007" startWordPosition="5526" endWordPosition="5529">required participants to build systems capable of both syntactic parsing and Semantic Role Labeling (SRL) (Gildea and Jurafsky, 2002), are the most notable attempts to encour11These accuracy figures are higher than what should be expected for unseen datasets; see Tratz and Hovy (2010) for more detail. 1265 age the development of parsers with additional semantic annotation. These tasks relied upon PropBank (2005) and NomBank (2004) for the semantic roles. A variety of other systems have focused on FrameNet-based (1998) SRL instead, including those that participated in the SemEval-2007 Task 19 (Baker et al., 2007) and work by Das et al. (2010). 7 Conclusion In this paper, we have described a new high-quality dependency tree conversion of the Penn Treebank (Marcus, et al., 1993) along with its labeled dependency scheme and presented a parser that is fast, accurate, supports non-projective trees and provides rich output, including not only informative dependency labels similar to Stanford’s but also additional semantic annotation for prepositions, possessives, and noun compound relations. We showed how the easy-first algorithm of Goldberg and Elhadad (Goldberg and Elhadad, 2010) can be extended to suppor</context>
</contexts>
<marker>Baker, Ellsworth, Erk, 2007</marker>
<rawString>Collin Baker, and Michael Ellsworth and Katrin Erk. 2007. SemEval’07 task 19: Frame Semantic Structure Extraction. In Proc. of the 4th International Workshop on Semantic Evaluations</rawString>
</citation>
<citation valid="true">
<authors>
<author>Collin Baker</author>
<author>Charles J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The Berkeley FrameNet Project.</title>
<date>1998</date>
<booktitle>In Proc. of the</booktitle>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin Baker, Charles J. Fillmore and John B. Lowe. 1998. The Berkeley FrameNet Project. In Proc. of the</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>In Computational Linguistics</journal>
<volume>22</volume>
<issue>1</issue>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam L. Berger, Vincent J. Della Pietra, and Stephen A. Della Pietra. 1996. A maximum entropy approach to natural language processing. In Computational Linguistics 22(1):39–71</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Peter V deSouza</author>
<author>Robert L Mercer</author>
<author>Vincent J Della Pietra</author>
<author>Jenifer C Lai</author>
</authors>
<title>ClassBased n-gram Models of Natural Language.</title>
<date>1992</date>
<journal>Computational Linguistics</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="20299" citStr="Brown et al. (1992)" startWordPosition="3124" endWordPosition="3127">clausal complements, and relative algorithm continues to update the weight vector as clauses. Unfortunately, there is insufficient space in long as any invalid action is scored higher than any this paper to describe them all here. However, a list valid action, not just the highest scoring valid acof feature templates will be provided with the parser tion; unfortunately, this change significantly slowed download. down the training process. In early experiments, this Several of the feature templates use unsupervised change produced a slight improvement in accuracy word clusters created with the Brown et al. (1992) though it also slowed training significantly. In later hierarchical clustering algorithm. The use of this al- experiments using additional feature templates, this gorithm was inspired by Koo et al. (2008), who used change ceased to have any notable impact on the the top branches of the cluster hierarchy as features. overall accuracy, but it was kept anyway. 6 However, unlike Koo et al.’s (2008) parser, the fine- The oracle used to determine whether a move opgrained cluster identifiers are used instead of just eration should be considered legal during the trainthe top 4-6 branches of the clust</context>
</contexts>
<marker>Brown, deSouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. ClassBased n-gram Models of Natural Language. Computational Linguistics 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<title>CoNLL-X shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proc. of CoNLL</booktitle>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In Proc. of CoNLL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
</authors>
<title>Experiments with a HigherOrder Projective Dependency Parser.</title>
<date>2007</date>
<booktitle>In Proc. of the CoNLL Shared Task Session of EMNLP-CoNLL</booktitle>
<contexts>
<context position="34286" citStr="Carreras, 2007" startWordPosition="5323" endWordPosition="5324">) Hajiˇc et al., 2009). work that uses chart constraints to achieve linear 6 Related Work time complexity for constituency parsing. Non-projectivity. There are two main approaches Effective features for parsing. A variety of work used in recent NLP literature for handling non- has investigated the use of more informative feaprojectivity in parse trees. The first is to use an al- tures for parsing. This includes work that integorithm, like the one presented in this paper, that grates second and even third order features (McDonhas inherent support for non-projective trees. Ex- ald et al., 2006; Carreras, 2007; Koo and Collins, amples of this include the Chu-Liu-Edmonds’ ap- 2010). Also, some work has incorporated unsuperproach for maximum spanning tree (MST) parsing vised word clusters as features, including that of Koo (McDonald et al., 2005) and Nivre’s (2009) swap- et al. (2008) and Suzuki et al. (2009), who utilized based reordering method for shift-reduce parsing. unsupervised word clusters created using the Brown The second approach is to create an initial projec- et al. (1992) hierarchical clustering algorithm. tive parse and then apply transformations to intro- Semantically-enriched output</context>
</contexts>
<marker>Carreras, 2007</marker>
<rawString>Xavier Carreras. 2007. Experiments with a HigherOrder Projective Dependency Parser. In Proc. of the CoNLL Shared Task Session of EMNLP-CoNLL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Cer</author>
<author>Marie-Catherine de Marneffe</author>
<author>Daniel Jurafsky</author>
<author>Christopher D Manning</author>
</authors>
<title>Parsing to Stanford Dependencies: Trade-offs between speed and accuracy.</title>
<date>2010</date>
<booktitle>In Proc. of LREC</booktitle>
<marker>Cer, de Marneffe, Jurafsky, Manning, 2010</marker>
<rawString>Daniel Cer, Marie-Catherine de Marneffe, Daniel Jurafsky, and Christopher D. Manning. 2010. Parsing to Stanford Dependencies: Trade-offs between speed and accuracy. In Proc. of LREC 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A Maximum-Entropy-Inspired Parser.</title>
<date>2000</date>
<booktitle>In Proc. of NAACL</booktitle>
<contexts>
<context position="2919" citStr="Charniak (2000)" startWordPosition="415" endWordPosition="416">isambiguation and noun compound interpretation (Section 5). We show how Nivre’s (2009) swap-based reordering technique for non-projective shift-reduce-style parsing can be integrated into the non-directional easy-first framework of Goldberg and Elhadad (2010) to support non-projectivity, and we report the results of our parsing experiments on the standard test section of the PTB, providing comparisons with several freely available parsers, including Goldberg and Elhadad’s (2010) implementation, MALTPARSER (Nivre et al., 2006), MSTPARSER (McDonald et al., 2005; McDonald and Pereira, 2006), the Charniak (2000) parser, and the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007). The experimental results show that the parser is substantially more accurate than Goldberg and Elhadad’s original implementation, with fairly similar overall speed. Furthermore, the results prove that Stanford-granularity dependency labels can be learned by modern dependency parsing systems when using our Treebank conversion, unlike the Stanford conversion, for which Cer et al. (2010) show that this isn’t the case. The optional semantic annotation modules also 1257 Proceedings of the 2011 Conference on Empirical Me</context>
<context position="26888" citStr="Charniak (2000)" startWordPosition="4181" endWordPosition="4182">), and MSTPARSER (McDonald and Pereira, 2006)8. The model trained using Goldberg and Elhadad’s (2010) easy-first parser serves as something of a baseline. The four MALTPARSER parsing models used the arceager, arc-standard, stack-eager, and stack-lazy algorithms. One of the MSTPARSER models used the Chu-Liu-Edmonds maximum spanning tree approach, and the other used the Eisner (1996) algorithm with second order features and a nonprojective rewriting post-processing step. Unfortunately, it is not possible to directly compare the parser’s accuracy with most popular constituent parsers such as the Charniak (2000) and Berkeley (Petrov et al., 2006; Petrov and Klein, 2007) parsers9 both because they do not produce functional tags for subjects, direct objects, etc., which are required for the final script of the constituent-to-dependency conversion routine, and because they determine part-of-speech tags in conjunction with the parsing. However, it is possible to compute approximate unlabeled accuracy scores by training the constituent parsers on the NP-patched (Vadas and Curran, 2007) version of the data and then running the test output through just the first conversion script—that is, the modified versi</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A Maximum-Entropy-Inspired Parser. In Proc. of NAACL 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-tofind-grained n-best parsing and discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proc. of ACL</booktitle>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarse-tofind-grained n-best parsing and discriminative reranking. In Proc. of ACL 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael A Covington</author>
</authors>
<title>A Fundamental Algorithm for Dependency Parsing.</title>
<date>2001</date>
<booktitle>In Proc. of the 39th Annual ACM Southeast Conference.</booktitle>
<marker>Covington, 2001</marker>
<rawString>Michael A. Covington. 2001. A Fundamental Algorithm for Dependency Parsing. In Proc. of the 39th Annual ACM Southeast Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Nathan Schneider</author>
<author>Desai Chen</author>
<author>Noah A Smith</author>
</authors>
<title>Probabilistic Frame-Semantic Parsing.</title>
<date>2010</date>
<booktitle>In Proc. of HLT-NAACL</booktitle>
<contexts>
<context position="35626" citStr="Das et al. (2010)" startWordPosition="5533" endWordPosition="5536">ystems capable of both syntactic parsing and Semantic Role Labeling (SRL) (Gildea and Jurafsky, 2002), are the most notable attempts to encour11These accuracy figures are higher than what should be expected for unseen datasets; see Tratz and Hovy (2010) for more detail. 1265 age the development of parsers with additional semantic annotation. These tasks relied upon PropBank (2005) and NomBank (2004) for the semantic roles. A variety of other systems have focused on FrameNet-based (1998) SRL instead, including those that participated in the SemEval-2007 Task 19 (Baker et al., 2007) and work by Das et al. (2010). 7 Conclusion In this paper, we have described a new high-quality dependency tree conversion of the Penn Treebank (Marcus, et al., 1993) along with its labeled dependency scheme and presented a parser that is fast, accurate, supports non-projective trees and provides rich output, including not only informative dependency labels similar to Stanford’s but also additional semantic annotation for prepositions, possessives, and noun compound relations. We showed how the easy-first algorithm of Goldberg and Elhadad (Goldberg and Elhadad, 2010) can be extended to support non-projective trees by addi</context>
</contexts>
<marker>Das, Schneider, Chen, Smith, 2010</marker>
<rawString>Dipanjan Das, Nathan Schneider, Desai Chen, and Noah A. Smith. 2010. Probabilistic Frame-Semantic Parsing. In Proc. of HLT-NAACL 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Christopher D Manning</author>
</authors>
<title>The Stanford typed dependencies representation.</title>
<date>2008</date>
<booktitle>In COLING Workshop on Cross-framework and Cross-domain Parser Evaluation.</booktitle>
<marker>de Marneffe, Manning, 2008</marker>
<rawString>Marie-Catherine de Marneffe and Christopher D. Manning. 2008. The Stanford typed dependencies representation. In COLING Workshop on Cross-framework and Cross-domain Parser Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Three New Probabilistic Models for Dependency Parsing: An Exploration.</title>
<date>1996</date>
<booktitle>In Proc. of COLING</booktitle>
<contexts>
<context position="24621" citStr="Eisner (1996)" startWordPosition="3821" endWordPosition="3822"> (31.3) 35.0 (36.4) 62.9 (65.3) 64.1 (66.5) ARC-EAGERMalt 89.8 (91.1) 91.3 (92.1) 31.6 (34.2) 37.4 (38.5) 19.5 (19.5) 20.3 (19.9) ARC-STANDARDMalt STACK-EAGERMalt STACK-LAZYMalt 88.3 (89.5) 89.7 (90.4) 31.4 (34.1) 36.1 (37.3) 13.1 (12.0) 13.9 (12.7) 90.0 (91.2) 91.5 (92.3) 34.5 (37.5) 40.4 (41.9) 51.8 (53.8) 53.8 (55.4) 90.4 (91.7) 91.9 (92.8) 34.8 (37.7) 40.6 (42.5) 61.8 (63.3) 63.3 (65.3) CHARNIAKt * 93.2 * 43.5 * 32.3 BERKELEYt * 93.3 * 43.6 * 34.3 Table 3: Parsing results for section 23 of the Penn Treebank (punctuation excluded). Results in parentheses were produced using gold POS tags. EEisner (1996) algorithm with non-projective rewriting and second order features. tResults not directly comparable; see text. *Labeled dependencies not available/comparable. 4.5 Evaluation The following split of the Penn Treebank (Marcus, et al., 1993) was used for the experiments: sections 2–21 for training, 22 for development, and 23 for testing. For part-of-speech (POS) tagging, we used an inhouse SVM-based POS tagger modeled after the work of Giménez and Márquez (2004) 7. The training data was tagged in a 10-fold fashion; each fold was tagged using a tagger trained from the nine remaining folds. The dev</context>
<context position="26657" citStr="Eisner (1996)" startWordPosition="4145" endWordPosition="4146">itional POS evaluation (Penn Treebank WSJ sections 22-24). trained for comparison using the following open source parsing packages: Goldberg and Elhadad’s (2010)’s non-directional easy-first parser, MALTPARSER (Nivre et al., 2006), and MSTPARSER (McDonald and Pereira, 2006)8. The model trained using Goldberg and Elhadad’s (2010) easy-first parser serves as something of a baseline. The four MALTPARSER parsing models used the arceager, arc-standard, stack-eager, and stack-lazy algorithms. One of the MSTPARSER models used the Chu-Liu-Edmonds maximum spanning tree approach, and the other used the Eisner (1996) algorithm with second order features and a nonprojective rewriting post-processing step. Unfortunately, it is not possible to directly compare the parser’s accuracy with most popular constituent parsers such as the Charniak (2000) and Berkeley (Petrov et al., 2006; Petrov and Klein, 2007) parsers9 both because they do not produce functional tags for subjects, direct objects, etc., which are required for the final script of the constituent-to-dependency conversion routine, and because they determine part-of-speech tags in conjunction with the parsing. However, it is possible to compute approxi</context>
<context position="31034" citStr="Eisner (1996)" startWordPosition="4830" endWordPosition="4831">e output than a traditional parser. Preposition Sense Disambiguation The PSD system is a newer version of the system described 10Lack of space prohibits a sufficiently thorough discussion of these individual components and their evaluations, but additional information will be available with the system download. 1264 by Tratz and Hovy (2009) and Hovy et al. (2010); it duce non-projectivity into it. Examples of this inachieves 85.7% accuracy on the SemEval-2007 fine- clude McDonald and Pereira’s (2006) rewriting of grain PSD task (Litkowski and Hargraves, 2007), projective trees produced by the Eisner (1996) alwhich is a statistically significant (p&lt;=0.05; upper- gorithm, and Nivre and Nilsson’s (2005) pseudotailed z test) increase over the previous best reported projective approach that creates projective trees with result for this dataset, Hovy et al.’s (2010) 84.8%. specially marked arcs that are later transformed into Noun Compound Interpretation The noun com- non-projective dependencies. pound interpretation system is a newer version of Descriptive dependency labels. While most rethe system described by Tratz and Hovy (2010) with cent dependency parsing research has used either similar accur</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>Jason Eisner. 1996. Three New Probabilistic Models for Dependency Parsing: An Exploration. In Proc. of COLING 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics.</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="35110" citStr="Gildea and Jurafsky, 2002" startWordPosition="5445" endWordPosition="5449">uding that of Koo (McDonald et al., 2005) and Nivre’s (2009) swap- et al. (2008) and Suzuki et al. (2009), who utilized based reordering method for shift-reduce parsing. unsupervised word clusters created using the Brown The second approach is to create an initial projec- et al. (1992) hierarchical clustering algorithm. tive parse and then apply transformations to intro- Semantically-enriched output. The 2008 and 2009 CoNLL shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009), which required participants to build systems capable of both syntactic parsing and Semantic Role Labeling (SRL) (Gildea and Jurafsky, 2002), are the most notable attempts to encour11These accuracy figures are higher than what should be expected for unseen datasets; see Tratz and Hovy (2010) for more detail. 1265 age the development of parsers with additional semantic annotation. These tasks relied upon PropBank (2005) and NomBank (2004) for the semantic roles. A variety of other systems have focused on FrameNet-based (1998) SRL instead, including those that participated in the SemEval-2007 Task 19 (Baker et al., 2007) and work by Das et al. (2010). 7 Conclusion In this paper, we have described a new high-quality dependency tree c</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics. 28(3):245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jesús Giménez</author>
<author>Lluís Márquez</author>
</authors>
<title>SVMTool: A General POS Tagger Generator Based on Support Vector Machines.</title>
<date>2004</date>
<booktitle>In Proc. of LREC</booktitle>
<contexts>
<context position="25084" citStr="Giménez and Márquez (2004)" startWordPosition="3887" endWordPosition="3890"> 34.3 Table 3: Parsing results for section 23 of the Penn Treebank (punctuation excluded). Results in parentheses were produced using gold POS tags. EEisner (1996) algorithm with non-projective rewriting and second order features. tResults not directly comparable; see text. *Labeled dependencies not available/comparable. 4.5 Evaluation The following split of the Penn Treebank (Marcus, et al., 1993) was used for the experiments: sections 2–21 for training, 22 for development, and 23 for testing. For part-of-speech (POS) tagging, we used an inhouse SVM-based POS tagger modeled after the work of Giménez and Márquez (2004) 7. The training data was tagged in a 10-fold fashion; each fold was tagged using a tagger trained from the nine remaining folds. The development and test sections were tagged by an instance of the tagger trained using the entire training set. The full details of the POS tagger are outside the scope of this paper; it is included with the parser download. The final parser was trained for 31 iterations, which is the point at which its performance on the development set peaked. One test run was performed with non-projectivity support disabled in order to get some idea of the impact of the move op</context>
</contexts>
<marker>Giménez, Márquez, 2004</marker>
<rawString>Jesús Giménez and Lluís Márquez 2004. SVMTool: A General POS Tagger Generator Based on Support Vector Machines. In Proc. of LREC 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Michael Elhadad</author>
</authors>
<title>An Efficient Algorithm for Easy-First Non-Directional Dependency Parsing.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL.</booktitle>
<contexts>
<context position="2563" citStr="Goldberg and Elhadad (2010)" startWordPosition="361" endWordPosition="364">on (Section 3) of the Penn Treebank (Marcus, et al., 1993) along with the associated dependency label scheme, which is based upon the Stanford parser’s popular scheme (de Marneffe and Manning, 2008), and a fast, accurate dependency parser with non-projectivity support (Section 4) and additional integrated semantic annotation modules for automatic preposition sense disambiguation and noun compound interpretation (Section 5). We show how Nivre’s (2009) swap-based reordering technique for non-projective shift-reduce-style parsing can be integrated into the non-directional easy-first framework of Goldberg and Elhadad (2010) to support non-projectivity, and we report the results of our parsing experiments on the standard test section of the PTB, providing comparisons with several freely available parsers, including Goldberg and Elhadad’s (2010) implementation, MALTPARSER (Nivre et al., 2006), MSTPARSER (McDonald et al., 2005; McDonald and Pereira, 2006), the Charniak (2000) parser, and the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007). The experimental results show that the parser is substantially more accurate than Goldberg and Elhadad’s original implementation, with fairly similar overall speed.</context>
<context position="15580" citStr="Goldberg and Elhadad (2010)" startWordPosition="2378" endWordPosition="2381">. Original New # of changes IN RB 1128 JJ NN 787 VBD VBN 601 RB IN 462 VBN VBD 441 NN JJ 409 NNPS NNP 405 IN WDT 388 VBG NN 223 DT IN 220 RB JJ 214 VB VBP 184 NN NNS 169 RB NN 157 NNS VBZ 148 Table 2: Top 15 part-of-speech tag changes performed by the conversion script. 1260 4 Parser in practice, especially for languages with relatively 4.1 Algorithm fixed word order such as English.5 Though GoldThe parsing approach is based upon the non- berg and Elhadad’s (2010) original implementation directional easy-first algorithm recently presented only supports unlabeled dependencies, the algorithm by Goldberg and Elhadad (2010). Their original al- itself is in no way limited in this regard, and it is gorithm behaves as follows. For a sentence of length simple enough to add labeled dependency support n, the algorithm performs a total of n steps. In each by treating each dependency label as a specific type step, one of the unattached tokens is added as a child of attach operation (e.g., attach_as_nsubj), which to one of its current neighbors and is then removed is the method used by this implementation. Pseufrom the list of unprocessed tokens. When only one docode for the non-directional easy-first algorithm token rem</context>
<context position="18874" citStr="Goldberg and Elhadad (2010)" startWordPosition="2903" endWordPosition="2906"> token in such a way that a previous move operation is undone, there can be at most O(n2) moves and the overall worst-case complexity becomes O(n2 log n). While theoretically slower, this has a limited impact upon actual parsing times else arc = createArc(best); tree.add(arc); i = words.index(getChild(arc)); words.remove(i); for x ∈ -k,...,k do stale.add(words.get(index+x)); return tree Algorithm 1: Modified version of Goldberg and Elhadad’s (2010) Easy-First Algorithm with nonprojective support. 5See Nivre (2009) for more information on the effect of reordering operations on parse time. 4See Goldberg and Elhadad (2010) for more explanation. 1261 4.2 Features tion. The general idea of the algorithm is to iterate One of the key aspects of the parser is the complex over the sentences and, whenever the model predicts set of features used. The feature set is based off an incorrect action, update the model weights. Folthe features used by Goldberg and Elhadad (2010) lowing Goldberg and Elhadad, parameter averaging but has a significant number of extensions. Various is used to reduce overfitting. feature templates are specifically designed to pro- Our implementation varies slightly from that of duce features that </context>
<context position="23335" citStr="Goldberg and Elhadad (2010)" startWordPosition="3619" endWordPosition="3622">mong near behind outside across ... pendency, which is permitted between any POS tag Clinton Bush Johnson Smith Brown Williams King ... pair). Early experiments utilizing similar constraints children companies women people men things students ... showed an improvement in parsing speed of about Figure 3: High frequency examples from 15 of the Brown 16% with no significant impact on accuracy, regardclusters. less of whether the constraints were enforced during 4.3 Training training. The parsing model is trained using a variant of the structured perceptron training algorithm used in the original Goldberg and Elhadad (2010) implementa1262 6See Goldberg and Elhadad (2010) for more description of the general training procedure. System Arc Accuracy Perfect Non-Proj Arcs Sentences Labeled Unlabeled Labeled Unlabeled Labeled Unlabeled THIS WORK 92.1 (93.3) 93.7 (94.3) 38.4 (42.5) 46.2 (48.5) 66.5 (69.7) 69.3 (71.7) THIS WORKno clusters THIS WORKmoves disabled 91.8 (93.1) 93.4 (94.1) 38.2 (42.3) 45.5 (47.3) 67.3 (70.9) 69.3 (72.5) 91.7 (92.9) 93.3 (93.9) 37.1 (40.8) 44.2 (46.2) 21.1 (21.1) 22.7 (21.9) NON-DIR EASY FIRST * 91.2 (92.0) * 37.8 (39.4) * 15.1 (16.3) EISNEREMST 90.9 (92.2) 92.8 (93.5) 32.1 (35.6) 40.6 (42.3</context>
<context position="29636" citStr="Goldberg and Elhadad (2010)" startWordPosition="4623" endWordPosition="4626"> 1.3% in labeled accuracy. The 93.2% and 93.3% accuracy scores achieved by the Charniak and Berkeley parsers are not too different from the 93.7% result, but, of course, it is important to remember that these scores are not directly comparable. Figure 5: Sentence length versus parse time. Median times for five runs over section 23. Figure 4: Parse times for Penn Treebank section 23 for the parsers on a PC with a 2.4Ghz Q6600 processor and 8GB RAM. MALTPARSER ran substantially slower than the others, perhaps due to its use of polynomial kernels, and isn’t shown. (C-L-E - Chu-Liu-Edmonds, G&amp;E - Goldberg and Elhadad (2010)). 4.5.1 Results Discussion The parser achieves 92.1% labeled and 93.7% unlabeled accuracy on the evaluation, a solid result and about 2.5% higher than the original easy-first implementation of Goldberg and Elhadad (2010). Furthermore, the parser processed the entire test section in 8Versions 1.4.1, 0.4.3b, and 0.2, respectively 9Versions 1.1 and 05Aug16, respectively 5 Shallow Semantic Annotation To create a more informative parse, the parser includes four optional modules, a preposition sense disambiguation (PSD) system, a work-in-progress ’s-possessive interpretation system, a noun compound</context>
<context position="36170" citStr="Goldberg and Elhadad, 2010" startWordPosition="5614" endWordPosition="5618">ated in the SemEval-2007 Task 19 (Baker et al., 2007) and work by Das et al. (2010). 7 Conclusion In this paper, we have described a new high-quality dependency tree conversion of the Penn Treebank (Marcus, et al., 1993) along with its labeled dependency scheme and presented a parser that is fast, accurate, supports non-projective trees and provides rich output, including not only informative dependency labels similar to Stanford’s but also additional semantic annotation for prepositions, possessives, and noun compound relations. We showed how the easy-first algorithm of Goldberg and Elhadad (Goldberg and Elhadad, 2010) can be extended to support non-projective trees by adding move actions similar to Nivre’s (2009) swap-based reordering for shiftreduce parsing and evaluated our parser on the standard test section of the Penn Treebank, comparing with several other freely available parsers. The Penn Treebank conversion process fixes a number of buggy trees and part-of-speech tags and produces dependency trees with a relatively small percentage of generic dep dependencies. The experimental results show that dependency parsers can generally produce Stanford-granularity labels with high accuracy when using the ne</context>
</contexts>
<marker>Goldberg, Elhadad, 2010</marker>
<rawString>Yoav Goldberg and Michael Elhadad. 2010. An Efficient Algorithm for Easy-First Non-Directional Dependency Parsing. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Michael Elhadad</author>
</authors>
<title>The CoNLL-2009 Shared Task: Syntactic and Semantic Dependencies in Multiple Languages.</title>
<date>2009</date>
<booktitle>In Proc. of the Thirteenth Conference on Computational Natural Language Learning: Shared Task.</booktitle>
<marker>Goldberg, Elhadad, 2009</marker>
<rawString>Yoav Goldberg and Michael Elhadad. 2009. The CoNLL-2009 Shared Task: Syntactic and Semantic Dependencies in Multiple Languages. In Proc. of the Thirteenth Conference on Computational Natural Language Learning: Shared Task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dirk Hovy</author>
<author>Stephen Tratz</author>
<author>Eduard Hovy</author>
</authors>
<title>What’s in a Preposition?—Dimensions of Sense Disambiguation for an Interesting Word Class.</title>
<date>2010</date>
<booktitle>In Proc. of COLING</booktitle>
<contexts>
<context position="30786" citStr="Hovy et al. (2010)" startWordPosition="4791" endWordPosition="4794">ork-in-progress ’s-possessive interpretation system, a noun compound interpretation system, and a PropBank-based semantic role labeling system10. Taken together, these integrated modules enable the parsing system to produce substantially more informative output than a traditional parser. Preposition Sense Disambiguation The PSD system is a newer version of the system described 10Lack of space prohibits a sufficiently thorough discussion of these individual components and their evaluations, but additional information will be available with the system download. 1264 by Tratz and Hovy (2009) and Hovy et al. (2010); it duce non-projectivity into it. Examples of this inachieves 85.7% accuracy on the SemEval-2007 fine- clude McDonald and Pereira’s (2006) rewriting of grain PSD task (Litkowski and Hargraves, 2007), projective trees produced by the Eisner (1996) alwhich is a statistically significant (p&lt;=0.05; upper- gorithm, and Nivre and Nilsson’s (2005) pseudotailed z test) increase over the previous best reported projective approach that creates projective trees with result for this dataset, Hovy et al.’s (2010) 84.8%. specially marked arcs that are later transformed into Noun Compound Interpretation Th</context>
</contexts>
<marker>Hovy, Tratz, Hovy, 2010</marker>
<rawString>Dirk Hovy, Stephen Tratz, and Eduard Hovy. 2010. What’s in a Preposition?—Dimensions of Sense Disambiguation for an Interesting Word Class. In Proc. of COLING 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kenji Sagae</author>
</authors>
<title>Dynamic Programming for Linear-Time Shift-Reduce Parsing.</title>
<date>2010</date>
<booktitle>In Proc. of ACL</booktitle>
<contexts>
<context position="33356" citStr="Huang and Sagae (2010)" startWordPosition="5170" endWordPosition="5173">o publicly available dependency conversion culated over both predicate disambiguation and ar- according to Tapanainen and Järvinen’s scheme. gument/adjunct classification (89.5 F1 on predicate Faster parsing. While the fastest reasonable disambiguation, 85.6 F1 on argument and adjuncts parsing algorithms are the O(n) shift-reduce algocorresponding to dependency links, and 86.8 F1); rithms, such as Nivre’s (2003) algorithm and an exthis score is not directly comparable to any previ- pected linear time dynamic programming approach ous work due to some differences, including differ- presented by Huang and Sagae (2010), a few other ences in both the parse tree conversion and the Prop- fast alternatives exist. Goldberg and Elhadad’s Bank conversion. The most similar work is that of (2010) easy-first algorithm is one such example. Anthe CoNLL shared task work (Surdeanu et al., 2008; other example, is Roark and Hollingshead’s (2009) Hajiˇc et al., 2009). work that uses chart constraints to achieve linear 6 Related Work time complexity for constituency parsing. Non-projectivity. There are two main approaches Effective features for parsing. A variety of work used in recent NLP literature for handling non- has in</context>
</contexts>
<marker>Huang, Sagae, 2010</marker>
<rawString>Liang Huang and Kenji Sagae. 2010. Dynamic Programming for Linear-Time Shift-Reduce Parsing. In Proc. of ACL 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Extended constituent-to-dependency conversion for english.</title>
<date>2007</date>
<booktitle>In Proc. of NODALIDA.</booktitle>
<contexts>
<context position="12815" citStr="Johansson and Nugues (2007)" startWordPosition="1928" endWordPosition="1931">B MD *-PRD SQ VP FRAG X UCP [QNVP]P|S*|UCP|NML|PR[NT]|RRC|NX|NAC|FRAG|INTJ|AD[JV]P|LST|WH*|X VP VBD|AUX VBN MD VBZ VB VBG VBP VP POS *-PRD ADJP JJ NN NNS NP|NML WHADJP CC JJ WRB ADJP WHADVP CC WRB|RB X [QNVP]P|S*|UCP|NML|PR[NT]|RRC|NX|NAC|FRAG|INTJ|AD[JV]P|LST|WH*|X|CONJP LST LS : DT|NN|SYM Figure 2: Modified head-finding rules. Underline indicates that the search is performed in a left-to-right fashion instead of the default right-to-left order. NML and JJP are both products of Vadas and Curran’s (2007) patch. Bold indicates an added or moved element; for the original rules, see the paper by Johansson and Nugues (2007). in bold, are provided in Figure 2. Finally, an additional script makes additional changes and converts the intermediate output into the dependency scheme. This dependency conversion has several advantages to it. Using the modified head-finding rules for Johansson and Nugues’ (2007) converter results in fewer buggy trees than were present in the CoNLL shared tasks, including fewer trees in which words are headed by punctuation marks. For sections 2– 21, there are far fewer generic dep/DEP relations (2,765) than with the Stanford conversion (34,134) or the CoNLL 2008 shared task conversion (23</context>
</contexts>
<marker>Johansson, Nugues, 2007</marker>
<rawString>Richard Johansson and Pierre Nugues. 2007. Extended constituent-to-dependency conversion for english. In Proc. of NODALIDA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>Simple Semi-supervised Dependency Parsing.</title>
<date>2008</date>
<booktitle>In Proc. of ACL</booktitle>
<contexts>
<context position="20504" citStr="Koo et al. (2008)" startWordPosition="3154" endWordPosition="3157">describe them all here. However, a list valid action, not just the highest scoring valid acof feature templates will be provided with the parser tion; unfortunately, this change significantly slowed download. down the training process. In early experiments, this Several of the feature templates use unsupervised change produced a slight improvement in accuracy word clusters created with the Brown et al. (1992) though it also slowed training significantly. In later hierarchical clustering algorithm. The use of this al- experiments using additional feature templates, this gorithm was inspired by Koo et al. (2008), who used change ceased to have any notable impact on the the top branches of the cluster hierarchy as features. overall accuracy, but it was kept anyway. 6 However, unlike Koo et al.’s (2008) parser, the fine- The oracle used to determine whether a move opgrained cluster identifiers are used instead of just eration should be considered legal during the trainthe top 4-6 branches of the cluster hierarchy. The ing phase is similar to Nivre et al.’s (2009) improved 175 word clusters utilized by the parser were created oracle based upon maximal projective subcompofrom the New York Times corpus (S</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple Semi-supervised Dependency Parsing. In Proc. of ACL 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Michael Collins</author>
</authors>
<title>Efficient Thirdorder Dependency Parsers.</title>
<date>2010</date>
<booktitle>In Proc. ofACL</booktitle>
<marker>Koo, Collins, 2010</marker>
<rawString>Terry Koo and Michael Collins. 2010. Efficient Thirdorder Dependency Parsers. In Proc. ofACL 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken Litkowski</author>
<author>Orin Hargraves</author>
</authors>
<title>SemEval2007 Task 06: Word-Sense Disambiguation of Prepositions.</title>
<date>2007</date>
<booktitle>In Proc. of the 4th International Workshop on Semantic Evaluations.</booktitle>
<contexts>
<context position="30986" citStr="Litkowski and Hargraves, 2007" startWordPosition="4821" endWordPosition="4824">nable the parsing system to produce substantially more informative output than a traditional parser. Preposition Sense Disambiguation The PSD system is a newer version of the system described 10Lack of space prohibits a sufficiently thorough discussion of these individual components and their evaluations, but additional information will be available with the system download. 1264 by Tratz and Hovy (2009) and Hovy et al. (2010); it duce non-projectivity into it. Examples of this inachieves 85.7% accuracy on the SemEval-2007 fine- clude McDonald and Pereira’s (2006) rewriting of grain PSD task (Litkowski and Hargraves, 2007), projective trees produced by the Eisner (1996) alwhich is a statistically significant (p&lt;=0.05; upper- gorithm, and Nivre and Nilsson’s (2005) pseudotailed z test) increase over the previous best reported projective approach that creates projective trees with result for this dataset, Hovy et al.’s (2010) 84.8%. specially marked arcs that are later transformed into Noun Compound Interpretation The noun com- non-projective dependencies. pound interpretation system is a newer version of Descriptive dependency labels. While most rethe system described by Tratz and Hovy (2010) with cent dependenc</context>
</contexts>
<marker>Litkowski, Hargraves, 2007</marker>
<rawString>Ken Litkowski and Orin Hargraves. 2007. SemEval2007 Task 06: Word-Sense Disambiguation of Prepositions. In Proc. of the 4th International Workshop on Semantic Evaluations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
</authors>
<title>Part-of-Speech Tagging from 97% to 100%: Is It Time for Some Linguistics?</title>
<date>2011</date>
<booktitle>In Proc. of the 12th International Conference on Intelligent Text Processing and Computational Linguistics (CICLing</booktitle>
<contexts>
<context position="13742" citStr="Manning (2011)" startWordPosition="2069" endWordPosition="2070">uggy trees than were present in the CoNLL shared tasks, including fewer trees in which words are headed by punctuation marks. For sections 2– 21, there are far fewer generic dep/DEP relations (2,765) than with the Stanford conversion (34,134) or the CoNLL 2008 shared task conversion (23,811). Also, the additional conversion script contains various rules for correcting part-of-speech (POS) errors using the syntactic structure as well as additional rules for some specific word forms, mostly common words with inconsistent taggings. Many of these changes cover part-of-speech problems discussed by Manning (2011), including VBD/VBN, VBZ/NNS, NNP/NNPS, and IN/WDT/DT issues. In total, the script changes over 9,500 part-of-speech tags, with the most common change being to change preposition tags (IN) into adverb tags (RB) for cases where there is no prepositional complement/object. The top fifteen of these changes are presented in Table 2. The conversion script contains a variety of additional rules for modifying the parse structure and fixing erroneous trees as well, including cases where one or more POS tags were incorrect and, as such, the initial dependency parse was flawed. Quick manual inspections </context>
</contexts>
<marker>Manning, 2011</marker>
<rawString>Christopher D. Manning. 2011. Part-of-Speech Tagging from 97% to 100%: Is It Time for Some Linguistics? In Proc. of the 12th International Conference on Intelligent Text Processing and Computational Linguistics (CICLing 2011).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of English: the Penn TreeBank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="1994" citStr="Marcus, et al., 1993" startWordPosition="284" endWordPosition="288">stems for information extraction, question answering, machine translation, recognition of textual entailment, summarization, and many others. Unfortunately, currently available dependency parsers suffer from at least one of several weaknesses including high running time, limited accuracy, vague dependency labels, and lack of non-projectivity support. Furthermore, few parsers include any sort of additional semantic interpretation, such as interpretations for prepositions, possessives, or noun compounds. In this paper, we describe 1) a new dependency conversion (Section 3) of the Penn Treebank (Marcus, et al., 1993) along with the associated dependency label scheme, which is based upon the Stanford parser’s popular scheme (de Marneffe and Manning, 2008), and a fast, accurate dependency parser with non-projectivity support (Section 4) and additional integrated semantic annotation modules for automatic preposition sense disambiguation and noun compound interpretation (Section 5). We show how Nivre’s (2009) swap-based reordering technique for non-projective shift-reduce-style parsing can be integrated into the non-directional easy-first framework of Goldberg and Elhadad (2010) to support non-projectivity, a</context>
<context position="11197" citStr="Marcus, et al., 1993" startWordPosition="1663" endWordPosition="1666">why dependency parsers have trouble learning and applying it. Normally, the head of the clause is a verb, but, under Stanford’s scheme, if the verb happens to be a copula, the complement of the copula (cop) is treated as the head of the clause instead. 3The parsing system includes an optional script that can convert vch arcs into aux and auxpass and the subject relations into csubjpass and nsubjpass. Figure 1: Example comparing Stanford’s (top) handling of copula and coordinating conjunctions with ours (bottom). 3.2 Conversion Process A three-step process is used to convert the Penn Treebank (Marcus, et al., 1993) from constituent parses into dependency trees labeled according to the dependency scheme presented in the prior section. The first step is to apply the noun phrase structure patch created by Vadas and Curran (2007), which adds structure to the otherwise flat noun phrases (NPs) of the Penn Treebank (e.g., ‘(metal soup pot cover)’ would become ‘(metal (soup pot) cover)’). The second step is to apply a version of Johansson and Nugues’ (2007) constituent-todependency converter with some head-finding rule modifications; these rules, with changes highlighted 1259 (WH)?NP|NX|NML|NAC FW|NML|NN* JJR $</context>
<context position="24859" citStr="Marcus, et al., 1993" startWordPosition="3850" endWordPosition="3853"> (12.0) 13.9 (12.7) 90.0 (91.2) 91.5 (92.3) 34.5 (37.5) 40.4 (41.9) 51.8 (53.8) 53.8 (55.4) 90.4 (91.7) 91.9 (92.8) 34.8 (37.7) 40.6 (42.5) 61.8 (63.3) 63.3 (65.3) CHARNIAKt * 93.2 * 43.5 * 32.3 BERKELEYt * 93.3 * 43.6 * 34.3 Table 3: Parsing results for section 23 of the Penn Treebank (punctuation excluded). Results in parentheses were produced using gold POS tags. EEisner (1996) algorithm with non-projective rewriting and second order features. tResults not directly comparable; see text. *Labeled dependencies not available/comparable. 4.5 Evaluation The following split of the Penn Treebank (Marcus, et al., 1993) was used for the experiments: sections 2–21 for training, 22 for development, and 23 for testing. For part-of-speech (POS) tagging, we used an inhouse SVM-based POS tagger modeled after the work of Giménez and Márquez (2004) 7. The training data was tagged in a 10-fold fashion; each fold was tagged using a tagger trained from the nine remaining folds. The development and test sections were tagged by an instance of the tagger trained using the entire training set. The full details of the POS tagger are outside the scope of this paper; it is included with the parser download. The final parser w</context>
<context position="35763" citStr="Marcus, et al., 1993" startWordPosition="5555" endWordPosition="5558">to encour11These accuracy figures are higher than what should be expected for unseen datasets; see Tratz and Hovy (2010) for more detail. 1265 age the development of parsers with additional semantic annotation. These tasks relied upon PropBank (2005) and NomBank (2004) for the semantic roles. A variety of other systems have focused on FrameNet-based (1998) SRL instead, including those that participated in the SemEval-2007 Task 19 (Baker et al., 2007) and work by Das et al. (2010). 7 Conclusion In this paper, we have described a new high-quality dependency tree conversion of the Penn Treebank (Marcus, et al., 1993) along with its labeled dependency scheme and presented a parser that is fast, accurate, supports non-projective trees and provides rich output, including not only informative dependency labels similar to Stanford’s but also additional semantic annotation for prepositions, possessives, and noun compound relations. We showed how the easy-first algorithm of Goldberg and Elhadad (Goldberg and Elhadad, 2010) can be extended to support non-projective trees by adding move actions similar to Nivre’s (2009) swap-based reordering for shiftreduce parsing and evaluated our parser on the standard test sec</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of English: the Penn TreeBank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
<author>Kiril Ribarov</author>
<author>Jan Hajiˇc</author>
</authors>
<title>Non-Projective Dependency Parsing Using Spanning Tree Algorithms.</title>
<date>2005</date>
<booktitle>In Proc. of HLTEMNLP</booktitle>
<marker>McDonald, Pereira, Ribarov, Hajiˇc, 2005</marker>
<rawString>Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Hajiˇc. 2005. Non-Projective Dependency Parsing Using Spanning Tree Algorithms. In Proc. of HLTEMNLP 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Online Learning of Approximate Dependency Parsing Algorithms.</title>
<date>2006</date>
<booktitle>In Proc. of EACL</booktitle>
<contexts>
<context position="2898" citStr="McDonald and Pereira, 2006" startWordPosition="409" endWordPosition="413">for automatic preposition sense disambiguation and noun compound interpretation (Section 5). We show how Nivre’s (2009) swap-based reordering technique for non-projective shift-reduce-style parsing can be integrated into the non-directional easy-first framework of Goldberg and Elhadad (2010) to support non-projectivity, and we report the results of our parsing experiments on the standard test section of the PTB, providing comparisons with several freely available parsers, including Goldberg and Elhadad’s (2010) implementation, MALTPARSER (Nivre et al., 2006), MSTPARSER (McDonald et al., 2005; McDonald and Pereira, 2006), the Charniak (2000) parser, and the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007). The experimental results show that the parser is substantially more accurate than Goldberg and Elhadad’s original implementation, with fairly similar overall speed. Furthermore, the results prove that Stanford-granularity dependency labels can be learned by modern dependency parsing systems when using our Treebank conversion, unlike the Stanford conversion, for which Cer et al. (2010) show that this isn’t the case. The optional semantic annotation modules also 1257 Proceedings of the 2011 Confe</context>
<context position="6724" citStr="McDonald and Pereira, 2006" startWordPosition="1004" endWordPosition="1007"> dependency scheme (de Marn- NMOD, PMOD), or Stanford’s dependency labels effe and Manning, 2008) has proven to be popular, (de Marneffe and Manning, 2008). Unlabeled derecent experiments by Cer et al. (2010) using the pendencies are clearly too impoverished for many Stanford conversion of the Penn Treebank indicate tasks. Similarly, the coarse labels of the CoNLL that it is difficult for current dependency parsers to tasks are not very specific; for example, the same relearn. Indeed, the highest scoring parsers trained us- lation, NMOD, is used for determiners, adjectives, ing the MSTPARSER (McDonald and Pereira, 2006) nouns, participle modifiers, relative clauses, etc. that and MALTPARSER (Nivre et al., 2006) parsing suites modify nouns. In contrast, the Stanford relations achieved only 78.8 and 81.1 labeled attachment provide a more reasonable level of granularity. F1, respectively. This contrasted with the much Our dependency relation scheme is similar to higher performance obtained using a constituent-to- Stanford’s basic scheme but has several differdependency conversion approach with accurate, but ences. It introduces several new relations including much slower, constituency parsers such as the Char- </context>
<context position="26318" citStr="McDonald and Pereira, 2006" startWordPosition="4092" endWordPosition="4095">s on the parser’s overall performance; also, since the parsers used for comparison had no access to the unsupervised word clusters, an additional instance of the parser was trained with every word treated as belonging to the same cluster so as to facilitate a more fair comparison. Seven different dependency parsing models were 797.42% accuracy on traditional POS evaluation (Penn Treebank WSJ sections 22-24). trained for comparison using the following open source parsing packages: Goldberg and Elhadad’s (2010)’s non-directional easy-first parser, MALTPARSER (Nivre et al., 2006), and MSTPARSER (McDonald and Pereira, 2006)8. The model trained using Goldberg and Elhadad’s (2010) easy-first parser serves as something of a baseline. The four MALTPARSER parsing models used the arceager, arc-standard, stack-eager, and stack-lazy algorithms. One of the MSTPARSER models used the Chu-Liu-Edmonds maximum spanning tree approach, and the other used the Eisner (1996) algorithm with second order features and a nonprojective rewriting post-processing step. Unfortunately, it is not possible to directly compare the parser’s accuracy with most popular constituent parsers such as the Charniak (2000) and Berkeley (Petrov et al., </context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>Ryan McDonald and Fernando Pereira. 2006. Online Learning of Approximate Dependency Parsing Algorithms. In Proc. of EACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Meyers</author>
<author>Ruth Reeves</author>
<author>Catherine Macleod</author>
<author>Rachel Szekely</author>
<author>Veronika Zielinska</author>
<author>Brian Young</author>
<author>Ralph Grishman</author>
</authors>
<title>The NomBank Project: An Interim Report.</title>
<date>2004</date>
<booktitle>In Proc. of the NAACL/HLT Workshop on Frontiers in Corpus Annotation.</booktitle>
<marker>Meyers, Reeves, Macleod, Szekely, Zielinska, Young, Grishman, 2004</marker>
<rawString>Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel Szekely, Veronika Zielinska, Brian Young and Ralph Grishman. 2004. The NomBank Project: An Interim Report. In Proc. of the NAACL/HLT Workshop on Frontiers in Corpus Annotation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Non-Projective Dependency Parsing in Expected Linear Time.</title>
<date>2009</date>
<booktitle>In Proc. of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP.</booktitle>
<contexts>
<context position="18014" citStr="Nivre (2009)" startWordPosition="2767" endWordPosition="2768">lly difficult deci- best = arg max cache[w,a] sions easier. The second advantage is that perform- a∈actions&amp;valid(a),w∈words ing parse actions in a more flexible order than left- if isMove(best) then to-right/right-to-left shift-reduce parsing reduces the i= chance of error propagation. words.index(getTokenToMove(best)); Unfortunately, the original algorithm does not words.move (i, isMoveLeft(best) ? -1 support non-projective trees. To extend the algo- : 1); rithm to support non-projective trees, we introduce move-right and move-left operations similar to the stack-to-buffer swaps proposed by Nivre (2009) for shift-reduce style parsing. Thus, instead of attaching a token to one of its neighbors at each step, the algorithm may instead decide to move a token past one of its neighbors. Provided that no node is allowed to be moved past a token in such a way that a previous move operation is undone, there can be at most O(n2) moves and the overall worst-case complexity becomes O(n2 log n). While theoretically slower, this has a limited impact upon actual parsing times else arc = createArc(best); tree.add(arc); i = words.index(getChild(arc)); words.remove(i); for x ∈ -k,...,k do stale.add(words.get(</context>
</contexts>
<marker>Nivre, 2009</marker>
<rawString>Joakim Nivre. 2009. Non-Projective Dependency Parsing in Expected Linear Time. In Proc. of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>An Efficient Algorithm for Projective Dependency Parsing.</title>
<date>2003</date>
<booktitle>In Proc. of the 8th International Workshop on Parsing Technologies (IWPT).</booktitle>
<marker>Nivre, 2003</marker>
<rawString>Joakim Nivre. 2003. An Efficient Algorithm for Projective Dependency Parsing. In Proc. of the 8th International Workshop on Parsing Technologies (IWPT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Marco Kuhlmann</author>
<author>Johan Hall</author>
</authors>
<title>An Improved Oracle for Dependency Parsing with Online Reordering.</title>
<date>2009</date>
<booktitle>In Proc. of the 11th International Conference on Parsing Technologies (IWPT).</booktitle>
<marker>Nivre, Kuhlmann, Hall, 2009</marker>
<rawString>Joakim Nivre, Marco Kuhlmann, and Johan Hall. 2009. An Improved Oracle for Dependency Parsing with Online Reordering. In Proc. of the 11th International Conference on Parsing Technologies (IWPT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Sandra Kübler</author>
<author>Ryan McDonald</author>
<author>Jens Nilsson</author>
<author>Sebastian Riedel</author>
<author>Deniz Yuret</author>
</authors>
<title>shared task on dependency parsing.</title>
<date>2007</date>
<journal>The CoNLL</journal>
<booktitle>In Proc. of EMNLP-CoNLL</booktitle>
<contexts>
<context position="4966" citStr="Nivre et al., 2007" startWordPosition="733" endWordPosition="736">at ‘ice’ is the substance of the larly to an earlier version described by Tratz and statue. Similarly, a parser will indicate which words Hovy (2010) at just over 79% accuracy. a preposition connects but will not give any seman2 Background tic interpretation (e.g., ‘the boy with the pirate hat’ The NLP community has recently seen a surge of → wearing or carrying, ‘wash with cold water’ → interest in dependency parsing, with several CoNLL means, ‘shave with the grain’ → in the same direcshared tasks focusing on it (Buchholz and Marsi, tion as). While, in some cases, it may be possible to 2006; Nivre et al., 2007). One of the main advan- use the output from a separate system for this purtages of dependency parsing is the relative ease with pose, doing so is often difficult in practice due to a which it can handle non-projectivity1. Additionally, wide variety of complications, including programsince each word is linked directly to its head via a ming language differences, alternative data formats, link that, ideally, indicates the syntactic dependency and, sometimes, other parsers. type, there is no difficulty in determining either the 3 Dependency Conversion syntactic head of a particular word or the s</context>
</contexts>
<marker>Nivre, Hall, Kübler, McDonald, Nilsson, Riedel, Yuret, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Sandra Kübler, Ryan McDonald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007. The CoNLL 2007 shared task on dependency parsing. In Proc. of EMNLP-CoNLL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>MaltParser: A Data-Driven Parser-Generator for Dependency Parsing.</title>
<date>2006</date>
<booktitle>In Proc. of LREC</booktitle>
<contexts>
<context position="2835" citStr="Nivre et al., 2006" startWordPosition="400" endWordPosition="403"> and additional integrated semantic annotation modules for automatic preposition sense disambiguation and noun compound interpretation (Section 5). We show how Nivre’s (2009) swap-based reordering technique for non-projective shift-reduce-style parsing can be integrated into the non-directional easy-first framework of Goldberg and Elhadad (2010) to support non-projectivity, and we report the results of our parsing experiments on the standard test section of the PTB, providing comparisons with several freely available parsers, including Goldberg and Elhadad’s (2010) implementation, MALTPARSER (Nivre et al., 2006), MSTPARSER (McDonald et al., 2005; McDonald and Pereira, 2006), the Charniak (2000) parser, and the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007). The experimental results show that the parser is substantially more accurate than Goldberg and Elhadad’s original implementation, with fairly similar overall speed. Furthermore, the results prove that Stanford-granularity dependency labels can be learned by modern dependency parsing systems when using our Treebank conversion, unlike the Stanford conversion, for which Cer et al. (2010) show that this isn’t the case. The optional sema</context>
<context position="6817" citStr="Nivre et al., 2006" startWordPosition="1017" endWordPosition="1020"> proven to be popular, (de Marneffe and Manning, 2008). Unlabeled derecent experiments by Cer et al. (2010) using the pendencies are clearly too impoverished for many Stanford conversion of the Penn Treebank indicate tasks. Similarly, the coarse labels of the CoNLL that it is difficult for current dependency parsers to tasks are not very specific; for example, the same relearn. Indeed, the highest scoring parsers trained us- lation, NMOD, is used for determiners, adjectives, ing the MSTPARSER (McDonald and Pereira, 2006) nouns, participle modifiers, relative clauses, etc. that and MALTPARSER (Nivre et al., 2006) parsing suites modify nouns. In contrast, the Stanford relations achieved only 78.8 and 81.1 labeled attachment provide a more reasonable level of granularity. F1, respectively. This contrasted with the much Our dependency relation scheme is similar to higher performance obtained using a constituent-to- Stanford’s basic scheme but has several differdependency conversion approach with accurate, but ences. It introduces several new relations including much slower, constituency parsers such as the Char- ccinit “initial coordinating conjunction”, cleft “cleft niak and Johnson (2005) and Berkeley </context>
<context position="26274" citStr="Nivre et al., 2006" startWordPosition="4086" endWordPosition="4089"> of the impact of the move operations on the parser’s overall performance; also, since the parsers used for comparison had no access to the unsupervised word clusters, an additional instance of the parser was trained with every word treated as belonging to the same cluster so as to facilitate a more fair comparison. Seven different dependency parsing models were 797.42% accuracy on traditional POS evaluation (Penn Treebank WSJ sections 22-24). trained for comparison using the following open source parsing packages: Goldberg and Elhadad’s (2010)’s non-directional easy-first parser, MALTPARSER (Nivre et al., 2006), and MSTPARSER (McDonald and Pereira, 2006)8. The model trained using Goldberg and Elhadad’s (2010) easy-first parser serves as something of a baseline. The four MALTPARSER parsing models used the arceager, arc-standard, stack-eager, and stack-lazy algorithms. One of the MSTPARSER models used the Chu-Liu-Edmonds maximum spanning tree approach, and the other used the Eisner (1996) algorithm with second order features and a nonprojective rewriting post-processing step. Unfortunately, it is not possible to directly compare the parser’s accuracy with most popular constituent parsers such as the C</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2006</marker>
<rawString>Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. MaltParser: A Data-Driven Parser-Generator for Dependency Parsing. In Proc. of LREC 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Jens Nilsson</author>
</authors>
<title>Pseudo-projective dependency parsing.</title>
<date>2005</date>
<booktitle>In Proc. of ACL-2005.</booktitle>
<marker>Nivre, Nilsson, 2005</marker>
<rawString>Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective dependency parsing. In Proc. of ACL-2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The Proposition Bank: An Annotated Corpus of Semantic Roles.</title>
<date>2005</date>
<booktitle>In Computational Linguistics. 31(1):71–106.</booktitle>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The Proposition Bank: An Annotated Corpus of Semantic Roles. In Computational Linguistics. 31(1):71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved Inference for Unlexicalized Parsing.</title>
<date>2007</date>
<booktitle>In Proc. of HLT-NAACL</booktitle>
<contexts>
<context position="2997" citStr="Petrov and Klein, 2007" startWordPosition="426" endWordPosition="429">w Nivre’s (2009) swap-based reordering technique for non-projective shift-reduce-style parsing can be integrated into the non-directional easy-first framework of Goldberg and Elhadad (2010) to support non-projectivity, and we report the results of our parsing experiments on the standard test section of the PTB, providing comparisons with several freely available parsers, including Goldberg and Elhadad’s (2010) implementation, MALTPARSER (Nivre et al., 2006), MSTPARSER (McDonald et al., 2005; McDonald and Pereira, 2006), the Charniak (2000) parser, and the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007). The experimental results show that the parser is substantially more accurate than Goldberg and Elhadad’s original implementation, with fairly similar overall speed. Furthermore, the results prove that Stanford-granularity dependency labels can be learned by modern dependency parsing systems when using our Treebank conversion, unlike the Stanford conversion, for which Cer et al. (2010) show that this isn’t the case. The optional semantic annotation modules also 1257 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1257–1268, Edinburgh, Scotland, UK</context>
<context position="7511" citStr="Petrov and Klein, 2007" startWordPosition="1114" endWordPosition="1117">ved only 78.8 and 81.1 labeled attachment provide a more reasonable level of granularity. F1, respectively. This contrasted with the much Our dependency relation scheme is similar to higher performance obtained using a constituent-to- Stanford’s basic scheme but has several differdependency conversion approach with accurate, but ences. It introduces several new relations including much slower, constituency parsers such as the Char- ccinit “initial coordinating conjunction”, cleft “cleft niak and Johnson (2005) and Berkeley (Petrov et clause”, combo “combined term”, extr “extraposed al., 2006; Petrov and Klein, 2007) parsers, which element”, infmark “infinitive marker ‘to’ ”, objcomp achieved 89.1 and 87.9 labeled F1 scores, respec- “object complement”, postloc “post-modifying lotively. cation”, sccomp “clausal complement of ‘so’ ”, vch “verbal chain” and whadvmod “wh- adverbial modifier”. The nsubjpass, csubjpass, and auxpass relations of Stanford’s are left out because adding them up front makes learning more difficult and the fact 1A tree is non-projective if the sequence of words visited in a left-to-right, depth-first traversal of the sentence’s parse tree is different than the actual word order of t</context>
<context position="26947" citStr="Petrov and Klein, 2007" startWordPosition="4189" endWordPosition="4192"> model trained using Goldberg and Elhadad’s (2010) easy-first parser serves as something of a baseline. The four MALTPARSER parsing models used the arceager, arc-standard, stack-eager, and stack-lazy algorithms. One of the MSTPARSER models used the Chu-Liu-Edmonds maximum spanning tree approach, and the other used the Eisner (1996) algorithm with second order features and a nonprojective rewriting post-processing step. Unfortunately, it is not possible to directly compare the parser’s accuracy with most popular constituent parsers such as the Charniak (2000) and Berkeley (Petrov et al., 2006; Petrov and Klein, 2007) parsers9 both because they do not produce functional tags for subjects, direct objects, etc., which are required for the final script of the constituent-to-dependency conversion routine, and because they determine part-of-speech tags in conjunction with the parsing. However, it is possible to compute approximate unlabeled accuracy scores by training the constituent parsers on the NP-patched (Vadas and Curran, 2007) version of the data and then running the test output through just the first conversion script—that is, the modified version of Johansson and Nugues’ (2007) converter. 1263 The resu</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved Inference for Unlexicalized Parsing. In Proc. of HLT-NAACL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proc. of COLING-ACL</booktitle>
<contexts>
<context position="2972" citStr="Petrov et al., 2006" startWordPosition="422" endWordPosition="425">ection 5). We show how Nivre’s (2009) swap-based reordering technique for non-projective shift-reduce-style parsing can be integrated into the non-directional easy-first framework of Goldberg and Elhadad (2010) to support non-projectivity, and we report the results of our parsing experiments on the standard test section of the PTB, providing comparisons with several freely available parsers, including Goldberg and Elhadad’s (2010) implementation, MALTPARSER (Nivre et al., 2006), MSTPARSER (McDonald et al., 2005; McDonald and Pereira, 2006), the Charniak (2000) parser, and the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007). The experimental results show that the parser is substantially more accurate than Goldberg and Elhadad’s original implementation, with fairly similar overall speed. Furthermore, the results prove that Stanford-granularity dependency labels can be learned by modern dependency parsing systems when using our Treebank conversion, unlike the Stanford conversion, for which Cer et al. (2010) show that this isn’t the case. The optional semantic annotation modules also 1257 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1257–1268</context>
<context position="26922" citStr="Petrov et al., 2006" startWordPosition="4185" endWordPosition="4188"> Pereira, 2006)8. The model trained using Goldberg and Elhadad’s (2010) easy-first parser serves as something of a baseline. The four MALTPARSER parsing models used the arceager, arc-standard, stack-eager, and stack-lazy algorithms. One of the MSTPARSER models used the Chu-Liu-Edmonds maximum spanning tree approach, and the other used the Eisner (1996) algorithm with second order features and a nonprojective rewriting post-processing step. Unfortunately, it is not possible to directly compare the parser’s accuracy with most popular constituent parsers such as the Charniak (2000) and Berkeley (Petrov et al., 2006; Petrov and Klein, 2007) parsers9 both because they do not produce functional tags for subjects, direct objects, etc., which are required for the final script of the constituent-to-dependency conversion routine, and because they determine part-of-speech tags in conjunction with the parsing. However, it is possible to compute approximate unlabeled accuracy scores by training the constituent parsers on the NP-patched (Vadas and Curran, 2007) version of the data and then running the test output through just the first conversion script—that is, the modified version of Johansson and Nugues’ (2007)</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proc. of COLING-ACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Kristy Hollingshead</author>
</authors>
<title>Linear complexity context-free parsing pipelines via chart constraints.</title>
<date>2009</date>
<booktitle>In Proc. of HLT-NAACL.</booktitle>
<marker>Roark, Hollingshead, 2009</marker>
<rawString>Brian Roark and Kristy Hollingshead. 2009. Linear complexity context-free parsing pipelines via chart constraints. In Proc. of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evan Sandhaus</author>
</authors>
<title>The New York Times Annotated Corpus. Linguistic Data Consortium,</title>
<date>2008</date>
<location>Philadelphia.</location>
<contexts>
<context position="21118" citStr="Sandhaus, 2008" startWordPosition="3260" endWordPosition="3261">), who used change ceased to have any notable impact on the the top branches of the cluster hierarchy as features. overall accuracy, but it was kept anyway. 6 However, unlike Koo et al.’s (2008) parser, the fine- The oracle used to determine whether a move opgrained cluster identifiers are used instead of just eration should be considered legal during the trainthe top 4-6 branches of the cluster hierarchy. The ing phase is similar to Nivre et al.’s (2009) improved 175 word clusters utilized by the parser were created oracle based upon maximal projective subcompofrom the New York Times corpus (Sandhaus, 2008). nents. As an additional restriction, during training, Some examples from the clusters are presented in move actions were only considered valid either if no Figure 3. The ideal number of such clusters was not other action was valid or if the token to be moved thoroughly investigated. already had all its children attached and moving it while where when although despite unless unlike ... caused it to be adjacent to its parent. This fits with why what whom whatever whoever whomever whence ... Nivre et al.’s (2009) intuition that it is best to delay based died involved runs ended lived charged bo</context>
</contexts>
<marker>Sandhaus, 2008</marker>
<rawString>Evan Sandhaus. 2008. The New York Times Annotated Corpus. Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Richard Johansson</author>
<author>Adam Meyers</author>
<author>Lluís Màrquez</author>
<author>Joakim Nivre</author>
</authors>
<title>The CoNLL2008 shared task on joint parsing of syntactic and semantic dependencies.</title>
<date>2008</date>
<booktitle>In Proc. of the Twelfth Conference on Computational Natural Language Learning.</booktitle>
<contexts>
<context position="33622" citStr="Surdeanu et al., 2008" startWordPosition="5215" endWordPosition="5218">gument and adjuncts parsing algorithms are the O(n) shift-reduce algocorresponding to dependency links, and 86.8 F1); rithms, such as Nivre’s (2003) algorithm and an exthis score is not directly comparable to any previ- pected linear time dynamic programming approach ous work due to some differences, including differ- presented by Huang and Sagae (2010), a few other ences in both the parse tree conversion and the Prop- fast alternatives exist. Goldberg and Elhadad’s Bank conversion. The most similar work is that of (2010) easy-first algorithm is one such example. Anthe CoNLL shared task work (Surdeanu et al., 2008; other example, is Roark and Hollingshead’s (2009) Hajiˇc et al., 2009). work that uses chart constraints to achieve linear 6 Related Work time complexity for constituency parsing. Non-projectivity. There are two main approaches Effective features for parsing. A variety of work used in recent NLP literature for handling non- has investigated the use of more informative feaprojectivity in parse trees. The first is to use an al- tures for parsing. This includes work that integorithm, like the one presented in this paper, that grates second and even third order features (McDonhas inherent suppor</context>
<context position="34947" citStr="Surdeanu et al., 2008" startWordPosition="5421" endWordPosition="5424">e the Chu-Liu-Edmonds’ ap- 2010). Also, some work has incorporated unsuperproach for maximum spanning tree (MST) parsing vised word clusters as features, including that of Koo (McDonald et al., 2005) and Nivre’s (2009) swap- et al. (2008) and Suzuki et al. (2009), who utilized based reordering method for shift-reduce parsing. unsupervised word clusters created using the Brown The second approach is to create an initial projec- et al. (1992) hierarchical clustering algorithm. tive parse and then apply transformations to intro- Semantically-enriched output. The 2008 and 2009 CoNLL shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009), which required participants to build systems capable of both syntactic parsing and Semantic Role Labeling (SRL) (Gildea and Jurafsky, 2002), are the most notable attempts to encour11These accuracy figures are higher than what should be expected for unseen datasets; see Tratz and Hovy (2010) for more detail. 1265 age the development of parsers with additional semantic annotation. These tasks relied upon PropBank (2005) and NomBank (2004) for the semantic roles. A variety of other systems have focused on FrameNet-based (1998) SRL instead, including those that participated</context>
</contexts>
<marker>Surdeanu, Johansson, Meyers, Màrquez, Nivre, 2008</marker>
<rawString>Mihai Surdeanu, Richard Johansson, Adam Meyers, Lluís Màrquez, and Joakim Nivre. 2008. The CoNLL2008 shared task on joint parsing of syntactic and semantic dependencies. In Proc. of the Twelfth Conference on Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Suzuki</author>
<author>Hideki Isozaki</author>
<author>Xavier Carrerras</author>
<author>Michael Collins</author>
</authors>
<title>An Empirical Study of Semisupervised Structured Conditional Models for Dependency Parsing.</title>
<date>2009</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="34589" citStr="Suzuki et al. (2009)" startWordPosition="5370" endWordPosition="5373">e use of more informative feaprojectivity in parse trees. The first is to use an al- tures for parsing. This includes work that integorithm, like the one presented in this paper, that grates second and even third order features (McDonhas inherent support for non-projective trees. Ex- ald et al., 2006; Carreras, 2007; Koo and Collins, amples of this include the Chu-Liu-Edmonds’ ap- 2010). Also, some work has incorporated unsuperproach for maximum spanning tree (MST) parsing vised word clusters as features, including that of Koo (McDonald et al., 2005) and Nivre’s (2009) swap- et al. (2008) and Suzuki et al. (2009), who utilized based reordering method for shift-reduce parsing. unsupervised word clusters created using the Brown The second approach is to create an initial projec- et al. (1992) hierarchical clustering algorithm. tive parse and then apply transformations to intro- Semantically-enriched output. The 2008 and 2009 CoNLL shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009), which required participants to build systems capable of both syntactic parsing and Semantic Role Labeling (SRL) (Gildea and Jurafsky, 2002), are the most notable attempts to encour11These accuracy figures are higher th</context>
</contexts>
<marker>Suzuki, Isozaki, Carrerras, Collins, 2009</marker>
<rawString>Jun Suzuki, Hideki Isozaki, Xavier Carrerras, and Michael Collins. 2009. An Empirical Study of Semisupervised Structured Conditional Models for Dependency Parsing. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pasi Tapanainen</author>
<author>Timo Järvinen</author>
</authors>
<title>A nonprojective dependency parser.</title>
<date>1997</date>
<booktitle>In Proc. of the fifth conference on applied natural language processing.</booktitle>
<marker>Tapanainen, Järvinen, 1997</marker>
<rawString>Pasi Tapanainen and Timo Järvinen. 1997. A nonprojective dependency parser. In Proc. of the fifth conference on applied natural language processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Tratz</author>
<author>Dirk Hovy</author>
</authors>
<title>Disambiguation of Preposition Sense using Linguistically Motivated Features.</title>
<date>2009</date>
<booktitle>In Proc. of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Student Research Workshop and Doctoral Consortium.</booktitle>
<contexts>
<context position="30763" citStr="Tratz and Hovy (2009)" startWordPosition="4786" endWordPosition="4789">iguation (PSD) system, a work-in-progress ’s-possessive interpretation system, a noun compound interpretation system, and a PropBank-based semantic role labeling system10. Taken together, these integrated modules enable the parsing system to produce substantially more informative output than a traditional parser. Preposition Sense Disambiguation The PSD system is a newer version of the system described 10Lack of space prohibits a sufficiently thorough discussion of these individual components and their evaluations, but additional information will be available with the system download. 1264 by Tratz and Hovy (2009) and Hovy et al. (2010); it duce non-projectivity into it. Examples of this inachieves 85.7% accuracy on the SemEval-2007 fine- clude McDonald and Pereira’s (2006) rewriting of grain PSD task (Litkowski and Hargraves, 2007), projective trees produced by the Eisner (1996) alwhich is a statistically significant (p&lt;=0.05; upper- gorithm, and Nivre and Nilsson’s (2005) pseudotailed z test) increase over the previous best reported projective approach that creates projective trees with result for this dataset, Hovy et al.’s (2010) 84.8%. specially marked arcs that are later transformed into Noun Com</context>
</contexts>
<marker>Tratz, Hovy, 2009</marker>
<rawString>Stephen Tratz and Dirk Hovy. 2009. Disambiguation of Preposition Sense using Linguistically Motivated Features. In Proc. of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Student Research Workshop and Doctoral Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Tratz</author>
<author>Eduard Hovy</author>
</authors>
<title>A Taxonomy, Dataset, and Classifier for Automatic Noun Compound Interpretation.</title>
<date>2010</date>
<booktitle>In Proc. of ACL</booktitle>
<contexts>
<context position="31566" citStr="Tratz and Hovy (2010)" startWordPosition="4906" endWordPosition="4909"> PSD task (Litkowski and Hargraves, 2007), projective trees produced by the Eisner (1996) alwhich is a statistically significant (p&lt;=0.05; upper- gorithm, and Nivre and Nilsson’s (2005) pseudotailed z test) increase over the previous best reported projective approach that creates projective trees with result for this dataset, Hovy et al.’s (2010) 84.8%. specially marked arcs that are later transformed into Noun Compound Interpretation The noun com- non-projective dependencies. pound interpretation system is a newer version of Descriptive dependency labels. While most rethe system described by Tratz and Hovy (2010) with cent dependency parsing research has used either similar accuracy (79.6% vs 79.3% using 10-fold vague labels, such as those of the CoNLL shared cross-validation11). tasks, or no labels at all, some descriptive depenPossessives Interpretation The possessive inter- dency label schemes exist. By far the most promipretation system assigns interpretations to ’s pos- nent of these is the Stanford typed dependency sessives (e.g., John’s arm → PART-OF, Mowgli’s scheme (de Marneffe and Manning, 2008). Ancapture → PATIENT/THEME). The current system other descriptive scheme that exists, but which i</context>
<context position="35262" citStr="Tratz and Hovy (2010)" startWordPosition="5472" endWordPosition="5475">uce parsing. unsupervised word clusters created using the Brown The second approach is to create an initial projec- et al. (1992) hierarchical clustering algorithm. tive parse and then apply transformations to intro- Semantically-enriched output. The 2008 and 2009 CoNLL shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009), which required participants to build systems capable of both syntactic parsing and Semantic Role Labeling (SRL) (Gildea and Jurafsky, 2002), are the most notable attempts to encour11These accuracy figures are higher than what should be expected for unseen datasets; see Tratz and Hovy (2010) for more detail. 1265 age the development of parsers with additional semantic annotation. These tasks relied upon PropBank (2005) and NomBank (2004) for the semantic roles. A variety of other systems have focused on FrameNet-based (1998) SRL instead, including those that participated in the SemEval-2007 Task 19 (Baker et al., 2007) and work by Das et al. (2010). 7 Conclusion In this paper, we have described a new high-quality dependency tree conversion of the Penn Treebank (Marcus, et al., 1993) along with its labeled dependency scheme and presented a parser that is fast, accurate, supports n</context>
</contexts>
<marker>Tratz, Hovy, 2010</marker>
<rawString>Stephen Tratz and Eduard Hovy. 2010. A Taxonomy, Dataset, and Classifier for Automatic Noun Compound Interpretation. In Proc. of ACL 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vadas</author>
<author>James R Curran</author>
</authors>
<title>Adding Noun Phrase Structure to the Penn Treebank.</title>
<date>2007</date>
<booktitle>In Proc. of ACL</booktitle>
<contexts>
<context position="11412" citStr="Vadas and Curran (2007)" startWordPosition="1698" endWordPosition="1701">ated as the head of the clause instead. 3The parsing system includes an optional script that can convert vch arcs into aux and auxpass and the subject relations into csubjpass and nsubjpass. Figure 1: Example comparing Stanford’s (top) handling of copula and coordinating conjunctions with ours (bottom). 3.2 Conversion Process A three-step process is used to convert the Penn Treebank (Marcus, et al., 1993) from constituent parses into dependency trees labeled according to the dependency scheme presented in the prior section. The first step is to apply the noun phrase structure patch created by Vadas and Curran (2007), which adds structure to the otherwise flat noun phrases (NPs) of the Penn Treebank (e.g., ‘(metal soup pot cover)’ would become ‘(metal (soup pot) cover)’). The second step is to apply a version of Johansson and Nugues’ (2007) constituent-todependency converter with some head-finding rule modifications; these rules, with changes highlighted 1259 (WH)?NP|NX|NML|NAC FW|NML|NN* JJR $|# CD|FW QP JJ|NAC JJS PRP ADJP RB[SR] VBG|DT|WP RB NP-C S|SBAR|UCP|PP SINV|SBARQ|SQ UH VP|NP VB|VBP ADJP|JJP NNS QP NN $|# JJ VBN VBG (AD|J)JP ADVP JJR NP|NML JJS DT FW RBR RBS SBAR RB ADVP RB|RBR|JJ|JJR RBS FW ADV</context>
<context position="27366" citStr="Vadas and Curran, 2007" startWordPosition="4250" endWordPosition="4253">ep. Unfortunately, it is not possible to directly compare the parser’s accuracy with most popular constituent parsers such as the Charniak (2000) and Berkeley (Petrov et al., 2006; Petrov and Klein, 2007) parsers9 both because they do not produce functional tags for subjects, direct objects, etc., which are required for the final script of the constituent-to-dependency conversion routine, and because they determine part-of-speech tags in conjunction with the parsing. However, it is possible to compute approximate unlabeled accuracy scores by training the constituent parsers on the NP-patched (Vadas and Curran, 2007) version of the data and then running the test output through just the first conversion script—that is, the modified version of Johansson and Nugues’ (2007) converter. 1263 The results of the experiment are given in Table 3, including accuracy for individual arcs, nonprojective arcs only, and full sentence match. Punctuation is excluded in all the result computations. To determine whether an arc is non-projective, the following heuristic was used. Traverse the sentence in a depth-first search, starting from the imaginary root node and pursuing child arcs in order of increasing absolute distanc</context>
</contexts>
<marker>Vadas, Curran, 2007</marker>
<rawString>David Vadas and James R. Curran. 2007. Adding Noun Phrase Structure to the Penn Treebank. In Proc. of ACL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyasu Yamada</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Statistical Dependency Analysis With Support Vector Machines.</title>
<date>2003</date>
<booktitle>In Proc. of 8th International Workshop on Parsing Technologies (IWPT).</booktitle>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical Dependency Analysis With Support Vector Machines. In Proc. of 8th International Workshop on Parsing Technologies (IWPT).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>