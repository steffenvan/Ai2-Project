<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001484">
<title confidence="0.978098">
Text Segmentation by Language Using Minimum Description Length
</title>
<author confidence="0.993634">
Hiroshi Yamaguchi Kumiko Tanaka-Ishii
</author>
<affiliation confidence="0.997349666666667">
Graduate School of Faculty and Graduate School of Information
Information Science and Technology, Science and Electrical Engineering,
University of Tokyo Kyushu University
</affiliation>
<email confidence="0.990405">
yamaguchi.hiroshi@ci.i.u-tokyo.ac.jp kumiko@ait.kyushu-u.ac.jp
</email>
<sectionHeader confidence="0.993704" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.995664375">
The problem addressed in this paper is to seg-
ment a given multilingual document into seg-
ments for each language and then identify the
language of each segment. The problem was
motivated by an attempt to collect a large
amount of linguistic data for non-major lan-
guages from the web. The problem is formu-
lated in terms of obtaining the minimum de-
scription length of a text, and the proposed so-
lution finds the segments and their languages
through dynamic programming. Empirical re-
sults demonstrating the potential of this ap-
proach are presented for experiments using
texts taken from the Universal Declaration of
Human Rights and Wikipedia, covering more
than 200 languages.
</bodyText>
<sectionHeader confidence="0.998745" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999947045454546">
For the purposes of this paper, a multilingual text
means one containing text segments, limited to those
longer than a clause, written in different languages.
We can often find such texts in linguistic resources
collected from the World Wide Web for many non-
major languages, which tend to also contain portions
of text in a major language. In automatic process-
ing of such multilingual texts, they must first be seg-
mented by language, and the language of each seg-
ment must be identified, since many state-of-the-art
NLP applications are built by learning a gold stan-
dard for one specific language. Moreover, segmen-
tation is useful for other objectives such as collecting
linguistic resources for non-major languages and au-
tomatically removing portions written in major lan-
guages, as noted above. The study reported here was
motivated by this objective. The problem addressed
in this article is thus to segment a multilingual text
by language and identify the language of each seg-
ment. In addition, for our objective, the set of target
languages consists of not only major languages but
also many non-major languages: more than 200 lan-
guages in total.
Previous work that directly concerns the problem
addressed in this paper is rare. The most similar
previous work that we know of comes from two
sources and can be summarized as follows. First,
(Teahan, 2000) attempted to segment multilingual
texts by using text segmentation methods used for
non-segmented languages. For this purpose, he used
a gold standard of multilingual texts annotated by
borders and languages. This segmentation approach
is similar to that of word segmentation for non-
segmented texts, and he tested it on six different
European languages. Although the problem set-
ting is similar to ours, the formulation and solution
are different, particularly in that our method uses
only a monolingual gold standard, not a multilin-
gual one as in Teahan’s study. Second, (Alex, 2005)
(Alex et al., 2007) solved the problem of detecting
words and phrases in languages other than the prin-
cipal language of a given text. They used statisti-
cal language modeling and heuristics to detect for-
eign words and tested the case of English embed-
ded in German texts. They also reported that such
processing would raise the performance of German
parsers. Here again, the problem setting is similar to
ours but not exactly the same, since the embedded
text portions were assumed to be words. Moreover,
the authors only tested for the specific language pair
of English embedded in German texts. In contrast,
our work considers more than 200 languages, and
the portions of embedded text are larger: up to the
paragraph level to accommodate the reality of mul-
tilingual texts. The extension of our work to address
the foreign word detection problem would be an in-
teresting future work.
From a broader view, the problem addressed in
this paper is further related to two genres of previ-
ous work. The first genre is text segmentation. Our
problem can be situated as a sub-problem from the
viewpoint of language change. A more common set-
ting in the NLP context is segmentation into seman-
tically coherent text portions, of which a represen-
tative method is text tiling as reported by (Hearst,
1997). There could be other possible bases for text
</bodyText>
<page confidence="0.989561">
969
</page>
<note confidence="0.9857785">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 969–978,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999951035714285">
segmentation, and our study, in a way, could lead
to generalizing the problem. The second genre is
classification, and the specific problem of text clas-
sification by language has drawn substantial atten-
tion (Grefenstette, 1995) (Kruengkrai et al., 2005)
(Kikui, 1996). Current state-of-the-art solutions use
machine learning methods for languages with abun-
dant supervision, and the performance is usually
high enough for practical use. This article con-
cerns that problem together with segmentation but
has another particularity in aiming at classification
into a substantial number of categories, i.e., more
than 200 languages. This means that the amount of
training data has to remain small, so the methods
to be adopted must take this point into considera-
tion. Among works on text classification into lan-
guages, our proposal is based on previous studies us-
ing cross-entropy such as (Teahan, 2000) and (Juola,
1997). We explain these works in further detail in
§3.
This article presents one way to formulate the seg-
mentation and identification problem as a combina-
torial optimization problem; specifically, to find the
set of segments and their languages that minimizes
the description length of a given multilingual text. In
the following, we describe the problem formulation
and a solution to the problem, and then discuss the
performance of our method.
</bodyText>
<sectionHeader confidence="0.959188" genericHeader="method">
2 Problem Formulation
</sectionHeader>
<bodyText confidence="0.999310581818182">
In our setting, we assume that a small amount (up
to kilobytes) of monolingual plain text sample data
is available for every language, e.g., the Universal
Declaration of Human Rights, which serves to gen-
erate the language model used for language identifi-
cation. This entails two sub-assumptions.
First, we assume that for all multilingual text,
every text portion is written in one of the given
languages; there is no input text of an unknown
language without learning data. In other words,
we use supervised learning. In line with recent
trends in unsupervised segmentation, the problem
of finding segments without supervision could be
solved through approaches such as Bayesian meth-
ods; however, we report our result for the supervised
setting since we believe that every segment must be
labeled by language to undergo further processing.
Second, we cannot assume a large amount of
learning data, since our objective requires us to con-
sider segmentation by both major and non-major
languages. For most non-major languages, only a
limited amount of corpus data is available.1
This constraint suggests the difficulty of applying
certain state-of the art machine learning methods re-
quiring a large learning corpus. Hence, our formu-
lation is based on the minimum description length
(MDL), which works with relatively small amounts
of learning data.
In this article, we use the following terms and
notations. A multilingual text to be segmented is
denoted as X = x1, ... , x|X|, where xi denotes
the i-th character of X and JXJ denotes the text’s
length. Text segmentation by language refers here
to the process of segmenting X by a set of borders
B = [B1, ... , B|B|], where JBJ denotes the num-
ber of borders, and each Bi indicates the location
of a language border as an offset number of charac-
ters from the beginning. Note that a pair of square
brackets indicates a list. Segmentation in this paper
is character-based, i.e., a Bi may refer to a position
inside a word. The list of segments obtained from
B is denoted as X = [X0, ... , X|B|], where the con-
catenation of the segments equals X. The language
of each segment Xi is denoted as Li, where Li E L,
the set of languages. Finally, L = [L0, ... , L|B|]
denotes the sequence of languages corresponding to
each segment Xi. The elements in each adjacent pair
in L must be different.
We formulate the problem of segmenting a multi-
lingual text by language as follows. Given a multi-
lingual text X, the segments X for a list of borders
B are obtained with the corresponding languages L.
Then, the total description length is obtained by cal-
culating each description length of a segment Xi for
the language Li:
</bodyText>
<equation confidence="0.984532">
�X, �L) = arg min
X,L
</equation>
<bodyText confidence="0.999229625">
The function dlLi(Xi) calculates the description
length of a text segment Xi through the use of a
language model for Li. Note that the actual total
description length must also include an additional
term, loge JXJ, giving information on the number
of segments (with the maximum to be segmented
&apos;In fact, our first motivation was to collect a certain amount
of corpus data for non-major languages from Wikipedia.
</bodyText>
<equation confidence="0.991617333333333">
(
∑ |B |dlLi(Xi). (1)
i=0
</equation>
<page confidence="0.972437">
970
</page>
<bodyText confidence="0.999397857142857">
by each character). Since this term is a common
constant for all possible segmentations and the min-
imization of formula (1) is not affected by this term,
we will ignore it.
The model defined by (1) is additive for Xi, so
the following formula can be applied to search for
language Li given a segment Xi :
</bodyText>
<equation confidence="0.9929285">
ˆLi = arg min dlLi(Xi), (2)
LicL
</equation>
<bodyText confidence="0.999270771428571">
under the constraint that Li =y� Li_1 for i E
11,... |B|}. The function dl can be further decom-
posed as follows to give the description length in an
information-theoretic manner:
Here, the first term corresponds to the code length
of the text chunk Xi given a language model for
Li, which in fact corresponds to the cross-entropy
of Xi for Li multiplied by |Xi|. The remaining
terms give the code lengths of the parameters used
to describe the length of the first term: the second
term corresponds to the segment location; the third
term, to the identified language; and the fourth term,
to the language model of language Li. This fourth
term will differ according to the language model
type; moreover, its value can be further minimized
through formula (2). Nevertheless, since we use a
uniform amount of training data for every language,
and since varying γ would prevent us from improv-
ing the efficiency of dynamic programming, as ex-
plained in §4, in this article we set γ to a constant
obtained empirically.
Under this formulation, therefore, when detect-
ing the language of a segment as in formula (2), the
terms of formula (3) other than the first term will be
constant: what counts is only the first term, simi-
larly to much of the previous work explained in the
following section. We thus perform language de-
tection itself by minimizing the cross-entropy rather
than the MDL. For segmentation, however, the con-
stant terms function as overhead and also serve to
prohibit excessive decomposition.
Next, after briefly introducing methods to calcu-
late the first term of formula (3), we explain the so-
lution to optimize the combinatorial problem of for-
mula (1).
</bodyText>
<sectionHeader confidence="0.641927" genericHeader="method">
3 Calculation of Cross-Entropy
</sectionHeader>
<bodyText confidence="0.999970666666667">
The first term of (3), − log2 PLi(Xi), is the cross-
entropy of Xi for Li multiplied by |Xi|. Vari-
ous methods for computing cross-entropy have been
proposed, and these can be roughly classified into
two types based on different methods of univer-
sal coding and the language model. For example,
(Benedetto et al., 2002) and (Cilibrasi and Vit´anyi,
2005) used the universal coding approach, whereas
(Teahan and Harper, 2001) and (Sibun and Reynar,
1996) were based on language modeling using PPM
and Kullback-Leibler divergence, respectively.
In this section, we briefly introduce two meth-
ods previously studied by (Juola, 1997) and (Teahan,
2000) as representative of the two types, and we fur-
ther explain a modification that we integrate into the
final optimization problem. We tested several other
coding methods, but they did not perform as well as
these two methods.
</bodyText>
<subsectionHeader confidence="0.999931">
3.1 Mean of Matching Statistics
</subsectionHeader>
<bodyText confidence="0.989570153846154">
(Farach et al., 1994) proposed a method to esti-
mate the entropy, through a simplified version of the
LZ algorithm (Ziv and Lempel, 1977), as follows.
Given a text X = x1x2 ... xixi+1 ..., Leni is de-
fined as the longest match length for two substrings
x1x2 ... xi and xi+1xi+2 .... In this article, we de-
fine the longest match for two strings A and B as the
shortest prefix of string B that is not a substring of
A. Letting the average of Leni be E [Len], Farach
proved that |E [Len] − log2 i
�(X) |probabilistically con-
verges to zero as i → oo, where H(X) indicates the
entropy of X. Then, H(X) is estimated as
</bodyText>
<equation confidence="0.976539">
ˆH(X) = log2 i
E [Len].
</equation>
<bodyText confidence="0.997792333333333">
(Juola, 1997) applied this method to estimate the
cross-entropy of two given texts. For two strings
Y = y1y2 ... yJY J and X = x1x2 ... xJXJ, let
Leni(Y) be the match length starting from xi of X
for Y 2. Based on this formulation, the cross-entropy
is approximately estimated as
</bodyText>
<equation confidence="0.937906">
ˆJY (X) = log2 |Y |
E [Leni(Y )].
2This is called a matching statistics value, which explains
the subsection title.
(3)
+ log2 |X |+ log2 |G |+ γ.
dlLi(Xi) = − log2 PLi(Xi)
</equation>
<page confidence="0.972108">
971
</page>
<bodyText confidence="0.9999196">
Since formula (1) of §2 is based on adding the
description length, it is important that the whole
value be additive to enable efficient optimization (as
will be explained in §4). We thus modified Juola’s
method as follows to make the length additive:
</bodyText>
<equation confidence="0.983539">
[ = E loge IY|
J0Y (X)
.
Leni(Y )
</equation>
<bodyText confidence="0.995742833333333">
Although there is no mathematical guarantee that
�JY (X) or �J0Y (X) actually converges to the cross-
entropy, our empirical tests showed a good estimate
for both cases3. In this article, we use �J0Y (X) as
a function to obtain the cross-entropy and for multi-
plication by |X |in formula (3).
</bodyText>
<subsectionHeader confidence="0.991783">
3.2 PPM
</subsectionHeader>
<bodyText confidence="0.99995925">
As a representative method for calculating the
cross-entropy through statistical language model-
ing, we adopt prediction by partial matching (PPM),
a language-based encoding method devised by
(Cleary and Witten, 1984). It has the particular char-
acteristic of using a variable n-gram length, unlike
ordinary n-gram models4. It models the probability
of a text X with a learning corpus Y as follows:
</bodyText>
<equation confidence="0.99029625">
PY (X) = PY (x1 ... x|X|)
|X|
=∏ PY (xt|xt−1 . . . xmax(1,t−n)),
t=1
</equation>
<bodyText confidence="0.999940555555556">
where n is a parameter of PPM, denoting the max-
imum length of the n-grams considered in the
models. The probability PY (X) is estimated by es-
cape probabilities favoring the longer sequences ap-
pearing in the learning corpus (Bell et al., 1990).
The total code length of X is then estimated as
− log PY (X). Since this value is additive and gives
the total code length of X for language Y , we adopt
this value in our approach.
</bodyText>
<sectionHeader confidence="0.891291" genericHeader="method">
4 Segmentation by Dynamic Programming
</sectionHeader>
<bodyText confidence="0.794844428571429">
By applying the above methods, we propose a solu-
tion to formula (1) through dynamic programming.
3This modification means that the original JY (X) is ob-
tained through the harmonic mean, with Len obtained
through the arithmetic mean, whereas �J1 (X) is obtained
through the arithmetic mean with Len as the harmonic
mean.
</bodyText>
<footnote confidence="0.90823">
4In the context of NLP, this is known as Witten-Bell smooth-
ing.
5In the experiments reported here, n is set to 5 throughout.
</footnote>
<bodyText confidence="0.922091428571429">
Considering the additive characteristic of the de-
scription length formulated previously as formula
(1), we denote the minimized description length for
a given text X simply as DP(X), which can be de-
composed recursively as follows6:
DP(X) = t∈{0,..r 1X1},L∈LI DP(x0 . . . xt−1) (4)
+ dlL(xt ... x|X|)},
In other words, the computation of DP(X) is de-
composed into obtaining the addition of two terms
by searching through t E 10, ... , |X|} and L E L.
The first term gives the MDL for the first t characters
of text X, while the second term, dlL(xt+1 ... x|X|),
gives the description length of the remaining charac-
ters under the language model for L.
We can straightforwardly implement this recur-
sive computation through dynamic programming, by
managing a table of size |X |x |L|. To fill a cell of
this table, formula (4) suggests referring to t x |L|
cells and calculating the description length of the
rest of the text for O(|X|−t) cells for each language.
Since t ranges up to |X|, the brute-force computa-
tional complexity is O(|X|3 x |L|2).
The complexity can be greatly reduced, however,
when the function dl is additive. First, the de-
scription length can be calculated from the previ-
ous result, decreasing O(|X |− t) to O(1) (to ob-
tain the code length of an additional character). Sec-
ond, the referred number of cells t x |L |is in fact
U x |L|, with U « |X|: for MMS, U can be
proven to be O(log |Y |), where |Y  |is the maximum
length among the learning corpora; and for PPM, U
corresponds to the maximum length of an n-gram.
Third, this factor U x |L |can be further decreased
to U x 2, since it suffices to possess the results for
the two7 best languages in computing the first term
of (4). Consequently, the complexity decreases to
O(U x |X |x |L|).
6This formula can be used directly to generate a set L in
which all adjacent elements differ. The formula can also be
used to generate segments for which some adjacent lan-
guages coincide and then further to generate L through
post-processing by concatenating segments of the same
language.
This number means the two best scores for different lan-
guages, which is required to obtain L directly: in addition
to the best score, if the language of the best coincides with
L in formula (4), then the second best is also needed. If
segments are subjected to post-processing, this value can
be one.
</bodyText>
<page confidence="0.994964">
972
</page>
<tableCaption confidence="0.999178">
Table 1: Number of languages for each writing system
</tableCaption>
<table confidence="0.986323666666667">
character kinds UDHR Wiki
Latin 260 158
Cyrillic 12 20
Devanagari 0 8
Arabic 1 6
Other 4 30
</table>
<sectionHeader confidence="0.992987" genericHeader="method">
5 Experimental Setting
</sectionHeader>
<subsectionHeader confidence="0.998393">
5.1 Monolingual Texts (Training / Test Data)
</subsectionHeader>
<bodyText confidence="0.985066428571429">
In this work, monolingual texts were used both for
training the cross-entropy computation and as test
data for cross-validation: the training data does not
contain any test data at all. Monolingual texts were
also used to build multilingual texts, as explained in
the following subsection.
Texts were collected from the World Wide Web
and consisted of two sets. The first data set con-
sisted of texts from the Universal Declaration of
Human Rights (UDHR)8. We consider UDHR the
most suitable text source for our purpose, since the
content of every monolingual text in the declaration
is unique. Moreover, each text has the tendency
to maximally use its own language and avoid vo-
cabulary from other languages. Therefore, UDHR-
derived results can be considered to provide an em-
pirical upper bound on our formulation. The set L
consists of 277 languages , and the texts consist of
around 10,000 characters on average.
The second data set was Wikipedia data from
Wikipedia Downloads9, denoted as “Wiki” in the
following discussion. We automatically assembled
the data through the following steps. First, tags in
the Wikipedia texts were removed. Second, short
lines were removed since they typically are not sen-
tences. Third, the amount of data was set to 10,000
characters for every language, in correspondence
with the size of the UDHR texts. Note that there
is a limit to the complete cleansing of data. After
these steps, the set L contained 222 languages with
sufficient data for the experiments.
Many languages adopt writing systems other than
the Latin alphabet. The numbers of languages for
various representative writing systems are listed in
Table 1 for both UDHR and Wiki, while the Ap-
</bodyText>
<footnote confidence="0.999717">
8http://www.ohchr.org/EN/UDHR/Pages/Introduction.aspx
9http://download.wikimedia.org/
</footnote>
<bodyText confidence="0.998946363636364">
pendix at the end of the article lists the actual lan-
guages. Note that in this article, a character means
a Unicode character throughout, which differs from
a character rendered in block form for some writing
systems.
To evaluate language identification for monolin-
gual texts, as will be reported in §6.1, we conducted
five-times cross-validation separately for both data
sets. We present the results in terms of the average
accuracy AL, the ratio of the number of texts with a
correctly identified language to |L|.
</bodyText>
<subsectionHeader confidence="0.99758">
5.2 Multilingual Texts (Test Data)
</subsectionHeader>
<bodyText confidence="0.999987277777778">
Multilingual texts were needed only to test the per-
formance of the proposed method. In other words,
we trained the model only through monolingual
data, as mentioned above. This differs from the
most similar previous study (Teahan, 2000), which
required multilingual learning data.
The multilingual texts were generated artificially,
since multilingual texts taken directly from the web
have other issues besides segmentation. First, proper
nouns in multilingual texts complicate the final judg-
ment of language and segment borders. In prac-
tical application, therefore, texts for segmentation
must be preprocessed by named entity recognition,
which is beyond the scope of this work. Second, the
sizes of text portions in multilingual web texts dif-
fer greatly, which would make it difficult to evaluate
the overall performance of the proposed method in a
uniform manner.
Consequently, we artificially generated two kinds
of test sets from a monolingual corpus. The first is
a set of multilingual texts, denoted as Test,, such
that each text is the conjunction of two portions in
different languages. Here, the experiment is focused
on segment border detection, which must segment
the text into two parts, provided that there are two
languages. Test, includes test data for all language
pairs, obtained by five-times cross-validation, giving
25 x |L |x (|L |−1) multilingual texts. Each portion
of text for a single language consists of 100 char-
acters taken from a random location within the test
data.
The second kind of test set is a set of multilingual
texts, denoted as Test2, each consisting of k seg-
ments in different languages. For the experiment, k
is not given to the procedure, and the task is to ob-
tain k as well as B and L through recursion. Test2
</bodyText>
<page confidence="0.995265">
973
</page>
<bodyText confidence="0.85855">
was generated through the following steps:
</bodyText>
<listItem confidence="0.997911666666667">
1. Choose k from among 1,... ,5.
2. Choose k languages randomly from L, where
some of the k languages can overlap.
3. Perform five-times cross-validation on the texts
of all languages. Choose a text length ran-
domly from {40,80,120,160}, and randomly
select this many characters from the test data.
4. Shuffle the k languages and concatenate the
text portions in the resultant order.
</listItem>
<bodyText confidence="0.999722051282051">
For this Test2 data set, every plot in the graphs
shown in §6.2 was obtained by randomly averaging
1,000 tests.
By default, the possibility of segmentation is con-
sidered at every character offset in a text, which
provides a lower bound for the proposed method.
Although language change within the middle of a
word does occur in real multilingual documents,
it might seem more realistic to consider language
change at word borders. Therefore, in addition to
choosing B from {1,..., |X|}, we also tested our
approach under the constraint of choosing borders
from bordering locations, which are the locations of
spaces. In this case, B is chosen from this subset of
{1, ... , |X|}, and, in step 3 above, text portions are
generated so as to end at these bordering locations.
Given a multilingual text, we evaluate the outputs
B and L through the following scores:
PB/RB: Precision/recall of the borders detected
(i.e., the correct borders detected, divided by
the detected/correct border).
PL/RL: Precision/recall of the languages detected
(i.e., the correct languages detected, divided by
the detected/correct language).
Ps and Rs are obtained by changing the param-
eter ry given in formula (3), which ranges over
1,2,4,... ,256 bits. In addition, we verify the speed,
i.e., the average time required for processing a text.
Although there are web pages consisting of texts
in more than 2 languages, we rarely see a web page
containing 5 languages at the same time. There-
fore, Test, reflects the most important case of 2 lan-
guages only, whereas Test2 reflects the case of mul-
tiple languages to demonstrate the general potential
of the proposed approach.
The experiment reported here might seem like a
case of over-specification, since all languages are
considered equally likely to appear. Since our mo-
tivation has been to eliminate a portion in a major
</bodyText>
<figure confidence="0.986661">
0 20 40 60 80 100 120 140 160 180 200
input length (characters)
</figure>
<figureCaption confidence="0.997254">
Figure 1: Accuracy of language identification for mono-
lingual texts
</figureCaption>
<bodyText confidence="0.999896125">
language from the text, there could be a formula-
tion specific to the problem. We consider it trivial,
however, to specify such a narrow problem within
our formulation, and it will lead to higher perfor-
mance than that of the reported results, in any case.
Therefore, we believe that our general formulation
and experiment show the broadest potential of our
approach to solving this problem.
</bodyText>
<sectionHeader confidence="0.999555" genericHeader="method">
6 Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.999544">
6.1 Language Identification Performance
</subsectionHeader>
<bodyText confidence="0.959511541666667">
We first show the performance of language identifi-
cation using formula (2), which is used as the com-
ponent of the text segmentation by language. Fig-
ure 1 shows the results for language identification
of monolingual texts with the UDHR and Wiki test
data. The horizontal axis indicates the size of the in-
put text in characters, the vertical axis indicates the
accuracy AL, and the graph contains four plotslc for
MMS and PPM for each set of data.
Overall, all plots rise quickly despite the se-
vere conditions of a large number of languages
(over 200), a small amount of input data, and a
small amount of learning data. The results show
that language identification through cross-entropy is
promising.
Two further global tendencies can be seen. First,
the performance was higher for UDHR than for
Wiki. This is natural, since the content of Wikipedia
is far broader than that of UDHR. In the case of
UDHR, when the test data had a length of 40 char-
acters, the accuracy was over 95% for both the PPM
and the MMS methods. Second, PPM achieved
&amp;quot;The results for PPM and MMS for UDHR are almost the
same, so the graph appears to contain only three plots.
</bodyText>
<figure confidence="0.953036071428571">
PPM (UDHR)
MMS (UDHR)
PPM (Wiki)
MMS (Wiki)
accuracy 1
0.95
0.9
0.85
0.8
0.75
0.7
974
-5 -4 -3 -2 -1 0 1 2 3 4 5
relative position (characters)
</figure>
<figureCaption confidence="0.999811">
Figure 2: Cumulative distribution of segment borders
</figureCaption>
<bodyText confidence="0.999928826086957">
slightly better performance than did MMS. When
the test data amounted to 100 characters, PPM
achieved language identification with accuracy of
about 91.4%. For MMS, the identification accu-
racy was a little less significant and was about 90.9%
even with 100 characters of test data.
The amount of learning data seemed sufficient for
both cases, with around 8,000 characters. In fact,
we conducted tests with larger amounts of learning
data and found a faster rise with respect to the input
length, but the maximum possible accuracy did not
show any significant increase.
Errors resulted from either noise or mistakes due
to the language family. The Wikipedia test data was
noisy, as mentioned in §5.1. As for language fam-
ily errors, the test data includes many similar lan-
guages that are difficult even for humans to correctly
judge. For example, Indonesian and Malay, Picard
and Walloon, and Norwegian Bokm˚al and Nynorsk
are all pairs representative of such confusion.
Overall, the language identification performance
seems sufficient to justify its application to our main
problem of text segmentation by language.
</bodyText>
<subsectionHeader confidence="0.998388">
6.2 Text Segmentation by Language
</subsectionHeader>
<bodyText confidence="0.999718272727273">
First, we report the results obtained using the Test,
data set. Figure 2 shows the cumulative distribution
obtained for segment border detection. The horizon-
tal axis indicates the relative location by character
with respect to the correct border at zero, and the
vertical axis indicates the cumulative proportion of
texts whose border is detected at that relative point.
The figure shows four plots for all combinations of
the two data sets and the two methods. Note that
segment borders are judged by characters and not
by bordering locations, as explained in §5.2.
</bodyText>
<figure confidence="0.75478725">
0.8 0.85 0.9 0.95 1
recall
0.6 0.65 0.7 0.75 0.8
recall
</figure>
<figureCaption confidence="0.992383333333333">
Figure 3: PL/RL (language, upper graph) and PB/RB
(border, lower graph) results, where borders were taken
from any character offset
</figureCaption>
<bodyText confidence="0.99996925">
Since the plots rise sharply at the middle of the
horizontal axis, the borders were detected at or very
near the correct place in many cases.
Next, we examine the results for Test2. Fig-
ure 3 shows the two precision/recall graphs for lan-
guage identification (upper graph) and segment bor-
der detection (lower graph), where borders were
taken from any character offset. In each graph,
the horizontal axis indicates precision and the ver-
tical axis indicates recall. The numbers appearing
in each figure are the maximum F-score values for
each method and data set combination. As can be
seen from these numbers, the language identifica-
tion performance was high. Since the text portion
size was chosen from among the values 40, 80, 120,
or 160, the performance is comprehensible from the
results shown in §6.1. Note also that PPM performed
slightly better than did MMS.
For segment border performance (lower graph),
however, the results were limited. The main reason
for this is that both MMS and PPM tend to detect
a border one character earlier than the correct loca-
tion, as was seen in Figure 2. At the same time,
much of the test data contains unrealistic borders
</bodyText>
<figure confidence="0.999208209302326">
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1
PPM (UDHR)
MMS (UDHR)
PPM (Wiki)
MMS (Wiki)
0.98
0.97
0.88
0.87
PPM (UDHR)
MMS (UDHR)
PPM (Wiki)
MMS (Wiki)
precision
0.75
0.65
0.8
0.7
0.6
PPM (UDHR)
MMS (UDHR)
PPM (Wiki)
MMS (Wiki)
0.68
0.70
0.76
0.77
cummulative proportion
precision 1
0.95
0.9
0.85
0.8
</figure>
<page confidence="0.892481">
975
</page>
<figureCaption confidence="0.999683">
Figure 5: Average processing speed for a text
</figureCaption>
<bodyText confidence="0.996348868421053">
within a word, since the data was generated by con-
catenating two text portions with random borders.
Therefore, we repeated the experiment with Test2
under the constraint that a segment border could oc-
cur only at a bordering location, as explained in §5.2.
The results with this constraint were significantly
better, as shown in Figure 4. The best result was for
UDHR with PPM at 0.9411. We could also observe
how PPM performed better at detecting borders in
this case. In actual application, it would be possible
to improve performance by relaxing the procedural
conditions, such as by decreasing the number of lan-
guage possibilities.
In this experiment for Test2, k ranged from 1 to
5, but the performance was not affected by the size
of k. When the F-score was examined with respect
to k, it remained almost equal to k in all cases. This
shows how each recursion of formula (4) works al-
most independently, having segmentation and lan-
guage identification functions that are both robust.
Lastly, we examine the speed of our method.
Since |L |is constant throughout the comparison,
&amp;quot;The language identification accuracy slightly increased as
well, by 0.002.
the time should increase linearly with respect to the
input length |X|, with increasing k having no ef-
fect. Figure 5 shows the speed for Test2 processing,
with the horizontal axis indicating the input length
and the vertical axis indicating the processing time.
Here, all character offsets were taken into consid-
eration, and the processing was done on a machine
with a Xeon5650 2.66-GHz CPU. The results con-
firm that the complexity increased linearly with re-
spect to the input length. When the text size became
as large as several thousand characters, the process-
ing time became as long as a second. This time
could be significantly decreased by introducing con-
straints on the bordering locations and languages.
</bodyText>
<sectionHeader confidence="0.999046" genericHeader="method">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999902448275862">
This article has presented a method for segmenting
a multilingual text into segments, each in a differ-
ent language. This task could serve for preprocess-
ing of multilingual texts before applying language-
specific analysis to each text. Moreover, the pro-
posed method could be used to generate corpora in a
variety of languages, since many texts in minor lan-
guages tend to contain chunks in a major language.
The segmentation task was modeled as an opti-
mization problem of finding the best segment and
language sequences to minimize the description
length of a given text. An actual procedure for ob-
taining an optimal result through dynamic program-
ming was proposed. Furthermore, we showed a way
to decrease the computational complexity substan-
tially, with each of our two methods having linear
complexity in the input length.
Various empirical results were shown for lan-
guage identification and segmentation. Overall,
when segmenting a text with up to five random por-
tions of different languages, where each portion con-
sisted of 40 to 120 characters, the best F-scores for
language identification and segmentation were 0.98
and 0.94, respectively.
For our future work, details of the methods must
be worked out. In general, the proposed approach
could be further applied to the actual needs of pre-
processing and to generating corpora of minor lan-
guages.
</bodyText>
<figure confidence="0.996865">
PPM (UDHR)
MMS (UDHR)
PPM (Wiki)
MMS (Wiki)
0.81
0.84
0.91
0.94
0.7 0.75 0.8 0.85 0.9 0.95 1
recall
</figure>
<figureCaption confidence="0.886848">
Figure 4: Pa/Ra, where borders were limited to spaces
</figureCaption>
<figure confidence="0.9989813">
precision 1
0.95
0.9
0.85
0.8
0.75
0.7
0 200 400 600 800 1000
input length (characters)
time (s)
0.8
0.6
0.4
0.2
0
1
PPM (UDHR)
MMS (UDHR)
PPM (Wiki)
MMS (Wiki)
</figure>
<page confidence="0.994642">
976
</page>
<sectionHeader confidence="0.989747" genericHeader="conclusions">
References
</sectionHeader>
<reference confidence="0.999399984126984">
Beatrice Alex, Amit Dubey, and Frank Keller. 2007.
Using foreign inclusion detection to improve parsing
performance. In Proceedings of the Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 151–160.
Beatrice Alex. 2005. An unsupervised system for iden-
tifying english inclusions in german text. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics, Student Research
Workshop, pages 133–138.
T.C. Bell, J.G. Cleary, and I. H. Witten. 1990. Text Com-
pression. Prentice Hall.
Dario Benedetto, Emanuele Caglioti, and Vittorio Loreto.
2002. Language trees and zipping. Physical Review
Letters, 88(4).
Rudi Cilibrasi and Paul Vit´anyi. 2005. Clustering by
compression. IEEE Transactions on Information The-
ory, 51(4):1523–1545.
John G. Cleary and Ian H. Witten. 1984. Data compres-
sion using adaptive coding and partial string matching.
IEEE Transactions on Communications, 32:396–402.
Martin Farach, Michiel Noordewier, Serap Savari, Larry
Shepp, Abraham J. Wyner, and Jacob Ziv. 1994. On
the entropy of dna: Algorithms and measurements
based on memory and rapid convergence. In Proceed-
ings of the Sixth Annual ACM-SIAM Symposium on
Discrete Algorithms, pages 48–57.
Gregory Grefenstette. 1995. Comparing two language
identification schemes. In Proceedings of 3rd Inter-
national Conference on Statistical Analysis of Textual
Data, pages 263–268.
Marti A. Hearst. 1997. Texttiling: Segmenting text into
multi-paragraph subtopic passages. Computational
Linguistics, 23(1):33–64.
Patrick Juola. 1997. What can we do with small cor-
pora? document categorization via cross-entropy. In
Proceedings of an Interdisciplinary Workshop on Sim-
ilarity and Categorization.
Gen-itiro Kikui. 1996. Identifying the coding system and
language of on-line documents on the internet. In Pro-
ceedings of 16th International Conference on Compu-
tational Linguistics, pages 652–657.
Casanai Kruengkrai, Prapass Srichaivattana, Virach
Sornlertlamvanich, and Hitoshi Isahara. 2005. Lan-
guage identification based on string kernels. In
Proceedings of the 5th International Symposium on
Communications and Information Technologies, pages
926–929.
Penelope Sibun and Jeffrey C. Reynar. 1996. Language
identification: Examining the issues. In Proceedings
of 5th Symposium on Document Analysis and Infor-
mation Retrieval, pages 125–135.
William J. Teahan and David J. Harper. 2001. Using
compression-based language models for text catego-
rization. In Proceedings of the Workshop on Language
Modeling and Information Retrieval, pages 83–88.
William John Teahan. 2000. Text classification and seg-
mentation using minimum cross-entropy. In RIAO,
pages 943–961.
Jacob Ziv and Abraham Lempel. 1977. A universal al-
gorithm for sequential data compression. IEEE Trans-
actions on Information Theory, 23(3):337–343.
</reference>
<sectionHeader confidence="0.796416" genericHeader="references">
Appendix
</sectionHeader>
<reference confidence="0.978260363636363">
This Appendix lists all the languages contained in our data sets,
as summarized in Table 1.
For UDHR
Latin
Achinese, Achuar-Shiwiar, Adangme, Afrikaans, Aguaruna,
Aja, Akuapem Akan, Akurio, Amahuaca, Amarakaeri, Ambo-
Pasco Quechua, Arabela, Arequipa-La Uni´on Quechua, Arpi-
tan, Asante Akan, Ash´aninka, Ash´eninka Pajonal, Asturian,
Auvergnat Occitan, Ayacucho Quechua, Aymara, Baatonum,
Balinese, Bambara, Baoul´e, Basque, Bemba, Beti, Bikol, Bini,
Bislama, Bokm˚al Norwegian, Bora, Bosnian, Breton, Buginese,
Cajamarca Quechua, Calder´on Highland Quichua, Candoshi-
Shapra, Caquinte, Cashibo-Cacataibo, Cashinahua, Catalan,
Cebuano, Central Kanuri, Central Mazahua, Central Nahuatl,
Chamorro, Chamula Tzotzil, Chayahuita, Chickasaw, Chiga,
Chokwe, Chuanqiandian Cluster Miao, Chuukese, Corsican,
Cusco Quechua, Czech, Dagbani, Danish, Dendi, Ditammari,
Dutch, Eastern Maninkakan, Emiliano-Romagnolo, English,
Esperanto, Estonian, Ewe, Falam Chin, Fanti, Faroese, Fi-
jian, Filipino, Finnish, Fon, French, Friulian, Ga, Gagauz,
Galician, Ganda, Garifuna, Gen, German, Gheg Albanian,
Gonja, Guarani, G¨uil´a Zapotec, Haitian Creole, Haitian Cre-
ole (popular), Haka Chin, Hani, Hausa, Hawaiian, Hiligaynon,
Huamal´ıes-Dos de Mayo Hu´anuco Quechua, Huautla Maza-
tec, Huaylas Ancash Quechua, Hungarian, Ibibio, Icelandic,
Ido, Igbo, Iloko, Indonesian, Interlingua, Irish, Italian, Ja-
vanese, Jola-Fonyi, K’iche’, Kabiy`e, Kabuverdianu, Kalaal-
lisut, Kaonde, Kaqchikel, Kasem, Kekchi, Kimbundu, Kin-
yarwanda, Kituba, Konzo, Kpelle, Krio, Kurdish, Lamnso’,
Languedocien Occitan, Latin, Latvian, Lingala, Lithuanian,
Lozi, Luba-Lulua, Lunda, Luvale, Luxembourgish, Madurese,
Makhuwa, Makonde, Malagasy, Maltese, Mam, Maori,
Mapudungun, Margos-Yarowilca-Lauricocha Quechua, Mar-
shallese, Mba, Mende, Metlat´onoc Mixtec, Mezquital Otomi,
Mi’kmaq, Miahuatl´an Zapotec, Minangkabau, Mossi, Mozara-
bic, Murui Huitoto, Miskito, Ndonga, Nigerian Pidgin, Nomat-
siguenga, North Junin Quechua, Northeastern Dinka, Northern
Conchucos Ancash Quechua, Northern Qiandong Miao, North-
ern Sami, Northern Kurdish, Nyamwezi, Nyanja, Nyemba,
Nynorsk Norwegian, Nzima, Ojitl´aan Chinantec, Oromo,
Palauan, Pampanga, Papantla Totonac, Pedi, Picard, Pichis
Ash´eninka, Pijin, Pipil, Pohnpeian, Polish, Portuguese, Pu-
laar, Purepecha, P´aez, Quechua, Rarotongan, Romanian, Ro-
mansh, Romany, Rundi, Salinan, Samoan, San Luis PotosiHuastec, Sango, Sardinian, Scots, Scottish Gaelic, Serbian,
</reference>
<page confidence="0.980745">
977
</page>
<reference confidence="0.995794030769231">
Serer, Seselwa Creole French, Sharanahua, Shipibo-Conibo,
Shona, Slovak, Somali, Soninke, South Ndebele, Southern
Dagaare, Southern Qiandong Miao, Southern Sotho, Spanish,
Standard Malay, Sukuma, Sundanese, Susu, Swahili, Swati,
Swedish, S˜aotomense, Tahitian, Tedim Chin, Tetum, Tidikelt
Tamazight, Timne, Tiv, Toba, Tojolabal, Tok Pisin, Tonga
(Tonga Islands), Tonga (Zambia), Tsonga, Tswana, Turkish,
Tzeltal, Umbundu, Upper Sorbian, Urarina, Uzbek, Veracruz
Huastec, Vili, Vlax Romani, Walloon, Waray, Wayuu, Welsh,
Western Frisian, Wolof, Xhosa, Yagua, Yanesha’, Yao, Yapese,
Yoruba, Yucateco, Zhuang, Zulu
Cyrillic
Abkhazian, Belarusian, Bosnian, Bulgarian, Kazakh, Mace-
donian, Ossetian, Russian, Serbian, Tuvinian, Ukrainian, Yakut
Arabic
Standard Arabic
Other
Japanese, Korean, Mandarin Chinese, Modern Greek
For Wiki
Latin
Afrikaans, Albanian, Aragonese, Aromanian, Arpitan, As-
turian, Aymara, Azerbaijani, Bambara, Banyumasan, Basque,
Bavarian, Bislama, Bosnian, Breton, Catal`a, Cebuano, Central
Bikol, Chavacano, Cornish, Corsican, Crimean Tatar, Croatian,
Czech, Danish, Dimli, Dutch, Dutch Low Saxon, Emiliano-
Romagnolo, English, Esperanto, Estonian, Ewe, Extremaduran,
Faroese, Fiji Hindi, Finnish, French, Friulian, Galician, Ger-
man, Gilaki, Gothic, Guarani, Hai//om, Haitian, Hakka Chi-
nese, Hawaiian, Hungarian, Icelandic, Ido, Igbo, Iloko, Indone-
sian, Interlingua, Interlingue, Irish, Italian, Javanese, Kabyle,
Kalaallisut, Kara-Kalpak, Kashmiri, Kashubian, Kongo, Ko-
rean, Kurdish, Ladino, Latin, Latvian, Ligurian, Limburgan,
Lingala, Lithuanian, Lojban, Lombard, Low German, Lower
Sorbian, Luxembourgish, Malagasy, Malay, Maltese, Manx,
Maori, Mazanderani, Min Dong Chinese, Min Nan Chinese,
Nahuatl, Narom, Navajo, Neapolitan, Northern Sami, Norwe-
gian, Norwegian Nynorsk, Novial, Occitan, Old English, Pam-
panga, Pangasinan, Panjabi, Papiamento, Pennsylvania Ger-
man, Piemontese, Pitcairn-Norfolk, Polish, Portuguese, Pushto,
Quechua, Romanian, Romansh, Samoan, Samogitian Lithua-
nian, Sardinian, Saterfriesisch, Scots, Scottish Gaelic, Serbo-
Croatian, Sicilian, Silesian, Slovak, Slovenian, Somali, Span-
ish, Sranan Tongo, Sundanese, Swahili, Swati, Swedish, Taga-
log, Tahitian, Tarantino Sicilian, Tatar, Tetum, Tok Pisin, Tonga
(Tonga Islands), Tosk Albanian, Tsonga, Tswana, Turkish,
Turkmen, Uighur, Upper Sorbian, Uzbek, Venda, Venetian,
Vietnamese, Vlaams, Vlax Romani, Volap¨uk, V˜oro, Walloon,
Waray, Welsh, Western Frisian, Wolof, Yoruba, Zeeuws, Zulu
Cyrillic
Abkhazian, Bashkir, Belarusian, Bulgarian, Chuvash, Erzya,
Kazakh, Kirghiz, Macedonian, Moksha, Moldovan, Mongo-
lian, Old Belarusian, Ossetian, Russian, Serbian, Tajik, Udmurt,
Ukrainian, Yakut
Arabic
Arabic, Egyptian Arabic, Gilaki, Mazanderani, Persian,
Pushto, Uighur, Urdu
Devanagari
Bihari, Hindi, Marathi, Nepali, Newari, Sanskrit
Other
Amharic, Armenian, Assamese, Bengali, Bishnupriya,
Burmese, Central Khmer, Chinese, Classical Chinese, Dhivehi,
Gan Chinese, Georgian, Gothic, Gujarati, Hebrew, Japanese,
Kannada, Lao, Malayalam, Modern Greek, Official Aramaic,
Panjabi, Sinhala, Tamil, Telugu, Thai, Tibetan, Wu Chinese,
Yiddish, Yue Chinese
</reference>
<page confidence="0.997743">
978
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.590517">
<title confidence="0.998319">Text Segmentation by Language Using Minimum Description Length</title>
<author confidence="0.965044">Hiroshi Yamaguchi Kumiko Tanaka-Ishii</author>
<affiliation confidence="0.998111666666667">Graduate School of Faculty and Graduate School of Information Information Science and Technology, Science and Electrical Engineering, University of Tokyo Kyushu University</affiliation>
<email confidence="0.620658">yamaguchi.hiroshi@ci.i.u-tokyo.ac.jpkumiko@ait.kyushu-u.ac.jp</email>
<abstract confidence="0.999267294117647">The problem addressed in this paper is to segment a given multilingual document into segments for each language and then identify the language of each segment. The problem was motivated by an attempt to collect a large amount of linguistic data for non-major languages from the web. The problem is formulated in terms of obtaining the minimum description length of a text, and the proposed solution finds the segments and their languages through dynamic programming. Empirical results demonstrating the potential of this approach are presented for experiments using texts taken from the Universal Declaration of Human Rights and Wikipedia, covering more than 200 languages.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Beatrice Alex</author>
<author>Amit Dubey</author>
<author>Frank Keller</author>
</authors>
<title>Using foreign inclusion detection to improve parsing performance.</title>
<date>2007</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>151--160</pages>
<contexts>
<context position="2995" citStr="Alex et al., 2007" startWordPosition="466" endWordPosition="469">(Teahan, 2000) attempted to segment multilingual texts by using text segmentation methods used for non-segmented languages. For this purpose, he used a gold standard of multilingual texts annotated by borders and languages. This segmentation approach is similar to that of word segmentation for nonsegmented texts, and he tested it on six different European languages. Although the problem setting is similar to ours, the formulation and solution are different, particularly in that our method uses only a monolingual gold standard, not a multilingual one as in Teahan’s study. Second, (Alex, 2005) (Alex et al., 2007) solved the problem of detecting words and phrases in languages other than the principal language of a given text. They used statistical language modeling and heuristics to detect foreign words and tested the case of English embedded in German texts. They also reported that such processing would raise the performance of German parsers. Here again, the problem setting is similar to ours but not exactly the same, since the embedded text portions were assumed to be words. Moreover, the authors only tested for the specific language pair of English embedded in German texts. In contrast, our work co</context>
</contexts>
<marker>Alex, Dubey, Keller, 2007</marker>
<rawString>Beatrice Alex, Amit Dubey, and Frank Keller. 2007. Using foreign inclusion detection to improve parsing performance. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 151–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beatrice Alex</author>
</authors>
<title>An unsupervised system for identifying english inclusions in german text.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, Student Research Workshop,</booktitle>
<pages>133--138</pages>
<contexts>
<context position="2975" citStr="Alex, 2005" startWordPosition="464" endWordPosition="465">lows. First, (Teahan, 2000) attempted to segment multilingual texts by using text segmentation methods used for non-segmented languages. For this purpose, he used a gold standard of multilingual texts annotated by borders and languages. This segmentation approach is similar to that of word segmentation for nonsegmented texts, and he tested it on six different European languages. Although the problem setting is similar to ours, the formulation and solution are different, particularly in that our method uses only a monolingual gold standard, not a multilingual one as in Teahan’s study. Second, (Alex, 2005) (Alex et al., 2007) solved the problem of detecting words and phrases in languages other than the principal language of a given text. They used statistical language modeling and heuristics to detect foreign words and tested the case of English embedded in German texts. They also reported that such processing would raise the performance of German parsers. Here again, the problem setting is similar to ours but not exactly the same, since the embedded text portions were assumed to be words. Moreover, the authors only tested for the specific language pair of English embedded in German texts. In c</context>
</contexts>
<marker>Alex, 2005</marker>
<rawString>Beatrice Alex. 2005. An unsupervised system for identifying english inclusions in german text. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, Student Research Workshop, pages 133–138.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T C Bell</author>
<author>J G Cleary</author>
<author>I H Witten</author>
</authors>
<title>Text Compression.</title>
<date>1990</date>
<publisher>Prentice Hall.</publisher>
<contexts>
<context position="14326" citStr="Bell et al., 1990" startWordPosition="2410" endWordPosition="2413">modeling, we adopt prediction by partial matching (PPM), a language-based encoding method devised by (Cleary and Witten, 1984). It has the particular characteristic of using a variable n-gram length, unlike ordinary n-gram models4. It models the probability of a text X with a learning corpus Y as follows: PY (X) = PY (x1 ... x|X|) |X| =∏ PY (xt|xt−1 . . . xmax(1,t−n)), t=1 where n is a parameter of PPM, denoting the maximum length of the n-grams considered in the models. The probability PY (X) is estimated by escape probabilities favoring the longer sequences appearing in the learning corpus (Bell et al., 1990). The total code length of X is then estimated as − log PY (X). Since this value is additive and gives the total code length of X for language Y , we adopt this value in our approach. 4 Segmentation by Dynamic Programming By applying the above methods, we propose a solution to formula (1) through dynamic programming. 3This modification means that the original JY (X) is obtained through the harmonic mean, with Len obtained through the arithmetic mean, whereas �J1 (X) is obtained through the arithmetic mean with Len as the harmonic mean. 4In the context of NLP, this is known as Witten-Bell smoot</context>
</contexts>
<marker>Bell, Cleary, Witten, 1990</marker>
<rawString>T.C. Bell, J.G. Cleary, and I. H. Witten. 1990. Text Compression. Prentice Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dario Benedetto</author>
<author>Emanuele Caglioti</author>
<author>Vittorio Loreto</author>
</authors>
<title>Language trees and zipping.</title>
<date>2002</date>
<journal>Physical Review Letters,</journal>
<volume>88</volume>
<issue>4</issue>
<contexts>
<context position="11358" citStr="Benedetto et al., 2002" startWordPosition="1879" endWordPosition="1882">segmentation, however, the constant terms function as overhead and also serve to prohibit excessive decomposition. Next, after briefly introducing methods to calculate the first term of formula (3), we explain the solution to optimize the combinatorial problem of formula (1). 3 Calculation of Cross-Entropy The first term of (3), − log2 PLi(Xi), is the crossentropy of Xi for Li multiplied by |Xi|. Various methods for computing cross-entropy have been proposed, and these can be roughly classified into two types based on different methods of universal coding and the language model. For example, (Benedetto et al., 2002) and (Cilibrasi and Vit´anyi, 2005) used the universal coding approach, whereas (Teahan and Harper, 2001) and (Sibun and Reynar, 1996) were based on language modeling using PPM and Kullback-Leibler divergence, respectively. In this section, we briefly introduce two methods previously studied by (Juola, 1997) and (Teahan, 2000) as representative of the two types, and we further explain a modification that we integrate into the final optimization problem. We tested several other coding methods, but they did not perform as well as these two methods. 3.1 Mean of Matching Statistics (Farach et al.,</context>
</contexts>
<marker>Benedetto, Caglioti, Loreto, 2002</marker>
<rawString>Dario Benedetto, Emanuele Caglioti, and Vittorio Loreto. 2002. Language trees and zipping. Physical Review Letters, 88(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rudi Cilibrasi</author>
<author>Paul Vit´anyi</author>
</authors>
<title>Clustering by compression.</title>
<date>2005</date>
<journal>IEEE Transactions on Information Theory,</journal>
<volume>51</volume>
<issue>4</issue>
<marker>Cilibrasi, Vit´anyi, 2005</marker>
<rawString>Rudi Cilibrasi and Paul Vit´anyi. 2005. Clustering by compression. IEEE Transactions on Information Theory, 51(4):1523–1545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John G Cleary</author>
<author>Ian H Witten</author>
</authors>
<title>Data compression using adaptive coding and partial string matching.</title>
<date>1984</date>
<journal>IEEE Transactions on Communications,</journal>
<pages>32--396</pages>
<contexts>
<context position="13834" citStr="Cleary and Witten, 1984" startWordPosition="2320" endWordPosition="2323">4). We thus modified Juola’s method as follows to make the length additive: [ = E loge IY| J0Y (X) . Leni(Y ) Although there is no mathematical guarantee that �JY (X) or �J0Y (X) actually converges to the crossentropy, our empirical tests showed a good estimate for both cases3. In this article, we use �J0Y (X) as a function to obtain the cross-entropy and for multiplication by |X |in formula (3). 3.2 PPM As a representative method for calculating the cross-entropy through statistical language modeling, we adopt prediction by partial matching (PPM), a language-based encoding method devised by (Cleary and Witten, 1984). It has the particular characteristic of using a variable n-gram length, unlike ordinary n-gram models4. It models the probability of a text X with a learning corpus Y as follows: PY (X) = PY (x1 ... x|X|) |X| =∏ PY (xt|xt−1 . . . xmax(1,t−n)), t=1 where n is a parameter of PPM, denoting the maximum length of the n-grams considered in the models. The probability PY (X) is estimated by escape probabilities favoring the longer sequences appearing in the learning corpus (Bell et al., 1990). The total code length of X is then estimated as − log PY (X). Since this value is additive and gives the t</context>
</contexts>
<marker>Cleary, Witten, 1984</marker>
<rawString>John G. Cleary and Ian H. Witten. 1984. Data compression using adaptive coding and partial string matching. IEEE Transactions on Communications, 32:396–402.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Farach</author>
<author>Michiel Noordewier</author>
<author>Serap Savari</author>
<author>Larry Shepp</author>
<author>Abraham J Wyner</author>
<author>Jacob Ziv</author>
</authors>
<title>On the entropy of dna: Algorithms and measurements based on memory and rapid convergence.</title>
<date>1994</date>
<booktitle>In Proceedings of the Sixth Annual ACM-SIAM Symposium on Discrete Algorithms,</booktitle>
<pages>48--57</pages>
<contexts>
<context position="11964" citStr="Farach et al., 1994" startWordPosition="1974" endWordPosition="1977"> et al., 2002) and (Cilibrasi and Vit´anyi, 2005) used the universal coding approach, whereas (Teahan and Harper, 2001) and (Sibun and Reynar, 1996) were based on language modeling using PPM and Kullback-Leibler divergence, respectively. In this section, we briefly introduce two methods previously studied by (Juola, 1997) and (Teahan, 2000) as representative of the two types, and we further explain a modification that we integrate into the final optimization problem. We tested several other coding methods, but they did not perform as well as these two methods. 3.1 Mean of Matching Statistics (Farach et al., 1994) proposed a method to estimate the entropy, through a simplified version of the LZ algorithm (Ziv and Lempel, 1977), as follows. Given a text X = x1x2 ... xixi+1 ..., Leni is defined as the longest match length for two substrings x1x2 ... xi and xi+1xi+2 .... In this article, we define the longest match for two strings A and B as the shortest prefix of string B that is not a substring of A. Letting the average of Leni be E [Len], Farach proved that |E [Len] − log2 i �(X) |probabilistically converges to zero as i → oo, where H(X) indicates the entropy of X. Then, H(X) is estimated as ˆH(X) = lo</context>
</contexts>
<marker>Farach, Noordewier, Savari, Shepp, Wyner, Ziv, 1994</marker>
<rawString>Martin Farach, Michiel Noordewier, Serap Savari, Larry Shepp, Abraham J. Wyner, and Jacob Ziv. 1994. On the entropy of dna: Algorithms and measurements based on memory and rapid convergence. In Proceedings of the Sixth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 48–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>Comparing two language identification schemes.</title>
<date>1995</date>
<booktitle>In Proceedings of 3rd International Conference on Statistical Analysis of Textual Data,</booktitle>
<pages>263--268</pages>
<contexts>
<context position="4737" citStr="Grefenstette, 1995" startWordPosition="755" endWordPosition="756">NLP context is segmentation into semantically coherent text portions, of which a representative method is text tiling as reported by (Hearst, 1997). There could be other possible bases for text 969 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 969–978, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics segmentation, and our study, in a way, could lead to generalizing the problem. The second genre is classification, and the specific problem of text classification by language has drawn substantial attention (Grefenstette, 1995) (Kruengkrai et al., 2005) (Kikui, 1996). Current state-of-the-art solutions use machine learning methods for languages with abundant supervision, and the performance is usually high enough for practical use. This article concerns that problem together with segmentation but has another particularity in aiming at classification into a substantial number of categories, i.e., more than 200 languages. This means that the amount of training data has to remain small, so the methods to be adopted must take this point into consideration. Among works on text classification into languages, our proposal </context>
</contexts>
<marker>Grefenstette, 1995</marker>
<rawString>Gregory Grefenstette. 1995. Comparing two language identification schemes. In Proceedings of 3rd International Conference on Statistical Analysis of Textual Data, pages 263–268.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Texttiling: Segmenting text into multi-paragraph subtopic passages.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>1</issue>
<contexts>
<context position="4265" citStr="Hearst, 1997" startWordPosition="685" endWordPosition="686">dded text are larger: up to the paragraph level to accommodate the reality of multilingual texts. The extension of our work to address the foreign word detection problem would be an interesting future work. From a broader view, the problem addressed in this paper is further related to two genres of previous work. The first genre is text segmentation. Our problem can be situated as a sub-problem from the viewpoint of language change. A more common setting in the NLP context is segmentation into semantically coherent text portions, of which a representative method is text tiling as reported by (Hearst, 1997). There could be other possible bases for text 969 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 969–978, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics segmentation, and our study, in a way, could lead to generalizing the problem. The second genre is classification, and the specific problem of text classification by language has drawn substantial attention (Grefenstette, 1995) (Kruengkrai et al., 2005) (Kikui, 1996). Current state-of-the-art solutions use machine learning methods for languages with abu</context>
</contexts>
<marker>Hearst, 1997</marker>
<rawString>Marti A. Hearst. 1997. Texttiling: Segmenting text into multi-paragraph subtopic passages. Computational Linguistics, 23(1):33–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Juola</author>
</authors>
<title>What can we do with small corpora? document categorization via cross-entropy.</title>
<date>1997</date>
<booktitle>In Proceedings of an Interdisciplinary Workshop on Similarity and Categorization.</booktitle>
<contexts>
<context position="5426" citStr="Juola, 1997" startWordPosition="862" endWordPosition="863">use machine learning methods for languages with abundant supervision, and the performance is usually high enough for practical use. This article concerns that problem together with segmentation but has another particularity in aiming at classification into a substantial number of categories, i.e., more than 200 languages. This means that the amount of training data has to remain small, so the methods to be adopted must take this point into consideration. Among works on text classification into languages, our proposal is based on previous studies using cross-entropy such as (Teahan, 2000) and (Juola, 1997). We explain these works in further detail in §3. This article presents one way to formulate the segmentation and identification problem as a combinatorial optimization problem; specifically, to find the set of segments and their languages that minimizes the description length of a given multilingual text. In the following, we describe the problem formulation and a solution to the problem, and then discuss the performance of our method. 2 Problem Formulation In our setting, we assume that a small amount (up to kilobytes) of monolingual plain text sample data is available for every language, e.</context>
<context position="11667" citStr="Juola, 1997" startWordPosition="1926" endWordPosition="1927">rm of (3), − log2 PLi(Xi), is the crossentropy of Xi for Li multiplied by |Xi|. Various methods for computing cross-entropy have been proposed, and these can be roughly classified into two types based on different methods of universal coding and the language model. For example, (Benedetto et al., 2002) and (Cilibrasi and Vit´anyi, 2005) used the universal coding approach, whereas (Teahan and Harper, 2001) and (Sibun and Reynar, 1996) were based on language modeling using PPM and Kullback-Leibler divergence, respectively. In this section, we briefly introduce two methods previously studied by (Juola, 1997) and (Teahan, 2000) as representative of the two types, and we further explain a modification that we integrate into the final optimization problem. We tested several other coding methods, but they did not perform as well as these two methods. 3.1 Mean of Matching Statistics (Farach et al., 1994) proposed a method to estimate the entropy, through a simplified version of the LZ algorithm (Ziv and Lempel, 1977), as follows. Given a text X = x1x2 ... xixi+1 ..., Leni is defined as the longest match length for two substrings x1x2 ... xi and xi+1xi+2 .... In this article, we define the longest matc</context>
</contexts>
<marker>Juola, 1997</marker>
<rawString>Patrick Juola. 1997. What can we do with small corpora? document categorization via cross-entropy. In Proceedings of an Interdisciplinary Workshop on Similarity and Categorization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gen-itiro Kikui</author>
</authors>
<title>Identifying the coding system and language of on-line documents on the internet.</title>
<date>1996</date>
<booktitle>In Proceedings of 16th International Conference on Computational Linguistics,</booktitle>
<pages>652--657</pages>
<contexts>
<context position="4777" citStr="Kikui, 1996" startWordPosition="761" endWordPosition="762">oherent text portions, of which a representative method is text tiling as reported by (Hearst, 1997). There could be other possible bases for text 969 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 969–978, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics segmentation, and our study, in a way, could lead to generalizing the problem. The second genre is classification, and the specific problem of text classification by language has drawn substantial attention (Grefenstette, 1995) (Kruengkrai et al., 2005) (Kikui, 1996). Current state-of-the-art solutions use machine learning methods for languages with abundant supervision, and the performance is usually high enough for practical use. This article concerns that problem together with segmentation but has another particularity in aiming at classification into a substantial number of categories, i.e., more than 200 languages. This means that the amount of training data has to remain small, so the methods to be adopted must take this point into consideration. Among works on text classification into languages, our proposal is based on previous studies using cross</context>
</contexts>
<marker>Kikui, 1996</marker>
<rawString>Gen-itiro Kikui. 1996. Identifying the coding system and language of on-line documents on the internet. In Proceedings of 16th International Conference on Computational Linguistics, pages 652–657.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Casanai Kruengkrai</author>
<author>Prapass Srichaivattana</author>
<author>Virach Sornlertlamvanich</author>
<author>Hitoshi Isahara</author>
</authors>
<title>Language identification based on string kernels.</title>
<date>2005</date>
<booktitle>In Proceedings of the 5th International Symposium on Communications and Information Technologies,</booktitle>
<pages>926--929</pages>
<contexts>
<context position="4763" citStr="Kruengkrai et al., 2005" startWordPosition="757" endWordPosition="760">tation into semantically coherent text portions, of which a representative method is text tiling as reported by (Hearst, 1997). There could be other possible bases for text 969 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 969–978, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics segmentation, and our study, in a way, could lead to generalizing the problem. The second genre is classification, and the specific problem of text classification by language has drawn substantial attention (Grefenstette, 1995) (Kruengkrai et al., 2005) (Kikui, 1996). Current state-of-the-art solutions use machine learning methods for languages with abundant supervision, and the performance is usually high enough for practical use. This article concerns that problem together with segmentation but has another particularity in aiming at classification into a substantial number of categories, i.e., more than 200 languages. This means that the amount of training data has to remain small, so the methods to be adopted must take this point into consideration. Among works on text classification into languages, our proposal is based on previous studi</context>
</contexts>
<marker>Kruengkrai, Srichaivattana, Sornlertlamvanich, Isahara, 2005</marker>
<rawString>Casanai Kruengkrai, Prapass Srichaivattana, Virach Sornlertlamvanich, and Hitoshi Isahara. 2005. Language identification based on string kernels. In Proceedings of the 5th International Symposium on Communications and Information Technologies, pages 926–929.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Penelope Sibun</author>
<author>Jeffrey C Reynar</author>
</authors>
<title>Language identification: Examining the issues.</title>
<date>1996</date>
<booktitle>In Proceedings of 5th Symposium on Document Analysis and Information Retrieval,</booktitle>
<pages>125--135</pages>
<contexts>
<context position="11492" citStr="Sibun and Reynar, 1996" startWordPosition="1899" endWordPosition="1902"> introducing methods to calculate the first term of formula (3), we explain the solution to optimize the combinatorial problem of formula (1). 3 Calculation of Cross-Entropy The first term of (3), − log2 PLi(Xi), is the crossentropy of Xi for Li multiplied by |Xi|. Various methods for computing cross-entropy have been proposed, and these can be roughly classified into two types based on different methods of universal coding and the language model. For example, (Benedetto et al., 2002) and (Cilibrasi and Vit´anyi, 2005) used the universal coding approach, whereas (Teahan and Harper, 2001) and (Sibun and Reynar, 1996) were based on language modeling using PPM and Kullback-Leibler divergence, respectively. In this section, we briefly introduce two methods previously studied by (Juola, 1997) and (Teahan, 2000) as representative of the two types, and we further explain a modification that we integrate into the final optimization problem. We tested several other coding methods, but they did not perform as well as these two methods. 3.1 Mean of Matching Statistics (Farach et al., 1994) proposed a method to estimate the entropy, through a simplified version of the LZ algorithm (Ziv and Lempel, 1977), as follows.</context>
</contexts>
<marker>Sibun, Reynar, 1996</marker>
<rawString>Penelope Sibun and Jeffrey C. Reynar. 1996. Language identification: Examining the issues. In Proceedings of 5th Symposium on Document Analysis and Information Retrieval, pages 125–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William J Teahan</author>
<author>David J Harper</author>
</authors>
<title>Using compression-based language models for text categorization.</title>
<date>2001</date>
<booktitle>In Proceedings of the Workshop on Language Modeling and Information Retrieval,</booktitle>
<pages>83--88</pages>
<contexts>
<context position="11463" citStr="Teahan and Harper, 2001" startWordPosition="1894" endWordPosition="1897">mposition. Next, after briefly introducing methods to calculate the first term of formula (3), we explain the solution to optimize the combinatorial problem of formula (1). 3 Calculation of Cross-Entropy The first term of (3), − log2 PLi(Xi), is the crossentropy of Xi for Li multiplied by |Xi|. Various methods for computing cross-entropy have been proposed, and these can be roughly classified into two types based on different methods of universal coding and the language model. For example, (Benedetto et al., 2002) and (Cilibrasi and Vit´anyi, 2005) used the universal coding approach, whereas (Teahan and Harper, 2001) and (Sibun and Reynar, 1996) were based on language modeling using PPM and Kullback-Leibler divergence, respectively. In this section, we briefly introduce two methods previously studied by (Juola, 1997) and (Teahan, 2000) as representative of the two types, and we further explain a modification that we integrate into the final optimization problem. We tested several other coding methods, but they did not perform as well as these two methods. 3.1 Mean of Matching Statistics (Farach et al., 1994) proposed a method to estimate the entropy, through a simplified version of the LZ algorithm (Ziv a</context>
</contexts>
<marker>Teahan, Harper, 2001</marker>
<rawString>William J. Teahan and David J. Harper. 2001. Using compression-based language models for text categorization. In Proceedings of the Workshop on Language Modeling and Information Retrieval, pages 83–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William John Teahan</author>
</authors>
<title>Text classification and segmentation using minimum cross-entropy.</title>
<date>2000</date>
<booktitle>In RIAO,</booktitle>
<pages>943--961</pages>
<contexts>
<context position="2391" citStr="Teahan, 2000" startWordPosition="373" endWordPosition="374">rtions written in major languages, as noted above. The study reported here was motivated by this objective. The problem addressed in this article is thus to segment a multilingual text by language and identify the language of each segment. In addition, for our objective, the set of target languages consists of not only major languages but also many non-major languages: more than 200 languages in total. Previous work that directly concerns the problem addressed in this paper is rare. The most similar previous work that we know of comes from two sources and can be summarized as follows. First, (Teahan, 2000) attempted to segment multilingual texts by using text segmentation methods used for non-segmented languages. For this purpose, he used a gold standard of multilingual texts annotated by borders and languages. This segmentation approach is similar to that of word segmentation for nonsegmented texts, and he tested it on six different European languages. Although the problem setting is similar to ours, the formulation and solution are different, particularly in that our method uses only a monolingual gold standard, not a multilingual one as in Teahan’s study. Second, (Alex, 2005) (Alex et al., 2</context>
<context position="5408" citStr="Teahan, 2000" startWordPosition="859" endWordPosition="860">-the-art solutions use machine learning methods for languages with abundant supervision, and the performance is usually high enough for practical use. This article concerns that problem together with segmentation but has another particularity in aiming at classification into a substantial number of categories, i.e., more than 200 languages. This means that the amount of training data has to remain small, so the methods to be adopted must take this point into consideration. Among works on text classification into languages, our proposal is based on previous studies using cross-entropy such as (Teahan, 2000) and (Juola, 1997). We explain these works in further detail in §3. This article presents one way to formulate the segmentation and identification problem as a combinatorial optimization problem; specifically, to find the set of segments and their languages that minimizes the description length of a given multilingual text. In the following, we describe the problem formulation and a solution to the problem, and then discuss the performance of our method. 2 Problem Formulation In our setting, we assume that a small amount (up to kilobytes) of monolingual plain text sample data is available for </context>
<context position="11686" citStr="Teahan, 2000" startWordPosition="1929" endWordPosition="1930">PLi(Xi), is the crossentropy of Xi for Li multiplied by |Xi|. Various methods for computing cross-entropy have been proposed, and these can be roughly classified into two types based on different methods of universal coding and the language model. For example, (Benedetto et al., 2002) and (Cilibrasi and Vit´anyi, 2005) used the universal coding approach, whereas (Teahan and Harper, 2001) and (Sibun and Reynar, 1996) were based on language modeling using PPM and Kullback-Leibler divergence, respectively. In this section, we briefly introduce two methods previously studied by (Juola, 1997) and (Teahan, 2000) as representative of the two types, and we further explain a modification that we integrate into the final optimization problem. We tested several other coding methods, but they did not perform as well as these two methods. 3.1 Mean of Matching Statistics (Farach et al., 1994) proposed a method to estimate the entropy, through a simplified version of the LZ algorithm (Ziv and Lempel, 1977), as follows. Given a text X = x1x2 ... xixi+1 ..., Leni is defined as the longest match length for two substrings x1x2 ... xi and xi+1xi+2 .... In this article, we define the longest match for two strings A</context>
<context position="20122" citStr="Teahan, 2000" startWordPosition="3402" endWordPosition="3403">red in block form for some writing systems. To evaluate language identification for monolingual texts, as will be reported in §6.1, we conducted five-times cross-validation separately for both data sets. We present the results in terms of the average accuracy AL, the ratio of the number of texts with a correctly identified language to |L|. 5.2 Multilingual Texts (Test Data) Multilingual texts were needed only to test the performance of the proposed method. In other words, we trained the model only through monolingual data, as mentioned above. This differs from the most similar previous study (Teahan, 2000), which required multilingual learning data. The multilingual texts were generated artificially, since multilingual texts taken directly from the web have other issues besides segmentation. First, proper nouns in multilingual texts complicate the final judgment of language and segment borders. In practical application, therefore, texts for segmentation must be preprocessed by named entity recognition, which is beyond the scope of this work. Second, the sizes of text portions in multilingual web texts differ greatly, which would make it difficult to evaluate the overall performance of the propo</context>
</contexts>
<marker>Teahan, 2000</marker>
<rawString>William John Teahan. 2000. Text classification and segmentation using minimum cross-entropy. In RIAO, pages 943–961.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Ziv</author>
<author>Abraham Lempel</author>
</authors>
<title>A universal algorithm for sequential data compression.</title>
<date>1977</date>
<journal>IEEE Transactions on Information Theory,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="12079" citStr="Ziv and Lempel, 1977" startWordPosition="1994" endWordPosition="1997">2001) and (Sibun and Reynar, 1996) were based on language modeling using PPM and Kullback-Leibler divergence, respectively. In this section, we briefly introduce two methods previously studied by (Juola, 1997) and (Teahan, 2000) as representative of the two types, and we further explain a modification that we integrate into the final optimization problem. We tested several other coding methods, but they did not perform as well as these two methods. 3.1 Mean of Matching Statistics (Farach et al., 1994) proposed a method to estimate the entropy, through a simplified version of the LZ algorithm (Ziv and Lempel, 1977), as follows. Given a text X = x1x2 ... xixi+1 ..., Leni is defined as the longest match length for two substrings x1x2 ... xi and xi+1xi+2 .... In this article, we define the longest match for two strings A and B as the shortest prefix of string B that is not a substring of A. Letting the average of Leni be E [Len], Farach proved that |E [Len] − log2 i �(X) |probabilistically converges to zero as i → oo, where H(X) indicates the entropy of X. Then, H(X) is estimated as ˆH(X) = log2 i E [Len]. (Juola, 1997) applied this method to estimate the cross-entropy of two given texts. For two strings Y</context>
</contexts>
<marker>Ziv, Lempel, 1977</marker>
<rawString>Jacob Ziv and Abraham Lempel. 1977. A universal algorithm for sequential data compression. IEEE Transactions on Information Theory, 23(3):337–343.</rawString>
</citation>
<citation valid="false">
<title>This Appendix lists all the languages contained in our data sets, as summarized in Table 1.</title>
<marker></marker>
<rawString>This Appendix lists all the languages contained in our data sets, as summarized in Table 1.</rawString>
</citation>
<citation valid="false">
<institution>For UDHR Latin</institution>
<marker></marker>
<rawString>For UDHR Latin</rawString>
</citation>
<citation valid="false">
<authors>
<author>Achuar-Shiwiar Achinese</author>
<author>Afrikaans Adangme</author>
<author>Aja Aguaruna</author>
<author>Akuapem Akan</author>
<author>Amahuaca Akurio</author>
<author>AmboPasco Quechua Amarakaeri</author>
<author>Arequipa-La Uni´on Quechua Arabela</author>
<author>Asante Akan Arpitan</author>
<author>Ash´eninka Pajonal Ash´aninka</author>
<author>Auvergnat Occitan Asturian</author>
<author>Ayacucho Quechua</author>
<author>Baatonum Aymara</author>
<author>Bambara Balinese</author>
<author>Basque Baoul´e</author>
<author>Beti Bemba</author>
<author>Bini Bikol</author>
<author>Bokm˚al Norwegian Bislama</author>
<author>Bosnian Bora</author>
<author>Breton</author>
</authors>
<title>Buginese, Cajamarca Quechua, Calder´on Highland Quichua, CandoshiShapra, Caquinte, Cashibo-Cacataibo, Cashinahua, Catalan, Cebuano, Central Kanuri, Central Mazahua, Central Nahuatl, Chamorro, Chamula Tzotzil,</title>
<location>Chayahuita, Chickasaw, Chiga, Chokwe, Chuanqiandian Cluster Miao, Chuukese, Corsican, Cusco Quechua, Czech, Dagbani, Danish, Dendi, Ditammari, Dutch, Eastern Maninkakan, Emiliano-Romagnolo, English, Esperanto, Estonian, Ewe, Falam Chin, Fanti, Faroese, Fijian, Filipino, Finnish, Fon, French, Friulian, Ga, Gagauz, Galician, Ganda, Garifuna, Gen, German, Gheg Albanian, Gonja, Guarani, G¨uil´a</location>
<marker>Achinese, Adangme, Aguaruna, Akan, Akurio, Amarakaeri, Arabela, Arpitan, Ash´aninka, Asturian, Quechua, Aymara, Balinese, Baoul´e, Bemba, Bikol, Bislama, Bora, Breton, </marker>
<rawString>Achinese, Achuar-Shiwiar, Adangme, Afrikaans, Aguaruna, Aja, Akuapem Akan, Akurio, Amahuaca, Amarakaeri, AmboPasco Quechua, Arabela, Arequipa-La Uni´on Quechua, Arpitan, Asante Akan, Ash´aninka, Ash´eninka Pajonal, Asturian, Auvergnat Occitan, Ayacucho Quechua, Aymara, Baatonum, Balinese, Bambara, Baoul´e, Basque, Bemba, Beti, Bikol, Bini, Bislama, Bokm˚al Norwegian, Bora, Bosnian, Breton, Buginese, Cajamarca Quechua, Calder´on Highland Quichua, CandoshiShapra, Caquinte, Cashibo-Cacataibo, Cashinahua, Catalan, Cebuano, Central Kanuri, Central Mazahua, Central Nahuatl, Chamorro, Chamula Tzotzil, Chayahuita, Chickasaw, Chiga, Chokwe, Chuanqiandian Cluster Miao, Chuukese, Corsican, Cusco Quechua, Czech, Dagbani, Danish, Dendi, Ditammari, Dutch, Eastern Maninkakan, Emiliano-Romagnolo, English, Esperanto, Estonian, Ewe, Falam Chin, Fanti, Faroese, Fijian, Filipino, Finnish, Fon, French, Friulian, Ga, Gagauz, Galician, Ganda, Garifuna, Gen, German, Gheg Albanian, Gonja, Guarani, G¨uil´a Zapotec, Haitian Creole, Haitian Creole (popular), Haka Chin, Hani, Hausa, Hawaiian, Hiligaynon, Huamal´ıes-Dos de Mayo Hu´anuco Quechua, Huautla Mazatec, Huaylas Ancash Quechua, Hungarian, Ibibio, Icelandic, Ido, Igbo, Iloko, Indonesian, Interlingua, Irish, Italian, Javanese, Jola-Fonyi, K’iche’, Kabiy`e, Kabuverdianu, Kalaallisut, Kaonde, Kaqchikel, Kasem, Kekchi, Kimbundu, Kinyarwanda, Kituba, Konzo, Kpelle, Krio, Kurdish, Lamnso’, Languedocien Occitan, Latin, Latvian, Lingala, Lithuanian, Lozi, Luba-Lulua, Lunda, Luvale, Luxembourgish, Madurese, Makhuwa, Makonde, Malagasy, Maltese, Mam, Maori, Mapudungun, Margos-Yarowilca-Lauricocha Quechua, Marshallese, Mba, Mende, Metlat´onoc Mixtec, Mezquital Otomi, Mi’kmaq, Miahuatl´an Zapotec, Minangkabau, Mossi, Mozarabic, Murui Huitoto, Miskito, Ndonga, Nigerian Pidgin, Nomatsiguenga, North Junin Quechua, Northeastern Dinka, Northern Conchucos Ancash Quechua, Northern Qiandong Miao, Northern Sami, Northern Kurdish, Nyamwezi, Nyanja, Nyemba, Nynorsk Norwegian, Nzima, Ojitl´aan Chinantec, Oromo, Palauan, Pampanga, Papantla Totonac, Pedi, Picard, Pichis Ash´eninka, Pijin, Pipil, Pohnpeian, Polish, Portuguese, Pulaar, Purepecha, P´aez, Quechua, Rarotongan, Romanian, Romansh, Romany, Rundi, Salinan, Samoan, San Luis PotosiHuastec, Sango, Sardinian, Scots, Scottish Gaelic, Serbian, Serer, Seselwa Creole French, Sharanahua, Shipibo-Conibo, Shona, Slovak, Somali, Soninke, South Ndebele, Southern Dagaare, Southern Qiandong Miao, Southern Sotho, Spanish, Standard Malay, Sukuma, Sundanese, Susu, Swahili, Swati, Swedish, S˜aotomense, Tahitian, Tedim Chin, Tetum, Tidikelt Tamazight, Timne, Tiv, Toba, Tojolabal, Tok Pisin, Tonga (Tonga Islands), Tonga (Zambia), Tsonga, Tswana, Turkish, Tzeltal, Umbundu, Upper Sorbian, Urarina, Uzbek, Veracruz Huastec, Vili, Vlax Romani, Walloon, Waray, Wayuu, Welsh, Western Frisian, Wolof, Xhosa, Yagua, Yanesha’, Yao, Yapese, Yoruba, Yucateco, Zhuang, Zulu</rawString>
</citation>
<citation valid="false">
<authors>
<author>Cyrillic Abkhazian</author>
<author>Belarusian</author>
</authors>
<location>Bosnian, Bulgarian, Kazakh, Macedonian, Ossetian, Russian, Serbian, Tuvinian, Ukrainian, Yakut</location>
<marker>Abkhazian, Belarusian, </marker>
<rawString>Cyrillic Abkhazian, Belarusian, Bosnian, Bulgarian, Kazakh, Macedonian, Ossetian, Russian, Serbian, Tuvinian, Ukrainian, Yakut</rawString>
</citation>
<citation valid="false">
<authors>
<author>Korean Japanese</author>
</authors>
<institution>Mandarin Chinese, Modern Greek For Wiki Latin</institution>
<marker>Japanese, </marker>
<rawString>Japanese, Korean, Mandarin Chinese, Modern Greek For Wiki Latin</rawString>
</citation>
<citation valid="false">
<authors>
<author>Albanian Afrikaans</author>
<author>Aromanian Aragonese</author>
<author>Asturian Arpitan</author>
<author>Azerbaijani Aymara</author>
<author>Banyumasan Bambara</author>
<author>Bavarian Basque</author>
<author>Bosnian Bislama</author>
<author>Catal`a Breton</author>
<author>Central Bikol Cebuano</author>
<author>Cornish Chavacano</author>
<author>Crimean Tatar Corsican</author>
<author>Czech Croatian</author>
<author>Dimli Danish</author>
<author>Dutch Low Saxon Dutch</author>
</authors>
<title>Islands), Tosk Albanian,</title>
<date></date>
<location>EmilianoRomagnolo, English, Esperanto, Estonian, Ewe, Extremaduran, Faroese, Fiji Hindi, Finnish, French, Friulian, Galician, German, Gilaki, Gothic, Guarani, Hai//om, Haitian, Hakka Chinese, Hawaiian, Hungarian, Icelandic, Ido, Igbo, Iloko, Indonesian, Interlingua, Interlingue, Irish, Italian, Javanese, Kabyle, Kalaallisut, Kara-Kalpak, Kashmiri, Kashubian, Kongo, Korean, Kurdish, Ladino, Latin, Latvian, Ligurian, Limburgan, Lingala, Lithuanian, Lojban, Lombard, Low German, Lower Sorbian, Luxembourgish, Malagasy, Malay, Maltese, Manx, Maori, Mazanderani, Min Dong Chinese, Min Nan Chinese, Nahuatl, Narom, Navajo, Neapolitan, Northern Sami, Norwegian, Norwegian Nynorsk, Novial, Occitan, Old English, Pampanga, Pangasinan, Panjabi, Papiamento, Pennsylvania German, Piemontese, Pitcairn-Norfolk, Polish, Portuguese, Pushto, Quechua, Romanian, Romansh, Samoan, Samogitian Lithuanian, Sardinian, Saterfriesisch, Scots, Scottish Gaelic, SerboCroatian, Sicilian, Silesian, Slovak, Slovenian, Somali, Spanish, Sranan Tongo, Sundanese, Swahili, Swati, Swedish, Tagalog, Tahitian, Tarantino Sicilian, Tatar, Tetum, Tok Pisin, Tonga</location>
<marker>Afrikaans, Aragonese, Arpitan, Aymara, Bambara, Basque, Bislama, Breton, Cebuano, Chavacano, Corsican, Croatian, Danish, Dutch, </marker>
<rawString>Afrikaans, Albanian, Aragonese, Aromanian, Arpitan, Asturian, Aymara, Azerbaijani, Bambara, Banyumasan, Basque, Bavarian, Bislama, Bosnian, Breton, Catal`a, Cebuano, Central Bikol, Chavacano, Cornish, Corsican, Crimean Tatar, Croatian, Czech, Danish, Dimli, Dutch, Dutch Low Saxon, EmilianoRomagnolo, English, Esperanto, Estonian, Ewe, Extremaduran, Faroese, Fiji Hindi, Finnish, French, Friulian, Galician, German, Gilaki, Gothic, Guarani, Hai//om, Haitian, Hakka Chinese, Hawaiian, Hungarian, Icelandic, Ido, Igbo, Iloko, Indonesian, Interlingua, Interlingue, Irish, Italian, Javanese, Kabyle, Kalaallisut, Kara-Kalpak, Kashmiri, Kashubian, Kongo, Korean, Kurdish, Ladino, Latin, Latvian, Ligurian, Limburgan, Lingala, Lithuanian, Lojban, Lombard, Low German, Lower Sorbian, Luxembourgish, Malagasy, Malay, Maltese, Manx, Maori, Mazanderani, Min Dong Chinese, Min Nan Chinese, Nahuatl, Narom, Navajo, Neapolitan, Northern Sami, Norwegian, Norwegian Nynorsk, Novial, Occitan, Old English, Pampanga, Pangasinan, Panjabi, Papiamento, Pennsylvania German, Piemontese, Pitcairn-Norfolk, Polish, Portuguese, Pushto, Quechua, Romanian, Romansh, Samoan, Samogitian Lithuanian, Sardinian, Saterfriesisch, Scots, Scottish Gaelic, SerboCroatian, Sicilian, Silesian, Slovak, Slovenian, Somali, Spanish, Sranan Tongo, Sundanese, Swahili, Swati, Swedish, Tagalog, Tahitian, Tarantino Sicilian, Tatar, Tetum, Tok Pisin, Tonga (Tonga Islands), Tosk Albanian, Tsonga, Tswana, Turkish, Turkmen, Uighur, Upper Sorbian, Uzbek, Venda, Venetian, Vietnamese, Vlaams, Vlax Romani, Volap¨uk, V˜oro, Walloon, Waray, Welsh, Western Frisian, Wolof, Yoruba, Zeeuws, Zulu Cyrillic</rawString>
</citation>
<citation valid="false">
<authors>
<author>Bashkir Abkhazian</author>
<author>Bulgarian Belarusian</author>
<author>Chuvash</author>
</authors>
<location>Erzya, Kazakh, Kirghiz, Macedonian, Moksha, Moldovan, Mongolian, Old Belarusian, Ossetian, Russian, Serbian, Tajik, Udmurt, Ukrainian, Yakut Arabic, Egyptian Arabic, Gilaki, Mazanderani, Persian, Pushto, Uighur, Urdu</location>
<marker>Abkhazian, Belarusian, Chuvash, </marker>
<rawString>Abkhazian, Bashkir, Belarusian, Bulgarian, Chuvash, Erzya, Kazakh, Kirghiz, Macedonian, Moksha, Moldovan, Mongolian, Old Belarusian, Ossetian, Russian, Serbian, Tajik, Udmurt, Ukrainian, Yakut Arabic, Egyptian Arabic, Gilaki, Mazanderani, Persian, Pushto, Uighur, Urdu</rawString>
</citation>
<citation valid="false">
<authors>
<author>Hindi Bihari</author>
</authors>
<location>Marathi, Nepali, Newari, Sanskrit Other</location>
<marker>Bihari, </marker>
<rawString>Bihari, Hindi, Marathi, Nepali, Newari, Sanskrit Other</rawString>
</citation>
<citation valid="false">
<authors>
<author>Armenian Amharic</author>
<author>Bengali Assamese</author>
<author>Bishnupriya</author>
</authors>
<title>Burmese, Central Khmer, Chinese, Classical Chinese, Dhivehi,</title>
<location>Gan Chinese, Georgian, Gothic, Gujarati, Hebrew, Japanese, Kannada, Lao, Malayalam, Modern Greek, Official Aramaic, Panjabi, Sinhala, Tamil, Telugu, Thai, Tibetan, Wu Chinese, Yiddish, Yue Chinese</location>
<marker>Amharic, Assamese, Bishnupriya, </marker>
<rawString>Amharic, Armenian, Assamese, Bengali, Bishnupriya, Burmese, Central Khmer, Chinese, Classical Chinese, Dhivehi, Gan Chinese, Georgian, Gothic, Gujarati, Hebrew, Japanese, Kannada, Lao, Malayalam, Modern Greek, Official Aramaic, Panjabi, Sinhala, Tamil, Telugu, Thai, Tibetan, Wu Chinese, Yiddish, Yue Chinese</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>