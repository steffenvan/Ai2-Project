<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.032742">
<title confidence="0.9954845">
ICSI-CRF: The Generation of References to the Main Subject and Named
Entities using Conditional Random Fields
</title>
<author confidence="0.986437">
Benoit Favre and Bernd Bohnet
</author>
<affiliation confidence="0.97684">
International Computer Science Institute
</affiliation>
<address confidence="0.886353">
1947 Center Street. Suite 600
Berkeley, CA 94704, USA
</address>
<email confidence="0.999822">
{favre|bohnet}@icsi.berkeley.edu
</email>
<sectionHeader confidence="0.997405" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999582125">
In this paper, we describe our contribution to
the Generation Challenge 2009 for the tasks of
generating Referring Expressions to the Main
Subject References (MSR) and Named Enti-
ties Generation (NEG). To generate the refer-
ring expressions, we employ the Conditional
Random Fields (CRF) learning technique due
to the fact that the selection of an expres-
sion depends on the selection of the previ-
ous references. CRFs fit very well to this
task since they are designed for the labeling
of sequences. For the MSR task, our system
has a String Accuracy of 0.68 and a REG08-
Type Accuracy of 0.76 and for the NEG task a
String Accuracy of 0.79 and REG08-Type Ac-
curacy of 0.83.
</bodyText>
<sectionHeader confidence="0.998546" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999624294117647">
The GREC Generation Challenge 2009 consists of
two tasks. The first task is to generate appropriate
references to an entity due to a given context which
is longer than a sentence. In the GREC-MSR task,
data sets are provided of possible referring expres-
sions which have to be selected. In the first shared
task on same topic (Belz and Varges, 2007), the main
task was to select the referring expression type cor-
rectly. In the GREC-MSR 2009 task, the main task
is to select the actual word string correctly, and the
main evaluation criterion is String Accuracy.
The GREC-NEG task is about the generation of
references to all person entities in a context longer
than a sentence. The NEG data also provides sets of
possible referring expressions to each entity (“he”),
groups of multiple entities (“they”) and nested refer-
ences (“his father”).
</bodyText>
<sectionHeader confidence="0.985004" genericHeader="method">
2 System Description
</sectionHeader>
<bodyText confidence="0.998834">
Our approach relies in mapping each input expres-
sion for a given reference to a class label. We use
the attributes of the REFEX tags as basic labels so
that, for instance, a REFEX with attributes REG08-
TYPE=“pronoun” CASE=“nominative” is mapped
to the label “nominative pronoun”. In order to de-
crease the number of potential textual units for a pre-
dicted label, we derive extra label information from
the text itself. For instance a qualifier “first name”
or “family name” is added to the expressions rela-
tive to a person. Similarly, types of pronouns (he,
him, his, who, whose, whom, emphasis) are speci-
fied in the class label, which is very useful for the
NEG task. Only the person labels have been refined
this way. While we experimented with a few ap-
proaches to remove the remaining ambiguity (same
label for different text), they generally did not per-
form better than a random selection. We opted for a
deterministic generation with the last element in the
list of possibilities given a class label.
For prediction of attributes, our system uses Con-
ditional Random Fields, as proposed by (Lafferty et
al., 2001). We use chain CRFs to estimate the prob-
ability of a sequence of labels (Y = Y1 ... Yn) given
a sequence of observations (X = X1 ... Xm).
</bodyText>
<equation confidence="0.995093">
⎛ ⎞
m
P(Y |X) ∝ exp λi fi(Yj−1,Yj, X) (1)
</equation>
<bodyText confidence="0.4976675">
Here, fi(·) are decision functions that depend on
n
</bodyText>
<page confidence="0.964257">
99
</page>
<note confidence="0.795527">
Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages
Suntec, Singapore, 6 August 2009.
ACL an
</note>
<page confidence="0.6860035">
99–100,
c�2009
</page>
<tableCaption confidence="0.439823">
d AFNLP
</tableCaption>
<table confidence="0.999968666666667">
Evaluation Metric R1 R2 MSR S2R S2O R1 R2 NEG S2R S2O
S1 S2 S1 S2
REG08 Type Accuracy 0.36 1.00 0.74 0.75 0.75 0.75 0.40 1.00 0.83 0.83 0.83 0.83
String Accuracy 0.12 0.82 0.62 0.67 0.66 0.75 0.12 0.70 0.52 0.79 0.79 0.80
Mean Edit Distance 2.52 0.31 0.95 0.85 0.87 0.72 2.38 0.61 1.07 0.53 0.52 0.49
Mean Norm. Edit Dist. 0.79 0.09 0.31 0.28 0.28 0.24 0.84 0.22 0.43 0.19 0.20 0.19
BLEU 1 0.19 0.88 0.65 0.69 0.68 0.74 0.17 0.79 0.64 0.81 0.81 0.83
BLEU 2 0.14 0.76 0.55 0.60 0.59 0.71 0.18 0.75 0.69 0.83 0.83 0.85
BLEU 3 0.10 0.69 0.51 0.56 0.55 0.70 0.18 0.73 0.71 0.83 0.84 0.86
</table>
<tableCaption confidence="0.995043">
Table 1: Results for the GREC MSR and NEG tasks. Are displayed: a random&apos; output (R1), a random output when
the attributes are guessed correctly (R2), the CRF system predicting basic attributes (S1), the CRF system predicting
refined attributes (S2), CRF-predicted attributes with random selection of text (S2R) and CRF-predicted attributes with
oracle selection of text (S2O).
</tableCaption>
<bodyText confidence="0.999287">
the examples and a clique of boundaries close to Yj,
and λi is the weight of fi estimated on training data.
For our experiments, we use the CRF++ toolkit,1
which allows binary decision functions dependent
on the current label and the previous label.
All features are used for both MSR and NEG
tasks, where applicable:
</bodyText>
<listItem confidence="0.999899857142857">
• word unigram and bigram before and after the
reference
• morphology of the previous and next words (-
ed, -ing, -s)
• punctuation type, before and after (comma,
parenthesis, period, nothing)
• SYNFUNC, SYNCAT and SEMCAT
• whether or not the previous reference is about
the same entity as the current one
• number of occurrence of the entity since the be-
ginning of the text (quantized 1,2,3,4+)
• number of occurrence of the entity since the
last change of entity (quantized)
• beginning of paragraph indicator
</listItem>
<bodyText confidence="0.9997976">
In the MSR case, this list is augmented with the fea-
tures of the two previous references. In the NEG
case, we use the features of the previous reference
and those of the previous occurrence of the same en-
tity.
</bodyText>
<footnote confidence="0.950629">
1http://crfpp.sourceforge.net/
</footnote>
<sectionHeader confidence="0.99938" genericHeader="evaluation">
3 Results and Conclusion
</sectionHeader>
<bodyText confidence="0.999963238095238">
Table 1 shows the results for the GREC MSR and
NEG tasks.2 We observe that for both tasks, our sys-
tem exceeds the performance of a random3 selection
(columns R1 vs. S2). In the MSR task, guessing
correctly the attributes seems more important than
in the NEG task, as suggested by the difference in
string accuracy when randomly selecting the refer-
ences with the right attributes (columns R2). Gener-
ating more specific attributes from the text is espe-
cially important for the NEG task (columns S1 vs.
S2). This was expected because we only refined
the attributes for person entities. We also observe
that a deterministic disambiguation of the references
with the same attributes is not distinguishable from
a random selection (columns S2 vs. S2R). However
it seems that selecting the right text, as in the ora-
cle experiment, would hardly help in the NEG task
while the gap is larger for the MSR task. This shows
that refined classes work well for person entities but
more refinements are needed for other types (city,
mountain, river...).
</bodyText>
<sectionHeader confidence="0.999477" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998250875">
J. Lafferty, A. McCallum, and F. Pereira. 2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data. In Proc. of
ICML, pages 282-289
A. Belz and S. Varges. 2007 Generation of Repeated
References to Discourse Entities. In Proceedings of
the 11th European Workshop on Natural Language
Generation (ENLG07), pages 9-16.
</reference>
<footnote confidence="0.903403333333333">
2Our system is available http://www.icsi.berkeley.edu/
∼favre/grec/
3All random experiments are averaged over 100 runs.
</footnote>
<page confidence="0.969295">
100
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.646131">
<title confidence="0.9818445">ICSI-CRF: The Generation of References to the Main Subject and Entities using Conditional Random Fields</title>
<author confidence="0.91853">Favre</author>
<affiliation confidence="0.998802">International Computer Science</affiliation>
<address confidence="0.9951805">1947 Center Street. Suite Berkeley, CA 94704,</address>
<abstract confidence="0.976018235294118">In this paper, we describe our contribution to the Generation Challenge 2009 for the tasks of generating Referring Expressions to the Main Subject References (MSR) and Named Entities Generation (NEG). To generate the referring expressions, we employ the Conditional Random Fields (CRF) learning technique due to the fact that the selection of an expression depends on the selection of the previous references. CRFs fit very well to this task since they are designed for the labeling of sequences. For the MSR task, our system has a String Accuracy of 0.68 and a REG08- Type Accuracy of 0.76 and for the NEG task a String Accuracy of 0.79 and REG08-Type Accuracy of 0.83.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data.</title>
<date>2001</date>
<booktitle>In Proc. of ICML,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="2942" citStr="Lafferty et al., 2001" startWordPosition="487" endWordPosition="490">ssions relative to a person. Similarly, types of pronouns (he, him, his, who, whose, whom, emphasis) are specified in the class label, which is very useful for the NEG task. Only the person labels have been refined this way. While we experimented with a few approaches to remove the remaining ambiguity (same label for different text), they generally did not perform better than a random selection. We opted for a deterministic generation with the last element in the list of possibilities given a class label. For prediction of attributes, our system uses Conditional Random Fields, as proposed by (Lafferty et al., 2001). We use chain CRFs to estimate the probability of a sequence of labels (Y = Y1 ... Yn) given a sequence of observations (X = X1 ... Xm). ⎛ ⎞ m P(Y |X) ∝ exp λi fi(Yj−1,Yj, X) (1) Here, fi(·) are decision functions that depend on n 99 Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages Suntec, Singapore, 6 August 2009. ACL an 99–100, c�2009 d AFNLP Evaluation Metric R1 R2 MSR S2R S2O R1 R2 NEG S2R S2O S1 S2 S1 S2 REG08 Type Accuracy 0.36 1.00 0.74 0.75 0.75 0.75 0.40 1.00 0.83 0.83 0.83 0.83 String Accuracy 0.12 0.82 0.62 0.67 0.66 0.75 0.12 0.70 0</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. In Proc. of ICML, pages 282-289</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Belz</author>
<author>S Varges</author>
</authors>
<title>Generation of Repeated References to Discourse Entities.</title>
<date>2007</date>
<booktitle>In Proceedings of the 11th European Workshop on Natural Language Generation (ENLG07),</booktitle>
<pages>9--16</pages>
<contexts>
<context position="1311" citStr="Belz and Varges, 2007" startWordPosition="212" endWordPosition="215">Fs fit very well to this task since they are designed for the labeling of sequences. For the MSR task, our system has a String Accuracy of 0.68 and a REG08- Type Accuracy of 0.76 and for the NEG task a String Accuracy of 0.79 and REG08-Type Accuracy of 0.83. 1 Introduction The GREC Generation Challenge 2009 consists of two tasks. The first task is to generate appropriate references to an entity due to a given context which is longer than a sentence. In the GREC-MSR task, data sets are provided of possible referring expressions which have to be selected. In the first shared task on same topic (Belz and Varges, 2007), the main task was to select the referring expression type correctly. In the GREC-MSR 2009 task, the main task is to select the actual word string correctly, and the main evaluation criterion is String Accuracy. The GREC-NEG task is about the generation of references to all person entities in a context longer than a sentence. The NEG data also provides sets of possible referring expressions to each entity (“he”), groups of multiple entities (“they”) and nested references (“his father”). 2 System Description Our approach relies in mapping each input expression for a given reference to a class </context>
</contexts>
<marker>Belz, Varges, 2007</marker>
<rawString>A. Belz and S. Varges. 2007 Generation of Repeated References to Discourse Entities. In Proceedings of the 11th European Workshop on Natural Language Generation (ENLG07), pages 9-16.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>