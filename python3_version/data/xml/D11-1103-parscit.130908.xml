<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.995762">
A Fast Re-scoring Strategy to Capture Long-Distance Dependencies
</title>
<author confidence="0.603358">
Anoop Deoras Tom´aˇs Mikolov Kenneth Church
</author>
<affiliation confidence="0.638066">
HLT-COE and CLSP Brno University of Technology HLT-COE and CLSP
Johns Hopkins University Speech@FIT Johns Hopkins University
</affiliation>
<address confidence="0.734598">
Baltimore MD 21218, USA Czech Republic Baltimore MD 21218, USA
</address>
<email confidence="0.995489">
adeoras@jhu.edu, imikolov@fit.vutbr.cz, kenneth.church@jhu.edu
</email>
<sectionHeader confidence="0.99859" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999885565217391">
A re-scoring strategy is proposed that makes
it feasible to capture more long-distance de-
pendencies in the natural language. Two pass
strategies have become popular in a num-
ber of recognition tasks such as ASR (au-
tomatic speech recognition), MT (machine
translation) and OCR (optical character recog-
nition). The first pass typically applies a
weak language model (n-grams) to a lattice
and the second pass applies a stronger lan-
guage model to N best lists. The stronger lan-
guage model is intended to capture more long-
distance dependencies. The proposed method
uses RNN-LM (recurrent neural network lan-
guage model), which is a long span LM, to re-
score word lattices in the second pass. A hill
climbing method (iterative decoding) is pro-
posed to search over islands of confusability
in the word lattice. An evaluation based on
Broadcast News shows speedups of 20 over
basic N best re-scoring, and word error rate
reduction of 8% (relative) on a highly compet-
itive setup.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999817666666666">
Statistical Language Models (LMs) have received
considerable attention in the past few decades. They
have proved to be an essential component in many
statistical recognition systems such as ASR (au-
tomatic speech recognition), MT (machine trans-
lation) and OCR (optical character recognition).
The task of a language model is to assign prob-
ability to any word sequence possible in the lan-
guage. The probability of the word sequence W ≡
</bodyText>
<equation confidence="0.7429535">
w1, ... , wm ≡ wm1 is typically factored using the
chain rule:
P(wi|wi−1
1 ) (1)
</equation>
<bodyText confidence="0.99976725">
In modern statistical recognition systems, an LM
tends to be restricted to simple n-gram models,
where the distribution of the predicted word depends
on the previous (n − 1) words i.e. P(wi|wi−1
</bodyText>
<equation confidence="0.943781333333333">
1 ) ≈
P (wi|wi−1
i−n+1).
</equation>
<bodyText confidence="0.996586523809524">
Noam Chomsky argued that n-grams cannot learn
long-distance dependencies that span over more than
n words (Chomsky, 1957, pp.13). While that might
seem obvious in retrospect, there was a lot of ex-
citement at the time over the Shannon-McMillan-
Breiman Theorem (Shannon, 1948) which was inter-
preted to say that, in the limit, under just a couple of
minor caveats and a little bit of not-very-important
fine print, n-gram statistics are sufficient to capture
all the information in a string (such as an English
sentence). Chomsky realized that while that may be
true in the limit, n-grams are far from the most parsi-
monious representation of many linguistic facts. In
a practical system, we will have to truncate n-grams
at some (small) fixed n (such as trigrams or perhaps
5-grams). Truncated n-gram systems can capture
many agreement facts, but not all.1
By long-distance dependencies, we mean facts
like agreement and collocations that can span over
many words. With increasing order of n-gram mod-
els we can, in theory, capture more regularities in the
</bodyText>
<footnote confidence="0.83438">
1The discussion in this paragraph is taken as-is from an arti-
cle (to appear) by Church (2012).
</footnote>
<equation confidence="0.980994333333333">
m
P(wm1 ) =
i=1
</equation>
<page confidence="0.944792">
1116
</page>
<note confidence="0.958371">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1116–1127,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.998770365079365">
language. In addition, if we can move to more gen-
eral models then we could hope to capture more, as
well. However, due to data sparsity, it is hard to es-
timate a robust n-gram distribution for large values
of n ( say, n &gt; 10) using the conventional Max-
imum Likelihood techniques, unless a more robust
technique is employed for modeling which gener-
alizes well on unseen events. Some of these well
known long span / complex language models which
have shown to perform very well on many speech
tasks include: structured language model (Chelba
and Jelinek, 2000; Roark, 2001; Wang and Harper,
2002; Filimonov and Harper, 2009), latent seman-
tic analysis language model (Bellegarda, 2000),
topic mixture language models (Iyer and Ostendorf,
1999), whole sentence exponential language mod-
els (Rosenfeld, 1997; Rosenfeld et al., 2001), feed-
forward neural networks (Bengio et al., 2001), re-
current neural network language models (Mikolov
et al., 2010), among many others.
Although better modeling techniques can now
capture longer dependencies in a language, their
incorporation in decoders of speech recognition or
machine translation systems becomes computation-
ally challenging. Due to the prohibitive increase in
the search space of sentence hypotheses (or longer
length word sub sequences), it becomes challenging
to use a long span language model in the first pass
decoding. A word graph (word lattices for speech
recognition systems and hypergraphs for machine
translation systems), encoding exponential number
of hypotheses is hence outputted at the first pass out-
put on which a sophisticated and complex language
model is deployed for re-scoring. However, some-
times even re-scoring of this refined search space
can be computationally expensive due to explosion
of state space.
Previously, we showed in (Deoras et al., 2011)
how to tackle the problem of incorporating long span
information during decoding in speech recogni-
tion systems by variationaly approximating (Bishop,
2006, pp. 462) the long span language model by a
tractable substitute such that this substitute model
comes closest to the long span model (closest in
terms of Kullback Leibler Divergence (Cover and
J.A.Thomas, 1991, pp. 20)). The tractable substi-
tute was then used directly in the first pass speech
recognition systems. In this paper we propose an
approach that keeps the model intact but approxi-
mates the search space instead (which can become
intractable to handle especially under a long span
model), thus enabling the use of full blown model
for re-scoring.With this approach, we can achieve
full lattice re-scoring with a complex model, at a
cost more than 20 times less than of a naive brute
force approach that is commonly used today.
The rest of the paper is organized as follows:
We discuss a particular form of long span language
model in Sec. 2. In Sec. 3 we discuss two standard
re-scoring techniques and then describe and demon-
strate our proposed technique in Sec. 4. We present
experimental results in Sec. 5 followed by conclu-
sions and some remarks in Sec. 6.
</bodyText>
<sectionHeader confidence="0.976314" genericHeader="method">
2 Recurrent Neural Networks (RNN)
</sectionHeader>
<bodyText confidence="0.9999877">
There is a long history of using neural networks to
model sequences. Elman (1990) used recurrent neu-
ral network for modeling sentences of words gen-
erated by an artificial grammar. Work on statistical
language modeling of real natural language data, to-
gether with an empirical comparison of performance
to standard techniques was done by Bengio et al.
(2001). His work has been followed by Schwenk
(2007), who has shown that neural network language
models actually work very well in the state-of-the-
art speech recognition systems. Recurrent Neu-
ral Network based Language Models (RNN-LMs)
(Mikolov et al., 2010) improved the ability of the
original model to capture patterns in the language
without using any additional features (such as part
of speech, morphology etc) i.e. other than lexical
ones. The RNN-LM was shown to have superior
performance than the original feedforward neural
network (Mikolov et al., 2011b). Recently, we also
showed that this model outperforms many other ad-
vanced language modeling techniques (Mikolov et
al., 2011a). We hence decided to work with this
model. This model uses whole history to make pre-
dictions, thus it lies outside the family of n-gram
models. Power of the model comes at a considerable
computational cost. Due to the requirement of un-
limited history, many optimization tricks for rescor-
ing with feedforward-based NNLMs as presented by
Schwenk (2007) cannot be applied during rescoring
with RNN LM. Thus, this model is a good candidate
</bodyText>
<page confidence="0.990173">
1117
</page>
<figure confidence="0.988983666666667">
W( t ) y( t )
S( t )
(delayed)
</figure>
<figureCaption confidence="0.987922">
Figure 1: Schematic Representation of Recurrent Neu-
ral Network Language Model. The network has an input
layer w, a hidden layer s and an output layer y. Matrices
U and V represent synapses.
</figureCaption>
<bodyText confidence="0.9971607">
to show effectiveness and importance of our work.
The basic RNNLM is shown in Fig. 1. The model
has an input layer w(t) that encodes previous word
using 1 of N coding (thus, the size of the input layer
is equal to the size of the vocabulary, and only the
neuron that corresponds to the previous word in a
sequence is set to 1). The hidden layer s(t) has addi-
tional recurrent connections that are delayed by one
time step. After the network is trained, the output
layer y(t) represents probability distribution for the
current word, given the previous word and the state
of the hidden layer from the previous time step.
The training is performed by ‘backpropagation-
through-time’ algorithm that is commonly used for
training recurrent neural networks (Rumelhart et al.,
1986). More details about training, setting initial pa-
rameters, choosing size of the hidden layer etc. are
presented in (Mikolov et al., 2010). Additional ex-
tensions that allow this model to be trained on large
corpora are presented in (Mikolov et al., 2011b).
</bodyText>
<sectionHeader confidence="0.933748" genericHeader="method">
3 Standard Approaches for Rescoring
</sectionHeader>
<subsectionHeader confidence="0.999262">
3.1 Word Lattice Rescoring
</subsectionHeader>
<bodyText confidence="0.999561483870968">
A word lattice, L, obtained at the output of the first
pass decoding, encodes exponential number (expo-
nential in the number of states (nodes) present in
the lattice) of hypotheses in a very compact data
structure. It is a directed acyclic graph G =
(V, £, ns, Ne), where V and £ denote set of vertices
(nodes / states) and edges (arcs / links), respectively.
ns and Ne denote the unique start state and set of
end states.
A path, 7r, in a lattice is an element of £* with
consecutive transitions. We will denote the origin /
previous state of this path by p[7r] and destination /
next state of this path by n[7r]. A path, 7r is called
a complete path if p[7r] = ns and n[7r] E Ne. A
path, 7r, is called a partial path if p[7r] = ns but n[7r]
may or may not belong to Ne. A path, 7r, is called
a trailing path if p[7r] may or may not be equal to
ns and n[7r] E Ne. We will also denote the time
stamp at the start of the path by Ts[7r] and the time
stamp at the end of the path by Te[7r]. Since there
are nodes attached to the start and end of any path,
we will denote the time stamp at any node u E V by
T [u]. Associated with every path, 7r, is also a word
sequence W[7r] E W*, where W is the vocabulary
used during speech recognition. For the sake of sim-
plicity, we will distinguish word sequence of length
1 from the word sequences of length greater than 1
by using lower and upper casing i.e. w[·] and W[·]
respectively.
The acoustic likelihood of the path 7r E £* is then
given as:
</bodyText>
<equation confidence="0.998976">
A[7r] = � |π |P(aj|w[7rj])
j=1
</equation>
<bodyText confidence="0.989294">
where bj E 11, 2, ... , |7r|} 7rj E £, 7r = 0|π|
j=17rj
and P (aj|w[7rj]) is the acoustic likelihood of the
acoustic substring aj, spanning between Ts[7rj] and
Te[7rj], conditioned on the word w[7rj] associated
with the edge 7rj.2 Similarly, the language model
score of the path 7r is given as:
</bodyText>
<equation confidence="0.998789">
L[7r] = � |π |P(w[7rj]|w[7rj−1], ... , w[7rj−m+1])
j=1
</equation>
<bodyText confidence="0.9960977">
where P(w[7rj]|w[7rj−1], ... , w[7rj−m+1]) is the
m-th order Markov approximation for estimating the
probability of a word given the context upto that
point. The speech recognizer, which uses m-th or-
der Markov LM for first pass recognition, imposes a
constraint on the word lattice such that at each state
there exists an unambiguous context of consecutive
m − 1 words.
A first pass output is then a path 7r* having Max-
imum a Posterior (MAP) probability.3 Thus 7r* is
</bodyText>
<footnote confidence="0.995867666666667">
2We will use O symbol to denote concatenation of paths or
word strings.
3Note that asterisk symbol here connotes that the path is op-
</footnote>
<page confidence="0.977095">
1118
</page>
<bodyText confidence="0.547054">
obtained as:
</bodyText>
<equation confidence="0.971221666666667">
7r* = arg max A[7r]&apos;&apos;L[7r],
7r:p[7r]=ns
n[7r]ENe
</equation>
<bodyText confidence="0.999939818181818">
where -y is the scaling parameter needed to balance
the dynamic variability between the distributions of
acoustic and language model (Ogawa et al., 1998).
Efficient algorithms such as single source shortest
path (Mohri et al., 2000) can be used for finding out
the MAP path.
Under a new n-gram Language Model, rescor-
ing involves replacing the existing language model
scores of all paths 7r. If we denote the new language
model by Lnew and correspondingly the score of the
path 7r by Lnew[7r], then it is simply obtained as:
</bodyText>
<equation confidence="0.998948">
Lnew[7r] = � |7r |P(w[7rj]|w[7rj_1], ... , w[7rj_n+1])
j=1
</equation>
<bodyText confidence="0.999894833333333">
where P(w[7rj]|w[7rj_1], ... , w[7rj_n+1]) is the n-
th order Markov approximation for estimating the
probability of a word given the unambiguous con-
text of n − 1 words under the new rescoring LM.
If the Markov rescoring n-gram LM needs a bigger
context for the task of prediction (i.e. n &gt; m, where
m − 1 is the size of the unambiguous context main-
tained at every state of the word lattice), then each
state of the lattice has to be split until an unambigu-
ous context of length as large as that required by the
new re-scoring language model is not maintained.
The best path, 7r* is then obtained as:
</bodyText>
<equation confidence="0.815958333333333">
7r* = arg max A[7r]ηLnew[7r],
7r:p[7r]=ns
n[7r]ENe
</equation>
<bodyText confidence="0.999931888888889">
where q acts as the new scaling parameter which
may or may not be equal to the old scaling parameter
-y.
It should be noted that if the rescoring LM needs a
context of the entire past in order to predict the next
word, then the lattice has to be expanded by splitting
the states many more times. This usually blows up
the search space even for a reasonably small number
timal under some model. This should not be confused with the
Kleene stars appearing as superscripts for E and W, which serve
the purpose of regular expressions implying 0 or many occu-
rances of the element of E and V respectively.
of state splitting iterations. When the task is to do
rescoring under a long span LM, such as RNN-LM,
then exact lattice re-scoring option is not feasible. In
order to tackle this problem, a suboptimal approach
via N best list rescoring is utilized. The details of
this method are presented next.
</bodyText>
<subsectionHeader confidence="0.998504">
3.2 N best List Rescoring
</subsectionHeader>
<bodyText confidence="0.999738333333333">
N best list re-scoring is a popular way to cap-
ture some long-distance dependencies, though the
method can be slow and it can be biased toward the
weaker language model that was used in the first
pass.
Given a word lattice, L, top N paths
{7r1, ... , 7rNJ are extracted such that their joint
likelihood under the baseline acoustic and language
models are in descending order i.e. that:
</bodyText>
<equation confidence="0.960153">
A[7r1]&apos;&apos;L[7r1] &gt; A[7r2]&apos;&apos;L[7r2] &gt; ... &gt; A[7rN]&apos;&apos;L[7rN]
</equation>
<bodyText confidence="0.999652777777778">
Efficient algorithms exist for extracting N best paths
from word lattices (Chow and Schwartz, 1989;
Mohri and Riley, 2002). If a new language model,
Lnew, is provided, which now need not be restricted
to finite state machine family, then that can be de-
ployed to get the score of the entire path 7r. If we
denote the new LM scores by Lnew[], then under N
best list paradigm, optimal path 7r˜ is found out such
that:
</bodyText>
<equation confidence="0.9993175">
7r˜ = arg max A[7r]ηLnew[7r], (2)
7rE17r1,...,7rN }
</equation>
<bodyText confidence="0.9995956875">
where q acts as the new scaling parameter which
may or may not be equal to -y. If N « |L |(where
|L |is the total number of complete paths in word lat-
tice, which are exponentially many), then the path
obtained using (2) is not guaranteed to be optimal
(under the rescoring model). The short list of hy-
potheses so used for re-scoring would yield subop-
timal output if the best path 7r* (according to the
new model) is not present among the top N candi-
dates extracted from the lattice. This search space
is thus said to be biased towards a weaker model
mainly because the N best lists are representative of
the model generating them. To illustrate the idea,
we demonstrate below a simple analysis on a rel-
atively easy task of speech transcription on WSJ
data.4 In this setup, the recognizer made use of a bi-
</bodyText>
<footnote confidence="0.9643645">
4Full details about the setup can be found in (Deoras et al.,
2010)
</footnote>
<page confidence="0.996442">
1119
</page>
<bodyText confidence="0.999676235294118">
gram LM to produce lattices and hence N best lists.
Each hypothesis in this set got a rank with the top
most and highest scoring hypothesis getting a rank
of 1, while the bottom most hypothesis getting a
rank of N. We then re-scored these hypotheses with
a better language model (either with a higher order
Markov LM i.e. a trigram LM (tg) or the log linear
combination of n-gram models and syntactic mod-
els (n-gram+syntactic) and re-ranked the hypothe-
ses to obtain their new ranks. We then used Spear-
man’s rank correlation factor, p, which takes values
in [−1,+1], with −1 meaning that the two ranked
lists are negatively correlated (one list is in a reverse
order with respect to the other list) and +1 mean-
ing that the two ranked lists are positively correlated
(the two lists are exactly the same). Spearman’s rank
correlation factor is given as:
</bodyText>
<equation confidence="0.973621">
N(N2 − 1), (3)
</equation>
<bodyText confidence="0.999785818181818">
where dn is the difference between the old and new
rank of the nth entry (in our case, difference between
n(E 11, 2, ... , NJ) and the new rank which the nth
hypothesis got under the rescoring model).
Table 1 shows how the correlation factor drops
dramatically when a better and a complementary
LM is used for re-scoring, suggesting that the N best
lists are heavily biased towards the starting models.
Huge re-rankings suggests there is an opportunity to
improve and also a need to explore more hypotheses,
i.e. beyond N best lists.
</bodyText>
<table confidence="0.98976">
Model (p) WER (%)
bg 1.00 18.2%
tg 0.41 17.4%
n-gram+syntactic 0.33 15.8%
</table>
<tableCaption confidence="0.998736">
Table 1: Spearman Rank Correlation on the N best list
</tableCaption>
<bodyText confidence="0.905023285714286">
extracted from a bi-gram language model (bg) and re-
scored with relatively better language models including,
trigram LM (tg), and the log linear combination of n-
gram models, and syntactic models (n-gram+syntactic).
With a bigger and a better LM, the WER decreases at
the expense of huge re-rankings of N best lists, only
suggesting the fact that N best lists generated under a
weaker model, are not reflective enough of a relatively
better model.
In the next section, we propose an algorithm
which keeps the representation of search space as
simple as that of N best list, but does not restrict it-
self to top N best paths alone and hence does not get
biased towards the starting weaker model.
</bodyText>
<sectionHeader confidence="0.973237" genericHeader="method">
4 Proposed Approach for Rescoring
</sectionHeader>
<bodyText confidence="0.999927707317073">
A high level idea of our proposed approach is to
identify islands of confusability in the word lattice
and replace the problem of global search over word
lattice by series of local search problems over these
islands in an iterative manner. The motivation be-
hind this strategy is the observation that the recog-
nizer produces bursts of errors such that they have
a temporal scope. The recognizer output (sentence
hypotheses) when aligned together typically shows
a pattern of confusions both at the word level and
at the phrase level. Regions where there are sin-
gleton words competing with one another (reminis-
cent of a confusion bin of a Confusion Network
(CN) (Mangu, 2000)), choice of 1 word edit dis-
tance works well for the formation of local neigh-
borhood. Regions where there are phrases com-
peting with other phrases, choice of variable length
neighborhood works well. Previously, Richardson
et al. (1995) demonstrated a hill climbing frame-
work by exploring 1 word edit distance neighbor-
hood, while in our own previous work (Deoras and
Jelinek, 2009), we demonstrated working of iterative
decoding algorithm, a hill climbing framework, for
CNs, in which the neighborhood was formed by all
words competing with each other in any given time
slot, as defined by a confusion bin.
In this work, we propose a technique which gen-
eralizes very well on word lattices and overcomes
the limitations posed by a CN or by the limited na-
ture of local neighborhood. The size of the neigh-
borhood in our approach is a variable factor which
depends upon the confusability in any particular re-
gion of the word lattice. Thus the local neighbor-
hood are in some sense a function of the confusabil-
ity present in the lattice rather than some predeter-
mined factor. Below we describe the process, virtue
of which, we can cut the lattice to form many self
contained smaller sized sub lattices. Once these sub
lattices are formed, we follow a similar hill climbing
procedure as proposed in our previous work (Deoras
and Jelinek, 2009).
</bodyText>
<equation confidence="0.93113075">
6
N 2
p
�n=1 dn = 1 −
</equation>
<page confidence="0.893676">
1120
</page>
<subsectionHeader confidence="0.977174">
4.1 Islands of Confusability
</subsectionHeader>
<bodyText confidence="0.990402875">
We will continue to follow the notation introduced
in section 3.1. Before we define the procedure for
cutting the lattice into many small self contained
lattices, we will define some more terms necessary
for the ease of understandability of the algorithm.5
For any node v E V, we define forward probability,
α(v), as the probability of any partial path 7r E £*,
s.t. p[7r] = ns, n[7r] = v and it is given as:
</bodyText>
<equation confidence="0.959839666666667">
�α(v) = A[7r]γL[7r] (4)
πEE∗
s.t.p[π]=ns,n[π]=v
</equation>
<bodyText confidence="0.97972675">
Similarly, for any node v E V, we define the
backward probability, β(v), as the probability of any
trailing path 7r E £*, s.t. p[7r] = v, n[7r] E Ne and it
is given as:
</bodyText>
<equation confidence="0.937693333333333">
β(v) = � A[7r]γL[7r] (5)
πEE∗
s.t.p[π]=v,n[π]ENe
</equation>
<bodyText confidence="0.999731">
If we define the sum of joint likelihood under the
baseline acoustic and language models of all paths
in the lattice by Z, then it can simply be obtained as:
</bodyText>
<equation confidence="0.760781">
Z = EuENe α(u) = β(ns)
</equation>
<bodyText confidence="0.978372333333333">
In order to cut the lattice, we want to identify sets
of nodes, S1, S2, ... , S|S |such that for any set Si E
S following conditions are satisfied:
</bodyText>
<listItem confidence="0.9983845">
1. For any two nodes u, v E Si we have that:
T [u] = T [v]. We will define this common time
stamp of the nodes in the set by T [Si].
2. A 7r E £ such that Ts[7r] &lt; T [Si] &lt; Te[7r].
</listItem>
<bodyText confidence="0.975366">
The first property can be easily checked by first
pushing states into a linked list associated with each
time marker (this can be done by iterating over all
the states of the graph) then iterating over the unique
time markers and retrieving back the nodes asso-
ciated with it.The second property can be checked
by first iterating over the unique time markers and
for each of the marker, iterating over the arcs and
terminating the loop as soon as some arc is found
5Goel and Byrne (2000) previously demonstrated the lat-
tice segmentation procedure to solve the intractable problem of
MBR decoding. The cutting procedure in our work is different
from theirs in the sense that we rely on time information for
collating competing phrases, while they do not.
out violating property 2 for the specific time marker.
Thus the time complexity for checking property 1 is
O(|V|) and that for property 2 is O(|T |x|£|), where
|T  |is the total number of unique time markers. Usu-
ally |T  |« |£ |and hence the time complexity for
checking property 2 is almost linear in the number
of edges. Thus effectively, the time complexity for
cutting the lattice is O(|V |+ |£|).
Having formed such sets, we can now cut the
lattice at time stamps associated with these sets
i.e. that: T [S1], ... , T [S|S|]. It can be easily seen
that the number of sub lattices, C, will be equal
to |S |− 1.We will identify these sub lattices as
L1, L1, ... , LC. At this point, we have not formed
self contained lattices yet by simply cutting the par-
ent lattice at the cut points.
Once we cut the lattice at these cut points, we im-
plicitly introduce many new starting nodes and end-
ing nodes for any sub lattice. We will refer to these
nodes as exposed starting nodes and exposed end-
ing nodes. Thus for some jth sub lattice, Lj, there
will be as many new exposed starting nodes as there
are nodes in the set Sj and as many exposed ending
nodes as there are nodes in the set Sj+1. In order
to make these sub lattices consistent with the defini-
tion of a word lattice (see Sec. 3.1), we unify all the
exposed starting nodes and exposed ending nodes.
To unify the exposed starting nodes, we introduce
as many new edges as there are nodes in the set Sj
such that they have a common starting node, ns[Lj],
(newly created) and distinct ending nodes present
in Sj. To unify the exposed ending nodes of Lj,
we introduce as many new edges as there are nodes
in the set Sj+1 such that they have distinct starting
nodes present in Sj+1 and a common ending node
ne[Lj] (newly created). From the totality of these
new edges and nodes along with the ones already
present in Lj forms an induced directed acyclic sub-
graph G[Lj] = (V[Lj], £[Lj], ns[Lj], ne[Lj]).
For any path 7r E £[Lj] such that p[7r] = ns[Lj]
and n[7r] E Sj, we assign the value of α(n[7r])
to denote the joint likelihood A[7r]γL[7r] and as-
sign epsilon for word associated with these edges
i.e. w[7r]. We assign T [Sj] − δT to denote Ts[7r]
and T [Sj] to denote Te[7r]. Similarly, for any path
7r E £[Lj] such that p[7r] E Sj+1 and n[7r] = ne[Lj],
</bodyText>
<page confidence="0.990966">
1121
</page>
<bodyText confidence="0.999928857142857">
we assign the value of β(p[7r])6 to denote the joint
likelihood A[7r]γL[7r] and assign epsilon for word
associated with these edges i.e. w[7r]. We assign
T [5j+1] to denote Ts[7r] and T [5j+1] + ST to de-
note Te[7r]. This completes the process and we ob-
tain self contained lattices, which if need be, can be
independently decoded and/or analyzed.
</bodyText>
<subsectionHeader confidence="0.982987">
4.2 Iterative Decoding on Word Lattices
</subsectionHeader>
<bodyText confidence="0.999627222222222">
Once we have formed the self contained lattices,
L1, L1, ... , LC, where C is the total number of sub
lattices formed, then the idea is to divide the re-
scoring problem into many small re-scoring prob-
lems carried over the sub lattices one at a time by
fixing single best paths from all the remaining sub
lattices.
The inputs to the algorithm are the sub lattices
(produced by cutting the parent lattice generated un-
der some Markov n-gram LM) and a new rescor-
ing LM, which now need not be restricted to fi-
nite state machine family. The output of the al-
gorithm is a word string, W*, such that it is the
concatenation of final decoded word strings from
each sub lattice. Thus if we denote the final de-
coded path (under some decoding scheme, which
will become apparent next) in the jth sub lattice
by 7rj* and the concatenation symbol by ’·’, then
</bodyText>
<equation confidence="0.952892777777778">
W* = W[7r*1] · W[7r*2] · ... · W[7r* C] = 0C j=1W[7r*j].
Algorithm 1 Iterative Decoding on word lattices.
Require: {L1, L1, ... , LC}, Lnew
PrevHyp ← null
CurrentHyp ← OC j=1W [ˆ7rj]
while PrevHyp =6 CurrentHyp do
for i ← 1... C do
ˆ7ri ← argmax
πiE£∗i :
p[πi]=ns[Gi]
n[πi]=ne[Gi]
×A[7ri]η rkj=1
jai
end for
PrevHyp ← CurrentHyp
CurrentHyp ← OC j=1W [ˆ7rj]
end while
∀j ∈ {1, 2, ... , C} 7rj* ← ˆ7rj
</equation>
<footnote confidence="0.931494">
6The values of α(·) and β(·) are computed under parent lat-
tice structure.
</footnote>
<bodyText confidence="0.9998769">
The algorithm is initialized by setting PrevHypo
to null and CurrHypo to the concatenation of 1-best
output from each sub lattice. During the initializa-
tion step, each sub lattice is analyzed independent of
any other sub lattice and under the baseline acoustic
scores and baseline n-gram LM scores, 1-best path
is found out. Thus if we define the best path under
baseline model in some jth sub-lattice by ˆ7rj, Cur-
rHypo is then initialized to: W [ˆ7r1] · W [ˆ7r2] · ... ·
W[ˆ7rC]. The algorithm then runs as long as Cur-
rHypo is not equal to PrevHypo. In each iteration,
the algorithm sequentially re-scores each sub-lattice
by keeping the surrounding context fixed. Once all
the sub lattices are re-scored, that constitutes one it-
eration. At the end of each iteration, CurrHypo is
set to the concatenation of 1 best paths from each
sub lattice while PrevHypo is set to the old value
of CurrHypo. Thus if we are analyzing some ith
sub-lattice in some iteration, then 1-best paths from
all but this sub-lattice is kept fixed and a new 1-best
path under the re-scoring LM is found out. It is not
hard to see that the likelihood of the output under
the new re-scoring model is guaranteed to increase
monotonically after every decoding step.
Since the cutting of parent lattices produce many
small lattices with considerably lesser number of
nodes, in practice, an exhaustive search for the 1-
best hypothesis can be carried out via N best list.
Algorithm 1 outlines the steps for iterative decoding
on word lattices.
</bodyText>
<subsectionHeader confidence="0.991304">
4.3 Entropy Pruning
</subsectionHeader>
<bodyText confidence="0.9999911875">
In this section, we will discuss a speed up technique
based on entropy of the lattice. Entropy of a lattice
reflects the confidence of the recognizer in recogniz-
ing the acoustics. Based on the observation that if
the N best list / lattice generated under some model
has a very low entropy, then the Spearman’s rank
correlation factor, ρ (Eqn. 3), tends to be higher
even when the N best lists / lattice is re-ranked with
a bigger and a better model. A low entropy under
the baseline model only reflects the confidence of
the recognizer in recognizing the acoustic. Table 2
shows the rank correlation values between two sets
of N best lists. Both sets are produced by a bi-
gram LM (bg). The entropy of N best lists in the
first set is 0.05 nats or less. The N best lists in the
second set have an entropy greater than 0.05 nats.
</bodyText>
<equation confidence="0.9606565">
(Lnew[ ˆ7r1 · ... · 7ri · ... · ˆ7rk]
A[ˆ7rj]ηl
</equation>
<page confidence="0.960599">
1122
</page>
<bodyText confidence="0.999967625">
Both these sets are re-ranked with bigger and bet-
ter models (see Table 1 for model definitions). We
can see from Table 2 that the rank correlation values
tend to be higher (indicating little re-rankings) when
the entropy of the N best list, under the baseline
model, is lower. Similarly, the rank-correlation val-
ues tend to be lower (indicating more re-rankings)
whenever the entropy of the N best list is higher.
Note that these entropy values are computed with re-
spect to the starting model (in this case, bigram LM).
Of course, if the starting LM is much weaker than
the rescoring model, then the entropy values need
not be reflective of the difficulty of the overall task.
This observation then suggests that it is safe to re-
score only those N best lists whose entropy under
the starting model is higher than some threshold.
</bodyText>
<table confidence="0.99858625">
Rescoring Model P(x≤0.05) P(x&gt;0.05)
bg 1.00 1.00
tg 0.58 0.38
n-gram+syntactic 0.54 0.31
</table>
<tableCaption confidence="0.982124">
Table 2: Spearman Rank Correlation on the N best list
</tableCaption>
<figureCaption confidence="0.7881045">
extracted from a bi-gram language model (bg) and re-
scored with relatively better language models (see Table 1
for model definitions). Entropy under the baseline model
correlates well with the rank correlation factor, suggest-
ing that exhaustive search need not be necessary for ut-
terances yielding lower entropy.
</figureCaption>
<bodyText confidence="0.999778142857143">
While computation of entropy for N best list is
tractable, for a word lattice, the computation of en-
tropy is intractable if one were to enumerate all the
hypotheses. Even if we were able to enumerate all
hypotheses, this method tends to be slower. Using
efficient semiring techniques introduced by Li and
Eisner (2009) or using posterior probabilities on the
edges leading to end states, we can compute the en-
tropy of a lattice in one single forward pass using
dynamic programming. It should, however, be noted
that, for dynamic programming technique to work,
only n-gram LMs can be used. One has to resort to
approximate entropy computation via N best list, if
entropy under long span LM is desired.
</bodyText>
<subsectionHeader confidence="0.598429">
4.3.1 Speed Up for Iterative Decoding
</subsectionHeader>
<bodyText confidence="0.99983475">
Our speed up technique is simple. Once we have
formed self contained sub lattices, we want to prune
all but the top few best complete paths (obtained un-
der baseline / starting model) of those sub lattices
whose entropy is below some threshold. Thus, be-
lieving in the original model’s confidence, we want
to focus only on those sub lattices which the recog-
nizer found difficult to decode in the first pass. All
other part of the parent lattice will be not be ana-
lyzed. The thresholds for pruning is very application
and corpus specific and needs to be tuned on some
held out data.
</bodyText>
<sectionHeader confidence="0.994291" genericHeader="evaluation">
5 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999905121212121">
We performed recognition on the Broadcast News
(BN) dev04f, rt03 and rt04 task using the state-
of-the-art acoustic models trained on the English
Broadcast News (BN) corpus (430 hours of audio)
provided to us by IBM (Chen et al., 2009). IBM also
provided us its state-of-the-art speech recognizer,
Attila (Soltau et al., 2010) and two Kneser-Ney
smoothed backoff n-gram LMs containing 4.7M n-
grams (n &lt; 4) and 54M n-grams (n &lt; 4), both
trained on 400M word tokens. We will refer to them
as KN:BN-Small and KN:BN-Big respectively. We
refer readers to (Chen et al., 2009) for more details
about the recognizer and corpora used for training
the models.
We trained two RNN based language models -
the first one, denoted further as RNN-limited, was
trained on a subset of the training data (58M tokens).
It used 400 neurons in the hidden layer. The second
model, denoted as RNN-all, was trained on all of
the training data (400M tokens), but due to the com-
putational complexity issues, we had to restrict its
hidden layer size to 320 neurons.
We followed IBM’s multi-pass decoding recipe
using KN:BN-Small in the first pass followed by ei-
ther N best list re-scoring or word lattice re-scoring
using bigger and better models.7 For the purpose
of re-scoring, we combined all the relevant statisti-
cal models in one unified log linear framework rem-
iniscent of work by Beyerlein (1998). We, however,
trained the model weights by optimizing expected
WER rather than 1-best loss as described in (De-
oras et al., 2010). Training was done on N best
lists of size 2K. We will refer to the log linear com-
</bodyText>
<footnote confidence="0.9079">
7The choice of the order and size of LM to be used in the
first pass decoding was determined by taking into consideration
the capabilities of the decoder.
</footnote>
<page confidence="0.936071">
1123
</page>
<bodyText confidence="0.307296">
Plot of 1 best WER v/s Search Space Size
Size of Search Space (Number of Hypotheses for evaluation)
</bodyText>
<figureCaption confidence="0.8817869">
Figure 2: Plot of WER (y axis) on rt03+dev04f set versus
the size of the search space (x axis). The baseline WER
obtained using KN:BN-Small is 12% which then drops
to 11% when KN:BN-Big is used for re-scoring. N best
list search method obtains the same reduction in WER
by evaluating as many as 228K sentence hypotheses on
an average. The proposed method obtains the same re-
duction by evaluating 14 times smaller search space. The
search effort reduces further to 40 times if entropy based
pruning is employed during re-scoring.
</figureCaption>
<bodyText confidence="0.999851055555556">
bination of KN:BN-Big and RNN-limited by KN-
RNN-lim; KN:BN-Big and RNN-all by KN-RNN-
all and KN:BN-Big, RNN-limited and RNN-all by
KN-RNN-lim-all.
We used two sets for decoding: rt03+dev04f set
was used as a development set while rt04 was used
as a blind set for the purpose of evaluating the per-
formance of long span RNN models using the pro-
posed approach. We made use of OpenFst C++ li-
braries (Allauzen et al., 2007) for manipulating lat-
tice graphs and generating N best lists. Due to the
presence of hesitation tokens in reference transcripts
and the need to access the silence/pause tokens for
penalizing short sentences, we treated these tokens
as regular words before extracting sentence hypothe-
ses. This, and poorly segmented nature of the test
corpora, led to huge enumeration of sentence hy-
potheses.
</bodyText>
<subsectionHeader confidence="0.998546">
5.1 n-gram LM for re-scoring
</subsectionHeader>
<bodyText confidence="0.999989457142857">
In this setup, we used KN:BN-Small as the base-
line starting LM which yielded the WER of 12%
on rt03+dev04f set. Using KN:BN-Big as the re-
scoring LM, the WER dropped to 11%. Since the
re-scoring LM belonged to the n-gram family, it was
possible to compute the optimal word string by re-
scoring the whole lattice (see Sec. 3.1). We now
compare the performance of N best list approach
(Sec. 3.2) with our proposed approach (Sec. 4).
N best list achieved the best possible reduction by
evaluating as many as 228K sentence hypotheses
on an average. As against that, our proposed ap-
proach achieved the same performance by evaluat-
ing 16.6K sentence hypotheses, thus reducing the
search efforts by 13.75 times. By carrying out en-
tropy pruning (see Sec. 4.3 ) on sub lattices, our pro-
posed approach required as little as 5.6K sentence
hypotheses evaluations to obtain the same optimal
performance, reducing the search effort by as much
as 40.46 times. For the purpose of this experiment,
entropy based pruning was carried out when the en-
tropy of the sub lattice was below 5 nats. Table 3
compares the two search methods for this setup and
Fig. 2 shows a plot of WER versus the size of the
search space (in terms of number of sentence hy-
potheses evaluated by an n-gram language model).
On rt04, the KN:BN-Small LM gave a WER of
14.1% which then dropped to 13.1% after re-scoring
with KN:BN-Big. Since the re-scoring model was
an n-gram LM, it was possible to obtain the opti-
mal performance via lattice update technique (see
Sec. 3.1). We then carried out the re-scoring of the
word lattices under KN:BN-Big using our proposed
technique and found it to give the same performance
yielding the WER of 13.1%.
</bodyText>
<subsectionHeader confidence="0.998967">
5.2 Long Span LM for re-scoring
</subsectionHeader>
<bodyText confidence="0.999853">
In this setup, we used the strongest n-gram LM
as our baseline. We thus used KN:BN-Big as the
baseline LM which yielded the WER of 11% on
rt03+dev04f. We then used KN-RNN-lim-all for re-
scoring. Due to long span nature of the re-scoring
LM, it was not possible to obtain the optimal WER
performance. Hence we have compared the perfor-
mance of our proposed method with N best list ap-
proach. N best list achieved the lowest possible
WER after evaluating as many as 33.8K sentence
hypotheses on an average. As against that, our pro-
posed approach in conjunction with entropy pruning
obtained the same performance by evaluating just
1.6K sentence hypotheses, thus reducing the search
by a factor of 21. Fig 3 shows a plot of WER versus
</bodyText>
<figure confidence="0.976632777777778">
12
N Best
Iter. Dec. (ID)
ID with Ent. Pruning
Viterbi Baseline
Viterbi Rescoring
11.2
11
10.8
100 101 102 103 104 105 106
11.8
1 best WER(%)
11.6
11.4
1124
Plot of 1 best WER v/s Search Space Size
11.1
11
10.9
10.8
10.7
10.6
10.5
10.4
10.3
100 101 102 103 104
Size of Search Space (Number of Hypotheses for evaluation)
</figure>
<figureCaption confidence="0.8796075">
Figure 3: Plot of WER (y axis) on rt03+dev04f set versus
the size of the search space (x axis). The baseline WER
</figureCaption>
<bodyText confidence="0.995845866666667">
obtained using KN:BN-Big is 11% which then drops to
10.4% when KN-RNN-lim-all is used for re-scoring. N
best list search method obtains this reduction in WER by
evaluating as many as 33.8K sentence hypotheses on an
average, while the proposed method (with entropy prun-
ing) obtains the same reduction by evaluating 21 times
smaller search space.
the size of the search space (in terms of number of
sentence hypotheses evaluated by a long span lan-
guage model).
In-spite of starting off with a very strong n-gram
LM, the N best lists so extracted were still not repre-
sentative enough of the long span rescoring models.
Had we started off with KN:BN-Small, the N best
list re-scoring method would have had no chance of
finding the optimal hypothesis in reasonable size of
hypotheses search space. Table 4 compares the two
search methods for this setup when many other long
span LMs were also used for re-scoring.
On rt04, the KN:BN-Big LM gave a WER of
13.1% which then dropped to 12.15% after re-
scoring with KN-RNN-lim-all using our proposed
technique.8 Since the re-scoring model was not an
n-gram LM, it was not possible to obtain the optimal
performance but we could enumerate huge N best
list to approximate this value. Our proposed method
is much faster than huge N best lists and no worse
in terms of WER. As far as we know, the result ob-
tained on these sets is the best performance ever
reported on the Broadcast News corpus for speech
</bodyText>
<footnote confidence="0.995133">
8The WER obtained using KN-RNN-lim and KN-RNN-all
were 12.5% and 12.3% respectively.
</footnote>
<table confidence="0.870611">
recognition.
Models WER NBest ID Saving
KN:BN-Small 12.0 - - -
KN:BN-Big 11.0 228K 5.6K 40
</table>
<tableCaption confidence="0.986628">
Table 3: The starting LM is a weak n-gram LM (KN:BN-
</tableCaption>
<figureCaption confidence="0.835477833333333">
Small) and the re-scoring LM is a much stronger but n-
gram LM (KN:BN-Big). The baseline WER in this case
is 12% and the optimal performance by the re-scoring LM
is 11.0%. The proposed method outperforms N best list
approach, in terms of search efforts, obtaining optimal
WER.
</figureCaption>
<table confidence="0.9835402">
Models WER NBest ID Saving
KN:BN-Big 11.0 - - -
KN-RNN-lim 10.5 42K 1.1K 38
KN-RNN-all 10.5 26K 1.3K 20
KN-RNN-lim-all 10.4 34K 1.6K 21
</table>
<tableCaption confidence="0.982115">
Table 4: The starting LM is a strong n-gram LM
</tableCaption>
<bodyText confidence="0.559858">
(KN:BN-Big) and the re-scoring model is a long span
LM (KN-RNN-*). The baseline WER is 11.0%. Due
to long span nature of the LM, optimal WER could not
be estimated. The proposed method outperfoms N best
list approach on every re-scoring task.
</bodyText>
<sectionHeader confidence="0.999263" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999936333333333">
We proposed and demonstrated a new re-scoring
technique for general word graph structures such as
word lattices. We showed its efficacy by demonstrat-
ing huge reductions in the search effort to obtain a
new state-of-the-art performance on a very compet-
itive speech task of Broadcast news. As part of the
future work, we plan to extend this technique for hy-
pergraphs and lattices in re-scoring MT outputs with
complex and long span language models.
</bodyText>
<sectionHeader confidence="0.957226" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.999956125">
This work was partly funded by Human Language
Technology, Center of Excellence and by Tech-
nology Agency of the Czech Republic grant No.
TA01011328, and Grant Agency of Czech Repub-
lic project No. 102/08/0707. We would also like to
acknowledge the contribution of Frederick Jelinek
towards this work. He would be a co-author if he
were available and willing to give his consent.
</bodyText>
<figure confidence="0.8253285">
1 best WER(%)
N Best
ID with Ent. Pruning
Viterbi Baseline
</figure>
<page confidence="0.977957">
1125
</page>
<sectionHeader confidence="0.994935" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999797116504854">
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. OpenFst:
A General and Efficient Weighted Finite-State Trans-
ducer Library. In Proceedings of the Ninth Interna-
tional Conference on Implementation and Application
of Automata, (CIAA 2007), volume 4783 of Lecture
Notes in Computer Science, pages 11–23. Springer.
J. R. Bellegarda. 2000. Exploiting latent semantic infor-
mation in statistical language modeling. Proceedings
of IEEE, 88(8):1279–1296.
Yoshua Bengio, R´ejean Ducharme, and Pascal Vincent.
2001. A Neural Probabilistic Language Model. In
Proceedings of Advances in Neural Information Pro-
cessing Systems.
Peter Beyerlein. 1998. Discriminative Model Combina-
tion. In Proc. of IEEE International Conference on
Acoustics, Speech, and Signal Processing (ICASSP).
Christopher M. Bishop. 2006. Pattern Recognition and
Machine Learning. Springer.
Ciprian Chelba and Frederick Jelinek. 2000. Struc-
tured Language Modeling. Computer Speech and Lan-
guage, 14(4):283–332.
S. F. Chen, L. Mangu, B. Ramabhadran, R. Sarikaya,
and A. Sethy. 2009. Scaling shrinkage-based lan-
guage models. In Proc. of IEEE Workshop on Auto-
matic Speech Recognition and Understanding (ASRU),
pages 299–304.
Noam Chomsky. 1957. Syntactic Structures. The
Hague: Mouton.
Yen-Lu Chow and Richard Schwartz. 1989. The N-Best
algorithm: an efficient procedure for finding top N sen-
tence hypotheses. In Proceedings of the workshop on
Speech and Natural Language, HLT ’89, pages 199–
202, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Kenneth Church. 2012. A Pendulum Swung Too Far.
Linguistic Issues in Language Technology - LiLT. to
appear.
T.M. Cover and J.A.Thomas. 1991. Elements of Infor-
mation Theory. John Wiley and Sons, Inc. N.Y.
Anoop Deoras and Frederick Jelinek. 2009. Iterative De-
coding: A Novel Re-Scoring Framework for Confu-
sion Networks. In Proc. of IEEE Workshop on Auto-
matic Speech Recognition and Understanding (ASRU),
pages 282 –286.
Anoop Deoras, Denis Filimonov, Mary Harper, and Fred
Jelinek. 2010. Model Combination for Speech Recog-
nition using Empirical Bayes Risk Minimization. In
Proc. of IEEE Workshop on Spoken Language Tech-
nology (SLT).
Anoop Deoras, Tom´a&amp;quot;s Mikolov, Stefan Kombrink, Mar-
tin Karafi´at, and Sanjeev Khudanpur. 2011. Varia-
tional Approximation of Long-Span Language Mod-
els for LVCSR. In Proc. of IEEE International Con-
ference on Acoustics, Speech, and Signal Processing
(ICASSP).
Jeffery Elman. 1990. Finding Structure in Time. In Cog-
nitive Science, volume 14, pages 179–211.
Denis Filimonov and Mary Harper. 2009. A Joint Lan-
guage Model with Fine-grain Syntactic Tags. In Proc.
of 2009 Conference on Empirical Methods in Natural
Language Processing.
V. Goel and W. Byrne. 2000. Minimum Bayes Risk Au-
tomatic Speech Recognition. Computer, Speech and
Language.
Rukmini Iyer and Mari Ostendorf. 1999. Modeling Long
Distance Dependence in Language: Topic Mixtures
Versus Dynamic Cache Models. IEEE Transactions
on Speech and Audio Processing, 7(1):30–39.
Zhifei Li and Jason Eisner. 2009. First- and second-order
expectation semirings with applications to minimum-
risk training on translation forests. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 40–51, Singapore,
August.
Lidia Luminita Mangu. 2000. Finding consensus in
speech recognition. Ph.D. thesis, The Johns Hopkins
University. Adviser-Brill, Eric.
Tom´a&amp;quot;s Mikolov, Martin Karafi´at, Luk´a&amp;quot;s Burget,
Jan “Honza” &amp;quot;Cernock´y, and Sanjeev Khudanpur.
2010. Recurrent Neural Network Based Language
Model. In Proc. of the ICSLP-Interspeech.
Tom´a&amp;quot;s Mikolov, Anoop Deoras, Stefan Kombrink, Luk´a&amp;quot;s
Burget, and Jan “Honza” &amp;quot;Cernock´y. 2011a. Empirical
Evaluation and Combination of Advanced Language
Modeling Techniques. In Proc. of Interspeech.
Tom´a&amp;quot;s Mikolov, Stefan Kombrink, Luk´a&amp;quot;s Burget,
Jan “Honza” &amp;quot;Cernock´y, and Sanjeev Khudanpur.
2011b. Extensions of Recurrent Neural Network Lan-
guage Model. In Proc. of IEEE International Con-
ference on Acoustics, Speech, and Signal Processing
(ICASSP).
Mehryar Mohri and Michael Riley. 2002. An Efficient
Algorithm for the N-Best-Strings Problem. In Pro-
ceedings of the International Conference on Spoken
Language Processing (ICSLP).
M. Mohri, F.C.N. Pereira, and M. Riley. 2000. The de-
sign principles of a weighted finite-state transducer li-
brary. Theoretical Computer Science, 231:17-32.
A. Ogawa, K. Takeda, and F. Itakura. 1998. Balanc-
ing Acoustic and Linguistic Probabilities. In Proc. of
IEEE International Conference on Acoustics, Speech,
and Signal Processing (ICASSP).
</reference>
<page confidence="0.86684">
1126
</page>
<reference confidence="0.999763757575758">
F. Richardson, M. Ostendorf, and J.R. Rohlicek. 1995.
Lattice-based search strategies for large vocabulary
speech recognition. In Proc. of IEEE International
Conference on Acoustics, Speech, and Signal Process-
ing (ICASSP).
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249–276.
Roni Rosenfeld, Stanley F. Chen, and Xiaojin Zhu. 2001.
Whole-Sentence Exponential Language Models: a Ve-
hicle for Linguistic-Statistical Integration. Computer
Speech and Language, 15(1).
Roni Rosenfeld. 1997. A Whole Sentence Maximum
Entropy Language Model. In Proc. of IEEE workshop
on Automatic Speech Recognition and Understanding
(ASRU), Santa Barbara, California, December.
D.E. Rumelhart, G. E. Hinton, and R.J. Williams. 1986.
Learning representations by back-propagating errors.
Nature, 323:533–536.
Holger Schwenk. 2007. Continuous space language
models. Computer Speech and Language, 21(3):492–
518.
C. E. Shannon. 1948. A Mathematical Theory of
Communication. The Bell System Technical Journal,
27:379–423, 623–656.
H. Soltau, G. Saon, and B. Kingsbury. 2010. The IBM
Attila speech recognition toolkit. In Proc. of IEEE
Workshop on Spoken Language Technology (SLT).
Wen Wang and Mary Harper. 2002. The SuperARV lan-
guage model: investigating the effectiveness of tightly
integrating multiple knowledge sources. In Proceed-
ings of the Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP).
</reference>
<page confidence="0.995212">
1127
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.877572">
<title confidence="0.999958">A Fast Re-scoring Strategy to Capture Long-Distance Dependencies</title>
<author confidence="0.976006">Anoop Deoras Tom´aˇs Mikolov Kenneth Church</author>
<affiliation confidence="0.965268">HLT-COE and CLSP Brno University of Technology HLT-COE and CLSP Johns Hopkins University Speech@FIT Johns Hopkins University</affiliation>
<address confidence="0.997318">Baltimore MD 21218, USA Czech Republic Baltimore MD 21218,</address>
<email confidence="0.999548">adeoras@jhu.edu,imikolov@fit.vutbr.cz,kenneth.church@jhu.edu</email>
<abstract confidence="0.997868625">A re-scoring strategy is proposed that makes it feasible to capture more long-distance dependencies in the natural language. Two pass strategies have become popular in a number of recognition tasks such as ASR (automatic speech recognition), MT (machine translation) and OCR (optical character recognition). The first pass typically applies a language model to a lattice and the second pass applies a stronger lanmodel to lists. The stronger language model is intended to capture more longdistance dependencies. The proposed method uses RNN-LM (recurrent neural network language model), which is a long span LM, to rescore word lattices in the second pass. A hill climbing method (iterative decoding) is proto search over of confusability in the word lattice. An evaluation based on News shows speedups of re-scoring, and word error rate of on a highly competitive setup.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Cyril Allauzen</author>
<author>Michael Riley</author>
<author>Johan Schalkwyk</author>
<author>Wojciech Skut</author>
<author>Mehryar Mohri</author>
</authors>
<title>OpenFst: A General and Efficient Weighted Finite-State Transducer Library.</title>
<date>2007</date>
<booktitle>In Proceedings of the Ninth International Conference on Implementation and Application of Automata, (CIAA</booktitle>
<volume>4783</volume>
<pages>11--23</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="33909" citStr="Allauzen et al., 2007" startWordPosition="5933" endWordPosition="5936">an average. The proposed method obtains the same reduction by evaluating 14 times smaller search space. The search effort reduces further to 40 times if entropy based pruning is employed during re-scoring. bination of KN:BN-Big and RNN-limited by KNRNN-lim; KN:BN-Big and RNN-all by KN-RNNall and KN:BN-Big, RNN-limited and RNN-all by KN-RNN-lim-all. We used two sets for decoding: rt03+dev04f set was used as a development set while rt04 was used as a blind set for the purpose of evaluating the performance of long span RNN models using the proposed approach. We made use of OpenFst C++ libraries (Allauzen et al., 2007) for manipulating lattice graphs and generating N best lists. Due to the presence of hesitation tokens in reference transcripts and the need to access the silence/pause tokens for penalizing short sentences, we treated these tokens as regular words before extracting sentence hypotheses. This, and poorly segmented nature of the test corpora, led to huge enumeration of sentence hypotheses. 5.1 n-gram LM for re-scoring In this setup, we used KN:BN-Small as the baseline starting LM which yielded the WER of 12% on rt03+dev04f set. Using KN:BN-Big as the rescoring LM, the WER dropped to 11%. Since t</context>
</contexts>
<marker>Allauzen, Riley, Schalkwyk, Skut, Mohri, 2007</marker>
<rawString>Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wojciech Skut, and Mehryar Mohri. 2007. OpenFst: A General and Efficient Weighted Finite-State Transducer Library. In Proceedings of the Ninth International Conference on Implementation and Application of Automata, (CIAA 2007), volume 4783 of Lecture Notes in Computer Science, pages 11–23. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Bellegarda</author>
</authors>
<title>Exploiting latent semantic information in statistical language modeling.</title>
<date>2000</date>
<booktitle>Proceedings of IEEE,</booktitle>
<pages>88--8</pages>
<contexts>
<context position="4135" citStr="Bellegarda, 2000" startWordPosition="670" endWordPosition="671"> we could hope to capture more, as well. However, due to data sparsity, it is hard to estimate a robust n-gram distribution for large values of n ( say, n &gt; 10) using the conventional Maximum Likelihood techniques, unless a more robust technique is employed for modeling which generalizes well on unseen events. Some of these well known long span / complex language models which have shown to perform very well on many speech tasks include: structured language model (Chelba and Jelinek, 2000; Roark, 2001; Wang and Harper, 2002; Filimonov and Harper, 2009), latent semantic analysis language model (Bellegarda, 2000), topic mixture language models (Iyer and Ostendorf, 1999), whole sentence exponential language models (Rosenfeld, 1997; Rosenfeld et al., 2001), feedforward neural networks (Bengio et al., 2001), recurrent neural network language models (Mikolov et al., 2010), among many others. Although better modeling techniques can now capture longer dependencies in a language, their incorporation in decoders of speech recognition or machine translation systems becomes computationally challenging. Due to the prohibitive increase in the search space of sentence hypotheses (or longer length word sub sequence</context>
</contexts>
<marker>Bellegarda, 2000</marker>
<rawString>J. R. Bellegarda. 2000. Exploiting latent semantic information in statistical language modeling. Proceedings of IEEE, 88(8):1279–1296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
</authors>
<title>A Neural Probabilistic Language Model.</title>
<date>2001</date>
<booktitle>In Proceedings of Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="4330" citStr="Bengio et al., 2001" startWordPosition="696" endWordPosition="699">Likelihood techniques, unless a more robust technique is employed for modeling which generalizes well on unseen events. Some of these well known long span / complex language models which have shown to perform very well on many speech tasks include: structured language model (Chelba and Jelinek, 2000; Roark, 2001; Wang and Harper, 2002; Filimonov and Harper, 2009), latent semantic analysis language model (Bellegarda, 2000), topic mixture language models (Iyer and Ostendorf, 1999), whole sentence exponential language models (Rosenfeld, 1997; Rosenfeld et al., 2001), feedforward neural networks (Bengio et al., 2001), recurrent neural network language models (Mikolov et al., 2010), among many others. Although better modeling techniques can now capture longer dependencies in a language, their incorporation in decoders of speech recognition or machine translation systems becomes computationally challenging. Due to the prohibitive increase in the search space of sentence hypotheses (or longer length word sub sequences), it becomes challenging to use a long span language model in the first pass decoding. A word graph (word lattices for speech recognition systems and hypergraphs for machine translation systems</context>
<context position="6893" citStr="Bengio et al. (2001)" startWordPosition="1107" endWordPosition="1110">in Sec. 2. In Sec. 3 we discuss two standard re-scoring techniques and then describe and demonstrate our proposed technique in Sec. 4. We present experimental results in Sec. 5 followed by conclusions and some remarks in Sec. 6. 2 Recurrent Neural Networks (RNN) There is a long history of using neural networks to model sequences. Elman (1990) used recurrent neural network for modeling sentences of words generated by an artificial grammar. Work on statistical language modeling of real natural language data, together with an empirical comparison of performance to standard techniques was done by Bengio et al. (2001). His work has been followed by Schwenk (2007), who has shown that neural network language models actually work very well in the state-of-theart speech recognition systems. Recurrent Neural Network based Language Models (RNN-LMs) (Mikolov et al., 2010) improved the ability of the original model to capture patterns in the language without using any additional features (such as part of speech, morphology etc) i.e. other than lexical ones. The RNN-LM was shown to have superior performance than the original feedforward neural network (Mikolov et al., 2011b). Recently, we also showed that this mode</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, 2001</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, and Pascal Vincent. 2001. A Neural Probabilistic Language Model. In Proceedings of Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Beyerlein</author>
</authors>
<title>Discriminative Model Combination. In</title>
<date>1998</date>
<booktitle>Proc. of IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP).</booktitle>
<contexts>
<context position="32492" citStr="Beyerlein (1998)" startWordPosition="5684" endWordPosition="5685">bset of the training data (58M tokens). It used 400 neurons in the hidden layer. The second model, denoted as RNN-all, was trained on all of the training data (400M tokens), but due to the computational complexity issues, we had to restrict its hidden layer size to 320 neurons. We followed IBM’s multi-pass decoding recipe using KN:BN-Small in the first pass followed by either N best list re-scoring or word lattice re-scoring using bigger and better models.7 For the purpose of re-scoring, we combined all the relevant statistical models in one unified log linear framework reminiscent of work by Beyerlein (1998). We, however, trained the model weights by optimizing expected WER rather than 1-best loss as described in (Deoras et al., 2010). Training was done on N best lists of size 2K. We will refer to the log linear com7The choice of the order and size of LM to be used in the first pass decoding was determined by taking into consideration the capabilities of the decoder. 1123 Plot of 1 best WER v/s Search Space Size Size of Search Space (Number of Hypotheses for evaluation) Figure 2: Plot of WER (y axis) on rt03+dev04f set versus the size of the search space (x axis). The baseline WER obtained using </context>
</contexts>
<marker>Beyerlein, 1998</marker>
<rawString>Peter Beyerlein. 1998. Discriminative Model Combination. In Proc. of IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher M Bishop</author>
</authors>
<date>2006</date>
<booktitle>Pattern Recognition and Machine Learning.</booktitle>
<publisher>Springer.</publisher>
<contexts>
<context position="5430" citStr="Bishop, 2006" startWordPosition="862" endWordPosition="863">ding. A word graph (word lattices for speech recognition systems and hypergraphs for machine translation systems), encoding exponential number of hypotheses is hence outputted at the first pass output on which a sophisticated and complex language model is deployed for re-scoring. However, sometimes even re-scoring of this refined search space can be computationally expensive due to explosion of state space. Previously, we showed in (Deoras et al., 2011) how to tackle the problem of incorporating long span information during decoding in speech recognition systems by variationaly approximating (Bishop, 2006, pp. 462) the long span language model by a tractable substitute such that this substitute model comes closest to the long span model (closest in terms of Kullback Leibler Divergence (Cover and J.A.Thomas, 1991, pp. 20)). The tractable substitute was then used directly in the first pass speech recognition systems. In this paper we propose an approach that keeps the model intact but approximates the search space instead (which can become intractable to handle especially under a long span model), thus enabling the use of full blown model for re-scoring.With this approach, we can achieve full la</context>
</contexts>
<marker>Bishop, 2006</marker>
<rawString>Christopher M. Bishop. 2006. Pattern Recognition and Machine Learning. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>Frederick Jelinek</author>
</authors>
<title>Structured Language Modeling.</title>
<date>2000</date>
<journal>Computer Speech and Language,</journal>
<volume>14</volume>
<issue>4</issue>
<contexts>
<context position="4010" citStr="Chelba and Jelinek, 2000" startWordPosition="650" endWordPosition="653">July 27–31, 2011. c�2011 Association for Computational Linguistics language. In addition, if we can move to more general models then we could hope to capture more, as well. However, due to data sparsity, it is hard to estimate a robust n-gram distribution for large values of n ( say, n &gt; 10) using the conventional Maximum Likelihood techniques, unless a more robust technique is employed for modeling which generalizes well on unseen events. Some of these well known long span / complex language models which have shown to perform very well on many speech tasks include: structured language model (Chelba and Jelinek, 2000; Roark, 2001; Wang and Harper, 2002; Filimonov and Harper, 2009), latent semantic analysis language model (Bellegarda, 2000), topic mixture language models (Iyer and Ostendorf, 1999), whole sentence exponential language models (Rosenfeld, 1997; Rosenfeld et al., 2001), feedforward neural networks (Bengio et al., 2001), recurrent neural network language models (Mikolov et al., 2010), among many others. Although better modeling techniques can now capture longer dependencies in a language, their incorporation in decoders of speech recognition or machine translation systems becomes computationall</context>
</contexts>
<marker>Chelba, Jelinek, 2000</marker>
<rawString>Ciprian Chelba and Frederick Jelinek. 2000. Structured Language Modeling. Computer Speech and Language, 14(4):283–332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S F Chen</author>
<author>L Mangu</author>
<author>B Ramabhadran</author>
<author>R Sarikaya</author>
<author>A Sethy</author>
</authors>
<title>Scaling shrinkage-based language models.</title>
<date>2009</date>
<booktitle>In Proc. of IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU),</booktitle>
<pages>299--304</pages>
<contexts>
<context position="31353" citStr="Chen et al., 2009" startWordPosition="5488" endWordPosition="5491"> below some threshold. Thus, believing in the original model’s confidence, we want to focus only on those sub lattices which the recognizer found difficult to decode in the first pass. All other part of the parent lattice will be not be analyzed. The thresholds for pruning is very application and corpus specific and needs to be tuned on some held out data. 5 Experiments and Results We performed recognition on the Broadcast News (BN) dev04f, rt03 and rt04 task using the stateof-the-art acoustic models trained on the English Broadcast News (BN) corpus (430 hours of audio) provided to us by IBM (Chen et al., 2009). IBM also provided us its state-of-the-art speech recognizer, Attila (Soltau et al., 2010) and two Kneser-Ney smoothed backoff n-gram LMs containing 4.7M ngrams (n &lt; 4) and 54M n-grams (n &lt; 4), both trained on 400M word tokens. We will refer to them as KN:BN-Small and KN:BN-Big respectively. We refer readers to (Chen et al., 2009) for more details about the recognizer and corpora used for training the models. We trained two RNN based language models - the first one, denoted further as RNN-limited, was trained on a subset of the training data (58M tokens). It used 400 neurons in the hidden lay</context>
</contexts>
<marker>Chen, Mangu, Ramabhadran, Sarikaya, Sethy, 2009</marker>
<rawString>S. F. Chen, L. Mangu, B. Ramabhadran, R. Sarikaya, and A. Sethy. 2009. Scaling shrinkage-based language models. In Proc. of IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), pages 299–304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>Syntactic Structures.</title>
<date>1957</date>
<publisher>The Hague: Mouton.</publisher>
<contexts>
<context position="2208" citStr="Chomsky, 1957" startWordPosition="350" endWordPosition="351">slation) and OCR (optical character recognition). The task of a language model is to assign probability to any word sequence possible in the language. The probability of the word sequence W ≡ w1, ... , wm ≡ wm1 is typically factored using the chain rule: P(wi|wi−1 1 ) (1) In modern statistical recognition systems, an LM tends to be restricted to simple n-gram models, where the distribution of the predicted word depends on the previous (n − 1) words i.e. P(wi|wi−1 1 ) ≈ P (wi|wi−1 i−n+1). Noam Chomsky argued that n-grams cannot learn long-distance dependencies that span over more than n words (Chomsky, 1957, pp.13). While that might seem obvious in retrospect, there was a lot of excitement at the time over the Shannon-McMillanBreiman Theorem (Shannon, 1948) which was interpreted to say that, in the limit, under just a couple of minor caveats and a little bit of not-very-important fine print, n-gram statistics are sufficient to capture all the information in a string (such as an English sentence). Chomsky realized that while that may be true in the limit, n-grams are far from the most parsimonious representation of many linguistic facts. In a practical system, we will have to truncate n-grams at </context>
</contexts>
<marker>Chomsky, 1957</marker>
<rawString>Noam Chomsky. 1957. Syntactic Structures. The Hague: Mouton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yen-Lu Chow</author>
<author>Richard Schwartz</author>
</authors>
<title>The N-Best algorithm: an efficient procedure for finding top N sentence hypotheses.</title>
<date>1989</date>
<booktitle>In Proceedings of the workshop on Speech and Natural Language, HLT ’89,</booktitle>
<pages>199--202</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="14562" citStr="Chow and Schwartz, 1989" startWordPosition="2471" endWordPosition="2474">s utilized. The details of this method are presented next. 3.2 N best List Rescoring N best list re-scoring is a popular way to capture some long-distance dependencies, though the method can be slow and it can be biased toward the weaker language model that was used in the first pass. Given a word lattice, L, top N paths {7r1, ... , 7rNJ are extracted such that their joint likelihood under the baseline acoustic and language models are in descending order i.e. that: A[7r1]&apos;&apos;L[7r1] &gt; A[7r2]&apos;&apos;L[7r2] &gt; ... &gt; A[7rN]&apos;&apos;L[7rN] Efficient algorithms exist for extracting N best paths from word lattices (Chow and Schwartz, 1989; Mohri and Riley, 2002). If a new language model, Lnew, is provided, which now need not be restricted to finite state machine family, then that can be deployed to get the score of the entire path 7r. If we denote the new LM scores by Lnew[], then under N best list paradigm, optimal path 7r˜ is found out such that: 7r˜ = arg max A[7r]ηLnew[7r], (2) 7rE17r1,...,7rN } where q acts as the new scaling parameter which may or may not be equal to -y. If N « |L |(where |L |is the total number of complete paths in word lattice, which are exponentially many), then the path obtained using (2) is not guar</context>
</contexts>
<marker>Chow, Schwartz, 1989</marker>
<rawString>Yen-Lu Chow and Richard Schwartz. 1989. The N-Best algorithm: an efficient procedure for finding top N sentence hypotheses. In Proceedings of the workshop on Speech and Natural Language, HLT ’89, pages 199– 202, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Church</author>
</authors>
<title>A Pendulum Swung Too Far. Linguistic Issues in Language Technology - LiLT.</title>
<date>2012</date>
<note>to appear.</note>
<contexts>
<context position="3233" citStr="Church (2012)" startWordPosition="522" endWordPosition="523">ed that while that may be true in the limit, n-grams are far from the most parsimonious representation of many linguistic facts. In a practical system, we will have to truncate n-grams at some (small) fixed n (such as trigrams or perhaps 5-grams). Truncated n-gram systems can capture many agreement facts, but not all.1 By long-distance dependencies, we mean facts like agreement and collocations that can span over many words. With increasing order of n-gram models we can, in theory, capture more regularities in the 1The discussion in this paragraph is taken as-is from an article (to appear) by Church (2012). m P(wm1 ) = i=1 1116 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1116–1127, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics language. In addition, if we can move to more general models then we could hope to capture more, as well. However, due to data sparsity, it is hard to estimate a robust n-gram distribution for large values of n ( say, n &gt; 10) using the conventional Maximum Likelihood techniques, unless a more robust technique is employed for modeling which generalizes well on unseen events. Som</context>
</contexts>
<marker>Church, 2012</marker>
<rawString>Kenneth Church. 2012. A Pendulum Swung Too Far. Linguistic Issues in Language Technology - LiLT. to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T M Cover</author>
<author>J A Thomas</author>
</authors>
<title>Elements of Information Theory.</title>
<date>1991</date>
<publisher>John Wiley and Sons, Inc. N.Y.</publisher>
<marker>Cover, Thomas, 1991</marker>
<rawString>T.M. Cover and J.A.Thomas. 1991. Elements of Information Theory. John Wiley and Sons, Inc. N.Y.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anoop Deoras</author>
<author>Frederick Jelinek</author>
</authors>
<title>Iterative Decoding: A Novel Re-Scoring Framework for Confusion Networks. In</title>
<date>2009</date>
<booktitle>Proc. of IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU),</booktitle>
<pages>282--286</pages>
<contexts>
<context position="19115" citStr="Deoras and Jelinek, 2009" startWordPosition="3275" endWordPosition="3278">aligned together typically shows a pattern of confusions both at the word level and at the phrase level. Regions where there are singleton words competing with one another (reminiscent of a confusion bin of a Confusion Network (CN) (Mangu, 2000)), choice of 1 word edit distance works well for the formation of local neighborhood. Regions where there are phrases competing with other phrases, choice of variable length neighborhood works well. Previously, Richardson et al. (1995) demonstrated a hill climbing framework by exploring 1 word edit distance neighborhood, while in our own previous work (Deoras and Jelinek, 2009), we demonstrated working of iterative decoding algorithm, a hill climbing framework, for CNs, in which the neighborhood was formed by all words competing with each other in any given time slot, as defined by a confusion bin. In this work, we propose a technique which generalizes very well on word lattices and overcomes the limitations posed by a CN or by the limited nature of local neighborhood. The size of the neighborhood in our approach is a variable factor which depends upon the confusability in any particular region of the word lattice. Thus the local neighborhood are in some sense a fun</context>
</contexts>
<marker>Deoras, Jelinek, 2009</marker>
<rawString>Anoop Deoras and Frederick Jelinek. 2009. Iterative Decoding: A Novel Re-Scoring Framework for Confusion Networks. In Proc. of IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), pages 282 –286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anoop Deoras</author>
<author>Denis Filimonov</author>
<author>Mary Harper</author>
<author>Fred Jelinek</author>
</authors>
<title>Model Combination for Speech Recognition using Empirical Bayes Risk Minimization.</title>
<date>2010</date>
<booktitle>In Proc. of IEEE Workshop on Spoken Language Technology (SLT).</booktitle>
<contexts>
<context position="15802" citStr="Deoras et al., 2010" startWordPosition="2703" endWordPosition="2706"> (under the rescoring model). The short list of hypotheses so used for re-scoring would yield suboptimal output if the best path 7r* (according to the new model) is not present among the top N candidates extracted from the lattice. This search space is thus said to be biased towards a weaker model mainly because the N best lists are representative of the model generating them. To illustrate the idea, we demonstrate below a simple analysis on a relatively easy task of speech transcription on WSJ data.4 In this setup, the recognizer made use of a bi4Full details about the setup can be found in (Deoras et al., 2010) 1119 gram LM to produce lattices and hence N best lists. Each hypothesis in this set got a rank with the top most and highest scoring hypothesis getting a rank of 1, while the bottom most hypothesis getting a rank of N. We then re-scored these hypotheses with a better language model (either with a higher order Markov LM i.e. a trigram LM (tg) or the log linear combination of n-gram models and syntactic models (n-gram+syntactic) and re-ranked the hypotheses to obtain their new ranks. We then used Spearman’s rank correlation factor, p, which takes values in [−1,+1], with −1 meaning that the two</context>
<context position="32621" citStr="Deoras et al., 2010" startWordPosition="5703" endWordPosition="5707">ained on all of the training data (400M tokens), but due to the computational complexity issues, we had to restrict its hidden layer size to 320 neurons. We followed IBM’s multi-pass decoding recipe using KN:BN-Small in the first pass followed by either N best list re-scoring or word lattice re-scoring using bigger and better models.7 For the purpose of re-scoring, we combined all the relevant statistical models in one unified log linear framework reminiscent of work by Beyerlein (1998). We, however, trained the model weights by optimizing expected WER rather than 1-best loss as described in (Deoras et al., 2010). Training was done on N best lists of size 2K. We will refer to the log linear com7The choice of the order and size of LM to be used in the first pass decoding was determined by taking into consideration the capabilities of the decoder. 1123 Plot of 1 best WER v/s Search Space Size Size of Search Space (Number of Hypotheses for evaluation) Figure 2: Plot of WER (y axis) on rt03+dev04f set versus the size of the search space (x axis). The baseline WER obtained using KN:BN-Small is 12% which then drops to 11% when KN:BN-Big is used for re-scoring. N best list search method obtains the same redu</context>
</contexts>
<marker>Deoras, Filimonov, Harper, Jelinek, 2010</marker>
<rawString>Anoop Deoras, Denis Filimonov, Mary Harper, and Fred Jelinek. 2010. Model Combination for Speech Recognition using Empirical Bayes Risk Minimization. In Proc. of IEEE Workshop on Spoken Language Technology (SLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anoop Deoras</author>
<author>Tom´as Mikolov</author>
<author>Stefan Kombrink</author>
<author>Martin Karafi´at</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Variational Approximation of Long-Span Language Models for LVCSR.</title>
<date>2011</date>
<booktitle>In Proc. of IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP).</booktitle>
<marker>Deoras, Mikolov, Kombrink, Karafi´at, Khudanpur, 2011</marker>
<rawString>Anoop Deoras, Tom´a&amp;quot;s Mikolov, Stefan Kombrink, Martin Karafi´at, and Sanjeev Khudanpur. 2011. Variational Approximation of Long-Span Language Models for LVCSR. In Proc. of IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffery Elman</author>
</authors>
<title>Finding Structure in Time.</title>
<date>1990</date>
<journal>In Cognitive Science,</journal>
<volume>14</volume>
<pages>179--211</pages>
<contexts>
<context position="6617" citStr="Elman (1990)" startWordPosition="1065" endWordPosition="1066">h, we can achieve full lattice re-scoring with a complex model, at a cost more than 20 times less than of a naive brute force approach that is commonly used today. The rest of the paper is organized as follows: We discuss a particular form of long span language model in Sec. 2. In Sec. 3 we discuss two standard re-scoring techniques and then describe and demonstrate our proposed technique in Sec. 4. We present experimental results in Sec. 5 followed by conclusions and some remarks in Sec. 6. 2 Recurrent Neural Networks (RNN) There is a long history of using neural networks to model sequences. Elman (1990) used recurrent neural network for modeling sentences of words generated by an artificial grammar. Work on statistical language modeling of real natural language data, together with an empirical comparison of performance to standard techniques was done by Bengio et al. (2001). His work has been followed by Schwenk (2007), who has shown that neural network language models actually work very well in the state-of-theart speech recognition systems. Recurrent Neural Network based Language Models (RNN-LMs) (Mikolov et al., 2010) improved the ability of the original model to capture patterns in the l</context>
</contexts>
<marker>Elman, 1990</marker>
<rawString>Jeffery Elman. 1990. Finding Structure in Time. In Cognitive Science, volume 14, pages 179–211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Denis Filimonov</author>
<author>Mary Harper</author>
</authors>
<title>A Joint Language Model with Fine-grain Syntactic Tags.</title>
<date>2009</date>
<booktitle>In Proc. of 2009 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="4075" citStr="Filimonov and Harper, 2009" startWordPosition="660" endWordPosition="663">tics language. In addition, if we can move to more general models then we could hope to capture more, as well. However, due to data sparsity, it is hard to estimate a robust n-gram distribution for large values of n ( say, n &gt; 10) using the conventional Maximum Likelihood techniques, unless a more robust technique is employed for modeling which generalizes well on unseen events. Some of these well known long span / complex language models which have shown to perform very well on many speech tasks include: structured language model (Chelba and Jelinek, 2000; Roark, 2001; Wang and Harper, 2002; Filimonov and Harper, 2009), latent semantic analysis language model (Bellegarda, 2000), topic mixture language models (Iyer and Ostendorf, 1999), whole sentence exponential language models (Rosenfeld, 1997; Rosenfeld et al., 2001), feedforward neural networks (Bengio et al., 2001), recurrent neural network language models (Mikolov et al., 2010), among many others. Although better modeling techniques can now capture longer dependencies in a language, their incorporation in decoders of speech recognition or machine translation systems becomes computationally challenging. Due to the prohibitive increase in the search spac</context>
</contexts>
<marker>Filimonov, Harper, 2009</marker>
<rawString>Denis Filimonov and Mary Harper. 2009. A Joint Language Model with Fine-grain Syntactic Tags. In Proc. of 2009 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Goel</author>
<author>W Byrne</author>
</authors>
<title>Minimum Bayes Risk Automatic Speech Recognition. Computer, Speech and Language.</title>
<date>2000</date>
<contexts>
<context position="21799" citStr="Goel and Byrne (2000)" startWordPosition="3776" endWordPosition="3779"> [u] = T [v]. We will define this common time stamp of the nodes in the set by T [Si]. 2. A 7r E £ such that Ts[7r] &lt; T [Si] &lt; Te[7r]. The first property can be easily checked by first pushing states into a linked list associated with each time marker (this can be done by iterating over all the states of the graph) then iterating over the unique time markers and retrieving back the nodes associated with it.The second property can be checked by first iterating over the unique time markers and for each of the marker, iterating over the arcs and terminating the loop as soon as some arc is found 5Goel and Byrne (2000) previously demonstrated the lattice segmentation procedure to solve the intractable problem of MBR decoding. The cutting procedure in our work is different from theirs in the sense that we rely on time information for collating competing phrases, while they do not. out violating property 2 for the specific time marker. Thus the time complexity for checking property 1 is O(|V|) and that for property 2 is O(|T |x|£|), where |T |is the total number of unique time markers. Usually |T |« |£ |and hence the time complexity for checking property 2 is almost linear in the number of edges. Thus effecti</context>
</contexts>
<marker>Goel, Byrne, 2000</marker>
<rawString>V. Goel and W. Byrne. 2000. Minimum Bayes Risk Automatic Speech Recognition. Computer, Speech and Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rukmini Iyer</author>
<author>Mari Ostendorf</author>
</authors>
<title>Modeling Long Distance Dependence in Language: Topic Mixtures Versus Dynamic Cache Models.</title>
<date>1999</date>
<booktitle>IEEE Transactions on Speech and Audio Processing,</booktitle>
<volume>7</volume>
<issue>1</issue>
<contexts>
<context position="4193" citStr="Iyer and Ostendorf, 1999" startWordPosition="676" endWordPosition="679">due to data sparsity, it is hard to estimate a robust n-gram distribution for large values of n ( say, n &gt; 10) using the conventional Maximum Likelihood techniques, unless a more robust technique is employed for modeling which generalizes well on unseen events. Some of these well known long span / complex language models which have shown to perform very well on many speech tasks include: structured language model (Chelba and Jelinek, 2000; Roark, 2001; Wang and Harper, 2002; Filimonov and Harper, 2009), latent semantic analysis language model (Bellegarda, 2000), topic mixture language models (Iyer and Ostendorf, 1999), whole sentence exponential language models (Rosenfeld, 1997; Rosenfeld et al., 2001), feedforward neural networks (Bengio et al., 2001), recurrent neural network language models (Mikolov et al., 2010), among many others. Although better modeling techniques can now capture longer dependencies in a language, their incorporation in decoders of speech recognition or machine translation systems becomes computationally challenging. Due to the prohibitive increase in the search space of sentence hypotheses (or longer length word sub sequences), it becomes challenging to use a long span language mod</context>
</contexts>
<marker>Iyer, Ostendorf, 1999</marker>
<rawString>Rukmini Iyer and Mari Ostendorf. 1999. Modeling Long Distance Dependence in Language: Topic Mixtures Versus Dynamic Cache Models. IEEE Transactions on Speech and Audio Processing, 7(1):30–39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Jason Eisner</author>
</authors>
<title>First- and second-order expectation semirings with applications to minimumrisk training on translation forests.</title>
<date>2009</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>40--51</pages>
<location>Singapore,</location>
<contexts>
<context position="30093" citStr="Li and Eisner (2009)" startWordPosition="5267" endWordPosition="5270">from a bi-gram language model (bg) and rescored with relatively better language models (see Table 1 for model definitions). Entropy under the baseline model correlates well with the rank correlation factor, suggesting that exhaustive search need not be necessary for utterances yielding lower entropy. While computation of entropy for N best list is tractable, for a word lattice, the computation of entropy is intractable if one were to enumerate all the hypotheses. Even if we were able to enumerate all hypotheses, this method tends to be slower. Using efficient semiring techniques introduced by Li and Eisner (2009) or using posterior probabilities on the edges leading to end states, we can compute the entropy of a lattice in one single forward pass using dynamic programming. It should, however, be noted that, for dynamic programming technique to work, only n-gram LMs can be used. One has to resort to approximate entropy computation via N best list, if entropy under long span LM is desired. 4.3.1 Speed Up for Iterative Decoding Our speed up technique is simple. Once we have formed self contained sub lattices, we want to prune all but the top few best complete paths (obtained under baseline / starting mod</context>
</contexts>
<marker>Li, Eisner, 2009</marker>
<rawString>Zhifei Li and Jason Eisner. 2009. First- and second-order expectation semirings with applications to minimumrisk training on translation forests. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 40–51, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lidia Luminita Mangu</author>
</authors>
<title>Finding consensus in speech recognition.</title>
<date>2000</date>
<tech>Ph.D. thesis,</tech>
<publisher>Adviser-Brill, Eric.</publisher>
<institution>The Johns Hopkins University.</institution>
<contexts>
<context position="18735" citStr="Mangu, 2000" startWordPosition="3215" endWordPosition="3216">ands of confusability in the word lattice and replace the problem of global search over word lattice by series of local search problems over these islands in an iterative manner. The motivation behind this strategy is the observation that the recognizer produces bursts of errors such that they have a temporal scope. The recognizer output (sentence hypotheses) when aligned together typically shows a pattern of confusions both at the word level and at the phrase level. Regions where there are singleton words competing with one another (reminiscent of a confusion bin of a Confusion Network (CN) (Mangu, 2000)), choice of 1 word edit distance works well for the formation of local neighborhood. Regions where there are phrases competing with other phrases, choice of variable length neighborhood works well. Previously, Richardson et al. (1995) demonstrated a hill climbing framework by exploring 1 word edit distance neighborhood, while in our own previous work (Deoras and Jelinek, 2009), we demonstrated working of iterative decoding algorithm, a hill climbing framework, for CNs, in which the neighborhood was formed by all words competing with each other in any given time slot, as defined by a confusion</context>
</contexts>
<marker>Mangu, 2000</marker>
<rawString>Lidia Luminita Mangu. 2000. Finding consensus in speech recognition. Ph.D. thesis, The Johns Hopkins University. Adviser-Brill, Eric.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom´as Mikolov</author>
<author>Martin Karafi´at</author>
<author>Luk´as Burget</author>
<author>Jan “Honza” Cernock´y</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Recurrent Neural Network Based Language Model.</title>
<date>2010</date>
<booktitle>In Proc. of the ICSLP-Interspeech.</booktitle>
<marker>Mikolov, Karafi´at, Burget, Cernock´y, Khudanpur, 2010</marker>
<rawString>Tom´a&amp;quot;s Mikolov, Martin Karafi´at, Luk´a&amp;quot;s Burget, Jan “Honza” &amp;quot;Cernock´y, and Sanjeev Khudanpur. 2010. Recurrent Neural Network Based Language Model. In Proc. of the ICSLP-Interspeech.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Tom´as Mikolov</author>
<author>Anoop Deoras</author>
<author>Stefan Kombrink</author>
<author>Luk´as Burget</author>
<author>Jan “Honza”</author>
</authors>
<title>Cernock´y. 2011a. Empirical Evaluation and Combination of Advanced Language Modeling Techniques.</title>
<booktitle>In Proc. of Interspeech.</booktitle>
<marker>Mikolov, Deoras, Kombrink, Burget, “Honza”, </marker>
<rawString>Tom´a&amp;quot;s Mikolov, Anoop Deoras, Stefan Kombrink, Luk´a&amp;quot;s Burget, and Jan “Honza” &amp;quot;Cernock´y. 2011a. Empirical Evaluation and Combination of Advanced Language Modeling Techniques. In Proc. of Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom´as Mikolov</author>
<author>Stefan Kombrink</author>
<author>Luk´as Burget</author>
</authors>
<title>Honza” &amp;quot;Cernock´y, and Sanjeev Khudanpur. 2011b. Extensions of Recurrent Neural Network Language Model.</title>
<date></date>
<booktitle>In Proc. of IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP).</booktitle>
<marker>Mikolov, Kombrink, Burget, </marker>
<rawString>Tom´a&amp;quot;s Mikolov, Stefan Kombrink, Luk´a&amp;quot;s Burget, Jan “Honza” &amp;quot;Cernock´y, and Sanjeev Khudanpur. 2011b. Extensions of Recurrent Neural Network Language Model. In Proc. of IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
<author>Michael Riley</author>
</authors>
<title>An Efficient Algorithm for the N-Best-Strings Problem.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing (ICSLP).</booktitle>
<contexts>
<context position="14586" citStr="Mohri and Riley, 2002" startWordPosition="2475" endWordPosition="2478">f this method are presented next. 3.2 N best List Rescoring N best list re-scoring is a popular way to capture some long-distance dependencies, though the method can be slow and it can be biased toward the weaker language model that was used in the first pass. Given a word lattice, L, top N paths {7r1, ... , 7rNJ are extracted such that their joint likelihood under the baseline acoustic and language models are in descending order i.e. that: A[7r1]&apos;&apos;L[7r1] &gt; A[7r2]&apos;&apos;L[7r2] &gt; ... &gt; A[7rN]&apos;&apos;L[7rN] Efficient algorithms exist for extracting N best paths from word lattices (Chow and Schwartz, 1989; Mohri and Riley, 2002). If a new language model, Lnew, is provided, which now need not be restricted to finite state machine family, then that can be deployed to get the score of the entire path 7r. If we denote the new LM scores by Lnew[], then under N best list paradigm, optimal path 7r˜ is found out such that: 7r˜ = arg max A[7r]ηLnew[7r], (2) 7rE17r1,...,7rN } where q acts as the new scaling parameter which may or may not be equal to -y. If N « |L |(where |L |is the total number of complete paths in word lattice, which are exponentially many), then the path obtained using (2) is not guaranteed to be optimal (un</context>
</contexts>
<marker>Mohri, Riley, 2002</marker>
<rawString>Mehryar Mohri and Michael Riley. 2002. An Efficient Algorithm for the N-Best-Strings Problem. In Proceedings of the International Conference on Spoken Language Processing (ICSLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Mohri</author>
<author>F C N Pereira</author>
<author>M Riley</author>
</authors>
<title>The design principles of a weighted finite-state transducer library.</title>
<date>2000</date>
<journal>Theoretical Computer Science,</journal>
<pages>231--17</pages>
<contexts>
<context position="12100" citStr="Mohri et al., 2000" startWordPosition="2031" endWordPosition="2034">e such that at each state there exists an unambiguous context of consecutive m − 1 words. A first pass output is then a path 7r* having Maximum a Posterior (MAP) probability.3 Thus 7r* is 2We will use O symbol to denote concatenation of paths or word strings. 3Note that asterisk symbol here connotes that the path is op1118 obtained as: 7r* = arg max A[7r]&apos;&apos;L[7r], 7r:p[7r]=ns n[7r]ENe where -y is the scaling parameter needed to balance the dynamic variability between the distributions of acoustic and language model (Ogawa et al., 1998). Efficient algorithms such as single source shortest path (Mohri et al., 2000) can be used for finding out the MAP path. Under a new n-gram Language Model, rescoring involves replacing the existing language model scores of all paths 7r. If we denote the new language model by Lnew and correspondingly the score of the path 7r by Lnew[7r], then it is simply obtained as: Lnew[7r] = � |7r |P(w[7rj]|w[7rj_1], ... , w[7rj_n+1]) j=1 where P(w[7rj]|w[7rj_1], ... , w[7rj_n+1]) is the nth order Markov approximation for estimating the probability of a word given the unambiguous context of n − 1 words under the new rescoring LM. If the Markov rescoring n-gram LM needs a bigger conte</context>
</contexts>
<marker>Mohri, Pereira, Riley, 2000</marker>
<rawString>M. Mohri, F.C.N. Pereira, and M. Riley. 2000. The design principles of a weighted finite-state transducer library. Theoretical Computer Science, 231:17-32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ogawa</author>
<author>K Takeda</author>
<author>F Itakura</author>
</authors>
<title>Balancing Acoustic and Linguistic Probabilities. In</title>
<date>1998</date>
<booktitle>Proc. of IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP).</booktitle>
<contexts>
<context position="12021" citStr="Ogawa et al., 1998" startWordPosition="2019" endWordPosition="2022">r Markov LM for first pass recognition, imposes a constraint on the word lattice such that at each state there exists an unambiguous context of consecutive m − 1 words. A first pass output is then a path 7r* having Maximum a Posterior (MAP) probability.3 Thus 7r* is 2We will use O symbol to denote concatenation of paths or word strings. 3Note that asterisk symbol here connotes that the path is op1118 obtained as: 7r* = arg max A[7r]&apos;&apos;L[7r], 7r:p[7r]=ns n[7r]ENe where -y is the scaling parameter needed to balance the dynamic variability between the distributions of acoustic and language model (Ogawa et al., 1998). Efficient algorithms such as single source shortest path (Mohri et al., 2000) can be used for finding out the MAP path. Under a new n-gram Language Model, rescoring involves replacing the existing language model scores of all paths 7r. If we denote the new language model by Lnew and correspondingly the score of the path 7r by Lnew[7r], then it is simply obtained as: Lnew[7r] = � |7r |P(w[7rj]|w[7rj_1], ... , w[7rj_n+1]) j=1 where P(w[7rj]|w[7rj_1], ... , w[7rj_n+1]) is the nth order Markov approximation for estimating the probability of a word given the unambiguous context of n − 1 words und</context>
</contexts>
<marker>Ogawa, Takeda, Itakura, 1998</marker>
<rawString>A. Ogawa, K. Takeda, and F. Itakura. 1998. Balancing Acoustic and Linguistic Probabilities. In Proc. of IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Richardson</author>
<author>M Ostendorf</author>
<author>J R Rohlicek</author>
</authors>
<title>Lattice-based search strategies for large vocabulary speech recognition.</title>
<date>1995</date>
<booktitle>In Proc. of IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP).</booktitle>
<contexts>
<context position="18970" citStr="Richardson et al. (1995)" startWordPosition="3251" endWordPosition="3254">bservation that the recognizer produces bursts of errors such that they have a temporal scope. The recognizer output (sentence hypotheses) when aligned together typically shows a pattern of confusions both at the word level and at the phrase level. Regions where there are singleton words competing with one another (reminiscent of a confusion bin of a Confusion Network (CN) (Mangu, 2000)), choice of 1 word edit distance works well for the formation of local neighborhood. Regions where there are phrases competing with other phrases, choice of variable length neighborhood works well. Previously, Richardson et al. (1995) demonstrated a hill climbing framework by exploring 1 word edit distance neighborhood, while in our own previous work (Deoras and Jelinek, 2009), we demonstrated working of iterative decoding algorithm, a hill climbing framework, for CNs, in which the neighborhood was formed by all words competing with each other in any given time slot, as defined by a confusion bin. In this work, we propose a technique which generalizes very well on word lattices and overcomes the limitations posed by a CN or by the limited nature of local neighborhood. The size of the neighborhood in our approach is a varia</context>
</contexts>
<marker>Richardson, Ostendorf, Rohlicek, 1995</marker>
<rawString>F. Richardson, M. Ostendorf, and J.R. Rohlicek. 1995. Lattice-based search strategies for large vocabulary speech recognition. In Proc. of IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
</authors>
<title>Probabilistic top-down parsing and language modeling.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>2</issue>
<contexts>
<context position="4023" citStr="Roark, 2001" startWordPosition="654" endWordPosition="655">ssociation for Computational Linguistics language. In addition, if we can move to more general models then we could hope to capture more, as well. However, due to data sparsity, it is hard to estimate a robust n-gram distribution for large values of n ( say, n &gt; 10) using the conventional Maximum Likelihood techniques, unless a more robust technique is employed for modeling which generalizes well on unseen events. Some of these well known long span / complex language models which have shown to perform very well on many speech tasks include: structured language model (Chelba and Jelinek, 2000; Roark, 2001; Wang and Harper, 2002; Filimonov and Harper, 2009), latent semantic analysis language model (Bellegarda, 2000), topic mixture language models (Iyer and Ostendorf, 1999), whole sentence exponential language models (Rosenfeld, 1997; Rosenfeld et al., 2001), feedforward neural networks (Bengio et al., 2001), recurrent neural network language models (Mikolov et al., 2010), among many others. Although better modeling techniques can now capture longer dependencies in a language, their incorporation in decoders of speech recognition or machine translation systems becomes computationally challenging</context>
</contexts>
<marker>Roark, 2001</marker>
<rawString>Brian Roark. 2001. Probabilistic top-down parsing and language modeling. Computational Linguistics, 27(2):249–276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roni Rosenfeld</author>
<author>Stanley F Chen</author>
<author>Xiaojin Zhu</author>
</authors>
<title>Whole-Sentence Exponential Language Models: a Vehicle for Linguistic-Statistical Integration.</title>
<date>2001</date>
<journal>Computer Speech and Language,</journal>
<volume>15</volume>
<issue>1</issue>
<contexts>
<context position="4279" citStr="Rosenfeld et al., 2001" startWordPosition="688" endWordPosition="691">es of n ( say, n &gt; 10) using the conventional Maximum Likelihood techniques, unless a more robust technique is employed for modeling which generalizes well on unseen events. Some of these well known long span / complex language models which have shown to perform very well on many speech tasks include: structured language model (Chelba and Jelinek, 2000; Roark, 2001; Wang and Harper, 2002; Filimonov and Harper, 2009), latent semantic analysis language model (Bellegarda, 2000), topic mixture language models (Iyer and Ostendorf, 1999), whole sentence exponential language models (Rosenfeld, 1997; Rosenfeld et al., 2001), feedforward neural networks (Bengio et al., 2001), recurrent neural network language models (Mikolov et al., 2010), among many others. Although better modeling techniques can now capture longer dependencies in a language, their incorporation in decoders of speech recognition or machine translation systems becomes computationally challenging. Due to the prohibitive increase in the search space of sentence hypotheses (or longer length word sub sequences), it becomes challenging to use a long span language model in the first pass decoding. A word graph (word lattices for speech recognition syst</context>
</contexts>
<marker>Rosenfeld, Chen, Zhu, 2001</marker>
<rawString>Roni Rosenfeld, Stanley F. Chen, and Xiaojin Zhu. 2001. Whole-Sentence Exponential Language Models: a Vehicle for Linguistic-Statistical Integration. Computer Speech and Language, 15(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roni Rosenfeld</author>
</authors>
<title>A Whole Sentence Maximum Entropy Language Model.</title>
<date>1997</date>
<booktitle>In Proc. of IEEE workshop on Automatic Speech Recognition and Understanding (ASRU),</booktitle>
<location>Santa Barbara, California,</location>
<contexts>
<context position="4254" citStr="Rosenfeld, 1997" startWordPosition="686" endWordPosition="687">on for large values of n ( say, n &gt; 10) using the conventional Maximum Likelihood techniques, unless a more robust technique is employed for modeling which generalizes well on unseen events. Some of these well known long span / complex language models which have shown to perform very well on many speech tasks include: structured language model (Chelba and Jelinek, 2000; Roark, 2001; Wang and Harper, 2002; Filimonov and Harper, 2009), latent semantic analysis language model (Bellegarda, 2000), topic mixture language models (Iyer and Ostendorf, 1999), whole sentence exponential language models (Rosenfeld, 1997; Rosenfeld et al., 2001), feedforward neural networks (Bengio et al., 2001), recurrent neural network language models (Mikolov et al., 2010), among many others. Although better modeling techniques can now capture longer dependencies in a language, their incorporation in decoders of speech recognition or machine translation systems becomes computationally challenging. Due to the prohibitive increase in the search space of sentence hypotheses (or longer length word sub sequences), it becomes challenging to use a long span language model in the first pass decoding. A word graph (word lattices fo</context>
</contexts>
<marker>Rosenfeld, 1997</marker>
<rawString>Roni Rosenfeld. 1997. A Whole Sentence Maximum Entropy Language Model. In Proc. of IEEE workshop on Automatic Speech Recognition and Understanding (ASRU), Santa Barbara, California, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D E Rumelhart</author>
<author>G E Hinton</author>
<author>R J Williams</author>
</authors>
<title>Learning representations by back-propagating errors.</title>
<date>1986</date>
<journal>Nature,</journal>
<pages>323--533</pages>
<contexts>
<context position="9015" citStr="Rumelhart et al., 1986" startWordPosition="1463" endWordPosition="1466">ng 1 of N coding (thus, the size of the input layer is equal to the size of the vocabulary, and only the neuron that corresponds to the previous word in a sequence is set to 1). The hidden layer s(t) has additional recurrent connections that are delayed by one time step. After the network is trained, the output layer y(t) represents probability distribution for the current word, given the previous word and the state of the hidden layer from the previous time step. The training is performed by ‘backpropagationthrough-time’ algorithm that is commonly used for training recurrent neural networks (Rumelhart et al., 1986). More details about training, setting initial parameters, choosing size of the hidden layer etc. are presented in (Mikolov et al., 2010). Additional extensions that allow this model to be trained on large corpora are presented in (Mikolov et al., 2011b). 3 Standard Approaches for Rescoring 3.1 Word Lattice Rescoring A word lattice, L, obtained at the output of the first pass decoding, encodes exponential number (exponential in the number of states (nodes) present in the lattice) of hypotheses in a very compact data structure. It is a directed acyclic graph G = (V, £, ns, Ne), where V and £ de</context>
</contexts>
<marker>Rumelhart, Hinton, Williams, 1986</marker>
<rawString>D.E. Rumelhart, G. E. Hinton, and R.J. Williams. 1986. Learning representations by back-propagating errors. Nature, 323:533–536.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
</authors>
<title>Continuous space language models.</title>
<date>2007</date>
<journal>Computer Speech and Language,</journal>
<volume>21</volume>
<issue>3</issue>
<pages>518</pages>
<contexts>
<context position="6939" citStr="Schwenk (2007)" startWordPosition="1117" endWordPosition="1118">ing techniques and then describe and demonstrate our proposed technique in Sec. 4. We present experimental results in Sec. 5 followed by conclusions and some remarks in Sec. 6. 2 Recurrent Neural Networks (RNN) There is a long history of using neural networks to model sequences. Elman (1990) used recurrent neural network for modeling sentences of words generated by an artificial grammar. Work on statistical language modeling of real natural language data, together with an empirical comparison of performance to standard techniques was done by Bengio et al. (2001). His work has been followed by Schwenk (2007), who has shown that neural network language models actually work very well in the state-of-theart speech recognition systems. Recurrent Neural Network based Language Models (RNN-LMs) (Mikolov et al., 2010) improved the ability of the original model to capture patterns in the language without using any additional features (such as part of speech, morphology etc) i.e. other than lexical ones. The RNN-LM was shown to have superior performance than the original feedforward neural network (Mikolov et al., 2011b). Recently, we also showed that this model outperforms many other advanced language mod</context>
</contexts>
<marker>Schwenk, 2007</marker>
<rawString>Holger Schwenk. 2007. Continuous space language models. Computer Speech and Language, 21(3):492– 518.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C E Shannon</author>
</authors>
<title>A Mathematical Theory of Communication.</title>
<date>1948</date>
<journal>The Bell System Technical Journal,</journal>
<volume>27</volume>
<pages>623--656</pages>
<contexts>
<context position="2361" citStr="Shannon, 1948" startWordPosition="375" endWordPosition="376">he probability of the word sequence W ≡ w1, ... , wm ≡ wm1 is typically factored using the chain rule: P(wi|wi−1 1 ) (1) In modern statistical recognition systems, an LM tends to be restricted to simple n-gram models, where the distribution of the predicted word depends on the previous (n − 1) words i.e. P(wi|wi−1 1 ) ≈ P (wi|wi−1 i−n+1). Noam Chomsky argued that n-grams cannot learn long-distance dependencies that span over more than n words (Chomsky, 1957, pp.13). While that might seem obvious in retrospect, there was a lot of excitement at the time over the Shannon-McMillanBreiman Theorem (Shannon, 1948) which was interpreted to say that, in the limit, under just a couple of minor caveats and a little bit of not-very-important fine print, n-gram statistics are sufficient to capture all the information in a string (such as an English sentence). Chomsky realized that while that may be true in the limit, n-grams are far from the most parsimonious representation of many linguistic facts. In a practical system, we will have to truncate n-grams at some (small) fixed n (such as trigrams or perhaps 5-grams). Truncated n-gram systems can capture many agreement facts, but not all.1 By long-distance dep</context>
</contexts>
<marker>Shannon, 1948</marker>
<rawString>C. E. Shannon. 1948. A Mathematical Theory of Communication. The Bell System Technical Journal, 27:379–423, 623–656.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Soltau</author>
<author>G Saon</author>
<author>B Kingsbury</author>
</authors>
<title>The IBM Attila speech recognition toolkit.</title>
<date>2010</date>
<booktitle>In Proc. of IEEE Workshop on Spoken Language Technology (SLT).</booktitle>
<contexts>
<context position="31444" citStr="Soltau et al., 2010" startWordPosition="5501" endWordPosition="5504">us only on those sub lattices which the recognizer found difficult to decode in the first pass. All other part of the parent lattice will be not be analyzed. The thresholds for pruning is very application and corpus specific and needs to be tuned on some held out data. 5 Experiments and Results We performed recognition on the Broadcast News (BN) dev04f, rt03 and rt04 task using the stateof-the-art acoustic models trained on the English Broadcast News (BN) corpus (430 hours of audio) provided to us by IBM (Chen et al., 2009). IBM also provided us its state-of-the-art speech recognizer, Attila (Soltau et al., 2010) and two Kneser-Ney smoothed backoff n-gram LMs containing 4.7M ngrams (n &lt; 4) and 54M n-grams (n &lt; 4), both trained on 400M word tokens. We will refer to them as KN:BN-Small and KN:BN-Big respectively. We refer readers to (Chen et al., 2009) for more details about the recognizer and corpora used for training the models. We trained two RNN based language models - the first one, denoted further as RNN-limited, was trained on a subset of the training data (58M tokens). It used 400 neurons in the hidden layer. The second model, denoted as RNN-all, was trained on all of the training data (400M tok</context>
</contexts>
<marker>Soltau, Saon, Kingsbury, 2010</marker>
<rawString>H. Soltau, G. Saon, and B. Kingsbury. 2010. The IBM Attila speech recognition toolkit. In Proc. of IEEE Workshop on Spoken Language Technology (SLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen Wang</author>
<author>Mary Harper</author>
</authors>
<title>The SuperARV language model: investigating the effectiveness of tightly integrating multiple knowledge sources.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="4046" citStr="Wang and Harper, 2002" startWordPosition="656" endWordPosition="659">r Computational Linguistics language. In addition, if we can move to more general models then we could hope to capture more, as well. However, due to data sparsity, it is hard to estimate a robust n-gram distribution for large values of n ( say, n &gt; 10) using the conventional Maximum Likelihood techniques, unless a more robust technique is employed for modeling which generalizes well on unseen events. Some of these well known long span / complex language models which have shown to perform very well on many speech tasks include: structured language model (Chelba and Jelinek, 2000; Roark, 2001; Wang and Harper, 2002; Filimonov and Harper, 2009), latent semantic analysis language model (Bellegarda, 2000), topic mixture language models (Iyer and Ostendorf, 1999), whole sentence exponential language models (Rosenfeld, 1997; Rosenfeld et al., 2001), feedforward neural networks (Bengio et al., 2001), recurrent neural network language models (Mikolov et al., 2010), among many others. Although better modeling techniques can now capture longer dependencies in a language, their incorporation in decoders of speech recognition or machine translation systems becomes computationally challenging. Due to the prohibitiv</context>
</contexts>
<marker>Wang, Harper, 2002</marker>
<rawString>Wen Wang and Mary Harper. 2002. The SuperARV language model: investigating the effectiveness of tightly integrating multiple knowledge sources. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>