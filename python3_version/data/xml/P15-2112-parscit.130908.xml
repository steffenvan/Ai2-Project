<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.014076">
<title confidence="0.977842">
Rhetoric Map of an Answer to Compound Queries
</title>
<author confidence="0.9756">
Dmitry Ilvovsky
</author>
<affiliation confidence="0.772031333333333">
National Research Universi-
ty Higher School of Eco-
nomics, Moscow, Russia
</affiliation>
<email confidence="0.984417">
dilvovsky@hse.ru
</email>
<author confidence="0.940589">
Sergey O. Kuznetsov
</author>
<affiliation confidence="0.750325333333333">
National Research Universi-
ty Higher School of Eco-
nomics, Moscow, Russia
</affiliation>
<email confidence="0.976313">
skuznetsov@hse.ru
</email>
<note confidence="0.896614666666667">
Boris Galitsky
Knowledge Trail Inc.
San-Francisco, USA
</note>
<email confidence="0.747617">
bgalitsky@hotmail.com
</email>
<sectionHeader confidence="0.988481" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999990954545455">
Given a discourse tree for a text as a can-
didate answer to a compound query, we
propose a rule system for valid and inva-
lid occurrence of the query keywords in
this tree. To be a valid answer to a query,
its keywords need to occur in a chain of
elementary discourse unit of this answer
so that these units are fully ordered and
connected by nucleus – satellite relations.
An answer might be invalid if the que-
ries’ keywords occur in the answer&apos;s sat-
ellite discourse units only. We build the
rhetoric map of an answer to prevent it
from firing by queries whose keywords
occur in non-adjacent areas of the An-
swer Map. We evaluate the improvement
of search relevance by filtering out
search results not satisfying the proposed
rule system, demonstrating a 4% increase
of accuracy with respect to the nearest
neighbor learning approach which does
not use the discourse tree structure.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999969262295082">
Answering compound queries, where its key-
words are distributed through text of a candidate
answer, is a sophisticated problem requiring deep
linguistic analysis. If the query keywords occur
in an answer text in a linguistically connected
manner, this answer is most likely relevant. This
is usually true when all these keywords occur in
the same sentence: they should be connected
syntactically. For the inter-sentence connections,
these keywords need to be connected via anapho-
ra, refer to the same entity or sub-entity, or be
linked via rhetoric discourse.
If the query keywords occur in different sen-
tences, there should be linguistic cues for some
sort of connections between these occurrences. If
there is no connection, then different constraints
for an object expressed by a query might be ap-
plied to different objects in the answer text,
therefore, this answer is perhaps irrelevant.
There are following possibilities of such connec-
tions.
Anaphora. If two areas of keyword occurrenc-
es are connected with anaphoric relation, the an-
swer is most likely relevant.
Communicative actions. If the text contains a
dialogue, and some question keywords are in a
request and other are in the reply to this request,
then these keywords are connected and the an-
swer is relevant. To identify such situation, one
needs to find a pair of communicative actions
and to confirm that this pair is of request-reply
kind.
Rhetoric relations. They indicate the coher-
ence structure of a text (Mann and Thompson,
1988). Rhetoric relations for text can be repre-
sented by a Discourse tree (DT) which is a la-
beled tree. The leaves of this tree correspond to
contiguous units for clauses (elementary dis-
course units, EDU). Adjacent EDUs as well as
higher-level (larger) discourse units are orga-
nized in a hierarchy by rhetoric relation (e.g.,
background, attribution). Anti-symmetric rela-
tion takes a pair of EDUs: nuclei, which are core
parts of the relation, and satellites, the supportive
parts of the rhetoric relation.
The most important class of connections we
focus in this study is rhetoric. Once an answer
text is split into EDUs, and rhetoric relations are
established between them, it is possible to estab-
lish rules for whether query keywords occurring
in text are connected by rhetoric relations (and
therefore, this answer is likely relevant) or not
connected (and this answer is most likely irrele-
vant). Hence we use the DT as a base for an An-
swer Map of a text: certain sets of nodes in DT
correspond to queries so that this text is a valid
answer, and certain sets of nodes correspond to
an invalid answer. Our definition of the Answer
Map follows the methodology of inverse index
for search: instead of taking queries and consid-
ering all valid answers for it from a set of text,
</bodyText>
<page confidence="0.922907">
681
</page>
<bodyText confidence="0.974265298507463">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 681–686,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
we take a text (answer) and consider the totality
of valid and invalid queries consisting of the
keywords from this text.
Usually, the main clause of a compound query
includes the main entity and some of its con-
straints, and the supplementary clause includes
the other constraint. In the most straightforward
way, the main clause of a query is mapped into a
nucleus and the supplementary clause is mapped
into a satellite of RST relation such as elabora-
tion. Connection by other RST relation, where a
satellite introduces additional constraints for a
nucleus, has the same meaning for answer validi-
ty. This validity still holds when two EDUs are
connected with a symmetric relation such as
joint. However, when the images of the main and
supplementary clause of the query are satellites
of different nucleus, it most likely means that
they express constraints for different entities, and
therefore constitute an irrelevant answer for this
query.
There is a number of recent studies employing
RST features for passage re-ranking under ques-
tion answering (Joty and Moschitti, 2014;
Surdeanu et al., 2014). In the former study, the
feature space of subtrees of parse trees includes
the RST relations to improve question answer
accuracy. In the latter project, RST features con-
tributed to the totality of features learned to re-
rank the answers. In (Galitsky et al., 2014) rheto-
ric structure, in particular, was used to broaden
the set of parse trees to enrich the feature space
by taking into account overall discourse structure
of candidate answers. Statistical learning in these
studies demonstrated that rhetoric relation can be
leveraged for better search relevance. In the cur-
rent study, we formulate the explicit rules for
how a query can be mapped into the answer DT
and the relevance of this map can be verified.
2 Example of an Answer Map
Ex. 1. DT including 6 nodes {e1...e6} is shown
in Fig 1 (Joty and Moschitti, 2014). Text is split
into six EDUs:
[what’s more,]e1 [he be-
lieves]e2 [seasonal swings in
the auto industry this year
aren’t occurring at the same
time in the past,]e3 [because
of production and pricing dif-
ferences]e4 [that are curbing
the accuracy of seasonal ad-
justments]e5 ] [built into the
employment data.]e6
Fig.1. Discourse tree for the Example 1
Horizontal lines indicate text segments; satel-
lites are connected to their nuclei by curved ar-
rows. One can see that this text is a relevant an-
swer to the query
Are seasonal swings in the auto
industry due to pricing differ-
ences?
but is an irrelevant answer to the query
Are pricing differences built
into employment data?
</bodyText>
<figureCaption confidence="0.752821">
Fig. 2. An Answer Map and its areas for valid
and invalid answers
</figureCaption>
<bodyText confidence="0.96170848">
A valid set of nodes of an Answer Map is de-
fined as the one closed under common ancestor
relations in a DT. For example, the i-nodes on
the bottom-left of DT in Fig. 2 constitute the in-
valid set, and the v-nodes on the right of DT con-
stitute the valid set.
Ex. 2.
I went to watch a movie because
I had nothing else to do. I en-
joyed the movie which was about
animals finding food in a de-
sert. To feed in a desert envi-
ronment, zebras run hundreds of
miles in search of sources of
water.
This answer is valid for the following queries
(phrases) since their keywords form v-set:
- enjoy movie watched when
nothing else to do
- I went to watch a movie
about feeding in desert en-
vironment
- I went to watch a movie
about zebras run hundreds of
miles
</bodyText>
<page confidence="0.993581">
682
</page>
<bodyText confidence="0.8744584">
- I went to watch a movie
about searching sources of
water
And this text is not a correct answer for the
following queries (phrases), since their keywords
form i-sets:
- animals find food in desert
when have nothing else to do
- I had nothing else except
finding food in a desert
- I had nothing else to do but
run hundreds of miles in
search of water
- finding food in a desert - a
good thing to do
</bodyText>
<sectionHeader confidence="0.9925565" genericHeader="introduction">
3 Definition and Construction Algo-
rithm
</sectionHeader>
<bodyText confidence="0.999853581395349">
Discourse tree includes directed arcs for anti-
symmetric rhetoric relation and undirected arcs
for symmetric rhetoric relations such as joint,
time sequence, and others. For two nodes of DT
we define its directed common ancestor as a
common ancestor node which is connected with
these nodes via directed arcs.
The valid set of EDUs which is a result of
mapping of a query is closed under common di-
rected ancestor relation: it should contain the set
of all directed common ancestor for all EDUs.
Hence this constraint is applied for antisymmet-
ric RST relations; query terms can occur in
symmetric EDU nodes in an arbitrary way.
To construct an Answer Map from DT, firstly,
we need to map keywords and phrases of a query
into EDUs of an answer. For each noun phrase
for a query, we find one or more EDUs which
include noun phrases with the same head noun.
Not each keyword has to be mapped, but there
should be not more than a single EDU each key-
word is mapped under a given mapping. For ex-
ample, noun phrase from the query family do-
ing its taxes is mapped into the EDU in-
cluding how individuals and families
file their taxes since they have the same
head noun tax. If a multiple mapping exists for
a query, we need to find at least one valid occur-
rence to conclude that this query is a valid one
for the given map.
For a query Q, if its keywords occur in candi-
date answer A and the set of EDUs Q𝑒𝑑u, then
commonAncestorsDT(A)(Q𝑒𝑑u)  Q𝑒𝑑u.
For a real-word search system, the enforce-
ment of RST rules occurs at indexing time, since
RST parsing is rather slow.
For answer text A, we produce a sequence of
texts 𝐴𝑒 &lt; {A directed common ancestor I} for
all pairs of EDU nodes connected with their par-
ents by directed arcs. Then the match of the set
of keyword occurs with the extended index in the
regular manner: there is no element 𝐴𝑒 for inva-
lid mapping Q to Q𝑒𝑑u .
</bodyText>
<sectionHeader confidence="0.970505" genericHeader="method">
4 Approach Scalability
</sectionHeader>
<bodyText confidence="0.999934173913044">
In terms of search engineering, enforcing of the
condition of the Rhetoric Map of an answer re-
quires additional part of the index besides the
inverse one. Building this additional index re-
quires enumeration of all maximal sequences of
keywords from Rhetoric Map for every docu-
ment (potential answer A). Once A is determined
to be fired by query Q using the regular search
index, there should be an entry in Rhetoric Map
which is fired by a query formed as a conjunc-
tion of terms in Q.
Since application of Rhetoric Map rules oc-
curs via an inverse index, the search time is con-
stant with respect to the size of the overall RM
index and size of a given document. The index-
ing time is significantly higher due to rhetoric
parsing, and the size of index is increased ap-
proximately by the number of average maximal
paths in a DT graph, which is 3-5. Hence alt-
hough the performance of search will not signifi-
cantly change, the amount of infrastructure ef-
forts associated with RM technology is substan-
tial.
</bodyText>
<sectionHeader confidence="0.997894" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.999742">
We used the TREC evaluation dataset as a list of
topics: http://trec.nist.gov/data/qa/. Given a short
factoid question for entity, person, organization,
event, etc. such as #EVENT Pakistan earth-
quakes of October 2005# we ran a web
search and automatically (using shallow parsing
provided by Stanford NLP) extracted compound
sentences from search expressions, such as A
massive earthquake struck Pakistan
and parts of India and Afghanistan
on Saturday morning October 8, 2005.
This was the strongest earthquake in
the area during the last hundred
years.
Ten to twenty such queries were derived for a
topic. Those portions of text were selected with
obvious rhetoric relation between the clauses.
We then fed Bing Search Engine API such que-
ries and built the Answer Map for each candidate
answer. We then ran the Answer Map - based
</bodyText>
<page confidence="0.99856">
683
</page>
<bodyText confidence="0.998793704545454">
filter. Finally, we manually verify that these fil-
tered answers are relevant to the initial questions
and to the queries.
We evaluated improvement of search rele-
vance for compound queries by applying the DT
rules. These rules provide Boolean decisions for
candidate answers, but we compare them with
score-based answer re-ranking based on ML of
baseline SVM tree kernel (Moschitti, 2006), dis-
course-based SVM (Ilvovsky, 2014) and nearest-
neighbor Parse Thicket-based approach (Galitsky
et al., 2013).
The approach based on SVM tree kernel takes
question-answer pairs (also from TREC dataset)
and forms the positive set from the correct pairs
and negative set from the incorrect pairs. The
tree kernel learning (Duffy and Collins, 2002) for
the pairs of extended parse trees produces multi-
ple parse trees for each sentence, linking them by
discourse relations of anaphora, communicative
actions, “same entity” relation and rhetoric rela-
tions (Galitsky et al., 2014).
In the Nearest Neighbor approach to question
– answer classification one takes the same data
of parse trees connected by discourse relations
and instead of applying SVM learning to pairs,
compare these data for question and answer di-
rectly, finding the highest similarity.
To compare the score-based answer re-ranking
approaches with the rule-based answer filtering
one, we took first 20 Bing answers and classified
them as valid (top 10) and invalid (bottom 10)
under the former set of approaches and selected
up to 10 acceptable (using the original ranking)
under the latter approach. Hence the order of
these selected set of 10 answers is irrelevant for
our evaluation and we measured the percentage
of valid answers among them (the focus of eval-
uation is search precision, not recall).
Answer validity was assessed by Amazon Me-
chanical Turk. The assessors were asked to
choose relevant answers from the randomly sort-
ed list of candidate answers. Table 1 shows the
evaluation results.
</bodyText>
<tableCaption confidence="0.999024">
Table 1. Evaluation results
</tableCaption>
<table confidence="0.998197823529412">
Filtering method Baseline SVM TK SVM TK Nearest Answer
Bing search, learning of QA learning for neighbor for Map, %
% pairs (baseline the pairs for question –
improvement), extended parse answer, %
% trees, %
Sources / Source of - - Anaphora, same entity, selected Discourse
Query discourse discourse relations Tree
types information
Clauses connected with 68.3 69.4 73.9 74.6 79.2
elaboration
Clauses connected with 67.5 70.1 72.7 75.1 78.8
attribution
Clauses connected with 64.9 66.3 70.2 74.0 78.0
summary
Clauses in 64.1 65.2 68.1 72.3 76.3
joint/sequence relation
Average 66.2 67.8 71.2 74.0 78.0
</table>
<bodyText confidence="0.999880421052632">
The top two rows show the answer filtering
methods and sources of discourse information.
Bottom rows show evaluation results for queries
with various rhetoric relations between clauses.
One can observe just a 1.5% improvement by
using SVM tree kernel without discourse, further
3.5% improvement by using discourse-enabled
SVM tree kernel, and further improvement of
2.8% by using nearest neighbor learning. The
latter is still 4% lower than the Answer Map ap-
proach, which is the focus of this study. We ob-
serve that the baseline search improvement,
SVM tree kernel approach has a limited capabil-
ity of filtering out irrelevant search results in our
evaluation settings. Also, the role of discourse
information in improving search results for que-
ries with symmetric rhetoric relation between
clauses is lower than that of the anti-symmetric
relations.
</bodyText>
<page confidence="0.996723">
684
</page>
<bodyText confidence="0.9894885">
Code and examples are available at
code.google.com/p/relevance-based-on-parse-
trees/ (package
opennlp.tools.parse_thicket.external_rst).
</bodyText>
<sectionHeader confidence="0.994588" genericHeader="conclusions">
6 Discussion and Conclusion
</sectionHeader>
<bodyText confidence="0.999946825">
Overall, our evaluation settings are focused on
compound queries where most answers correctly
belong to the topic of interest in a query and
there is usually sufficient number of keywords to
assure this. However, in the selected search do-
main irrelevant answers are those based on for-
eign entities or mismatched attributes of these
entities. Hence augmenting keyword statistics
with the structured information of parse trees is
not critical to search accuracy improvement. At
the same time, discourse information for candi-
date answers is essential to properly form and
interpret the constraints expressed in queries.
Although there has been a substantial ad-
vancement in document-level RST parsing, in-
cluding the rich linguistic features-based of
(Feng and Hirst, 2012) and powerful parsing
models (Joty et al., 2013), document level dis-
course analysis has not found a broad range of
applications such as search. The most valuable
information from DT includes global discourse
features and long range structural dependencies
between DT constituents.
Despite other studies (Surdeanu et al., 2014)
showed that discourse information is beneficial
for search via learning, we believe this is the first
study demonstrating how Answer Map affects
search directly. To be a valid answer for a ques-
tion, its keywords need to occur in adjacent EDU
chain of this answer so that these EDUs are fully
ordered and connected by nucleus – satellite rela-
tions. Note the difference between the proximity
in text as a sequence of words and proximity in
DT (Croft et al., 2009). An answer is expected to
be invalid if the questions&apos; keywords occur in the
answer&apos;s satellite EDUs and not in their nucleus
EDUs. The purpose of the rhetoric map of an
answer is to prevent it from being fired by ques-
tions whose keywords occur in non-adjacent are-
as of this map.
</bodyText>
<sectionHeader confidence="0.998988" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999855586206897">
S. Joty and A. Moschitti. 2014. Discriminative Re-
ranking of Discourse Parses Using Tree Kernels.
Proceedings of the 2014 Conference on Empirical
Methods in Natural Language Processing
(EMNLP), pages 2049–2060, October 25-29, 2014,
Doha, Qatar.
V. Wei Feng and G. Hirst. 2012. Text-level discourse
parsing with rich linguistic features. In Proceedings
of the 50th Annual Meeting of the Association for
Computational Linguistics (ACL-2012), pages 60-
68, Jeju, Korea.
P. Jansen, M. Surdeanu, and P. Clark. 2014. Dis-
course Complements Lexical Semantics for Non-
factoid Answer Reranking. In Proceedings of the
52nd Annual Meeting of the Association for Com-
putational Linguistics (ACL).
S. Joty, G. Carenini, and R. T. Ng. 2012. A Novel
Discriminative Framework for Sentence-Level
Discourse Analysis. In Proceedings of the 2012
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning, EMNLP-CoNLL’12, pages
904–915, Jeju Island, Korea. Association for Com-
putational Linguistics.
W. Mann, S. Thompson. 1988. Rhetorical Structure
Theory: Toward a Functional Theory of Text Or-
ganization. Text, 8(3):243–281.
S. Joty, G. Carenini, R. Ng, Y. Mehdad. 2013. Com-
bining Intra- and Multi-sentential Rhetorical Pars-
ing for Document-level Discourse Analysis. In
Proceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics, Sofia,
Bulgaria.
B. Galitsky, D. Ilvovsky, S.O. Kuznetsov, F. Strok.
2013. Matching sets of parse trees for answering
multi-sentence questions. In Proceedings of the
Recent Advances in Natural Language Processing
(RANLP), Shoumen, Bulgaria, pages 285–294.
D. Ilvovsky. 2014. Going beyond sentences when
applying tree kernels. Proceedings of the Student
Research Workshop ACL 2014, pp. 56-63.
B. Galitsky, D. Usikov, S.O. Kuznetsov. 2013. Parse
Thicket Representations for Answering Multi-
sentence questions. 20th International Conference
on Conceptual Structures, ICCS 2013.
B. Galitsky, S.O. Kuznetsov. 2008. Learning commu-
nicative actions of conflicting human agents. J.
Exp. Theor. Artif. Intell. 20(4): 277-317.
B. Galitsky. 2012. Machine Learning of Syntactic
Parse Trees for Search and Classification of Text.
Engineering Application of AI.
A. Moschitti. 2006. Efficient Convolution Kernels for
Dependency and Constituent Syntactic Trees. In
Proceedings of the 17th European Conference on
Machine Learning, Berlin, Germany.
A. Severyn, A. Moschitti. 2012. Structural relation-
ships for large-scale learning of answer re-ranking.
SIGIR 2012: 741-750.
</reference>
<page confidence="0.984913">
685
</page>
<reference confidence="0.999930625">
A. Severyn, A. Moschitti. 2012. Fast Support Vector
Machines for Convolution Tree Kernels. Data Min-
ing Knowledge Discovery 25: 325-357.
M. Collins and N. Duffy. 2002. Convolution kernels
for natural language. In Proceedings of NIPS, 625–
632.
H. Lee, A. Chang, Y. Peirsman, N. Chambers, Mihai
Surdeanu and Dan Jurafsky. 2013. Deterministic
coreference resolution based on entity-centric,
precision-ranked rules. Computational Linguistics
39(4).
B. Croft, D. Metzler, T. Strohman. 2009. Search En-
gines - Information Retrieval in Practice. Pearson
Education. North America.
V. Vapnik. 1995. The Nature of Statistical Learning
Theory. – Springer-Verlag.
</reference>
<page confidence="0.998744">
686
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.110847">
<title confidence="0.997579">Rhetoric Map of an Answer to Compound Queries</title>
<author confidence="0.708543">Dmitry</author>
<affiliation confidence="0.636418">Research Higher School of</affiliation>
<address confidence="0.969927">nomics, Moscow, Russia</address>
<email confidence="0.991847">dilvovsky@hse.ru</email>
<author confidence="0.626938">O Sergey</author>
<affiliation confidence="0.608869">Research Higher School of</affiliation>
<address confidence="0.915729">nomics, Moscow, Russia</address>
<email confidence="0.988851">skuznetsov@hse.ru</email>
<author confidence="0.721564">Boris</author>
<affiliation confidence="0.922046">Knowledge Trail</affiliation>
<address confidence="0.852843">San-Francisco, USA</address>
<email confidence="0.999859">bgalitsky@hotmail.com</email>
<abstract confidence="0.999796304347826">Given a discourse tree for a text as a candidate answer to a compound query, we propose a rule system for valid and invalid occurrence of the query keywords in this tree. To be a valid answer to a query, its keywords need to occur in a chain of elementary discourse unit of this answer so that these units are fully ordered and by nucleus relations. An answer might be invalid if the quekeywords occur in the answer&apos;s ellite discourse units only. We build the rhetoric map of an answer to prevent it from firing by queries whose keywords occur in non-adjacent areas of the Answer Map. We evaluate the improvement of search relevance by filtering out search results not satisfying the proposed rule system, demonstrating a 4% increase of accuracy with respect to the nearest neighbor learning approach which does not use the discourse tree structure.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Joty</author>
<author>A Moschitti</author>
</authors>
<title>Discriminative Reranking of Discourse Parses Using Tree Kernels.</title>
<date>2014</date>
<booktitle>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>2049--2060</pages>
<location>Doha, Qatar.</location>
<contexts>
<context position="5376" citStr="Joty and Moschitti, 2014" startWordPosition="876" endWordPosition="879">ion. Connection by other RST relation, where a satellite introduces additional constraints for a nucleus, has the same meaning for answer validity. This validity still holds when two EDUs are connected with a symmetric relation such as joint. However, when the images of the main and supplementary clause of the query are satellites of different nucleus, it most likely means that they express constraints for different entities, and therefore constitute an irrelevant answer for this query. There is a number of recent studies employing RST features for passage re-ranking under question answering (Joty and Moschitti, 2014; Surdeanu et al., 2014). In the former study, the feature space of subtrees of parse trees includes the RST relations to improve question answer accuracy. In the latter project, RST features contributed to the totality of features learned to rerank the answers. In (Galitsky et al., 2014) rhetoric structure, in particular, was used to broaden the set of parse trees to enrich the feature space by taking into account overall discourse structure of candidate answers. Statistical learning in these studies demonstrated that rhetoric relation can be leveraged for better search relevance. In the curr</context>
</contexts>
<marker>Joty, Moschitti, 2014</marker>
<rawString>S. Joty and A. Moschitti. 2014. Discriminative Reranking of Discourse Parses Using Tree Kernels. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2049–2060, October 25-29, 2014, Doha, Qatar.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Wei Feng</author>
<author>G Hirst</author>
</authors>
<title>Text-level discourse parsing with rich linguistic features.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL-2012),</booktitle>
<pages>60--68</pages>
<location>Jeju,</location>
<contexts>
<context position="16253" citStr="Feng and Hirst, 2012" startWordPosition="2731" endWordPosition="2734">is usually sufficient number of keywords to assure this. However, in the selected search domain irrelevant answers are those based on foreign entities or mismatched attributes of these entities. Hence augmenting keyword statistics with the structured information of parse trees is not critical to search accuracy improvement. At the same time, discourse information for candidate answers is essential to properly form and interpret the constraints expressed in queries. Although there has been a substantial advancement in document-level RST parsing, including the rich linguistic features-based of (Feng and Hirst, 2012) and powerful parsing models (Joty et al., 2013), document level discourse analysis has not found a broad range of applications such as search. The most valuable information from DT includes global discourse features and long range structural dependencies between DT constituents. Despite other studies (Surdeanu et al., 2014) showed that discourse information is beneficial for search via learning, we believe this is the first study demonstrating how Answer Map affects search directly. To be a valid answer for a question, its keywords need to occur in adjacent EDU chain of this answer so that th</context>
</contexts>
<marker>Feng, Hirst, 2012</marker>
<rawString>V. Wei Feng and G. Hirst. 2012. Text-level discourse parsing with rich linguistic features. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL-2012), pages 60-68, Jeju, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Jansen</author>
<author>M Surdeanu</author>
<author>P Clark</author>
</authors>
<title>Discourse Complements Lexical Semantics for Nonfactoid Answer Reranking.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<marker>Jansen, Surdeanu, Clark, 2014</marker>
<rawString>P. Jansen, M. Surdeanu, and P. Clark. 2014. Discourse Complements Lexical Semantics for Nonfactoid Answer Reranking. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Joty</author>
<author>G Carenini</author>
<author>R T Ng</author>
</authors>
<title>A Novel Discriminative Framework for Sentence-Level Discourse Analysis.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL’12,</booktitle>
<pages>904--915</pages>
<institution>Jeju Island, Korea. Association for Computational Linguistics.</institution>
<marker>Joty, Carenini, Ng, 2012</marker>
<rawString>S. Joty, G. Carenini, and R. T. Ng. 2012. A Novel Discriminative Framework for Sentence-Level Discourse Analysis. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL’12, pages 904–915, Jeju Island, Korea. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Mann</author>
<author>S Thompson</author>
</authors>
<title>Rhetorical Structure Theory: Toward a Functional Theory of Text Organization.</title>
<date>1988</date>
<tech>Text, 8(3):243–281.</tech>
<contexts>
<context position="2744" citStr="Mann and Thompson, 1988" startWordPosition="443" endWordPosition="446">ps irrelevant. There are following possibilities of such connections. Anaphora. If two areas of keyword occurrences are connected with anaphoric relation, the answer is most likely relevant. Communicative actions. If the text contains a dialogue, and some question keywords are in a request and other are in the reply to this request, then these keywords are connected and the answer is relevant. To identify such situation, one needs to find a pair of communicative actions and to confirm that this pair is of request-reply kind. Rhetoric relations. They indicate the coherence structure of a text (Mann and Thompson, 1988). Rhetoric relations for text can be represented by a Discourse tree (DT) which is a labeled tree. The leaves of this tree correspond to contiguous units for clauses (elementary discourse units, EDU). Adjacent EDUs as well as higher-level (larger) discourse units are organized in a hierarchy by rhetoric relation (e.g., background, attribution). Anti-symmetric relation takes a pair of EDUs: nuclei, which are core parts of the relation, and satellites, the supportive parts of the rhetoric relation. The most important class of connections we focus in this study is rhetoric. Once an answer text is</context>
</contexts>
<marker>Mann, Thompson, 1988</marker>
<rawString>W. Mann, S. Thompson. 1988. Rhetorical Structure Theory: Toward a Functional Theory of Text Organization. Text, 8(3):243–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Joty</author>
<author>G Carenini</author>
<author>R Ng</author>
<author>Y Mehdad</author>
</authors>
<title>Combining Intra- and Multi-sentential Rhetorical Parsing for Document-level Discourse Analysis.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="16301" citStr="Joty et al., 2013" startWordPosition="2739" endWordPosition="2742">this. However, in the selected search domain irrelevant answers are those based on foreign entities or mismatched attributes of these entities. Hence augmenting keyword statistics with the structured information of parse trees is not critical to search accuracy improvement. At the same time, discourse information for candidate answers is essential to properly form and interpret the constraints expressed in queries. Although there has been a substantial advancement in document-level RST parsing, including the rich linguistic features-based of (Feng and Hirst, 2012) and powerful parsing models (Joty et al., 2013), document level discourse analysis has not found a broad range of applications such as search. The most valuable information from DT includes global discourse features and long range structural dependencies between DT constituents. Despite other studies (Surdeanu et al., 2014) showed that discourse information is beneficial for search via learning, we believe this is the first study demonstrating how Answer Map affects search directly. To be a valid answer for a question, its keywords need to occur in adjacent EDU chain of this answer so that these EDUs are fully ordered and connected by nucl</context>
</contexts>
<marker>Joty, Carenini, Ng, Mehdad, 2013</marker>
<rawString>S. Joty, G. Carenini, R. Ng, Y. Mehdad. 2013. Combining Intra- and Multi-sentential Rhetorical Parsing for Document-level Discourse Analysis. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Galitsky</author>
<author>D Ilvovsky</author>
<author>S O Kuznetsov</author>
<author>F Strok</author>
</authors>
<title>Matching sets of parse trees for answering multi-sentence questions.</title>
<date>2013</date>
<booktitle>In Proceedings of the Recent Advances in Natural Language Processing (RANLP), Shoumen, Bulgaria,</booktitle>
<pages>285--294</pages>
<contexts>
<context position="12380" citStr="Galitsky et al., 2013" startWordPosition="2132" endWordPosition="2135">g Search Engine API such queries and built the Answer Map for each candidate answer. We then ran the Answer Map - based 683 filter. Finally, we manually verify that these filtered answers are relevant to the initial questions and to the queries. We evaluated improvement of search relevance for compound queries by applying the DT rules. These rules provide Boolean decisions for candidate answers, but we compare them with score-based answer re-ranking based on ML of baseline SVM tree kernel (Moschitti, 2006), discourse-based SVM (Ilvovsky, 2014) and nearestneighbor Parse Thicket-based approach (Galitsky et al., 2013). The approach based on SVM tree kernel takes question-answer pairs (also from TREC dataset) and forms the positive set from the correct pairs and negative set from the incorrect pairs. The tree kernel learning (Duffy and Collins, 2002) for the pairs of extended parse trees produces multiple parse trees for each sentence, linking them by discourse relations of anaphora, communicative actions, “same entity” relation and rhetoric relations (Galitsky et al., 2014). In the Nearest Neighbor approach to question – answer classification one takes the same data of parse trees connected by discourse re</context>
</contexts>
<marker>Galitsky, Ilvovsky, Kuznetsov, Strok, 2013</marker>
<rawString>B. Galitsky, D. Ilvovsky, S.O. Kuznetsov, F. Strok. 2013. Matching sets of parse trees for answering multi-sentence questions. In Proceedings of the Recent Advances in Natural Language Processing (RANLP), Shoumen, Bulgaria, pages 285–294.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ilvovsky</author>
</authors>
<title>Going beyond sentences when applying tree kernels.</title>
<date>2014</date>
<booktitle>Proceedings of the Student Research Workshop ACL</booktitle>
<pages>56--63</pages>
<contexts>
<context position="12307" citStr="Ilvovsky, 2014" startWordPosition="2124" endWordPosition="2125">ith obvious rhetoric relation between the clauses. We then fed Bing Search Engine API such queries and built the Answer Map for each candidate answer. We then ran the Answer Map - based 683 filter. Finally, we manually verify that these filtered answers are relevant to the initial questions and to the queries. We evaluated improvement of search relevance for compound queries by applying the DT rules. These rules provide Boolean decisions for candidate answers, but we compare them with score-based answer re-ranking based on ML of baseline SVM tree kernel (Moschitti, 2006), discourse-based SVM (Ilvovsky, 2014) and nearestneighbor Parse Thicket-based approach (Galitsky et al., 2013). The approach based on SVM tree kernel takes question-answer pairs (also from TREC dataset) and forms the positive set from the correct pairs and negative set from the incorrect pairs. The tree kernel learning (Duffy and Collins, 2002) for the pairs of extended parse trees produces multiple parse trees for each sentence, linking them by discourse relations of anaphora, communicative actions, “same entity” relation and rhetoric relations (Galitsky et al., 2014). In the Nearest Neighbor approach to question – answer classi</context>
</contexts>
<marker>Ilvovsky, 2014</marker>
<rawString>D. Ilvovsky. 2014. Going beyond sentences when applying tree kernels. Proceedings of the Student Research Workshop ACL 2014, pp. 56-63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Galitsky</author>
<author>D Usikov</author>
<author>S O Kuznetsov</author>
</authors>
<title>Parse Thicket Representations for Answering Multisentence questions.</title>
<date>2013</date>
<booktitle>20th International Conference on Conceptual Structures, ICCS</booktitle>
<contexts>
<context position="12380" citStr="Galitsky et al., 2013" startWordPosition="2132" endWordPosition="2135">g Search Engine API such queries and built the Answer Map for each candidate answer. We then ran the Answer Map - based 683 filter. Finally, we manually verify that these filtered answers are relevant to the initial questions and to the queries. We evaluated improvement of search relevance for compound queries by applying the DT rules. These rules provide Boolean decisions for candidate answers, but we compare them with score-based answer re-ranking based on ML of baseline SVM tree kernel (Moschitti, 2006), discourse-based SVM (Ilvovsky, 2014) and nearestneighbor Parse Thicket-based approach (Galitsky et al., 2013). The approach based on SVM tree kernel takes question-answer pairs (also from TREC dataset) and forms the positive set from the correct pairs and negative set from the incorrect pairs. The tree kernel learning (Duffy and Collins, 2002) for the pairs of extended parse trees produces multiple parse trees for each sentence, linking them by discourse relations of anaphora, communicative actions, “same entity” relation and rhetoric relations (Galitsky et al., 2014). In the Nearest Neighbor approach to question – answer classification one takes the same data of parse trees connected by discourse re</context>
</contexts>
<marker>Galitsky, Usikov, Kuznetsov, 2013</marker>
<rawString>B. Galitsky, D. Usikov, S.O. Kuznetsov. 2013. Parse Thicket Representations for Answering Multisentence questions. 20th International Conference on Conceptual Structures, ICCS 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Galitsky</author>
<author>S O Kuznetsov</author>
</authors>
<title>Learning communicative actions of conflicting human agents.</title>
<date>2008</date>
<journal>J. Exp. Theor. Artif. Intell.</journal>
<volume>20</volume>
<issue>4</issue>
<pages>277--317</pages>
<marker>Galitsky, Kuznetsov, 2008</marker>
<rawString>B. Galitsky, S.O. Kuznetsov. 2008. Learning communicative actions of conflicting human agents. J. Exp. Theor. Artif. Intell. 20(4): 277-317.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Galitsky</author>
</authors>
<title>Machine Learning of Syntactic Parse Trees for Search and Classification of Text. Engineering Application of AI.</title>
<date>2012</date>
<marker>Galitsky, 2012</marker>
<rawString>B. Galitsky. 2012. Machine Learning of Syntactic Parse Trees for Search and Classification of Text. Engineering Application of AI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Moschitti</author>
</authors>
<title>Efficient Convolution Kernels for Dependency and Constituent Syntactic Trees.</title>
<date>2006</date>
<booktitle>In Proceedings of the 17th European Conference on Machine Learning,</booktitle>
<location>Berlin, Germany.</location>
<contexts>
<context position="12269" citStr="Moschitti, 2006" startWordPosition="2119" endWordPosition="2120"> Those portions of text were selected with obvious rhetoric relation between the clauses. We then fed Bing Search Engine API such queries and built the Answer Map for each candidate answer. We then ran the Answer Map - based 683 filter. Finally, we manually verify that these filtered answers are relevant to the initial questions and to the queries. We evaluated improvement of search relevance for compound queries by applying the DT rules. These rules provide Boolean decisions for candidate answers, but we compare them with score-based answer re-ranking based on ML of baseline SVM tree kernel (Moschitti, 2006), discourse-based SVM (Ilvovsky, 2014) and nearestneighbor Parse Thicket-based approach (Galitsky et al., 2013). The approach based on SVM tree kernel takes question-answer pairs (also from TREC dataset) and forms the positive set from the correct pairs and negative set from the incorrect pairs. The tree kernel learning (Duffy and Collins, 2002) for the pairs of extended parse trees produces multiple parse trees for each sentence, linking them by discourse relations of anaphora, communicative actions, “same entity” relation and rhetoric relations (Galitsky et al., 2014). In the Nearest Neighbo</context>
</contexts>
<marker>Moschitti, 2006</marker>
<rawString>A. Moschitti. 2006. Efficient Convolution Kernels for Dependency and Constituent Syntactic Trees. In Proceedings of the 17th European Conference on Machine Learning, Berlin, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Severyn</author>
<author>A Moschitti</author>
</authors>
<title>Structural relationships for large-scale learning of answer re-ranking. SIGIR</title>
<date>2012</date>
<pages>741--750</pages>
<marker>Severyn, Moschitti, 2012</marker>
<rawString>A. Severyn, A. Moschitti. 2012. Structural relationships for large-scale learning of answer re-ranking. SIGIR 2012: 741-750.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Severyn</author>
<author>A Moschitti</author>
</authors>
<title>Fast Support Vector Machines for Convolution Tree Kernels.</title>
<date>2012</date>
<journal>Data Mining Knowledge Discovery</journal>
<volume>25</volume>
<pages>325--357</pages>
<marker>Severyn, Moschitti, 2012</marker>
<rawString>A. Severyn, A. Moschitti. 2012. Fast Support Vector Machines for Convolution Tree Kernels. Data Mining Knowledge Discovery 25: 325-357.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>N Duffy</author>
</authors>
<title>Convolution kernels for natural language.</title>
<date>2002</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<pages>625--632</pages>
<marker>Collins, Duffy, 2002</marker>
<rawString>M. Collins and N. Duffy. 2002. Convolution kernels for natural language. In Proceedings of NIPS, 625– 632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Lee</author>
<author>A Chang</author>
<author>Y Peirsman</author>
<author>N Chambers</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
</authors>
<title>Deterministic coreference resolution based on entity-centric, precision-ranked rules.</title>
<date>2013</date>
<journal>Computational Linguistics</journal>
<volume>39</volume>
<issue>4</issue>
<marker>Lee, Chang, Peirsman, Chambers, Surdeanu, Jurafsky, 2013</marker>
<rawString>H. Lee, A. Chang, Y. Peirsman, N. Chambers, Mihai Surdeanu and Dan Jurafsky. 2013. Deterministic coreference resolution based on entity-centric, precision-ranked rules. Computational Linguistics 39(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Croft</author>
<author>D Metzler</author>
<author>T Strohman</author>
</authors>
<title>Search Engines - Information Retrieval in Practice.</title>
<date>2009</date>
<publisher>Pearson Education. North America.</publisher>
<marker>Croft, Metzler, Strohman, 2009</marker>
<rawString>B. Croft, D. Metzler, T. Strohman. 2009. Search Engines - Information Retrieval in Practice. Pearson Education. North America.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory.</title>
<date>1995</date>
<publisher>Springer-Verlag.</publisher>
<marker>Vapnik, 1995</marker>
<rawString>V. Vapnik. 1995. The Nature of Statistical Learning Theory. – Springer-Verlag.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>