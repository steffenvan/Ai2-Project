<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001431">
<title confidence="0.986259">
Czech-English Dependency-based Machine Translation
</title>
<author confidence="0.997911">
Martin &apos;emejrek, Jan Cufin, and JirI Havelka
</author>
<affiliation confidence="0.995879333333333">
Institute of Formal and Applied Linguistics,
and Center for Computational Linguistics
Charles University in Prague
</affiliation>
<email confidence="0.988406">
fcmejrek,curin,havelkal@ufal.mff.cuni.cz
</email>
<sectionHeader confidence="0.995541" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999873705882353">
We present some preliminary results of a
Czech-English translation system based
on dependency trees. The fully auto-
mated process includes: morphological
tagging, analytical and tectogrammat-
ical parsing of Czech, tectogrammati-
cal transfer based on lexical substitu-
tion using word-to-word translation dic-
tionaries enhanced by the information
from the English-Czech parallel corpus
of WSJ, and a simple rule-based system
for generation from English tectogram-
matical representation. In the evalua-
tion part, we compare results of the fully
automated and the manually annotated
processes of building the tectogrammat-
ical representation.&apos;
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999932222222222">
The experiment described in this paper is an at-
tempt to develop a full MT system based on de-
pendency trees (DBMT). Dependency trees repre-
sent the sentence structure as concentrated around
the verb and its valency. We use tectogrammatical
dependency trees capturing the linguistic meaning
of the sentence. In a tectogrammatical dependency
tree, only autosemantic (lexical) words are repre-
sented as nodes, dependencies (edges) are labeled
</bodyText>
<footnote confidence="0.987528">
1This research was supported by the following grants:
MMT &apos;d2 Grant No. LNO0A063 and NSF Grant No. IIS-
0121285.
</footnote>
<bodyText confidence="0.999871794117647">
by tectogrammatical functors denoting the seman-
tic roles, the information conveyed by auxiliary
words is stored in attributes of the nodes. For de-
tails about the tectogrammatical representation see
Haji6ova et al. (2000), an example of a tectogram-
matical tree can be found in Figure 3.
MAGENTA (Haji 6 et al., 2002) is an exper-
imental framework for machine translation im-
plemented during 2002 NLP Workshop at CLSP,
Johns Hopkins University in Baltimore. Modules
for parsing of Czech, lexical transfer, a prototype
of a statistical tree-to-tree transducer for structural
transformations used during transfer and genera-
tion, and a language model for English based on
dependency syntax are integrated in one pipeline.
For processing the Czech part of the data,
we reuse some modules of the MAGENTA sys-
tem, but instead of MAGENTA&apos;s statistical tree-
to-tree transducing module and subsequent lan-
guage model, we implement a rule-based method
for generating English output directly from the
tectogrammatical representation.
First, we summarize resources available for the
experiments (Section 2). Section 3 describes the
automatic procedures used for the preparation of
both training and testing data, including morpho-
logical tagging, and analytical and tectogrammat-
ical parsing of Czech input. In Section 4 we de-
scribe the process of the filtering of dictionaries
used in the transfer procedure (for its character-
ization, see Section 5). The generation process
consisting mainly of word reordering and lexical
insertions is explained in Section 6, an example il-
lustrating the generation steps is presented in Sec-
</bodyText>
<page confidence="0.997883">
83
</page>
<bodyText confidence="0.9997268">
tion 7. For the evaluation of the results we use
the BLEU score (Papineni et al., 2001). Section 8
compares translations generated from automati-
cally built and manually annotated tectogrammat-
ical representations. We also compare the results
with the output generated by the statistical trans-
lation system GIZA++/ISI ReWrite Decoder (Al-
Onaizan et al., 1999; Och and Ney, 2000; Ger-
mann et al., 2001), trained on the same parallel
corpus.
</bodyText>
<sectionHeader confidence="0.981535" genericHeader="introduction">
2 Data Resources
</sectionHeader>
<subsectionHeader confidence="0.969079">
2.1 The Prague Dependency Treebank
</subsectionHeader>
<bodyText confidence="0.99989675">
The Prague Dependency Treebank project
(Bohmova et al., 2001) aims at complex anno-
tation of a corpus containing about 1.8M word
occurrences (about 80,000 running text sentences)
in Czech. The annotation, which is based on
dependency syntax, is carried out in three steps:
morphological, analytical, and tectogrammatical.
The first two have been finished so far, presently,
there are about 18,000 sentences tectogrammat-
ically annotated. See Haji 6 et al. (2001) and
Haji6ova et al. (2000) for details on analytical and
on tectogrammatical annotation, respectively.
</bodyText>
<subsectionHeader confidence="0.996734">
2.2 English to Czech translation of Penn
Treebank
</subsectionHeader>
<bodyText confidence="0.9878246875">
So far, there was no considerably large manu-
ally syntactically annotated English-Czech paral-
lel corpus, so we decided to translate by human
translators a part of an existing syntactically anno-
tated English corpus (we chose articles from Wall
Street Journal included in Penn Treebank 3), rather
than to syntactically annotate existing English-
Czech parallel texts. The translators were asked to
translate each English sentence as a single Czech
sentence and also to stick to the original sentence
construction if possible. For the experiment, there
were 11,189 WSJ sentences translated into Czech
by human translators (see Table 1). This parallel
corpus was split into three parts, namely training,
devtest and evaltest parts.2 The work on transla-
tions still continues, aiming at covering the whole
Penn Treebank.
training data, heldout data for running tests, and data for
the final evaluation, respectively
For both training and evaluation measured by
BLEU metric, 490 sentences from devtest and
evaltest data sets were retranslated back from
Czech into English by 4 different translators (see
an example of retranslations in Figure 2 and Sec-
tion 8 for details on the evaluation).
To be able to observe the relationship between
the tectogrammatical structure of a Czech sen-
tence and its English translation (without distor-
tions caused by automatic parsing), we have man-
ually annotated on the tectogrammatical level the
Czech sentences from devtest and evaltest data
sets.
</bodyText>
<table confidence="0.9411295">
data category #sentence pairs
training 10,699
devtest 242
evaltest 248
</table>
<tableCaption confidence="0.904347">
Table I: Number of sentence pairs in English-
Czech WSJ corpus
</tableCaption>
<subsectionHeader confidence="0.998658">
2.3 English Monolingual Corpus
</subsectionHeader>
<bodyText confidence="0.9988995">
The Penn Treebank data contain manually as-
signed morphological tags and this informa-
tion substantially simplifies lemmatization. The
lemmatization procedure searches a list of triples
containing word form, morphological tag and
lemma, extracted from a large corpus. It looks
for a triple with a matching word form and mor-
phological tag, and chooses the lemma from this
triple. The large corpus of English3 used in
this experiment was automatically morphologi-
cally tagged by MXPOST tagger (Ratnaparkhi,
1996) and lemmatized by the morpha tool (Min-
nen et al., 2001), and contains 365 million words
in 13 million sentences.
</bodyText>
<footnote confidence="0.999352">
3It consists of English part of French-English Canadian
Hansards corpus, English part of English-Czech Readers&apos; Di-
gest corpus, English part of English-Czech IBM corpus, Wall
Street Journal (years 95, 96), L.A. Times/Wash. Post (May
1994 — August 1997), Reuters General News (April 1994 —
December 1996), Reuters Financial News (April 1994 — De-
cember 1996).
</footnote>
<page confidence="0.999541">
84
</page>
<sectionHeader confidence="0.994265" genericHeader="method">
3 Czech Data Processing
</sectionHeader>
<subsectionHeader confidence="0.994656">
3.1 Morphological Tagging and
Lemmatization
</subsectionHeader>
<bodyText confidence="0.9998958">
The Czech translations of Penn Treebank were
automatically tokenized and morphologically
tagged, each word form was assigned a basic form
— lemma by Hajie and Hladka (1998) tagging
tools.
</bodyText>
<subsectionHeader confidence="0.999874">
3.2 Analytical Parsing
</subsectionHeader>
<bodyText confidence="0.999975777777778">
The analytical parsing of Czech runs in two steps:
the statistical dependency parser, which creates the
structure of a dependency tree, and a classifier as-
signing analytical functors. We carried out two
parallel experiments with two parsers available for
Czech, parser I (Hajie et al., 1998) and parser II
(Charniak, 1999). In the second step, we used
a module for automatic analytical functor assign-
ment (2abokrtskyT et al., 2002).
</bodyText>
<subsectionHeader confidence="0.9333485">
3.3 Conversion into Tectogrammatical
Representation
</subsectionHeader>
<bodyText confidence="0.999760714285714">
During the tectogrammatical parsing of Czech,
the analytical tree structure is converted into the
tectogrammatical one. These automatic transfor-
mations are based on linguistic rules (Bohmova,
2001). Subsequently, tectogrammatical functors
are assigned by the C4.5 classifier (2abokrtsk9 et
al., 2002).
</bodyText>
<sectionHeader confidence="0.986871" genericHeader="method">
4 Czech-English Word-to-Word
Translation Dictionaries
</sectionHeader>
<subsectionHeader confidence="0.999854">
4.1 Manual Dictionary Sources
</subsectionHeader>
<bodyText confidence="0.999913">
There were three different sources of Czech-
English manual dictionaries available, two of
them were downloaded from the Web (WinGED,
GNU/FDL), and one was extracted from the Czech
and English EuroWordNet. See dictionary param-
eters in Table 2.
</bodyText>
<subsectionHeader confidence="0.992974">
4.2 Dictionary Filtering
</subsectionHeader>
<bodyText confidence="0.9997815">
For a subsequent use of these dictionaries for a
simple transfer from the Czech to the English tec-
togrammatical trees (see Section 5), a relatively
huge number of possible translations for each en-
</bodyText>
<table confidence="0.6905052">
dictionary #entries #transl weight
EuroWordNet 12,052 48,525 3
GNU/FDL 12,428 17,462 3
WinGED 16,296 39,769 2
merged 33,028 87,955
</table>
<tableCaption confidence="0.977211">
Table 2: Dictionary parameters and weights
</tableCaption>
<bodyText confidence="0.99914">
try4 had to be filtered. The aim of the filtering is
to exclude synonyms from the translation list, i.e.
to choose one representative per meaning
First, all dictionaries are converted into a uni-
fied XML format and merged together preserving
information about the source dictionary.
This merged dictionary consisting of en-
try/translation pairs (Czech entries and English
translations in our case) is enriched by the follow-
ing procedures:
</bodyText>
<listItem confidence="0.99937875">
• Frequencies of English word obtained from
large English monolingual corpora are added
to each translation. See description of the
corpora in Section 2.3.
• Czech POS tag and stem are added to each
entry using the Czech morphological ana-
lyzer (Haji 6 and Hladka, 1998).
• English POS tag is added to each transla-
</listItem>
<bodyText confidence="0.96696775">
tion. If there is more than one English POS
tag obtained from the English morpholog-
ical analyzer (Ratnaparkhi, 1996), the En-
glish POS tag is &amp;quot;disambiguated&amp;quot; accord-
ing to the Czech POS in the appropriate en-
try/translation pair.
We select several relevant translations for each
entry taking into account the sum of the weights
of the source dictionaries (see dictionary weights
in Table 2), the frequencies from English monolin-
gual corpora, and the correspondence of the Czech
and English POS tags.
</bodyText>
<subsectionHeader confidence="0.999715">
4.3 Scoring Translations Using GIZA++
</subsectionHeader>
<bodyText confidence="0.999775">
To make the dictionary more sensitive to a spe-
cific domain, which is in our case the domain of
</bodyText>
<footnote confidence="0.979227666666667">
4For example for WinGED dictionary it is 2.44 transla-
tions per entry in average, and excluding 1-1 entry/translation
pairs even 4.51 translations/entry.
</footnote>
<page confidence="0.997321">
85
</page>
<figure confidence="0.996008866666667">
&lt;e&gt;zesilit&lt;t&gt;V 5 Czech-English Lexical Transfer
[FSG1&lt;tr&gt;increase&lt;trt&gt;V&lt;prob&gt;0.327524
[FSG1&lt;tr&gt;reinforce&lt;trt&gt;V&lt;prob&gt;0.280199
[FSG1&lt;tr&gt;amplify&lt;trt&gt;V&lt;prob&gt;0.280198
[G]&lt;tr&gt;re-enforce&lt;trt&gt;V&lt;prob&gt;0.0560397
[G]&lt;tr&gt;reenforce&lt;trt&gt;V&lt;prob&gt;0.0560397
&lt;e&gt;vyber&lt;t&gt;N
[FSG1&lt;tr&gt;choice&lt;trt&gt;N&lt;prob&gt;0.404815
[FSG1&lt;tr&gt;selection&lt;trt&gt;N&lt;prob&gt;0.328721
[G]&lt;tr&gt;option&lt;trt&gt;N&lt;prob&gt;0.0579416
[G]&lt;tr&gt;digest&lt;trt&gt;N&lt;prob&gt;0.0547869
[G]&lt;tr&gt;compilation&lt;trt&gt;N&lt;prob&gt;0.054786
11&lt;tr&gt;alternative&lt;trt&gt;N&lt;prob&gt;0.0519888
[]&lt;tr&gt;sample&lt;trt&gt;N&lt;prob&gt;0.0469601
&lt;e&gt;selekce&lt;t&gt;N
</figure>
<bodyText confidence="0.999086142857143">
In this step, tectogrammatical trees automatically
created from Czech input text are transfered into
&amp;quot;English&amp;quot; tectogrammatical trees. The transfer
procedure itself is a lexical replacement of the
tectogrammatical base form (trlemma) attribute
of autosemantic nodes by its English equivalent
found in the Czech-English probabilistic dictio-
</bodyText>
<listItem confidence="0.524016">
9 nary.
</listItem>
<bodyText confidence="0.999064">
For practical reasons such as time efficiency, a
simplified version, taking into account only the
most probable translation, was used. Also 1-2
translations were handled as 1-1 — two words in
one trlemma attribute.
Compare an example of a Czech tectogrammat-
ical tree after the lexical transfer step (Figure 3),
with the original English sentence in Figure 2.
</bodyText>
<figure confidence="0.99454">
[FSG]&lt;tr&gt;selection&lt;trt&gt;N&lt;prob&gt;0.542169
[FSG1&lt;tr&gt;choice&lt;trt&gt;N&lt;prob&gt;0.457831
[S] ... dictionary weight selection
[G] ... GIZA++ selection
[F] ... final selection
</figure>
<figureCaption confidence="0.972162">
Figure 1: Sample of the Czech-English dictionary
used for the transfer.
</figureCaption>
<bodyText confidence="0.999972083333333">
financial news, we created a probabilistic Czech-
English dictionary by running GIZA++ training
(translation models 1-4, see Och and Ney (2000))
on the training part of the English-Czech WSJ par-
allel corpus extended by the parallel corpus of en-
try/translation pairs from the manual dictionary.
As a result, the entry/translation pairs seen in the
parallel corpus of WSJ become more probable.
For entry/translation pairs not seen in the paral-
lel text, the probability distribution among transla-
tions is uniform. The translation is &amp;quot;GIZA++ se-
lected&amp;quot; if its probability is higher than a threshold,
which is in our case set to 0.10.
The final selection contains translations selected
by both the dictionary and GIZA++ selectors. In
addition, translations not covered by the original
dictionary can be included into the final selection,
if they were newly discovered in the parallel cor-
pus by GIZA++ training and their probability is
significant (higher than the most probable transla-
tion so far).
The translations from the final selection are
used in the transfer. See sample of the dictionary
in Figure 1.
</bodyText>
<sectionHeader confidence="0.998397" genericHeader="method">
6 Generating English Output
</sectionHeader>
<bodyText confidence="0.999894666666667">
When generating from the tectogrammatical rep-
resentation, two kinds of operations (although of-
ten interfering) have to be performed: lexical in-
sertions and transformations modifying word or-
der.
Since only autosemantic (lexical) words are
represented in the tectogrammatical structure of
the sentence, for a successful generation of En-
glish plain-text output, insertion of synsemantic
(functional) words (such as prepositions, auxiliary
verbs, and articles) is needed. Unlike in Czech,
where different semantic roles are expressed by
different cases, in English, it is both prepositions
and word order that are used to convey their mean-
ing.
In our implementation, the generation process
consists of the following five consecutive groups
of generation tasks:
</bodyText>
<listItem confidence="0.9947016">
1. determining contextual boundness
2. reordering of constituents
3. generation of verb forms
4. insertion of prepositions and articles
5. morphology
</listItem>
<page confidence="0.962786">
86
</page>
<figure confidence="0.50218">
Original: Kaufman &amp; Broad, a home building company, declined to identify the institutional investors.
Czech: Kaufman &amp; Broad, firma specializujici se na byrovou vysta.vbii, (Amnia institucion5M1 Mvestory jmenovat.
R1: Kaufman &amp; Broad, a company specializing in housing development, refused to give the names of their corporate investors.
R2: Kaufman &amp; Broad, a firm specializing in apartment building, refused to list institutional investors.
R3: Kaufman &amp; Broad, a firm specializing in housing construction, refused to name the institutional investors.
R4: Residential construction company Kaufman &amp; Broad refused to name the institutional investors.
</figure>
<figureCaption confidence="0.961537">
Figure 2: A sample English sentence from WSJ, its Czech translation, and four reference retranslations.
</figureCaption>
<figure confidence="0.998963724137931">
SENT
PRED
Predicate
decline
.0 \ 0
e Pee
/ Apposition
/ APS jmenovat
PAT
Patient
name
i &amp;Forn; firma &amp;Cor investor
i ACT AP ACT AP ACT A
i Actor Actor Actor Patient
firm investor
o- ..-ii.
Kaufman &amp; Broad specializujici se institucionalni
FPHR FPHR FPHR RSTR N RSTR
ForeignPhrase ForeignPhrase ForeignPhrase RestrictionNN Restriction
Kaufman &amp; Broad specializing institutional
0
/vjistavba
/ PAT
/ Patient
construction
/fbytto
RSTR
Restriction
ia
</figure>
<figureCaption confidence="0.999607">
Figure 3: An example of a manually annotated Czech tectogrammatical tree with Czech lemmas, tec-
</figureCaption>
<bodyText confidence="0.487573">
togrammatical functors, their glosses, and automatic word-to-word translations to English.
</bodyText>
<listItem confidence="0.901437">
Cz Kaufman &amp; Broad firma specializujici_se bytovy vystayba
0. Kaufman 8i Broad firm specializing flat construction
1. Kaufman &amp; Broad firm specializing flat construction
2. Kaufman 8i Broad firm specializing flat construction
3. Kaufman &amp; Broad firm specializing flat construction
4. Kaufman &amp; Broad DEF firm specializing INDEF flat construction
5. Kaufman &amp; Broad the firm specializing a flat construction
</listItem>
<figureCaption confidence="0.97645075">
odmitnout instit, investor jmenovat
decline instit. investor name
decline instit. investor name
decline name instit. investor
decline to name instit. investor
decline to name DEF instit. investor
declined to name the instit. investors
Figure 4: An illustration of the generation process for the resulting English sentence:
</figureCaption>
<bodyText confidence="0.715573">
Kaufman &amp; Broad, the firm specializing a flat construction declined to name the institutional investors.
</bodyText>
<page confidence="0.997127">
87
</page>
<bodyText confidence="0.9999276">
In each of these steps, the whole tectogrammati-
cal tree is traversed and rules pertaining to a partic-
ular group are applied. Considering the nature of
the selected data, our system is limited to declara-
tive sentences only.
</bodyText>
<subsectionHeader confidence="0.795437">
Contextual boundness
</subsectionHeader>
<bodyText confidence="0.999977857142857">
Since neither the automatically created nor the
manually annotated tectogrammatical trees cap-
ture topic—focus articulation (information struc-
ture), we make use of the fact that Czech is a lan-
guage with a relatively high degree of word order
freedom and uses mainly the left to right order-
ing to express the information structure. In written
text, given (contextually bound) information tends
to be placed at the beginning of the sentence, while
new (contextually non-bound) information is ex-
pressed towards the end of the sentence. The de-
gree of communicative dynamism increases from
left to right, and the boundary between the contex-
tually bound nodes on the left-hand side and the
contextually non-bound nodes on the right-hand
side is the verb. We consider information struc-
ture to be recursive in the dependency tree, and
use it both for the reordering of constituents in
the English counterpart of the Czech sentence, and
for determining the definiteness of noun phrases in
English.
</bodyText>
<subsectionHeader confidence="0.872363">
Reordering of constituents
</subsectionHeader>
<bodyText confidence="0.986665481481481">
Unlike Czech, English is a language with quite
a rigid SVO word order, therefore verb comple-
ments and adjuncts have to be rearranged in order
to conform with the constraints of English gram-
mar, according to the sentence modality. In the
basic case of a simple declarative sentence, we
place first the contextually bound adjuncts, then
the subject, the verb, verb complements (such
as direct and indirect objects), and contextually
non-bound adjuncts, preserving the relative order
of constituents in all these groups. The func-
tors in a tectogrammatical tree denote the seman-
tic roles of nodes. So we can use the contextual
boundness/non-boundness of ACTor (deep sub-
ject), PATient (deep object), or ADDRessee, and
realize the most contextually bound node as the
surface subject.
Generation of verb forms
According to the semantic role selected as the
subject of the verb, the active or passive voice of
the verb is chosen. Categories such as tense and
mood are taken over from the information stored
in the Czech tectogrammatical node. Person is de-
termined by agreement with the subject. Auxiliary
verbs needed to create a complex verb form are
inserted as separate children nodes of the lexical
verb.
</bodyText>
<subsectionHeader confidence="0.883078">
Insertion of prepositions and articles
</subsectionHeader>
<bodyText confidence="0.9999812">
The correspondence between tectogrammatical
functors and auxiliary words is a complex task.
In some cases, there is one predominant surface
realization of the functor, but, unfortunately, in
other cases, there are several possible surface re-
alizations, none of them significantly dominant
(mostly in cases of spatial and temporal adjuncts).
For deciding on the appropriate surface realization
of a preposition, both the original Czech preposi-
tion and the English lexical word being generated
should be taken into account.
The task of generating articles in English is non-
trivial and challenging due to the absence of ar-
ticles in Czech. The first hint about what article
should be used is the contextual boundness/non-
boundness of a noun phrase. The definite article
is inserted when the noun phrase is either contex-
tually bound, postmodified, or is premodified by
a superlative adjective or ordinal numeral. Other-
wise, the indefinite article is used.
An article may be prevented from being inserted
altogether in cases where uncountable or proper
nouns are concerned, or the noun phrase is prede-
termined by some other means (such as possessive
and demonstrative pronouns).
</bodyText>
<subsectionHeader confidence="0.613651">
Morphology
</subsectionHeader>
<bodyText confidence="0.999807666666667">
When generating the surface word form, we are
searching through the table of triples [word form,
morphological tag, lemma] (see Section 2.3) for
the word form corresponding to the given lemma
and morphological tag. Should we fail in find-
ing it, we generate the form using simple rules.
Also, the appropriate form of the indefinite article
is selected according to the immediately following
word.
</bodyText>
<page confidence="0.997564">
88
</page>
<table confidence="0.993043375">
MT system BLEU — devtest BLEU — evaltest
DBMT with parser I 0.1857 0.1634
DBMT with parser II 0.1916 0.1705
DBMT on manually annotated trees 0.1974 0.1704
GIZA++ &amp; ReWrite — plain text 0.0971 0.0590
GIZA++ &amp; ReWrite — lemmatized 0.2222 0.2017
MAGENTA WS&apos;02 0.0640 0.0420
Avg. BLEU score of human retranslations 0.5560
</table>
<tableCaption confidence="0.999736">
Table 3: BLEU score of different MT systems
</tableCaption>
<sectionHeader confidence="0.746539" genericHeader="method">
7 An Example
</sectionHeader>
<bodyText confidence="0.994286391304348">
Figure 4 illustrates the whole process of trans-
lating a sample Czech sentence, starting from its
manually annotated tectogrammatical representa-
tion (Figure 3). The first line contains lemmas
of the autosemantic words of the sample sentence
from Figure 2. The next line, labeled 0, shows
their word-to-word translations. The remaining
lines correspond to the generation steps described
in Section 6.
The order of nodes is used to determine their
contextual boundness (line 1, contextually non-
bound nodes are in italics). In line 2, the con-
stituents are reordered according to contextual
boundness and their tectogrammatical functors.
The form of the complex verb is handled in step 3.
In the next step, prepositions and articles are in-
serted. However, not every functor&apos;s realization
can be reconstructed easily, as can be seen in the
case of the missing preposition &amp;quot;in&amp;quot;. It is also hard
to decide whether a particular word was used in an
uncountable sense (see the wrongly inserted indef-
inite article). The last line contains the final mor-
phological realization of the sentence.
</bodyText>
<sectionHeader confidence="0.949501" genericHeader="evaluation">
8 Evaluation of Results
</sectionHeader>
<bodyText confidence="0.999957190476191">
We evaluated our translations with IBM&apos;s BLEU
evaluation metric (Papineni et al., 2001), using the
same evaluation method and reference retransla-
tions that were used for evaluation at HLT Work-
shop 2002 at CLSP (Haji 6 et al., 2002). We used
four reference retranslations of 490 sentences se-
lected from the WSJ sections 22, 23, and 24,
which were themselves used as the fifth reference.
The evaluation method used is to hold out each ref-
erence in turn and evaluate it against the remaining
four, averaging the five BLEU scores.
Table 3 shows final results of our system com-
pared with GIZA++ and MAGENTA&apos;s results.
The DBMT with parser I and parser II ex-
periments represent a fully automated translation,
while the DBMT experiment on manually anno-
tated trees generates from the Czech tectogram-
matical trees prepared by human annotators.
For the purposes of comparison, GIZA++ statis-
tical machine translation toolkit with the ReWrite
decoder were customized to translate from Czech
to English and two experiments with different con-
figurations were performed. The first one takes
the Czech plain text as the input, the second
one translates from lemmatized Czech. In ad-
dition, the word-to-word dictionary described in
Section 4 was added to the training data (every
entry-translation pair as one sentence pair). The
language model was trained on a large mono-
lingual corpus of Wall Street Journal containing
about 52M words. The corpus was selected from
the corpus mentioned in Section 2.3.
We also present the score reached by the MA-
GENTA system.
All systems were evaluated against the same
sets of references.
Both our experiments show a considerable im-
provement over MAGENTA&apos;s performance, they
also score better than GIZA++/ReWrite trained
on word forms. We were still outperformed by
GIZA++/ReWrite trained on lemmas, but it makes
use of a large language model.
</bodyText>
<sectionHeader confidence="0.974843" genericHeader="conclusions">
9 Conclusion and Further Development
</sectionHeader>
<bodyText confidence="0.99646">
The system described comprises the whole way
from the Czech plain-text sentence to the English
</bodyText>
<page confidence="0.999298">
89
</page>
<bodyText confidence="0.999966611111111">
one. It integrates the latest results in analytical and
tectogrammatical parsing of Czech, experiments
with existing word-to-word dictionaries combined
with those automatically obtained from a paral-
lel corpus, lexical transfer, and simple rule-based
generation from the tectogrammatical representa-
tion.
In spite of certain known shortcomings of state-
of-the-art parsers of Czech, we are convinced that
the most significant improvement of our system
can be achieved by further refining and broaden-
ing the coverage of structural transformations and
lexical insertions. We consider allowing multi-
ple translation possibilities and using additional
sources of information relevant for surface real-
ization of tectogrammatical functors. Finally, an
integrated language model would discriminate the
best of the hypotheses.
</bodyText>
<sectionHeader confidence="0.998509" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99993435064935">
Yaser Al-Onaizan, Jan Cuiin, Michael Jahr, Kevin
Knight, John Lafferty, Dan Melamed, Franz-Josef
Och, David Purdy, Noah A. Smith, and David
Yarowsky. 1999. The statistical machine transla-
tion. Technical report. WS&apos; 99, Johns Hopkins Uni-
versity.
Alena Bohmova, Jan Haji, Eva Hajieova, and Barbora
Hladka. 2001. The Prague Dependency Treebank:
Three-Level Annotation Scenario, In Anne Abeille,
editor, Treebanks: Building and Using Syntactically
Annotated Corpora. Kluwer Academic Publishers.
Alena Bohmova. 2001. Automatic procedures in
tectogrammatical tagging. The Prague Bulletin of
Mathematical Linguistics, 76.
Eugene Charniak. 1999. A maximum-entropy-
inspired parser. Technical Report CS-99-12.
Ulrich Germann, Michael Jahr, Kevin Knight, Daniel
Marcu, and Kenji Yamada. 2001. Fast decoding
and optimal decoding for machine translation. In
Proceedings of the 39th Annual Meeting of the As-
sociation for Computational Linguistics, pages 228-
235.
Jan Haji e and Barbora Hladka. 1998. Tagging Inflec-
tive Languages: Prediction of Morphological Cate-
gories for a Rich, Structured Tagset. In Proceedings
of COLING-ACL Conference, pages 483-490, Mon-
treal, Canada.
Jan Haji, Eric Brill, Michael Collins, Barbora Hladka,
Douglas Jones, Cynthia Ku o, Lance Ramshaw, Oren
Schwartz, Christopher Tillmann, and Daniel Zeman.
1998. Core Natural Language Processing Technol-
ogy Applicable to Multiple Languages. Technical
Report Research Note 37, Center for Language and
Speech Processing, Johns Hopkins University, Bal-
timore, MD.
Jan Haile, Jarmila Panevova, Eva Buraliova, Zdefika
Uregova, Alla Bemova., Jan &apos;tepanek, Petr Pajas,
and Jill Karnfk, 2001. A Manual for Analytic
Layer Tagging of the Prague Dependency Treebank.
Prague, Czech Republic. English translation of
the original Czech version, http: //shadow.
ms.mff.cuni.cz/pdt/Corpora/PDT 1.
0/References/aman en.pdf96.
Jan Haji, Martin &apos;Cmejrek, Bonnie Dorr, Yuan Ding,
Jason Eisner, Daniel Gildea, Terry Koo, Kristen Par-
ton, Gerald Penn, Dragomir Radev, and Owen Ram-
bow. 2002. Natural language generation in the
context of machine translation. Technical report.
WS&apos; 02, Johns Hopkins University — in preparation.
Eva Hajieova, Jarmila Panevova, and Petr Sgall.
2000. A Manual for Tectogrammatic Tagging of
the Prague Dependency Treebank. Technical Re-
port TR-2000-09, f_JFAL MFF UK, Prague, Czech
Republic.
G. Minnen, J. Carroll, and D. Pearce. 2001. Applied
morphological processing of English. Natural Lan-
guage Engineering, 7(3):207-223.
F. J. Och and H. Ney. 2000. Improved statistical align-
ment models. In Proc. of the 38th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 440-447, Hongkong, China, October.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic
evaluation of machine translation. Technical Report
RC22176, IBM.
Adwait Ratnaparkhi. 1996. A maximum entropy part-
of-speech tagger. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 133-142, University of Pennsylvania,
May. ACL.
Zdenek 2abokrts14, Petr Sgall, and Meroski Sago.
2002. Machine learning approach to automatic
functor assignment in the Prague Dependency Tree-
bank. In Proceedings of LREC 2002 (Third Interna-
tional Conference on Language Resources and Eval-
uation), volume V, pages 1513-1520, Las Palmas de
Gran Canaria, Spain.
</reference>
<page confidence="0.99864">
90
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.388276">
<title confidence="0.999874">Czech-English Dependency-based Machine Translation</title>
<author confidence="0.999263">Martin &apos;emejrek</author>
<author confidence="0.999263">Jan Cufin</author>
<author confidence="0.999263">JirI Havelka</author>
<affiliation confidence="0.942200333333333">Institute of Formal and Applied Linguistics, and Center for Computational Linguistics Charles University in Prague</affiliation>
<email confidence="0.99222">fcmejrek,curin,havelkal@ufal.mff.cuni.cz</email>
<abstract confidence="0.999839470588235">We present some preliminary results of a Czech-English translation system based on dependency trees. The fully automated process includes: morphological tagging, analytical and tectogrammatical parsing of Czech, tectogrammatical transfer based on lexical substitution using word-to-word translation dictionaries enhanced by the information from the English-Czech parallel corpus of WSJ, and a simple rule-based system for generation from English tectogrammatical representation. In the evaluation part, we compare results of the fully automated and the manually annotated processes of building the tectogrammat-</abstract>
<intro confidence="0.470416">ical representation.&apos;</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yaser Al-Onaizan</author>
<author>Jan Cuiin</author>
<author>Michael Jahr</author>
<author>Kevin Knight</author>
<author>John Lafferty</author>
<author>Dan Melamed</author>
<author>Franz-Josef Och</author>
<author>David Purdy</author>
<author>Noah A Smith</author>
<author>David Yarowsky</author>
</authors>
<title>The statistical machine translation.</title>
<date>1999</date>
<tech>Technical report. WS&apos; 99,</tech>
<institution>Johns Hopkins University.</institution>
<marker>Al-Onaizan, Cuiin, Jahr, Knight, Lafferty, Melamed, Och, Purdy, Smith, Yarowsky, 1999</marker>
<rawString>Yaser Al-Onaizan, Jan Cuiin, Michael Jahr, Kevin Knight, John Lafferty, Dan Melamed, Franz-Josef Och, David Purdy, Noah A. Smith, and David Yarowsky. 1999. The statistical machine translation. Technical report. WS&apos; 99, Johns Hopkins University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alena Bohmova</author>
<author>Jan Haji</author>
<author>Eva Hajieova</author>
<author>Barbora Hladka</author>
</authors>
<title>The Prague Dependency Treebank: Three-Level Annotation Scenario,</title>
<date>2001</date>
<editor>In Anne Abeille, editor,</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="3613" citStr="Bohmova et al., 2001" startWordPosition="535" endWordPosition="538">n 6, an example illustrating the generation steps is presented in Sec83 tion 7. For the evaluation of the results we use the BLEU score (Papineni et al., 2001). Section 8 compares translations generated from automatically built and manually annotated tectogrammatical representations. We also compare the results with the output generated by the statistical translation system GIZA++/ISI ReWrite Decoder (AlOnaizan et al., 1999; Och and Ney, 2000; Germann et al., 2001), trained on the same parallel corpus. 2 Data Resources 2.1 The Prague Dependency Treebank The Prague Dependency Treebank project (Bohmova et al., 2001) aims at complex annotation of a corpus containing about 1.8M word occurrences (about 80,000 running text sentences) in Czech. The annotation, which is based on dependency syntax, is carried out in three steps: morphological, analytical, and tectogrammatical. The first two have been finished so far, presently, there are about 18,000 sentences tectogrammatically annotated. See Haji 6 et al. (2001) and Haji6ova et al. (2000) for details on analytical and on tectogrammatical annotation, respectively. 2.2 English to Czech translation of Penn Treebank So far, there was no considerably large manuall</context>
</contexts>
<marker>Bohmova, Haji, Hajieova, Hladka, 2001</marker>
<rawString>Alena Bohmova, Jan Haji, Eva Hajieova, and Barbora Hladka. 2001. The Prague Dependency Treebank: Three-Level Annotation Scenario, In Anne Abeille, editor, Treebanks: Building and Using Syntactically Annotated Corpora. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alena Bohmova</author>
</authors>
<title>Automatic procedures in tectogrammatical tagging.</title>
<date>2001</date>
<journal>The Prague Bulletin of Mathematical Linguistics,</journal>
<volume>76</volume>
<contexts>
<context position="7743" citStr="Bohmova, 2001" startWordPosition="1169" endWordPosition="1170">ependency parser, which creates the structure of a dependency tree, and a classifier assigning analytical functors. We carried out two parallel experiments with two parsers available for Czech, parser I (Hajie et al., 1998) and parser II (Charniak, 1999). In the second step, we used a module for automatic analytical functor assignment (2abokrtskyT et al., 2002). 3.3 Conversion into Tectogrammatical Representation During the tectogrammatical parsing of Czech, the analytical tree structure is converted into the tectogrammatical one. These automatic transformations are based on linguistic rules (Bohmova, 2001). Subsequently, tectogrammatical functors are assigned by the C4.5 classifier (2abokrtsk9 et al., 2002). 4 Czech-English Word-to-Word Translation Dictionaries 4.1 Manual Dictionary Sources There were three different sources of CzechEnglish manual dictionaries available, two of them were downloaded from the Web (WinGED, GNU/FDL), and one was extracted from the Czech and English EuroWordNet. See dictionary parameters in Table 2. 4.2 Dictionary Filtering For a subsequent use of these dictionaries for a simple transfer from the Czech to the English tectogrammatical trees (see Section 5), a relativ</context>
</contexts>
<marker>Bohmova, 2001</marker>
<rawString>Alena Bohmova. 2001. Automatic procedures in tectogrammatical tagging. The Prague Bulletin of Mathematical Linguistics, 76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropyinspired parser.</title>
<date>1999</date>
<tech>Technical Report CS-99-12.</tech>
<contexts>
<context position="7383" citStr="Charniak, 1999" startWordPosition="1119" endWordPosition="1120">ber 1996). 84 3 Czech Data Processing 3.1 Morphological Tagging and Lemmatization The Czech translations of Penn Treebank were automatically tokenized and morphologically tagged, each word form was assigned a basic form — lemma by Hajie and Hladka (1998) tagging tools. 3.2 Analytical Parsing The analytical parsing of Czech runs in two steps: the statistical dependency parser, which creates the structure of a dependency tree, and a classifier assigning analytical functors. We carried out two parallel experiments with two parsers available for Czech, parser I (Hajie et al., 1998) and parser II (Charniak, 1999). In the second step, we used a module for automatic analytical functor assignment (2abokrtskyT et al., 2002). 3.3 Conversion into Tectogrammatical Representation During the tectogrammatical parsing of Czech, the analytical tree structure is converted into the tectogrammatical one. These automatic transformations are based on linguistic rules (Bohmova, 2001). Subsequently, tectogrammatical functors are assigned by the C4.5 classifier (2abokrtsk9 et al., 2002). 4 Czech-English Word-to-Word Translation Dictionaries 4.1 Manual Dictionary Sources There were three different sources of CzechEnglish </context>
</contexts>
<marker>Charniak, 1999</marker>
<rawString>Eugene Charniak. 1999. A maximum-entropyinspired parser. Technical Report CS-99-12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulrich Germann</author>
<author>Michael Jahr</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Kenji Yamada</author>
</authors>
<title>Fast decoding and optimal decoding for machine translation.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>228--235</pages>
<contexts>
<context position="3461" citStr="Germann et al., 2001" startWordPosition="511" endWordPosition="515">ure (for its characterization, see Section 5). The generation process consisting mainly of word reordering and lexical insertions is explained in Section 6, an example illustrating the generation steps is presented in Sec83 tion 7. For the evaluation of the results we use the BLEU score (Papineni et al., 2001). Section 8 compares translations generated from automatically built and manually annotated tectogrammatical representations. We also compare the results with the output generated by the statistical translation system GIZA++/ISI ReWrite Decoder (AlOnaizan et al., 1999; Och and Ney, 2000; Germann et al., 2001), trained on the same parallel corpus. 2 Data Resources 2.1 The Prague Dependency Treebank The Prague Dependency Treebank project (Bohmova et al., 2001) aims at complex annotation of a corpus containing about 1.8M word occurrences (about 80,000 running text sentences) in Czech. The annotation, which is based on dependency syntax, is carried out in three steps: morphological, analytical, and tectogrammatical. The first two have been finished so far, presently, there are about 18,000 sentences tectogrammatically annotated. See Haji 6 et al. (2001) and Haji6ova et al. (2000) for details on analyt</context>
</contexts>
<marker>Germann, Jahr, Knight, Marcu, Yamada, 2001</marker>
<rawString>Ulrich Germann, Michael Jahr, Kevin Knight, Daniel Marcu, and Kenji Yamada. 2001. Fast decoding and optimal decoding for machine translation. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics, pages 228-235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Haji e</author>
<author>Barbora Hladka</author>
</authors>
<title>Tagging Inflective Languages: Prediction of Morphological Categories for a Rich, Structured Tagset.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL Conference,</booktitle>
<pages>483--490</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="7022" citStr="e and Hladka (1998)" startWordPosition="1061" endWordPosition="1064">million sentences. 3It consists of English part of French-English Canadian Hansards corpus, English part of English-Czech Readers&apos; Digest corpus, English part of English-Czech IBM corpus, Wall Street Journal (years 95, 96), L.A. Times/Wash. Post (May 1994 — August 1997), Reuters General News (April 1994 — December 1996), Reuters Financial News (April 1994 — December 1996). 84 3 Czech Data Processing 3.1 Morphological Tagging and Lemmatization The Czech translations of Penn Treebank were automatically tokenized and morphologically tagged, each word form was assigned a basic form — lemma by Hajie and Hladka (1998) tagging tools. 3.2 Analytical Parsing The analytical parsing of Czech runs in two steps: the statistical dependency parser, which creates the structure of a dependency tree, and a classifier assigning analytical functors. We carried out two parallel experiments with two parsers available for Czech, parser I (Hajie et al., 1998) and parser II (Charniak, 1999). In the second step, we used a module for automatic analytical functor assignment (2abokrtskyT et al., 2002). 3.3 Conversion into Tectogrammatical Representation During the tectogrammatical parsing of Czech, the analytical tree structure </context>
</contexts>
<marker>e, Hladka, 1998</marker>
<rawString>Jan Haji e and Barbora Hladka. 1998. Tagging Inflective Languages: Prediction of Morphological Categories for a Rich, Structured Tagset. In Proceedings of COLING-ACL Conference, pages 483-490, Montreal, Canada.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jan Haji</author>
</authors>
<title>Eric Brill, Michael Collins, Barbora Hladka,</title>
<location>Douglas Jones, Cynthia Ku o, Lance Ramshaw, Oren</location>
<marker>Haji, </marker>
<rawString>Jan Haji, Eric Brill, Michael Collins, Barbora Hladka, Douglas Jones, Cynthia Ku o, Lance Ramshaw, Oren</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Tillmann Schwartz</author>
<author>Daniel Zeman</author>
</authors>
<title>Core Natural Language Processing Technology Applicable to Multiple Languages.</title>
<date>1998</date>
<tech>Technical Report Research Note 37,</tech>
<institution>Center for Language and Speech Processing, Johns Hopkins University,</institution>
<location>Baltimore, MD.</location>
<marker>Schwartz, Zeman, 1998</marker>
<rawString>Schwartz, Christopher Tillmann, and Daniel Zeman. 1998. Core Natural Language Processing Technology Applicable to Multiple Languages. Technical Report Research Note 37, Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Haile</author>
<author>Jarmila Panevova</author>
<author>Eva Buraliova</author>
<author>Zdefika Uregova</author>
<author>Alla Bemova</author>
<author>Jan &apos;tepanek</author>
<author>Petr Pajas</author>
<author>Jill Karnfk</author>
</authors>
<title>A Manual for Analytic Layer Tagging of the Prague Dependency Treebank. Prague, Czech Republic. English translation of the original Czech version, http: //shadow. ms.mff.cuni.cz/pdt/Corpora/PDT 1. 0/References/aman en.pdf96.</title>
<date>2001</date>
<marker>Haile, Panevova, Buraliova, Uregova, Bemova, &apos;tepanek, Pajas, Karnfk, 2001</marker>
<rawString>Jan Haile, Jarmila Panevova, Eva Buraliova, Zdefika Uregova, Alla Bemova., Jan &apos;tepanek, Petr Pajas, and Jill Karnfk, 2001. A Manual for Analytic Layer Tagging of the Prague Dependency Treebank. Prague, Czech Republic. English translation of the original Czech version, http: //shadow. ms.mff.cuni.cz/pdt/Corpora/PDT 1. 0/References/aman en.pdf96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Haji</author>
<author>Martin &apos;Cmejrek</author>
<author>Bonnie Dorr</author>
<author>Yuan Ding</author>
<author>Jason Eisner</author>
<author>Daniel Gildea</author>
<author>Terry Koo</author>
<author>Kristen Parton</author>
<author>Gerald Penn</author>
<author>Dragomir Radev</author>
<author>Owen Rambow</author>
</authors>
<title>Natural language generation in the context of machine translation.</title>
<date>2002</date>
<tech>Technical report. WS&apos; 02,</tech>
<institution>Johns Hopkins University</institution>
<note>in preparation.</note>
<marker>Haji, &apos;Cmejrek, Dorr, Ding, Eisner, Gildea, Koo, Parton, Penn, Radev, Rambow, 2002</marker>
<rawString>Jan Haji, Martin &apos;Cmejrek, Bonnie Dorr, Yuan Ding, Jason Eisner, Daniel Gildea, Terry Koo, Kristen Parton, Gerald Penn, Dragomir Radev, and Owen Rambow. 2002. Natural language generation in the context of machine translation. Technical report. WS&apos; 02, Johns Hopkins University — in preparation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eva Hajieova</author>
<author>Jarmila Panevova</author>
<author>Petr Sgall</author>
</authors>
<title>A Manual for Tectogrammatic Tagging of the Prague Dependency Treebank.</title>
<date>2000</date>
<tech>Technical Report TR-2000-09, f_JFAL MFF</tech>
<location>UK, Prague, Czech Republic.</location>
<marker>Hajieova, Panevova, Sgall, 2000</marker>
<rawString>Eva Hajieova, Jarmila Panevova, and Petr Sgall. 2000. A Manual for Tectogrammatic Tagging of the Prague Dependency Treebank. Technical Report TR-2000-09, f_JFAL MFF UK, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Minnen</author>
<author>J Carroll</author>
<author>D Pearce</author>
</authors>
<title>Applied morphological processing of English.</title>
<date>2001</date>
<journal>Natural Language Engineering,</journal>
<pages>7--3</pages>
<contexts>
<context position="6364" citStr="Minnen et al., 2001" startWordPosition="959" endWordPosition="963">n EnglishCzech WSJ corpus 2.3 English Monolingual Corpus The Penn Treebank data contain manually assigned morphological tags and this information substantially simplifies lemmatization. The lemmatization procedure searches a list of triples containing word form, morphological tag and lemma, extracted from a large corpus. It looks for a triple with a matching word form and morphological tag, and chooses the lemma from this triple. The large corpus of English3 used in this experiment was automatically morphologically tagged by MXPOST tagger (Ratnaparkhi, 1996) and lemmatized by the morpha tool (Minnen et al., 2001), and contains 365 million words in 13 million sentences. 3It consists of English part of French-English Canadian Hansards corpus, English part of English-Czech Readers&apos; Digest corpus, English part of English-Czech IBM corpus, Wall Street Journal (years 95, 96), L.A. Times/Wash. Post (May 1994 — August 1997), Reuters General News (April 1994 — December 1996), Reuters Financial News (April 1994 — December 1996). 84 3 Czech Data Processing 3.1 Morphological Tagging and Lemmatization The Czech translations of Penn Treebank were automatically tokenized and morphologically tagged, each word form wa</context>
</contexts>
<marker>Minnen, Carroll, Pearce, 2001</marker>
<rawString>G. Minnen, J. Carroll, and D. Pearce. 2001. Applied morphological processing of English. Natural Language Engineering, 7(3):207-223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proc. of the 38th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>440--447</pages>
<location>Hongkong, China,</location>
<contexts>
<context position="3438" citStr="Och and Ney, 2000" startWordPosition="507" endWordPosition="510">the transfer procedure (for its characterization, see Section 5). The generation process consisting mainly of word reordering and lexical insertions is explained in Section 6, an example illustrating the generation steps is presented in Sec83 tion 7. For the evaluation of the results we use the BLEU score (Papineni et al., 2001). Section 8 compares translations generated from automatically built and manually annotated tectogrammatical representations. We also compare the results with the output generated by the statistical translation system GIZA++/ISI ReWrite Decoder (AlOnaizan et al., 1999; Och and Ney, 2000; Germann et al., 2001), trained on the same parallel corpus. 2 Data Resources 2.1 The Prague Dependency Treebank The Prague Dependency Treebank project (Bohmova et al., 2001) aims at complex annotation of a corpus containing about 1.8M word occurrences (about 80,000 running text sentences) in Czech. The annotation, which is based on dependency syntax, is carried out in three steps: morphological, analytical, and tectogrammatical. The first two have been finished so far, presently, there are about 18,000 sentences tectogrammatically annotated. See Haji 6 et al. (2001) and Haji6ova et al. (2000</context>
<context position="11708" citStr="Och and Ney (2000)" startWordPosition="1710" endWordPosition="1713">anslation, was used. Also 1-2 translations were handled as 1-1 — two words in one trlemma attribute. Compare an example of a Czech tectogrammatical tree after the lexical transfer step (Figure 3), with the original English sentence in Figure 2. [FSG]&lt;tr&gt;selection&lt;trt&gt;N&lt;prob&gt;0.542169 [FSG1&lt;tr&gt;choice&lt;trt&gt;N&lt;prob&gt;0.457831 [S] ... dictionary weight selection [G] ... GIZA++ selection [F] ... final selection Figure 1: Sample of the Czech-English dictionary used for the transfer. financial news, we created a probabilistic CzechEnglish dictionary by running GIZA++ training (translation models 1-4, see Och and Ney (2000)) on the training part of the English-Czech WSJ parallel corpus extended by the parallel corpus of entry/translation pairs from the manual dictionary. As a result, the entry/translation pairs seen in the parallel corpus of WSJ become more probable. For entry/translation pairs not seen in the parallel text, the probability distribution among translations is uniform. The translation is &amp;quot;GIZA++ selected&amp;quot; if its probability is higher than a threshold, which is in our case set to 0.10. The final selection contains translations selected by both the dictionary and GIZA++ selectors. In addition, trans</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>F. J. Och and H. Ney. 2000. Improved statistical alignment models. In Proc. of the 38th Annual Meeting of the Association for Computational Linguistics, pages 440-447, Hongkong, China, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2001</date>
<tech>Technical Report RC22176, IBM.</tech>
<contexts>
<context position="3151" citStr="Papineni et al., 2001" startWordPosition="465" endWordPosition="468">ts (Section 2). Section 3 describes the automatic procedures used for the preparation of both training and testing data, including morphological tagging, and analytical and tectogrammatical parsing of Czech input. In Section 4 we describe the process of the filtering of dictionaries used in the transfer procedure (for its characterization, see Section 5). The generation process consisting mainly of word reordering and lexical insertions is explained in Section 6, an example illustrating the generation steps is presented in Sec83 tion 7. For the evaluation of the results we use the BLEU score (Papineni et al., 2001). Section 8 compares translations generated from automatically built and manually annotated tectogrammatical representations. We also compare the results with the output generated by the statistical translation system GIZA++/ISI ReWrite Decoder (AlOnaizan et al., 1999; Och and Ney, 2000; Germann et al., 2001), trained on the same parallel corpus. 2 Data Resources 2.1 The Prague Dependency Treebank The Prague Dependency Treebank project (Bohmova et al., 2001) aims at complex annotation of a corpus containing about 1.8M word occurrences (about 80,000 running text sentences) in Czech. The annotat</context>
<context position="21480" citStr="Papineni et al., 2001" startWordPosition="3231" endWordPosition="3234">ng to contextual boundness and their tectogrammatical functors. The form of the complex verb is handled in step 3. In the next step, prepositions and articles are inserted. However, not every functor&apos;s realization can be reconstructed easily, as can be seen in the case of the missing preposition &amp;quot;in&amp;quot;. It is also hard to decide whether a particular word was used in an uncountable sense (see the wrongly inserted indefinite article). The last line contains the final morphological realization of the sentence. 8 Evaluation of Results We evaluated our translations with IBM&apos;s BLEU evaluation metric (Papineni et al., 2001), using the same evaluation method and reference retranslations that were used for evaluation at HLT Workshop 2002 at CLSP (Haji 6 et al., 2002). We used four reference retranslations of 490 sentences selected from the WSJ sections 22, 23, and 24, which were themselves used as the fifth reference. The evaluation method used is to hold out each reference in turn and evaluate it against the remaining four, averaging the five BLEU scores. Table 3 shows final results of our system compared with GIZA++ and MAGENTA&apos;s results. The DBMT with parser I and parser II experiments represent a fully automat</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2001. Bleu: a method for automatic evaluation of machine translation. Technical Report RC22176, IBM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A maximum entropy partof-speech tagger.</title>
<date>1996</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>133--142</pages>
<publisher>ACL.</publisher>
<institution>University of Pennsylvania,</institution>
<contexts>
<context position="6308" citStr="Ratnaparkhi, 1996" startWordPosition="951" endWordPosition="952">t 242 evaltest 248 Table I: Number of sentence pairs in EnglishCzech WSJ corpus 2.3 English Monolingual Corpus The Penn Treebank data contain manually assigned morphological tags and this information substantially simplifies lemmatization. The lemmatization procedure searches a list of triples containing word form, morphological tag and lemma, extracted from a large corpus. It looks for a triple with a matching word form and morphological tag, and chooses the lemma from this triple. The large corpus of English3 used in this experiment was automatically morphologically tagged by MXPOST tagger (Ratnaparkhi, 1996) and lemmatized by the morpha tool (Minnen et al., 2001), and contains 365 million words in 13 million sentences. 3It consists of English part of French-English Canadian Hansards corpus, English part of English-Czech Readers&apos; Digest corpus, English part of English-Czech IBM corpus, Wall Street Journal (years 95, 96), L.A. Times/Wash. Post (May 1994 — August 1997), Reuters General News (April 1994 — December 1996), Reuters Financial News (April 1994 — December 1996). 84 3 Czech Data Processing 3.1 Morphological Tagging and Lemmatization The Czech translations of Penn Treebank were automatically</context>
<context position="9435" citStr="Ratnaparkhi, 1996" startWordPosition="1432" endWordPosition="1433">tion about the source dictionary. This merged dictionary consisting of entry/translation pairs (Czech entries and English translations in our case) is enriched by the following procedures: • Frequencies of English word obtained from large English monolingual corpora are added to each translation. See description of the corpora in Section 2.3. • Czech POS tag and stem are added to each entry using the Czech morphological analyzer (Haji 6 and Hladka, 1998). • English POS tag is added to each translation. If there is more than one English POS tag obtained from the English morphological analyzer (Ratnaparkhi, 1996), the English POS tag is &amp;quot;disambiguated&amp;quot; according to the Czech POS in the appropriate entry/translation pair. We select several relevant translations for each entry taking into account the sum of the weights of the source dictionaries (see dictionary weights in Table 2), the frequencies from English monolingual corpora, and the correspondence of the Czech and English POS tags. 4.3 Scoring Translations Using GIZA++ To make the dictionary more sensitive to a specific domain, which is in our case the domain of 4For example for WinGED dictionary it is 2.44 translations per entry in average, and e</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A maximum entropy partof-speech tagger. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 133-142, University of Pennsylvania, May. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zdenek 2abokrts14</author>
<author>Petr Sgall</author>
<author>Meroski Sago</author>
</authors>
<title>Machine learning approach to automatic functor assignment in the Prague Dependency Treebank.</title>
<date>2002</date>
<booktitle>In Proceedings of LREC 2002 (Third International Conference on Language Resources and Evaluation),</booktitle>
<volume>volume V,</volume>
<pages>1513--1520</pages>
<institution>Las Palmas de Gran Canaria,</institution>
<marker>2abokrts14, Sgall, Sago, 2002</marker>
<rawString>Zdenek 2abokrts14, Petr Sgall, and Meroski Sago. 2002. Machine learning approach to automatic functor assignment in the Prague Dependency Treebank. In Proceedings of LREC 2002 (Third International Conference on Language Resources and Evaluation), volume V, pages 1513-1520, Las Palmas de Gran Canaria, Spain.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>