<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000913">
<title confidence="0.995324">
Spectral Clustering for Example Based Machine Translation
</title>
<author confidence="0.926775">
Rashmi Gangadharaiah
</author>
<affiliation confidence="0.887892">
LTI
Carnegie Mellon University
</affiliation>
<address confidence="0.619216">
Pittsburgh P.A. 15213
</address>
<email confidence="0.997591">
rgangadh@andrew.cmu.edu
</email>
<author confidence="0.857312">
Ralf Brown
</author>
<affiliation confidence="0.8229545">
LTI
Carnegie Mellon University
</affiliation>
<address confidence="0.608019">
Pittsburgh P.A. 15213
</address>
<email confidence="0.997257">
ralf@cs.cmu.edu
</email>
<author confidence="0.90639">
Jaime Carbonell
</author>
<affiliation confidence="0.8616565">
LTI
Carnegie Mellon University
</affiliation>
<address confidence="0.613915">
Pittsburgh P.A. 15213
</address>
<email confidence="0.997092">
jgc@cs.cmu.edu
</email>
<sectionHeader confidence="0.993851" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999755411764706">
Prior work has shown that generaliza-
tion of data in an Example Based Ma-
chine Translation (EBMT) system, re-
duces the amount of pre-translated text re-
quired to achieve a certain level of accu-
racy (Brown, 2000). Several word clus-
tering algorithms have been suggested to
perform these generalizations, such as k-
Means clustering or Group Average Clus-
tering. The hypothesis is that better con-
textual clustering can lead to better trans-
lation accuracy with limited training data.
In this paper, we use a form of spectral
clustering to cluster words, and this is
shown to result in as much as 29.08% im-
provement over the baseline EBMT sys-
tem.
</bodyText>
<sectionHeader confidence="0.998995" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.98587652">
In EBMT, the source sentence to be translated
is matched against the source language sentences
present in a corpus of source-target sentence pairs.
When a partial match is found, the corresponding
target translations are obtained through subsenten-
tial alignment. These partial matches are put to-
gether to obtain the final translation by optimizing
translation and alignment scores and using a statisti-
cal target language model in the decoding process.
Prior work has shown that EBMT requires large
amounts of data (in the order of two to three mil-
lion words) (Brown, 2000) of pre-translated text, to
function reasonably well. Thus, some modification
of the basic EBMT method is required to make it ef-
fective when less data is available. In order to use
the available text efficiently, systems such as, (Veale
and Way, 1997) and (Brown, 1999), convert the ex-
amples in the corpus into templates against which
the new text can be matched. Thus, source-target
sentence pairs are converted to source-target gener-
alized template pairs. An example of such a pair is
shown below:
The session opened at 2p.m
La s´eance est ouverte a´ 2 heures
The &lt;event&gt; &lt;verb-past-tense&gt; at &lt;time&gt;
La &lt;event&gt; &lt;verb-past-tense&gt; a &lt;time&gt;
This single template can be used to translate differ-
ent source sentences, including for example,
The session adjourned at 6p.m
The seminar opened at 8a.m
if ‘session’ and ‘seminar’ are both generalized to
‘&lt;event&gt;’, ‘opened’ and ‘adjourned’ are both gen-
eralized to ‘&lt;verb-past-tense&gt;’ and finally ‘6p.m’
and ‘8a.m’ are both generalized to ‘&lt;time&gt;’.
The system used by (Brown, 1999) performs
its generalization using both equivalence classes of
words and a production rule grammar. This paper
describes the use of spectral clustering (Ng. et. al.,
2001; Zelnik-Manor and Perona, 2004), for auto-
mated extraction of equivalence classes. Spectral
clustering is seen to be superior to Group Average
Clustering (GAC) (Brown, 2000) both in terms of
semantic similarity of words falling in a single clus-
ter, and overall BLEU score (Papineni. et. al., 2002)
in a large scale EBMT system.
The next section explains the term vectors ex-
tracted for each word, which are then used to cluster
words into equivalence classes and provides an out-
line of the Standard GAC algorithm. Section 3 de-
scribes the spectral clustering algorithm used. Sec-
</bodyText>
<page confidence="0.990416">
41
</page>
<note confidence="0.8769635">
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 41–44,
New York, June 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.993397">
tion 4 lists results obtained in a full evaluation of the
algorithm. Section 5 concludes and discusses direc-
tions for future work.
</bodyText>
<sectionHeader confidence="0.880109" genericHeader="method">
2 Term vectors for clustering
</sectionHeader>
<bodyText confidence="0.99997976">
Using a bilingual dictionary, usually created using
statistical methods such as those of (Brown et. al.,
1990) or (Brown, 1997), and the parallel text, a
rough mapping between source and target words can
be created. This word pair is then treated as an in-
divisible token for future processing. For each such
word pair we then accumulate counts for each to-
ken in the surrounding context of its occurrences
(N words, currently 3, immediately prior to and
N words immediately following). The counts are
weighted with respect to distance from occurrence,
with a linear decay (from 1 to 1/N) to give great-
est importance to the words immediately adjacent to
the word pair being examined. These counts form a
pseudo-document for each pair, which are then con-
verted into term vectors for clustering.
In this paper, we compare our algorithm against
the incremental GAC algorithm(Brown, 2000). This
method examines each word pair in turn, comput-
ing a similarity measure to every existing cluster.
If the best similarity measure is above a predeter-
mined threshold, the new word is placed in the cor-
responding cluster, otherwise a new cluster is cre-
ated if the maximum number of clusters has not yet
been reached.
</bodyText>
<sectionHeader confidence="0.973366" genericHeader="method">
3 Spectral clustering
</sectionHeader>
<bodyText confidence="0.998516692307692">
Spectral clustering is a general term used to de-
scribe a group of algorithms that cluster points using
the eigenvalues of ‘distance matrices’ obtained from
data. In our case, the algorithm described in (Ng.
et. al., 2001) was performed with certain variations
that were proposed by (Zelnik-Manor and Perona,
2004) to compute the scaling factors automatically
and for the k-Means orthogonal treatment (Verma
and Meila, 2003) during the initialization. These
scaling factors help in self-tuning distances between
points according to the local statistics of the neigh-
borhoods of the points. The algorithm is briefly de-
scribed below.
</bodyText>
<listItem confidence="0.993240333333333">
1. Let S =s1, s2, ....sn, denote the term vectors to
be clustered into k classes.
2. Form the affinity matrix A defined by
</listItem>
<equation confidence="0.82546575">
Aij = exp(−d2(si, sj)/σiσj) for i =� j
Aii = 1
Where, d(si, sj) = 1/(sim(si, sj) + E)
sim(si, sj) is the Cosine similarity between si
</equation>
<listItem confidence="0.916127347826087">
and sj, E is used to prevent the ratio from be-
coming infinity
σi is the set of local scaling parameters for si.
σi = d(si, sT) where, sT is the Tth neighbor of
point si for some fixed T (7 for this paper).
3. Define D to be the diagonal matrix given by,
Dii = ΣjAij
4. Compute L = D−1/2AD−1/2
5. Select k eigenvectors corresponding to k
largest eigenvalues (k is presently an externally
set parameter). The eigenvectors are normal-
ized to have unit length. Form matrix U by
stacking all the eigenvectors in columns.
6. Form the matrix Y by normalizing U’s rows,
&apos;✓Yij = Uij/ (ΣjU2 ij)
7. Perform k-Means clustering treating each row
of Y as a point in k dimensions. The k-Means
algorithm is initialized either with random cen-
ters or with orthogonal vectors.
8. After clustering, assign the point si to cluster c
if the corresponding row i of the matrix Y was
assigned to cluster c.
9. Sum the distances between the members and
</listItem>
<bodyText confidence="0.648774">
the centroid of each cluster to obtain the classi-
fication cost.
10. Goto step 7, iterate for a fixed number of it-
erations. In this paper, 20 iterations were per-
formed with orthogonal k-Means initialization
and 5 iterations with random k-Means initial-
ization.
11. The clusters obtained from the iteration with
least classification cost are selected as the k
clusters.
</bodyText>
<sectionHeader confidence="0.994177" genericHeader="method">
4 Preliminary Results
</sectionHeader>
<bodyText confidence="0.999901571428571">
The clusters obtained from the spectral clustering
method are seen by inspection to correspond to more
natural and intuitive word classes than those ob-
tained by GAC. Even though this is subjective and
not guaranteed to lead to improve translation perfor-
mance, it shows that maybe the increased power of
spectral clustering to represent non-convex classes
</bodyText>
<page confidence="0.995793">
42
</page>
<bodyText confidence="0.999974895833334">
(non-convex in the term vector domain) could be
useful in a real translation experiment. Some ex-
ample classes are shown in Table 1. The first
class in an intuitive sense corresponds to measure-
ment units. We see that in the &lt;units&gt; case,
GAC misses some of the members which are ac-
tually distributed among many different classes and
hence these are not well generalized. In the second
class &lt;months&gt;, spectral clustering has primarily
the months in a single class whereas GAC adds a
number of seemingly unrelated words to the clus-
ter. The classes were all obtained by finding 80
clusters in a 20,000-sentence pair subset of the IBM
Hansard Corpus (Linguistic Data Consortium, 1997)
for spectral clustering. 80 was chosen as the number
of clusters since it gave the highest BLEU score in
the evaluation. For GAC, 300 clusters were used as
this gave the best performance.
To show the effectiveness of the clustering meth-
ods in an actual evaluation, we set up the following
experiment for an English to French translation task
on the Hansard corpus. The training data consists of
three sets of size 10,000 (set1), 20,000 (set2) and
30,000 (set3) sentence pairs chosen from the first
six files of the Hansard Corpus. Only sentences of
length 5 to 21 words were taken. Only words with
frequency of occurrence greater than 9 were chosen
for clustering because more contextual information
would be available when the word occurs frequently
and this would help in obtaining better clusters. The
test data was chosen to be a set of 500 sentences ob-
tained from files 20, 40, 60 and 80 of the Hansard
corpus with 125 sentences from each file. Each of
the methods was run with different number of clus-
ters and results are reported only for the optimal
number of clusters in each case.
The results in Table 2 show that spectral clus-
tering requires moderate amounts of data to get a
large improvement. For small amounts of data it is
slightly worse than GAC, but neither gives much im-
provement over the baseline. For larger amounts of
data, again both methods are very similar, though
spectral clustering is better. Finally, for moderate
amounts of data, when generalization is the most
useful, spectral clustering gives a significant im-
provement over the baseline as well as over GAC.
By looking at the clusters obtained with varying
amounts of data, it can be concluded that high pu-
</bodyText>
<tableCaption confidence="0.747779">
Table 1: Clusters for &lt;units&gt; and &lt;months&gt;
</tableCaption>
<table confidence="0.979197535714286">
Spectral clustering GAC
“adjourned” “hre” “adjourned” “hre”
“cent” “%”
“days” “jours”
“families” “familles” “families” “familles”
“hours” “heures”
“million” “millions” “million” “millions”
“minutes” “minutes”
“o’clock” “heures” “o’clock” “heures”
“p.m.” “heures” “p.m.” “heures”
“p.m.” “hre”
“people” “personnes” “people” “personnes”
“per” “%” “per” “%”
“times” “fois” “times” “fois”
“years” “ans”
“august” “aoˆut” “august” “aoˆut”
“december” “d´ecembre” “december” “d´ecembre”
“february” “f´evrier” “february” “f´evrier”
“january” “janvier” “january” “janvier”
“march” “mars” “march” “mars”
“may” “mai” “may” “mai”
“november” “novembre” “november” “novembre”
“october” “octobre” “october” “octobre”
“only” “seulement” “only” “seulement”
“june” “juin” “june” “juin”
“july” “juillet” “july” “juillet”
“april” “avril” “april” “avril”
“september” “septembre” “september” “septembre”
</table>
<figure confidence="0.826821111111111">
“page” “page”
“per” “$”
“recognize” “parole”
“recognized” “parole”
“recorded” “page”
“section” “article”
“since” “depuis” “since” “depuis”
“took” “s´eance”
“under” “loi”
</figure>
<page confidence="0.998658">
43
</page>
<tableCaption confidence="0.919888">
Table 2: % Relative improvement over baseline EBMT
# clus is the number of clusters for best performance
</tableCaption>
<table confidence="0.999308">
GAC Spectral
% Rel imp #clus % Rel imp #clus
10k 3.33 50 1.37 20
20k 22.47 300 29.08 80
30k 2.88 300 3.88 200
</table>
<bodyText confidence="0.81929">
rity clusters can be obtained with even just moderate
amounts of data.
</bodyText>
<sectionHeader confidence="0.992849" genericHeader="conclusions">
5 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.9999893">
From the experimental results we see that spectral
clustering leads to relatively purer and more intu-
itive clusters. These clusters result in an improved
BLEU score in comparison with the clusters ob-
tained through GAC. GAC can only collect clusters
in convex regions in the term vector space, while
spectral clustering is not limited in this regard. The
ability of spectral clustering to represent non-convex
shapes arises due to the projection onto the eigen-
vectors as described in (Ng. et. al., 2001).
As future work, we would like to analyze the
variation in performance as the amount of data in-
creases. It is widely known that increasing the
amount of training data in a generalized EBMT sys-
tem eventually leads to saturation of performance,
where all clustering methods perform about as well
as baseline. Thus, all methods have an operating re-
gion where they are the most useful. We would like
to locate and extend this region for spectral cluster-
ing.
Also, it would be interesting to compare the clus-
ters obtained with spectral clustering and the Part of
Speech tags of the words in the same cluster, espe-
cially for languages such as English where good tag-
gers are available.
Finally, an important direction of research is in
automatically selecting the number of clusters for
the clustering algorithm. To do this, we could use
information from the eigenvalues or the distribution
of points in the clusters.
</bodyText>
<sectionHeader confidence="0.959385" genericHeader="acknowledgments">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.995166">
This work was funded by National Business Center
award NBCHC050082.
</bodyText>
<sectionHeader confidence="0.969447" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996364862745098">
Andrew Ng, Michael Jordan, and Yair Weiss 2001. On
Spectral Clustering: Analysis and an algorithm. In Ad-
vances in Neural Information Processing Systems 14:
Proceeding of the 2001 Conference, pages 849-856,
Vancouver, British Columbia, Canada, December.
Deepak Verma and Marina Meila. 2003. Comparison of
Spectral Clustering Algorithms. http://www.ms.
washington.edu/—spectral/.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei
Jing Zhu. 2002. BLEU: a method for Automatic
Evaluation of Machine Translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL 2002), pages 311-
318,Philadelphia, PA, July. http://acl.ldc.
upenn.edu/P/P02
Linguistic Data Consortium. 1997. Hansard Corpus of
Parallel English and French. Linguistic Data Con-
sortium, December. http://www.ldc.upenn.
edu/
L. Zelnik-Manor and P. Perona 2004 Self-Tuning Spec-
tral Clustering. In Advances in Neural Information
Processing Systems 17: Proceeding of the 2004 Con-
ference.
Peter Brown, J. Cocke, S. Della Pietra, V. Della Pietra, F.
Jelinek, J. Lafferty, R. Mercer and P. Roossin. 1990.
A Statistical Approach to Machine Translation. Com-
putational Linguistics, 16:79-85.
Ralf D. Brown. 1997. Automated Dictionary Extrac-
tion for “Knowledge-Free” Example-Based Transla-
tion. In Proceedings of the Seventh International Con-
ference on Theoretical and Methodological Issues in
Machine Translation (TMI-97), pages 111-118, Santa
Fe, New Mexico, July. http://www.cs.cmu.
edu/—ralf/papers.html
Ralf D. Brown. 1999. Adding Linguistic Knowledge
to a Lexical Example-Based Translation System. In
Proceedings of the Eighth International Conference
on Theoretical and Methodological Issues in Machine
Translation(TMI-99), pages 22-32, August. http:
//www.cs.cmu.edu/—ralf/papers.html
Ralf. D. Brown. 2000. Automated Generalization of
Translation Examples. In Proceedings of Eighteenth
International Conference on Computational Linguis-
tics (COLING-2000), pages 125-131, Saarbr¨ucken,
Germany.
Tony Veale and Andy Way. 1997. Gaijin: A Template-
Driven Bootstrapping Approach to Example-Based
Machine Translation. In Proceedings of NeMNLP97,
New Methods in Natural Language Processing, Sofia,
Bulgaria, September. http://www.compapp.
dcu.ie/—tonyv/papers/gaijin.html.
</reference>
<page confidence="0.999294">
44
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.102916">
<title confidence="0.999606">Spectral Clustering for Example Based Machine Translation</title>
<author confidence="0.9889">Rashmi</author>
<affiliation confidence="0.996714">Carnegie Mellon</affiliation>
<address confidence="0.49306">Pittsburgh P.A.</address>
<email confidence="0.999635">rgangadh@andrew.cmu.edu</email>
<author confidence="0.975281">Ralf</author>
<affiliation confidence="0.994661">Carnegie Mellon</affiliation>
<address confidence="0.490033">Pittsburgh P.A.</address>
<email confidence="0.999341">ralf@cs.cmu.edu</email>
<author confidence="0.993704">Jaime</author>
<affiliation confidence="0.986845">Carnegie Mellon</affiliation>
<address confidence="0.572942">Pittsburgh P.A.</address>
<email confidence="0.999685">jgc@cs.cmu.edu</email>
<abstract confidence="0.98633">Prior work has shown that generalization of data in an Example Based Machine Translation (EBMT) system, reduces the amount of pre-translated text required to achieve a certain level of accuracy (Brown, 2000). Several word clustering algorithms have been suggested to these generalizations, such as Means clustering or Group Average Clustering. The hypothesis is that better contextual clustering can lead to better translation accuracy with limited training data. In this paper, we use a form of spectral clustering to cluster words, and this is shown to result in as much as 29.08% improvement over the baseline EBMT system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Andrew Ng</author>
<author>Michael Jordan</author>
<author>Yair Weiss</author>
</authors>
<title>On Spectral Clustering: Analysis and an algorithm.</title>
<date>2001</date>
<booktitle>In Advances in Neural Information Processing Systems 14: Proceeding of the 2001 Conference,</booktitle>
<pages>849--856</pages>
<location>Vancouver, British Columbia, Canada,</location>
<marker>Ng, Jordan, Weiss, 2001</marker>
<rawString>Andrew Ng, Michael Jordan, and Yair Weiss 2001. On Spectral Clustering: Analysis and an algorithm. In Advances in Neural Information Processing Systems 14: Proceeding of the 2001 Conference, pages 849-856, Vancouver, British Columbia, Canada, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deepak Verma</author>
<author>Marina Meila</author>
</authors>
<title>Comparison of Spectral Clustering Algorithms.</title>
<date>2003</date>
<note>http://www.ms. washington.edu/—spectral/.</note>
<contexts>
<context position="5310" citStr="Verma and Meila, 2003" startWordPosition="848" endWordPosition="851">e a predetermined threshold, the new word is placed in the corresponding cluster, otherwise a new cluster is created if the maximum number of clusters has not yet been reached. 3 Spectral clustering Spectral clustering is a general term used to describe a group of algorithms that cluster points using the eigenvalues of ‘distance matrices’ obtained from data. In our case, the algorithm described in (Ng. et. al., 2001) was performed with certain variations that were proposed by (Zelnik-Manor and Perona, 2004) to compute the scaling factors automatically and for the k-Means orthogonal treatment (Verma and Meila, 2003) during the initialization. These scaling factors help in self-tuning distances between points according to the local statistics of the neighborhoods of the points. The algorithm is briefly described below. 1. Let S =s1, s2, ....sn, denote the term vectors to be clustered into k classes. 2. Form the affinity matrix A defined by Aij = exp(−d2(si, sj)/σiσj) for i =� j Aii = 1 Where, d(si, sj) = 1/(sim(si, sj) + E) sim(si, sj) is the Cosine similarity between si and sj, E is used to prevent the ratio from becoming infinity σi is the set of local scaling parameters for si. σi = d(si, sT) where, sT</context>
</contexts>
<marker>Verma, Meila, 2003</marker>
<rawString>Deepak Verma and Marina Meila. 2003. Comparison of Spectral Clustering Algorithms. http://www.ms. washington.edu/—spectral/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei Jing Zhu</author>
</authors>
<title>BLEU: a method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>311--318</pages>
<location>PA,</location>
<note>http://acl.ldc. upenn.edu/P/P02</note>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wei Jing Zhu. 2002. BLEU: a method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL 2002), pages 311-318,Philadelphia, PA, July. http://acl.ldc. upenn.edu/P/P02</rawString>
</citation>
<citation valid="true">
<authors>
<author>Linguistic Data Consortium</author>
</authors>
<date>1997</date>
<booktitle>Hansard Corpus of Parallel English and French. Linguistic Data Consortium,</booktitle>
<note>http://www.ldc.upenn. edu/</note>
<contexts>
<context position="8127" citStr="Consortium, 1997" startWordPosition="1339" endWordPosition="1340">l translation experiment. Some example classes are shown in Table 1. The first class in an intuitive sense corresponds to measurement units. We see that in the &lt;units&gt; case, GAC misses some of the members which are actually distributed among many different classes and hence these are not well generalized. In the second class &lt;months&gt;, spectral clustering has primarily the months in a single class whereas GAC adds a number of seemingly unrelated words to the cluster. The classes were all obtained by finding 80 clusters in a 20,000-sentence pair subset of the IBM Hansard Corpus (Linguistic Data Consortium, 1997) for spectral clustering. 80 was chosen as the number of clusters since it gave the highest BLEU score in the evaluation. For GAC, 300 clusters were used as this gave the best performance. To show the effectiveness of the clustering methods in an actual evaluation, we set up the following experiment for an English to French translation task on the Hansard corpus. The training data consists of three sets of size 10,000 (set1), 20,000 (set2) and 30,000 (set3) sentence pairs chosen from the first six files of the Hansard Corpus. Only sentences of length 5 to 21 words were taken. Only words with f</context>
</contexts>
<marker>Consortium, 1997</marker>
<rawString>Linguistic Data Consortium. 1997. Hansard Corpus of Parallel English and French. Linguistic Data Consortium, December. http://www.ldc.upenn. edu/</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Zelnik-Manor</author>
<author>P Perona</author>
</authors>
<title>Self-Tuning Spectral Clustering.</title>
<date>2004</date>
<booktitle>In Advances in Neural Information Processing Systems 17: Proceeding of the 2004 Conference.</booktitle>
<contexts>
<context position="2774" citStr="Zelnik-Manor and Perona, 2004" startWordPosition="432" endWordPosition="435">t &lt;time&gt; La &lt;event&gt; &lt;verb-past-tense&gt; a &lt;time&gt; This single template can be used to translate different source sentences, including for example, The session adjourned at 6p.m The seminar opened at 8a.m if ‘session’ and ‘seminar’ are both generalized to ‘&lt;event&gt;’, ‘opened’ and ‘adjourned’ are both generalized to ‘&lt;verb-past-tense&gt;’ and finally ‘6p.m’ and ‘8a.m’ are both generalized to ‘&lt;time&gt;’. The system used by (Brown, 1999) performs its generalization using both equivalence classes of words and a production rule grammar. This paper describes the use of spectral clustering (Ng. et. al., 2001; Zelnik-Manor and Perona, 2004), for automated extraction of equivalence classes. Spectral clustering is seen to be superior to Group Average Clustering (GAC) (Brown, 2000) both in terms of semantic similarity of words falling in a single cluster, and overall BLEU score (Papineni. et. al., 2002) in a large scale EBMT system. The next section explains the term vectors extracted for each word, which are then used to cluster words into equivalence classes and provides an outline of the Standard GAC algorithm. Section 3 describes the spectral clustering algorithm used. Sec41 Proceedings of the Human Language Technology Conferen</context>
<context position="5200" citStr="Zelnik-Manor and Perona, 2004" startWordPosition="832" endWordPosition="835">ch word pair in turn, computing a similarity measure to every existing cluster. If the best similarity measure is above a predetermined threshold, the new word is placed in the corresponding cluster, otherwise a new cluster is created if the maximum number of clusters has not yet been reached. 3 Spectral clustering Spectral clustering is a general term used to describe a group of algorithms that cluster points using the eigenvalues of ‘distance matrices’ obtained from data. In our case, the algorithm described in (Ng. et. al., 2001) was performed with certain variations that were proposed by (Zelnik-Manor and Perona, 2004) to compute the scaling factors automatically and for the k-Means orthogonal treatment (Verma and Meila, 2003) during the initialization. These scaling factors help in self-tuning distances between points according to the local statistics of the neighborhoods of the points. The algorithm is briefly described below. 1. Let S =s1, s2, ....sn, denote the term vectors to be clustered into k classes. 2. Form the affinity matrix A defined by Aij = exp(−d2(si, sj)/σiσj) for i =� j Aii = 1 Where, d(si, sj) = 1/(sim(si, sj) + E) sim(si, sj) is the Cosine similarity between si and sj, E is used to preve</context>
</contexts>
<marker>Zelnik-Manor, Perona, 2004</marker>
<rawString>L. Zelnik-Manor and P. Perona 2004 Self-Tuning Spectral Clustering. In Advances in Neural Information Processing Systems 17: Proceeding of the 2004 Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Brown</author>
<author>J Cocke</author>
<author>S Della Pietra</author>
<author>V Della Pietra</author>
<author>F Jelinek</author>
<author>J Lafferty</author>
<author>R Mercer</author>
<author>P Roossin</author>
</authors>
<title>A Statistical Approach to Machine Translation.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<pages>16--79</pages>
<marker>Brown, Cocke, Pietra, Pietra, Jelinek, Lafferty, Mercer, Roossin, 1990</marker>
<rawString>Peter Brown, J. Cocke, S. Della Pietra, V. Della Pietra, F. Jelinek, J. Lafferty, R. Mercer and P. Roossin. 1990. A Statistical Approach to Machine Translation. Computational Linguistics, 16:79-85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralf D Brown</author>
</authors>
<title>Automated Dictionary Extraction for “Knowledge-Free” Example-Based Translation.</title>
<date>1997</date>
<booktitle>In Proceedings of the Seventh International Conference on Theoretical and Methodological Issues in Machine Translation (TMI-97),</booktitle>
<pages>111--118</pages>
<location>Santa Fe, New Mexico,</location>
<note>http://www.cs.cmu. edu/—ralf/papers.html</note>
<contexts>
<context position="3790" citStr="Brown, 1997" startWordPosition="599" endWordPosition="600">o equivalence classes and provides an outline of the Standard GAC algorithm. Section 3 describes the spectral clustering algorithm used. Sec41 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 41–44, New York, June 2006. c�2006 Association for Computational Linguistics tion 4 lists results obtained in a full evaluation of the algorithm. Section 5 concludes and discusses directions for future work. 2 Term vectors for clustering Using a bilingual dictionary, usually created using statistical methods such as those of (Brown et. al., 1990) or (Brown, 1997), and the parallel text, a rough mapping between source and target words can be created. This word pair is then treated as an indivisible token for future processing. For each such word pair we then accumulate counts for each token in the surrounding context of its occurrences (N words, currently 3, immediately prior to and N words immediately following). The counts are weighted with respect to distance from occurrence, with a linear decay (from 1 to 1/N) to give greatest importance to the words immediately adjacent to the word pair being examined. These counts form a pseudo-document for each </context>
</contexts>
<marker>Brown, 1997</marker>
<rawString>Ralf D. Brown. 1997. Automated Dictionary Extraction for “Knowledge-Free” Example-Based Translation. In Proceedings of the Seventh International Conference on Theoretical and Methodological Issues in Machine Translation (TMI-97), pages 111-118, Santa Fe, New Mexico, July. http://www.cs.cmu. edu/—ralf/papers.html</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralf D Brown</author>
</authors>
<title>Adding Linguistic Knowledge to a Lexical Example-Based Translation System.</title>
<date>1999</date>
<booktitle>In Proceedings of the Eighth International Conference on Theoretical and Methodological Issues in Machine Translation(TMI-99),</booktitle>
<pages>22--32</pages>
<note>http: //www.cs.cmu.edu/—ralf/papers.html</note>
<contexts>
<context position="1820" citStr="Brown, 1999" startWordPosition="285" endWordPosition="286"> through subsentential alignment. These partial matches are put together to obtain the final translation by optimizing translation and alignment scores and using a statistical target language model in the decoding process. Prior work has shown that EBMT requires large amounts of data (in the order of two to three million words) (Brown, 2000) of pre-translated text, to function reasonably well. Thus, some modification of the basic EBMT method is required to make it effective when less data is available. In order to use the available text efficiently, systems such as, (Veale and Way, 1997) and (Brown, 1999), convert the examples in the corpus into templates against which the new text can be matched. Thus, source-target sentence pairs are converted to source-target generalized template pairs. An example of such a pair is shown below: The session opened at 2p.m La s´eance est ouverte a´ 2 heures The &lt;event&gt; &lt;verb-past-tense&gt; at &lt;time&gt; La &lt;event&gt; &lt;verb-past-tense&gt; a &lt;time&gt; This single template can be used to translate different source sentences, including for example, The session adjourned at 6p.m The seminar opened at 8a.m if ‘session’ and ‘seminar’ are both generalized to ‘&lt;event&gt;’, ‘opened’ and </context>
</contexts>
<marker>Brown, 1999</marker>
<rawString>Ralf D. Brown. 1999. Adding Linguistic Knowledge to a Lexical Example-Based Translation System. In Proceedings of the Eighth International Conference on Theoretical and Methodological Issues in Machine Translation(TMI-99), pages 22-32, August. http: //www.cs.cmu.edu/—ralf/papers.html</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Brown</author>
</authors>
<title>Automated Generalization of Translation Examples.</title>
<date>2000</date>
<booktitle>In Proceedings of Eighteenth International Conference on Computational Linguistics (COLING-2000),</booktitle>
<pages>125--131</pages>
<location>Saarbr¨ucken, Germany.</location>
<contexts>
<context position="1551" citStr="Brown, 2000" startWordPosition="240" endWordPosition="241">he baseline EBMT system. 1 Introduction In EBMT, the source sentence to be translated is matched against the source language sentences present in a corpus of source-target sentence pairs. When a partial match is found, the corresponding target translations are obtained through subsentential alignment. These partial matches are put together to obtain the final translation by optimizing translation and alignment scores and using a statistical target language model in the decoding process. Prior work has shown that EBMT requires large amounts of data (in the order of two to three million words) (Brown, 2000) of pre-translated text, to function reasonably well. Thus, some modification of the basic EBMT method is required to make it effective when less data is available. In order to use the available text efficiently, systems such as, (Veale and Way, 1997) and (Brown, 1999), convert the examples in the corpus into templates against which the new text can be matched. Thus, source-target sentence pairs are converted to source-target generalized template pairs. An example of such a pair is shown below: The session opened at 2p.m La s´eance est ouverte a´ 2 heures The &lt;event&gt; &lt;verb-past-tense&gt; at &lt;time</context>
<context position="2915" citStr="Brown, 2000" startWordPosition="455" endWordPosition="456">ourned at 6p.m The seminar opened at 8a.m if ‘session’ and ‘seminar’ are both generalized to ‘&lt;event&gt;’, ‘opened’ and ‘adjourned’ are both generalized to ‘&lt;verb-past-tense&gt;’ and finally ‘6p.m’ and ‘8a.m’ are both generalized to ‘&lt;time&gt;’. The system used by (Brown, 1999) performs its generalization using both equivalence classes of words and a production rule grammar. This paper describes the use of spectral clustering (Ng. et. al., 2001; Zelnik-Manor and Perona, 2004), for automated extraction of equivalence classes. Spectral clustering is seen to be superior to Group Average Clustering (GAC) (Brown, 2000) both in terms of semantic similarity of words falling in a single cluster, and overall BLEU score (Papineni. et. al., 2002) in a large scale EBMT system. The next section explains the term vectors extracted for each word, which are then used to cluster words into equivalence classes and provides an outline of the Standard GAC algorithm. Section 3 describes the spectral clustering algorithm used. Sec41 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 41–44, New York, June 2006. c�2006 Association for Computational Linguistics tion 4 lists </context>
<context position="4545" citStr="Brown, 2000" startWordPosition="725" endWordPosition="726"> for future processing. For each such word pair we then accumulate counts for each token in the surrounding context of its occurrences (N words, currently 3, immediately prior to and N words immediately following). The counts are weighted with respect to distance from occurrence, with a linear decay (from 1 to 1/N) to give greatest importance to the words immediately adjacent to the word pair being examined. These counts form a pseudo-document for each pair, which are then converted into term vectors for clustering. In this paper, we compare our algorithm against the incremental GAC algorithm(Brown, 2000). This method examines each word pair in turn, computing a similarity measure to every existing cluster. If the best similarity measure is above a predetermined threshold, the new word is placed in the corresponding cluster, otherwise a new cluster is created if the maximum number of clusters has not yet been reached. 3 Spectral clustering Spectral clustering is a general term used to describe a group of algorithms that cluster points using the eigenvalues of ‘distance matrices’ obtained from data. In our case, the algorithm described in (Ng. et. al., 2001) was performed with certain variation</context>
</contexts>
<marker>Brown, 2000</marker>
<rawString>Ralf. D. Brown. 2000. Automated Generalization of Translation Examples. In Proceedings of Eighteenth International Conference on Computational Linguistics (COLING-2000), pages 125-131, Saarbr¨ucken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tony Veale</author>
<author>Andy Way</author>
</authors>
<title>Gaijin: A TemplateDriven Bootstrapping Approach to Example-Based Machine Translation.</title>
<date>1997</date>
<booktitle>In Proceedings of NeMNLP97, New Methods in Natural Language Processing,</booktitle>
<location>Sofia, Bulgaria,</location>
<note>http://www.compapp. dcu.ie/—tonyv/papers/gaijin.html.</note>
<contexts>
<context position="1802" citStr="Veale and Way, 1997" startWordPosition="280" endWordPosition="283"> translations are obtained through subsentential alignment. These partial matches are put together to obtain the final translation by optimizing translation and alignment scores and using a statistical target language model in the decoding process. Prior work has shown that EBMT requires large amounts of data (in the order of two to three million words) (Brown, 2000) of pre-translated text, to function reasonably well. Thus, some modification of the basic EBMT method is required to make it effective when less data is available. In order to use the available text efficiently, systems such as, (Veale and Way, 1997) and (Brown, 1999), convert the examples in the corpus into templates against which the new text can be matched. Thus, source-target sentence pairs are converted to source-target generalized template pairs. An example of such a pair is shown below: The session opened at 2p.m La s´eance est ouverte a´ 2 heures The &lt;event&gt; &lt;verb-past-tense&gt; at &lt;time&gt; La &lt;event&gt; &lt;verb-past-tense&gt; a &lt;time&gt; This single template can be used to translate different source sentences, including for example, The session adjourned at 6p.m The seminar opened at 8a.m if ‘session’ and ‘seminar’ are both generalized to ‘&lt;even</context>
</contexts>
<marker>Veale, Way, 1997</marker>
<rawString>Tony Veale and Andy Way. 1997. Gaijin: A TemplateDriven Bootstrapping Approach to Example-Based Machine Translation. In Proceedings of NeMNLP97, New Methods in Natural Language Processing, Sofia, Bulgaria, September. http://www.compapp. dcu.ie/—tonyv/papers/gaijin.html.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>