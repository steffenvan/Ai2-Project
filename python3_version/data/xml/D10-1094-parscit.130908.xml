<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000524">
<title confidence="0.996891">
Generating Confusion Sets for Context-Sensitive Error Correction
</title>
<author confidence="0.995307">
Alla Rozovskaya and Dan Roth
</author>
<affiliation confidence="0.996794">
University of Illinois at Urbana-Champaign
</affiliation>
<address confidence="0.787583">
Urbana, IL 61801
</address>
<email confidence="0.998849">
{rozovska,danr}@illinois.edu
</email>
<sectionHeader confidence="0.995639" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999559">
In this paper, we consider the problem of gen-
erating candidate corrections for the task of
correcting errors in text. We focus on the
task of correcting errors in preposition usage
made by non-native English speakers, using
discriminative classifiers. The standard ap-
proach to the problem assumes that the set of
candidate corrections for a preposition con-
sists of all preposition choices participating
in the task. We determine likely preposition
confusions using an annotated corpus of non-
native text and use this knowledge to produce
smaller sets of candidates.
We propose several methods of restricting
candidate sets. These methods exclude candi-
date prepositions that are not observed as valid
corrections in the annotated corpus and take
into account the likelihood of each preposi-
tion confusion in the non-native text. We find
that restricting candidates to those that are ob-
served in the non-native data improves both
the precision and the recall compared to the
approach that views all prepositions as pos-
sible candidates. Furthermore, the approach
that takes into account the likelihood of each
preposition confusion is shown to be the most
effective.
</bodyText>
<sectionHeader confidence="0.999338" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999432692307693">
We address the problem of generating candidate cor-
rections for the task of correcting context-dependent
mistakes in text, mistakes that involve confusing
valid words in a language. A well-studied instance
of this problem – context-sensitive spelling errors –
has received a lot of attention in natural language
research (Golding and Roth, 1999; Carlson et al.,
2001; Carlson and Fette, 2007; Banko and Brill,
2001). The context-sensitive spelling correction task
addresses the problem of correcting spelling mis-
takes that result in legitimate words, such as confus-
ing their and there or your and you’re. In this task, a
candidate set or a confusion set is defined that spec-
ifies a list of confusable words, e.g., {their, there}
or {cite, site, sight}. Each occurrence of a confus-
able word in text is represented as a vector of fea-
tures derived from a small context window around
the target. A classifier is trained on text assumed
to be error-free, replacing each target word occur-
rence (e.g. their) with a confusion set consisting of
{their, there}, thus generating both positive and neg-
ative examples, respectively, from the same context.
Given a text to correct, for each word in text that be-
longs to the confusion set the classifier predicts the
most likely candidate in the confusion set.
More recently, work in error correction has taken
an interesting turn and focused on correcting errors
made by English as a Second Language (ESL) learn-
ers, with a special interest given to errors in article
and preposition usage. These mistakes are some of
the most common mistakes for non-native English
speakers of all proficiency levels (Dalgish, 1985;
Bitchener et al., 2005; Leacock et al., 2010). Ap-
proaches to correcting these mistakes have adopted
the methods of the context-sensitive spelling cor-
rection task. A system is usually trained on well-
formed native English text (Izumi et al., 2003; Eeg-
Olofsson and Knuttson, 2003; Han et al., 2006; Fe-
lice and Pulman, 2008; Gamon et al., 2008; Tetreault
</bodyText>
<page confidence="0.96577">
961
</page>
<note confidence="0.8183325">
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 961–970,
MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999168022222222">
and Chodorow, 2008; Elghaari et al., 2010; Tetreault
et al., 2010), but several works incorporate into
training error-tagged data (Gamon, 2010; Han et
al., 2010) or error statistics (Rozovskaya and Roth,
2010b). The classifier is then applied to non-native
text to predict the correct article/preposition in con-
text. The possible candidate selections include the
set of all articles or all prepositions.
While in the article correction task the candidate
set is small (a, the, no article), systems for correct-
ing preposition errors, even when they consider the
most common prepositions, may include between 9
to 34 preposition classes. For each preposition in
the non-native text, every other candidate in the con-
fusion set is viewed as a potential correction. This
approach, however, does not take into account that
writers do not make mistakes randomly: Not all can-
didates are equally likely given the preposition cho-
sen by the author and errors may depend on the first
language (L1) of the writer. In this paper, we de-
fine L1-dependent candidate sets for the preposition
correction task (Section 4.1). L1-dependent can-
didate sets reflect preposition confusions observed
with the speakers of the first language L1. We pro-
pose methods of enforcing L1-dependent candidate
sets in training and testing.
We consider mistakes involving the top ten En-
glish prepositions. As our baseline system, we train
a multi-class classifier in one-vs-all approach, which
is a standard approach to multi-class classification.
In this approach, a separate binary classifier for each
preposition pi, 1 G i G 10, is trained, s.t. all pi
examples are positive examples for the classifier and
all other nine classes act as negative examples. Thus,
for each preposition pi in non-native text there are
ten1 possible prepositions that the classifier can pro-
pose as corrections for pi.
We contrast this baseline method to two methods
that enforce L1-dependent candidate sets in train-
ing. First, we train a separate classifier for each
preposition pi on the prepositions that belong to L1-
dependent candidate set of pi. In this setting, the
negative examples for pi are those that belong to L1-
dependent candidate set of pi.
The second method of enforcing L1-dependent
</bodyText>
<footnote confidence="0.818354">
1This includes the preposition pi itself. If proposed by the
classifier, it would not be flagged as an error.
</footnote>
<bodyText confidence="0.999662243243243">
candidate sets in training is to train on native data
with artificial preposition errors in the spirit of Ro-
zovskaya and Roth (2010b), where the errors mimic
the error rates and error patterns of the non-native
text. This method requires more knowledge, since
it uses a distribution of errors from an error-tagged
corpus.
We also propose a method of enforcing L1-
dependent candidate sets in testing, through the use
of a confidence threshold. We consider two ways of
applying a threshold: (1) the standard way, when a
correction is proposed only if the classifier’s con-
fidence is sufficiently high and (2) L1-dependent
threshold, when a correction is proposed only if it
belongs to L1-dependent candidate set.
We show that the methods of restricting candidate
sets to L1-dependent confusions improve the prepo-
sition correction system. We demonstrate that re-
stricting candidate sets to those prepositions that are
confusable in the data by L1 writers is beneficial,
when compared to a system that assumes an unre-
stricted candidate set by considering as valid correc-
tions all prepositions participating in the task. Fur-
thermore, we find that the most effective method is
the one that uses knowledge about the likelihoods of
preposition confusions in the non-native text intro-
duced through artificial errors in training.
The rest of the paper is organized as follows.
First, we describe related work on error correction.
Section 3 presents the ESL data and statistics on
preposition errors. Section 4 describes the meth-
ods of restricting candidate sets in training and test-
ing. Section 5 describes the experimental setup. We
present and discuss the results in Section 6. The key
findings are summarized in Table 5 and Fig. 1 in
Section 6. We conclude with a brief discussion of
directions for future work.
</bodyText>
<sectionHeader confidence="0.999764" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99898425">
Work in text correction has focused primarily on
correcting context-sensitive spelling errors (Golding
and Roth, 1999; Banko and Brill, 2001; Carlson et
al., 2001; Carlson and Fette, 2007) and mistakes
made by ESL learners, especially errors in article
and preposition usage.
Roth (1998) takes a unified approach to resolving
semantic and syntactic ambiguities in natural lan-
</bodyText>
<page confidence="0.995839">
962
</page>
<bodyText confidence="0.999613547619048">
guage by treating several related problems, includ-
ing word sense disambiguation, word selection, and
context-sensitive spelling correction as instances of
the disambiguation task. Given a candidate set or a
confusion set of confusable words, the task is to se-
lect the most likely candidate in context. Examples
of confusion sets are {sight, site, cite} for context-
sensitive spelling correction, {among, between} for
word selection, or a set of prepositions for the prepo-
sition correction problem.
Each occurrence of a candidate word in text is
represented as a vector of features. A classifier is
trained on a large corpus of error-free text. Given
text to correct, for each word in text that belongs to
the confusion set the classifier is used to predict the
most likely candidate in the confusion set given the
word’s context.
In the same spirit, models for correcting ESL er-
rors are generally trained on well-formed native text.
Han et al. (2006) train a maximum entropy model to
correct article mistakes. Chodorow et. al (2007),
Tetreault and Chodorow (2008), and De Felice and
Pulman (2008) train a maximum entropy model and
De Felice and Pulman (2007) train a voted percep-
tron algorithm to correct preposition errors. Gamon
et al. (2008) train a decision tree model and a lan-
guage model to correct errors in article and preposi-
tion usage. Bergsma et al. (2009) propose a Naive
Bayes algorithm with web-scale N-grams as fea-
tures, for preposition selection and context-sensitive
spelling correction.
The set of valid candidate corrections for a target
word includes all words in the confusion set. For the
preposition correction task, the entire set of prepo-
sitions considered for the task is viewed as the set
of possible corrections for each preposition in non-
native text. Given a preposition with its surround-
ing context, the model selects the most likely prepo-
sition from the set of all candidates, where the set
of candidates consists of nine (Felice and Pulman,
2008), 12 (Gamon, 2010), or 34 (Tetreault et al.,
2010; Tetreault and Chodorow, 2008) prepositions.
</bodyText>
<subsectionHeader confidence="0.999562">
2.1 Using Error-tagged Data in Training
</subsectionHeader>
<bodyText confidence="0.999708294117647">
Several recent works explore ways of using anno-
tated non-native text when training error correction
models.
One way to incorporate knowledge about which
confusions are likely with ESL learners into the error
correction system is to train a model on error-tagged
data. Preposition confusions observed in the non-
native text can then be included in training, by us-
ing the preposition chosen by the author (the source
preposition) as a feature. This is not possible with a
system trained on native data, because each source
preposition is always the correct preposition.
Han et al. (2010) train a model on partially anno-
tated Korean learner data. The error-tagged model
trained on one million prepositions obtains a slightly
higher recall and a significant improvement in preci-
sion (from 0.484 to 0.817) over a model fives times
larger trained on well-formed text.
Gamon (2010) proposes a hybrid system for
preposition and article correction, by incorporating
the scores of a language model and class probabil-
ities of a maximum entropy model, both trained on
native data, into a meta-classifier that is trained on
a smaller amount of annotated ESL data. The meta-
classifier outperforms by a large margin both of the
native models, but it requires large amounts of ex-
pensive annotated data, especially in order to correct
preposition errors, where the problem complexity is
much larger.
Rozovskaya and Roth (2010b) show that by intro-
ducing into native training data artificial article er-
rors it is possible to improve the performance of the
article correction system, when compared to a clas-
sifier trained on native data. In contrast to Gamon
(2010) and Han et al. (2010) that use annotated data
for training, the system is trained on native data, but
the native data are transformed to be more like L1
data through artificial article errors that mimic the
error rates and error patterns of non-native writers.
This method is cheaper, since obtaining error statis-
tics requires much less annotated data than training.
Moreover, the training data size is not restricted by
the amount of the error-tagged data available. Fi-
nally, the source article of the writer can be used in
training as a feature, in the exact same way as with
the models trained on error-tagged data, providing
knowledge about which confusions are likely. Un-
like article errors, preposition errors lend themselves
very well to a study of confusion sets because the set
of prepositions participating in the task is a lot big-
ger than the set of article choices.
</bodyText>
<page confidence="0.997643">
963
</page>
<sectionHeader confidence="0.997191" genericHeader="method">
3 ESL Data
</sectionHeader>
<subsectionHeader confidence="0.999987">
3.1 Preposition Errors in Learner Data
</subsectionHeader>
<bodyText confidence="0.9997075">
Preposition errors are one of the most common mis-
takes that non-native speakers make. In the Cam-
bridge Learner Corpus2 (CLC), which contains data
by learners of different first language backgrounds
and different proficiency levels, preposition errors
account for about 13.5% of all errors and occur on
average in 10% of all sentences (Leacock et al.,
2010). Similar error rates have been reported for
other annotated ESL corpora, e.g. (Izumi et al.,
2003; Rozovskaya and Roth, 2010a; Tetreault et al.,
2010). Learning correct preposition usage in En-
glish is challenging for learners of all first language
backgrounds (Dalgish, 1985; Bitchener et al., 2005;
Gamon, 2010; Leacock et al., 2010).
</bodyText>
<subsectionHeader confidence="0.999564">
3.2 The Annotated Corpus
</subsectionHeader>
<bodyText confidence="0.999992333333333">
We use data from an annotated corpus of essays
written by ESL students. The essays were fully cor-
rected and error-tagged by native English speakers.
For each preposition used incorrectly by the author,
the annotator also indicated the correct preposition
choice. Rozovskaya and Roth (2010a) provide a de-
tailed description of the annotation of the data.
The annotated data include sentences by speakers
of five first language backgrounds: Chinese, Czech,
Italian, Russian, and Spanish. The Czech, Italian,
Russian and Spanish data come from the Interna-
tional Corpus of Learner English (ICLE, (Granger
et al., 2002)), which is a collection of essays writ-
ten by advanced learners of English. The Chinese
data is a part of the Chinese Learners of English cor-
pus (CLEC, (Gui and Yang, 2003)) that contains es-
says by students of all levels of proficiency. Table 1
shows preposition statistics based on the annotated
data.
The combined data include 4185 prepositions,
8.4% of which were judged to be incorrect by the
annotators. Table 1 demonstrates that the error rates
in the Chinese speaker data, for which different pro-
ficiency levels are available, are 2 or 3 times higher
than the error rates in other language groups. The
data for other languages come from very advanced
learners and, while there are also proficiency differ-
</bodyText>
<footnote confidence="0.934421">
2http://www.cambridge.org/elt
</footnote>
<table confidence="0.9999055">
Source Total Incorrect Error
language preps. preps. rate
Chinese 953 144 15.1%
Czech 627 28 4.5%
Italian 687 43 6.3%
Russian 1210 85 7.0%
Spanish 708 52 7.3%
All 4185 352 8.4%
</table>
<tableCaption confidence="0.999775">
Table 1: Statistics on prepositions in the ESL data.
</tableCaption>
<bodyText confidence="0.99464925">
Column Incorrect denotes the number of prepositions
judged to be incorrect by the native annotators. Column
Error rate denotes the proportion of prepositions used in-
correctly.
ences among advanced speakers, their error rates are
much lower.
We would also like to point out that we take as
the baseline3 for the task the accuracy of the non-
native data, or the proportion of prepositions used
correctly. Using the error rate numbers shown in
Table 1, the baseline for Chinese speakers is thus
84.9%, and for all the data combined it is 91.6%.
</bodyText>
<subsectionHeader confidence="0.999971">
3.3 Preposition Errors and L1
</subsectionHeader>
<bodyText confidence="0.999528133333333">
We focus on preposition confusion errors, mistakes
that involve an incorrectly selected preposition4. We
consider ten most frequent prepositions in English:
on, from, for, of, about, to, at, in, with, and by5.
We mentioned in Section 2 that not all preposition
confusions are equally likely to occur and preposi-
tion errors may depend on the first language of the
writer. Han et al. (2010) show that preposition er-
rors in the annotated corpus by Korean learners are
not evenly distributed, some confusions occurring
more often than others. We also observe that con-
fusion frequencies differ by L1. This is consistent
with other studies, which show that learners’ errors
are influenced by their first language (Lee and Sen-
eff, 2008; Leacock et al., 2010).
</bodyText>
<footnote confidence="0.9931255">
3It is argued in Rozovskaya and Roth (2010b) that the most
frequent class baselines are not relevant for error correction
tasks. Instead, the error rate in the data need to be considered,
when determining the baseline.
4We do not address errors of missing or extraneous preposi-
tions.
5It is common to restrict the systems that detect errors in
preposition usage to the top prepositions. In the CLC corpus,
the usage of the ten most frequent prepositions accounts for
82% of all preposition errors (Leacock et al., 2010).
</footnote>
<page confidence="0.992436">
964
</page>
<sectionHeader confidence="0.920212" genericHeader="method">
4 Methods of Improving Candidate Sets
</sectionHeader>
<bodyText confidence="0.999833210526316">
In this section, we describe methods of restricting
candidate sets according to the first language of the
writer. For the preposition correction task, the stan-
dard approach considers all prepositions participat-
ing in the task as valid corrections for every prepo-
sition in the non-native data.
In Section 3.3, we pointed out that (1) not all
preposition confusions are equally likely to occur
and (2) preposition errors may depend on the first
language of the writer. The methods of restricting
confusion sets proposed in this work use knowledge
about which prepositions are confusable based on
the data by speakers of language L1.
We refer to the preposition originally chosen by
the author in the non-native text as the source prepo-
sition, and label denotes the correct preposition
choice, as chosen by the annotator. Consider, for ex-
ample, the following sentences from the annotated
corpus.
</bodyText>
<listItem confidence="0.9971656">
1. We ate by*/with our hands.
2. To tell the truth, time spent in jail often changes prisoners to*/for
the worse.
3. And the problem that immediately appeared was that men were
unable to cope with the new woman image.
</listItem>
<bodyText confidence="0.9998902">
In example 1, the annotator replaced by with with;
by is the source preposition and with is the label. In
example 2, to is the source and for is the label. In
example 3, the preposition with is judged as correct.
Thus, with is both the source and the label.
</bodyText>
<subsectionHeader confidence="0.996841">
4.1 L1-Dependent Confusion Sets
</subsectionHeader>
<bodyText confidence="0.883373307692308">
Let source preposition pi denote a preposition that
appears in the data by speakers of L1. Let Conf-
Set denote the set of all prepositions that the sys-
tem can propose as a correction for source preposi-
tion pi. We define two types of confusion sets Con-
fSet. An unrestricted confusion set AllConfSet in-
cludes all ten prepositions. L1-dependent confusion
set L1ConfSet(pi) is defined as follows:
Definition L1ConfSet(pi) = {pjII a sentence in
which an L1 writer replaced preposition pj with pi }
For example, in the Spanish speaker data, from
is used incorrectly in place of of and for. Then for
Spanish speakers, L1ConfSet(from)={from, of, for}.
</bodyText>
<table confidence="0.9979164">
Source L1ConfSet(pi)
prep. pi
on {on, about, of, to, at, in, with, by}
by {with, by, in}
from {of, from, for}
</table>
<tableCaption confidence="0.997211">
Table 2: L1-dependent confusion sets for three preposi-
tions based on data by Chinese speakers.
</tableCaption>
<bodyText confidence="0.84529625">
Table 2 shows for Chinese speakers three preposi-
tions and their L1-dependent confusion sets.
We now describe methods of enforcing L1-
dependent confusion sets in training and testing.
</bodyText>
<subsectionHeader confidence="0.894257">
4.2 Enforcing L1-dependent Confusion Sets in
Training
</subsectionHeader>
<bodyText confidence="0.99983534375">
We propose two methods of enforcing L1-dependent
confusion sets in training. They are contrasted to
the typical method of training a multi-class 10-way
classifier, where each class corresponds to one of the
ten participating prepositions.
First, we describe the typical training setting.
NegAll Training proceeds in a standard way of
training a multi-class classifier (one-vs-all ap-
proach) on all ten prepositions using well-
formed native English data. For each prepo-
sition pi, pi examples are positive and the other
nine prepositions are negative examples.
We now describe two methods of enforcing L1-
dependent confusion sets in training.
NegL1 This method explores the difference be-
tween training with nine types as negative ex-
amples and (fewer than nine) L1-dependent
negative examples.
For every preposition pi, we train a classifier
using only examples that are in L1ConfSet(pi).
In contrast to NegAll, for each source prepo-
sition, the negative examples are not all other
nine types, but only those that belong in
L1ConfSet(pi). For each language L1, we train
ten classifiers, one for each source preposition.
For source preposition pi in test, we consult
the classifier for pi. In this model, the con-
fusion set for source pi is restricted through
training, since for source pi, the possible can-
didate replacements are only those that the
classifier sees in training, and they are all in
L1ConfSet(pi).
</bodyText>
<page confidence="0.987008">
965
</page>
<table confidence="0.9999228">
Training Negative examples
data
NegAll NegL1
Clean NegAll-Clean NegL1-Clean
ErrorL1 NegAll-ErrorL1 -
</table>
<tableCaption confidence="0.9923895">
Table 3: Training conditions that result in unrestricted
(All) and L1-dependent training paradigms.
</tableCaption>
<bodyText confidence="0.990604">
ErrorL1 This method restricts the candidate set to
L1ConfSet(pi) by generating artificial preposi-
tion errors in the spirit of Rozovskaya and Roth
(2010b). The training data are thus no longer
well-formed or clean, but augmented with L1
error statistics. Specifically, each preposition
pi in training is replaced with a different prepo-
sition pj with probability probConf, s.t.
</bodyText>
<equation confidence="0.995861">
probConf = prob(piIpj) (1)
</equation>
<bodyText confidence="0.999887964285714">
Suppose 10% of all source prepositions to in
the Russian speaker data correspond to label
for. Then for is replaced with to with proba-
bility 0.1.
The classifier uses in training the source prepo-
sition as a feature, which cannot be done when
training on well-formed text, as discussed in
Section 2.1. By providing the source prepo-
sition as a feature, we enforce L1-dependent
confusion sets in training, because the system
learns which candidate corrections occur with
source preposition pi. An important distinction
of this approach is that it does not simply pro-
vide L1-dependent confusion sets in training:
Because errors are generated using L1 writers’
error statistics, the likelihood of each candidate
correction is also provided. This approach is
also more knowledge-intensive, as it requires
annotated data to obtain error statistics.
It should be noted that this method is orthogo-
nal to the NegAll and NegL1 methods of train-
ing described above and can be used in con-
junction with each of them, only that it trans-
forms the training data to account in a more
natural way for ESL writing.
We combine the proposed methods NegAll,
NegL1 with the Clean or ErrorL1 methods and cre-
ate three training approaches shown in Table 3.
</bodyText>
<subsectionHeader confidence="0.998909">
4.3 Restricting Confusion Sets in Testing
</subsectionHeader>
<bodyText confidence="0.999799444444444">
To reduce the number of false alarms, correction
systems generally use a threshold on the confidence
of the classifier, following (Carlson et al., 2001), and
propose a correction only when the confidence of the
classifier is above the threshold. We show in Section
5 that the system trained on data with artificial er-
rors performs competitively even without a thresh-
old. The other systems use a threshold. We consider
two ways of applying a threshold6:
</bodyText>
<listItem confidence="0.999716">
1. ThreshAll A correction for source preposition
pi is proposed only when the confidence of
the classifier exceeds the threshold. For each
preposition in the non-native data, this method
considers all candidates as valid corrections.
2. ThreshL1Conf A correction for source prepo-
</listItem>
<bodyText confidence="0.5692678">
sition pi is proposed only when the confi-
dence of the classifier exceeds the empirically
found threshold and the preposition proposed
as a correction for pi is in the confusion set
L1ConfSet(pi).
</bodyText>
<sectionHeader confidence="0.997713" genericHeader="method">
5 Experimental Setup
</sectionHeader>
<bodyText confidence="0.998274">
In this section, we describe experiments with L1-
dependent confusion sets. Combining the three
training conditions shown in Table 3 with the two
ways of thresholding described in Section 4.3, we
build four systems7:
</bodyText>
<listItem confidence="0.957156">
1. NegAll-Clean-ThreshAll This system assumes
</listItem>
<bodyText confidence="0.825853428571429">
both in training and in testing stages that all
preposition confusions are possible. The sys-
tem is trained as a multi-class 10-way classifier,
where for each preposition pi, all other nine
prepositions are negative examples. In testing,
when applying the threshold, all prepositions
are considered as valid corrections.
</bodyText>
<listItem confidence="0.997043">
2. NegAll-Clean-ThreshL1 This system is
trained exactly as NegAll-Clean-ThreshAll
but in testing only corrections that belong
</listItem>
<footnote confidence="0.967211666666667">
6Thresholds are found empirically: We divide the evaluation
data into three equal parts and to each part apply the threshold,
which is optimized on the other two parts of the data.
7In testing, it is not possible to consider a confusion set
larger than the one used in training. Therefore, ThreshAll is
only possible with NegAll training condition.
</footnote>
<page confidence="0.997826">
966
</page>
<bodyText confidence="0.938451">
to L1ConfSet(pi) are considered as valid
corrections for pi.
</bodyText>
<listItem confidence="0.813004857142857">
3. NegL1-Clean-ThreshL1 For each preposition
pi, a separate classifier is trained on the prepo-
sitions that are in L1ConfSet(pi), where pi ex-
amples are positive and a set of (fewer than
nine) pi-dependent prepositions are negative.
Only corrections that belong to L1ConfSet(pi)
are considered as valid corrections for pi.8 Ten
pi-dependent classifiers for each L1 are trained.
4. NegAll-ErrorL1-NoThresh A system is trained
as a multi-class 10-way classifier with artifi-
cial preposition errors that mimic the errors
rates and confusion patterns of the non-native
text. For each L1, an L1-dependent system is
trained. This system does not use a threshold.
</listItem>
<bodyText confidence="0.993724769230769">
We discuss this in more detail below.
The system NegAll-Clean-ThreshAll is our base-
line system. It assumes both in training and in test-
ing that all preposition confusions are possible.
All of the systems are trained on the same set of
word and part-of-speech features using the same set
of training examples. Features are extracted from a
window of eight words around the preposition and
include words, part-of-speech tags and conjunctions
of words and tags of lengths two, three, and four.
Training data are extracted from English Wikipedia
and the New York Times section of the Gigaword
corpus (Linguistic Data Consortium, 2003).
In each training paradigm, we follow a discrimi-
native approach, using an online learning paradigm
and making use of the Averaged Perceptron Algo-
rithm (Freund and Schapire, 1999) – we use the
regularized version in Learning Based Java9 (LBJ,
(Rizzolo and Roth, 2007)). While classical Per-
ceptron comes with generalization bound related to
the margin of the data, Averaged Perceptron also
comes with a PAC-like generalization bound (Fre-
und and Schapire, 1999). This linear learning al-
gorithm is known, both theoretically and experi-
mentally, to be among the best linear learning ap-
proaches and is competitive with SVM and Logistic
</bodyText>
<footnote confidence="0.95167175">
8ThreshAll is not possible with this training option, as the
system never proposes a correction that is not in L1ConfSet(pz).
9LBJ code is available at http://cogcomp.cs.
illinois.edu/page/software
</footnote>
<bodyText confidence="0.9885275">
Regression, while being more efficient in training.
It also has been shown to produce state-of-the-art
results on many natural language applications (Pun-
yakanok et al., 2008).
</bodyText>
<sectionHeader confidence="0.997775" genericHeader="evaluation">
6 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999181454545454">
Table 4 shows performance of the four systems
by the source language. For each source lan-
guage, the methods that restrict candidate sets in
training or testing outperform the baseline system
NegAll-Clean-ThreshAll that does not restrict can-
didate sets. The NegAll-ErrorL1-NoThresh system
performs better than the other three systems for all
languages, except for Italian. In fact, for the Czech
speaker data, all systems other than NegAll-ErrorL1-
NoThresh, have a precision and a recall of 0, since
no errors are detected10.
</bodyText>
<table confidence="0.999208148148148">
Source System Acc. P R
lang.
NegAll-Clean-ThreshAll 84.78 47.58 11.46
NegAll-Clean-ThreshL1 84.84 48.05 15.28
CH NegL1-Clean-ThreshL1 84.94 50.87 11.46
NegAll-ErrorL1-NoThresh 86.36 55.27 27.43
Baseline 84.89
NegAll-Clean-ThreshAll 94.74 0.00 0.00
NegAll-Clean-ThreshL1 94.98 0.00 0.00
CZ NegL1-Clean-ThreshL1 94.66 0.00 0.00
NegAll-ErrorL1-NoThresh 95.85 75.00 10.71
Baseline 95.53
NegAll-Clean-ThreshAll 93.23 26.14 8.14
NegAll-Clean-ThreshL1 94.03 51.59 18.60
IT NegL1-Clean-ThreshL1 93.16 35.00 16.28
NegAll-ErrorL1-NoThresh 93.60 44.95 10.47
Baseline 93.74
NegAll-Clean-ThreshAll 92.73 31.11 3.53
NegAll-Clean-ThreshL1 93.02 48.81 8.24
RU NegL1-Clean-ThreshL1 92.44 34.42 8.82
NegAll-ErrorL1-NoThresh 93.14 52.38 12.94
Baseline 92.98
NegAll-Clean-ThreshAll 91.95 26.14 5.77
NegAll-Clean-ThreshL1 92.02 28.64 5.77
SP NegL1-Clean-ThreshL1 92.44 40.00 7.69
NegAll-ErrorL1-NoThresh 93.71 77.50 19.23
Baseline 92.66
</table>
<tableCaption confidence="0.996727">
Table 4: Performance results for the 4 systems. All sys-
</tableCaption>
<bodyText confidence="0.926433714285714">
tems, except for NegAll-ErrorL1-NoThresh, use a thresh-
old, which is optimized for accuracy on the development
set. Baseline denotes the percentage of prepositions used
correctly in the data. The baseline allows us to evaluate
the systems with respect to accuracy, the percentage of
prepositions, on which the prediction of the system is the
same as the label. Averaged results over 2 runs.
</bodyText>
<footnote confidence="0.992734">
10The Czech data set is the smallest and contains a total of
627 prepositions and only 28 errors.
</footnote>
<page confidence="0.994556">
967
</page>
<bodyText confidence="0.999849666666667">
The NegAll-ErrorL1-NoThresh system does not
use a threshold. However, as shown in Fig. 1, it
is possible to increase the precision of the NegAll-
ErrorL1-NoThresh system by applying a threshold,
at the expense of a lower recall.
While the ordering of the systems with respect to
quality is not consistent from Table 4, due to modest
test data sizes, Table 5 and Fig. 1 show results for the
models on all data combined and thus give a better
idea of how the systems compare against each other.
Table 5 shows performance results for all
data combined. Both NegAll-Clean-ThreshL1 and
NegL1-Clean-ThreshL1 achieve a better precision
and recall over the system with an unrestricted can-
didate set NegAll-Clean-ThreshAll. Recall that both
of the systems restrict candidate sets, the former at
testing stage, the latter by training a separate clas-
sifier for each source preposition. NegAll-Clean-
ThreshL1 performs slightly better than NegL1-
Clean-ThreshL1. We hypothesize that the NegAll-
Clean-ThreshAll performance may be affected be-
cause the classifiers for different source preposi-
tions contain different number of classes, depend-
ing on the size of L1ConfSet confusion sets, which
makes it more difficult to find a unified thresh-
old. The best performing system overall is NegAll-
ErrorL1-NoThresh. While NegAll-Clean-ThreshL1
and NegL1-Clean-ThreshL1 restrict candidate sets,
NegAll-ErrorL1-NoThresh also provides informa-
tion about the likelihood of each confusion, which
benefits the classifier. The differences between
NegAll-ErrorL1-ThreshL1 and each of the other
three systems are statistically significant11 (McNe-
mar’s test, p &lt; 0.01). The table also demon-
strates that the results on the correction task may
vary widely. For example, the recall varies by lan-
guage between 10.47% and 27.43% for the NegAll-
ErrorL1-NoThresh system. The highest recall num-
bers are obtained for Chinese speakers. These
speakers also have the highest error rate, as we noted
in Section 3.
11Tests of statistical significance compare the combined re-
sults from all language groups for each model. For example, to
compare the model NegAll-Clean-ThreshAll to NegAll-ErrorL1-
NoThresh, we combine the results from the five language-
specific models NegAll-ErrorL1-NoThresh and compare them
to the results on the combined data from the five language
groups achieved by the model NegAll-Clean-ThreshAll.
</bodyText>
<table confidence="0.999319">
System Acc. P R
NegAll-Clean-ThreshAll 90.90 31.11 7.95
NegAll-Clean-ThreshL1 91.11 37.82 12.78
NegL1-Clean-ThreshL1 90.97 34.34 9.66
NegAll-ErrorL1-NoThresh 92.23 58.47 19.60
</table>
<tableCaption confidence="0.997000857142857">
Table 5: Comparison of the performance of the 4 sys-
tems on all data combined. All systems, except for
NegAll-ErrorL1-NoThresh, use a threshold, which is op-
timized for accuracy on the development set. The dif-
ferences between NegAll-ErrorL1-ThreshL1 and each of
the other three systems are statistically significant (Mc-
Nemar’s test, p &lt; 0.01).
</tableCaption>
<bodyText confidence="0.9991905">
Finally, Fig. 1 shows precision/recall curves for
the systems12. The curves are obtained by varying
a decision threshold for each system. Before we ex-
amine the differences between the models, it should
be noted that in error correction tasks precision is
favored over recall due to the low level of error.
</bodyText>
<figure confidence="0.975463857142857">
�
������T,��a����
0 10 20 30 40 50 60
R
NegAll-Clean-ThreshAll
NegAll-Clean-ThreshL1
NegAll-ErrorL1-ThreshL1
</figure>
<figureCaption confidence="0.997928666666667">
Figure 1: Precision and recall (%) for three mod-
els: NegAll-Clean-ThreshAll, NegAll-Clean-ThreshL1,
and NegAll-ErrorL1-ThreshL1.
</figureCaption>
<bodyText confidence="0.907148857142857">
The curves demonstrate that NegAll-Clean-
ThreshL1 and NegAll-ErrorL1-ThreshL1 are supe-
rior to the baseline system NegAll-Clean-ThreshAll:
on the same recall points, the precision for both
systems is consistently better than for the base-
12NegL1-Clean-ThreshL1 is not shown, since it is similar in
its behavior to NegAll-Clean-ThreshL1.
</bodyText>
<figure confidence="0.99511725">
100
80
60
P
40
20
0
�
</figure>
<page confidence="0.990617">
968
</page>
<bodyText confidence="0.999905">
line model13. Moreover, while restricting candi-
date sets improves the results, providing informa-
tion to the classifier about the likelihoods of differ-
ent confusions is more helpful, which is reflected
in the precision differences between NegAll-Clean-
ThreshL1 and NegAll-ErrorL1-ThreshL1. In fact,
NegAll-ErrorL1-ThreshL1 achieves a higher preci-
sion compared to the other systems, even when no
threshold is used (Tables 4 and 5). This is because,
unlike the other models, this system does not tend to
propose too many false alarms.
</bodyText>
<subsectionHeader confidence="0.998318">
6.1 Comparison to Other Systems
</subsectionHeader>
<bodyText confidence="0.998336151515152">
It is difficult to compare performance to other sys-
tems, since training and evaluation are not per-
formed on the same data, and results may vary
widely depending on the first language and profi-
ciency level of the writer. However, in Table 6 we
list several systems and their performance on the
task. Tetreault et al. (2010) train on native data and
obtain a precision of 48.6% and a recall of 22.5%
with top 34 prepositions on essays from the Test
of English as a Foreign Language exams. Han et
al. (2010) obtain a precision of 81.7% and a recall
of 13.2% using a model trained on partially error-
tagged data by Korean speakers on top ten preposi-
tions. A model trained on 2 million examples from
clean text achieved on the same data set a precision
of 46.3% and a recall of 11.6%.
Gamon (2010) shows precision/recall curves on
the combined task of detecting missing, extrane-
ous and confused prepositions. For recall points
10% and 20%, precisions of 55% and 40%, respec-
tively, are obtained. For our data, a recall of 10%
corresponds to a precision of 46% for the worst-
performing model and 78% for the best-performing
model. For 20% recall, we obtain a precision of
33% for the worst-performing model and 58% for
the best-performing model. We would like to em-
phasize that these comparisons should be interpreted
with caution.
13While significance tests did not show differences between
NegAll-Clean-ThreshAll and NegAll-Clean-ThreshL1, perhaps
due to a modest test set size, the curves demonstrate that the lat-
ter system indeed provides a stable advantage over the baseline
unrestricted approach.
</bodyText>
<sectionHeader confidence="0.964952" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999848">
In this paper, we proposed methods for improving
candidate sets for the task of detecting and correct-
ing errors in text. To correct errors in preposition
usage made by non-native speakers of English, we
proposed L1-dependent confusion sets that deter-
mine valid candidate corrections using knowledge
about preposition confusions observed in the non-
native text. We found that restricting candidates to
</bodyText>
<table confidence="0.998637428571429">
System Training Data P R
Tetreault et al., 2010 native; 34 preps. 48.6 22.5
Han et al., 2010 partially error-tagged; 81.7 13.2
10 preps.
Han et al., 2010 native; 10 preps. 46.3 11.6
Gamon, 2010 native; 12 preps.+ 33.0 10.0
extraneous+missing
Gamon, 2010 native+error-tagged; 55.0 10.0
12 preps.+
extraneous+missing
NegAll-Clean-ThreshAll native; 10 preps. 46.0 10.0
NegAll-ErrorL1-ThreshL1 native with 78.0 10.0
L1 error statistics;
10 preps.
</table>
<tableCaption confidence="0.8182328">
Table 6: Comparison to other systems. Please note
that a direct comparison is not possible, since the systems
are trained and evaluated on different data sets. Gamon
(2010) also considers missing and extraneous preposition
errors.
</tableCaption>
<bodyText confidence="0.999924307692308">
those that are observed in the non-native data im-
proves both the precision and the recall compared to
a classifier that considers as possible candidates the
set of all prepositions. Furthermore, the approach
that takes into account the likelihood of each prepo-
sition confusion is shown to be the most effective.
The methods proposed in this paper make use of
select characteristics that the error-tagged data can
provide. We would also like to compare the pro-
posed methods to the quality of a model trained on
error-tagged data. Improving the system is also in
our future work, but orthogonal to the current con-
tribution.
</bodyText>
<sectionHeader confidence="0.998847" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.992916">
We thank Nick Rizzolo for helpful discussions on
LBJ. We also thank Peter Chew and the anonymous
reviewers for their insightful comments. This re-
search is partly supported by a grant from the U.S.
Department of Education.
</bodyText>
<page confidence="0.997938">
969
</page>
<sectionHeader confidence="0.995886" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999899355769231">
M. Banko and E. Brill. 2001. Scaling to very very large
corpora for natural language disambiguation. In Pro-
ceedings of 39th Annual Meeting of the Association for
Computational Linguistics, pages 26–33, Toulouse,
France, July.
J. Bitchener, S. Young, and D. Cameron. 2005. The ef-
fect of different types of corrective feedback on ESL
student writing. Journal of Second Language Writing.
A. Carlson and I. Fette. 2007. Memory-based context-
sensitive spelling correction at web scale. In Proceed-
ings of the IEEE International Conference on Machine
Learning and Applications (ICMLA).
A. J. Carlson, J. Rosen, and D. Roth. 2001. Scaling
up context sensitive text correction. In Proceedings of
the National Conference on Innovative Applications of
Artificial Intelligence (IAAI), pages 45–50.
M. Chodorow, J. Tetreault, and N.-R. Han. 2007. Detec-
tion of grammatical errors involving prepositions. In
Proceedings of the Fourth ACL-SIGSEM Workshop on
Prepositions, pages 25–30, Prague, Czech Republic,
June. Association for Computational Linguistics.
G. Dalgish. 1985. Computer-assisted ESL research.
CALICO Journal, 2(2).
J. Eeg-Olofsson and O. Knuttson. 2003. Automatic
grammar checking for second language learners - the
use of prepositions. Nodalida.
A. Elghaari, D. Meurers, and H. Wunsch. 2010. Ex-
ploring the data-driven prediction of prepositions in
english. In Proceedings of COLING 2010, Beijing,
China.
R. De Felice and S. Pulman. 2007. Automatically ac-
quiring models of preposition use. In Proceedings of
the Fourth ACL-SIGSEM Workshop on Prepositions,
pages 45–50, Prague, Czech Republic, June.
R. De Felice and S. Pulman. 2008. A classifier-based ap-
proach to preposition and determiner error correction
in L2 English. In Proceedings of the 22nd Interna-
tional Conference on Computational Linguistics (Col-
ing 2008), pages 169–176, Manchester, UK, August.
Y. Freund and R. E. Schapire. 1999. Large margin clas-
sification using the perceptron algorithm. Machine
Learning, 37(3):277–296.
M. Gamon, J. Gao, C. Brockett, A. Klementiev,
W. Dolan, D. Belenko, and L. Vanderwende. 2008.
Using contextual speller techniques and language
modeling for ESL error correction. In Proceedings of
IJCNLP.
M. Gamon. 2010. Using mostly native data to correct
errors in learners’ writing. In NAACL, pages 163–171,
Los Angeles, California, June.
A. R. Golding and D. Roth. 1999. A Winnow based
approach to context-sensitive spelling correction. Ma-
chine Learning, 34(1-3):107–130.
S. Granger, E. Dagneaux, and F. Meunier. 2002. Inter-
national Corpus of Learner English. Presses universi-
taires de Louvain.
S. Gui and H. Yang. 2003. Zhongguo Xuexizhe Yingyu
Yuliaohu. (Chinese Learner English Corpus). Shang-
hai Waiyu Jiaoyu Chubanshe. (In Chinese).
N. Han, M. Chodorow, and C. Leacock. 2006. Detecting
errors in English article usage by non-native speakers.
Journal of Natural Language Engineering, 12(2):115–
129.
N. Han, J. Tetreault, S. Lee, and J. Ha. 2010. Us-
ing an error-annotated learner corpus to develop and
ESL/EFL error correction system. In LREC, Malta,
May.
E. Izumi, K. Uchimoto, T. Saiga, T. Supnithi, and H. Isa-
hara. 2003. Automatic error detection in the Japanese
learners’ English spoken data. In The Companion Vol-
ume to the Proceedings of 41st Annual Meeting of
the Association for Computational Linguistics, pages
145–148, Sapporo, Japan, July.
C. Leacock, M. Chodorow, M. Gamon, and J. Tetreault.
2010. Morgan and Claypool Publishers.
J. Lee and S. Seneff. 2008. An analysis of grammatical
errors in non-native speech in English. In Proceedings
of the 2008 Spoken Language Technology Workshop.
V. Punyakanok, D. Roth, and W. Yih. 2008. The impor-
tance of syntactic parsing and inference in semantic
role labeling. Computational Linguistics, 34(2).
N. Rizzolo and D. Roth. 2007. Modeling Discriminative
Global Inference. In Proceedings of the First Inter-
national Conference on Semantic Computing (ICSC),
pages 597–604, Irvine, California, September. IEEE.
D. Roth. 1998. Learning to resolve natural language am-
biguities: A unified approach. In Proceedings of the
National Conference on Artificial Intelligence (AAAI),
pages 806–813.
A. Rozovskaya and D. Roth. 2010a. Annotating ESL
errors: Challenges and rewards. In Proceedings of the
NAACL Workshop on Innovative Use of NLPfor Build-
ing Educational Applications.
A. Rozovskaya and D. Roth. 2010b. Training paradigms
for correcting errors in grammar and usage. In Pro-
ceedings of the NAACL-HLT.
J. Tetreault and M. Chodorow. 2008. The ups and
downs of preposition error detection in ESL writing.
In Proceedings of the 22nd International Conference
on Computational Linguistics (Coling 2008), pages
865–872, Manchester, UK, August.
J. Tetreault, J. Foster, and M. Chodorow. 2010. Using
parse features for preposition selection and error de-
tection. In ACL.
</reference>
<page confidence="0.997781">
970
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.812471">
<title confidence="0.999931">Generating Confusion Sets for Context-Sensitive Error Correction</title>
<author confidence="0.998737">Alla Rozovskaya</author>
<author confidence="0.998737">Dan</author>
<affiliation confidence="0.998519">University of Illinois at</affiliation>
<address confidence="0.921074">Urbana, IL</address>
<abstract confidence="0.9958215">In this paper, we consider the problem of generating candidate corrections for the task of correcting errors in text. We focus on the task of correcting errors in preposition usage made by non-native English speakers, using discriminative classifiers. The standard approach to the problem assumes that the set of candidate corrections for a preposition consists of all preposition choices participating in the task. We determine likely preposition confusions using an annotated corpus of nonnative text and use this knowledge to produce smaller sets of candidates. We propose several methods of restricting candidate sets. These methods exclude candidate prepositions that are not observed as valid corrections in the annotated corpus and take into account the likelihood of each preposition confusion in the non-native text. We find that restricting candidates to those that are observed in the non-native data improves both the precision and the recall compared to the approach that views all prepositions as possible candidates. Furthermore, the approach that takes into account the likelihood of each preposition confusion is shown to be the most effective.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Banko</author>
<author>E Brill</author>
</authors>
<title>Scaling to very very large corpora for natural language disambiguation.</title>
<date>2001</date>
<booktitle>In Proceedings of 39th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>26--33</pages>
<location>Toulouse, France,</location>
<contexts>
<context position="1783" citStr="Banko and Brill, 2001" startWordPosition="267" endWordPosition="270">e approach that views all prepositions as possible candidates. Furthermore, the approach that takes into account the likelihood of each preposition confusion is shown to be the most effective. 1 Introduction We address the problem of generating candidate corrections for the task of correcting context-dependent mistakes in text, mistakes that involve confusing valid words in a language. A well-studied instance of this problem – context-sensitive spelling errors – has received a lot of attention in natural language research (Golding and Roth, 1999; Carlson et al., 2001; Carlson and Fette, 2007; Banko and Brill, 2001). The context-sensitive spelling correction task addresses the problem of correcting spelling mistakes that result in legitimate words, such as confusing their and there or your and you’re. In this task, a candidate set or a confusion set is defined that specifies a list of confusable words, e.g., {their, there} or {cite, site, sight}. Each occurrence of a confusable word in text is represented as a vector of features derived from a small context window around the target. A classifier is trained on text assumed to be error-free, replacing each target word occurrence (e.g. their) with a confusi</context>
<context position="7884" citStr="Banko and Brill, 2001" startWordPosition="1253" endWordPosition="1256">er is organized as follows. First, we describe related work on error correction. Section 3 presents the ESL data and statistics on preposition errors. Section 4 describes the methods of restricting candidate sets in training and testing. Section 5 describes the experimental setup. We present and discuss the results in Section 6. The key findings are summarized in Table 5 and Fig. 1 in Section 6. We conclude with a brief discussion of directions for future work. 2 Related Work Work in text correction has focused primarily on correcting context-sensitive spelling errors (Golding and Roth, 1999; Banko and Brill, 2001; Carlson et al., 2001; Carlson and Fette, 2007) and mistakes made by ESL learners, especially errors in article and preposition usage. Roth (1998) takes a unified approach to resolving semantic and syntactic ambiguities in natural lan962 guage by treating several related problems, including word sense disambiguation, word selection, and context-sensitive spelling correction as instances of the disambiguation task. Given a candidate set or a confusion set of confusable words, the task is to select the most likely candidate in context. Examples of confusion sets are {sight, site, cite} for cont</context>
</contexts>
<marker>Banko, Brill, 2001</marker>
<rawString>M. Banko and E. Brill. 2001. Scaling to very very large corpora for natural language disambiguation. In Proceedings of 39th Annual Meeting of the Association for Computational Linguistics, pages 26–33, Toulouse, France, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bitchener</author>
<author>S Young</author>
<author>D Cameron</author>
</authors>
<title>The effect of different types of corrective feedback on ESL student writing.</title>
<date>2005</date>
<journal>Journal of Second Language Writing.</journal>
<contexts>
<context position="3041" citStr="Bitchener et al., 2005" startWordPosition="477" endWordPosition="480"> thus generating both positive and negative examples, respectively, from the same context. Given a text to correct, for each word in text that belongs to the confusion set the classifier predicts the most likely candidate in the confusion set. More recently, work in error correction has taken an interesting turn and focused on correcting errors made by English as a Second Language (ESL) learners, with a special interest given to errors in article and preposition usage. These mistakes are some of the most common mistakes for non-native English speakers of all proficiency levels (Dalgish, 1985; Bitchener et al., 2005; Leacock et al., 2010). Approaches to correcting these mistakes have adopted the methods of the context-sensitive spelling correction task. A system is usually trained on wellformed native English text (Izumi et al., 2003; EegOlofsson and Knuttson, 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault 961 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 961–970, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics and Chodorow, 2008; Elghaari et al., 2010; Tetreault et al., 2010), but</context>
<context position="13466" citStr="Bitchener et al., 2005" startWordPosition="2164" endWordPosition="2167"> most common mistakes that non-native speakers make. In the Cambridge Learner Corpus2 (CLC), which contains data by learners of different first language backgrounds and different proficiency levels, preposition errors account for about 13.5% of all errors and occur on average in 10% of all sentences (Leacock et al., 2010). Similar error rates have been reported for other annotated ESL corpora, e.g. (Izumi et al., 2003; Rozovskaya and Roth, 2010a; Tetreault et al., 2010). Learning correct preposition usage in English is challenging for learners of all first language backgrounds (Dalgish, 1985; Bitchener et al., 2005; Gamon, 2010; Leacock et al., 2010). 3.2 The Annotated Corpus We use data from an annotated corpus of essays written by ESL students. The essays were fully corrected and error-tagged by native English speakers. For each preposition used incorrectly by the author, the annotator also indicated the correct preposition choice. Rozovskaya and Roth (2010a) provide a detailed description of the annotation of the data. The annotated data include sentences by speakers of five first language backgrounds: Chinese, Czech, Italian, Russian, and Spanish. The Czech, Italian, Russian and Spanish data come fr</context>
</contexts>
<marker>Bitchener, Young, Cameron, 2005</marker>
<rawString>J. Bitchener, S. Young, and D. Cameron. 2005. The effect of different types of corrective feedback on ESL student writing. Journal of Second Language Writing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Carlson</author>
<author>I Fette</author>
</authors>
<title>Memory-based contextsensitive spelling correction at web scale.</title>
<date>2007</date>
<booktitle>In Proceedings of the IEEE International Conference on Machine Learning and Applications (ICMLA).</booktitle>
<contexts>
<context position="1759" citStr="Carlson and Fette, 2007" startWordPosition="263" endWordPosition="266">the recall compared to the approach that views all prepositions as possible candidates. Furthermore, the approach that takes into account the likelihood of each preposition confusion is shown to be the most effective. 1 Introduction We address the problem of generating candidate corrections for the task of correcting context-dependent mistakes in text, mistakes that involve confusing valid words in a language. A well-studied instance of this problem – context-sensitive spelling errors – has received a lot of attention in natural language research (Golding and Roth, 1999; Carlson et al., 2001; Carlson and Fette, 2007; Banko and Brill, 2001). The context-sensitive spelling correction task addresses the problem of correcting spelling mistakes that result in legitimate words, such as confusing their and there or your and you’re. In this task, a candidate set or a confusion set is defined that specifies a list of confusable words, e.g., {their, there} or {cite, site, sight}. Each occurrence of a confusable word in text is represented as a vector of features derived from a small context window around the target. A classifier is trained on text assumed to be error-free, replacing each target word occurrence (e.</context>
<context position="7932" citStr="Carlson and Fette, 2007" startWordPosition="1261" endWordPosition="1264">e related work on error correction. Section 3 presents the ESL data and statistics on preposition errors. Section 4 describes the methods of restricting candidate sets in training and testing. Section 5 describes the experimental setup. We present and discuss the results in Section 6. The key findings are summarized in Table 5 and Fig. 1 in Section 6. We conclude with a brief discussion of directions for future work. 2 Related Work Work in text correction has focused primarily on correcting context-sensitive spelling errors (Golding and Roth, 1999; Banko and Brill, 2001; Carlson et al., 2001; Carlson and Fette, 2007) and mistakes made by ESL learners, especially errors in article and preposition usage. Roth (1998) takes a unified approach to resolving semantic and syntactic ambiguities in natural lan962 guage by treating several related problems, including word sense disambiguation, word selection, and context-sensitive spelling correction as instances of the disambiguation task. Given a candidate set or a confusion set of confusable words, the task is to select the most likely candidate in context. Examples of confusion sets are {sight, site, cite} for contextsensitive spelling correction, {among, betwee</context>
</contexts>
<marker>Carlson, Fette, 2007</marker>
<rawString>A. Carlson and I. Fette. 2007. Memory-based contextsensitive spelling correction at web scale. In Proceedings of the IEEE International Conference on Machine Learning and Applications (ICMLA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A J Carlson</author>
<author>J Rosen</author>
<author>D Roth</author>
</authors>
<title>Scaling up context sensitive text correction.</title>
<date>2001</date>
<booktitle>In Proceedings of the National Conference on Innovative Applications of Artificial Intelligence (IAAI),</booktitle>
<pages>45--50</pages>
<contexts>
<context position="1734" citStr="Carlson et al., 2001" startWordPosition="259" endWordPosition="262">oth the precision and the recall compared to the approach that views all prepositions as possible candidates. Furthermore, the approach that takes into account the likelihood of each preposition confusion is shown to be the most effective. 1 Introduction We address the problem of generating candidate corrections for the task of correcting context-dependent mistakes in text, mistakes that involve confusing valid words in a language. A well-studied instance of this problem – context-sensitive spelling errors – has received a lot of attention in natural language research (Golding and Roth, 1999; Carlson et al., 2001; Carlson and Fette, 2007; Banko and Brill, 2001). The context-sensitive spelling correction task addresses the problem of correcting spelling mistakes that result in legitimate words, such as confusing their and there or your and you’re. In this task, a candidate set or a confusion set is defined that specifies a list of confusable words, e.g., {their, there} or {cite, site, sight}. Each occurrence of a confusable word in text is represented as a vector of features derived from a small context window around the target. A classifier is trained on text assumed to be error-free, replacing each t</context>
<context position="7906" citStr="Carlson et al., 2001" startWordPosition="1257" endWordPosition="1260">ows. First, we describe related work on error correction. Section 3 presents the ESL data and statistics on preposition errors. Section 4 describes the methods of restricting candidate sets in training and testing. Section 5 describes the experimental setup. We present and discuss the results in Section 6. The key findings are summarized in Table 5 and Fig. 1 in Section 6. We conclude with a brief discussion of directions for future work. 2 Related Work Work in text correction has focused primarily on correcting context-sensitive spelling errors (Golding and Roth, 1999; Banko and Brill, 2001; Carlson et al., 2001; Carlson and Fette, 2007) and mistakes made by ESL learners, especially errors in article and preposition usage. Roth (1998) takes a unified approach to resolving semantic and syntactic ambiguities in natural lan962 guage by treating several related problems, including word sense disambiguation, word selection, and context-sensitive spelling correction as instances of the disambiguation task. Given a candidate set or a confusion set of confusable words, the task is to select the most likely candidate in context. Examples of confusion sets are {sight, site, cite} for contextsensitive spelling </context>
<context position="22924" citStr="Carlson et al., 2001" startWordPosition="3707" endWordPosition="3710">otated data to obtain error statistics. It should be noted that this method is orthogonal to the NegAll and NegL1 methods of training described above and can be used in conjunction with each of them, only that it transforms the training data to account in a more natural way for ESL writing. We combine the proposed methods NegAll, NegL1 with the Clean or ErrorL1 methods and create three training approaches shown in Table 3. 4.3 Restricting Confusion Sets in Testing To reduce the number of false alarms, correction systems generally use a threshold on the confidence of the classifier, following (Carlson et al., 2001), and propose a correction only when the confidence of the classifier is above the threshold. We show in Section 5 that the system trained on data with artificial errors performs competitively even without a threshold. The other systems use a threshold. We consider two ways of applying a threshold6: 1. ThreshAll A correction for source preposition pi is proposed only when the confidence of the classifier exceeds the threshold. For each preposition in the non-native data, this method considers all candidates as valid corrections. 2. ThreshL1Conf A correction for source preposition pi is propose</context>
</contexts>
<marker>Carlson, Rosen, Roth, 2001</marker>
<rawString>A. J. Carlson, J. Rosen, and D. Roth. 2001. Scaling up context sensitive text correction. In Proceedings of the National Conference on Innovative Applications of Artificial Intelligence (IAAI), pages 45–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Chodorow</author>
<author>J Tetreault</author>
<author>N-R Han</author>
</authors>
<title>Detection of grammatical errors involving prepositions.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth ACL-SIGSEM Workshop on Prepositions,</booktitle>
<pages>25--30</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<marker>Chodorow, Tetreault, Han, 2007</marker>
<rawString>M. Chodorow, J. Tetreault, and N.-R. Han. 2007. Detection of grammatical errors involving prepositions. In Proceedings of the Fourth ACL-SIGSEM Workshop on Prepositions, pages 25–30, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Dalgish</author>
</authors>
<date>1985</date>
<journal>Computer-assisted ESL research. CALICO Journal,</journal>
<volume>2</volume>
<issue>2</issue>
<contexts>
<context position="3017" citStr="Dalgish, 1985" startWordPosition="475" endWordPosition="476">{their, there}, thus generating both positive and negative examples, respectively, from the same context. Given a text to correct, for each word in text that belongs to the confusion set the classifier predicts the most likely candidate in the confusion set. More recently, work in error correction has taken an interesting turn and focused on correcting errors made by English as a Second Language (ESL) learners, with a special interest given to errors in article and preposition usage. These mistakes are some of the most common mistakes for non-native English speakers of all proficiency levels (Dalgish, 1985; Bitchener et al., 2005; Leacock et al., 2010). Approaches to correcting these mistakes have adopted the methods of the context-sensitive spelling correction task. A system is usually trained on wellformed native English text (Izumi et al., 2003; EegOlofsson and Knuttson, 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault 961 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 961–970, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics and Chodorow, 2008; Elghaari et al., 2010; Tetr</context>
<context position="13442" citStr="Dalgish, 1985" startWordPosition="2162" endWordPosition="2163"> are one of the most common mistakes that non-native speakers make. In the Cambridge Learner Corpus2 (CLC), which contains data by learners of different first language backgrounds and different proficiency levels, preposition errors account for about 13.5% of all errors and occur on average in 10% of all sentences (Leacock et al., 2010). Similar error rates have been reported for other annotated ESL corpora, e.g. (Izumi et al., 2003; Rozovskaya and Roth, 2010a; Tetreault et al., 2010). Learning correct preposition usage in English is challenging for learners of all first language backgrounds (Dalgish, 1985; Bitchener et al., 2005; Gamon, 2010; Leacock et al., 2010). 3.2 The Annotated Corpus We use data from an annotated corpus of essays written by ESL students. The essays were fully corrected and error-tagged by native English speakers. For each preposition used incorrectly by the author, the annotator also indicated the correct preposition choice. Rozovskaya and Roth (2010a) provide a detailed description of the annotation of the data. The annotated data include sentences by speakers of five first language backgrounds: Chinese, Czech, Italian, Russian, and Spanish. The Czech, Italian, Russian </context>
</contexts>
<marker>Dalgish, 1985</marker>
<rawString>G. Dalgish. 1985. Computer-assisted ESL research. CALICO Journal, 2(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eeg-Olofsson</author>
<author>O Knuttson</author>
</authors>
<title>Automatic grammar checking for second language learners - the use of prepositions.</title>
<date>2003</date>
<journal>Nodalida.</journal>
<marker>Eeg-Olofsson, Knuttson, 2003</marker>
<rawString>J. Eeg-Olofsson and O. Knuttson. 2003. Automatic grammar checking for second language learners - the use of prepositions. Nodalida.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Elghaari</author>
<author>D Meurers</author>
<author>H Wunsch</author>
</authors>
<title>Exploring the data-driven prediction of prepositions in english.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING 2010,</booktitle>
<location>Beijing, China.</location>
<contexts>
<context position="3611" citStr="Elghaari et al., 2010" startWordPosition="565" endWordPosition="568">iency levels (Dalgish, 1985; Bitchener et al., 2005; Leacock et al., 2010). Approaches to correcting these mistakes have adopted the methods of the context-sensitive spelling correction task. A system is usually trained on wellformed native English text (Izumi et al., 2003; EegOlofsson and Knuttson, 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault 961 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 961–970, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics and Chodorow, 2008; Elghaari et al., 2010; Tetreault et al., 2010), but several works incorporate into training error-tagged data (Gamon, 2010; Han et al., 2010) or error statistics (Rozovskaya and Roth, 2010b). The classifier is then applied to non-native text to predict the correct article/preposition in context. The possible candidate selections include the set of all articles or all prepositions. While in the article correction task the candidate set is small (a, the, no article), systems for correcting preposition errors, even when they consider the most common prepositions, may include between 9 to 34 preposition classes. For e</context>
</contexts>
<marker>Elghaari, Meurers, Wunsch, 2010</marker>
<rawString>A. Elghaari, D. Meurers, and H. Wunsch. 2010. Exploring the data-driven prediction of prepositions in english. In Proceedings of COLING 2010, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R De Felice</author>
<author>S Pulman</author>
</authors>
<title>Automatically acquiring models of preposition use.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth ACL-SIGSEM Workshop on Prepositions,</booktitle>
<pages>45--50</pages>
<location>Prague, Czech Republic,</location>
<marker>De Felice, Pulman, 2007</marker>
<rawString>R. De Felice and S. Pulman. 2007. Automatically acquiring models of preposition use. In Proceedings of the Fourth ACL-SIGSEM Workshop on Prepositions, pages 45–50, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R De Felice</author>
<author>S Pulman</author>
</authors>
<title>A classifier-based approach to preposition and determiner error correction in L2 English.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<pages>169--176</pages>
<location>Manchester, UK,</location>
<marker>De Felice, Pulman, 2008</marker>
<rawString>R. De Felice and S. Pulman. 2008. A classifier-based approach to preposition and determiner error correction in L2 English. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 169–176, Manchester, UK, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>R E Schapire</author>
</authors>
<title>Large margin classification using the perceptron algorithm.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>37</volume>
<issue>3</issue>
<contexts>
<context position="26307" citStr="Freund and Schapire, 1999" startWordPosition="4236" endWordPosition="4239">. All of the systems are trained on the same set of word and part-of-speech features using the same set of training examples. Features are extracted from a window of eight words around the preposition and include words, part-of-speech tags and conjunctions of words and tags of lengths two, three, and four. Training data are extracted from English Wikipedia and the New York Times section of the Gigaword corpus (Linguistic Data Consortium, 2003). In each training paradigm, we follow a discriminative approach, using an online learning paradigm and making use of the Averaged Perceptron Algorithm (Freund and Schapire, 1999) – we use the regularized version in Learning Based Java9 (LBJ, (Rizzolo and Roth, 2007)). While classical Perceptron comes with generalization bound related to the margin of the data, Averaged Perceptron also comes with a PAC-like generalization bound (Freund and Schapire, 1999). This linear learning algorithm is known, both theoretically and experimentally, to be among the best linear learning approaches and is competitive with SVM and Logistic 8ThreshAll is not possible with this training option, as the system never proposes a correction that is not in L1ConfSet(pz). 9LBJ code is available </context>
</contexts>
<marker>Freund, Schapire, 1999</marker>
<rawString>Y. Freund and R. E. Schapire. 1999. Large margin classification using the perceptron algorithm. Machine Learning, 37(3):277–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gamon</author>
<author>J Gao</author>
<author>C Brockett</author>
<author>A Klementiev</author>
<author>W Dolan</author>
<author>D Belenko</author>
<author>L Vanderwende</author>
</authors>
<title>Using contextual speller techniques and language modeling for ESL error correction.</title>
<date>2008</date>
<booktitle>In Proceedings of IJCNLP.</booktitle>
<contexts>
<context position="3358" citStr="Gamon et al., 2008" startWordPosition="531" endWordPosition="534">used on correcting errors made by English as a Second Language (ESL) learners, with a special interest given to errors in article and preposition usage. These mistakes are some of the most common mistakes for non-native English speakers of all proficiency levels (Dalgish, 1985; Bitchener et al., 2005; Leacock et al., 2010). Approaches to correcting these mistakes have adopted the methods of the context-sensitive spelling correction task. A system is usually trained on wellformed native English text (Izumi et al., 2003; EegOlofsson and Knuttson, 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault 961 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 961–970, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics and Chodorow, 2008; Elghaari et al., 2010; Tetreault et al., 2010), but several works incorporate into training error-tagged data (Gamon, 2010; Han et al., 2010) or error statistics (Rozovskaya and Roth, 2010b). The classifier is then applied to non-native text to predict the correct article/preposition in context. The possible candidate selections include the set of all articles or al</context>
<context position="9366" citStr="Gamon et al. (2008)" startWordPosition="1493" endWordPosition="1496">f error-free text. Given text to correct, for each word in text that belongs to the confusion set the classifier is used to predict the most likely candidate in the confusion set given the word’s context. In the same spirit, models for correcting ESL errors are generally trained on well-formed native text. Han et al. (2006) train a maximum entropy model to correct article mistakes. Chodorow et. al (2007), Tetreault and Chodorow (2008), and De Felice and Pulman (2008) train a maximum entropy model and De Felice and Pulman (2007) train a voted perceptron algorithm to correct preposition errors. Gamon et al. (2008) train a decision tree model and a language model to correct errors in article and preposition usage. Bergsma et al. (2009) propose a Naive Bayes algorithm with web-scale N-grams as features, for preposition selection and context-sensitive spelling correction. The set of valid candidate corrections for a target word includes all words in the confusion set. For the preposition correction task, the entire set of prepositions considered for the task is viewed as the set of possible corrections for each preposition in nonnative text. Given a preposition with its surrounding context, the model sele</context>
</contexts>
<marker>Gamon, Gao, Brockett, Klementiev, Dolan, Belenko, Vanderwende, 2008</marker>
<rawString>M. Gamon, J. Gao, C. Brockett, A. Klementiev, W. Dolan, D. Belenko, and L. Vanderwende. 2008. Using contextual speller techniques and language modeling for ESL error correction. In Proceedings of IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gamon</author>
</authors>
<title>Using mostly native data to correct errors in learners’ writing.</title>
<date>2010</date>
<booktitle>In NAACL,</booktitle>
<pages>163--171</pages>
<location>Los Angeles, California,</location>
<contexts>
<context position="3712" citStr="Gamon, 2010" startWordPosition="581" endWordPosition="582">akes have adopted the methods of the context-sensitive spelling correction task. A system is usually trained on wellformed native English text (Izumi et al., 2003; EegOlofsson and Knuttson, 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault 961 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 961–970, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics and Chodorow, 2008; Elghaari et al., 2010; Tetreault et al., 2010), but several works incorporate into training error-tagged data (Gamon, 2010; Han et al., 2010) or error statistics (Rozovskaya and Roth, 2010b). The classifier is then applied to non-native text to predict the correct article/preposition in context. The possible candidate selections include the set of all articles or all prepositions. While in the article correction task the candidate set is small (a, the, no article), systems for correcting preposition errors, even when they consider the most common prepositions, may include between 9 to 34 preposition classes. For each preposition in the non-native text, every other candidate in the confusion set is viewed as a pot</context>
<context position="10118" citStr="Gamon, 2010" startWordPosition="1620" endWordPosition="1621">Bayes algorithm with web-scale N-grams as features, for preposition selection and context-sensitive spelling correction. The set of valid candidate corrections for a target word includes all words in the confusion set. For the preposition correction task, the entire set of prepositions considered for the task is viewed as the set of possible corrections for each preposition in nonnative text. Given a preposition with its surrounding context, the model selects the most likely preposition from the set of all candidates, where the set of candidates consists of nine (Felice and Pulman, 2008), 12 (Gamon, 2010), or 34 (Tetreault et al., 2010; Tetreault and Chodorow, 2008) prepositions. 2.1 Using Error-tagged Data in Training Several recent works explore ways of using annotated non-native text when training error correction models. One way to incorporate knowledge about which confusions are likely with ESL learners into the error correction system is to train a model on error-tagged data. Preposition confusions observed in the nonnative text can then be included in training, by using the preposition chosen by the author (the source preposition) as a feature. This is not possible with a system trained</context>
<context position="11878" citStr="Gamon (2010)" startWordPosition="1904" endWordPosition="1905"> model, both trained on native data, into a meta-classifier that is trained on a smaller amount of annotated ESL data. The metaclassifier outperforms by a large margin both of the native models, but it requires large amounts of expensive annotated data, especially in order to correct preposition errors, where the problem complexity is much larger. Rozovskaya and Roth (2010b) show that by introducing into native training data artificial article errors it is possible to improve the performance of the article correction system, when compared to a classifier trained on native data. In contrast to Gamon (2010) and Han et al. (2010) that use annotated data for training, the system is trained on native data, but the native data are transformed to be more like L1 data through artificial article errors that mimic the error rates and error patterns of non-native writers. This method is cheaper, since obtaining error statistics requires much less annotated data than training. Moreover, the training data size is not restricted by the amount of the error-tagged data available. Finally, the source article of the writer can be used in training as a feature, in the exact same way as with the models trained on</context>
<context position="13479" citStr="Gamon, 2010" startWordPosition="2168" endWordPosition="2169">at non-native speakers make. In the Cambridge Learner Corpus2 (CLC), which contains data by learners of different first language backgrounds and different proficiency levels, preposition errors account for about 13.5% of all errors and occur on average in 10% of all sentences (Leacock et al., 2010). Similar error rates have been reported for other annotated ESL corpora, e.g. (Izumi et al., 2003; Rozovskaya and Roth, 2010a; Tetreault et al., 2010). Learning correct preposition usage in English is challenging for learners of all first language backgrounds (Dalgish, 1985; Bitchener et al., 2005; Gamon, 2010; Leacock et al., 2010). 3.2 The Annotated Corpus We use data from an annotated corpus of essays written by ESL students. The essays were fully corrected and error-tagged by native English speakers. For each preposition used incorrectly by the author, the annotator also indicated the correct preposition choice. Rozovskaya and Roth (2010a) provide a detailed description of the annotation of the data. The annotated data include sentences by speakers of five first language backgrounds: Chinese, Czech, Italian, Russian, and Spanish. The Czech, Italian, Russian and Spanish data come from the Intern</context>
<context position="34289" citStr="Gamon (2010)" startWordPosition="5435" endWordPosition="5436">age and proficiency level of the writer. However, in Table 6 we list several systems and their performance on the task. Tetreault et al. (2010) train on native data and obtain a precision of 48.6% and a recall of 22.5% with top 34 prepositions on essays from the Test of English as a Foreign Language exams. Han et al. (2010) obtain a precision of 81.7% and a recall of 13.2% using a model trained on partially errortagged data by Korean speakers on top ten prepositions. A model trained on 2 million examples from clean text achieved on the same data set a precision of 46.3% and a recall of 11.6%. Gamon (2010) shows precision/recall curves on the combined task of detecting missing, extraneous and confused prepositions. For recall points 10% and 20%, precisions of 55% and 40%, respectively, are obtained. For our data, a recall of 10% corresponds to a precision of 46% for the worstperforming model and 78% for the best-performing model. For 20% recall, we obtain a precision of 33% for the worst-performing model and 58% for the best-performing model. We would like to emphasize that these comparisons should be interpreted with caution. 13While significance tests did not show differences between NegAll-C</context>
<context position="35714" citStr="Gamon, 2010" startWordPosition="5660" endWordPosition="5661">n and Future Work In this paper, we proposed methods for improving candidate sets for the task of detecting and correcting errors in text. To correct errors in preposition usage made by non-native speakers of English, we proposed L1-dependent confusion sets that determine valid candidate corrections using knowledge about preposition confusions observed in the nonnative text. We found that restricting candidates to System Training Data P R Tetreault et al., 2010 native; 34 preps. 48.6 22.5 Han et al., 2010 partially error-tagged; 81.7 13.2 10 preps. Han et al., 2010 native; 10 preps. 46.3 11.6 Gamon, 2010 native; 12 preps.+ 33.0 10.0 extraneous+missing Gamon, 2010 native+error-tagged; 55.0 10.0 12 preps.+ extraneous+missing NegAll-Clean-ThreshAll native; 10 preps. 46.0 10.0 NegAll-ErrorL1-ThreshL1 native with 78.0 10.0 L1 error statistics; 10 preps. Table 6: Comparison to other systems. Please note that a direct comparison is not possible, since the systems are trained and evaluated on different data sets. Gamon (2010) also considers missing and extraneous preposition errors. those that are observed in the non-native data improves both the precision and the recall compared to a classifier that</context>
</contexts>
<marker>Gamon, 2010</marker>
<rawString>M. Gamon. 2010. Using mostly native data to correct errors in learners’ writing. In NAACL, pages 163–171, Los Angeles, California, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A R Golding</author>
<author>D Roth</author>
</authors>
<title>A Winnow based approach to context-sensitive spelling correction.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="1712" citStr="Golding and Roth, 1999" startWordPosition="255" endWordPosition="258">n-native data improves both the precision and the recall compared to the approach that views all prepositions as possible candidates. Furthermore, the approach that takes into account the likelihood of each preposition confusion is shown to be the most effective. 1 Introduction We address the problem of generating candidate corrections for the task of correcting context-dependent mistakes in text, mistakes that involve confusing valid words in a language. A well-studied instance of this problem – context-sensitive spelling errors – has received a lot of attention in natural language research (Golding and Roth, 1999; Carlson et al., 2001; Carlson and Fette, 2007; Banko and Brill, 2001). The context-sensitive spelling correction task addresses the problem of correcting spelling mistakes that result in legitimate words, such as confusing their and there or your and you’re. In this task, a candidate set or a confusion set is defined that specifies a list of confusable words, e.g., {their, there} or {cite, site, sight}. Each occurrence of a confusable word in text is represented as a vector of features derived from a small context window around the target. A classifier is trained on text assumed to be error-</context>
<context position="7861" citStr="Golding and Roth, 1999" startWordPosition="1249" endWordPosition="1252">ing. The rest of the paper is organized as follows. First, we describe related work on error correction. Section 3 presents the ESL data and statistics on preposition errors. Section 4 describes the methods of restricting candidate sets in training and testing. Section 5 describes the experimental setup. We present and discuss the results in Section 6. The key findings are summarized in Table 5 and Fig. 1 in Section 6. We conclude with a brief discussion of directions for future work. 2 Related Work Work in text correction has focused primarily on correcting context-sensitive spelling errors (Golding and Roth, 1999; Banko and Brill, 2001; Carlson et al., 2001; Carlson and Fette, 2007) and mistakes made by ESL learners, especially errors in article and preposition usage. Roth (1998) takes a unified approach to resolving semantic and syntactic ambiguities in natural lan962 guage by treating several related problems, including word sense disambiguation, word selection, and context-sensitive spelling correction as instances of the disambiguation task. Given a candidate set or a confusion set of confusable words, the task is to select the most likely candidate in context. Examples of confusion sets are {sigh</context>
</contexts>
<marker>Golding, Roth, 1999</marker>
<rawString>A. R. Golding and D. Roth. 1999. A Winnow based approach to context-sensitive spelling correction. Machine Learning, 34(1-3):107–130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Granger</author>
<author>E Dagneaux</author>
<author>F Meunier</author>
</authors>
<date>2002</date>
<booktitle>International Corpus of Learner English. Presses universitaires de Louvain.</booktitle>
<contexts>
<context position="14142" citStr="Granger et al., 2002" startWordPosition="2269" endWordPosition="2272"> Corpus We use data from an annotated corpus of essays written by ESL students. The essays were fully corrected and error-tagged by native English speakers. For each preposition used incorrectly by the author, the annotator also indicated the correct preposition choice. Rozovskaya and Roth (2010a) provide a detailed description of the annotation of the data. The annotated data include sentences by speakers of five first language backgrounds: Chinese, Czech, Italian, Russian, and Spanish. The Czech, Italian, Russian and Spanish data come from the International Corpus of Learner English (ICLE, (Granger et al., 2002)), which is a collection of essays written by advanced learners of English. The Chinese data is a part of the Chinese Learners of English corpus (CLEC, (Gui and Yang, 2003)) that contains essays by students of all levels of proficiency. Table 1 shows preposition statistics based on the annotated data. The combined data include 4185 prepositions, 8.4% of which were judged to be incorrect by the annotators. Table 1 demonstrates that the error rates in the Chinese speaker data, for which different proficiency levels are available, are 2 or 3 times higher than the error rates in other language gro</context>
</contexts>
<marker>Granger, Dagneaux, Meunier, 2002</marker>
<rawString>S. Granger, E. Dagneaux, and F. Meunier. 2002. International Corpus of Learner English. Presses universitaires de Louvain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Gui</author>
<author>H Yang</author>
</authors>
<title>Zhongguo Xuexizhe Yingyu Yuliaohu. (Chinese Learner English Corpus). Shanghai Waiyu Jiaoyu Chubanshe. (In Chinese).</title>
<date>2003</date>
<contexts>
<context position="14314" citStr="Gui and Yang, 2003" startWordPosition="2301" endWordPosition="2304">ion used incorrectly by the author, the annotator also indicated the correct preposition choice. Rozovskaya and Roth (2010a) provide a detailed description of the annotation of the data. The annotated data include sentences by speakers of five first language backgrounds: Chinese, Czech, Italian, Russian, and Spanish. The Czech, Italian, Russian and Spanish data come from the International Corpus of Learner English (ICLE, (Granger et al., 2002)), which is a collection of essays written by advanced learners of English. The Chinese data is a part of the Chinese Learners of English corpus (CLEC, (Gui and Yang, 2003)) that contains essays by students of all levels of proficiency. Table 1 shows preposition statistics based on the annotated data. The combined data include 4185 prepositions, 8.4% of which were judged to be incorrect by the annotators. Table 1 demonstrates that the error rates in the Chinese speaker data, for which different proficiency levels are available, are 2 or 3 times higher than the error rates in other language groups. The data for other languages come from very advanced learners and, while there are also proficiency differ2http://www.cambridge.org/elt Source Total Incorrect Error la</context>
</contexts>
<marker>Gui, Yang, 2003</marker>
<rawString>S. Gui and H. Yang. 2003. Zhongguo Xuexizhe Yingyu Yuliaohu. (Chinese Learner English Corpus). Shanghai Waiyu Jiaoyu Chubanshe. (In Chinese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Han</author>
<author>M Chodorow</author>
<author>C Leacock</author>
</authors>
<title>Detecting errors in English article usage by non-native speakers.</title>
<date>2006</date>
<journal>Journal of Natural Language Engineering,</journal>
<volume>12</volume>
<issue>2</issue>
<pages>129</pages>
<contexts>
<context position="3313" citStr="Han et al., 2006" startWordPosition="522" endWordPosition="525">ction has taken an interesting turn and focused on correcting errors made by English as a Second Language (ESL) learners, with a special interest given to errors in article and preposition usage. These mistakes are some of the most common mistakes for non-native English speakers of all proficiency levels (Dalgish, 1985; Bitchener et al., 2005; Leacock et al., 2010). Approaches to correcting these mistakes have adopted the methods of the context-sensitive spelling correction task. A system is usually trained on wellformed native English text (Izumi et al., 2003; EegOlofsson and Knuttson, 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault 961 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 961–970, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics and Chodorow, 2008; Elghaari et al., 2010; Tetreault et al., 2010), but several works incorporate into training error-tagged data (Gamon, 2010; Han et al., 2010) or error statistics (Rozovskaya and Roth, 2010b). The classifier is then applied to non-native text to predict the correct article/preposition in context. The possible candidate sel</context>
<context position="9072" citStr="Han et al. (2006)" startWordPosition="1445" endWordPosition="1448"> {sight, site, cite} for contextsensitive spelling correction, {among, between} for word selection, or a set of prepositions for the preposition correction problem. Each occurrence of a candidate word in text is represented as a vector of features. A classifier is trained on a large corpus of error-free text. Given text to correct, for each word in text that belongs to the confusion set the classifier is used to predict the most likely candidate in the confusion set given the word’s context. In the same spirit, models for correcting ESL errors are generally trained on well-formed native text. Han et al. (2006) train a maximum entropy model to correct article mistakes. Chodorow et. al (2007), Tetreault and Chodorow (2008), and De Felice and Pulman (2008) train a maximum entropy model and De Felice and Pulman (2007) train a voted perceptron algorithm to correct preposition errors. Gamon et al. (2008) train a decision tree model and a language model to correct errors in article and preposition usage. Bergsma et al. (2009) propose a Naive Bayes algorithm with web-scale N-grams as features, for preposition selection and context-sensitive spelling correction. The set of valid candidate corrections for a </context>
</contexts>
<marker>Han, Chodorow, Leacock, 2006</marker>
<rawString>N. Han, M. Chodorow, and C. Leacock. 2006. Detecting errors in English article usage by non-native speakers. Journal of Natural Language Engineering, 12(2):115– 129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Han</author>
<author>J Tetreault</author>
<author>S Lee</author>
<author>J Ha</author>
</authors>
<title>Using an error-annotated learner corpus to develop and ESL/EFL error correction system.</title>
<date>2010</date>
<booktitle>In LREC,</booktitle>
<location>Malta,</location>
<contexts>
<context position="3731" citStr="Han et al., 2010" startWordPosition="583" endWordPosition="586">pted the methods of the context-sensitive spelling correction task. A system is usually trained on wellformed native English text (Izumi et al., 2003; EegOlofsson and Knuttson, 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault 961 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 961–970, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics and Chodorow, 2008; Elghaari et al., 2010; Tetreault et al., 2010), but several works incorporate into training error-tagged data (Gamon, 2010; Han et al., 2010) or error statistics (Rozovskaya and Roth, 2010b). The classifier is then applied to non-native text to predict the correct article/preposition in context. The possible candidate selections include the set of all articles or all prepositions. While in the article correction task the candidate set is small (a, the, no article), systems for correcting preposition errors, even when they consider the most common prepositions, may include between 9 to 34 preposition classes. For each preposition in the non-native text, every other candidate in the confusion set is viewed as a potential correction. </context>
<context position="10819" citStr="Han et al. (2010)" startWordPosition="1730" endWordPosition="1733">ng Error-tagged Data in Training Several recent works explore ways of using annotated non-native text when training error correction models. One way to incorporate knowledge about which confusions are likely with ESL learners into the error correction system is to train a model on error-tagged data. Preposition confusions observed in the nonnative text can then be included in training, by using the preposition chosen by the author (the source preposition) as a feature. This is not possible with a system trained on native data, because each source preposition is always the correct preposition. Han et al. (2010) train a model on partially annotated Korean learner data. The error-tagged model trained on one million prepositions obtains a slightly higher recall and a significant improvement in precision (from 0.484 to 0.817) over a model fives times larger trained on well-formed text. Gamon (2010) proposes a hybrid system for preposition and article correction, by incorporating the scores of a language model and class probabilities of a maximum entropy model, both trained on native data, into a meta-classifier that is trained on a smaller amount of annotated ESL data. The metaclassifier outperforms by </context>
<context position="16071" citStr="Han et al. (2010)" startWordPosition="2593" endWordPosition="2596">ive data, or the proportion of prepositions used correctly. Using the error rate numbers shown in Table 1, the baseline for Chinese speakers is thus 84.9%, and for all the data combined it is 91.6%. 3.3 Preposition Errors and L1 We focus on preposition confusion errors, mistakes that involve an incorrectly selected preposition4. We consider ten most frequent prepositions in English: on, from, for, of, about, to, at, in, with, and by5. We mentioned in Section 2 that not all preposition confusions are equally likely to occur and preposition errors may depend on the first language of the writer. Han et al. (2010) show that preposition errors in the annotated corpus by Korean learners are not evenly distributed, some confusions occurring more often than others. We also observe that confusion frequencies differ by L1. This is consistent with other studies, which show that learners’ errors are influenced by their first language (Lee and Seneff, 2008; Leacock et al., 2010). 3It is argued in Rozovskaya and Roth (2010b) that the most frequent class baselines are not relevant for error correction tasks. Instead, the error rate in the data need to be considered, when determining the baseline. 4We do not addre</context>
<context position="34002" citStr="Han et al. (2010)" startWordPosition="5379" endWordPosition="5382">ke the other models, this system does not tend to propose too many false alarms. 6.1 Comparison to Other Systems It is difficult to compare performance to other systems, since training and evaluation are not performed on the same data, and results may vary widely depending on the first language and proficiency level of the writer. However, in Table 6 we list several systems and their performance on the task. Tetreault et al. (2010) train on native data and obtain a precision of 48.6% and a recall of 22.5% with top 34 prepositions on essays from the Test of English as a Foreign Language exams. Han et al. (2010) obtain a precision of 81.7% and a recall of 13.2% using a model trained on partially errortagged data by Korean speakers on top ten prepositions. A model trained on 2 million examples from clean text achieved on the same data set a precision of 46.3% and a recall of 11.6%. Gamon (2010) shows precision/recall curves on the combined task of detecting missing, extraneous and confused prepositions. For recall points 10% and 20%, precisions of 55% and 40%, respectively, are obtained. For our data, a recall of 10% corresponds to a precision of 46% for the worstperforming model and 78% for the best-</context>
<context position="35613" citStr="Han et al., 2010" startWordPosition="5641" endWordPosition="5644"> the latter system indeed provides a stable advantage over the baseline unrestricted approach. 7 Conclusion and Future Work In this paper, we proposed methods for improving candidate sets for the task of detecting and correcting errors in text. To correct errors in preposition usage made by non-native speakers of English, we proposed L1-dependent confusion sets that determine valid candidate corrections using knowledge about preposition confusions observed in the nonnative text. We found that restricting candidates to System Training Data P R Tetreault et al., 2010 native; 34 preps. 48.6 22.5 Han et al., 2010 partially error-tagged; 81.7 13.2 10 preps. Han et al., 2010 native; 10 preps. 46.3 11.6 Gamon, 2010 native; 12 preps.+ 33.0 10.0 extraneous+missing Gamon, 2010 native+error-tagged; 55.0 10.0 12 preps.+ extraneous+missing NegAll-Clean-ThreshAll native; 10 preps. 46.0 10.0 NegAll-ErrorL1-ThreshL1 native with 78.0 10.0 L1 error statistics; 10 preps. Table 6: Comparison to other systems. Please note that a direct comparison is not possible, since the systems are trained and evaluated on different data sets. Gamon (2010) also considers missing and extraneous preposition errors. those that are obs</context>
</contexts>
<marker>Han, Tetreault, Lee, Ha, 2010</marker>
<rawString>N. Han, J. Tetreault, S. Lee, and J. Ha. 2010. Using an error-annotated learner corpus to develop and ESL/EFL error correction system. In LREC, Malta, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Izumi</author>
<author>K Uchimoto</author>
<author>T Saiga</author>
<author>T Supnithi</author>
<author>H Isahara</author>
</authors>
<title>Automatic error detection in the Japanese learners’ English spoken data.</title>
<date>2003</date>
<booktitle>In The Companion Volume to the Proceedings of 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>145--148</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="3263" citStr="Izumi et al., 2003" startWordPosition="513" endWordPosition="516">he confusion set. More recently, work in error correction has taken an interesting turn and focused on correcting errors made by English as a Second Language (ESL) learners, with a special interest given to errors in article and preposition usage. These mistakes are some of the most common mistakes for non-native English speakers of all proficiency levels (Dalgish, 1985; Bitchener et al., 2005; Leacock et al., 2010). Approaches to correcting these mistakes have adopted the methods of the context-sensitive spelling correction task. A system is usually trained on wellformed native English text (Izumi et al., 2003; EegOlofsson and Knuttson, 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault 961 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 961–970, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics and Chodorow, 2008; Elghaari et al., 2010; Tetreault et al., 2010), but several works incorporate into training error-tagged data (Gamon, 2010; Han et al., 2010) or error statistics (Rozovskaya and Roth, 2010b). The classifier is then applied to non-native text to predict the correct article/</context>
<context position="13265" citStr="Izumi et al., 2003" startWordPosition="2134" endWordPosition="2137">ts because the set of prepositions participating in the task is a lot bigger than the set of article choices. 963 3 ESL Data 3.1 Preposition Errors in Learner Data Preposition errors are one of the most common mistakes that non-native speakers make. In the Cambridge Learner Corpus2 (CLC), which contains data by learners of different first language backgrounds and different proficiency levels, preposition errors account for about 13.5% of all errors and occur on average in 10% of all sentences (Leacock et al., 2010). Similar error rates have been reported for other annotated ESL corpora, e.g. (Izumi et al., 2003; Rozovskaya and Roth, 2010a; Tetreault et al., 2010). Learning correct preposition usage in English is challenging for learners of all first language backgrounds (Dalgish, 1985; Bitchener et al., 2005; Gamon, 2010; Leacock et al., 2010). 3.2 The Annotated Corpus We use data from an annotated corpus of essays written by ESL students. The essays were fully corrected and error-tagged by native English speakers. For each preposition used incorrectly by the author, the annotator also indicated the correct preposition choice. Rozovskaya and Roth (2010a) provide a detailed description of the annotat</context>
</contexts>
<marker>Izumi, Uchimoto, Saiga, Supnithi, Isahara, 2003</marker>
<rawString>E. Izumi, K. Uchimoto, T. Saiga, T. Supnithi, and H. Isahara. 2003. Automatic error detection in the Japanese learners’ English spoken data. In The Companion Volume to the Proceedings of 41st Annual Meeting of the Association for Computational Linguistics, pages 145–148, Sapporo, Japan, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Leacock</author>
<author>M Chodorow</author>
<author>M Gamon</author>
<author>J Tetreault</author>
</authors>
<date>2010</date>
<publisher>Morgan and Claypool Publishers.</publisher>
<contexts>
<context position="3064" citStr="Leacock et al., 2010" startWordPosition="481" endWordPosition="484">sitive and negative examples, respectively, from the same context. Given a text to correct, for each word in text that belongs to the confusion set the classifier predicts the most likely candidate in the confusion set. More recently, work in error correction has taken an interesting turn and focused on correcting errors made by English as a Second Language (ESL) learners, with a special interest given to errors in article and preposition usage. These mistakes are some of the most common mistakes for non-native English speakers of all proficiency levels (Dalgish, 1985; Bitchener et al., 2005; Leacock et al., 2010). Approaches to correcting these mistakes have adopted the methods of the context-sensitive spelling correction task. A system is usually trained on wellformed native English text (Izumi et al., 2003; EegOlofsson and Knuttson, 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault 961 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 961–970, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics and Chodorow, 2008; Elghaari et al., 2010; Tetreault et al., 2010), but several works incorpor</context>
<context position="13167" citStr="Leacock et al., 2010" startWordPosition="2118" endWordPosition="2121">ikely. Unlike article errors, preposition errors lend themselves very well to a study of confusion sets because the set of prepositions participating in the task is a lot bigger than the set of article choices. 963 3 ESL Data 3.1 Preposition Errors in Learner Data Preposition errors are one of the most common mistakes that non-native speakers make. In the Cambridge Learner Corpus2 (CLC), which contains data by learners of different first language backgrounds and different proficiency levels, preposition errors account for about 13.5% of all errors and occur on average in 10% of all sentences (Leacock et al., 2010). Similar error rates have been reported for other annotated ESL corpora, e.g. (Izumi et al., 2003; Rozovskaya and Roth, 2010a; Tetreault et al., 2010). Learning correct preposition usage in English is challenging for learners of all first language backgrounds (Dalgish, 1985; Bitchener et al., 2005; Gamon, 2010; Leacock et al., 2010). 3.2 The Annotated Corpus We use data from an annotated corpus of essays written by ESL students. The essays were fully corrected and error-tagged by native English speakers. For each preposition used incorrectly by the author, the annotator also indicated the cor</context>
<context position="16434" citStr="Leacock et al., 2010" startWordPosition="2652" endWordPosition="2655">t prepositions in English: on, from, for, of, about, to, at, in, with, and by5. We mentioned in Section 2 that not all preposition confusions are equally likely to occur and preposition errors may depend on the first language of the writer. Han et al. (2010) show that preposition errors in the annotated corpus by Korean learners are not evenly distributed, some confusions occurring more often than others. We also observe that confusion frequencies differ by L1. This is consistent with other studies, which show that learners’ errors are influenced by their first language (Lee and Seneff, 2008; Leacock et al., 2010). 3It is argued in Rozovskaya and Roth (2010b) that the most frequent class baselines are not relevant for error correction tasks. Instead, the error rate in the data need to be considered, when determining the baseline. 4We do not address errors of missing or extraneous prepositions. 5It is common to restrict the systems that detect errors in preposition usage to the top prepositions. In the CLC corpus, the usage of the ten most frequent prepositions accounts for 82% of all preposition errors (Leacock et al., 2010). 964 4 Methods of Improving Candidate Sets In this section, we describe method</context>
</contexts>
<marker>Leacock, Chodorow, Gamon, Tetreault, 2010</marker>
<rawString>C. Leacock, M. Chodorow, M. Gamon, and J. Tetreault. 2010. Morgan and Claypool Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lee</author>
<author>S Seneff</author>
</authors>
<title>An analysis of grammatical errors in non-native speech in English.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Spoken Language Technology Workshop.</booktitle>
<contexts>
<context position="16411" citStr="Lee and Seneff, 2008" startWordPosition="2647" endWordPosition="2651">sider ten most frequent prepositions in English: on, from, for, of, about, to, at, in, with, and by5. We mentioned in Section 2 that not all preposition confusions are equally likely to occur and preposition errors may depend on the first language of the writer. Han et al. (2010) show that preposition errors in the annotated corpus by Korean learners are not evenly distributed, some confusions occurring more often than others. We also observe that confusion frequencies differ by L1. This is consistent with other studies, which show that learners’ errors are influenced by their first language (Lee and Seneff, 2008; Leacock et al., 2010). 3It is argued in Rozovskaya and Roth (2010b) that the most frequent class baselines are not relevant for error correction tasks. Instead, the error rate in the data need to be considered, when determining the baseline. 4We do not address errors of missing or extraneous prepositions. 5It is common to restrict the systems that detect errors in preposition usage to the top prepositions. In the CLC corpus, the usage of the ten most frequent prepositions accounts for 82% of all preposition errors (Leacock et al., 2010). 964 4 Methods of Improving Candidate Sets In this sect</context>
</contexts>
<marker>Lee, Seneff, 2008</marker>
<rawString>J. Lee and S. Seneff. 2008. An analysis of grammatical errors in non-native speech in English. In Proceedings of the 2008 Spoken Language Technology Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Punyakanok</author>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>The importance of syntactic parsing and inference in semantic role labeling.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="27130" citStr="Punyakanok et al., 2008" startWordPosition="4360" endWordPosition="4364">ptron also comes with a PAC-like generalization bound (Freund and Schapire, 1999). This linear learning algorithm is known, both theoretically and experimentally, to be among the best linear learning approaches and is competitive with SVM and Logistic 8ThreshAll is not possible with this training option, as the system never proposes a correction that is not in L1ConfSet(pz). 9LBJ code is available at http://cogcomp.cs. illinois.edu/page/software Regression, while being more efficient in training. It also has been shown to produce state-of-the-art results on many natural language applications (Punyakanok et al., 2008). 6 Results and Discussion Table 4 shows performance of the four systems by the source language. For each source language, the methods that restrict candidate sets in training or testing outperform the baseline system NegAll-Clean-ThreshAll that does not restrict candidate sets. The NegAll-ErrorL1-NoThresh system performs better than the other three systems for all languages, except for Italian. In fact, for the Czech speaker data, all systems other than NegAll-ErrorL1- NoThresh, have a precision and a recall of 0, since no errors are detected10. Source System Acc. P R lang. NegAll-Clean-Thres</context>
</contexts>
<marker>Punyakanok, Roth, Yih, 2008</marker>
<rawString>V. Punyakanok, D. Roth, and W. Yih. 2008. The importance of syntactic parsing and inference in semantic role labeling. Computational Linguistics, 34(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Rizzolo</author>
<author>D Roth</author>
</authors>
<title>Modeling Discriminative Global Inference.</title>
<date>2007</date>
<booktitle>In Proceedings of the First International Conference on Semantic Computing (ICSC),</booktitle>
<pages>597--604</pages>
<publisher>IEEE.</publisher>
<location>Irvine, California,</location>
<contexts>
<context position="26395" citStr="Rizzolo and Roth, 2007" startWordPosition="4251" endWordPosition="4254">the same set of training examples. Features are extracted from a window of eight words around the preposition and include words, part-of-speech tags and conjunctions of words and tags of lengths two, three, and four. Training data are extracted from English Wikipedia and the New York Times section of the Gigaword corpus (Linguistic Data Consortium, 2003). In each training paradigm, we follow a discriminative approach, using an online learning paradigm and making use of the Averaged Perceptron Algorithm (Freund and Schapire, 1999) – we use the regularized version in Learning Based Java9 (LBJ, (Rizzolo and Roth, 2007)). While classical Perceptron comes with generalization bound related to the margin of the data, Averaged Perceptron also comes with a PAC-like generalization bound (Freund and Schapire, 1999). This linear learning algorithm is known, both theoretically and experimentally, to be among the best linear learning approaches and is competitive with SVM and Logistic 8ThreshAll is not possible with this training option, as the system never proposes a correction that is not in L1ConfSet(pz). 9LBJ code is available at http://cogcomp.cs. illinois.edu/page/software Regression, while being more efficient </context>
</contexts>
<marker>Rizzolo, Roth, 2007</marker>
<rawString>N. Rizzolo and D. Roth. 2007. Modeling Discriminative Global Inference. In Proceedings of the First International Conference on Semantic Computing (ICSC), pages 597–604, Irvine, California, September. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
</authors>
<title>Learning to resolve natural language ambiguities: A unified approach.</title>
<date>1998</date>
<booktitle>In Proceedings of the National Conference on Artificial Intelligence (AAAI),</booktitle>
<pages>806--813</pages>
<contexts>
<context position="8031" citStr="Roth (1998)" startWordPosition="1278" endWordPosition="1279">n 4 describes the methods of restricting candidate sets in training and testing. Section 5 describes the experimental setup. We present and discuss the results in Section 6. The key findings are summarized in Table 5 and Fig. 1 in Section 6. We conclude with a brief discussion of directions for future work. 2 Related Work Work in text correction has focused primarily on correcting context-sensitive spelling errors (Golding and Roth, 1999; Banko and Brill, 2001; Carlson et al., 2001; Carlson and Fette, 2007) and mistakes made by ESL learners, especially errors in article and preposition usage. Roth (1998) takes a unified approach to resolving semantic and syntactic ambiguities in natural lan962 guage by treating several related problems, including word sense disambiguation, word selection, and context-sensitive spelling correction as instances of the disambiguation task. Given a candidate set or a confusion set of confusable words, the task is to select the most likely candidate in context. Examples of confusion sets are {sight, site, cite} for contextsensitive spelling correction, {among, between} for word selection, or a set of prepositions for the preposition correction problem. Each occurr</context>
</contexts>
<marker>Roth, 1998</marker>
<rawString>D. Roth. 1998. Learning to resolve natural language ambiguities: A unified approach. In Proceedings of the National Conference on Artificial Intelligence (AAAI), pages 806–813.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rozovskaya</author>
<author>D Roth</author>
</authors>
<title>Annotating ESL errors: Challenges and rewards.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL Workshop on Innovative Use of NLPfor Building Educational Applications.</booktitle>
<contexts>
<context position="3778" citStr="Rozovskaya and Roth, 2010" startWordPosition="590" endWordPosition="593">ve spelling correction task. A system is usually trained on wellformed native English text (Izumi et al., 2003; EegOlofsson and Knuttson, 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault 961 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 961–970, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics and Chodorow, 2008; Elghaari et al., 2010; Tetreault et al., 2010), but several works incorporate into training error-tagged data (Gamon, 2010; Han et al., 2010) or error statistics (Rozovskaya and Roth, 2010b). The classifier is then applied to non-native text to predict the correct article/preposition in context. The possible candidate selections include the set of all articles or all prepositions. While in the article correction task the candidate set is small (a, the, no article), systems for correcting preposition errors, even when they consider the most common prepositions, may include between 9 to 34 preposition classes. For each preposition in the non-native text, every other candidate in the confusion set is viewed as a potential correction. This approach, however, does not take into acco</context>
<context position="6056" citStr="Rozovskaya and Roth (2010" startWordPosition="958" endWordPosition="962">We contrast this baseline method to two methods that enforce L1-dependent candidate sets in training. First, we train a separate classifier for each preposition pi on the prepositions that belong to L1- dependent candidate set of pi. In this setting, the negative examples for pi are those that belong to L1- dependent candidate set of pi. The second method of enforcing L1-dependent 1This includes the preposition pi itself. If proposed by the classifier, it would not be flagged as an error. candidate sets in training is to train on native data with artificial preposition errors in the spirit of Rozovskaya and Roth (2010b), where the errors mimic the error rates and error patterns of the non-native text. This method requires more knowledge, since it uses a distribution of errors from an error-tagged corpus. We also propose a method of enforcing L1- dependent candidate sets in testing, through the use of a confidence threshold. We consider two ways of applying a threshold: (1) the standard way, when a correction is proposed only if the classifier’s confidence is sufficiently high and (2) L1-dependent threshold, when a correction is proposed only if it belongs to L1-dependent candidate set. We show that the met</context>
<context position="11641" citStr="Rozovskaya and Roth (2010" startWordPosition="1862" endWordPosition="1865">om 0.484 to 0.817) over a model fives times larger trained on well-formed text. Gamon (2010) proposes a hybrid system for preposition and article correction, by incorporating the scores of a language model and class probabilities of a maximum entropy model, both trained on native data, into a meta-classifier that is trained on a smaller amount of annotated ESL data. The metaclassifier outperforms by a large margin both of the native models, but it requires large amounts of expensive annotated data, especially in order to correct preposition errors, where the problem complexity is much larger. Rozovskaya and Roth (2010b) show that by introducing into native training data artificial article errors it is possible to improve the performance of the article correction system, when compared to a classifier trained on native data. In contrast to Gamon (2010) and Han et al. (2010) that use annotated data for training, the system is trained on native data, but the native data are transformed to be more like L1 data through artificial article errors that mimic the error rates and error patterns of non-native writers. This method is cheaper, since obtaining error statistics requires much less annotated data than train</context>
<context position="13292" citStr="Rozovskaya and Roth, 2010" startWordPosition="2138" endWordPosition="2141">f prepositions participating in the task is a lot bigger than the set of article choices. 963 3 ESL Data 3.1 Preposition Errors in Learner Data Preposition errors are one of the most common mistakes that non-native speakers make. In the Cambridge Learner Corpus2 (CLC), which contains data by learners of different first language backgrounds and different proficiency levels, preposition errors account for about 13.5% of all errors and occur on average in 10% of all sentences (Leacock et al., 2010). Similar error rates have been reported for other annotated ESL corpora, e.g. (Izumi et al., 2003; Rozovskaya and Roth, 2010a; Tetreault et al., 2010). Learning correct preposition usage in English is challenging for learners of all first language backgrounds (Dalgish, 1985; Bitchener et al., 2005; Gamon, 2010; Leacock et al., 2010). 3.2 The Annotated Corpus We use data from an annotated corpus of essays written by ESL students. The essays were fully corrected and error-tagged by native English speakers. For each preposition used incorrectly by the author, the annotator also indicated the correct preposition choice. Rozovskaya and Roth (2010a) provide a detailed description of the annotation of the data. The annota</context>
<context position="16478" citStr="Rozovskaya and Roth (2010" startWordPosition="2660" endWordPosition="2663">, of, about, to, at, in, with, and by5. We mentioned in Section 2 that not all preposition confusions are equally likely to occur and preposition errors may depend on the first language of the writer. Han et al. (2010) show that preposition errors in the annotated corpus by Korean learners are not evenly distributed, some confusions occurring more often than others. We also observe that confusion frequencies differ by L1. This is consistent with other studies, which show that learners’ errors are influenced by their first language (Lee and Seneff, 2008; Leacock et al., 2010). 3It is argued in Rozovskaya and Roth (2010b) that the most frequent class baselines are not relevant for error correction tasks. Instead, the error rate in the data need to be considered, when determining the baseline. 4We do not address errors of missing or extraneous prepositions. 5It is common to restrict the systems that detect errors in preposition usage to the top prepositions. In the CLC corpus, the usage of the ten most frequent prepositions accounts for 82% of all preposition errors (Leacock et al., 2010). 964 4 Methods of Improving Candidate Sets In this section, we describe methods of restricting candidate sets according to</context>
<context position="21250" citStr="Rozovskaya and Roth (2010" startWordPosition="3434" endWordPosition="3437">i in test, we consult the classifier for pi. In this model, the confusion set for source pi is restricted through training, since for source pi, the possible candidate replacements are only those that the classifier sees in training, and they are all in L1ConfSet(pi). 965 Training Negative examples data NegAll NegL1 Clean NegAll-Clean NegL1-Clean ErrorL1 NegAll-ErrorL1 - Table 3: Training conditions that result in unrestricted (All) and L1-dependent training paradigms. ErrorL1 This method restricts the candidate set to L1ConfSet(pi) by generating artificial preposition errors in the spirit of Rozovskaya and Roth (2010b). The training data are thus no longer well-formed or clean, but augmented with L1 error statistics. Specifically, each preposition pi in training is replaced with a different preposition pj with probability probConf, s.t. probConf = prob(piIpj) (1) Suppose 10% of all source prepositions to in the Russian speaker data correspond to label for. Then for is replaced with to with probability 0.1. The classifier uses in training the source preposition as a feature, which cannot be done when training on well-formed text, as discussed in Section 2.1. By providing the source preposition as a feature</context>
</contexts>
<marker>Rozovskaya, Roth, 2010</marker>
<rawString>A. Rozovskaya and D. Roth. 2010a. Annotating ESL errors: Challenges and rewards. In Proceedings of the NAACL Workshop on Innovative Use of NLPfor Building Educational Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rozovskaya</author>
<author>D Roth</author>
</authors>
<title>Training paradigms for correcting errors in grammar and usage.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL-HLT.</booktitle>
<contexts>
<context position="3778" citStr="Rozovskaya and Roth, 2010" startWordPosition="590" endWordPosition="593">ve spelling correction task. A system is usually trained on wellformed native English text (Izumi et al., 2003; EegOlofsson and Knuttson, 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault 961 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 961–970, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics and Chodorow, 2008; Elghaari et al., 2010; Tetreault et al., 2010), but several works incorporate into training error-tagged data (Gamon, 2010; Han et al., 2010) or error statistics (Rozovskaya and Roth, 2010b). The classifier is then applied to non-native text to predict the correct article/preposition in context. The possible candidate selections include the set of all articles or all prepositions. While in the article correction task the candidate set is small (a, the, no article), systems for correcting preposition errors, even when they consider the most common prepositions, may include between 9 to 34 preposition classes. For each preposition in the non-native text, every other candidate in the confusion set is viewed as a potential correction. This approach, however, does not take into acco</context>
<context position="6056" citStr="Rozovskaya and Roth (2010" startWordPosition="958" endWordPosition="962">We contrast this baseline method to two methods that enforce L1-dependent candidate sets in training. First, we train a separate classifier for each preposition pi on the prepositions that belong to L1- dependent candidate set of pi. In this setting, the negative examples for pi are those that belong to L1- dependent candidate set of pi. The second method of enforcing L1-dependent 1This includes the preposition pi itself. If proposed by the classifier, it would not be flagged as an error. candidate sets in training is to train on native data with artificial preposition errors in the spirit of Rozovskaya and Roth (2010b), where the errors mimic the error rates and error patterns of the non-native text. This method requires more knowledge, since it uses a distribution of errors from an error-tagged corpus. We also propose a method of enforcing L1- dependent candidate sets in testing, through the use of a confidence threshold. We consider two ways of applying a threshold: (1) the standard way, when a correction is proposed only if the classifier’s confidence is sufficiently high and (2) L1-dependent threshold, when a correction is proposed only if it belongs to L1-dependent candidate set. We show that the met</context>
<context position="11641" citStr="Rozovskaya and Roth (2010" startWordPosition="1862" endWordPosition="1865">om 0.484 to 0.817) over a model fives times larger trained on well-formed text. Gamon (2010) proposes a hybrid system for preposition and article correction, by incorporating the scores of a language model and class probabilities of a maximum entropy model, both trained on native data, into a meta-classifier that is trained on a smaller amount of annotated ESL data. The metaclassifier outperforms by a large margin both of the native models, but it requires large amounts of expensive annotated data, especially in order to correct preposition errors, where the problem complexity is much larger. Rozovskaya and Roth (2010b) show that by introducing into native training data artificial article errors it is possible to improve the performance of the article correction system, when compared to a classifier trained on native data. In contrast to Gamon (2010) and Han et al. (2010) that use annotated data for training, the system is trained on native data, but the native data are transformed to be more like L1 data through artificial article errors that mimic the error rates and error patterns of non-native writers. This method is cheaper, since obtaining error statistics requires much less annotated data than train</context>
<context position="13292" citStr="Rozovskaya and Roth, 2010" startWordPosition="2138" endWordPosition="2141">f prepositions participating in the task is a lot bigger than the set of article choices. 963 3 ESL Data 3.1 Preposition Errors in Learner Data Preposition errors are one of the most common mistakes that non-native speakers make. In the Cambridge Learner Corpus2 (CLC), which contains data by learners of different first language backgrounds and different proficiency levels, preposition errors account for about 13.5% of all errors and occur on average in 10% of all sentences (Leacock et al., 2010). Similar error rates have been reported for other annotated ESL corpora, e.g. (Izumi et al., 2003; Rozovskaya and Roth, 2010a; Tetreault et al., 2010). Learning correct preposition usage in English is challenging for learners of all first language backgrounds (Dalgish, 1985; Bitchener et al., 2005; Gamon, 2010; Leacock et al., 2010). 3.2 The Annotated Corpus We use data from an annotated corpus of essays written by ESL students. The essays were fully corrected and error-tagged by native English speakers. For each preposition used incorrectly by the author, the annotator also indicated the correct preposition choice. Rozovskaya and Roth (2010a) provide a detailed description of the annotation of the data. The annota</context>
<context position="16478" citStr="Rozovskaya and Roth (2010" startWordPosition="2660" endWordPosition="2663">, of, about, to, at, in, with, and by5. We mentioned in Section 2 that not all preposition confusions are equally likely to occur and preposition errors may depend on the first language of the writer. Han et al. (2010) show that preposition errors in the annotated corpus by Korean learners are not evenly distributed, some confusions occurring more often than others. We also observe that confusion frequencies differ by L1. This is consistent with other studies, which show that learners’ errors are influenced by their first language (Lee and Seneff, 2008; Leacock et al., 2010). 3It is argued in Rozovskaya and Roth (2010b) that the most frequent class baselines are not relevant for error correction tasks. Instead, the error rate in the data need to be considered, when determining the baseline. 4We do not address errors of missing or extraneous prepositions. 5It is common to restrict the systems that detect errors in preposition usage to the top prepositions. In the CLC corpus, the usage of the ten most frequent prepositions accounts for 82% of all preposition errors (Leacock et al., 2010). 964 4 Methods of Improving Candidate Sets In this section, we describe methods of restricting candidate sets according to</context>
<context position="21250" citStr="Rozovskaya and Roth (2010" startWordPosition="3434" endWordPosition="3437">i in test, we consult the classifier for pi. In this model, the confusion set for source pi is restricted through training, since for source pi, the possible candidate replacements are only those that the classifier sees in training, and they are all in L1ConfSet(pi). 965 Training Negative examples data NegAll NegL1 Clean NegAll-Clean NegL1-Clean ErrorL1 NegAll-ErrorL1 - Table 3: Training conditions that result in unrestricted (All) and L1-dependent training paradigms. ErrorL1 This method restricts the candidate set to L1ConfSet(pi) by generating artificial preposition errors in the spirit of Rozovskaya and Roth (2010b). The training data are thus no longer well-formed or clean, but augmented with L1 error statistics. Specifically, each preposition pi in training is replaced with a different preposition pj with probability probConf, s.t. probConf = prob(piIpj) (1) Suppose 10% of all source prepositions to in the Russian speaker data correspond to label for. Then for is replaced with to with probability 0.1. The classifier uses in training the source preposition as a feature, which cannot be done when training on well-formed text, as discussed in Section 2.1. By providing the source preposition as a feature</context>
</contexts>
<marker>Rozovskaya, Roth, 2010</marker>
<rawString>A. Rozovskaya and D. Roth. 2010b. Training paradigms for correcting errors in grammar and usage. In Proceedings of the NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Tetreault</author>
<author>M Chodorow</author>
</authors>
<title>The ups and downs of preposition error detection in ESL writing.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<pages>865--872</pages>
<location>Manchester, UK,</location>
<contexts>
<context position="9185" citStr="Tetreault and Chodorow (2008)" startWordPosition="1462" endWordPosition="1465">r a set of prepositions for the preposition correction problem. Each occurrence of a candidate word in text is represented as a vector of features. A classifier is trained on a large corpus of error-free text. Given text to correct, for each word in text that belongs to the confusion set the classifier is used to predict the most likely candidate in the confusion set given the word’s context. In the same spirit, models for correcting ESL errors are generally trained on well-formed native text. Han et al. (2006) train a maximum entropy model to correct article mistakes. Chodorow et. al (2007), Tetreault and Chodorow (2008), and De Felice and Pulman (2008) train a maximum entropy model and De Felice and Pulman (2007) train a voted perceptron algorithm to correct preposition errors. Gamon et al. (2008) train a decision tree model and a language model to correct errors in article and preposition usage. Bergsma et al. (2009) propose a Naive Bayes algorithm with web-scale N-grams as features, for preposition selection and context-sensitive spelling correction. The set of valid candidate corrections for a target word includes all words in the confusion set. For the preposition correction task, the entire set of prepo</context>
</contexts>
<marker>Tetreault, Chodorow, 2008</marker>
<rawString>J. Tetreault and M. Chodorow. 2008. The ups and downs of preposition error detection in ESL writing. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 865–872, Manchester, UK, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Tetreault</author>
<author>J Foster</author>
<author>M Chodorow</author>
</authors>
<title>Using parse features for preposition selection and error detection.</title>
<date>2010</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="3636" citStr="Tetreault et al., 2010" startWordPosition="569" endWordPosition="572">1985; Bitchener et al., 2005; Leacock et al., 2010). Approaches to correcting these mistakes have adopted the methods of the context-sensitive spelling correction task. A system is usually trained on wellformed native English text (Izumi et al., 2003; EegOlofsson and Knuttson, 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault 961 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 961–970, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics and Chodorow, 2008; Elghaari et al., 2010; Tetreault et al., 2010), but several works incorporate into training error-tagged data (Gamon, 2010; Han et al., 2010) or error statistics (Rozovskaya and Roth, 2010b). The classifier is then applied to non-native text to predict the correct article/preposition in context. The possible candidate selections include the set of all articles or all prepositions. While in the article correction task the candidate set is small (a, the, no article), systems for correcting preposition errors, even when they consider the most common prepositions, may include between 9 to 34 preposition classes. For each preposition in the no</context>
<context position="10149" citStr="Tetreault et al., 2010" startWordPosition="1624" endWordPosition="1627">web-scale N-grams as features, for preposition selection and context-sensitive spelling correction. The set of valid candidate corrections for a target word includes all words in the confusion set. For the preposition correction task, the entire set of prepositions considered for the task is viewed as the set of possible corrections for each preposition in nonnative text. Given a preposition with its surrounding context, the model selects the most likely preposition from the set of all candidates, where the set of candidates consists of nine (Felice and Pulman, 2008), 12 (Gamon, 2010), or 34 (Tetreault et al., 2010; Tetreault and Chodorow, 2008) prepositions. 2.1 Using Error-tagged Data in Training Several recent works explore ways of using annotated non-native text when training error correction models. One way to incorporate knowledge about which confusions are likely with ESL learners into the error correction system is to train a model on error-tagged data. Preposition confusions observed in the nonnative text can then be included in training, by using the preposition chosen by the author (the source preposition) as a feature. This is not possible with a system trained on native data, because each s</context>
<context position="13318" citStr="Tetreault et al., 2010" startWordPosition="2142" endWordPosition="2145"> in the task is a lot bigger than the set of article choices. 963 3 ESL Data 3.1 Preposition Errors in Learner Data Preposition errors are one of the most common mistakes that non-native speakers make. In the Cambridge Learner Corpus2 (CLC), which contains data by learners of different first language backgrounds and different proficiency levels, preposition errors account for about 13.5% of all errors and occur on average in 10% of all sentences (Leacock et al., 2010). Similar error rates have been reported for other annotated ESL corpora, e.g. (Izumi et al., 2003; Rozovskaya and Roth, 2010a; Tetreault et al., 2010). Learning correct preposition usage in English is challenging for learners of all first language backgrounds (Dalgish, 1985; Bitchener et al., 2005; Gamon, 2010; Leacock et al., 2010). 3.2 The Annotated Corpus We use data from an annotated corpus of essays written by ESL students. The essays were fully corrected and error-tagged by native English speakers. For each preposition used incorrectly by the author, the annotator also indicated the correct preposition choice. Rozovskaya and Roth (2010a) provide a detailed description of the annotation of the data. The annotated data include sentences</context>
<context position="33820" citStr="Tetreault et al. (2010)" startWordPosition="5344" endWordPosition="5347"> NegAll-ErrorL1-ThreshL1. In fact, NegAll-ErrorL1-ThreshL1 achieves a higher precision compared to the other systems, even when no threshold is used (Tables 4 and 5). This is because, unlike the other models, this system does not tend to propose too many false alarms. 6.1 Comparison to Other Systems It is difficult to compare performance to other systems, since training and evaluation are not performed on the same data, and results may vary widely depending on the first language and proficiency level of the writer. However, in Table 6 we list several systems and their performance on the task. Tetreault et al. (2010) train on native data and obtain a precision of 48.6% and a recall of 22.5% with top 34 prepositions on essays from the Test of English as a Foreign Language exams. Han et al. (2010) obtain a precision of 81.7% and a recall of 13.2% using a model trained on partially errortagged data by Korean speakers on top ten prepositions. A model trained on 2 million examples from clean text achieved on the same data set a precision of 46.3% and a recall of 11.6%. Gamon (2010) shows precision/recall curves on the combined task of detecting missing, extraneous and confused prepositions. For recall points 1</context>
<context position="35568" citStr="Tetreault et al., 2010" startWordPosition="5632" endWordPosition="5635">a modest test set size, the curves demonstrate that the latter system indeed provides a stable advantage over the baseline unrestricted approach. 7 Conclusion and Future Work In this paper, we proposed methods for improving candidate sets for the task of detecting and correcting errors in text. To correct errors in preposition usage made by non-native speakers of English, we proposed L1-dependent confusion sets that determine valid candidate corrections using knowledge about preposition confusions observed in the nonnative text. We found that restricting candidates to System Training Data P R Tetreault et al., 2010 native; 34 preps. 48.6 22.5 Han et al., 2010 partially error-tagged; 81.7 13.2 10 preps. Han et al., 2010 native; 10 preps. 46.3 11.6 Gamon, 2010 native; 12 preps.+ 33.0 10.0 extraneous+missing Gamon, 2010 native+error-tagged; 55.0 10.0 12 preps.+ extraneous+missing NegAll-Clean-ThreshAll native; 10 preps. 46.0 10.0 NegAll-ErrorL1-ThreshL1 native with 78.0 10.0 L1 error statistics; 10 preps. Table 6: Comparison to other systems. Please note that a direct comparison is not possible, since the systems are trained and evaluated on different data sets. Gamon (2010) also considers missing and extr</context>
</contexts>
<marker>Tetreault, Foster, Chodorow, 2010</marker>
<rawString>J. Tetreault, J. Foster, and M. Chodorow. 2010. Using parse features for preposition selection and error detection. In ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>