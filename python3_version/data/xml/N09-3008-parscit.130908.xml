<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000958">
<title confidence="0.986909">
Multiple Word Alignment with Profile Hidden Markov Models
</title>
<author confidence="0.980901">
Aditya Bhargava and Grzegorz Kondrak
</author>
<affiliation confidence="0.9988625">
Department of Computing Science
University of Alberta
</affiliation>
<address confidence="0.594972">
Edmonton, Alberta, Canada, T6G 2E8
</address>
<email confidence="0.999057">
{abhargava,kondrak}@cs.ualberta.ca
</email>
<sectionHeader confidence="0.997376" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998693714285714">
Profile hidden Markov models (Profile
HMMs) are specific types of hidden Markov
models used in biological sequence analysis.
We propose the use of Profile HMMs for
word-related tasks. We test their applicability
to the tasks of multiple cognate alignment and
cognate set matching, and find that they work
well in general for both tasks. On the latter
task, the Profile HMM method outperforms
average and minimum edit distance. Given
the success for these two tasks, we further
discuss the potential applications of Profile
HMMs to any task where consideration of a
set of words is necessary.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999963210526316">
In linguistics, it is often necessary to align words or
phonetic sequences. Covington (1996) uses align-
ments of cognate pairs for the historical linguis-
tics task of comparative reconstruction and Ner-
bonne and Heeringa (1997) use alignments to com-
pute relative distances between words from various
Dutch dialects. Algorithms for aligning pairs of
words have been proposed by Covington (1996) and
Kondrak (2000). However, it is often necessary to
align multiple words. Covington (1998) proposed
a method to align multiple words based on a hand-
crafted scale of similarity between various classes
of phonemes, again for the purpose of comparative
reconstruction of languages.
Profile hidden Markov models (Profile HMMs)
are specific types of hidden Markov models used
in biological sequence analysis, where they have
yielded success for the matching of given sequences
to sequence families as well as to multiple sequence
</bodyText>
<page confidence="0.996652">
43
</page>
<bodyText confidence="0.999850814814815">
alignment (Durbin et al., 1998). In this paper, we
show that Profile HMMs can be adapted to the task
of aligning multiple words. We apply them to sets
of multilingual cognates and show that they pro-
duce good alignments. We also use them for the re-
lated task of matching words to established cognate
sets, useful for a situation where it is not immedi-
ately obvious to which cognate set a word should be
matched. The accuracy on the latter task exceeds the
accuracy of a method based on edit distance.
Profile HMMs could also potentially be used for
the computation of word similarity when a word
must be compared not to another word but to an-
other set of words, taking into account properties
of all constituent words. The use of Profile HMMs
for multiple sequence alignment also presents ap-
plications to the acquisition of mapping dictionaries
(Barzilay and Lee, 2002) and sentence-level para-
phrasing (Barzilay and Lee, 2003).
This paper is organized as follows: we first de-
scribe the uses of Profile HMMs in computational
biology, their structure, and then discuss their appli-
cations to word-related tasks. We then discuss our
data set and describe the tasks that we test and their
experimental setups and results. We conclude with
a summary of the results and a brief discussion of
potential future work.
</bodyText>
<sectionHeader confidence="0.949796" genericHeader="introduction">
2 Profile hidden Markov models
</sectionHeader>
<bodyText confidence="0.999916833333333">
In computational biology, it is often necessary to
deal with multiple sequences, including DNA and
protein sequences. For such biological sequence
analysis, Profile HMMs are applied to the common
tasks of simultaneously aligning multiple related se-
quences to each other, aligning a new sequence to
</bodyText>
<note confidence="0.9122465">
Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 43–48,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<figureCaption confidence="0.878792">
Figure 1: A prototypical Profile HMM of length L. Mi is
the ith match state, Ii is the ith insert state, and Di is the
ith delete state. Delete states are silent and are used to
indicate gaps in a sequence.
</figureCaption>
<bodyText confidence="0.999911303030303">
an already-aligned family of sequences, and evalu-
ating a new sequence for membership in a family of
sequences.
Profile HMMs consist of several types of states:
match states, insert states, delete states, as well as
a begin and end state. For each position in a Pro-
file HMM, there is one match state, one insert state,
and one delete state. A Profile HMM can thus be vi-
sualized as a series of columns, where each column
represents a position in the sequence (see Figure 1).
Any arbitrary sequence can then be represented as a
traversal of states from column to column.
Match states form the core of the model; each
match state is represented by a set of emission prob-
abilities for each symbol in the output alphabet.
These probabilities indicate the distribution of val-
ues for a given position in a sequence. Each match
state can probabilistically transition to the next (i.e.
next-column) match and delete states as well as the
current (i.e. current-column) insert state.
Insert states represent possible values that can be
inserted at a given position in a sequence (before a
match emission or deletion). They are represented
in the same manner as match states, with each out-
put symbol having an associated probability. Insert
states are used to account for symbols that have been
inserted to a given position that might not other-
wise have occurred “naturally” via a match state. In-
sert states can probabilistically transition to the next
match and delete states as well as the current insert
state (i.e. itself). Allowing insert states to transition
to themselves enables the consideration of multiple-
symbol inserts.
</bodyText>
<figure confidence="0.956741333333333">
MMIIIM
AG...C
A-AG.C
AG.AA-
--AAAC
AG...C
</figure>
<figureCaption confidence="0.9923375">
Figure 2: A small DNA multiple alignment from (Durbin
et al., 1998, p. 123).
</figureCaption>
<bodyText confidence="0.999971264705882">
Similarly, delete states represent symbols that
have been removed from a given position. For a se-
quence to use a delete state for a given position indi-
cates that a given character position in the model has
no corresponding characters in the given sequence.
Hence, delete states are by nature silent and thus
have no emission probabilities for the output sym-
bols. This is an important distinction from match
states and insert states. Each delete state can prob-
abilistically transition to the next match and delete
states as well as the current insert state.
Figure 2 shows a small example of a set of DNA
sequences. The match columns and insert columns
are marked with the letters M and I respectively in
the first line. Where a word has a character in a
match column, it is a match state emission; when
there is instead a gap, it is a delete state occur-
rence. Any characters in insert columns are insert
state emissions, and gaps in insert columns repre-
sent simply that the particular insert state was not
used for the sequence in question.
Durbin et al. (1998) describe the uses of Pro-
file HMMs for tasks in biological sequence analy-
sis. Firstly, a Profile HMM must be constructed. If
a Profile HMM is to be constructed from a set of
aligned sequences, it is necessary to designate cer-
tain columns as match columns and others as insert
column. The simple heuristic that we adopt is to
label those columns match states for which half or
more of the sequences have a symbol present (rather
than a gap). Other columns are labelled insert states.
Then the probability akl of state k transitioning to
state l can be estimated by counting the number of
times Akl that the transition is used in the alignment:
</bodyText>
<equation confidence="0.432115">
akl = El&apos; Akl&apos;
</equation>
<bodyText confidence="0.9882975">
Similarly, the probability ek(a) of state k emitting
symbol a is estimated by counting the number of
</bodyText>
<figure confidence="0.9984008">
Begin
I0
M1
D1
I1
ML
DL
IL
End
Akl
</figure>
<page confidence="0.992592">
44
</page>
<bodyText confidence="0.955777">
times Ek(a) that the emission is used in the align-
ment:
</bodyText>
<equation confidence="0.995713">
Ek(a)
ek(a) =
Eal Ek(a&apos;)
</equation>
<bodyText confidence="0.99996794117647">
There is the danger that some probabilities may be
set to zero, so it is essential to add pseudocounts.
The pseudocount methods that we explore are de-
scribed in section 3.
If a Profile HMM is to be constructed from a set
of unaligned sequences, an initial model is gener-
ated after which it can be trained to the sequences
using the Baum-Welch algorithm. The length of the
model must be chosen, and is usually set to the av-
erage length of the unaligned sequences. To gener-
ate the initial model, which amounts to setting the
transition and emission probabilities to some initial
values, the probabilities are sampled from Dirichlet
distributions.
Once a Profile HMM has been constructed, it can
be used to evaluate a given sequence for member-
ship in the family. This is done via a straightforward
application of the forward algorithm (to get the full
probability of the given sequence) or the Viterbi al-
gorithm (to get the alignment of the sequence to the
family). For the alignment of multiple unaligned se-
quences, a Profile HMM is constructed and trained
as described above and then each sequence can be
aligned using the Viterbi algorithm.
It should also be noted that Profile HMMs are
generalizations of Pair HMMs, which have been
used for cognate identification and word similar-
ity (Mackay and Kondrak, 2005) between pairs of
words. Unlike Pair HMMs, Profile HMMs are
position-specific; this is what allows their applica-
tion to multiple sequences but also means that each
Profile HMM must be trained to a given set of se-
quences, whereas Pair HMMs can be trained over a
very large data set of pairs of words.
</bodyText>
<sectionHeader confidence="0.9853" genericHeader="method">
3 Adapting Profile HMMs to words
</sectionHeader>
<bodyText confidence="0.976876452380952">
Using Profile HMMs for biological sequences in-
volves defining an alphabet and working with related
sequences consisting of symbols from that alphabet.
One could perform tasks with cognates sets in a sim-
ilar manner; cognates are, after all, related words,
and words are nothing more than sequences of sym-
bols from an alphabet. Thus Profile HMMs present
potential applications to similar tasks for cognate
sets. We apply Profile HMMs to the multiple align-
ment of cognate sets, which is done in the same
manner as multiple sequence alignment for biolog-
ical sequences described above. We also test Pro-
file HMMs for determining the correct cognate set
to which a word belongs when given a variety of
cognate sets for the same meaning; this is done in a
similar manner to the sequence membership evalua-
tion task described above.
Although there are a number of Profile HMM
packages available (e.g. HMMER), we decided to
develop an implementation from scratch in order to
achieve greater control over various adjustable pa-
rameters.1 We investigated the following parame-
ters:
Favouring match states When constructing a Pro-
file HMM from unaligned sequences, the
choice of initial model probabilities can have a
significant effect on results. It may be sensible
to favour match states compared to other states
when constructing the initial model; since the
transition probabilities are sampled from a
Dirichlet distribution, the option of favouring
match states assigns the largest returned proba-
bility to the transition to a match state.
Pseudocount method We implemented three pseu-
docount methods from (Durbin et al., 1998). In
the following equations, ej(a) is the probability
of state j emitting character a. cja represents
the observed counts of state j emitting symbol
a. A is the weight given to the pseudocounts.
Constant value A constant value AC is added
to each count. This is a generalization of
Laplace’s rule, where C = 1A.
</bodyText>
<equation confidence="0.936065333333333">
ej(a) = Ea, cja&apos; + A
cja + AC
Background frequency Pseudocounts are
</equation>
<bodyText confidence="0.986727">
added in proportion to the background
frequency qa, which is the frequency of
occurrence of character a.
</bodyText>
<equation confidence="0.9054875">
cja + Aqa
ej(a) = Eal cja&apos; + A
</equation>
<footnote confidence="0.992096">
1Our implementation is available online at http://www.
cs.ualberta.ca/˜ab31/profilehmm.
</footnote>
<page confidence="0.998715">
45
</page>
<bodyText confidence="0.9997006">
Substitution matrix (Durbin et al., 1998)
Given a matrix s(a, b) that gives the log-
odds similarity of characters a and b, we
can determine the conditional probability
of a character b given character a:
</bodyText>
<equation confidence="0.997043">
P(bla) = qbes(a,b)
</equation>
<bodyText confidence="0.949141333333333">
Then we define fja to be the probability
derived from the counts:
Then the pseudocount values are set to:
</bodyText>
<equation confidence="0.6129545">
�αja = A fjbP(alb)
b
</equation>
<bodyText confidence="0.966296">
Finally, the pseudocount values are added
to the real counts as above:
</bodyText>
<equation confidence="0.9960175">
ej(a) = Ear cjar + αjar
cja + αja
</equation>
<bodyText confidence="0.999903181818182">
Pseudocount weight The weight that the pseudo-
counts are given (A in the above equations).
Smoothing during Baum-Welch The problem has
many local optima and it is therefore easy for
the Baum-Welch algorithm to get stuck around
one of these. In order to avoid local optima,
we tested the option of adding pseudocounts
during Baum-Welch (i.e. between iterations)
rather than after it. This serves as a form
of noise injection, effectively bumping Baum-
Welch away from local optima.
</bodyText>
<sectionHeader confidence="0.999413" genericHeader="method">
4 Data for experiments
</sectionHeader>
<bodyText confidence="0.999720153846154">
Our data come from the Comparative Indoeuropean
Data Corpus (Dyen et al., 1992). The data consist
of words in 95 languages in the Indoeuropean fam-
ily organized into word lists corresponding to one
of 200 meanings. Each word is represented in the
English alphabet. Figure 3 shows a sample from
the original corpus data. We manually converted the
data into disjoint sets of cognate words, where each
cognate set contains only one word from each lan-
guage. We also removed words that were not cog-
nate with any other words.
On average, there were 4.37 words per cognate
set. The smallest cognate set had two words (since
</bodyText>
<figure confidence="0.705365333333333">
a 026 DAY 003
...
b
</figure>
<table confidence="0.982214190476191">
026 53 Bulgarian DEN
026 47 Czech E DENY
026 45 Czech DEN
026 43 Lusatian L ZEN
026 44 Lusatian U DZEN
026 50 Polish DZIEN
026 51 Russian DEN
026 54 Serbocroatian DAN
026 42 Slovenian DAN
026 41 Latvian DIENA
026 05 Breton List DEIZ, DE(Z)
026 04 Welsh C DYDD
026 20 Spanish DIA
026 17 Sardinian N DIE
026 11 Ladin DI
026 08 Rumanian List ZI
026 09 Vlach ZUE
026 15 French Creole C ZU
026 13 French JOUR
026 14 Walloon DJOU
026 10 Italian GIORNO
</table>
<figure confidence="0.762953">
...
</figure>
<figureCaption confidence="0.940554666666667">
Figure 3: An excerpt from the original corpus data. The
first two numbers denote the meaning and the language,
respectively.
</figureCaption>
<bodyText confidence="0.9980052">
we excluded those words that were not cognate with
any other words), and the largest had 84 words.
There were on average 10.92 cognate sets in a mean-
ing. The lowest number of cognate sets in a meaning
was 1, and the largest number was 22.
</bodyText>
<sectionHeader confidence="0.99391" genericHeader="method">
5 Multiple cognate alignment
</sectionHeader>
<bodyText confidence="0.999893230769231">
Similar to their use for multiple sequence alignment
of sequences in a family, we test Profile HMMs for
the task of aligning cognates. As described above,
an initial model is generated. We use the aforemen-
tioned heuristic of setting the initial model length to
the average length of the sequences. The transition
probabilities are sampled from a uniform-parameter
Dirichlet distribution, with each parameter having
a value of 5.0. The insert-state emission probabil-
ities are set to the background frequencies and the
match-state emission probabilities are sampled from
a Dirichlet distribution with parameters set in pro-
portion to the background frequency. The model is
</bodyText>
<figure confidence="0.578402333333333">
cja
fja =
Ear cjar
</figure>
<page confidence="0.845737">
46
</page>
<figure confidence="0.9843219">
MIIMIIMI MIIMIIMI
D--E--N- D--E--NY
Z--E--N- DZ-E--N-
DZIE--N- D--A--N-
DI-E--NA D--E--IZ
D--I--A- D--Y--DD
D--I--E- Z U-
Z--U--E- Z I-
J--O--UR D I-
DJ-O--U- G--IORNO
</figure>
<figureCaption confidence="0.999735">
Figure 4: The alignment generated via the Profile HMM
</figureCaption>
<bodyText confidence="0.9374307">
method for some cognates. These were aligned together,
but we show them in two columns to preserve space.
trained to the cognate set via the Baum-Welch algo-
rithm, and then each word in the set is aligned to
the model using the Viterbi algorithm. The words
are added to the training via a summation; therefore,
the order in which the words are considered has no
effect, in contrast to iterative pairwise methods.
The setting of the parameter values is discussed in
section 6.
</bodyText>
<subsectionHeader confidence="0.804229">
5.1 Results
</subsectionHeader>
<bodyText confidence="0.999976386363636">
To evaluate Profile HMMs for multiple cognate
alignment, we analyzed the alignments generated for
a number of cognate sets. We found that increasing
the pseudocount weight to 100 improved the quality
of the alignments by effectively biasing the model
towards similar characters according to the substitu-
tion matrix.
Figure 4 shows the Profile HMM alignment for a
cognate set of words with the meaning “day.” As
with Figure 2, the alignment’s first line is a guide
label used to indicate which columns are match
columns and which are insert columns; note that
consecutive insert columns represent the same insert
state and so are not aligned by the Profile HMM.
While there were some duplicate words (i.e. words
that had identical English orthographic representa-
tions but came from different languages), we do not
show them here for brevity.
In this example, we see that the Profile HMM
manages to identify those columns that are more
highly conserved as match states. The ability to
identify characters that are similar and align them
correctly can be attributed to the provided substitu-
tion matrix.
Note that the characters in the insert columns
should not be treated as aligned even though they
represent emissions from the same insert state (this
highlights the difference between match and insert
states). For example, Y, A, Z, D, R, and O are all
placed in a single insert column even though they
cannot be traced to a single phoneme in a protoform
of the cognate set. Particularly infrequent charac-
ters are more likely to be put together than separated
even if they are phonetically dissimilar.
There is some difficulty, also evident from other
alignments we generated, in isolating phonemes rep-
resented by pairs of characters (digraphs) as singular
entities. In the given example, this means that the dz
in dzien was modelled as a match state and then an
insert state. This is, however, an inherent difficulty
in using data represented only with the English al-
phabet, which could potentially be addressed if the
data were instead represented in a standard phonetic
notation such as IPA.
</bodyText>
<sectionHeader confidence="0.994926" genericHeader="method">
6 Cognate set matching
</sectionHeader>
<bodyText confidence="0.99995776">
Evaluating alignments in a principled way is diffi-
cult because of the lack of a gold standard. To adjust
for this, we also evaluate Profile HMMs for the task
of matching a word to the correct cognate set from
a list of cognate sets with the same meaning as the
given word, similar to the evaluation of a biologi-
cal sequence for membership in a family. This is
realized by removing one word at a time from each
word list and then using the resulting cognate sets
within the meaning as possible targets. A model is
generated from each possible target and a log-odds
score is computed for the word using the forward
algorithm. The scores are then sorted and the high-
est score is taken to be the cognate set to which the
given word belongs. The accuracy is then the frac-
tion of times the correct cognate set is identified.
To determine the best parameter values, we used
a development set of 10 meanings (roughly 5%
of the data). For the substitution matrix pseudo-
count method, we used a log-odds similarity ma-
trix derived from Pair HMM training (Mackay and
Kondrak, 2005). The best results were achieved
with favouring of match states enabled, substitution-
matrix-based pseudocount, pseudocount weight of
0.5, and pseudocounts added during Baum-Welch.
</bodyText>
<page confidence="0.998332">
47
</page>
<sectionHeader confidence="0.618927" genericHeader="evaluation">
6.1 Results
</sectionHeader>
<bodyText confidence="0.999991066666666">
We employed two baselines to generate scores be-
tween a given word and cognate set. The first base-
line uses the average edit distance of the test word
and the words in the given cognate set as the score
of the word against the set. The second baseline is
similar but uses the minimum edit distance between
the test word and any word in the given cognate set
as the score of the word against the entire set. For ex-
ample, in the example set given in Figure 4, the aver-
age edit distance between zen and all other words in
the set is 2.58 (including the hidden duplicate words)
and the minimum edit distance is 1. All other can-
didate sets are similarly scored and the one with the
lowest score is considered to be the correct cluster
with ties broken randomly.
With the parameter settings described in the pre-
vious section, the Profile HMM method correctly
identifies the corresponding cognate set with an ac-
curacy of 93.2%, a substantial improvement over the
average edit distance baseline, which obtains an ac-
curacy of 77.0%.
Although the minimum edit distance baseline also
yields an impressive accuracy of 91.0%, its score is
based on a single word in the candidate set, and so
would not be appropriate for cases where consider-
ation of the entire set is necessary. Furthermore, the
baseline benefits from the frequent presence of du-
plicate words in the cognate sets. Profile HMMs are
more robust, thanks to the presence of identical or
similar characters in corresponding positions.
</bodyText>
<sectionHeader confidence="0.999584" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999957125">
Profile HMMs present an approach for working with
sets of words. We tested their use for two cognate-
related tasks. The method produced good-quality
multiple cognate alignments, and we believe that
they could be further improved with phonetically
transcribed data. For the task of matching words to
correct cognate sets, we achieved an improvement
over the average edit distance and minimum edit dis-
tance baselines.
Since Profile HMM training is highly sensitive to
the choice of initial model, we would like to ex-
plore more informed methods of constructing the
initial model. Similarly, for building models from
unaligned sequences, the addition of domain knowl-
edge would likely prove beneficial. We also plan to
investigate better pseudocount methods, as well as
the possibility of using n-grams as output symbols.
By simultaneously considering an entire set of re-
lated words, Profile HMMs provide a distinct ad-
vantage over iterative pairwise methods. The suc-
cess on our tasks of multiple alignment and cognate
set matching suggests applicability to similar tasks
involving words, such as named entity recognition
across potentially multi-lingual corpora.
</bodyText>
<sectionHeader confidence="0.997962" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9990182">
We thank Qing Dou for organizing the cognate sets
from the original data. We are also grateful to the
anonymous reviewers for their valuable comments.
This research was funded in part by the Natural Sci-
ences and Engineering Research Council of Canada.
</bodyText>
<sectionHeader confidence="0.999669" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999824032258064">
Regina Barzilay and Lillian Lee. 2002. Bootstrapping
lexical choice via multiple-sequence alignment. In
Proc. of EMNLP, pages 164–171.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proc. of NAACL-HLT, pages
16–23.
Michael A. Covington. 1996. An algorithm to align
words for historical comparison. Computational Lin-
guistics, 22(4):481–496.
Michael A. Covington. 1998. Alignment of multi-
ple languages for historical comparison. In Proc. of
COLING-ACL, pages 275–279.
Richard Durbin, Sean R. Eddy, Anders Krogh, and
Graeme Mitchison. 1998. Biological sequence analy-
sis: probabilistic models ofproteins and nucleic acids.
Cambridge University Press.
Isidore Dyen, Joseph B. Kruskal, and Paul Black. 1992.
An Indoeuropean classification: A lexicostatistical ex-
periment. Transactions of the American Philosophical
Society, 82(5).
Grzegorz Kondrak. 2000. A new algorithm for the align-
ment of phonetic sequences. In Proc. of NAACL, pages
288–295.
Wesley Mackay and Grzegorz Kondrak. 2005. Comput-
ing word similarity and identifying cognates with pair
hidden Markov models. In Proc. of CoNLL, pages 40–
47.
John Nerbonne and Wilbert Heeringa. 1997. Measur-
ing dialect distance phonetically. In Proc. of the Third
Meeting of ACL SIGPHON.
</reference>
<page confidence="0.999353">
48
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.941712">
<title confidence="0.999689">Multiple Word Alignment with Profile Hidden Markov Models</title>
<author confidence="0.990176">Aditya Bhargava</author>
<author confidence="0.990176">Grzegorz</author>
<affiliation confidence="0.999073">Department of Computing University of</affiliation>
<address confidence="0.958298">Edmonton, Alberta, Canada, T6G</address>
<abstract confidence="0.999476733333334">Profile hidden Markov models (Profile HMMs) are specific types of hidden Markov models used in biological sequence analysis. We propose the use of Profile HMMs for word-related tasks. We test their applicability to the tasks of multiple cognate alignment and cognate set matching, and find that they work well in general for both tasks. On the latter task, the Profile HMM method outperforms average and minimum edit distance. Given the success for these two tasks, we further discuss the potential applications of Profile HMMs to any task where consideration of a set of words is necessary.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Bootstrapping lexical choice via multiple-sequence alignment.</title>
<date>2002</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>164--171</pages>
<contexts>
<context position="2624" citStr="Barzilay and Lee, 2002" startWordPosition="415" endWordPosition="418">related task of matching words to established cognate sets, useful for a situation where it is not immediately obvious to which cognate set a word should be matched. The accuracy on the latter task exceeds the accuracy of a method based on edit distance. Profile HMMs could also potentially be used for the computation of word similarity when a word must be compared not to another word but to another set of words, taking into account properties of all constituent words. The use of Profile HMMs for multiple sequence alignment also presents applications to the acquisition of mapping dictionaries (Barzilay and Lee, 2002) and sentence-level paraphrasing (Barzilay and Lee, 2003). This paper is organized as follows: we first describe the uses of Profile HMMs in computational biology, their structure, and then discuss their applications to word-related tasks. We then discuss our data set and describe the tasks that we test and their experimental setups and results. We conclude with a summary of the results and a brief discussion of potential future work. 2 Profile hidden Markov models In computational biology, it is often necessary to deal with multiple sequences, including DNA and protein sequences. For such bio</context>
</contexts>
<marker>Barzilay, Lee, 2002</marker>
<rawString>Regina Barzilay and Lillian Lee. 2002. Bootstrapping lexical choice via multiple-sequence alignment. In Proc. of EMNLP, pages 164–171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Learning to paraphrase: An unsupervised approach using multiplesequence alignment.</title>
<date>2003</date>
<booktitle>In Proc. of NAACL-HLT,</booktitle>
<pages>16--23</pages>
<contexts>
<context position="2681" citStr="Barzilay and Lee, 2003" startWordPosition="423" endWordPosition="426">s, useful for a situation where it is not immediately obvious to which cognate set a word should be matched. The accuracy on the latter task exceeds the accuracy of a method based on edit distance. Profile HMMs could also potentially be used for the computation of word similarity when a word must be compared not to another word but to another set of words, taking into account properties of all constituent words. The use of Profile HMMs for multiple sequence alignment also presents applications to the acquisition of mapping dictionaries (Barzilay and Lee, 2002) and sentence-level paraphrasing (Barzilay and Lee, 2003). This paper is organized as follows: we first describe the uses of Profile HMMs in computational biology, their structure, and then discuss their applications to word-related tasks. We then discuss our data set and describe the tasks that we test and their experimental setups and results. We conclude with a summary of the results and a brief discussion of potential future work. 2 Profile hidden Markov models In computational biology, it is often necessary to deal with multiple sequences, including DNA and protein sequences. For such biological sequence analysis, Profile HMMs are applied to th</context>
</contexts>
<marker>Barzilay, Lee, 2003</marker>
<rawString>Regina Barzilay and Lillian Lee. 2003. Learning to paraphrase: An unsupervised approach using multiplesequence alignment. In Proc. of NAACL-HLT, pages 16–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael A Covington</author>
</authors>
<title>An algorithm to align words for historical comparison.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>4</issue>
<contexts>
<context position="927" citStr="Covington (1996)" startWordPosition="137" endWordPosition="138">els used in biological sequence analysis. We propose the use of Profile HMMs for word-related tasks. We test their applicability to the tasks of multiple cognate alignment and cognate set matching, and find that they work well in general for both tasks. On the latter task, the Profile HMM method outperforms average and minimum edit distance. Given the success for these two tasks, we further discuss the potential applications of Profile HMMs to any task where consideration of a set of words is necessary. 1 Introduction In linguistics, it is often necessary to align words or phonetic sequences. Covington (1996) uses alignments of cognate pairs for the historical linguistics task of comparative reconstruction and Nerbonne and Heeringa (1997) use alignments to compute relative distances between words from various Dutch dialects. Algorithms for aligning pairs of words have been proposed by Covington (1996) and Kondrak (2000). However, it is often necessary to align multiple words. Covington (1998) proposed a method to align multiple words based on a handcrafted scale of similarity between various classes of phonemes, again for the purpose of comparative reconstruction of languages. Profile hidden Marko</context>
</contexts>
<marker>Covington, 1996</marker>
<rawString>Michael A. Covington. 1996. An algorithm to align words for historical comparison. Computational Linguistics, 22(4):481–496.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael A Covington</author>
</authors>
<title>Alignment of multiple languages for historical comparison.</title>
<date>1998</date>
<booktitle>In Proc. of COLING-ACL,</booktitle>
<pages>275--279</pages>
<contexts>
<context position="1318" citStr="Covington (1998)" startWordPosition="197" endWordPosition="198">er discuss the potential applications of Profile HMMs to any task where consideration of a set of words is necessary. 1 Introduction In linguistics, it is often necessary to align words or phonetic sequences. Covington (1996) uses alignments of cognate pairs for the historical linguistics task of comparative reconstruction and Nerbonne and Heeringa (1997) use alignments to compute relative distances between words from various Dutch dialects. Algorithms for aligning pairs of words have been proposed by Covington (1996) and Kondrak (2000). However, it is often necessary to align multiple words. Covington (1998) proposed a method to align multiple words based on a handcrafted scale of similarity between various classes of phonemes, again for the purpose of comparative reconstruction of languages. Profile hidden Markov models (Profile HMMs) are specific types of hidden Markov models used in biological sequence analysis, where they have yielded success for the matching of given sequences to sequence families as well as to multiple sequence 43 alignment (Durbin et al., 1998). In this paper, we show that Profile HMMs can be adapted to the task of aligning multiple words. We apply them to sets of multilin</context>
</contexts>
<marker>Covington, 1998</marker>
<rawString>Michael A. Covington. 1998. Alignment of multiple languages for historical comparison. In Proc. of COLING-ACL, pages 275–279.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Durbin</author>
<author>Sean R Eddy</author>
<author>Anders Krogh</author>
<author>Graeme Mitchison</author>
</authors>
<title>Biological sequence analysis: probabilistic models ofproteins and nucleic acids.</title>
<date>1998</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="1787" citStr="Durbin et al., 1998" startWordPosition="268" endWordPosition="271">ning pairs of words have been proposed by Covington (1996) and Kondrak (2000). However, it is often necessary to align multiple words. Covington (1998) proposed a method to align multiple words based on a handcrafted scale of similarity between various classes of phonemes, again for the purpose of comparative reconstruction of languages. Profile hidden Markov models (Profile HMMs) are specific types of hidden Markov models used in biological sequence analysis, where they have yielded success for the matching of given sequences to sequence families as well as to multiple sequence 43 alignment (Durbin et al., 1998). In this paper, we show that Profile HMMs can be adapted to the task of aligning multiple words. We apply them to sets of multilingual cognates and show that they produce good alignments. We also use them for the related task of matching words to established cognate sets, useful for a situation where it is not immediately obvious to which cognate set a word should be matched. The accuracy on the latter task exceeds the accuracy of a method based on edit distance. Profile HMMs could also potentially be used for the computation of word similarity when a word must be compared not to another word</context>
<context position="5496" citStr="Durbin et al., 1998" startWordPosition="890" endWordPosition="893">They are represented in the same manner as match states, with each output symbol having an associated probability. Insert states are used to account for symbols that have been inserted to a given position that might not otherwise have occurred “naturally” via a match state. Insert states can probabilistically transition to the next match and delete states as well as the current insert state (i.e. itself). Allowing insert states to transition to themselves enables the consideration of multiplesymbol inserts. MMIIIM AG...C A-AG.C AG.AA--AAAC AG...C Figure 2: A small DNA multiple alignment from (Durbin et al., 1998, p. 123). Similarly, delete states represent symbols that have been removed from a given position. For a sequence to use a delete state for a given position indicates that a given character position in the model has no corresponding characters in the given sequence. Hence, delete states are by nature silent and thus have no emission probabilities for the output symbols. This is an important distinction from match states and insert states. Each delete state can probabilistically transition to the next match and delete states as well as the current insert state. Figure 2 shows a small example o</context>
<context position="10704" citStr="Durbin et al., 1998" startWordPosition="1787" endWordPosition="1790">arious adjustable parameters.1 We investigated the following parameters: Favouring match states When constructing a Profile HMM from unaligned sequences, the choice of initial model probabilities can have a significant effect on results. It may be sensible to favour match states compared to other states when constructing the initial model; since the transition probabilities are sampled from a Dirichlet distribution, the option of favouring match states assigns the largest returned probability to the transition to a match state. Pseudocount method We implemented three pseudocount methods from (Durbin et al., 1998). In the following equations, ej(a) is the probability of state j emitting character a. cja represents the observed counts of state j emitting symbol a. A is the weight given to the pseudocounts. Constant value A constant value AC is added to each count. This is a generalization of Laplace’s rule, where C = 1A. ej(a) = Ea, cja&apos; + A cja + AC Background frequency Pseudocounts are added in proportion to the background frequency qa, which is the frequency of occurrence of character a. cja + Aqa ej(a) = Eal cja&apos; + A 1Our implementation is available online at http://www. cs.ualberta.ca/˜ab31/profile</context>
</contexts>
<marker>Durbin, Eddy, Krogh, Mitchison, 1998</marker>
<rawString>Richard Durbin, Sean R. Eddy, Anders Krogh, and Graeme Mitchison. 1998. Biological sequence analysis: probabilistic models ofproteins and nucleic acids. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isidore Dyen</author>
<author>Joseph B Kruskal</author>
<author>Paul Black</author>
</authors>
<title>An Indoeuropean classification: A lexicostatistical experiment.</title>
<date>1992</date>
<journal>Transactions of the American Philosophical Society,</journal>
<volume>82</volume>
<issue>5</issue>
<contexts>
<context position="12346" citStr="Dyen et al., 1992" startWordPosition="2068" endWordPosition="2071">ve: ej(a) = Ear cjar + αjar cja + αja Pseudocount weight The weight that the pseudocounts are given (A in the above equations). Smoothing during Baum-Welch The problem has many local optima and it is therefore easy for the Baum-Welch algorithm to get stuck around one of these. In order to avoid local optima, we tested the option of adding pseudocounts during Baum-Welch (i.e. between iterations) rather than after it. This serves as a form of noise injection, effectively bumping BaumWelch away from local optima. 4 Data for experiments Our data come from the Comparative Indoeuropean Data Corpus (Dyen et al., 1992). The data consist of words in 95 languages in the Indoeuropean family organized into word lists corresponding to one of 200 meanings. Each word is represented in the English alphabet. Figure 3 shows a sample from the original corpus data. We manually converted the data into disjoint sets of cognate words, where each cognate set contains only one word from each language. We also removed words that were not cognate with any other words. On average, there were 4.37 words per cognate set. The smallest cognate set had two words (since a 026 DAY 003 ... b 026 53 Bulgarian DEN 026 47 Czech E DENY 02</context>
</contexts>
<marker>Dyen, Kruskal, Black, 1992</marker>
<rawString>Isidore Dyen, Joseph B. Kruskal, and Paul Black. 1992. An Indoeuropean classification: A lexicostatistical experiment. Transactions of the American Philosophical Society, 82(5).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grzegorz Kondrak</author>
</authors>
<title>A new algorithm for the alignment of phonetic sequences.</title>
<date>2000</date>
<booktitle>In Proc. of NAACL,</booktitle>
<pages>288--295</pages>
<contexts>
<context position="1244" citStr="Kondrak (2000)" startWordPosition="186" endWordPosition="187">d minimum edit distance. Given the success for these two tasks, we further discuss the potential applications of Profile HMMs to any task where consideration of a set of words is necessary. 1 Introduction In linguistics, it is often necessary to align words or phonetic sequences. Covington (1996) uses alignments of cognate pairs for the historical linguistics task of comparative reconstruction and Nerbonne and Heeringa (1997) use alignments to compute relative distances between words from various Dutch dialects. Algorithms for aligning pairs of words have been proposed by Covington (1996) and Kondrak (2000). However, it is often necessary to align multiple words. Covington (1998) proposed a method to align multiple words based on a handcrafted scale of similarity between various classes of phonemes, again for the purpose of comparative reconstruction of languages. Profile hidden Markov models (Profile HMMs) are specific types of hidden Markov models used in biological sequence analysis, where they have yielded success for the matching of given sequences to sequence families as well as to multiple sequence 43 alignment (Durbin et al., 1998). In this paper, we show that Profile HMMs can be adapted</context>
</contexts>
<marker>Kondrak, 2000</marker>
<rawString>Grzegorz Kondrak. 2000. A new algorithm for the alignment of phonetic sequences. In Proc. of NAACL, pages 288–295.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wesley Mackay</author>
<author>Grzegorz Kondrak</author>
</authors>
<title>Computing word similarity and identifying cognates with pair hidden Markov models.</title>
<date>2005</date>
<booktitle>In Proc. of CoNLL,</booktitle>
<pages>40--47</pages>
<contexts>
<context position="8757" citStr="Mackay and Kondrak, 2005" startWordPosition="1468" endWordPosition="1471">it can be used to evaluate a given sequence for membership in the family. This is done via a straightforward application of the forward algorithm (to get the full probability of the given sequence) or the Viterbi algorithm (to get the alignment of the sequence to the family). For the alignment of multiple unaligned sequences, a Profile HMM is constructed and trained as described above and then each sequence can be aligned using the Viterbi algorithm. It should also be noted that Profile HMMs are generalizations of Pair HMMs, which have been used for cognate identification and word similarity (Mackay and Kondrak, 2005) between pairs of words. Unlike Pair HMMs, Profile HMMs are position-specific; this is what allows their application to multiple sequences but also means that each Profile HMM must be trained to a given set of sequences, whereas Pair HMMs can be trained over a very large data set of pairs of words. 3 Adapting Profile HMMs to words Using Profile HMMs for biological sequences involves defining an alphabet and working with related sequences consisting of symbols from that alphabet. One could perform tasks with cognates sets in a similar manner; cognates are, after all, related words, and words ar</context>
<context position="18322" citStr="Mackay and Kondrak, 2005" startWordPosition="3104" endWordPosition="3107">lting cognate sets within the meaning as possible targets. A model is generated from each possible target and a log-odds score is computed for the word using the forward algorithm. The scores are then sorted and the highest score is taken to be the cognate set to which the given word belongs. The accuracy is then the fraction of times the correct cognate set is identified. To determine the best parameter values, we used a development set of 10 meanings (roughly 5% of the data). For the substitution matrix pseudocount method, we used a log-odds similarity matrix derived from Pair HMM training (Mackay and Kondrak, 2005). The best results were achieved with favouring of match states enabled, substitutionmatrix-based pseudocount, pseudocount weight of 0.5, and pseudocounts added during Baum-Welch. 47 6.1 Results We employed two baselines to generate scores between a given word and cognate set. The first baseline uses the average edit distance of the test word and the words in the given cognate set as the score of the word against the set. The second baseline is similar but uses the minimum edit distance between the test word and any word in the given cognate set as the score of the word against the entire set.</context>
</contexts>
<marker>Mackay, Kondrak, 2005</marker>
<rawString>Wesley Mackay and Grzegorz Kondrak. 2005. Computing word similarity and identifying cognates with pair hidden Markov models. In Proc. of CoNLL, pages 40– 47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Nerbonne</author>
<author>Wilbert Heeringa</author>
</authors>
<title>Measuring dialect distance phonetically.</title>
<date>1997</date>
<booktitle>In Proc. of the Third Meeting of ACL SIGPHON.</booktitle>
<contexts>
<context position="1059" citStr="Nerbonne and Heeringa (1997)" startWordPosition="155" endWordPosition="159">cability to the tasks of multiple cognate alignment and cognate set matching, and find that they work well in general for both tasks. On the latter task, the Profile HMM method outperforms average and minimum edit distance. Given the success for these two tasks, we further discuss the potential applications of Profile HMMs to any task where consideration of a set of words is necessary. 1 Introduction In linguistics, it is often necessary to align words or phonetic sequences. Covington (1996) uses alignments of cognate pairs for the historical linguistics task of comparative reconstruction and Nerbonne and Heeringa (1997) use alignments to compute relative distances between words from various Dutch dialects. Algorithms for aligning pairs of words have been proposed by Covington (1996) and Kondrak (2000). However, it is often necessary to align multiple words. Covington (1998) proposed a method to align multiple words based on a handcrafted scale of similarity between various classes of phonemes, again for the purpose of comparative reconstruction of languages. Profile hidden Markov models (Profile HMMs) are specific types of hidden Markov models used in biological sequence analysis, where they have yielded suc</context>
</contexts>
<marker>Nerbonne, Heeringa, 1997</marker>
<rawString>John Nerbonne and Wilbert Heeringa. 1997. Measuring dialect distance phonetically. In Proc. of the Third Meeting of ACL SIGPHON.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>