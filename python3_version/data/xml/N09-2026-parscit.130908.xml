<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.021767">
<title confidence="0.994094">
Multi-scale Personalization for Voice Search Applications
</title>
<author confidence="0.991469">
Daniel Bola˜nos Geoffrey Zweig
</author>
<affiliation confidence="0.96697">
Center for Spoken Language Research Microsoft Research
University of Colorado at Boulder, USA One Microsoft Way, Redmond, WA 98052
</affiliation>
<email confidence="0.994691">
bolanos@cslr.colorado.edu gzweig@microsoft.com
</email>
<author confidence="0.997028">
Patrick Nguyen
</author>
<affiliation confidence="0.914924">
Microsoft Research
One Microsoft Way, Redmond, WA 98052
</affiliation>
<email confidence="0.9982">
panguyen@microsoft.com
</email>
<sectionHeader confidence="0.998588" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998979545454545">
Voice Search applications provide a very con-
venient and direct access to a broad variety
of services and information. However, due to
the vast amount of information available and
the open nature of the spoken queries, these
applications still suffer from recognition er-
rors. This paper explores the utilization of per-
sonalization features for the post-processing
of recognition results in the form of n-best
lists. Personalization is carried out from three
different angles: short-term, long-term and
Web-based, and a large variety of features are
proposed for use in a log-linear classification
framework.
Experimental results on data obtained from a
commercially deployed Voice Search system
show that the combination of the proposed
features leads to a substantial sentence error
rate reduction. In addition, it is shown that
personalization features which are very dif-
ferent in nature can successfully complement
each other.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998392">
Search engines are a powerful mechanism to find
specific content through the use of queries. In recent
years, due to the vast amount of information avail-
able, there has been significant research on the use of
recommender algorithms to select what information
will be presented to the user. These systems try to
predict what content a user may want based not only
on the user’s query but on the user’s past queries,
history of clicked results, and preferences. In (Tee-
van et al., 1996) it was observed that a significant
</bodyText>
<page confidence="0.985959">
101
</page>
<bodyText confidence="0.999981264705882">
percent of the queries made by a user in a search
engine are associated to a repeated search. Recom-
mender systems like (Das et al., 2007) and (Dou et
al., 2007) take advantage of this fact to refine the
search results and improve the search experience.
In this paper, we explore the use of personaliza-
tion in the context of voice searches rather than web
queries. Specifically, we focus on data from a multi-
modal cellphone-based business search application
(Acero et al., 2008). In such an application, repeated
queries can be a powerful tool for personalization.
These can be classified into short and long-term rep-
etitions. Short-term repetitions are typically caused
by a speech recognition error, which produces an in-
correct search result and makes the user repeat or
reformulate the query. On the other hand, long-term
repetitions, as in text-based search applications, oc-
cur when the user needs to access some information
that was accessed previously, for example, the exact
location of a pet clinic.
This paper proposes several different user per-
sonalization methods for increasing the recognition
accuracy in Voice Search applications. The pro-
posed personalization methods are based on extract-
ing short-term, long-term and Web-based features
from the user’s history. In recent years, other user
personalization methods like deriving personalized
pronunciations have proven successful in the context
of mobile applications (Deligne et al., 2002).
The rest of this paper is organized as follows: Sec-
tion 2 describes the classification method used for
rescoring the recognition hypotheses. Section 3 de-
scribes the proposed personalization methods. Sec-
tion 4 describes the experiments carried out. Finally,
</bodyText>
<subsubsectionHeader confidence="0.599425">
Proceedings of NAACL HLT 2009: Short Papers, pages 101–104,
</subsubsectionHeader>
<bodyText confidence="0.4629965">
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
conclusions from this work are drawn in section 5.
</bodyText>
<sectionHeader confidence="0.774496" genericHeader="method">
2 Rescoring procedure
</sectionHeader>
<subsectionHeader confidence="0.997509">
2.1 Log linear classification
</subsectionHeader>
<bodyText confidence="0.9999044">
Our work will proceed by using a log-linear clas-
sifier similar to the maximum entropy approach of
(Berger and Della Pietra, 1996) to predict which
word sequence W appearing on an n-best list N is
most likely to be correct. This is estimated as
</bodyText>
<equation confidence="0.970279">
exp(�i Aifi(W, N))
P(W|N) = (1)
EW&apos;∈N exp(Ei Aifi(WI, N))
</equation>
<bodyText confidence="0.999746111111111">
The feature functions fi(W, N) can represent ar-
bitrary attributes of W and N. This can be seen
to be the same as a maximum entropy formulation
where the class is defined as the word sequence (thus
allowing potentially infinite values) but with sums
restricted as a computational convenience to only
those class values (word strings) appearing on the n-
best list. The models were estimated with a widely
available toolkit (Mahajan, 2007).
</bodyText>
<subsectionHeader confidence="0.998147">
2.2 Feature extraction
</subsectionHeader>
<bodyText confidence="0.999989666666667">
Given the use of a log-linear classifier, the crux of
our work lies in the specific features used. As a base-
line, we take the hypothesis rank, which results in
the 1-best accuracy of the decoder. Additional fea-
tures were obtained from the personalization meth-
ods described in the following section.
</bodyText>
<sectionHeader confidence="0.999739" genericHeader="method">
3 Personalization methods
</sectionHeader>
<subsectionHeader confidence="0.996481">
3.1 Short-term personalization
</subsectionHeader>
<bodyText confidence="0.99887080952381">
Short-term personalization aims at modeling the re-
pair/repetition behavior of the user. Short-term fea-
tures are a mechanism suitable for representing neg-
ative evidence: if the user repeats a utterance it nor-
mally means that the hypotheses in the previous n-
best lists are not correct. For this reason, if a hy-
pothesis is contained in a preceding n-best list, that
hypothesis should be weighted negatively during the
rescoring.
A straightforward method for identifying likely
repetitions consists of using a fixed size time win-
dow and considering all the user queries within that
window as part of the same repetition round. Once
an appropriate window size has been determined,
the proposed short-term features can be extracted for
each hypothesis using a binary tree like the one de-
picted in figure 1, where feature values are in the
leaves of the tree.
Does a recent (60s) n-best
list contain the hypothesis
we are scoring?
</bodyText>
<figureCaption confidence="0.999822">
Figure 1: Short-term feature extraction (note that over-
lines mean “do not”).
</figureCaption>
<bodyText confidence="0.999834333333333">
Given these features, we expect “seen and not
clicked” to have a negative weight while “seen and
clicked” should have a positive weight.
</bodyText>
<subsectionHeader confidence="0.995522">
3.2 Long-term personalization
</subsectionHeader>
<bodyText confidence="0.99996">
Long-term personalization consists of using the user
history (i.e. recognition hypotheses that were con-
firmed by the user in the past) to predict which
recognition results are more likely. The assumption
here is that recognition hypotheses in the n-best list
that match or “resemble” those in the user history are
more likely to be correct. The following list enumer-
ates the long-term features proposed in this work:
</bodyText>
<listItem confidence="0.992960545454545">
• User history (occurrences): number of times
the hypothesis appears in the user history.
• User history (alone): 1 if the hypothesis ap-
pears in the user history and no other compet-
ing hypothesis does, otherwise 0.
• User history (most clicked): 1 if the hypothe-
sis appears in the user history and was clicked
more times than any other competing hypothe-
sis.
• User history (most recent): 1 if the hypothe-
sis appears in the user history and was clicked
</listItem>
<equation confidence="0.8393264">
No
seen = 1
seen &amp; clicked = 0
seen &amp; clicked = 0
Yes
</equation>
<bodyText confidence="0.855561">
Did the user click
on that hypothesis?
</bodyText>
<equation confidence="0.963856125">
No
seen = 0
seen &amp; clicked = 1
seen &amp; clicked = 0
Yes
seen = 0
seen &amp; clicked = 0
seen &amp; clicked = 1
</equation>
<page confidence="0.988277">
102
</page>
<bodyText confidence="0.983953">
more recently than any other competing hy-
pothesis.
</bodyText>
<listItem confidence="0.983669823529412">
• User history (edit distance): minimum edit dis-
tance between the hypothesis and the closest
query in the user history, normalized by the
number of words.
• User history (words in common): maximum
number of words in common between the hy-
pothesis and each of the queries in the user his-
tory, normalized by the number of words in the
hypothesis.
• User history (plural/singular): 1 if either the
plural or singular version of the hypothesis ap-
pears in the user history, otherwise 0.
• Global history: 1 if the hypothesis has ever
been clicked by any user, otherwise 0.
• Global history (alone): 1 if the hypothesis is the
only one in the n-best that has ever been clicked
by any user, otherwise 0.
</listItem>
<bodyText confidence="0.998477">
Note that the last two features proposed make
use of the “global history” which comprises all the
queries made by any user.
</bodyText>
<subsectionHeader confidence="0.987774">
3.3 LiveSearch-based features
</subsectionHeader>
<bodyText confidence="0.9999114">
Typically, users ask for businesses that exist, and if
a business exists it probably appears in a Web docu-
ment indexed by Live Search (Live Search, 2006). It
is reasonable to assume that the relevance of a given
business is connected to the number of times it ap-
pears in the indexed Web documents, and in this sec-
tion we derive such features.
For the scoring process, an application has been
built that makes automated queries to Live Search,
and for each hypothesis in the n-best list obtains the
number of Web documents in which it appears. De-
noting by x the number of Web documents in which
the hypothesis (the exact sequence of words, e.g.
“tandoor indian restaurant”) appears, the following
features are proposed:
</bodyText>
<listItem confidence="0.999505">
• Logarithm of the absolute count: log(x).
• Search results rank: sort the hypotheses in the
n-best list by their relative value of x and use
the rank as a feature.
• Relative relevance (I): 1 if the hypothesis was
not found and there is another hypothesis in the
n-best list that was found more than 100 times,
otherwise 0.
• Relative relevance (II): 1 if the the hypothesis
</listItem>
<bodyText confidence="0.84504">
appears fewer than 10 times and there is an-
other hypothesis in the n-best list that appears
more than 100 times, otherwise 0.
</bodyText>
<sectionHeader confidence="0.99836" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.986491">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.999983896551724">
The data used for the experiments comprises 22473
orthographically transcribed business utterances ex-
tracted from a commercially deployed large vocabu-
lary directory assistance system.
For each of the transcribed utterances two n-best
lists were produced, one from the commercially de-
ployed system and other from an enhanced decoder
with a lower sentence error rate (SER). In the exper-
iments, due to their lower oracle error rate, n-bests
from the enhanced decoder were used for doing the
rescoring. However, these n-bests do not correspond
to the listings shown in the user’s device screen (i.e.
do not match the user interaction) so are not suit-
able for identifying repetitions. For this reason, the
short term features were computed by comparing a
hypothesis from the enhanced decoder with the orig-
inal n-best list from the immediate past. Note that all
other features were computed solely with reference
to the n-bests from the enhanced decoder.
A rescoring subset was made from the original
dataset using only those utterances in which the n-
best lists contain the correct hypothesis (in any po-
sition) and have more than one hypothesis. For all
other utterances, rescoring cannot have any effect.
The size of the rescoring subset is 43.86% the size
of the original dataset for a total of 9858 utterances.
These utterances were chronologically partitioned
into a training set containing two thirds and a test
set with the rest.
</bodyText>
<subsectionHeader confidence="0.581243">
4.2 Results
</subsectionHeader>
<bodyText confidence="0.999784">
The baseline system for the evaluation of the pro-
posed features consist of a ME classifier trained on
only one feature, the hypothesis rank. The resulting
sentence error rate (SER) of this classifier is that of
the best single path, and it is 24.73%. To evaluate
</bodyText>
<page confidence="0.998638">
103
</page>
<bodyText confidence="0.999456888888889">
the contribution of each of the features proposed in
section 3, a different ME classifier was trained us-
ing that feature in addition to the baseline feature.
Finally, another ME classifier was trained on all the
features together.
Table 1 summarizes the Sentence Error Rate
(SER) for each of the proposed features in isolation
and all together respect to the baseline. “UH” stands
for user history.
</bodyText>
<figure confidence="0.945985157894737">
Features SER
Hypothesis rank (baseline) 24.73%
base + repet. (seen) 24.48%
base + repet. (seen &amp; clicked) 24.32%
base + repet. (seen &amp; clicked) 24.73%
base + UH (occurrences) 23.76%
base + UH (alone) 23.79%
base + UH (most clicked) 23.73%
base + UH (most recent) 23.88%
base + UH (edit distance) 23.76%
base + UH (words in common) 24.60%
base + UH (plural/singular) 24.76%
base + GH 24.63%
base + GH (alone) 24.66%
base + Live Search (absolute count) 24.35%
base + Live Search (rank) 24.85%
base + Live Search (relative I) 23.51%
base + Live Search (relative II) 23.69%
base + all 21.54%
</figure>
<tableCaption confidence="0.9496955">
Table 1: Sentence Error Rate (SER) for each of the fea-
tures in isolation and for the combination of all of them.
</tableCaption>
<sectionHeader confidence="0.999285" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.9999655">
The proposed features reduce the SER of the base-
line system by 3.19% absolute on the rescoring set,
and by 1.40% absolute on the whole set of tran-
scribed utterances.
Repetition based features are moderately useful;
by incorporating them into the rescoring it is possi-
ble to reduce the SER from 24.73% to 24.32%. Al-
though repetitions cover a large percentage of the
data, it is believed that inconsistencies in the user
interaction (the right listing is displayed but not con-
firmed by the user) prevented further improvement.
As expected, long-term personalization based fea-
tures contribute to improve the classification accu-
racy. The UH (occurrences) feature by itself is able
to reduce the SER in about a 1%.
Live Search has shown a very good potential for
feature extraction. In this respect it is interesting to
note that a right design of the features seems critical
to take full advantage of it. The relative number of
counts of one hypothesis respect to other hypotheses
in the n-best list is more informative than an absolute
or ranked count. A simple feature using this kind of
information, like Live Search (relative I), can reduce
the SER in more than 1% respect to the baseline.
Finally, it has been shown that personalization
based features can complement each other very well.
</bodyText>
<sectionHeader confidence="0.999678" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999912142857143">
Alex Acero, Neal Bernstein, Rob Chambers, Yun-Cheng
Ju, Xiao Li, Julian Odell, Patrick Nguyen, Oliver
Scholtz and Geoffrey Zweig. 2008. Live Search
for Mobile: Web Services by Voice on the Cellphone.
ICASSP 2008, March 31 2008-April 4 2008. Las Ve-
gas, NV, USA.
Adam L. Berger; Vincent J. Della Pietra; Stephen A.
Della Pietra. 1996. A Maximum Entropy Approach
to Natural Language Processing. Computational Lin-
guistics, 1996. 22(1): p. 39-72.
Abhinandan Das, Mayur Datar and Ashutosh Garg.
2007. Google News Personalization: Scalable Online
Collaborative Filtering. WWW 2007 / Track: Indus-
trial Practice and Experience May 8-12, 2007. Banff,
Alberta, Canada.
Sabine Deligne, Satya Dharanipragada, Ramesh
Gopinath, Benoit Maison, Peder Olsen and Harry
Printz. 2002. A robust high accuracy speech recog-
nition system for mobile applications. Speech and
Audio Processing, IEEE Transactions on, Nov 2002,
Volume: 10, Issue: 8, On page(s): 551- 561.
Zhicheng Dou, Ruihua Song, and Ji-Rong Wen. 2007.
A large-scale evaluation and analysis ofpersonalized
search strategies. In WWW ’07: Proceedings of the
16th international conference on World Wide Web,
pages 581 - 590, New York, NY, USA, 2007. ACM
Press.
Live Search. “http://www.live.com,”.
Milind Mahajan. 2007. Conditional Maximum-Entropy
Training Tool http://research.microsoft.com/en-
us/downloads/9f199826-49d5-48b6-ba1b-
f623ecf36432/.
Jaime Teevan, Eytan Adar, Rosie Jones and Michael A.
S. Potts. 2007. Information Re-Retrieval: Repeat
Queries in Yahoos Logs. SIGIR, 2007.
</reference>
<page confidence="0.998787">
104
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.518488">
<title confidence="0.999939">Multi-scale Personalization for Voice Search Applications</title>
<author confidence="0.999968">Geoffrey Zweig</author>
<affiliation confidence="0.999867">Center for Spoken Language Research Microsoft Research</affiliation>
<address confidence="0.560987">University of Colorado at Boulder, USA One Microsoft Way, Redmond, WA 98052</address>
<email confidence="0.996136">bolanos@cslr.colorado.edugzweig@microsoft.com</email>
<author confidence="0.989661">Patrick</author>
<affiliation confidence="0.963773">Microsoft</affiliation>
<address confidence="0.981803">One Microsoft Way, Redmond, WA</address>
<email confidence="0.999846">panguyen@microsoft.com</email>
<abstract confidence="0.999339956521739">Voice Search applications provide a very convenient and direct access to a broad variety of services and information. However, due to the vast amount of information available and the open nature of the spoken queries, these applications still suffer from recognition errors. This paper explores the utilization of personalization features for the post-processing of recognition results in the form of n-best lists. Personalization is carried out from three different angles: short-term, long-term and Web-based, and a large variety of features are proposed for use in a log-linear classification framework. Experimental results on data obtained from a commercially deployed Voice Search system show that the combination of the proposed features leads to a substantial sentence error rate reduction. In addition, it is shown that personalization features which are very different in nature can successfully complement each other.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alex Acero</author>
<author>Neal Bernstein</author>
<author>Rob Chambers</author>
<author>Yun-Cheng Ju</author>
<author>Xiao Li</author>
<author>Julian Odell</author>
<author>Patrick Nguyen</author>
<author>Oliver Scholtz</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Live Search for Mobile: Web Services by Voice on the Cellphone. ICASSP</title>
<date>2008</date>
<volume>31</volume>
<location>Las Vegas, NV, USA.</location>
<contexts>
<context position="2315" citStr="Acero et al., 2008" startWordPosition="355" endWordPosition="358"> but on the user’s past queries, history of clicked results, and preferences. In (Teevan et al., 1996) it was observed that a significant 101 percent of the queries made by a user in a search engine are associated to a repeated search. Recommender systems like (Das et al., 2007) and (Dou et al., 2007) take advantage of this fact to refine the search results and improve the search experience. In this paper, we explore the use of personalization in the context of voice searches rather than web queries. Specifically, we focus on data from a multimodal cellphone-based business search application (Acero et al., 2008). In such an application, repeated queries can be a powerful tool for personalization. These can be classified into short and long-term repetitions. Short-term repetitions are typically caused by a speech recognition error, which produces an incorrect search result and makes the user repeat or reformulate the query. On the other hand, long-term repetitions, as in text-based search applications, occur when the user needs to access some information that was accessed previously, for example, the exact location of a pet clinic. This paper proposes several different user personalization methods for</context>
</contexts>
<marker>Acero, Bernstein, Chambers, Ju, Li, Odell, Nguyen, Scholtz, Zweig, 2008</marker>
<rawString>Alex Acero, Neal Bernstein, Rob Chambers, Yun-Cheng Ju, Xiao Li, Julian Odell, Patrick Nguyen, Oliver Scholtz and Geoffrey Zweig. 2008. Live Search for Mobile: Web Services by Voice on the Cellphone. ICASSP 2008, March 31 2008-April 4 2008. Las Vegas, NV, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
</authors>
<title>A Maximum Entropy Approach to Natural Language Processing. Computational Linguistics,</title>
<date>1996</date>
<volume>22</volume>
<issue>1</issue>
<pages>39--72</pages>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam L. Berger; Vincent J. Della Pietra; Stephen A. Della Pietra. 1996. A Maximum Entropy Approach to Natural Language Processing. Computational Linguistics, 1996. 22(1): p. 39-72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abhinandan Das</author>
<author>Mayur Datar</author>
<author>Ashutosh Garg</author>
</authors>
<title>Google News Personalization: Scalable Online Collaborative Filtering.</title>
<date>2007</date>
<booktitle>WWW 2007 / Track: Industrial Practice and Experience</booktitle>
<location>Banff, Alberta, Canada.</location>
<contexts>
<context position="1975" citStr="Das et al., 2007" startWordPosition="298" endWordPosition="301">ind specific content through the use of queries. In recent years, due to the vast amount of information available, there has been significant research on the use of recommender algorithms to select what information will be presented to the user. These systems try to predict what content a user may want based not only on the user’s query but on the user’s past queries, history of clicked results, and preferences. In (Teevan et al., 1996) it was observed that a significant 101 percent of the queries made by a user in a search engine are associated to a repeated search. Recommender systems like (Das et al., 2007) and (Dou et al., 2007) take advantage of this fact to refine the search results and improve the search experience. In this paper, we explore the use of personalization in the context of voice searches rather than web queries. Specifically, we focus on data from a multimodal cellphone-based business search application (Acero et al., 2008). In such an application, repeated queries can be a powerful tool for personalization. These can be classified into short and long-term repetitions. Short-term repetitions are typically caused by a speech recognition error, which produces an incorrect search r</context>
</contexts>
<marker>Das, Datar, Garg, 2007</marker>
<rawString>Abhinandan Das, Mayur Datar and Ashutosh Garg. 2007. Google News Personalization: Scalable Online Collaborative Filtering. WWW 2007 / Track: Industrial Practice and Experience May 8-12, 2007. Banff, Alberta, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Deligne</author>
<author>Satya Dharanipragada</author>
<author>Ramesh Gopinath</author>
<author>Benoit Maison</author>
<author>Peder Olsen</author>
<author>Harry Printz</author>
</authors>
<title>A robust high accuracy speech recognition system for mobile applications. Speech and Audio Processing,</title>
<date>2002</date>
<journal>IEEE Transactions on,</journal>
<volume>10</volume>
<pages>551--561</pages>
<contexts>
<context position="3290" citStr="Deligne et al., 2002" startWordPosition="499" endWordPosition="502">ions, as in text-based search applications, occur when the user needs to access some information that was accessed previously, for example, the exact location of a pet clinic. This paper proposes several different user personalization methods for increasing the recognition accuracy in Voice Search applications. The proposed personalization methods are based on extracting short-term, long-term and Web-based features from the user’s history. In recent years, other user personalization methods like deriving personalized pronunciations have proven successful in the context of mobile applications (Deligne et al., 2002). The rest of this paper is organized as follows: Section 2 describes the classification method used for rescoring the recognition hypotheses. Section 3 describes the proposed personalization methods. Section 4 describes the experiments carried out. Finally, Proceedings of NAACL HLT 2009: Short Papers, pages 101–104, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics conclusions from this work are drawn in section 5. 2 Rescoring procedure 2.1 Log linear classification Our work will proceed by using a log-linear classifier similar to the maximum entropy approach of (</context>
</contexts>
<marker>Deligne, Dharanipragada, Gopinath, Maison, Olsen, Printz, 2002</marker>
<rawString>Sabine Deligne, Satya Dharanipragada, Ramesh Gopinath, Benoit Maison, Peder Olsen and Harry Printz. 2002. A robust high accuracy speech recognition system for mobile applications. Speech and Audio Processing, IEEE Transactions on, Nov 2002, Volume: 10, Issue: 8, On page(s): 551- 561.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhicheng Dou</author>
<author>Ruihua Song</author>
<author>Ji-Rong Wen</author>
</authors>
<title>A large-scale evaluation and analysis ofpersonalized search strategies.</title>
<date>2007</date>
<booktitle>In WWW ’07: Proceedings of the 16th international conference on World Wide Web,</booktitle>
<pages>581--590</pages>
<publisher>ACM Press.</publisher>
<location>New York, NY, USA,</location>
<contexts>
<context position="1998" citStr="Dou et al., 2007" startWordPosition="303" endWordPosition="306">rough the use of queries. In recent years, due to the vast amount of information available, there has been significant research on the use of recommender algorithms to select what information will be presented to the user. These systems try to predict what content a user may want based not only on the user’s query but on the user’s past queries, history of clicked results, and preferences. In (Teevan et al., 1996) it was observed that a significant 101 percent of the queries made by a user in a search engine are associated to a repeated search. Recommender systems like (Das et al., 2007) and (Dou et al., 2007) take advantage of this fact to refine the search results and improve the search experience. In this paper, we explore the use of personalization in the context of voice searches rather than web queries. Specifically, we focus on data from a multimodal cellphone-based business search application (Acero et al., 2008). In such an application, repeated queries can be a powerful tool for personalization. These can be classified into short and long-term repetitions. Short-term repetitions are typically caused by a speech recognition error, which produces an incorrect search result and makes the use</context>
</contexts>
<marker>Dou, Song, Wen, 2007</marker>
<rawString>Zhicheng Dou, Ruihua Song, and Ji-Rong Wen. 2007. A large-scale evaluation and analysis ofpersonalized search strategies. In WWW ’07: Proceedings of the 16th international conference on World Wide Web, pages 581 - 590, New York, NY, USA, 2007. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>“http www live com</author>
</authors>
<date>2007</date>
<journal>Conditional Maximum-Entropy Training Tool</journal>
<pages>9--199826</pages>
<marker>com, 2007</marker>
<rawString>Live Search. “http://www.live.com,”. Milind Mahajan. 2007. Conditional Maximum-Entropy Training Tool http://research.microsoft.com/enus/downloads/9f199826-49d5-48b6-ba1bf623ecf36432/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaime Teevan</author>
<author>Eytan Adar</author>
<author>Rosie Jones</author>
<author>Michael A S Potts</author>
</authors>
<title>Information Re-Retrieval: Repeat Queries in Yahoos Logs.</title>
<date>2007</date>
<publisher>SIGIR,</publisher>
<marker>Teevan, Adar, Jones, Potts, 2007</marker>
<rawString>Jaime Teevan, Eytan Adar, Rosie Jones and Michael A. S. Potts. 2007. Information Re-Retrieval: Repeat Queries in Yahoos Logs. SIGIR, 2007.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>