<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.942510333333333">
Squibs and Discussions
Measuring Word Alignment Quality for Statistical
Machine Translation
</title>
<author confidence="0.995045">
Alexander Fraser*
</author>
<affiliation confidence="0.99847">
University of Southern California
</affiliation>
<author confidence="0.994633">
Daniel Marcu*
</author>
<affiliation confidence="0.99864">
University of Southern California
</affiliation>
<subsubsectionHeader confidence="0.449859">
Automatic word alignment plays a critical role in statistical machine translation. Unfortu-
</subsubsectionHeader>
<bodyText confidence="0.988339">
nately, the relationship between alignment quality and statistical machine translation perform-
ance has not been well understood. In the recent literature, the alignment task has frequently
been decoupled from the translation task and assumptions have been made about measuring
alignment quality for machine translation which, it turns out, are not justified. In particular,
none of the tens of papers published over the last five years has shown that significant decreases
in alignment error rate (AER) result in significant increases in translation performance. This
paper explains this state of affairs and presents steps towards measuring alignment quality in a
way which is predictive of statistical machine translation performance.
</bodyText>
<sectionHeader confidence="0.996123" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999717538461539">
Automatic word alignment (Brown et al. 1993) is a vital component of all statistical
machine translation (SMT) approaches. There were a number of research papers pre-
sented from 2000 to 2005 at ACL, NAACL, HLT, COLING, WPT03, WPT05, and so
forth, outlining techniques for attempting to increase word alignment quality. Despite
this high level of interest, none of these techniques has been shown to result in a large
gain in translation performance as measured by BLEU (Papineni et al. 2001) or any
other metric. We find this lack of correlation between previous word alignment quality
metrics and BLEU counterintuitive, because we and other researchers have measured
this correlation in the context of building SMT systems that have benefited from using
the BLEU metric in improving performance in open evaluations such as the NIST
evaluations.1
We confirm experimentally that previous metrics do not predict BLEU well and
develop a methodology for measuring alignment quality that is predictive of BLEU. We
</bodyText>
<footnote confidence="0.947222666666667">
* USC/ISI - Natural Language Group, 4676 Admiralty Way, Suite 1001, Marina del Rey, CA 90292-6601.
E-mail: fraser@isi.edu, marcu@isi.edu.
1 Because in our experiments we use BLEU to compare the performance of systems built using a common
framework where the only difference is the word alignment, we make no claims about the utility of BLEU
for measuring translation quality in absolute terms, nor its utility for comparing two completely different
MT systems.
</footnote>
<note confidence="0.864024">
© 2007 Association for Computational Linguistics
Computational Linguistics Volume 33, Number 3
</note>
<bodyText confidence="0.9561905">
also show that alignment error rate (AER) is not correctly derived from F-Measure and
is therefore unlikely to be useful as a metric.
</bodyText>
<sectionHeader confidence="0.932153" genericHeader="keywords">
2. Experimental Methodology
2.1 Data
</sectionHeader>
<bodyText confidence="0.999951891891892">
To build an SMT system we require a bitext and a word alignment of that bitext, as well
as language models built from target language data. In all of our experiments, we will
hold the bitext and target language resources constant, and only vary how we construct
the word alignment.
The gold standard word alignment sets we use have been manually annotated
using links between words showing translational correspondence. Links which must
be present in a hypothesized alignment are called Sure links. Some of the alignment
sets also have links which are not Sure links but are Possible links (Och and Ney 2003).
Possible links which are not Sure2 may be present but need not be present.
We evaluate the translation performance of SMT systems by translating a held-
out translation test set and measuring the BLEU score of our hypothesized translations
against one or more reference translations. We also have an additional held-out transla-
tion set, the development set, which is employed by the MT system to train the weights
of its log-linear model to maximize BLEU (Och 2003). We work with data sets for three
different language pairs, examining French to English, Arabic to English, and Romanian
to English translation tasks.
The training data for the French/English data set is taken from the LDC Canadian
Hansard data set, from which the word aligned data (presented in Och and Ney 2003)
was also taken. The English side of the bitext is 67.4 million words. We used a separate
Canadian Hansard data set (released by ISI) as the source of the translation test set and
development set. We evaluate two different tasks using this data, a medium task where
1/8 of the data (8.4 million English words) is used as the fixed bitext, and a large task
where all of the data is used as the fixed bitext. The 484 sentences in the gold standard
word alignments have 4,376 Sure links and 19,222 Possible links.
The Arabic/English training corpus is the data used for the NIST 2004 machine
translation evaluation.3 The English side of the bitext is 99.3 million words. The trans-
lation development set is the “NIST 2002 Dry Run,” and the test set is the “NIST 2003
evaluation set.” We have annotated gold standard alignments for 100 parallel sentences
using Sure links, following the Blinker guidelines (Melamed 1998), which call for Sure
links only (there were 2,154 Sure links). Here we also examine a medium task using 1/8
of the data (12.4 million English words) and a large task using all of the data.
The Romanian/English training data was used for the tasks on Romanian/English
alignment at WPT03 (Mihalcea and Pederson 2003) and WPT05 (Martin, Mihalcea,
and Pedersen 2005). We carefully removed two sections of news bitext to use as the
translation development and test sets. The English side of the training corpus is 964,000
words. The alignment set is the first 148 annotated sentences used for the 2003 task
(there were 3,181 Sure links).
</bodyText>
<footnote confidence="0.899574">
2 Sure links are by definition also Possible.
3 http://www.nist.gov/speech/tests/summaries/2004/mt04.htm.
</footnote>
<page confidence="0.994539">
294
</page>
<bodyText confidence="0.37024">
Fraser and Marcu Measuring Word Alignment Quality for Statistical Machine Translation
</bodyText>
<subsectionHeader confidence="0.999617">
2.2 Measuring Translation Performance Changes Caused By Alignment
</subsectionHeader>
<bodyText confidence="0.999980545454545">
In phrased-based SMT (Koehn, Och, and Marcu 2003) the knowledge sources which
vary with the word alignment are the phrase translation lexicon (which maps source
phrases to target phrases using counts from the word alignment) and some of the word
level translation parameters (sometimes called lexical smoothing). However, many
knowledge sources do not vary with the final word alignment, such as rescoring with
IBM Model 1, n-gram language models, and the length penalty. In our experiments,
we use a state-of-the-art phrase-based system, similar to Koehn, Och, and Marcu.
The weights of the different knowledge sources in the log-linear model used by our
system are trained using Maximum BLEU (Och 2003), which we run for 25 iterations
individually for each system. Two language models are used, one built using the target
language training data and the other built using additional news data.
</bodyText>
<subsectionHeader confidence="0.998337">
2.3 Generating Alignments of Varying Quality
</subsectionHeader>
<bodyText confidence="0.999992208333333">
We have observed in the past that generative models used for statistical word alignment
create alignments of increasing quality as they are exposed to more data. The intuition
behind this is simple; as more co-occurrences of source and target words are observed,
the word alignments are better. If we wish to increase the quality of a word alignment,
we allow the alignment process access to extra data, which is used only during the
alignment process and then removed. If we wish to decrease the quality of a word
alignment, we divide the bitext into pieces and align the pieces independently of one
another, finally concatenating the results together.
To generate word alignments we use GIZA++ (Och and Ney 2003), which im-
plements both the IBM Models of Brown et al. (1993) and the HMM model (Vogel,
Ney, and Tillmann 1996). We use Model 1, HMM, and Model 4, in that order. The
output of these models is an alignment of the bitext which projects one language to
another. GIZA++ is run end-to-end twice. In one case we project the source language
to the target language, and in the other we project the target language to the source
language. The output of GIZA++ is then post-processed using the three “symmetriza-
tion heuristics” described in Och and Ney (2003). We evaluate our approaches using
these heuristics because we would like to account for alignments generated in different
fashions. These three heuristics were used as the baselines in virtually all recent work
on automatic word alignment, and most of the best SMT systems use these techniques
as well.
When applying the union symmetrization heuristic, we take the transitive closure
of the bipartite graph created, which results in fully connected components indicating
translational correspondence.4 Each of the presented alignments are equivalent from a
translational correspondence perspective and the first two will be mapped to the third
</bodyText>
<footnote confidence="0.6513285">
4 We have no need to do this for the “refined” and “intersection” heuristics, because they only produce
alignments in which the components are fully connected.
</footnote>
<page confidence="0.988104">
295
</page>
<figure confidence="0.760567733333333">
Computational Linguistics Volume 33, Number 3
in order to ensure consistency between the number of links an alignment has and the
translational equivalences licensed by that alignment.
A B C
�������
���������������
A B C
��������������� �
������� � �
� �
� �
A B C
�X�/
D E D E D E
3. Word Alignment Quality Metrics
</figure>
<subsectionHeader confidence="0.997835">
3.1 Alignment Error Rate is Not a Useful Measure
</subsectionHeader>
<bodyText confidence="0.99904475">
We begin our study of metrics for word alignment quality by testing AER (Och and Ney
2003). AER requires a gold standard manually annotated set of Sure links and Possible
links (referred to as S and P). Given a hypothesized alignment consisting of the link set
A, three measures are defined:
</bodyText>
<equation confidence="0.999685">
Precision(A,P) = |P ∩ A |(1)
|A|
Recall(A,S) = |S ∩ A |(2)
|S|
AER(A,P,S) = 1 − |P ∩ A |+ |S ∩ A |(3)
|S |+ |A|
</equation>
<bodyText confidence="0.9983372">
In our graphs, we will present 1 − AER so that we have an accuracy measure.
We created alignments of varying quality for the medium French/English training
set. We broke the data into separate pieces corresponding to 1/16, 1/8, 1/4, and 1/2
of the original data to generate degraded alignments, and we used 2, 4, and 8 times
the original data to generate enhanced alignments. For the “fractional” alignments we
report the average AER of the pieces.5
The graph in Figure 1 shows the correlation of 1 − AER with BLEU. High correlation
would look like a line from the bottom left corner to the top right corner. As can be seen
by looking at the graph, there is low correlation between 1 − AER and the BLEU score. A
concise mathematical description of correlation is the coefficient of determination (r2),
5 For example, for 1/16, we perform 16 pairs of alignments, each of which includes the full gold standard
text, and another 16 pairs of alignments without the gold standard text. We then apply the
symmetrization heuristics to these pairs. We use the symmetrized alignments including the text from the
gold standard set to measure AER and we concatenate those not including the gold standard text to build
SMT systems for measuring BLEU.
</bodyText>
<page confidence="0.989506">
296
</page>
<figure confidence="0.534878">
Fraser and Marcu Measuring Word Alignment Quality for Statistical Machine Translation
</figure>
<figureCaption confidence="0.852803">
Figure 1
French 1 − AER versus BLEU, r2 = 0.16.
</figureCaption>
<bodyText confidence="0.998243857142857">
which is the square of the Pearson product-moment correlation coefficient (r). Here,
r2 = 0.16, which is low.
The correlation is low because of a significant shortcoming in the mathematical
formulation of AER, which to our knowledge has not been previously reported. Och
and Ney (2003) state that AER is derived from F-Measure. But AER does not share a
very important property of F-Measure, which is that unbalanced precision and recall
are penalized, where S C P (i.e., when we make the Sure versus Possible distinction).6
We will show this using an example.
We first define the measure “F-Measure with Sure and Possible” using Och and
Ney’s Precision and Recall formulae together with the standard F-Measure formula
(van Rijsbergen 1979). In the F-Measure formula (4) there is a parameter α which sets
the trade-off between Precision and Recall. When an equal trade-off is desired, α is set
to 0.5.
F-Measure with Sure and Possible(A, P, S, α) = α 1 (1−α) (4)
</bodyText>
<equation confidence="0.976676">
Precision(A,P) + Recall(A,S)
</equation>
<bodyText confidence="0.999001">
We compare two hypothesized alignments where |A|, the number of hypothesized
alignment links, is the same, for instance, |A |= 100. Let |S |= 100. In the first case, let
|P fl A |= 50 and |S fl A |= 50. Precision is 0.50 and Recall is 0.50. In the second case, let
|P fl A |= 75 and |S fl A |= 25. Precision is 0.75 and Recall is 0.25.
The basic property of F-Measure, if we set α equal to 0.5, is that unbalanced
precision and recall should be penalized. The first hypothesized alignment has an
F-Measure with Sure and Possible score of 0.50, whereas the second has a worse score,
0.375.
However, if we substitute the relevant values into the formula for AER (Equa-
tion (3)), we see that 1 − AER for both of the hypothesized alignments is 0.5. Therefore
AER does not share the property of F-Measure (with α = 0.5) that unbalanced precision
and recall are penalized. Because of this, it is possible to maximize AER by favoring
precision over recall, which can be done by simply guessing very few alignment links.
Unfortunately, when S C P, this leads to strong biases, which makes AER not useful as
a metric.
</bodyText>
<footnote confidence="0.548107">
6 Note that if S = P then AER reduces to balanced F-Measure.
</footnote>
<page confidence="0.89705">
297
</page>
<figure confidence="0.823051">
Computational Linguistics Volume 33, Number 3
</figure>
<figureCaption confidence="0.973836">
Figure 2
</figureCaption>
<bodyText confidence="0.939420333333333">
French F-Measure with Sure and Possible α = 0.5 versus BLEU, r2 = 0.20.
Goutte, Yamada, and Gaussier (2004) previously observed that AER could be un-
fairly optimized by using a bias toward precision which was unlikely to improve the
usefulness of the alignments. Possible problems with AER were discussed at WPT 2003
and WPT 2005.
Examining the graph in Figure 2, we see that F-Measure with Sure and Possible has
some predictive power for the data points generated using a single heuristic, but the
overall correlation is still low, r2 = 0.20. We need a measure that predicts BLEU without
having a dependency on the way the alignments are generated.
</bodyText>
<subsectionHeader confidence="0.997083">
3.2 Balanced F-Measure is Better, but Still Inadequate
</subsectionHeader>
<bodyText confidence="0.998242166666667">
We wondered whether the low correlation was caused by the Sure and Possible dis-
tinction. We reannotated the first 110 sentences of the French test set using the Blinker
guidelines (there were 2,292 Sure links). We define F-Measure without the Sure versus
Possible distinction (i.e., all links are Sure) in Equation (5), and set α = 0.5. This measure
has been extensively used with other word alignment test sets. Figure 3 shows the
results. Correlation is higher: r2 = 0.67.
</bodyText>
<equation confidence="0.9998125">
F-Measure(A, S, α) = α 1 (1−α) (5)
Precision(A,S) + Recall(A,S)
</equation>
<subsectionHeader confidence="0.997856">
3.3 Varying the Trade-Off Between Precision and Recall Works Well
</subsectionHeader>
<bodyText confidence="0.9976918">
We then hypothesized that the trade-off between precision and recall is important. This
is controlled in both formulae by the constant α. We search α = 0.1, 0.2, ..., 0.9. The best
results are: α = 0.1 for the original annotation annotated with Sure and Possible (see
Figure 4), and α = 0.4 for the first 110 sentences as annotated by us (see Figure 5).7 The
relevant r2 scores were 0.80 and 0.85, respectively. With a good α setting, we are able
</bodyText>
<footnote confidence="0.93037">
7 We also checked the first 110 sentences using the original annotation to ensure that the differences
observed were not an effect of restricting our annotation to these sentences.
</footnote>
<page confidence="0.993141">
298
</page>
<figure confidence="0.740660666666667">
Fraser and Marcu Measuring Word Alignment Quality for Statistical Machine Translation
Figure 3
French F-Measure α = 0.5 versus BLEU, r2 = 0.67.
</figure>
<figureCaption confidence="0.90594">
Figure 4
French F-Measure with Sure and Possible α = 0.1 versus BLEU, r2 = 0.80.
Figure 5
</figureCaption>
<bodyText confidence="0.9767086">
French F-Measure α = 0.4 versus BLEU, r2 = 0.85.
to predict the machine translation results reasonably well. For the original annotation,
recall is very highly weighted, whereas for our annotation, recall is still more important
than precision.8 Our results also suggest that better correlation will be achieved when
using Sure-only annotation than with Sure and Possible annotation.
</bodyText>
<footnote confidence="0.8045445">
8 α less than 0.5 weights recall higher, whereas α greater than 0.5 weights precision higher; see the
F-Measure formulae.
</footnote>
<page confidence="0.995404">
299
</page>
<figure confidence="0.745111">
Computational Linguistics Volume 33, Number 3
Figure 6
Arabic F-Measure α = 0.1 versus BLEU, r2 = 0.93.
</figure>
<figureCaption confidence="0.999481">
Figure 7
</figureCaption>
<bodyText confidence="0.971950857142857">
Large French F-Measure α = 0.4 (110 sentences) versus BLEU, r2 = 0.64.
We then tried the medium Arabic training set. Results are shown in Figure 6, the
best setting of α = 0.1, and r2 = 0.93. F-Measure is effective in predicting machine
translation performance for this set.
We also tried the larger tasks, where we can only decrease alignment quality, as
we have no additional data. For the large French/English corpus the best results are
with α = 0.2 for the original annotation of 484 sentences and α = 0.4 for the new
</bodyText>
<figureCaption confidence="0.765618">
Figure 8
</figureCaption>
<bodyText confidence="0.523146">
Large Arabic F-Measure α = 0.1 (100 sentences) versus BLEU, r2 = 0.90.
</bodyText>
<page confidence="0.970961">
300
</page>
<bodyText confidence="0.968416375">
Fraser and Marcu Measuring Word Alignment Quality for Statistical Machine Translation
annotation of 110 sentences with only Sure links (see Figure 7). Relevant r2 scores were
0.62 and 0.64, respectively. Disappointingly, our measures are not able to fully explain
MT performance for the large French/English task.
For the large Arabic/English corpus, the results were better: the best correlation
was at α = 0.1, for which r2 = 0.90 (see Figure 8). We can predict MT performance for
this set. It is worth noting that the Arabic/English translation task and data set has
been tested in conjunction with our translation system over a long period, but the
French/English translation task and data has not. As a result, there may be hidden
factors that affect the performance of our MT system, which only appear in conjunction
with the large French/English task.
One well-studied task on a smaller data set is the Romanian/English shared word
alignment task from the Workshop on Parallel Text at ACL 2005 (Martin, Mihalcea, and
Pedersen 2005). We only decreased alignment quality and used 5 data points for each
symmetrization heuristic due to the small bitext. The best setting of α was α = 0.2, for
which r2 = 0.94, showing that F-Measure is again effective in predicting BLEU.
</bodyText>
<sectionHeader confidence="0.986208" genericHeader="conclusions">
4. Conclusion
</sectionHeader>
<bodyText confidence="0.999975">
We have presented an empirical study of the use of simple evaluation metrics based on
gold standard alignment of a small number of sentences to predict machine translation
performance. Based on our experiments we can now draw the following conclusions:
</bodyText>
<listItem confidence="0.9927798125">
1. When S ⊂ P, AER does not share the important property of F-Measure
that unequal precision and recall are penalized, making it easy to obtain
good AER scores by simply guessing fewer alignment links. As a result
AER is a misleading metric that should no longer be used.
2. Good correlation was obtained for the medium French and Arabic data
sets, the large Arabic data set, and the small Romanian data set. We have
explained most of the effect of alignment quality on these sets, and if we
are given the F-Measure of a hypothesized word alignment for the bitext,
we can make a reasonable prediction as to what the resulting BLEU score
will be.
3. We have only partially explained the effect of alignment quality on BLEU
for the large French data set, and further investigation is warranted.
4. We recommend using the Blinker guidelines as a starting point for new
alignment annotation efforts, and that Sure-only annotation be used. If a
larger gold standard is available and was already annotated using the Sure
versus Possible distinction, this is likely to have only slightly worse results.
</listItem>
<bodyText confidence="0.999628333333333">
Although we have addressed measuring alignment quality for phrasal SMT, similar
work is now required to see how to measure alignment quality for other settings of
machine translation and for other tasks. For an evaluation campaign the organizers
should pick a specific task, such as improving phrasal SMT, and calculate an appropriate
α to be used. Individual researchers working on the same phrasal SMT tasks as those
reported here (or on very similar tasks) could use the values of α we calculated.
Our work invalidates some of the conclusions of recent alignment work which
presented only evaluations based on metrics like AER or balanced F-Measure, and
explains the lack of correlation in the few works which presented both such a metric
</bodyText>
<page confidence="0.994589">
301
</page>
<note confidence="0.593395">
Computational Linguistics Volume 33, Number 3
</note>
<bodyText confidence="0.999617">
and final MT results. A good example of the former are our own results (Fraser and
Marcu 2005). The work presented there had the highest balanced F-Measure scores for
the Romanian/English WPT05 shared task, but based on the findings here it is possible
that a different algorithm tuned for the correct criterion would have had better MT
performance. Other work includes many papers working on alignment models where
words are allowed to participate in a maximum of one link. These models generally
have higher precision and lower recall than IBM Model 4 symmetrized using the “Re-
fined” or “Union” heuristics. Recall that in Section 3.1 we showed that AER is broken in
a way that favors precision. It is therefore likely that the results reported in these papers
are affected by the AER bias and that the corresponding improvements in AER score do
not correlate with increases in phrasal SMT performance.
We suggest comparing alignment algorithms by measuring performance in an
identified final task such as machine translation. F-Measure with an appropriate setting
of α will be useful during the development process of new alignment models, or as a
maximization criterion for discriminative training of alignment models (Cherry and Lin
2003; Ayan, Dorr, and Monz 2005; Ittycheriah and Roukos 2005; Liu, Liu, and Lin 2005;
Fraser and Marcu 2006; Lacoste-Julien et al. 2006; Moore, Yih, and Bode 2006).
</bodyText>
<sectionHeader confidence="0.997361" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9753332">
This work was supported by DARPA-ITO
grant NN66001-00-1-9814, NSF grant
IIS-0326276, and the DARPA GALE Program.
We thank USC High Performance
Computing &amp; Communications.
</bodyText>
<sectionHeader confidence="0.998604" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997424833333333">
Ayan, Necip Fazil, Bonnie J. Dorr, and
Christof Monz. 2005. Neuralign:
Combining word alignments using neural
networks. In Proceedings of HLT-EMNLP,
pages 65–72, Vancouver, Canada.
Brown, Peter F., Stephen A. Della Pietra,
Vincent J. Della Pietra, and R. L. Mercer.
1993. The mathematics of statistical
machine translation: Parameter
estimation. Computational Linguistics,
19(2):263–311.
Cherry, Colin and Dekang Lin. 2003. A
probability model to improve word
alignment. In Proceedings of ACL,
pages 88–95, Sapporo, Japan.
Fraser, Alexander and Daniel Marcu. 2005.
Isi’s participation in the Romanian-English
alignment task. In Proceedings of the ACL
Workshop on Building and Using Parallel
Texts, pages 91–94, Ann Arbor, MI.
Fraser, Alexander and Daniel Marcu. 2006.
Semi-supervised training for word
alignment. In Proceedings of COLING-ACL,
pages 769–776, Sydney, Australia.
Goutte, Cyril, Kenji Yamada, and Eric
Gaussier. 2004. Aligning words using
matrix factorisation. In Proceedings of ACL,
pages 502–509, Barcelona, Spain.
Ittycheriah, Abraham and Salim Roukos.
2005. A maximum entropy word aligner
for Arabic-English machine translation. In
Proceedings of HLT-EMNLP, pages 89–96,
Vancouver, Canada.
Koehn, Philipp, Franz J. Och, and Daniel
Marcu. 2003. Statistical phrase-based
translation. In Proceedings of HLT-NAACL,
pages 127–133, Edmonton, Canada.
Lacoste-Julien, Simon, Dan Klein,
Ben Taskar, and Michael Jordan.
2006. Word alignment via quadratic
assignment. In Proceedings
of HLT-NAACL, pages 112–119,
New York, NY.
Liu, Yang, Qun Liu, and Shouxun Lin.
2005. Log-linear models for word
alignment. In Proceedings of ACL,
pages 459–466, Ann Arbor, MI.
Martin, Joel, Rada Mihalcea, and
Ted Pedersen. 2005. Word
alignment for languages with
scarce resources. In Proceedings of the
ACL Workshop on Building and
Using Parallel Texts, pages 65–74,
Ann Arbor, MI.
Melamed, I. Dan. 1998. Manual annotation
of translational equivalence: The
Blinker project. Technical Report 98–07,
Institute for Research in Cognitive
Science, University of Pennsylvania,
Philadelphia, PA.
Mihalcea, Rada and Ted Pederson. 2003.
An evaluation exercise for word
alignment. In Proceedings of the
HLT-NAACL Workshop on Building
and Using Parallel Texts, pages 1–10,
Edmonton, Canada.
</reference>
<page confidence="0.971209">
302
</page>
<note confidence="0.347767">
Fraser and Marcu Measuring Word Alignment Quality for Statistical Machine Translation
</note>
<reference confidence="0.9999257">
Moore, Robert C., Wen-Tau Yih, and Andreas
Bode. 2006. Improved discriminative
bilingual word alignment. In Proceedings
of COLING-ACL, pages 513–520,
Sydney, Australia.
Och, Franz J. 2003. Minimum error rate
training in statistical machine translation.
In Proceedings of ACL, pages 160–167,
Sapporo, Japan.
Och, Franz J. and Hermann Ney. 2003.
A systematic comparison of various
statistical alignment models.
Computational Linguistics, 29(1):19–51.
Papineni, Kishore A., Salim Roukos,
Todd Ward, and Wei-Jing Zhu.
2001. BLEU: A method for
automatic evaluation of machine
translation. Technical Report
RC22176 (W0109-022), IBM Research
Division, Thomas J. Watson
Research Center, Yorktown
Heights, NY.
van Rijsbergen, Keith. 1979. Information
Retrieval. Butterworth-Heinemann,
Newton, MA.
Vogel, Stephan, Hermann Ney, and
Christoph Tillmann. 1996. HMM-based
word alignment in statistical translation.
In Proceedings of COLING, pages 836–841,
Copenhagen, Denmark.
</reference>
<page confidence="0.999469">
303
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.558324">
<title confidence="0.853866333333333">Squibs and Discussions Measuring Word Alignment Quality for Statistical Machine Translation</title>
<affiliation confidence="0.994879">University of Southern California University of Southern California</affiliation>
<abstract confidence="0.992809777777778">Automatic word alignment plays a critical role in statistical machine translation. Unfortunately, the relationship between alignment quality and statistical machine translation performance has not been well understood. In the recent literature, the alignment task has frequently been decoupled from the translation task and assumptions have been made about measuring alignment quality for machine translation which, it turns out, are not justified. In particular, none of the tens of papers published over the last five years has shown that significant decreases in alignment error rate (AER) result in significant increases in translation performance. This paper explains this state of affairs and presents steps towards measuring alignment quality in a way which is predictive of statistical machine translation performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Necip Fazil Ayan</author>
<author>Bonnie J Dorr</author>
<author>Christof Monz</author>
</authors>
<title>Neuralign: Combining word alignments using neural networks.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP,</booktitle>
<pages>65--72</pages>
<location>Vancouver, Canada.</location>
<marker>Ayan, Dorr, Monz, 2005</marker>
<rawString>Ayan, Necip Fazil, Bonnie J. Dorr, and Christof Monz. 2005. Neuralign: Combining word alignments using neural networks. In Proceedings of HLT-EMNLP, pages 65–72, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1079" citStr="Brown et al. 1993" startWordPosition="147" endWordPosition="150">requently been decoupled from the translation task and assumptions have been made about measuring alignment quality for machine translation which, it turns out, are not justified. In particular, none of the tens of papers published over the last five years has shown that significant decreases in alignment error rate (AER) result in significant increases in translation performance. This paper explains this state of affairs and presents steps towards measuring alignment quality in a way which is predictive of statistical machine translation performance. 1. Introduction Automatic word alignment (Brown et al. 1993) is a vital component of all statistical machine translation (SMT) approaches. There were a number of research papers presented from 2000 to 2005 at ACL, NAACL, HLT, COLING, WPT03, WPT05, and so forth, outlining techniques for attempting to increase word alignment quality. Despite this high level of interest, none of these techniques has been shown to result in a large gain in translation performance as measured by BLEU (Papineni et al. 2001) or any other metric. We find this lack of correlation between previous word alignment quality metrics and BLEU counterintuitive, because we and other res</context>
<context position="7684" citStr="Brown et al. (1993)" startWordPosition="1216" endWordPosition="1219">data. The intuition behind this is simple; as more co-occurrences of source and target words are observed, the word alignments are better. If we wish to increase the quality of a word alignment, we allow the alignment process access to extra data, which is used only during the alignment process and then removed. If we wish to decrease the quality of a word alignment, we divide the bitext into pieces and align the pieces independently of one another, finally concatenating the results together. To generate word alignments we use GIZA++ (Och and Ney 2003), which implements both the IBM Models of Brown et al. (1993) and the HMM model (Vogel, Ney, and Tillmann 1996). We use Model 1, HMM, and Model 4, in that order. The output of these models is an alignment of the bitext which projects one language to another. GIZA++ is run end-to-end twice. In one case we project the source language to the target language, and in the other we project the target language to the source language. The output of GIZA++ is then post-processed using the three “symmetrization heuristics” described in Och and Ney (2003). We evaluate our approaches using these heuristics because we would like to account for alignments generated in</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Brown, Peter F., Stephen A. Della Pietra, Vincent J. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>Dekang Lin</author>
</authors>
<title>A probability model to improve word alignment.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>88--95</pages>
<location>Sapporo, Japan.</location>
<marker>Cherry, Lin, 2003</marker>
<rawString>Cherry, Colin and Dekang Lin. 2003. A probability model to improve word alignment. In Proceedings of ACL, pages 88–95, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Fraser</author>
<author>Daniel Marcu</author>
</authors>
<title>Isi’s participation in the Romanian-English alignment task.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Building and Using Parallel Texts,</booktitle>
<pages>91--94</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="20276" citStr="Fraser and Marcu 2005" startWordPosition="3371" endWordPosition="3374">ific task, such as improving phrasal SMT, and calculate an appropriate α to be used. Individual researchers working on the same phrasal SMT tasks as those reported here (or on very similar tasks) could use the values of α we calculated. Our work invalidates some of the conclusions of recent alignment work which presented only evaluations based on metrics like AER or balanced F-Measure, and explains the lack of correlation in the few works which presented both such a metric 301 Computational Linguistics Volume 33, Number 3 and final MT results. A good example of the former are our own results (Fraser and Marcu 2005). The work presented there had the highest balanced F-Measure scores for the Romanian/English WPT05 shared task, but based on the findings here it is possible that a different algorithm tuned for the correct criterion would have had better MT performance. Other work includes many papers working on alignment models where words are allowed to participate in a maximum of one link. These models generally have higher precision and lower recall than IBM Model 4 symmetrized using the “Refined” or “Union” heuristics. Recall that in Section 3.1 we showed that AER is broken in a way that favors precisio</context>
</contexts>
<marker>Fraser, Marcu, 2005</marker>
<rawString>Fraser, Alexander and Daniel Marcu. 2005. Isi’s participation in the Romanian-English alignment task. In Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 91–94, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Fraser</author>
<author>Daniel Marcu</author>
</authors>
<title>Semi-supervised training for word alignment.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL,</booktitle>
<pages>769--776</pages>
<location>Sydney, Australia.</location>
<marker>Fraser, Marcu, 2006</marker>
<rawString>Fraser, Alexander and Daniel Marcu. 2006. Semi-supervised training for word alignment. In Proceedings of COLING-ACL, pages 769–776, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cyril Goutte</author>
<author>Kenji Yamada</author>
<author>Eric Gaussier</author>
</authors>
<title>Aligning words using matrix factorisation.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>502--509</pages>
<location>Barcelona,</location>
<marker>Goutte, Yamada, Gaussier, 2004</marker>
<rawString>Goutte, Cyril, Kenji Yamada, and Eric Gaussier. 2004. Aligning words using matrix factorisation. In Proceedings of ACL, pages 502–509, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abraham Ittycheriah</author>
<author>Salim Roukos</author>
</authors>
<title>A maximum entropy word aligner for Arabic-English machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP,</booktitle>
<pages>89--96</pages>
<location>Vancouver, Canada.</location>
<marker>Ittycheriah, Roukos, 2005</marker>
<rawString>Ittycheriah, Abraham and Salim Roukos. 2005. A maximum entropy word aligner for Arabic-English machine translation. In Proceedings of HLT-EMNLP, pages 89–96, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz J Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>127--133</pages>
<location>Edmonton, Canada.</location>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Koehn, Philipp, Franz J. Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of HLT-NAACL, pages 127–133, Edmonton, Canada.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Simon Lacoste-Julien</author>
<author>Dan Klein</author>
<author>Ben Taskar</author>
<author>Michael Jordan</author>
</authors>
<marker>Lacoste-Julien, Klein, Taskar, Jordan, </marker>
<rawString>Lacoste-Julien, Simon, Dan Klein, Ben Taskar, and Michael Jordan.</rawString>
</citation>
<citation valid="true">
<title>Word alignment via quadratic assignment.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>112--119</pages>
<marker>2006</marker>
<rawString>2006. Word alignment via quadratic assignment. In Proceedings of HLT-NAACL, pages 112–119,</rawString>
</citation>
<citation valid="false">
<location>New York, NY.</location>
<marker></marker>
<rawString>New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Log-linear models for word alignment.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>459--466</pages>
<location>Ann Arbor, MI.</location>
<marker>Liu, Liu, Lin, 2005</marker>
<rawString>Liu, Yang, Qun Liu, and Shouxun Lin. 2005. Log-linear models for word alignment. In Proceedings of ACL, pages 459–466, Ann Arbor, MI.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Joel Martin</author>
</authors>
<location>Rada Mihalcea, and</location>
<marker>Martin, </marker>
<rawString>Martin, Joel, Rada Mihalcea, and</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
</authors>
<title>Word alignment for languages with scarce resources.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Building and Using Parallel Texts,</booktitle>
<pages>65--74</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="5436" citStr="Pedersen 2005" startWordPosition="864" endWordPosition="865">ds. The translation development set is the “NIST 2002 Dry Run,” and the test set is the “NIST 2003 evaluation set.” We have annotated gold standard alignments for 100 parallel sentences using Sure links, following the Blinker guidelines (Melamed 1998), which call for Sure links only (there were 2,154 Sure links). Here we also examine a medium task using 1/8 of the data (12.4 million English words) and a large task using all of the data. The Romanian/English training data was used for the tasks on Romanian/English alignment at WPT03 (Mihalcea and Pederson 2003) and WPT05 (Martin, Mihalcea, and Pedersen 2005). We carefully removed two sections of news bitext to use as the translation development and test sets. The English side of the training corpus is 964,000 words. The alignment set is the first 148 annotated sentences used for the 2003 task (there were 3,181 Sure links). 2 Sure links are by definition also Possible. 3 http://www.nist.gov/speech/tests/summaries/2004/mt04.htm. 294 Fraser and Marcu Measuring Word Alignment Quality for Statistical Machine Translation 2.2 Measuring Translation Performance Changes Caused By Alignment In phrased-based SMT (Koehn, Och, and Marcu 2003) the knowledge sou</context>
<context position="17789" citStr="Pedersen 2005" startWordPosition="2955" endWordPosition="2956">2 = 0.90 (see Figure 8). We can predict MT performance for this set. It is worth noting that the Arabic/English translation task and data set has been tested in conjunction with our translation system over a long period, but the French/English translation task and data has not. As a result, there may be hidden factors that affect the performance of our MT system, which only appear in conjunction with the large French/English task. One well-studied task on a smaller data set is the Romanian/English shared word alignment task from the Workshop on Parallel Text at ACL 2005 (Martin, Mihalcea, and Pedersen 2005). We only decreased alignment quality and used 5 data points for each symmetrization heuristic due to the small bitext. The best setting of α was α = 0.2, for which r2 = 0.94, showing that F-Measure is again effective in predicting BLEU. 4. Conclusion We have presented an empirical study of the use of simple evaluation metrics based on gold standard alignment of a small number of sentences to predict machine translation performance. Based on our experiments we can now draw the following conclusions: 1. When S ⊂ P, AER does not share the important property of F-Measure that unequal precision an</context>
</contexts>
<marker>Pedersen, 2005</marker>
<rawString>Ted Pedersen. 2005. Word alignment for languages with scarce resources. In Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 65–74, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
</authors>
<title>Manual annotation of translational equivalence: The Blinker project.</title>
<date>1998</date>
<tech>Technical Report 98–07,</tech>
<contexts>
<context position="5073" citStr="Melamed 1998" startWordPosition="803" endWordPosition="804"> words) is used as the fixed bitext, and a large task where all of the data is used as the fixed bitext. The 484 sentences in the gold standard word alignments have 4,376 Sure links and 19,222 Possible links. The Arabic/English training corpus is the data used for the NIST 2004 machine translation evaluation.3 The English side of the bitext is 99.3 million words. The translation development set is the “NIST 2002 Dry Run,” and the test set is the “NIST 2003 evaluation set.” We have annotated gold standard alignments for 100 parallel sentences using Sure links, following the Blinker guidelines (Melamed 1998), which call for Sure links only (there were 2,154 Sure links). Here we also examine a medium task using 1/8 of the data (12.4 million English words) and a large task using all of the data. The Romanian/English training data was used for the tasks on Romanian/English alignment at WPT03 (Mihalcea and Pederson 2003) and WPT05 (Martin, Mihalcea, and Pedersen 2005). We carefully removed two sections of news bitext to use as the translation development and test sets. The English side of the training corpus is 964,000 words. The alignment set is the first 148 annotated sentences used for the 2003 ta</context>
</contexts>
<marker>Melamed, 1998</marker>
<rawString>Melamed, I. Dan. 1998. Manual annotation of translational equivalence: The Blinker project. Technical Report 98–07,</rawString>
</citation>
<citation valid="false">
<institution>Institute for Research in Cognitive Science, University of Pennsylvania,</institution>
<location>Philadelphia, PA.</location>
<marker></marker>
<rawString>Institute for Research in Cognitive Science, University of Pennsylvania, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Ted Pederson</author>
</authors>
<title>An evaluation exercise for word alignment.</title>
<date>2003</date>
<booktitle>In Proceedings of the HLT-NAACL Workshop on Building and Using Parallel Texts,</booktitle>
<pages>1--10</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="5388" citStr="Mihalcea and Pederson 2003" startWordPosition="855" endWordPosition="858">aluation.3 The English side of the bitext is 99.3 million words. The translation development set is the “NIST 2002 Dry Run,” and the test set is the “NIST 2003 evaluation set.” We have annotated gold standard alignments for 100 parallel sentences using Sure links, following the Blinker guidelines (Melamed 1998), which call for Sure links only (there were 2,154 Sure links). Here we also examine a medium task using 1/8 of the data (12.4 million English words) and a large task using all of the data. The Romanian/English training data was used for the tasks on Romanian/English alignment at WPT03 (Mihalcea and Pederson 2003) and WPT05 (Martin, Mihalcea, and Pedersen 2005). We carefully removed two sections of news bitext to use as the translation development and test sets. The English side of the training corpus is 964,000 words. The alignment set is the first 148 annotated sentences used for the 2003 task (there were 3,181 Sure links). 2 Sure links are by definition also Possible. 3 http://www.nist.gov/speech/tests/summaries/2004/mt04.htm. 294 Fraser and Marcu Measuring Word Alignment Quality for Statistical Machine Translation 2.2 Measuring Translation Performance Changes Caused By Alignment In phrased-based SM</context>
</contexts>
<marker>Mihalcea, Pederson, 2003</marker>
<rawString>Mihalcea, Rada and Ted Pederson. 2003. An evaluation exercise for word alignment. In Proceedings of the HLT-NAACL Workshop on Building and Using Parallel Texts, pages 1–10, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
<author>Wen-Tau Yih</author>
<author>Andreas Bode</author>
</authors>
<title>Improved discriminative bilingual word alignment.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL,</booktitle>
<pages>513--520</pages>
<location>Sydney, Australia.</location>
<marker>Moore, Yih, Bode, 2006</marker>
<rawString>Moore, Robert C., Wen-Tau Yih, and Andreas Bode. 2006. Improved discriminative bilingual word alignment. In Proceedings of COLING-ACL, pages 513–520, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="3843" citStr="Och 2003" startWordPosition="593" endWordPosition="594">d alignment are called Sure links. Some of the alignment sets also have links which are not Sure links but are Possible links (Och and Ney 2003). Possible links which are not Sure2 may be present but need not be present. We evaluate the translation performance of SMT systems by translating a heldout translation test set and measuring the BLEU score of our hypothesized translations against one or more reference translations. We also have an additional held-out translation set, the development set, which is employed by the MT system to train the weights of its log-linear model to maximize BLEU (Och 2003). We work with data sets for three different language pairs, examining French to English, Arabic to English, and Romanian to English translation tasks. The training data for the French/English data set is taken from the LDC Canadian Hansard data set, from which the word aligned data (presented in Och and Ney 2003) was also taken. The English side of the bitext is 67.4 million words. We used a separate Canadian Hansard data set (released by ISI) as the source of the translation test set and development set. We evaluate two different tasks using this data, a medium task where 1/8 of the data (8.</context>
<context position="6673" citStr="Och 2003" startWordPosition="1050" endWordPosition="1051">d alignment are the phrase translation lexicon (which maps source phrases to target phrases using counts from the word alignment) and some of the word level translation parameters (sometimes called lexical smoothing). However, many knowledge sources do not vary with the final word alignment, such as rescoring with IBM Model 1, n-gram language models, and the length penalty. In our experiments, we use a state-of-the-art phrase-based system, similar to Koehn, Och, and Marcu. The weights of the different knowledge sources in the log-linear model used by our system are trained using Maximum BLEU (Och 2003), which we run for 25 iterations individually for each system. Two language models are used, one built using the target language training data and the other built using additional news data. 2.3 Generating Alignments of Varying Quality We have observed in the past that generative models used for statistical word alignment create alignments of increasing quality as they are exposed to more data. The intuition behind this is simple; as more co-occurrences of source and target words are observed, the word alignments are better. If we wish to increase the quality of a word alignment, we allow the </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Och, Franz J. 2003. Minimum error rate training in statistical machine translation. In Proceedings of ACL, pages 160–167, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="3378" citStr="Och and Ney 2003" startWordPosition="514" endWordPosition="517">2.1 Data To build an SMT system we require a bitext and a word alignment of that bitext, as well as language models built from target language data. In all of our experiments, we will hold the bitext and target language resources constant, and only vary how we construct the word alignment. The gold standard word alignment sets we use have been manually annotated using links between words showing translational correspondence. Links which must be present in a hypothesized alignment are called Sure links. Some of the alignment sets also have links which are not Sure links but are Possible links (Och and Ney 2003). Possible links which are not Sure2 may be present but need not be present. We evaluate the translation performance of SMT systems by translating a heldout translation test set and measuring the BLEU score of our hypothesized translations against one or more reference translations. We also have an additional held-out translation set, the development set, which is employed by the MT system to train the weights of its log-linear model to maximize BLEU (Och 2003). We work with data sets for three different language pairs, examining French to English, Arabic to English, and Romanian to English tr</context>
<context position="7623" citStr="Och and Ney 2003" startWordPosition="1204" endWordPosition="1207">ignments of increasing quality as they are exposed to more data. The intuition behind this is simple; as more co-occurrences of source and target words are observed, the word alignments are better. If we wish to increase the quality of a word alignment, we allow the alignment process access to extra data, which is used only during the alignment process and then removed. If we wish to decrease the quality of a word alignment, we divide the bitext into pieces and align the pieces independently of one another, finally concatenating the results together. To generate word alignments we use GIZA++ (Och and Ney 2003), which implements both the IBM Models of Brown et al. (1993) and the HMM model (Vogel, Ney, and Tillmann 1996). We use Model 1, HMM, and Model 4, in that order. The output of these models is an alignment of the bitext which projects one language to another. GIZA++ is run end-to-end twice. In one case we project the source language to the target language, and in the other we project the target language to the source language. The output of GIZA++ is then post-processed using the three “symmetrization heuristics” described in Och and Ney (2003). We evaluate our approaches using these heuristics</context>
<context position="9436" citStr="Och and Ney 2003" startWordPosition="1509" endWordPosition="1512">We have no need to do this for the “refined” and “intersection” heuristics, because they only produce alignments in which the components are fully connected. 295 Computational Linguistics Volume 33, Number 3 in order to ensure consistency between the number of links an alignment has and the translational equivalences licensed by that alignment. A B C ������� ��������������� A B C ��������������� � ������� � � � � � � A B C �X�/ D E D E D E 3. Word Alignment Quality Metrics 3.1 Alignment Error Rate is Not a Useful Measure We begin our study of metrics for word alignment quality by testing AER (Och and Ney 2003). AER requires a gold standard manually annotated set of Sure links and Possible links (referred to as S and P). Given a hypothesized alignment consisting of the link set A, three measures are defined: Precision(A,P) = |P ∩ A |(1) |A| Recall(A,S) = |S ∩ A |(2) |S| AER(A,P,S) = 1 − |P ∩ A |+ |S ∩ A |(3) |S |+ |A| In our graphs, we will present 1 − AER so that we have an accuracy measure. We created alignments of varying quality for the medium French/English training set. We broke the data into separate pieces corresponding to 1/16, 1/8, 1/4, and 1/2 of the original data to generate degraded ali</context>
<context position="11412" citStr="Och and Ney (2003)" startWordPosition="1855" endWordPosition="1858">use the symmetrized alignments including the text from the gold standard set to measure AER and we concatenate those not including the gold standard text to build SMT systems for measuring BLEU. 296 Fraser and Marcu Measuring Word Alignment Quality for Statistical Machine Translation Figure 1 French 1 − AER versus BLEU, r2 = 0.16. which is the square of the Pearson product-moment correlation coefficient (r). Here, r2 = 0.16, which is low. The correlation is low because of a significant shortcoming in the mathematical formulation of AER, which to our knowledge has not been previously reported. Och and Ney (2003) state that AER is derived from F-Measure. But AER does not share a very important property of F-Measure, which is that unbalanced precision and recall are penalized, where S C P (i.e., when we make the Sure versus Possible distinction).6 We will show this using an example. We first define the measure “F-Measure with Sure and Possible” using Och and Ney’s Precision and Recall formulae together with the standard F-Measure formula (van Rijsbergen 1979). In the F-Measure formula (4) there is a parameter α which sets the trade-off between Precision and Recall. When an equal trade-off is desired, α</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Och, Franz J. and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Kishore A Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<marker>Papineni, Roukos, Ward, Zhu, </marker>
<rawString>Papineni, Kishore A., Salim Roukos, Todd Ward, and Wei-Jing Zhu.</rawString>
</citation>
<citation valid="true">
<title>BLEU: A method for automatic evaluation of machine translation.</title>
<date>2001</date>
<journal>IBM Research Division, Thomas J. Watson Research</journal>
<tech>Technical Report RC22176 (W0109-022),</tech>
<location>Center, Yorktown Heights, NY.</location>
<marker>2001</marker>
<rawString>2001. BLEU: A method for automatic evaluation of machine translation. Technical Report RC22176 (W0109-022), IBM Research Division, Thomas J. Watson Research Center, Yorktown Heights, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keith van Rijsbergen</author>
</authors>
<title>Information Retrieval.</title>
<date>1979</date>
<publisher>Butterworth-Heinemann,</publisher>
<location>Newton, MA.</location>
<marker>van Rijsbergen, 1979</marker>
<rawString>van Rijsbergen, Keith. 1979. Information Retrieval. Butterworth-Heinemann, Newton, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>HMM-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>836--841</pages>
<location>Copenhagen, Denmark.</location>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Vogel, Stephan, Hermann Ney, and Christoph Tillmann. 1996. HMM-based word alignment in statistical translation. In Proceedings of COLING, pages 836–841, Copenhagen, Denmark.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>