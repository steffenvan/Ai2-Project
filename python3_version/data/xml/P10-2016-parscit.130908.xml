<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000938">
<title confidence="0.982182">
Tackling Sparse Data Issue in Machine Translation Evaluation *
</title>
<author confidence="0.986173">
Ondˇrej Bojar, Kamil Kos, and David Mareˇcek
</author>
<affiliation confidence="0.979556">
Charles University in Prague, Institute of Formal and Applied Linguistics
</affiliation>
<email confidence="0.995172">
{bojar,marecek}@ufal.mff.cuni.cz, kamilkos@email.cz
</email>
<sectionHeader confidence="0.993825" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999489">
We illustrate and explain problems of
n-grams-based machine translation (MT)
metrics (e.g. BLEU) when applied to
morphologically rich languages such as
Czech. A novel metric SemPOS based
on the deep-syntactic representation of the
sentence tackles the issue and retains the
performance for translation to English as
well.
</bodyText>
<sectionHeader confidence="0.998797" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998490192307692">
Automatic metrics of machine translation (MT)
quality are vital for research progress at a fast
pace. Many automatic metrics of MT quality have
been proposed and evaluated in terms of correla-
tion with human judgments while various tech-
niques of manual judging are being examined as
well, see e.g. MetricsMATR08 (Przybocki et al.,
2008)1, WMT08 and WMT09 (Callison-Burch et
al., 2008; Callison-Burch et al., 2009)2.
The contribution of this paper is twofold. Sec-
tion 2 illustrates and explains severe problems of a
widely used BLEU metric (Papineni et al., 2002)
when applied to Czech as a representative of lan-
guages with rich morphology. We see this as an
instance of the sparse data problem well known
for MT itself: too much detail in the formal repre-
sentation leading to low coverage of e.g. a transla-
tion dictionary. In MT evaluation, too much detail
leads to the lack of comparable parts of the hy-
pothesis and the reference.
* This work has been supported by the grants EuroMa-
trixPlus (FP7-ICT-2007-3-231720 of the EU and 7E09003
of the Czech Republic), FP7-ICT-2009-4-247762 (Faust),
GA201/09/H057, GAUK 1163/2010, and MSM 0021620838.
We are grateful to the anonymous reviewers for further re-
search suggestions.
</bodyText>
<footnote confidence="0.8541865">
1http://nist.gov/speech/tests
/metricsmatr/2008/results/
2http://www.statmt.org/wmt08 and wmt09
0.06 0.08 0.10 0.12 0.14 BLEU
</footnote>
<figureCaption confidence="0.930605666666667">
Figure 1: BLEU and human ranks of systems par-
ticipating in the English-to-Czech WMT09 shared
task.
</figureCaption>
<bodyText confidence="0.999503">
Section 3 introduces and evaluates some new
variations of SemPOS (Kos and Bojar, 2009), a
metric based on the deep syntactic representation
of the sentence performing very well for Czech as
the target language. Aside from including depen-
dency and n-gram relations in the scoring, we also
apply and evaluate SemPOS for English.
</bodyText>
<sectionHeader confidence="0.797932" genericHeader="method">
2 Problems of BLEU
</sectionHeader>
<bodyText confidence="0.999697333333333">
BLEU (Papineni et al., 2002) is an established
language-independent MT metric. Its correlation
to human judgments was originally deemed high
(for English) but better correlating metrics (esp.
for other languages) were found later, usually em-
ploying language-specific tools, see e.g. Przy-
bocki et al. (2008) or Callison-Burch et al. (2009).
The unbeaten advantage of BLEU is its simplicity.
Figure 1 illustrates a very low correlation to hu-
man judgments when translating to Czech. We
plot the official BLEU score against the rank es-
tablished as the percentage of sentences where a
system ranked no worse than all its competitors
(Callison-Burch et al., 2009). The systems devel-
oped at Charles University (cu-) are described in
Bojar et al. (2009), uedin is a vanilla configuration
of Moses (Koehn et al., 2007) and the remaining
ones are commercial MT systems.
In a manual analysis, we identified the reasons
for the low correlation: BLEU is overly sensitive
to sequences and forms in the hypothesis matching
</bodyText>
<figure confidence="0.993153333333333">
pctrans eurotranxp
� �
�cu-tectomt uedin •
ju-bojar
google
•
Rank
0.6
0.4
</figure>
<page confidence="0.959207">
86
</page>
<note confidence="0.742598">
Proceedings of the ACL 2010 Conference Short Papers, pages 86–91,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<table confidence="0.995378428571429">
Con- Error
firmed Flags 1-grams 2-grams 3-grams 4-grams
Yes Yes 6.34% 1.58% 0.55% 0.29%
Yes No 36.93% 13.68% 5.87% 2.69%
No Yes 22.33% 41.83% 54.64% 63.88%
No No 34.40% 42.91% 38.94% 33.14%
Total n-grams 35,531 33,891 32,251 30,611
</table>
<tableCaption confidence="0.982991">
Table 1: n-grams confirmed by the reference and
containing error flags.
</tableCaption>
<bodyText confidence="0.995901707317073">
the reference translation. This focus goes directly
against the properties of Czech: relatively free
word order allows many permutations of words
and rich morphology renders many valid word
forms not confirmed by the reference.3 These
problems are to some extent mitigated if several
reference translations are available, but this is of-
ten not the case.
Figure 2 illustrates the problem of “sparse data”
in the reference. Due to the lexical and morpho-
logical variance of Czech, only a single word in
each hypothesis matches a word in the reference.
In the case of pctrans, the match is even a false
positive, “do” (to) is a preposition that should be
used for the “minus” phrase and not for the “end
of the day” phrase. In terms of BLEU, both hy-
potheses are equally poor but 90% of their tokens
were not evaluated.
Table 1 estimates the overall magnitude of this
issue: For 1-grams to 4-grams in 1640 instances
(different MT outputs and different annotators) of
200 sentences with manually flagged errors4, we
count how often the n-gram is confirmed by the
reference and how often it contains an error flag.
The suspicious cases are n-grams confirmed by
the reference but still containing a flag (false posi-
tives) and n-grams not confirmed despite contain-
ing no error flag (false negatives).
Fortunately, there are relatively few false posi-
tives in n-gram based metrics: 6.3% of unigrams
and far fewer higher n-grams.
The issue of false negatives is more serious and
confirms the problem of sparse data if only one
reference is available. 30 to 40% of n-grams do
not contain any error and yet they are not con-
3Condon et al. (2009) identify similar issues when eval-
uating translation to Arabic and employ rule-based normal-
ization of MT output to improve the correlation. It is beyond
the scope of this paper to describe the rather different nature
of morphological richness in Czech, Arabic and also other
languages, e.g. German or Finnish.
</bodyText>
<footnote confidence="0.97195">
4The dataset with manually flagged errors is available at
http://ufal.mff.cuni.cz/euromatrixplus/
</footnote>
<bodyText confidence="0.999621217391304">
firmed by the reference. This amounts to 34% of
running unigrams, giving enough space to differ in
human judgments and still remain unscored.
Figure 3 documents the issue across languages:
the lower the BLEU score itself (i.e. fewer con-
firmed n-grams), the lower the correlation to hu-
man judgments regardless of the target language
(WMT09 shared task, 2025 sentences per lan-
guage).
Figure 4 illustrates the overestimation of scores
caused by too much attention to sequences of to-
kens. A phrase-based system like Moses (cu-
bojar) can sometimes produce a long sequence of
tokens exactly as required by the reference, lead-
ing to a high BLEU score. The framed words
in the illustration are not confirmed by the refer-
ence, but the actual error in these words is very
severe for comprehension: nouns were used twice
instead of finite verbs, and a misleading transla-
tion of a preposition was chosen. The output by
pctrans preserves the meaning much better despite
not scoring in either of the finite verbs and produc-
ing far shorter confirmed sequences.
</bodyText>
<sectionHeader confidence="0.927082" genericHeader="method">
3 Extensions of SemPOS
</sectionHeader>
<bodyText confidence="0.9987554">
SemPOS (Kos and Bojar, 2009) is inspired by met-
rics based on overlapping of linguistic features in
the reference and in the translation (Gim´enez and
M´arquez, 2007). It operates on so-called “tec-
togrammatical” (deep syntactic) representation of
the sentence (Sgall et al., 1986; Hajiˇc et al., 2006),
formally a dependency tree that includes only au-
tosemantic (content-bearing) words.5 SemPOS as
defined in Kos and Bojar (2009) disregards the
syntactic structure and uses the semantic part of
speech of the words (noun, verb, etc.). There are
19 fine-grained parts of speech. For each semantic
part of speech t, the overlapping O(t) is set to zero
if the part of speech does not occur in the reference
or the candidate set and otherwise it is computed
as given in Equation 1 below.
5We use TectoMT (ˇZabokrtsk´y and Bojar, 2008),
http://ufal.mff.cuni.cz/tectomt/, for the lin-
guistic pre-processing. While both our implementation of
SemPOS as well as TectoMT are in principle freely avail-
able, a stable public version has yet to be released. Our plans
include experiments with approximating the deep syntactic
analysis with a simple tagger, which would also decrease the
installation burden and computation costs, at the expense of
accuracy.
</bodyText>
<page confidence="0.992526">
87
</page>
<figure confidence="0.45214125">
SRC Prague Stock Market falls to minus by the end of the trading day
REF praˇzsk´a burza se ke konci obchodov´anipropadla do minusu
cu-bojar praha stock market klesne k minus na konci obchodniho dne
pctrans praha trh cenn´ych papir˚u pad´a minus do konce obchodniho dne
</figure>
<figureCaption confidence="0.9153868">
Figure 2: Sparse data in BLEU evaluation: Large chunks of hypotheses are not compared at all. Only a
single unigram in each hypothesis is confirmed in the reference.
Figure 3: BLEU correlates with its correlation to human judgments. BLEU scores around 0.1 predict
little about translation quality.
(1)
</figureCaption>
<bodyText confidence="0.999893538461538">
The semantic part of speech is denoted t; ci
and ri are the candidate and reference translations
of sentence i, and cnt(w, t, rc) is the number of
words w with type t in rc (the reference or the can-
didate). The matching is performed on the level of
lemmas, i.e. no morphological information is pre-
served in ws. See Figure 5 for an example; the
sentence is the same as in Figure 4.
The final SemPOS score is obtained by macro-
averaging over all parts of speech:
where T is the set of all possible semantic parts
of speech types. (The degenerate case of blank
candidate and reference has SemPOS zero.)
</bodyText>
<subsectionHeader confidence="0.999757">
3.1 Variations of SemPOS
</subsectionHeader>
<bodyText confidence="0.9763045">
This section describes our modifications of Sem-
POS. All methods are evaluated in Section 3.2.
</bodyText>
<subsectionHeader confidence="0.766581">
Different Classification of Autosemantic
</subsectionHeader>
<bodyText confidence="0.992426542857143">
Words. SemPOS uses semantic parts of speech
to classify autosemantic words. The tectogram-
matical layer offers also a feature called Functor
describing the relation of a word to its governor
similarly as semantic roles do. There are 67
functor types in total.
Using Functor instead of SemPOS increases the
number of word classes that independently require
a high overlap. For a contrast we also completely
remove the classification and use only one global
class (Void).
Deep Syntactic Relations in SemPOS. In
SemPOS, an autosemantic word of a class is con-
firmed if its lemma matches the reference. We uti-
lize the dependency relations at the tectogrammat-
ical layer to validate valence by refining the over-
lap and requiring also the lemma of 1) the parent
(denoted “par”), or 2) all the children regardless of
their order (denoted “sons”) to match.
Combining BLEU and SemPOS. One of the
major drawbacks of SemPOS is that it completely
ignores word order. This is too coarse even for
languages with relatively free word order like
Czech. Another issue is that it operates on lemmas
and it completely disregards correct word forms.
Thus, a weighted linear combination of SemPOS
and BLEU (computed on the surface representa-
tion of the sentence) should compensate for this.
For the purposes of the combination, we compute
BLEU only on unigrams up to fourgrams (denoted
BLEU1, ... , BLEU4) but including the brevity
penalty as usual. Here we try only a few weight
settings in the linear combination but given a held-
out dataset, one could optimize the weights for the
best performance.
</bodyText>
<equation confidence="0.9730715">
O(t) = 1: 1: min(cnt(w, t, ri), cnt(w, t, ci))
iEI wErinci
1: 1: max(cnt(w, t, ri), cnt(w, t, ci))
iEI wEriUci
1
SemPOS =
 |T  |1:O(t) (2)
tE�
</equation>
<page confidence="0.992629">
88
</page>
<figure confidence="0.850502">
SRC Congress yields: US government can pump 700 billion dollars into banks
REF kongres ustoupil : vl´ada usa m˚uˇze do bank napumpovat 700 miliard dolar˚u
cu-bojar kongres v´ynosy
: vl´ada usa m˚uˇze ˇcerpadlo 700 miliard dolar˚u
v bank´ach
pctrans kongres vyn´aˇs´ı : us vl´ada m˚uˇze ˇcerpat 700 miliardu dolar˚u do bank
</figure>
<figureCaption confidence="0.889777">
Figure 4: Too much focus on sequences in BLEU: pctrans’ output is better but does not score well.
</figureCaption>
<bodyText confidence="0.858555666666667">
BLEU gave credit to cu-bojar for 1, 3, 5 and 8 fourgrams, trigrams, bigrams and unigrams, resp., but
only for 0, 0, 1 and 8 n-grams produced by pctrans. Confirmed sequences of tokens are underlined and
important errors (not considered by BLEU) are framed.
</bodyText>
<figure confidence="0.795594333333333">
REF kongres/n ustoupit/v :/n vl´ada/n usa/n banka/n napumpovat/v 700/n miliarda/n dolar/n
cu-bojar kongres/n v´ynos/n :/n vl´ada/n usa/n moci/v ˇcerpadlo/n 700/n miliarda/n dolar/n banka/n
pctrans kongres/n vyn´aˇset/v :/n us/n vl´ada/n ˇcerpat/v 700/n miliarda/n dolar/n banka/n
</figure>
<figureCaption confidence="0.987665">
Figure 5: SemPOS evaluates the overlap of lemmas of autosemantic words given their semantic part of
speech (n, v, ... ). Underlined words are confirmed by the reference.
</figureCaption>
<bodyText confidence="0.8923798">
SemPOS for English. The tectogrammatical
layer is being adapted for English (Cinkov´a et al.,
2004; Hajiˇc et al., 2009) and we are able to use the
available tools to obtain all SemPOS features for
English sentences as well.
</bodyText>
<subsectionHeader confidence="0.999755">
3.2 Evaluation of SemPOS and Friends
</subsectionHeader>
<bodyText confidence="0.998396147540984">
We measured the metric performance on data used
in MetricsMATR08, WMT09 and WMT08. For
the evaluation of metric correlation with human
judgments at the system level, we used the Pearson
correlation coefficient p applied to ranks. In case
of a tie, the systems were assigned the average po-
sition. For example if three systems achieved the
same highest score (thus occupying the positions
1, 2 and 3 when sorted by score), each of them
would obtain the average rank of 2 = 1+2+3
3 .
When correlating ranks (instead of exact scores)
and with this handling of ties, the Pearson coeffi-
cient is equivalent to Spearman’s rank correlation
coefficient.
The MetricsMATR08 human judgments include
preferences for pairs of MT systems saying which
one of the two systems is better, while the WMT08
and WMT09 data contain system scores (for up to
5 systems) on the scale 1 to 5 for a given sentence.
We assigned a human ranking to the systems based
on the percent of time that their translations were
judged to be better than or equal to the translations
of any other system in the manual evaluation. We
converted automatic metric scores to ranks.
Metrics’ performance for translation to English
and Czech was measured on the following test-
sets (the number of human judgments for a given
source language in brackets):
To English: MetricsMATR08 (cn+ar: 1652),
WMT08 News Articles (de: 199, fr: 251),
WMT08 Europarl (es: 190, fr: 183), WMT09
(cz: 320, de: 749, es: 484, fr: 786, hu: 287)
To Czech: WMT08 News Articles (en: 267),
WMT08 Commentary (en: 243), WMT09
(en: 1425)
The MetricsMATR08 testset contained 4 refer-
ence translations for each sentence whereas the re-
maining testsets only one reference.
Correlation coefficients for English are shown
in Table 2. The best metric is Voidpar closely fol-
lowed by Voidsons. The explanation is that Void
compared to SemPOS or Functor does not lose
points by an erroneous assignment of the POS or
the functor, and that Voidpar profits from check-
ing the dependency relations between autoseman-
tic words. The combination of BLEU and Sem-
POS6 outperforms both individual metrics, but in
case of SemPOS only by a minimal difference.
Additionally, we confirm that 4-grams alone have
little discriminative power both when used as a
metric of their own (BLEU4) as well as in a lin-
ear combination with SemPOS.
The best metric for Czech (see Table 3) is a lin-
ear combination of SemPOS and 4-gram BLEU
closely followed by other SemPOS and BLEU,,
combinations. We assume this is because BLEU4
can capture correctly translated fixed phrases,
which is positively reflected in human judgments.
Including BLEU1 in the combination favors trans-
lations with word forms as expected by the refer-
</bodyText>
<footnote confidence="0.957321">
6For each n E {1, 2, 3, 41, we show only the best weight
setting for SemPOS and BLEU�.
</footnote>
<page confidence="0.996269">
89
</page>
<table confidence="0.999972347826087">
Metric Avg Best Worst
Voidpar 0.75 0.89 0.60
Voidsons 0.75 0.90 0.54
Void 0.72 0.91 0.59
Functorsons 0.72 1.00 0.43
GTM 0.71 0.90 0.54
4·SemPOS+1·BLEU2 0.70 0.93 0.43
SemPOSpar 0.70 0.93 0.30
1·SemPOS+4·BLEU3 0.70 0.91 0.26
4·SemPOS+1·BLEU1 0.69 0.93 0.43
NIST 0.69 0.90 0.53
SemPOSsons 0.69 0.94 0.40
SemPOS 0.69 0.95 0.30
2·SemPOS+1·BLEU4 0.68 0.91 0.09
BLEU1 0.68 0.87 0.43
BLEU2 0.68 0.90 0.26
BLEU3 0.66 0.90 0.14
BLEU 0.66 0.91 0.20
TER 0.63 0.87 0.29
PER 0.63 0.88 0.32
BLEU4 0.61 0.90 -0.31
Functorpar 0.57 0.83 -0.03
Functor 0.55 0.82 -0.09
</table>
<tableCaption confidence="0.997921">
Table 2: Average, best and worst system-level cor-
</tableCaption>
<bodyText confidence="0.939426722222222">
relation coefficients for translation to English from
various source languages evaluated on 10 different
testsets.
ence, thus allowing to spot bad word forms. In
all cases, the linear combination puts more weight
on SemPOS. Given the negligible difference be-
tween SemPOS alone and the linear combinations,
we see that word forms are not the major issue for
humans interpreting the translation—most likely
because the systems so far often make more im-
portant errors. This is also confirmed by the obser-
vation that using BLEU alone is rather unreliable
for Czech and BLEU-1 (which judges unigrams
only) is even worse. Surprisingly BLEU-2 per-
formed better than any other n-grams for reasons
that have yet to be examined. The error metrics
PER and TER showed the lowest correlation with
human judgments for translation to Czech.
</bodyText>
<sectionHeader confidence="0.995771" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999881909090909">
This paper documented problems of single-
reference BLEU when applied to morphologically
rich languages such as Czech. BLEU suffers from
a sparse data problem, unable to judge the quality
of tokens not confirmed by the reference. This is
confirmed for other languages as well: the lower
the BLEU score the lower the correlation to hu-
man judgments.
We introduced a refinement of SemPOS, an
automatic metric of MT quality based on deep-
syntactic representation of the sentence tackling
</bodyText>
<table confidence="0.999923">
Metric Avg Best Worst
3·SemPOS+1·BLEU4 0.55 0.83 0.14
2·SemPOS+1·BLEU2 0.55 0.83 0.14
2·SemPOS+1·BLEU1 0.53 0.83 0.09
4·SemPOS+1·BLEU3 0.53 0.83 0.09
SemPOS 0.53 0.83 0.09
BLEU2 0.43 0.83 0.09
SemPOSpar 0.37 0.53 0.14
Functorsons 0.36 0.53 0.14
GTM 0.35 0.53 0.14
BLEU4 0.33 0.53 0.09
Void 0.33 0.53 0.09
NIST 0.33 0.53 0.09
Voidsons 0.33 0.53 0.09
BLEU 0.33 0.53 0.09
BLEU3 0.33 0.53 0.09
BLEU1 0.29 0.53 -0.03
SemPOSsons 0.28 0.42 0.03
Functorpar 0.23 0.40 0.14
Functor 0.21 0.40 0.09
Voidpar 0.16 0.53 -0.08
PER 0.12 0.53 -0.09
TER 0.07 0.53 -0.23
</table>
<tableCaption confidence="0.689652666666667">
Table 3: System-level correlation coefficients for
English-to-Czech translation evaluated on 3 differ-
ent testsets.
</tableCaption>
<bodyText confidence="0.998693333333333">
the sparse data issue. SemPOS was evaluated on
translation to Czech and to English, scoring better
than or comparable to many established metrics.
</bodyText>
<sectionHeader confidence="0.99813" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999159653846154">
Ond&amp;quot;rej Bojar, David Mare&amp;quot;cek, V´aclav Nov´ak, Mar-
tin Popel, Jan Pt´a&amp;quot;cek, Jan Rou&amp;quot;s, and Zden&amp;quot;ek
&amp;quot;Zabokrtsk´y. 2009. English-Czech MT in 2008. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, Athens, Greece, March. Asso-
ciation for Computational Linguistics.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2008.
Further meta-evaluation of machine translation. In
Proceedings of the Third Workshop on Statisti-
cal Machine Translation, pages 70–106, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
workshop on statistical machine translation. In Pro-
ceedings of the Fourth Workshop on Statistical Ma-
chine Translation, Athens, Greece. Association for
Computational Linguistics.
Silvie Cinkov´a, Jan Haji&amp;quot;c, Marie Mikulov´a, Lu-
cie Mladov´a, Anja Nedoluz&amp;quot;ko, Petr Pajas, Jarmila
Panevov´a, Ji&amp;quot;r´ı Semeck´y, Jana &amp;quot;Sindlerov´a, Josef
Toman, Zde&amp;quot;nka Ure&amp;quot;sov´a, and Zden&amp;quot;ek &amp;quot;Zabokrtsk´y.
2004. Annotation of English on the tectogram-
matical level. Technical Report TR-2006-35,
´UFAL/CKL, Prague, Czech Republic, December.
</reference>
<page confidence="0.977362">
90
</page>
<reference confidence="0.999931416666666">
Sherri Condon, Gregory A. Sanders, Dan Parvaz, Alan
Rubenstein, Christy Doran, John Aberdeen, and
Beatrice Oshika. 2009. Normalization for Auto-
mated Metrics: English and Arabic Speech Transla-
tion. In MT Summit XII.
Jes´us Gim´enez and Llu´ıs M´arquez. 2007. Linguis-
tic Features for Automatic Evaluation of Heteroge-
nous MT Systems. In Proceedings of the Second
Workshop on Statistical Machine Translation, pages
256–264, Prague, June. Association for Computa-
tional Linguistics.
Jan Haji&amp;quot;c, Silvie Cinkov´a, Krist´yna &amp;quot;Cerm´akov´a, Lu-
cie Mladov´a, Anja Nedolu&amp;quot;zko, Petr Pajas, Ji&amp;quot;r´ı Se-
meck´y, Jana &amp;quot;Sindlerov´a, Josef Toman, Krist´yna
Tom&amp;quot;s˚u, Mat&amp;quot;ej Korvas, Magdal´ena Rysov´a, Kate&amp;quot;rina
Veselovsk´a, and Zden&amp;quot;ek &amp;quot;Zabokrtsk´y. 2009. Prague
English Dependency Treebank 1.0. Institute of For-
mal and Applied Linguistics, Charles University in
Prague, ISBN 978-80-904175-0-2, January.
Jan Haji&amp;quot;c, Jarmila Panevov´a, Eva Haji&amp;quot;cov´a, Petr
Sgall, Petr Pajas, Jan &amp;quot;St&amp;quot;ep´anek, Ji&amp;quot;r´ı Havelka,
Marie Mikulov´a, Zden&amp;quot;ek &amp;quot;Zabokrtsk´y, and Magda
&amp;quot;Sev&amp;quot;c´ıkov´a Raz´ımov´a. 2006. Prague Dependency
Treebank 2.0. LDC2006T01, ISBN: 1-58563-370-
4.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond&amp;quot;rej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In ACL 2007, Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguis-
tics Companion Volume Proceedings of the Demo
and Poster Sessions, pages 177–180, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Kamil Kos and Ond&amp;quot;rej Bojar. 2009. Evaluation of Ma-
chine Translation Metrics for Czech as the Target
Language. Prague Bulletin of Mathematical Lin-
guistics, 92.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In ACL 2002,
Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics, pages 311–
318, Philadelphia, Pennsylvania.
M. Przybocki, K. Peterson, and S. Bronsart. 2008. Of-
ficial results of the NIST 2008 ”Metrics for MA-
chine TRanslation” Challenge (MetricsMATR08).
Petr Sgall, Eva Haji&amp;quot;cov´a, and Jarmila Panevov´a. 1986.
The Meaning of the Sentence and Its Semantic
and Pragmatic Aspects. Academia/Reidel Publish-
ing Company, Prague, Czech Republic/Dordrecht,
Netherlands.
Zden&amp;quot;ek &amp;quot;Zabokrtsk´y and Ond&amp;quot;rej Bojar. 2008. TectoMT,
Developer’s Guide. Technical Report TR-2008-39,
Institute of Formal and Applied Linguistics, Faculty
of Mathematics and Physics, Charles University in
Prague, December.
</reference>
<page confidence="0.999171">
91
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.744158">
<title confidence="0.999488">Sparse Data Issue in Machine Translation Evaluation</title>
<author confidence="0.94427">Ondˇrej Bojar</author>
<author confidence="0.94427">Kamil Kos</author>
<author confidence="0.94427">David Mareˇcek</author>
<affiliation confidence="0.857706">Charles University in Prague, Institute of Formal and Applied Linguistics</affiliation>
<email confidence="0.930473">kamilkos@email.cz</email>
<abstract confidence="0.9934889">We illustrate and explain problems of machine translation (MT) metrics (e.g. BLEU) when applied to morphologically rich languages such as Czech. A novel metric SemPOS based on the deep-syntactic representation of the sentence tackles the issue and retains the performance for translation to English as well.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ondrej Bojar</author>
<author>David Marecek</author>
<author>V´aclav Nov´ak</author>
<author>Martin Popel</author>
<author>Jan Pt´acek</author>
<author>Jan Rous</author>
<author>Zdenek Zabokrtsk´y</author>
</authors>
<title>English-Czech MT in</title>
<date>2009</date>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Athens, Greece,</location>
<marker>Bojar, Marecek, Nov´ak, Popel, Pt´acek, Rous, Zabokrtsk´y, 2009</marker>
<rawString>Ond&amp;quot;rej Bojar, David Mare&amp;quot;cek, V´aclav Nov´ak, Martin Popel, Jan Pt´a&amp;quot;cek, Jan Rou&amp;quot;s, and Zden&amp;quot;ek &amp;quot;Zabokrtsk´y. 2009. English-Czech MT in 2008. In Proceedings of the Fourth Workshop on Statistical Machine Translation, Athens, Greece, March. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Cameron Fordyce</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Josh Schroeder</author>
</authors>
<title>Further meta-evaluation of machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation,</booktitle>
<pages>70--106</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="962" citStr="Callison-Burch et al., 2008" startWordPosition="135" endWordPosition="138">LEU) when applied to morphologically rich languages such as Czech. A novel metric SemPOS based on the deep-syntactic representation of the sentence tackles the issue and retains the performance for translation to English as well. 1 Introduction Automatic metrics of machine translation (MT) quality are vital for research progress at a fast pace. Many automatic metrics of MT quality have been proposed and evaluated in terms of correlation with human judgments while various techniques of manual judging are being examined as well, see e.g. MetricsMATR08 (Przybocki et al., 2008)1, WMT08 and WMT09 (Callison-Burch et al., 2008; Callison-Burch et al., 2009)2. The contribution of this paper is twofold. Section 2 illustrates and explains severe problems of a widely used BLEU metric (Papineni et al., 2002) when applied to Czech as a representative of languages with rich morphology. We see this as an instance of the sparse data problem well known for MT itself: too much detail in the formal representation leading to low coverage of e.g. a translation dictionary. In MT evaluation, too much detail leads to the lack of comparable parts of the hypothesis and the reference. * This work has been supported by the grants EuroMa</context>
</contexts>
<marker>Callison-Burch, Fordyce, Koehn, Monz, Schroeder, 2008</marker>
<rawString>Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and Josh Schroeder. 2008. Further meta-evaluation of machine translation. In Proceedings of the Third Workshop on Statistical Machine Translation, pages 70–106, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Josh Schroeder</author>
</authors>
<title>Findings of the 2009 workshop on statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Athens, Greece.</location>
<contexts>
<context position="992" citStr="Callison-Burch et al., 2009" startWordPosition="139" endWordPosition="142">gically rich languages such as Czech. A novel metric SemPOS based on the deep-syntactic representation of the sentence tackles the issue and retains the performance for translation to English as well. 1 Introduction Automatic metrics of machine translation (MT) quality are vital for research progress at a fast pace. Many automatic metrics of MT quality have been proposed and evaluated in terms of correlation with human judgments while various techniques of manual judging are being examined as well, see e.g. MetricsMATR08 (Przybocki et al., 2008)1, WMT08 and WMT09 (Callison-Burch et al., 2008; Callison-Burch et al., 2009)2. The contribution of this paper is twofold. Section 2 illustrates and explains severe problems of a widely used BLEU metric (Papineni et al., 2002) when applied to Czech as a representative of languages with rich morphology. We see this as an instance of the sparse data problem well known for MT itself: too much detail in the formal representation leading to low coverage of e.g. a translation dictionary. In MT evaluation, too much detail leads to the lack of comparable parts of the hypothesis and the reference. * This work has been supported by the grants EuroMatrixPlus (FP7-ICT-2007-3-23172</context>
<context position="2708" citStr="Callison-Burch et al. (2009)" startWordPosition="406" endWordPosition="409">f SemPOS (Kos and Bojar, 2009), a metric based on the deep syntactic representation of the sentence performing very well for Czech as the target language. Aside from including dependency and n-gram relations in the scoring, we also apply and evaluate SemPOS for English. 2 Problems of BLEU BLEU (Papineni et al., 2002) is an established language-independent MT metric. Its correlation to human judgments was originally deemed high (for English) but better correlating metrics (esp. for other languages) were found later, usually employing language-specific tools, see e.g. Przybocki et al. (2008) or Callison-Burch et al. (2009). The unbeaten advantage of BLEU is its simplicity. Figure 1 illustrates a very low correlation to human judgments when translating to Czech. We plot the official BLEU score against the rank established as the percentage of sentences where a system ranked no worse than all its competitors (Callison-Burch et al., 2009). The systems developed at Charles University (cu-) are described in Bojar et al. (2009), uedin is a vanilla configuration of Moses (Koehn et al., 2007) and the remaining ones are commercial MT systems. In a manual analysis, we identified the reasons for the low correlation: BLEU </context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Schroeder, 2009</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, and Josh Schroeder. 2009. Findings of the 2009 workshop on statistical machine translation. In Proceedings of the Fourth Workshop on Statistical Machine Translation, Athens, Greece. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silvie Cinkov´a</author>
<author>Jan Hajic</author>
<author>Marie Mikulov´a</author>
</authors>
<title>Lucie Mladov´a, Anja Nedoluz&amp;quot;ko, Petr Pajas, Jarmila Panevov´a, Ji&amp;quot;r´ı Semeck´y, Jana &amp;quot;Sindlerov´a, Josef Toman, Zde&amp;quot;nka Ure&amp;quot;sov´a, and Zden&amp;quot;ek &amp;quot;Zabokrtsk´y.</title>
<date>2004</date>
<tech>Technical Report TR-2006-35,</tech>
<location>UFAL/CKL, Prague, Czech Republic,</location>
<marker>Cinkov´a, Hajic, Mikulov´a, 2004</marker>
<rawString>Silvie Cinkov´a, Jan Haji&amp;quot;c, Marie Mikulov´a, Lucie Mladov´a, Anja Nedoluz&amp;quot;ko, Petr Pajas, Jarmila Panevov´a, Ji&amp;quot;r´ı Semeck´y, Jana &amp;quot;Sindlerov´a, Josef Toman, Zde&amp;quot;nka Ure&amp;quot;sov´a, and Zden&amp;quot;ek &amp;quot;Zabokrtsk´y. 2004. Annotation of English on the tectogrammatical level. Technical Report TR-2006-35, ´UFAL/CKL, Prague, Czech Republic, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sherri Condon</author>
<author>Gregory A Sanders</author>
<author>Dan Parvaz</author>
<author>Alan Rubenstein</author>
<author>Christy Doran</author>
<author>John Aberdeen</author>
<author>Beatrice Oshika</author>
</authors>
<title>Normalization for Automated Metrics: English and Arabic Speech Translation.</title>
<date>2009</date>
<booktitle>In MT Summit XII.</booktitle>
<contexts>
<context position="5538" citStr="Condon et al. (2009)" startWordPosition="878" endWordPosition="882">count how often the n-gram is confirmed by the reference and how often it contains an error flag. The suspicious cases are n-grams confirmed by the reference but still containing a flag (false positives) and n-grams not confirmed despite containing no error flag (false negatives). Fortunately, there are relatively few false positives in n-gram based metrics: 6.3% of unigrams and far fewer higher n-grams. The issue of false negatives is more serious and confirms the problem of sparse data if only one reference is available. 30 to 40% of n-grams do not contain any error and yet they are not con3Condon et al. (2009) identify similar issues when evaluating translation to Arabic and employ rule-based normalization of MT output to improve the correlation. It is beyond the scope of this paper to describe the rather different nature of morphological richness in Czech, Arabic and also other languages, e.g. German or Finnish. 4The dataset with manually flagged errors is available at http://ufal.mff.cuni.cz/euromatrixplus/ firmed by the reference. This amounts to 34% of running unigrams, giving enough space to differ in human judgments and still remain unscored. Figure 3 documents the issue across languages: the</context>
</contexts>
<marker>Condon, Sanders, Parvaz, Rubenstein, Doran, Aberdeen, Oshika, 2009</marker>
<rawString>Sherri Condon, Gregory A. Sanders, Dan Parvaz, Alan Rubenstein, Christy Doran, John Aberdeen, and Beatrice Oshika. 2009. Normalization for Automated Metrics: English and Arabic Speech Translation. In MT Summit XII.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Llu´ıs M´arquez</author>
</authors>
<title>Linguistic Features for Automatic Evaluation of Heterogenous MT Systems.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>256--264</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague,</location>
<marker>Gim´enez, M´arquez, 2007</marker>
<rawString>Jes´us Gim´enez and Llu´ıs M´arquez. 2007. Linguistic Features for Automatic Evaluation of Heterogenous MT Systems. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 256–264, Prague, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jan Hajic</author>
</authors>
<title>Silvie Cinkov´a, Krist´yna &amp;quot;Cerm´akov´a, Lucie Mladov´a, Anja Nedolu&amp;quot;zko, Petr Pajas, Ji&amp;quot;r´ı Semeck´y, Jana &amp;quot;Sindlerov´a, Josef Toman, Krist´yna Tom&amp;quot;s˚u, Mat&amp;quot;ej Korvas, Magdal´ena Rysov´a, Kate&amp;quot;rina Veselovsk´a, and Zden&amp;quot;ek &amp;quot;Zabokrtsk´y.</title>
<date>2009</date>
<booktitle>Prague English Dependency Treebank 1.0. Institute of Formal and Applied Linguistics, Charles University in Prague, ISBN</booktitle>
<pages>978--80</pages>
<marker>Hajic, 2009</marker>
<rawString>Jan Haji&amp;quot;c, Silvie Cinkov´a, Krist´yna &amp;quot;Cerm´akov´a, Lucie Mladov´a, Anja Nedolu&amp;quot;zko, Petr Pajas, Ji&amp;quot;r´ı Semeck´y, Jana &amp;quot;Sindlerov´a, Josef Toman, Krist´yna Tom&amp;quot;s˚u, Mat&amp;quot;ej Korvas, Magdal´ena Rysov´a, Kate&amp;quot;rina Veselovsk´a, and Zden&amp;quot;ek &amp;quot;Zabokrtsk´y. 2009. Prague English Dependency Treebank 1.0. Institute of Formal and Applied Linguistics, Charles University in Prague, ISBN 978-80-904175-0-2, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajic</author>
<author>Jarmila Panevov´a</author>
<author>Eva Hajicov´a</author>
<author>Petr Sgall</author>
<author>Petr Pajas</author>
</authors>
<title>St&amp;quot;ep´anek, Ji&amp;quot;r´ı Havelka, Marie Mikulov´a, Zden&amp;quot;ek &amp;quot;Zabokrtsk´y, and Magda &amp;quot;Sev&amp;quot;c´ıkov´a Raz´ımov´a.</title>
<date></date>
<booktitle>Prague Dependency Treebank 2.0. LDC2006T01, ISBN:</booktitle>
<pages>1--58563</pages>
<marker>Hajic, Panevov´a, Hajicov´a, Sgall, Pajas, </marker>
<rawString>Jan Haji&amp;quot;c, Jarmila Panevov´a, Eva Haji&amp;quot;cov´a, Petr Sgall, Petr Pajas, Jan &amp;quot;St&amp;quot;ep´anek, Ji&amp;quot;r´ı Havelka, Marie Mikulov´a, Zden&amp;quot;ek &amp;quot;Zabokrtsk´y, and Magda &amp;quot;Sev&amp;quot;c´ıkov´a Raz´ımov´a. 2006. Prague Dependency Treebank 2.0. LDC2006T01, ISBN: 1-58563-370-4.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: Open Source Toolkit for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In ACL 2007, Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="3179" citStr="Koehn et al., 2007" startWordPosition="485" endWordPosition="488">. for other languages) were found later, usually employing language-specific tools, see e.g. Przybocki et al. (2008) or Callison-Burch et al. (2009). The unbeaten advantage of BLEU is its simplicity. Figure 1 illustrates a very low correlation to human judgments when translating to Czech. We plot the official BLEU score against the rank established as the percentage of sentences where a system ranked no worse than all its competitors (Callison-Burch et al., 2009). The systems developed at Charles University (cu-) are described in Bojar et al. (2009), uedin is a vanilla configuration of Moses (Koehn et al., 2007) and the remaining ones are commercial MT systems. In a manual analysis, we identified the reasons for the low correlation: BLEU is overly sensitive to sequences and forms in the hypothesis matching pctrans eurotranxp � � �cu-tectomt uedin • ju-bojar google • Rank 0.6 0.4 86 Proceedings of the ACL 2010 Conference Short Papers, pages 86–91, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics Con- Error firmed Flags 1-grams 2-grams 3-grams 4-grams Yes Yes 6.34% 1.58% 0.55% 0.29% Yes No 36.93% 13.68% 5.87% 2.69% No Yes 22.33% 41.83% 54.64% 63.88% No No 34.40% 42.91%</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ond&amp;quot;rej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In ACL 2007, Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 177–180, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kamil Kos</author>
<author>Ondrej Bojar</author>
</authors>
<title>Evaluation of Machine Translation Metrics for Czech as the Target Language.</title>
<date>2009</date>
<journal>Prague Bulletin of Mathematical Linguistics,</journal>
<volume>92</volume>
<contexts>
<context position="2110" citStr="Kos and Bojar, 2009" startWordPosition="313" endWordPosition="316">s and the reference. * This work has been supported by the grants EuroMatrixPlus (FP7-ICT-2007-3-231720 of the EU and 7E09003 of the Czech Republic), FP7-ICT-2009-4-247762 (Faust), GA201/09/H057, GAUK 1163/2010, and MSM 0021620838. We are grateful to the anonymous reviewers for further research suggestions. 1http://nist.gov/speech/tests /metricsmatr/2008/results/ 2http://www.statmt.org/wmt08 and wmt09 0.06 0.08 0.10 0.12 0.14 BLEU Figure 1: BLEU and human ranks of systems participating in the English-to-Czech WMT09 shared task. Section 3 introduces and evaluates some new variations of SemPOS (Kos and Bojar, 2009), a metric based on the deep syntactic representation of the sentence performing very well for Czech as the target language. Aside from including dependency and n-gram relations in the scoring, we also apply and evaluate SemPOS for English. 2 Problems of BLEU BLEU (Papineni et al., 2002) is an established language-independent MT metric. Its correlation to human judgments was originally deemed high (for English) but better correlating metrics (esp. for other languages) were found later, usually employing language-specific tools, see e.g. Przybocki et al. (2008) or Callison-Burch et al. (2009). </context>
<context position="7042" citStr="Kos and Bojar, 2009" startWordPosition="1122" endWordPosition="1125"> tokens. A phrase-based system like Moses (cubojar) can sometimes produce a long sequence of tokens exactly as required by the reference, leading to a high BLEU score. The framed words in the illustration are not confirmed by the reference, but the actual error in these words is very severe for comprehension: nouns were used twice instead of finite verbs, and a misleading translation of a preposition was chosen. The output by pctrans preserves the meaning much better despite not scoring in either of the finite verbs and producing far shorter confirmed sequences. 3 Extensions of SemPOS SemPOS (Kos and Bojar, 2009) is inspired by metrics based on overlapping of linguistic features in the reference and in the translation (Gim´enez and M´arquez, 2007). It operates on so-called “tectogrammatical” (deep syntactic) representation of the sentence (Sgall et al., 1986; Hajiˇc et al., 2006), formally a dependency tree that includes only autosemantic (content-bearing) words.5 SemPOS as defined in Kos and Bojar (2009) disregards the syntactic structure and uses the semantic part of speech of the words (noun, verb, etc.). There are 19 fine-grained parts of speech. For each semantic part of speech t, the overlapping</context>
</contexts>
<marker>Kos, Bojar, 2009</marker>
<rawString>Kamil Kos and Ond&amp;quot;rej Bojar. 2009. Evaluation of Machine Translation Metrics for Czech as the Target Language. Prague Bulletin of Mathematical Linguistics, 92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In ACL 2002, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, Pennsylvania.</location>
<contexts>
<context position="1141" citStr="Papineni et al., 2002" startWordPosition="164" endWordPosition="167">rformance for translation to English as well. 1 Introduction Automatic metrics of machine translation (MT) quality are vital for research progress at a fast pace. Many automatic metrics of MT quality have been proposed and evaluated in terms of correlation with human judgments while various techniques of manual judging are being examined as well, see e.g. MetricsMATR08 (Przybocki et al., 2008)1, WMT08 and WMT09 (Callison-Burch et al., 2008; Callison-Burch et al., 2009)2. The contribution of this paper is twofold. Section 2 illustrates and explains severe problems of a widely used BLEU metric (Papineni et al., 2002) when applied to Czech as a representative of languages with rich morphology. We see this as an instance of the sparse data problem well known for MT itself: too much detail in the formal representation leading to low coverage of e.g. a translation dictionary. In MT evaluation, too much detail leads to the lack of comparable parts of the hypothesis and the reference. * This work has been supported by the grants EuroMatrixPlus (FP7-ICT-2007-3-231720 of the EU and 7E09003 of the Czech Republic), FP7-ICT-2009-4-247762 (Faust), GA201/09/H057, GAUK 1163/2010, and MSM 0021620838. We are grateful to </context>
<context position="2398" citStr="Papineni et al., 2002" startWordPosition="361" endWordPosition="364"> research suggestions. 1http://nist.gov/speech/tests /metricsmatr/2008/results/ 2http://www.statmt.org/wmt08 and wmt09 0.06 0.08 0.10 0.12 0.14 BLEU Figure 1: BLEU and human ranks of systems participating in the English-to-Czech WMT09 shared task. Section 3 introduces and evaluates some new variations of SemPOS (Kos and Bojar, 2009), a metric based on the deep syntactic representation of the sentence performing very well for Czech as the target language. Aside from including dependency and n-gram relations in the scoring, we also apply and evaluate SemPOS for English. 2 Problems of BLEU BLEU (Papineni et al., 2002) is an established language-independent MT metric. Its correlation to human judgments was originally deemed high (for English) but better correlating metrics (esp. for other languages) were found later, usually employing language-specific tools, see e.g. Przybocki et al. (2008) or Callison-Burch et al. (2009). The unbeaten advantage of BLEU is its simplicity. Figure 1 illustrates a very low correlation to human judgments when translating to Czech. We plot the official BLEU score against the rank established as the percentage of sentences where a system ranked no worse than all its competitors </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a Method for Automatic Evaluation of Machine Translation. In ACL 2002, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311– 318, Philadelphia, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Przybocki</author>
<author>K Peterson</author>
<author>S Bronsart</author>
</authors>
<title>Metrics for MAchine TRanslation” Challenge (MetricsMATR08).</title>
<date>2008</date>
<booktitle>Official results of the NIST</booktitle>
<contexts>
<context position="915" citStr="Przybocki et al., 2008" startWordPosition="128" endWordPosition="131">ed machine translation (MT) metrics (e.g. BLEU) when applied to morphologically rich languages such as Czech. A novel metric SemPOS based on the deep-syntactic representation of the sentence tackles the issue and retains the performance for translation to English as well. 1 Introduction Automatic metrics of machine translation (MT) quality are vital for research progress at a fast pace. Many automatic metrics of MT quality have been proposed and evaluated in terms of correlation with human judgments while various techniques of manual judging are being examined as well, see e.g. MetricsMATR08 (Przybocki et al., 2008)1, WMT08 and WMT09 (Callison-Burch et al., 2008; Callison-Burch et al., 2009)2. The contribution of this paper is twofold. Section 2 illustrates and explains severe problems of a widely used BLEU metric (Papineni et al., 2002) when applied to Czech as a representative of languages with rich morphology. We see this as an instance of the sparse data problem well known for MT itself: too much detail in the formal representation leading to low coverage of e.g. a translation dictionary. In MT evaluation, too much detail leads to the lack of comparable parts of the hypothesis and the reference. * Th</context>
<context position="2676" citStr="Przybocki et al. (2008)" startWordPosition="400" endWordPosition="404">uates some new variations of SemPOS (Kos and Bojar, 2009), a metric based on the deep syntactic representation of the sentence performing very well for Czech as the target language. Aside from including dependency and n-gram relations in the scoring, we also apply and evaluate SemPOS for English. 2 Problems of BLEU BLEU (Papineni et al., 2002) is an established language-independent MT metric. Its correlation to human judgments was originally deemed high (for English) but better correlating metrics (esp. for other languages) were found later, usually employing language-specific tools, see e.g. Przybocki et al. (2008) or Callison-Burch et al. (2009). The unbeaten advantage of BLEU is its simplicity. Figure 1 illustrates a very low correlation to human judgments when translating to Czech. We plot the official BLEU score against the rank established as the percentage of sentences where a system ranked no worse than all its competitors (Callison-Burch et al., 2009). The systems developed at Charles University (cu-) are described in Bojar et al. (2009), uedin is a vanilla configuration of Moses (Koehn et al., 2007) and the remaining ones are commercial MT systems. In a manual analysis, we identified the reason</context>
</contexts>
<marker>Przybocki, Peterson, Bronsart, 2008</marker>
<rawString>M. Przybocki, K. Peterson, and S. Bronsart. 2008. Official results of the NIST 2008 ”Metrics for MAchine TRanslation” Challenge (MetricsMATR08).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Petr Sgall</author>
<author>Eva Hajicov´a</author>
<author>Jarmila Panevov´a</author>
</authors>
<title>The Meaning of the Sentence and Its Semantic and Pragmatic Aspects.</title>
<date>1986</date>
<publisher>Academia/Reidel Publishing Company,</publisher>
<location>Prague, Czech Republic/Dordrecht, Netherlands.</location>
<marker>Sgall, Hajicov´a, Panevov´a, 1986</marker>
<rawString>Petr Sgall, Eva Haji&amp;quot;cov´a, and Jarmila Panevov´a. 1986. The Meaning of the Sentence and Its Semantic and Pragmatic Aspects. Academia/Reidel Publishing Company, Prague, Czech Republic/Dordrecht, Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zdenek Zabokrtsk´y</author>
<author>Ondrej Bojar</author>
</authors>
<title>TectoMT, Developer’s Guide.</title>
<date>2008</date>
<tech>Technical Report TR-2008-39,</tech>
<institution>Institute of Formal and Applied Linguistics, Faculty of Mathematics and Physics, Charles University in Prague,</institution>
<marker>Zabokrtsk´y, Bojar, 2008</marker>
<rawString>Zden&amp;quot;ek &amp;quot;Zabokrtsk´y and Ond&amp;quot;rej Bojar. 2008. TectoMT, Developer’s Guide. Technical Report TR-2008-39, Institute of Formal and Applied Linguistics, Faculty of Mathematics and Physics, Charles University in Prague, December.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>