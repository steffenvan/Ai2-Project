<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.331945">
<title confidence="0.980508">
Using Noisy Bilingual Data for Statistical Machine Translation
</title>
<author confidence="0.989888">
Stephan Vogel
</author>
<affiliation confidence="0.979118666666667">
Interactive Systems Lab
Language Technologies Institute
Carnegie Mellon University
</affiliation>
<email confidence="0.996553">
vogel+@cs.cmu.edu
</email>
<sectionHeader confidence="0.99859" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999873">
SMT systems rely on sufficient amount
of parallel corpora to train the trans-
lation model. This paper investigates
possibilities to use word-to-word and
phrase-to-phrase translations extracted
not only from clean parallel corpora
but also from noisy comparable corpora.
Translation results for a Chinese to En-
glish translation task are given.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999485944444445">
Statistical machine translation systems typically
use a translation model trained on bilingual data
and a language model for the target language,
trained on perhaps some larger monolingual data.
Often the amount of clean parallel data is limited.
This leads to the question of whether translation
quality can be improved by using additional nois-
ier bilingual data.
Some approaches, like (Fung and MxKeown,
1997), have been developed to extract word trans-
lations from non-parallel corpora. In (Munteanu
and Marcu, 2002) bilingual suffix trees are used to
extract parallel sequences of words from a com-
parable corpus. 95% of those phrase translation
pairs were judged to be correct. However, no re-
sults where reported if these additional translation
correspondences resulted in improved translation
quality.
</bodyText>
<sectionHeader confidence="0.993848" genericHeader="method">
2 The SMT System
</sectionHeader>
<bodyText confidence="0.999955571428572">
Statistical translation as introduced in (Brown et
al., 1993) is based on word-to-word translations.
The SMT system used in this study relies on multi-
word to multi-word translations. The term phrase
translations will be used throughout this paper
without implying that these multi-word translation
pairs are phrases in some linguistic sense. Phrase
translations can be extracted from the Viterbi
alignment of the alignment model.
Phrase translation pairs are seen only a few
times. Actually, most of the longer phrases are
seen only once in even the larger corpora. Using
relative frequency to estimate the translation prob-
ability would make most of the phrase translation
probabilities 1.0. This would lead to two conse-
quences: First, phrase translation would always
be preferred over a translation generated using
word translations from the statistical and manual
lexicons, even if the phrase translation is wrong,
due to misalignment. Secondly, two translations
would often have the same probability. As the
language model probability is larger for shorter
phrases this will usually result in overall shorter
sentences, which sometimes are too short.
To make phrase translations comparable to the
word translations the translation probability is cal-
culated on the basis of the word translation proba-
bilities resulting from IBM 1-type alignment.
</bodyText>
<equation confidence="0.9450166">
n 1
P(fZIelk) = Ep(filei)
i=m i=k
This now gives the desired property that longer
(1)
</equation>
<page confidence="0.985009">
175
</page>
<bodyText confidence="0.99996275">
translations get higher probabilities. If the addi-
tional word should not be part of the phrase trans-
lation then these additional probabilities kb
which go into the sum will be small, i.e. the phrase
translation probabilities will be very similar and
the language model gives a bias toward the shorter
translation. If, however, this additional word is ac-
tually the translation of one of the words in the
source phrase then the additional probabilities go-
ing into the summation are large, resulting in an
overall larger phrase translation probability.
More importantly, calculating the phrase trans-
lation probability on the basis of word transla-
tion probabilities increases the robustness. Wrong
phrase pairs resulting from errors in the Viterbi
alignment will have a low probability.
</bodyText>
<sectionHeader confidence="0.974604" genericHeader="method">
3 What&apos;s in the Training Data
</sectionHeader>
<subsectionHeader confidence="0.99428">
3.1 The Corpora
</subsectionHeader>
<bodyText confidence="0.999972015151515">
To train the Chinese-to-English translation sys-
tem 4 different corpora were used: 1) Chinese
tree-bank data (LDC2002E17): this is a small
corpus (90K words) for which a tree-bank has
been built. 2) Chinese news stories, collected and
translated by the Foreign Broadcast Information
Service (FBIS). 3) Hong Kong news corpus dis-
tributed through LDC (LDC2000T46). 4) Xinhua
news: Chinese and English news stories publish
by the Xinhua news agency.
The first three corpora are truly bilingual cor-
pora in that the English part is actually a transla-
tion of the Chinese. Together, the form the clean
corpus which has 9.7 million words.
The Xinhua news corpus (XN) is not a paral-
lel corpus. The Chinese and English news sto-
ries are typically not translations of each other.
The Chinese news contains more national news
whereas the English news is more about interna-
tional events. Only a small percentage of all sto-
ries is close enough to be considered as compara-
ble. Identification of these story pairs was done
automatically at LDC using lexical information as
described in (Xiaoyi Ma, 1999). In this approach
a document B is considered an approximate trans-
lation of document A if the similarity between A
and B is above some threshold, where similarity is
defined as the ratio of tokens from A for which a
translation appears in document B in a nearby po-
sition. The document with the highest similarity
is selected. For the Xinhua News corpus less then
2% of the entire news stories could be aligned. In-
spection showed that even these pairs can not be
considered to be true translations of each other.
In our translation experiments we also used the
LDC Chinese English dictionary (LDC2002E27).
This dictionary has about 53,000 Chinese entries
with on average 3 translations each.
The FBIS, Hong Kong news and Xinhua news
corpora all required sentence alignment. Different
sentence alignment methods have been proposed
and shown to give reliable results for parallel cor-
pora. For non-parallel but comparable corpora
sentence alignment is more challenging as it re-
quires — in addition to finding a good alignment —
also a means to distinguish between sentence pairs
which are likely to be translations of each other
and those which are aligned to each other but can
not be considered translations.
An iterative approach to sentence alignment
for this kind of noisy data has been described in
(Bing Zhao, 2002). This approached was used
to sentence align the Xinhua News stories. Sen-
tence length and lexical information is used to
calculate sentence alignment scores. The align-
ment algorithm allows for insertions and dele-
tions. These sentences are removed as are sen-
tence pairs which have a low overall sentence
alignment score. About 30% of the sentence pairs
were deleted to result in the final corpus of 2.7 mil-
lion words.
The test data used in the following analysis
and also in the translation experiments is a set of
993 sentences from different Chinese news wires,
which has been used in the TIDES MT evaluation
in December 2001.
</bodyText>
<subsectionHeader confidence="0.998278">
3.2 Analysis: Vocabulary Coverage
</subsectionHeader>
<bodyText confidence="0.999929625">
To get good translations requires first of all that the
vocabulary of the test sentences is well covered by
the training data. Coverage can be expressed in
terms of tokens, i.e. how many of the tokens in
the test sentences are covered by the vocabulary
of the training corpus, and in terms of types, i.e.
how many of the word types in the test sentences
have been seen in the training data.
</bodyText>
<page confidence="0.998744">
176
</page>
<tableCaption confidence="0.975788666666667">
Table 1: Corpus coverage (C-Voc) and vocabulary
coverage of the test data given different training
corpora.
</tableCaption>
<table confidence="0.9975655">
Corpus Voc C-Coy V-Coy
Clean 46,706 99.51 97.89
Clean + XN 69,269 99.80 98.88
Clean + XN + LDC 74,014 99.84 99.10
</table>
<bodyText confidence="0.999167384615385">
A problem with Chinese is of course that the
vocabulary depends heavily on the word segmen-
tation. In a way the vocabulary has to be deter-
mined first, as a word list is typically used to do
the segmentation. There is a certain trade-off: a
large word list for segmentation will result in more
unseen words in the test sentences with respect to
a training corpus. A small word list will lead to
more errors in segmentation. For the experiments
reported in this paper a word list with 43, 959 en-
tries was used for word segmentation.
Table 1 gives corpus and vocabulary coverage
for each of the Chinese corpora.
</bodyText>
<subsectionHeader confidence="0.998907">
3.3 Analysis: N-gram coverage
</subsectionHeader>
<bodyText confidence="0.999505107142857">
Our statistical translation system uses not only
word-to-word translations but also phrase transla-
tions. The more phrases in the test sentences are
found in the training data, the better. And longer
phrases will generally result in better translations,
as they show larger cohesiveness and better word
order in the target language. The n-gram cover-
age analysis takes all n-grams from the test sen-
tences for n=2, n=3, ... and finds all occurrences
of these n-grams in the different training corpora.
From Table 2 we see that the Xinhua news cor-
pus, which is only about a quarter of the size of
the clean data, contains a much larger number of
long word sequences occurring also in the test
data. This is no surprise, as part of the test sen-
tences come from Xinhua news, even though they
date from a year not included in the training data.
Adding this corpus to the other training data there-
fore gives the potential to extract more and longer
phrase to phrase translations which could result in
better translations.
Many of the detected n-grams are actually over-
lapping, resulting from a very small number of
very long matches was detected. And each n-gram
contains m (n-m+1)-grams. The longest matching
n-grams in the Xinhua news corpus were 56, 53,
43, 34, 31, 28, 24, 21 words long, each occurring
once.
</bodyText>
<tableCaption confidence="0.8479985">
Table 2: Number of n-grams from test sentences
found in the different corpora.
</tableCaption>
<table confidence="0.9817623">
n Clean XN Clean + XN
2 12621 11503 13683
3 6990 6525 8663
4 2396 2735 3628
5 810 1283 1611
6 314 745 884
7 123 486 545
8 53 368 395
9 29 310 321
10 18 275 281
</table>
<subsectionHeader confidence="0.988342">
3.4 Training the Alignment Models
</subsectionHeader>
<bodyText confidence="0.996735428571429">
IBM1 alignments (Brown et al., 1993) and HMM
alignments (Vogel et al., 1996) were trained for
both the clean parallel corpus and for the extended
corpus with the noisy Xinhua News data. The
alignment models were trained for Chinese to En-
glish as well as English to Chinese. Phrase-to-
phrase translations were extracted from the Viterbi
path of the HMM alignment. The reverse align-
ment, i.e. English to Chinese, was used for phrase
pair extraction as this resulted in higher transla-
tion quality in our experiments. The translation
probabilities, however, where calculated using the
lexicon trained with the IBM1 Chinese to English
alignment.
</bodyText>
<tableCaption confidence="0.953136">
Table 3: Training perplexity for clean and clean
plus noisy data.
</tableCaption>
<table confidence="0.997582">
Model Clean Clean + XN
IBM1 123.44 142.85
IBM1-rev 105.72 120.48
HMM 101.34 121.34
HMM-rev 78.61 92.79
</table>
<tableCaption confidence="0.8604915">
Table 3 gives the alignment perplexities for the
different runs. English to Chinese alignment gives
</tableCaption>
<page confidence="0.996821">
177
</page>
<bodyText confidence="0.999951142857143">
lower perplexity than Chinese to English. Adding
the noisy Xinhua news data leads to significantly
higher alignment perplexities. In this situation, the
additional data gives us more and longer phrase
translations, but the translations are less reliable.
And the question is, what is the overall effect on
translation quality.
</bodyText>
<sectionHeader confidence="0.995159" genericHeader="method">
4 Translation Results
</sectionHeader>
<bodyText confidence="0.999998441176471">
The decoder uses a translation model (the LDC
glossary, the IBM1 lexicon, and the phrase trans-
lation) and a language model to find the best trans-
lation. The first experiment was designed to am-
plify the effect the noisy data has on the translation
model by using an oracle language model built
from the reference translations. This language
model will pick optimal or nearly optimal trans-
lations, given a translation model. To evaluate
translation quality the NIST MTeval scoring script
was used (MTeval, 2002). Using word and phrase
translations extracted form the clean parallel data
resulted in an MTeval score of 8.12. Adding the
Xinhua News corpus improved the translation sig-
nificantly to 8.75. This shows that useful transla-
tions have been extracted from the additional noisy
data.
The next step was to test if this improvement is
also possible when using a proper language model.
The language model used was trained on a cor-
pus of 100 million words from the English news
stories published by the Xinhua News Agency be-
tween 1992 and 2001. Unfortunately, the MTe-
val score dropped from 7.59 to 7.31 when adding
the noisy data. Restricting the lexicon, however,
to a small number of high probabilty translations,
thereby reducing the noise in the lexiocn, the score
improved only marginally for the clean data sys-
tem, but considerably for the noisy data system.
The noisy data system then outperformed the clean
data system. These results are summarized in Ta-
ble 4. A t-test run on the sentence level scores
showed that the difference between 7.62 and 7.69
is statistically significant at the 99% level.
</bodyText>
<sectionHeader confidence="0.999382" genericHeader="conclusions">
5 Summary
</sectionHeader>
<bodyText confidence="0.9807895">
Initial translation experiments have shown that us-
ing word and phrase translations extracted from
</bodyText>
<tableCaption confidence="0.997949">
Table 4: Translation results.
</tableCaption>
<table confidence="0.98888325">
System Setup Clean Noisy
LM-Oracle 8.12 8.75
LM-100m 7.59 7.31
LM-100m, lexicon prunded 7.62 7.69
</table>
<bodyText confidence="0.999617777777778">
noisy parallel data can improve translation quality.
A detailed analysis will be carried out to see how
the different training corpora contributed to the
translations. This will include a human evaluation
of the quality of phrase translations extracted from
the noisier data. Next steps will include training
the statistical lexicon on clean data only and us-
ing this to filter the phrase-to-phrase translations
extracted from comparable corpora.
</bodyText>
<sectionHeader confidence="0.999678" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9998029">
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer, &amp;quot;The Mathe-
matics of Statistical Machine Translation: Parame-
ter Estimation,&amp;quot; Computational Linguistics, vol. 19,
no. 2, pp. 263-311,1993.
Pascale Fung and Kathleen McKeown. 1997. A Tech-
nical Word- and Term-Translation Aid Using Noisy
Parallel Corpora across Language Groups. In Ma-
chine Translation, volume 12, numbers 1-2 (Special
issue), Kluwer Academic Publisher, Dordrecht, The
Netherlands, pp. 53-87.
Xiaoyi Ma and Mark Y. Lieberman. 1999. BITS: A
Method for Bilingual Text Search over the Web..
Machine Translation Sutntnit VII.
Dragos Stefan Munteanu and Daniel Marcu. 2002.
Processing Comparable Corpora With Bilingual
Suffi x Trees. Empirical Methods in Natural Lan-
guage Processing , Philadelphia, PA.
NIST MT evaluation kit version 9. Available at:
http://www.nist.gov/speechltests/mtl.
Stephan Vogel, Hermann Ney, and Christoph Tillmann,
HMM-based Word Alignment in Statistical Transla-
tion, in COLING &apos;96: The 16th Int. Conf. on Com-
putational Linguistics, Copenhagen, August 1996,
pp. 836-841.
Bing Zhao and Stephan Vogel, 2002. Adaptive Paral-
lel Sentence Mining from Web Bilingual News Col-
lection. ICDM &apos;02: The 2002 IEEE International
Conference on Data Mining , Maebashi City, Japan,
December 2002.
</reference>
<page confidence="0.997155">
178
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.944535">
<title confidence="0.999996">Using Noisy Bilingual Data for Statistical Machine Translation</title>
<author confidence="0.999953">Stephan Vogel</author>
<affiliation confidence="0.999547">Interactive Systems Lab Language Technologies Institute Carnegie Mellon University</affiliation>
<email confidence="0.991287">vogel+@cs.cmu.edu</email>
<abstract confidence="0.9937699">rely on sufficient amount of parallel corpora to train the translation model. This paper investigates possibilities to use word-to-word and phrase-to-phrase translations extracted not only from clean parallel corpora but also from noisy comparable corpora. Translation results for a Chinese to English translation task are given.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The Mathematics of Statistical Machine Translation: Parameter Estimation,&amp;quot;</title>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<pages>263--311</pages>
<marker>Brown, Pietra, Pietra, Mercer, </marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer, &amp;quot;The Mathematics of Statistical Machine Translation: Parameter Estimation,&amp;quot; Computational Linguistics, vol. 19, no. 2, pp. 263-311,1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
<author>Kathleen McKeown</author>
</authors>
<title>A Technical Word- and Term-Translation Aid Using Noisy Parallel Corpora across Language Groups.</title>
<date>1997</date>
<journal>In Machine Translation,</journal>
<volume>12</volume>
<pages>1--2</pages>
<publisher>Kluwer Academic Publisher,</publisher>
<location>Dordrecht, The</location>
<note>(Special issue),</note>
<marker>Fung, McKeown, 1997</marker>
<rawString>Pascale Fung and Kathleen McKeown. 1997. A Technical Word- and Term-Translation Aid Using Noisy Parallel Corpora across Language Groups. In Machine Translation, volume 12, numbers 1-2 (Special issue), Kluwer Academic Publisher, Dordrecht, The Netherlands, pp. 53-87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoyi Ma</author>
<author>Mark Y Lieberman</author>
</authors>
<title>BITS: A Method for Bilingual Text Search over the Web.. Machine Translation Sutntnit VII.</title>
<date>1999</date>
<marker>Ma, Lieberman, 1999</marker>
<rawString>Xiaoyi Ma and Mark Y. Lieberman. 1999. BITS: A Method for Bilingual Text Search over the Web.. Machine Translation Sutntnit VII.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragos Stefan Munteanu</author>
<author>Daniel Marcu</author>
</authors>
<title>Processing Comparable Corpora With Bilingual Suffi x Trees.</title>
<date>2002</date>
<booktitle>Empirical Methods in Natural Language Processing ,</booktitle>
<location>Philadelphia, PA.</location>
<contexts>
<context position="1062" citStr="Munteanu and Marcu, 2002" startWordPosition="148" endWordPosition="151">omparable corpora. Translation results for a Chinese to English translation task are given. 1 Introduction Statistical machine translation systems typically use a translation model trained on bilingual data and a language model for the target language, trained on perhaps some larger monolingual data. Often the amount of clean parallel data is limited. This leads to the question of whether translation quality can be improved by using additional noisier bilingual data. Some approaches, like (Fung and MxKeown, 1997), have been developed to extract word translations from non-parallel corpora. In (Munteanu and Marcu, 2002) bilingual suffix trees are used to extract parallel sequences of words from a comparable corpus. 95% of those phrase translation pairs were judged to be correct. However, no results where reported if these additional translation correspondences resulted in improved translation quality. 2 The SMT System Statistical translation as introduced in (Brown et al., 1993) is based on word-to-word translations. The SMT system used in this study relies on multiword to multi-word translations. The term phrase translations will be used throughout this paper without implying that these multi-word translati</context>
</contexts>
<marker>Munteanu, Marcu, 2002</marker>
<rawString>Dragos Stefan Munteanu and Daniel Marcu. 2002. Processing Comparable Corpora With Bilingual Suffi x Trees. Empirical Methods in Natural Language Processing , Philadelphia, PA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>NIST MT</author>
</authors>
<title>evaluation kit version 9.</title>
<note>Available at: http://www.nist.gov/speechltests/mtl.</note>
<marker>MT, </marker>
<rawString>NIST MT evaluation kit version 9. Available at: http://www.nist.gov/speechltests/mtl.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>HMM-based Word Alignment in Statistical Translation,</title>
<date>1996</date>
<booktitle>in COLING &apos;96: The 16th Int. Conf. on Computational Linguistics, Copenhagen,</booktitle>
<pages>836--841</pages>
<contexts>
<context position="9607" citStr="Vogel et al., 1996" startWordPosition="1583" endWordPosition="1586">n-grams are actually overlapping, resulting from a very small number of very long matches was detected. And each n-gram contains m (n-m+1)-grams. The longest matching n-grams in the Xinhua news corpus were 56, 53, 43, 34, 31, 28, 24, 21 words long, each occurring once. Table 2: Number of n-grams from test sentences found in the different corpora. n Clean XN Clean + XN 2 12621 11503 13683 3 6990 6525 8663 4 2396 2735 3628 5 810 1283 1611 6 314 745 884 7 123 486 545 8 53 368 395 9 29 310 321 10 18 275 281 3.4 Training the Alignment Models IBM1 alignments (Brown et al., 1993) and HMM alignments (Vogel et al., 1996) were trained for both the clean parallel corpus and for the extended corpus with the noisy Xinhua News data. The alignment models were trained for Chinese to English as well as English to Chinese. Phrase-tophrase translations were extracted from the Viterbi path of the HMM alignment. The reverse alignment, i.e. English to Chinese, was used for phrase pair extraction as this resulted in higher translation quality in our experiments. The translation probabilities, however, where calculated using the lexicon trained with the IBM1 Chinese to English alignment. Table 3: Training perplexity for cle</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Stephan Vogel, Hermann Ney, and Christoph Tillmann, HMM-based Word Alignment in Statistical Translation, in COLING &apos;96: The 16th Int. Conf. on Computational Linguistics, Copenhagen, August 1996, pp. 836-841.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Zhao</author>
<author>Stephan Vogel</author>
</authors>
<title>Adaptive Parallel Sentence Mining from Web Bilingual News Collection.</title>
<date>2002</date>
<booktitle>ICDM &apos;02: The 2002 IEEE International Conference on Data Mining ,</booktitle>
<location>Maebashi City, Japan,</location>
<marker>Zhao, Vogel, 2002</marker>
<rawString>Bing Zhao and Stephan Vogel, 2002. Adaptive Parallel Sentence Mining from Web Bilingual News Collection. ICDM &apos;02: The 2002 IEEE International Conference on Data Mining , Maebashi City, Japan, December 2002.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>